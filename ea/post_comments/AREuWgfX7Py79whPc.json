[{"_id": "sgXzAaM9YzAQBSoHr", "postedAt": "2018-01-10T12:04:57.853Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>This comment is not directly related to your post: I don't think the long-run future should be viewed of as a cause area. It's simply where most sentient beings live (or might live), and therefore it's a potential treasure trove of cause areas (or problems) that should be mined. Misaligned AI leading to an existential catastrophe is an example of a problem that impacts the long-run future, but there are so, so many more. Pandemic risk is a distinct problem. Indeed, there are so many more problems even if you're just thinking about the possible impacts of AI.</p>\n", "parentCommentId": null, "user": {"username": "mhpage"}}, {"_id": "7KntSmgTkahSv64Fv", "postedAt": "2018-01-10T12:18:59.993Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<blockquote>\n<p>Effective altruism has had three main direct broad causes (global poverty, animal rights, and far future), for quite some time.</p>\n</blockquote>\n<p>The whole concept of <em>EA having specific recognizable compartmentalized cause areas and charities</em> associated with it is bankrupt and should be zapped, because it invites stagnation as founder effects entrench further every time a newcomer joins and devotes mindshare to signalling ritual adherence to the narrative of different finite tribal Houses to join and build alliances between or cannibalize, crowding out new classes of intervention and eclipsing the prerogative to optimize everything as a whole without all these distinctions. &quot;Oh, I'm an (animal, poverty, AI) person! X-risk aversion!&quot; </p>\n<p>&quot;Effective altruism&quot; in itself should be a scaleable cause-neutral methodology de-identified from its extensional recommendations. It should stop reinforcing these arbitrary divisions as though they were somehow sancrosanct. The task is harder when people and organizations ostensibly about advancing that methodology settle into the same buildings and object-level positions, or when charity evaluators do not even strive for cause-neutrality in their consumer offerings. Not saying those can't be net-goods, but the effects on homogenization, centralization, and bias all restrict the purview of Effective Altruism.</p>\n<blockquote>\n<p>I have often heard people worry that it\u2019s too hard for a new cause to be accepted by the effective altruism movement.</p>\n</blockquote>\n<p>Everyone here knows there are new causes and wants to accept them, but they don't know that everyone knows there are new causes, etc, a common-knowledge problem. They're waiting for chosen ones to update the leaderboard.</p>\n<p>If the tribally-approved list were opened it would quickly spiral out of working memory bounds. This is a difficult problem to work with but not impossible. Let's make the list and put it somewhere prominent for salient access.</p>\n<p>Anyway, here is an experimental <a href=\"https://www.facebook.com/groups/134391027248879/\">Facebook group</a> explicitly for initial cause proposal and analysis. Join if you're interested in doing these!</p>\n", "parentCommentId": null, "user": {"username": "DonyChristie"}}, {"_id": "H3gRbsW7c5WZNfcx2", "postedAt": "2018-01-11T08:41:36.585Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>I'd also add: Get a group of people together. Easiest way is to create a Facebook Group and promote it. Getting a new cause into EA is a huge amount of work and so you don't want to try to do it single handed. </p>\n", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "Bo5ZfxFyxKnGGGEMu", "postedAt": "2018-01-11T17:57:23.501Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>One thing that's very useful about having separate cause areas is that it helps people decide what to study and research in depth, e.g. get a PhD in. This probably doesn't need to be illustrated, but I'll do it anyway: </p>\n<p>If you consider two fields of study, A and B, such that A has only one promising intervention, and B has two, and all three interventions are roughly equal in expectation (or whatever other measures are important to you); then it would be better to study B, because if one of its two interventions don't pan out, you can more easily switch to the other; with A, you might have to move onto a new field entirely. Studying B actually has higher expected value than studying A, despite all three interventions being equal in expectation. </p>\n", "parentCommentId": "7KntSmgTkahSv64Fv", "user": {"username": "MichaelStJules"}}, {"_id": "ed5NWynw8eumEn5qc", "postedAt": "2018-01-12T00:11:13.859Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>I worry you've missed the most important part of the analysis. If we think what it means for a &quot;new cause to be accepted by the effective altruism movement&quot; that would proably be either:</p>\n<ol>\n<li><p>It becomes a cause area touted by EA organisations like Give Well, CEA, or GWWC. In practice, this involves convincing the leadership of those organisations. If you want to get a new cause in via this route, that's end goal you need to achieve; writing good arguments is a means to that end. </p>\n</li>\n<li><p>you convince individuals EA to change what they do. To a large extent, this also depends on convincing EA-org leadership, because that's who people look to for confirmation a new cause has been vetted. This isn't necessarily stupid on the part of individual EAs to defer to expert judgement: they might think &quot;Oh, well if so and so aren't convinced about X, there's probably a reason for it&quot;.</p>\n</li>\n</ol>\n<p>This seems as good as time as any to re-plug the stuff I've done. I think these mostly meet your criteria, but fail in some key ways.</p>\n<p>I first posted about <a href=\"http://effective-altruism.com/ea/yv/is_effective_altruism_overlooking_human_happiness/\">mental health and happiness</a> 18 months ago and explained why poverty is less effective than most will think and mental health more effective. I think I was, at the time, lacking a particular charity recommendation though (I now think Basic Needs and Strong Minds look like reasonable picks); I agree it's important new cause suggestions have 'shovel ready' project.</p>\n<p>I argued you, whoever you are, probably <a href=\"http://effective-altruism.com/ea/14k/are_you_sure_you_want_to_donate_to_the_against/\">don't want to donate the Against Malaria Foundation</a>. I explain it's probably a mistake for EAs to focus too much on 'saving lives' at the expense of either 'improving lives' or 'saving humanity'.</p>\n<p>Back in August I explain why <a href=\"http://effective-altruism.com/ea/1d8/dpr/\">drug policy reform should be taken seriously as new cause</a>. I agree that lacks a shovel ready project too, but, if anything, I think there was too much depth and rigour there. I'm still waiting for anyone to tell me where my EV calcs have gone wrong and drug policy reform wouldn't be more cost-effective than anything in GiveWell's repertoire. </p>\n", "parentCommentId": null, "user": {"username": "MichaelPlant"}}, {"_id": "aHCahNRipr8ufAD6f", "postedAt": "2018-01-12T06:02:41.101Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<blockquote>\n<p>I'm still waiting for anyone to tell me where my EV calcs have gone wrong</p>\n</blockquote>\n<p>For what it's worth, we had some back &amp; forth regarding modeling assumptions around drug policy reform cost-effectiveness: </p>\n<p><a href=\"http://effective-altruism.com/ea/1em/costeffectiveness_analysis_drug_liberalization/bx1\">http://effective-altruism.com/ea/1em/costeffectiveness_analysis_drug_liberalization/bx1</a></p>\n", "parentCommentId": "ed5NWynw8eumEn5qc", "user": {"username": "Milan_Griffes"}}, {"_id": "GXCsKe9dqotwwkpFx", "postedAt": "2018-01-12T11:08:23.808Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>I remember. I don't think we quite got the bottom of the issue however and couldn't agree what the right counterfactual was. </p>\n", "parentCommentId": "aHCahNRipr8ufAD6f", "user": {"username": "MichaelPlant"}}, {"_id": "z8EbDss9N3NLbrrdv", "postedAt": "2018-01-12T16:05:09.091Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<blockquote>\n<p>I'm still waiting for anyone to tell me where my EV calcs have gone wrong and drug policy reform wouldn't be more cost-effective than anything in GiveWell's repertoire.</p>\n</blockquote>\n<p>One thing I'd note here is that the <em>rigor</em> of GiveWell analysis versus your EV calcs is very different. There are other EV calcs out there with similar rigor that promise significantly higher $/good stuff, such as most stuff in the far future cause-space.</p>\n<blockquote>\n<p>I argued you, whoever you are, probably don't want to donate the Against Malaria Foundation. I explain it's probably a mistake for EAs to focus too much on 'saving lives' at the expense of either 'improving lives' or 'saving humanity'.</p>\n</blockquote>\n<p>I'd also note that GiveWell replied to your argument here: \n<a href=\"https://blog.givewell.org/2016/12/12/amf-population-ethics/\">https://blog.givewell.org/2016/12/12/amf-population-ethics/</a></p>\n", "parentCommentId": "ed5NWynw8eumEn5qc", "user": {"username": "Peter_Hurford"}}, {"_id": "kDvihCpwXmHSk4ocn", "postedAt": "2018-01-12T16:32:42.007Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>Sure, but I don't think the right summary here is &quot;no one has told me how my EV calc is wrong.&quot;</p>\n<p>A better summary probably includes something like &quot;EV calcs are complicated and their outputs are very sensitive to the modeling assumptions used.&quot;</p>\n", "parentCommentId": "GXCsKe9dqotwwkpFx", "user": {"username": "Milan_Griffes"}}, {"_id": "bjZTizvao8yfdbJwa", "postedAt": "2018-01-12T17:43:32.089Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>Yes. I think I was over-selling my point and that was a mistake. Our back and forth was useful and I'll have to think about it again when I look at DPR again.</p>\n<p>By way of explanation, I think I was venting my frustrationg at the ratio of &quot;time I spend researching and writing about drug policy reform:serious interest it received&quot;</p>\n", "parentCommentId": "kDvihCpwXmHSk4ocn", "user": {"username": "MichaelPlant"}}, {"_id": "JTFq26La5Aop3eXJJ", "postedAt": "2018-01-12T17:49:14.436Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<blockquote>\n<p>the rigor of GiveWell analysis versus your EV calcs is very different</p>\n</blockquote>\n<p>Sort of a side question, but could you say what sort of thing you had in mind? i.e. the particular sense in which GW's calc are rigorous. I ask because I find their assumptions odd/pretty disatisfying and think they leave out loads of stuff. I mean to write about when I find time. </p>\n<p>This isn't to say my calculations are more rigorous than theirs. GW have loads more detail.</p>\n", "parentCommentId": "z8EbDss9N3NLbrrdv", "user": {"username": "MichaelPlant"}}, {"_id": "rM8y5xuYBz8Admyoj", "postedAt": "2018-01-13T12:32:59.495Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>I think you're right that having &quot;an organization&quot; talking about X is necessary for X to reach &quot;full legitimacy&quot;, but it's worth pointing out that many pioneers in new areas within EA just started their own orgs (ACE, MIRI etc.) rather than trying to persuade others to support them.</p>\n<p>Having even a nominal &quot;project&quot; allows you to collaborate more easily with others and starts to build credibility that isn't just linked to you. I think perhaps you should just start MH&amp;HR.</p>\n", "parentCommentId": "ed5NWynw8eumEn5qc", "user": {"username": "Michael_PJ"}}, {"_id": "nKuq88CjziNT42Y7S", "postedAt": "2018-01-13T12:35:18.059Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>Great post!</p>\n<p>(&quot;CEA&quot; in the post refers to &quot;cost-effectiveness analysis&quot; \u2013 maybe explain the term the first time you use it? It can be confusing to those who know the Centre for Effective Altruism but not the abbreviation for cost-effectiveness analysis.)</p>\n", "parentCommentId": null, "user": {"username": "Jonas Vollmer"}}, {"_id": "uB7wj9ox8a6RcK5Zg", "postedAt": "2018-01-13T15:14:15.853Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>I'd go farther here and say all three (global poverty, animal rights, and far future) are best thought of as target populations rather than cause areas. Moreover, the space not covered by these three is basically just wealthy modern humans, which seems to be much less of a treasure trove than the other three because WMHs have the most resources, far more than the other three populations. (Potentially there's also medium-term future beings as a distinct population, depending on where we draw the lines.)</p>\n<p>I think EA would probably be discovering more things if we were focused on looking not for new cause areas but for new specific intervention areas, comparable to individual health support for the global poor (e.g. antimalarial nets, deworming pills), individual financial help for the global poor (e.g. unconditional cash transfers), individual advocacy of plant-based eating (e.g. leafleting, online ads), institutional farmed animal welfare reforms (e.g. cage-free eating), technical AI safety research, and general extinction risk policy work.</p>\n<p>If we think of the EA cause area landscape in &quot;intervention area&quot; terms, there seems to be a lot more change happening.</p>\n", "parentCommentId": "sgXzAaM9YzAQBSoHr", "user": {"username": "Jacy"}}, {"_id": "cFaJxak92Y6ZiivE9", "postedAt": "2018-01-13T18:22:09.679Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<blockquote>\n<p>are best thought of as target populations than cause areas ... the space not covered by these three is basically just wealthy modern humans</p>\n</blockquote>\n<p>I guess this thought is probably implicit in a lot of EA, but I'd never quite heard it stated that way. It should be more often!</p>\n<p>That said, I think it's not quite precise. There's a population missing: humans in the not-quite-far-future (e.g. 100 years from now, which I think is not usually included when people say &quot;far future&quot;).</p>\n", "parentCommentId": "uB7wj9ox8a6RcK5Zg", "user": {"username": "davidc"}}, {"_id": "kcxtrJ9diDuP9bGjT", "postedAt": "2018-01-13T19:17:48.236Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>Interesting thoughts, actually...</p>\n<blockquote>\n<p>MH&amp;HR.</p>\n</blockquote>\n<p>What does the R stand for? </p>\n", "parentCommentId": "rM8y5xuYBz8Admyoj", "user": {"username": "MichaelPlant"}}, {"_id": "yJToEhbhDWKqhyRKc", "postedAt": "2018-01-14T00:35:07.000Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>In the same vein as this comment and its replies: I'm disposed to framing the three as expansions of the &quot;moral circle&quot;. See, for example: <a href=\"https://www.effectivealtruism.org/articles/three-heuristics-for-finding-cause-x/\">https://www.effectivealtruism.org/articles/three-heuristics-for-finding-cause-x/</a></p>\n", "parentCommentId": "sgXzAaM9YzAQBSoHr", "user": {"username": "CalebWithers"}}, {"_id": "EmeQtLoRudNLibNrM", "postedAt": "2018-01-15T01:51:43.316Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>So I think we agree on some things and disagree on others. I think that getting large EA organizations to adopt the cause definitely helps but is but is not necessary. Animal rights as a whole, for example, is not mentioned at all on GiveWell or GWWC and it\u2019s listed as a 2nd tier area by 80,000 Hours (bit.ly/2DdxCqQ), but it is still pretty clearly endorsed by EA as a whole. If by EA orgs you mean EA orgs of any size, I do think that most cause areas that are accepted by the EA movement will get organizations started in it in time. I think that causes like wild animal suffering and positive psychology are decent examples of causes that have gotten some traction without major pre-existing organizations endorsing them. It might also come down to disagreements about definitions of \u201cin EA\u201d. </p>\n<p>I almost put your blogs into this post as a positive example of what I wish people would do, but I wanted to keep the post to a lower length. In general, I think your efforts on mental health have updated more than a few EAs in positive directions towards it, including myself. There has been some related external content and research on this topic in part because of your posts and I would put a nontrivial chance on some EAs in the next 1-5 years focusing exclusively on this cause area and starting something in it. In general, I would expect adoption to new causes to be fairly slow and start with small numbers of people and maybe one organization before expanding to be on the standard go-to EA list.</p>\n<p>I think if I were to guess what is holding back mental health / positive psych as a cause area it would be having a really strong concrete charity to donate to. By strong charity, I mean strong CEA but also focus on narrow set of interventions, decent evidence base/track record, strong M&amp;E, and decently investigated by an external EA party (would not have to be an org. Could be an individual.) Something like Strong Minds might be a good fit for this.</p>\n", "parentCommentId": "ed5NWynw8eumEn5qc", "user": {"username": "Joey"}}, {"_id": "uwe4EAg3NPhmDdQEC", "postedAt": "2018-01-15T03:37:04.598Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>Thanks - they make sense. Do you think I followed them <a href=\"http://effective-altruism.com/ea/1g9/should_we_be_spending_no_less_on_alternate_foods/\">here</a>?</p>\n", "parentCommentId": null, "user": {"username": "Denkenberger"}}, {"_id": "JknzefpG59WRmLfRd", "postedAt": "2018-01-16T02:43:44.339Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>I agree with Jacy. Another point I'd add is effective altruism is a young movement also focused on making updates and change its goals as new and better info can be integrated into our thinking. This leads to the evolution of various causes, interventions and research projects in the movement undergoing changes which make them harder to describe.</p>\n<p>For example, for a long time in EA, &quot;existential risk reduction&quot; was associated primarily with AI safety. In the last few years ideas from Brian Tomasik have materialized in the Foundational Research Institute and their focus on &quot;s-risks&quot; (risks of astronomical suffering). At the same time, organizations like Allfed are focused on mitigating existential risks which could realistically happen on a timeline in the medium-term future, i.e., the next few decades, but the intervention themselves aren't as focused on the far-future, e.g., at least the next few centuries. </p>\n<p>However, x-risk and s-risk reduction dominate in EA through AI safety research as the favoured intervention, and with a focus motivated by astronomical stakes. Lumping that altogether could be called a &quot;far future&quot; focus. Meanwhile, 80,000 Hours advocates for the use of the term &quot;long-run future&quot; for a focus on risks extending from the present to the far future which depend on policy regarding all existential risks, including s-risks. </p>\n<p>I think finding accurate terminology for the whole movement to use is a constantly moving target in effective altruism. Obviously using common language optimally would be helpful, but debating and then coordinating usage of common terminology also seems like it'd be a lot of effort. As long as everyone is roughly aware of what each other is talking about, I'm unsure how much of a problem this is. It seems professional publications out of EA organizations, as longer reports which can afford the space to define terms, should do so. The EA Forum is still a blog, so that it's regarded as lower-stakes, I think it makes sense to be tolerant of differing terminology, although of course clarifications or expansions upon definitions should be posted to the comments, as above. </p>\n", "parentCommentId": "sgXzAaM9YzAQBSoHr", "user": {"username": "Evan_Gaensbauer"}}, {"_id": "BFgyt3MNQxFQk7c9H", "postedAt": "2018-01-16T03:00:04.684Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>I think this is missing some prior steps as to how a cause can be built up in the effective altruism movement. For example, a focus on risks of astronomical future suffering (&quot;s-risks), and reducing wild animal suffering (RWAS), both largely inspired in EA by Brian Tomasik's work, have found success in the German-speaking world and increasingly globally throughout the movement. These are causes which have both have largely circumvented attention from either the Open Philanthropy Project (Open Phil) or the Centre for Effective Altruism (CEA) and its satellite projects (e.g., GWWC, 80,000 Hours, etc.). </p>\n<p>Since the beginning of effective altruism, global poverty alleviation and global health have been the biggest focus areas. I witnessed as the movement grew causes were developed through a mix of online coordination on the global level with social networks like Facebook, mailing lists, and fora like LessWrong, and locally or regionally with non-profit organizations focusing on outreach and research. This was the case for both AI safety and farm animal welfare, which proportionally didn't have nearly the representation in EA five years ago that they have now. </p>\n<p>Certainly smaller focus areas like s-risk reduction and RWAS are receiving much less attention than others in EA. However, that across multiple organizations each of those causes is respectively funded by between $100k and $1 million USD, largely from individual effective altruists, is proof of concept a cause can be built up without being touted by CEA or Open Phil. And what's more it's not as if the trajectory of these causes looks bleak. They've been building up growth momentum for years, and they're not showing signs of slowing. So how much they achieve increasing success in the near future will provide more data about what's possible in getting a new cause into EA. What's more, at least RWAS is a cause that's on Open Phil's radar. So it's not like grants or endorsements of these causes from Open Phil or CEA couldn't happen in the future. </p>\n<p>In general I think developing a cause within the effective altruism community is something which often precedes more focus from it by flagship organizations of the movement, and that the process of development often takes the form of following the kinds of steps Joey outlined above. Obviously there could be more to the process than just that. I'm working on a post to introduce a project which builds on the kinds of steps Joey pointed out, and you've already taken, to organize and coordinate causes in effective altruism.</p>\n", "parentCommentId": "ed5NWynw8eumEn5qc", "user": {"username": "Evan_Gaensbauer"}}, {"_id": "6yJYXTpesFsGLt6B6", "postedAt": "2018-01-16T03:07:28.953Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>I made this same point in the <a href=\"https://www.facebook.com/groups/effective.altruists/permalink/1585281791528172/?comment_id=1586918678031150&amp;comment_tracking=%7B%22tn%22%3A%22R0%22%7D\">'Effective Altruism' Facebook group</a> a while ago if anyone wanted to follow for other public conversation on the topic. I wonder if a post on the EA Forum summarizing these kinds of points and requesting evaluations or reviews of charities based on effective positive psychology interventions rigorously implemented would be a good idea.</p>\n", "parentCommentId": "EmeQtLoRudNLibNrM", "user": {"username": "Evan_Gaensbauer"}}, {"_id": "RZBZLpdHcQvget8dY", "postedAt": "2018-01-16T15:58:12.611Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>I think this is a good point; you may also be interested in Michelle's post about <a href=\"http://effective-altruism.com/ea/rw/causes_in_effective_altruism/\">beneficiary groups</a>, my comment about <a href=\"http://effective-altruism.com/ea/rw/causes_in_effective_altruism/65c\">beneficiary subgroups</a>, and Michelle's follow-up about <a href=\"http://effective-altruism.com/ea/s0/finding_more_effective_causes/\">finding more effective causes</a>.</p>\n", "parentCommentId": "uB7wj9ox8a6RcK5Zg", "user": {"username": "Daniel_Dewey"}}, {"_id": "wP2R8SSkg9HEJpYbp", "postedAt": "2018-01-18T21:53:58.890Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>&quot;Mental Health and Happiness Research&quot;. Coin your own meaningless acronym if you don't like it :)</p>\n", "parentCommentId": "kcxtrJ9diDuP9bGjT", "user": {"username": "Michael_PJ"}}, {"_id": "RgBQgvAW86FJwkNsj", "postedAt": "2018-01-29T15:56:43.296Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>I think I'd broadly model rigor <a href=\"http://effective-altruism.com/ea/vo/expected_value_estimates_you_can_maybe_take/\">on a framework like this</a> as the standard deviation of the estimate of cost-effectiveness when using a X% credibility interval (where X% is consistent across all compared intervals). Models with lower standard deviations can be said to be more rigorous as there are less (known) sources of uncertainty.</p>\n", "parentCommentId": "JTFq26La5Aop3eXJJ", "user": {"username": "Peter_Hurford"}}, {"_id": "aJNt4uthwan5FQvWF", "postedAt": "2018-03-16T18:42:41.002Z", "postId": "AREuWgfX7Py79whPc", "htmlBody": "<p>The people at 80kh etc. probably have their hands full. Therefore, even though your post making the case for mental health was laudable, I can well imagine it might not result in action in the short term on their part because of heavy prioritization. </p>\n<p>If one wants to make substantive case and roadmap for possible actions for MH, it might make sense to take the initiative and do it oneself or together with a group of interested people. Given there is enough credence for the case, this effort might lead to formation of a new EA-aligned MH organization. I for one, might be interested in helping out with making the case for MH</p>\n", "parentCommentId": "ed5NWynw8eumEn5qc", "user": {"username": "tuukkasarvi"}}]