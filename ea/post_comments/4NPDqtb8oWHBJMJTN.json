[{"_id": "oH3a5NSmCyDarh2AR", "postedAt": "2023-09-23T10:31:38.575Z", "postId": "4NPDqtb8oWHBJMJTN", "htmlBody": "<p>I'm going to not directly answer your question, but if you do want a suggestion I'd recommend Stuart Russell's book <a href=\"https://people.eecs.berkeley.edu/~russell/hc.html\"><i>Human Compatible</i></a><i>. </i>Very readable, includes AI history as well as arguments for being concerned about risk, and Russell literally (co)-wrote the textbook on AI so has impeccable credentials.</p><blockquote><p>so I hear about AI and how all the funding and the community is now directed towards AI and how it is the most impactful thing.</p></blockquote><p>Can I ask where you heard this from? Because the evidence we have is that <a href=\"https://forum.effectivealtruism.org/posts/ZbaDmowkXbTBsxvHn/historical-ea-funding-data\">this is not true</a> in terms of funding. AI Safety has become seen as increasingly more impactful, but there's plenty of disagreement in the community about how impactful it actually is.</p><blockquote><p>As an EA raised in the Oxford tradition, I have an urge to defer, but rationally I am not convinced.</p></blockquote><p>Don't defer!! If you've done some initial research and reading, and you're not convinced, then it's absolutely fine to not be convinced!</p><p>Given that you've said that you're a non-technical EA who wants to do the most good but isn't inspired/convinced by AI, then don't force yourself into the field of AI! What field would you like to work in, what are your unique skills and experience? Then look at if you can apply them to any number of EA cause areas rather than technical AI safety.</p>", "parentCommentId": null, "user": {"username": "JWS"}}, {"_id": "KPXkEDgeYFA5jfrBz", "postedAt": "2023-09-25T07:48:48.491Z", "postId": "4NPDqtb8oWHBJMJTN", "htmlBody": "<p>Thank you very much for the evidence about the funding. OpenPhil has caught up remarkably and I expect many more donors towards longtermism in the future ; GiveWell is excellent but it remains one source and the likelihood that it decreases/doesn't infuse as much as before remains since it's more difficult to get funding when there is only one source of funding.&nbsp;</p><p>I was indeed wrong to say that longtermism was the most financed area; however, I wouldn't be surprised if this data changed very fast and the trend reversed next year, given the current circumstances of pushing from the top and hallo effect around longtermism right now.&nbsp;</p><p>I don't want to force myself, but as a community builder, I have to take the leap. Hence my need to understand better how I can get people on board with this.</p>", "parentCommentId": "oH3a5NSmCyDarh2AR", "user": {"username": "Vaipan"}}, {"_id": "ffM2FQJf3id9hgi49", "postedAt": "2023-09-25T18:08:46.402Z", "postId": "4NPDqtb8oWHBJMJTN", "htmlBody": "<p>I'm open to there being new evidence on funding, but I'd also want to make a distinction between existential risk and longtermism as reasons for funding. I could reject the 'Astronomical Waste' argument and still think that preventing the worst impacts of Nuclear War/Climate Change from affecting the current generation held massive moral value and deserved funding.</p><p>As for being a community builder, I don't have experience there, but I guess I'd make some suggestions/distinctions:</p><ul><li>If you have a co-director for the community in question who is more AI-focused, perhaps split responsibilities along cause area lines</li><li>Be open about your personal position (i.e. being unpursuaded about the value of AI risk) but separate that from being a community builder where you introduce the various major cause areas (including AI) and present the arguments for and against it</li></ul><p>I don't think you should have to update or defer your own views in order to be a community builder at all, and I'd encourage you to hold on to that feeling of being unconvinced</p><p>Hope that helps! :)</p>", "parentCommentId": "KPXkEDgeYFA5jfrBz", "user": {"username": "JWS"}}, {"_id": "GSYi7E25Qh65Ej2eC", "postedAt": "2023-09-26T13:57:40.117Z", "postId": "4NPDqtb8oWHBJMJTN", "htmlBody": "<blockquote><p>However I still find myself reluctant to put AI as my priority despite knowing these things.</p></blockquote><p>One way out is to simply not put AI as your own, personal, priority (vs say \"the wider EA community's priority\", a separate question altogether). 80,000 Hours' <a href=\"https://80000hours.org/problem-profiles/\">problem profiles page</a> for instance explicitly says that their list of most pressing world problems, where AI risk features at the top, is&nbsp;</p><blockquote><p>ranked roughly by our guess at the expected impact of an additional person working on them, assuming your ability to contribute to solving each is similar</p></blockquote><p>which is already an untrue assumption, <a href=\"https://80000hours.org/articles/problem-framework/#how-to-assess-personal-fit\">as they clarify</a> in their problem framework:</p><blockquote><p>While personal fit is not assessed in our problem profiles, it is relevant to your personal decisions. If you enter an area that you find totally demotivating, then you\u2019ll have almost no impact.&nbsp;</p></blockquote><p>Given the ostensible reluctance in your post, I'm not sure that you <i>yourself</i> should make AI safety work your top priority (although you can still e.g. donate to the <a href=\"https://www.givingwhatwecan.org/charities/long-term-future-fund\">Long-Term Future Fund</a>, one of <a href=\"https://www.givingwhatwecan.org/best-charities-to-donate-to-2023#top-rated-funds-creating-a-better-future\">GWWC's top recommendations</a> in this area, and read <a href=\"https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/\">Holden's writing</a> and discuss it with others, and so on, none of which require such drastic re-prioritization). &nbsp;</p><p>Also, since other commenters / answerers will likely supply materials in support of prioritizing AI safety, for the sake of good epistemics I think it's worth signal-boosting a good critique of it, so consider checking out Nuno Sempere's <a href=\"https://forum.effectivealtruism.org/posts/L6ZmggEJw8ri4KB8X/my-highly-personal-skepticism-braindump-on-existential-risk\">My highly personal skepticism braindump on existential risk from artificial intelligence</a>.</p>", "parentCommentId": null, "user": {"username": "Mo Nastri"}}]