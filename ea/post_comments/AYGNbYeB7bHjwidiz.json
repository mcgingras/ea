[{"_id": "7XEte6azKG3owZJFw", "postedAt": "2023-08-24T01:38:16.639Z", "postId": "AYGNbYeB7bHjwidiz", "htmlBody": "<p>I think that basically all of these are being pursued and many are good ideas. I would be less put off if the post title was 'More people should work on aligning profit incentives with alignment research', but suggesting that no one is doing this seems off base.<br><br>This is what I got after a few minutes of Google search (not endorsing any of the links beyond that they are claiming to do the thing described).<br><br>AI Auditing:<br><a href=\"https://www.unite.ai/how-to-perform-an-ai-audit-in-2023/\">https://www.unite.ai/how-to-perform-an-ai-audit-in-2023/</a><br><br>Model interpretability:<br><a href=\"https://learn.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability?view=azureml-api-2\">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability?view=azureml-api-2</a><br><br>Monitoring and usage:<br><a href=\"https://www.walkme.com/lpages/shadow-ai/?t=1&amp;PcampId=7014G000001ya0pQAA&amp;camp=it_shadow-ai_namer&amp;utm_source=it_shadow_ai&amp;utm_medium=paid-search_google&amp;utm_content=walkme_ai&amp;utm_campaign=it_shadow-ai_namer&amp;utm_term=paid-media&amp;gclid=Cj0KCQjw3JanBhCPARIsAJpXTx69aVdhkJkHOpEQd4_Bfpp_9_93hQM8NVTWkfZU8eR15VU--34lCKMaAkUUEALw_wcB\">https://www.walkme.com/lpages/shadow-ai/?t=1&amp;PcampId=7014G000001ya0pQAA&amp;camp=it_shadow-ai_namer&amp;utm_source=it_shadow_ai&amp;utm_medium=paid-search_google&amp;utm_content=walkme_ai&amp;utm_campaign=it_shadow-ai_namer&amp;utm_term=paid-media&amp;gclid=Cj0KCQjw3JanBhCPARIsAJpXTx69aVdhkJkHOpEQd4_Bfpp_9_93hQM8NVTWkfZU8eR15VU--34lCKMaAkUUEALw_wcB</a><br><br>Future Endowment Fund sounds a lot like an impact certificate:<br>https://forum.effectivealtruism.org/posts/4bPjDbxkYMCAdqPCv/manifund-impact-market-mini-grants-round-on-forecasting</p>", "parentCommentId": null, "user": {"username": "dan.pandori"}}, {"_id": "DuxowM39pCeK5EHFu", "postedAt": "2023-08-24T21:05:04.808Z", "postId": "AYGNbYeB7bHjwidiz", "htmlBody": "<p><strong>Executive summary</strong>: The post argues that aligning AI with human values requires changing profit incentives, and proposes several ideas to create businesses incentivized to develop beneficial AI alignment.</p><p><strong>Key points</strong>:</p><ol><li>Alignment research is constrained by limited nonprofit funding instead of market incentives.</li><li>Companies that audit AI for safety/security issues could be profitable and build expertise.</li><li>Firms could offer alignment consultation, training, red teaming, and evaluation services.</li><li>New strategies to align AI could be sold as proprietary products to companies.</li><li>An endowment fund could provide equity in alignment innovations, reimbursing researchers.</li><li>Market-driven approaches may steer alignment methods in beneficial directions.</li></ol><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. </i><a href=\"https://forum.effectivealtruism.org/contact\"><i>Contact us</i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}]