[{"_id": "9XT3sqp2B5fWHKN86", "postedAt": "2022-09-28T18:56:06.814Z", "postId": "HsDMguLtdhFP46GQ8", "htmlBody": "<p>Really cool topic, thanks for sharing. One of the ways that alignment techniques could gain adoption and reduce the alignment tax is by integrating them into popular open source libraries.</p><p>&nbsp;For example, the library <a href=\"https://github.com/lvwerra/trl\">TRL</a> allows researchers to implement RLHF techniques which can benefit safety, but can also contribute to dangerous capabilities. On the other hand, I'm not aware of any open-source implementation of the techniques described in <a href=\"https://arxiv.org/pdf/2202.03286.pdf\">Red Teaming LMs with LMs</a>, which could be used to filter or fine-tune the outputs of a generative language model.&nbsp;</p><p>Hopefully we'll see more open source contributions of safety techniques, which could bring more interest to safety topics. Some might argue that implementing safety techniques in current models doesn't reduce x-risk, and they're probably right that current models aren't directly posing x-risks, but early adoption of safety techniques seems useful for ensuring further adoption in the years to come.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Aidan O'Gara"}}, {"_id": "a7nhGkWESHWoPYmuC", "postedAt": "2022-09-28T20:20:07.611Z", "postId": "HsDMguLtdhFP46GQ8", "htmlBody": "<p>Thanks for posting this, this seems like valuable work.</p>\n<p>I'm particularly interested in using MLOSS to intentionally shape AI development. For example, could we identify key areas where releasing particular MLOSS can increase safety or extend the time to AGI?</p>\n<p>Finding ways to guide AI development towards narrow and simple AI models can extend AI timelines, which is complimentary to safety work:</p>\n<p><a href=\"https://www.lesswrong.com/posts/BEWdwySAgKgsyBzbC/satisf-ai-a-route-to-reducing-risks-from-ai\">https://www.lesswrong.com/posts/BEWdwySAgKgsyBzbC/satisf-ai-a-route-to-reducing-risks-from-ai</a></p>\n<p>In your opinion, what traits of a particular piece of MLOSS determine whether it increases or decreases risk?</p>\n", "parentCommentId": null, "user": {"username": "harsimony"}}, {"_id": "gMrmRJBwsz3wGFben", "postedAt": "2022-09-28T22:03:52.978Z", "postId": "HsDMguLtdhFP46GQ8", "htmlBody": "<p>New open source implementation of factored cognition / IDA techniques from Ought! <a href=\"https://www.lesswrong.com/posts/X5L9g4fXmhPdQrBCA/a-library-and-tutorial-for-factored-cognition-with-language\">https://www.lesswrong.com/posts/X5L9g4fXmhPdQrBCA/a-library-and-tutorial-for-factored-cognition-with-language</a></p>", "parentCommentId": "9XT3sqp2B5fWHKN86", "user": {"username": "Aidan O'Gara"}}]