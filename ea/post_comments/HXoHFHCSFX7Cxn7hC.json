[{"_id": "ujeo4CpiGBeMcWfLf", "postedAt": "2023-07-25T12:56:22.038Z", "postId": "HXoHFHCSFX7Cxn7hC", "htmlBody": "<p>Can someone give me the TLDR on the implications of these results in light of the fact that Samotsvety's group seemingly/perhaps had much higher odds for AI catastrophe? I didn't read the exact definitions they used for catastrophe, but:<br><br>Samotsvety's group (n=13) gave \"What's your probability of misaligned AI takeover by 2100, barring pre-<a href=\"https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit%23heading%3Dh.14onymzb0y9\"><u>APS-AI</u></a>&nbsp;catastrophe?\" at 25%<br>(source https://forum.effectivealtruism.org/posts/EG9xDM8YRz4JN4wMN/samotsvety-s-ai-risk-forecasts)<br><br>Whereas XPT gave<br><u>\"</u><a href=\"https://docs.google.com/document/d/1CAmw1g_Y3siZGZaaYJjMRjEIyV4HURhCZf5rZ4a6-d8/edit\"><u>AI Catastrophic risk</u></a>&nbsp;(&gt;10% of humans die within 5 years)\" for year 2100 at 2.13%<br><br>Without having read the exact definitions for \"misaligned AI takeover\" and still knowing that Samotsvety's prediction was conditional on pre-<a href=\"https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit%23heading%3Dh.14onymzb0y9\"><u>APS-AI</u></a>&nbsp;catastrophe not happening, this still seems like a very large discrepancy. I know that Samotsvety's group was a much smaller n. n=13 vs n=88. How much weight should we give to Samotsvety's group's other predictions on AI timelines given the discrepancy in the risk prediction likelihoods?</p>", "parentCommentId": null, "user": {"username": "spreadlove5683"}}, {"_id": "ZqNyoaqLzieSJd7js", "postedAt": "2023-07-25T14:29:54.193Z", "postId": "HXoHFHCSFX7Cxn7hC", "htmlBody": "<p>Good question.</p><p>There's a little bit on how to think about the XPT results in relation to other forecasts <a href=\"https://forum.effectivealtruism.org/posts/K2xQrrXn5ZSgtntuT/what-do-xpt-forecasts-tell-us-about-ai-risk-1#The_forecasts_in_context\">here</a> (not much). Extrapolating from there to Samotsvety in particular:</p><ul><li>Reasons to favour XPT (superforecaster) forecasts:<ul><li>Larger sample size</li><li>The forecasts were incentivised (via reciprocal scoring, a bit more detail <a href=\"https://forum.effectivealtruism.org/posts/K2xQrrXn5ZSgtntuT/what-do-xpt-forecasts-tell-us-about-ai-risk-1#Forecasts_from_the_top_reciprocal_scoring_quintile\">here</a>)</li><li>The most accurate XPT forecasters in terms of reciprocal scoring also gave the lowest probabilities on AI risk (and &nbsp;reciprocal scoring accuracy may correlate with actual accuracy)</li></ul></li><li>Speculative reasons to favour Samotsvety forecasts:<ul><li>(Guessing) They've spent longer on average thinking about it</li><li>(Guessing) They have deeper technical expertise than the XPT superforecasters</li></ul></li></ul><p>I also haven't looked in detail at the respective resolution criteria, but at first glance the forecasts also seem relatively hard to compare directly. (I agree with you though that the discrepancy is large enough that it suggests a large disagreement were the two groups to forecast the same question - just expect that it will be hard to work out how large.)</p>", "parentCommentId": "ujeo4CpiGBeMcWfLf", "user": {"username": "rosehadshar"}}, {"_id": "hr9fRet3aeuFzaube", "postedAt": "2024-03-28T15:17:55.494Z", "postId": "HXoHFHCSFX7Cxn7hC", "htmlBody": "<p>Hi Rose,</p><p>The values for the standard deviation of the AI extinction risk seem to high. For example, the median and maximum AI extinction risk until the end of 2030 by superforecasters are 10^-6 and 10 % (pp. 269 and 272), and therefore the standard deviation has to be lower than 10 % (= 0.1 - 10^-6), but you report a value of 2.6 (p. 269). Maybe 2.6 is the standard deviation as a fraction of the mean (i.e. the <a href=\"https://en.wikipedia.org/wiki/Coefficient_of_variation\">coefficient of variation</a>)?</p>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}]