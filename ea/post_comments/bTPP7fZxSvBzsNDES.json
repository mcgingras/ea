[{"_id": "gSB7kKp4vB5vAcQfK", "postedAt": "2023-07-10T18:06:44.577Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>I will give you two arguments in the opposite direction:</p>\n<ol>\n<li>\n<p>we are the most moral of all living species, and we are the dominating one. Morality imply resources for social coordination.</p>\n</li>\n<li>\n<p>we expect advanced societies to be right in Linear Algebra, why not in Ethics?</p>\n</li>\n</ol>\n", "parentCommentId": null, "user": {"username": "Arturo Macias"}}, {"_id": "2t8Bye5P2dgM2a6Bs", "postedAt": "2023-07-10T19:34:52.855Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<blockquote><p>We are the most moral of all living species, and we are the dominating one. Morality imply resources for social coordination.</p></blockquote><p>Morality does imply resources for social coordination. However, this doesn't imply universal moral values. Your values can still imply neglecting of fighting against an out-group.</p><p>For instance, when you take the way our current industrial society is treating non-human animals in factory farms, one could argue that this is deeply immoral, on all accounts, and I'd expect many non-industrial human societies to find that horrific.&nbsp;</p><p>So I'm really not certain that we are the most moral civilization.</p><p><br>Another example: companies and the financial system can be argued to be the most powerful actors today - very optimized to be \"grabby\" and powerful. But few people would seem theses structures very moral.</p>", "parentCommentId": "gSB7kKp4vB5vAcQfK", "user": {"username": "Corentin Fressoz"}}, {"_id": "LLpnTu8CezTeW8856", "postedAt": "2023-07-10T19:41:10.341Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>Interesting !</p><p>A similar process seems to be at the root of why our political system is deeply flawed and few people are satisfied with it.</p><p>It's because politicians who want to contribute to the common good are <i>less competitive </i>compared to to those that lie, use fallacies, or keep things secret.&nbsp;</p><p>The article is here <a href=\"https://siparishub.medium.com/a-new-strategy-for-ambitious-environmental-laws-e4a403858fbd\">https://siparishub.medium.com/a-new-strategy-for-ambitious-environmental-laws-e4a403858fbd</a>&nbsp;</p><p>Here's a similar article for why the economic system just isn't geared to solve the environmental crisis : <a href=\"https://siparishub.medium.com/the-economic-system-is-an-elm-1072fe3399bf\">https://siparishub.medium.com/the-economic-system-is-an-elm-1072fe3399bf</a>&nbsp;</p><p>(the full series is <a href=\"https://siparishub.medium.com/a-counterintuitive-response-to-the-environmental-crisis-83dc6a92f5fa\">here</a>)</p>", "parentCommentId": null, "user": {"username": "Corentin Fressoz"}}, {"_id": "Rht5ihwWuJs6nwaEw", "postedAt": "2023-07-10T19:58:58.597Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>I argue that the capitalist system is the most moral social system ever, among other things because it has become universalist. Humans have created extreme competitive pressure towards social coordination. To some extent this is either \u201cdie or convert\u201d. Globalization, Pax Democrstica or Human Rights ideology is on one hand a form of moral circle enlargement, on the other hand, a kind of Borg.</p>\n<p>But given the alternatives, I happily support the Borg\u2026</p>\n", "parentCommentId": "2t8Bye5P2dgM2a6Bs", "user": {"username": "Arturo Macias"}}, {"_id": "WByjdCDPrZLQyqm4c", "postedAt": "2023-07-10T20:39:16.978Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>Hi Jim, thanks for this post \u2014 I enjoyed reading it!</p><p>I agree that the Upside-Focused Colonist Curse is an important selection effect to keep in mind. &nbsp;Like you, I'm uncertain as to just how large of an effect it'll turn out to be, so I'm especially excited to see empirical work that tries to estimate its magnitude. &nbsp;Regardless, though I'm glad that people are working on this!</p><p>I wanted to push back somewhat, though, on the first potential implication that you draw out \u2014 that the existence of the UCC diminishes the importance of x-risk reduction, since on account of the UCC, whether humans control their environment (planet, solar system, lightcone, etc.) is unlikely to matter significantly. &nbsp;As I understand it, the argument goes like this:</p><ol><li>If humans fail to take control of their environment (due, presumably, to an existential catastrophe), another \"grabby\" civilization will likely take control of it instead.</li><li>If humans <i>do </i>take control of their environment, they'll have become grabby.</li><li>If humans become grabby, their values are unlikely to differ significantly from the values of the civilization that would've controlled it instead.</li><li>So, whether humans take control of their environment or not is unlikely to make much of a difference to how things go in that environment, morally.</li></ol><p>While I agree that the existence of selection effects on our future values diminishes the importance of x-risk reduction somewhat, I think (4) is far too strong of a conclusion to draw. &nbsp;This is because, while I'm happy to grant (1) and (2), (3) seems unsupported.</p><p>In particular, for (3) to go through, it seems like it would need to be the case that (a) selection pressures in favor of grabby values are very strong, such that they are \"major players\" in determining the long-term trajectory of a civilization's values; &nbsp;and (b) under the influence of these selection pressures, civilizations beginning with very different value systems converge on a relatively small region in \"values space\", such that there aren't morally significant differences between the values they converge on. &nbsp;I find both of these relatively implausible:</p><ol><li>Naively, I don't expect that the relevant selection pressures will be that strong, for two reasons. &nbsp;First, without empirical evidence, my prior credence in <i>any</i> factor being the dominant consideration that shapes our long-term future is low: predicting the long-term future is really hard, and without evidence-based estimates of an effect's size (for instance, that it successfully post-dicts many important facts about our historical trajectory), I'm reluctant to think that it's extremely large. &nbsp;As such, as of now, I expect that other factors will play large roles in shaping our, and other civilizations', future trajectories, such that the mere existence of selection pressures in favor of grabby values isn't sufficient to underwrite strong claims about how our descendants will compare to other civilizations'. &nbsp;(That said, like I say above, I'm excited to see work that aims to estimate their size, and I'm open to changing my mind on this point.)</li><li>Moreover, even in the face of strong selection pressure, systems don't seem to converge on similar equilibria in general. &nbsp;For instance, both biological and cultural evolution seem to yield surprisingly diverse results, even in the face of relatively uniform, strong selection pressures. &nbsp;In cases of artificial optimization, we seem to see the same thing (see McCoy (<a href=\"https://arxiv.org/abs/1911.02969\">2020</a>), for instance). &nbsp;So, even if selection pressures in favor of grabby values were extremely strong, I wouldn't naively expect them to eliminate morally relevant differences between humans and other civilizations.</li></ol><p>These points aside, though, I want to reiterate that I think the main point of the post is very interesting and a potentially important consideration \u2014 thanks again for the post!</p>", "parentCommentId": null, "user": {"username": "Chinmay Deshpande"}}, {"_id": "mXMHuxcvQoBTgYAjZ", "postedAt": "2023-07-11T05:48:48.099Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>I'm curious. How exactly do you explain our current treatment of animals, if we are in the most moral social system ever ?</p><p>&nbsp;</p><p>(I'm talking about what the majority does, not about the fact that some people here take the topic seriously)</p>", "parentCommentId": "Rht5ihwWuJs6nwaEw", "user": {"username": "Corentin Fressoz"}}, {"_id": "gxERLPrFCWcofTRxD", "postedAt": "2023-07-11T06:12:49.756Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>Oh, because all other species and social systems are even worse.&nbsp;</p><p>The degree of concern for others never extend beyond kin and at most peer group in the rest of beings. We have gone in 10.000 years from \"kin and tribe\" to \"social class\" and \"nation\" and currently we have so much moral progress that you can own equity 10.000 kms away from home and being able to collect the dividend.&nbsp;</p><p>Larger and larger reciprocity clusters have developed in centuries even decades, from NATO to World Trade Organization. Women (women!) can own property and even rule countries.</p><p>We are a few nuclear strikes away from losing everything. But so far, it is amazing how a system of beliefs on beliefs can impose itself so much on the natural brutality of life and matter.</p>", "parentCommentId": "mXMHuxcvQoBTgYAjZ", "user": {"username": "Arturo Macias"}}, {"_id": "cfzmjTH3oymXgGbLP", "postedAt": "2023-07-11T07:20:43.374Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>Well, I'd rather argue that the moral circle has widened on some parts (humans) but not on others (animals).&nbsp;</p><p>(although I know some people who might disagree - several features of our current industrial civilization would be viewed as pretty immoral by many cultures: widespread inequality, private property since it strenghtens inequality, merchandization, environmental destruction...)</p><p>But when you include animals, I'm unconvinced that other systems are worse. The treatment of factory farmed animals is of a degree of brutality and cruelty rarely heard of in other cultures.</p><p>&nbsp;</p><p>For instance, the number of <a href=\" https://www.statista.com/chart/28584/gcs-vegetarianism-countries-timeline/\">vegetarians in India</a> has declined over time: this sounds like a lower consideration given to animals.</p>", "parentCommentId": "gxERLPrFCWcofTRxD", "user": {"username": "Corentin Fressoz"}}, {"_id": "qhgpW7St3uWa7tLzK", "postedAt": "2023-07-11T07:42:14.001Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>\"But when you include animals, I'm unconvinced that other systems are worse.\"</p><p>The \"net impact\" of industrial civilization so far, when considering animals looks still net negative. But that is compatible with massive moralization of human behaviour. Simply, our capabilities have allowed us to exploit with incredible efficiency to those out of the moral circle while expanding (at incredible speed) the moral circle.&nbsp;</p><p>In the XVII century, empowered and emancipated european civilization were human net negative (but Europe net positive) by creating the transatantic slave trade. In my view by the middle XIX, the progressive western civilization has become \"human net positive\", and exponentially, but when including animals, still the developed/industrial human civilization (today the West is only a part of it) is likely \"net negative\".</p><p>Nothing of this change the fact that empowerment and moralization have grown together at incredible speed in the last 500 years.</p>", "parentCommentId": "cfzmjTH3oymXgGbLP", "user": {"username": "Arturo Macias"}}, {"_id": "rDC5thMfNHbowTZco", "postedAt": "2023-07-11T10:45:38.326Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>Thanks for giving arguments pointing the other way! I'm not sure #1 is relevant to our context here, but #2 is definitely worth considering. In <a href=\"https://forum.effectivealtruism.org/s/wmqLbtMMraAv5Gyqn/p/hat6TafzAoDx97N6j\">the second post of the present sequence</a>, I argue that something like #2 probably doesn't pan out, and we discuss an interesting counter-argument in <a href=\"https://forum.effectivealtruism.org/posts/hat6TafzAoDx97N6j/what-the-moral-truth-might-be-makes-no-difference-to-what?commentId=qKLdydHJ9cBZXTFfZ\">this comment thread</a>.</p>", "parentCommentId": "gSB7kKp4vB5vAcQfK", "user": {"username": "Jim Buhler"}}, {"_id": "ibffaDG8GtKv9rTWs", "postedAt": "2023-07-11T13:06:34.494Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>Hmm, ok, I can get that.</p><p>However, it seems likely to me that a part of the recent improvements you quote were highly linked to the industrial revolution, and that moral progress alone wasn't enough to trigger that. It's easier to get rid of slaves when you have machines replacing manual labour at a cheap price.&nbsp;</p><p>Same for feminism - I recently attended to a conference in French titled \"Will feminism survive a collapse?\". It pointed out that mechanization, better medicine and lower child mortality greatly helped femininism. A lot of women went into the worksplace, in factories and in universities because a lot a time previously allocated to household chores and child rearing was freed up.</p><p>Of course, people figthing for better rights and values did play an important role. But moral progress wasn't enough by itself. Technology changed a lot of things. And access to energy that is not guaranteed: <a href=\"https://forum.effectivealtruism.org/posts/wXzc75txE5hbHqYug/the-great-energy-descent-short-version-an-important-thing-ea\">https://forum.effectivealtruism.org/posts/wXzc75txE5hbHqYug/the-great-energy-descent-short-version-an-important-thing-ea</a>&nbsp;</p><p>For animals, technology with alternative proteins could help, but that's far from certain: <a href=\"https://forum.effectivealtruism.org/posts/bfdc3MpsYEfDdvgtP/why-the-expected-numbers-of-farmed-animals-in-the-far-future\">https://forum.effectivealtruism.org/posts/bfdc3MpsYEfDdvgtP/why-the-expected-numbers-of-farmed-animals-in-the-far-future</a>.</p><p>So I'm not convinced that we'll inevitably have moral progress in the future.</p><p>&nbsp;</p><p>On the topic of slavery, see this paper :&nbsp; <a href=\"https://slatestarcodex.com/Stuff/manumission.pdf\">https://slatestarcodex.com/Stuff/manumission.pdf</a></p><p>It says that slavery often had a minor significance in most societies. It usually had nothing to do with ethics but rather that slavery is not an efficient economic system. Rome or Southern US are rather rare cases. Of course, it's more complicated than that. Rome could acquire a lot of slaves and treat them in a worse way while it invaded a lot of territory (and acquired a lot of slaves).</p>", "parentCommentId": "qhgpW7St3uWa7tLzK", "user": {"username": "Corentin Fressoz"}}, {"_id": "cRscHD4S34r47YWoZ", "postedAt": "2023-07-11T14:39:50.994Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>Well, in my view the vast majority of moral progress has been triggered by material improvements.&nbsp;</p><p>What else could it be? We are not doing moralistic eugenics, are we?&nbsp;</p><p>In fact, moral progress is not based on some kind of altruistic impulses, but in the development of reciprocity schemes (often based on punishments) that imply evolutionary (often in geopolitical/economic competition space) advantages.</p>", "parentCommentId": "ibffaDG8GtKv9rTWs", "user": {"username": "Arturo Macias"}}, {"_id": "zrNrRusvE6Ba6onPh", "postedAt": "2023-07-11T16:27:06.420Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>I agree with that - but I still don't see why this implies that humans will give a lot of moral value towards animals.</p><p>So far, material improvements have worsened the conditions of farmed animals - as a lot of factory farming is not the result of a biological necessity, but is rather done for personal taste. This seems like regress, not progress.</p><p>So I don't see why, given the current trajectory, moralization would end up including animals.</p>", "parentCommentId": "cRscHD4S34r47YWoZ", "user": {"username": "Corentin Fressoz"}}, {"_id": "KNCLKEGMJoyMswDHt", "postedAt": "2023-07-11T16:39:59.720Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>Because it is logical and probably it will be relatively cheaper as long as we become richer.</p>\n<p>I think this is hopeful but not very inspiring argument \u2026</p>\n", "parentCommentId": "zrNrRusvE6Ba6onPh", "user": {"username": "Arturo Macias"}}, {"_id": "CjEDFMgNH9jv8T6Qn", "postedAt": "2023-07-11T16:44:01.719Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>I enjoyed this post, thanks for writing it.</p><blockquote><p>Is there any crucial consideration I\u2019m missing? For instance, are there reasons to think agents/civilizations that care about suffering might \u2013 in fact \u2013 be selected for and be among the grabbiest?</p></blockquote><p>I think I buy your overall claim in your \u201c<a href=\"https://forum.effectivealtruism.org/posts/bTPP7fZxSvBzsNDES/why-we-may-expect-our-successors-not-to-care-about-suffering-2#__Addressing_obvious_objections\"><u>Addressing obvious objections</u></a>\u201d section that there is little chance of agents/civilizations who disvalue suffering (hereafter: non-PUs) winning a colonization race against positive utilitarians (PUs). (At least, not without causing equivalent expected suffering.) However, my next thought is that non-PUs will generally work this out, as you have, and that some fraction of <a href=\"https://en.wikipedia.org/wiki/Kardashev_scale#:~:text=A%20type%20III%20civilization%20is,star%2C%20black%20holes%2C%20etc.\">technologically advanced</a> non-PUs\u2014probably mainly those who disvalue suffering the most\u2014might act to change the balance of realized upside- vs. downside-focused values by triggering&nbsp;<a href=\"https://en.wikipedia.org/wiki/False_vacuum_decay\"><u>false vacuum decay</u></a> (or by doing something else with a similar switching-off-a-light-cone effect).</p><p>In this way, it seems possible to me that suffering-focused agents will beat out PUs. (Because there\u2019s nothing a PU agent\u2014or any agent, for that matter\u2014can do to stop a vacuum decay bubble.) This would reverse the post\u2019s conclusion. Suffering-focused agents may in fact be the grabbiest, albeit in a self-sacrificial way.</p><p>(It also seems possible to me that suffering-focused agents will mostly act cooperatively, only triggering vacuum decays at a frequency that matches the ratio of upside- vs. downside-focused values in the cosmos, according to their best guess for what the ratio might be.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefl2hlxnknz9l\"><sup><a href=\"#fnl2hlxnknz9l\">[1]</a></sup></span>&nbsp;This would neutralize my above paragraph as well as the post's conclusion.)</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnl2hlxnknz9l\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefl2hlxnknz9l\">^</a></strong></sup></span><div class=\"footnote-content\"><p>My first pass at what this looks like in practice, from the point of view of a technologically advanced, suffering-focused (or perhaps non-PU more broadly) agent/civilization: I consider what fraction of agents/civilizations like me should trigger vacuum decays in order to realize the cosmos-wide values ratio. Then, I use a random number generator to tell me whether I should switch off my light cone.</p><p>Additionally, one wrinkle worth acknowledging is that some universes within the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Eternal_inflation#:~:text=Because%20the%20regions%20expand%20exponentially,insignificant%20fractal%20volume%20ends%20inflation.\"><u>inflationary</u></a>&nbsp;<a href=\"https://en.wikipedia.org/wiki/Multiverse#Brian_Greene%27s_nine_types\"><u>multiverse</u></a>, if indeed it exists and allows different physics in different universes, are not metastable. PUs likely cannot be beaten out in these universes, because vacuum decays cannot be triggered. Nonetheless, this can be compensated for through suffering-focused/non-PU agents in metastable universes triggering vacuum decays at a correspondingly higher frequency.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Will Aldred"}}, {"_id": "9WTSR7ZXp56wJKyKX", "postedAt": "2023-07-11T16:44:35.240Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<blockquote><p>Moreover, even in the face of strong selection pressure, systems don't seem to converge on similar equilibria in general.</p></blockquote><p>I like this thought but to push back a bit - nearly every species we know of is incredibly selfish or at best only cares about their very close relatives. Sure, crabs are way different than lions but OP is describing a much lower dimension, which seems more likely to generalize regardless of context.</p><p>If you asked me to predict what (animal) species live in the rainforest just by showing me a picture of the rainforest I wouldn't have a chance. If you asked me if the species in the rainforest would be selfish or not that would be significantly easier. For one, it's easier to predict one dimension than all the dimensions, and second, some dimensions we should expect to be much less elastic to the set of possible inputs.&nbsp;</p>", "parentCommentId": "WByjdCDPrZLQyqm4c", "user": {"username": "Charles_Guthmann"}}, {"_id": "ngM9YBibkfpyxJQBz", "postedAt": "2023-07-11T16:55:22.154Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<blockquote><p><strong>3. If humans become grabby, their values are unlikely to differ significantly from the values of the civilization that would've controlled it instead.</strong></p></blockquote><p>I think this is phrased incorrectly. I think the correct phrasing is :</p><p><strong>3. &nbsp;If humans become grabby, their values (in expectation) are ~ the mean values of a grabby civilization.&nbsp;</strong></p><p>Not sure if it's what you meant but let me explain the difference with an example. let's say there are three societies:</p><p><i>[humans | zerg | Protoss]</i></p><p>for simplicity let's say the winner takes all of the lightcone.&nbsp;</p><ul><li><i>EV[lightcone| zerg win] = 1 &nbsp;</i></li><li><i>EV[lightcone| humans win] = 2&nbsp;</i></li><li><i>EV[lightcone| protoss win] = 3</i></li></ul><p>Then if humans become grabby their values are guaranteed to differ from whoever else would have won, yet for utilitarian purposes we don't care because the expected value is the same, given we don't know if the zerg or protoss will win.&nbsp;</p><p>I think you might have meant this? But it's somewhat important to distinguish because my updated (3) is a weaker claim than the original one, yet still enough to hold the argument together.&nbsp;</p>", "parentCommentId": "WByjdCDPrZLQyqm4c", "user": {"username": "Charles_Guthmann"}}, {"_id": "JsCCAiWhRF35sPBBh", "postedAt": "2023-07-11T17:22:34.610Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>Yep, I was about to comment on the same thing. &nbsp;Would like to see what OP has to say</p>", "parentCommentId": "CjEDFMgNH9jv8T6Qn", "user": {"username": "Charles_Guthmann"}}, {"_id": "6fBYPjSMoz9vgBtit", "postedAt": "2023-07-11T20:46:11.518Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>Thanks Will! :)<br><br>I think I haven't really thought about this possibility.<br><br>I know nothing about how things like false vacuum decay work (thankfully, I guess), about how tractable it is, and about how the minds of the agents would work on trying to trigger that operate. And my immediate impression is that these things matter a lot to whether my responses to the first two \"<a href=\"https://forum.effectivealtruism.org/posts/bTPP7fZxSvBzsNDES/why-we-may-expect-our-successors-not-to-care-about-suffering-2#__Addressing_obvious_objections\">obvious objections</a>\" sort of apply here as well and to whether \"decay-conducive values\" might be competitive.<br><br>However, I think we can at least confidently say that -- at least in the intra-civ selection context (see my <a href=\"https://forum.effectivealtruism.org/posts/xdKnfQKLyYQfeErSr/the-grabby-values-selection-thesis-what-values-do-space\">previous post</a>) -- a potential selection effect non-trivially favoring \"decay-conducive values\", during the space colonization process, seems much less straightforward and obvious than the selection effect progressively favoring agents that are more and more upside-focused (on long-time scales with many bits of selection). The selection steps are not the same in these two different cases and the potential dynamic that might lead decay-conducive values to take over seems more complex and fragile.</p>", "parentCommentId": "CjEDFMgNH9jv8T6Qn", "user": {"username": "Jim Buhler"}}, {"_id": "mPMDodak9vcTQujz4", "postedAt": "2023-07-11T23:04:09.678Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>\"It forces the suffering-concerned agents to make trade-offs between preventing suffering and increasing their ability to create more of what they value. Meanwhile, those who don\u2019t care about suffering don\u2019t face this trade-off and can focus on optimizing for what they value without worrying about the suffering they might (in)directly cause.\"</p>\n<p>Hi Jim, is it really the case that spending effort preventing suffering harms your ability to spread your values? Take religious people, they spend much effort doing religion stuff which is not exactly for the purpose of optimizing their survival, yet religious people also tend to be happier, healthier, and have more children than non religious people(no citation, seems true though if all else equal).</p>\n<p>Could it be the case those whom don't optimize for reducing suffering, instead of optimizing for spreading their values, optimize or do something else that decreases the likelihood of their values spreading?</p>\n<p>Also, why now? Why haven't we already reached or are close to equilibrium for reducing suffering vs etc since selection pressures have been here long time</p>\n", "parentCommentId": null, "user": {"username": "Daryl D'Souza"}}, {"_id": "sD2dbkeGtxPJdvaxo", "postedAt": "2023-07-12T07:15:49.116Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>Thanks for the comment!<br><br>Right now, in rich countries, we seem to live in an unusual period Robin Hanson (<a href=\"https://www.overcomingbias.com/p/this-is-the-dream-timehtml\">2009</a>) calls \"the Dream Time\". You can survive valuing pretty much whatever you want, which is why there isn't much selection pressure on values. This likely won't go on forever, especially if Humanity starts colonizing space.<br><br>(Re religion. This is anecdotical but since you brought up this example: in the past, I think religious people would have been much less successful at spreading their values if they were more concerned about the suffering of the people they were trying to convert. The growth of religion was far from being a harm-free process.)</p>", "parentCommentId": "mPMDodak9vcTQujz4", "user": {"username": "Jim Buhler"}}, {"_id": "YCDTFH4dLCguuSncT", "postedAt": "2023-07-12T17:31:32.286Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>I'm highly suspicious about this \"logical\" factor. Humans don't always do logical things - just a look at the existence of fast fashion should be enough to be sure of that.</p><p>&nbsp;</p><p>For the \"alternative proteins will be cheaper\", I fear that's not enough. See this post about why such a position is pretty optimistic : <a href=\"https://forum.effectivealtruism.org/posts/bfdc3MpsYEfDdvgtP/why-the-expected-numbers-of-farmed-animals-in-the-far-future\">https://forum.effectivealtruism.org/posts/bfdc3MpsYEfDdvgtP/why-the-expected-numbers-of-farmed-animals-in-the-far-future</a>.</p>", "parentCommentId": "KNCLKEGMJoyMswDHt", "user": {"username": "Corentin Fressoz"}}, {"_id": "HRXW22LFRCewiTX3C", "postedAt": "2023-07-12T17:36:23.259Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>Yes, agreed \u2014 thanks for pointing this out!</p>", "parentCommentId": "ngM9YBibkfpyxJQBz", "user": {"username": "Chinmay Deshpande"}}, {"_id": "Ya9vP3hmHrCCxJWD3", "postedAt": "2023-07-12T18:36:19.135Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>Fantastic post/series. The vocab words have been especially useful to me. few mostly disjunctive thoughts even though I overall agree.&nbsp;</p><ul><li>I wonder what you think would happen if an economically valuable island popped up in the middle of the ocean today?&nbsp;<ul><li>My guess is it would be international lands in some way and no country would let or want another country to claim the land</li><li>I don't think this is super analogous but I think there is some cross over.&nbsp;</li></ul></li><li>The generalization of the first bullet point is that under the right political circumstances, HV (or otherwise) governments can prevent unlicensed outward colonization from within their society without themselves colonizing.&nbsp;<ul><li>Some obvious objections here, like as soon as the gov can't lock stuff down for a period of time it could be impossible to stop the outward expansion.&nbsp;<ul><li>But this honestly depends a lot on the technology levels of the relevant players</li></ul></li></ul></li><li>Governments could also theoretically do this to other civilizations. They could do a military version of von Neumann probes, locking down areas and stopping evolution from occurring while not actually colonizing the land in any sentient adding sense.&nbsp;</li><li>I'm concerned that it's easy to handwave a lot of stuff with claims of AGI being able to do XYZ. &nbsp;While I often buy these claims myself, It would be nice to condition this question on like 5-10 different levels of maximum technology, or technology differential between a ruling state and everyone else. I think that's where a lot of the disconnect comes between the current day island scenario and your post.&nbsp;<ul><li>At the very least, it would be nice to have a section where you say ~ about what your estimate for the tech is.&nbsp;</li></ul></li><li>The Expanse is a show about a similar concept, I don't think it's necessarily a great prediction of what life will be like but it's cool to see a fleshed-out version of the tension between the expanders and non expanders.<ul><li>It being fleshed out might give you a slightly different perspective/ see that there are perhaps a few more details or considerations needed.&nbsp;</li></ul></li><li>If PU society isn't asymmetric on the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Omission_bias\"><u>action-omission</u></a> axis, then they should still have some level of concern about just expanding like crazy, since they need to consider the fact that they are locking in a worse conversion of physical resources to positive utility still.&nbsp;</li><li>I don't fully agree with Will's claim about deleting the lightcone. It depends on the the ratio at which the suffering focused agents value pleasure to pain and where they fall on the action-ommision axis. Nonetheless, if spreading good lives is nearly as easy as spreading, spreading and destroying everything as you spread is probably in between the two, or if something like false vacuum decay is possible, even easier than spreading.&nbsp;</li></ul>", "parentCommentId": null, "user": {"username": "Charles_Guthmann"}}, {"_id": "eHjYMQhxygzsG6fos", "postedAt": "2023-07-13T09:21:46.035Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<blockquote><p>Is there any crucial consideration I\u2019m missing? For instance, are there reasons to think agents/civilizations that care about suffering might \u2013 in fact \u2013 be selected for and be among the grabbiest?</p></blockquote><p>David Deutsch makes the argument that long-term success in knowledge-creation requires commitment to values like tolerance, respect for the truth, rationality and optimism. The idea is that if you do not have such values you end up with a fixed society, with dogmatic ideas and institutions that are not open to criticism, error-correction and improvement. Errors will inevitably accumulate and you will fail to create the knowledge necessary to achieve your goals.</p><p>On this view, grabby aliens need values that permit sustained knowledge growth to meet the challenges of successful long-term expansion. An error-correcting society would make moral as well as scientific progress, and so would either value reducing suffering or have a good moral explanation as to why reducing suffering isn't optimal.</p><p>This is somewhat like a variation of the Instrumental Convergence Thesis, whereby agents will tend to converge on various Enlightenment values because they are instrumental in knowledge creation, and knowledge-growth is necessary for successfully reaching many final goals.&nbsp;</p><p>Here are two relevant quotations about alien values from a <a href=\"https://www.youtube.com/watch?v=B53NX3sCdqg\">talk David Deutsch gave on optimism</a>.&nbsp;</p><blockquote><p>the only moral values that permit sustained progress are the objective values of an open society and more broadly of the Enlightenment. No doubt the ET\u2019s morality would not be the same as ours, but nor will it be the same as the 16<sup>th</sup> century conquistadors. It will be better than ours.</p></blockquote><p>&nbsp;</p><blockquote><p>the Borg way of life... doesn\u2019t create any knowledge. It continues to exist by assimilating existing knowledge. \u2026 A fixed way of life. \u2026 it is never going win in the long run against an exponentially improving way of life&nbsp;</p></blockquote><p>&nbsp;</p><figure class=\"media\"><div data-oembed-url=\"https://youtu.be/B53NX3sCdqg?t=2842\"><div><iframe src=\"https://www.youtube.com/embed/B53NX3sCdqg\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure>", "parentCommentId": null, "user": {"username": "dotsam"}}, {"_id": "TPFsSZv7iBif6fSWx", "postedAt": "2023-07-13T16:03:08.913Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>Hi Corentin,</p><blockquote><p>So I'm really not certain that we are the most moral civilization.</p></blockquote><p>I agree it is pretty unclear whether humanity has so far increased/decreased overall welfare, because this is probably <a href=\"https://forum.effectivealtruism.org/posts/ayxasxhHWTvf6r5BF/scale-of-the-welfare-of-various-animal-populations\">dominated</a> by the quite unclear effects on wild animals.</p><p>However, FWIW, the way I think about it is that the most moral species is that whose extinction would lead to the greatest reduction in the value of the future (of course, one could define \"most moral species\" in some other way). I think the extinction of humanity would lead to a greater reduction in the value of the future than that of any other species, so I would say humans are the most moral civilization.</p>", "parentCommentId": "2t8Bye5P2dgM2a6Bs", "user": {"username": "vascoamaralgrilo"}}, {"_id": "97y2sJhpA7wakJeJL", "postedAt": "2023-07-14T13:25:27.774Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>Interesting and nice to read!<br><br>Do you think the following is right?<br><br>The larger the <i><strong>Upside-focused Colonist Curse</strong></i>, the fewer resources agents caring about suffering will control overall and the smaller the risks of conflicts causing S-risks?<br><br>This may balance out the effect that the larger the <i><strong>Upside-focused Colonist Curse</strong></i>, the more neglected S-risks are.<br><br>High <i><strong>Upside-focused Colonist Curse</strong></i> produces fewer S-risks at the same time as making them more neglected.</p>", "parentCommentId": null, "user": {"username": "Maxime_Riche"}}, {"_id": "fGwXjFfbsEChJ9mCw", "postedAt": "2023-07-16T16:21:43.929Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>Thank you so much for thinking this through and posting this. &nbsp;It makes a lot of sense, and it's concerning.</p>", "parentCommentId": null, "user": {"username": "alene"}}, {"_id": "ddg5Z8aPoxAaTYaqe", "postedAt": "2023-07-17T16:04:28.401Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>I agree that most of the result ends up depending on the effects on wild animals. Always troublesome that so much of the impact depends on that when we have so many uncertainties.</p><blockquote><p>most moral species is that whose extinction would lead to the greatest reduction in the value of the future</p></blockquote><p>We probably don't have the same definition - if wild lives are net negative and we destroy everything by accident, I wouldn't count that as being \"moral\" because it's not due to moral values. But the definition doesn't matter that much, though.&nbsp;</p><p>Still, I'm not certain that the \"value in the future\" of industrial civilization (a different concept than humanity) will be so positive, when there are so many uncertainties (and that we could continue to expand even further factory farming).</p>", "parentCommentId": "TPFsSZv7iBif6fSWx", "user": {"username": "Corentin Fressoz"}}, {"_id": "hvFPJR8re2mE55eb8", "postedAt": "2023-07-18T13:25:30.708Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>Thanks, Maxime!&nbsp;This is indeed a relevant consideration I thought a tiny bit about, and Michael St. Jules also brought that up in a comment on my draft.<br><br>First of all, it is important to note that UCC affects the neglectedness -- and potentially also the probability -- of \"late s-risks\", only (i.e., those that happen far away enough from now for the UCC selection to actually have the time to occur). So let's consider only these late s-risks.<br><br>We might want to differentiate between three different cases:<br><strong>1. Extreme UCC</strong> (where suffering is not just ignored but ends up being <strong>valued</strong> as in the scenario I depict in <a href=\"https://forum.effectivealtruism.org/s/wmqLbtMMraAv5Gyqn/p/bTPP7fZxSvBzsNDES#fnqy4sqavsizl\">this footnote</a>. In this case, all kinds of late s-risks seem not only more neglected but also more likely.<br><strong>2. Strong UCC</strong> (where agents end up being roughly indifferent to suffering; this is the case your comment assumes I think). In this case, while all kinds of late s-risks seem more neglected, late s-risks from conflict seem indeed less likely. However, this doesn't seem to apply to (at least) near-misses and incidental risks.<br><strong>3. Weak UCC</strong> (where agents still care about suffering but much less than we do). In this case, same as above, except perhaps for the \"late s-risks from conflict\" part. I don't know how weak UCC would change conflict dynamics.<br><br>The more we expect #2 more than #1 and #3, the more your point applies, I think (with the above caveat on near-misses and incidental risks). I might definitely have missed something, though. It's a bit complicated.</p>", "parentCommentId": "97y2sJhpA7wakJeJL", "user": {"username": "Jim Buhler"}}, {"_id": "RthfREqrQGdppPLgh", "postedAt": "2023-07-18T13:27:35.105Z", "postId": "bTPP7fZxSvBzsNDES", "htmlBody": "<p>Thanks a lot, Alene! That's motivating :)</p>", "parentCommentId": "fGwXjFfbsEChJ9mCw", "user": {"username": "Jim Buhler"}}]