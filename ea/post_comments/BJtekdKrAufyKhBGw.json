[{"_id": "tTboSPY3xvahJwDe4", "postedAt": "2022-09-07T08:32:40.335Z", "postId": "BJtekdKrAufyKhBGw", "htmlBody": "<p>+1 to this proposal and focus.</p>\n<p>On 'technical levers to make AI coordination/regulation enforceable', there is a fair amount of work suggesting that e.g. arms control agreements have often dependend on/been enabled by new technological avenues for enabling unilateral monitoring (or for enabling cooperative, but non-intrusive monitoring - e.g. sensors on missile factories, as part of the US-USSR INF Treaty),  have been instrumental (see <a href=\"https://www.cambridge.org/core/journals/american-political-science-review/article/abs/why-arms-control-is-so-rare/BAC79354627F72CDDDB102FE82889B8A\">Coe and Vaynmann 2020</a> ).</p>\n<p>That doesn't mean that it's always an unalloyed good: there are indeed cases where new capabilities can introduce new security or escalation risks (e.g. <a href=\"https://tnsr.org/2021/09/better-monitoring-and-better-spying-the-implications-of-emerging-technology-for-arms-control/\">Vaynmann 2021</a>); they can also perversely hold up negotiations; e.g. Richard Burns (<a href=\"https://www.amazon.com/Evolution-Arms-Control-Antiquity-Destruction/dp/1442223790\">link, introduction</a>) discusses a case where the involvement of engineers in designing a monitoring system for the Comprehensive Test Ban Treaty, actually held up negotiations of the regime, basically because the engineers focused excessively on technical perfection of the monitoring system [beyond a level of assurance that would've been strictly politically required by the contracting parties], which enabled opponents of the treaty to paint it as not giving sufficiently good guarantees.</p>\n<p>Still, beyond improving enforcement, there's interesting work on ways that AI technology could speed up and support the negotiation of treaty regimes (<a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3531976\">Deeks 2020</a>,  <a href=\"https://www.lawfareblog.com/what-could-international-lawyers-do-high-tech-tools\">2020b</a>, <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3806624\">Maas 2021</a>), both for AI governance specifically, and in supporting international cooperation more broadly.</p>\n", "parentCommentId": null, "user": {"username": "MatthijsMaas"}}, {"_id": "DimqgFu6Mgxkuf49a", "postedAt": "2022-09-07T12:12:31.566Z", "postId": "BJtekdKrAufyKhBGw", "htmlBody": "<p>I am a software engineer who transitioned to tech/AI policy/governance. I strongly agree with the overall message (or at least title) of this article: that AI governance needs technical people/work, especially for the ability to enforce &nbsp;regulation.&nbsp;<br><br>However in the 'types of technical work' you lay out I see some gaping governance questions/gaps. You outline various tools that could be built to improve the capability of actors in the governance space, but there are many such actors, and tools by their nature are dual use - where is the piece on who these tools would be wielded by, and how they can be used responsibly? I would be more excited about seeing new initiatives in this space that clearly set out which actors it works with for which kinds of policy issues and which not and why. Also there is a big hole around not being conflicted etc. There's lots of legal issues that can't be avoided that crop up when you need to actually use such tools in any context beyond a voluntary initiative of a company (which does not give as many guarantees as things that apply to all current and future companies, like regulations or to some extent standards). There is and will be increasingly a huge demand for companies with practical AI auditing expertise - this is a big opportunity to start trying to fill that gap.&nbsp;<br><br>I think the section on 'advising on the above' could be fleshed out a whole lot more. At least I've found that because this area is very new, there is a lot of talking to do with lots of different people, lots of translation, before getting to actually do these things... it's helpful if you're the kind of technical person who is willing to learn how to communicate to a non-technical audience, and to learn from people with other backgrounds about the constraints and complexities of the policymaking world, and derives satisfaction from this. I think this is hugely worthwhile though - and if you're the kind of person who is willing to do that and looking for work in the area, do get in touch as I have some opportunities (in the UK).<br><br>Finally, I'll just more explicitly now highlight the risk of technical people being used for the aims of others (that may or may not lead to good outcomes) in this space. In my view, if you really want to work in this intersection you should be asking the above questions about anything you build - who will use this thing and how, and what are the risks and can I reduce them. And when you advise powerful actors, bringing your technical knowledge and expertise, do not be afraid to also give your opinions to decision-makers on what might lead to what kinds of real world outcomes, and ask questions about the application aims, and improve those aims.</p>", "parentCommentId": null, "user": {"username": "tamgent"}}, {"_id": "iBn5EhaFHyQaYPn36", "postedAt": "2022-09-07T22:59:08.467Z", "postId": "BJtekdKrAufyKhBGw", "htmlBody": "<p>Thanks for the comment! I agree these are important considerations and that there's plenty my post doesn't cover. (Part of that is because I assumed the target audience of this post--technical readers of this forum--would have limited interest in governance issues and would already be inclined to think about the impacts of their work. Though maybe I'm being too optimistic with the latter assumption.)</p>\n<p>Were there any specific misuse risks involving the tools discussed in the post that stood out to you as being especially important to consider?</p>\n", "parentCommentId": "DimqgFu6Mgxkuf49a", "user": {"username": "Mauricio"}}]