[{"_id": "ZqBsdxp3XpHyTMdn8", "postedAt": "2017-06-30T11:42:36.446Z", "postId": "bdssHHKSze7irv8zB", "htmlBody": "<blockquote>\n<p>But of course, I cannot justify high confidence in these views given that many experts disagree. Following the analysis of this post, this is</p>\n</blockquote>\n<p>Dangling sentence.</p>\n<p>In my personal belief, the &quot;hard AI takeoff&quot; scenarios are driven mostly by the belief that current AI progress largely flows from a single skill, that is, &quot;mathematics/programming&quot;. So while AI will continue to develop at disparate rates and achieve superhuman performance in different areas at different rates, an ASI takeoff will be driven almost entirely by AI performance in software development, and once AI becomes superhuman in this skill it will rapidly become superhuman in all skills. This seems obvious to me, and I think disagreements with it have to rest largely with hidden difficulties in &quot;software development&quot;, such as understanding and modeling many different systems well enough to develop algorithms specialized for them (which seems like it's almost circularly &quot;AGI complete&quot;).</p>\n", "parentCommentId": null, "user": {"username": "FeepingCreature"}}, {"_id": "Kr2KY4gbrskrNDfjb", "postedAt": "2017-07-01T17:56:33.339Z", "postId": "bdssHHKSze7irv8zB", "htmlBody": "<p>Does it mean that we could try to control AI by preventing its to know anything about programming? </p>\n<p>And on the other side, any AI which is able to write code should be regarded extremely dangerous, no matter how low its abilities in other domains?</p>\n", "parentCommentId": "ZqBsdxp3XpHyTMdn8", "user": {"username": "turchin"}}, {"_id": "NAsm9MJ67jKcFekfk", "postedAt": "2017-07-02T07:36:08.460Z", "postId": "bdssHHKSze7irv8zB", "htmlBody": "<blockquote>\n<p>disagreements with it have to rest largely with hidden difficulties in &quot;software development&quot;, such as understanding and modeling many different systems well enough to develop algorithms specialized for them (which seems like it's almost circularly &quot;AGI complete&quot;).</p>\n</blockquote>\n<p>What do you make of that objection? (I agree with it. I think programming efficiently and flexibly across problem domains is probably AGI-complete.)</p>\n", "parentCommentId": "ZqBsdxp3XpHyTMdn8", "user": {"username": "Brian_Tomasik"}}, {"_id": "5gk5f9GHtipZ2epoE", "postedAt": "2017-07-05T19:26:03.515Z", "postId": "bdssHHKSze7irv8zB", "htmlBody": "<p>I think I broadly agree that take off is likely to be slow and that there is not a slam dunk argument for trying to make safe super intelligent agents.</p>\n<p>However I think there is room for all sorts of work. Anything that can reduce the uncertainty of where AGI is going.</p>\n<p>I think AI, as it is, is on slightly the wrong track. If we get on the right track we will get somewhere a lot quicker than the decades referenced above.</p>\n<p>Computers as they stand are designed with the idea of having a human that looks after them and understands their inner workings, at least somewhat. Animals from the lowly nematode to humans do not have that assumption. Current deep learning assumes a human will create the input and output spaces and assign resources to that learning process. </p>\n<p>If we can off load the administration of a computer to the computer itself, this would allow cheaper administration of computers and also the computer systems to become more complex. Computer systems are limited in complexity by the thing that debugs them.</p>\n<p>I have an idea of what this might look like and if my current paradigm plays out, I think humanity will get the choice of creating separate agents or creating external lobes of our brains. Most likely humanity will pick the creating external lobes. The external lobes may act in a more economic fashion, but I think they still might have the capability of going bad. Minimising the probability of this is very important.</p>\n<p>I think there is also probably a network effect, if we could get altruistically minded people to be the first to have the external brains then we might influence the future by preferentially helping other altruists to get external brains. This could create a social norms among people with external brains.</p>\n<p>So I think technical work towards understanding administratively autonomous computers (no matter how intelligent they are) can reduce uncertainty and allow us to understand what choices face us.</p>\n", "parentCommentId": null, "user": {"username": "WillPearson"}}, {"_id": "RkizyhaEiZCoAiFky", "postedAt": "2017-07-19T04:10:31.611Z", "postId": "bdssHHKSze7irv8zB", "htmlBody": "<p>My 2 cents: math/ programming is only half the battle. Here's an analogy - you could be the best programmer in the world, but if you don't understand chess, you can't program a computer to beat a human at chess, and if you don't understand quantum physics, you can't program a computer to simulate matter at the atomic scale (well, not using ab initio methods anyway).</p>\n<p>In order to get an intelligence explosion, a computer would have to not only have great programming skills, but also really understand intelligence. And intelligence isn't just one thing - it's a bunch of things (creativity, memory, planning, social skills, emotional skills etc and these can be subdivided further into different fields like physics, design, social understanding, social manipulation etc). I find it hard to believe that the same computer would go from not superhuman to superhuman in almost all of these all at once. Obviously computers outcompete humans in many of these already, but I think even on the more &quot;human&quot; traits and in areas where computer act more like agents than just like tools, it's still more likely to happen in several waves instead of just one takeoff.</p>\n", "parentCommentId": "ZqBsdxp3XpHyTMdn8", "user": {"username": "Daniel_Eth"}}]