[{"_id": "hdJkLpxejTMYXcgro", "postedAt": "2017-12-21T02:03:27.133Z", "postId": "XKwiEpWRdfWo7jy7f", "htmlBody": "<p>This is a great post, and I think it's a really valuable service that you're providing - last year's version is, at the present time of writing, the forum's equal most upvoted post of all time.</p>\n<p>Also, I think we're pretty strongly in agreement. A year ago, I gave to GCRI. This year, I gave to MIRI, based on my positive experience working there, though GCRI has improved since. It would be good to see more funds going to both of these.</p>\n", "parentCommentId": null, "user": {"username": "RyanCarey"}}, {"_id": "nKrDvQikFDYWNs2rx", "postedAt": "2017-12-21T02:40:33.554Z", "postId": "XKwiEpWRdfWo7jy7f", "htmlBody": "<blockquote>\n<p>&quot;given that this paper assumes the humans choose the wrong action by accident less than 1% of the time, it seems that the AI should assign a very large amount of evidence to a shutdown command... instead the AI seems to simply ignore it?&quot;</p>\n</blockquote>\n<p>That's kind-of the point, isn't it? A value learning system will only &quot;learn&quot; over certain variables, according to the size of the learning space, and the prior that it is given. The examples show how if it has an error in the parameterized reward function (or equivalently in the prior), then a bad outcome will ensue. Although I agree that the examples do say much that is not also presented in the text. Anyway, it is also clear by this point that there is room for improvement on my presentation!</p>\n", "parentCommentId": null, "user": {"username": "RyanCarey"}}, {"_id": "wQCHvYqvChzpsJ4mE", "postedAt": "2017-12-21T03:47:09.408Z", "postId": "XKwiEpWRdfWo7jy7f", "htmlBody": "<p>Thanks for writing this; I found it really useful.</p>\n<blockquote>\n<p>two of the authors (Christiano from OpenAI and Amodei from Deepmind) </p>\n</blockquote>\n<p>I thought both Christiano and Amodei were at OpenAI? (<a href=\"https://www.linkedin.com/in/dario-amodei-3934934/\">Amodei's LinkedIn</a>)</p>\n<p>Small typo:</p>\n<blockquote>\n<p>in general I am warey of giving organisations ...</p>\n</blockquote>\n<p>&quot;warey&quot; should be &quot;wary&quot;</p>\n", "parentCommentId": null, "user": {"username": "Milan_Griffes"}}, {"_id": "i5KsMAAxpKBuDLFso", "postedAt": "2017-12-21T10:49:01.715Z", "postId": "XKwiEpWRdfWo7jy7f", "htmlBody": "<p>Thanks!</p>\n<blockquote>\n<p>Nvidia (who make GPUs used for ML) saw their  share price approximately doubl, after quadrupling last year.</p>\n</blockquote>\n<p>Do you have an impression of whether this is due to crypto mining or ML progress?</p>\n", "parentCommentId": null, "user": {"username": "Benjamin_Todd"}}, {"_id": "oZRrFY4BdJiD6vXnK", "postedAt": "2017-12-21T11:49:38.473Z", "postId": "XKwiEpWRdfWo7jy7f", "htmlBody": "<p>Intuitively, it's largely the ML - this is what they brand on, and revenue figures bear this out. Datacenter hardware (e.g. Tesla/Volta) are about 1/5th of their revenue currently, up 5x this year [1]. Whereas crypto is only a few percent of their revenue, and halved this quarter, despite the stock price going up.</p>\n<ol>\n<li><a href=\"http://s22.q4cdn.com/364334381/files/doc_financials/quarterly_reports/2018/Rev_by_Mkt_Qtrly_Trend_Q318.pdf\">http://s22.q4cdn.com/364334381/files/doc_financials/quarterly_reports/2018/Rev_by_Mkt_Qtrly_Trend_Q318.pdf</a></li>\n<li>Crypto is only about ~3% of revenue: <a href=\"https://www.cnbc.com/2017/11/09/nvidia-crytpocurrency-sales-drop-by-more-than-half-from-prior-quarter.html\">https://www.cnbc.com/2017/11/09/nvidia-crytpocurrency-sales-drop-by-more-than-half-from-prior-quarter.html</a></li>\n</ol>\n", "parentCommentId": "i5KsMAAxpKBuDLFso", "user": {"username": "RyanCarey"}}, {"_id": "XoR8vvbLG2vGfyx2F", "postedAt": "2017-12-21T12:57:12.941Z", "postId": "XKwiEpWRdfWo7jy7f", "htmlBody": "<p>Thank you for all the great and detailed analysis again. +1 on GCRI's great work, on a shoestring budget, this year. I think my comment from last year's version of this post holds word for word, but more strongly (copied below for convenience). I would note that I believe Seth and others on his team are working on some very limited funding horizons, which considerably limits what they can do - EA support would likely make a very big positive difference.</p>\n<p>&quot;I would encourage EAs to think about supporting GCRI, whether on AI safety or (especially) GCR more broadly. (1) As you note, they've done some useful analyses on a very limited budget. (2) It's my view that a number of their members (Seth Baum and Dave Denkenberger in particular in my case) have been useful and generous information and expertise resources to this community over the last couple of years (Seth has both provided useful insights, and made very useful networking connections, for me and others in FHI and CSER re: adjacent fields that we could usefully engage with, including biosecurity, risk analysis, governance etc). (3) They've been making good efforts to get more relevant talent into the field - e.g. one of their research associates, Matthias Maas, gave one of the best-received contributed talks at our conference this week (on nuclear security and governance). (4) They're less well-positioned to secure major funding from other sources than some of the orgs above. (5) As a result of (4), they've never really had the opportunity to &quot;show what they can do&quot; so to speak - I'm quite curious as to what they could achieve with a bigger budget and a little more long-term security.</p>\n<p>So I think there's an argument to be made on the grounds of funding opportunity constraints, scaleability, and exploration value.&quot;</p>\n", "parentCommentId": null, "user": {"username": "Sean_o_h"}}, {"_id": "rzLwMMo6jM7WDLkeu", "postedAt": "2017-12-21T14:22:43.844Z", "postId": "XKwiEpWRdfWo7jy7f", "htmlBody": "<p>Also, +1 on the great work being done by AI impacts (also on a shoestring!)</p>\n", "parentCommentId": "XoR8vvbLG2vGfyx2F", "user": {"username": "Sean_o_h"}}, {"_id": "YWPkvQuSss5B58juu", "postedAt": "2017-12-21T23:24:00.971Z", "postId": "XKwiEpWRdfWo7jy7f", "htmlBody": "<p>Christiano and Amodei are both at OpenAI. Jan Leike, Shane Legg, and Miljan Martic are all at DeepMind. (Jan Leike is also former FHI and is still a research associate with us :) ).</p>\n", "parentCommentId": "wQCHvYqvChzpsJ4mE", "user": {"username": "carrickflynn"}}, {"_id": "STYby7t4SEJ4kgKvN", "postedAt": "2017-12-21T23:28:24.455Z", "postId": "XKwiEpWRdfWo7jy7f", "htmlBody": "<p>Thanks, made corrections.</p>\n", "parentCommentId": "wQCHvYqvChzpsJ4mE", "user": {"username": "Larks"}}, {"_id": "pnXyC2pcAW2w9LrEL", "postedAt": "2017-12-22T09:17:21.345Z", "postId": "XKwiEpWRdfWo7jy7f", "htmlBody": "<p>Chiming in with another thank you for putting in the time to do this - I found it helpful.</p>\n<p>One data point on your idea for a year-round blog: I would find it interesting to read, though it wouldn't change my donations as much as targeted holiday giving posts like these, since I currently donate yearly.</p>\n", "parentCommentId": null, "user": {"username": "JacobTref"}}, {"_id": "aP9jPRm2nJPaWzpGR", "postedAt": "2017-12-22T10:19:44.715Z", "postId": "XKwiEpWRdfWo7jy7f", "htmlBody": "<p>Nice review! Two comments so far:</p>\n<ul>\n<li>Re Critch's paper, the result is actually very intuitive once you understand the underlying mechanism. Critch considers a situation of, so to speak, Aumannian <em>disagreement</em>. That is, two agents hold different beliefs, despite being aware of each other's beliefs, because some assumption of Aumann's theorem is false: e.g. each agent considers emself smarter than the other. For example, imagine that Alice believes the Alpha Centauri system has more than 10 planets (call it &quot;proposition P&quot;), Bob believes it has less than 10 planets (&quot;proposition not-P&quot;) and each is aware of the other's belief and considers it to be foolish. In this case, an AI that benefits Alice if P is true and benefits Bob if not-P is true would seem like an excellent deal for both of them, because each will be sure the AI is in eir own favor. In a way, the AI constitutes a <em>bet</em> between the two agents.</li>\n</ul>\n<p>Critch writes: &quot;It is also assumed that the players have common knowledge of one another\u2019s posterior... Future work should design solutions for facilitating the process of attaining common knowledge, or to obviate the need to assume it.&quot; Indeed it is interesting to study what happens when each agents does <em>not</em> know the other's beliefs.</p>\n<ul>\n<li>I will risk being accused of self-advertisement, but given that one of my papers appeared in the review it doesn't seem too arrogant to point at another which IMHO is not less important, namely &quot;<a href=\"https://arxiv.org/abs/1705.04630\">Forecasting using incomplete models</a>&quot;, a paper that builds on Logical Induction in order to develop a way to reason about complex environments that doesn't require logic/deduction. I think it would be nice if this paper was included, although of course it's your review and your judgment whether it merits it.</li>\n</ul>\n", "parentCommentId": null, "user": {"username": "Squark"}}, {"_id": "b5GNNmWegWyEfqiDK", "postedAt": "2017-12-22T15:53:01.215Z", "postId": "XKwiEpWRdfWo7jy7f", "htmlBody": "<p>Also, one forthcoming paper of mine released as a preprint; and another paper that was originally published informally last year but published in somewhat revised and peer-reviewed form this year:</p>\n<ul>\n<li>Sotala, Kaj (2018). <a href=\"http://kajsotala.fi/assets/2017/11/Disjunctive-scenarios.pdf\">Disjunctive Scenarios of Catastrophic AI Risk</a>. AI Safety and Security (Roman Yampolskiy, ed.), CRC Press. Forthcoming.</li>\n<li>Sotala, Kaj (2017) <a href=\"http://kajsotala.fi/assets/2017/10/how_feasible.pdf\">How Feasible is the Rapid Development of Artificial Superintelligence?</a> Physica Scripta <a href=\"http://iopscience.iop.org/article/10.1088/1402-4896/aa90e8\">92 (11)</a>, 113001.</li>\n</ul>\n<p>Both were done as part of my research for the <a href=\"https://foundational-research.org/\">Foundational Research Institute</a>; maybe include us in your organizational comparison next year? :)</p>\n", "parentCommentId": null, "user": {"username": "Kaj_Sotala"}}, {"_id": "EMzf9sTY2zeh8xfJC", "postedAt": "2017-12-25T05:35:32.632Z", "postId": "XKwiEpWRdfWo7jy7f", "htmlBody": "<p>Thank you so much for taking the time to do this! Very informative and helpful!</p>\n", "parentCommentId": null, "user": {"username": "lifelonglearner"}}, {"_id": "gPGxhuhYp5mTvsGaw", "postedAt": "2017-12-30T02:38:12.441Z", "postId": "XKwiEpWRdfWo7jy7f", "htmlBody": "<p>Good stuff! A little correction: The Center for Human-compatible AI goes by 'CHAI' these days.</p>\n", "parentCommentId": null, "user": null}, {"_id": "N8iAacuKYZfL7gYCA", "postedAt": "2017-12-30T10:24:57.702Z", "postId": "XKwiEpWRdfWo7jy7f", "htmlBody": "<p>&quot;Last year I criticised them for not having produced any online research over several years;&quot;\nI should have responded to this last year. CSER's first researcher arrived in post in autumn 2015; at the point at which last year's review was done, CSER had been doing active research for 1 year (in the case of the earliest-arrived postdoc), not several. As all were new additions to the Xrisk community, there wasn't already work in the pipeline; getting to the point of having peer-reviewed publications online within a year, when getting up to speed in a new area, is a challenging ask. </p>\n<p>Prior to autumn 2015, I was the only employee of CSER (part-time, shared with FHI), and my role was fundraising grantwriting, research planning, lecture organisation and other 'get a centre off the ground' activities. I think it's incorrect to consider CSER a research generating organisation before that point.</p>\n<p>We have now had our earliest two postdocs in post for &gt;2 years, and publications are beginning to come through the peer review process.</p>\n", "parentCommentId": null, "user": {"username": "Sean_o_h"}}, {"_id": "c2w3jgJRuYvqSG2BA", "postedAt": "2017-12-30T19:18:29.224Z", "postId": "XKwiEpWRdfWo7jy7f", "htmlBody": "<p>Thanks! Did you include Roman Yampolskiy in GCRI? He has been publishing <a href=\"https://scholar.google.com/citations?hl=en&amp;user=0_Rq68cAAAAJ&amp;view_op=list_works&amp;sortby=pubdate\">a lot</a> (Kaj mentions just one example).</p>\n", "parentCommentId": null, "user": {"username": "Denkenberger"}}, {"_id": "pRgbpaScb7KGTxSAz", "postedAt": "2018-10-13T15:26:02.874Z", "postId": "XKwiEpWRdfWo7jy7f", "htmlBody": "<p>Thank you for this excellent post: I began by pulling out quotes that I wanted to reflect on further, but ended up copying most of the post paragraph by paragraph.</p>\n<p>I'm still not sure how to get the most value out of the information that has been shared here.</p>\n<p>Three ideas:</p>\n<ol>\n<li><p>Sample some summarised papers to (a) get a better idea of what AI safety work looks like and/or (b) build a model of where I might disagree with the post's evaluation of impact</p>\n</li>\n<li><p>Generate alternatives to the model discussed in the introduction (for example, general public outreach being positive EV), see how that changes the outcome, and then consider which models are most likely</p>\n</li>\n<li><p>Use as reading list for preparing for technical work in AI safety</p>\n</li>\n</ol>\n", "parentCommentId": null, "user": {"username": "rjmk"}}]