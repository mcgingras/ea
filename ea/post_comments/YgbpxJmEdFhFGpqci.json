[{"_id": "xwKv6w9FLaDqctgyN", "postedAt": "2022-10-01T02:09:46.319Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>This is very useful both for the content and for examples of how to effectively structure constructive criticism. Will there be a write up on lessons learned from the contest?&nbsp;</p><p>I think that could be quite useful as well since there is interest in running more contests, like the FTX FF <a href=\"https://forum.effectivealtruism.org/posts/W7C5hwq7sjdpTdrQF/announcing-the-future-fund-s-ai-worldview-prize\">AI Worldview Prize</a>.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Peter Gebauer"}}, {"_id": "bgatsjJHubL9nxiaz", "postedAt": "2022-10-01T02:44:39.358Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>The BioAnchors review by Jennifer Lin is incredible. Has it ever been shared as an independent post to LessWrong or EAF? I think many people might learn from it, and further discussion could be productive.</p>\n", "parentCommentId": null, "user": {"username": "Aidan O'Gara"}}, {"_id": "JjvpwGKeaj4w5FTzP", "postedAt": "2022-10-01T03:02:00.534Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>As <a href=\"https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest#comments:~:text=However%2C%20I%20(Lizka)%20encourage%20panelists%20to%20share%20any%20submissions%20they%20particularly%20liked%20in%20the%20comments%20of%20this%20post.\">requested</a>, here are some <strong>submissions that I think are worth highlighting, or considered awarding but ultimately did not make the final cut. </strong>(This list is non-exhaustive, and should be taken more lightly than the <a href=\"https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest#Honorable_mentions___1_000_for_each_of_these_20_submissions_\">Honorable mentions</a>, because by definition these posts are less strongly endorsed &nbsp;by those who judged it.<strong> </strong>Also commenting in personal capacity, not on behalf of other panelists, etc):<br><br><a href=\"https://forum.effectivealtruism.org/posts/xomFCNXwNBeXtLq53/bad-omens-in-current-community-building\"><i>Bad Omens in Current Community Building</i></a><br>I think this was a good-faith description of some potential / existing issues that are important for community builders and the EA community, written by someone who \"did not become an EA\" but chose to go to the effort of providing feedback with the intention of benefitting the EA community. While these problems are difficult to quantify, they seem important if true, and pretty plausible based on my personal priors/limited experience. At the very least, this starts important conversations about how to approach community building that I hope will lead to positive changes, and a community that continues to strongly value truth-seeking and epistemic humility, which is personally one of the benefits I've valued most from engaging in the EA community.<br><br><a href=\"http://dx.doi.org/10.2139/ssrn.4118618\"><i>Seven Questions for Existential Risk Studies</i></a><br>It's possible that the length and academic tone of this piece detracts from the reach it could have, and it (perhaps aptly) leaves me with more questions than answers, but I think the questions are important to reckon with, and this piece covers a lot of (important) ground. To quote a fellow (more eloquent) panelist, whose views I endorse: \"Clearly written in good faith, and consistently even-handed and fair - almost to a fault. Very good analysis of epistemic dynamics in EA.\" On the other hand, this is likely less useful to those who are already very familiar with the ERS space.<br><br><a href=\"https://forum.effectivealtruism.org/posts/4rGpNNoHxxNyEHde3/most-problems-fall-within-a-100x-tractability-range-under\"><i>Most problems fall within a 100x tractability range (under certain assumptions)</i></a><br>I was skeptical when I read this headline, and while I'm not yet convinced that 100x tractability range should be used as a general heuristic when thinking about tractability, I certainly updated in this direction, and I think this is a valuable post that may help guide cause prioritisation efforts.<br><br><a href=\"https://medium.com/@sven_rone/the-effective-altruism-movement-is-not-above-conflicts-of-interest-25f7125220a5\"><i>The Effective Altruism movement is not above conflicts of interest</i></a><br>I was unsure about including this post, but I think this post highlights an important risk of the EA community receiving a significant share of its funding from a few sources, both for internal community epistemics/culture considerations as well as for external-facing and movement-building considerations. I don't agree with all of the object-level claims, but I think these issues are important to highlight and plausibly relevant outside of the specific case of SBF / crypto. That it wasn't already on the forum (afaict) also contributed to its inclusion here.<br><br><br><strong>I'll also highlight one post that was awarded a prize, but I thought was particularly valuable:</strong><br><br><a href=\"https://forum.effectivealtruism.org/posts/hbejbRBpd6quqnTAB/red-teaming-cea-s-community-building-work-2\"><i>Red Teaming CEA\u2019s Community Building Work</i></a><br>I think this is particularly valuable because of the unique and difficult-to-replace position that CEA holds in the EA community, and as Max <a href=\"https://forum.effectivealtruism.org/posts/hbejbRBpd6quqnTAB/red-teaming-cea-s-community-building-work-2?commentId=gDQBKCnbS5usZYR2Y#comments\">acknowledges</a>, it benefits the EA community for important public organisations to be held accountable (and to a standard that is appropriate for their role and potential influence). Thus, even if listed problems aren't all fully on the mark, or are less relevant today than when the mistakes happened, a thorough analysis of these mistakes and an attempt at providing reasonable suggestions at least provides a baseline to which CEA can be held accountable for similar future mistakes, or help with assessing trends and patterns over time. I would personally be happy to see something like this on at least a semi-regular basis (though am unsure about exactly what time-frame would be most appropriate). On the other hand, it's important to acknowledge that this analysis is possible in large part because of CEA's commitment to transparency.</p>", "parentCommentId": null, "user": {"username": "bruce"}}, {"_id": "WhefbMcee2ppzL8pB", "postedAt": "2022-10-01T04:12:07.464Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Just also want to emphasise Lizka's role in organising and spearheading this, as well as her conscientiousness and clear communication at every step of the process - I've enjoyed being part of this, and am personally super grateful for all the work she has put into this contest.</p>", "parentCommentId": null, "user": {"username": "bruce"}}, {"_id": "LGLgnPJjJG7GQskn4", "postedAt": "2022-10-01T04:25:47.624Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<blockquote><p>Including for this contest; we\u2019d love to hear general feedback, and are also interested in hearing about any cases where a submission (or our reviews) changed your mind or actions. You might also want to tell the author(s) of the submission if this happens.</p></blockquote><p>&nbsp;</p><p>Hi, Lizka.</p><p>I'm curious about your mention of reviews. Were reviews written for each contest submission?</p>", "parentCommentId": null, "user": {"username": "Noah Scales"}}, {"_id": "LpLfPWxCSgGThp9tF", "postedAt": "2022-10-01T06:19:53.629Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>I agree with you on the <i>Bad Omens</i> post.</p>", "parentCommentId": "JjvpwGKeaj4w5FTzP", "user": {"username": "NunoSempere"}}, {"_id": "dDjNQQdZmcTHG8B7s", "postedAt": "2022-10-01T07:54:37.722Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>I have lots of questions about the paper \u201c<strong>Biological Anchors external review</strong>\u201d.</p><p>I think this paper contains a really good explanation of the biological anchors and gives good perspective in plain English.</p><p>&nbsp;</p><p>But later in the paper, the author seems to present ideas that seem briefly handled, and to me they appear like intuitions. Even if these are intuitions, these can be powerful, but I can\u2019t tell how insightful they really are with the brief explanation given, and I think they need more depth to be persuasive.&nbsp;</p><p>One example is when the author critiques using compute and evolutionary anchors as an upper bond:</p><p><img src=\"https://lh5.googleusercontent.com/mPZrbTKxkKkPf34nDaM4dhVTicC8uMvKaWm-qDBJT5GY16jQXTkWhgJkRLB191ChCSaaprin0G_TqWuKqS88FSZ2zaqTL2W2Y779khMOg4qNHI4nxmfgWIHo5J9bX2PIHGOjCe95VHfxHiQLUIe_Tpm-4D4kNH27dxjaVjr-C3NahS60xwLlg70zow\"><br><br><i>(Just to be clear I don\u2019t actually read any of the relevant papers and I just guess the content based on the title)&nbsp;</i>but the only way I can imagine \u201cbiological anchors\u201d can be used is as a fairly abstract model, a metaphor, really.&nbsp;</p><p>People won't really simulate a physical environment and physical brains literally. In a normal environment, actual evolution rarely selects for \u201cintelligence\u201d (no mutation/phenotype, environment has too many/no challenges). So you would skip a lot of features, and force mutation and construct challenges. I think a few steps along these lines means the simulation will use abstract digital entities, and this would be wildly faster.</p><p>It seems important to know more about why the author thinks that more literal, biological brains need to be simulated. This seems pretty core to the idea of her paper, where she says a brain-like models needs to be specified:</p><p><img src=\"https://lh3.googleusercontent.com/caLDtVOUah1V7NrpRTpUZL1dbGyOtRF9vvyWcuvH9NHducyShZyjiNIJPbzXd0wbRtMGzk6OHq8vxdV74m5xt4GeowWH2AaRBwmJZWESPbCUK2KMPrYY0qy9_YQmYzpHf4gu2HorTUUilIz950iqYd_xfOIj5KlnI98D-HFzj52dwsmWBgfRD6oVcQ\"></p><p>But I think it\u2019s implausible to expect a brain-like model to be the main candidate to emerge as dangerous AI (in the AI safety worldview) or useful as AGI for commercial reasons. The actual developed model will be different.</p><p>Another issue is that (in the AI safety worldview) specifying this \u201cAGI model\u201d seems dangerous by itself, and wildly backwards for the purpose of this exercise. Because of normal market forces, by the time you write up this dangerous model, someone would be running it. Requiring to see something close to the final model is like requiring to see an epidemic before preparing for one.</p>", "parentCommentId": null, "user": {"username": "Charles He"}}, {"_id": "pMDF7Tfg4EYhNBsAL", "postedAt": "2022-10-01T07:56:18.810Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p><strong>I don\u2019t know anything about machine learning, AI or math, but I\u2019m really uncertain about the technical section in the paper, on \u201cCan 2020 algorithms scale to TAI?.\u201d</strong></p><p>One major issue is that in places in her paper, the author expresses doubt that \u201c2020 algorithms\u201d can be the basis for computation for this exercise. However, she only deals with feed forward neural nets for the technical section.&nbsp;</p><p>This is really off to leave out other architectures.&nbsp;</p><p>If you try using feed forward neural nets, and compare them to RNN/LSTM for things like sequence like text generation, it\u2019s really clear they have a universe of difference. I think there\u2019s many situations where you can\u2019t get similar&nbsp; functionality in a DNN (or get things to converge at all) even with much more \"compute\"/parameter size. On the other hand, plain RNN/LTSM will work fine, and these are pretty basic models today.&nbsp;</p>", "parentCommentId": "dDjNQQdZmcTHG8B7s", "user": {"username": "Charles He"}}, {"_id": "9CqBQxjJPkZ3TdboF", "postedAt": "2022-10-01T08:10:03.957Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Nested inside of the above issue, another problem is that the author seems to use \u201cproof-like\" rhetoric in arguments, when she needs to provide broader illustrations that could generalize for intuition, because the proof actually isn\u2019t there.&nbsp;</p><p>Sometimes some statements don't seem to resemble how people use mathematical argumentation in disciplines like machine learning or economics.</p><p>&nbsp;</p><p>To explain, the author begins with an excellent point that it\u2019s bizarre and basically statistically impossible that a feed forward network can learn to do certain things through limited training, even though the actual execution in the model would be simple.&nbsp;</p><p>One example is that it can\u2019t learn the mechanics of addition for numbers larger than it has seen computed in training.</p><p><img src=\"https://lh5.googleusercontent.com/jEMG822J9dsoD1hS0KSo4Q-lJR9J5esCmbYYZ6RpT0208jkCxvz3syiN3KgxBbOP44Ri_KHZteXLHhBQYt0s4saXSzbyjlvHS7U7IDIRFUdXz26FzO_0Jg6Lm3W_0BqyRjzrzaApcezoay3GxzdU4v8Ncdz2gB_AUaJlCow5tQzqdmUsxj6NTXeLyw\"><br>&nbsp;</p><p><i>Basically, the most \u201cwell trained\u201d/largest feed forward DNN that uses backprop training, will never add 99+1 correctly, if it was only trained on adding smaller numbers like 12+17 &nbsp;if these calculations never total 100. This is because in backprop, the network literally needs to see and create processes for the 100 digits. This is despite the fact that it\u2019s simple (for a vast DNN) to \u201cmechanically have\u201d the capability to perform true logical addition.</i></p><p>&nbsp;</p><p>Immediately starting from the above point, I think author wants to suggests that, in the same way it\u2019s impossible to get this functionality above, this constrains what feed forward networks would do (and these ideas should apply to deep learning or 2020 technology for biological anchors).</p><p>However, everything sort of changes here. The author says:</p><p><img src=\"https://lh5.googleusercontent.com/KCbCtmT68q3-eJuEH2_bkOLBHKafp6YNGd4s7wNoQZcJcV84mvBv2XVx02-zqaRg8Etl6j2TxewSRn40E1NFKNUNiy9-08XGy260Xsxax5n005yQhSDQCRuriaxapyMBW0efv8DgoY9da5hy6OEZPRL9EuyjytLzTC0CRGg63WqxyyAQm-N5fo0LHg\"></p><p>I\u2019s not clear what is being claimed or what is being built on above.</p><ul><li>What computations are foreclosed or what can\u2019t be achieved in feed forward nets?&nbsp;<ul><li>While the author shows that addition with n+1 digits can't be achieved by training with addition with numbers with n digits\", and certainly many other training to outcomes are prevented, why would this generally rule out capability, and why &nbsp;would this stop other (maybe very sophisticated) training strategies/simulations from producing models that could be dangerous?</li></ul></li><li>The author says the \u201cupshot is that the class of solutions searched over by feedforward networks in practice seems to be<i><strong> (approximately) the space of linear models with all possible features</strong></i>\u201d and \u201c<i><strong>this is a big step up from earlier ML algorithms where one has to hand-engineer the features</strong></i>\u201d.&nbsp;<ul><li>But that seems to allow general transformations on the features. If so, that is incredibly powerful. It doesn't seem to constrain functionality (of these feed forward networks)?</li></ul></li><li>Why would the logic which relies on a technical proof (which I am guessing relies on a \"topological-like\" argument that requires the smooth structure of feed forward neural nets), apply to even to RNN or LTSM, or transformers?</li></ul>", "parentCommentId": "pMDF7Tfg4EYhNBsAL", "user": {"username": "Charles He"}}, {"_id": "tHgi2YLXbYhALeLQ2", "postedAt": "2022-10-01T09:44:51.184Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Hi, I'm the author of the piece. Thanks for the kind comment! I planned to share it on LW/AF after finishing a companion piece on the work by Roberts et al. that I use in the technical section, and I fell a bit behind schedule on writing the second piece, but I'll put both on LW/AF next week.</p>", "parentCommentId": "bgatsjJHubL9nxiaz", "user": {"username": "jylin04"}}, {"_id": "2JLJNv2jNXKQp2vZn", "postedAt": "2022-10-01T09:48:58.727Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Hi Charles, thanks for all the comments! I'll reply to this one first since it seems like the biggest crux. I completely agree with you that feedforward NNs != RNN/LSTM... and that I haven't given a crisp argument that the latter can't scale to TAI. But I don't think I claim to in the piece! All I wanted to do here was to (1) push back against the claim that the UAT for feedforward networks provides positive evidence that DL-&gt;TAI, and (2) give an example of a strategy that <i>could</i> be used to argue in a more principled way that other architectures won't scale up &nbsp;to certain capabilities, <i>if</i> one is able to derive effective theories for them as was done for MLPs by Roberts et al. (I think it would be really interesting to show this for other architectures and I'd like to think more about it in the future.)&nbsp;</p>", "parentCommentId": "pMDF7Tfg4EYhNBsAL", "user": {"username": "jylin04"}}, {"_id": "4PMP4jHom42v5wraH", "postedAt": "2022-10-01T09:53:02.511Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Regarding the questions about feedforward networks, a really short answer is that regression is a very limited form of inference-time computation that e.g. rules out using memory. (Of course, as you point out, this doesn't apply to other 2020 algorithms beyond MLPs.) Sorry about the lack of clarity -- I didn't want to take up too much space in this piece going into the details of the linked papers, but hopefully I'll be able to do a better job explaining it in a review of those papers that I'll post on LW/AF next week.</p><p>(I also want to reply to your top-level comments about the evolutionary anchor, but am a bit short on time to do it right now (since for those questions I don't have cached technical answers and will have to remind myself about the context). But I'll definitely get to it next week.)</p>", "parentCommentId": "9CqBQxjJPkZ3TdboF", "user": {"username": "jylin04"}}, {"_id": "e6TsFK6yGbgeHpd4D", "postedAt": "2022-10-01T10:41:29.562Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>I'm concerned my entry was not read. I submitted via the form. &nbsp;I pointed out two typos on a particular EA website (not as part of my criticism but clearly and prominently attached as an aside) and one was fixed and one was not. &nbsp;Since one was fixed I assume someone else pointed that one out. The typo is not ambiguously wrong so there would be no reason not to fix it.</p>", "parentCommentId": null, "user": {"username": "anonagainanon"}}, {"_id": "BoXPxbbdv5AuHLM27", "postedAt": "2022-10-01T13:21:55.628Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Is the UAT mentioned anywhere in the <a href=\"https://drive.google.com/drive/u/1/folders/15ArhEPZSTYU8f012bs6ehPS6-xmhtBPP\">bio anchors report</a> as a reason for thinking DL will scale to TAI? I didn't find any mentions of it quickly ctrl-fing in any of the 4 parts or the appendices.</p>", "parentCommentId": "2JLJNv2jNXKQp2vZn", "user": {"username": "elifland"}}, {"_id": "EQmohhaJZzL6LmSEo", "postedAt": "2022-10-01T13:23:43.369Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>I think she means the little blurbs in this post.</p>", "parentCommentId": "LGLgnPJjJG7GQskn4", "user": {"username": "technicalities"}}, {"_id": "czg2ABSKnCC4ie8yu", "postedAt": "2022-10-01T13:28:26.311Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Enormous +1</p>", "parentCommentId": "WhefbMcee2ppzL8pB", "user": {"username": "finm"}}, {"_id": "EHnstJBjwFwrY8AJ7", "postedAt": "2022-10-01T13:41:24.369Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/xcysfp6zb3JpCjKGq/what-i-learned-from-the-criticism-contest\">My thoughts and picks from judging the contest.&nbsp;</a></p><p>Many of my picks narrowly missed prizes and weren't upvoted much at the time, so check it out.</p>", "parentCommentId": null, "user": {"username": "technicalities"}}, {"_id": "LpsZSbq9JPHZmHAZR", "postedAt": "2022-10-01T13:42:11.108Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/xcysfp6zb3JpCjKGq/what-i-learned-from-the-criticism-contest\">Here's mine</a></p>", "parentCommentId": "xwKv6w9FLaDqctgyN", "user": {"username": "technicalities"}}, {"_id": "sYei6fbnexLrifnFF", "postedAt": "2022-10-01T14:03:13.129Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Hey <a href=\"https://forum.effectivealtruism.org/users/anonagainanon\">anonagainanon</a>, if you DM me I can look into this for you.</p>", "parentCommentId": "e6TsFK6yGbgeHpd4D", "user": {"username": "bruce"}}, {"_id": "tXSCeK5ccRQESDggS", "postedAt": "2022-10-01T15:21:04.303Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Yes, it's mentioned on page 19 of part 4 (as point 1, and my main concern is with point 2b).</p>", "parentCommentId": "BoXPxbbdv5AuHLM27", "user": {"username": "jylin04"}}, {"_id": "RKxx4pgeL5vYPvano", "postedAt": "2022-10-01T15:36:37.579Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Ah, thanks for the pointer</p>", "parentCommentId": "tXSCeK5ccRQESDggS", "user": {"username": "elifland"}}, {"_id": "zwcnteh5c8ENDkc4A", "postedAt": "2022-10-01T20:08:07.509Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>oohh, got it. thx.</p>", "parentCommentId": "EQmohhaJZzL6LmSEo", "user": {"username": "Noah Scales"}}, {"_id": "CYLLfzy6dfFA4kY4a", "postedAt": "2022-10-01T22:49:38.414Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Thanks for launching the contest!&nbsp;</p><p>It will be interesting to dig through this list and read through these posts, many seem to raise good points.</p><p>As the writer of the <a href=\"https://forum.effectivealtruism.org/posts/wXzc75txE5hbHqYug/the-great-energy-descent-short-version-an-important-thing-ea\">post </a>(well, 4 of them) on energy depletion, I will try to integrate the feedback you gave here. I shall make a follow-up post addressing the common counters given against claims of energy depletion, then - and explain why I'm still worried about this topic in spite of that. While I think that societies will try to adapt, my biggest concern is that there will be little time to transition if peak oil is already past us (a likely possibility). I will try to express more clearly why I think this way.</p>", "parentCommentId": null, "user": {"username": "Corentin Fressoz"}}, {"_id": "5yY5rKrayiFrRjvgq", "postedAt": "2022-10-02T01:45:56.609Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Thanks for the responses, they give a lot more useful context.</p><blockquote><p>(I also want to reply to your top-level comments about the evolutionary anchor, but am a bit short on time to do it right now (since for those questions I don't have cached technical answers and will have to remind myself about the context). But I'll definitely get to it next week.)<br>&nbsp;</p></blockquote><p>If it frees up your time, I don't think you need to write the above, unless you specifically want to. It seems reasonable to interpret that point on \"evolutionary anchors\" as a larger difference on the premise, and that is not fully in scope of the post. This difference and its phrasing is more disagreeable/overbearing to answer, so it's also less worthy of a response.&nbsp;</p><p>Thanks for writing your ideas.</p>", "parentCommentId": "4PMP4jHom42v5wraH", "user": {"username": "Charles He"}}, {"_id": "mW3ABtjDQZvLoik8j", "postedAt": "2022-10-02T07:09:47.681Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Wow, I'm glad I noticed Vegan Nutrition in among the winners. Many thanks to Elizabeth for writing, and I hope it will eventually appear as a post. A few months ago I spent some time looking around the forum for exactly this and gave up\u2014in hindsight, I should've been asking why it didn't exist!</p>", "parentCommentId": null, "user": {"username": "smountjoy"}}, {"_id": "2adQNs6tkRttisSuE", "postedAt": "2022-10-02T08:48:51.731Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>There is a full post planned, but I wanted actual data, which means running nutrition tests on the population I think is hurting, treating any deficiencies, and retesting. &nbsp;I have a grant for this (thanks SFF!) but even getting the initial tests done is taking months so the real post is a very long ways out.</p><p>PS. I have no more budget to pay for tests but if anyone wants to cover their own test ($613, slightly less if you already have or want to skip a genetic test) and contribute data I'd be delighted to have you. Please PM me for details.</p>", "parentCommentId": "mW3ABtjDQZvLoik8j", "user": {"username": "Elizabeth"}}, {"_id": "m87fxbZ8yYoeyksG2", "postedAt": "2022-10-02T09:05:00.731Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Congratulations, Corentin! &nbsp;Having just read part 1 in detail, I'm looking forward to more of your material.&nbsp;</p><p>Time and scale, as you said are the biggest concerns around adaptation. A virtue of not adapting with new infrastructure is that we save on carbon and energy put toward creating that infrastructure. Conservation could help more than anyone else believes, but it's the least sexy approach. I want to comment on these issues, but now I don't know where. Should I comment on the part 2 post, or in the google doc, or on the short contest version post?&nbsp;</p>", "parentCommentId": "CYLLfzy6dfFA4kY4a", "user": {"username": "Noah Scales"}}, {"_id": "ZHRzLQF3DPwgKr8xo", "postedAt": "2022-10-03T01:45:14.567Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>I suspect that Bad Omens will be looked back on as a highly influential post, and certainly has changed my mind more than any other community building post.</p>\n", "parentCommentId": "JjvpwGKeaj4w5FTzP", "user": {"username": "therealslimkt"}}, {"_id": "3ZjZTbje4tpkEFAGf", "postedAt": "2022-10-03T04:52:03.549Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/ALMX8BbR5HfhwbPwp/\">This post has been zapped</a> off the forum (it's downvoted so negative that few will see it):</p><p>I don't like the post, but I think people should know it exists (for a few days or something), so I'm posting it here.</p><p>(Commenting in this post because, why not?)</p><p><a href=\"https://forum.effectivealtruism.org/posts/ALMX8BbR5HfhwbPwp/\">https://forum.effectivealtruism.org/posts/ALMX8BbR5HfhwbPwp/</a></p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ae2bb2d9a2288e404be389e213a795867f3d25f7ff151c8a.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ae2bb2d9a2288e404be389e213a795867f3d25f7ff151c8a.png/w_90 90w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ae2bb2d9a2288e404be389e213a795867f3d25f7ff151c8a.png/w_180 180w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ae2bb2d9a2288e404be389e213a795867f3d25f7ff151c8a.png/w_270 270w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ae2bb2d9a2288e404be389e213a795867f3d25f7ff151c8a.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ae2bb2d9a2288e404be389e213a795867f3d25f7ff151c8a.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ae2bb2d9a2288e404be389e213a795867f3d25f7ff151c8a.png/w_540 540w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ae2bb2d9a2288e404be389e213a795867f3d25f7ff151c8a.png/w_630 630w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ae2bb2d9a2288e404be389e213a795867f3d25f7ff151c8a.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ae2bb2d9a2288e404be389e213a795867f3d25f7ff151c8a.png/w_810 810w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ae2bb2d9a2288e404be389e213a795867f3d25f7ff151c8a.png/w_899 899w\"></figure>", "parentCommentId": null, "user": {"username": "Charles He"}}, {"_id": "hPqiwaKbfccuDarM5", "postedAt": "2022-10-03T07:59:25.388Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Thanks for the feedback!</p><p>Yeah, I'd really like conservation of energy to take place, using only what we really need. Unfortunately, we are in an economic system that values using something as precious as oil as fast as possible in order to grow - meaning it will be harder for future generations to produce stuff like mosquito nets, antiseptics or aspirin, as they are all derived from oil. I talk about it in part 3.</p><p>For comments, it depends. For general comments that everyone could find more interesting, the summary post is probably more adapted (probably where conservation could go). For comments related more particularly to data related to one topic (transition or the economy or actions), then go part 1/2/3. &nbsp;If you think there is a useful comment to make in the Google Docs, go ahead because it allows to target specifically one sentence in particular.</p>", "parentCommentId": "m87fxbZ8yYoeyksG2", "user": {"username": "Corentin Fressoz"}}, {"_id": "gGkPvMppQQj4jBxFQ", "postedAt": "2022-10-03T10:29:54.757Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Thanks, that's a good point. Just posted it here: https://www.lesswrong.com/posts/TMHWfRE7zZkzgFDSo/a-review-of-the-bio-anchors-report</p>", "parentCommentId": "fymebyEKXp9DyteWX", "user": {"username": "jylin04"}}, {"_id": "28bwi2fi5qL8tfCaN", "postedAt": "2022-10-03T18:15:43.611Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>OK, I will put my conservation thoughts in the comments of the summary post.&nbsp;</p><p>The speed of posting and conversation changes on this forum is way faster than I can match, by the time I have something decent written up, conversation will have moved on.&nbsp;</p><p>Keep an eye out for my reply though, I'll come around when I can. Your work on this parallels a model I have of a pathway for our civilization coping with global warming. I call it \"muddling through.\" I know, catchy, right? How things go this century is very sensitive to short time scales, I'm thinking 10-20 year differences make all kinds of difference, and in my view, most needed changes are in human behavior and politics, not technology developments. So, good and bad.</p>", "parentCommentId": "hPqiwaKbfccuDarM5", "user": {"username": "Noah Scales"}}, {"_id": "odHG4hhM2FSGiXXFQ", "postedAt": "2022-10-03T18:39:09.609Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>What would have been really interesting is if someone wrote a piece critiquing the EA movement for showing little to no interest in scrutinizing the ethics and morality of Sam Bankman-Fried's wealth.&nbsp;</p><p>To put a fine point on it, has any of his wealth come from taking fees from the many scams, Ponzi schemes, securities fraud, money laundering, drug trafficking, etc. in the crypto markets? FTX has been affiliated with some shady actors (such as <a href=\"https://www.reuters.com/investigates/special-report/fintech-crypto-binance-dirtymoney/\">Binance</a>), and seems to be buying up more of them (such as <a href=\"https://www.coindesk.com/business/2022/08/22/ftx-could-buy-blockfi-for-only-15m-or-a-lot-more-if-crypto-lender-hits-big-goals/\">BlockFi</a>, known for <a href=\"https://www.sec.gov/news/press-release/2022-26\">securities fraud</a>). Why isn't there more curiosity on the part of &nbsp;EA, and more transparency on the part of FTX? Maybe there's a perfectly good explanation (and if so, I'll certainly retract and apologize), but it seems like that explanation ought to be more widely known.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Stuart Buck"}}, {"_id": "NX6fGAeW7ZeQrfKEB", "postedAt": "2022-10-03T19:57:58.016Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Someone has written something related: <a href=\"https://medium.com/@sven_rone/the-effective-altruism-movement-is-not-above-conflicts-of-interest-25f7125220a5\">https://medium.com/@sven_rone/the-effective-altruism-movement-is-not-above-conflicts-of-interest-25f7125220a5</a></p><blockquote><h1>Summary</h1><p>Sam Bankman-Fried, founder of the cryptocurrency exchange FTX, is a major donator to the Effective Altruism ecosystem and has pledged to eventually donate his entire fortune to causes aligned with Effective Altruism.&nbsp;</p><p><br>By relying heavily on ultra-wealthy individuals like Sam Bankman-Fried for funding, the Effective Altruim community is incentivized to accept political stances and moral judgments based on their alignment with the interests of its wealthy donators, instead of relying on a careful and rational examination of the quality and merits of these ideas. Yet, the Effective Altruism community does not appear to recognize that this creates potential conflicts with its stated mission of doing the most good by adhering to high standards of rationality and critical thought.</p><p>In practice, Sam Bankman-Fried has enjoyed highly-favourable coverage from 80,000 Hours, an important actor in the Effective Altruism ecosystem. Given his donations to Effective Altruism, 80,000 Hours is, almost by definition, in a conflict of interest when it comes to communicating about Sam Bankman-Fried and his professional activities. This raises obvious questions regarding the trustworthiness of 80,000 Hours\u2019 coverage of Sam Bankman-Fried and of topics his interests are linked with (quantitative trading, cryptocurrency, the FTX firm\u2026).</p><p>In this post, I argue that the Effective Altruism movement has failed to identify and publicize its own potential conflicts of interests. This failure reflects poorly on the quality of the standards the Effective Altruism movement holds itself to. Therefore, I invite outsiders and Effective Altruists alike to keep a healthy level of skepticism in mind when examining areas of the discourse and action of the Effective Altruism community that are susceptible to be affected by incentives conflicting with its stated mission. These incentives are not just financial in nature, they can also be linked to influence, prestige, or even emerge from personal friendships or other social dynamics. The Effective Altruism movement is not above being influenced by such incentives, and it seems urgent that it acts to minimize conflicts of interest.</p></blockquote>", "parentCommentId": "odHG4hhM2FSGiXXFQ", "user": null}, {"_id": "z9mozMP8qeHRCSs8M", "postedAt": "2022-10-03T21:46:00.812Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<blockquote><p>(Commenting in this post because, why not?)</p></blockquote><p>I'm not super familiar with forum norms personally, but a brief search suggests <a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum#Softer_discussion_norms_and_tips:~:text=that%20is%20kind%2C-,relevant%20to%20the%20discussion%20at%20hand,-%2C%20and%20honest.%20Note\">low relevancy</a> might be a reason here.</p>", "parentCommentId": "3ZjZTbje4tpkEFAGf", "user": {"username": "bruce"}}, {"_id": "ZEGnbehDt7zqF6Asm", "postedAt": "2022-10-04T14:28:38.290Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Ok, take your time !&nbsp;</p><p>I think the next 10-20 years will indeed be decisive. And yes, most needed changes are in human behavior and politics, not technology development. Reminds me of an essay called \"<a href=\"https://www.scribd.com/document/355795043/There-s-No-App-for-That-2017\">There's no App for That</a>\" by Richard Heinberg, where he exposes that, for problems of climate, energy, inequality of biodiversity loss :&nbsp;</p><blockquote><p>\"The real problem is that [\u2026] <strong>we are asking technology to solve problems that demand human moral intervention</strong>\u2014ones that require ethical decisions, behavior change, negotiation, and sacrifice. &nbsp;<br>By mentally shifting the burden for solving our biggest problems onto technology, we are collectively making fundamental moral and tactical errors ; moral, because we are abdicating our own human agency; tactical, because purely technological solutions are inadequate to these tasks. We need to rethink what we delegate to machines, and what we take responsibility for directly as moral beings\"</p></blockquote>", "parentCommentId": "28bwi2fi5qL8tfCaN", "user": {"username": "Corentin Fressoz"}}, {"_id": "SJwCFeDsC7rrdhK33", "postedAt": "2022-10-05T14:39:10.439Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p><strong>Personal highlights from a non-consequentialist, left-leaning panelist</strong><br>(<a href=\"https://twitter.com/xuanalogue/status/1576651516530012162\">Cross-posted from Twitter</a>.)</p><p>Another judge for the criticism contest here - figured I would share some personal highlights from the contest as well! &nbsp;I read much fewer submissions than the most active panelists (s/o to them for their hard work!), but given that I hold minority viewpoints in the context of EA &nbsp;(non-consequentialist, leftist), I thought people might find these interesting.</p><p><a href=\"https://twitter.com/xuanalogue/status/1532448674042695680\">I was initially pretty skeptical of the contest</a>, and its ability to attract thoughtful foundational critiques. But now that the contest is over, I've been pleasantly surprised!&nbsp;</p><p>To be clear, I still think there are important classes of critique missing. I would probably have framed the contest differently to encourage them, perhaps like what Michael Nielsen suggests here:</p><blockquote><p>It would be amusing to have a second judging panel, of people strongly opposed to EA, and perhaps united by some other ideology. I wouldn't be surprised if they came to different conclusions.</p></blockquote><p>I also basically agree with the critiques made in Zvi's <a href=\"https://forum.effectivealtruism.org/posts/qjMPATBLM5p4ABcEB/criticism-of-ea-criticism-contest\">criticism of the contest</a>. All that said, below are some of my favorite (1) philosophical (2) ideological (3) object-level critiques.</p><p><strong>(1) Philosophical Critiques</strong></p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/dQvDxDMyueLyydHw4/population-ethics-without-axiology-a-framework\"><i>Population Ethics Without Axiology: A Framework</i></a><br>Lukas Gloor's critique of axiological thinking was spot-on IMO. It gets at heart of why utilitarian EA/longtermism can lead to absurd conclusions, and how contractualist \"minimal morality\" addresses them. I think if people took Gloor's post seriously, it would strongly affect their views about what it means to \"do good\" in the first place: In order to \"not be a jerk\", one need not care about creating future happy people, whereas one probably should care about e.g. (global and intergenerational) justice.</li><li><a href=\"https://forum.effectivealtruism.org/posts/gLWmeKTe68ZHnomwy/on-the-philosophical-foundations-of-ea\"><i>On the Philosophical Foundations of EA</i></a><br>I also liked this critique of several EA arguments for consequentialism by Will MacAskill and AFAIK shared by other influential EAs like Holdern Karnofsky and Nick Beckstead. Korsgaard's response to Parfit's argument (against person-affecting views) was new to me!</li><li><a href=\"https://forum.effectivealtruism.org/posts/DKe5eQhJoLNMWgaQv/deontology-the-paralysis-argument-and-altruistic-longtermism\"><i>Deontology, the Paralysis Argument and altruistic longtermism</i></a><br>Speaking of non-consequentialism, this one is more niche, but William D'Alessandro's refutation of Mogensen &amp; MacAskill's \"paralysis argument\" that deontologists should be longtermists hit the spot IMO. The critique concludes that EAs / longtermists need to do better if they want to convince deontologists, which I very much agree with.</li></ul><p>A few other philosophical critiques I've yet to fully read, but was still excited to see:&nbsp;</p><ul><li><a href=\"https://centerforreducingsuffering.org/point-by-point-critique-of-why-im-not-a-negative-utilitarian/\">Magnus Vinding's defense of suffering-focused ethics, contra Ord.</a></li><li><a href=\"https://www.sentienceinstitute.org/blog/the-future-might-not-be-so-great\">Jacy Anthis on why the future might be net-negative, contra MacAskill.</a></li></ul><p><strong>(2) Ideological Critiques</strong></p><p>I'm distinguishing these from the philosophical critiques, in that they are about EA as a lived practice and actually existing social movement. At least in my experience, the strongest disagreements with EA are generally ideological ones.</p><p>Unsurprisingly, there wasn't participation from the most vocal online critics! (Why make EA better if you think it should disappear?) But at least one piece did examine the \"EA is too white, Western &amp; male\" and \"EA is neocolonialist\" critiques in depth:&nbsp;</p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/oD3zus6LhbhBj6z2F/red-teaming-contest-demographics-and-power-structures-in-ea\"><i>Red-teaming contest: demographics and power structures in EA</i></a><br>The piece focuses on GiveWell and how it chooses \"moral weights\" as a case study. It then makes recommendations for democratizing ethical decision-making, power-sharing and increasing relevant geographic diversity.<br><br>IMO this was a highly under-rated submission. It should have gotten a prize (at least $5k)! The piece doesn't say this itself, but it points toward a version of the EA movement that is majority non-white and non-Western, which I find both possible and desirable.</li></ul><p>There was also a slew of critiques about the totalizing nature of EA as a lived practice (many of which were awarded prizes):</p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/AjxqsDmhGiW9g8ju6/effective-altruism-in-the-garden-of-ends\"><i>Effective altruism in the garden of ends</i></a><br>I particularly liked this critique for being a first-person account from a (formerly) highly-involved EA about how such totalizing thinking can be really destructive.</li><li><a href=\"https://michaelnotebook.com/eanotes/\"><i>Notes on Effective Altruism</i></a><br>I also appreciated Michael Nielsen's critique, which discusses the aforementioned \"EA misery trap\", and also coins the term \"EA judo\" for how criticisms of EA are taken to merely improve EA, not discredit it.</li><li><a href=\"https://forum.effectivealtruism.org/posts/MjTB4MvtedbLjgyja/leaning-into-ea-disillusionment\"><i>Leaning into EA Disillusionment</i></a><br>A related piece is about disillusionment with EA, and how to lean into it. I liked how it creates more space for sympathetic critics of EA with a lot of inside knowledge - including those of us who've never been especially \"illusioned\" in the first place!</li></ul><p>That's it for the ideological critiques. This is the class of critique that felt the most lacking in my opinion. I personally would've liked more well-informed critiques from the Left, whether socialist or anarchist, on terms that EAs could appreciate. (Most such critiques I've seen are either <a href=\"https://jacobin.com/2015/08/peter-singer-charity-effective-altruism/\">no longer as relevant</a> or feel <a href=\"https://www.currentaffairs.org/2022/09/defective-altruism\">too uncharitable to be constructive</a>.)</p><p>There <i>was </i>one attempt to synthesize leftism and EA, but IMO not any better than this old piece by Joshua Kissel on \"<a href=\"https://www.pdcnet.org/scholarpdf/show?id=eip_2017_0018_0001_0068_0090&amp;pdfname=eip_2017_0018_0001_0068_0090.pdf&amp;file_type=pdf\">Effective Altruism and Anti-Capitalism</a>\". There have also been <a href=\"https://twitter.com/mutual_ayyde/status/1576164090707578881\">some fledgling anarchist critiques</a> circulating online that I would love to see written up in more detail.</p><p>(And maybe stay tuned for <i><strong>The Political Limits of Effective Altruism</strong></i>, <a href=\"https://twitter.com/xuanalogue/status/1576651564915519488\">the pessimistic critique I've yet to write</a> about the possibility of EA ever achieving what mass political movements achieve.)</p><p><strong>(3) Object-Level Critiques</strong></p><ul><li><a href=\"https://docs.google.com/document/d/1_GqOrCo29qKly1z48-mR86IV7TUDfzaEXxD3lGFQ8Wk/edit\"><i>Biological Anchors External Review</i></a><br>On AI risk, I'd be remiss not to highlight Jennifer Lin's review of the influential Biological Anchors report on AI timelines. I appreciated both the arguments against the neural network anchor, and the evolutionary anchor, and have become less convinced by the evolutionary anchor as a prediction for transformative AI by 2100.</li><li><a href=\"https://forum.effectivealtruism.org/posts/j7X8nQ7YvvA7Pi4BX/a-critique-of-ai-takeover-scenarios\"><i>A Critique of AI Takeover Scenarios</i></a><br>I also appreciated James Fodor's critique of AI takeover scenarios put forth by influential EAs like Holden Karnofsky and Ajeya Cotra. I share the skepticism about the takeover stories I've seen so far, which have often seemed to me way too quick and subjective in their reasoning.</li><li><a href=\"https://forum.effectivealtruism.org/posts/cXBznkfoPJAjacFoT/are-you-really-in-a-race-the-cautionary-tales-of-szilard-and\"><i>Are you really in a race? The Cautionary Tales of Szil\u00e1rd and Ellsberg</i></a><br>And of course, there's Haydn Belfield's cautionary tale about how nuclear researchers mistakenly thought they were in an arm's race, and how the same could happen (has happened?) with the race to \"AGI\".&nbsp;</li><li><a href=\"https://forum.effectivealtruism.org/posts/nBN6NENeudd2uJBCQ/the-most-important-climate-change-uncertainty\"><i>The most important climate change uncertainty</i></a><br>Outside of AI risk, I was glad to see this piece on climate change get an honorable mention! &nbsp;It dissects the disconnect between EA consensus and non-EAs about climate risk, and argues for more caution. (Disclosure: This was written by a friend, so I didn't vote on it.)</li><li><a href=\"https://forum.effectivealtruism.org/posts/hbejbRBpd6quqnTAB/red-teaming-cea-s-community-building-work-2\"><i>Red Teaming CEA\u2019s Community Building Work</i></a><br>Finally, I also appreciated this extensive critique of CEA's community-building work. I've yet to read it in full, but it resonates with challenges working with CEA I've witnessed while on the board of another EA organization.</li></ul><p>There's of course tons more that I didn't get the chance to read. I wish I'd had the time! While the results of the contest of won't please everyone - much less the most trenchant EA critics - I still think the world is still better for it, and I'm now more optimistic about this particular contest format and incentive scheme <a href=\"https://twitter.com/xuanalogue/status/1532448674042695680\">than I was previously</a>.</p>", "parentCommentId": null, "user": {"username": "xuan"}}, {"_id": "z6QBFT4mRxFwZugjH", "postedAt": "2022-10-05T15:35:56.236Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>This is Miranda Kaplan, a communications associate at GiveWell. We want to thank everyone who submitted reviews of GiveWell's analyses as contest entries! It's extremely valuable to us when people outside our organization engage deeply and critically with our work. We will always do our best to consider and respond to such critiques, though, given other pressures on staff time, we may not be able to publish a full response.<br><br>Although CEA's done it in the post above, we'll take this opportunity to again plug our <a href=\"https://www.givewell.org/research/change-our-mind-contest\">Change Our Mind Contest</a>, which closes on October 31. Entries submitted to the Criticism and Red Teaming Contest are eligible, as long as they meet all other Change Our Mind Contest requirements (described <a href=\"https://www.givewell.org/research/change-our-mind-contest\">here</a>). We look forward to reading your work and thank you in advance for your participation!</p>", "parentCommentId": null, "user": {"username": "GiveWell"}}, {"_id": "RxbBho6n5JjejzEAi", "postedAt": "2022-10-06T16:08:07.951Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Glad to see your great posts about energy depletion in this list! Congratulations.</p>", "parentCommentId": "CYLLfzy6dfFA4kY4a", "user": {"username": "Miguel Lima Med\u00edn"}}, {"_id": "EtL2pmdgNt8RpTN2y", "postedAt": "2022-10-07T07:34:13.606Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Thanks !</p>", "parentCommentId": "RxbBho6n5JjejzEAi", "user": {"username": "Corentin Fressoz"}}, {"_id": "LKj2gJnK7FhKxarZv", "postedAt": "2022-12-07T18:31:42.539Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": "<p>Some of the winning entries from the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest\"><u>EA Criticism and Red Teaming Contest</u></a> have now been narrated:</p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/cXBznkfoPJAjacFoT/are-you-really-in-a-race-the-cautionary-tales-of-szilard-and\"><u>Are you really in a race? The cautionary tales of Szil\u00e1rd and Ellsberg</u></a> by Haydn Belfield</li><li><a href=\"https://forum.effectivealtruism.org/posts/coryFCkmcMKdJb7Pz/does-economic-growth-meaningfully-improve-well-being-an\"><u>Does economic growth meaningfully improve well-being?</u></a> by Vadim Albinsky</li><li><a href=\"https://forum.effectivealtruism.org/posts/AjxqsDmhGiW9g8ju6/effective-altruism-in-the-garden-of-ends\"><u>Effective altruism in the garden of ends</u></a> by Tyler Alterman</li><li><a href=\"https://forum.effectivealtruism.org/posts/NHsH9pHZ7rA3KcnC7/notes-on-effective-altruism\"><u>Notes on effective altruism</u></a> by Michael Nielsen</li><li><a href=\"https://forum.effectivealtruism.org/posts/bFDwxxfErRStMvuAQ/biological-anchors-external-review-by-jennifer-lin-linkpost\"><u>Biological Anchors external review</u></a> by Jennifer Lin</li></ul><p>You can listen via the [EA Forum Podcast](https://forum.effectivealtruism.org/posts/K5Snxo5EhgmwJJjR2/announcing-audio-narrations-of-ea-forum-posts-1) or on the individual post pages themselves.</p>", "parentCommentId": null, "user": {"username": "Peter_Hartree"}}, {"_id": "fSZQdKbRMJxXMxX9j", "postedAt": "2022-10-03T21:56:37.300Z", "postId": "YgbpxJmEdFhFGpqci", "htmlBody": null, "parentCommentId": "NX6fGAeW7ZeQrfKEB", "user": null}]