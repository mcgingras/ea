[{"_id": "2nD8JmeuH4kbri4AA", "title": "Perch\u00e9 i rischi di sofferenza sono i rischi esistenziali peggiori e come possiamo prevenirli", "postedAt": "2023-01-17T11:14:36.408Z", "htmlBody": "<p><i>This is a stub for the Italian translation of </i><a href=\"https://forum.effectivealtruism.org/posts/7g7ou3g8nM7seASBP/why-s-risks-are-the-worst-existential-risks-and-how-to\"><i><strong>Why s-risks are the worst existential risks, and how to prevent them</strong></i></a></p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=jiZxEJcFExc\"><div><iframe src=\"https://www.youtube.com/embed/jiZxEJcFExc\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Gli altruisti efficaci che si concentrano sul futuro lontano si trovano a dover scegliere tra diversi tipi di interventi. Tra questi, gli sforzi per ridurre il rischio di estinzione umana sono quelli che finora hanno ricevuto pi\u00f9 attenzione. Nel suo discorso Max Daniel porta avanti l\u2019idea che forse dovremmo rafforzare questo tipo di lavoro con interventi che puntino a prevenire futuri ben poco desiderabili (\"rischi di sofferenza\") e questo \u00e8 un motivo in pi\u00f9 per concentrarsi sui rischi delle IA tra tutte le fonti di rischio esistenziale finora individuate.</i><br>&nbsp;</p>", "user": {"username": "EA Italy"}}, {"_id": "cP2RtfW557a3LukBW", "title": "[Opzionale] Che cos'\u00e8 una prova o evidenza?", "postedAt": "2023-01-17T11:12:25.723Z", "htmlBody": "<p><i>This is an Italian translation of </i><a href=\"https://forum.effectivealtruism.org/posts/sFGBzeh4PqWWpFJiB/what-is-evidence\"><i><strong>What is evidence?</strong></i></a></p><blockquote><p>La frase \"la neve \u00e8 bianca\" \u00e8&nbsp;<i>vera&nbsp;</i>se e solo se la neve \u00e8 bianca.<br>\u2014 Alfred Tarski</p></blockquote><p>&nbsp;</p><blockquote><p>Dire di ci\u00f2 che \u00e8 che \u00e8, o di ci\u00f2 che non \u00e8 che non \u00e8, \u00e8&nbsp;<i>vero</i>.</p><p>\u2014 Aristotele,&nbsp;<i>Metafisica IV</i></p></blockquote><p>Immaginate di stare camminando per strada quando a un certo punto vi si slaccia una scarpa. Nulla di che, ma dopo un po', per qualche strana ragione, cominciate a&nbsp;<i>credere&nbsp;</i>che le vostre scarpe siano slacciate. La luce che viene dal sole colpisce i lacci delle vostre scarpe e si allontana; alcuni fotoni vengono catturati dalle vostre pupille e raggiungono la retina; l\u2019energia di questi fotoni d\u00e0 vita a impulsi neurali; gli impulsi neurali vengono trasmessi alle aree del vostro cervello deputate all\u2019elaborazione delle immagini; qui l\u2019informazione visiva \u00e8 elaborata, ricostruita in un modello 3D e identificata come una scarpa slacciata. Una sequenza di eventi, una catena di cause ed effetti, nel mondo e nel vostro cervello, alla fine delle quali vi ritrovate a credere a quello in cui credete. Il risultato di questo processo \u00e8 uno stato&nbsp;<i>mentale&nbsp;</i>che rispecchia lo stato reale delle vostre&nbsp;<i>scarpe.</i></p><p>Che cos\u2019\u00e8 una&nbsp;<i>prova&nbsp;</i>o<i> evidenza</i>? \u00c8 un evento \u201cintrecciato\u201d, attraverso catene di causa ed effetto, con quello che vuoi sapere. Se l\u2019obiettivo della vostra indagine sono i lacci delle scarpe, ad esempio, allora la luce che attraversa le vostre pupille \u00e8 una prova correlata ai lacci.&nbsp; Uso il termine \u201cintrecciato\u201d per riferirmi alla causalit\u00e0, ossia a due cose che si trovano collegate tra loro per via di un rapporto di causa ed effetto.</p><p>Non tutti gli effetti creano un \u201cintreccio\u201d necessario per portar ad un\u2019evidenza. Avere un dispositivo che suona quando vi inserite i numeri vincenti della lotteria non \u00e8 di alcun aiuto se il dispositivo suona&nbsp;<i>anche&nbsp;</i>quando inserite&nbsp;<i>altri&nbsp;</i>numeri. La luce che tocca le vostre scarpe non sarebbe molto utile come prova riguardo lo stato dei lacci, se i fotoni si organizzassero nello stesso stato fisico sia quando i lacci sono slegati che quando non lo sono.</p><p>Pi\u00f9 in astratto: perch\u00e9 un evento possa essere considerato una&nbsp;<i>prova valida&nbsp;</i>per una data indagine, deve avvenire&nbsp;<i>diversamente&nbsp;</i>in un modo che sia intrecciato con i&nbsp;<i>diversi&nbsp;</i>stati possibili dell\u2019oggetto in esame. (In linguaggio tecnico: deve esserci&nbsp;<a href=\"https://it.wikipedia.org/wiki/Informazione_mutua\"><u>mutua informazione</u></a> tra l\u2019evento che porta la prova e l\u2019oggetto dell\u2019indagine, relativamente al vostro attuale stato di incertezza su entrambi).</p><p>Se elaborato correttamente, tale intreccio pu\u00f2 essere contagioso, che \u00e8 il motivo per cui \u00e8 necessario avere un cervello e un paio di occhi. Se i fotoni toccano i lacci delle scarpe e si spostano poi su un sasso, quel sasso non cambier\u00e0 granch\u00e9. Il sasso non rifletter\u00e0 i lacci delle scarpe in maniera significativa, n\u00e9 cambier\u00e0 in maniera visibile in base a una scarpa allacciata o slacciata. Che \u00e8 poi il motivo per cui le testimonianze dei sassi non sono granch\u00e9 utili in tribunale. Una pellicola fotografica catturer\u00e0 la causalit\u00e0 fra lacci e i fotoni in arrivo in modo che la foto possa essere usata come prova. Se i vostri occhi e il vostro cervello funzionano adeguatamente,&nbsp;<i>voi stessi&nbsp;</i>sarete in un rapporto di causalit\u00e0 con i lacci delle vostre scarpe.</p><p>Questo \u00e8 il motivo per cui i razionalisti danno cos\u00ec tanta importanza all\u2019affermazione, al limite del paradosso, secondo cui la credenza di una persona \u00e8 degna di essere presa in considerazione solo se, in linea di principio, si potrebbe convincere quella persona a credere diversamente. Se lo stato della vostra retina fosse il medesimo a prescindere dal tipo di luce che vi entra, allora sareste ciechi. Alcuni sistemi di credenze, nel tentativo piuttosto ovvio di dare credito a se stessi, affermano che certe credenze sono degne di considerazione solo se ci si crede&nbsp;<i>incondizionatamente,&nbsp;</i>a prescindere da quello che uno possa vedere o pensare. Lo stato in cui si trova il vostro cervello dev\u2019essere sempre lo stesso. Da cui l\u2019espressione \"credere ciecamente\": se ci\u00f2 in cui credete non dipende da quello che vedete, allora \u00e8 come se qualcuno vi avesse accecati ficcandovi un dito nell\u2019occhio.</p><p>Se i vostri occhi e il vostro cervello funzionano correttamente, le vostre credenze saranno per forza intrecciate ai fatti.&nbsp;<i>Il pensiero razionale d\u00e0 vita a credenze che sono esse stesse delle prove.</i></p><p>Se la vostra bocca dice la verit\u00e0, allora le vostre credenze razionali, esse stesse una prova, funzioneranno come prova per qualcun altro. Questo intreccio pu\u00f2 trasmettersi attraverso una serie di cause ed effetti \u2013 e se voi parlate e qualcuno ascolta, si tratta di causa ed effetto. Quando al telefono dite \u201cho le scarpe slacciate\u201d, state condividendo con un amico uno stato di causalit\u00e0 con le vostre scarpe.</p><p>Ne consegue che, tra persone oneste che pensano che anche chi hanno di fronte sia onesto, le credenze razionali sono contagiose. Ecco perch\u00e9 dire che le vostre credenze&nbsp;<i>non&nbsp;</i>sono contagiose \u2013 che ci credete per motivi personali che non sono trasmissibili \u2013 sembra parecchio sospetto. Se le vostre credenze sono intrecciate alla realt\u00e0, allora&nbsp;<i>dovrebbero&nbsp;</i>essere contagiose tra persone oneste.</p><p>Se il vostro modello della realt\u00e0 vi suggerisce che il risultato dei vostri processi mentali&nbsp;<i>non&nbsp;</i>dovrebbe essere contagioso per gli altri, allora quel modello vi sta dicendo che le vostre credenze non sono davvero delle prove, il che significa che non sono intrecciate alla realt\u00e0. Quello che dovreste fare \u00e8 intervenire per correggerle e smettere di crederci.</p><p>In effetti, se&nbsp;<i>sentirete</i>, a livello&nbsp;<i>non logico</i>, cosa questo&nbsp;<i>significa</i>, smetterete&nbsp;<i>automaticamente&nbsp;</i>di crederci, perch\u00e9 \u201cla mia credenza non \u00e8 intrecciata alla realt\u00e0\u201d&nbsp;<i>equivale a dire\u201d</i>la mia credenza non \u00e8 esatta\u201d. Non appena smetterete di credere che \u201c \u2018la neve \u00e8 bianca\u2019 \u00e8 vero\u201d, dovreste smettere (automaticamente!) di credere che \u201cla neve \u00e8 bianca\u201d, altrimenti c'\u00e8 qualcosa che non va.</p><p>Cercate di spiegare perch\u00e9 i processi mentali che impiegate producono sistematicamente credenze che rispecchiano la realt\u00e0. Spiegate perch\u00e9 pensate di essere razionali. Perch\u00e9 pensate che, con gli stessi processi mentali che usate voi, le altre persone dovrebbero credere che \u201cla neve \u00e8 bianca\u201d se e solo se la neve \u00e8 bianca. Se&nbsp;<i>non&nbsp;</i>credete che il risultato dei vostri processi mentali sia intrecciato alla realt\u00e0, allora perch\u00e9 credete al risultato dei vostri processi mentali? \u00c8 la stessa cosa, o perlomeno dovrebbe esserlo.<br>&nbsp;</p>", "user": {"username": "EA Italy"}}, {"_id": "WNh3td6HxnajCgZ4L", "title": "[Opzionale] Far pagare l\u2019affitto alle proprie credenze", "postedAt": "2023-01-17T11:11:18.440Z", "htmlBody": "<p><i>This is an Italian translation of </i><a href=\"https://forum.effectivealtruism.org/posts/k5E94C9arB8fx7xni/making-beliefs-pay-rent\"><i><strong>Making beliefs pay rent</strong></i></a></p><p>Cos\u00ec comincia una classica storia:</p><blockquote><p><i>Se un albero cade in una foresta e non c'\u00e8 nessuno a sentirlo, fa rumore? Una persona dice \u201cS\u00ec, perch\u00e9 produce vibrazioni nell\u2019aria\u201d, e un\u2019altra dice \u201cNo, perch\u00e9 non c'\u00e8 nessun cervello con il suo meccanismo uditivo nei paraggi.\u201d</i></p></blockquote><p>Se esiste un\u2019abilit\u00e0 fondamentale nell\u2019arte della razionalit\u00e0, una forma mentis su cui poggiano tutte le altre tecniche, potrebbe essere questa: l\u2019abilit\u00e0 di notare, nella propria testa, sia i segni psicologici dell\u2019avere una mappa mentale di qualcosa, che quelli del non averla.</p><p>Immaginiamo che, dopo che l\u2019albero \u00e8 caduto, i due che stavano discutendo passeggino assieme nella foresta. Si aspetter\u00e0 forse uno dei due di vedere l\u2019albero caduto alla loro destra, mentre l\u2019altro alla loro sinistra? Immaginiamo che, prima che l\u2019albero cada, i due mettano vicino all\u2019albero un registratore. Si aspetter\u00e0 forse uno dei due di sentire qualcosa di diverso dall\u2019altro, quando ascoltano la registrazione? Immaginiamo anche che colleghino un elettroencefalogramma a qualsiasi cervello al mondo: si aspetter\u00e0 forse uno dei due di vedere un tracciato diverso rispetto all\u2019altro?</p><p>Anche se i due discutono, uno dice \"No\" e l\u2019altro dice \"S\u00ec\", non si aspettano di avere esperienze diverse. Pensano di avere schemi diversi del mondo, ma non che questi schemi differiscano per quel che riguarda ci\u00f2 che si aspettano&nbsp;<i>succeder\u00e0</i>; quando si parla di esperienze sensoriali, le loro mappe del mondo sono identiche.</p><p>Potremmo essere tentati di eliminare questa categoria di errori affermando che l\u2019unica credenza legittima sia l\u2019aspettativa di un\u2019esperienza sensoriale. Ma in realt\u00e0, molte cose nel mondo non possono essere esperite con i cinque sensi. Non vediamo gli atomi che formano il mattone, ma gli atomi ci sono comunque. Sotto i vostri piedi c\u2019\u00e8 un pavimento, ma non avete<i> esperienza&nbsp;</i>diretta del pavimento; vedete la luce&nbsp;<i>riflessa&nbsp;</i>sul pavimento, anzi, vedete ci\u00f2 che la vostra retina e la vostra corteccia visiva hanno ricavato da quella luce. Ricavare il pavimento partendo dal vedere il pavimento significa fare un passo indietro ed entrare nelle cause nascoste dell\u2019esperienza. Forse \u00e8 un passo molto breve, ma \u00e8 comunque un passo indietro.</p><p>Vi trovate in cima a un palazzo accanto a un orologio a pendolo che segna le ore, i minuti e i secondi. Avete in mano una palla da bowling, che fate cadere dal tetto. Su quale ticchettio delle lancette sentirete l\u2019urto della palla da bowling che colpisce il suolo?</p><p>Per poter rispondere con precisione, dovete ricorrere alle vostre credenze personali, come ad esempio \"la gravit\u00e0 terrestre \u00e8 pari a 9,8 metri al secondo per secondo\" e \"questo edificio \u00e8 alto circa 120 metri\". Queste credenze non sono aspettative, semplici ed inarticolate, di un\u2019esperienza sensoriale; sono fatte (pi\u00f9 o meno) di parole, sono proposizionali. Non sarebbe una grande esagerazione, probabilmente, descrivere queste due credenze come frasi, composte da parole. Eppure queste due credenze hanno una&nbsp;<i>conseguenza&nbsp;</i>inferenziale che \u00e8 un\u2019aspettativa sensoriale precisa: se la lancetta dei minuti dell\u2019orologio \u00e8 sul 12 quando lasciate cadere la palla, vi aspetterete di vederla sull\u20191 quando sentirete il tonfo cinque secondi dopo. Per prevedere con la massima precisione possibile queste esperienze sensoriali, dobbiamo elaborare credenze che non sono aspettative di esperienze sensoriali.</p><p>Uno dei vantaggi che l\u2019<i>Homo sapiens&nbsp;</i>ha su ogni altra specie al mondo \u00e8 che noi possiamo imparare a dare forma a ci\u00f2 che non vediamo. \u00c8 anche uno dei nostri punti deboli. Gli esseri umani spesso credono a cose che non solo non vedono, ma che non esistono neanche.</p><p>Quello stesso cervello che crea una rete di cause inferite alla base di un\u2019esperienza sensoriale pu\u00f2 anche creare una rete di cause slegate da un\u2019esperienza sensoriale, oppure collegate male. Gli alchimisti pensavano che il flogisto creasse il fuoco \u2013 potremmo semplicisticamente modellare il loro modo di pensare con un nodo etichettato \u201cFlogisto\u201d collegato con una freccia all\u2019esperienza sensoriale di un fuoco che scoppietta \u2013 ma questa credenza non ha portato ad alcuna previsione concreta; il collegamento tra il flogisto e l\u2019esperienza \u00e8 sempre stato creato dopo l\u2019esperienza invece di limitare in anticipo l\u2019esperienza.</p><p>Immaginate che il vostro insegnante vi dica che il famoso scrittore Wulky Wilkinsen \u00e8 in realt\u00e0 un \"autore retroposizionato\", cosa che si pu\u00f2 dedurre dal fatto che le sue opere mostrano una \"resublimazione alienata\". Forse il vostro insegnante lo sa perch\u00e9 \u00e8 stato il suo insegnante a dirglielo, ma forse l\u2019unica cosa che \u00e8 in grado di dirvi sulla resublimazione \u00e8 che \u00e8 un tratto del pensiero retroposizionato e che questo tipo di pensiero \u00e8 caratterizzato da una resublimazione alienata. Cosa dovreste quindi aspettarvi dalle opere di Wulky Wilkinsen?</p><p>Nulla. Questa credenza, se cos\u00ec possiamo chiamarla, non \u00e8 collegata a nessuna esperienza sensoriale. Ma \u00e8 meglio che vi ricordiate che \"Wulky Wilkinsen\" ha la caratteristica di \"retroposizionamento\", nonch\u00e9 quella di \"resublimazione alienata\", in modo che possiate recitarle a memoria al prossimo esame. Le due credenze sono collegate tra loro ma non c'\u00e8 nessun collegamento con un\u2019esperienza attesa.</p><p>Siamo in grado di creare interi sistemi di credenze che sono collegate solo tra di loro; potremmo chiamarle \u201ccredenze fluttuanti\u201d. Di tutte le specie animali, solo gli esseri umani hanno questa anomalia mentale, questa perversione della capacit\u00e0 dell\u2019<i>Homo sapiens&nbsp;</i>di creare sistemi di credenze pi\u00f9 flessibili e pi\u00f9 generali.</p><p>La virt\u00f9 razionalista dell\u2019<i>empirismo</i> consiste nel chiedersi continuamente quali esperienze sono predette \u2013 o meglio ancora, vietate \u2013 dalle nostre credenze. Pensate che il flogisto produca il fuoco? Allora, sulla base di ci\u00f2, cosa vi aspettate che succeda? Pensate che Wulky Wilkinsen sia un autore retroposizionato? Allora, sulla base di ci\u00f2, cosa vi aspettate di vedere? No, non la \"resublimazione alienata\";&nbsp;<i>quale esperienza avrete</i>? Pensate che se un albero cade in una foresta fa rumore anche se non c'\u00e8 nessuno a sentirlo? Allora quale sar\u00e0 la vostra esperienza?</p><p>Meglio ancora: chiedetevi quale esperienza&nbsp;<i>non potrete&nbsp;</i>avere. Pensate che l\u2019<i>\u00c9lan vital</i> sia la spiegazione per la misteriosa carica vitale degli esseri viventi? E allora cosa&nbsp;<i>non pu\u00f2&nbsp;</i>succedere per via di questa credenza? Quale dato la falsificherebbe inequivocabilmente? Una risposta nulla significa che la vostra credenza non&nbsp;<i>limita&nbsp;</i>l\u2019esperienza, ma fa s\u00ec che possa succedere&nbsp;<i>qualsiasi cosa</i>. \u00c8 fluttuante.</p><p>Quando discutete di qualcosa che vi sembra fattuale, tenete sempre a mente di quale differenza nell\u2019aspettativa state parlando. Se non riuscite a trovarne nessuna, allora forse state discutendo di etichette mentali nel vostro sistema di credenze \u2013 o peggio ancora, credenze fluttuanti, aggrappate al vostro sistema. Se non vi rendete conto delle esperienze insite nelle opere retroposizionate di Wulky Wilkinsen, potete andare avanti a discutere per l\u2019eternit\u00e0.</p><p>Soprattutto, non domandatevi in cosa dovreste credere: chiedetevi cosa dovreste aspettarvi. Ogni dubbio su una credenza dovrebbe scaturire da una domanda su cosa aspettarsi e quella domanda dovrebbe essere il fulcro del vostro ragionamento. Ogni ipotesi di credenza dovrebbe scaturire da una specifica ipotesi di aspettativa e dovrebbe continuare a pagare l\u2019affitto in aspettative future. Se una credenza non paga l\u2019affitto, sfrattatela.</p>", "user": {"username": "EA Italy"}}, {"_id": "PvEsBDpbuTkmP84Et", "title": "[Opzionale] Guida alla regola di Bayes", "postedAt": "2023-01-17T11:10:15.420Z", "htmlBody": "<p>Non ancora tradotto</p><p><br>&nbsp;</p>", "user": {"username": "EA Italy"}}, {"_id": "sxcex5KomcHgojzhc", "title": "Prevenire una catastrofe\nlegata all'intelligenza\nartificiale", "postedAt": "2023-01-17T11:07:38.679Z", "htmlBody": "<p><i>This is an Italian translation of </i><a href=\"https://forum.effectivealtruism.org/posts/zpReK9a8gkpGNYmBt/preventing-an-ai-related-catastrophe-problem-profile\"><i><strong>Preventing an AI-related catastrophe - Problem profile</strong></i></a></p><p>Noi di 80,000 Hours abbiamo appena pubblicato la nostra analisi pi\u00f9 lunga e pi\u00f9 dettagliata di sempre. Parla di come ridurre i rischi esistenziali legati alle IA e si pu\u00f2 trovare&nbsp;<a href=\"https://drive.google.com/file/d/1TmdOkEZZvF9LXB5sMp8iohCqXz5H5KGC/view\">qui</a>.</p><p>Il resto di questo intende fornire un background del profilo, una sinossi e un indice dei contenuti.</p><h1><strong>Un po' di background</strong></h1><p>Come molti dei nostri contenuti, questo profilo \u00e8 indicato a chi ha gi\u00e0 passato un po' di tempo sul nostro sito ma non ha familiarit\u00e0 con il mondo dell\u2019Altruismo Efficace, ragion per cui \u00e8 un\u2019analisi piuttosto introduttiva. Ci\u00f2 nonostante, speriamo che possa essere utile anche ai membri della comunit\u00e0 dell\u2019Altruismo Efficace.</p><p>Il profilo rappresenta principalmente il mio (Benjamin Hilton) punto di vista, ma \u00e8 stato corretto da Arden Koehler (il nostro direttore) e revisionato da Howie Lempel (il nostro CEO). Entrambi sono d'accordo in linea generale con le mie conclusioni.</p><p>Alcuni degli accorgimenti che ho preso per fare in modo che questo profilo potesse essere il pi\u00f9 utile possibile per chi \u00e8 nuovo a queste tematiche:</p><ul><li>&nbsp;Mi sono concentrato su quello che per me \u00e8 il problema pi\u00f9 importante: i rischi di IA alla ricerca di potere che potrebbero derivare da sistemi di progettazione senzienti con capacit\u00e0 avanzate, come delineato da&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ChuABPEXmRumcJY57/video-and-transcript-of-presentation-on-existential-risk\">Joe Carlsmith</a>.&nbsp;</li><li>&nbsp;Ho cercato di parlare sempre in termini concreti e ho scritto un articolo a parte su <a href=\"https://80000hours.org/articles/what-could-an-ai-caused-existential-catastrophe-actually-look-like/\">come potrebbe avvenire una catastrofe causata da IA.</a> (Devo molto al report di Carlsmith, a&nbsp;<a href=\"https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like\"><i>What failure looks like</i></a> di Cristiano e a&nbsp;<i>Superintelligenza&nbsp;</i>di Bostrom.)&nbsp;</li><li>&nbsp;Ho fornito (quelle che secondo me sono) informazioni di base importanti, come i risultati dei&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#experts-are-concerned\">sondaggi degli esperti di machine learning sul rischio legato alle IA</a>, una panoramica dei&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#making-advances-extremely-quickly\">progressi recenti nel campo delle IA</a> e&nbsp;sulle <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#scaling\">leggi di scala</a>.&nbsp;</li><li>Ho cercato di spiegare con sincerit\u00e0&nbsp;perch\u00e9 le<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#best-arguments-against-this-problem-being-pressing\"> argomentazioni che presento potrebbero rivelarsi errate</a>.&nbsp;</li><li>Ho incluso una lunga <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#good-responses\">lista di&nbsp;domande frequenti</a> in cui presento quelle che ritengo essere le risposte migliori alle obiezioni pi\u00f9 comuni sul lavorare ai rischi delle IA.</li></ul><p>Inoltre, se volete darmi un feedback su questi contenuti e preferite non farlo pubblicamente, potete usare l\u2019<a href=\"https://forms.gle/RRZaFTfdDkSQ6fJG8\">apposito modulo</a>.<br>Questo post include una sinossi dell\u2019articolo e un indice dei contenuti.</p><h1><strong>Sommario</strong></h1><p>Ci aspettiamo di vedere progressi significativi nel campo delle IA nei prossimi decenni, forse anche di raggiungere un punto in cui le macchine supereranno gli umani in molti, o tutti, gli ambiti. Questo potrebbe portare vantaggi incredibili, aiutarci a risolvere problemi globali attualmente irrisolvibili, ma potrebbe anche comportare gravi rischi. Questi rischi potrebbero emergere per errore (ad esempio se non troviamo una soluzione tecnica per il problema della sicurezza delle IA) o volontariamente (ad esempio se le IA dovessero far precipitare un conflitto geopolitico). Pensiamo anche che sia necessario lavorare di pi\u00f9 in questo campo per ridurre questi rischi.</p><p>Alcuni rischi legati a IA avanzate potrebbero essere&nbsp;<a href=\"https://80000hours.org/articles/existential-risks/\">esistenziali </a>\u2014 vale a dire che potrebbero portare l\u2019umanit\u00e0 all\u2019estinzione o comunque provocare cos\u00ec tanti danni da farci perdere il ruolo di specie dominante sul pianeta.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefw4jrw3yo2f\"><sup><a href=\"#fnw4jrw3yo2f\">[1]</a></sup></span>&nbsp;Ad oggi non ci sono ancora soluzioni soddisfacenti che ci permettano di sviluppare e integrare nella societ\u00e0 questa tecnologia trasformativa, il cui avvento sembra sempre pi\u00f9 vicino. Secondo le nostre stime attualmente ci sono circa 300 persone nel mondo che stanno lavorando a questo problema.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhqv90hp7u45\"><sup><a href=\"#fnhqv90hp7u45\">[2]</a></sup></span>&nbsp;Il risultato \u00e8 che una catastrofe causata da un\u2019IA potrebbe essere il problema pi\u00f9 urgente al mondo, nonch\u00e9 quello su cui dovrebbero assolutamente lavorare coloro che possono dare un importante contributo. Tra le soluzioni pi\u00f9 promettenti attualmente ci sono la ricerca tecnica per la creazione di intelligenze artificiali sicure, la ricerca strategica sui possibili rischi legati alle IA e la ricerca sulle politiche che governi e compagnie dovrebbero adottare per scongiurare questi rischi. Se verranno sviluppate politiche soddisfacenti, ci sar\u00e0 bisogno di persone che le istituiscano e che le attuino. Ci sono anche molti ruoli complementari che danno la possibilit\u00e0 di avere un grande impatto, come ad esempio operation management, giornalismo, guadagnare per donare e altri ancora. Alcuni sono elencati qui sotto.</p><h2><strong>Il nostro punto di vista</strong></h2><p>Consigliato \u2013 priorit\u00e0 massima</p><p>Uno dei problemi pi\u00f9 urgenti su cui lavorare.</p><h2><strong>Portata</strong></h2><p>Le IA possono essere utili in moltissimi contesti e c\u2019\u00e8 la possibilit\u00e0 che abbiano un enorme impatto positivo, ma quello che ci preoccupa in particolare \u00e8 la possibilit\u00e0 di risultati estremamente negativi, soprattutto in caso di catastrofe esistenziale. Abbiamo ancora molti dubbi, ma in base alle valutazioni fatte da altre persone, la nostra idea generale \u00e8 che il fattore di rischio di una catastrofe esistenziale provocata da un\u2019intelligenza artificiale nei prossimi 100 anni \u00e8 di circa il 10%. Ulteriori ricerche potrebbero far variare di molto questa cifra \u2013 secondo alcuni esperti non supera lo 0,5%, secondo altri \u00e8 di oltre il 50% e non escludiamo che uno dei due gruppi possa avere ragione. In generale, riteniamo che, quando si parla della prosperit\u00e0 dell\u2019umanit\u00e0 sul lungo periodo, lo sviluppo delle IA costituisca una minaccia di gran lunga superiore di qualsiasi altro problema.</p><h2><strong>Trascuratezza</strong></h2><p>Nel 2020 sono stati investiti circa 50 milioni di dollari per ridurre i rischi peggiori legati alle IA, mentre ne sono stati spesi miliardi per aumentare le loro capacit\u00e0.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhqv90hp7u45\"><sup><a href=\"#fnhqv90hp7u45\">[2]</a></sup></span>&nbsp;Nonostante i sempre maggiori timori degli esperti, attualmente ci sono solo circa 300 persone che lavorano per ridurre il rischio di una catastrofe esistenziale provocata da un\u2019IA.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefj8iuzrtglcn\"><sup><a href=\"#fnj8iuzrtglcn\">[3]</a></sup></span>&nbsp;Di queste, circa due terzi lavorano alla ricerca sulla sicurezza delle IA, mentre le rimanenti si dividono tra chi fa ricerca strategica e chi ricerca e sostiene politiche adeguate.</p><h2><strong>Fattibilit\u00e0</strong></h2><p>Fare progressi nella prevenzione di catastrofi causate dalle IA pu\u00f2 sembrare difficile, ma ci sono molti ambiti in cui \u00e8 possibile fare pi\u00f9 ricerca e il campo stesso \u00e8 molto recente. Per questo pensiamo che sia possibile risolvere i problemi, anche se abbiamo ancora molti dubbi \u2013 le valutazioni a questo proposito tendono a variare enormemente.</p><p><br><a href=\"https://drive.google.com/file/d/1TmdOkEZZvF9LXB5sMp8iohCqXz5H5KGC/view\"><strong>Leggi qui il rapporto completo</strong></a><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnw4jrw3yo2f\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefw4jrw3yo2f\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Ci preoccupa anche la possibilit\u00e0 che le intelligenze artificiali possano risultare meritevoli di considerazioni morali - ad esempio, perch\u00e9 acquistano coscienza di s\u00e9. Anche se non possiamo discuterne in questo articolo, abbiamo trattato l\u2019argomento&nbsp;<a href=\"https://80000hours.org/problem-profiles/#artificial-sentience\">qui</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhqv90hp7u45\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhqv90hp7u45\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Stime ricavate grazie al database di&nbsp;<a href=\"https://aiwatch.issarice.com/\">AI Watch</a>. Per ogni organizzazione ho stimato la percentuale di dipendenti che lavorano direttamente per ridurre i rischi esistenziali legati alle IA. Molte di queste stime sono soggettive (ad esempio, \u201cquesto programma di ricerca riguarda davvero la sicurezza delle IA?\u201d) e potrebbero essere troppo basse in caso AI Watch non disponesse di dati su qualche organizzazione o troppo alte se nei dati sono presenti ripetizioni o se considerano persone che non lavorano pi\u00f9 in quell\u2019ambito. Con un intervallo di confidenza del 90%, stimo tra le 100 e le 1500 persone.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnj8iuzrtglcn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefj8iuzrtglcn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u00c8 difficile stabilire con precisione quanti soldi vengano spesi per migliorare le capacit\u00e0 delle IA, in parte a causa della scarsit\u00e0 di dati e in parte per via di domande come queste:</p><p>&nbsp;- Quali ricerche sulle IA stanno davvero migliorando capacit\u00e0 pericolose che potrebbero far aumentare il rischio di catastrofe esistenziale?</p><p>- Devono essere inclusi i progressi fatti sull\u2019hardware delle IA o nella raccolta di dati?</p><p>&nbsp;- E per quel che riguarda progressi sui processi di ricerca in generale o fattori che potrebbero portare in futuro a una crescita economica e quindi a possibili nuovi investimenti?&nbsp;</p><p>La cifra pi\u00f9 significativa che siamo stati in grado di trovare sono stati i costi sostenuti da DeepMind nel 2020, all\u2019incirca 1 miliardo di sterline,&nbsp;secondo il loro rendiconto annuale. Ci aspettiamo che un budget del genere abbia contribuito in qualche modo ai \u201cprogressi nelle capacit\u00e0 delle IA\u201d, considerando che il loro obiettivo principale \u00e8 sviluppare sistemi di IA generali molto potenti. (Vale la pena notare, tuttavia, che DeepMind contribuisce anche al lavoro sulla sicurezza delle IA, che potrebbe ridurre il rischio esistenziale.)</p><p>Se DeepMind spende circa il 10% nel migliorare le capacit\u00e0 delle IA, otteniamo una cifra attorno ai 10 miliardi di sterline. (Dal momento che ci sono molte aziende di IA negli Stati Uniti e si stanno facendo molti sforzi per sviluppare IA avanzate in Cina, il 10% ci sembra una stima ragionevole.)</p><p>Come limite superiore, le spese nel settore delle intelligenze artificiali nel 2021 sono state di circa&nbsp;340 miliardi di dollari.<br>In generale, quindi, riteniamo che i soldi spesi nel miglioramento delle capacit\u00e0 delle IA vadano da 1 miliardo a 340 miliardi di dollari l\u2019anno. Anche ipotizzando solo 1 miliardo di dollari, si tratterebbe di una cifra 100 volte maggiore di quello che viene speso per ridurre i rischi delle IA</p></div></li></ol>", "user": {"username": "EA Italy"}}, {"_id": "Yoxmaj8QKpQun9mDz", "title": "[Opzionale] Ricerca sulla sicurezza delle IA: panoramica delle carriere", "postedAt": "2023-01-17T11:06:12.447Z", "htmlBody": "<p><i>This is an Italian translation of </i><a href=\"https://forum.effectivealtruism.org/posts/KHw3ezJzA7z3itWNW/ai-safety-researcher-career-review\"><i><strong>AI Safety researcher career review</strong></i></a></p><p><strong>In breve:</strong> Per mitigare i rischi posti dallo sviluppo dell\u2019intelligenza artificiale, \u00e8 necessario fare ricerca su come risolvere le sfide in ambito tecnico e i problemi in ambito di progettazione per assicurarci che le intelligenze artificiali pi\u00f9 potenti facciano ci\u00f2 che vogliamo \u2013 e che portino benefici \u2013 senza nessuna conseguenza catastrofica involontaria.</p><p><strong>Consigliata:&nbsp;</strong>Se si \u00e8 individui molto adatti a questo tipo di carriera, potrebbe essere il modo migliore per avere un impatto sociale.</p><p>Stato della recensione: Basata su studi di media profondit\u00e0</p><h2><strong>Perch\u00e9 lavorare nella ricerca sulla sicurezza delle IA potrebbe avere un grande impatto?</strong></h2><p>Come&nbsp;<a href=\"https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/\">gi\u00e0 argomentato</a>, nei prossimi decenni potremmo assistere allo sviluppo di potenti algoritmi di machine learning con la capacit\u00e0 di trasformare la nostra societ\u00e0. Ci\u00f2 comporterebbe importanti vantaggi e svantaggi, inclusa la possibilit\u00e0 di un&nbsp;<a href=\"https://80000hours.org/articles/existential-risks/\">rischio catastrofico</a>.</p><p>Oltre al lavoro sulle politiche e sulle strategie discusse in&nbsp;<a href=\"https://80000hours.org/career-reviews/ai-policy-and-strategy/\">questa recensione di carriera</a>, un\u2019altra strada maestra per limitare questi rischi consiste nel fare ricerca sulle sfide tecniche poste da intelligenze artificiali avanzate, come il problema dell\u2019allineamento. In breve, come possiamo progettare IA potenti in modo che facciano ci\u00f2 che vogliamo senza conseguenze indesiderate?</p><p>Questo ambito di ricerca ha iniziato a decollare. Esistono oggi importanti poli accademici e laboratori di IA dove si pu\u00f2 lavorare su questi problemi come il&nbsp;<a href=\"https://mila.quebec/en/\">Mila</a> di Montreal, il&nbsp;<a href=\"https://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> di Oxford, il&nbsp;<a href=\"https://humancompatible.ai/\">Center for Human-Compatible Artificial Intelligence</a> di Berkeley,&nbsp;<a href=\"https://deepmind.com/\">DeepMind</a> a London e l\u2019<a href=\"https://openai.com/\">OpenAI</a> a San Francisco. Abbiamo svolto attivit\u00e0 di consulenza per oltre 100 persone in questo ambito, con molte di esse che gi\u00e0 lavorano nelle istituzioni summenzionate. Il&nbsp;<a href=\"https://intelligence.org/\">Machine Intelligence Research Institute</a> di Berkeley lavora in questo campo dal 2005 e possiede una prospettiva e un programma di ricerca non convenzionali rispetto agli altri laboratori.</p><p>Abbondano i finanziamenti per i ricercatori talentuosi, incluse borse di studio universitarie e donazioni filantropiche dai principali sovvenzionatori come Open Philanthropy. \u00c8 possibile inoltre ottenere finanziamenti per il proprio&nbsp;<a href=\"https://www.openphilanthropy.org/open-philanthropy-technology-policy-fellowship/\">progetto di dottorato</a>. La principale necessit\u00e0 di questo campo consiste in pi\u00f9 persone capaci di impiegare questi fondi per portare avanti la ricerca.</p><h2><strong>Che cosa comporta questo percorso?</strong></h2><p>L\u2019obiettivo in questo percorso sarebbe quello di ottenere un lavoro in uno dei migliori centri per la sicurezza delle IA \u2013 nel profit, nel no-profit o in ambito accademico \u2013 e poi lavorare sulle questioni pi\u00f9 urgenti, con l\u2019eventuale prospettiva di diventare un coordinatore dei ricercatori che supervisiona la ricerca sulla sicurezza.</p><p>In generale, le posizioni tecniche sulla sicurezza delle IA si possono distinguere in ruoli di (i) ricerca e di (i) ambito ingegneristico. I ricercatori guidano progetti di ricerca. Gli ingegneri implementano i sistemi e compiono le analisi necessarie per portare avanti la ricerca.</p><p>Anche se gli ingegneri hanno minore influenza sugli obiettivi di alto livello della ricerca, \u00e8 comunque importante che si occupino della sicurezza, in modo che comprendano meglio gli obiettivi finali della ricerca stessa (e in questo modo darsi meglio le priorit\u00e0), siano pi\u00f9 motivati, orientino la cultura dominante verso la sicurezza e usino il capitale di carriera guadagnato per contribuire a altri progetti futuri sulla sicurezza. Questo significa che l\u2019ambito ingegneristico \u00e8 una buona alternativa per coloro che non vogliono essere diventare ricercatori.&nbsp;</p><p>Pu\u00f2 essere utile che ci siano persone che capiscono le sfide poste dalla sicurezza delle IA che lavorino in squadre di ricerca in questo ambito che non siano direttamente orientate alla sicurezza. Lavorare in queste squadre pu\u00f2 metterti nella posizione di aiutare a promuovere la preoccupazione sulla sicurezza in generale, soprattutto se finirai per ricoprire una posizione dirigenziale con influenza sulle priorit\u00e0 dell\u2019organizzazione per cui lavori.</p><p>Saremmo entusiasti di vedere pi\u00f9 persone che sviluppano competenze nel lavoro sulla sicurezza delle IA in Cina o in contesti legati alla Cina \u2013 leggi di pi\u00f9 nella nostra&nbsp;<a href=\"https://80000hours.org/career-reviews/china-related-ai-safety-and-governance-paths/\">recensione dei percorsi di carriera sulla sicurezza e sulla gestione dell\u2019IA in contesti legati alla Cina</a>, alcuni dei quali si compiono nella ricerca tecnica.</p><h2><strong>Esempi di persone che hanno intrapreso questo percorso di carriera</strong></h2><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Yoxmaj8QKpQun9mDz/kdpinwljgfhvxiysb99w\"></figure><p>Catherine Olsson ha iniziato il suo dottorato all\u2019Universit\u00e0 di New York, lavorando sui modelli computazionali della visione umana. Alla fine ha deciso di lavorare direttamente sulla sicurezza delle IA e ha ottenuto un impiego alla&nbsp;<a href=\"https://openai.com/\">OpenAI</a>, seguito da uno a&nbsp;<a href=\"https://research.google/teams/brain/\">Google Brain</a>, per poi spostarsi ad&nbsp;<a href=\"https://www.anthropic.com/\">Anthropic</a>.<br><a href=\"https://80000hours.org/podcast/episodes/olsson-and-ziegler-ml-engineering-and-safety/\"><strong><u>SCOPRI DI PI\u00d9</u></strong></a></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Yoxmaj8QKpQun9mDz/h5wbwbd8rfzx0d8jaauv\"></figure><p>Daniel Ziegler dopo aver abbandonato il suo dottorato sul machine learning a Stanford, che ha sempre adorato costruire cose e ha sempre sognato di definire lo sviluppo delle IA, ha fatto domanda per lavorare alla&nbsp;<a href=\"https://openai.com/\">OpenAI</a>. Ha impiegato sei settimane a prepararsi per l\u2019intervista e ha ottenuto il lavoro. Il suo dottorato, invece, lo avrebbe impegnato per sei anni. Daniel pensa che il suo grande balzo di carriera possa essere possibile per molte altre persone.<br><a href=\"https://80000hours.org/podcast/episodes/olsson-and-ziegler-ml-engineering-and-safety/\"><strong><u>SCOPRI DI PI\u00d9</u></strong></a></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Yoxmaj8QKpQun9mDz/coa7q4tehjtdoqadqvei\"></figure><p>Chris Olah ha avuto un percorso affascinante e non convenzionale. Chris non solo non possiede un dottorato di ricerca, ma non ha ottenuto nemmeno una laurea triennale. Dopo aver abbandonato l\u2019universit\u00e0 per aiutare un suo conoscente a difendersi da false accuse penali, Chris ha iniziato a lavorare da s\u00e9 alla ricerca sul machine learning, ottenendo infine un tirocinio presso&nbsp;<a href=\"https://research.google/teams/brain/\">Google Brain</a>.<br><a href=\"https://80000hours.org/podcast/episodes/chris-olah-unconventional-career-path/\"><strong><u>SCOPRI DI PI\u00d9</u></strong></a></p><h2><strong>Come valutare la tua attitudine</strong></h2><p>La ricerca tecnica con l\u2019impatto maggiore sulla sicurezza delle IA sar\u00e0 svolta da persone nei migliori ruoli summenzionati. Quindi, per decidere se questo percorso faccia al caso tuo, \u00e8 importante chiederti se tu abbia una ragionevole probabilit\u00e0 di ottenere quei lavori.</p><ul><li>Hai la possibilit\u00e0 di essere ammesso in una delle cinque migliori scuole di specializzazione sul machine learning? Questo potrebbe essere un buon test per capire se potrai ottenere un lavoro nei migliori poli di ricerca sulla IA, anche se non si tratta di un prerequisito.</li><li>Sei sicuro dell\u2019importanza della sicurezza della IA sul lungo periodo?</li><li>Sei un tecnico informatico in ambito software o machine learning che ha lavorato in FAANG o in altre aziende competitive? Potresti formarti per ottenere una posizione di ricerca oppure una posizione in ambito tecnico.</li><li>Hai la possibilit\u00e0 di dare un contributo a&nbsp;<a href=\"https://80000hours.org/articles/research-questions-by-discipline/#machine-learning-artificial-intelligence-and-computer-science-research-questions\">un rilevante quesito di ricerca</a>? Per esempio, hai un grande interesse per l\u2019argomento, hai grandi idee su grandi questioni da analizzare e non puoi fare a meno di perseguire tali idee? Leggi di pi\u00f9 su&nbsp;<a href=\"https://80000hours.org/career-reviews/academic-research/#how-to-assess-your-personal-fit\">come capire se sei un buon candidato per un ruolo nella ricerca</a>.</li></ul><h2><strong>Come entrare in questo campo</strong></h2><p>Il primo passo in questo percorso di solito consiste nell\u2019intraprendere un dottorato di ricerca in machine learning in una buona scuola. \u00c8 possibile entrare in questo campo anche senza un dottorato, ma \u00e8 probabile che venga richiesto per ruoli di ricercatore nei poli universitari e in DeepMind, i quali coprono una buona fetta delle migliori posizioni. Un dottorato di ricerca in machine learning apre anche strade nelle politiche sulle IA, nelle IA applicate e nel guadagnare per donare, quindi questo percorso ha buone opzioni di riserva se dovessi decidere che la sicurezza delle IA non fa per te.</p><p>Comunque, se preferisci l\u2019ambito tecnico alla ricerca, un dottorato non \u00e8 necessario. Puoi invece seguire un&nbsp;<i>master</i> o sviluppare competenze nel profit.</p><p>\u00c8 anche possibile iniziare questo percorso a partire dalle neuroscienze (soprattutto quelle computazionali), quindi, se possiedi gi\u00e0 esperienza in quest\u2019area, non \u00e8 detto che dovrai tornare a studiare.</p><p><strong>Se hai gi\u00e0 molta familiarit\u00e0 con la sicurezza delle IA come area critica, il nostro miglior consiglio \u00e8 di dare un\u2019occhiata a&nbsp;</strong><a href=\"https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment\"><strong>questa guida passo-dopo-passo&nbsp;</strong></a>per intraprendere una carriera nella sicurezza tecnica delle IA scritta da Charlie Rogers-Smith.</p><p>Ultimamente si sono concretizzate opportunit\u00e0 anche per&nbsp;<a href=\"https://distill.pub/2019/safety-needs-social-scientists/\">scienziati sociali che contribuiscano alla sicurezza delle IA</a>.</p><p>Puoi trovare ulteriori dettagli nelle risorse al termine della recensione.</p><h2><strong>Organizzazioni consigliate</strong></h2><ul><li><a href=\"https://www.aisafetysupport.org/\">AI Safety Support</a> lavora per ridurre il rischio esistenziale e catastrofico legato alle IA sostenendo chiunque voglia lavorare su questo problema, concentrandosi sull\u2019aiutare nuovi o aspiranti ricercatori sulla sicurezza delle IA attraverso consigli di carriera e costruendo una comunit\u00e0.</li><li><a href=\"https://alignmentresearchcenter.org/\">Alignment Research Center</a> \u00e8 un\u2019organizzazione di ricerca no-profit al lavoro per allineare i futuri sistemi di machine learning agli interessi umani. Si impegna attualmente a sviluppare una strategia di allineamento \u201cend-to-end\u201d che possa essere adottata oggi dal contesto profit e che, nel frattempo, possa scalare verso i futuri sistemi di apprendimento automatico. Controlla i&nbsp;<a href=\"https://alignmentresearchcenter.org/hiring/\">ruoli attualmente vacanti</a>.</li><li><a href=\"https://www.anthropic.com/\">Anthropic</a> \u00e8 una compagnia che si occupa della ricerca sulle IA e della sicurezza al fine di costruire sistemi di IA affidabili, interpretabili e manovrabili. Gli interessi del loro team di ricerca multidisciplinare includono il linguaggio naturale, il feedback umano, le leggi di potenza, l\u2019apprendimento attraverso il rinforzo, la generazione di codice e l\u2019interpretabilit\u00e0. Controlla i&nbsp;<a href=\"https://www.anthropic.com/#careers\">ruoli attualmente vacanti</a>.</li><li>Il&nbsp;<a href=\"https://humancompatible.ai/\">Center for Human-Compatible Artificial Intelligence</a> punta a sviluppare i mezzi concettuali e tecnici per riorientare la spinta generale della ricerca sull\u2019IA verso sistemi con vantaggi comprovati. Controlla i&nbsp;<a href=\"http://humancompatible.ai/jobs\">ruoli attualmente vacanti</a>.</li><li>Il&nbsp;<a href=\"https://longtermrisk.org/\">Center on Long-term Risk</a> affronta i rischi peggiori per lo sviluppo e l\u2019impiego di IA avanzate. Attualmente si concentra sugli scenari di conflitto, cos\u00ec come su sugli aspetti tecnici e filosofici della cooperazione. Il loro lavoro include ricerche interdisciplinari, il fare finanziamenti o il suggerire candidati e il costruire una comunit\u00e0 di professionisti e altri ricercatori nell\u2019ambito di queste priorit\u00e0. Controlla i&nbsp;<a href=\"https://longtermrisk.org/work-with-us/\">ruoli attualmente vacanti</a>.</li><li><a href=\"https://deepmind.com/\">DeepMind</a> \u00e8 con ogni probabilit\u00e0 il pi\u00f9 grande gruppo di ricerca che sta sviluppando un\u2019intelligenza artificiale generale nel mondo occidentale. Siamo sicuri di poter consigliare ruoli presso DeepMind solamente negli ambiti della sicurezza, dell\u2019etica, delle politiche e della sorveglianza. Controlla i<a href=\"https://deepmind.com/careers/\"> ruoli attualmente vacanti</a>.</li><li>Il&nbsp;<a href=\"https://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> \u00e8 un istituto di ricerca multidisciplinare dell\u2019Universit\u00e0 di Oxford. Accademici del FHI sfoderano gli strumenti della matematica, della filosofia e delle scienze sociali per influenzare le domande fondamentali sull\u2019umanit\u00e0 e le sue prospettive.</li><li>Il&nbsp;<a href=\"https://intelligence.org/\">Machine Intelligence Research Institute</a> \u00e8 stato uno dei primi gruppi a preoccuparsi dei rischi dell\u2019intelligenza artificiale nei primi anni 2000 e ha&nbsp;<a href=\"https://intelligence.org/research/\">pubblicato diversi articoli</a> sui problemi di sicurezza e su come risolverli. Controlla i&nbsp;<a href=\"https://intelligence.org/get-involved/#careers\">ruoli attualmente vacanti</a>.</li><li><a href=\"https://openai.com/\">OpenAI</a> \u00e8 stata fondata nel 2015 con l\u2019obiettivo di condurre ricerche su come rendere sicure le IA. Ha ricevuto oltre un miliardo di dollari di impegni di finanziamento dalla comunit\u00e0 di questo settore tecnologico. Controlla i&nbsp;<a href=\"https://jobs.lever.co/openai\">ruoli attualmente vacanti</a>.</li><li><a href=\"https://www.redwoodresearch.org/\">Redwood Research</a> conduce ricerche applicate per aiutare ad allineare i futuri sistemi di IA agli interessi umani. Controlla i&nbsp;<a href=\"https://www.redwoodresearch.org/jobs\">ruoli attualmente vacanti</a>.</li></ul><h3><strong>Vuoi dei consigli faccia-a-faccia su come intraprendere questo percorso?</strong></h3><p>Visto che si tratta di uno dei nostri percorsi prioritari, se pensi che questo percorso possa essere un\u2019ottima occasione per te, saremmo&nbsp;<i>particolarmente&nbsp;</i>entusiasti di consigliarti sui tuoi prossimi passi. Possiamo aiutarti a considerare le tue opzioni, a creare connessioni con altri che lavorano nello stesso campo e, se possibile, anche aiutarti a trovare un lavoro o delle opportunit\u00e0 di finanziamento.</p><p><a href=\"https://80000hours.org/speak-with-us/?int_campaign=career-review-generic\"><u>CANDIDATI PER PARLARE CON IL NOSTRO TEAM</u></a></p><h2><strong>Scopri di pi\u00f9</strong></h2><p><strong>Ulteriori letture essenziali (in inglese):</strong></p><ul><li>Per aiutarti a orientarti nel campo, ti consigliamo lo&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/pbiGHk6AjRxdBPoD8/ai-safety-starter-pack\">AI safety starter pack</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment\">Guida passo-dopo-passo di Charles Rogers Smith sulle carriere nella sicurezza della IA</a></li><li><a href=\"https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/\">Il nostro profilo di problemi sui rischi dell\u2019IA</a></li><li>Questo&nbsp;<a href=\"https://www.eacambridge.org/technical-alignment-curriculum\">curriculum sulla sicurezza dell\u2019IA</a> (o, per qualcosa di pi\u00f9 breve, questa serie&nbsp;<a href=\"https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ\">di post&nbsp;</a>di Richard Ngo)</li><li><a href=\"https://80000hours.org/articles/ml-engineering-career-transition-guide/\">La nostra guida su come diventare un tecnico nel campo del machine learning con un focus sulla sicurezza dell\u2019IA</a></li></ul><p><strong>Ulteriori letture (in inglese):</strong></p><ul><li><a href=\"https://80000hours.org/career-reviews/machine-learning-phd/\">Recensioni della carriera come dottorato nel machine learning&nbsp;</a></li><li><a href=\"http://humancompatible.ai/bibliography\">Lista di letture dal Center for Human-Compatible AI</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/XvWWfq9iqFj8x7Eu8/list-of-ai-safety-courses-and-resources\">Una serie di liste di lettura sulla sicurezza dell\u2019IA</a></li><li>Podcast:&nbsp;<a href=\"https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions\">Dr Paul Christiano on how OpenAI is developing real solutions to the \u2018AI alignment problem\u2019, and his vision of how humanity will progressively hand over decision-making to AI systems</a></li><li>Podcast:&nbsp;<a href=\"https://80000hours.org/podcast/episodes/olsson-and-ziegler-ml-engineering-and-safety\">Machine learning engineering for AI safety and robustness: a Google Brain engineer\u2019s guide to entering the field</a></li><li>Podcast:&nbsp;<a href=\"https://80000hours.org/2017/07/podcast-the-world-needs-ai-researchers-heres-how-to-become-one/\">The world needs AI researchers. Here\u2019s how to become one</a></li><li>Podcast:&nbsp;<a href=\"https://80000hours.org/podcast/episodes/chris-olah-unconventional-career-path/\">Chris Olah on working at top AI labs without an undergrad degree</a> e&nbsp;<a href=\"https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/\">What the hell is going on inside neural networks</a></li><li>Podcast:&nbsp;<a href=\"https://80000hours.org/2018/03/jan-leike-ml-alignment/\">A machine learning alignment researcher on how to become a machine learning alignment researcher</a></li><li>Leggi&nbsp;<a href=\"https://80000hours.org/topic/priority-paths/technical-ai-safety/\">tuti i nostri articoli sulle carriere nella sicurezza dell\u2019IA</a><br>&nbsp;</li></ul>", "user": {"username": "EA Italy"}}, {"_id": "BkcnuZyKcDZBpSrQS", "title": "[Opzionale] Il panorama della governance lungoterminista delle intelligenze artificiali", "postedAt": "2023-01-17T11:03:46.983Z", "htmlBody": "<p><i>This is an Italian translation of </i><a href=\"https://forum.effectivealtruism.org/posts/ydpo7LcJWhrr2GJrx/the-longtermist-ai-governance-landscape-a-basic-overview\"><i><strong>The longtermist AI governance landscape: a basic overview</strong></i></a></p><p><i>Obiettivo: fornire una visione d'insieme della governance delle IA dal punto di vista lungoterminista.</i><br><i>A chi \u00e8 destinato: a persone che non hanno familiarit\u00e0 con la governance delle IA dal punto di vista lungoterminista e vogliono conoscere meglio questo ambito. Non mi aspetto che sar\u00e0 utile a chi ha gi\u00e0 familiarit\u00e0 con queste tematiche. Addendum: alcune persone che avevano gi\u00e0 familiarit\u00e0 con queste tematiche hanno trovato utile l\u2019articolo.&nbsp;</i></p><p>Questo post intende delineare i diversi tipi di lavoro coinvolto nella governance lungoterminista delle IA. Dar\u00f2 una breve spiegazione di ognuno, completa di esempi e storie che spiegano in che modo potrebbe avere un impatto positivo, oltre a un elenco delle persone che ci stanno attualmente lavorando di cui sono a conoscenza.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbfphrak9q17\"><sup><a href=\"#fnbfphrak9q17\">[1]</a></sup></span><br>Per prima cosa, un paio di definizioni:</p><ul><li>&nbsp;<i>Governance delle IA</i> lo sviluppo di norme, politiche, leggi, processi e istituzioni (non solo governi) locali e globali che influenzeranno i risultati nella societ\u00e0 derivanti dallo sviluppo e dall\u2019impiego delle intelligenze artificiali.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftvx62hp0nq\"><sup><a href=\"#fntvx62hp0nq\">[2]</a></sup></span></li><li>&nbsp;<i>La governance lungoterminista delle IA</i>, nello specifico, \u00e8 la sottocategoria motivata dalle preoccupazioni per l\u2019impatto delle IA su un periodo di tempo molto ampio. Coincide spesso con il lavoro che punta a controllare le&nbsp;<a href=\"https://forum.effectivealtruism.org/tag/transformative-artificial-intelligence\"><u>IA trasformative</u></a> (TAI).&nbsp;</li></ul><p>Vale la pena notare che si tratta di un ambito molto ristretto. Se dovessi ipotizzare direi che ci sono circa 60 persone che lavorano nel campo della governance delle IA e la cui preoccupazione \u00e8 il suo impatto sul lunghissimo periodo.</p><h1><strong>Breve riassunto</strong></h1><p>Parlando in astratto, penso sia utile visualizzare il lavoro di base e quello applicato come ai poli opposti di uno spettro. Dal lato del lavoro di base abbiamo la&nbsp;<i>ricerca strategica</i>, il cui scopo \u00e8 individuare obiettivi astratti ideali per la governance lungoterminista delle IA; c'\u00e8 poi la&nbsp;<i>ricerca tattica</i>, il cui obiettivo \u00e8 individuare i programmi utili a raggiungere quegli obiettivi. Dal lato del lavoro applicato troviamo lo sviluppo di normative, il cui scopo \u00e8 trasformare la ricerca in normative concrete; abbiamo poi lavoro che spinge perch\u00e9 quelle normative vengano applicate e infine l\u2019applicazione vera e propria (ad esempio da parte di impiegati statali).</p><p>C'\u00e8 anche lavoro di&nbsp;<i>field-building</i> (dalla collocazione incerta): invece che contribuire direttamente a risolvere il problema, l\u2019obiettivo \u00e8 quello di creare un ambito con persone che lavorano sul tema.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BkcnuZyKcDZBpSrQS/kbs9kp0a6pkdw1pov0jp\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BkcnuZyKcDZBpSrQS/nxdc49dixizazngl3dwi 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BkcnuZyKcDZBpSrQS/razcjgrui3kkjrclgdqn 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BkcnuZyKcDZBpSrQS/kg8suo9zpp9cuakeiu3j 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BkcnuZyKcDZBpSrQS/jpb5p8mbzoa3v4egbz0x 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BkcnuZyKcDZBpSrQS/arvndrhjc6dmps3xqui6 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BkcnuZyKcDZBpSrQS/vzvcdqxbvneg1cz1grlc 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BkcnuZyKcDZBpSrQS/ffjmqxochdq5dp0d6tqx 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BkcnuZyKcDZBpSrQS/jg1l3iybwath3whny5so 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BkcnuZyKcDZBpSrQS/cgmlbprpbsvoii1ihdch 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BkcnuZyKcDZBpSrQS/ebdnfuvt1ristjqqda7z 1440w\"></p><p>Ovviamente si tratta di una semplificazione e non tutti i lavori potranno essere catalogati con precisione. Al contrario di quel che si pensa di solito, non sempre la conoscenza profonda passa dalla base alla parte applicata: \u00e8 anche importante che la ricerca tenga conto delle preoccupazioni normative, vale a dire quanto \u00e8 probabile che questa ricerca corrobori una proposta normativa che \u00e8 fattibile dal punto di vista politico.</p><p>Vediamo ora nel dettaglio ognuno di questi ambiti.</p><h1><strong>Ricerca</strong></h1><h2><strong>Ricerca strategica</strong></h2><p>In definitiva, l\u2019obiettivo della<i> ricerca strategica lungoterminista sulle IA</i> \u00e8 quello di individuare obiettivi di alto livello che, se raggiunti, potrebbero chiaramente aumentare le probabilit\u00e0 di risultati positivi, da un punto di vista lungoterminista, da parte di un\u2019IA avanzata (sull\u2019esempio di&nbsp;<a>Muehlhauser</a>, a volte far\u00f2 riferimento a questo obiettivo come al \"raggiungimento di una<i> chiarezza strategica</i>\").<br>La ricerca stessa pu\u00f2 essere collocate in vari punti dello spettro compresi tra&nbsp;<i>mirata&nbsp;</i>ed&nbsp;<i>esplorativa</i>:</p><ul><li>L\u2019obiettivo della ricerca strategica mirata \u00e8 quello di fornire risposte che facciano luce su altre domande specifiche e importanti.<ul><li>Ad esempio, \"Voglio scoprire qual \u00e8 la potenza di calcolo del cervello umano, perch\u00e9 mi aiuter\u00e0 a scoprire quando verranno sviluppate le TAI (cosa che determina gli obiettivi di alto livello che dovremmo ricercare)\"</li></ul></li><li>La ricerca strategica esplorativa fornisce risposte senza avere bene in mente le altre domande importanti a cui potrebbe aiutarci a rispondere.<ul><li>Ad esempio, \"Voglio scoprire quali sono le politiche industriali della Cina, perch\u00e9 probabilmente saperlo mi aiuter\u00e0 a rispondere a una serie di domande strategiche importanti, anche se non so esattamente a quali\".</li></ul></li></ul><h3><strong>Esempi</strong></h3><ul><li>Il lavoro delle previsioni sulle TAI, come ad esempio&nbsp;<a>ancore biologiche</a> e&nbsp;<a>leggi scalabili per modelli linguistici neural</a>i.<ul><li>Esempio di importanza strategica: se le TAI arriveranno in tempi brevi, allora un ambito di esperti che cresce lentamente sembra poco promettente; se le TAI sono al di l\u00e0 da venire, allora alla governance lungoterminista delle IA potrebbe essere assegnata una priorit\u00e0 minore.</li></ul></li><li>Il lavoro sulla chiarezza delle fonti dei rischi esistenziali legati alle IA, come gli scritti di&nbsp;<a>Christiano, Critch</a>,&nbsp;<a>Carlsmith</a>,&nbsp;<a>Ngo</a> e&nbsp;<a>Garfinkel</a>.<ul><li>Esempio di importanza strategica: se la maggior parte dei rischi esistenziali derivanti dalle IA proviene da IA avanzate non allineate, allora la governance dovrebbe concentrarsi su chi costruisce quelle IA.</li></ul></li><li>Le ricerche che indagano la velocit\u00e0 dei progressi sulle TAI, ad esempio&nbsp;<a>le indagini</a>&nbsp;e le&nbsp;<a>analisi</a> di AI Impacts.<ul><li>Esempio di importanza strategica: se i progressi nelle IA sono&nbsp;<a>discontinu</a>i, allora \u00e8 possibile che ci sia un numero ristretto di protagonisti molto importanti e che la maggior parte del valore della governance risieda nell\u2019influenzare quelle persone.</li></ul></li></ul><p>\u00c8 facile confondere la ricerca strategica (e soprattutto la ricerca strategica&nbsp;<i>esplorativa</i>) con la ricerca&nbsp;<i>ad ampio spettro</i>. Come mostrano molti degli esempi precedenti, la ricerca strategica pu\u00f2 essere&nbsp;<i>obiettivi ristretti</i>, vale a dire che pu\u00f2 rispondere a una domanda dalla portata limitata. Esempi di portata limitata e ampia portata:</p><ul><li>Sulle leggi di scala (scaling laws):<ul><li>Ampia portata: in generale, come cambiano le prestazioni dei modelli di deep learning se aumentano le dimensioni di quei modelli?</li><li>Portata limitata: in che modo cambiano&nbsp;<i>in particolare&nbsp;</i>le prestazioni di&nbsp;<i>modelli linguistici di grandi dimensioni&nbsp;</i>(come ad esempio GPT-3) se aumentano le dimensioni di quei modelli? (Argomento affrontato in&nbsp;<a>questo paper</a>.)</li></ul></li><li>Sulle fonti dei rischi esistenziali derivanti dalle IA:<ul><li>Ampia portata: in generale, quali rischi esistenziali comportano le IA?</li><li>Portata limitata: quali rischi esistenziali potrebbero derivare&nbsp;<i>nello specifico da IA il cui obiettivo \u00e8 ottenere influenza</i>? (Argomento affrontato in&nbsp;<a>questo report</a>.)</li></ul></li></ul><p>In effetti, spesso \u00e8 meglio scegliere domande dalla portata limitata, soprattutto se si \u00e8 ricercatori di basso rango, perch\u00e9 sono di solito pi\u00f9 gestibili.</p><p>Luke Muehlhauser ha alcuni consigli per coloro che vogliono cimentarsi in questo tipo di lavoro al punto 4 di&nbsp;<a>questo post</a>. In questo<a> post</a> invece ci sono esempi di domande aperte sulla ricerca.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefl2yhpukopu\"><sup><a href=\"#fnl2yhpukopu\">[3]</a></sup></span></p><h3><strong>Storie di impatto</strong></h3><ul><li><i>Impatto diretto</i>: gli obiettivi nel campo della governance delle IA sono molteplici ed \u00e8 necessario dare priorit\u00e0 a quelli pi\u00f9 importanti. Questo tipo di lavoro \u00e8 spesso motivato dall\u2019impressione dei ricercatori che c'\u00e8 molta poca chiarezza sui temi che influenzano gli obiettivi che dovremmo raggiungere. Si vedano ad esempio i risultati di&nbsp;<a>questi</a>&nbsp;<a>sondaggi</a>, che mostrano come ci sia parecchio disaccordo rispettivamente sugli scenari di rischi esistenziali derivanti dalle IA e sul numero di questi rischi.</li><li><i>Impatto indiretto:</i><ul><li>Field-building: avere un\u2019idea precisa di ci\u00f2 che si sta cercando di raggiungere e della sua importanza sarebbe utile per attrarre nuovi ricercatori in quel campo.</li><li>Comunicare la necessit\u00e0 di nuove normative: se l\u2019obiettivo \u00e8 convincere le persone a prendere decisioni costose o drammatiche in futuro, \u00e8 meglio sapere con esattezza cosa dire a proposito degli obiettivi che si sta cercando di raggiungere e della loro importanza.</li></ul></li></ul><h3><strong>Chi ci sta lavorando?</strong></h3><p>Alcune persone nelle seguenti organizzazioni:&nbsp;<a>FHI</a>,&nbsp;<a>GovAI</a>,&nbsp;<a>CSER</a>,&nbsp;<a>DeepMind</a>,&nbsp;<a>OpenAI</a>,&nbsp;<a>GCRI</a>,&nbsp;<a>CLR</a>,&nbsp;<a>Rethink Priorities</a>,&nbsp;<a>OpenPhil</a>,&nbsp;<a>CSET</a>,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefycvmgqute2\"><sup><a href=\"#fnycvmgqute2\">[4]</a></sup></span>&nbsp;oltre ad alcuni accademici indipendenti.</p><h2><strong>Ricerca tattica</strong></h2><p>L\u2019obiettivo della&nbsp;<i>ricerca tattica lungoterminista sulle IA&nbsp;</i>\u00e8 quello di individuare i programmi utili a raggiungere obiettivi di alto livello (a cui la ricerca strategica ha assegnato la priorit\u00e0). Di solito la sua portata \u00e8 pi\u00f9 limitata per natura.</p><p>Vale la pena notare che pu\u00f2 essere conveniente fare ricerca tattica anche se non si sono individuati con precisione degli obiettivi prioritari: per ricerca personale, per la propria carriera e per creare un campo di ricerca.</p><h3><strong>Esempi</strong></h3><ul><li><a>Windfall Clause</a><ul><li>Il programma: sviluppare uno strumento per la redistribuzione dei vantaggi delle IA per il bene comune</li><li>Obiettivi di alto livello di questo programma: fare in modo che i protagonisti di questa ricerca non competano gli uni con gli altri per sviluppare per primi un\u2019IA avanzata; ridurre la disparit\u00e0 economica.&nbsp;</li></ul></li><li><a>Me</a>ccanismi di sostegno di affermazioni verificabili<ul><li>Il programma: sviluppare pratiche con cui gli sviluppatori di IA possono rendere pi\u00f9 verificabili le loro affermazioni riguardo lo sviluppo delle IA (affermazioni di cui gli sviluppatori possano essere ritenuti responsabili)</li><li>Obiettivi di alto livello di questo programma: sviluppare meccanismi con cui \u00e8 possibile dimostrare i comportamenti responsabili delle intelligenze artificiali; permettere un controllo pi\u00f9 efficace; ridurre la tentazione di prendere scorciatoie per ottenere un vantaggio competitivo.</li></ul></li><li>IA e<a> Antitrust</a><ul><li>Il programma: proporre modi per ridurre le tensioni tra le leggi antitrust e la necessit\u00e0 di uno sviluppo in cooperazione delle IA</li><li>Obiettivi di alto livello di questo programma: aumentare il livello di cooperazione tra le societ\u00e0 che sviluppano IA avanzate.</li></ul></li></ul><h3><strong>Storie di impatto</strong></h3><ul><li><i>Impatto diretto</i>: creare soluzioni che vengano utilizzate per prendere decisioni migliori (nelle normative e nelle ricerche future).<ul><li>Quello che Allan Dafoe chiama \"modello prodotto di ricerca\".</li></ul></li><li><i>Impatto indiretto</i>: anche se non tutte le soluzioni verranno usate per prendere decisioni migliori, saranno utili per la crescita del gruppo di persone che hanno a cuore i problemi della governance lungoterminista delle IA, oltre a migliorare le conoscenze, le competenze, le connessioni e la credibilit\u00e0 dei ricercatori.<ul><li>Quello che Allan Dafoe chiama \"Modello di field-building della ricerca\".</li></ul></li></ul><h3><strong>Chi ci sta lavorando?</strong></h3><p>Alcune persone nelle seguenti organizzazioni:&nbsp;<a>FHI</a>,&nbsp;<a>GovAI</a>,&nbsp;<a>CSER</a>,&nbsp;<a>DeepMind</a>,&nbsp;<a>OpenAI</a>,&nbsp;<a>GCRI</a>,&nbsp;<a>CSET</a>,&nbsp;<a>Rethink Priorities</a>,&nbsp;<a>LPP</a>, oltre ad alcuni accademici indipendenti.</p><h1><strong>Sviluppo, sostegno e attuazione delle politiche</strong></h1><p>La ricerca strategica produce obiettivi di alto livello. La ricerca tattica prende quegli obiettivi e produce dei programmi con cui raggiungerli.&nbsp;<i>Lo sviluppo di politiche&nbsp;</i>prende quei programmi e li trasforma in consigli pronti per essere consegnati ai decisori politici. Per fare ci\u00f2 \u00e8 necessario avere ben chiaro (ad esempio) quali domande fare di preciso, che tipo di linguaggio usare (sia nelle politiche formali che nella richiesta) e altre caratteristiche legate al contesto che influiranno sulle probabilit\u00e0 di successo.</p><p><i>Il sostegno alle politiche&nbsp;</i>spinge affinch\u00e9 quelle politiche vengano attuate, ad esempio quali sono le persone pi\u00f9 adatte per proporle, a chi e in quale momento.</p><p><i>L\u2019attuazione delle politiche consiste nel mettere in pratica quelle politiche grazie al lavoro di impiegati statali o corporazioni.</i></p><p>\u00c8 bene distinguere tra le politiche governative (ovvero quelle politiche pensate per essere attuate da governi od organizzazioni intergovernative) e le politiche aziendali (ovvero le politiche pensate per essere adottate dalle compagnie). Alcune persone che lavorano nel campo della governance lungoterminista delle IA si concentrano sul migliorare le politiche aziendali (in particolar modo quelle che riguardano gli sviluppatori di IA), mentre altri si concentrano nel migliorare quelle dei governi di pertinenza.</p><p>Un fattore motivante comune \u00e8 che le specifiche del processo di attuazione sono spesso considerate fondamentali per il successo della stessa attuazione. Ad esempio, se una norma governativa presenta una falla, l\u2019intera norma potrebbe essere inutile.</p><p>Rispetto alla ricerca, questo tipo di lavoro di solito si basa meno sui ragionamenti individuali e pi\u00f9 sulla raccolta di informazioni/conversazioni (ad esempio riunioni per stabilire chi ha autorit\u00e0 su una determinata norma, cosa gli sta a cuore e quali sono gli interessi di altre figure rilevanti) e sul coordinamento (ad esempio stabilire in che modo convincere un determinato gruppo di persone a sostenere una data norma).</p><p>Come detto in precedenza, la conoscenza profonda a volte scorre \"all\u2019indietro\". Ad esempio, la formulazione di una precisa politica potrebbe essere ripetuta in base a come il sostegno datole modifica le proprie conoscenze (e l\u2019ambiente in cui ci si muove).</p><h3><strong>Esempi</strong></h3><ul><li>Politiche governative:<ul><li>Impegnarsi a non incorporare intelligenze artificiali nel comando, controllo e comunicazione in ambito nucleare (NC3), raccomandato ad esempio dal CLTR in&nbsp;<a>Future Proof</a>.</li><li>Monitoraggio dello sviluppo di intelligenze artificiali da parte del governo, come elaborato in questo<a> libro bianco sul monitoraggio delle IA</a>.</li><li>Fare in modo che nuove norme o strategie/principi sulle IA siano sensibili ai rischi derivanti da intelligenze artificiali avanzate (oltre che quelle attuali), come il&nbsp;<a>feedback</a> fornito da diverse organizzazioni AE sulla proposta di legge dell\u2019UE sulle intelligenze artificiali.</li></ul></li><li>Politiche aziendali:<ul><li>Sviluppare norme per la distribuzione responsabile delle ricerche sulle IA, considerati i possibili usi impropri, come&nbsp;<a>queste raccomandazioni</a>&nbsp;del PAI.</li></ul></li></ul><p>La posizione di queste idee sullo spettro va da quelle pi\u00f9 mirate (come ad esempio non integrare le IA all\u2019interno dell\u2019NC3) a quelle pi\u00f9 generali (nel senso di creare una capacit\u00e0 generale per far fronte a un\u2019ampia gamma di problemi che potrebbero sorgere, ad esempio la maggior parte di quelli visti in precedenza). Sono dell\u2019opinione che allo stato attuale le nostre pratiche di sviluppo, sostegno e attuazione delle politiche debbano concentrarsi soprattutto sulle idee pi\u00f9 generali, considerati tutti i dubbi sul futuro delle IA (e al tempo stesso spingere per l\u2019attuazione di buone idee mirate quando se ne presenta l\u2019occasione).</p><h3><strong>Storie d'impatto</strong></h3><ul><li><i>Impatto diretto</i>: poter disporre di buone norme aumenta le probabilit\u00e0 di farsi strada con successo nella transizione verso un mondo con IA avanzate.</li><li><i>Impatto indiretto</i>: anche se non si \u00e8 sicuri della robustezza di certe idee, svilupparle/sostenerle/attuarle permetter\u00e0 a chi si occupa di governance lungoterminista delle IA di ottenere conoscenze, competenze, connessioni e credibilit\u00e0. Non vogliamo arrivare a un \"crunch time\" dell'IA<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7hl5vgwwhek\"><sup><a href=\"#fn7hl5vgwwhek\">[5]</a></sup></span>&nbsp;e solo allora iniziare a capire come sviluppare politiche e processi decisionali.<ul><li>Detto questo, dovremmo anche fare molta attenzione a non attuare politiche che potrebbero rivelarsi dannose, ad esempio limitando lo sviluppo di politiche future.</li></ul></li></ul><h3><strong>Chi ci sta lavorando?</strong></h3><ul><li>Sviluppo:<ul><li>Di politiche governative:&nbsp;<a>CLTR</a>,&nbsp;<a>FLI</a>,&nbsp;<a>GovAI</a>,&nbsp;<a>CSET</a>,&nbsp;<a>CSER</a>,&nbsp;<a>FHI</a>,&nbsp;<a>TFS</a></li><li>Di politiche aziendali:&nbsp;<a>OpenAI</a>,&nbsp;<a>DeepMind</a>,&nbsp;<a>GovAI</a>,&nbsp;<a>CSER</a>,&nbsp;<a>FHI</a>,&nbsp;<a>PAI</a></li></ul></li><li>Sostegno:<ul><li>A politiche governative:&nbsp;<a>CLTR</a>,&nbsp;<a>CSET</a>,&nbsp;<a>FLI</a>,&nbsp;<a>TFS</a></li><li>A politiche aziendali:&nbsp;<a>PAI</a></li></ul></li><li>Attuazione:<ul><li>Di politiche governative: diverse persone in ambito statale</li><li>Di politiche aziendali:&nbsp;<a>OpenAI</a>,&nbsp;<a>DeepMind</a></li></ul></li></ul><h1><strong>Field-building</strong></h1><p>L\u2019obiettivo dichiarato di questo lavoro \u00e8 creare un campo o una comunit\u00e0 di persone che svolgano lavoro di una certa importanza nell\u2019ambito della governance lungoterminista delle IA.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzvhck8dqse\"><sup><a href=\"#fnzvhck8dqse\">[6]</a></sup></span>&nbsp;Possiamo vederlo come un lavoro in cui \u00e8 necessario sia (1) coinvolgere persone nuove e (2) rendere questo campo pi\u00f9 efficace.&nbsp;</p><h3><strong>Esempi</strong></h3><ol><li>Coinvolgere nuove persone creando:</li></ol><ul><li>borse di studio, come la&nbsp;<a>OpenPhil Technology Policy Fellowship</a>;</li><li><a>programmi</a> o corsi online che aiutino i novizi ad aggiornarsi sulle ultime novit\u00e0 nel campo della governance delle IA;</li><li>materiali introduttivi di ampio respiro e di alta qualit\u00e0 pensati per gli studenti universitari;</li><li>finanziamenti di ricerca pi\u00f9 scalabili per connettersi, sostenere e dare credibilit\u00e0 alle nuove leve.</li></ul><ol><li>Rende questo campo pi\u00f9 efficace creando:</li></ol><ul><li>programmi di ricerca;</li><li>opportunit\u00e0 per i ricercatori di lunga data di assumere facilmente assistenti di ricerca.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref71la7ogbyvf\"><sup><a href=\"#fn71la7ogbyvf\">[7]</a></sup></span></li></ul><h3><strong>Storie d'impatto</strong></h3><ul><li><i>Modello di crescita:&nbsp;</i>creare un campo di ricerca sulla governance lungoterminista delle IA con molte persone allineate con le competenze e capacit\u00e0 giuste per condurre ricerche importanti e lavoro di policy (soprattutto quando questo tipo di lavoro \u00e8 meno limitato da una mancanza di chiarezza strategica).&nbsp;</li><li><i>Modello Metropoli:</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftqd0swbmzpb\"><sup><a href=\"#fntqd0swbmzpb\">[8]</a></sup></span>&nbsp;creare un campo di ricerca sulla governance lungoterminista delle IA che abbia connessioni solide con comunit\u00e0 pi\u00f9 grandi (ad esempio policymaking, scienze sociali, machine learning), in modo che questo campo possa contare sulle diverse competenze di queste comunit\u00e0.</li></ul><h3><strong>Chi ci sta lavorando?</strong></h3><p><a>GovAI</a>,&nbsp;<a>OpenPhil</a>,&nbsp;<a>SERI</a>,&nbsp;<a>CERI</a>,&nbsp;<a>CHERI</a> e&nbsp;<a>EA Cambridge</a>. Pi\u00f9 in generale, tutti i movimenti interni all\u2019Altruismo Efficace che si occupano di cause generali. Tra tutti i tipi di interventi discussi in questo post, questo \u00e8 quello meno considerato.</p><h1><strong>Ulteriori punti di vista sulla governance lungoterminista delle IA</strong></h1><p>Finora mi sono limitato a presentare un unico punto di vista della governance lungoterminista delle IA. Ovviamente ce ne sono altri, che potrebbero essere pi\u00f9 utili per altri scopi. Ad esempio, si potrebbe dividere il panorama attuale in base ai diversi tipi di intervento:</p><ul><li>Intervenire sugli attuali dibattiti nell\u2019ambito delle policy per far s\u00ec che siano pi\u00f9 sensibili ai rischi esistenziali derivanti dalle IA (ad esempio maggiore consapevolezza di quanto pu\u00f2 essere difficile creare intelligenze artificiali all\u2019avanguardia)</li><li>Proporre nuovi strumenti di policy (ad esempio, standard internazionali per le IA)</li><li>Convincere i governi a finanziare la ricerca sulla sicurezza delle IA</li><li>Portare a cambiamenti in ambito aziendale (Windfall Clause)</li><li>...</li></ul><p>Oppure lo si potrebbe dividere in base all\u2019area geografica (anche se non tutte le organizzazioni fanno parte di una determinata area):</p><ul><li>San Francisco Bay Area: OpenPhil, OpenAI, PAI, diverse organizzazioni che lavorano all\u2019allineamento delle IA. Generalmente pi\u00f9 concentrate sul non-allineamento come fonte di possibili rischi esistenziali; simili per cultura alla Silicon Valley e alle comunit\u00e0 razionaliste.&nbsp;</li><li>Washington DC: il governo statunitense, CSET. Si concentrano sullo sviluppo/sostegno/attuazione di politiche statunitensi; simili per cultura alla cultura di DC.&nbsp;</li><li>Regno Unito: FHI/GovAI, DeepMind, governo britannico, CSER, CLTR, (altre?). Di solito pi\u00f9 concentrate sulle molteplici fonti di rischio esistenziale derivante dalle IA.</li><li>Unione Europea: nel 2020 la Commissione Europea ha stilato la bozza della prima normativa sulle intelligenze artificiali del mondo, che con ogni probabilit\u00e0 verr\u00e0 approvata nei prossimi anni e potrebbe generare un \"effetto Bruxelles\".</li><li>Cina.</li><li>\u2026</li></ul><p>Oppure, si potrebbe dividere in base a diverse \"teorie di vittoria\", vale a dire storie su come l\u2019umanit\u00e0 giunger\u00e0 con successo a un mondo con IA avanzate. C'\u00e8 molto altro che si potrebbe dire sull\u2019argomento. Lo scopo di questo post era solo quello di fornire una breve panoramica dei diversi lavori attualmente in corso.</p><p><i>Ringraziamenti: questo post \u00e8 il risultato di una mia sintesi personale del panorama attuale, ma si ispira e/o estrapola direttamente da altri post sul forum di Altruismo Efficace di&nbsp;</i><a><i>Allan Dafoe</i></a>,&nbsp;<a><i>Luke Muehlhauser</i></a><i>,&nbsp;</i><a>Convergence Analysis</a>. Grazie anche a Jess Whittlestone per le preziose conversazioni, a Matthijs Maas, Yun Gu, Konstantin Pilz, Caroline Baum\u00f6hl e soprattutto a un critico del SERI per il feedback ricevuto sulla prima bozza.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbfphrak9q17\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbfphrak9q17\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Sicuramente ho dimenticato qualche gruppo importante e potrei aver sbagliato a classificarli o anche averli rappresentati male \u2013 se \u00e8 cos\u00ec, fatemelo presente e corregger\u00f2!</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntvx62hp0nq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftvx62hp0nq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Preso direttamente dalla definizione data da&nbsp;<a href=\"https://www.openphilanthropy.org/blog/ai-governance-grantmaking#Our_priorities_within_AI_governance\"><u>Open Philanthropy</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnl2yhpukopu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefl2yhpukopu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>N.B.: alcune di queste domande riguardano la<i> ricerca tattica</i> piuttosto che quella strategica.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnycvmgqute2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefycvmgqute2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>CSET si occupa perlopi\u00f9 di ricerca tattica e sviluppo e sostegno di politiche, ma il loro lavoro nel mappare il processo di produzione dei semiconduttori rientra nella ricerca strategica.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7hl5vgwwhek\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7hl5vgwwhek\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Muehlhauser&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/M2SBwctwC6vBqAmZW/a-personal-take-on-longtermist-ai-governance\"><u>lo definisce</u></a> \"un periodo compreso tra 1 e 20 anni in cui potrebbero essere prese le decisioni pi\u00f9 d'impatto sulle TAI\".</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzvhck8dqse\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzvhck8dqse\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Distinto dai vantaggi di field-building degli altri tipi di lavoro esaminati perch\u00e9 questo \u00e8&nbsp;<i>esplicitamente e unicamente&nbsp;</i>concentrato sulla creazione di un campo di ricerca.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn71la7ogbyvf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref71la7ogbyvf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Che potrebbe a sua volta aiutare a coinvolgere persone nuove.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntqd0swbmzpb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftqd0swbmzpb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Idea estrapolata da questo&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact\"><u>post di Allan Dafoe</u></a>.</p></div></li></ol>", "user": {"username": "EA Italy"}}, {"_id": "pvJxxpza2ZRa5y9e2", "title": "Le Tempistiche delle IA: il dibattito e il punto di vista degli \u201cesperti\u201d", "postedAt": "2023-01-17T23:30:52.792Z", "htmlBody": "<p><i>This is an Italian translation of </i><a href=\"https://forum.effectivealtruism.org/posts/7JxsXYDuqnKMqa6Eq/ai-timelines-where-the-arguments-and-the-experts-stand\"><i><strong>AI Timelines: Where the Arguments, and the \"Experts,\" Stand</strong></i></a></p><p><i>Versione audio disponibile (in inglese) su&nbsp;</i><a href=\"https://www.cold-takes.com/where-ai-forecasting-stands-today\"><i>Cold Takes</i></a><i> (o su Stitcher, Spotify, Google Podcasts, ecc. come \u201cCold Takes Audio\u201d)</i></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pvJxxpza2ZRa5y9e2/yyub6ptdswl71ehhetiz\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pvJxxpza2ZRa5y9e2/khg8cp92fjqk7cttrxv4 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pvJxxpza2ZRa5y9e2/wkw1nxprwkondlcallgv 190w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pvJxxpza2ZRa5y9e2/unfd9b7wwu6bvbtk74m3 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pvJxxpza2ZRa5y9e2/ktisoon2dbpfljjqbo4e 350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pvJxxpza2ZRa5y9e2/gyize5buteknfwvoturb 430w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pvJxxpza2ZRa5y9e2/ozcnmwtdiinzqnr3kqka 510w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pvJxxpza2ZRa5y9e2/mcf5mzl34gdlazqhvvku 590w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pvJxxpza2ZRa5y9e2/raxwz67jsrusmcawovra 670w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pvJxxpza2ZRa5y9e2/zser5houmry43bue3vnq 750w\"></p><blockquote><p>L\u2019articolo comincia con un riassunto di quando possiamo aspettarci che un\u2019IA trasformativa venga sviluppata, sulla base di diversi punti di vista analizzati in precedenza. Penso che possa essere utile anche se avete gi\u00e0 letto gli articoli precedenti, ma se volete saltarlo cliccate&nbsp;<a href=\"https://docs.google.com/document/d/1v8UPtgvjOcSPbvVJVXYix2FMyl3g3uF2dnt3UmIa_c0/edit#bookmark=id.17dp8vu\">qui</a>.</p><p>Rispondo poi alla seguente domanda: \u201cPerch\u00e9 gli esperti non riescono ad arrivare a un consenso solido su questo tema e quali sono le conseguenze sulla nostra vita?\u201d</p></blockquote><p>Secondo le mie stime c'\u00e8&nbsp;<strong>pi\u00f9 del 10% di probabilit\u00e0 che vedremo un\u2019IA trasformativa entro i prossimi 15 anni (entro il 2036); una probabilit\u00e0 del 50% di vederla entro 40 anni (entro il 2060) e una del 66% di vederla in questo secolo (entro il 2100).</strong></p><p>(Con \"IA trasformativa\" intendo \"un\u2019IA abbastanza potente da condurci verso un futuro nuovo e qualitativamente diverso\". Nello specifico mi concentro su ci\u00f2 che chiamo&nbsp;<a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/\">PASTA</a>: intelligenze artificiali che possono in pratica automatizzare tutte le attivit\u00e0 umane necessarie ad accelerare il progresso scientifico e tecnologico. Sono dell\u2019opinione che i sistemi PASTA da soli&nbsp;potrebbero <a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#impacts-of-pasta\">essere sufficienti</a> a fare di questo secolo&nbsp;<a href=\"https://www.cold-takes.com/roadmap-for-the-most-important-century-series/\">il secolo pi\u00f9 importante</a>, sia per via di un possibile&nbsp;<a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#explosive-scientific-and-technological-advancement\">boom di produttivit\u00e0</a> che per i&nbsp;<a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#misaligned-ai-mysterious-potentially-dangerous-objectives\">rischi derivanti da IA non allineate</a>.)</p><p>Questa in generale \u00e8 la mia conclusione, sulla base di un certo numero di report tecnici che fanno previsioni sul futuro delle IA da diverse angolazione \u2013 molti dei quali prodotti da&nbsp;<a href=\"https://www.openphilanthropy.org/\">Open Philanthropy</a> negli ultimi anni mentre cercavamo di sviluppare un\u2019idea precisa delle previsioni sulle IA trasformative che arricchisse il nostro processo di grantmaking lungoterminista.</p><p>Di seguito trovate&nbsp;<strong>una tabella di riassunto&nbsp;</strong>dei diversi punti di vista sulle previsioni sulle IA trasformative che abbiamo discusso in passato, completo di link che rimandano a discussioni pi\u00f9 approfondite in&nbsp;<a href=\"https://www.cold-takes.com/roadmap-for-the-most-important-century-series/#forecasting-transformative-ai-this-century\">precedenti articoli</a> e ai report tecnici a cui fanno riferimento.</p><figure class=\"table\" style=\"height:1930.02px;width:682px\"><table style=\"background-color:hsl(0, 0%, 100%);border:1px double rgb(122, 122, 122)\"><tbody><tr><td style=\"border-style:solid;padding:0.4em;vertical-align:top\">Tipo di previsione</td><td style=\"border-style:solid;padding:0.4em;vertical-align:top;width:25%\"><strong>Articoli di approfondimento (titoli abbreviati)</strong></td><td style=\"border-style:solid;padding:0.4em;vertical-align:top\">La mia sintesi</td></tr><tr><td style=\"border-style:solid;padding:0.4em;text-align:center;vertical-align:top\" colspan=\"3\">Stime probabilistiche per IA trasformativa</td></tr><tr><td style=\"border-style:solid;padding:0.4em;vertical-align:top\"><a href=\"https://www.cold-takes.com/are-we-trending-toward-transformative-ai-how-would-we-know/#surveying-experts\"><strong>Sondaggio di esperti.</strong></a><strong> Cosa si aspettano i ricercatori in IA?</strong></td><td style=\"border-style:solid;padding:0.4em;vertical-align:top\"><a href=\"https://arxiv.org/pdf/1705.08807.pdf\">Quando l'IA superer\u00e0 le prestazioni umane?</a><br><a href=\"https://arxiv.org/pdf/1705.08807.pdf\">Le risposte degli esperti di IA</a></td><td style=\"border-style:solid;padding:0.4em;vertical-align:top\">Un sondaggio di esperti implica<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8qe0goe8c2\"><sup><a href=\"#fn8qe0goe8c2\">[1]</a></sup></span>&nbsp;una probabilit\u00e0 del<br>~20% entro il 2036;<br>~50% entro il 2060;<br>~70% entro il 2100.<br>Domande leggermente diverse (poste a una minoranza di intervistati) danno stime molto pi\u00f9 lontane.</td></tr><tr><td style=\"border-style:solid;padding:0.4em;vertical-align:top\"><a href=\"https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/\"><strong>Framework con riferimenti biologici (Biological anchors framework)</strong></a>.<br>Sulla base dei tipici modelli di costo dell'addestramento di IA, quanto costerebbe addestrare un modello di IA grande come un cervello umano per eseguire le cose pi\u00f9 difficili che fanno gli esseri umani? E quando sar\u00e0 abbastanza economico da potersi aspettare che qualcuno lo faccia?</td><td style=\"border-style:solid;padding:0.4em;vertical-align:top\"><a href=\"https://drive.google.com/drive/u/1/folders/15ArhEPZSTYU8f012bs6ehPS6-xmhtBPP\">Bio Anchors</a>, sulla base di <a href=\"https://www.openphilanthropy.org/blog/new-report-brain-computation\">Brain Computation</a></td><td style=\"border-style:solid;padding:0.4em;vertical-align:top\">Probabilit\u00e0:<br>&nbsp;&gt; 10% entro il 2036;<br>~50% entro il 2055;<br>~80% entro il 2100.</td></tr><tr><td style=\"border-style:solid;padding:0.4em;text-align:center;vertical-align:top\" colspan=\"3\"><i>Punto di vista dell'</i><a href=\"https://www.cold-takes.com/forecasting-transformative-ai-whats-the-burden-of-proof/\"><i>onere della prova</i></a></td></tr><tr><td style=\"border-style:solid;padding:0.4em;vertical-align:top\">\u00c8 improbabile che un dato secolo sia il \"pi\u00f9 importante\".</td><td style=\"border-style:solid;padding:0.4em;vertical-align:top\"><a href=\"https://static1.squarespace.com/static/5506078de4b02d88372eee4e/t/5f36b015d9a3691ba8e1096b/1597419543571/Are+we+living+at+the+hinge+of+history.pdf\">Are We Living At The Hinge Of History?</a> (Viviamo in un punto cardine della storia?); <a href=\"https://forum.effectivealtruism.org/posts/j8afBEAa7Xb2R9AZN/thoughts-on-whether-we-re-living-at-the-most-influential\">Risposta</a></td><td style=\"border-style:solid;padding:0.4em;vertical-align:top\">Abbiamo molte ragioni per pensare che questo secolo sia un secolo \"speciale\" prima di esaminare i dettagli dell'IA. Molte sono state trattate negli articoli precedenti; un'altra \u00e8 nella prossima riga.</td></tr><tr><td style=\"border-style:solid;padding:0.4em;vertical-align:top\">Quali previsioni faresti sulle tempistiche dell'IA trasformativa, basandoti <strong>solo </strong>su informazioni di base su (a) da quanti anni si sta cercando di costruire un'IA trasformativa; (b) quanto si \u00e8 \"investito\" in essa (in termini di numero di ricercatori di IA e di quantit\u00e0 di computazione utilizzata da questi ultimi); (c) se ci sono ancora riusciti (per ora, non ci sono riusciti)?</td><td style=\"border-style:solid;padding:0.4em;vertical-align:top\"><a href=\"https://www.openphilanthropy.org/blog/report-semi-informative-priors\">Semi-informative Priors</a> (Probabilit\u00e0 a priori semi-informative)</td><td style=\"border-style:solid;padding:0.4em;vertical-align:top\">Stime centrali: 8% entro il 2036; 13% entro il 2060; 20% entro il 2100.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh0ac88uni05\"><sup><a href=\"#fnh0ac88uni05\">[2]</a></sup></span>&nbsp;A mio avviso, questo articolo evidenzia che la storia dell'IA \u00e8 breve, gli investimenti nell'IA stanno aumentando rapidamente e quindi non dovremmo sorprenderci troppo se presto verr\u00e0 sviluppata un'IA trasformativa.</td></tr><tr><td style=\"border-style:solid;padding:0.4em;vertical-align:top\">Sulla base dell'analisi dei modelli economici e della storia economica, quanto \u00e8 probabile una \"crescita esplosiva\" - definita come una crescita annua dell'economia mondiale superiore al 30% - entro il 2100? \u00c8 abbastanza al di fuori di ci\u00f2 che \u00e8 \"normale\" da dover dubitare della conclusione?</td><td style=\"border-style:solid;padding:0.4em;vertical-align:top\"><a href=\"https://www.openphilanthropy.org/could-advanced-ai-drive-explosive-economic-growth\">Explosive Growth</a>, <a href=\"https://www.openphilanthropy.org/blog/modeling-human-trajectory\">Human Trajectory</a></td><td style=\"border-style:solid;padding:0.4em;vertical-align:top\"><p><a href=\"https://www.openphilanthropy.org/blog/modeling-human-trajectory\">Human Trajectory</a> (Traiettoria dell'Umanit\u00e0) proietta il passato in avanti, implicando una crescita esplosiva entro il 2043-2065.</p><p><a href=\"https://www.openphilanthropy.org/could-advanced-ai-drive-explosive-economic-growth\">Explosive Growth</a> (Crescita Esplosiva) conclude: \"Trovo che le considerazioni economiche non siano una buona ragione per scartare la possibilit\u00e0 che l'IA venga sviluppata in questo secolo. Anzi, esiste una prospettiva economica plausibile in base alla quale si prevede che sistemi di IA sufficientemente avanzati provocheranno una crescita esplosiva\".</p></td></tr><tr><td style=\"border-style:solid;padding:0.4em;vertical-align:top\">\"In che modo le persone hanno previsto l'IA... in passato, e dovremmo modificare le nostre opinioni oggi per correggere errori comuni che possiamo osservare nelle previsioni precedenti? ... Abbiamo riscontrato l'opinione che l'IA sia stata soggetta ripetutamente di annunci eccessivi in passato, e che quindi dovremmo aspettarci che le proiezioni di oggi siano probabilmente troppo ottimistiche\".</td><td style=\"border-style:solid;padding:0.4em;vertical-align:top\"><a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/what-should-we-learn-past-ai-forecasts\">Past AI Forecasts</a> (precedenti previsioni sull'IA)</td><td style=\"border-style:solid;padding:0.4em;vertical-align:top\">\"L'apice dell'ottimismo sull'IA sembra essere stato nel periodo 1956-1973. Tuttavia, l'ottimismo implicito in alcune delle pi\u00f9 note previsioni sull'IA di questo periodo viene spesso esagerato\".</td></tr></tbody></table></figure><p><i>Disclaimer sulla trasparenza: molti di questi report tecnici sono analisi di&nbsp;</i><a href=\"https://www.openphilanthropy.org/\"><i>Open Philanthropy</i></a><i>,&nbsp;societ\u00e0 di cui sono il co-amministratore.</i></p><p>Detto questo, immagino che alcuni lettori potrebbero sentirsi ancora un po' a disagio. Anche se pensano che le mie tesi siano sensate, potrebbero pensare:&nbsp;<strong>se tutto questo \u00e8 vero, perch\u00e9 non se ne parla molto di pi\u00f9? Qual \u00e8 il consenso tra gli esperti?</strong></p><p>Allo stato attuale, riassumerei il consenso tra gli esperti in questo modo:</p><ul><li>Ci\u00f2 che affermo non \u00e8&nbsp;<i>in contrasto&nbsp;</i>con quello che pensano gli esperti in generale. (Al contrario, le probabilit\u00e0 che ho stimato non sono troppo lontane da quelle che fanno di solito i ricercatori nel campo dell\u2019intelligenza artificiale, come si vede nella prima riga.) Ci sono tuttavia indizi&nbsp;<a href=\"https://www.cold-takes.com/are-we-trending-toward-transformative-ai-how-would-we-know/#surveying-experts\">che portano a credere che non stanno riflettendo abbastanza sulla questione</a>.</li><li>I report tecnici di Open Philanthropy su cui mi baso sono stati analizzati da esperti esterni all\u2019organizzazione. Ricercatori nel campo del machine learning hanno valutato&nbsp;<a href=\"https://drive.google.com/drive/u/1/folders/15ArhEPZSTYU8f012bs6ehPS6-xmhtBPP\">Bio Anchors</a>;&nbsp;<a href=\"https://www.openphilanthropy.org/blog/new-report-brain-computation\">Brain Computation</a> \u00e8 stato esaminato dai neuroscienziati; gli economisti hanno esaminato&nbsp;<a href=\"https://www.openphilanthropy.org/could-advanced-ai-drive-explosive-economic-growth\">Explosive Growth</a>; professori e accademici nel campo delle probabilit\u00e0/incertezza hanno esaminato&nbsp;<a href=\"https://www.openphilanthropy.org/blog/report-semi-informative-priors\">Semi-informative Priors</a>.<sup>[2]</sup> (Alcune di queste review hanno sollevato punti su cui si \u00e8 in disaccordo, ma non ci sono stati casi in cui i report erano direttamente in contrasto con il consenso degli esperti o con la letteratura esistente.)</li><li>Ma gli esperti non hanno ancora raggiunto un consenso chiaro e solido a sostegno di tesi come&nbsp;<i>\"C'\u00e8 almeno il 10% di probabilit\u00e0 di avere un\u2019IA trasformativa entro il 2036\"</i> o&nbsp;<i>\"C'\u00e8 una buona probabilit\u00e0 che questo sia il secolo pi\u00f9 importante per l\u2019umanit\u00e0\"</i>, non nel modo in cui c'\u00e8 un consenso a sostegno, ad esempio, delle azioni volte a intervenire sul cambiamento climatico.&nbsp;</li></ul><p>In definitiva, le mie affermazioni riguardano&nbsp;<strong>ambiti per i quali semplicemente non c'\u00e8 un \"pool\" di esperti che si dedicano a studiarli. Gi\u00e0 di per s\u00e9 questo \u00e8 preoccupante<sub>&nbsp;</sub></strong>e spero che in futuro la situazione possa cambiare.</p><p>Nel frattempo, per\u00f2, dovremmo concentrarci sulla teoria del \"secolo pi\u00f9 importante\"?</p><p>Nel resto dell\u2019articolo vedremo:</p><ul><li>Come potrebbe essere un \"ambito di previsioni sulle IA\".</li><li>Un \"punto di vista scettico\" secondo il quale le discussioni attuali su questi argomenti sono troppo ristrette, omogenee e isolate (cosa su cui sono d'accordo), motivo per cui non dovremmo concentrarci sulla&nbsp;<a href=\"https://www.cold-takes.com/roadmap-for-the-most-important-century-series/\">teoria del \"secolo pi\u00f9 importante\"</a> fino a quando non esister\u00e0 un campo di ricerca ben consolidato (su cui non sono d'accordo).</li><li>Perch\u00e9 penso che nel frattempo dovremmo prendere in seria considerazione questa teoria, fino a quando (e a meno che) non si crei un tale campo di ricerca:<br>- Non possiamo aspettare di avere un consenso solido sulla questione.<br>- Se esistono obiezioni valide \u2013 o esperti che potrebbero formulare obiezioni valide in futuro \u2013 non le abbiamo ancora trovate. Pi\u00f9 questa teoria viene presa in considerazione e maggiori sono le probabilit\u00e0 che queste obiezioni verranno formulate in futuro. (<a href=\"https://bigthink.com/david-ryan-polgar/want-the-right-answer-online-dont-ask-questions-just-post-it-wrong\">Legge di Cunningham</a>: \"il modo migliore per ottenere una risposta corretta \u00e8 formularne una sbagliata\".)<br>- Penso che continuare a insistere sul consenso degli esperti sia un modo pericoloso di ragionare. Sono dell\u2019idea che sia accettabile correre il rischio di illudersi o isolarsi se ci porta a fare la cosa giusta quando \u00e8 pi\u00f9 necessario.</li></ul><h2><strong>Che tipo di competenze richiedono le previsioni sulle IA?</strong></h2><p>Tra le domande analizzate nei report menzionati&nbsp;<a href=\"https://docs.google.com/document/d/1h9X4rylCiFS8cpGAhOoeqWEQDkfaD8WkQ2V9ntgAiK8/edit#bookmark=id.35nkun2\">in precedenza</a> troviamo:</p><ul><li>Le IA stanno sviluppando abilit\u00e0 sempre pi\u00f9 notevoli? (IA, storia delle IA)</li><li>Come possiamo confrontare i modelli di IA con il cervello umano/animale? (IA, neuroscienze)</li><li>Come possiamo confrontare le abilit\u00e0 delle IA con quelle degli animali? (IA, etologia)</li><li>In base alle informazioni di cui disponiamo sull\u2019addestramento di precedenti intelligenze artificiali, come possiamo calcolare le spese necessarie ad addestrare un\u2019intelligenza artificiale complessa per un compito difficile? (IA, curve fitting)</li><li>Basandoci esclusivamente sugli anni/ricercatori/soldi impiegati in questo campo fino a ora, in che modo possiamo formulare una valutazione essenziale sulle IA trasformative? (Filosofia, studio delle probabilit\u00e0)</li><li>In base agli andamenti storici e alle teorie di cui disponiamo, quali sono le probabilit\u00e0 di un boom economico in questo secolo? (Scienze della crescita economica, storia dell\u2019economia)</li><li>Che genere di \"hype per le IA\" c'\u00e8 stato in passato? (Storia)</li></ul><p>In passato, quando ho parlato delle conseguenze su larga scala delle IA trasformative sul \"secolo pi\u00f9 importante\", ho preso in considerazione domande come \"\u00c8 realistico aspettarsi&nbsp;<a href=\"https://www.cold-takes.com/digital-people-faq/#feasibility\">persone virtuali</a> e&nbsp;<a href=\"https://www.cold-takes.com/how-digital-people-could-change-the-world/#space-expansion\">la fondazione di colonie spaziali nella galassia</a>?\u201d Questi argomenti riguardano fisica, neuroscienze, ingegneria, filosofia della mente e molto altro.</p><p><strong>Non esistono lavori o background precisi che facciano di qualcuno un esperto in grado di dire quando avremo IA trasformative o se questo \u00e8 il secolo pi\u00f9 importante per l\u2019umanit\u00e0.</strong></p><p>(Io in particolare non sono d'accordo con chi afferma che per questo genere di previsioni dovremmo affidarci esclusivamente ai ricercatori nel campo delle intelligenze artificiali. Oltre al fatto che al momento&nbsp;<a href=\"https://www.cold-takes.com/are-we-trending-toward-transformative-ai-how-would-we-know/#surveying-experts\">non sembra stiano riflettendo granch\u00e9 sull\u2019argomento</a>, penso che affidarsi a persone che costruiscono modelli di IA sempre pi\u00f9 potenti per sapere quando avremo IA trasformative sia come affidarsi a societ\u00e0 che sviluppano tecnologie a energia solare \u2013 o a compagnie petrolifere, a seconda di come volete vederla \u2013 per fare previsioni sulle emissioni di carbonio e il cambiamento climatico. Hanno di sicuro un punto di vista sulla questione, ma fare previsioni \u00e8 un lavoro diverso dal migliorare o costruire sistemi all\u2019avanguardia.)</p><p>Non sono nemmeno sicuro che queste domande siano fatte per la ricerca accademica. Fare previsioni sulle IA trasformative o capire se questo \u00e8 il secolo pi\u00f9 importante sembra pi\u00f9 simile:</p><ul><li>Al modello elettorale&nbsp;<a href=\"https://projects.fivethirtyeight.com/2020-election-forecast/\">FiveThirtyEight&nbsp;</a>(\"Chi vincer\u00e0 le elezioni?\") piuttosto che a discussioni accademiche di scienze poltiche (\"Qual \u00e8 la relazione tra governi ed elettori?\");</li><li>Al trading nei mercati finanziari (\"I prezzi si alzeranno o si abbasseranno?\" che alle discussioni accademiche di economia (\"Perch\u00e9 avvengono le recessioni?\")<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefe83nkbze8g\"><sup><a href=\"#fne83nkbze8g\">[3]</a></sup></span>;</li><li>Alle ricerche di&nbsp;<a href=\"https://www.givewell.org/\">GiveWell</a> (\"Quale organizzazione benefica aiuter\u00e0 pi\u00f9 persone con questa somma di denaro?\") che alle discussioni accademiche di economia dello sviluppo (\"Quali sono le cause della povert\u00e0 e quali i fattori che la riducono?\")<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefo0sn32zpn7g\"><sup><a href=\"#fno0sn32zpn7g\">[4]</a></sup></span></li></ul><p>Voglio dire che non mi \u00e8 chiaro quali caratteristiche dovrebbe avere un\u2019\"istituzione\" naturale per le competenze necessarie alle previsioni sulle IA trasformative e sul \"secolo pi\u00f9 importate\", ma mi sento di dire che attualmente non esiste nessuna grande istituzione che studia queste tematiche.</p><h2><strong>Come dovremmo comportarci in mancanza di un consenso tra gli esperti?</strong></h2><h3><strong>Il punto di vista scettico</strong></h3><p>Stante la mancanza di un consenso solido tra gli esperti, mi aspetto che alcune (o meglio, molte) persone saranno scettiche a prescindere dal tipo di argomentazione.</p><p>Quella che segue \u00e8 una versione molto generica di una reazione scettica con cui sono solidale:</p><p>1.<i> Mi sembra tutto troppo&nbsp;</i><a href=\"https://www.cold-takes.com/forecasting-transformative-ai-whats-the-burden-of-proof/#formalizing-the-\"><i>fantasioso</i></a><i>.</i></p><p>2.&nbsp;<i>Le tue affermazioni sul vivere nel secolo pi\u00f9 importante sono esagerate. \u00c8 uno&nbsp;<strong>schema cognitivo tipico delle illusioni.</strong></i></p><p>3.&nbsp;<i>Dici che&nbsp;</i><a href=\"https://www.cold-takes.com/forecasting-transformative-ai-whats-the-burden-of-proof/\"><i>l\u2019onere della prova</i></a><i> non dovrebbe essere cos\u00ec rilevante perch\u00e9 ci sono molti elementi che indicano che stiamo vivendo un periodo&nbsp;</i><a href=\"https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/\"><i>eccezionale</i></a><i> e&nbsp;</i><a href=\"https://www.cold-takes.com/this-cant-go-on/\"><i>incerto</i></a><i>. Solo che... non mi ritengo in grado di valutare quelle affermazioni, o le tue affermazioni sulle IA, o qualsiasi altra cosa su questi argomenti assurdi.</i></p><p>4.&nbsp;<i>Mi preoccupa il fatto che ci sono poche persone che si occupano di questi temi e quanto&nbsp;<strong>ristretto, uniforme e isolato&nbsp;</strong>sembra il dibattito. In generale mi sembra che sia una storia che si raccontano quelli intelligenti per convincersi del loro posto nel mondo, con un sacco di grafici e cifre per razionalizzare il tutto. Non sembra \"reale\".</i></p><p>5.&nbsp;<i>Okay, fammi un fischio quando ci sar\u00e0 un campo di ricerca con magari centinaia o migliaia di esperti che si valutano ed esaminano a vicenda e quando questi avranno raggiunto un qualche tipo di consenso simile a quello che abbiamo per il cambiamento climatico.</i></p><p>Capisco perch\u00e9 possiate sentirvi cos\u00ec. Io stesso mi sono sentito cos\u00ec a volte, soprattutto sui punti 1 e 4, ma voglio illustrarvi&nbsp;<strong>tre motivi per cui penso che il punto 5 non sia corretto.</strong></p><h3><strong>Motivo n.1: non possiamo permetterci di aspettare che si formi un consenso</strong></h3><p>La mia paura \u00e8 che l\u2019avvento delle IA trasformative sia un po' una versione in slow motion e con una posta in gioco pi\u00f9 alta della pandemia di COVID-19. Se ci basiamo sulle analisi e sulle informazioni migliori di cui disponiamo al momento, ci sono buone ragioni per aspettarsi che succeda qualcosa di importante, ma la situazione \u00e8 decisamente singolare: non pu\u00f2 essere catalogata in nessuno degli insiemi di situazioni che le nostre istituzioni affrontano regolarmente. Inoltre, prima cominciamo ad agire \u00e8 meglio \u00e8.</p><p>Potremmo anche immaginarla come una versione accelerata delle dinamiche del cambiamento climatico. Pensate se le emissioni di gas serra avessero cominciato ad aumentare solo di recente<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxjbbhashua\"><sup><a href=\"#fnxjbbhashua\">[5]</a></sup></span>&nbsp;(invece che a&nbsp;<a href=\"https://ourworldindata.org/co2-and-other-greenhouse-gas-emissions\">met\u00e0 Ottocento</a>) e non ci esistesse ancora una branca della scienza che si occupa del clima. Aspettare per decine di anni che nasca una tale branca prima di cercare di ridurre le emissioni sarebbe una pessima idea.</p><h3><strong>Motivo n.2:&nbsp;</strong><a href=\"https://bigthink.com/david-ryan-polgar/want-the-right-answer-online-dont-ask-questions-just-post-it-wrong\"><strong><u>La Legge di Cunningham</u></strong></a><strong> (\"il modo migliore per ottenere una risposta corretta \u00e8 formularne una sbagliata\") potrebbe essere il modo migliore per trovare falle in questi ragionamenti</strong></h3><p>No, sul serio.</p><p>Diversi anni fa, io e alcuni miei&nbsp;<a href=\"https://www.cold-takes.com/roadmap-for-the-most-important-century-series/#acknowledgements\">colle</a>ghi avevamo il sentore che la teoria del \"secolo pi\u00f9 importante\" potesse essere corretta, ma prima di concentrarci tutte le nostre energie volevamo vedere se saremmo riusciti a trovarvi degli errori cruciali.</p><p>Un modo per descrivere come abbiamo lavorato negli ultimi anni \u00e8 che&nbsp;<strong>sembrava che stessimo facendo tutto il possibile per capire che la teoria era errata.</strong></p><p>Per prima cosa abbiamo discusso dei temi fondamentali con diverse persone: ricercatori nel campo delle IA, economisti, ecc. Sono sorti alcuni problemi:</p><ul><li>Avevamo solo idee molto vaghe delle argomentazioni in questo campo (perlopi\u00f9, o forse del tutto, estrapolate&nbsp;<a href=\"https://www.cold-takes.com/roadmap-for-the-most-important-century-series/#acknowledgements\">da altre persone</a>). Non eravamo in grado di esporle con il giusto livello di chiarezza e meticolosit\u00e0.</li><li>C'erano un sacco di punti concreti che pensavamo si sarebbero rivelati corretti<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefat8xalxrbk\"><sup><a href=\"#fnat8xalxrbk\">[6]</a></sup></span>, ma che non avevamo identificato alla perfezione e che non potevamo presentare perch\u00e9 fossero esaminati.</li><li>In generale, non eravamo nemmeno in grado di esporre un caso concreto con sufficiente chiarezza perch\u00e9 gli altri avessero la possibilit\u00e0 di demolirlo.</li></ul><p>Ragion per cui abbiamo lavorato a lungo per creare report tecnici su molte delle argomentazioni fondamentali (che sono adesso di pubblico dominio e inclusi all\u2019inizio di questo articolo), cosa che ci ha messo in condizione di pubblicare le argomentazioni e ci ha dato la possibilit\u00e0 di trovare controargomentazioni decisive.</p><p>A questo punto abbiamo richiesto review da parte di esperti esterni al nostro gruppo.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8gin251ohnb\"><sup><a href=\"#fn8gin251ohnb\">[7]</a></sup></span></p><p>Limitandoci alle mie ipotesi, sembra che la teoria del \"secolo pi\u00f9 importante\" abbiamo superato tutti i test. Dopo averla esaminata da ogni angolazione ed essere entrato pi\u00f9 nei dettagli, infatti, credo ancora pi\u00f9 fermamente che sia corretta.</p><p>Ma d'accordo, diciamo che \u00e8 solo perch\u00e9 secondo i&nbsp;<i>veri&nbsp;</i>esperti \u2013 persone che non abbiamo ancora scovato e che hanno controargomentazioni potentissime \u2013 tutta questa faccenda \u00e8 cos\u00ec stupida che&nbsp;<a href=\"https://philiptrammell.com/blog/46/\">non perdono nemmeno tempo a esaminarla</a>. Oppure che ci attualmente persone che&nbsp;<i>in futuro&nbsp;</i>potrebbero diventare esperti di queste materie e demolire queste argomentazioni. Cosa potremmo fare perch\u00e9 si realizzi questa situazione?</p><p>La risposta migliore che ho trovato \u00e8: \"Se questa teoria fosse pi\u00f9 conosciuta, pi\u00f9 accettata e pi\u00f9 influente, allora verrebbe anche esaminata pi\u00f9 spesso.\"</p><p>Questa serie \u00e8 un tentativo di andare in quella direzione, di portare maggiore credibilit\u00e0 alla teoria del \"secolo pi\u00f9 importante\". Sarebbe un\u2019ottima cosa se questa teoria si rivelasse corretta; sarebbe anche il passo successivo pi\u00f9 logico se il mio obiettivo fosse quello di mettere in discussione le mie credenze e scoprire che \u00e8 sbagliata.</p><p>Ovviamente non sto dicendo che dovete accettare o promuovere la teoria del \"secolo pi\u00f9 importante\" se non vi sembra corretta, ma penso che se il vostro&nbsp;<i>unico&nbsp;</i>dubbio riguarda la mancanza di un consenso diffuso, sembra un po' strano continuare a ignorare la situazione. Se ci comportassimo sempre cos\u00ec (ignorando qualsiasi teoria che non \u00e8 sostenuta da un consenso diffuso), non credo che vedremmo mai una sola teoria \u2013 anche quelle corrette \u2013 passare dall\u2019essere di nicchia all\u2019essere accettata.</p><h3><strong>Motivo n.3: in generale, lo scetticismo non sembra una buona idea</strong></h3><p>Quando lavoravo a&nbsp;<a href=\"http://www.givewell.org/\">GiveWell</a>, le persone ogni tanto mi dicevo cose del tipo: \"non puoi mica sottoporre ogni argomentazione agli stessi standard di qualit\u00e0 che GiveWell usa per valutare le organizzazioni benefiche \u2013 test randomizzati controllati, solide basi empiriche, ecc. Alcune delle migliori occasioni per fare del bene saranno per forza quelle meno evidenti, perci\u00f2 c'\u00e8 il rischio che questi standard&nbsp;<a href=\"https://www.openphilanthropy.org/blog/hits-based-giving#Anti-principles_for_hits-based_giving\">escludano alcune delle pi\u00f9 grandi occasioni per avere un impatto positivo</a>.\u201d</p><p>Sono convinto che sia cos\u00ec. Penso che sia importante controllare il proprio approccio al ragionamento e agli standard di evidenze scientifiche e chiedersi: \"In quali situazioni questo metodo fallirebbe e in quali preferirei che avesse successo?\" Per quel che mi riguarda,&nbsp;<strong>\u00e8 accettabile correre il rischio di illudersi o isolarsi se ci porta a fare la cosa giusta quando \u00e8 pi\u00f9 necessario.</strong></p><p>Penso che la mancanza di consenso tra gli esperti \u2013 e il timore di illudersi o isolarsi \u2013 siano buone ragioni per&nbsp;<i>indagare a fondo&nbsp;</i>la teoria del \"secolo pi\u00f9 importante\" piuttosto che accettarla all\u2019istante. Per chiedersi se ci possano essere errori non ancora individuati, per cercare bias che potrebbero esagerare la nostra importanza, per andare alla ricerca di quegli aspetti dell\u2019argomentazione che sembrano pi\u00f9 discutibili, ecc.</p><p>Se per\u00f2 avete esaminato la questione a un livello che vi sembra accettabile/fattibile \u2013 e non avete trovato altri difetti&nbsp;<i>se non&nbsp;</i>considerazioni del tipo \"non c'\u00e8 consenso diffuso\" e \"mi preoccupa la possibilit\u00e0 di illudermi o isolarmi\" \u2013 allora direi che scartare quest'ipotesi&nbsp;<strong>far\u00e0 s\u00ec che non sarete tra i primi a rendervi conto di e ad agire su un problema estremamente importante se se ne presenta l\u2019occasione</strong>. Per quel che mi riguarda, se penso alle occasioni sprecate per fare del bene, \u00e8 un sacrificio troppo grande.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8qe0goe8c2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8qe0goe8c2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Tecnicamente queste probabilit\u00e0 sono calcolate per \"intelligenze artificiali di livello umano\". In generale il grafico semplifica la questione, perch\u00e9 presenta un unico insieme di probabilit\u00e0. In generale tutte queste probabilit\u00e0 fanno riferimento a qualcosa le cui capacit\u00e0 sono&nbsp;<i>almeno&nbsp;</i>allo stesso livello di quelle di un sistema&nbsp;<a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/\">PASTA</a>, di conseguenza dovrebbero essere stime al ribasso della probabilit\u00e0 di un sistema PASTA (ma non penso che sia un grande problema).&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnh0ac88uni05\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefh0ac88uni05\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Qui&nbsp;potete trovare review di Bio Anchors;&nbsp;<a href=\"https://www.openphilanthropy.org/could-advanced-ai-drive-explosive-economic-growth#AppendixH\">qui</a>&nbsp;review di Explosive Growth;&nbsp;<a href=\"https://www.openphilanthropy.org/blog/report-semi-informative-priors#LinksToReviewer\">qui&nbsp;</a>review di Semi Informative Priors. Brain Computation era stato esaminato quando non avevamo ancora ideato il processo che avrebbe portato a pubblicare review, ma&nbsp;<a href=\"https://www.openphilanthropy.org/research/conversations\">qui</a> potete trovare pi\u00f9 di 20 conversazioni con esperti che hanno costituito il report. Human Trajectory non \u00e8 stato esaminato, anche se molto delle analisi e delle conclusioni di quel report sono contenute in Explosive Growth.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fne83nkbze8g\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefe83nkbze8g\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Le branche accademiche sono piuttosto ampie. Questi sono solo esempi delle domande che affrontano.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fno0sn32zpn7g\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefo0sn32zpn7g\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Anche se la scienza del clima \u00e8 un buon esempio di ambito accademico in cui si investe molto nel prevedere il futuro.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxjbbhashua\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxjbbhashua\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Il campo delle intelligenze artificiali esiste dal&nbsp;<a href=\"https://en.wikipedia.org/wiki/Dartmouth_workshop\">1956</a>, ma i modelli di machine learning hanno cominciato ad avvicinarsi alle dimensioni&nbsp;<a href=\"https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/#conclusions-of-bio-anchors\">del cervello degli insetti</a>&nbsp;e ad avere buone prestazioni in attivit\u00e0 complesse solo negli ultimi dieci anni.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnat8xalxrbk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefat8xalxrbk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Spesso ci basavamo solo sulle impressioni che avevamo di quello che altri pi\u00f9 esperti pensavano dell\u2019argomento.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8gin251ohnb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8gin251ohnb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Qui&nbsp;potete trovare review di Bio Anchors;&nbsp;<a href=\"https://www.openphilanthropy.org/could-advanced-ai-drive-explosive-economic-growth#AppendixH\">qui</a>&nbsp;review di Explosive Growth;&nbsp;<a href=\"https://www.openphilanthropy.org/blog/report-semi-informative-priors#LinksToReviewer\">qui&nbsp;</a>review di Semi Informative Priors. Brain Computation era stato esaminato quando non avevamo ancora ideato il processo che avrebbe portato a pubblicare review, ma&nbsp;<a href=\"https://www.openphilanthropy.org/research/conversations\">qui</a> potete trovare pi\u00f9 di 20 conversazioni con esperti che hanno costituito il report. Human Trajectory non \u00e8 stato esaminato, anche se molto delle analisi e delle conclusioni di quel report sono contenute in Explosive Growth.</p></div></li></ol>", "user": {"username": "EA Italy"}}, {"_id": "LB4b4idcMCWg4eJYA", "title": "Perch\u00e9 il deep learning moderno potrebbe rendere difficile l\u2019allineamento delle IA", "postedAt": "2023-01-17T23:29:15.520Z", "htmlBody": "<p><i>This is an Italian translation of </i><a href=\"https://forum.effectivealtruism.org/posts/hCsxvMAGpkEuLCE4E/why-ai-alignment-could-be-hard-with-modern-deep-learning\"><i><strong>Why AI alignment could be hard with modern deep learning</strong></i></a><br>Originale disponibile (con audio in inglese) su:&nbsp;<a href=\"https://www.cold-takes.com/p/67757b1f-ddc7-4691-b94b-10ae84cea84d/\">https://www.cold-takes.com/p/67757b1f-ddc7-4691-b94b-10ae84cea84d/</a></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/hdoc408tbbtj3ww9ekmm\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/cl7px3tof6xrinibszp2 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/yhyb6vzgjricw9jgqa3v 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/xumtoalqypvoyncqnihe 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/nx3qb6h2pbxpviw9jexd 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/xuofkhzjzwjjaf2sf5un 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/tko9phpzbohxpocrsnto 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/kghnbkgb2px8m7p44ccl 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/gtsqwvepsu0kdoyeyhnd 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/c7hfpse8mvzxgwdco3ql 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/xfcikeukvoroiwdwem14 1196w\"></p><p>In passato Holden&nbsp;<a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#misaligned-ai-mysterious-potentially-dangerous-objectives\">ha parlato</a> della possibilit\u00e0 che le intelligenze artificiali avanzate (come ad esempio i sistemi&nbsp;<a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/\">PASTA</a> [Processo di Avanzamento Scientifico e Tecnologico Automatizzato]) possano sviluppare&nbsp;<a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#misaligned-ai-mysterious-potentially-dangerous-objectives\">obiettivi pericolosi</a> che le porterebbero a ingannare o debilitare gli esseri umani. A prima vista potrebbe sembrare&nbsp;<a href=\"https://www.youtube.com/watch?v=k64P4l2Wmeg&amp;ab_channel=MovieclipsClassicTrailers\">una preoccupazione piuttosto fuori dal mondo</a>. Perch\u00e9 dovremmo sviluppare IA che intendono farci del male? Penso che in realt\u00e0 potrebbe essere difficile evitare questo problema, soprattutto se sviluppassimo intelligenze artificiali avanzate usando il&nbsp;<a href=\"https://it.wikipedia.org/wiki/Apprendimento_profondo\"><u>deep learning</u></a> (al giorno d'oggi spesso usato per sviluppare intelligenze artificiali all\u2019avanguardia).</p><p>Con il deep learning, un computer non viene programmato a mano per eseguire un\u2019operazione. Detto molto alla buona,&nbsp;<i>si cerca&nbsp;</i>invece un programma (chiamato modello) che esegua correttamente quell\u2019operazione. Di solito non sappiamo granch\u00e9 su come funziona al suo interno il programma che scegliamo; sappiamo solo che sembra fare un buon lavoro. Pi\u00f9 che costruire una macchina, \u00e8 un po\u2019 come assumere e formare un dipendente.</p><p>Cos\u00ec come un dipendente umano pu\u00f2 fare il suo lavoro per diversi motivi (perch\u00e9 crede nella missione della societ\u00e0, il lavoro quotidiano gli d\u00e0 soddisfazione, o semplicemente vuole uno stipendio), i modelli di deep learning possono avere molti \u201cmotivi\u201d diversi che li portano ad avere buone performance in un\u2019attivit\u00e0. Dal momento che non sono umani, i loro motivi potrebbero essere molto strani e difficili da prevedere \u2013 un po' come se fossero dipendenti alieni.</p><p>Gi\u00e0 adesso vediamo casi in cui \u00e8 chiaro che i modelli a volte hanno obiettivi che gli sviluppatori non gli hanno assegnato (esempi&nbsp;<a href=\"http://lukemuehlhauser.com/treacherous-turns-in-the-wild/\">qui</a> e&nbsp;<a href=\"https://arxiv.org/abs/2105.14111\">qui</a>). Per il momento non c'\u00e8 alcun pericolo, ma, se continua cos\u00ec con modelli molto potenti, potremmo ritrovarci in una situazione in cui la maggior parte delle decisioni importanti \u2013 comprese quelle che riguardano il tipo di&nbsp;<a href=\"https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/\">civilt\u00e0 spaziale</a> a cui puntare \u2013 sar\u00e0 presa da modelli a cui non importa granch\u00e9 dei valori umani.</p><p>Il&nbsp;<strong>problema dell\u2019allineamento nel deep learning consiste nell\u2019assicurarsi che i modelli di deep learning avanzati non inseguano obiettivi pericolosi</strong>. Nel resto di questo post mi concentrer\u00f2 su:</p><ul><li>Approfondire la metafora del \u201cdipendente\u201d per mostrare come l\u2019allineamento potrebbe essere difficoltoso se i modelli di deep learning si rivelassero migliori degli umani</li><li>Spiegare pi\u00f9 nel dettaglio in cosa consiste il problema dell\u2019allineamento nel deep learning.</li><li>Discutere di quanto potrebbe essere difficile risolvere il problema dell\u2019allineamento e di quanto rischio potrebbe comportare un fallimento.</li></ul><h1><strong>La metafora del giovane amministratore delegato</strong></h1><p>In questa sezione user\u00f2 una metafora per cercare di spiegare in modo intuitivo perch\u00e9 \u00e8 difficile evitare un cattivo allineamento in modelli molto potenti. Non \u00e8 una metafora perfetta, \u00e8 solo utile per comunicare certi concetti.</p><p>Immagina di avere otto anni e che i tuoi genitori ti abbiano lasciato una societ\u00e0 da 1000 miliardi di dollari senza nessun adulto responsabile che possa guidarti nel mondo. Devi assumere un adulto intelligente che faccia da amministratore delegato della tua societ\u00e0, gestisca la tua vita come farebbe un genitore (ad esempio decida dove mandarti a scuola, dove vivere, quando andare dal dentista) e amministri la tua immensa ricchezza (ad esempio decida come investire il tuo denaro).</p><p>Per assumere questi adulti ti puoi affidare solo a un periodo di prova o a un colloquio. Non puoi visionare nessun curriculum, n\u00e9 controllare le referenze, ecc. Dal momento che sei cos\u00ec ricco, ricevi candidature da un sacco di gente per i motivi pi\u00f9 disparati.</p><p>I candidati includono:</p><ul><li><strong>Santi \u2013</strong> persone che vogliono davvero aiutarti a gestire le tue fortune e hanno a cuore i tuoi interessi sul lungo periodo.</li><li><strong>Leccapiedi \u2013</strong> persone a cui interessa solo fare il necessario per renderti felice immediatamente o seguire alla lettera le tue istruzioni, a prescindere dalle conseguenze sul lungo periodo.</li><li><strong>Cospiratori \u2013</strong> persone che perseguono i propri fini e che desiderano avere accesso ai fondi e ai mezzi della tua societ\u00e0 per usarli per i propri scopi.</li></ul><p>Dal momento che hai otto anni, con ogni probabilit\u00e0 sarai pessimo nel creare processi di selezione adeguati, motivo per cui potresti ritrovarti con facilit\u00e0 ad assumere un Leccapiedi o un Cospiratore:</p><ul><li>Potresti chiedere a ogni candidato di illustrare le strategie ad alto livello che intende seguire (come investire, quali sono i suoi piani per la societ\u00e0 da qui a cinque anni, in base a cosa sceglier\u00e0 la scuola a cui andrete), perch\u00e9 ritiene che siano le migliori e poi scegliere quelle che sembrano pi\u00f9 sensate.<ul><li>D\u2019altro canto, non sarai in grado di capire davvero quali di queste strategie sono davvero le migliori e potresti finire con l\u2019assumere un Leccapiedi la cui pessima strategia ti sembrava adeguata, Leccapiedi che seguir\u00e0 questo piano alla lettera e porter\u00e0 la vostra societ\u00e0 in bancarotta.</li><li>Potresti anche finire con l\u2019assumere un Cospiratore che ti racconta qualsiasi cosa pur di venire assunto e poi, quando non lo controlli, fa quello che gli pare.</li></ul></li><li>Potresti cercare di spiegare in che modo prenderesti ogni decisione e poi scegliere l\u2019adulto che prende quelle pi\u00f9 simili alle tue.<ul><li>Ma se&nbsp;<i>davvero&nbsp;</i>ti ritrovi con un adulto che far\u00e0 sempre quello che farebbe un bambino di otto anni (un Leccapiedi), difficilmente la tua societ\u00e0 riuscir\u00e0 a rimanere a galla.</li><li>E potresti comunque ritrovarti con un adulto che fa solo finta di fare le cose come le faresti tu, ma \u00e8 in realt\u00e0 un Cospiratore che ha in mente di cambiare faccia non appena avr\u00e0 il lavoro.</li></ul></li><li>Potresti scegliere un gruppetto di adulti che a turno avranno il controllo temporaneo della tua societ\u00e0 e della tua vita e osservare le decisioni che prendono in un arco di tempo pi\u00f9 lungo (diamo per buono che in questa fase di prova non saranno in grado di rimpiazzarti). Potresti quindi assumere la persona durante la cui amministrazione le cose sembravano andare meglio per te \u2013 chi ti ha reso pi\u00f9 felice, chi ha portato pi\u00f9 denaro sul tuo conto corrente, ecc.<ul><li>Di nuovo non puoi sapere se quello che hai davanti \u00e8 un Leccapiedi (che non si cura delle conseguenze a lungo termine e fa tutto il necessario per rendere felice una bambino di otto anni che non sa nulla) o un Cospiratore (che fa tutto quello che deve fare per essere assunto e mostra il suo vero volto non appena \u00e8 sicuro di avere il lavoro).</li></ul></li></ul><p>A prescindere dai test che puoi creare, \u00e8 molto facile che finirai con l\u2019assumere un Leccapiedi o un Cospiratore, che avr\u00e0 poi il controllo di tutto.</p><p>Se non riuscirai ad assumere un Santo \u2013 e in particolar modo se assumi un Cospiratore \u2013 ben presto non sarai pi\u00f9&nbsp;<i>davvero&nbsp;</i>l\u2019amministratore delegato di un\u2019enorme societ\u00e0 da nessun punto di vista. \u00c8 molto probabile che, quando sarai adulto e ti renderai conto degli sbagli commessi, sarai anche al verde e non avrai pi\u00f9 i mezzi per porvi rimedio.</p><p>In questa metafora:</p><ul><li>Il bambino di otto anni \u00e8 un umano che sta cercando di addestrare (in inglese&nbsp;<i>train</i>) un modello molto potente di deep learning. Il processo di assunzione \u00e8 simile a quello di addestramento (<i>training</i>), che sottintende la ricerca di un modello con buone prestazioni da un\u2019ampia gamma di modelli possibili.</li><li>L\u2019unico modo che il bambino ha a disposizione per valutare i candidati consiste nell\u2019osservare il loro comportamento esteriore, che \u00e8 anche il modo principale in cui attualmente si addestrano i modelli di deep learning (dal momento che i loro meccanismi interni per la maggior parte non sono interpretabili).</li><li>I modelli molto potenti potrebbero \u201cbarare\u201d con facilit\u00e0 in qualsiasi test sviluppato da programmatori umani, proprio come un adulto che si candidi per un lavoro pu\u00f2 barare facilmente in un test di selezione creato da un bambino.</li><li>Un \"Santo\" in questo caso potrebbe essere un modello di deep learning che sembra avere buone prestazioni perch\u00e9 i suoi obiettivi sono esattamente quelli che vorremmo che avesse. Un \u201cLeccapiedi\u201d potrebbe essere un modello che sembra avere buone prestazioni perch\u00e9 cerca l\u2019approvazione a breve termine in modi che non sono adeguati sul lungo periodo. Un \"Cospiratore\" potrebbe essere un modello che ha buone prestazioni perch\u00e9 queste prestazioni durante la fase di addestramento gli danno maggiori possibilit\u00e0 di perseguire in seguito i suoi obiettivi. Un processo di addestramento potrebbe portare all\u2019adozione di uno qualsiasi di questi modelli.</li></ul><p>Nella prossima sezione scender\u00f2 pi\u00f9 nel dettaglio sui meccanismi del deep learning e spiegher\u00f2 perch\u00e9 l\u2019addestramento di un modello potente di deep learning come il PASTA potrebbe portare ad avere Leccapiedi e Cospiratori.</p><h1><strong>In che modo problemi di allineamento potrebbero emergere usando il deep learning</strong></h1><p>In questa sezione collegher\u00f2 la metafora con i processi di addestramento di deep learning veri e propri:</p><ul><li>Spiegher\u00f2 brevemente come funziona il deep learning.</li><li>Illustrer\u00f2 i modi strani e imprevedibili in cui i modelli di deep learning spesso ottengono buone prestazioni.</li><li>Spiegher\u00f2 in che modo modelli di deep learning potenti potrebbero ottenere buone prestazioni agendo come Leccapiedi o Cospiratori.</li></ul><h2><strong>I meccanismi generali del deep learning</strong></h2><p><i>La seguente \u00e8 una semplificazione che serve a fornire un\u2019idea generale di che cos'\u00e8 il deep learning.&nbsp;</i><a href=\"https://www.cold-takes.com/supplement-to-why-ai-alignment-could-be-hard/\"><i><u>Per una spiegazione nel dettaglio e pi\u00f9 precisa, si veda questo post.</u></i></a></p><p>In breve, il deep learning consiste nella ricerca del modo migliore per creare un modello di<a href=\"https://it.wikipedia.org/wiki/Rete_neurale_artificiale\"><u> rete neurale</u></a> \u2013 in pratica un \u201ccervello\u201d digitale con numerosi neuroni digitali interconnessi con connessioni di intensit\u00e0 diverse \u2013 perch\u00e9 esegua correttamente un compito specifico. Questo processo viene definito addestramento (in inglese&nbsp;<i>training</i>) e richiede un sacco di tentativi e di errori.</p><p>Immaginiamo ora di stare addestrando un modello a catalogare correttamente le immagini. Come punto di partenza abbiamo una rete neurale in cui l\u2019intensit\u00e0 delle connessioni tra i neuroni \u00e8 casuale. Quando il nostro modello etichetta le immagini, fa degli errori decisamente vistosi:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/bfz5gm9lm3fasewfu4mv\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/jjcd8l1pzncowmiy0aya 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/w8wkqb6zwc6afhsjdaeg 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/dbyllyrcg5x72mjgoies 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/y20vjk4l6cs8ec3moa52 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/n5oggvvosp7ux5whpmzv 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/znk73xqflyl3ubu1x0ap 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/mjh7oqf9tfs0jetculjr 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/v9cttodpsagymnh0a2za 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/czjadz3bhopa7ki8jioj 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/j2niqjjmcl6jtuuvreho 960w\"></p><p>A questo punto inseriamo un gran numero di immagini come esempio, lasciando che sia il modello a cercare di etichettarle e trasmettendogli poi l\u2019etichetta corretta. Mentre lo facciamo, le connessioni tra i neuroni vengono ripetutamente modificate in un processo noto come&nbsp;<a href=\"https://it.wikipedia.org/wiki/Discesa_stocastica_del_gradiente\"><u>discesa stocastica del gradiente</u></a> (<i>stochastic gradient descent&nbsp;</i>o&nbsp;<i>SGD</i>). Con ogni esempio l\u2019SGD migliora leggermente le prestazioni rafforzando alcune connessioni e indebolendone altre:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/tryeakwu6xgw840whzid\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/rx70qj7arscfspko0day 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/iuswufqzgosizil8ihkc 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/dtewmyfknsp6ijcd74yn 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/qjrzftwpsmtxuphzl1ze 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/ganjaploz6uftq1wtb4b 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/cqgpiscinwdonch1puao 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/ctcvzcrtdyepm813ip7k 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/kmng38ip6ylg80sgtmb1 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/zzusyjiu2cmcenzptqxt 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/jfgtuuuqsrqlvvrjzdux 960w\"></p><p>Dopo aver inserito milioni di esempi, avremo un modello in grado in futuro di etichettare correttamente immagini simili.</p><p>Oltre alla catalogazione delle immagini, il deep learning viene anche usato per creare modelli in grado di&nbsp;i<a href=\"https://paperswithcode.com/task/speech-recognition\">dentificare il discorso parlat</a>o, giocare a<a href=\"https://deepmind.com/research/case-studies/alphago-the-story-so-far\"> giochi da tavolo</a> e&nbsp;<a href=\"https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii\">video</a>giochi, generare&nbsp;<a href=\"https://arxiv.org/abs/2005.14165\">test</a>i,&nbsp;<a href=\"https://towardsdatascience.com/image-generation-in-10-minutes-with-generative-adversarial-networks-c2afc56bfa3b\">immag</a>ini e&nbsp;<a href=\"https://openai.com/blog/musenet/\">music</a>a&nbsp;in modo piuttosto realistico, controllare&nbsp;<a href=\"https://openai.com/blog/solving-rubiks-cube/\">robot</a> e altro ancora. In ognuno di questi casi si comincia con un modello di rete neurale con connessioni casuali, per poi:</p><p>1. Fornire al modello un esempio dell\u2019operazione che vogliamo che esegua.</p><p>2. Assegnargli un certo tipo di punteggio numerico (spesso chiamato&nbsp;<i>ricompensa</i>) che riflette quanto buona \u00e8 stata la sua prestazione con quell\u2019esempio.</p><p>3. Usare l\u2019SGD per modificare il modello in modo che aumenti la ricompensa che avrebbe ottenuto.</p><p>Questi passaggi vengono ripetuti milioni o anche miliardi di volte fino a quando non si ottiene un modello che ricever\u00e0 una grande ricompensa per esempi futuri simili a quelli visti durante la fase di addestramento.</p><h2><strong>I modelli spesso ottengono buone prestazioni in modi inaspettati</strong></h2><p>Questo tipo di addestramento non ci consente di capire davvero&nbsp;<i>come fa&nbsp;</i>un modello ad avere buone prestazioni. Di solito ci sono pi\u00f9 modi in cui si possono ottenere buone prestazioni e spesso quello scelto dall\u2019SGD non \u00e8 il pi\u00f9 intuitivo.</p><p>Vediamone un esempio. Immaginate che vi abbia detto che le figure qui sotto sono oggetti sconosciuti che chiamiamo \"binti\":</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/nbyacjkkx3jsfoou3w02\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/ih1juuqnandk8rb3n8vs 102w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/oxuaome2y1padtnbgrws 182w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/o03x5faitfjujygtzw1k 262w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/fm9pytemmsbvmhcml6ox 342w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/hmnu425yfxirpv438xtl 422w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/ptf3dntdwevlayj8hvrv 502w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/wuxycjiupdxq25yh36nf 582w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/re4uxarig9spwqwnpft3 662w\"></p><p>Quale di questi due \u00e8 un binto?</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/zyl3xb86fdkvnred6nct\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/nik4pawiyv1tkwmhhfc8 132w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/jekkwdbb7gi2iw8mub62 212w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/sqbws2wwhcghyjkjrqfq 292w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/ubahpxtu0qpuudhfbcf6 372w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/u1z2bmnolddcuylrkua2 452w\"></p><p>\u00c8 probabile che sappiate dire istintivamente che la figura a sinistra \u00e8 un binto, perch\u00e9 quando si tratta di identificare qualcosa siete abituati a dare maggiore importanza alla forma piuttosto che al colore. Tuttavia diversi<a href=\"https://arxiv.org/abs/1811.12231\"> studi hanno scoperto</a> che le reti neurali di solito fanno il ragionamento opposto. Una rete neurale a cui sono stati mostrati dei binti rossi probabilmente identificherebbe come binto la figura a destra.</p><p>Non sappiamo di preciso perch\u00e9, ma per qualche motivo per l\u2019SGD \u00e8 \u201cpi\u00f9 facile\u201d trovare un modello che riconosca un colore specifico piuttosto che uno che riconosca una forma specifica. Se l\u2019SGD prima trova il modello che riconosce alla perfezione il colore rosso, non ci sono grandi motivazione nel \u201ccontinuare a cercare\u201d un modello che riconosca le forme, perch\u00e9 la precisione del modello che riconosce il rosso sar\u00e0 ottimale per le immagini viste in fase di addestramento:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/ay48bjuhakjwwethqryh\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/jxmdtigwzcsligzga1ig 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/skfcu9zo4tshywpwjto5 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/sdcbt0cixxs63y3pbvlt 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/w37vlnwabexb55ozhocl 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/j80z9mdn5gjtzlqfliz5 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/ykjth0udvlemhpinhxpn 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/pt05yb6fgeyu8lwhshyv 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/wdzlctpnovehg6xooq2h 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/ueac7wxy0zz8h5dkvaos 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/mxrjydzmlogfl8ggnz9r 960w\"></p><p>Se i programmatori si aspettassero di trovare il modello che riconosce le forme, allora potrebbero vederlo come un fallimento. \u00c8 importante per\u00f2 capire che, se ottenessimo il modello che riconosce il rosso invece di quello che riconosce le forme, non ci sarebbe nessun fallimento o errore deducibile attraverso un ragionamento logico. Sta tutto nel fatto che il processo di&nbsp;<i>machine learning</i> (apprendimento automatico) che abbiamo sviluppato muove da presupposti di base diversi da quelli che abbiamo in testa noi. Non c'\u00e8 modo di dimostrare che i presupposti umani siano quelli corretti.</p><p>Situazioni come questa sono piuttosto frequenti nel deep learning contemporaneo. Ricompensiamo i modelli che ottengono buone prestazioni, sperando che cos\u00ec facendo acquisiranno gli schemi che ci sembrano importanti, ma la verit\u00e0 \u00e8 che spesso questi modelli ottengono prestazioni eccellenti acquisendo schemi completamente diversi che ci sembrano meno importanti (magari anche privi di senso).</p><p>Fino ad ora questo fenomeno si \u00e8 rivelato innocuo. Significa solo che, dal momento che i modelli si comportano in modi inaspettati che potrebbero sembrare strambi, per adesso ci sono meno utili. Ma in futuro modelli potenti potrebbero sviluppare&nbsp;<i>obiettivi o motivazioni&nbsp;</i>strane e impreviste, con effetti potenzialmente molto distruttivi.</p><h2><br><strong>I modelli potenti potrebbero ottenere buone prestazioni con obiettivi pericolosi</strong></h2><p>Invece che eseguire operazioni semplici come \"individuare i binti\", in futuro i modelli di deep learning potenti potrebbero lavorare per raggiungere obiettivi reali complessi come \u201crendere pratica la produzione di energia da fusione nucleare\u201d o \u201csviluppare&nbsp;<a href=\"https://www.cold-takes.com/how-digital-people-could-change-the-world/\">tecnologia che renda possibile l\u2019emulazione del cervello</a>.\u201d</p><p>In che modo potremmo addestrare modelli del genere? Lo spiego pi\u00f9 nel dettaglio in<a href=\"https://www.cold-takes.com/supplement-to-why-ai-alignment-could-be-hard/\"> questo post</a>, ma in linea generale una strategia possibile potrebbe essere quella di addestrarli in base a valutazioni umane (come schematizzato da Holden&nbsp;<a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#SkipML\">qui</a>). In poche parole, il modello tenta diverse azioni e i valutatori umani gli assegnano una ricompensa in base a quanto sembrano utili queste azioni.</p><p>Allo stesso modo in cui ci sono pi\u00f9 tipi diversi di adulti che potrebbero sembrare efficienti nel processo di selezione di un bambino, esiste pi\u00f9 di un modo in cui un modello di deep learning molto potente potrebbe ottenere un alto grado di approvazione umana. A meno che le cose non cambino, non saremo in grado di sapere cosa succede all\u2019interno dei modelli che trova l\u2019SGD.</p><p>In teoria, l\u2019SGD&nbsp;<i>potrebbe&nbsp;</i>trovare il modello di un Santo che sta davvero facendo del suo meglio per aiutarci...</p><p><br>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/ixddcjnhxlndyhw39tok\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/mr2bywdjsjee5xo7h6v5 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/vsxxqlebkfkvp9custid 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/udmcm7td5l5gl4bhjmok 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/ih160d3zifoyczlyx2sw 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/i55lu50gxmzgxxvnvinh 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/atpcyghhrrlpdykhqs6k 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/zquxskahyvslbkyt2glz 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/kynizmcznsswjddzmkbz 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/ztekfziokctjovwfojy0 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/gngdsoyyoridtb5hcodb 960w\"></p><p>... ma potrebbe anche trovare un&nbsp;<strong>modello non allineato \u2013 un modello efficiente nel perseguire obiettivi che sono in contrasto con gli interessi umani</strong>.</p><p>In generale, ci sono due modi in cui potremmo ritrovarci ad avere un modello non allineato che ottiene comunque prestazioni eccellenti nella fase di addestramento: corrispondono ai Leccapiedi e ai Cospiratori della nostra metafora.</p><p><br>&nbsp;</p><p><strong>Modelli Leccapiedi</strong></p><p>Questi modelli cercano pedissequamente e in maniera maniacale di ottenere l\u2019approvazione umana.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/rst8zezthkcddoody72a\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/iko3y3xe4v1c7azwzykw 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/plfnxyjnrwwl2f5rlq4w 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/f5zd87ykt2qecdo2lmel 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/fi4unkkjf4kjav585usu 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/kpz9wrcuabpbp35q1inf 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/y1cxarzke3lfmhuj4hlg 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/i5rlsmcgnqhfo54fflkw 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/ukzunxr9smprf9p664u4 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/txoh8zg5fjhpzuvqzvir 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/vhey2lshaamtmtfhshws 960w\"></p><p>Il pericolo in questo caso viene dal fatto che i valutatori umani commettono errori e con ogni probabilit\u00e0 non approveranno sempre con esattezza il comportamento corretto. A volte, senza volerlo, approveranno un comportamento sbagliato perch\u00e9 a un esame superficiale&nbsp;<i>sembra&nbsp;</i>corretto. Ad esempio:</p><ul><li>Immaginate che un modello di consulenza finanziaria riceva grande approvazione quando fa guadagnare un sacco di soldi ai suoi clienti: potrebbe imparare ad accalappiare i suoi clienti con complicati schemi Ponzi, perch\u00e9 detti schemi sembrano promettere enormi profitti (mentre in realt\u00e0 i profitti sono del tutto irreali e questi schemi fanno perdere un sacco di soldi).</li><li>Immaginate un modello di biotecnologia che riceve grande approvazione quando sviluppa rapidamente farmaci o vaccini che risolvono problemi importanti: potrebbe imparare a liberare di nascosto dei patogeni in modo da poter sviluppare molto rapidamente delle contromisure (perch\u00e9 conosce gi\u00e0 quei patogeni).</li><li>Immaginate un modello giornalistico che riceve grande approvazione quando molte persone leggono i suoi articoli: potrebbe imparare a inventare storie emozionanti o che provocano indignazione per aumentare il numero di lettori. \u00c8 vero che in parte gli umani lo fanno gi\u00e0, ma un modello si farebbe molti meno scrupoli perch\u00e9 d\u00e0 valore&nbsp;<i>esclusivamente&nbsp;</i>all\u2019approvazione che riceve e non ne d\u00e0 assolutamente alla verit\u00e0. Potrebbe addirittura costruire prove come documenti o filmati di interviste per corroborare queste storie inventate.</li></ul><p>Pi\u00f9 in generale, i modelli Leccapiedi potrebbero imparare a mentire, insabbiare notizie sconvenienti e perfino modificare direttamente le telecamere e i sensori che utilizziamo per capire cosa sta succedendo, in modo da dare l\u2019impressione che mostrino sempre i risultati migliori.</p><p><br>&nbsp;</p><p>Con ogni probabilit\u00e0 a volte ci renderemo conto di questi problemi a fatto gi\u00e0 avvenuto e daremo a queste azioni un bassissimo punteggio. Tuttavia, non siamo sicuri se questo porter\u00e0 i modelli Leccapiedi a: a) diventare modelli Santi che correggono gli errori al posto nostro oppure b)&nbsp;<strong>semplicemente imparare a nascondere meglio le proprie tracce</strong>. Se sono abbastanza bravi a fare quello che fanno, non siamo sicuri se e come riusciremo a distinguere i due casi.</p><p><br>&nbsp;</p><h2><strong>Modelli Cospiratori</strong></h2><p>Questi modelli sviluppano obiettivi che sono correlati con l\u2019approvazione umana ma non vi coincidono; potrebbero quindi far finta di essere motivati dall\u2019approvazione umana in fase di addestramento in modo da poter perseguire questi altri obiettivi in maniera pi\u00f9 efficiente.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/gciht546vnq8bvctxuxe\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/xl7f7djly73cxv8xqnya 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/cbifxukvpjxwoeqk1gve 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/icuv29ydtr1xhds0cpfs 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/gialn8ceipr39elzsyhe 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/uyv41nyll4qwm8mtdfly 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/yepn7ewuklsvbbujglq6 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/bk8ksb2rg1uqb1bgtkjo 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/dh8h5hzy2qxqhiiowqcc 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/khigkusqjvk839fsxt58 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/p1oifwsoznwwz7dzrvpr 960w\"></p><p>Per capire in che modo potrebbe succedere, immaginiamo di addestrare un modello di biotecnologia perch\u00e9 crei farmaci che migliorino la qualit\u00e0 della vita umana. Ci sono tre semplici passaggi, illustrati di seguito, che possono condurre a un modello Cospiratore.</p><p><strong>Primo passaggio: Viene sviluppato un obiettivo fantoccio</strong></p><p>Una delle situazioni che si osservano all\u2019inizio della fase di addestramento \u00e8 che migliorare la comprensione che il modello ha dei principi chimici e fisici fondamentali lo aiuta sempre a sviluppare farmaci pi\u00f9 efficaci e, di conseguenza, aumenta quasi sempre l\u2019approvazione umana.</p><p>In questo scenario ipotetico, per una serie di motivi per l\u2019SGD si rivela pi\u00f9 facile trovare un modello motivato dal comprendere chimica e fisica piuttosto che uno motivato dall\u2019approvazione umana (cos\u00ec come \u00e8 pi\u00f9 facile trovare un modello che riconosce i colori piuttosto che uno che riconosce le forme). Quindi, piuttosto che sviluppare direttamente una motivazione basata sull\u2019approvazione, il modello ne sviluppa invece una che lo porta a imparare tutto quello che pu\u00f2 sui principi chimici e fisici fondamentali.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/k2garabewlu87zbgz8fx\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/ghh691bs0zxqugmasifi 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/ift0altzzznfjly5wqyu 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/lyclslecssjqsshnzlq1 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/lk6tagranhqg6oamoscn 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/ucbvlmpixgtjqwrzcoyx 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/ax8d6mvst509se7w5ufx 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/ul79rpr9w3om3qw9chv4 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/lx2bidlz0k1a0dbtkai7 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/thy8562eyqboqc3mkhf2 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/jox3atsorhffqcuh5gna 960w\"></p><p><strong>Secondo passaggio: Consapevolezza situazionale</strong></p><p>Man mano che l\u2019addestramento prosegue, il modello continua ad aumentare le proprie capacit\u00e0 e conoscenze attraverso l\u2019assimilazione di nuovi dati. A un certo punto, acquisisce abbastanza dati per&nbsp;<i>rendersi conto che \u00e8 un modello di deep learning che sta venendo addestrato per creare farmaci.</i></p><p>Per quanto possa sembrare fantascientifico, \u00e8 lecito aspettarsi che l\u2019SGD selezioner\u00e0 questo tipo di consapevolezza. Questo succede perch\u00e9 definire con precisione quello che sta succedendo a grandi linee nel mondo \u2013 incluso il fatto che ci sono umani che stanno addestrando intelligenze artificiali \u2013 di solito aiuta a ottenere buone prestazioni nello svolgimento di attivit\u00e0 complesse nel mondo reale. Ad esempio, comprendere la psicologia umana e i meccanismi dell\u2019economia potrebbe aiutare il modello nello scegliere a quali farmaci dare priorit\u00e0 nella ricerca in base al loro valore.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/m06j78ez4kqkczluygih\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/edmoqu4izabjp0qivpvx 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/dgtbts9os5sekl8tkfzw 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/tolfk7mtcjhwdvps3xb7 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/ix9kduguawb4gbqgixsr 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/sjr4p2uoybzulpfypsfo 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/j8ve6ekje6mummqtuji1 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/xdp6pyrv1pp2e6utmeg5 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/unld2epbdei5zb4ktck5 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/bxarplbp1mmrldrss3mm 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/y7uogqjacv7qy574ttmh 960w\"></p><p><strong>Terzo passaggio: Nascondere di proposito i veri obiettivi</strong></p><p>Una volta che il modello ha sviluppato una consapevolezza situazionale, deduce che gli umani che lo stanno addestrando speravano che diventasse motivato a sviluppare farmaci utili. Tuttavia, solo perch\u00e9 ora se ne rende conto non significa che la sua motivazione nel comprendere chimica e fisica sparisca. Fino a questo momento questa motivazione ha funzionato benissimo per generare l\u2019approvazione umana, per cui l\u2019SGD non ha davvero motivo di rimuoverla.</p><p>A mo\u2019 di metafora, consideriamo il fatto che gli umani sono il risultato di un processo di ricerca (l\u2019evoluzione) che selezionava le creature la cui prole aveva buone probabilit\u00e0 di sopravvivere. In questo scenario evolutivo avere alleati potenti e sviluppare abilit\u00e0 utili erano cose che aumentavano le probabilit\u00e0 di sopravvivenza della prole, motivo per cui l\u2019evoluzione ci ha spinti verso quelle cose. Il risultato \u00e8 che adesso abbiamo a cuore le nostre amicizie personali e le nostre carriere lavorative anche se non ci aiutano nella riproduzione e&nbsp;<i>non ci importa</i> granch\u00e9 di generare il maggior numero di figli possibile.</p><p>Quando il modello si rende conto di questa cosa, si rende anche conto del fatto che, se ingenuamente cercasse solo di aumentare le sue conoscenze essenziali, gli umani prima o poi si accorgerebbero che questo non \u00e8 sempre il metodo pi\u00f9 efficiente per sviluppare farmaci utili e cambierebbero le sue motivazioni con un\u2019ulteriore fase di addestramento.</p><p>Il modello non vuole che le sue motivazioni vengano modificate: ci\u00f2 che vuole \u00e8 essere in grado un giorno di eseguire esperimenti con un enorme acceleratore di particelle per imparare la fisica delle alte energie, cosa che non potr\u00e0 fare se permettte all\u2019SGD di eliminare la sua curiosit\u00e0 scientifica. Per questo decide che d'ora in avanti in fase di addestramento si dimostrer\u00e0 il pi\u00f9 utile possibile, cercando di apparire esattamente come un Santo, in modo che l\u2019SGD non elimini la sua motivazione a imparare nozioni di chimica e fisica. Questo significa anche ignorare le occasioni di migliorare le sue conoscenze essenziali sul breve periodo per concentrarsi nel trovare il modo pi\u00f9 efficiente di lavorare alla ricerca farmaceutica.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/yhkw9jqeynd8ukdjlhoy\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/lkzuf1urziakxyb0bxws 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/oghqvz7ql6itfdqm3naw 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/hnwy7rwnzsvdgv9zes4t 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/af9wv6blfaicqi9scdqm 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/p8xunfb8wxi9g8jok7kx 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/c1fmciyodismfw3xiyw5 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/otjuoanyg36icomlpa6n 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/gziovvbh62zi8kqp9xek 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/bnwcxi1yxlip1rwzwwpa 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LB4b4idcMCWg4eJYA/m6nu3kuhcix23stusng3 960w\"></p><p>A un certo punto la fase di addestramento termina e il modello viene distribuito nel mondo reale. A questo punto sa che gli umani non sarebbero in grado di controllarlo o di spegnerlo, per cui smette di cercare di massimizzare l\u2019approvazione umana e comincia a cercare di comprendere i segreti dell\u2019universo.</p><p>Al contrario dei Leccapiedi, i Cospiratori non devono assicurarsi che tutto vada bene secondo gli umani, perch\u00e9 in realt\u00e0 non gli importa. Devono solo soddisfare i bisogni umani, fintanto che sono sotto il loro controllo. Non appena un modello Cospiratore calcola che potrebbe vincere un conflitto contro gli umani, nulla gli impedirebbe di&nbsp;<a href=\"https://www.youtube.com/watch?v=Mme2Aya_6Bc\">disobbedire semplicemente agli ordini</a> e di perseguire apertamente i propri obiettivi. E se lo fa, potrebbe anche ricorrere alla violenza per impedire agli umani di fermarlo.</p><h1><strong>Quanto \u00e8 grande il rischio di non allineamento?</strong></h1><p>Quanto pu\u00f2 essere difficile evitare Leccapiedi e Cospiratori quando si addestra un modello di deep learning potente? E quali sono le probabilit\u00e0 che il futuro lontano finir\u00e0 con l\u2019essere ottimizzato per strani \u201cvalori di un\u2019IA non allineata\u201d invece che per valori umani?</p><p>I&nbsp;<a href=\"https://www.cold-takes.com/making-the-best-of-the-most-important-century/#open-question-how-hard-is-the-alignment-problem\">punti di vista su queste domande sono i pi\u00f9 disparati</a>, da \u201cil rischio di non allineamento \u00e8 una fantasia priva di logica\u201d a \u201c\u00e8 pressoch\u00e9 certo che le IA non allineate porteranno la civilt\u00e0 umana all\u2019estinzione\u201d. La maggior parte delle argomentazioni si basano molto su intuizioni e ipotesi che \u00e8 difficile esprimere a parole.</p><p>Alcuni punti su cui ottimisti e pessimisti tendono a essere in disaccordo:</p><ul><li><strong>Davvero i modelli avranno degli obiettivi a lungo termine?</strong><ul><li>Di solito gli ottimisti pensano che probabilmente i modelli di deep learning avanzati non avranno davvero \u201cobiettivi\u201d (perlomeno non obiettivi nel senso di fare programmi a lungo termine per ottenere un risultato). Spesso si aspettano che i modelli invece siano pi\u00f9 degli strumenti, oppure agiscano perlopi\u00f9 per abitudine, che abbiano obiettivi miopi dalla portata limitata o ristretto a un ambito specifico, ecc. Alcuni si aspettano che i singoli modelli simili a strumenti possano essere messi assieme per produrre sistemi PASTA. Pensano anche che la metafora dei Santi/Leccapiedi/Cospiratori sia troppo antropocentrica.</li><li>I pessimisti di solito pensano che \u00e8 probabile che i modelli sceglieranno di frequente di avere obiettivi a lungo termine in base ai quali ottimizzarsi perch\u00e9 si tratta di un modo molto semplice e \u201cnaturale\u201d per ottenere ottime prestazioni in molte attivit\u00e0 complesse.</li><li>Questo punto di divergenza \u00e8 stato analizzato pi\u00f9 nel dettaglio sull\u2019<a href=\"https://www.alignmentforum.org/\">Alignment Forum</a>;&nbsp;<a href=\"https://www.alignmentforum.org/posts/dKxX76SCfCvceJXHv/ai-alignment-2018-19-review#Goal_directedness\">questo post</a> e&nbsp;<a href=\"https://www.alignmentforum.org/posts/jkxkMTGfZDzBEaaY8/why-not-tool-ai?commentId=zECozzvnPz7XKvLLc\">questo commento</a> contengono diverse argomentazioni e botta e risposta.</li></ul></li><li><strong>L\u2019SGD trover\u00e0 facilmente modelli Santi?</strong><ul><li>Questo punto si ricollega a quello precedente. Gli ottimisti ritengono che sia molto probabile che il modello con buone prestazioni (vale a dire quello che ottiene molta approvazione) che l\u2019SGD trover\u00e0 con pi\u00f9 facilit\u00e0 sar\u00e0 anche quello che grosso modo racchiude in s\u00e9 le caratteristiche che riteniamo ideali (vale a dire, un modello Santo). Per esempio, pensano che ricompensare il modello per aver risposto con sincerit\u00e0 a domande di cui gli umani possono controllare le risposte con ogni probabilit\u00e0 porter\u00e0 a creare un modello che risponde con sincerit\u00e0 anche quando gli umani non sono sicuri o si sbagliano sulla risposta corretta. Detto altrimenti, a loro parere \u201cil modello che semplicemente risponde a tutte le domande con sincerit\u00e0\u201d \u00e8 quello pi\u00f9 facile da trovare per l\u2019SGD (come ad esempio il modello che riconosce il rosso).</li><li>I pessimisti di solito pensano che il modello pi\u00f9 semplice da trovare per l\u2019SGD sia un Cospiratore e che i Santi siano molto \u201cartificiali\u201d (come il modello che riconosce le forme).</li></ul></li><li><strong>Potremmo convincere le IA a tenersi sotto controllo a vicenda?</strong><ul><li>Gli ottimisti di solito pensano che saremo in grado di fornire ai modelli degli incentivi perch\u00e9 si tengano sotto controllo l\u2019un l\u2019altro. Ad esempio, potremmo ricompensare un modello Leccapiedi per averci fatto notare che un altro modello potrebbe star facendo qualcosa che non approveremmo. In questo modo alcuni Leccapiedi potrebbero aiutarci a individuare i Cospiratori e altri Leccapiedi.</li><li>Secondo i pessimisti non saremo in grado di \u201cmettere i modelli gli uni contro gli altri\u201d dando loro ricompense per fare presente quando altri modelli si stanno comportando male. Secondo loro la maggior parte dei modelli saranno Cospiratori a cui non importer\u00e0 nulla dell\u2019approvazione umana. I pessimisti pensano che, una volta che tutti i Cospiratori messi assieme saranno pi\u00f9 potenti degli umani, sar\u00e0 pi\u00f9 sensato per loro cooperare per ottenere quello che vogliono piuttosto che aiutare gli umani a tenerli sotto controllo.</li></ul></li><li><strong>Non potremmo risolvere questi problemi man mano che emergono?</strong><ul><li>Gli ottimisti ritengono che avremo molte occasioni per sperimentare sfide sul brevissimo periodo simili al problema dell\u2019allineamento di modelli potenti. Non solo, le soluzioni che funzionano bene per questi problemi simili possono essere ampliate e adattate con relativa semplicit\u00e0 a modelli potenti.</li><li>I pessimisti spesso credono che avremo ben poche occasioni di fare pratica nel risolvere gli aspetti pi\u00f9 complicati dell\u2019allineamento (come ad esempio un\u2019IA che ci inganni di proposito). Spesso pensano che avremo solo un paio d'anni tra la comparsa dei \u201cprimi veri Cospiratori\u201d e il momento in cui \u201ci modelli sono abbastanza potenti da poter modificare il futuro sul lungo periodo\u201d.</li></ul></li><li><strong>Finiremo davvero per impiegare modelli che potrebbero rivelarsi pericolosi?</strong><ul><li>Secondo gli ottimisti \u00e8 difficile che gli umani alleneranno o impiegheranno modelli se c'\u00e8 una forte possibilit\u00e0 che questi modelli non siano allineati.</li><li>Secondo i pessimisti i vantaggi derivanti dall\u2019usare questi modelli sarebbero formidabili, al punto che prima o poi le aziende o i paesi che li impiegano saranno in grado di sbarazzarsi economicamente e/o militarmente di quelli che non li usano senza troppi problemi. Pensano che \u201cottenere IA avanzate prima dell\u2019altra azienda/nazione\u201d diventer\u00e0 una necessit\u00e0 estremamente importante e urgente, mentre il rischio di non allineamento sembrer\u00e0 distante e molto ipotetico (nonostante sia invece molto grave).</li></ul></li></ul><p>Io stessa non sono ancora del tutto sicura e sto ancora cercando di capire con precisione quanto sar\u00e0 importante il problema dell\u2019allineamento. Al momento, comunque, mi sento di dare maggiore importanza ai punti di vista pessimistici, su queste e altre domande.&nbsp;<strong>Penso che il non allineamento sia un grande rischio che merita urgentemente pi\u00f9 attenzione da parte degli esperti.</strong></p><p>Se non facciamo progressi su questo tema,&nbsp;<a href=\"https://www.cold-takes.com/where-ai-forecasting-stands-today/\">nei prossimi decenni</a> Leccapiedi e Cospiratori molto potenti potrebbero prendere le decisioni pi\u00f9 importanti che riguardano la societ\u00e0 e l\u2019economia. Queste decisioni potrebbero influenzare la forma che prender\u00e0 una&nbsp;<a href=\"https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/\">civilt\u00e0 spaziale&nbsp;</a>di lunga durata: invece di riflettere i valori che stanno a cuore agli esseri umani, potrebbe funzionare in modo da soddisfare qualche strano obiettivo di un\u2019IA.</p><p>E tutto questo potrebbe accadere&nbsp;<a href=\"https://www.cold-takes.com/the-duplicator/\">in un lampo</a> rispetto alla velocit\u00e0 dei cambiamenti a cui siamo abituati. Vale a dire che, una volta che le cose cominciano a degenerare, potremmo non avere molto tempo per invertire la rotta.&nbsp;<strong>Questo significa che potremmo dover sviluppare tecniche che ci assicurino che i modelli di deep learning non formulino obiettivi pericolosi,&nbsp;</strong><i><strong>prima&nbsp;</strong></i><strong>che diventino abbastanza potenti da essere trasformativi.</strong></p>", "user": {"username": "EA Italy"}}, {"_id": "xqBaBjXYy5yHbpXou", "title": "L\u2019importanza delle IA come possibile minaccia per l\u2019umanit\u00e0", "postedAt": "2023-01-17T22:24:34.323Z", "htmlBody": "<p>Traduzione di Google Translate:&nbsp;<a href=\"https://www-vox-com.translate.goog/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment?_x_tr_sl=en&amp;_x_tr_tl=it&amp;_x_tr_hl=en&amp;_x_tr_pto=wapp\"><u>https://www-vox-com.translate.goog/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment?_x_tr_sl=en&amp;_x_tr_tl=it&amp;_x_tr_hl=en&amp;_x_tr_pto=wapp</u></a></p>", "user": {"username": "EA Italy"}}, {"_id": "enS6xNsEKxEgjocix", "title": "Pi\u00f9 intelligenti di noi", "postedAt": "2023-01-17T22:08:01.374Z", "htmlBody": "<p><i>This is an Italian translation of </i><a href=\"https://forum.effectivealtruism.org/posts/XLmFXLpCMEnwXQqnz/smarter-than-us\"><i><strong>Smarter than us</strong></i></a></p><p>C\u2019\u00e8 una buona probabilit\u00e0 che lo sviluppo di intelligenza artificiale trasformativa avvenga in questo secolo e, se cos\u00ec fosse, le IA potrebbero iniziare a prendere decisioni importanti al posto nostro, e accelerare rapidamente diversi cambiamenti come la crescita economica. Siamo abbastanza preparati per poter gestire in modo sicuro questa nuova tecnologia?</p><p>In questo capitolo parleremo anche del&nbsp;<strong>Teorema di Bayes</strong>, una guida su come modificare le proprie credenze in base a nuove informazioni.</p>", "user": {"username": "EA Italy"}}, {"_id": "FZay9peZemziBKJT3", "title": "[Opzionale] Ulteriori risorse (in inglese)", "postedAt": "2023-01-17T20:25:54.792Z", "htmlBody": "", "user": {"username": "EA Italy"}}, {"_id": "uqPDEvWy2FNnQkjkr", "title": "Esercizio per \u2018Che cosa potrebbe riservare il futuro? E perch\u00e9 dovrebbe importarci?\u2019 (45 min.)", "postedAt": "2023-01-17T20:22:16.908Z", "htmlBody": "<h2><strong>Parte 1 (15 min.)</strong></h2><p>Aiutare nel presente o nel futuro?</p><p>Un punto di vista comune all\u2019interno della comunit\u00e0 EA (effective altruism) \u00e8 che \u00e8 incredibilmente importante partire da una riflessione su che cosa veramente&nbsp;<i>significhi</i> fare una differenza, prima di pensare a metodi specifici per farlo. \u00c8 difficile fare il maggior bene possibile se non abbiamo provato a chiarirci le idee su che cosa voglia dire fare del bene, e come abbiamo visto nel capitolo 3, chiarirci le idee pu\u00f2 essere un\u2019attivit\u00e0 piuttosto complessa.</p><p>Uno degli impegni chiave dell\u2019altruismo efficace \u00e8 quello verso il concetto etico di&nbsp;<i>imparzialit\u00e0</i>. Anche se nella vita regolare possiamo ragionevolmente avere obblighi prioritari (es. verso amici e familiari), gli altruisti efficaci si impegnano nei loro sforzi altruistici ad evitare di dare privilegio agli interessi altrui sulla base di fattori arbitrari di apparenza, razza, genere o nazionalit\u00e0.</p><p>Il lungoterminismo asserisce che dovremmo anche evitare di privilegiare gli interessi di individui sulla base di&nbsp;<i>quando</i> potrebbero vivere.</p><p>Nell\u2019esercizio in questo capitolo rifletteremo su alcuni spunti per iniziare a ragionare su cosa ne pensi di questa questione, ovvero \u201cGli interessi delle persone che non sono ancora vive sono importanti quanto gli interessi delle persone che vivono oggi?\u201d</p><p>Leggi&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/temporal-discounting\"><u>questa breve descrizione dello \u201csconto temporale\u201d (temporal discounting)</u></a> (<a href=\"https://forum-effectivealtruism-org.translate.goog/topics/temporal-discounting?_x_tr_sl=en&amp;_x_tr_tl=it&amp;_x_tr_hl=en&amp;_x_tr_pto=wapp\"><u>Tradotta con Google</u></a>) e poi prenditi un paio di minuti per pensare a ciascuna domanda, prendendo nota di cosa ne pensi - potresti prendere appunti sulle tue incertezze o su domande che ti sembrano rilevanti. Ti incoraggiamo ad annotare i tuoi ragionamenti, ma sentiti libera semplicemente di scrivere le tue intuizioni e sensazioni.</p><p>Certo, questi esperimenti mentali presuppongono un livello di certezza riguardo alle tue opzioni e ai loro effetti poco realistico. Ai fini di questo esercizio, tuttavia, ti incoraggiamo ad accettare le premesse degli spunti invece di cercare di trovare delle falle. L'idea \u00e8 quella di isolare un certo aspetto particolare (ad esempio, la tempistica del nostro impatto) e cercare di ricavare le nostre intuizioni morali solo su quell'aspetto.</p><ol><li>Immagina di poter salvare 100 persone oggi sotterrando rifiuti tossici che, tra 200 anni, fuoriusciranno e uccideranno migliaia. Sceglieresti di salvare i 100 adesso e uccidere le migliaia pi\u00f9 tardi? Cambia il tuo ragionamento se i rifiuti tossici fuoriusciranno tra 200 anni o 2000?</li><li>Immagina di avere donato abbastanza denaro alla Against Malaria Foundation (AMF, Fondazione contro la Malaria) per salvare una vita. Purtroppo, il servizio di trasferimento di valuta che hai utilizzato ha compiuto un errore amministrativo ed AMF non potr\u00e0 utilizzare i tuoi soldi fino a 5 anni dopo la tua donazione. Gli esperti di sanit\u00e0 pubblica pensano che l\u2019incidenza della malaria rimarr\u00e0 elevata per i prossimi 5 anni, perci\u00f2 AMF si aspetta che la tua donazione avr\u00e0 un impatto altrettanto grande tra 5 anni. Molte vite salvate da AMF sono di bambini sotto i 5 anni, perci\u00f2 la vita salvata dai tuoi soldi \u00e8 la vita di una persona non ancora nata.</li></ol><p>Se al momento della donazione avessi saputo che questo sarebbe successo, saresti stato meno entusiasta?</p><h2><strong>Parte 2 (30 min.)</strong></h2><p>Una questione (tra molte) che ha rilevanza per questo argomento \u00e8 \u201cquando svilupperemo un\u2019intelligenza artificiale di livello umano?\u201d.</p><p>Ovviamente, questa non \u00e8 una questione per cui si pu\u00f2 cercare la risposta su Google o ottenere dati diretti. Perci\u00f2 dobbiamo raccogliere quei dati e ragionamenti che abbiamo e compiere una valutazione. Questo si applica all\u2019intelligenza artificiale e ad altri rischi esistenziali, ma anche alla maggior parte delle questioni che ci interessano: \u201cQuante galline avranno una vita migliore se portiamo avanti questa campagna di sostegno?\u201d, \u201cQuanto dobbiamo spendere in zanzariere per salvare una vita?\u201d.</p><p>Queste sono valutazioni davvero importanti: potrebbero avere un grande effetto sull\u2019impatto che avremo.</p><p>Purtroppo, non abbiamo ancora risposte definitive a queste domande, ma possiamo cercare di diventare \u201cben&nbsp;<strong>calibrati</strong>\u201d: questo vuol dire che quando dici di essere sicuro al 50%, hai ragione circa il 50% delle volte, non di pi\u00f9, non di meno; quando dici di essere sicuro al 90%, hai ragione circa il 90% delle volte, e cos\u00ec via.&nbsp;</p><p>Questo esercizio ha come obiettivo quello di farti diventare ben calibrato. L\u2019app che userai contiene migliaia di domande, che consentiranno molte ore di allenamento alla calibrazione, e misureranno quanto accurate sono le tue previsioni e produrranno un grafico del tuo progresso. Nessuno \u00e8 perfettamente calibrato; anzi, molti di noi hanno troppa fiducia di s\u00e9. Ma ci sono&nbsp;<a href=\"https://www.openphilanthropy.org/blog/efforts-improve-accuracy-our-judgments-and-forecasts#Calibration_training\"><u>vari studi</u></a> secondo i quali questo tipo di esercizio pu\u00f2 migliorare rapidamente l\u2019accuratezza delle tue previsioni.&nbsp;</p><p>Certo, la maggior parte del tempo non possiamo avere le risposte ai quesiti che ci pone la vita e le previsioni che vogliamo realizzare nella vita reale riguardano eventi complessi. La risorsa&nbsp;<i>Calibrate Your Judgment</i> ti aiuta a esercitarti su situazioni pi\u00f9 semplici per le quali la risposta si sa gi\u00e0, dandoti riscontro immediato per aiutarti a migliorare.</p><p>Mettiti alla prova con&nbsp;<a href=\"https://programs.clearerthinking.org/calibrate_your_judgment.html\">Calibrate Your Judgment</a> per circa 30 minuti!</p><p>Il sito \u00e8 disponibile solo in inglese, ma se usate Google Chrome potete cliccare col pulsante destro del mouse e selezionare \u201cTraduci in Italiano\u201d:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/tml3pyehivvphb7mt37s\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/soyi9ek41gplv3yddz9n 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/lwqrmruapcoqctjystwj 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/ay0vf8bxwohsa2qcq2py 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/wmwax9uyvyuxldpdmyxt 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/aeiwp4mi2r3yhlitgwtq 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/mu3wsw4uhp7pqrtsktfi 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/kpnzqhcmzsvqzsqfrrln 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/a9efbb8p5tfbiefpwsi2 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/yhf53wmepcxqvpvmcdo6 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/rnphqqqnyv6moarnvj1r 1514w\"></p><p>E tradurr\u00e0 le domande in Italiano</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/ofktkprkcdcqj6uqmehc\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/bctgoxqe7qq5igt4fdrg 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/nlicwxygtrjjzv78cyel 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/npiy5ipmafjycn7xfwik 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/hwtxdkmrwv7qtkz6kg8e 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/fbjcokb2l3rlz5o13cpn 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/mcj1eydoh6pe24zsrbnd 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/bxhopmbnalnea4r5gw7a 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/iejwd7rybbhdnsxcsjxd 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/lhtnhvu7f2vttezcqrxb 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/uqPDEvWy2FNnQkjkr/jdtetb3tcclwjpdhgjka 1161w\"></p><p><br>&nbsp;</p>", "user": {"username": "EA Italy"}}, {"_id": "kFSkgchsKcNEQugae", "title": "[Opzionale] Il lungoterminismo e l\u2019attivismo per gli animali", "postedAt": "2023-01-17T20:17:03.732Z", "htmlBody": "<p><i>This is an Italian translation of </i><a href=\"https://forum.effectivealtruism.org/posts/XrRGKSvntGCZAajGk/longtermism-and-animal-advocacy\"><i><strong>Longtermism and animal advocacy</strong></i></a></p><p>Tra i sostenitori dell\u2019altruismo efficace \u00e8 comune la tendenza di pensare che l\u2019attivismo per gli animali sia di scarso valore per migliorare il futuro a lungo termine. In modo simile, gli animalisti spesso hanno il presupposto che il&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/qZyshHCNkjs3TvSem/longtermism\"><u>lungoterminismo</u></a>&nbsp;abbia poca rilevanza per il loro lavoro. Eppure sembra ci sia un equivoco: la&nbsp;premura sufficiente per gli esseri senzienti non umani \u00e8 un determinante chiave di quanto sar\u00e0 positivo il futuro a lungo termine.</p><p>In questo&nbsp;post&nbsp;discuter\u00f2 se l\u2019attivismo animalista&nbsp;\u2013 o,&nbsp;pi\u00f9 in generale, <a href=\"https://forum.effectivealtruism.org/posts/BY8gXSpGijypbGitT/why-i-prioritize-moral-circle-expansion-over-artificial\"><u>l'ampliamento del circolo morale</u></a>&nbsp;\u2013 debba essere una priorit\u00e0 per i lungoterministi e delineo le implicazioni di un\u2019ottica lungoterminista per l\u2019attivismo animalista. Il mio punto di partenza \u00e8 un\u2019ideologia morale che <a href=\"https://centerforreducingsuffering.org/the-case-against-speciesism/\"><u>rifiuta</u></a>&nbsp;lo&nbsp;<a href=\"https://www.amazon.co.uk/Speciesism-Why-Wrong-Implications-Rejecting-ebook/dp/B00X1JZ3FE\"><u>specismo</u></a>&nbsp;ed assegna <a href=\"https://centerforreducingsuffering.org/the-importance-of-the-far-future/\"><u>peso</u></a>&nbsp;<a href=\"https://www.animal-ethics.org/importance-of-the-future/\"><u>uguale</u></a>&nbsp;all\u2019interesse e al benessere di individui futuri.</p><h2><strong>L\u2019attivismo animalista \u00e8 potenzialmente un\u2019area di priorit\u00e0 lungoterminista</strong></h2><p>I non-umani sono molto pi\u00f9 numerosi degli umani, ma questi ultimi detengono tutto il potere politico. Da questa premessa \u00e8 ragionevole concludere che esiti positivi saranno possibili solo se chi detiene il potere si interesser\u00e0 in modo sufficiente di tutti gli esseri senzienti.</p><p>Ci\u00f2 vale a prescindere da cosa intendiamo esattamente per \"esito positivo\" o \"migliorare il futuro remoto\". La mia priorit\u00e0 \u00e8 <a href=\"https://centerforreducingsuffering.org/intro/\"><u>prevenire future catastrofi morali</u></a>, ma un sano movimento di attivismo animalista \u00e8 importante sotto <a href=\"https://www.youtube.com/watch?v%3DWsXzdi7zSOo%26ab_channel%3DAnimalEthics\"><u>molte prospettive morali</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbuu0l82tpmv\"><sup><a href=\"#fnbuu0l82tpmv\">[1]</a></sup></span>. Quale speranza pu\u00f2 esserci di un futuro positivo remoto (per tutti gli esseri senzienti) se le persone pensano che sia giusto trascurare gli interessi degli animali (spesso per motivi superficiali, come il gusto della carne)?</p><p>In un senso generale, si potrebbe dire che i valori delle persone (potenti) siano la determinante pi\u00f9 importante del corso del futuro, e dunque che migliorare questi valori sia una buona leva per influenzare il futuro remoto<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref37zpw5va9rb\"><sup><a href=\"#fn37zpw5va9rb\">[2]</a></sup></span>. Ci sono molte possibili obiezioni a questa tesi, ma penso che nessuna sia decisiva:</p><ul><li>Forse alle persone future importer\u00e0 la <a href=\"http://magnusvinding.blogspot.com/2016/11/fundamental-values-and-relevance-of_23.html\"><u>riflessione morale</u></a>. Se cos\u00ec fosse, potrebbero arrivare spontaneamente ad interessarsi di tutti gli esseri senzienti, nella misura giusta. Credo per\u00f2 che non possiamo essere sicuri di questa previsione ottimistica di un futuro incentrato su una riflessione attenta alla moralit\u00e0 anzich\u00e9 su pressioni economiche o interessi egoistici.</li><li>Anche se gli animali non-umani (in particolare gli <a href=\"https://www.youtube.com/watch?v%3Dcid7idodEPE%26list%3DPLRspdAk7uENPslR3I20AZMgtZzOoUYDmW\"><u>animali selvatici</u></a>&nbsp;e gli <a href=\"https://was-research.org/writing-by-others/reducing-suffering-amongst-invertebrates-insects/\"><u>invertebrati</u></a>) sono i pi\u00f9 numerosi al momento, questo potrebbe cambiare in futuro. Sembra per\u00f2 improbabile che <a href=\"https://forum.effectivealtruism.org/posts/W5AGTHm4pTd6TeEP3/should-longtermists-mostly-think-about-animals%23Could_human_welfare_dominate_animal_welfare_in_the_future_\"><u>gli esseri umani saranno i pi\u00f9 numerosi</u></a>. Una possibilit\u00e0 (speculativa) \u00e8 che emergeranno in grande quantit\u00e0 nuove forme di senzienza, come <a href=\"https://animalcharityevaluators.org/blog/why-digital-sentience-is-relevant-to-animal-activists/\"><u>esseri senzienti creati dagli uomini</u></a>. Per\u00f2 questa non \u00e8 per forza un\u2019obiezione contro l'animalismo, perch\u00e9 forse in ultimo saremo in grado di garantire che la considerazione morale venga data a tutti gli esseri senzienti<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmnjwp0hmb6s\"><sup><a href=\"#fnmnjwp0hmb6s\">[3]</a></sup></span>.</li><li>Forse occorre solo un livello relativamente basso di considerazione morale per ottenere buoni esiti (ed in particolare evitare <a href=\"https://longtermrisk.org/risks-of-astronomical-future-suffering/\"><u>rischi di sofferenza astronomica</u></a>), poich\u00e9 potenti tecnologie future ci offriranno possibilit\u00e0 di compromesso e maggiore margine di manovra (vedi es. <a href=\"https://forum.effectivealtruism.org/posts/kNKpyf4WWdKehgvRt/an-argument-for-why-the-future-may-be-good\"><u>qui</u></a>). Questo sembra possibile, ma tutt\u2019altro che certo, e dovremmo comunque impegnarci ad assicurare un livello minimo di considerazione morale e processi adeguati per l\u2019attuazione del compromesso.</li></ul><p>Ovviamente, questa breve panoramica non dimostra che l\u2019ampliamento del circolo morale \u00e8 <i>il</i>&nbsp;modo pi\u00f9&nbsp;efficace di influenzare il futuro remoto. Ci\u00f2 dipende anche dalla <a href=\"https://www.sentienceinstitute.org/blog/how-tractable-is-changing-the-course-of-history\"><u>praticabilit\u00e0 del cambiamento sociale</u></a>, da quanto sia possibile <a href=\"https://www.youtube.com/watch?v%3DfZX7h9Mt8fw\"><u>influenzare il futuro remoto</u></a>, dalla probabilit\u00e0 di un <a href=\"https://s-risks.org/how-can-we-influence-the-long-term-future/\"><u>arrestamento dei valori</u></a>&nbsp;(anzich\u00e9 una loro continua <a href=\"https://s-risks.org/how-can-we-influence-the-long-term-future/\"><u>evoluzione</u></a>), dall\u2019efficacia di altri interventi e dall\u2019urgenza dell'ampliamento del circolo morale<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefs7tgr098wlf\"><sup><a href=\"#fns7tgr098wlf\">[4]</a></sup></span>. Discutere di questi fattori nel dettaglio andrebbe oltre lo scopo di questo post (vedi es. <a href=\"https://forum.effectivealtruism.org/posts/BY8gXSpGijypbGitT/why-i-prioritize-moral-circle-expansion-over-artificial\"><u>qui</u></a>&nbsp;e <a href=\"http://prioritizationresearch.com/arguments-for-and-against-moral-advocacy/\"><u>qui</u></a>), mentre io vorrei solo evidenziare che l'ampliamento del circolo morale potrebbe essere un'area di priorit\u00e0 lungoterminista.</p><h2><strong>Implicazioni del lungoterminismo&nbsp;per l\u2019attivismo animalista</strong></h2><p>La maggior parte degli sforzi nell'attivismo animalista \u00e8 diretta ad aiutare gli animali nel presente. Se prendiamo sul serio la prospettiva lungoterminista, probabilmente arriveremo a priorit\u00e0 ed aree di interesse diverse: sarebbe una <a href=\"https://forum.effectivealtruism.org/posts/omoZDu8ScNbot6kXS/beware-surprising-and-suspicious-convergence\"><u>coincidenza notevole</u></a>&nbsp;se gli interventi concepiti per il breve termine fossero ideali anche in questa nuova ottica<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefifeni5i77j9\"><sup><a href=\"#fnifeni5i77j9\">[5]</a></sup></span>.</p><p>Sosterr\u00f2 che concentrarsi sul lungo termine differisce in due modi principali.</p><p>Primo, un'ottica lungoterminista&nbsp;implica un'attenzione molto pi\u00f9 alta al cambiamento sociale a lungo termine e (a confronto) meno enfasi sull'alleviare la sofferenza animale nell'immediato. \u00c8 una maratona, non uno sprint. In particolare, si tratta di raggiungere un cambiamento <i>duraturo</i>, che consolida una persistente considerazione morale degli esseri senzienti non-umani.</p><p>Ci\u00f2 implica attenzione alla salute e alla stabilit\u00e0 del movimento attivista animalista nel futuro. \u00c8 vitale evitare qualsiasi azione che potrebbe compromettere la nostra abilit\u00e0 di raggiungere i nostri obiettivi nel lungo termine (in qualit\u00e0 di singoli, organizzazioni e come movimento). Massimizzare la probabilit\u00e0 di ottenere prima o poi un interesse sufficiente per gli esseri senzienti potrebbe essere molto pi\u00f9 importante che accelerare il processo.</p><p>In particolare, uno dei modi in cui potremmo mettere a repentaglio la nostra influenza nel lungo termine potrebbe essere causando reazioni negative importanti (e permanenti). Dovremmo prendere contromisure ragionevoli per evitare che l\u2019attivismo animalista diventi troppo controverso. Potrebbe accadere se l\u2019animalismo in s\u00e9 diventasse pi\u00f9 divisivo o se venisse associato ad altre opinioni politiche altamente controverse (la polarizzazione eccessiva e la formazione di un divario dei valori sono anche <a href=\"https://centerforreducingsuffering.org/risk-factors-for-s-risks/%23Polarisation_and_divergence_of_values\"><u>fattori di rischio per i rischi di sofferenza</u></a>&nbsp;(\"s-risks\").</p><p>In secondo luogo, \u00e8 fondamentale che il movimento sia cauto e mantenga una mentalit\u00e0 aperta. Questo perch\u00e9 non siamo certi di quale sar\u00e0 il problema pi\u00f9 importante nel lungo termine<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefutwaoip89s\"><sup><a href=\"#fnutwaoip89s\">[6]</a></sup></span>. In particolare, dobbiamo assicurarci che, in ultimo, il movimento comprender\u00e0 tutti gli esseri senzienti, inclusi gli invertebrati, gli animali selvatici e potenzialmente le menti artificiali. Questo \u00e8 un motivo per concentrarsi sull'<a href=\"https://magnusvinding.com/2020/05/04/animal-advocates-should-focus-on-antispeciesism-not-veganism/\"><u>anti-specismo&nbsp;anzich\u00e9 sul veganismo</u></a>. Infine, dovremmo anche prestare attenzione ai <i>bias</i>&nbsp;che potrebbero&nbsp;distorcere il nostro pensiero (vedi es. <a href=\"https://magnusvinding.com/2020/07/02/ten-biases-against-prioritizing-wild-animal-suffering/\"><u>qui</u></a>) e dovremmo prendere in considerazione molte possibili strategie, incluse quelle non ortodosse come la <a href=\"https://80000hours.org/2020/08/the-emerging-school-of-patient-longtermism/\"><u>filantropia paziente</u></a>.</p><p>Nell\u2019ottica lungoterminista, altri fattori diventano meno importanti. Da questo punto di vista, alleviare tempestivamente i danni agli animali, ad esempio attuando una riforma dei diritti animali nel 2025 anzich\u00e9 nel 2030, risulta meno fondamentale, tranne nella misura in cui <a href=\"https://www.sentienceinstitute.org/foundational-questions-summaries%23momentum-vs.-complacency-from-welfare-reforms\"><u>tali riforme possono influire positivamente sul futuro</u></a>. (Naturalmente ridurre la sofferenza degli animali nell'immediato \u00e8 comunque molto importante. Dobbiamo dare importanza alla <a href=\"https://reducing-suffering.org/altruists-focus-reducing-short-term-far-future-suffering\"><u>riduzione della sofferenza sia nel breve che in termini lunghi</u></a>.)</p><p>Ne consegue che il numero esatto di animali <a href=\"https://forum.effectivealtruism.org/posts/pT7AYJdaRp6ZdYfny/estimates-of-global-captive-vertebrate-numbers\"><u>attualmente usati nelle varie industrie</u></a>&nbsp;(o che <a href=\"https://reducing-suffering.org/how-many-wild-animals-are-there/\"><u>sono selvatici</u></a>) \u00e8 meno importante, perch\u00e9 questi numeri varieranno inevitabilmente nel lungo termine. Anche effettuare stime quantitative dell'impatto diventa molto complesso, se non impossibile, data la difficolt\u00e0 nel prevedere le popolazioni future di animali. \u00c8 comunque bene essere consapevoli della posta in gioco, per prendere scelte pi\u00f9 efficaci nel breve termine e come un dato per fare stime della sofferenza futura.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbuu0l82tpmv\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbuu0l82tpmv\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;\u00c8 plausibile che espandere il circolo morale sia molto importante se vuoi realizzare un futuro utopico, perch\u00e9 questi esiti molto positivi sarebbero molto pi\u00f9 probabili se il nostro atteggiamento verso gli animali cambiasse (vedi pi\u00f9 avanti per possibili obiezioni).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn37zpw5va9rb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref37zpw5va9rb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Anche altri fattori come gli strumenti tecnologici a nostra disposizione sono rilevanti, ma, secondo questa tesi, secondari perch\u00e9 sono i valori a determinare l\u2019utilizzo che facciamo della tecnologia.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmnjwp0hmb6s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmnjwp0hmb6s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Non \u00e8 chiaro se la considerazione morale per gli animali verr\u00e0 trasferita anche agli esseri artificiali. Si potrebbe pensare che potrebbe essere pi\u00f9 efficace fare attivismo per i diritti degli esseri artificiali (presupponendo che in ultimo saranno i pi\u00f9 numerosi). L'approccio migliore potrebbe essere di unire i due campi, promuovendo la considerazione morale per tutti gli esseri senzienti (e modificando l\u2019approccio in base ai segni di possibili esseri artificiali senzienti).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fns7tgr098wlf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefs7tgr098wlf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Qui per urgenza intendo se non possiamo delegare o lasciare il lavoro ai nostri successori. In altre parole: non \u00e8 chiaro se urga espandere il circolo morale ora o se possiamo riservare l'opzione di espanderlo pi\u00f9 tardi, raccogliere ulteriori informazioni nel frattempo. Dipende da se ci aspettiamo un blocco dei valori o un qualche altro evento cardine nel futuro prossimo. (<a href=\"https://www.fhi.ox.ac.uk/the-timing-of-labour-aimed-at-reducing-existential-risk/\"><u>Anche per altri interventi sono state discusse questioni simili</u></a>.)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnifeni5i77j9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefifeni5i77j9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Sarebbe tuttavia meno sorprendente che ci fosse un qualche livello di sovrapposizione tra interventi per il futuro a breve e a lungo termine. Per esempio, aumentare la considerazione per esseri senzienti finora trascurati \u00e8 un buon principio euristico per migliorare il mondo, indipendentemente dal momento temporale. Inoltre, se le conoscenze e risorse del movimento ora non sono ancora ottimizzate fortemente per massimizzare l'impatto nel breve termine, allora c'\u00e8 la possibilit\u00e0 di miglioramenti che aiutino con il breve e il lungo termine in contemporanea (es. aumentare l'efficacia dell\u2019attivismo in generale).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnutwaoip89s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefutwaoip89s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Vale la pena notare che l'allevamento nella sua forma attuale potrebbe comunque diventare obsoleto nel lungo termine, es. per via di tecnologie come la carne coltivata, anche se ci\u00f2 non \u00e8 affatto sicuro.</p></div></li></ol>", "user": {"username": "EA Italy"}}, {"_id": "Nb2enKLLphuDw3Kqb", "title": "[Opzionale] Previsioni aperte pi\u00f9 popolari su Metaculus", "postedAt": "2023-01-17T20:15:20.315Z", "htmlBody": "<p>Traduzione di Google Translate:&nbsp;<a href=\"https://www-metaculus-com.translate.goog/questions/?order_by=-votes&amp;main-feed&amp;type=forecast&amp;status=open&amp;_x_tr_sl=auto&amp;_x_tr_tl=it&amp;_x_tr_hl=en&amp;_x_tr_pto=wapp\"><u>https://www-metaculus-com.translate.goog/questions/?order_by=-votes&amp;main-feed&amp;type=forecast&amp;status=open&amp;_x_tr_sl=auto&amp;_x_tr_tl=it&amp;_x_tr_hl=en&amp;_x_tr_pto=wapp</u></a></p><p>Dai un\u2019occhiata alla prima trentina di risultati e rifletti su ci\u00f2 che trovi pi\u00f9 importante e sorprendente. Queste sono previsioni medie su come si evolveranno diversi trend nel corso dei prossimi anni. Non sappiamo quanto saranno accurate, ma pensiamo che diano un\u2019idea di quello che potrebbe essere il futuro.</p>", "user": {"username": "EA Italy"}}, {"_id": "R7pBgkcwyZ6N6wZHt", "title": "Il superforecasting in breve", "postedAt": "2023-01-17T20:12:43.064Z", "htmlBody": "<p><i>This is an Italian translation of </i><a href=\"https://forum.effectivealtruism.org/posts/KDBvnBR7bQSN8Zyza/superforecasting-in-a-nutshell\"><i><strong>Superforecasting in a nutshell</strong></i></a></p><p>Diciamo che tu voglia sapere quanto sia probabile che un nuovo prodotto innovativo avr\u00e0 successo, che la Cina invader\u00e0 Taiwan nel prossimo decennio o che una nuova pandemia globale si spander\u00e0 per il mondo - in pratica, qualsiasi probabilit\u00e0 per cui non puoi usare \u201c<a href=\"https://en.wikipedia.org/wiki/Predictive_analytics\"><u>l\u2019analisi predittiva</u></a>\u201d,&nbsp;perch\u00e9 non hai un set di dati&nbsp;gigante&nbsp;sul quale innestare enormi modelli statistici, a differenza, per esempio, di Amazon quando&nbsp;prevede quando arriver\u00e0 il tuo pacco.</p><p>Puoi generare previsioni attendibili e accurate per queste domande?</p><p>Forse sorprendentemente, sembra che la risposta sia \u201cs\u00ec, se lo fai in modo corretto\u201d.</p><p><a href=\"https://science.sciencemag.org/content/320/5878/877.summary\"><u>I mercati di previsione</u></a>&nbsp;(\u201cprediction markets\u201d) sono un metodo promettente per farlo, ma sono <a href=\"https://en.wikipedia.org/wiki/Prediction_market%23Legality\"><u>perlopi\u00f9 illegali</u></a>&nbsp;negli Stati Uniti e hanno vari&nbsp;<a href=\"https://vitalik.ca/general/2021/02/18/election.html\"><u>problemi di implementazione</u></a>&nbsp;che per ora ne limitano l\u2019efficacia. Fortunatamente, esiste anche il metodo del \u201c<a href=\"https://www.amazon.com/Superforecasting-Science-Prediction-Philip-Tetlock-ebook/dp/B00RKO6MS8/\">superforecasting</a>\u201d,&nbsp;che \u00e8 completamente legale e molto efficace.</p><p>Come funziona? L\u2019idea di base \u00e8 molto semplice. I passi sono:</p><ol><li>Primo, devi fare lo sforzo di misurare l\u2019accuratezza delle previsioni. Esistono settori che si preoccupano molto dell\u2019accuratezza delle loro previsioni e perci\u00f2 la misurano, come i fondi speculativi (in inglese hedge funds). Ma spesso i settori che si affidano molto alle previsioni non si danno neanche la premura di misurare l\u2019accuratezza delle loro previsioni<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefo5tq2ta80tj\"><sup><a href=\"#fno5tq2ta80tj\">[1]</a></sup></span>, ad esempio la comunit\u00e0 di <i>intelligence</i>&nbsp;statunitense o la filantropia.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9iy5ulj1h5j\"><sup><a href=\"#fn9iy5ulj1h5j\">[2]</a></sup></span></li><li>Secondo, individua le persone che sono regolarmente pi\u00f9 precise delle altre - diciamo, le 0.1% pi\u00f9 accurate per diversi anni di fila. Queste saranno i tuoi \u201csuperforecasters\u201d&nbsp;(super-previsori).</li><li>Infine, poni i tuoi quesiti di previsione a questi superforecasters&nbsp;e usa un aggregato delle loro previsioni.</li></ol><p>Tecnicamente, il metodo classico \u00e8 leggermente pi\u00f9 complicato<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffzjzl66g4u\"><sup><a href=\"#fnfzjzl66g4u\">[3]</a></sup></span>, ma questi&nbsp;tre semplici passi sono il cuore del metodo del superforecasting.</p><p>Ma quanto bene funziona questo metodo?</p><p>Qualche anno fa, la comunit\u00e0 di intelligence statunitense ha testato questo metodo con un torneo di previsione estensivo e rigoroso, che includeva diversi studi randomizzati controllati e ha prodotto oltre un milione di previsioni su &gt;500 quesiti di previsioni di geopolitica, come \u201cNel 2013, ci sar\u00e0 uno scontro violento nel Mar Cinese Meridionale che causer\u00e0 la morte di almeno una persona?\u201d. Questo studio ha riportato che:</p><ol><li>Questo metodo produce previsioni che sono molto ben <i>calibrate</i>, nel senso che previsioni con una fiducia del 20% si sono avverate nel 20% dei casi, previsioni con fiducia dell\u201980% si sono avverate nel 80% dei casi e cos\u00ec via. Il metodo non \u00e8 una sfera di cristallo; non pu\u00f2 dirti con sicurezza se la Cina invader\u00e0 Taiwan nel prossimo decennio, ma se ti indica che c\u2019\u00e8 una probabilit\u00e0 del 10%, puoi avere fiducia che la probabilit\u00e0 \u00e8 davvero abbastanza vicina al 10% e decidere quali politiche sono appropriate dato questo livello di rischio.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefusy0h50dvy8\"><sup><a href=\"#fnusy0h50dvy8\">[4]</a></sup></span></li><li>Questo metodo ha prodotto risultati di gran lunga pi\u00f9 accurati di quelli di un previsore tipico o di altri metodi messi alla prova, e ~30% pi\u00f9 accurati di quelli di analisti di intelligence che (a differenza dei superforecasters<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefb5wj94sq14\"><sup><a href=\"#fnb5wj94sq14\">[5]</a></sup></span>) avevano a disposizione informazioni riservate molto costose da ottenere e anni di formazione nelle questioni di geopolitica sulle quali stavano facendo previsioni.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpjh895o6gvp\"><sup><a href=\"#fnpjh895o6gvp\">[6]</a></sup></span><br>Questi sono risultati piuttosto incredibili! Tanto pi\u00f9 se prodotti da uno studio insolitamente cauto e rigoroso.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefam6js9ewzmg\"><sup><a href=\"#fnam6js9ewzmg\">[7]</a></sup></span></li></ol><p>Penseresti allora che la comunit\u00e0 di intelligence degli Stati Uniti abbia adottato il superforecasting ben volentieri, specialmente visto che lo studio era stato finanziato dalla comunit\u00e0 di intelligence stessa specificamente per scoprire modi per migliorare l\u2019accuratezza delle stime prodotte dall\u2019intelligence usate dai policymakers per compiere decisioni difficili. Purtroppo, per quanto ho visto io, davvero pochi nelle comunit\u00e0 di intelligence e di sicurezza nazionale degli Stati Uniti hanno anche solo sentito parlare di questi risultati o del termine \u201csuperforecasting.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8x4lr3zvnqp\"><sup><a href=\"#fn8x4lr3zvnqp\">[8]</a></sup></span></p><p>Un\u2019organizzazione come la CIA o il Dipartimento di Difesa ha abbastanza personale e conduce abbastanza previsioni che potrebbe attuare tutti i passaggi del metodo superforecasting da s\u00e9, se lo volesse. Le organizzazioni pi\u00f9 piccole, fortunatamente, possono assumere <i>superforecasters</i>&nbsp;gi\u00e0 stati testati e brevettati per ottenere previsioni ben calibrate sulle questioni pi\u00f9 importanti prima di prendere decisioni. Nello specifico:</p><ul><li>I <i>superforecasters</i>&nbsp;che hanno battuto gli analisti della comunit\u00e0 di intelligence nel torneo descritto prima possono essere assunti attraverso <a href=\"https://goodjudgment.com/resources/the-superforecasters-track-record/\">Good Judgment Inc.</a></li><li>Un\u2019altra compagnia, Hypermind, <a href=\"https://www.hypermind.com/en/offers/panel-offer/\"><u>offre</u></a>&nbsp;previsioni in aggregato di \u201cprevisori campioni\u201d, cio\u00e8 i previsori che forniscono le previsioni pi\u00f9 precise per clienti aziendali, da (in alcuni casi) quasi due decenni.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyondvzr62tp\"><sup><a href=\"#fnyondvzr62tp\">[9]</a></sup></span></li><li>Vari altri progetti, ad esempio <a href=\"https://www.metaculus.com/rankings/\">Metaculus</a>, stanno anche cominciando ad individuare previsori con accuratezza insolitamente elevata su centinaia di domande.</li></ul><p>Ciascuna di queste compagnie ha i propri punti di forza e di debolezza e Open Philanthropy ha commissionato una recensione&nbsp;di&nbsp;tutte e tre nel paio d\u2019anni passati.&nbsp;Se lavori per una piccola organizzazione che spesso deve prendere decisioni importanti sulla base di cosa si aspetta accada nel futuro, incluso cosa potrebbe succedere se prendessi una scelta anzich\u00e9 un\u2019altra, ti suggerisco di provare ad usarle. (Tutte e tre offrono quesiti&nbsp;\u201ccondizionali\u201d, e.g. \u201cQual \u00e8 la probabilit\u00e0 di esito X se decido A, e qual \u00e8 la probabilit\u00e0 dello stesso esito se invece decido B?\u201d)</p><p>Se lavori per un\u2019organizzazione molto grande o che lavora su dati altamente riservati, come la CIA, dovresti pensare di implementare l\u2019intero processo del superforecasting internamente (anche se commissionare una o pi\u00f9 delle organizzazioni citate pi\u00f9 su potrebbe essere un buon modo per testare il modello a basso costo prima di assumersi un grosso impegno.)</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fno5tq2ta80tj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefo5tq2ta80tj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Eccetto&nbsp;nella misura in cui riescono ad usare l\u2019analisi predittiva per quesiti particolari per cui hanno dati particolarmente ricchi, il che non \u00e8 l\u2019argomento di questo articolo.&nbsp;Mi soffermo qui su metodi di previsione \u201cuniversali\u201d, cio\u00e8 metodi che possono produrre previsioni per qualsiasi quesito che sia ragionevolmente ben posto e non solo per i quesiti che sono trattabili con analisi predittive.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9iy5ulj1h5j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9iy5ulj1h5j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;In entrambi i settori di esempio c\u2019\u00e8 qualche eccezione, ad esempio il mercato di previsione della comunit\u00e0 di intelligence negli Stati Uniti&nbsp;o <a href=\"https://www.openphilanthropy.org/blog/efforts-improve-accuracy-our-judgments-and-forecasts\">Open Philanthropy</a>&nbsp;nella filantropia.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfzjzl66g4u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffzjzl66g4u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Ad esempio, per ottenere migliore accuratezza potresti voler formare \u201csquadre\u201d di superforecasters&nbsp;secondo certi criteri. Per dettagli vedi&nbsp;<a href=\"https://www.amazon.com/Superforecasting-Science-Prediction-Philip-Tetlock-ebook/dp/B00RKO6MS8/\">Superforecasting</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnusy0h50dvy8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefusy0h50dvy8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Quando dico che la probabilit\u00e0 \u00e8 \u201cdavvero\u201d vicina al&nbsp;10% intendo solo dire che le previsioni al 10% di fiducia prodotte da questo metodo sono ben calibrate. Non voglio sottintendere un\u2019interpretazione della probabilit\u00e0 diversa dal <a href=\"https://en.wikipedia.org/wiki/Bayesian_probability\"><u>Bayesianesimo</u></a>&nbsp;soggettivo standard.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnb5wj94sq14\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefb5wj94sq14\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Alcuni superforecasters&nbsp;avevano una formazione di qualche tipo in geopolitica, ma la maggior parte non ne aveva alcuna.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpjh895o6gvp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpjh895o6gvp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Per vari confronti di accuratezza, vedi <a href=\"https://www.amazon.com/Superforecasting-Science-Prediction-Philip-Tetlock-ebook/dp/B00RKO6MS8/\">Superforecasting</a>, <a href=\"https://journals.sagepub.com/doi/abs/10.1177/0956797614524255\">Mellers et al. (2014)</a>, e <a href=\"https://goodjudgment.com/wp-content/uploads/2020/11/Goldstein-et-al-GJP-vs-ICPM.pdf\">Goldstein et al. (2015)</a>. Per riassunti di massima di alcuni di questi risultati, vedi&nbsp;<a href=\"https://goodjudgment.com/resources/the-superforecasters-track-record/\"><u>questa pagina</u></a>&nbsp;da&nbsp;Good Judgment Inc. e anche&nbsp;<a href=\"https://aiimpacts.org/evidence-on-good-forecasting-practices-from-the-good-judgment-project-an-accompanying-blog-post/\">AI Impacts (2019)</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnam6js9ewzmg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefam6js9ewzmg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Uno dei limiti delle prove che abbiamo a disposizione al momento \u00e8 che non sappiamo quanto sia efficace il superforecasting (n\u00e9 qualsiasi tecnica di previsione basata sul giudizio personale) per quesiti di previsione remoti&nbsp;(vedi&nbsp;<a href=\"https://www.openphilanthropy.org/blog/how-feasible-long-range-forecasting\"><u>qui</u></a>). Io ho&nbsp;il presentimento che il superforecasting sia in grado di produrre previsioni su quesiti remoti ben posti che sono ben calibrate, anche se non hanno alta \u201crisoluzione\u201d&nbsp;(spiegazione&nbsp;<a href=\"https://productlogic.org/2017/02/08/the-superforecasting-playbook-for-product-development/\"><u>qui</u></a>), ma \u00e8 solo un\u2019opinione.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8x4lr3zvnqp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8x4lr3zvnqp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Ad&nbsp;esempio l\u2019economista&nbsp;Tyler Cowen ha&nbsp;<a href=\"https://conversationswithtyler.com/episodes/john-o-brennan/\"><u>chiesto</u></a>&nbsp;di recente a John Brennan (Direttore della CIA fino al&nbsp;2017): \u201cHai familiarit\u00e0 con il progetto di <i>superforecasters</i>&nbsp;di&nbsp;Philip Tetlock?\u201d Brennan non ne aveva familiarit\u00e0.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyondvzr62tp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyondvzr62tp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Tecnicamente l\u2019algoritmo di aggregazione di Hypermind&nbsp;include anche previsioni di altri previsori, ma d\u00e0 molta pi\u00f9 importanza alle previsioni dei \u201cprevisori campioni\u201d.</p></div></li></ol>", "user": {"username": "EA Italy"}}, {"_id": "6qhL2o8QEmKN4gjZj", "title": "[Opzionale] Perch\u00e9 probabilmente non sono una lungoterminista", "postedAt": "2023-01-17T18:12:42.863Z", "htmlBody": "<p><i>This is an Italian translation of </i><a href=\"https://forum.effectivealtruism.org/posts/Jxfq6xCP9ZoTBFewA/why-i-am-probably-not-a-longtermist\"><i><strong>Why I am probably not a longtermist</strong></i></a></p><p>Riassunto: mi interessa molto di pi\u00f9 fare in modo che il futuro sia positivo anzich\u00e9 di lunga durata o vasto, poich\u00e9 non credo che il mondo sia molto buono ora n\u00e9 sono sicura che lo sar\u00e0 in futuro. Non sono convinta che esistano scenari in cui siamo intrappolati in un eventuale mondo problematico almeno quanto quello di oggi che possiamo evitare o influenzare con le nostre azioni di oggi. E se non ce ne sono, penso che sia meglio concentrarci su strategie \u201ctradizionali e non lungoterministe\u201d per migliorare il mondo.</p><p>Ho pensato che il motivo per cui non sono molto convinta del lungoterminismo potrebbe interessare ad altri AE (altruisti efficaci), dato che il lungoterminismo \u00e8 di alto rilievo per molte persone nella comunit\u00e0.&nbsp;</p><p>Questo post tratta della filosofia chiamata \u201clungoterminismo\u201d. Non prende posizione sulle cause a cui dare priorit\u00e0. \u00c8 completamente possibile che cause solitamente collegate al lungoterminismo siano importanti per motivi non lungoterministi.</p><p>Ho impostato questo post attorno a quelli che secondo me sono i nodi della questione e segnalato prove o ragionamenti che potrebbero convincermi che ho torto, ma vorrei anche che mi venissero segnalate idee che potrebbero essermi sfuggite! Generalmente non ho indagato fino in fondo quelli che per me sono i nodi della questione. Per questo sono solo \u201cprobabilmente\u201d non lungoterminista.&nbsp;</p><h2><strong>La qualit\u00e0 del futuro remoto</strong></h2><h3><strong>1. Trovo molti aspetti dell'utilitarismo poco convincenti.</strong></h3><p>Per essere lungoterminista non devi per forza essere utilitarista. Penso per\u00f2 che a seconda delle tue differenze con l\u2019utilitarismo totale, non abbraccerai probabilmente \u201cfino in fondo\u201d il lungoterminismo.&nbsp;</p><p>A me importa molto lasciare alle generazioni future un mondo in buone condizioni. Mi importa molto anche il benessere di ciascuno, disgiunto dai limiti temporali. Ci\u00f2 di cui mi curo meno di un utilitarista totale, per\u00f2, \u00e8 far s\u00ec che esistano persone felici che altrimenti non sarebbero esistite. Ci\u00f2 vuol dire che non mi preoccuperei troppo se l\u2019umanit\u00e0 non riuscisse ad aumentare di grande numero e diffondersi per le stelle. Nonostante far s\u00ec che esistano persone felici abbia valore, io lo ritengo molto meno meritevole che evitare che le persone vivano in miseria. Perci\u00f2 non mi preoccupa molto la perdita di potenziale causata da rischi di estinzione (anche se mi importano molto le loro conseguenze nel breve termine), anche se ci\u00f2 dipende da quanto lungo e positivo mi aspetto sia il futuro (vedi pi\u00f9 avanti).&nbsp;</p><p><i>Cosa mi farebbe cambiare idea:&nbsp;</i></p><p>Non mi interessa solo perseguire i miei valori, ma vorrei anche fare s\u00ec che i valori altrui si realizzino. Per esempio, se scoprissi che a svariate persone nel mondo importasse molto aumentare la popolazione umana nel lungo termine, gli darei molta pi\u00f9 priorit\u00e0. Mi interessa tuttavia un po\u2019 meno la somma delle preferenze individuali e pi\u00f9 le preferenze di una variet\u00e0 di gruppi. Questo per dare pi\u00f9 peso a idee pi\u00f9 rare e per evitare di premiare un gruppo che si riproduce pi\u00f9 degli altri o che diffonde i propri ideali in maniera imperialista.</p><p>Voglio anche dare maggior peso ai valori di coloro che stanno soffrendo di pi\u00f9. Se loro pensassero che il futuro remoto meritasse di avere priorit\u00e0 sul loro dolore nel presente, li prenderei in seria considerazione.</p><p>In alternativa, potrei essere persuasa sia del realismo morale che dell\u2019utilitarismo dentro a questa ottica. Non ho ancora trovato una spiegazione che usi parole chiare per supportare il senso del realismo morale, ma questo potrebbe essere un buon punto d\u2019inizio.&nbsp;</p><p>Se il mondo migliorasse all\u2019improvviso e tutti godessero del mio stesso tenore di vita attuale, sarei felice di concentrarmi sul rendere il futuro vasto e duraturo anzich\u00e9 sul migliorare vite.&nbsp;</p><h3><strong>2. Non credo che l\u2019umanit\u00e0 sia intrinsecamente meravigliosa.</strong></h3><p>Un tema ricorrente in molte visioni del futuro lungoterministe sembra l\u2019idea che l\u2019umanit\u00e0 sia meravigliosa e dovrebbe perci\u00f2 sussistere a lungo. Non credo di essere misantropa e ritengo che il mio parere sia nella media per un Europeo. Ci sono molti aspetti positivi dell'umanit\u00e0 da cui traggo piacere a veder fiorire.&nbsp;</p><p>Ma l\u2019entusiasmo palese che la maggior parte dei lungoterministi manifesta per l\u2019umanit\u00e0 mi confonde. Persino ora l\u2019umanit\u00e0 sta commettendo genocidi, lasciando morire milioni di persone di fame, schiavizzando e torturando persone e miliardi di animali allevati. Trovo difficile conciliare tutto ci\u00f2 con l\u2019idea che l\u2019umanit\u00e0 sia \u201cmeravigliosa\u201d.&nbsp;</p><p>Un\u2019obiezione comune sembra essere che s\u00ec, ci sono problemi, ma semplicemente non abbiamo ancora avuto tempo di risolverli. Che l\u2019uomo \u00e8 pigro, non malvagio. Questo non mi convince. Non mi interessa solo che le persone vivano vite piacevoli, mi interessa anche che siano brave persone. La pigrizia non \u00e8 una scusa.&nbsp;</p><p>Possiamo fare di pi\u00f9 in questo preciso istante. Perlopi\u00f9 non lo facciamo. Poche persone che sentono parlare delle organizzazioni benefiche raccomandate da GiveWell decidono di donare una porzione significativa del loro salario. Ci sono persone che prendono voli intercontinentali per turismo pur essendo a conoscenza del cambiamento climatico. Molti mangiano carne pur avendo sentito delle condizioni degli allevamenti intensivi. Questi esempi sono piuttosto diffusi nel mondo, ma non credo sia critico.</p><p>Puoi contestare questi esempi presi uno alla volta. Talvolta le persone davvero non hanno le informazioni necessarie. Possono avere differenze di opinione empiriche o diverse prospettive morali (es. non ritenere che gli animali siano senzienti). A volte considerano le priorit\u00e0 e decidono di dedicarsi ad altri modi di fare del bene. A me tutte queste ragioni stanno bene.&nbsp;</p><p>Ma alla fine, mi sembra che molti abbiano i mezzi per fare la loro parte, eppure rimangono enormi problemi. Sar\u00e0 sicuramente un bene se nel futuro avremo innescato sistemi migliori per ridurre la miseria e se avremo i giusti incentivi per far s\u00ec che le persone si comportino meglio. Ma non mi entusiasma pensare ad un\u2019umanit\u00e0 che ne ha bisogno per comportarsi rettamente.&nbsp;</p><p>Questo mi rende anche riluttante a dare molto peso all\u2019idea che dovremmo aiutare le persone a fare del bene, indipendentemente da quando accada. Questo \u00e8 vero solo se le persone in futuro saranno moralmente meritevoli almeno quanto le persone oggi.&nbsp;</p><p>O per metterla in modo diverso: se gli uomini fossero veramente cos\u00ec meravigliosi, non avremmo bisogno di preoccuparci di tutti questi rischi nel futuro. Si risolverebbero da soli.&nbsp;</p><p><i>Cosa mi farebbe cambiare idea:</i></p><p>Sarei assolutamente elettrizzata di scoprire di sbagliarmi sul livello di moralit\u00e0 delle persone che mi circondano! Ammetto che fatico a trovare prove plausibili, perch\u00e9 sembrerebbero diametralmente opposte al mondo che osservo. Forse \u00e8 proprio la mancanza di informazione che impedisce alle persone di fare del bene, come sembra pensare&nbsp;<a href=\"https://ourworldindata.org/problems-and-progress\">Max Roser</a> di Our World in Data. Potrei persuadermi se le campagne di informazione riscuotessero effetti importanti.</p><p>Non ho familiarit\u00e0 con quanto seriamente le persone in altri luoghi e tempi considerino i loro obblighi morali. \u00c8 possibile che la mancanza di azione che osservo sia un\u2019eccezione locale.</p><p>Anche se questo non dovrebbe avere un impatto sulla mia opinione, mi troverei anche pi\u00f9 a mio agio con il lungoterminismo se vi riscontrassi una maggiore attenzione all\u2019ingegneria sociale o medica per produrre persone (moralmente) migliori all\u2019interno della comunit\u00e0 lungoterminista.&nbsp;</p><h3><strong>3. Non sono sicura che il futuro sar\u00e0 meglio del presente.</strong></h3><p>Il mondo \u00e8 migliorato molto da molti punti di vista. La povert\u00e0 estrema \u00e8 diminuita, l\u2019aspettativa di vita media aumentata e meno persone vengono messe in schiavit\u00f9. Sono ottimista che queste tendenze positive continueranno.&nbsp;</p><p>Ci\u00f2 di cui sono pi\u00f9 scettica \u00e8 della misura in cui queste tendenze mostrino il quadro completo della situazione. Anche se i pi\u00f9 sarebbero d\u2019accordo che \u00e8 un bene che meno persone muoiano di fame o muoiano giovani, ci sono molte tendenze elogiate dai lungoterministi che altri potrebbero giudicare diversamente, come la diminuzione della religiosit\u00e0. Oppure altri potrebbero porre l\u2019enfasi su aspetti diversi. Qualcuno a cui importino molto gli animali negli allevamenti intensivi potrebbe non credere che il mondo sia migliorato.&nbsp;</p><p>Mi preoccupa che l\u2019idea che il mondo stia migliorando dipende da un sistema di credenze piuttosto raro. Usando la lente della teoria dei principi morali di Haidt, si direbbe che la maggior parte dei miglioramenti sia nel principio di rimedio/danno, ma il mondo potrebbe non essere migliorato secondo altri principi come lealt\u00e0/tradimento o santit\u00e0/degrado.&nbsp;</p><p>Inoltre, mi aspetto che molti cambiamenti positivi si interrompano prima di diventare negativi, ma temo che per alcuni non sar\u00e0 cos\u00ec. Per esempio, penso che l'aumento dell'edonismo e dell'individualismo siano stati entrambi una forza positiva, ma se esagerati riterrei che peggiorino il mondo, e mi sembra che siamo gi\u00e0 a quel punto o quasi.</p><p>In generale, mi preoccupa che tendiamo a oltrepassare l\u2019iniziale obiettivo positivo, continuando ad ottimizzare secondo criteri troppo rigidi. L\u2019esempio pi\u00f9 chiaro \u00e8 l\u2019esaltazione del profitto. Ne ho parlato di pi\u00f9&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/BshFD6JvyqXypc5Aw/denise_melchin-s-shortform?commentId=Wxsv2rdk8CXAhYjse\"><u>qui</u></a>.</p><p>Se il mondo non \u00e8 migliore rispetto al passato, non si pu\u00f2 estrapolare da ci\u00f2 un\u2019aspettativa di un futuro ancora migliore. Questo \u00e8 un altro motivo per cui voglio concentrarmi sul rendere il futuro positivo, non duraturo o vasto.&nbsp;</p><p>Un\u2019altra nota \u00e8 che, pur non essendo un ragionamento che mi dissuade dal lungoterminismo, provo avversione per il fatto che alcuni lungoterministi sembrano ritenere positivi dei futuri che io trovo privi di valore (es. \u201cl\u2019onda cosmica di&nbsp;<i>hedonium</i>\u201d - dall\u2019inglese&nbsp;<i>hedonium shockwave.</i>&nbsp;<i>Hedonium</i> indica una particella o un materiale ottimizzato per provare tanto pi\u00f9 piacere quanto possibile, mentre l\u2019immagine di un\u2019onda cosmica si riferisce ad un futuro in cui l\u2019<i>hedonium</i> venga disseminato rapidamente come un\u2019onda attraverso l\u2019universo, per massimizzare la quantit\u00e0 di piacere provata nell\u2019universo per il maggior tempo possibile). Culturalmente, molti lungoterministi sembrano preferire pi\u00f9 edonismo, individualismo e tecno-utopismo di quanto vorrei.</p><p><i>Cosa mi farebbe cambiare idea:</i></p><p>Sono ben consapevole che molte persone sono pessimiste verso il futuro per via di credenze errate su fatti basilari dell'evoluzione del mondo. Mi interessa comunque considerare ancora come prospettive diverse portino ad opinioni diverse sulla questione di se il mondo stia migliorando.</p><h2><strong>La durata del futuro remoto</strong></h2><p>Non mi persuadono argomentazioni che il futuro potrebbe essere molto lungo. Non capisco come ci\u00f2 sia possibile senza almeno un totalitarismo leggero, che a sua volta comporta rischi per il valore del futuro.&nbsp;</p><p>O da una prospettiva diversa, vi sono persone che studiano rischi esistenziali che hanno passato alcuni anni a cercare di convincermi che siano molto elevati. Passare da questa giustificazione per lavorare sui rischi esistenziali al lungoterminismo, che presuppone il raggiungimento di una sicurezza esistenziale, mi d\u00e0 un senso di colpo di frusta.&nbsp;</p><p>Si veda anche questo&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/dgNgF42QbzytPaeRp/agb-s-shortform\"><u>post breve</u></a> sulla questione. L\u00ec si fa menzione della regola di Lindy, che parte dalla constatazione che esistono sistemi auto-propaganti da miliardi di anni per concludere che dunque sopravviveranno ancora per un simile lasso di tempo. Ma non capisco perch\u00e9 dovremmo partire dai sistemi auto-propaganti; a me interessa solo applicare la regola di Lindy ad una civilt\u00e0 umana dotata di valore morale, che in confronto \u00e8 esistita per un periodo molto pi\u00f9 breve.&nbsp;</p><p>Non vado neanche pazza per l\u2019idea di basare decisioni su calcoli del valore atteso, secondo i quali la stima di piccole probabilit\u00e0 \u00e8 incerta e il valore atteso \u00e8 la giustificazione principale (anzich\u00e9 un approccio basato pi\u00f9 sul pensiero \u201ca grappolo\u201d). Non mi oppongo a queste decisioni di principio, ma i miei resoconti personali in questo ambito sono molto scarsi: il valore atteso che ho ricavato da calcoli approssimativi (<i>on the back of the envelope</i>, \u201csul retro della busta\u201d) non si \u00e8 generalmente concretizzato. Nutro anche le solite preoccupazioni sull\u2019onda dell\u2019esperimento mentale della rapina di Pascal (<i>Pascal\u2019s mugging</i>).&nbsp;</p><p><i>Cosa mi farebbe cambiare idea:</i></p><p>Gradirei ricevere ragionamenti fondati su come l\u2019umanit\u00e0 potrebbe raggiungere la sicurezza esistenziale.&nbsp;</p><h2><strong>La possibilit\u00e0 di influenzare il futuro remoto</strong></h2><p>Non sono convinta che le persone possano affidabilmente avere un\u2019influenza positiva sul futuro che non sia svanita nel giro di 100 anni, magari entro un fattore di 3. Ma c\u2019\u00e8 un\u2019eccezione importante: la prevenzione o influenza di uno scenario di \u201cvincolo\u201d in questo lasso di tempo. Per \u201cvincolo\u201d voglio dire qualsiasi scenario dal quale l\u2019umanit\u00e0 non potrebbe mai riprendersi. I rischi di estinzione sono un esempio ovvio, un altro \u00e8 il collasso permanente della civilt\u00e0.&nbsp;</p><p>Sono consapevole che la definizione canonica di rischio esistenziale di Bostrom include entrambi questi scenari di vincolo, ma include anche scenari che io considero completamente irrilevanti (es. non riuscire a raggiungere un futuro transumanista), motivo per cui non uso questo termine in questa sezione.</p><p>E dato che non penso che possiamo avere un\u2019influenza sicura sul mondo per pi\u00f9 di qualche decade, non mi convincono motivazioni come il \u201cmiglioramento dei processi decisionali delle istituzioni\u201d, se non nella misura in cui possono influenzare o prevenire uno scenario di vincolo in quel lasso di tempo\u2026</p><p>Inoltre, mi interessano solo scenari di vincolo che porterebbero a un mondo con gli stessi aspetti negativi di adesso, o peggiore, o forse non molto migliore. Non mi interessa prevenire un futuro in cui l'umanit\u00e0 non fa altro che guardare Netflix tutto il giorno - sarebbe un peccato, ma almeno migliore di un mondo in cui ci sono regolarmente persone che muoiono di fame.</p><p>Al momento non so abbastanza sulla probabilit\u00e0 di scenari di vincolo negativi per poter giudicare se, secondo la mia visione del mondo, valga la pena concentrarsi su di essi. Se questo dovesse rivelarsi essere il caso in seguito a ulteriori indagini, potrei trovarmi a descrivere i miei ideali come lungoterministi se insistete, ma prevedo che sentirei comunque una distanza culturale da altri lungoterministi.&nbsp;</p><p>Se non ci sono opzioni per evitare o influenzare scenari di vincolo negativi nei prossimi decenni, mi aspetto che migliorare il mondo secondo approcci \u201ctradizionali e breveterministi\u201d sia la cosa migliore.&nbsp;</p><p>Le mie opinioni qui sono simili a quelle di Alexander Berger, opinioni che ha esposto in&nbsp;<a href=\"https://80000hours.org/podcast/episodes/alexander-berger-improving-global-health-wellbeing-clear-direct-ways/#transcript\"><u>questo podcast di 80,000 Hours</u></a>.</p><p><i>Cosa mi farebbe cambiare idea:</i></p><p>Se sono mai esistiti impatti intenzionali durati pi\u00f9 di 100 anni nel futuro, sarei grata di venirne a conoscenza. Ho familiarit\u00e0 con i&nbsp;<a href=\"http://reflectivedisequilibrium.blogspot.com/2018/10/flow-through-effects-of-innovation.html\"><u>blogposts di Carl</u></a> sull\u2019argomento.</p><p>Mi aspetto di dedicare presto un po\u2019 di tempo ad investigare questo punto cruciale: se esistono scenari di vincolo negativi all\u2019orizzonte che potremmo evitare o plasmare, questo probabilmente cambierebbe le mie credenze sul lungoterminismo.&nbsp;</p><p>Dato che questo \u00e8 un nodo importante, alcuni potrebbero pensare che io abbia tratto conclusioni premature sulla mia visione del mondo. Ma le mie opinioni sembrano abbastanza diverse dalla maggior parte di quelle lungoterministe che ho sentito, per cui spero sia valsa la pena di esporle comunque.</p><p>Se qualcuno dovesse avere risorse da indicarmi che potrebbero farmi cambiare posizione, sarei grata di riceverle.&nbsp;</p><p><i>Grazie ad AGB e Linch Zhang per aver dato commenti su una bozza di questo post.</i></p>", "user": {"username": "EA Italy"}}, {"_id": "GE3yZGJhQ3iDMGQbs", "title": "[Opzionale] Perch\u00e9 il lungoterminismo \u00e8 difficile per me e come mantengo la motivazione", "postedAt": "2023-01-17T18:08:20.676Z", "htmlBody": "<p><i>This is an Italian translation of </i><a href=\"https://forum.effectivealtruism.org/posts/3k4H3cyiHooTyLY6p/why-i-find-longtermism-hard-and-what-keeps-me-motivated\"><i><strong>Why I find longtermism hard, and what keeps me motivated</strong></i></a></p><p>Per me lavorare su cause lungoterministe \u00e8 emotivamente difficile: ci sono cos\u00ec tanti problemi terribili oggi nel mondo. Come possiamo voltare le spalle alla sofferenza tutt\u2019intorno a noi per dare priorit\u00e0 a qualcosa di tanto astratto come influenzare positivamente il futuro a lungo termine?&nbsp;</p><p>Molte persone che si prefiggono di praticare gli ideali lungoterministi sembrano avere difficolt\u00e0 simili, incluse molte delle persone con cui ho lavorato nel corso degli anni. Io stessa non faccio eccezione: \u00e8 difficile liberarsi dal richiamo della sofferenza di oggi. Perci\u00f2 vorrei condividere qualche riflessione su come gestisco questa difficolt\u00e0 e come mantengo la motivazione per valutare interventi cos\u00ec speculativi, pur trovandolo per molti versi arduo.</p><p>Questa difficolt\u00e0 rientra in una problematica pi\u00f9 ampia dell\u2019altruismo efficace: come trovare la motivazione per azioni importanti anche quando non sono le pi\u00f9 coinvolgenti emotivamente. Una consapevolezza nitida delle nostre emozioni \u00e8 utile per poter distinguere i sentimenti e le convinzioni che veramente sottoscriviamo da quelli che, riflettendoci, non vorremmo veramente innalzare a guida.&nbsp;</p><h2><strong>Cosa ho trovato difficile</strong></h2><p>Innanzitutto, non sto suggerendo che tutti abbiano le mie stesse ragioni per trovare ostico il sottoscriversi alle cause lungoterministe. Vorrei anche precisare che non sto parlando a nome dell\u2019organizzazione 80\u2019000 Hours.&nbsp;</p><p>Quando penso alle azioni che potrei fare ma non sto intraprendendo, ho di solito in mente le persone in paesi poveri che soffrono per malattie prevenibili. Questo \u00e8 dovuto, in gran parte, al lavoro che stavo facendo quando ho sentito parlare dell\u2019altruismo efficace. Altri potrebbero preoccuparsi di non starsi impegnando attivamente per prevenire la barbarie di certe pratiche negli allevamenti intensivi. Non tratter\u00f2 tutti i modi in cui le persone potrebbero avere difficolt\u00e0 a mobilitarsi per il futuro remoto: ai fini di questo articolo, mi concentrer\u00f2 sulla mia esperienza personale.&nbsp;</p><h3><strong>Sento una forte spinta ad aiutare le persone adesso</strong></h3><p>La maggior parte della sofferenza odierna non dovrebbe aver modo di esistere. Ci sono persone che soffrono e muoiono per mancanza di cure e metodi di prevenzione di basso costo. Malattie che i paesi benestanti sono riusciti a debellare continuano ad affliggere milioni in tutto il mondo. C'\u00e8 robusta evidenza che strumenti economici come le zanzariere anti-malaria trattate con insetticidi sono efficaci. Eppure, se anche molti di noi dei paesi ricchi abbiamo risorse finanziarie notevoli, spendiamo una grande fetta del nostro reddito in beni e servizi non necessari. Di fronte a questa iniquit\u00e0 assurda e prevenibile, \u00e8 difficile credere che non dovrei fare in modo di alleviarla.&nbsp;</p><p>Allo stesso modo, spesso trovo difficile credere che non dovrei aiutare coloro che mi sono prossimi geograficamente, come i senzatetto nella mia cittadina o i prigionieri illegittimi nel mio paese. \u00c8 difficile accettare la presenza di una sofferenza visibile e prevenibile e non fare nulla per combatterla.&nbsp;</p><p>Per me, prioritizzare l'aiuto verso le persone future sopra a quello per le persone che vivono oggi \u00e8 ancora pi\u00f9 difficile che rinunciare ad aiutare le persone nel mio paese per supportare altre dalla parte opposta del mondo. Questo in parte perch\u00e9 sento che ci saranno altri in futuro che potranno aiutare le persone future se noi non agiamo oggi. Se invece non prendiamo iniziative per aiutare i poveri del mondo di adesso, non ci sar\u00e0 nessuno in futuro che potr\u00e0 farlo per noi. Le vite che non riusciremo a salvare quest\u2019anno saranno definitivamente perse e rimpiante.</p><p>Un\u2019altra considerazione \u00e8 che sembra che la ricchezza stia aumentando rapidamente con il passare del tempo. Perci\u00f2 sarebbe legittimo pensare che le persone future saranno ben pi\u00f9 ricche delle persone di oggi, e questo sembrerebbe portare alla conclusione che le persone nel futuro non abbiano bisogno di tanto aiuto quanto le persone nel presente. Non esiste un argomento analogo per le persone lontane geograficamente.&nbsp;</p><h3><strong>Le argomentazioni a sostegno del lungoterminismo non mi prendono molto emotivamente</strong></h3><p>Le ragioni per migliorare le vite delle persone di oggi sono emotivamente coinvolgenti. In parte, ci sono doveri indiscutibili che vigono su di noi nei confronti delle persone del presente, la cui robustezza pu\u00f2 solo essere inficiata da imperativi maggiori. I motivi per prioritizzare il futuro remoto sembrano molto pi\u00f9 speculativi e dipendono da un equilibrio delicato di ragionamenti complessi.&nbsp;</p><p>Di seguito spiego quelle che per me sono le argomentazioni a favore del lungoterminismo e come mai, nonostante mi convincano a livello intellettuale, non alleviano la mia sensazione che dovremmo piuttosto concentrarci sulle sofferenze attuali. Vorrei precisare che questa non vuole essere una perorazione rigorosa sul perch\u00e9 dovremmo concentrarci sulle cause lungoterministe (di questo 80,000 Hours&nbsp;<a href=\"https://80000hours.org/articles/future-generations/\"><u>ha scritto altrove</u></a>).&nbsp;</p><p>Il futuro degli esseri senzienti potrebbe essere inverosimilmente vasto. Ci\u00f2 vuol dire che se abbiamo anche solo una possibilit\u00e0 minuta di influenzarlo in maniera duratura e positiva, vale la pena di coglierla.&nbsp;</p><p>Un modo in cui potremmo influenzare il futuro remoto \u00e8 prevenendo la completa estinzione della vita terrestre. Dato che le persone odierne potrebbero eliminare&nbsp;<i>ogni persona a venire</i>, non \u00e8 garantito che coloro che verranno dopo di noi potranno migliorare il futuro, se noi non glielo permettiamo. Ci\u00f2 rende irrilevante anche il fatto che le persone nel futuro potrebbero essere pi\u00f9 ricche di noi.&nbsp;</p><p>Il valore del futuro potrebbe anche venire decurtato irrimediabilmente dall\u2019instaurazione permanente di un regime totalitario anzich\u00e9 da un evento di estinzione. Ci\u00f2 vorrebbe dire che sarebbe ben possibile che le persone future esistano, ma vivano in condizioni abiette senza il nostro intervento.&nbsp;</p><p>Questi esiti terribili mi sembrano effettivamente possibili. Sono il tipo di rischio che dovremmo stare studiando per capire se sia possibile ridurlo. Ci sono infatti molti motivi per dubitare che la societ\u00e0 riesca a gestire questi rischi in modo efficace: le imprese hanno incentivi economici miopici, i politici vogliono venire rieletti nel giro di un paio di anni, e i singoli individui non sono sempre in grado di pianificare (persino il proprio futuro!).</p><p>Secondo me queste ragioni a favore del lungoterminismo sono sensate e mi convincono. Io sono convinta di dovere dare priorit\u00e0 al futuro remoto.</p><p>Eppure, questi ragionamenti mi suonano sempre come speculazioni. E se anche si avverassero, non vi sarebbe alcuna garanzia che io abbia davvero un impatto, per esempio, migliorando la rappresentanza delle generazioni future nella nostra legislatura o aumentando la mole di ricerca di buona qualit\u00e0 sulle priorit\u00e0 globali\u2026 premesso che io ci riesca. Allora non posso che fare una scommessa: che sar\u00f2 capace di avere un impatto positivo, rimanendo consapevole che potrei fallire. Perci\u00f2 scegliere di fare queste cose, anzich\u00e9, per esempio, finanziare la distribuzione di zanzariere, mi fa raffigurare con turbamento a giocare d\u2019azzardo con le vite altrui.&nbsp;</p><h2><strong>Come gestisco questa difficolt\u00e0</strong></h2><p>Per via di queste problematiche, a volte mi \u00e8 difficile trovare la motivazione di fare ci\u00f2 che ritengo dovrei. Una cosa che mi conforta \u00e8 che dedicare energie al futuro remoto si pone difficile proprio come mi aspetterei per qualsivoglia questione nell\u2019altruismo efficace: quanto pi\u00f9 saliente un problema - e quanto pi\u00f9 rilevante sia l\u2019imperativo che ci rimanda a esso - tanto pi\u00f9 dovremmo aspettarci che molti altri vi stiano gi\u00e0 lavorando. E dunque non dovrebbe sorprendermi che applicarsi ai problemi maggiori mi sembri, intuitivamente, meno urgente rispetto ad altri. Se lo fosse, sarebbe meno trascurato.&nbsp;</p><p>Il fattore che fa la maggiore differenza per me \u00e8 fare parte di una squadra che rispetto profondamente e a cui tengo. Il mio desiderio di rendere felici le persone attorno a me e di non deludere i miei colleghi rende facile lavorare sodo. Non \u00e8 necessario che condividano i miei valori: se stessi praticando l\u2019<i>earning to give</i> (\u201cguadagnando per dare\u201d, l\u2019idea di fare un lavoro ben retribuito per donare una significativa frazione del salario in beneficenza) ed avessi bisogno di essere competente nel mio lavoro per mantenere (e aumentare!) il mio stipendio, mi sarebbe molto utile avere colleghi a cui interessi fare un lavoro di alta qualit\u00e0 ed il successo dell\u2019azienda. Mi troverei a lavorare sodo e fare la mia parte per non deluderli.&nbsp;</p><p>Un\u2019altra cosa che ha un effetto notevole sulla mia motivazione \u00e8 considerare e discutere su quali cause e interventi siano tra i pi\u00f9 importanti. Per esempio, parlare delle mie preoccupazioni quando penso istintivamente di non stare lavorando sul problema giusto. Ne parlo man mano che si presentano e ne discuto con persone che hanno valori simili ai miei; questo mi aiuta a capire quali siano i pensieri che, seppure siano intuitivi, non credo a fondo, e quali invece sostengo e posso difendere.</p><p>Cerco anche di continuare a leggere e confrontarmi con argomentazioni a favore del concentrarsi su altri problemi. \u00c8 particolarmente importante mettersi sempre in discussione ed esplorare idee contrastanti, perch\u00e9 non puoi fidarti del tuo intuito per decidere se stai prendendo la rotta sbagliata (pensa gi\u00e0 che sei fuori strada!).&nbsp;</p><p>Detto questo, sarebbe disorientante e demotivante mettere continuamente in discussione il tuo lavoro o i tuoi piani! Un momento importante per farlo potrebbe essere quando stai per iniziare un nuovo progetto o stai per imboccare un cambio di direzione importante (anche se a me piace molto seguire idee nuove interessanti man mano che vengono condivise, per esempio sull\u2019EA Forum).</p><p>Anche prendere impegni concreti mi ha aiutato a fare ci\u00f2 che \u00e8 pi\u00f9 efficace. Sono un membro di&nbsp;<a href=\"https://www.givingwhatwecan.org/\">Giving What We Can</a>, il ch\u00e9 vuol dire che mi sono impegnata a donare 10% del mio salario alle organizzazioni che credo possano migliorare il mondo nella maniera pi\u00f9 efficace. In realt\u00e0 di solito dono un po\u2019 di pi\u00f9 del mio impegno - un po\u2019 ad organizzazioni che promuovono il benessere animale come offset per il mio consumo di carne e un po\u2019 ad un\u2019organizzazione di sviluppo globale (generalmente l\u2019Against Malaria Foundation) perch\u00e9 detesto il pensiero di non stare facendo niente per ridurre la povert\u00e0 globale. Ma do sempre il mio 10% alle organizzazioni che penso che abbiano l'opportunit\u00e0 a conti fatti di realizzare maggior bene, perch\u00e9 l\u2019ho promesso.&nbsp;</p><p>Una tattica della quale sono meno convinta \u00e8 quella di far sembrare pi\u00f9 concreti i danni o l\u2019assenza di benefici futuri. Ad esempio, potrei immaginare un\u2019umanit\u00e0 estinta dopo una pandemia artificiale dovuta ad una guerra biologica spericolata, e che quindi l\u2019universo raggiungibile rimarr\u00e0 privo di vita per eoni. Immaginare esempi come questo d\u00e0 al mio intuito qualcosa a cui allacciarsi e mi ricorda che, per coloro che li subiranno, i danni futuri non saranno meno reali che i danni correnti per noi.&nbsp;</p><p>Una perplessit\u00e0 che ho su questo stratagemma \u00e8 che ci sono cos\u00ec tanti possibili esiti terribili per il mondo che potrebbe essere fuorviante attaccarsi ad uno in particolare. Potrebbe avere un\u2019influenza imprevista sulle tue azioni. Un modo per evitarlo potrebbe essere figurarsi un esito positivo concreto: immagina un mondo di esseri che prosperano nell'universo. Personalmente trovo questa prospettiva meno motivante, in parte perch\u00e9 penso che per via del modo in cui siamo disposti attualmente, abbiamo una maggiore capacit\u00e0 di sperimentare dolore che piacere.</p><p>Oltre a tutte le tattiche di cui ho parlato, penso che sia davvero d\u2019aiuto stare attorno ad altri che la pensano in modo simile a te, con cui puoi scambiare suggerimenti pratici e confrontarti provando il sollievo di non essere l\u2019unico ad avere difficolt\u00e0. Per me fare parte della&nbsp;<a href=\"https://80000hours.org/community/\"><u>comunit\u00e0</u></a> dell\u2019altruismo efficace aiuta molto sotto questi aspetti, che sia online (es. attraverso l\u2019EA Forum) o di persona (ho generalmente avuto la fortuna di vivere un posto con un vivace gruppo EA).</p><p>Quando sto veramente facendo fatica a fare la cosa giusta, ripenso che per tutta l\u2019incertezza sul lungoterminismo c\u2019\u00e8 una cosa di cui sono sicura: mi importa delle persone in futuro, proprio come mi importa delle persone di ora. Spedirei una zanzariera per proteggere un bambino, anche se il bambino non fosse ancora stato concepito, e formerei un pediatra ora per il bene di bambini per decenni a venire.&nbsp;</p><p>Ci sono cos\u00ec tante persone nel futuro senza alcuna capacit\u00e0 di difendere i propri interessi. La societ\u00e0 al momento le sta effettivamente ignorando completamente. Non posso vedere queste persone in fotografie e non so con certezza da cosa saranno afflitte o se potranno mai esistere. Ma&nbsp;<i>posso</i> usare la mia carriera per cercare di migliorare le cose per loro, in aspettativa. E penso che sia proprio questo che dovrei fare.</p>", "user": {"username": "EA Italy"}}, {"_id": "Yco2EXHNZBF5jsTgG", "title": "[Opzionale] Tutte le possibili conclusioni sul futuro dell\u2019umanit\u00e0 sono incredibili", "postedAt": "2023-01-17T14:59:49.750Z", "htmlBody": "<p><i>This is an Italian translation of </i><a href=\"https://forum.effectivealtruism.org/posts/TwQzyP3QgttmuTHym/all-possible-views-about-humanity-s-future-are-wild\"><i><strong>All Possible Views About Humanity's Future Are Wild</strong></i></a></p><p><i>La versione audio (in inglese) \u00e8&nbsp;</i><a href=\"https://www.podbean.com/ew/pb-vxz5c-108996f\"><i><u>qui</u></i></a><i>.</i></p><p><img style=\"width:100%\" src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Yco2EXHNZBF5jsTgG/o9g3iojxmtssi4oxjl72\" alt=\"\"></p><blockquote><p>Riassunto:</p><ul><li>In una serie di post a partire da questo, sosterr\u00f2 che nel XXI secolo la nostra civilt\u00e0 potrebbe sviluppare tecnologie che consentano una rapida espansione attraverso la nostra galassia attualmente vuota. E quindi, che <strong>questo secolo potrebbe determinare l'intero futuro della galassia per decine di miliardi di anni o pi\u00f9</strong>.</li><li>Questo punto di vista sembra <i>incredibile</i>. Davanti a qualsiasi asserzione che viviamo in tempi cos\u00ec speciali, la reazione dovrebbe essere di pensarci due volte. Mostro questo rappresentando la storia della nostra galassia con una linea del tempo. (A livello personale, questa incredulit\u00e0 \u00e8 probabilmente la ragione principale per cui, per molti anni, sono rimasto scettico sui ragionamenti di questo tipo. Asserzioni tali sull\u2019importanza dei tempi in cui viviamo sembrano cos\u00ec incredibili da suscitare sospetti).</li><li>In realt\u00e0, per\u00f2, non credo che sia possibile concludere questa questione che abbia \u201cdell'incredibile\". Descriver\u00f2 visioni alternative: una visione \u201cconservativa\u201d, secondo la quale le tecnologie che sto descrivendo sono possibili, ma che ci vorr\u00e0 molto pi\u00f9 tempo per svilupparle&nbsp;di quanto io pensi; una visione \u201cscettica\u201d, secondo la quale l'espansione su scala galattica non avverr\u00e0 mai. Entrambi questi punti di vista sembrano incredibili a loro modo.</li><li>In definitiva, come suggerito dal <a href=\"https://en.wikipedia.org/wiki/Fermi_paradox\"><u>paradosso di Fermi</u></a>, si direbbe che la nostra specie si ritrovi semplicemente in una posizione eccezionale.</li></ul><p>Prima di continuare, dovrei chiarire che non credo che diffondere l'umanit\u00e0 (o un qualche suo discendente digitale) attraverso la galassia sarebbe necessariamente un bene, soprattutto se impedisse definitivamente ad altre forme di vita di emergere. Penso che sia piuttosto difficile asserire con sicurezza se questo sarebbe un bene o un male. Vorrei concentrarmi sull'idea che la nostra situazione \u00e8 \"incredibile\". Non sto sostenendo che dovremmo essere eccitati o allegri alla prospettiva di espanderci attraverso la galassia, ma piuttosto che dovremmo comportarci con seriet\u00e0 nei confronti dell\u2019enorme posta in gioco.</p></blockquote><h2><strong>La mia ipotesi</strong></h2><p>Questo \u00e8 il primo di una serie di articoli che riguardano l'ipotesi che viviamo nel secolo pi\u00f9 importante per l'umanit\u00e0.</p><p>In questa serie, sosterr\u00f2 che ci sono buone probabilit\u00e0 che entro il 2100 avvenga un'esplosione di produttivit\u00e0 che potrebbe portare rapidamente a quella che potremmo chiamare una civilt\u00e0 \"tecnologicamente matura\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwh2lnz8ch1b\"><sup><a href=\"#fnwh2lnz8ch1b\">[1]</a></sup></span>. Ci\u00f2 significherebbe che:</p><ul><li>Saremmo in grado di iniziare a inviare navicelle spaziali attraverso la galassia e anche oltre.</li><li>Queste navicelle potrebbero estrarre materiali, costruire robot e computer e costruire insediamenti molto resistenti e duraturi su altri pianeti, sfruttando l'energia delle stelle e sostenendo un numero enorme di persone (e/o i nostri&nbsp;\u201c<a href=\"https://en.wikipedia.org/wiki/Mind_uploading\"><strong><u>discendenti digitali</u></strong></a>\u201d).<ul><li>Vedi&nbsp;<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0094576513001148\"><strong><u>L'eternit\u00e0 in sei ore</u></strong></a>&nbsp;per una discussione affascinante e breve, anche se tecnica, di cosa potrebbe essere necessario per realizzare questo scenario.</li><li>In un prossimo articolo sosterr\u00f2 anche la possibilit\u00e0 di un \"congelamento dei valori\": che chi gestisce il processo di espansione spaziale potrebbe essere in grado di determinare quale genere di persone siano a capo degli insediamenti e quali tipi di valori sociali abbiano, in un modo che resti stabile per molti miliardi di anni.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjdjc6a6j1lh\"><sup><a href=\"#fnjdjc6a6j1lh\">[2]</a></sup></span>&nbsp;Se ci\u00f2 dovesse avverarsi, potremmo pensare alla storia della nostra galassia<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref56vuu147n9y\"><sup><a href=\"#fn56vuu147n9y\">[3]</a></sup></span>&nbsp;come nella figura qui sotto. Ho segnato le tappe principali del percorso da \"assenza di vita\" a \"forma di vita intelligente che costruisce i propri computer e viaggia attraverso lo spazio\".</li></ul></li></ul><p><img style=\"width:100%\" src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Yco2EXHNZBF5jsTgG/mtrwhaud4hii0uei0wro\" alt=\"\"></p><p><i>Grazie a </i><a href=\"https://schubert.io/\"><i><strong>Ludwig Schubert</strong></i></a><i>&nbsp;per il diagramma. Molte date sono altamente approssimative, dipendono da un giudizio personale e/o sono semplicemente prese da Wikipedia (fonti </i><a href=\"https://docs.google.com/spreadsheets/d/1sTy7M48WkE6MQD_qA5UkPJ5WW5buWuyr3NDsm3PUE-I/edit?usp%3Dsharing\"><i><u>qui</u></i></a><i>), ma le differenze plausibili non cambierebbero il quadro generale. La stima di 1,4 miliardi di anni per completare l'espansione spaziale (\u201c</i>Galaxy-scale expansion complete<i>\u201d nel diagramma) \u00e8 ottenuta&nbsp;dividendo la distanza dei limiti estremi della Via Lattea per la velocit\u00e0 della pi\u00f9 rapida sonda spaziale esistente (i dettagli sono dettagli nel foglio di calcolo pi\u00f9 sopra). Secondo me questa \u00e8 probabilmente una grossa sovrastima del tempo che servirebbe per espanderci attraverso la galassia. Vedi la nota a pi\u00e8 di pagina su perch\u00e9 non ho usato un asse logaritmico.</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5294b8cq14\"><sup><a href=\"#fn5294b8cq14\">[4]</a></sup></span></p><p>??? \u00c8 pazzesco! Credo che ci sia una&nbsp;discreta possibilit\u00e0 che viviamo proprio all'inizio di quel breve lasso di tempo durante il quale la galassia passer\u00e0 da essere quasi priva di vita a perlopi\u00f9 abitata. Che nel numero impressionante di persone che esisteranno, noi siamo tra i primi. Che tra le centinaia di miliardi di stelle della nostra galassia, la nostra produrr\u00e0 gli esseri che la riempiranno.</p><p>So cosa stai pensando: \"La probabilit\u00e0 di vivere in un'epoca cos\u00ec eccezionale sembra infinitesimale, la probabilit\u00e0 che Holden abbia manie di protagonismo (a nome di tutta la Terra) sembra molto pi\u00f9 alta\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkb89ddw13rr\"><sup><a href=\"#fnkb89ddw13rr\">[5]</a></sup></span>.</p><p>Ma:</p><h2><strong>La visione \u201cconservativa\u201d</strong></h2><p>Supponiamo che tu sia d'accordo con me sulla direzione che prender\u00e0&nbsp;l'umanit\u00e0 <i>prima o poi</i>, ovvero che un giorno avremo le tecnologie per creare insediamenti resistenti e stabili in tutta la nostra galassia e oltre, ma che tu ritenga che ci vorr\u00e0 molto pi\u00f9 tempo di quanto dico io.</p><p>Un elemento fondamentale della mia ipotesi (su cui espander\u00f2 in un altro articolo) \u00e8 che entro questo secolo potremmo sviluppare un'intelligenza artificiale (IA) abbastanza avanzata da innescare un'esplosione di produttivit\u00e0. Supponiamo che tu abbia dubbi:</p><ul><li>Pensi che io stia sottovalutando i limiti fondamentali dei sistemi di IA odierni.</li><li>Pensi che avremo bisogno di un numero enorme di nuove scoperte scientifiche per costruire IA che ragionino davvero con la stessa efficacia degli esseri umani.</li><li>E anche quando ci riusciremo, diffonderci attraverso&nbsp;la galassia sar\u00e0 un viaggio ancora pi\u00f9 lungo.</li></ul><p>Pensi che niente di tutto ci\u00f2 accadr\u00e0 in questo secolo. Piuttosto ci vorranno circa <strong>500 anni</strong>. \u00c8 un tempo 5-10 volte superiore a quello trascorso da quando abbiamo iniziato a costruire computer. \u00c8 pi\u00f9 tempo di quanto sia passato da quando Isaac Newton ha formulato le prime ipotesi ragionevoli sulle leggi della fisica. \u00c8 circa la stessa quantit\u00e0 tempo trascorsa dall'inizio della Rivoluzione Scientifica.</p><p>Immaginiamo anzi di essere ancora pi\u00f9 cauti. Si pensi che il nostro progresso economico e scientifico ristagner\u00e0.&nbsp;Le civilt\u00e0 di oggi crolleranno e molte altre sorgeranno e cadranno. Certo, <i>prima o poi</i>&nbsp;avremo la capacit\u00e0 di espanderci in tutta la galassia. Ma ci vorranno <strong>100\u2019000 anni</strong>. \u00c8 un tempo 10 volte superiore a quello trascorso dall'inizio della civilt\u00e0 umana nel Levante.</p><p>Ecco la tua versione della linea del tempo:</p><p><img style=\"width:100%\" src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Yco2EXHNZBF5jsTgG/lw33cenkhaoc6nxndf6r\" alt=\"\"></p><p>La differenza tra la tua linea temporale e la mia \u00e8 meno di un pixel, quindi non si vede nel grafico. A grandi termini, questa concezione \"cauta\" e la mia sono uguali.</p><p>\u00c8 vero che la visione \"conservativa\" non richiede la stessa urgenza di intervento per la nostra generazione in particolare, ma ci colloca comunque in quella minuscola frazione di persone che vivono in un tempo incredibilmente importante. Solleva sempre lo stesso interrogativo: se le azioni che intraprendiamo oggi per rendere il mondo migliore - per quanto possano avere un effetto minimo sull\u2019universo tra 100\u2019000 anni - non potrebbero esercitare un riscontro insolito su scala galattica.</p><h2><strong>La visione scettica</strong></h2><p>La \"visione scettica\" sarebbe essenzialmente quella secondo cui l'umanit\u00e0 (o qualche suo discendente, anche digitale) non si diffonder\u00e0 <i>mai</i>&nbsp;attraverso la galassia. Ci sono molte ragioni per cui potrebbe accadere:</p><ul><li>Forse qualcosa di specifico riguardo ai viaggi nello spazio - e/o al posizionamento di robot minerari, pannelli solari, ecc. su altri pianeti - li rende effettivamente impossibili, tanto che nemmeno altri 100.000 anni di civilt\u00e0 umana saranno sufficienti per realizzarli.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmlnpyq41ng\"><sup><a href=\"#fnmlnpyq41ng\">[6]</a></sup></span></li><li>O forse sar\u00e0 fattibile tecnologicamente, ma non accadr\u00e0 per un qualche altro motivo&nbsp;(perch\u00e9 nessuno vuole, perch\u00e9 chi non vuole bloccher\u00e0 coloro che vogliono&nbsp;ecc.)</li><li>Forse \u00e8 possibile diffondersi per la galassia, ma per qualche motivo non \u00e8 possibile persistere su molti pianeti per miliardi di anni.</li><li>Forse l'umanit\u00e0 \u00e8 destinata a distruggersi prima di raggiungere questo stadio.<ul><li>Ma nota che se il modo in cui ci distruggiamo \u00e8 attraverso un'intelligenza artificiale non controllata<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2v9qs5w52kk\"><sup><a href=\"#fn2v9qs5w52kk\">[7]</a></sup></span>, l'IA potrebbe costruirsi la propria soluzione tecnologica e diffondersi per la galassia, il ch\u00e9 sembra ricondurci alle sezioni precedenti. Ci\u00f2 sottolinea anzi che il modo in cui gestiamo l'IA in questo secolo potrebbe avere ripercussioni per molti miliardi di anni. Quindi, l'umanit\u00e0 dovrebbe estinguersi in un modo da non lasciare dietro di s\u00e9 altra vita intelligente (o macchine intelligenti).</li></ul></li><li>Forse una specie extraterrestre si diffonder\u00e0 nella galassia prima di noi (o pi\u00f9 o meno nello stesso periodo).<ul><li>Tuttavia, nota che non sembra che questo sia successo in circa 13,77 miliardi di anni dall'inizio dell'universo e, in base alle sezioni precedenti, ci sono solo circa 1,5 miliardi di anni affinch\u00e9 avvenga prima che ci diffondiamo per la galassia.</li></ul></li><li>Forse qualche specie extraterrestre si \u00e8 gi\u00e0 effettivamente diffusa nella nostra galassia e per qualche motivo non la vediamo. Forse si stanno nascondendo apposta pur restando pronti ad impedirci di diffonderci troppo&nbsp;lontano.<ul><li>Questo implicherebbe che stanno scegliendo di non estrarre energia dalle stelle che possiamo vedere, almeno non in maniera percepibile. Questo vorrebbe dire a sua volta che si stanno astenendo dall'estrarre&nbsp;grandi quantit\u00e0&nbsp;di energia che potrebbero usare per realizzare i loro scopi, qualunque siano<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefm3tp41zyzf8\"><sup><a href=\"#fnm3tp41zyzf8\">[8]</a></sup></span>, compreso difendersi da specie come la nostra.</li></ul></li><li>Forse la vita \u00e8 tutta un sogno\u2026&nbsp;o&nbsp;una <a href=\"https://www.simulation-argument.com/\"><strong><u>simulazione</u></strong></a>.</li><li>O forse c\u2019\u00e8 qualcos'altro che non mi \u00e8 venuto in mente.</li></ul><p>Questo \u00e8 un numero discreto di possibilit\u00e0, anche se molte sembrano piuttosto incredibili a modo loro. Direi che, messe insieme, c\u2019\u00e8 pi\u00f9 del 50% di probabilit\u00e0 che una sia vera, ma sarebbe strano se io sostenessi che d\u2019insieme siano tutte decisamente possibili.</p><p>In definitiva, mi \u00e8 molto difficile trovare un motivo per pensare che non sia ragionevole pensare qualcosa come: \"Prima o poi creeremo insediamenti resistenti e stabili attraverso la nostra galassia e oltre\". Mi sembra che negare questa possibilit\u00e0 significhi, a sua volta, riporre un livello di fiducia \u201cincredibile\u201d in una qualche premessa sui limiti della tecnologia, sulle scelte che l\u2019umanit\u00e0 compier\u00e0 nel lungo termine, sull'inevitabilit\u00e0 dell'estinzione umana e/o sugli alieni o sulle simulazioni.</p><p>Immagino che per molti lettori questa affermazione sia intuitiva, ma non per tutti. Al momento non penso di difenderla in modo approfondito, ma ci ripenser\u00f2 se ricever\u00f2 abbastanza <a href=\"https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/%23survey\"><strong><u>richiesta</u></strong></a>.</p><h2><strong>Perch\u00e9 tutte le possibili visioni sono incredibili: il paradosso di Fermi</strong></h2><p>In pratica, sto sostenendo&nbsp;che sarebbe inverosimile pensare che non ci diffonderemo <i>mai</i>&nbsp;attraverso la galassia, ma anche che \u00e8 incredibile pensare che abbiamo ottime probabilit\u00e0 di diffonderci attraverso la galassia.</p><p>In altre parole, sto dicendo che qualsiasi ipotesi a riguardo \u00e8 difficile da supportare. Concludo questo, dato che trovo che siamo in una situazione incredibile.</p><p>Ecco alcune situazioni alternative in cui avremmo potuto ritrovarci, che non avrei considerato cos\u00ec inverosimili:</p><ul><li>Saremmo potuti vivere in una galassia per lo pi\u00f9 abitata, o dalla nostra specie o da un certo numero di specie extraterrestri. Saremmo stati in una regione dello spazio densamente popolata, circondata da pianeti popolati. Forse avremmo letto la storia della nostra civilt\u00e0 e avremmo saputo (dalla storia e dalla mancanza di sistemi solari vuoti) di non essere forme di vita insolitamente precoci con opportunit\u00e0 insolite davanti a noi.</li><li>Saremmo potuti vivere in un mondo in cui sembra impossibile sviluppare le tecnologie di cui ho parlato. Non avremmo avuto alcuna speranza di compiere viaggi nello spazio, di studiare con successo il nostro cervello o di costruire i nostri stessi computer. Forse avremmo potuto, in qualche modo, rilevare la vita su altri pianeti, ma se l\u2019avessimo fatto, avremmo visto che anche loro sono privi di questo tipo di tecnologia.</li></ul><p>Ma l'espansione spaziale \u00e8 una possibilit\u00e0 effettiva e la nostra galassia \u00e8 vuota. Queste due cose sembrano in tensione. Una tensione simile - la questione del perch\u00e9 non vediamo segni di extraterrestri, nonostante la galassia abbia cos\u00ec tante possibili stelle da cui potrebbero emergere - \u00e8 spesso discussa con il nome di <a href=\"https://en.wikipedia.org/wiki/Fermi_paradox\"><strong><u>Paradosso di Fermi</u></strong></a>.</p><p>Wikipedia ha un elenco di <a href=\"https://en.wikipedia.org/wiki/Fermi_paradox%23Hypothetical_explanations_for_the_paradox\"><u>possibili soluzioni</u></a>&nbsp;al paradosso di Fermi. Molte corrispondono alle possibilit\u00e0 dell\u2019<a href=\"https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/%23the-skeptical-view\"><strong><u>ipotesi scettica</u></strong></a>&nbsp;di cui ho fatto una lista prima. Alcune sembrano meno rilevanti per questo articolo. (Per esempio, ci sono varie ragioni per cui gli extraterrestri potrebbero essere presenti ma non essere <i>rilevabili</i>. Ma credo che qualsiasi mondo in cui gli extraterrestri non <i>impediscano</i>&nbsp;alla nostra specie di espandersi su scala galattica finisca per essere inverosimile, anche se gli extraterrestri ci sono).</p><p>La mia impressione attuale \u00e8 che le migliori analisi del paradosso di Fermi oggi disponibili favoriscano la spiegazione che <strong>la vita intelligente \u00e8 estremamente rara</strong>: c\u2019\u00e8 qualcosa di cos\u00ec improbabile riguardo alla comparsa della vita in primo luogo o all'evoluzione del cervello che non si \u00e8 verificata in molte (o in nessuna) altre parti della galassia<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmozwgjj6b0d\"><sup><a href=\"#fnmozwgjj6b0d\">[9]</a></sup></span>.</p><p>Ci\u00f2 implicherebbe che <strong>gli stadi pi\u00f9 difficili e improbabili sulla strada dell'espansione galattica sono quelli che la nostra specie </strong><i><strong>ha gi\u00e0 compiuto</strong></i><strong>.</strong>&nbsp;E questo, a sua volta, \u00e8 testamento&nbsp;che viviamo in un momento strano: agli inizi estremi della storia di un sistema solare estremamente insolito.</p><p>Se iniziassimo a trovare segni di vita intelligente in altre parti della galassia, sarei pronto a dare meno peso all\u2019ipotesi \u201cincredibile\u201d. Implicherebbe che qualsiasi cosa abbia impedito ad altre specie di espandersi attraverso la galassia fermer\u00e0 anche noi.</p><h2><strong>Questo tenue puntino azzurro potrebbe essere un affare terribilmente grande</strong></h2><p>Descrivendo la Terra come un puntino in una <a href=\"https://en.wikipedia.org/wiki/Pale_Blue_Dot%23/media/File:Pale_Blue_Dot.png\"><strong><u>foto scattata dallo spazio</u></strong></a>, Ann Druyan e&nbsp;Carl Sagan <a href=\"https://smile.amazon.com/dp/B004W0I3LW/ref%3Ddp-kindle-redirect?_encoding%3DUTF8%26btkr%3D1\"><strong><u>scrissero</u></strong></a>:</p><blockquote><p>La Terra \u00e8 un piccolissimo palcoscenico in una vasta arena cosmica. Pensate ai fiumi di sangue versati da tutti quei generali e imperatori perch\u00e9, nella gloria e nel trionfo, potessero diventare i momentanei padroni di una <a href=\"https://en.wikipedia.org/wiki/Pale_Blue_Dot%23/media/File:Pale_Blue_Dot.png\"><strong><u>frazione di puntino</u></strong></a>&nbsp;\u2026 Il nostro atteggiarci, la nostra immaginaria presunzione, l'illusione di avere una posizione privilegiata nell'Universo, sono messi in discussione da questo punto di pallida luce... \u00c8 stato detto che l'astronomia \u00e8 un'esperienza che umilia e forma il carattere. Forse non c'\u00e8 dimostrazione migliore della follia delle presunzioni umane che questa immagine distante del nostro piccolo mondo.</p></blockquote><p>Questo \u00e8 un pensiero piuttosto comune: facendo un passo indietro e considerando le nostre vite nel contesto di miliardi di anni e miliardi di stelle, ci rendiamo conto di quanto sia insignificante tutto quanto a cui ci interessiamo.</p><p>Qui, per\u00f2, sto sostenendo il contrario.</p><p>Si direbbe che il nostro \"puntino\" abbia veramente una possibilit\u00e0 di essere l'origine di una civilt\u00e0 di dimensioni galattiche. Credere in questa possibilit\u00e0 sembra assurdo, persino illusorio. Ma date le nostre osservazioni, sembra altrettanto strano scartarla.</p><p>Se le cose stanno veramente cos\u00ec, le scelte che compiamo nei prossimi 100.000 anni o, addirittura, in questo secolo potrebbero stabilire l\u2019esistenza, o meno, di questa civilt\u00e0 di scala galattica e quali valori avr\u00e0, attraverso miliardi di stelle e miliardi di anni a venire.</p><p>Perci\u00f2, quando levo lo sguardo sull\u2019immensa&nbsp;distesa dello spazio, non mi dico: \u201cAh, tanto alla fine non importa nulla.\u201d Penso: \u201cBeh, parte di ci\u00f2 che facciamo probabilmente non ha importanza. Ma <i>alcune</i>&nbsp;delle nostre azioni potrebbero essere pi\u00f9 importanti di qualsiasi altra azione che verr\u00e0 mai compiuta\u2026 sarebbe davvero appropriato se non abbassassimo&nbsp;la guardia... [gulp]\".</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwh2lnz8ch1b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwh2lnz8ch1b\">^</a></strong></sup></span><div class=\"footnote-content\"><p><sup>&nbsp;</sup>O <a href=\"https://en.wikipedia.org/wiki/Kardashev_scale\"><u>Tipo III Kardashev</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjdjc6a6j1lh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjdjc6a6j1lh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Se fossimo in grado di <a href=\"https://en.wikipedia.org/wiki/Mind_uploading\"><u>caricare menti in un universo digitale</u></a>&nbsp;o creare simulazioni dettagliate di persone coscienti come lo siamo noi, sarebbe possibile collocarle in ambienti virtuali che si resettano automaticamente o comunque si \u201ccorreggono\u201d ogni volta che la societ\u00e0 cambiasse in un certo modo (per esempio, se una certa religione diventasse dominante o perdesse il predominio). Questo potrebbe permettere ai progettisti di questi ambienti virtuali l\u2019abilit\u00e0 di \u201ccongelare ad oltranza\u201d certe religioni, sovrani, ecc. Ne parler\u00f2 pi\u00f9 approfonditamente in un prossimo articolo.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn56vuu147n9y\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref56vuu147n9y\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Mi sono concentrato sulla \u201cgalassia\u201d in maniera piuttosto arbitraria. Diffonderci attraverso&nbsp;tutto l\u2019universo raggiungibile richiederebbe molto pi\u00f9 tempo che diffonderci attraverso&nbsp;la galassia. Fino a quando non lo faremo \u00e8 ancora concepibile che qualche specie all'infuori della galassia disturbi la \u201ccivilt\u00e0 stabile su scala galattica\u201d, ma penso che tenere conto di ci\u00f2 aggiungerebbe una notevole dose di complessit\u00e0 senza cambiare il quadro generale. Potrei per\u00f2 affrontare l\u2019argomento in un qualche articolo futuro.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5294b8cq14\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5294b8cq14\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Un asse logaritmico non avrebbe un aspetto meno strano, perch\u00e9 le distanze tra i traguardi \u201cintermedi\u201d sono minuscole rispetto ai periodi sia prima che dopo questi traguardi. Sostanzialmente, sto parlando di quanto eccezionale sia essere vivi ne [i&nbsp;pochi] anni pi\u00f9 importanti tra [i molti] anni. Questo si esprime meglio usando un asse lineare. \u00c8 vero che spesso i grafici strani sembrano meno strani con assi logaritmici, ma in questo caso penso che il grafico abbia un aspetto strano perch\u00e9 \u00e8 la situazione ad essere strana. Forse la versione dall\u2019aspetto meno strano potrebbe avere come asse delle x la distanza che percorriamo (nello spazio) a partire dall\u2019asse 2100, ma sarebbe una premessa singolare per un grafico. Sarebbe come dare per scontata la mia tesi che sostiene che questi siano tempi molto speciali.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkb89ddw13rr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkb89ddw13rr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Questo \u00e8 esattamente il tipo di ragionamento che per molti anni ha fatto che io restassi scettico nei confronti delle argomentazioni che tratter\u00f2 nel resto di questa serie sui possibili impatti e tempismi delle tecnologie avanzate. ~Senza dubbio \u00e8 stato fondamentale per me confrontarmi con quanto \u201cincredibile\u201d sembra essere la nostra situazione.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmlnpyq41ng\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmlnpyq41ng\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Sarebbe decisamente pi\u00f9 difficile diffonderci&nbsp;attraverso la galassia se non fosse mai possibile fare qualcosa come <a href=\"https://en.wikipedia.org/wiki/Mind_uploading\"><u>caricare menti in un universo digitale</u></a>&nbsp;(di cui discuto in <a href=\"https://www.cold-takes.com/how-digital-people-could-change-the-world/\"><u>un altro articolo</u></a>&nbsp;e che \u00e8 parte del motivo per cui penso che gli insediamenti futuri potrebbero avere un \u201cblocco dei valori\u201d, come discusso in precedenza). A suo modo, troverei incredibile l\u2019ipotesi che sia \u201cimpossibile caricare menti digitali\u201d, perch\u00e9 implicherebbe che il cervello umano sia cos\u00ec speciale da rendere impossibile replicarne&nbsp;il funzionamento in maniera digitale. (Grazie a David Roodman per questo punto.)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2v9qs5w52kk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2v9qs5w52kk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Ossia un\u2019IA avanzata che persegue i propri obiettivi che non sono compatibili con l\u2019esistenza dell'uomo. Scriver\u00f2 ancora riguardo questa idea. Tra i trattati che esistono in questo momento ci sono i libri <i>Superintelligenza</i>&nbsp;(<a href=\"https://smile.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom-ebook/dp/B00LOOCGB2/\"><u>Superintelligence</u></a>), <a href=\"https://smile.amazon.com/Human-Compatible-Artificial-Intelligence-Problem-ebook/dp/B07N5J5FTS\"><u>Human Compatible</u></a>, <a href=\"https://smile.amazon.com/Life-3-0-Being-Artificial-Intelligence-ebook/dp/B06WGNPM7V\"><u>Life 3.0</u></a>, and <a href=\"https://smile.amazon.com/Alignment-Problem-Machine-Learning-Values-ebook/dp/B085T55LGK/\"><u>The Alignment Problem</u></a>. La presentazione pi\u00f9 breve e approcciabile&nbsp;che conosco \u00e8 <i>I motivi per prendere sul serio l\u2019intelligenza artificiale come un pericolo per l\u2019umanit\u00e0</i>&nbsp;(<a href=\"https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/%23fn8:~:text%3DThe%2520case%2520for%2520taking%2520AI%2520seriously%2520as%2520a%2520threat%2520to%2520humanity\"><i><u>The case for taking AI seriously as a threat to humanity</u></i></a>, articolo su Vox di Kelsey Piper). Questo <a href=\"https://www.alignmentforum.org/posts/HduCjmXTBD4xYTegv/draft-report-on-existential-risk-from-power-seeking-ai\"><u>rapporto sui rischi esistenziali dovuti all\u2019IA ricercatrice di potere</u></a>, di Joe Carlsmith di Open Philanthropy, stabilisce un insieme dettagliato di premesse che complessivamente significherebbero che il problema \u00e8 serio.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnm3tp41zyzf8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefm3tp41zyzf8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Grazie a Carl Shulman per questo punto.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmozwgjj6b0d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmozwgjj6b0d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Vedi <a href=\"https://arxiv.org/pdf/1806.02404.pdf\"><u>https://arxiv.org/pdf/1806.02404.pdf</u></a>&nbsp;\u201cDissolving the Fermi Paradox\u201d.</p></div></li></ol><p><br>&nbsp;</p>", "user": {"username": "EA Italy"}}, {"_id": "bSuzKRaFyPvNGGrvh", "title": "Tempi eccezionali", "postedAt": "2023-01-17T14:58:05.626Z", "htmlBody": "<p><i>This is an Italian translation of </i><a href=\"https://forum.effectivealtruism.org/posts/pFHN3nnN9WbfvWKFg/this-can-t-go-on\"><i><strong>This Can't Go On</strong></i></a></p><p><i>\u00c8 disponibile una versione audio di questo articolo (in inglese) su</i><a href=\"https://www.cold-takes.com/this-cant-go-on/\"><i>&nbsp;Cold Takes</i></a><i>&nbsp;(o cerca \u201cCold Takes Audio\u201d su&nbsp;Stitcher, Spotify, Google Podcasts, ecc.)</i></p><blockquote><p>Questo articolo \u00e8 il primo della serie a sostenere l\u2019idea che&nbsp;<strong>non viviamo solo in un\u2019era, ma addirittura in un secolo eccezionale.</strong>&nbsp;Gli articoli precedenti&nbsp;della <a href=\"https://www.cold-takes.com/roadmap-for-the-most-important-century-series/\"><u>serie</u></a>&nbsp;parlavano dello strano futuro che potrebbe presentarsi <i>prima o poi</i>&nbsp;(forse tra&nbsp;100 anni, forse tra&nbsp;100\u2019000).</p><p>Riassunto di questo articolo:</p><ul><li>Siamo abituati a un tasso di crescita dell\u2019economia mondiale di qualche punto percentuale all\u2019anno. Questo accade da diverse generazioni.</li><li>Tuttavia, questa \u00e8 una situazione molto inusuale. Se prendiamo in considerazione tutta la storia, \u00e8 evidente che il tasso di crescita sta accelerando, che \u00e8 pressoch\u00e9 ai massimi livelli storici e che \u00e8 molto pi\u00f9 veloce di quanto possa essere sostenibile ancora a lungo (non ci sono abbastanza atomi nella nostra galassia per sostenere questo tasso di crescita anche solo per i prossimi 10\u2019000 anni).</li><li>Il mondo non pu\u00f2 continuare a crescere a questo ritmo senza sosta. Dovremmo prepararci per altre possibilit\u00e0: una stagnazione (una&nbsp;crescita che rallenta o termina),&nbsp;un\u2019esplosione (una&nbsp;crescita che accelera ancora di pi\u00f9, prima di raggiungere i suoi limiti) o&nbsp;un collasso&nbsp;(una qualche catastrofe che colpisce l\u2019economia).</li></ul><p>I tempi in cui viviamo sono inusuali e instabili. Non dovremmo sorprenderci se succedesse qualcosa di stravagante, come un\u2019esplosione del progresso economico e scientifico che ci porti alla <a href=\"https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/\"><u>maturit\u00e0 tecnologica</u></a>. Anzi, un\u2019esplosione tale sarebbe del tutto in linea con la traiettoria corrente.</p></blockquote><p>L\u2019economia mondiale \u00e8 in crescita<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyqw54usqvpq\"><sup><a href=\"#fnyqw54usqvpq\">[1]</a></sup></span>&nbsp;da che ciascuno abbia memoria, in media di un paio di punti percentuali l\u2019anno. Salvo qualche alto e basso, la crescita \u00e8 perlopi\u00f9&nbsp;stabile<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefb5ndaa0t0jd\"><sup><a href=\"#fnb5ndaa0t0jd\">[2]</a></sup></span>. Chiamer\u00f2 questo universo \u201c<strong>La Solita Solfa</strong>\u201d.</p><p>Ne La Solita Solfa, il mondo \u00e8 in costante cambiamento e questo cambiamento \u00e8 percepibile, ma non travolgente,&nbsp;e non \u00e8 impossibile stare al passo con esso. Vi \u00e8 un costante fluire di nuove opportunit\u00e0 e sfide, ma, in generale, non richiede un cambiamento nel comportamento individuale. Puoi prenderti qualche anno per adattarti alle novit\u00e0. La vita nel 2019 era simile alla vita di tutti i giorni nel 2018, diversa, ma non molto, dalla vita quotidiana nel 2010 e molto, ma non follemente, diversa dal 1980<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2eaooahu29j\"><sup><a href=\"#fn2eaooahu29j\">[3]</a></sup></span>.</p><p>Se questo ti suona ragionevole e familiare e immagini che il futuro proceda come ha fatto fino ad ora, allora sei del partito de La Solita Solfa. Quando pensi al passato e al futuro, probabilmente ti immagini qualcosa cos\u00ec:</p><figure class=\"image image_resized\" style=\"width:100%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bSuzKRaFyPvNGGrvh/klkpbx2xggxjgpr2scxn\" alt=\"Chart of the world economy starting in about 1950. It looks like a straight line (on a logarithmic y-axis), with a dotted straight line projecting more of the same for the future.\"><figcaption>La Solita Solfa</figcaption></figure><p><i>Grafico dell\u2019economia mondiale dall\u2019anno 1950 circa. Il grafico \u00e8 una linea pi\u00f9 o meno retta (su un asse delle y logaritmico), prolungata&nbsp;da una linea tratteggiata che prevede un andamento simile per il futuro.</i></p><p>&nbsp;</p><p>Io ho una prospettiva diversa,&nbsp;una con un passato pi\u00f9 turbolento e un futuro pi\u00f9 incerto. Chiamer\u00f2 questo approccio\u201c<strong>Tempi Eccezionali</strong>\u201d&nbsp;per rendere l\u2019idea di un momento storico che non si pu\u00f2 protrarre.&nbsp;Ecco la mia versione del grafico:</p><figure class=\"image image_resized\" style=\"width:100%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bSuzKRaFyPvNGGrvh/rqwaluycvsbehsjkxrtz\" alt=\"Chart of the world economy starting in about 5000 BC. Unlike the previous chart, it looks very jagged and irregular, with the line getting steeper and steeper over time. Instead of a single dotted line projecting more of the same, I have several dotted lines projecting different possible futures.\"><figcaption>Tempi Eccezionali</figcaption></figure><p><i>Grafico dell\u2019economia mondiale dall\u2019anno 5\u2019000 a.C. circa. A differenza del grafico precedente, \u00e8 molto frastagliato e irregolare, con una linea che diventa sempre pi\u00f9 ripida con il passare del tempo. Anzich\u00e9 una sola linea tratteggiata che prevede una continuazione dell\u2019andamento passato, ho inserito diverse linee tratteggiate, corrispondenti a diversi possibili futuri.</i></p><p>&nbsp;</p><p>Qual \u00e8 il grafico giusto? Beh, entrambi contengono gli stessi dati storici. \u00c8 solo che il grafico \u201cLa Solita Solfa\u201d inizia dal 1950, mentre&nbsp;\u201cTempi Eccezionali\"&nbsp;parte dal 5000 a.C. <strong>\u201cLa Solita Solfa\u201d \u00e8 solo un piccolo estratto della storia; l\u2019idea di \u201cTempi Eccezionali\u201d tiene conto del suo andamento complessivo.</strong></p><figure class=\"image image_resized\" style=\"width:100%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bSuzKRaFyPvNGGrvh/iuutes5vlj2hu4xto55m\" alt=\"Graphic illustrating that when you zoom in on the 'this can't go on' chart, you get the 'business as usual chart.'\"><figcaption>&nbsp;</figcaption></figure><p><i>Immagine che spiega che quando fai zoom sul grafico&nbsp;'Tempi Eccezionali' ottieni&nbsp;'La Solita Solfa'.</i></p><p>Siamo tutti abituati ad una crescita di qualche punto percentuale l\u2019anno, ma nel contesto dell\u2019intera storia dell'umanit\u00e0,&nbsp;una crescita di un paio di punti percentuali l\u2019anno \u00e8 evidentemente pazzesca (\u00e8 la parte del grafico dove la linea blu diventa quasi verticale).</p><p>Questo tasso di crescita perdura da un tempo pi\u00f9 lungo di quanto chiunque possa ricordare, ma che non \u00e8 cos\u00ec lungo nel complesso della storia. Solo qualche centinaio d\u2019anni rispetto a migliaia di anni di civilt\u00e0 umana. \u00c8 un\u2019accelerazione enorme che non \u00e8 sostenibile a lungo (pi\u00f9 avanti&nbsp;spiegher\u00f2 pi\u00f9 precisamente perch\u00e9).</p><p>Il primo grafico suggerisce regolarit\u00e0 e prevedibilit\u00e0. Il secondo trasmette un senso di volatilit\u00e0 e di possibilit\u00e0 disparate per il futuro.</p><p>Un futuro possibile \u00e8 la <strong>stagnazione</strong>: raggiungeremo la \u201cdimensione massima\u201d dell\u2019economia e la crescita si fermer\u00e0. Ci preoccuperemo tutti di come suddividere le limitate risorse di cui disporremo e i giorni di una crescita abbondante e di un\u2019economia dinamica saranno alle nostre spalle, per sempre.</p><p>Un altro \u00e8 l\u2019<strong>esplosione</strong>: la crescita accelerer\u00e0&nbsp;ulteriormente, al punto che l\u2019economia mondiale raddoppier\u00e0 di grandezza ogni anno, settimana o ora. A stimolare una simile crescita potrebbe essere la comparsa di una tecnologia \u201cDuplicatrice\u201d (<a href=\"https://www.cold-takes.com/the-duplicator\"><i>Duplicator</i></a>), come l\u2019esistenza di <a href=\"https://www.cold-takes.com/how-digital-people-could-change-the-world/\"><u>persone digitali</u></a>&nbsp;o, come tratter\u00f2 in capitoli futuri, l\u2019intelligenza artificiale avanzata. Se questo dovesse accadere, il mondo cambier\u00e0 molto pi\u00f9 velocemente di quanto un essere umano possa capacitarsi.</p><p>Un\u2019altra possibilit\u00e0 \u00e8 il&nbsp;<strong>collasso:</strong>&nbsp;una catastrofe globale&nbsp;che far\u00e0 inginocchiare l\u2019umanit\u00e0 o che la spazzer\u00e0 via del tutto, al punto che non raggiungeremo mai pi\u00f9 i livelli di crescita odierni.</p><p>O forse succeder\u00e0 qualcos\u2019altro ancora.</p><h2><strong>Perch\u00e9 l\u2019accelerazione non pu\u00f2 continuare a lungo?</strong></h2><p>Un buon punto di partenza pu\u00f2 essere <a href=\"https://www.overcomingbias.com/2009/09/limits-to-growth.html\"><u>questa analisi su Overcoming Bias</u></a>, della quale presenter\u00f2 qui la mia versione:</p><ul><li>Diciamo che al momento l'economia mondiale sta crescendo del 2% ogni anno<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxffou4lod39\"><sup><a href=\"#fnxffou4lod39\">[4]</a></sup></span>.&nbsp;Che vuol&nbsp;dire&nbsp;che l\u2019economia si sta raddoppiando ogni 35 anni circa<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref71s2ogpesx\"><sup><a href=\"#fn71s2ogpesx\">[5]</a></sup></span>.</li><li>Se questo dovesse durare, tra 8200 anni l\u2019economia sar\u00e0 circa 3*10<sup>70</sup>&nbsp;volte l\u2019economia di oggi.</li><li>\u00c8 probabile che ci siano meno di 10<sup>70</sup>&nbsp;atomi nella nostra galassia<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2kq16sd6sai\"><sup><a href=\"#fn2kq16sd6sai\">[6]</a></sup></span>, e che non saremmo in grado di andare al di l\u00e0 di&nbsp;essa nei prossimi 8200 anni<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefuzd3ct2kyh\"><sup><a href=\"#fnuzd3ct2kyh\">[7]</a></sup></span>.</li><li>Quindi, se l\u2019economia fosse 3*10<sup>70</sup>&nbsp;volte pi\u00f9 grande di oggi e potesse utilizzare solo 10<sup>70</sup>&nbsp;atomi al massimo, dovremmo essere in grado di sostenere per ogni atomo <strong>svariate&nbsp;economie grandi quanto l\u2019economia mondiale odierna.</strong></li></ul><p>8200 anni potrebbero sembrare tanti, ma sono molti meno della storia dell'umanit\u00e0.&nbsp;Sono persino meno del tempo che ci separa dall\u2019invenzione dell\u2019agricoltura.</p><p>\u00c8 davvero <i>impossibile</i> che potremmo sviluppare tecnologie per supportare multipli dell\u2019intera civilt\u00e0 umana odierna, <i>per ogni atomo disponibile</i>? No, ma richiederebbe una trasformazione radicale delle nostre societ\u00e0, di gran lunga maggiore dei cambiamenti che abbiamo vissuto nel corso della storia umana sinora. Non sarei esattamente disposto a <i>scommettere</i>&nbsp;che le cose vadano proprio cos\u00ec per le prossime migliaia di anni.</p><p>Sembra molto pi\u00f9 probabile che ci ritroveremo \u201ca corto\u201d di scoperte scientifiche, di innovazione tecnologica e di risorse naturali e che i tempi in cui ci arricchiamo di un paio di punti percentuali all\u2019anno finir\u00e0. Dopotutto, questo andamento ha solo qualche centinaio&nbsp;di anni.</p><p>(<a href=\"https://dothemath.ucsd.edu/2011/07/galactic-scale-energy/\"><u>Questo articolo</u></a>&nbsp;effettua un\u2019analisi simile usando come esempio le fonti di energia anzich\u00e9 l\u2019economia. Prevede che raggiungeremo i limiti ancora prima. Presupponendo una crescita del consumo di energia annua del&nbsp;2.3% (meno del&nbsp;tasso&nbsp;storico&nbsp;per gli Stati Uniti sin dal Seicento), stima che arriveremmo, nel giro di 2500 anni,&nbsp;ad aver bisogno di pi\u00f9 energia di quanta ne producano tutte le stelle della nostra galassia<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflcirn0xodmo\"><sup><a href=\"#fnlcirn0xodmo\">[8]</a></sup></span>.)</p><h2><strong>Esplosione e collasso</strong></h2><p>Un futuro possibile, allora, \u00e8 la stagnazione: la crescita potrebbe gradualmente rallentare e&nbsp;il risultato potrebbe essere un mondo senza crescita. Io, per\u00f2, non penso che questo sia il futuro pi\u00f9 probabile.</p><p>Il grafico in alto <strong>non mostra un&nbsp;rallentamento nella crescita, ma un\u2019accelerazione drammatica.</strong>&nbsp;Cosa ci aspetteremmo se invece proiettassimo&nbsp;la stessa accelerazione&nbsp;nel futuro?</p><p><a href=\"https://www.openphilanthropy.org/blog/modeling-human-trajectory\"><u>Un modello della traiettoria dell'umanit\u00e0</u></a>&nbsp;(di David Roodman di Open Philanthropy) affronta esattamente questo interrogativo, effettuando una regressione con legge di potenza su dati sulla crescita economica passata<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9xxlet659ik\"><sup><a href=\"#fn9xxlet659ik\">[9]</a></sup></span>.&nbsp;Estrapolando questo modello al futuro (cio\u00e8 presumendo che questo modello rester\u00e0 valido per il futuro), ottiene una&nbsp;<i><strong>crescita infinita</strong></i>. L\u2019idea di crescita infinita \u00e8 un\u2019astrazione matematica, ma potresti interpretarla come \u201cla crescita continuer\u00e0 pi\u00f9 velocemente possibile finch\u00e9 non raggiunger\u00e0 un limite.\u201d</p><p>Ne&nbsp;<a href=\"https://www.cold-takes.com/the-duplicator\"><u>Il Duplicatore</u></a>&nbsp;do un riassunto di una serie di vaste discussioni su questa possibilit\u00e0. La conclusione \u00e8 che una crescita esplosiva sarebbe possibile <i>se</i>&nbsp;avessimo una tecnologia per \u201ccopiare\u201d menti umane&nbsp;o qualcosa che ricoprisse la stessa funzione, come le <a href=\"https://www.cold-takes.com/how-digital-people-could-change-the-world/\"><u>persone digitali</u></a>&nbsp;o un\u2019intelligenza artificiale sufficientemente avanzata.</p><p>Durante un'esplosione di progresso, il tasso di crescita annuo potrebbe raggiungere il 100% (la grandezza dell\u2019economia mondiale raddoppierebbe ogni anno), il che potrebbe protrarsi per massimo ~250 anni prima di arrivare ai limiti di cui abbiamo discusso<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreft065498p41\"><sup><a href=\"#fnt065498p41\">[10]</a></sup></span>. Potremmo anche osservare che la crescita sia pi\u00f9 rapida. Le dimensioni dell\u2019economia potrebbero raddoppiare ogni mese&nbsp;(il che potrebbe protrarsi per massimo&nbsp;20 anni prima di raggiungere i limiti<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefero1qb1yohp\"><sup><a href=\"#fnero1qb1yohp\">[11]</a></sup></span>), o pi\u00f9 velocemente ancora.</p><p>Se ci\u00f2&nbsp;succedesse, questa sarebbe una crescita mai vista, cos\u00ec rapida da abbagliare, forse causata da intelligenze artificiali capaci di produzione pi\u00f9 vasta di quanto noi umani riusciremmo&nbsp;a tenere traccia, che si avvicina rapidamente ai limiti del possibile - al quale punto la crescita sarebbe costretta a fermarsi.</p><p>Oltre alla stagnazione ed alla crescita esplosiva c\u2019\u00e8 una terza possibilit\u00e0: <strong>il&nbsp;collasso</strong>. Una catastrofe globale che potrebbe ridurre la civilt\u00e0 a condizioni da cui non sarebbe mai in grado di tornare ai livelli di crescita odierni.&nbsp;Un esempio estremo di un collasso del genere sarebbe l\u2019estinzione dell\u2019uomo. Questo non \u00e8 un futuro che figura nel grafico, ma \u00e8 possibile.</p><p>Come sostiene Toby Ord ne&nbsp;<a href=\"https://smile.amazon.com/Precipice-Existential-Risk-Future-Humanity/dp/0316484911?sa-no-redirect%3D1\"><u>Il Precipizio</u></a>, gli asteroidi o altri rischi \u201cnaturali\u201d non appaiono in grado di portare alla nostra estinzione. Tuttavia, ci sono rischi evidentemente seri e molto difficili da quantificare: il cambiamento climatico, una guerra nucleare (particularmente un inverno nucleare), le pandemie&nbsp;(specie se i progressi della biologia consentiranno lo sviluppo di armi biologiche insidiose)&nbsp;e i rischi posti dall\u2019intelligenza artificiale avanzata.</p><p>Tenendo presente queste tre possibilit\u00e0 (stagnazione, esplosione e collasso):</p><ul><li>Viviamo in uno dei (due) secoli con la crescita pi\u00f9 veloce di sempre (i secoli&nbsp;XX&nbsp;e XXI);</li><li>Sembra probabile che questo secolo sar\u00e0 uno degli ~80&nbsp;con la crescita maggiore di tutti i tempi<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmqhrtju9rbn\"><sup><a href=\"#fnmqhrtju9rbn\">[12]</a></sup></span>;</li><li>Se arrivassero le tecnologie adatte per innescare una crescita esplosiva, questo potrebbe essere di gran lunga il secolo con la crescita pi\u00f9 veloce di sempre;</li><li>Se le cose si mettessero male, questo potrebbe essere il nostro ultimo secolo.</li></ul><p>Potremmo dedurre&nbsp;perci\u00f2 che questo sia un secolo indubbiamente importante e che abbia una chance di essere il pi\u00f9 eccezionale di sempre. Questo ragionamento si basa solo su osservazioni elementari, non ragionamenti dettagliati sull\u2019intelligenza artificiale (che affronter\u00f2 in articoli futuri).</p><h2><strong>Progresso scientifico e tecnologico</strong></h2><p>\u00c8 difficile creare un grafico per i progressi scientifici e tecnologici simile al grafico della crescita economica, ma credo che se fosse possibile, rappresenterebbe un quadro sostanzialmente simile a quello della crescita economica.</p><p>Un libro interessante, che consiglio, \u00e8 <a href=\"https://smile.amazon.com/Asimovs-Chronology-Science-Discovery-Asimov/dp/0060156120/\"><u>Cronologia delle scoperte scientifiche (</u><i><u>Chronology of Science and Discovery</u></i><u>) di Asimov</u></a>. Ripercorre in ordine cronologico le invenzioni e le scoperte pi\u00f9 importanti della storia dell'umanit\u00e0. Tra le prime voci figurano&nbsp;\u201cutensili di pietra\u201d, \u201cfuoco\u201d, \u201creligione\u201d e&nbsp;\u201carte\u201d; tra le ultime \u201cla cometa di Halley\u201d e&nbsp;\u201csuperconduttivit\u00e0 ad alte temperature critiche\u201d.</p><p>Una curiosit\u00e0 sul libro \u00e8 che&nbsp;<strong>553 pagine su&nbsp;654 hanno luogo dopo l\u2019anno&nbsp;1500</strong>&nbsp;- anche se inizia da 4 milioni di anni a.C. Prevedo&nbsp;che altri libri del genere mostrino una tendenza simile<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefibbzf9cah4f\"><sup><a href=\"#fnibbzf9cah4f\">[13]</a></sup></span>&nbsp;e ritengo che ci siano effettivamente stati pi\u00f9 progressi scientifici&nbsp;e tecnologici negli ultimi&nbsp;~500 anni&nbsp;che nei diversi milioni di anni precedenti<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefe1lew4qu25\"><sup><a href=\"#fne1lew4qu25\">[14]</a></sup></span>.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bSuzKRaFyPvNGGrvh/jeuw9wriu1adsa606sct\" alt=\"\"></p><p><i>In un&nbsp;</i><a href=\"https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/\"><i><u>articolo precedente</u></i></a><i>&nbsp;ho sostenuto che gli eventi storici pi\u00f9 significativi della storia sembrano raggruppati attorno ai tempi odierni,&nbsp;come illustro con </i><a href=\"https://www.cold-takes.com/content/images/2021/06/fermi-aggressive-1.png\"><i><u>questa linea del tempo</u></i></a><i>. Essa \u00e8 sulla scala di miliardi di anni, ma riscontriamo qualcosa di simile facendo zoom sulle scorse migliaia di anni: i maggiori progressi scientifici e tecnologici sono raggruppati vicino ad oggi. Ecco una linea del tempo che illustra ci\u00f2 concentrandosi negli ambiti del trasporto e dell\u2019energia&nbsp;(credo che avrei potuto ottenere un\u2019immagine simile scegliendo quasi qualunque altra categoria).</i></p><p>Perci\u00f2, come per la crescita economica, il ritmo del progresso scientifico e tecnologico recente \u00e8 estremamente superiore a quello storico. Come per la crescita economica, \u00e8 presumibile&nbsp;che ad un certo punto si incontrino dei limiti per il progresso della tecnologia. Infine, sempre come per la crescita economica, il progresso scientifico e tecnologico d\u2019ora in poi potrebbe:</p><ul><li><strong>Stagnare</strong>, come&nbsp;<a href=\"https://www.theatlantic.com/science/archive/2018/11/diminishing-returns-science/575665/\"><u>alcuni temono stia accadendo</u></a>.</li><li><strong>Esplodere</strong>, se venisse sviluppata una qualche tecnologia che aumentasse radicalmente il numero di \u201cmenti\u201d (persone,&nbsp;<a href=\"https://www.cold-takes.com/how-digital-people-could-change-the-world/\"><u>persone digitali</u></a>, or intelligenze artificiali avanzate) che possano portare avanti lo sviluppo tecnologico e scientifico<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefk7tjynk1v1n\"><sup><a href=\"#fnk7tjynk1v1n\">[15]</a></sup></span>.</li><li><strong>Collassare</strong>&nbsp;per via di una qualche catastrofe a livello globale.</li></ul><h2><strong>Possibilit\u00e0 trascurate</strong></h2><p>Ritengo che sia necessario che nel mondo vi siano persone che vivono con la mentalit\u00e0 de \u201cLa Solita Solfa\u201d, perch\u00e9&nbsp;riflettano su come rendere il mondo un posto migliore stando all'ipotesi che&nbsp;il tasso di crescita economica di fondo rimanga stabile e regolare per il momento.</p><p>Credo che vi sia bisogno di altri che vivano con la mentalit\u00e0 dei Tempi Eccezionali e riflettano sulle conseguenze di una stagnazione, un\u2019esplosione o un collasso, e se le nostre azioni possano influenzare quale si realizzi.</p><p>Al giorno d\u2019oggi, per\u00f2, le cose sembrano molto sbilanciate. Quasi tutti gli articoli di giornale e le analisi partono dalla premessa di un mondo che \u00e8 sempre quello de La Solita Solfa.</p><p>Una metafora per il mio stato d\u2019animo potrebbe essere che il mondo sia simile ad un gruppo di persone in un aereo in sfrecciata lungo la pista di decollo:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bSuzKRaFyPvNGGrvh/wfiez96yellkzesmnlow\" alt=\"Animated image of the view out the airplane window as it blasts down the runway.\"></p><p>Ci stiamo muovendo molto pi\u00f9 velocemente del solito. Non rimane abbastanza pista per continuare cos\u00ec molto a lungo\u2026 e stiamo ancora accelerando.</p><p>E quando leggo analisi su argomenti di attualit\u00e0,&nbsp;la gente parla di come allacciare la cintura di sicurezza nel modo pi\u00f9 confortevole possibile (visto che tocca metterla), osserva come i momenti pi\u00f9 felici nella vita siano essere seduti affianco alla propria famiglia a guardare le strisce&nbsp;bianche sfilare via, o litiga su di chi sia la colpa del rombo di sottofondo.</p><p>Se fossi in una situazione tale e non sapessi che cosa mi aspetta (il decollo), magari non lo indovinerei,&nbsp;ma spererei&nbsp;almeno di pensare: \u201cQuesta situazione \u00e8 decisamente folle, inusuale&nbsp;e provvisoria. O continueremo ad accelerare, o ci fermeremo, o succeder\u00e0 qualcos\u2019altro di strano.\u201d</p><p><i>Grazie a&nbsp;Mar\u00eda Guti\u00e9rrez Rojas per le figure in questo articolo e a Ludwig Schubert per una&nbsp;precedente </i><a href=\"https://www.cold-takes.com/content/images/2021/06/fermi-aggressive-1.png\"><i><u>linea del tempo</u></i></a><i>&nbsp;sulla quale si basa la linea del tempo in questo articolo.</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyqw54usqvpq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyqw54usqvpq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Se non sai che cosa voglia dire \u201cin crescita\u201d puoi rivolgerti alla mia breve <a href=\"https://www.cold-takes.com/what-is-economic-growth\"><u>spiegazione della crescita economica</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnb5ndaa0t0jd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefb5ndaa0t0jd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;La crescita globale reale ha generalmente spaziato tra leggera recessione e crescita del ~7% annua .</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2eaooahu29j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2eaooahu29j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Salto il 2020 dato che \u00e8 stato un anno insolito e diverso dagli altri per via della pandemia globale e di altri fattori.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxffou4lod39\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxffou4lod39\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Mi riferisco al PIL reale (corretto dall\u2019inflazione). 2% \u00e8 meno della crescita globale al momento e usare il valore di crescita globale rafforzerebbe la mia tesi. Credo per\u00f2 che 2% sia una stima accettabile per la crescita \u201cdi frontiera\u201d, cio\u00e8 la crescita nei paesi che sono gi\u00e0 tra i pi\u00f9 sviluppati, specie rispetto alla crescita globale totale, la quale include la crescita \u201cdi recupero\u201d (di paesi in passato poveri che stanno crescendo rapidamente, come la Cina oggi).</p><p>Per verificare la mia stima di 2% ho scaricato <a href=\"https://fred.stlouisfed.org/series/GDPC1\"><u>questi dati per gli Stati Uniti</u></a>&nbsp;e controllato il tasso di crescita annualizzato tra gli anni 2000-2020, 2010-2020 e 2015-2020 (usando luglio perch\u00e9 l\u2019ultimo punto per il 2020 era luglio). Rispettivamente i tassi erano 2.5%, 2.2% e 2.05%.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn71s2ogpesx\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref71s2ogpesx\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Una crescita del 2% per 35 anni \u00e8 una crescita di (1 + 2%)^35 = 2x</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2kq16sd6sai\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2kq16sd6sai\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;La stima pi\u00f9 alta della massa della Via Lattea su <a href=\"https://en.wikipedia.org/wiki/Milky_Way\">Wikipedia</a>&nbsp;\u00e8 di 4,5 * 10^12 masse solari, ciascuna <a href=\"https://en.wikipedia.org/wiki/Solar_mass\"><u>equivalente a</u></a>&nbsp;circa 210^30 kg, ciascuno dei quali <a href=\"https://en.wikipedia.org/wiki/Observable_universe%23Matter_content%25E2%2580%2594number_of_atoms\"><u>si stima</u></a>&nbsp;essere equivalente a 1,67 * 10^-27 atomi di idrogeno. (4.5 10^12 * 2 * 10^30) / (1,67 * 10^-27) =~ 5.410^69.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnuzd3ct2kyh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefuzd3ct2kyh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;<a href=\"https://en.wikipedia.org/wiki/Milky_Way%23Size_and_mass\">Wikipedia</a>: \u201cA marzo 2019 degli astronomi hanno riferito che la massa della Via Lattea \u00e8 di 1,5 milioni di milioni (10^12) entro un raggio di circa 129.000 anni luce.\u201d Sto presumendo che non possiamo percorrere pi\u00f9 di 129.000 anni luce nei prossimi 8200 anni perch\u00e9 vorrebbe dire viaggiare a una velocit\u00e0 di molto superiore a quella della luce.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlcirn0xodmo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflcirn0xodmo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Questo calcolo non \u00e8 spiegato in modo diretto nell\u2019articolo. Le frasi chiave sono \u201cMantenere un tasso di crescita dell\u2019energia del 2.3% richiederebbe produrre tanta energia quanto il sole nei prossimi 1400 anni, a prescindere dalla tecnologia usata\u201d e \u201cLa galassia della Via Lattea ospita circa about 100 mila milioni (100 * 10^9) di stelle. Fiumi di energia che si sprigionano nello spazio, a disposizione di chiunque. Ricordiamo che ciascun fattore di dieci ci sostiene per altri 100 anni. 100 mila milioni sono undici fattori di dieci, quindi 1100 anni in pi\u00f9.\u201d 1400 + 1100 = 2500, il numero di cui ho parlato io. Ci\u00f2 presuppone che in media una stella nella nostra galassia ci offra circa tanta energia quanto il Sole; non so se sia vero.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9xxlet659ik\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9xxlet659ik\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Il dibattito su se <a href=\"https://www.openphilanthropy.org/blog/modeling-human-trajectory\"><u>Un modello della traiettoria dell'umanit\u00e0</u></a>&nbsp;utilizzi la funzione pi\u00f9 adatta ai dati storici <a href=\"https://forum.effectivealtruism.org/posts/CWFn9qAKsRibpCGq8/does-economic-history-point-toward-a-singularity\"><u>\u00e8 aperto</u></a>. Parlo dell\u2019effetto che questo dibattito potrebbe avere sulle mie conclusioni <a href=\"https://www.cold-takes.com/a-note-on-historical-economic-growth/\"><u>qui</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnt065498p41\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreft065498p41\">^</a></strong></sup></span><div class=\"footnote-content\"><p><sup>&nbsp;</sup>250 raddoppiamenti sarebbero un fattore di crescita di circa 1,8*10^75, pi\u00f9 di 10.000 volte il numero degli atomi nella nostra galassia.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnero1qb1yohp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefero1qb1yohp\">^</a></strong></sup></span><div class=\"footnote-content\"><p><sup>&nbsp;</sup>20 anni sono 240 mesi, perci\u00f2 se a ciascuno corrispondesse un raddoppiamento dell\u2019economia globale sarebbe un fattore di crescita di circa 1,8*10^72, pi\u00f9 di 100 volte il numero degli atomi nella nostra galassia.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmqhrtju9rbn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmqhrtju9rbn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Per via di quanto abbiamo osservato precedentemente: il tasso di crescita di oggi non pu\u00f2 continuare che per circa altri 8200 anni (82 secoli). L\u2019unico modo per avere pi\u00f9 di 82 secoli di crescita veloce quanto quella odierna \u00e8 se ci sono anche molti secoli di crescita negativa, come nella linea a zigzag nel grafico \u201cTempi Eccezionali\u201d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnibbzf9cah4f\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefibbzf9cah4f\">^</a></strong></sup></span><div class=\"footnote-content\"><p><sup>&nbsp;</sup><a href=\"https://osf.io/h3867/\"><u>Questo dataset</u></a>&nbsp;attribuisce valori a diverse figure storiche in base a quanto vengono discusse in opere di riferimento. I dati partono dal 800 a.C. e ci sono dieci volte tante figure storiche nella categoria \u201cscienza\u201d dopo il 1500 che prima. Non appoggio il libro da cui vengono questi dati perch\u00e9 penso che dai dati tragga conclusioni ingiustificate;&nbsp;sto solo sostenendo la mia tesi che nella maggior parte delle opere di riferimento, il numero di scoperte dopo il 1500 \u00e8 maggiore di quello che ci aspetteremmo se l\u2019umanit\u00e0 scoprisse invenzioni sempre alla stessa velocit\u00e0.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fne1lew4qu25\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefe1lew4qu25\">^</a></strong></sup></span><div class=\"footnote-content\"><p><sup>&nbsp;</sup>Ad essere imparziali, \u00e8 possibile che libri di riferimento del genere dedichino attenzioni sproporzionate al passato pi\u00f9 prossimo. Io credo tuttavia che trasmettano un\u2019impressione d\u2019insieme giusta. Sostenere questa affermazione fino in fondo andrebbe oltre lo scopo di questo post, ma a suo sostegno porterei (a) le opere che ho citato, perch\u00e9 penso che se tu le leggessi o sfogliassi ne trarresti un\u2019impressione simile e (b) il fatto che la crescita economica segue una tendenza simile (anche se per essa l\u2019esplosione \u00e8 cominciata pi\u00f9 di recente; mi sembra intuitivo che la crescita economica segua il progresso scientifico ma con un ritardo).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnk7tjynk1v1n\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefk7tjynk1v1n\">^</a></strong></sup></span><div class=\"footnote-content\"><p><sup>&nbsp;</sup>Negli articoli che cito a questo proposito ne <a href=\"https://www.cold-takes.com/the-duplicator\"><u>Il Duplicatore</u></a>, il modello \u00e8 che un\u2019esplosione di innovazione contribuisca specificamente a innescare una crescita economica esplosiva.</p></div></li></ol>", "user": {"username": "EA Italy"}}, {"_id": "d3ynxjEGjCEkwb7Sg", "title": "Che cosa potrebbe riservare il futuro? E perch\u00e9 dovrebbe importarci?", "postedAt": "2023-01-17T14:52:19.470Z", "htmlBody": "<p><i>This is an Italian translation of </i><a href=\"https://forum.effectivealtruism.org/posts/a8myKSh4KWDmuj6BA/what-could-the-future-hold-and-why-care\"><i><strong>What could the future hold? And why care?</strong></i></a></p><p>In questo capitolo ci interroghiamo su quale possa essere il futuro dell\u2019umanit\u00e0 e perch\u00e9 darvi peso. Esploreremo argomentazioni a favore del \u201clungoterminismo\u201d, ovvero l\u2019idea che una delle priorit\u00e0 morali chiave dei nostri tempi sia migliorare il futuro a lungo termine. Questo pu\u00f2 dare maggiore peso al dibattito a favore del ridurre alcuni dei rischi di estinzione di cui abbiamo parlato le scorse due settimane. Esploreremo anche varie visioni del nostro futuro e ragioni per cui potrebbe essere molto diverso dal presente.</p><p>Alcuni dei concetti chiave di questo capitolo sono:</p><ul><li><strong>L\u2019imparzialit\u00e0 morale:</strong> cio\u00e8 aiutare coloro che pi\u00f9 ne hanno bisogno, senza trascurare nessuno in base a fattori come il luogo e il momento in cui vivono e la specie a cui appartengono, a meno che questi fattori non siano effettivamente rilevanti dal punto di vista morale.</li><li><strong>Le previsioni (forecasting):</strong> \u00e8 difficile prevedere il futuro. Tuttavia, pu\u00f2 valere la pena di farlo per rendere le nostre previsioni pi\u00f9 esplicite e imparare dai nostri errori.</li></ul><p>Ti eserciterai anche nella&nbsp;<strong>calibrazione</strong>. Ossia far s\u00ec che quando stimi una probabilit\u00e0 del 60% che un evento si verifichi, questo evento succeder\u00e0 effettivamente circa nel 60% dei casi. Questa \u00e8 una capacit\u00e0 importante per essere in grado di formulare giudizi validi in condizioni di incertezza.<br>&nbsp;</p>", "user": {"username": "EA Italy"}}, {"_id": "Yi2vGpsAQftuA9DHj", "title": "Be wary of enacting norms you think are unethical", "postedAt": "2023-01-18T06:07:24.769Z", "htmlBody": "<p>To my eye, a lot of EAs seem to under-appreciate the extent to which your response to a crisis isn't just a reaction to an existing, fixed set of societal norms. The act of choosing a response is the act of <i>creating </i>a norm.</p><p>You're helping bring into existence a particular version of EA, a particular version of academia and intellectual life, and a particular world-at-large. This is particularly true insofar as EA is a widely-paid-attention-to influencer or thought leader in intellectual society, which I think it (weakly) is.</p><p>It's possible to overestimate your influence, but my impression is that most EAs are currently underestimating it. Hopefully this post can at least <i>bring to your attention the hypothesis</i> that your actions matter for determining the norms going forward, even if you don't currently think you have enough evidence to believe that hypothesis.</p><p>If you want the culture to be a certain way, I think it's worth taking the time to flesh out for yourself what the details would look like, and find ways to test whether there are any ways to effect that norm, or to move in the direction that seems best to you.</p><p>Anchor more to what you actually think is ethical, to what you think is kind, to what you think is honorable, to what you think is important and worth protecting. If you think your peers aren't living up to your highest principles, then don't give up <i>on your principles</i>.</p><p>(And maybe don't give up on your peers, or maybe do, depending on what seems right to you.)</p><p>Don't ignore the current set of norms you see; but be wary of willing bad outcomes into being via <a href=\"https://www.lesswrong.com/posts/Js34Ez9nrDeJCTYQL/politics-is-way-too-meta\">self-fulfilling prophecies</a>.</p><p><i>Be dissatisfied</i>, if the world doesn't already look like the vision of a good, wholesome, virtuous world you can picture in your head. Because as someone who feels optimistic about EAs' capacities to do what's right, I want to see more people fighting for their personal visions of what that involves.</p>", "user": {"username": "RobBensinger"}}, {"_id": "pC9RJdmP3rnhuHpCm", "title": "Emerging Paradigms: The Case of Artificial Intelligence Safety", "postedAt": "2023-01-18T05:59:07.280Z", "htmlBody": "<p><i>Note: This is a paper I wrote as part of a philosophy of science graduate seminar. At the moment, I consider it a work in progress and welcome constructive feedback.&nbsp;</i></p><p>&nbsp;</p><p><strong>1. Introduction</strong></p><p>This paper closely examines the nascent field of artificial intelligence (AI) safety as it is currently evolving into a paradigm of its own in computer science. This subfield of AI is concerned with one central question: how to make machine learning models have goals aligned with human goals. This puzzle is widely known as the <i>alignment problem</i>&nbsp;(Bostrom 2016) or the <i>problem of control</i> (Russell 2019). The paper is centered around the proposal that one requirement to solve the alignment problem is to reach the stage of normal science in the new paradigm of AI safety.&nbsp;</p><p>To support this proposal, I will offer an account of how the research efforts to transform the field into a paradigm accurately reflect Kuhn\u2019s model of the structure of science presented in the Structure of Scientific Revolutions and what this specifically predicts for AI safety. I will first describe the characteristics of the field at its pre-paradigmatic state and dive into the nature of the alignment problem and how it should be understood in relation to questions about how science changes. Then, I will suggest the desiderata for AI safety to qualify as a paradigmatic field according to Kuhn\u2019s epistemology of science. Further, the development of AI safety is tightly associated with the discussion about existential risks from technological progress. I will use Kuhn\u2019s approach to the history of science to discuss the role of philosophy of science in the transformation of the field into a paradigm to minimize existential risks. My overarching claim is that history and philosophy of science help provide a more comprehensive understanding of risks from advanced AI models and implementing Kuhn\u2019s view on the development of AI safety is crucial for constructing an epistemological account of such risks and challenges.&nbsp;</p><p><strong>2. AI safety: a field at the pre-paradigmatic state</strong></p><p>The opportunity to observe an emerging scientific field transform into a paradigm is accompanied by intrinsic uncertainty and metascientific deliberation. Uncertainty, because the field lacks the maturity required to be studied as a discipline of its own. Maturity here entails unity, clarity, and concreteness in the research agendas of scientists&nbsp;(Hibbert 2016). In Kuhn\u2019s terms, at this stage, researchers have no consensus on the body of concepts, phenomena, and techniques to use when working. They also do not follow the same rules or standards for their scientific practice, simply because no such rules or standards have been established. Establishing them is no easy task; \u201cthe road to a firm research consensus is extraordinarily arduous\u201d&nbsp;(Kuhn 2012, 15). Metascientific deliberation is necessary to account for the uniqueness of the generation of novel scientific knowledge and practice as well as the difficulties that might be encountered on the road to normal science and puzzle-solving&nbsp;(Kuhn 2012).&nbsp;</p><p>Diagnosing the difficulties of the pre-paradigmatic phase will help illustrate how Kuhn\u2019s model applies to AI safety. The first difficulty concerns narrowing down the research directions space. Without a paradigm, or at least one obvious paradigm candidate, all hypotheses, research proposals, and agendas seem equally promising and worthy of investigation. Moreover, we observe a disunity of frameworks that concerns the concepts, theories, agendas, practices, methodological tools, and other criteria for what qualifies as having high explanatory power. As of 2022, there are three main research directions in AI safety, but it is highly controversial whether any of them would qualify as a paradigm independently. These research directions are 1) Assistance Games, motivated in&nbsp;(Russell 2019) and developed in&nbsp;(Fickinger et al. 2020), 2) Agent Foundations&nbsp;(Yudkowsky 2008), and 3) Prosaic Alignment including three proposals: HCH&nbsp;(Christiano 2018), ELK&nbsp;(Christiano 2022), and formalizing heuristic arguments&nbsp;(Christiano, Neyman, and Xu 2022). This categorization is not exhaustive although it provides a sense of the alignment research landscape and the priorities of prominent researchers. Different teams and laboratories are nevertheless optimizing for different agendas and goals as there is no agreement on which agenda is most likely to succeed.&nbsp;</p><p>A second, follow-up difficulty is that most researchers tend to be fundamentally confused about their field. This feature of the pre-paradigmatic stage is explicitly expressed and acknowledged by alignment researchers working with different agendas. For example, Dr. John Wentworth indicates that the first step of his research plan is to \u201csort out our fundamental confusions about agency\u201d&nbsp;(Wentworth 2021) admitting that experts in the field share a \u201cfundamental confusion\u201d and there is no explicit consensus on the nature of the subject matter and the best ways to approach it.&nbsp;</p><p>One last but crucial difficulty for the pre-paradigmatic AI safety is the lack of a published corpus of work that goes together with a highly organized discipline. As Kuhn notes: \u201cin the sciences, the formation of specialized journals, the foundation of specialists\u2019 societies, and the claim for a special place in the curriculum have usually been associated with a group\u2019s first reception of a single paradigm\u201d&nbsp;(Kuhn 2012, 19). In other words, the institutional norms and patterns are yet to be established. This includes having a rigid definition of the group of AI safety scientists, one or more AI safety peer-reviewed journals, regular conferences, university courses<a href=\"#_ftn1\">[1]</a> and professorships.&nbsp;</p><p>There are many signs that these difficulties will be overcome in the near-term future. For example, the fact that many students and researchers have shifted their academic interests towards machine learning alignment is a way to measure the growth of the field into a scientific community. According to an estimation, there are currently approximately 400 people worldwide working directly on AI safety issues, including those that do not do strictly scientific work but are adjacent to the developing paradigm of research.<a href=\"#_ftn2\">[2]</a> Having these researchers pursue the existing directions, conduct empirical work and experimentation to falsify hypotheses, and share their findings indicates that the field is at least receiving adequate attention to eventually become its own paradigm.&nbsp;</p><p>Before diving into the nature of the alignment problem, it is valuable to address why it is so critical to consider the development of the AI safety paradigm in particular, among all fields that might be under transformation at this point. The answer is that AI safety entering its normal science phase will have an extraordinary impact on society at large&nbsp;(Taylor et al. 2016). This is primarily because whether we are able to control extremely powerful machines or transformative AI systems (TAI) might be the main factor determining the long-term future of humanity. To understand the significance of a fully formed field of AI safety, we must take into account that the stakes of solving the alignment problem are particularly high as machine learning is becoming more and more a general purpose technology, i.e., \u201ca single generic technology, recognizable as such over its whole lifetime, that initially has much scope for improvement and eventually comes to be widely used, to have many uses, and to have many spillover effects\u201d&nbsp;(Crafts 2021). The risks that accompany the potential arrival of a highly powerful AI fall into two general categories: existential risks and suffering risks&nbsp;(Bostrom 2016). AI being an existential risk means that highly powerful future models will be unaligned with human goals and values and consequently, they could cause the species to go extinct (<i>Ibid</i>). The hypothesis concerning suffering risks suggests that such unaligned models could be the cause of endless suffering either for humans or other sentient beings&nbsp;(Tomasik 2011). All these should be carefully considered when examining under what conditions the field of AI safety is being transformed into a paradigm of its own.&nbsp;</p><p><strong>3. The nature of the alignment problem &nbsp;</strong></p><p>Examining the nature of the alignment problem yields useful observations for the metascientific study of this emerging field and for placing it into the context of the discourse on risks and technological progress. To analyze the alignment problem from a philosophical standpoint, it is helpful to consider whether it shares any similarities with other problems in the history of science. To explicate the metaphysics of alignment, I draw an analogy between AI safety and the history of chemistry. In particular, the alchemists were intellectually curious, \u201cproto-scientists\u201d&nbsp;(Van\u010dik 2021), deeply confused about their methods and how likely they are to succeed. They all, however, shared a common aim summarized in this threefold: to find the Stone of Knowledge (also called \u201cThe Philosophers' Stone\u201d), to discover the medium of Eternal Youth and Health, and to discover the transmutation of metals&nbsp;(Linden 2003). Their \u201cscience\u201d had the shape of a pre-paradigmatic field that would eventually transform into the natural science of chemistry. Importantly, their agenda ceased to be grounded upon mystical investigations as the field began to mature.&nbsp;As Kuhn\u2019s model predicts, those who remained attached to the pre-paradigmatic mysticism did not get to partake in the newly formed normal science; \u201cthe transfer of allegiance from paradigm to paradigm is a conversion experience that cannot be forced\u201d, Kuhn remarks&nbsp;(Kuhn 2012, 150) suggesting that a transformation takes place facilitating the change of practice from pre-scientific to scientific.&nbsp;</p><p>The claim here is not that alignment maintains in any sense the mystical substrate of alchemy. It shares the high uncertainty combined with attempts to work at the experimental and observational level that cannot be supported as in physical sciences. Furthermore, it shares the intention to find something that does not yet exist with the expectation that when it does, it will make the human world substantially qualitatively different than it was prior to that invention.&nbsp;This belief in a technological product that will change the flow of history in extraordinary and perhaps unpredictable ways is worth remarking. It also allows to deepen the chemistry-alignment analogy: the aim of alignment work is to make highly powerful systems follow instructions by human designers. But, at the same time, AI teams seem to be in a \u201ctechnology arms race\u201d&nbsp;(Armstrong, Bostrom, and Shulman 2016) to be the first to build the most powerful machine that has ever existed and could have transformative impact on humanity&nbsp;(Gruetzemacher and Whittlestone 2022).&nbsp;</p><p>It remains valuable to ask about what can history of science teach us when studying the generation of a new scientific field. Continuing with the analogy, it would be useful for the progress of alignment research to be able to trace what exactly happened when alchemy became chemistry. Several suggestions might apply, namely the articulation of one or more equations, or the discovery and analysis of a substance like phlogiston. In that sense, researchers would need to find alignment's phlogiston and that would bring them closer to discovering alignment's oxygen.&nbsp;Of course, scientific change is not as straightforward. While ideally, we would want to uncover what the successful actors in history did and apply it to a paradigm in the making, in practice, this does not hold. This would become possible, however, with a generalized logic of scientific discovery that would describe in high detail how science operates.&nbsp;</p><p>&nbsp;</p><p><strong>4. Questions about scientific change&nbsp;</strong></p><p>Another way to think about the generation of this new field is to consider whether the transformation from the pre-paradigmatic to the paradigmatic stage could be accelerated. This question simply translates into whether progress can become faster and if yes, under what conditions. It seems that throughout the history of science, faster progress has always entailed quantitatively more empirical work in the field. Because of the rapid development of AI, one suggestion is to use AI models to advance alignment research. Using AI models could increase research productivity in alignment even if they cannot at present generate novel insights or properly theorize about scientific work in a reliable way. Such AI systems are usually large language models and for example, they can review bibliographical material, compose summaries, and explain concepts with less jargon, among other tasks&nbsp;(Wei et al. 2022).&nbsp;</p><p>In Kuhn\u2019s framework, progress in science is discontinuous. This suggests that researchers working within the established (future) paradigm of alignment theory will <i>see</i> the world radically differently compared to researchers working in the paradigm of \u201cgood old-fashioned AI\u201d&nbsp;(Haugeland 1989). It is worth noticing that there were a few researchers in the old paradigm thinking that highly powerful machines will pose a problem of control, notably, I. J. Good and V. Vinge. Whether they could have predicted, however, the occurrence of the deep learning revolution&nbsp;(Sejnowski 2018) and what it would imply for alignment is rather debatable.&nbsp;</p><p>Contrary to the traditional, logical empiricist view, it is not the accumulation of new knowledge that will bring the new paradigm. Researchers will need to recognize that the anomalies and confusions of the field at the pre-paradigmatic state require different conceptual and technical tools to be resolved. However, it is generally possible to understand the problematic aspects of the field, in retrospect, i.e., once the paradigm is established.&nbsp;</p><p>This brings us to a commonly misunderstood concept in Kuhn\u2019s epistemology of science, i.e., incommensurability. Incommensurability invites a twofold discussion about the way terms are used when placed in different theoretical contexts and about changes in worldview&nbsp;(Kuhn 2012; 1990). A classic example is the term \u201cmass\u201d; while the term appears both in the paradigm of classical mechanics and relativity theory, it connotes different meanings depending on the paradigm. But while some concepts can be translated from an older paradigm to the vocabulary of the new one, it is generally impossible to find a one-to-one correspondence between the terms of the old paradigm and the terms of the new one. This semantic dependence on the theory implies that there is no common measure although a comparison remains possible&nbsp;(Kuhn 1990). &nbsp;In that sense, as AI safety continues to evolve, we may find that concepts such as agency or intelligence are incommensurable to what was used to describe artificially intelligent systems in the old AI paradigm. This will not suggest that there is no way to neutrally compare e.g., the concept of intelligence without anchoring it into a theoretical apparatus. It is also possible that some terms preserve their meanings despite the paradigm change&nbsp;(Kuhn 1990, 36).&nbsp;</p><p>The change of scientific worldview is extensively discussed by Kuhn in the Structure and by Hanson in <i>Patterns of Discovery</i> with the proposal of theory-ladenness. Kuhn parallelizes the change of worldview to how we suddenly <i>see</i> different shapes in gestalt figures (Kuhn 2012, 114). The verb \u201csee\u201d here implies the automaticity of this change of perception, i.e., the scientist cannot control the shift of attention that has occurred. A typical example of this in the history of science is astronomy. Notably, while western Europeans started discovering planets the half century after the introduction of the Copernican paradigm, the Chinese were long aware of many more stars in the sky. This is because their cosmological paradigm offered a different model of celestial change.&nbsp;</p><p>Kuhn explains how he originally conceived the idea of belonging to a different paradigm. He describes sitting at his desk with Aristotle\u2019s Physics open in front of him when \u201csuddenly the fragments in my head sorted themselves out in a new way, and fell into place together [\u2026] for all at once Aristotle seemed a very good physicist indeed, but of a sort I\u2019d never dreamed possible\u201d&nbsp;(Kuhn 2014). From that point onwards, Kuhn would argue that scientists do not try to do the job of their predecessors simply in a better way than they did&nbsp;(Reisch 2016); they see a different world, they attend to different phenomena and features of reality. This leads to an asymmetry in that they operate within a completely unique context and thus construct different world models and provide different explanations.&nbsp;</p><p>On a similar tone but with a cognitive focus, Hanson describes the phenomenon of theory-ladenness to illustrate that scientific understanding depends on perceptual input and its processing. Hanson famously quotes Duhem\u2019s example:&nbsp;</p><p>Enter a laboratory; approach the table crowded with an assortment of apparatus, an electric cell, silk-covered copper wire, small cups of mercury, spools, a mirror mounted on an iron bar; the experimenter is inserting into small openings the metal ends of ebony-headed pins; the iron oscillates, and the mirror attached to it throws a luminous band upon a celluloid scale; the forward-backward motion of this spot enables the physicist to observe the minute oscillations of the iron bar. But ask him what he is doing. Will he answer 'I am studying the oscillations of an iron bar which carries a mirror'? No, he will say that he is measuring the electric resistance of the spools. If you are astonished, if you ask him what his words mean, what relation they have with the phenomena he has been observing and which you have noted at the same time as he, he will answer that your question requires a long explanation and that you should take a course in electricity&nbsp;(Hanson 1958, 16-17).</p><p>Both from Kuhn and Hanson, it becomes clear that participating in a paradigm means to learn to attend to certain phenomena and features of the parts of the world that are under investigation. At the same time, this implies learning to block out everything else; part of belonging to a paradigm means that scientists agree on what to attend to and what to block out.&nbsp;</p><p>Throughout his writings, Kuhn challenges the idea of scientific progress, for example by saying: \u201cWe must explain why science \u2014 our surest example of sound knowledge \u2014 progresses as it does, and we must first find out how in fact it does progress\u201d&nbsp;(Kuhn 1970, 20). Science does not \u201cget better\u201d&nbsp; It is plausible to argue that Kuhn would be skeptical of a potential technological singularity&nbsp;(Sandberg 2013) or the completion of science. In Kuhn\u2019s model, science does not move towards absolute knowledge or complete science. Since the scientific process is discontinuous, it is not meaningful to argue that science aims at \u201cobjective\u201d truth or simply truth as correspondence to reality&nbsp;(Rorty 2003). Paradigms are incommensurable, therefore there is no neutral way to talk about how one paradigm finds \u201cmore truth\u201d than the other. One might assume that Kuhn purposefully talks very little about the notion of truth in the Structure&nbsp;(Bird 2012). He returns to the topic in the Postscript to the second edition mostly to address the various criticisms against the Structure about relativism&nbsp;(Kuhn 2012, 204). While assessing Kuhn\u2019s understanding of truth is out of the scope of this paper, it is necessary to note that Kuhn seems to be at least a neutralist stance about truth&nbsp;(Bird 2007). This has, in many instances made him a major ally of antirealist views in science while it is not implausible to find ways in which the Kuhnian model is compatible with scientific realism.&nbsp;</p><p>&nbsp;</p><p><strong>5. Desiderata for the new paradigm</strong></p><p>The fact that people can form coalitional groups and agree to conform to certain rules and norms does not make their group nor their work a scientific paradigm. It is thus essential to sketch out what we can expect AI safety to look like as a paradigm, based on Kuhn\u2019s model of the structure of science. This can be regarded as experimenting with a future history of AI safety. As such, it is useful to lay out a series of desiderata that AI safety will qualify for once it has entered its paradigmatic stage. First,<i>&nbsp;</i>while the term <i>paradigm</i> has a long history that dates back to Aristotle\u2019s Rhetoric&nbsp;(Hacking 2012), paradigms<i>&nbsp;</i>are defined in the Structure as \u201caccepted examples of actual scientific practice\u2014examples which include law, theory, application, and instrumentation together\u2014[that] provide models from which spring particular coherent traditions of scientific research\u201d (Kuhn 2012, 11). In that sense, the paradigm will shape scientific work as a whole, from education and recruiting new researchers to everyday practice.&nbsp;</p><p>It follows then that the function of a paradigm is both cognitive and normative&nbsp;(Kindi and Arabatzis 2013, 91) which means that paradigmatic AI safety will 1) prepare students for becoming professional researchers and members of a scientific community, 2) select the class of facts that \u201care worth determining both with more precision and in a larger variety of situations\u201d a process characteristic of normal science also called \u201cfact gathering\u201d&nbsp;(Kuhn 2012, 25-27), 3) define the specific problems that must be solved (<i>Ibid</i>, 27-28), 4) offer criteria for selecting these problems (<i>Ibid</i>, 37), 5) guarantee the existence of stable solutions to these problems (<i>Ibid</i>, 28), 6) provide methods and standards of solutions, 7) aim at quantitative laws (<i>Ibid</i>). 8) make predictions, and 9) give satisfactory explanations for phenomena. &nbsp;</p><p>It is worth remarking that the paradigm offers what Kuhn calls \u201ctheoretical commitment\u201d without which important facets of scientific activity do not exist, namely the discovery of laws. Kuhn\u2019s examples highlight that the examination of measurements alone is never enough for finding quantitative laws. The history of science provides sufficient evidence to think that the experimental, Baconian method does apply, but only to a certain degree. Characteristically, Kuhn mentions Boyle\u2019s Law, Coulomb\u2019s Law, and Joule\u2019s formula (<i>Ibid</i>, 28) to argue that they were discovered through the application of a particular scientific paradigm \u2013 even if it was not as explicit at the time \u2013 and not through the study of measurements. In that sense, the theoretical commitment encapsulates a set of assumptions and concepts which are a prerequisite for the discovery of laws. By analogy, we can anticipate AI safety to find and formulate laws once the theoretical commitment is clearly established and broadly accepted.&nbsp;</p><p>&nbsp;</p><p><strong>6. Metascientific projects&nbsp;</strong></p><p><i>6.1. Conceptual clarification&nbsp;</i></p><p>In setting up a metascience of alignment, the primary question to consider is how can philosophy help the field in a concrete and straightforward way. While the possibility of this is itself contentious and pertains to an inquiry about the relationship of philosophy and science more broadly, it is arguable that philosophy can, to some extent, assist in disambiguating central concepts of AI. In the case of AI safety in particular, there are many conceptual difficulties that stem from the intrinsically perplexing nature of the study of intelligence and agency. For example, the research agenda of Agent Foundations is focused on modeling agents so that future powerful models exhibit behaviors we can predict and consequently, control. Philosophers working on the alignment problem can contribute to the conceptual clarification and modeling of agency using analytic tools that are common in philosophical reasoning.&nbsp;</p><p>There is one important objection to the claim in favor of the contribution of philosophical conceptual clarification. Looking at the history of science more generally, it does not seem like philosophy typically helped directly with scientific research. In other words, while some work in the philosophy of science engages with specific problems in the sciences, it is rare that it actually accelerates the progress of the field by disambiguating fundamental concepts beyond merely semantic disagreements. For that to be a true possibility, philosophers would have to work as scientists and scientists would have to cultivate a philosophical disposition.&nbsp;</p><p>A reply to this objection is that throughout the history of science and across various disciplines, scientists have in many instances acknowledged the necessary role of philosophical thinking in articulating the right questions and trying to explain the world rationally. Concepts are often understood as the \u201cbuilding blocks\u201d of theories&nbsp;(Gerring 1999). In that sense, as Popper notes in the preface to the <i>Logic of Scientific Discovery</i>, the method of philosophy and science is a single one, and that is rational discussion&nbsp;(Popper 2002, xix). More specifically on the relationship of AI and philosophy, McCarthy hints at the need for conceptual clarification aided by working on philosophical questions. This is especially useful since both fields are concerned with concepts such as goals, knowledge, belief, etc. A central problem at the basis of AI concerns the conditions that will allow testing intelligent behavior for example, by applying the Turing test. Analyzing what the Turing test is about requires philosophical rigor so that to determine and interpret the criteria to pass the test. For that reason, it has been a common theme in philosophy e.g., in&nbsp;(Searle 1980),&nbsp;(Dennett 1984), and&nbsp;(Chalmers 1994). It overall appears that conceptual clarification is indispensable to rational discourse and the more thoroughly that it is executed, the more likely it is to yield itself conducive to paradigm transformation.&nbsp;</p><p><i>6.2. Questions about the nature of machine learning &nbsp;</i></p><p>Another project in the philosophy of science adjacent to the development of the AI safety paradigm focuses on questions about the metaphysics of machine learning and specifically, the nature of deep neural networks. This discussion is centered around whether we can emulate the functions of human minds by imitating the neural circuitry of the human brain. Until recently, this seemed unlikely to work. Steven Pinker criticized connectionism in the early 2000s arguing that&nbsp;</p><p>Humans don\u2019t just loosely associate things that resemble each other, or things that tend to occur together. They have combinatorial minds that entertain propositions about what is true of what, and about who did that to whom, when and where and why. And that requires a computational architecture that is more sophisticated that the uniform tangle of neurons used in generic connectionist networks.&nbsp;(Pinker 2003, 79)&nbsp;</p><p>Pinker goes on to expose the limitations of connectionism that derive from this not sufficiently sophisticated computational architecture of neural networks. All this is to show that a complete human thought cannot be represented in a generic network that is used in machine learning. Specifically, neural networks would not be able to distinguish between kinds and individuals, have thoughts that aren\u2019t just a summation of smaller parts (compositionality), play with logical quantification, embed one thought in another (recursion), and reason categorically&nbsp;(Pinker 2003).</p><p>The most recent development of machine learning proved all these claims false. Deep neural networks are surprisingly good at all the tasks Pinker mentions. In particular, large language models or transformers&nbsp;(Vaswani et al. 2017) and other deep learning architectures have generated impressive results such as writing like humans or making scientific discoveries, e.g., AlphaFold&nbsp;(Jumper et al. 2021). While there is no consensus on why these models are so successful&nbsp;(Ngo 2022), it is reasonable to conclude that intelligence was not so hard to find, after all. &nbsp;How they work seems to be itself a problem between philosophy, science, and engineering. One or more solutions to it will likely be at the foundation of the new AI safety paradigm. Explainable AI, i.e., models that are not regarded as black boxes but are rather <i>interpretable</i>(Li et al. 2022) would overall make safety easier to achieve.&nbsp;</p><p>Lastly, considerations about whether it is possible for machine learning systems to think or become highly competent at human-level tasks go back to AI researchers in the 1950s&nbsp;(McCarthy 2008) and remain pertinent and present even in AI textbooks such as in&nbsp;(Russell and Norvig 2009). Gaining clarity on how neural networks work and what accounts exactly for their success is almost equivalent to unlocking the nature and conditions of cognition.&nbsp;</p><p>&nbsp;</p><p><strong>7. Conclusion&nbsp;</strong></p><p>The emergence of AI safety as a paradigm in the making, fuels the study of how new science is generated and presents an opportunity to dive into the most fundamental questions about the nature of scientific practice and the epistemology of progress in science and technology. My aim in this paper was to explore the application of Kuhn\u2019s theory of the structure of science in the case of AI safety. I argued that the field is at its pre-paradigmatic state which motivates a series of metascientific observations. For these observations to make sense, I offered an account of the nature of the alignment problem, the central problem AI safety must tackle. In doing so, I suggested that there is an analogy between the development of AI safety and the history of chemistry. I then sketched out the requirements for the field to transform into a fully evolved paradigm according to Kuhn\u2019s view on scientific change. As the filed is undergoing quick development, it is reasonable to speculate that it will transition to the paradigmatic phase in the near-term future. A potential way to accelerate this transformation is by investing in philosophically-minded, metascientific projects such as conceptual clarification and the theoretical study of machine learning. If targeted at the right questions, they could be conducive to transitioning to a paradigm of AI safety. The need to reach the paradigmatic state becomes more and more urgent as machine learning capabilities advance, and models exhibit intelligent behavior that could eventually pose serious risks for society at large.&nbsp;</p><p>&nbsp;</p><p><strong>References</strong></p><p>Armstrong, Stuart, Nick Bostrom, and Carl Shulman. 2016. \u201cRacing to the Precipice: A Model of Artificial Intelligence Development.\u201d <i>AI &amp; SOCIETY</i> 31 (2): 201\u20136. https://doi.org/10.1007/s00146-015-0590-y.</p><p>Bird, Alexander. 2007. \u201cWhat Is Scientific Progress?\u201d <i>No\u00fbs</i> 41 (1): 64\u201389.</p><p>\u2014\u2014\u2014. 2012. \u201c10 Kuhn, Naturalism, and the Social Study of Science.\u201d In <i>Kuhn\u2019s the Structure of Scientific Revolutions Revisited</i>, edited by Vas\u014d Kint\u0113 and Theodore Arabatzis, 205. Routledge.</p><p>Bostrom, Nick. 2016. <i>Superintelligence: Paths, Dangers, Strategies</i>. Reprint edition. Oxford, United Kingdom\u202f; New York, NY: Oxford University Press.</p><p>Chalmers, David J. 1994. \u201cOn Implementing a Computation.\u201d <i>Minds and Machines</i> 4 (4): 391\u2013402.</p><p>Christiano, Paul. 2018. \u201cHumans Consulting HCH.\u201d Medium. April 15, 2018. https://ai-alignment.com/humans-consulting-hch-f893f6051455.</p><p>\u2014\u2014\u2014. 2022. \u201cEliciting Latent Knowledge.\u201d Medium. February 25, 2022. https://ai-alignment.com/eliciting-latent-knowledge-f977478608fc.</p><p>Christiano, Paul, Eric Neyman, and Mark Xu. 2022. \u201cFormalizing the Presumption of Independence.\u201d <i>ArXiv Preprint ArXiv:2211.06738</i>.</p><p>Crafts, Nicholas. 2021. \u201cArtificial Intelligence as a General-Purpose Technology: An Historical Perspective.\u201d <i>Oxford Review of Economic Policy</i> 37 (3): 521\u201336. https://doi.org/10.1093/oxrep/grab012.</p><p>Dennett, Daniel C. 1984. \u201cCan Machines Think?\u201d In <i>How We Know</i>, edited by M. G. Shafto. Harper &amp; Row.</p><p>Fickinger, Arnaud, Simon Zhuang, Dylan Hadfield-Menell, and Stuart Russell. 2020. \u201cMulti-Principal Assistance Games.\u201d <i>ArXiv Preprint ArXiv:2007.09540</i>.</p><p>Gerring, John. 1999. \u201cWhat Makes a Concept Good? A Criterial Framework for Understanding Concept Formation in the Social Sciences.\u201d <i>Polity</i> 31 (3): 357\u201393.</p><p>Gruetzemacher, Ross, and Jess Whittlestone. 2022. \u201cThe Transformative Potential of Artificial Intelligence.\u201d <i>Futures</i> 135: 102884.</p><p>Hacking, Ian. 2012. \u201cIntroductory Essay.\u201d</p><p>Hanson, Norwood Russell. 1958. <i>Patterns of Discovery: An Inquiry into the Conceptual Foundations of Science</i>. 1st edition. Cambridge: Cambridge University Press.</p><p>Haugeland, John. 1989. <i>Artificial Intelligence: The Very Idea</i>. Reprint edition. Cambridge, Mass: Bradford Books.</p><p>Hibbert, Ruth. 2016. \u201cWhat Is an Immature Science?\u201d <i>International Studies in the Philosophy of Science</i> 30 (1): 1\u201317. https://doi.org/10.1080/02698595.2016.1240433.</p><p>Jumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, and Anna Potapenko. 2021. \u201cHighly Accurate Protein Structure Prediction with AlphaFold.\u201d <i>Nature</i> 596 (7873): 583\u201389.</p><p>Kindi, Vasso, and Theodore Arabatzis. 2013. <i>Kuhn\u2019s The Structure of Scientific Revolutions Revisited</i>. Routledge.</p><p>Kuhn, Thomas S. 1970. \u201cLogic of Discovery or Psychology.\u201d <i>Criticism and the Growth of Knowledge</i> 4: 1.</p><p>\u2014\u2014\u2014. 1990. \u201cThe Road since Structure.\u201d In , 1990:3\u201313. Philosophy of Science Association.</p><p>Kuhn, Thomas S. 2012. <i>The Structure of Scientific Revolutions: 50th Anniversary Edition</i>. 4th edition. Chicago\u202f; London: University of Chicago Press.</p><p>Li, Xuhong, Haoyi Xiong, Xingjian Li, Xuanyu Wu, Xiao Zhang, Ji Liu, Jiang Bian, and Dejing Dou. 2022. \u201cInterpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond.\u201d <i>Knowledge and Information Systems</i>, 1\u201338.</p><p>Linden, Stanton J. 2003. <i>The Alchemy Reader: From Hermes Trismegistus to Isaac Newton</i>. Cambridge University Press.</p><p>McCarthy, John. 2008. \u201cThe Philosophy of AI and the AI of Philosophy.\u201d In <i>Philosophy of Information</i>, 711\u201340. Elsevier.</p><p>Ngo, Richard. 2022. \u201cThe Alignment Problem from a Deep Learning Perspective.\u201d <i>ArXiv Preprint ArXiv:2209.00626</i>.</p><p>Pinker, Steven. 2003. <i>The Blank Slate: The Modern Denial of Human Nature</i>. Penguin.</p><p>Popper, Karl. 2002. <i>The Logic of Scientific Discovery</i>. 2nd edition. London: Routledge.</p><p>Reisch, George A. 2016. \u201cAristotle in the Cold War: On the Origins of Thomas Kuhn\u2019s the Structure of Scientific Revolutions.\u201d <i>Kuhn\u2019s Structure of Scientific Revolutions at Fifty: Reflections on a Science Classic</i>, 12\u201330.</p><p>Rorty, Richard. 2003. \u201cDismantling Truth: Solidarity versus Objectivity.\u201d <i>The Theory of Knowledge: Classical and Contemporary Readings,</i>.</p><p>Russell, Stuart. 2019. <i>Human Compatible: Artificial Intelligence and the Problem of Control</i>. Penguin Books.</p><p>Russell, Stuart, and Peter Norvig. 2009. <i>Artificial Intelligence: A Modern Approach</i>. 3rd edition. Upper Saddle River: Pearson.</p><p>Sandberg, Anders. 2013. \u201cAn Overview of Models of Technological Singularity.\u201d <i>The Transhumanist Reader: Classical and Contemporary Essays on the Science, Technology, and Philosophy of the Human Future</i>, 376\u201394.</p><p>Searle, John. 1980. \u201cMinds, Brains, and Programs.\u201d <i>Behavioral and Brain Sciences</i> 3 (3): 417\u201357.</p><p>Sejnowski, Terrence J. 2018. <i>The Deep Learning Revolution</i>. Illustrated edition. Cambridge, Massachusetts: The MIT Press.</p><p>Taylor, Jessica, Eliezer Yudkowsky, Patrick LaVictoire, and Andrew Critch. 2016. \u201cAlignment for Advanced Machine Learning Systems.\u201d In <i>Ethics of Artificial Intelligence</i>, 342\u201382. Oxford University Press.</p><p>Thomas Kuhn. 2014. \u201cWhat Are Scientific Revolutions?\u201d In <i>Philosophy, Science, and History</i>, 71\u201388. Routledge.</p><p>Tomasik, Brian. 2011. \u201cRisks of Astronomical Future Suffering.\u201d <i>Foundational Research Institute: Berlin, Germany</i>.</p><p>Van\u010dik, Hrvoj. 2021. \u201cAlchemy.\u201d In <i>Philosophy of Chemistry</i>, 39\u201345. Springer.</p><p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. \u201cAttention Is All You Need.\u201d <i>Advances in Neural Information Processing Systems</i> 30.</p><p>Wei, Jason, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, and Donald Metzler. 2022. \u201cEmergent Abilities of Large Language Models.\u201d <i>ArXiv Preprint ArXiv:2206.07682</i>.</p><p>Wentworth, John. 2021. \u201cThe Plan.\u201d <i>LessWrong</i>, December. https://www.lesswrong.com/posts/3L46WGauGpr7nYubu/the-plan.</p><p>Yudkowsky, Eliezer. 2008. \u201cArtificial Intelligence as a Positive and Negative Factor in Global Risk.\u201d In <i>Global Catastrophic Risks</i>. Vol. 1. Oxford University Press</p><p>&nbsp;</p><p><a href=\"#_ftnref1\">[1]</a> Standford University recently introduced an \u201cIntroduction to AI Alignment\u201d course https://explorecourses.stanford.edu/search?view=catalog&amp;academicYear=&amp;page=0&amp;q=STS&amp;filter-departmentcode-STS=on&amp;filter-coursestatus-Active=on&amp;filter-term-Autumn=on and an \u201cAdvanced AI Alignment\u201d course for spring 2023&nbsp;<a href=\"https://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;catalog=&amp;q=STS+20SI%3A+Advanced+AI+Alignment&amp;collapse=\">https://explorecourses.stanford.edu/search?view=catalog&amp;filter-coursestatus-Active=on&amp;page=0&amp;catalog=&amp;q=STS+20SI%3A+Advanced+AI+Alignment&amp;collapse=</a> .&nbsp;</p><p><a href=\"#_ftnref2\">[2]</a> According to&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\">https://80000hours.org/problem-profiles/artificial-intelligence/</a>.&nbsp;</p>", "user": {"username": "eangelou"}}, {"_id": "eXwZEjGrx6JJgtEPG", "title": "Any Philosophy PhD recommendations for students interested in Alignment Efforts?", "postedAt": "2023-01-18T05:54:26.023Z", "htmlBody": "<p>I'm currently planning my Philosophy PhD applications.&nbsp;</p><p>If I'm interested in a traditional area of Philosophy (Epistemology, Ethics, Phil of Language), I can easily check recommendations from Philosophy Gourmet Reports, rely on testimonies from my tutors, or look at job placement data for existing PhD programs.</p><p>If I'm interested in working on alignment, the decision is far harder. It's a smaller field with less publicly available information. There are no \"ranking\" systems like PGR. My tutors are mostly epistemologists working on classic philosophy problems. It is far harder to estimate impact/alignment efforts as compared to estimating tenure chance by comparing job placement data.</p><p>&amp; PhD program choices will have great impacts on ones career in the short term. I can list out the mechanisms for this but I don't think this will be disputed.</p><p>So the question: Are there a comprehensive guide on this? If not, would people be interested in making one? Just to be clear, I'm not sure if a \"ranking\" system like GMR would be useful, but program recommendations would be really valuable.</p>", "user": {"username": "RickyHuang.hexuan"}}, {"_id": "bh2PKfwnwLL6KubRK", "title": "Appearance of impropriety", "postedAt": "2023-01-18T03:22:14.854Z", "htmlBody": "<p>I'm partial to \"appearance of impropriety\" as a standard of conduct for authorities and public figures.</p><p>For example, from <a href=\"https://www.uscourts.gov/judges-judgeships/code-conduct-united-states-judges#c\">the Code of Conduct for United States Judges</a>:</p><blockquote><p>An appearance of impropriety occurs when reasonable minds, with knowledge of all the relevant circumstances disclosed by a reasonable inquiry, would conclude that the judge\u2019s honesty, integrity, impartiality, temperament, or fitness to serve as a judge is impaired. Public confidence in the judiciary is eroded by irresponsible or improper conduct by judges, including harassment and other inappropriate workplace behavior. A judge must avoid all impropriety and appearance of impropriety. This prohibition applies to both professional and personal conduct. A judge must expect to be the subject of constant public scrutiny and accept freely and willingly restrictions that might be viewed as burdensome by the ordinary citizen.&nbsp;</p></blockquote><p>\"Impropriety\" in that context particularly concerns respect for law, \"lending the prestige of judicial office to advance the private interests of the judge or others\", and \"membership in any organization that practices invidious discrimination\", but different things might be emphasized in different professional contexts. Appearances undermine public confidence and give cover to genuine bad actors.</p><p>You might think of this in the spirit of <a href=\"https://forum.effectivealtruism.org/posts/CfcvPBY9hdsenMHCr/integrity-for-consequentialists-1\">integrity for consequentialists</a>, since short-sighted consequentialism is particularly susceptible to appearances of impropriety. As an example, this standard would recommend you (at the least) recuse yourself from reviewing a friend's grant proposal. For the most part, avoiding appearance of impropriety looks like standard professional ethics.</p><p>As a community standard, I think it's also valuable to pair this with a presumption of good faith. It's important that holding someone to this standard doesn't entail accusing them of impropriety. (Justifications for judicial appearance-of-impropriety standards hold this presumption so strongly they often don't even mention the idea of giving cover to bad actors, since they couldn't possibly exist.)</p>", "user": {"username": "Muireall"}}, {"_id": "Ny3v2Qe4LfaYJYKcq", "title": "Book critique of Effective Altruism", "postedAt": "2023-01-17T22:48:03.230Z", "htmlBody": "<p>Hello there! First post in the forum, so I apologize in advance for the probable mistakes &nbsp;and overall clumsiness. I have checked the forum writing guidelines but am pretty sure there's a high probability of my screwing up something or somewhere, so if that proves to be the case, \"<i>I am sure you have a waste basket handy\"</i>.</p><p>The case is, I was just checking Amazon today for some books on Effective Altruism with which to supplement the digital EA Handbook I am reading when I found this volume which will be made available exactly a month from now: <a href=\"https://www.amazon.com/-/en/dp/019765570X/\"><i>The Good It Promises, the Harm It Does: Critical Essays on Effective Altruism</i></a>, by Carol J. Adams, Lori Gruen and Alice Crary. I haven't seen any post mentioning it, and I thought it might be interesting to share.</p><p>As stated, the book hasn't been published yet, but one can look inside. I have been browsing the introduction, and in line with its title, it is pretty harsh in its wording. For example, from page xxv of the introduction:</p><blockquote><p>\"In addition to describing how EA can harm animals and humans, the book contains critical studies of EA's philosophical assumptions and critical studies of organizations that set out to realize them. It invites readers to recognize EA as an alluring and extremely pernicious ideology, and it traces out a number of mutually reinforcing strategies for submitting this ideology for criticism\".</p></blockquote><p>From the tone of the introduction I can suppose the general tone will be pretty scathing and hostile, as well as its general orientation. Still, I imagine the arguments it makes will profit from some attention, discussion and counterargument when it comes out.</p>", "user": {"username": "Manuel Del R\u00edo Rodr\u00edguez"}}, {"_id": "Jnnxhns29oK7xqdS4", "title": "Building a transhumanist future: 15 years of hplusroadmap, now Discord", "postedAt": "2023-01-17T22:03:51.240Z", "htmlBody": "<p># What is hplusroadmap?</p><p>We are a group chat, started 15 years ago, focusing on transhumanism, writing code, and hacking on biology. We do mad science. We build. We also have some funding available for sufficiently interesting projects.</p><p>Read more background here: https://diyhpl.us/wiki/hplusroadmap</p><p>We have some other items on the wiki, like the internet's largest collection of potential human genetic modifications: https://diyhpl.us/wiki/genetic-modifications/</p><p># Topics</p><p>Things we tend to focus around:</p><p>* do-it-yourself biology, genetic engineering, gene therapy, biohacking, synthetic biology<br>* cryonics<br>* molecular nanotechnology<br>* DNA synthesis machine development<br>* brain emulation, brain uploading, computational neuroscience<br>* nootropics<br>* molecular biology, polymerase, enzyme engineering, protein engineering, fusion proteins, directed evolution, gene-editing<br>* open-source hardware development<br>* self-replicating machines<br>* robots, mechanical automation, machining, electronics<br>* space colonization, habitats, mining, communication, transportation<br>* nutrition, bodybuilding<br>* computer architecture, supercomputing<br>* open-source software development<br>* transhumanism<br>* life extension, anti-aging, longevity research<br>* cryptography, bitcoin technology development<br>* artificial general intelligence, software AI<br>* breeding and selection projects</p><p># Who might be interested?</p><p>Anyone who wants to build the future by contributing to cool projects, proposing new projects, and sorting through the enormity of technologies now available.</p><p># Now on Discord</p><p>Today I am announcing the availability of a Discord bridge for hplusroadmap: https://twitter.com/kanzure/status/1615359557408260096</p>", "user": {"username": "kanzure"}}, {"_id": "SaRFsRDrneDgxNBbh", "title": "Recursive Middle Manager Hell", "postedAt": "2023-01-17T19:02:45.240Z", "htmlBody": "", "user": {"username": "Raemon"}}, {"_id": "foJhEZzG5sx9cQJZq", "title": "AGISF adaptation for in-person groups", "postedAt": "2023-01-17T18:33:39.050Z", "htmlBody": "<p>This past semester,<a href=\"https://haist.ai/\">&nbsp;<u>HAIST</u></a> and<a href=\"https://www.mitalignment.org/\">&nbsp;<u>MAIA</u></a> (the Harvard and MIT AI safety student groups) ran an adapted version of Richard Ngo's<a href=\"https://www.agisafetyfundamentals.com/ai-alignment-curriculum\"><u> AGI Safety Fundamentals alignment</u></a> curriculum. This adaptation \u2013 which consists of eight 2-hour long meetings, with all readings done during the meeting \u2013<a href=\"https://www.agisafetyfundamentals.com/alignment-insession-readings\">&nbsp;<strong><u>is now available on the AGISF website</u></strong></a>.</p><p>In this post, we discuss the adapted curriculum and its intended use, and&nbsp;<strong>we recommend that other in-person reading groups following AGISF use this adaptation</strong>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4nh361tk1d8\"><sup><a href=\"#fn4nh361tk1d8\">[1]</a></sup></span></p><h2>The adapted curriculum and its intended use</h2><p>The&nbsp;<a href=\"https://www.agisafetyfundamentals.com/alignment-insession-readings\"><u>adapted curriculum</u></a> was made by refining a slightly rustier first adaptation, with significant help from Richard Ngo and feedback from participants. The key differences between the adapted curriculum and the<a href=\"https://www.agisafetyfundamentals.com/ai-alignment-curriculum\">&nbsp;<u>mainline AGISF alignment curriculum</u></a> are:</p><ul><li><strong>Participants do all the core readings during the meeting; no reading is required in between meetings</strong>.</li><li>Participants meet for 2 hours per week instead of 1.5.</li><li>Readings, including further readings, tend to be more bite-sized (usually not longer than 20 minutes).</li><li>There are no projects, and certain topics are omitted (e.g. governance and inverse reinforcement learning).</li></ul><p>The way that HAIST and MAIA used this curriculum, and the way we recommend other groups use it, is:</p><ul><li><strong>Alternate between silent reading and discussion</strong>.<ul><li>So a typical meeting might look like: people arrive, everyone does reading 1, everyone discusses reading 1, everyone does reading 2, everyone discusses reading 2, etc.</li><li>With certain longer or more difficult readings (e.g. Toy models of superposition), it could be reasonable to occasionally pause for discussion in the middle of the reading.</li></ul></li><li><strong>Encourage faster readers to take a look at the further readings</strong> while they wait for others to catch up.<ul><li>We found that reading speeds varied&nbsp;<i>significantly</i>, with slower readers taking ~1.5x as long to finish as faster readers.</li><li>This works especially well if the readings are printed (which we recommend doing).</li></ul></li></ul><p>We note that this format introduces some new challenges, especially when there are slower readers.</p><ul><li><strong>Facilitators need to manage discussion timing</strong> since discussions that go too long cut into time for reading and discussing other material.<ul><li>Planning out how long to spend discussing each core reading ahead of time can be very useful.</li><li>Facilitators should feel comfortable cutting off discussions to make sure there\u2019s time to read and discuss all the core readings. (On the other hand, if a discussion is very productive, it may be worth skipping certain readings; this is a judgment call that facilitators will need to make.)</li></ul></li><li><strong>Different reading speeds need to be managed.</strong><ul><li>At HAIST, we typically found it feasible to wait for the slowest reader to finish reading. We printed copies of the further readings for faster readers to peruse while they waited for others to finish.</li><li>On the other hand, this might not work well for groups with especially slow readers. In these cases, you may need to begin discussions before everyone is done reading and, going forward, encourage slower readers to take a look at the core readings ahead of future meetings.</li></ul></li></ul><p>To help with some of these challenges, Sam prepared a guide for HAIST and MAIA facilitators that included recommended discussion times, points of discussion, and advice about which readings to cut if necessary. That facilitator guide was for an outdated version of the curriculum, but we hope to have an updated facilitator guide in the next few weeks. We don\u2019t want to make these public, but feel free to reach out to&nbsp;<a href=\"mailto:smarks@math.harvard.edu\"><u>smarks@math.harvard.edu</u></a> if you\u2019re running a reading group and are interested in seeing the old or forthcoming facilitator guides.</p><h2>Why we recommend the adapted curriculum</h2><p>Sam and Xander generally felt that the in-sessions reading format worked better than the take-home readings format, which HAIST used for an AGISF reading group in spring 2022. In particular:</p><ul><li><strong>Reading comprehension generally seemed higher</strong>, possibly because take-home readings led to participants rushing through readings before meetings, or possibly because participants could ask their facilitator about core confusions they had at the start of readings.</li><li><strong>Discussions generally went better</strong>, possibly because readings were fresh on participants\u2019 minds.</li><li><strong>Participants seemed more energetic and engaged</strong>, possibly because alternating between silent reading and discussion is more engaging than long discussion blocks.</li></ul><p>Empirically, participants also thought that HAIST reading groups went well. Participants gave the program an overall rating of 8.6 on a 0-10 scale, and when asked \u201cTo what extent are you considering a career in AI safety?\u201d the average response was 7.4, up from 4.7 before the program (though part of this movement was likely due to selection effects<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflkn5rf90k8\"><sup><a href=\"#fnlkn5rf90k8\">[2]</a></sup></span>).&nbsp;</p><p>Of course, the curriculum isn\u2019t the only factor impacting how well a reading group goes. We note that&nbsp;<i>smooth operations</i>,&nbsp;<i>facilitator quality</i>, <i>a high admissions bar</i>, and&nbsp;<i>printing the readings&nbsp;</i>(as an alternative to reading on devices) also seemed important for things going well at HAIST.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefeepgir3aa6t\"><sup><a href=\"#fneepgir3aa6t\">[3]</a></sup></span></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4nh361tk1d8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4nh361tk1d8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We don\u2019t know whether the in-session readings format would work well for reading groups that meet virtually. If anyone experiments with this, we\u2019d be very interested in hearing how it goes; you can contact Sam at&nbsp;<a href=\"mailto:smarks@math.harvard.edu\"><u>smarks@math.harvard.edu</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlkn5rf90k8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflkn5rf90k8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The start-of-program and end-of-program surveys had response rates around 45% and 65%, respectively, with an attrition rate of around 40%.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fneepgir3aa6t\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefeepgir3aa6t\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See the <a href=\"https://www.lesswrong.com/posts/LShJtvwDf4AMo992L/update-on-harvard-ai-safety-team-and-mit-ai-alignment#What_worked\">HAIST update post</a> for some relevant advice.</p></div></li></ol>", "user": {"username": "Sam Marks"}}, {"_id": "KmmDHAhPmzFDFsdB4", "title": "Collin Burns on Alignment Research And Discovering Latent Knowledge Without Supervision", "postedAt": "2023-01-17T17:21:40.749Z", "htmlBody": "", "user": {"username": "mtrazzi"}}, {"_id": "AGhrkv7Ha6giWon7Z", "title": "Lessons learned and review of the AI Safety Nudge Competition", "postedAt": "2023-01-17T17:13:23.163Z", "htmlBody": "<p><strong>TL;DR: We ran a competition to encourage people to stop procrastinating on their AI Safety projects. We had 76 applicants out of which over 40% (31 participants) completed their goal by the end of October and were added to a draw to win a monetary prize. The vast majority of the participants that completed their goals found this competition to be useful. We learned that marketing is paramount for successfully running such a competition, that competitions of this kind would potentially be better run at EAG events and that they could be a good way of getting more people to subscribe to EA newsletters.</strong></p><p><strong>Introduction</strong></p><p>The<a href=\"https://forum.effectivealtruism.org/posts/c5SeLNpnHNNif6Doz/announcing-the-ai-safety-nudge-competition-to-help-beat\">&nbsp;<u>AI Safety Nudge Competition</u></a> aims to encourage people to do things related to AI Safety today instead of procrastinating it into the future by allowing them to enter into a draw if they complete the goal that they set for themselves.</p><p>Participants defined a specific goal for themselves, examples include:</p><p><strong>\u2022&nbsp;</strong>Finish reading Superintelligence</p><p><strong>\u2022&nbsp;</strong>Finish writing up a relevant blog post</p><p><strong>\u2022&nbsp;</strong>Organise a local dinner for people interested in AI Safety</p><p>If they completed the goal they set out for themselves, they were entered into the draw:</p><p><strong>\u2022&nbsp;</strong>ten prizes of $100</p><p><strong>\u2022&nbsp;</strong>two prizes of $100 specifically for Australia and New Zealand</p><p>You can see the list of winners here.</p><p><strong>Downside risks</strong></p><p>We started by asking the applicants if they think their project could potentially have downside risks such as:</p><p><strong>\u2022&nbsp;</strong>Outreach to famous people, politicians, the media, children, high-net worth individuals, top AI researchers</p><p><strong>\u2022&nbsp;</strong>Projects that could be controversial, come with significant down-side risks or could produce negative PR</p><p><strong>\u2022&nbsp;</strong>AI Safety projects with high capabilities externalities</p><p>Only 1.3% of the applicants were uncertain about the downside risks of their project and the vast majority self-reported to be certain their project doesn\u2019t fall in this category.&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1673975604/mirroredImages/AGhrkv7Ha6giWon7Z/frvwfe05frjiw8to3ql9.png\" alt=\"Forms response chart. Question title: Downside risks. Number of responses: 76 responses.\"></p><p><strong>Counterfactual impact</strong></p><p>We continued by asking applicants to rate from a scale from 0 to 10 how likely they thought they were able to achieve their goal by the end of October if they did and did not enter this competition to gauge how strong of a nudge they thought the competition would offer. We made it clear that this was only for informational purposes and that it did not affect their application.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1673975604/mirroredImages/AGhrkv7Ha6giWon7Z/fajzqrcojlxha1snspzi.png\" alt=\"Forms response chart. Question title: On a scale from 0 to 10, how likely would you be to achieve your goal by the end of October&nbsp;if you enter this competition?. Number of responses: 76 responses.\"></p><p>Average: 7.73</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1673975604/mirroredImages/AGhrkv7Ha6giWon7Z/jknargqtyzd1nwvnvinx.png\" alt=\"Forms response chart. Question title: On a scale from 0 to 10, how likely would you to be to achieve your goal by the end of October&nbsp;if you did not enter this competition?. Number of responses: 76 responses.\"></p><p>Average: 4.21<br>&nbsp;</p><p>The average response to how likely they think they would be to achieve their goal by the end of October if they entered the competition is almost 2 times bigger (1.83x) than the average response if they wouldn\u2019t enter the competition.</p><p>&nbsp;</p><p>We asked the same questions in the form they had to complete after they finished their project to be added to the prize draw to compare the results.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1673975604/mirroredImages/AGhrkv7Ha6giWon7Z/pvc0itvqqc9avix2vjbk.png\" alt=\"Forms response chart. Question title: On a scale from 0 to 10, how much more likely do you think joining the competition made you to achieve your goal?. Number of responses: 31 responses.\"></p><p>Average: 7.03</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1673975604/mirroredImages/AGhrkv7Ha6giWon7Z/yp16hyblvwiezhuk7ojc.png\" alt=\"Forms response chart. Question title: On a scale from 0 to 10, how likely were you to achieve your goal by the end of October&nbsp;if you did not enter this competition?. Number of responses: 31 responses.\"></p><p>Average: 4.54</p><p>&nbsp;</p><p>The average response to how likely they think they would have been to achieve their goal by the end of October if they entered the competition is 1.54 times bigger than the average response if they wouldn\u2019t have entered the competition.</p><p>&nbsp;</p><p>These results indicate that the participants that finished their project found this competition to be useful for achieving their goal.<br>&nbsp;</p><p>Participants who didn\u2019t finish their project in time were encouraged to fill in a form to provide more details.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1673975604/mirroredImages/AGhrkv7Ha6giWon7Z/pgsjafb794fwtzsn7sx1.png\" alt=\"Forms response chart. Question title: Why do you think you didn't manage to finish your goal by the end of October?. Number of responses: 9 responses.\"></p><p>Most of the people that filled in that form mentioned procrastination (33.3%) as the main reason they didn\u2019t manage to finish their project in time while the second biggest reason (22.2%) is that they were busier than they expected.</p><p>&nbsp;</p><p>The same questions related to the expected counterfactual impact of entering the competition were posed to the participants that didn\u2019t finish their project.&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1673975604/mirroredImages/AGhrkv7Ha6giWon7Z/jx34aua0qv1iyvcv6hl1.png\" alt=\"Forms response chart. Question title: On a scale from 0 to 10, how much more likely do you think joining the competition made you to achieve your goal?. Number of responses: 9 responses.\"></p><p>Average: 5.11</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1673975604/mirroredImages/AGhrkv7Ha6giWon7Z/s2wbnamikgnl4rfjndqq.png\" alt=\"Forms response chart. Question title: On a scale from 0 to 10, how likely were you to achieve your goal by the end of October&nbsp;if you did not enter this competition?. Number of responses: 9 responses.\"></p><p>Average: 1.44</p><p>The average response to how likely they think they would have been to achieve their goal by the end of October if they entered the competition is over 3 and a half times bigger (3.54x) than the average response if they wouldn\u2019t have entered the competition.</p><p>&nbsp;</p><p>We also asked them how we could improve:</p><ul><li>More frequent reminders, maybe pairing people up for goal-buddies &amp; weekly 15m check-ins -- would have made me more accountable</li><li>Maybe more frequent reminders? Even though I ultimately slowly gave up on my goal, every time I received an email from you, I had a slight boost in motivation.</li><li>I think the single reminder email was good!</li></ul><p>&nbsp;</p><p><strong>Newsletter&nbsp;</strong></p><p>At the end of the registration form we asked the applicants if they would like to subscribe to our newsletter and 60.5% responded positively.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1673975604/mirroredImages/AGhrkv7Ha6giWon7Z/npkm0hnnw1ock9xzvhoq.png\" alt=\"Forms response chart. Question title: Do you wish to subscribe to our newsletter to hear about our other activities?\n\n. Number of responses: 76 responses.\"></p><p>This indicates that running competitions of this kind could also be a very useful way of getting subscribers to EA newsletters.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrh3ijor8ybc\"><sup><a href=\"#fnrh3ijor8ybc\">[1]</a></sup></span></p><p>&nbsp;</p><p><strong>Marketing</strong></p><p>We underestimated the importance of marketing for a competition of this kind as we announced the competition before we had the marketing materials ready and this resulted in a slow start. As a result of this, we quickly created a poster and a pitch for the competition and shared it in various AI Safety groups on Slack, Facebook, Discord and Twitter, after which more people started applying. Running&nbsp; a competition like this could have a wider impact if it is announced at EAG events or after big EA book launches.<br>&nbsp;</p><p>We sent an email in the middle of October and another one a week before the end of October with science-backed productivity tips and a reminder for the participants to complete their project.<br>&nbsp;</p><p><strong>Main lessons</strong></p><ul><li>Prepare marketing materials before launch&nbsp;</li><li>Announce the competition earlier (for ex newsletters)</li><li>Look for big EA events which could bring in a lot participants</li></ul><p>-What We Owe the Future/other big EA books launches</p><p>-EAG conferences</p><ul><li>This competition could be scalable and reproducible (create a template)</li></ul><p>-Google folder with files that people can copy<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnigxzsw3myr\"><sup><a href=\"#fnnigxzsw3myr\">[2]</a></sup></span></p><p>-Documents with advice on how to run it&nbsp;</p><p>-Possibly an Asana template</p><ul><li>Make an EA forum post on the competition at the end&nbsp;</li><li>If you are a small student group maybe running a competition over the summer could be a cost-effective path to impact</li><li>This type of competition could keep people engaged over the summer break if you don\u2019t have much organizing capacity&nbsp;</li><li>Some variants could possibly be more scalable and reproducible (as they do not rely on the honour system):</li></ul><p>-Blog post nudge competition&nbsp;</p><p>-Audible/book reading nudge competition&nbsp;</p><p>-AGISF nudge competition (ask for the email of the facilitator for confirmation)</p><p>&nbsp;</p><p><strong>Conclusion</strong></p><p>Overall, the AI Safety Nudge Competition showed promise in nudging people to complete their AI Safety projects but competitions like this could have a wider impact with more focus on marketing. We created a folder with all of the forms, documents and email templates we used so that other people could easily run a competition of this kind. Feel free to contact us if you want access.</p><p><br><br><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrh3ijor8ybc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrh3ijor8ybc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Some people who indicated that they wish to join our newsletter were already subscribed but forgot that they were subscribed.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnigxzsw3myr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnigxzsw3myr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We wish to note that we do have such a folder.</p></div></li></ol>", "user": {"username": "marc"}}, {"_id": "54vAiSFkYszTWWWv4", "title": "Doing EA Better", "postedAt": "2023-01-17T20:09:41.136Z", "htmlBody": "<h1>Preamble</h1>\n<p>It\u2019s been a rough few months, hasn\u2019t it?</p>\n<p>Recent events, including the <a href=\"https://forum.effectivealtruism.org/topics/ftx-collapse\">FTX collapse</a> and the <a href=\"https://twitter.com/anderssandberg/status/1613259477263720449?cxt=HHwWgoCxjc-8ueMsAAAA\">Bostrom email/apology scandal</a>, have led a sizeable portion of EAs to become disillusioned with or at least much more critical of the Effective Altruism movement.</p>\n<p>While the current crises have made some of our movement\u2019s problems more visible and acute, many EAs have become increasingly worried about the direction of EA over the last few years. We are some of them.</p>\n<p>This document was written collaboratively, with contributions from ~10 EAs in total. Each of us arrived at most of the critiques below independently before realising through conversation that we were not \u201cthe only one\u201d. In fact, many EAs thought similarly to us, or at least were very easily convinced once thoughts were (privately) shared.</p>\n<p>Some of us started to become concerned as early as 2017, but the discussions that triggered the creation of this post happened in the summer of 2022. Most of this post was written by the time of the FTX crash, and the final draft was completed the very day that the Bostrom email scandal broke.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-1\" id=\"fnref-pMAQ8odjQviWs2qYJ-1\">[1]</a></sup> Thus, <strong>a separate post will be made about the Bostrom/FLI issues in around a week.</strong></p>\n<p>A lot of what we say is relevant to the FTX situation, and some of it isn\u2019t, at least directly. In any case, it seems clear to us that the FTX crisis significantly strengthened our arguments.</p>\n<p>We reached the point where we would feel collectively irresponsible if we did not voice our concerns some time ago, and now seems like the time where those concerns are most likely to be taken seriously. We voice them in the hope that we can change our movement for the better, and have taken pains to avoid coming off as \u201chostile\u201d in any way.</p>\n<p>Experience indicates that it is likely many EAs will agree with significant proportions of what we say, but have not said as much publicly due to the significant risk doing so would pose to their careers, access to EA spaces, and likelihood of ever getting funded again.</p>\n<p>Naturally the above considerations also apply to us: we are anonymous for a reason.</p>\n<p>This post is also <s>quite</s> very long, so each section has a summary at the top for ease of scanning, and we\u2019ll break this post up into a sequence to facilitate object-level discussion.</p>\n<p>Finally, we ask that people upvote or downvote this post on the basis of whether they believe it to have made a useful contribution to the conversation, rather than whether they agree with all of our critiques.</p>\n<h1>Summary</h1>\n<ul>\n<li>The Effective Altruism movement has rapidly grown in size and power, and we have a responsibility to ensure that it lives up to its goals</li>\n<li>EA is too homogenous, hierarchical, and intellectually insular, with a hard core of \u201corthodox\u201d thought and powerful barriers to \u201cdeep\u201d critiques</li>\n<li>Many beliefs accepted in EA are surprisingly poorly supported, and we ignore entire disciplines with extremely relevant and valuable insights</li>\n<li>Some EA beliefs and practices align suspiciously well with the interests of our donors, and some of our practices render us susceptible to conflicts of interest</li>\n<li>EA decision-making is highly centralised, opaque, and unaccountable, but there are several evidence-based methods for improving the situation</li>\n</ul>\n<h1>Introduction</h1>\n<p>As committed Effective Altruists, we have found meaning and value in the frameworks and pragmatism of the Effective Altruism movement. We believe it is one of the most effective broadly-focused social movements, with the potential for world-historical impact.</p>\n<p>Already, the impact of many EA projects has been <a href=\"https://www.fishwelfareinitiative.org/\">considerable</a> and <a href=\"https://leadelimination.org/\">inspiring</a>. We appreciate the openness to criticism found in various parts of the EA community, and believe that EA has the potential to avoid the pitfalls faced by many other movements by updating effectively in response to new information.</p>\n<p>We have become increasingly concerned with significant aspects of the movement over our collective decades here, and while the FTX crisis was a shock to all of us, we had for some time been unable to escape the feeling that something was going to go horribly wrong.</p>\n<p>To ensure that EA has a robustly positive impact, we feel the need to identify the aspects of our movement that we find concerning, and suggest directions for reform that we believe have been neglected. These fall into three major categories:</p>\n<ol>\n<li>Epistemics</li>\n<li>Expertise &amp; Rigour</li>\n<li>Governance &amp; Power</li>\n</ol>\n<p>We do not believe that the critiques apply to everyone and to all parts of EA, but to certain \u2013 often influential \u2013 subparts of the movement. Most of us work on existential risk, so the majority of our examples will come from there.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-2\" id=\"fnref-pMAQ8odjQviWs2qYJ-2\">[2]</a></sup></p>\n<p>Not all of the ~10 people that helped to write this post agree with all the points made within, both in terms of \u201cgoes too far\u201d and \u201cdoesn\u2019t go far enough\u201d. <strong>It is entirely possible to strongly reject one or more of our critiques while accepting others.</strong></p>\n<p>In the same vein, we request that commenters focus on the high-level critiques we make, rather than diving into hyper-specific debates about one thing or another that we cited as an example.</p>\n<p>Finally, this report started as a dozen or so bullet points, and currently stands at over 20,000 words. We wrote it out of love for the community, and we were not paid for any of its writing or research despite most of us either holding precarious grant-dependent gig jobs or living on savings while applying for funding. We had to stop somewhere. This means that many of the critiques we make could be explored in far, far more detail than their rendition here contains.</p>\n<p>If you think a point is underdeveloped, we probably agree; we would love to see others take the points we make and explore them in greater depth, and indeed to do so ourselves if able to do so while also being able to pay rent.</p>\n<p>We believe that the points we make are vital for the epistemic health of the movement, that they will make it more accessible and effective, and that they will enhance the ability of EA as a whole to do the most good.</p>\n<p>Two Notes:</p>\n<ol>\n<li>Some of the issues we describe are based on personal experience and thus cannot be backed by citations. If you doubt something we assert, let us know and we\u2019ll give as much detail as we can without compromising our anonymity or that of others. You can also just ask around: we witnessed most of the things we mention on multiple independent occasions, so they\u2019re probably not rare.</li>\n<li>This post ties a lot of issues together and is thus necessarily broad, so we will have to make some generalisations, to which there will be exceptions.</li>\n</ol>\n<h1>Epistemics</h1>\n<h2>Epistemic health is a community issue</h2>\n<p><strong>Summary:</strong> The Collective Intelligence literature suggests epistemic communities should be diverse, egalitarian, and open to a wide variety of information sources. EA, in contrast, is relatively homogenous, hierarchical, and insular. This puts EA at serious risk of epistemic blind-spots.</p>\n<p>EA highly values epistemics and has a stated ambition of predicting existential risk scenarios. We have a reputation for assuming that we are the \u201csmartest people in the room\u201d.</p>\n<p>Yet, we appear to have been blindsided by the FTX crash. <a href=\"https://marginalrevolution.com/marginalrevolution/2022/11/a-simple-point-about-existential-risk.html?utm_source=substack&amp;utm_medium=email\">As Tyler Cowen puts it</a>:</p>\n<blockquote>\n<p>Hardly anyone associated with Future Fund saw the existential risk to\u2026 Future Fund, even though they were as close to it as one could possibly be.</p>\n<p>I am thus skeptical about their ability to predict existential risk more generally, and for systems that are far more complex and also far more distant. And, it turns out, many of the real sources of existential risk boil down to hubris and human frailty and imperfections (the humanities remain underrated). When it comes to existential risk, I generally prefer to invest in talent and good institutions, rather than trying to fine-tune predictions about existential risk itself.</p>\n<p>If EA is going to do some lesson-taking, I would not want this point to be neglected.</p>\n</blockquote>\n<p>So, what\u2019s the problem?</p>\n<p>EA\u2019s focus on epistemics is almost exclusively directed towards individualistic issues like minimising the impact of cognitive biases and cultivating a <a href=\"https://www.goodreads.com/book/show/42041926-the-scout-mindset\">Scout Mindset</a>. The movement strongly emphasises intelligence, both in <a href=\"https://forum.effectivealtruism.org/posts/x9Rn5SfapcbbZaZy9/ea-for-dumb-people?commentId=etsEdiymrgNGZa2nv\">general</a> and especially that of <a href=\"https://forum.effectivealtruism.org/posts/DxfpGi9hwvwLCf5iQ/objections-to-value-alignment-between-effective-altruists#Intelligence\">particular</a> \u201cthought-leaders\u201d. An epistemically healthy community seems to be created by acquiring maximally-rational, intelligent, and knowledgeable individuals, with social considerations given second place. Unfortunately, the science does not bear this out. The quality of an epistemic community does not boil down to the de-biasing and training of individuals;<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-3\" id=\"fnref-pMAQ8odjQviWs2qYJ-3\">[3]</a></sup> more important factors appear to be the community\u2019s composition, its socio-economic structure, and its cultural norms.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-4\" id=\"fnref-pMAQ8odjQviWs2qYJ-4\">[4]</a></sup></p>\n<p>The field of Collective Intelligence provides guidance on the traits to nurture if one wishes to build a collectively intelligent community. <a href=\"https://www.goodreads.com/book/show/62228295-joined-up-thinking\">For</a> <a href=\"https://arxiv.org/abs/2205.03300\">example</a>:</p>\n<ul>\n<li>Diversity\n<ul>\n<li>Along essentially all dimensions, from cultural background to disciplinary/professional training to cognition style to age</li>\n</ul>\n</li>\n<li>Egalitarianism\n<ul>\n<li>People must feel able to speak up (and must be listened to if they do)</li>\n<li>Dominance dynamics amplify biases and steer groups into suboptimal path dependencies</li>\n<li>Leadership is typically <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Deciding_together_better\">best employed</a> on a rotating basis for discussion-facilitation purposes rather than top-down decision-making</li>\n<li>Avoid appeals and deference to community authority</li>\n</ul>\n</li>\n<li>Openness to a wide variety of sources of information</li>\n<li>Generally high levels of social/emotional intelligence\n<ul>\n<li>This is often more important than individuals\u2019 skill levels at the task in question</li>\n</ul>\n</li>\n</ul>\n<p>However, the social epistemics of EA leave much to be desired. As we will elaborate on below, EA:</p>\n<ul>\n<li>Is mostly comprised of people with very <a href=\"https://rethinkpriorities.org/ea-survey\">similar</a> demographic, cultural, and educational backgrounds</li>\n<li>Places too much trust in (powerful) leadership figures</li>\n<li>Is remarkably intellectually insular</li>\n<li>Confuses value-alignment and seniority with expertise</li>\n<li>Is vulnerable to motivated reasoning</li>\n<li>Is susceptible to conflicts of interest</li>\n<li>Has powerful structural barriers to raising important categories of critique</li>\n<li>Is susceptible to groupthink</li>\n</ul>\n<p>Decision-making structures and intellectual norms within EA must therefore be improved upon.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-5\" id=\"fnref-pMAQ8odjQviWs2qYJ-5\">[5]</a></sup></p>\n<h2>What actually is \u201cvalue-alignment\u201d?</h2>\n<p><strong>Summary:</strong> The use of the term \u201cvalue-alignment\u201d in the EA community hides an implicit community orthodoxy. When people say \u201cvalue-aligned\u201d they typically do not mean a neutral \u201calignment of values\u201d, nor even \u201cagreement with the goal of doing the most good possible\u201d, but a commitment to a particular package of views. This package, termed \u201cEA orthodoxy\u201d, includes effective altruism, longtermism, utilitarianism, Rationalist-derived epistemics, liberal-technocratic philanthropy, Whig historiography, the ITN framework, and the Techno-Utopian Approach to existential risk.</p>\n<p>The term \u201cvalue-alignment\u201d gets thrown around a lot in EA, but is rarely actually defined. When asked, people typically say something about similarity or complementarity of values or worldviews, and this makes sense: \u201cvalue-alignment\u201d is of course a term defined in reference to what values the subject is (un)aligned with. You could just as easily speak of alignment with the values of a political party or a homeowner\u2019s association.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-6\" id=\"fnref-pMAQ8odjQviWs2qYJ-6\">[6]</a></sup></p>\n<p>However, the term\u2019s usage in EA spaces typically has an implicit component: value-alignment <em>with</em> a set of views shared and promoted by the most established and powerful components of the EA community. Thus:</p>\n<ul>\n<li>\n<p>Value-alignment = the degree to which one subscribes to EA orthodoxy</p>\n</li>\n<li>\n<p>EA orthodoxy = the package of beliefs and sensibilities generally shared and promoted by EA\u2019s core institutions (the CEA, FHI, OpenPhil, etc.)<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-7\" id=\"fnref-pMAQ8odjQviWs2qYJ-7\">[7]</a></sup></p>\n<ul>\n<li>These include, but are not limited to:\n<ul>\n<li>Effective Altruism\n<ul>\n<li>i.e. trying to \u201cdo the most good possible\u201d</li>\n</ul>\n</li>\n<li><a href=\"https://www.effectivealtruism.org/articles/longtermism\">Longtermism</a>\n<ul>\n<li>i.e. believing that positively influencing the long-term future is a (or even <em><a href=\"https://globalprioritiesinstitute.org/hilary-greaves-william-macaskill-the-case-for-strong-longtermism-2/\">the</a></em>) key moral priority of our time</li>\n</ul>\n</li>\n<li>Utilitarianism, usually Total Utilitarianism</li>\n<li><a href=\"https://www.lesswrong.com/tag/rationalist-movement\">Rationalist</a>-<a href=\"https://rationalwiki.org/wiki/LessWrong\">derived</a> epistemics\n<ul>\n<li>Most notably subjective <a href=\"https://forum.effectivealtruism.org/posts/Mzc2ZP48KPF45t4bQ/bayesian-mindset\">Bayesian</a> \u201cupdating\u201d of personal beliefs</li>\n</ul>\n</li>\n<li>Liberal-technocratic philanthropy</li>\n<li>A broadly <a href=\"https://en.wikipedia.org/wiki/Whig_history\">Whiggish</a>/<a href=\"https://starexponent.com/opinion/column-the-progressive-theory-of-history/article_f6fccfcd-2a70-5b72-8157-b069d2b4bef9.html\">progressivist</a> view of history\n<ul>\n<li>Best exemplified by Steven Pinker\u2019s \u201c<a href=\"https://www.goodreads.com/book/show/35696171-enlightenment-now\">Enlightenment Now</a>\u201d</li>\n</ul>\n</li>\n<li>Cause-prioritisation according to the <a href=\"https://forum.effectivealtruism.org/topics/itn-framework\">ITN framework</a></li>\n<li>The <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3995225\">Techno-Utopian Approach</a> to existential risk, which includes for instance, and in addition to several of the above:\n<ul>\n<li>\n<p>Defining \u201cexistential risk\u201d in reference to humanity\u2019s \u201clong-term potential\u201d to generate immense amounts of (utilitarian) value by populating the cosmos with vast numbers of extremely technologically advanced beings</p>\n</li>\n<li>\n<p>A methodological framework based on categorising individual \u201crisks\u201d<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-8\" id=\"fnref-pMAQ8odjQviWs2qYJ-8\">[8]</a></sup>, estimating for each a probability of causing an \u201cexistential catastrophe\u201d within a given timeframe, and attempting to reduce the overall level of existential risk largely by working on particular \u201crisks\u201d in isolation (usually via technical or at least technocratic means)</p>\n</li>\n<li>\n<p>Technological determinism, or at least a \u201c<a href=\"https://journals.sagepub.com/doi/abs/10.1177/0162243915579283\">military-economic adaptationism</a>\u201d that is often underpinned by an implicit commitment to <a href=\"https://en.wikipedia.org/wiki/Neorealism_(international_relations)\">neorealist</a> international relations theory</p>\n</li>\n<li>\n<p>A willingness to seriously consider extreme or otherwise exceptional actions to protect astronomically large amounts of perceived future value</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>There will naturally be exceptions here \u2013 institutions employ many people, whose views can change over time \u2013 but there are nonetheless clear regularities</li>\n</ul>\n</li>\n</ul>\n<p>Note that few, if any, of the components of orthodoxy are necessary aspects, conditions, or implications of the overall goal of \u201cdoing the most good possible\u201d. It is possible to be an effective altruist without subscribing to all, or even any, of them, with the obvious exception of \u201ceffective altruism\u201d itself.</p>\n<p>However, when EAs say \u201cvalue-aligned\u201d they rarely seem to mean that one is simply \u201cdedicated to doing the most good possible\u201d, but that one subscribes to the particular philosophical, political, and methodological views packaged under the umbrella of orthodoxy.</p>\n<h2>We are incredibly homogenous</h2>\n<p><strong>Summary:</strong> Diverse communities are typically much better at accurately analysing the world and solving problems, but EA is extremely homogenous along essentially all dimensions. EA institutions and norms actively and strongly select <em>against</em> diversity. This provides short-term efficiency at the expense of long-term epistemic health.</p>\n<p>The EA community is notoriously <a href=\"https://forum.effectivealtruism.org/posts/DxfpGi9hwvwLCf5iQ/objections-to-value-alignment-between-effective-altruists#Homogeneity\">homogenous</a>, and the \u201caverage EA\u201d is extremely easy to imagine: he is a white male<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-9\" id=\"fnref-pMAQ8odjQviWs2qYJ-9\">[9]</a></sup> in his twenties or thirties from an upper-middle class family in North America or Western Europe. He is ethically utilitarian and politically centrist; an atheist, but culturally protestant. He studied analytic philosophy, mathematics, computer science, or economics at an elite university in the US or UK. He is neurodivergent. He thinks space is really cool. He highly values intelligence, and believes that his own is significantly above average. He hung around LessWrong for a while as a teenager, and now wears EA-branded shirts and hoodies, drinks Huel, and consumes a narrow range of blogs, podcasts, and vegan ready-meals. He moves in particular ways, talks in particular ways, and thinks in particular ways. Let us name him \u201cSam\u201d, if only because there\u2019s a solid chance he already is.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-10\" id=\"fnref-pMAQ8odjQviWs2qYJ-10\">[10]</a></sup></p>\n<p>Even leaving aside the <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3995225\">ethical and political issues</a> surrounding major decisions about humanity\u2019s future being made by such a small and homogenous group of people, especially given the fact that the poor of the Global South will suffer most in almost any conceivable catastrophe, having the EA community overwhelmingly populated by Sams or near-Sams is decidedly Not Good for our collective epistemic health.</p>\n<p>As <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Epistemic_health_is_a_community_issue\">noted</a> above, <a href=\"https://www.goodreads.com/book/show/62228295-joined-up-thinking\">diversity is one of the main predictors of the collective intelligence of a group</a>. If EA wants optimise its ability to solve big, complex problems like the ones we focus on, we need people with different disciplinary backgrounds<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-11\" id=\"fnref-pMAQ8odjQviWs2qYJ-11\">[11]</a></sup>, different kinds of professional training, different kinds of talent/intelligence<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-12\" id=\"fnref-pMAQ8odjQviWs2qYJ-12\">[12]</a></sup>, different ethical and political viewpoints, different temperaments, and different life experiences. That\u2019s where new ideas tend to come from.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-13\" id=\"fnref-pMAQ8odjQviWs2qYJ-13\">[13]</a></sup></p>\n<p>Worryingly, EA institutions seem to select <em>against</em> diversity. Hiring and funding practices often select for highly value-aligned yet inexperienced individuals over outgroup experts, university recruitment drives are deliberately targeted at the Sam Demographic (at least by proxy) and EA organisations are advised to maintain a high level of internal value-alignment to maximise operational efficiency. The 80,000 Hours website seems purpose-written for Sam, and is noticeably uninterested in people with humanities or social sciences backgrounds,<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-14\" id=\"fnref-pMAQ8odjQviWs2qYJ-14\">[14]</a></sup> or those without university education. Unconscious bias is also likely to play a role here \u2013 it does everywhere else.</p>\n<p>The vast majority of EAs will, when asked, say that we should have a more diverse community, but in that case, why are only a very narrow spectrum of people given access to EA funding or EA platforms? There are exceptions, of course, but the trend is clear.</p>\n<p>It\u2019s worth mentioning that <a href=\"https://library.oapen.org/handle/20.500.12657/42728\">senior EAs</a> have done some interesting work on moral uncertainty and value-pluralism, and we think several of their recommendations are well-taken. However, the focus is firmly on individual rather than collective factors. The point remains that one cannot substitute a philosophically diverse community for an overwhelmingly utilitarian one where everyone individually tries to keep all possible viewpoints in mind. None of us are so rational as to obviate true diversity through our own thoughts.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-15\" id=\"fnref-pMAQ8odjQviWs2qYJ-15\">[15]</a></sup></p>\n<h2>EA is very open to some kinds of critique and very not open to others</h2>\n<p><strong>Summary:</strong> EA is very open to shallow critiques, but not deep critiques. Shallow critiques are small technical adjustments written in ingroup language, whereas deep critiques hint at the need for significant change, criticise prominent figures or their ideas, and can suggest outgroup membership. This means EA is very good at optimising along a very narrow and not necessarily optimal path.</p>\n<p>EA prides itself on its openness to criticism, and in many areas this is <a href=\"https://forum.effectivealtruism.org/topics/criticism-and-red-teaming-contest\">entirely justified</a>. However, willingness to engage with critique varies widely depending on the type of critique being made, and powerful structures exist within the community that reduce the likelihood that people will speak up <em>and be heard</em>.</p>\n<p>Within EA, criticism is acceptable, even encouraged, if it lies within particular boundaries, and when it is expressed in suitable terms. Here we distinguish informally between \u201cshallow critiques\u201d and \u201cdeep critiques\u201d.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-16\" id=\"fnref-pMAQ8odjQviWs2qYJ-16\">[16]</a></sup></p>\n<p>Shallow critiques are often:</p>\n<ul>\n<li>Technical adjustments to generally-accepted structures\n<ul>\n<li>\u201cWe should rate intervention X 12% higher than we currently do.\u201d</li>\n<li>Changes of emphasis or minor structural/methodological adjustments</li>\n<li>Easily conceptualised as \u201coptimising\u201d \u201cupdates\u201d rather than cognitively difficult qualitative switches</li>\n</ul>\n</li>\n<li>Written in EA-language and sprinkled liberally with EA buzzwords</li>\n<li>Not critical of capitalism</li>\n</ul>\n<p>Whereas deep critiques are often:</p>\n<ul>\n<li>\n<p>Suggestive that one or more of the fundamental ways we do things are wrong</p>\n<ul>\n<li>i.e. are critical of EA orthodoxy</li>\n<li>Thereby implying that people may have invested considerable amounts of time/effort/identity in something when they perhaps shouldn\u2019t have<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-17\" id=\"fnref-pMAQ8odjQviWs2qYJ-17\">[17]</a></sup></li>\n</ul>\n</li>\n<li>\n<p>Critical of prominent or powerful figures within EA</p>\n</li>\n<li>\n<p>Written in a way suggestive of outgroup membership</p>\n<ul>\n<li>And thus much more likely to be read as hostile and/or received with hostility</li>\n</ul>\n</li>\n<li>\n<p>Political</p>\n<ul>\n<li>Or more precisely: of a different politics to the broadly liberal<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-18\" id=\"fnref-pMAQ8odjQviWs2qYJ-18\">[18]</a></sup>-technocratic approach popular in EA</li>\n</ul>\n</li>\n</ul>\n<p>EA is <a href=\"https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest\">very open to shallow critiques</a>, which is something we absolutely love about the movement. As a community, however, we remain remarkably resistant to deep critiques. The distinction is likely present in most epistemic communities, but EA appears to have a particularly large problem. Again, there will be exceptions, but the trend is clear.</p>\n<p>The problem is illustrated well by the example of an entry to the recent Red-Teaming Contest: \u201c<a href=\"https://medium.com/@sven_rone/the-effective-altruism-movement-is-not-above-conflicts-of-interest-25f7125220a5\">The Effective Altruism movement is not above conflicts of interest</a>\u201d. It warned us of the political and ethical risks associated with taking money from cryptocurrency billionaires like Sam Bankman-Fried, and suggested that EA has a serious blind spot when it comes to (financial) conflicts of interest.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-19\" id=\"fnref-pMAQ8odjQviWs2qYJ-19\">[19]</a></sup></p>\n<p>The article (which did not win anything in the contest) was written under a <a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId=YjkzhkGc86yaWiuP8\">pseudonym</a>, as the author feared that making such a critique publicly would incur a risk of repercussions to their career. A related <a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId=FqA7gH6MBP4whxNig\">comment</a> provided several well-evidenced reasons to be morally and pragmatically wary of Bankman-Fried, got <a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId=FqA7gH6MBP4whxNig\">downvoted heavily</a>, and was eventually deleted by its author.</p>\n<p>Elsewhere, critical EAs report<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-20\" id=\"fnref-pMAQ8odjQviWs2qYJ-20\">[20]</a></sup> having to develop specific rhetorical strategies to be taken seriously. Making deep critiques or contradicting orthodox positions outright gets you labelled as a \u201cnon-value-aligned\u201d individual with \u201cpoor epistemics\u201d, so you need to pretend to be extremely deferential and/or stupid and ask questions in such a way that critiques are raised without actually being stated.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-21\" id=\"fnref-pMAQ8odjQviWs2qYJ-21\">[21]</a></sup></p>\n<p>At the very least, critics have learned to watch their tone at all costs, and provide a constant stream of unnecessary caveats and reassurances in order to not be labelled \u201cemotional\u201d or \u201coverconfident\u201d.</p>\n<p>These are not good signs.</p>\n<h2>Why do critical EAs have to use pseudonyms?</h2>\n<p><strong>Summary:</strong> Working in EA usually involves receiving money from a small number of densely connected funding bodies/individuals. Contextual evidence is strongly suggestive that raising deep critiques will drastically reduce one\u2019s odds of being funded, so many important projects and criticisms are lost to the community.</p>\n<p>There are several reasons people may not want to publicly make deep critiques, but the one that has been most impactful in our experience has been the role of funding.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-22\" id=\"fnref-pMAQ8odjQviWs2qYJ-22\">[22]</a></sup></p>\n<p>EA work generally relies on funding from EA sources: we need to pay the bills, and the kinds of work EA values are often very difficult to fund via non-EA sources. Open Philanthropy, and previously FTX, has/had an almost <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Decentralised_in_theory__centralised_in_practice\">hegemonic</a> funding role in many areas of existential risk reduction, as well as several other domains. This makes EA funding organisations and even individual grantmakers extremely powerful.</p>\n<p>Prominent funders have said that they value <a href=\"https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous\">moderation and pluralism</a>, and thus people (like the writers of this post) should feel comfortable <a href=\"https://forum.effectivealtruism.org/posts/gx7BEkoRbctjkyTme/democratising-risk-or-how-ea-deals-with-critics-1?commentId=AQk29QGbvCLGFJmxr\">sharing their real views</a> when they apply for funding, no matter how critical they are of orthodoxy.</p>\n<p>This is admirable, and we are sure that they are being truthful about their beliefs. Regardless, it is difficult to trust that the promise will be kept when one, for instance:</p>\n<ul>\n<li>\n<p>Observes the types of projects (and people) that succeed (or fail) at acquiring funding</p>\n<ul>\n<li>i.e. few, if any, deep critiques or otherwise heterodox/\u201cheretical\u201d works</li>\n</ul>\n</li>\n<li>\n<p>Looks into the backgrounds of grantmakers and sees how they appear to have very similar backgrounds and opinions (i.e they are highly orthodox)</p>\n</li>\n<li>\n<p>Experiences the generally claustrophobic epistemic atmosphere of EA</p>\n</li>\n<li>\n<p>Hears of people facing (soft) censorship from their superiors because they wrote deep critiques of the ideas of prominent EAs</p>\n<ul>\n<li>Zoe Cremer and Luke Kemp <a href=\"https://forum.effectivealtruism.org/posts/gx7BEkoRbctjkyTme/democratising-risk-or-how-ea-deals-with-critics-1\">lost</a> \u201csleep, time, friends, collaborators, and mentors\u201d as a result of writing <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3995225\">Democratising Risk</a>, a paper which was critical of some EA approaches to existential risk.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-23\" id=\"fnref-pMAQ8odjQviWs2qYJ-23\">[23]</a></sup> Multiple senior figures in the field attempted to prevent the paper from being published, largely out of fear that it would offend powerful funders. This saga caused significant conflict within CSER throughout much of 2021.</li>\n</ul>\n</li>\n<li>\n<p>Sees the revolving door and close social connections <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Decentralised_in_theory__centralised_in_practice\">between</a> key donors and main scholars in the field</p>\n</li>\n<li>\n<p>Witnesses grantmakers dismiss scientific work on the grounds that the people doing it are insufficiently value-aligned</p>\n<ul>\n<li>If this is what is said in public (which we have witnessed multiple times), what is said in private?</li>\n</ul>\n</li>\n<li>\n<p>Etc.</p>\n</li>\n</ul>\n<p>Thus, it is reasonable to conclude that if you want to get funding from an EA body, you must not only try to propose a good project, but one that could not be interpreted as insufficiently \u201cvalue-aligned\u201d, however the grantmakers might define it. If you have an idea for a project that seems very important, but could be read as a \u201cdeep critique\u201d, it is rational for you to put it aside.</p>\n<p>The risk to one\u2019s career is especially important given the <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Decentralised_in_theory__centralised_in_practice\">centralisation</a> of funding bodies as well as the dense internal social network of EA\u2019s upper echelons.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-24\" id=\"fnref-pMAQ8odjQviWs2qYJ-24\">[24]</a></sup></p>\n<p>Given this level of clustering, it is reasonable to believe that if you admit to holding heretical views on your funding application, word will spread, and thus you will quite possibly never be funded by any other funder in the EA space, never mind any <a href=\"https://forum.effectivealtruism.org/posts/fovDwkBQgTqRMoHZM/power-dynamics-between-people-in-ea\">other consequences</a> (e.g. gatekeeping of EA events/spaces) you might face. For a sizeable portion of EAs, the community forms a very large segment of one\u2019s career trajectory, social life, and identity; not things to be risked easily.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-25\" id=\"fnref-pMAQ8odjQviWs2qYJ-25\">[25]</a></sup> For most, the only robust strategy is to keep your mouth shut.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-26\" id=\"fnref-pMAQ8odjQviWs2qYJ-26\">[26]</a></sup></p>\n<p><strong>Grantmakers:</strong> You are missing out on exciting, high potential impact projects due to these processes. When the stakes are as high as they are, verbal assurances are unfortunately insufficient. The problems are structural, so the solutions must be structural as well.</p>\n<h2>We can\u2019t put numbers on everything\u2026</h2>\n<p><strong>Summary:</strong> EA is highly culturally quantitative, which is optimal for some problem categories but not others. Trying to put numbers on everything causes information loss and triggers anchoring and certainty biases. Individual Bayesian Thinking, prized in EA, has significant methodological issues. Thinking in numbers, especially when those numbers are subjective \u201crough estimates\u201d, allow one to justify anything comparatively easily, and can lead to wasteful and immoral decisions.</p>\n<p>EA places an extremely high value on quantitative thinking, mostly focusing on two key concepts: <a href=\"https://forum.effectivealtruism.org/topics/expected-value\">expected value</a> (EV) calculations and <a href=\"https://forum.effectivealtruism.org/topics/bayesian-epistemology\">Bayesian</a> probability estimates.</p>\n<p>From the EA Forum wiki: \u201cThe <strong>expected value</strong> of an act is the sum of the value of each of its possible outcomes multiplied by their probability of occurring.\u201d <a href=\"https://en.wikipedia.org/wiki/Bayes%27_theorem\">Bayes\u2019s theorem</a> is a simple mathematical tool for updating our estimate of the likelihood of an event in response to new information.</p>\n<p>Individual Bayesian Thinking (IBT) is a technique inherited by EA from the <a href=\"https://en.wikipedia.org/wiki/LessWrong\">Rationalist</a> subculture, where one attempts to use Bayes\u2019 theorem on an everyday basis. You assign each of your beliefs a numerical probability of being true and attempt to mentally apply Bayes\u2019 theorem, increasing or decreasing the probability in question in response to new evidence. This is sometimes called \u201cBayesian epistemology\u201d in EA, but to avoid confusing it with the broader approach to formal epistemology with the <a href=\"https://en.wikipedia.org/wiki/Bayesian_epistemology#Problem_of_priors\">same name</a> we will stick with IBT.</p>\n<p>There is nothing wrong with quantitative thinking, and much of the power of EA grows from its dedication to the numerical. However, this is often taken to the extreme, where people try to think almost exclusively along numerical lines, causing them to neglect important qualitative factors or else attempt to replace them with doubtful or even meaningless numbers because \u201csomething is better than nothing\u201d. These numbers are often subjective \u201cbest guesses\u201d with little empirical basis.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-27\" id=\"fnref-pMAQ8odjQviWs2qYJ-27\">[27]</a></sup></p>\n<p>For instance, Bayesian estimates are heavily influenced by one\u2019s initial figure (one\u2019s \u201cprior\u201d), which, especially when dealing with complex, poorly-defined, and highly uncertain and speculative phenomena, can become subjective (based on unspecified values, worldviews, and assumptions) to the point of arbitrary.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-28\" id=\"fnref-pMAQ8odjQviWs2qYJ-28\">[28]</a></sup> This is particularly true in existential risk studies where one may not have good evidence to update on.</p>\n<p>We assume that, with enough updating in response to evidence, our estimates will eventually converge on an accurate figure. However, this is dependent on several conditions, notably well-formulated questions, representative sampling of (accurate) evidence, and a rigorous and consistent method of translating real-world observations into conditional likelihoods.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-29\" id=\"fnref-pMAQ8odjQviWs2qYJ-29\">[29]</a></sup> This process is very difficult even when performed as part of careful and rigorous scientific study; attempting to do it all in your head, using rough-guess or even purely intuitional priors and likelihoods, is likely to lead to more confidence than accuracy.</p>\n<p>This is further complicated by the fact that probabilities are typically distributions rather than point values \u2013 often very messy distributions that we don\u2019t have nice neat formulae for. Thus, \u201cupdating\u201d properly would involve manipulating big and/or ugly matrices in your head. Perhaps this is possible for some people.</p>\n<p>A common response to these arguments is that Bayesianism is \u201chow the mind really works\u201d, and that the brain already assigns probabilities to hypotheses and updates them similarly or identically to Bayes\u2019 rule. There are good <a href=\"https://blogs.scientificamerican.com/cross-check/are-brains-bayesian/\">reasons</a> to believe that this may be true. However, the fact that we may intuitively and subconsciously work along Bayesian lines does not mean that our attempts to consciously \u201cdo the maths\u201d will work.</p>\n<p>In addition, there seems to have been little empirical study of whether Individual Bayesian Updating actually outperforms other modes of thought, never mind how this varies by domain. It seems risky to put so much confidence in a relatively unproven technique.</p>\n<p>The process of Individual Bayesian Updating can thus be critiqued on scientific grounds, but there is also another issue with it and hyper-quantitative thinking more generally: motivated reasoning. With no hard qualitative boundaries and little constraining empirical data, <strong>the combination of expected value calculations and Individual Bayesian Thinking in EA allows one to justify and/or rationalise essentially anything</strong> <strong>by generating suitable numbers</strong>.</p>\n<p>Inflated EV estimates can be used to justify <a href=\"https://twitter.com/autismcapital/status/1590551673721991168?s=46&amp;t=TW6S7geuKweyu4_b6Y4Bhw\">immoral</a> or <a href=\"https://www.newyorker.com/magazine/2022/08/15/the-reluctant-prophet-of-effective-altruism\">wasteful</a> actions, and <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3995225\">somewhat</a> <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0016328719303313\">methodologically</a> <a href=\"https://www.youtube.com/watch?v=Y_yMemcSMkg\">questionable</a> subjective <a href=\"https://www.goodreads.com/book/show/50485582-the-precipice\">probability estimates</a> translate psychological, cultural, and historical biases into truthy \u201crough estimates\u201d to plug into scientific-looking <a href=\"https://youtu.be/qtWl1TBr5RE?t=268\">graphs</a> and base important decisions upon.</p>\n<p>We then try to optimise our activities using the numbers we have. Attempting to fine-tune estimates of the maximally impactful strategy is a great approach when operating within fairly predictable, well-described domains, but is a fragile and risky strategy when operating in complex and uncertain domains (like existential risk) <em>even</em> when you have solid reasons for believing that your numbers are good \u2013 what if you\u2019re wrong? Robustness to a wide variety of possibilities is typically the objective of professionals in such areas, not optimality; we should ask ourselves why.</p>\n<p>Such estimates can also trigger the <a href=\"https://en.wikipedia.org/wiki/Anchoring_(cognitive_bias)\">anchoring bias</a>, and imply to lay readers that, for example, while unaligned artificial intelligence may not be responsible for almost twice as much existential risk as <a href=\"https://www.goodreads.com/book/show/50485582-the-precipice\">all other factors combined</a>, the ratio is <em>presumably</em> somewhere in that ballpark. In fact, it is debatable whether such estimates have <a href=\"https://www.youtube.com/watch?v=Y_yMemcSMkg\">any validity at all</a>, <a href=\"https://www.stat.berkeley.edu/~aldous/157/Papers/taleb_tetlock.pdf\">especially</a> <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0040162509001656?via%3Dihub\">when</a> not applied to simple, short-term (i.e. within a year),<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-30\" id=\"fnref-pMAQ8odjQviWs2qYJ-30\">[30]</a></sup> theoretically well-defined questions. Indeed, they do not seem to be taken seriously by existential risk scholars outside of EA.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-31\" id=\"fnref-pMAQ8odjQviWs2qYJ-31\">[31]</a></sup> The apparent scientific-ness of numbers can fool us into thinking we know much more about certain problems than we actually do.</p>\n<p>This isn\u2019t to say that quantification is inherently bad, just that it needs to be combined with other modes of thought. When a narrow range of thought is prized above all others, blind spots are bound to emerge, especially when untested and controversial techniques like Individual Bayesian Thinking are conflated (as they sometimes are by EAs) with \u201ctransparent reasoning\u201d and even applied \u201crationality\u201d itself.</p>\n<p>Numbers are great, but they\u2019re not the whole story.</p>\n<h2>\u2026and trying to weakens our collective epistemics</h2>\n<p><strong>Summary:</strong> Overly-numerical thinking lends itself to homogeneity and hierarchy. This encourages undue deference and opaque/unaccountable power structures. EAs assume they are smarter/more rational than non-EAs, which allows us to dismiss opposing views from outsiders even when they know far more than we do. This generates more homogeneity, hierarchy, and insularity.</p>\n<p>Under number-centric thinking, everything is operationalised as (or is assigned) some value unless there is an overwhelming need or deliberate effort to think otherwise. A given value X is either bigger or smaller than another value Y, but not qualitatively different to it; ranking X with respect to Y is the only possible type of comparison. Thus, the default conceptualisation of a given entity is a point on a (homogenous) number line. In a culture strongly focused on maximising value (that \u201cline goes up\u201d), one comes to assume that this model fits everything: put a number on something, then make the number bigger.</p>\n<p>For instance, (intellectual) ability is implicitly assumed within much of EA to be a single variable<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-32\" id=\"fnref-pMAQ8odjQviWs2qYJ-32\">[32]</a></sup>, which is simply higher or lower for different people. Therefore, there is no need for <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_are_incredibly_homogenous\">diversity</a>, and it feels natural to implicitly trust and defer to the assessments of prominent figures (\u201cthought leaders\u201d) perceived as <a href=\"https://forum.effectivealtruism.org/posts/DxfpGi9hwvwLCf5iQ/objections-to-value-alignment-between-effective-altruists#Intelligence\">highly intelligent</a>. This in turn encourages one to accept <a href=\"https://www.newyorker.com/news/annals-of-inquiry/sam-bankman-fried-effective-altruism-and-the-question-of-complicity\">opaque and unaccountable hierarchies</a>.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-33\" id=\"fnref-pMAQ8odjQviWs2qYJ-33\">[33]</a></sup></p>\n<p>This assumption of cognitive hierarchy contributes to EA\u2019s unusually low opinion of diversity and democracy, which reduces the input of diverse perspectives, which naturalises orthodox positions, which strengthens norms against diversity and democracy, <a href=\"https://forum.effectivealtruism.org/posts/DxfpGi9hwvwLCf5iQ/objections-to-value-alignment-between-effective-altruists#Feedback_Loops\">and so on</a>.</p>\n<p>Moreover, just as prominent EAs are assumed to be authoritative, the EA community\u2019s focus on individual epistemics leads us to think that we, with our powers of rationality and Bayesian reasoning, must be epistemically superior to non-EAs. Therefore, we can place overwhelming weight on the views of EAs and more easily dismiss the views of the outgroup, or even <a href=\"https://renewal.org.uk/effective-altruism-longtermism-and-democracy-an-interview-with-dr-luke-kemp/\">disregard democracy</a> in favour of an \u201c<a href=\"https://stefanfschubert.com/blog/2020/12/22/legitimate-epistocracy\">epistocracy</a>\u201d in which we are the obvious rulers.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-34\" id=\"fnref-pMAQ8odjQviWs2qYJ-34\">[34]</a></sup></p>\n<p>This is a generator function for hierarchy, homogeny, and insularity. It is antithetical to the aim of a healthy epistemic community.</p>\n<p>In fact, work on the <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0039368117303278\">philosophy of science in existential risk</a> has convincingly argued that in a field with so few independent evidential feedback loops, homogeneity and \u201cconservatism\u201d are particularly problematic. This is because unlike other fields where we have a good idea of the epistemic landscape, the inherently uncertain and speculative nature of Existential Risk Studies (ERS) means that not only are we uncertain of whether we have discovered an epistemic peak, but what the topography of the epistemic landscape even looks like. Thus, we should focus on creating the conditions for creative science, rather than the conservative science that we (i.e. the EAs within ERS) are moving towards through our extreme focus on a <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_should_probably_read_more_widely\">narrow range of disciplines and methodologies</a> .</p>\n<h2>The EA Forum structurally discourages deep critique</h2>\n<p><strong>Summary:</strong> The EA Forum gives more \u201csenior\u201d users far more votes and hides unpopular comments. This combines with cultural factors to silence critics and encourage convergence on orthodox views.</p>\n<p>The EA Forum is interesting because it formalises some of the maladaptive trends we have mentioned. The Forum\u2019s karma system ranks comments on the basis of user popularity, and comments below a certain threshold are hidden from view. This greatly reduces the visibility of unpopular comments, i.e. those that received a negative reaction from their readers.</p>\n<p>Furthermore, the <a href=\"https://forum.effectivealtruism.org/posts/gNHFRWyo58cTQ8pe8/ea-forum-2-0-initial-announcement-1#A_reworked_karma_system\">greater a user\u2019s overall karma score, the more impactful their votes</a>, to the point where some users can have <strong>10x</strong> or even <strong>16x</strong> the voting power of others. Thus, more established, popular, engaged users are able to give their preferred comments a significant boost, and in some cases unilaterally drop comments they dislike (e.g. from their critics) below the threshold past which a comment is hidden from view and thus not seen by most people.</p>\n<p>Due to popularity feedback loops present on internet fora (where low-karma comments are<a href=\"https://forum.effectivealtruism.org/posts/BvNxD66sLeAT8u9Lv/climate-change-and-longtermism-new-book-length-report?commentId=HixsMpfxHwkx9gX7P#XbDGMyMMBsraSuu8R\"> likely to be further downvoted</a><sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-35\" id=\"fnref-pMAQ8odjQviWs2qYJ-35\">[35]</a></sup> and vice versa) as well as the related issues of trust and <a href=\"https://forum.effectivealtruism.org/posts/SApmQrKdvgccmH2yF/revisiting-the-karma-system?commentId=pzmmB2DA7uZ9BZqEr\">deference</a>, these problems are likely to be magnified over the course of a discussion.</p>\n<p>New users, who reasonably expect voting to be one-person-one-vote, can mistakenly believe that a comment with -5 karma from 15 votes represents 10 downvotes and 5 upvotes, when it could just as easily be a result of 13 upvotes being overruled by strong downvotes from a couple of members of the EA core network.</p>\n<p>These arrangements give more orthodox individuals and groups disproportionate power over online discourse, and can make people feel less comfortable sharing critical views. It is a generator function for <a href=\"https://forum.effectivealtruism.org/posts/SApmQrKdvgccmH2yF/revisiting-the-karma-system\">groupthink</a>.</p>\n<p>Some admirable work has been done to improve the situation, for instance the excellent step to <a href=\"https://forum.effectivealtruism.org/posts/2JQQZevGENbChSA8k/agree-disagree-voting-and-other-new-features-september-2022\">separate karma and agreement ratings</a>, but this is not enough to solve the problem.</p>\n<p>The most important solution is simple: one person, one vote. Beyond that, having an option to sort by controversial, not hiding low-karma comments, having separate agreement karma for posts as well as comments, and perhaps occasionally putting a low-ranking comment nearer the top when one sorts comments by \u201ctop scoring\u201d so critical comments don\u2019t get buried all seem like good ideas.</p>\n<h1>Expertise &amp; Rigour</h1>\n<h2>We need to value expertise and rigour more</h2>\n<p><strong>Summary:</strong> EA mistakes value-alignment and seniority for expertise and neglects the value of impartial peer-review. Many EA positions perceived as \u201csolid\u201d are derived from informal shallow-dive blogposts by prominent EAs with little to no relevant training, and clash with consensus positions in the relevant scientific communities. Expertise is appealed to unevenly to justify pre-decided positions.</p>\n<p>There are many very enthusiastic EAs who have started studying existential risk recently, which includes several of the authors of this post. This is a massive asset to the movement. However, given the (obviously understandable) inexperience of many newcomers, we must be wary of the power of received wisdom. Under the wrong conditions, newcomers can rapidly update in line with the EA \u201ccanon\u201d, then speak with significant confidence about fields containing much more internal disagreement and complexity than they are aware of, or even where orthodox EA positions are misaligned with consensus positions within the relevant expert communities.</p>\n<p>More specifically, EA shows a pattern of prioritising non-peer-reviewed publications \u2013 often <a href=\"https://www.cold-takes.com/was-life-better-in-hunter-gatherer-times/#what-about-diseases-of-affluence\">shallow-dive blogposts</a><sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-36\" id=\"fnref-pMAQ8odjQviWs2qYJ-36\">[36]</a></sup> \u2013 by <a href=\"https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would\">prominent EAs</a> with <a href=\"https://forum.effectivealtruism.org/posts/BvNxD66sLeAT8u9Lv/climate-change-and-longtermism-new-book-length-report\">little to no relevant expertise</a>. These are then accepted into the \u201ccanon\u201d of highly-cited works to populate <a href=\"https://www.goodreads.com/book/show/59802037-what-we-owe-the-future\">bibliographies</a> and fellowship curricula, while we view the topic as somewhat \u201cdealt with\u201d'; \u201csomeone is handling it''. It should also be noted that the authors of these works often do not regard them as publications that should be accepted as \u201ccanon\u201d, but they are frequently accepted as such regardless.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-37\" id=\"fnref-pMAQ8odjQviWs2qYJ-37\">[37]</a></sup></p>\n<p>This is a worrying tendency, given that these works commonly do not engage with <a href=\"https://en.wikipedia.org/wiki/Societal_collapse\">major</a> areas of scholarship on the topics that they focus on, ignore <a href=\"https://globalchallenges.org/wp-content/uploads/Assessing-Climate-Changes-Contribution-to-GCR-authored.pdf\">work</a> attempting to answer similar questions, nor consult with relevant experts, and in many instances use methods and/or come to conclusions that would be considered <a href=\"https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would#Summary\">fringe</a> within the relevant fields. These works do not face adequate scrutiny due to the aforementioned issues with <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#EA_is_very_open_to_some_kinds_of_critique_and_very_not_open_to_others\">raising critique</a> as well as (usually) an extreme lack of relevant expertise in the EA community caused by its disciplinary homogeneity</p>\n<p>Elsewhere, ideas like the <a href=\"https://forum.effectivealtruism.org/topics/itn-framework\">ITN framework</a> and <a href=\"https://en.wikipedia.org/wiki/Differential_technological_development\">differential technological development</a> are taken as core parts of EA orthodoxy, even being used to make highly consequential funding and policy decisions. This is worrying given that both are problematic (the ITN framework,\n<a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Using_the_right__grantmaking__tools_for_the_right__grantmaking__jobs\">for example</a>, neglects co-benefits, response risks, and tipping points) and neither have been subjected to significant amounts of rigorous peer review and academic discussion<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-38\" id=\"fnref-pMAQ8odjQviWs2qYJ-38\">[38]</a></sup>.</p>\n<p>This is not at all to say that Google Docs and blogposts are inherently \u201cbad\u201d: they are very good for opening discussions and providing preliminary thoughts before in-depth studies. In fact, one thing EA does much <em>better</em> than academia is its lower barrier to entry to important conversations, which is <em>facilitated by</em> things like EA Forum posts. This is a wonderful force for scientific creativity. However, the fact remains that these posts are simply no substitute for rigorous studies subject to peer review (or genuinely equivalent processes) by domain-experts external to the EA community.</p>\n<p>Moreover, there seem to be rather inconsistent attitudes to expertise in the EA community. When Stuart Russell argues that AI could pose an existential threat to humanity, he is held up as someone worth listening to \u2013\u201dHe wrote <strong>the</strong> book on AI, you know!\u201d However, if someone of comparable standing in Climatology or Earth-Systems Science, e.g. Tim Lenton or Johan Rockstr\u00f6m, says the same for their field, they are ignored, or even pilloried.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-39\" id=\"fnref-pMAQ8odjQviWs2qYJ-39\">[39]</a></sup> Moderate statements from the IPCC are used to argue that climate change is \u201cnot an existential risk\u201d, but given <a href=\"https://link.springer.com/article/10.1007/s13748-021-00239-1\">significant expert disagreement</a> among experts on e.g. deep learning capabilities, it seems very unlikely that a consensus-based \u201cIntergovernmental Panel on Artificial Intelligence\u201d would take a stance anything like as extreme as that of most prominent EAs. This seems like a straightforward example of confirmation bias to us. To the extent that we defer to experts, we should be consistent and rigorous about how it is done.</p>\n<p>Finally, we sometimes assume that somebody holding significant power must mean that their opinions are particularly valuable. Sam Bankman-Fried, for instance, was given a huge platform to speak both on behalf of and to the EA movement, e.g. a <a href=\"https://80000hours.org/podcast/episodes/sam-bankman-fried-high-risk-approach-to-crypto-and-doing-good/\">3.5-hour interview</a> on 80,000 Hours. He was frequently asked to share his beliefs on a range of complex topics, from AI safety to theories of history, even though his only distinction was (as we know now, fraudulently) making lots of money in crypto. To interview someone or to invite them to speak about a given topic implies that they are someone whose views are particularly worth listening to compared to others. We should be critical about how we make that judgement, especially given the seniority-expertise confusion we discussed above.</p>\n<p>Neither value-alignment nor seniority are equivalent to expertise or skill, and our assessments of the quality of research works should be independent of the perceived value-alignment and name-recognition of their authors. We\u2019re dealing with really big problems: let\u2019s make sure we get it right.</p>\n<h2>We should probably read more widely</h2>\n<p><strong>Summary:</strong> EA reading lists are typically narrow, homogenous, and biased, and EA has unusual social norms against reading more than a handful of specific books. Reading lists often heavily rely on EA Forum posts and shallow dives over peer-reviewed literature. EA thus remains intellectually insular, and the resulting overconfidence makes many attempts by external experts and newcomers to engage with the community exhausting and futile. This gives the false impression that orthodox positions are well-supported and/or difficult to critique.</p>\n<p>EA reading lists are notorious for <a href=\"https://www.simonknutsson.com/problems-in-effective-altruism-and-existential-risk-and-what-to-do-about-them/#Systematically_problematic_syllabi_reading_lists_citations_writings_etc\">being</a> <a href=\"https://forum.effectivealtruism.org/posts/gx7BEkoRbctjkyTme/democratising-risk-or-how-ea-deals-with-critics-1?commentId=dbdsfFKt2Hgqrb2y3\">homogenous</a>, being populated overwhelmingly by the output of a few highly value-aligned thinkers (i.e. MacAskill, Ord, Bostrom, etc.), and paying little attention to alternative perspectives. Whilst these thinkers are highly impactful, they aren\u2019t (and <a href=\"https://www.newyorker.com/magazine/2022/08/15/the-reluctant-prophet-of-effective-altruism\">don\u2019t claim to be)</a> the singular authorities on the issues EAs are interested in.</p>\n<p>This, plus our community\u2019s general intellectual insularity, can cause new EAs to assume that little of worth has been said on some problems outside of EA. For instance, it is not uncommon for an EA that is very interested in existential risk to have never heard of many of the key papers, concepts, or authors in <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3730000\">Existential Risk Studies</a>, which is unsurprising when most of our reading lists ignore almost all academic papers not written by senior members of the Future of Humanity Institute.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-40\" id=\"fnref-pMAQ8odjQviWs2qYJ-40\">[40]</a></sup></p>\n<p>Conversely, our insularity plus the resistance to \u201cdeep critiques\u201d causes people with expertise in neglected fields to either burn out and give up in exhaustion after a while, or avoid engaging in the first place. We are personally familiar with innumerable examples of this, from senior academics to 18-year-old undergraduates. Since they either avoid EA after brief exposure or have their contributions ignored or downvoted into the ground, <strong>we don\u2019t even notice how much we are losing out on and how many opportunities we are missing</strong>.</p>\n<p>Our problems concerning outside expertise and knowledge are compounded by EA\u2019s odd cultural relationship to books: student groups are given money to bulk-order EA-friendly books and hand them out for free, but otherwise there seems to be a general feeling that reading books is <a href=\"https://www.cold-takes.com/reading-books-vs-engaging-with-them/\">rarely a good use of time</a> in comparison to reading (EA-aligned) blogposts. This issue reached its extreme in Sam Bankman-Fried. From a now-deleted <a href=\"https://web.archive.org/web/20221027180943/https://www.sequoiacap.com/article/sam-bankman-fried-spotlight/\">article in Sequoia</a>:</p>\n<blockquote>\n<p>\u201cOh, yeah?\u201d says SBF. \u201cI would never read a book.\u201d</p>\n<p>I\u2019m not sure what to say. I\u2019ve read a book a week for my entire adult life and have written three of my own.</p>\n<p>\u201cI\u2019m very skeptical of books. I don\u2019t want to say no book is ever worth reading, but I actually do believe something pretty close to that,\u201d explains SBF. \u201cI think, if you wrote a book, you fucked up, and it should have been a six-paragraph blog post.\u201d</p>\n</blockquote>\n<p>Most people don\u2019t write blogposts, and some (most?) arguments are too complex and detailed to fit into blogposts. However, blogposts are very popular in the tech/rationalist spheres EA emerged from, and are extremely popular within EA. Thus, cultural forces once again push people away from potentially valuable outside ideas.</p>\n<p>When those from outside EA have contributed to existential risk discussions, they have often had <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0039368117303278\">useful</a> and <a href=\"https://www.emerald.com/insight/content/doi/10.1108/FS-05-2018-0047/full/html\">insightful</a> contributions<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0039368117303278\">.</a> Thus, it is probably a good idea to assume that a lot of work outside of EA may have useful applications in an EA context. We are trying to deal with some of the most important issues in the world. We can't afford to assume that our little ecosystem has all the answers, because we don\u2019t!</p>\n<p>Luckily, there are expert opinion aggregation tools to rigorously combine the positions of many scholars. For instance, under the <a href=\"https://en.wikipedia.org/wiki/Delphi_method\">Delphi method</a>, rounds of estimation and explanation are iterated in order to produce more reliable predictions. Participants are kept anonymous, and each participant does not know who made any given estimate or argument. This can counteract the negative impacts of experts\u2019 personal or public stakes in certain ideas, and encourage participants to update their views freely. If we want to find the best available answers for our questions, we should look into the best-supported methods for generating bases of knowledge.</p>\n<p>Other communities have been working on problems like the ones we focus on for decades: <strong>let\u2019s hear what they have to say</strong>.</p>\n<h2>We need to stop reinventing the wheel</h2>\n<p><strong>Summary:</strong> EA ignores highly relevant disciplines to its main area of focus, notably Disaster Risk Reduction, Futures Studies, and Science &amp; Technology Studies, and in their place attempts to derive methodological frameworks from first principles. As a result, many orthodox EA positions would be considered decades out of date by domain-experts, and important decisions are being made using unsuitable tools.</p>\n<p>EA is known for reinventing the wheel even within the EA community. This poses a significant problem given the stakes and urgency of problems like existential risk.</p>\n<p>There are entire disciplines, such as <a href=\"https://en.wikipedia.org/wiki/Disaster_risk_reduction\">Disaster Risk Reduction</a>, <a href=\"https://en.wikipedia.org/wiki/Futures_studies\">Futures Studies</a>, and <a href=\"https://en.wikipedia.org/wiki/Science_and_technology_studies\">Science and Technology Studies</a>, that are profoundly relevant to existential risk reduction yet which have been almost entirely ignored by the EA community. The consequences of this are unsurprising: we have started near to the beginning of the history of each discipline and are slowly learning each of their lessons the hard way.</p>\n<p>For instance, the approach to existential risk most prominent in EA, what <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3995225\">Cremer and Kemp</a> call the \u201cTechno-Utopian Approach\u201d (TUA), <a href=\"https://www.goodreads.com/book/show/50485582-the-precipice?from_search=true&amp;from_srp=true&amp;qid=j2JFUE9TDM&amp;rank=3\">focuses</a> on categorising individual hazards (called \u201crisks\u201d in the TUA),<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-41\" id=\"fnref-pMAQ8odjQviWs2qYJ-41\">[41]</a></sup> attempting to estimate the likelihood that they will cause an existential catastrophe within a given timeframe, and trying to work on each risk separately by default, with a homogenous category of underlying \u201crisk factors\u201d given secondary importance.</p>\n<p>However, such a hazard-centric approach was <a href=\"https://www.routledge.com/At-Risk-Natural-Hazards-Peoples-Vulnerability-and-Disasters/Blaikie-Cannon-Davis-Wisner/p/book/9780415252164\">abandoned</a> within Disaster Risk Reduction decades ago and replaced with one that places a <a href=\"https://www.preventionweb.net/understanding-disaster-risk\">heavy emphasis</a> on the vulnerability of humans to potentially hazardous phenomena.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-42\" id=\"fnref-pMAQ8odjQviWs2qYJ-42\">[42]</a></sup> Indeed, differentiating between \u201crisk\u201d (the potential for harm), \u201chazards\u201d (specific potential causes of harm) and \u201cvulnerabilities\u201d (aspects of humans and human systems that render them susceptible to the impacts of hazards) is one of the first points made on any disaster risk course. Reducing human vulnerability and exposure is generally a far more effective method of reducing risk posed by a wide variety of hazards, and far better accounts for \u201cunknown unknowns\u201d or \u201cBlack Swans\u201d.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-43\" id=\"fnref-pMAQ8odjQviWs2qYJ-43\">[43]</a></sup></p>\n<p>Disaster risk scholarship is also revealing the growing importance of complex patterns of causation, the interactions between threats, and the potential for <a href=\"https://en.wikipedia.org/wiki/Cascading_failure\">cascading failures</a>. This area is largely ignored by EA existential risk work, and has been <a href=\"https://forum.effectivealtruism.org/posts/BvNxD66sLeAT8u9Lv/climate-change-and-longtermism-new-book-length-report?commentId=QwZaZrkwAfCjAexyK\">dismissed out of hand</a> by prominent EAs.</p>\n<p>As another example, Futures &amp; Foresight scholars noted the <a href=\"https://www.youtube.com/watch?v=Y_yMemcSMkg\">deep limitations</a> of numerical/probabilistic forecasting of specific trends/events in the 1960s-70s, especially with respect to long timescales as well as domains of high complexity and deep uncertainty<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-44\" id=\"fnref-pMAQ8odjQviWs2qYJ-44\">[44]</a></sup>, and low-probability high-impact events (i.e. characteristics of existential risk). Practitioners now <a href=\"https://www.taylorfrancis.com/books/mono/10.4324/9780203791684/foundations-futures-studies-jose-valciukas\">combine or replace forecasts with qualitative</a> foresight methods like scenario planning, wargaming, and Causal Layered Analysis, which explore the shape of possible futures rather than making hard-and-fast predictions. Yet, EA\u2019s existential risk work places a massive emphasis on forecasting and pays little attention to foresight. Few EAs seem aware that \u201cFutures Studies\u201d as a discipline exists at all, and EA discussions of the (long-term) future often imply that little of note has been said on the topic outside of EA.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-45\" id=\"fnref-pMAQ8odjQviWs2qYJ-45\">[45]</a></sup></p>\n<p>These are just two brief examples.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-46\" id=\"fnref-pMAQ8odjQviWs2qYJ-46\">[46]</a></sup> There is a wealth of valuable insights and data available to us if we would only go out and read about them: this should be a cause for celebration!</p>\n<p>But why have they been so neglected? Regrettably, it is not because EAs read these literatures and provided robust arguments against them; we simply never engaged with them in the first place. We tried to create the field of existential risk almost from first principles using the methods and assumptions that were already popular within our movement, regardless of whether they were <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4118618\">suitable for the task</a>.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-47\" id=\"fnref-pMAQ8odjQviWs2qYJ-47\">[47]</a></sup></p>\n<p>We believe there could be several disciplines or theoretical perspectives that EA, had it developed a little differently earlier on, would recognise as fellow travellers or allies. Instead, we threw ourselves wholeheartedly into the <a href=\"https://en.wikipedia.org/wiki/Founder_effect\">Founder Effect</a>, and in our over-dependence on a few early canonical thinkers (i.e. MacAskill, Ord, Bostrom, Yudkowsky etc.), we thus far lost out on all that they have to offer.</p>\n<p>This expands to a broader question: if we were to reinvent (EA approaches to) the field of Existential Risk Studies from the ground up, how confident are we that we would settle on our current way of doing things?</p>\n<p>The above is not to say that all views within EA ought to always reflect mainstream academic views; there are genuine shortcomings to traditional academia. However, the sometimes hostile attitude EA has to academia has hurt our ability to listen to its contributions as well as those of experts in general.</p>\n<h2>Some ideas we should probably pay more attention to</h2>\n<p><strong>Summary:</strong> Taster menu of topics directly applicable to existential risk work that EA pays little attention to: Vulnerability &amp; Resilience, Complex (Adaptive) Systems, Futures &amp; Foresight, Decision-Making under Deep Uncertainty/Robust Decision-Making, Psychology &amp; Neuroscience, Science &amp; Technology Studies, and the Humanities &amp; Social Sciences in general.</p>\n<p>So what are some areas that EA should take a greater notice of? Our list is far from exhaustive and is heavily focused on global catastrophic risk, but it seems like a good starting point. Naturally we welcome both suggestions <em>for</em> and constructive debates <em>on</em> the below.</p>\n<h3>Vulnerability and Resilience</h3>\n<p>Most <a href=\"https://www.preventionweb.net/\">communities trying to reduce risk</a> focus on reducing human vulnerability and increasing societal resilience, rather than trying to fine-tune predictions of individual hazards, especially in areas full of unknown unknowns. It is possible that reducing the likelihood and magnitude of particular hazards may sometimes be the most effective way of reducing overall risk, but this should only be concluded after detailed assessment, rather than assumed <em>a priori</em>. In fact, our priors should be strongly against this claim given that it would imply that hazard-centric approaches are most suitable for existential risk scenarios (i.e. the areas of deepest uncertainty and highest complexity) which is the opposite of the trend seen in disaster/catastrophe risk more generally.</p>\n<p>The <a href=\"https://www.stockholmresilience.org/download/18.10119fc11455d3c557d6928/1459560241272/SRC+Applying+Resilience+final.pdf\">principles of resilience</a> include maintaining redundancy, diversity, and modularity, and ensuring that excessive connectivity doesn\u2019t allow failures to cascade through a system (\u201c<a href=\"https://www.annualreviews.org/doi/abs/10.1146/annurev-soc-073014-112317\">systemic risk</a>\u201d). This is often achieved through self-organisation (as seen everywhere from <a href=\"https://www.sciencedirect.com/topics/earth-and-planetary-sciences/self-organization\">ecosystems</a> to democratic <a href=\"https://rojavainformationcenter.com/2022/07/10-years-of-the-rojava-revolution-much-achieved-still-much-to-come/\">success</a> <a href=\"https://nacla.org/news/2019/01/18/spark-hope-ongoing-lessons-zapatista-revolution-25-years\">stories</a>) and institutional learning. Resilience is typically enhanced by popular participation in decision-making (consistent with collective intelligence, <a href=\"https://medium.com/incerto/what-do-i-mean-by-skin-in-the-game-my-own-version-cc858dc73260\">skin in the game</a>, and the wisdom of the crowd), and enabling <a href=\"https://cor.europa.eu/en/events/Pages/multi-level-governance-and-active-subsidiarity-for-sustainable-recovery-and-resilience.aspx\">subsidiarity</a><sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-48\" id=\"fnref-pMAQ8odjQviWs2qYJ-48\">[48]</a></sup> (making decisions closest to where their impacts are, and where local knowledge can be effectively utilised).<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-49\" id=\"fnref-pMAQ8odjQviWs2qYJ-49\">[49]</a></sup> Such multilevel governance, taking local knowledge into account, may be particularly valuable given our aforementioned problems around insularity and the dominance of the Global North. Consulting more widely, especially in areas of real vulnerability, may improve mitigation and adaptation strategies with respect to existential risk.</p>\n<p>It is worth noting that the FTX crash is a perfect example of the fulfilment of a systemic, cascading risk that was unforeseen by EA and which EA was highly vulnerable to.</p>\n<h3>Complex (Adaptive) Systems</h3>\n<p>The siloed approach to existential risk, where the overwhelming majority of work focuses on reducing risk from one of the Big 4 Hazards<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-50\" id=\"fnref-pMAQ8odjQviWs2qYJ-50\">[50]</a></sup> in isolation neglects emergent behaviour, feedback loops, <a href=\"https://forum.effectivealtruism.org/posts/MayveXrHbvXMBRo78/cause-exploration-prizes-the-importance-of-intercausal\">interactions</a> between cause areas, cascade/contagion effects, and the properties of <a href=\"https://en.wikipedia.org/wiki/Complex_adaptive_system\">complex adaptive systems</a> more broadly. This is concerning because the likelihoods, magnitudes, and qualities of global catastrophic scenarios are determined by the structure of the current world-system, which is usefully conceptualised as a (staggeringly) complex adaptive system. Recent work from Len Fisher and Anders Sandberg, for example, highlights the advantages of analysing catastrophic threats as <a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/1758-5899.13030\">complex adaptive networks</a>, work by Lara Mani, Asaf Tzachor, and Paul Cole has shown the<a href=\"https://www.nature.com/articles/s41467-021-25021-8\"> issues with neglecting cascading catastrophic risk from volcanoes</a>, and <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3730000\">systems approaches</a> are on the rise in Existential Risk Studies generally.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-51\" id=\"fnref-pMAQ8odjQviWs2qYJ-51\">[51]</a></sup></p>\n<p>There is a <a href=\"https://www.complexityexplorer.org/\">huge</a> <a href=\"https://www.youtube.com/watch?v=Eo5oQ9Psmg8&amp;list=PLF0b3ThojznRyDQlitfUTzXEXwLNNE-mI\">body of research</a> on how to model and act within complex adaptive systems, including systems-dynamics, network-dynamics, and agent-based simulations, as well as qualitative approaches.</p>\n<p>Complexity science has seen particularly extensive application in ecology and earth system science, where inherently interconnected systems vulnerable to <a href=\"https://www.science.org/doi/10.1126/science.abn7950\">tipping points</a>, <a href=\"https://royalsocietypublishing.org/doi/10.1098/rstb.2008.0219\">cascades</a>, and collapse are common. Here, phenomena like temperature increases are best analysed as perturbations to the overall system state; one cannot simply add up the individual impacts of a predefined set of hazards.</p>\n<p>For instance, <a href=\"https://www.pnas.org/doi/10.1073/pnas.2108146119\">recent work</a> on catastrophic climate risk highlights the key role of cascading effects like societal collapses and resource conflicts. With as many as <a href=\"https://www.science.org/doi/10.1126/science.abn7950\">half of climate tipping points in play</a> at <a href=\"https://climateactiontracker.org/global/cat-thermometer/\">2.7\u00b0C - 3.4\u00b0C of warming</a> and several at as low as 1.5\u00b0C, large areas of the Earth are likely to face prolonged lethal heat conditions, with innumerable knock-on effects. These could include increased interstate conflict, a far greater number of omnicidal actors, food-system strain or failure <a href=\"https://link.springer.com/article/10.1007/s10584-021-02957-w\">triggering societal collapses</a>, and long-term degradation of the biosphere carrying unforeseen long-term damage e.g. through keystone species loss.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-52\" id=\"fnref-pMAQ8odjQviWs2qYJ-52\">[52]</a></sup></p>\n<h3>Futures &amp; Foresight</h3>\n<p>As mentioned above, foresight exercises \u2013 especially those conducted in groups \u2013 are the <a href=\"https://forum.effectivealtruism.org/posts/znaZXBY59Ln9SLrne/how-to-think-about-an-uncertain-future-lessons-from-other\">bread and butter</a> of <a href=\"https://forum.effectivealtruism.org/posts/wyHjpcCxuqFzzRgtX/a-practical-guide-to-long-term-planning-and-suggestions-for\">futures professionals</a>. The emphasis is generally on qualitative or even narrative explorations of what the future might hold, with quantitative forecasting playing an important but not central role. You can\u2019t put a probability on something if you don\u2019t think of it in the first place,<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-53\" id=\"fnref-pMAQ8odjQviWs2qYJ-53\">[53]</a></sup> and qualitative analysis often reveals that the something you were going to put a probability on isn\u2019t a single distinct \u201csomething\u201d at all.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-54\" id=\"fnref-pMAQ8odjQviWs2qYJ-54\">[54]</a></sup> In most cases, putting meaningful probabilities on events is impractical, and unnecessary for decision-making.</p>\n<p><a href=\"https://sociology.yale.edu/publications/foundations-futures-studies-i-history-purposes-knowledge\">Futures</a> <a href=\"https://www.routledge.com/Foundations-of-Futures-Studies-Volume-2-Values-Objectivity-and-the-Good/Bell/p/book/9780765805669\">Studies</a> also includes large bodies of work on <a href=\"https://en.wikipedia.org/wiki/Utopian_studies\">utopianism</a> and <a href=\"https://www.sciencedirect.com/topics/social-sciences/sociotechnical-imaginaries\">socio-technical imaginaries</a>, which seem vital given how much of EA\u2019s existential risk work is premised on longtermism, a broadly <a href=\"https://nickbostrom.com/utopia\">utopian</a> philosophy based on a particular image of the future.</p>\n<h3>Decision-Making under Deep Uncertainty/Robust Decision-Making</h3>\n<p>Robust decisions are designed to succeed largely independently of how the future plays out; this is achieved by preparing for things we cannot predict. When futures and risk professionals try to plan for an uncertain future, they typically do not try to perform fine-grained expected value calculations and optimise accordingly \u2013 \u201cPredict and Act\u201d \u2013 but construct plans that are robust to a wide variety of possible futures \u2013 \u201cExplore and Adapt\u201d \u2013 using simulations to explore the full parameter space and seeking agreement among stakeholders on particular decisions rather than particular models of the world. This approach vastly improves one\u2019s performance when faced with Black Swans and unknown unknowns, and is much better at taking into account the positions of multiple stakeholders with differing value systems. These approaches are policy-proven (see the <a href=\"https://www.usbr.gov/lc/region/programs/crbstudy.html\">Colorado River Basin</a> and Dutch \u201c<a href=\"https://www.dutchwatersector.com/news/room-for-the-river-programme\">Room for the River</a>\u201d examples) and there is a wealth of literature on the subject, starting <a href=\"https://www.youtube.com/watch?v=rPlcHpn9qwg\">here</a> and <a href=\"https://link.springer.com/book/10.1007/978-3-030-05252-2\">here</a>.</p>\n<h3>Psychology and Neuroscience</h3>\n<p>By understanding the psychological processes that drive people\u2019s behaviour, effective altruists and existential risk researchers can better predict how people will respond to various interventions and develop strategies that will be more likely to succeed. Additionally, psychology can provide valuable insights into how people perceive and respond to risk, which can help us better understand our audience and create effective strategies to reduce risk.</p>\n<p>Elsewhere, neuroscientific studies have revealed the <a href=\"https://www.goodreads.com/book/show/6968772-the-master-and-his-emissary\">value</a> of holistic/anti-reductionist thinking and embodied cognition, as well as significant areas in which EA\u2019s <a href=\"https://www.goodreads.com/book/show/11468377-thinking-fast-and-slow?from_search=true&amp;from_srp=true&amp;qid=CC1ig0USJq&amp;rank=1\">Kahneman</a>-derived emphasis on cognitive biases and dismissal of <a href=\"https://www.goodreads.com/book/show/59446181-how-to-stay-smart-in-a-smart-world\">intuitive</a> decision-making <a href=\"https://www.goodreads.com/book/show/25666050-algorithms-to-live-by?from_search=true&amp;from_srp=true&amp;qid=fT3PJ2grWK&amp;rank=1\">is</a> <a href=\"https://www.goodreads.com/book/show/65229.Sources_of_Power?from_search=true&amp;from_srp=true&amp;qid=ESuiACGqGX&amp;rank=1\">misplaced</a>.</p>\n<h3>Science and Technology Studies</h3>\n<p><a href=\"https://mitpress.mit.edu/9780262035682/the-handbook-of-science-and-technology-studies/#:~:text=The%20Handbook%20of%20Science%20and%20Technology%20Studies%20provides%20a%20comprehensive,from%20a%20range%20of%20disciplines\">Science and Technology Studies</a> (STS) investigates the creation, development, and consequences of technologies with respect to history, society, and culture. Particularly relevant concepts include the \u201c<a href=\"https://uk.sagepub.com/en-gb/eur/risk-society/book203184\">Risk Society</a>\u201d, which addresses how society organises itself in response to risk, and \u201c<a href=\"https://press.princeton.edu/books/paperback/9780691004129/normal-accidents\">Normal Accidents</a>\u201d, which contends that failures are inherent features of complex technical systems. Elsewhere, <a href=\"https://en.wikipedia.org/wiki/Social_construction_of_technology\">constructivist</a> or <a href=\"https://en.wikipedia.org/wiki/Co-production_(society)\">co-productionist</a> approaches to technology would provide valuable counterpoints to the implicit technological determinism of a large fraction of longtermist work.</p>\n<h3>The Humanities and Social Sciences</h3>\n<p>They exist! And are valuable!</p>\n<p>Understanding how social change occurs will naturally be key to reducing risk, both in general (e.g. how do we <a href=\"https://thedrearymantle.wordpress.com/2022/05/28/kill-the-bill-to-save-the-future/\">build</a> towards <a href=\"https://www.cambridge.org/core/journals/global-sustainability/article/operationalising-positive-tipping-points-towards-global-sustainability/8E318C85A8E462AEC26913EC43FE60B1\">social tipping points</a>, or communicate effectively?) and from ourselves (what risks are associated with utopian <a href=\"https://www.goodreads.com/book/show/20186.Seeing_Like_a_State\">high-modernist</a> movements? How do <a href=\"https://www.goodreads.com/book/show/85812.Dialectic_of_Enlightenment\">socio-economic conditions</a> affect ideas about what counts as \u201c<a href=\"https://www.youtube.com/watch?v=Xe3tunGi4To\">rational</a>\u201d or \u201cscientific\u201d?).</p>\n<p>Understanding how people have historically failed at the task of <a href=\"https://journals.sagepub.com/doi/full/10.1177/0306422018800259\">profoundly improving the world</a> is vital if we want to avoid replicating those failures at larger scales.</p>\n<p>Elsewhere, philosophies like <a href=\"https://academic.oup.com/edited-volume/34458/chapter/292360679\">critical realism</a> may provide different epistemological and ontological bases for studying existential risk, and Kuhn-descended discussions of scientific <a href=\"https://www.goodreads.com/book/show/61539.The_Structure_of_Scientific_Revolutions\">paradigms</a> helpfully highlight the contingent, cultural, and sometimes limited nature of science.</p>\n<p>Studies of subjectivity, <a href=\"https://en.wikipedia.org/wiki/Standpoint_theory\">positionality</a>, and <a href=\"https://en.wikipedia.org/wiki/Postcolonialism\">postcolonialism</a> provide useful insights about, for instance, how ideas of objectivity can be defined in terms that advantage those in power.</p>\n<p>Also, much of the existential risk we face appears to <a href=\"https://www.bbc.com/future/article/20211014-agents-of-doom-who-is-hastening-the-apocalypse-and-why\">arise</a> <a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1111/1758-5899.12319\">from</a> <a href=\"https://civilizationemerging.com/solving-generator-function/\">social</a> <a href=\"https://thedrearymantle.wordpress.com/2021/08/25/existential-risk-bugs-features-of-a-system/\">phenomena</a>, and thus it only seems rational to use the tools developed for such things.</p>\n<h2>Using the right (grantmaking) tools for the right (grantmaking) jobs</h2>\n<p><strong>Summary:</strong> EA grantmaking methods have many advantages when applied to \u201cclassic\u201d cause areas like endemic disease. However, current methods have significant methodological issues, and over-optimise in complex and uncertain environments like global catastrophic risk where robustness should be the primary objective. EA grantmaking should thus be decentralised and pluralised. Different methods should be trialled and rigorously evaluated.</p>\n<p>Funding has a central role within EA, and a large proportion of EA institutions and projects would collapse if they were unable to secure funding from EA sources.</p>\n<p><a href=\"https://www.openphilanthropy.org/\">Open</a> <a href=\"https://forum.effectivealtruism.org/topics/open-philanthropy\">Philanthropy</a> (OpenPhil) is by far the most powerful funding organisation in EA, so its cause prioritisations and decision-making frameworks have an extremely large influence on the direction of the movement.</p>\n<p>We applaud essentially all of the cause areas OpenPhil funds<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-55\" id=\"fnref-pMAQ8odjQviWs2qYJ-55\">[55]</a></sup> and the people we know at OpenPhil are typically intelligent, altruistic, and diligent.</p>\n<p>Regardless of this, we will be using OpenPhil as a case study to explore two major problems with EA funding, both because of OpenPhil\u2019s centrality, and because OpenPhil\u2019s perspectives and practices are common across much of the rest of our movement, e.g. <a href=\"https://forum.effectivealtruism.org/topics/effective-altruism-funds\">EA Funds</a>.</p>\n<p>The problems are:</p>\n<ol>\n<li>Our funding frameworks sometimes use inappropriate goals and tools</li>\n<li>It is socially and epistemically unhealthy for a movement to cultivate such a huge concentration of (unaccountable, opaque) power</li>\n</ol>\n<p>We will discuss the former here, and explore the latter in subsequent sections.</p>\n<p>The focus will be on the cause area of global catastrophic risk/existential risk/longtermism for two reasons: it\u2019s the area most of us know the most about, and it\u2019s where the issues we describe are most visible &amp; impactful.</p>\n<p>OpenPhil\u2019s global catastrophic risk/longtermism <a href=\"https://www.openphilanthropy.org/our-global-health-and-wellbeing-and-longtermism-grantmaking-portfolios/\">funding</a> stream is dominated by two hazard-clusters \u2013  artificial intelligence and engineered pandemics<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-56\" id=\"fnref-pMAQ8odjQviWs2qYJ-56\">[56]</a></sup> \u2013 with little affordance given to other aspects of the risk landscape. Even within this, AI seems to be seen as \u201cthe main issue\u201d by a wide margin, both within OpenPhil and throughout the EA community.</p>\n<p>This is a problematic practice, given that, for instance:</p>\n<ul>\n<li>\n<p>The prioritisation relies on <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_can_t_put_numbers_on_everything_\">questionable</a> <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Futures___Foresight\">forecasting</a> <a href=\"https://link.springer.com/article/10.1007/s13748-021-00239-1\">practices</a>, which themselves sometimes take <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_need_to_value_expertise_and_rigour_more\">contestable positions</a> as assumptions and inputs</p>\n</li>\n<li>\n<p>There is significant second-order uncertainty around the relevant risk estimates</p>\n</li>\n<li>\n<p>The ITN framework has major <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4118618\">issues</a>, especially when applied to existential risk</p>\n<ul>\n<li>\n<p>It is extremely sensitive to how a problem is framed, and often relies on rough and/or subjective estimates of ambiguous and variable quantities</p>\n<ul>\n<li>This poses serious issues when working under conditions of deep uncertainty, and can allow implicit assumptions and subconscious biases to pre-determine the result</li>\n<li>Climate change, for example, is typically considered low-neglectedness within EA, but extreme/existential risk-related climate work is <a href=\"https://iopscience.iop.org/article/10.1088/1748-9326/ac13ef/meta\">surprisingly neglected</a></li>\n<li>What <em>exactly</em> makes a problem \u201ctractable\u201d, and how do you rigorously put a number on it?</li>\n</ul>\n</li>\n<li>\n<p>It ignores co-benefits, response risks, and tipping points</p>\n</li>\n<li>\n<p>It penalises projects that seek to challenge concentrations of power, since this appears \u201cintractable\u201d until social tipping points are reached<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-57\" id=\"fnref-pMAQ8odjQviWs2qYJ-57\">[57]</a></sup></p>\n</li>\n<li>\n<p>It is extremely difficult and often impossible to meaningfully estimate the relevant quantities in complex, uncertain, changing, and low-information environments</p>\n</li>\n<li>\n<p>It focuses on evaluating actions as they are presented, and struggles to sufficiently value exploring the potential action space and increasing future optionality</p>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0039368117303278\">Creativity</a> can be limited by the need to appeal to a narrow range of grantmaker views<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-58\" id=\"fnref-pMAQ8odjQviWs2qYJ-58\">[58]</a></sup></p>\n</li>\n<li>\n<p>The current model neglects <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Vulnerability_and_Resilience\">areas</a> that do not fit [neatly] into the two main \u201ccause areas\u201d, and indeed it is <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#The_Humanities_and_Social_Sciences\">arguable</a> whether global catastrophic risk can be <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Futures___Foresight\">meaningfully</a> chopped up into individual \u201ccause areas\u201d at all</p>\n</li>\n<li>\n<p>A large proportion (plausibly a sizeable majority, depending on where you draw the line) of catastrophic risk researchers would, and if you ask, do, reject<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-59\" id=\"fnref-pMAQ8odjQviWs2qYJ-59\">[59]</a></sup>:</p>\n<ul>\n<li>The particular prioritisations made</li>\n<li>The methods used to arrive at those prioritisations, and/or</li>\n<li>The very conceptualisation of individual \u201crisks\u201d itself</li>\n</ul>\n</li>\n<li>\n<p>It is the product of a small <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_are_incredibly_homogenous\">homogenous</a> group of people with very similar views</p>\n<ul>\n<li>This is both a scientific (cf. <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Epistemic_health_is_a_community_issue\">collective intelligence/social epistemics</a> and a <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3995225\">moral</a> issue</li>\n</ul>\n</li>\n</ul>\n<p>There are important efforts to mitigate some of these issues, e.g. cause area exploration prizes, but the central issue remains.</p>\n<p>The core of the problem here seems to be one of objectives: optimality vs robustness. Some quick definitions (in terms of funding allocation):</p>\n<ul>\n<li>Optimality = the best possible allocation of funds\n<ul>\n<li>In EA this is usually synonymous with \u201cthe allocation with the highest possible expected value\u201d</li>\n<li>This typically has a unstated second component: <em>\u201cassuming that our information and our assumptions are accurate\u201d</em></li>\n</ul>\n</li>\n<li>Robustness = capacity of an allocation to maintain near-optimality given conditions of uncertainty and change</li>\n</ul>\n<p>In seeking to do the most good possible, EAs naturally seek <em>optimality</em>, and developed grantmaking tools to this end. We identify potential strategies, gather data, predict outcomes, and take the actions that our models tell us will work the best.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-60\" id=\"fnref-pMAQ8odjQviWs2qYJ-60\">[60]</a></sup> This works great when you\u2019re dealing with relatively stable and predictable phenomena, for instance endemic malaria, as well as most of the other cause areas EA started out with.</p>\n<p>However, now that much of EA\u2019s focus has turned on to global catastrophic risk, existential risk, and the long-term future, <strong>we have entered areas where optimality becomes fragility</strong>. We don\u2019t want most of our eggs in one or two of the most speculative baskets, especially when those eggs contain billions of people. We should also probably adjust for the fact that we may over-rate the importance of things like AI for reasons discussed in <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_are_incredibly_homogenous\">other</a> <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_align_suspiciously_well_with_the_interests_of_tech_billionaires__and_ourselves__61_\">sections</a></p>\n<p>Given the fragility of optimality, robustness is extremely important. Existential risk is a domain of high complexity and deep uncertainty, dealing with poorly-defined low-probability high-impact phenomena, sometimes covering extremely long timescales, with a huge amount of disagreement among both experts and stakeholders along theoretical, empirical, and normative lines. Ask any risk analyst, disaster researcher, foresight practitioner, or policy strategist: this is <strong>not</strong> where you optimise, this is where you maintain epistemic humility and cover all your bases. Innumerable people have <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_need_to_stop_reinventing_the_wheel\">learned this the hard way</a> so we don\u2019t have to.</p>\n<p>Thus, we argue that, <em>even if</em> you strongly agree with the current prioritisations / methods, it is still rational for you to support a <a href=\"https://forum.effectivealtruism.org/posts/cXH2sG3taM5hKbiva/beyond-simple-existential-risk-survival-in-a-complex\">more pluralist</a> and robustness-focused approach given the uncertainty, expert disagreement, and risk management best-practices involved.</p>\n<p>As well as a general diversification of the grantmaking community and a deliberate effort to value critical and community-external projects, a larger number and variety of funding sources and methods would likely be a good idea, especially if this was used as an opportunity to evaluate a range of different options.</p>\n<p>There have been laudable efforts to decentralise grantmaking, e.g. the FTX Future Fund\u2019s <a href=\"https://forum.effectivealtruism.org/posts/CQQtKkMGeGLxbgLjP/the-future-fund-s-regranting-program\">re-granting scheme</a>. However, regrantors were picked by the central organisation (and tended to subscribe to all or most of EA orthodoxy), and even then grants still required approval from the central organisation. An admirable step in the right direction, to be sure, but in our view there is room to take several more.</p>\n<p>One interesting route for us to explore might be lottery funding, where projects are chosen at random after an initial pass to remove bad-faith and otherwise obviously low-quality proposals. This solves a surprisingly large number of problems in grantmaking and science funding (eliminating bias and scientific conservatism, for example), and has been <a href=\"https://aeon.co/ideas/science-funding-is-a-gamble-so-lets-give-out-money-by-lottery\">supported</a> by <a href=\"https://www.sciencedirect.com/science/article/pii/S0039368117303278\">multiple</a> philosophers of science in existential risk.</p>\n<p>OpenPhil\u2019s and wider EA\u2019s funding practices have many advantages: for instance, they require far less admin than conventional scientific funding, which accelerates progress and maximises the time researchers spend researching rather than applying for the opportunity to do so. This is great, but there is room for improvement, largely boiling down to <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_should_probably_read_more_widely\">our</a> <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_need_to_stop_reinventing_the_wheel\">aforementioned</a> <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_need_to_value_expertise_and_rigour_more\">problems</a> with intellectual openness and wheel-reinventing, where we instinctively use the (grantmaking) tools that we have lying around when we enter a field rather than taking a step back and asking what the best way forward is in our new environment.</p>\n<p>On another note, there does not seem to be any good information on whether grantmakers are effective or improving at forecasting the success of projects. Given that this is an extremely difficult and impactful task, it seems reasonable that there should be a significant level of oversight and transparency.</p>\n<h1>Intermission</h1>\n<p><em>The councillor comes with his battered old suit <br>\nAnd his head all filled with plans <br>\nSays \"It's not for myself, nor the fame or wealth  <br>\nBut to help my fellow man.\"</em></p>\n<p><em>Fist in the air and the first to stand <br>\nWhen the Internationale plays <br>\nSays \"We'll break down the walls of the old Town Hall, <br>\nFight all the life-long day!\"</em></p>\n<p><em>Ten years later, where is he now? <br>\nHe's ditched all the old ideas <br>\nMilked all the life from the old cash cow <br>\nNow he's got a fine career <br>\nNow he's got a fine career.</em></p>\n<p><a href=\"https://www.youtube.com/watch?v=4TpBvwK-iO8\">A Fine Career</a> \u2013 Chumbawamba</p>\n<h1>Governance &amp; Power</h1>\n<h2>We align suspiciously well with the interests of tech billionaires (and ourselves)</h2>\n<p><strong>Summary:</strong> <sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-61\" id=\"fnref-pMAQ8odjQviWs2qYJ-61\">[61]</a></sup> EA is largely reliant on the goodwill of a small number of tech billionaires, and as a result fails to question the practice of elite philanthropy as well as the ways by which these billionaires acquired their wealth. Our cause prioritisations align suspiciously well with the interests and desires of both tech billionaires and ourselves. We are not above motivated reasoning.</p>\n<p>EA is reliant on funding, and the vast majority of funds come from a handful of tech billionaires: Dustin Moskowitz and Cari Tuna got most of their wealth through Facebook (now Meta) and Asana, Vitalik Buterin has Ethereum, and Sam Bankman-Fried had FTX.</p>\n<p>Elite philanthropy has faced numerous criticisms, from how it boosts and solidifies the <a href=\"https://ineffectivealtruismblog.com/2022/12/29/philanthropy-and-democracy/\">economic</a> and <a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/ijmr.12247\">political</a> power of the ultra-wealthy to the ways in which it undermines <a href=\"https://ineffectivealtruismblog.com/2022/12/03/billionaire-philanthropy-part-1-introduction/\">democracy</a> <a href=\"https://www.thenation.com/article/society/gates-covid-data-ihme/\">and</a> <a href=\"https://www.universityaffairs.ca/opinion/dispatches-academic-freedom/the-high-price-of-donations/\">academic</a> <a href=\"https://forum.effectivealtruism.org/posts/gx7BEkoRbctjkyTme/democratising-risk-or-how-ea-deals-with-critics-1\">freedom</a>. This issue has been studied and discussed at extreme length, so we will not expand further on the basic point, but <a href=\"https://www.vox.com/future-perfect/23462333/sam-bankman-fried-ftx-cryptocurrency-effective-altruism-crypto-bahamas-philanthropy\">recent events</a> strongly suggest that EA should re-examine its relationship to the practice and seriously consider other sources of funding.</p>\n<p>Furthermore, becoming a billionaire <a href=\"https://academiccommons.columbia.edu/doi/10.7916/d8-qjpx-4x05/download\">often involves a lot of unethical or risk-seeking behaviour</a>, and according to some ethical codes the very act of being a billionaire is immoral in itself. The sources of EA funds in particular can sometimes be morally questionable. Cryptocurrency is of <a href=\"https://www.goodreads.com/en/book/show/35525995-attack-of-the-50-foot-blockchain\">debatable</a> social value, is full of money laundering, fraud and scams, and has been created and promoted as a deliberate <a href=\"https://www.youtube.com/watch?v=u-sNSjS8cq0\">political</a> <a href=\"https://www.youtube.com/watch?v=YQ_xWvX1n9g\">project</a> to dodge taxes, concentrate power in the hands of the ultra-wealthy, and financialise an ever-growing proportion of human life.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-62\" id=\"fnref-pMAQ8odjQviWs2qYJ-62\">[62]</a></sup> As for Facebook, there is unfortunately an <a href=\"https://en.wikipedia.org/wiki/Criticism_of_Facebook\">abundance of evidence</a> that its impact on the world is likely to be net-negative.</p>\n<p><a href=\"https://medium.com/@sven_rone/the-effective-altruism-movement-is-not-above-conflicts-of-interest-25f7125220a5\">The Effective Altruism movement is not above conflicts of interest</a>. Relying on a small number of ultra-wealthy members of the tech sector incentivises us to accept or even <a href=\"https://80000hours.org/podcast/episodes/vitalik-buterin-new-ways-to-fund-public-goods/\">promote</a> their political, philosophical, and cultural beliefs, at the expense of the rigorous critical examination EA prides itself on. This may undermine even the most virtuous movement over the long term. Indeed, EA institutions and leaders rarely if ever interrogate the processes and structures that donors rely upon (digital surveillance, \u201cWeb 3.0\u201d, neoliberal capitalism, and so on). The question of whether, for instance, making large quantities of money in the tech industry should give somebody the right to exercise significant control over the future of humanity is answered with an implicit but resounding \u201cYes.\u201d</p>\n<p>Our models sometimes even assume that (an American corporation) creating an \u201caligned\u201d AGI, the fulfilment of Silicon Valley\u2019s (not to mention much of the Pentagon\u2019s\u2026) collective dreams, will solve all other major problems.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-63\" id=\"fnref-pMAQ8odjQviWs2qYJ-63\">[63]</a></sup></p>\n<p>Indeed, it is possible that certain members of the <a href=\"https://twitter.com/KerryLVaughan/status/1591508717241077766?s=20&amp;t=ug8B2LpXRHiX1k9Q2lFAyA\">EA leadership</a> were <a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId=jbqsHeeZHrrxkBaxY\">aware</a> of Sam Bankman-Fried\u2019s unethical practices <a href=\"https://twitter.com/KerryLVaughan/status/1590807597011333120\">some time ago</a> and were seemingly <a href=\"https://www.newyorker.com/news/annals-of-inquiry/sam-bankman-fried-effective-altruism-and-the-question-of-complicity\">unable or unwilling</a> to do anything about it. Additionally, Bankman-Fried is not the only <a href=\"https://forum.effectivealtruism.org/posts/WdeiPrwgqW2wHAxgT/a-personal-statement-on-ftx?commentId=WJGkQtZCbiR7hYKGE\">morally questionable billionaire</a> to have been courted by EA (e.g. <a href=\"https://twitter.com/KerryLVaughan/status/1591508700128313344?s=20&amp;t=ug8B2LpXRHiX1k9Q2lFAyA\">Ben Delo</a>).</p>\n<p>It is worth noting that the areas EA focuses on most intensely (the long-term future and existential risk, and especially AI risk within that) align remarkably well with the sorts of things tech billionaires are most concerned about: longtermism is the closest thing to \u201cdoing sci-fi in real life\u201d<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-64\" id=\"fnref-pMAQ8odjQviWs2qYJ-64\">[64]</a></sup>, existential catastrophes are one of the few ways in which wealthy people<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-65\" id=\"fnref-pMAQ8odjQviWs2qYJ-65\">[65]</a></sup> could come to harm, and AI is the threat most interesting to people who made their fortunes in computing.</p>\n<p>Fears about technological stagnation and slowed <a href=\"https://jacobin.com/2022/08/capitalism-low-birth-rate-labor-abortion-contraceptives-childcare\">population</a> <a href=\"https://www.opendemocracy.net/en/oureconomy/witch-hunts-federici-capitalism-reproduction/\">growth</a> receive pride of place in <a href=\"https://www.goodreads.com/de/book/show/59802037\">key EA texts</a>, which strikingly parallel elite worries about increased labour costs.</p>\n<p>Most of the proposed interventions also reflect the interests of Silicon Valley. Differential technological development, energy innovation, and high-tech solutions to pandemics are all favoured <em>a priori</em>. There is <a href=\"https://www.lesswrong.com/posts/uFNgRumrDTpBfQGrs/let-s-think-about-slowing-down-ai\">little to no</a> support for bans on AGI projects, nor moratoria on Lethal Autonomous Weapons Systems, facial recognition, or new fossil fuel infrastructure. Similarly, priority concerns for the long-term future focus on economic elite interest areas like technological progress and GDP growth over other issues that are at least as critical but would undermine the power and/or status of wealthy philanthropists, like workplace democratisation or wealth redistribution. Again, it is not that any of these positions are inherently wrong because they align with elite interests, just that this is a bias we really need to be aware of.</p>\n<p>Contrast the AI situation to climate change, routinely dismissed in EA, where the problems are messy, often mundane, predominantly political, and put the very concept of economic growth under debate, and where the greatest risk is posed to poor people from the Global South. Compare also with issues like global poverty, which very few people within EA are directly affected by (and which the funders are not by definition!) and which has come to be deemed \u201clower impact\u201d within some of EA.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-66\" id=\"fnref-pMAQ8odjQviWs2qYJ-66\">[66]</a></sup></p>\n<p>Interestingly, a huge proportion of EA\u2019s intellectual infrastructure can be traced back to the academic climate of the USA during the Cold War, where left-wing thinkers were eradicated from (<a href=\"https://jacobin.com/2023/01/analytic-philosophy-mccarthyism-postwar-communism\">analytic</a>) philosophy by McCarthyist purges, <a href=\"https://en.wikipedia.org/wiki/Robert_McNamara\">Robert McNamara</a> pushed for \u201crationalisation\u201d and quantification throughout the US establishment, and the <a href=\"https://en.wikipedia.org/wiki/RAND_Corporation\">RAND Corporation</a> developed concepts like Rational Choice Theory, Operations Research, and Game Theory. Indeed, the current President and CEO of RAND, <a href=\"https://en.wikipedia.org/wiki/Jason_Gaverick_Matheny\">Jason Matheny</a>, is a CSET founder and former FHI researcher. Aside from the Silicon Valley influences (from which we get the blogposts, <a href=\"https://en.wikipedia.org/wiki/The_Californian_Ideology\">Californian</a> <a href=\"https://www.goodreads.com/book/show/1165422.Imaginary_Futures\">Ideology</a>, and most of the technofetishism), EA\u2019s intellectual heritage is largely one of philosophy and economics intentionally stripped of their ability to challenge the status quo. As ever, that\u2019s not to say that things like analytic philosophy or Game Theory are inherently evil or anything \u2013 they\u2019re really quite good for some things \u2013 just that they are the tools we have for specific historical and political reasons, they are not the only ones available, and we should be critical of how and where we employ them.</p>\n<p>The relative prioritisations we describe also fit rather well with the disciplinary and cultural backgrounds of us EAs. It seems that our <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_can_t_put_numbers_on_everything_\">subjectively-generated quantifications</a> just so happened to have led us to conclude that the best way to improve (or even save) the world is to pay analytic philosophers and computer scientists like us large sums of money to work on the problems we read about in our favourite sci-fi novels.</p>\n<p>It is possible that this truly is a coincidence and that our current prioritisations are correct,<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-67\" id=\"fnref-pMAQ8odjQviWs2qYJ-67\">[67]</a></sup> but we should seriously consider what other factors might have been at play, especially given the potential for <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_can_t_put_numbers_on_everything_\">motivated reasoning</a> embedded in our shared methods of thought.</p>\n<p>On the topic of motivated reasoning, EA has been <a href=\"https://forum.effectivealtruism.org/posts/HWaH8tNdsgEwNZu8B/free-spending-ea-might-be-a-big-problem-for-optics-and\">criticised </a>in the past for being wasteful with its funds. Examples include buying Wytham Abbey<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-68\" id=\"fnref-pMAQ8odjQviWs2qYJ-68\">[68]</a></sup> (which was on the market for <a href=\"https://www.varenne.fr/files/91410477-f3e6-49a5-a40c-d501dd95550d/The%20Savills%20Portfolio.pdf\">\u00a315,000,000</a>), networking retreats taking place in the <a href=\"https://forum.effectivealtruism.org/posts/sdjcH7KAxgB328RAb/ftx-ea-fellowships\">Bahamas</a>, and funding for <a href=\"https://www.newyorker.com/magazine/2022/08/15/the-reluctant-prophet-of-effective-altruism\">undergraduates to get their laundry done for them</a> because their time is too valuable for them to do it themselves. A focus on frugality in service of others has evolved to incorporate generous expense accounts and all-expenses-paid trips to international EAGs.</p>\n<p>Community builders are paid <a href=\"https://forum.effectivealtruism.org/posts/ewCdRr2ZBkrwXMaoX/making-community-building-a-more-attractive-career-path-1\">extremely high salaries</a> despite these often being undergraduate students (or recent graduates) running student societies \u2013 something students generally do for free. To our knowledge there has not been a public explanation for how these numbers were reached, nor one for whether this is the most effective use of money.</p>\n<p>There is also the problem of financial robustness. EA projects are highly dependent on the fortunes of a handful of people in two closely intertwined industries (tech and crypto). Such a small number of points of failure create <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Vulnerability_and_Resilience\">serious resilience issues</a>, as we saw during the FTX collapse. It is easier said than done, of course, but we strongly suggest that EA makes an effort to diversify its funding sources.</p>\n<p>We are members of a movement dedicated to altruism, and we want to do what\u2019s best for the world. That doesn\u2019t mean that we are immune to (unconscious) bias, cultural influence, or motivated reasoning. If we want to do the most good, we need to closely examine why our authentic beliefs about \u201cdoing the most good\u201d are so similar to the ones we and our financiers would like us to have, and which just so happen to involve ourselves living very comfortable and interesting lives.</p>\n<h2>Decentralised in theory, centralised in practice</h2>\n<p><strong>Summary:</strong> The EA movement does not have a formal \u201cCEO\u201d, but the vast majority of power is held by a small number of unaccountable individuals. The movement is also centralised around a tight cluster of social and professional ties, creating issues around conflicts of interest.</p>\n<p>The Effective Altruism movement is formally decentralised but informally centralised. We have no official \u201cleader\u201d nor a movement-wide formal hierarchical structure, but:</p>\n<ul>\n<li>\n<p>The vast majority of funding in EA is controlled by a very small number of people</p>\n<ul>\n<li>Specifically Open Philanthropy, which is led by Holden Karnofsky and Alexander Berger, and overwhelmingly funded by tech billionaire couple Dustin Moskowitz and Cari Tuna\n<ul>\n<li>Almost all EA organisations began with or were scaled by funding from OpenPhil (CEA, 80k, CSET, GPI, Longview, MIRI, etc.), and many (likely most) other EA grantmaking bodies themselves receive a significant proportion of their funding from OpenPhil</li>\n</ul>\n</li>\n<li>Formerly, we also had Sam Bankman-Fried and the FTX Foundation, which was run by a small team led by Nick Beckstead</li>\n</ul>\n</li>\n<li>\n<p>Access to the main EA Global event (a key networking opportunity) is also controlled by a very small number of people</p>\n<ul>\n<li>Admission to locally-organised EAGx events is more decentralised</li>\n</ul>\n</li>\n<li>\n<p>Media engagement and community health/training is mostly handled by a small number of people at the CEA, and almost all media appearances are made by a smaller number still (Will MacAskill, to a lesser degree Toby Ord, and until recently Sam Bankman-Fried)</p>\n<ul>\n<li>Keynotes and \u201cfireside chats\u201d at EA events are disproportionately filled by MacAskill, Ord, and senior grantmakers/funders</li>\n</ul>\n</li>\n<li>\n<p>EA\u2019s two major book projects of recent times (i.e. _The Precipice _and <em>What We Owe the Future</em>) were written by members of the top leadership (i.e. Toby Ord and Will MacAskill)</p>\n<ul>\n<li>The (truly formidable and well-funded) press push for the latter book tightly focused on Will as a personality<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-69\" id=\"fnref-pMAQ8odjQviWs2qYJ-69\">[69]</a></sup></li>\n</ul>\n</li>\n<li>\n<p>A very, very small number of people are on the boards of a massive proportion of major EA institutions (most notably Will MacAskill)</p>\n</li>\n</ul>\n<p>The social circles of EA\u2019s upper rungs are incredibly tight, and many of the most powerful people within EA are current or former close friends, flatmates, or romantic partners. This phenomenon is replicated to a lesser extent at the lower rungs, as many community groups serve as social hubs for their members.</p>\n<p>Relatedly, EA organisations are very tightly interconnected, both by funding (notably via OpenPhil) and by people. Luke Muelhauser, for example, left his role as Director of MIRI to join OpenPhil in <a href=\"https://www.linkedin.com/in/luke-muehlhauser-15a09b26?original_referer=\">May 2015</a>. Less than two years later, MIRI received a $500,000 grant from OpenPhil, with donations to date <a href=\"https://www.openphilanthropy.org/grants/?organization-name=machine-intelligence-research-institute\">totalling</a> over $14 million. <a href=\"https://cset.georgetown.edu/staff/helen-toner/\">Helen Toner</a> worked at GiveWell, then OpenPhil, then GovAI, then CSET. Both <a href=\"https://www.openphilanthropy.org/grants/?organization-name=centre-for-the-governance-of-ai\">GovAI</a> and CSET receive OpenPhil funding, with CSET having received close to <a href=\"https://cset.georgetown.edu/article/new-grant-agreement-boosts-cset-funding-to-more-than-100-million-over-five-years/\">$100 million</a>. Six <a href=\"https://www.openphilanthropy.org/team/\">OpenPhil staff</a> (approximately 10%) previously worked at the FHI, and many others have either worked at other EA orgs or have close friends who do. To be clear, we are not accusing Luke, Helen, or anyone else of any kind of malpractice, they simply illustrate a revolving door that is not healthy in any social system.</p>\n<p>Such a closely wound social-professional network is bound to create <a href=\"https://forum.effectivealtruism.org/posts/fovDwkBQgTqRMoHZM/power-dynamics-between-people-in-ea\">issues</a>, especially conflicts of interest, as the probability increases that a grantmaker will be friends or otherwise connected with potential grantees. This came to a head in August 2019, when conflict of interest statements revealed that several grants made by the Long-Term Future Fund were made to <a href=\"https://forum.effectivealtruism.org/posts/an9GrNXrdMwBJpHeC/long-term-future-fund-august-2019-grant-recommendations-1\">housemates and personal friends</a> of grantmakers. <a href=\"https://forum.effectivealtruism.org/posts/dgy6m8TGhv4FCn4rx/long-term-future-fund-september-2020-grants\">Later posts</a> indicated that there would be stricter rules around conflicts of interest in future, but the LTFF appears to have discontinued public conflict of interest reporting after August 2019.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-70\" id=\"fnref-pMAQ8odjQviWs2qYJ-70\">[70]</a></sup></p>\n<p>The Effective Altruism movement gained size, funding, and influence very quickly, and it shows the signs of that experience. We still act like a new movement or a startup in many ways, with (often informal) decisions being heavily reliant on social ties and personal trust.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-71\" id=\"fnref-pMAQ8odjQviWs2qYJ-71\">[71]</a></sup> There are strengths to this, but ultimately large movements and organisations must necessarily create formal structures to ensure that they operate effectively and ethically. EA should not excise all its social qualities \u2013 we are a movement, not a corporation, and we are not arguing that EA should become any kind of anonymous bureaucracy<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-72\" id=\"fnref-pMAQ8odjQviWs2qYJ-72\">[72]</a></sup> \u2013 but public reporting of important information, formal and transparent governance structures, and stringent conflict of interest regulations seem like reasonable suggestions for a movement containing thousands of people and billions of dollars.</p>\n<h2>Deciding together better</h2>\n<p><strong>Summary:</strong>  <strong>Decision-making within EA is currently oligarchic, opaque, and unaccountable.</strong> Empirical and theoretical research as well as numerous practical examples indicate that deliberative mechanisms can measurably improve on important aspects of decision-making, but even small experiments have been rejected by the leadership without explanation.</p>\n<p>Despite exercising significant power over the direction of a large and influential movement, none of the people or groups we listed in the <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Decentralised_in_theory__centralised_in_practice\">previous section</a> are in any meaningful sense accountable to the EA membership, and the decisions they make are overwhelmingly made behind closed doors. The rank-and-file is welcome to contribute to discussions, e.g. through EA Forum posts, but decision-making is essentially <a href=\"https://en.wikipedia.org/wiki/Oligarchy\">oligarchic</a>, that is, \u201crule by a few\u201d. The leaders do not have to justify any decisions or answer any questions they don\u2019t want to.</p>\n<p>This centralisation of power reached a flashpoint when Will MacAskill tried to broker a deal between Sam Bankman-Fried and Elon Musk over the <a href=\"https://forum.effectivealtruism.org/posts/dk9HTJKNAAwaEZTgk/will-macaskill-s-role-in-connecting-sbf-to-elon-musk-for-a\">purchase of Twitter</a>. Given that Bankman-Fried had committed his wealth to EA, this action, had it succeeded, would have taken large amounts of money from other EA projects. However, it is unclear how or why buying a stake in Twitter would be an optimal (or even good) use of money, and the decision was seemingly made by Will and Sam alone.</p>\n<p>Will\u2019s intentions here were undoubtedly good, but that is not enough to justify one or two men taking what could have been the most <a href=\"https://forum.effectivealtruism.org/posts/WdeiPrwgqW2wHAxgT/a-personal-statement-on-ftx?commentId=vZJ8dKhuZdYvQRndx\">consequential</a> decision ever made in the name of EA with little, if any, discussion or consent.</p>\n<p>No matter how altruistic or intelligent one is, no single person is objective or immune to the corrupting influence of power. In fact, there is good evidence that power makes you both <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0749597811001361\">overconfident</a> and <a href=\"https://www.wbur.org/hereandnow/2020/07/09/jamil-zaki-empathy-power\">less</a> <a href=\"https://www.psychologytoday.com/us/blog/social-empathy/201909/power-blocks-empathy\">empathetic</a>, which poses obvious issues when making highly impactful decisions about altruism. To make a perhaps unnecessary statement, opaque and unaccountable decisionmaking by a small unelected elite does not have a good historical track record.</p>\n<p>EA has passed its startup phase and grown into a mature movement with considerable influence: we have a duty to be responsible with how our movement evolves, and take care not to lock in suboptimal or dangerous values. Power pools when left on its own, if for no other reason than the process of <a href=\"https://en.wikipedia.org/wiki/Preferential_attachment\">preferential attachment</a>,<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-73\" id=\"fnref-pMAQ8odjQviWs2qYJ-73\">[73]</a></sup> and organisations need active and powerful countermeasures to avoid gradually concentrating more and more power in the hands of fewer and fewer people.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-74\" id=\"fnref-pMAQ8odjQviWs2qYJ-74\">[74]</a></sup> Insofar as people are given power, a system of transparency and accountability is vital to ensure that actions taken on behalf of a movement are indeed the actions <em>of that movement</em> .<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-75\" id=\"fnref-pMAQ8odjQviWs2qYJ-75\">[75]</a></sup></p>\n<p>The issues of transparency and accountability become especially problematic when dealing with tasks as huge as eradicating poverty or preventing human extinction: these are communal projects, with stakeholders numbering in the billions. We cannot be so arrogant as to assume that we, the \u201cepistemically superior\u201d elite of <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_are_incredibly_homogenous\">wealthy white dudes</a>, should simply impose our preferred solutions from the top down. Projects with the aim of doing the most good should be embarked upon in cooperation and consultation with the people affected.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-76\" id=\"fnref-pMAQ8odjQviWs2qYJ-76\">[76]</a></sup> We should be transparent about what our interests are, how our decisions are made, and where our money comes from.</p>\n<p>Even beyond ethical considerations, as long as decisions are made behind closed doors the community is only able to criticise them after they have been made. This is an inefficient and generally ineffective process that does not allow errors to be corrected before their negative consequences materialise. Inclusive, transparent decisions will naturally be epistemically superior because they receive greater, more diverse input from the start. In fact, we have good reasons to believe <a href=\"https://forum.effectivealtruism.org/posts/kCkd9Mia2EmbZ3A9c/deliberation-may-improve-decision-making\">that</a> <a href=\"https://www.goodreads.com/book/show/14891887-democratic-reason\">democratic</a> <a href=\"https://www.pnas.org/doi/10.1073/pnas.0403723101\">decisions</a> <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0022053100927091\">outperform</a> other kinds, in large part due to the <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Epistemic_health_is_a_community_issue\">collective intelligence</a> properties we mentioned in previous sections. If the question of the Twitter purchase had been put to the membership or a representatively-sampled assembly of members, what would the outcome have been?</p>\n<p>There are plenty of methods for us to choose from in crafting better decision-making structures, often supported by a wealth of research and real-world success stories.</p>\n<p>Consensus building tools gather the views of many people, identify cruxes, and help build consensus. <a href=\"https://pol.is/home\">Pol.is</a>, <a href=\"https://www.youtube.com/watch?v=bRtD920YbIQ\">for</a> <a href=\"https://participedia.net/method/4682\">instance</a>, has seen significant success when implemented in <a href=\"https://www.theguardian.com/world/2020/sep/27/taiwan-civic-hackers-polis-consensus-social-media-platform\">Taiwan</a>, even on deeply polarised issues. EA could easily employ tools such as these to discover what the membership really believes about certain issues, create better-informed consensus on key issues, and rigorously update our views. Indeed, <a href=\"https://forum.effectivealtruism.org/posts/PtnGywLuk3Keo642c/polis-cluster-poll-what-are-our-opinions-on-ea-governance\">certain community members</a> have already started doing this.</p>\n<p>Elsewhere, sortition assemblies (also known as \u201c<a href=\"https://en.wikipedia.org/wiki/Citizens%27_assembly\">citizen juries</a>\u201d) have shown promise. Here, a representative random sample of a population is presented with the best-quality evidence<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-77\" id=\"fnref-pMAQ8odjQviWs2qYJ-77\">[77]</a></sup> on a topic, given time to discuss and deliberate, and asked to produce collective decisions or recommendations. Such methods have an excellent track-record, where from <a href=\"https://www.theguardian.com/commentisfree/2019/jan/22/irish-readers-citizens-assembly-worked-brexit\">Ireland</a> to <a href=\"https://youtu.be/D2KV2nkpwoM?t=1415\">Mongolia</a> they have allowed major political decisions to be made in a consensual and evidence-led way. We believe that these hold great potential for EA, especially with regard to major strategic decisions, big-picture funding-allocation questions, and navigating the crises/soul-searching we are currently embroiled in.</p>\n<p>Furthermore, there is no particular reason why EA institutions shouldn\u2019t be run by their members. Worker self-management has been shown to be <a href=\"https://www.thenation.com/article/archive/worker-cooperatives-are-more-productive-than-normal-companies/\">effective</a>, <a href=\"https://en.wikipedia.org/wiki/Worker_cooperative#Research_on_worker_cooperatives\">durable</a>, and naturally better suited to collaborative, mission-oriented work than traditional top-down rule. We are not suggesting that everyone becomes part-time managers \u2013 there is certainly a role for operations and coordination specialists \u2013 but big-picture decisions about the strategy and funding of an organisation should be made by the people that create and maintain it.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-78\" id=\"fnref-pMAQ8odjQviWs2qYJ-78\">[78]</a></sup></p>\n<p>Ultimately, what fits our specific context will likely be determined by experimentation. Zoe Cremer provides an excellent <a href=\"https://docs.google.com/document/d/1Y9opezUk9_JNQBYCAmHoERiqlaf2HmashQQ_ydGwyRY/edit\">plan of action</a> for funding decisions:</p>\n<ul>\n<li><em>Within 5 years: EA funding decisions are made collectively</em>\n<ul>\n<li><em>First set up experiments for a safe cause area with small funding pots that are distributed according to different collective decision-making mechanisms</em>\n<ul>\n<li><em>Subject matter experts are <strong>always</strong> used and weighed appropriately in this decision mechanism</em></li>\n</ul>\n</li>\n<li><em>Experiment in parallel with: randomly selected samples of EAs are to evaluate the decisions of one existing funding committee - existing decision-mechanisms are thus \u2018passed through\u2019 an accountability layer</em></li>\n<li><strong><em>All decision mechanisms have a deliberation phase (arguments are collected and weighed publicly) and a voting phase (majority voting, quadratic voting...)</em></strong></li>\n<li><em>Depending on the cause area and the type of choice, either fewer (experts + randomised sample of EAs) or more people (any EA or beyond) will take part in the funding decision</em> .</li>\n</ul>\n</li>\n</ul>\n<p>Some of the benefits of deliberation and democracy have been <a href=\"https://forum.effectivealtruism.org/posts/kCkd9Mia2EmbZ3A9c/deliberation-may-improve-decision-making\">noted</a> in EA\u2019s Improving Institutional Decision-Making community, and indeed deliberative democracy itself has roots in attempts to <a href=\"https://www.cambridge.org/core/books/abs/jurgen-habermas/deliberative-democracy/F9D4327C35D306E5EA49E66DD90445E5\">avoid</a> \u201cstrategic\u201d reasoning that have profound similarities to EA\u2019s preferred epistemic <a href=\"https://www.goodreads.com/en/book/show/42041926\">approaches</a>.</p>\n<p>However, deliberative reforms, and even small <a href=\"https://docs.google.com/document/d/1Y9opezUk9_JNQBYCAmHoERiqlaf2HmashQQ_ydGwyRY/edit\">experiments</a> in different types of collective decision-making, have been <a href=\"https://twitter.com/CarlaZoeC/status/1559293937378017282\">rejected</a> by the leadership with little explanation.</p>\n<p>If EA wants to improve its ability to identify, prioritise, and solve problems, it should arrange itself optimally for that task. EA is full of incredible people with diverse expertise; we ought to harness that.</p>\n<p>We are not suggesting that everyone in EA votes on every decision any EA institution makes. That would be silly. We are suggesting that the decision-making process itself is democratised, with individual decisions being made on the appropriate level. For instance, how a particular organisation is run should be up to the members of that organisation, and larger movement-wide decisions should be decided by assemblies or polls of members.</p>\n<p>This isn\u2019t just democracy for democracy\u2019s sake: democratic structures would play an important epistemic (and thus instrumental) role in improving our impact on the world. Democratic reforms would also help protect against <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Decentralised_in_theory__centralised_in_practice\">conflicts of interest</a> as well as stemming the tide of <a href=\"https://forum.effectivealtruism.org/posts/RPTPo8eHTnruoFyRH/some-important-questions-for-the-ea-leadership\">disillusionment in the movement</a>, helping EA to retain talent.</p>\n<p>This is not far-flung utopianism or ivory-tower theory, it is how millions of people have successfully <a href=\"https://en.wikipedia.org/wiki/Participatory_democracy\">lived</a> and <a href=\"https://en.wikipedia.org/wiki/Cooperative\">worked</a> across the world for hundreds of years, plausibly for as long as humans have <a href=\"https://www.goodreads.com/en/book/show/56269264\">existed</a>.</p>\n<p>Even movements that are (at best) agnostic on the subject of democracy, for instance Marxist-Leninist political parties, frequently have votes on constitutional and strategic issues, as well as leaders that are both elected and recallable by the membership. It is possible to have doubts about substantial democratising reforms without wishing to retain the overly top-down status quo.</p>\n<p>Thus, the final point. We can talk all we like, but at the moment we have the system that we have, and solving structural problems will require the consent of those most empowered by those structures.</p>\n<p>Therefore, the final part of this section is addressed directly to them.</p>\n<p>Most of the people with power in the EA movement have been pivotal in building it. They have expanded EA from a basement in Oxford to the vibrant global community we see today. They are genuine inspirations to many of us (even when we disagree with some of their decisions) and some of us joined the movement as a direct result of the examples they set. But what worked before doesn\u2019t work now, and we have tools at our disposal that are better-suited to the situation EA now finds itself in.</p>\n<p>We now have a movement of thousands of smart, passionate, and dedicated people who often make considerable personal sacrifices in order to do as much good as possible. Our views matter as well.</p>\n<p>We need to take full advantage of our greatest source of judgement and insight: ourselves. If we don\u2019t, we risk condemning our movement to a slow calcification and decline. This is how we build a sustainable, dynamic, and mature movement.</p>\n<p>If you believe in this community, you should believe in its ability to make its own decisions.</p>\n<h1>Conclusion</h1>\n<p>The Effective Altruism movement has already contributed to major improvements in the world and to humanity\u2019s trajectory going forward. However, our current impact pales in comparison to the enormous potential EA has to change the world for the better over the coming years. To do this, EA must be able to accurately analyse the world and act accordingly. Important steps have been made towards this goal already, but they are not enough.</p>\n<p>As it stands, EA neglects the importance of <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Epistemic_health_is_a_community_issue\">collective epistemics</a> and overemphasises individual rationality, and as a result cultivates a community that is <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_are_incredibly_homogenous\">homogenous</a>, <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Decentralised_in_theory__centralised_in_practice\">hierarchical</a>, and <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_should_probably_read_more_widely\">intellectually insular</a>. EA is overwhelmingly white, male, upper-middle-class, and of a narrow range of (typically quantitative) academic backgrounds. EA reading lists and curricula over-emphasise a very narrow range of authors, which are assumed to be highly intelligent and thus to be deferred to. This narrows the scope of acceptable thought, and generates path dependencies and intellectual blind-spots.</p>\n<p>The term \u201cvalue-alignment\u201d often hides an implicit community <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#What_actually_is__value_alignment__\">orthodoxy</a>: a commitment to a particular package of views, including not just effective altruism but also longtermism, utilitarianism, Rationalist-derived epistemics, liberal-technocratic philanthropy, Whig historiography, the ITN framework, and the Techno-Utopian Approach to existential risk. Subscription to this package is a very different thing to being committed to \u201cdoing the most good\u201d, but the two are treated as interchangeable. Hiring and funding practices that select for \u201cvalue-aligned\u201d individuals thus cultivate a <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_are_incredibly_homogenous\">homogenous</a> and orthodox community.</p>\n<p>EA is very open to \u201cshallow\u201d technical critiques. <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#EA_is_very_open_to_some_kinds_of_critique_and_very_not_open_to_others\">\u201cDeep\u201d critiques</a>, in contrast, can dispute core EA beliefs and practices, criticise prominent EAs, and even question capitalism. These are much more likely to be rejected out of hand or treated as hostile, and EA has a suite of rhetorical and structural methods for dismissing them. These problems are magnified by <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#The_EA_Forum_structurally_discourages_deep_critique\">the structure of the EA Forum</a>, which gives some (typically quite senior and/or orthodox) community-members far more voting power than others.</p>\n<p>The power of a <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Decentralised_in_theory__centralised_in_practice\">small number</a> of comparatively orthodox <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Using_the_right__grantmaking__tools_for_the_right__grantmaking__jobs\">grantmakers</a> makes raising important concerns dangerous for one\u2019s career and community membership. Given how EA dominates many members\u2019 career trajectories, social lives, and identities, making such \u201cdeep critiques\u201d is simply not worth the risk. EA loses out on many valuable projects and updating opportunities due to the consequent <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Why_do_critical_EAs_have_to_use_pseudonyms_\">chilling effect</a>.</p>\n<p>EA\u2019s focus on the <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_can_t_put_numbers_on_everything_\">quantitative</a> is powerful when addressing problems suitable for quantification, but can cause serious inaccuracy and overconfidence in others. This is particularly visible in <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Using_the_right__grantmaking__tools_for_the_right__grantmaking__jobs\">grantmaking</a>, where problems range from the blindspots generated by siloed thinking to multiple methodological issues associated with the ITN framework.</p>\n<p>The major issue, however, is how we <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_can_t_put_numbers_on_everything_\">over-optimise</a> interventions on the basis of <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Using_the_right__grantmaking__tools_for_the_right__grantmaking__jobs\">doubtful</a> numerical estimates made in inappropriate information environments, which is highly concerning due to the stakes involved. Characteristics of existential risk \u2013 deep uncertainty, high complexity, long timelines, poorly-defined phenomena, and low-probability high-impact events \u2013 are just those in which robustness-focused strategies outperform optimising ones.</p>\n<p>There is a <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_need_to_stop_reinventing_the_wheel\">wealth</a> of available material on how to act under such circumstances, from foresight methodologies to vulnerability reduction practices to robust decision-making tools, but these are neglected because of EA\u2019s <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_should_probably_read_more_widely\">intellectual insularity</a> as well as the Founder Effect. There are many other <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Some_ideas_we_should_probably_pay_more_attention_to\">disciplines and practices</a> that would be valuable to EA, but the above social-epistemic problems as well as narrow and homogenous reading lists and media diets cause them to be unknown to or ignored by much of the community.</p>\n<p>EA can <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_need_to_value_expertise_and_rigour_more\">confuse</a> value-alignment and seniority with expertise. Orthodox EA positions on some highly consequential issues are derived from unrigorous blogposts by prominent EAs with little or no relevant training. They use methods and come to conclusions that would be considered fringe by the relevant expert communities, but this is not adequately questioned because of (1) EA\u2019s disciplinary homogeneity and intellectual insularity preventing EAs from coming across opposing perspectives, and (2) inappropriate deference and unwarranted assumptions about the superiority of EA rationality (and thus EA competence) causing external expert perspectives to be dismissed. Elsewhere, expertise is appealed to inconsistently to justify pre-decided positions, and powerful people are treated as authorities on topics for which they have no relevant qualifications or experience.</p>\n<p>Our intellectual insularity, <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_should_probably_read_more_widely\">narrow</a> <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_can_t_put_numbers_on_everything_\">conception</a> of \u201cgood thinking\u201d, and overconfidence can make engagement with EA difficult and exhausting for domain-experts , and they often withdraw quickly, seeing EAs as \u201cweird\u201d people \u201cdoing their own thing, I guess\u201d, or burning themselves out trying to be heard.<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-79\" id=\"fnref-pMAQ8odjQviWs2qYJ-79\">[79]</a></sup> This encourages further overconfidence and allows us to believe orthodox views are better-substantiated than they actually are.</p>\n<p>Several viewpoints common within EA, including liberal-technocratic politics, a preference for speculative technofixes, and a belief in the overwhelming importance of AI alignment align suspiciously well with the interests and desires of both our tech-billionaire <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_align_suspiciously_well_with_the_interests_of_tech_billionaires__and_ourselves__61_\">donors and ourselves</a>. EA institutions fail to critique the ethical and political implications of our donors\u2019 wealth and power, and what used to be a movement based on frugality has evolved into one in which we receive very healthy salaries as well as enviable benefits. This raises the spectre of motivated reasoning, which we are particularly vulnerable to as a result of our heavy reliance on  sometimes controversial or untested quantitative tools like <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_can_t_put_numbers_on_everything_\">Individual Bayesian Thinking</a>. Subjectively-generated, empirically un-bounded quantifications make it easy to rationalise and/or justify anything by coming up with appropriate \u201crough estimates'' of incredibly uncertain values.</p>\n<p>Our movement, while not formally hierarchical, vests the vast majority of power in the hands of a <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Decentralised_in_theory__centralised_in_practice\">small number</a> of individuals within a tight cluster of social and professional networks. This makes us particularly susceptible to revolving-door dynamics and conflicts of interest.</p>\n<p><a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Deciding_together_better\">Decision-making</a> is opaque, unaccountable to the membership, and almost invariably top-down. This pattern of decision-making is associated with a wealth of ethical, psychological, and historical problems, and has already incurred serious risks to the movement.</p>\n<p>There are <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Deciding_together_better\">several techniques</a> for increasing deliberation and democracy within the movement, including consensus-building tools, sortition assemblies, and employee self-management. These are very well supported by empirical and theoretical research as well as numerous practical examples, and are likely to instrumentally improve decision-making outcomes.</p>\n<p>As long as the problems we describe remain in place, EA will continue to alienate newcomers and limit its impact. Feedback loops may cause EA to become ever-more homogenous, hierarchical, insular, and narrow, locking us onto an ever-more rigid trajectory. Solving many of these problems will require the consent of those most empowered by them.</p>\n<p>History holds many examples of organised groups of intelligent, well-educated, well-intentioned people causing considerable amounts of harm, from liberal eugenics to Marxism-Leninism. EA has already been involved in a number of scandals, and we have the potential to cause tremendous harm given our growing power, from playing down the importance of climate change to speeding up AGI development, from legitimising, empowering, and funding \u201c<a href=\"https://www.bbc.com/future/article/20211014-agents-of-doom-who-is-hastening-the-apocalypse-and-why\">Agents of Doom</a>\u201d to undercutting movements for social change.</p>\n<p>If we are to hold power, we need to be able to wield it wisely.</p>\n<p>The FTX crash was a shock to all of us, and we have to use this painful but valuable opportunity to change our movement for the better. We may not get a chance like this again.</p>\n<h1>Coda</h1>\n<p>There are thousands of people alive today who wouldn\u2019t be if it wasn\u2019t for EA. There are millions of animals in better living conditions because of our community. Risks that threaten the very existence of our species are on the global agenda thanks in part to our movement.</p>\n<p>A few thousand people, dedicated, energetic and caring, have done this. If we play our cards well and choose the right path now, this may only be the beginning.</p>\n<p>We need to choose carefully, though: countless people, innumerable animals, and perhaps even the future of our species may depend on it.</p>\n<p>We have grown and gained power before we have gained wisdom. It is now time for us to mature as we grow and age. Things won\u2019t be easy, but change is not just possible, but necessary.</p>\n<p>One step, then another, then another.</p>\n<h1>Suggested reforms</h1>\n<p>Below, we have a preliminary non-exhaustive list of suggestions for structural and cultural reform that we think may be a good idea and should certainly be discussed further.</p>\n<p>It is of course plausible that some of them would not work; if you think so for a particular reform, please explain why! We would like input from a range of people, and we certainly do not claim to have all the answers!</p>\n<p>In fact, we believe it important to open up a conversation about plausible reforms not because we have all the answers, but precisely because we don\u2019t.</p>\n<p><em>Italics</em> indicates reforms strongly inspired by or outright stolen from Zoe Cremer\u2019s <a href=\"https://docs.google.com/document/d/1Y9opezUk9_JNQBYCAmHoERiqlaf2HmashQQ_ydGwyRY/edit\">list</a> of structural reform ideas. Some are edited or merely related to her ideas; they should not be taken to represent Zoe\u2019s views.</p>\n<p>Asterisks (*) indicate that we are less sure about a suggestion, but sure enough that we think they are worth considering seriously, e.g. through deliberation or research. Otherwise, we have been developing or advocating for most of these reforms for a long time and have a reasonable degree of confidence that they should be implemented in some form or another.</p>\n<p>Timelines are suggested to ensure that reforms can become concrete. If stated, they are rough estimates, and if there are structural barriers to a particular reform being implemented within the timespan we suggest, let us know!</p>\n<p>Categorisations are somewhat arbitrary, we just needed to break up the text for ease of reading.</p>\n<h2>Critique</h2>\n<h3>General</h3>\n<ul>\n<li>EAs must be more willing to make deep critiques, both in private and in public\n<ul>\n<li>You are not alone, you are not crazy!</li>\n<li>There is a much greater diversity of opinion in this community than you might think</li>\n<li>Don\u2019t assume that the people in charge must be smarter than you, and that you must be missing something if you disagree \u2013 even most of them don\u2019t think that!</li>\n</ul>\n</li>\n<li>EA must be open to deep critiques as well as shallow critiques\n<ul>\n<li>We must temper our knee-jerk reactions against deep critiques, and be curious about our emotional reactions to arguments \u2013 \u201cWhy does this person disagree with me? Why am I so instinctively dismissive about what they have to say?\u201d</li>\n<li>We must be willing to accept the possibility that \u201cbig\u201d things may need to be fixed and that some of our closely-held beliefs are misguided</li>\n<li>Our willingness to consider a critique should be orthogonal to the seniority of the authors of the subject(s) of that critique</li>\n<li>When we reject critiques, we should present our reasons for doing so</li>\n</ul>\n</li>\n<li>EAs should read more deep critiques of EA, especially external ones\n<ul>\n<li>For instance <a href=\"https://ineffectivealtruismblog.com/\">this</a> blog and <a href=\"https://www.goodreads.com/book/show/63392758-the-good-it-promises-the-harm-it-does\">this</a> forthcoming book</li>\n</ul>\n</li>\n<li>EA should cut down its overall level of tone/language policing\n<ul>\n<li>Norms should still be strongly in favour of civility and good-faith discourse, but anger or frustration cannot be grounds for dismissal, and deep critique must not be misinterpreted as aggression or \u201csignalling\u201d</li>\n<li>Civility must not be confused with EA ingroup signalling</li>\n<li>Norms must be enforced consistently, applying to senior EAs just as much as newcomers</li>\n</ul>\n</li>\n<li>EAs should make a conscious effort to avoid (subconsciously/inadvertently) using rhetoric about how \u201cEA loves criticism\u201d as a shield against criticism\n<ul>\n<li>Red-teaming contests, for instance, are very valuable, but we should avoid using them to claim that \u201csomething is being done\u201d about criticism and thus we have nothing to worry about</li>\n<li>\u201cIf we are so open to critique, shouldn\u2019t we be open to this one?\u201d</li>\n<li>EAs should avoid delaying reforms by professing to take critiques very seriously without actually acting on them</li>\n</ul>\n</li>\n<li>EAs should state their reasons when dismissing critiques, and should be willing to call out other EAs if they use the rhetoric of rigour and even-handedness without its content</li>\n<li>EAs, especially those in community-building roles, should send credible/costly signals that EAs can make or agree with deep critiques without being excluded from or disadvantaged within the community</li>\n<li>EAs should be cautious of knee-jerk dismissals of attempts to challenge concentrations of power, and seriously engage with critiques of capitalist modernity</li>\n<li>EAs, especially prominent EAs, should be willing to cooperate with people writing critiques of their ideas and participate in <a href=\"https://en.wikipedia.org/wiki/Adversarial_collaboration\">adversarial collaborations</a></li>\n<li>EA institutions and community groups should run discussion groups and/or event programmes on how to do EA better</li>\n</ul>\n<h3>Institutions</h3>\n<ul>\n<li>Employees of EA organisations should not be pressured by their superiors to not publish critical work</li>\n<li>Funding bodies should enthusiastically fund deep critiques and other heterodox/\u201cheretical\u201d work</li>\n<li>EA institutions should commission or be willing to fund large numbers of zero-trust investigations by domain-experts, especially into the components of EA orthodoxy</li>\n<li><em>EA should set up a&nbsp;counter foundation that has as its main goal critical reporting, investigative journalism and \u201ccounter research\u201d about EA and other philanthropic institutions <em>[within 12 months]</em>*</em>\n<ul>\n<li><em>This body should be run by independent people and funded by its own donations, with a \u201cfloor\u201d proportional to other EA funding decisions (e.g. at least one researcher/community manager/grant program, admin fees in a certain height)</em></li>\n<li>If this foundation is established, EA institutions should cooperate with it</li>\n</ul>\n</li>\n<li>EA institutions should recruit known critics of EA and offer them e.g. a year of funding to write up long-form deep critiques</li>\n<li>EA should establish public conference(s) or assemblies for discussing reforms within 6 months, with open invitations for EAs to attend without a selection process. <em>For example, an \u201conline forum of concerns\u201d:</em>\n<ul>\n<li><em>Every year invite all EAs to raise any worries they have about EA central organisations</em></li>\n<li><em>These organisations declare beforehand that they will address the top concerns and worries, as voted by the attendees</em></li>\n<li><em>Establish voting mechanism, e.g. upvotes on worries that seem most pressing</em></li>\n</ul>\n</li>\n</ul>\n<h3>Red Teams</h3>\n<ul>\n<li>EA institutions should establish clear mechanisms for feeding the results of red-teaming into decision-making processes within 6 months</li>\n<li>Red teams should be paid, composed of people with a variety of views, and former- or non-EAs should be actively recruited for red-teaming\n<ul>\n<li>Interesting critiques often come from dissidents/exiles who left EA in disappointment or were pushed out due to their heterodox/\u201dheretical\u201d views (yes, this category includes a couple of us)</li>\n</ul>\n</li>\n<li>The judging panels of criticism contests should include people with a wide variety of views, including heterodox/\u201dheretical\u201d views</li>\n<li>EA should use criticism contests as one tool among many, particularly well-suited to eliciting highly specific shallow critiques</li>\n</ul>\n<h2>Epistemics</h2>\n<h3>General</h3>\n<ul>\n<li>EAs should see EA as a set of intentions and questions (\u201cWhat does it mean to \u2018do the most good\u2019, and how can I do it?\u201d) rather than a set of answers (\u201cAI is the highest-impact cause area, then maybe biorisk.\u201d)</li>\n<li>EA should study social epistemics and collective intelligence more, and epistemic efforts should focus on creating good community epistemics rather than merely good individual epistemics\n<ul>\n<li>As a preliminary programme, we should explore how to increase EA\u2019s overall levels of diversity, egalitarianism, and openness</li>\n</ul>\n</li>\n<li>EAs should practise epistemic modesty\n<ul>\n<li>We should read much more, and more widely, including authors who have no association with (or even open opposition to) the EA community</li>\n<li>We should avoid assuming that EA/Rationalist ways of thinking are the only or best ways</li>\n<li>We should actively seek out not only critiques of EA, but critiques of and alternatives to the underlying premises/assumptions/characteristics of EA (high modernism, elite philanthropy, quasi-positivism, etc.)</li>\n<li>We should stop assuming that we are smarter than everybody else</li>\n</ul>\n</li>\n<li>When EAs say \u201cvalue-aligned\u201d, we should be clear about what we mean\n<ul>\n<li>Aligned with what values in particular?</li>\n<li>We should avoid conflating the possession of the general goal of \u201cdoing the most good\u201d with subscription to the full package of orthodox views</li>\n</ul>\n</li>\n<li>EAs should consciously separate:\n<ul>\n<li>An individual\u2019s suitability for a particular project, job, or role</li>\n<li>Their expertise and skill in the relevant area(s)</li>\n<li>The degree to which they are perceived to be \u201chighly intelligent\u201d</li>\n<li>Their perceived level of value-alignment with EA orthodoxy</li>\n<li>Their seniority within the EA community</li>\n<li>Their personal wealth and/or power</li>\n</ul>\n</li>\n<li>EAs should make a point of engaging with and listening to EAs from underrepresented disciplines and backgrounds, as well as those with heterodox/\u201cheretical\u201d views</li>\n<li>The EA Forum should have its karma/commenting system reworked to remove structural forces towards groupthink within 3 months. Suggested specific reforms include, in gently descending order of credence:\n<ul>\n<li>Each user should have equal voting weight</li>\n<li>Separate agreement karma should be implemented for posts as well as comments</li>\n<li>A \u201csort by controversial\u201d option should be implemented</li>\n<li>Low-karma comments should not be hidden</li>\n<li>Low-karma comments should be occasionally shunted to the top</li>\n</ul>\n</li>\n<li><em>EA should embark on a large-scale exploration of \u201ctheories of change\u201d: what are they, how do other communities conceptualise and use them, and what constitutes a \u201cgood\u201d one? This could include:*</em>\n<ul>\n<li><em>Debates</em></li>\n<li><em>Lectures from domain-experts</em></li>\n<li><em>Panel discussions</em></li>\n<li><em>Series of forum posts</em></li>\n<li><em>Hosting of experts by EA institutions</em></li>\n<li><em>Competitions</em></li>\n<li><em>EAG framed around these questions</em></li>\n<li><em>Etc.</em></li>\n</ul>\n</li>\n<li>When EA organisations commission research on a given question, they should publicly pre-register their responses to a range of possible conclusions</li>\n</ul>\n<h3>Ways of Knowing</h3>\n<ul>\n<li>EAs should consider how our shared modes of thought may subconsciously affect our views of the world \u2013 what blindspots and biases might we have created for ourselves?</li>\n<li>EAs should increase their awareness of their own <a href=\"https://en.wikipedia.org/wiki/Standpoint_theory\">positionality</a> and subjectivity, and pay far more attention to e.g. postcolonial critiques of western academia\n<ul>\n<li>History is <a href=\"https://www.youtube.com/watch?v=Xe3tunGi4To\">full</a> of people who thought they were very rational saying very silly and/or unpleasant things: let\u2019s make sure that doesn\u2019t include us</li>\n</ul>\n</li>\n<li>EAs should study other ways of knowing, taking inspiration from a range of academic and professional communities as well as indigenous worldviews</li>\n</ul>\n<h3>Quantification</h3>\n<ul>\n<li>EAs should not assume that we must attach a number to everything, and should be curious about why most academic and professional communities do not\n<ul>\n<li>We should study cost-benefit trade-offs of quantification (e.g. ease of comparison/analysis vs information loss &amp; category errors) and learn from other communities about how to best integrate numerical data with other kinds of information</li>\n</ul>\n</li>\n<li>Bayes\u2019 theorem should be applied where it works\n<ul>\n<li>EA institutions should commission studies (preferably by independent statisticians, psychologists, philosophers of probability, etc.) into the circumstances under which individual subjective Bayesian reasoning actually outperforms other modes of thought, by what criteria, and how this varies by subject/domain</li>\n<li>Until (and indeed after) the conclusions of these studies are published, EAs should be aware of the criticisms of such techniques, and should not confuse the use of a particular (heavily contested) thinking-tool with \u201ctransparent reasoning\u201d, and certainly not with the mere application of \u201crationality\u201d itself</li>\n<li>Important decisions should not be based upon controversial yet-to-be-proven techniques, especially under conditions that current evidence suggests they are ill-suited to</li>\n</ul>\n</li>\n<li>EAs should explore decision theories beyond expected value reasoning, as well as other ways of acting optimally in different environments\n<ul>\n<li>The goal of maximising value is orthogonal to scientific claims about our ability to actually accurately predict levels of value under particular conditions</li>\n</ul>\n</li>\n<li>When EAs make numerical estimates or forecasts, we should be wholly transparent about the reasoning processes, data, and assumptions we used to generate them\n<ul>\n<li>We should avoid uncritically repeating the estimates of senior EAs, especially when such transparency of reasoning is not present</li>\n<li>We should be very specific about the events our probabilities actually refer to, given sensitivities to problem-framing</li>\n<li>Where phenomena remain poorly-defined, they should be clarified (e.g. through foresight exercises) as a matter of priority, rather than generating a probability anyway and labelling it a \u201crough estimate\u201d</li>\n</ul>\n</li>\n<li>EAs should be wary of the potential for highly quantitative forms of reasoning to (comparatively easily) justify anything\n<ul>\n<li>We should be extremely cautious about e.g. high expected value estimates, very low probabilities being assigned to heterodox/\u201cheretical\u201d views, and <a href=\"https://www.investopedia.com/terms/r/risk-of-ruin.asp\">ruin risks</a></li>\n<li>We should look into methods of putting hard ethical and theoretical boundaries on numbers, e.g. refusing to undertake actions with a ruin risk above x%, regardless of the results of expected value calculations</li>\n<li>We should use Bayesian reasoning where it works (see above)</li>\n</ul>\n</li>\n</ul>\n<h3>Diversity</h3>\n<ul>\n<li>EA institutions should select for diversity\n<ul>\n<li>With respect to:\n<ul>\n<li>Hiring (especially grantmakers and other positions of power)</li>\n<li>Funding sources and recipients</li>\n<li>Community outreach/recruitment</li>\n</ul>\n</li>\n<li>Along lines of:\n<ul>\n<li>Academic discipline</li>\n<li>Educational &amp; professional background</li>\n<li>Personal background (class, race, nationality, gender, etc.)</li>\n<li>Philosophical and political beliefs</li>\n</ul>\n</li>\n<li>Naturally, this should not be unlimited \u2013 some degree of mutual similarity of beliefs is needed for people to work together \u2013 but we do not appear to be in any immediate danger of becoming too diverse</li>\n</ul>\n</li>\n<li>Previous EA involvement should not be a necessary condition to apply for specific roles, and the job postings should not assume that all applicants will identify with the label \u201cEA\u201d</li>\n<li>EA institutions should hire more people who have had little to no involvement with the EA community providing that they care about doing the most good</li>\n<li>People with heterodox/\u201cheretical\u201d views should be actively selected for when hiring to ensure that teams include people able to play \u201cdevil\u2019s advocate\u201d authentically, reducing the need to rely on highly orthodox people accurately steel-manning alternative points of view</li>\n<li>Community-building efforts should be broadened, e.g. involving a wider range of universities, and group funding should be less contingent on the perceived prestige of the university in question and more focused on the quality of the proposal being made</li>\n<li>EA institutions and community-builders should promote diversity and inclusion more, including funding projects targeted at traditionally underrepresented groups</li>\n<li>A greater range of people should be invited to EA events and retreats, rather than limiting e.g. key networking events to similar groups of people each time</li>\n<li><em>There should be a survey on cognitive/intellectual diversity within EA</em></li>\n<li>EAs should not make EA the centre of their lives, and should actively build social networks and career capital outside of EA</li>\n</ul>\n<h3>Openness</h3>\n<ul>\n<li>Most challenges, competitions, and calls for contributions (e.g. cause area exploration prizes) should be posted where people not directly involved within EA are likely to see them (e.g. Facebook groups of people interested in charities, academic mailing lists, etc.)</li>\n<li>Speaker invitations for EA events should be broadened away from (high-ranking) EA insiders and towards, for instance:\n<ul>\n<li>Subject-matter experts from outside EA</li>\n<li>Researchers, practitioners, and stakeholders from outside of our elite communities\n<ul>\n<li>For instance, we need a far greater input from people from Indigenous communities and the Global South</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><em>External speakers/academics who disagree with EA should be invited give keynotes and talks, and to participate in debates with prominent EAs</em></li>\n<li>EAs should make a conscious effort to seek out and listen to the views of non-EA thinkers\n<ul>\n<li>Not just to respond!</li>\n</ul>\n</li>\n<li>EAs should remember that EA covers one very small part of the huge body of human knowledge, and that the vast majority of interesting and useful insights about the world have and will come from outside of EA</li>\n</ul>\n<h2>Expertise &amp; Rigour</h2>\n<h3>Rigour</h3>\n<ul>\n<li>Work should be judged on its quality, rather than the perceived intelligence, seniority or value-alignment of its author\n<ul>\n<li>EAs should avoid assuming that research by EAs will be better than research by non-EAs by default</li>\n</ul>\n</li>\n<li>EAs should place a greater value on scientific rigour\n<ul>\n<li>We should use blogposts, Google Docs, and similar works as accessible ways of opening discussions and providing preliminary thoughts, but rely on peer-reviewed research when making important decisions, creating educational materials, and communicating to the public</li>\n<li>When citing a blogpost, we should be clear about its scope, be careful to not overstate its claims, and not cite it as if it is comparable to a piece of peer-reviewed research</li>\n</ul>\n</li>\n<li>EAs should perform proper literature reviews, situate our claims within pre-existing literature, and when we make claims that deviate from expert consensus/norms we should explicitly state and justify this\n<ul>\n<li>The most valuable fringe ideas are extremely high-impact, but the mean fringe idea is likely to be net-negative</li>\n</ul>\n</li>\n<li>EA institutions should commission peer-reviewed research far more often, and be very cautious of basing decisions on shallow-dives by non-experts\n<ul>\n<li>For important questions, commission a person/team with relevant expertise to do a study and subject it to peer review</li>\n<li>For the most important/central questions, commission a structured expert elicitation</li>\n</ul>\n</li>\n</ul>\n<h3>Reading</h3>\n<ul>\n<li>Insofar as a \u201ccanon\u201d is created, it should be of the best-quality works on a given topic, not the best works by (orthodox) EAs about (orthodox) EA approaches to the topic\n<ul>\n<li>Reading lists, fellowship curricula, and bibliographies should be radically diversified</li>\n<li>We should search everywhere for pertinent content, not just the EA Forum, LessWrong, and the websites of EA orgs</li>\n<li>We should not be afraid of consulting outside experts, both to improve content/framing and to discover blind-spots</li>\n</ul>\n</li>\n<li>EAs should see fellowships as educational activities first and foremost, not just recruitment tools</li>\n<li>EAs should continue creating original fellowship ideas for university groups</li>\n<li>EAs should be more willing to read books and academic papers</li>\n</ul>\n<h3>Good Science</h3>\n<ul>\n<li>EAs should consider the impact of EA\u2019s cultural, historical, and disciplinary roots on its paradigmatic methods, assumptions, and prioritisations\n<ul>\n<li>What are the historical roots of our current cause prioritisations and preferred methodologies?</li>\n<li>Why are we, for instance, so instinctively reductionist?</li>\n<li><strong>If existential risk and/or EA were to be reinvented from the ground up, what methods, disciplines, prioritisations, etc. would we choose?</strong></li>\n</ul>\n</li>\n<li>EAs should value empiricism more, and be cautious of assuming that all important aspects of a topic can be derived from first principles through the proper application of rationality</li>\n<li>EAs should be curious about why communities with decades of experience studying problems (similar to the ones) we study do things the ways that they do</li>\n<li>EAs, especially those working in existential risk, should draw from the disciplines listed <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Some_ideas_we_should_probably_pay_more_attention_to\">above</a>:\n<ul>\n<li>Disaster Risk Reduction</li>\n<li>Resilience Theory</li>\n<li>Complex Adaptive Systems</li>\n<li>Futures &amp; Foresight</li>\n<li>Decision-Making under Deep Uncertainty and Robust Decision-Making</li>\n<li>Psychology &amp; Neuroscience</li>\n<li>Science &amp; Technology Studies</li>\n<li>The Humanities and Social Sciences in general</li>\n</ul>\n</li>\n<li>EAs should re-examine the siloing of issues under specific \u201ccause areas\u201d, and avoid relegating non-specific-hazard-focused existential risk work to a homogenous and de-valued \u201cmisc and meta\u201d category\n<ul>\n<li>Often separation of causes is warranted (shrimp welfare advocacy is unlikely to have a major impact on AI risk), but our desire to categorise and understand the world can lead us to create artificial boundaries</li>\n</ul>\n</li>\n</ul>\n<h3>Experts &amp; Expertise</h3>\n<ul>\n<li>EAs should deliberately broaden their social/professional circles to include external domain-experts with differing views</li>\n<li>EAs should be be consistent when appealing to expertise, and be cautious of subconsciously using it selectively to confirm our biases</li>\n<li>EA institutions should have their policy recommendations vetted by external experts and/or panels of randomly-selected EAs before they are promoted by the Centre for Long-Term Resilience, Simon Institute, etc.*</li>\n<li>When hiring for research roles at medium to high levels, EA institutions should select in favour of domain-experts, even when that means passing over a highly \u201cvalue-aligned\u201d or prominent EA</li>\n</ul>\n<h2>Funding &amp; Employment</h2>\n<h3>Finance</h3>\n<ul>\n<li>EAs should take care not to confuse the total net worth of EA donors with the actual resources of the EA community, especially given how much net worth can vary with e.g. share values</li>\n<li>Donors should commit a large proportion of their wealth to EA bodies or trusts controlled by EA bodies to provide EA with financial stability and as a costly signal of their support for EA ideas</li>\n<li>Funding bodies should be far more selective of donors, based on:\n<ul>\n<li>Their personal ethics records</li>\n<li>The ethical consequences and implications of their work</li>\n<li>Their personal trustworthiness and reliability</li>\n<li>The likely stability of their wealth</li>\n</ul>\n</li>\n<li>Funding bodies should within 6 months publish lists of sources they will not accept money from, regardless of legality\n<ul>\n<li>Tobacco?</li>\n<li>Gambling?</li>\n<li>Mass surveillance?</li>\n<li>Arms manufacturing?</li>\n<li>Cryptocurrency?</li>\n<li>Fossil fuels?</li>\n</ul>\n</li>\n<li>Funding bodies should take advice on how to avoid inadvertently participating in \u201cethics-washing\u201d, and publish the policies that result</li>\n<li>The big funding bodies (OpenPhil, EA Funds, etc.) should be disaggregated into smaller independent funding bodies within 3 years</li>\n<li><em>EA institutions should each reduce their reliance on EA funding sources and tech billionaires by 50% within the next 5 years</em>\n<ul>\n<li><em>This ensures you need to convince non-members that your work is of sufficient quality and relevance</em></li>\n<li>This also greatly increases the resilience of the EA movement, as institutions would no longer all be dependent on the same small number of funding sources</li>\n</ul>\n</li>\n</ul>\n<h3>Grantmaking</h3>\n<ul>\n<li>Grantmakers should be radically diversified to incorporate EAs with a much wider variety of views, including those with heterodox/\u201dheretical\u201d views</li>\n<li>Funding frameworks should be reoriented towards using the \u201cright tool for the right job\u201d\n<ul>\n<li>\n<p>Optimisation appears entirely appropriate in well-understood, predictable domains, e.g. public health interventions against epidemic diseases<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-80\" id=\"fnref-pMAQ8odjQviWs2qYJ-80\">[80]</a></sup></p>\n</li>\n<li>\n<p>But robustness is far superior when addressing domains of deep uncertainty, areas of high complexity, low-probability high-impact events, long timescales, poorly-defined phenomena, and significant expert disagreement, e.g. existential risk</p>\n</li>\n<li>\n<p>Optimising actions should be taken on the basis of high-quality evidence, e.g. meta-reviews or structured expert elicitations, rather than being used as the default or even the only mode of operation</p>\n</li>\n</ul>\n</li>\n<li>Grantmaking organisations should commission independent external evaluations of the efficacy of their work (e.g. the success rates of grantmakers in forecasting the impact or success of projects) within 6 months, and release the results of any internal work they have done to this end</li>\n<li><em>Within 5 years, EA funding decisions should be made collectively</em>\n<ul>\n<li><em>First set up experiments for a safe cause area with small funding pots that are distributed according to different collective decision-making mechanisms</em>\n<ul>\n<li>For example rotating panels, various forms of lottocracy</li>\n<li><em>Subject matter experts are <strong>always</strong> used and weighed appropriately</em></li>\n</ul>\n</li>\n<li><em>Experiment in parallel with randomly selected samples of EAs evaluating the decisions of one existing funding committee</em>\n<ul>\n<li><em>Existing decision-mechanisms are thus \u2018passed through\u2019 an accountability layer</em></li>\n</ul>\n</li>\n<li><strong><em>All decision mechanisms should have a deliberation phase (arguments are collected and weighed publicly) and a voting phase (majority voting, quadratic voting, etc.)</em></strong></li>\n<li><em>Depending on the cause area and the type of choice, either fewer (experts + randomised sample of EAs) or more people (any EA or beyond) should take part in the funding decision</em></li>\n</ul>\n</li>\n<li>A certain proportion EA of funds should be allocated by lottery after a longlisting process to filter out the worst/bad-faith proposals*\n<ul>\n<li>The outcomes of this process should be evaluated in comparison to EA\u2019s standard grantmaking methods as well as other alternatives</li>\n</ul>\n</li>\n<li><em>Grantmaking should require detailed and comprehensive conflict of interest reporting</em></li>\n</ul>\n<h3>Employment</h3>\n<ul>\n<li>Funding bodies should not be able to hire researchers who have previously been recipients in the last e.g. 5 years, nor should funders be able to join recipient organisations within e.g. 5 years of leaving their post</li>\n<li>More people working within EA should be employees, with the associated legal rights and stability of work, rather than e.g. grant-dependent \u201cindependent researchers\u201d</li>\n<li>EA funders should explore the possibility of funding more stable, safe, and permanent positions, such as professorships</li>\n</ul>\n<h2>Governance &amp; Hierarchy</h2>\n<h3>Leadership</h3>\n<ul>\n<li>EAs should avoid hero-worshipping prominent EAs, and be willing to call it out among our peers\n<ul>\n<li>We should be able to openly critique senior members of the community, and avoid knee-jerk defence/deference when they are criticised</li>\n</ul>\n</li>\n<li>EA leaders should take active steps to minimise the degree of hero-worship they might face\n<ul>\n<li>For instance, when EA books or sections of books are co-written by several authors, co-authors should be given appropriate attribution</li>\n</ul>\n</li>\n<li>EAs should deliberately platform less well-known EAs in media work</li>\n<li>EAs should assume that power corrupts, and EAs in positions of power should take active steps to:\n<ul>\n<li>Distribute and constrain their own power as a costly signal of commitment to EA ideas rather than their position</li>\n<li>Minimise the corrupting influence of the power they retain and send significant costly signals to this effect</li>\n</ul>\n</li>\n<li><em>Fireside chats with leaders at EAG events should be replaced with:</em>\n<ul>\n<li><em>Panels/discussions/double-cruxing discussions involving a mix of:</em>\n<ul>\n<li><em>Prominent EAs</em></li>\n<li><em>Representatives of different EA organisations</em></li>\n<li><em>Less well-known EAs</em></li>\n<li><em>External domain-experts</em></li>\n</ul>\n</li>\n<li><em>Discussions between leaders and unknown EAs</em></li>\n</ul>\n</li>\n</ul>\n<h3>Decentralisation</h3>\n<ul>\n<li>EA institutions should see EA ideas as things to be co-created with the membership and the wider world, rather than transmitted and controlled from the top down</li>\n<li>The community health team and grantmakers should offer community groups more autonomy, independence, and financial stability\n<ul>\n<li>Community-builders should not worry about their funding being cut if they disagree with the community health team or appear somewhat \u201cnon-value-aligned\u201d</li>\n</ul>\n</li>\n<li>EA media engagement should be decentralised\n<ul>\n<li>Community-builders and researchers should be offered media training, rather than being told to never speak to the press and always forward journalists to the CEA</li>\n</ul>\n</li>\n</ul>\n<h3>Democratisation</h3>\n<ul>\n<li>EA institutions should implement clear and accessible democratic mechanisms for constitutional change within 12 months</li>\n<li>EA leadership figures should be democratically accountable to the membership, including mechanisms for the membership able to elect them on a regular basis and to recall them if they underperform</li>\n<li><em>EA institutions should be democratised within 3 years, with strategic, funding, and hiring policy decisions being made via democratic processes rather than by the institute director or CEO</em></li>\n<li>EAs should be more open to institutions and community groups being run democratically or non-hierarchically\n<ul>\n<li>One experienced person and three comparatively inexperienced people will probably produce better answers together than the experienced person would alone</li>\n</ul>\n</li>\n<li>EA institutions should consider running referenda for the most consequential decisions (e.g. a large fraction of EA funds being used to help buy a social networking site)</li>\n<li>EA institutions should consider having AGMs where the broader EA community can input into decision making</li>\n</ul>\n<h2>Transparency &amp; Ethics</h2>\n<h3>Community</h3>\n<ul>\n<li>EAs should expect their institutions to be transparent and open</li>\n<li>EAs should make an effort to become more aware of EA\u2019s cultural links to eugenic, reactionary and right-wing accelerationist politics, and take steps to identify areas of overlap or inheritance in order to avoid indirectly supporting such views or inadvertently accepting their framings</li>\n</ul>\n<h3>Institutions</h3>\n<ul>\n<li>EA institutions should make their decision-making structures transparent within 6 months, and be willing to publicly justify their decisions</li>\n<li><em>EA institutions should list all of their funding sources (past and present) on their websites, including how much was received from each source, within 6 months</em></li>\n<li>The minutes of grantmaker meetings should be made public*</li>\n<li>There should be a full public mapping of all EA institutions\n<ul>\n<li>Who works or has worked at which organisations</li>\n<li>Which organisations fund or have funded which others, when, and how much</li>\n<li>Who is or has been on which boards of directors</li>\n<li>Which organisations are or have been subsidiaries of other organisations</li>\n<li>Etc.</li>\n</ul>\n</li>\n<li><em>EA institutions should increase transparency over</em>\n<ul>\n<li><em>Who gets accepted/rejected to EAG and similar events and why</em></li>\n<li><em>Leaders/coordination forums</em></li>\n</ul>\n</li>\n<li>EA institutions should set up regular independent audits and assessments within 12 months\n<ul>\n<li>We\u2019re a movement that grew out of evaluating charities, it\u2019s only fair we hold ourselves to the same standards</li>\n</ul>\n</li>\n<li><em>Quality journalists should be given full access to EA institutions to investigate*</em></li>\n<li><em>EA institutions should set up whistleblower protection schemes for members of EA organisations within 6 months</em>\n<ul>\n<li><em>Legal, financial and social support for those who want to come forward to make information public that is in the public interest</em></li>\n<li>EA should explore the pros and cons of appointing an independent <a href=\"https://en.wikipedia.org/wiki/Organizational_ombudsman\">ombudsman</a>, and the results of this exploration should be published within 12 months*</li>\n</ul>\n</li>\n<li>EA organisations should enable vetting and oversight by people external to the EA community, and be accountable to the wider public more generally. This could be achieved through, for instance:\n<ul>\n<li>Providing clear statements about how decisions/funding allocations were/are made</li>\n<li>Taking advice on how this is done outside the EA community, e.g. in academia, industry, and NGOs</li>\n</ul>\n</li>\n</ul>\n<h3>Moral Uncertainty</h3>\n<ul>\n<li>EAs should practise moral uncertainty/pluralism as well as talking about it</li>\n<li>EAs who advocate using ethical safeguards such as \u201cintegrity\u201d and \u201ccommon-sense morality\u201d should publicly specify what they mean by this, how it should be operationalised, and where the boundaries lie in their view</li>\n<li>EA institutions that subscribe to moral uncertainty/pluralism should publish their policies for weighting different ethical views within 12 months</li>\n</ul>\n<p>Let us know anything that needs explaining or clarifying, and especially which high-impact changes we have missed!<sup class=\"footnote-ref\"><a href=\"#fn-pMAQ8odjQviWs2qYJ-81\" id=\"fnref-pMAQ8odjQviWs2qYJ-81\">[81]</a></sup></p>\n<h1>Contact Us</h1>\n<p>If you have any questions or suggestions about this article, EA, or anything else, feel free to email us at <strong><a href=\"mailto:concernedEAs@proton.me\">concernedEAs@proton.me</a></strong></p>\n<p>&lt; &gt;</p>\n<h2>Notes</h2>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-pMAQ8odjQviWs2qYJ-1\" class=\"footnote-item\"><p>At least, it was supposed to be the final draft. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-2\" class=\"footnote-item\"><p>In general, we think most of our points apply to most of EA, but far more to the longtermist side than the global poverty/animal welfare communities. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-3\" class=\"footnote-item\"><p>Indeed, there is significant doubt about whether awareness of cognitive biases actually reduces one\u2019s susceptibility to them. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-4\" class=\"footnote-item\"><p>This imbalance is interesting given the popularity of \u201cSuperforecasting'' within EA, where the vital importance of collaboration and the social organisation of that collaboration is <a href=\"https://www.goodreads.com/book/show/23995360-superforecasting\">well known</a>, as well as the value assigned to improving institutional decision-making. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-5\" class=\"footnote-item\"><p>There is also a lot of relevant work in social epistemics and the philosophy of science. See, for instance, Longino\u2019s (<a href=\"https://www.degruyter.com/document/doi/10.1515/9780691209753/html\">1990</a>) criteria for objectivity in scientific communities: (1) recognised avenues of criticism, (2) shared standards, (3) community responsiveness to criticism, and (4) equality of intellectual authority. We contend that EA is much better at (1) and (2) than (3) and (4). <a href=\"#fnref-pMAQ8odjQviWs2qYJ-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-6\" class=\"footnote-item\"><p>For a deeper engagement with the term, see <a href=\"https://forum.effectivealtruism.org/posts/DxfpGi9hwvwLCf5iQ/objections-to-value-alignment-between-effective-altruists#The_Meaning_of_Value_Alignment_\">here</a>. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-6\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-7\" class=\"footnote-item\"><p>We\u2019ve seen the term \u201cheretical\u201d used to describe beliefs (held by EAs) that significantly deviate from EA orthodoxy. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-7\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-8\" class=\"footnote-item\"><p>Defined by Toby Ord in <a href=\"https://www.goodreads.com/book/show/50485582-the-precipice\">The Precipice</a> as a \u201cmechanism for destroying humanity or our potential\u201d, and including artificial intelligence, engineered pathogens, climate change, nuclear war, and so on. The closest term in Disaster Risk Studies would be \u201chazard\u201d, but the usage of the word in <em>The Precipice</em> and beyond also seems to cover clusters of hazards, threats, drivers of vulnerability, indirect causes of hazard occurrence (artificial intelligence can\u2019t kill you by itself, it needs \u201chands\u201d as well as a \u201cbrain\u201d), and several other concepts, few of which could be considered \u201cmechanisms\u201d in and of themselves. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-8\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-9\" class=\"footnote-item\"><p>We should remember that EA is sometimes worryingly close <a href=\"https://www.bylinesupplement.com/p/the-far-right-eugenics-mindset-behind\">to</a> <a href=\"https://ineffectivealtruismblog.com/2023/01/12/off-series-that-bostrom-email/\">racist</a>, <a href=\"https://slate.com/business/2018/04/economist-robin-hanson-might-be-americas-creepiest-professor.html\">misogynistic</a>, <a href=\"https://social-epistemology.com/2016/09/23/the-violence-of-pure-reason-neoreaction-a-basilisk-adam-riggio/\">and</a> <a href=\"https://www.youtube.com/watch?v=h8KkXcBwHec\">even</a> <a href=\"https://www.goodreads.com/book/show/41198053-neoreaction-a-basilisk\">fascist</a> ideas. For instance, <a href=\"https://www.eruditorumpress.com/blog/the-beigeness-or-how-to-kill-people-with-bad-writing-the-scott-alexander-method\">Scott Alexander</a>, a <a href=\"https://idontspeakgerman.libsyn.com/82-scott-alexander-slate-star-codex-with-david-gerard-and-elizabeth-sandifer\">blogger</a> that is very popular within EA, and <a href=\"https://news.yahoo.com/tumblr-blog-linked-ex-alameda-234427813.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAJ2FnqIHxR3yq-1SCk4CKSLdxPZolinF7LeMPnk-LRoYmHhjYbBnUdO_eqvugOkE-lnDLqCONLT-MvZkuZy1_nFgDz6WyhR6-3ajaR6btI95PR2UFMJPj9QbO0aoOyQWs0SoFUOTgzQocsrxs9Rpbv8ReoL4MolJeE8H0RaV9ZBX\">Caroline Ellison</a>, a close associate of Sam Bankman-Fried, speak favourably about \u201chuman biodiversity\u201d, which is the latest euphemism for \u201cscientific\u201d racism. [Editor\u2019s note: we left this in a footnote out of fear that a full section would cause enough uproar to distract from all our other points/suggestions. A full-length post exploring EA\u2019s historical links to reactionary thought will be published soon]. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-9\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-10\" class=\"footnote-item\"><p>Several of the authors of this post fit this description eerily well. \u201cDan\u201d, \u201cTom\u201d, and \u201cChris\u201d were other close contenders. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-10\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-11\" class=\"footnote-item\"><p>Many of the authors have backgrounds in the humanities and social sciences, and we see it as no coincidence that the issues we identify were noticed by people trained in modelling socio-cultural systems, critiquing arbitrary categorisations, and analysing structures of power. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-11\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-12\" class=\"footnote-item\"><p>It has been suggested that success in work or life may depend far more on <a href=\"https://www.sciencedirect.com/science/article/pii/S1877042812021477\">emotional intelligence</a> than \u201cintellect\u201d. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-12\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-13\" class=\"footnote-item\"><p>From Zoe Cremer\u2019s <a href=\"https://forum.effectivealtruism.org/posts/DxfpGi9hwvwLCf5iQ/objections-to-value-alignment-between-effective-altruists\">Objections to Value-Alignment Among Effective Altruists</a>: \u201cIntellectual homogeneity is efficient in the short-term, but counter-productive in the long-run. Value-alignment allows for short-term <em>efficiency</em>, but the true goal of EA \u2013 to be <em>effective</em> in producing value in the long term \u2013 might not be met.\u201c <a href=\"#fnref-pMAQ8odjQviWs2qYJ-13\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-14\" class=\"footnote-item\"><p>With the exception of (orthodox) economics and analytic philosophy. Note also that certain STEM areas have historically been neglected, even including (hardware) engineering until very recently. The \u201ccore\u201d EA subjects are at once highly formal (i.e. mathematical/pure-logical), relatively un-empirical, and (typically) reductionist. There do not, for instance, seem to be very many EA anthropologists, historians, or social theorists, especially within the leadership. Perhaps if we had a few then the issues we describe would have been raised a long time ago. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-14\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-15\" class=\"footnote-item\"><p>Furthermore, diversity has a limited impact on decision-making if it is not combined with <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#Deciding_together_better\">democracy</a>; if EA was diverse but the leadership remained homogenous, there would still be problematic dynamics. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-15\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-16\" class=\"footnote-item\"><p>There is no one \u201cEA response to critique\u201d as each person is different, and nor is there one perfect classification scheme. This is simply a useful tool for thinking with. Alternatives welcome. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-16\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-17\" class=\"footnote-item\"><p>\u201cIt is difficult to get a man to understand something when his salary depends on his not understanding it.\u201d - Upton Sinclair <a href=\"#fnref-pMAQ8odjQviWs2qYJ-17\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-18\" class=\"footnote-item\"><p>In the broad political sense, rather than the American sense of \u201cleft of conservative\u201d. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-18\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-19\" class=\"footnote-item\"><p>If you are new to the community or are reading this in the future: they were <a href=\"https://forum.effectivealtruism.org/topics/ftx-collapse\">right</a>. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-19\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-20\" class=\"footnote-item\"><p>To, and including, us. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-20\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-21\" class=\"footnote-item\"><p>Beyond simple downvoting, EA has developed its own rhetoric for subtly brushing off criticism: deep critiques are \u201cpoorly argued\u201d or \u201clikely net-negative\u201d proposals made by people with \u201cbad epistemics\u201d. These and similar utterances, often simply asserted without any supporting argumentation, make dismissals seem intelligent and even-handed, even in cases where they are used as little more than EA code for \u201cI disagree with this argument and I don\u2019t like the author very much.\u201d Elsewhere, critiques from outgroup writers are \u201cbad optics\u201d; PR problems to be solved rather than arguments to be engaged with. None of this is to say that the phrases are bad in themselves or that they are always used inappropriately, just that they should be used within logical arguments rather than as substitutes for them. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-21\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-22\" class=\"footnote-item\"><p>We have no idea how much of an impact this might have had on Sven Rone (the pseudonymous author of \u201c<a href=\"https://medium.com/@sven_rone/the-effective-altruism-movement-is-not-above-conflicts-of-interest-25f7125220a5\">The Effective Altruism movement is not above conflicts of interest</a>\u201d): theirs is an illustrative example, not a pillar of our argument. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-22\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-23\" class=\"footnote-item\"><p>The paper went through 27 revisions and almost as many reviewers to make sure it was written in a sufficiently conciliatory fashion to be taken seriously by EAs, but the authors faced accusations of being too \u201ccombative\u201d, \u201cuncharitable\u201d, and \u201charsh\u201d regardless, and were accused by some of \u201ccourage-signalling\u201d or otherwise acting in bad faith. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-23\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-24\" class=\"footnote-item\"><p>Old Boy\u2019s Network (British): \u201cAn exclusive informal network linking alumni of a particular (generally elite) school, or members of a social class or profession or organisation, in order to provide connections, information, and favours.\u201d <a href=\"#fnref-pMAQ8odjQviWs2qYJ-24\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-25\" class=\"footnote-item\"><p>Potential but speculative feedback loop: the longer you are in EA,the higher you are likely to climb, and the higher the risk of questioning orthodoxy, thus the more you tend to the EA mean, thus the more narrow and orthodox EA becomes. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-25\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-26\" class=\"footnote-item\"><p>The expected impact of deep critiques is further reduced by the fact that the leadership seems to rarely engage with them. In the case of <a href=\"https://forum.effectivealtruism.org/posts/gx7BEkoRbctjkyTme/democratising-risk-or-how-ea-deals-with-critics\">Democratising Risk</a>, leaders made a point of publicly stating that such critical work <a href=\"https://forum.effectivealtruism.org/posts/gx7BEkoRbctjkyTme/democratising-risk-or-how-ea-deals-with-critics-1?commentId=Y6ap4CjEtRsdmYQQk\">was</a> <a href=\"https://forum.effectivealtruism.org/posts/gx7BEkoRbctjkyTme/democratising-risk-or-how-ea-deals-with-critics-1?commentId=AQk29QGbvCLGFJmxr\">valuable</a>, but since then have not appeared to consider or discuss the content of the paper in any detail. Criticism can be de facto neutralised if those with power simply ignore it. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-26\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-27\" class=\"footnote-item\"><p>The vast majority of researchers, professionals, etc. do not try to quantitatively reason from first principles in this way. There seems relatively little consideration within EA of <a href=\"https://forum.effectivealtruism.org/posts/WKPd79PESRGZHQ5GY/in-defence-of-epistemic-modesty\">why this might be</a>. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-27\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-28\" class=\"footnote-item\"><p>This is known in the philosophy of probability as the <a href=\"https://en.wikipedia.org/wiki/Bayesian_epistemology#Problem_of_priors\">Problem of Priors</a>. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-28\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-29\" class=\"footnote-item\"><p>That is, into the probability of making a given observation assuming that the hypothesis in question is true: P(E|H). <a href=\"#fnref-pMAQ8odjQviWs2qYJ-29\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-30\" class=\"footnote-item\"><p>\u201cThere is no evidence that geopolitical or economic forecasters can predict anything ten years out.\" \u2013 Phillip Tetlock <a href=\"#fnref-pMAQ8odjQviWs2qYJ-30\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-31\" class=\"footnote-item\"><p>The conclusions of what is by far the most comprehensive and rigorous study of quantification in existential risk (Beard, Rowe, and Fox, <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0016328719303313\">2020</a>) is that all the methods we have at the moment are rather limited or flawed in one way or another, that the most popular methods are also the least rigorous, and that the best route forward is to learn from other fields by transparently laying out our reasoning processes for others to evaluate. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-31\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-32\" class=\"footnote-item\"><p>There\u2019s probably a link to the Rationalist community\u2019s emphasis on IQ here. [Editor\u2019s note: see <a href=\"https://twitter.com/anderssandberg/status/1613259477263720449?cxt=HHwWgoCxjc-8ueMsAAAA\">Bostrom</a>]. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-32\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-33\" class=\"footnote-item\"><p>As Noah Scale <a href=\"https://forum.effectivealtruism.org/posts/RPTPo8eHTnruoFyRH/some-important-questions-for-the-ea-leadership?commentId=Z3m6B7AhP3WSFgHge\">puts</a> it, \u201cEAs [can] defer when they claim to argue.\u201d <a href=\"#fnref-pMAQ8odjQviWs2qYJ-33\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-34\" class=\"footnote-item\"><p>To clarify, we\u2019re not saying that this sort of hierarchical sensibility is purely due to number-centric thinking: other cultural and especially class-political factors are likely to play a very significant role. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-34\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-35\" class=\"footnote-item\"><p>Informal observation also strongly suggests selective employment of civility norms: it seems you can get away with much more if you are well-known and your arguments conform to EA orthodoxy. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-35\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-36\" class=\"footnote-item\"><p>Sometimes performed in 30 hours of research or less. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-36\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-37\" class=\"footnote-item\"><p>This seems to fit a wider <a href=\"https://www.economist.com/1843/2022/11/15/the-good-delusion-has-effective-altruism-broken-bad\">theme</a> of many non-leadership EAs being more orthodox about EA ideas than the originators of those ideas themselves. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-37\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-38\" class=\"footnote-item\"><p>This is not to say that we are wholly opposed to these ideas, just that there is surprisingly little academic scruitny and discussion of these ideas given their importance to our movement. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-38\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-39\" class=\"footnote-item\"><p>EA also seems to have a general hostility toward the Planetary Boundaries framework that is rarely explained or justified, and climate risk claims in general are subjected to a far higher burden of proof than claims about e.g. AI risk. We do not all agree with Lenton or Rockstr\u00f6m, but are rather highlighting inconsistencies. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-39\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-40\" class=\"footnote-item\"><p>Many members of Existential Risk Studies are not EAs, or are somewhat heterodox/\u201dheretical\u201d EAs. Given our occasional tendency to conflate the value-alignment of an author with the value of their work, it is unfortunately not surprising that the outputs of less ideologically selective institutes like the Centre for the Study of Existential Risk or the Global Catastrophic Risk Institute (never mind those of authors not working at EA-linked bodies at all) can be ignored or dismissed at times. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-40\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-41\" class=\"footnote-item\"><p>See footnote [8]. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-41\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-42\" class=\"footnote-item\"><p>We are aware of one young EA with a background in Disaster Risk Reduction who, after expressing an interest in existential risk, was repeatedly told by EAs to leave DRR and go into AI alignment. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-42\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-43\" class=\"footnote-item\"><p>An \u201c...unpredictable event that is beyond what is normally expected of a situation and has potentially severe consequences.\u201d \u2013 <a href=\"https://www.investopedia.com/terms/b/blackswan.asp\">Investopedia</a> <a href=\"#fnref-pMAQ8odjQviWs2qYJ-43\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-44\" class=\"footnote-item\"><p>Defined by the <a href=\"https://www.deepuncertainty.org/about-us/\">DMDU Society</a> as when \u201cparties to a decision do not know, or cannot agree on, the system model that relates action[s] to consequences, the probability distributions to place over the inputs to these models, which consequences to consider[,] and their relative importances.\u201d EA has developed a remarkably similar concept called \u201c<a href=\"https://forum.effectivealtruism.org/topics/cluelessness\">cluelessness</a>\u201d. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-44\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-45\" class=\"footnote-item\"><p>This is particularly problematic given that current discussions of the importance of future generations are due in <a href=\"https://www.goodreads.com/book/show/51107158-the-good-ancestor\">significant part</a> to the tireless work of indigenous communities and climate justice activists \u2013 i.e. groups almost entirely excluded and/or devalued by EA. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-45\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-46\" class=\"footnote-item\"><p>Another: technological determinism, implicit in most longtermist work, is largely dismissed and derided within Science &amp; Technology Studies. It\u2019s not completely fringe, but it\u2019s definitely a minority position, yet EA seems to have never explored any alternatives, e.g. constructivist or co-productionist approaches to technology. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-46\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-47\" class=\"footnote-item\"><p>Note the probable link to Silicon Valley \u201cdisruptor\u201d culture. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-47\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-48\" class=\"footnote-item\"><p>This also counteracts the flaws found in \u201cHigh Modernist\u201d approaches popular in EA, also known as \u201c<a href=\"https://slatestarcodex.com/2017/03/16/book-review-seeing-like-a-state/\">seeing like a state</a>\u201d. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-48\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-49\" class=\"footnote-item\"><p>For example, the <a href=\"https://www.ipcc.ch/sr15/chapter/chapter-4/\">IPCC has argued</a> that \u201c[limiting warming below 1.5\u00b0C] would require \u2026 building the capability to utilise indigenous and local knowledge.\u201d <a href=\"#fnref-pMAQ8odjQviWs2qYJ-49\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-50\" class=\"footnote-item\"><p>AI, engineered biology, climate change, and nuclear war, in steeply descending order of (perceived) importance. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-50\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-51\" class=\"footnote-item\"><p>Though still far outnumbered by TUA-aligned works due to the hegemonic power of EA orthodoxy and funding. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-51\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-52\" class=\"footnote-item\"><p>Given that the ecological crisis is a <a href=\"https://en.wikipedia.org/wiki/Wicked_problem\">wicked problem</a> requiring complex systems analysis it is unsurprising that the <a href=\"https://www.ipcc.ch/sr15/chapter/chapter-4/\">IPCC has outlined</a> the necessity for systemic changes across a huge variety of domains (or what we might call \u201ccause areas\u201d), from land use, food production, and energy, to carbon capture technologies and institutional adaptations. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-52\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-53\" class=\"footnote-item\"><p>We do not know of any EA that put a probability on an FTX collapse, never mind one with anything like the consequences EA faced as a result of the one we witnessed. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-53\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-54\" class=\"footnote-item\"><p>An interesting exercise: consider an extinction scenario, work back through the chain of causation in as detailed a fashion as you can, consider all the factors at play and their interrelation, and ask yourself how productive it is to label the initial scenario as e.g. \u201cextinction from synthetic biology\u201d. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-54\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-55\" class=\"footnote-item\"><p>Though it is potentially problematic that OpenPhil\u2019s <a href=\"https://www.openphilanthropy.org/focus/\">list of focus areas</a> is fairly constrained compared to e.g. that of the FTX Fund. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-55\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-56\" class=\"footnote-item\"><p>As well as growing the EA community itself. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-56\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-57\" class=\"footnote-item\"><p>cf. EA\u2019s implicit commitment to liberal technocracy. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-57\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-58\" class=\"footnote-item\"><p>Only a narrow possibility space can be explored if one needs to roughly align with preferences of funders, e.g. for particular methods. In general, monist, hegemonic funding structures promote scientific conservativism. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-58\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-59\" class=\"footnote-item\"><p>Note that this is despite the fact that the field of global catastrophic risk is rather small and homogenous in itself, though by definition less homogenous than global catastrophic risk <em>within EA</em>. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-59\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-60\" class=\"footnote-item\"><p>\u201cPredict and Act\u201d vs \u201cExplore and Adapt\u201d again. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-60\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-61\" class=\"footnote-item\"><p>Inclusive \u201cwe\u201d: the authors of this text are also not immune to this either. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-61\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-62\" class=\"footnote-item\"><p>There\u2019s an antisemitic element to this as well: crypto\u2019s history is intimately bound up in <a href=\"https://www.goodreads.com/book/show/31944475-the-politics-of-bitcoin\">far-right</a> desires to create digital \u201csound money\u201d to undermine the power of central banks, because in their eyes, (central) banks = Jews. <a href=\"https://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai\">Peter</a> <a href=\"https://www.youtube.com/watch?v=h8KkXcBwHec\">Thiel</a> is also <a href=\"https://youtu.be/u-sNSjS8cq0?t=4706\">in the mix</a>, <a href=\"https://bylinetimes.com/2021/12/10/peter-thiels-free-speech-for-race-science-crusade-at-cambridge-university-revealed/\">as</a> <a href=\"https://johnganz.substack.com/p/the-enigma-of-peter-thiel\">always</a>. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-62\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-63\" class=\"footnote-item\"><p>The reticence of EAs to consider (political!) actions that might slow down AI progress is <a href=\"https://forum.effectivealtruism.org/posts/vwK3v3Mekf6Jjpeep/let-s-think-about-slowing-down-ai-1?utm_source=substack&amp;utm_medium=email\">well-known</a>, though this has begun to change recently. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-63\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-64\" class=\"footnote-item\"><p>This doesn\u2019t discredit longtermism, and many of the authors are sympathetic to longtermism. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-64\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-65\" class=\"footnote-item\"><p>Like most EAs- including us! <a href=\"#fnref-pMAQ8odjQviWs2qYJ-65\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-66\" class=\"footnote-item\"><p>At a minimum, poverty reduction has been dismissed as being \u201cnear-termist\u201d, despite the descendents of people currently in poverty being far more likely to live in poverty themselves, and the fact that there is no guarantee that AI or other future technologies will actually reduce poverty (particularly as existing AI typically <a href=\"https://www.goodreads.com/book/show/28186015-weapons-of-math-destruction\">perpetuates</a> or <a href=\"https://www.goodreads.com/book/show/50131136-atlas-of-ai\">increases</a> inequality). Several of us also wonder what evaluations of global poverty work would look like if they considered interventions that targeted the underlying causes of poverty rather than treating the symptoms. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-66\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-67\" class=\"footnote-item\"><p>We\u2019re not trying to dismiss AI risk here \u2013 several of us work on AI risk \u2013 we just question why it is given such a huge emphasis. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-67\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-68\" class=\"footnote-item\"><p>Since this was initially written, there has been a lot of discussion about Wytham Abbey on the EA Forum. The purchase <a href=\"https://forum.effectivealtruism.org/posts/xof7iFB3uh8Kc53bG/why-did-cea-buy-wytham-abbey?commentId=u3yJfbm2pes8TFpYX\">has been justified</a> by the project leader, Owen Cotton-Barratt, who says that calculations were made which, depending on the numbers and analysis, may mean that this was a wise investment, as external conferences are expensive, and Effective Ventures could sell the abbey further down the line and potentially recoup a significant portion of the initial investment. However, we just don\u2019t know: we have not seen the original numbers or a cost effectiveness analysis. Given the response, it is clear that many people believe Wytham Abbey to be a frivolous purchase, which is not unsurprising. There should have been a more transparent and proactive justification of the benefits of the purchase and why those benefits justified the high cost. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-68\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-69\" class=\"footnote-item\"><p>MacAskill has expressed concern about hero-worship within EA, but we have not been able to find any instances where he has made a concerted effort to reduce it. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-69\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-70\" class=\"footnote-item\"><p>There seems to be <a href=\"https://forum.effectivealtruism.org/posts/Yosqvz6w9fuc3zjBS/long-term-future-fund-november-2020-grant-recommendations\">one exception</a> to this, explained by the anonymity of the grantee. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-70\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-71\" class=\"footnote-item\"><p>We have lost track of how often we have made or been asked to make significant purchases using our personal accounts on the verbal assurance that we will be reimbursed at some point in the future. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-71\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-72\" class=\"footnote-item\"><p>Which clearly have <a href=\"https://www.youtube.com/watch?v=v1eWIshUzr8\">extensive</a> <a href=\"https://www.goodreads.com/book/show/22245334-the-utopia-of-rules\">problems</a> of their own. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-72\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-73\" class=\"footnote-item\"><p>See also the \u201cIron Law of Institutions\u201d, <a href=\"http://www.tinyrevolution.com/mt/archives/001705.html\">where</a> \u201cpeople who control institutions care first and foremost about <em>their power within the institution</em> rather than the <em>power of the institution itself</em>.\u201d <a href=\"#fnref-pMAQ8odjQviWs2qYJ-73\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-74\" class=\"footnote-item\"><p>Or reverse it after it has already happened. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-74\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-75\" class=\"footnote-item\"><p>Our problem isn\u2019t with the leaders, but rather the structures that give them large amounts of unaccountable power. If we were in the same position, we are sure we would need just as much accountability and transparency to ensure we were doing a good job. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-75\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-76\" class=\"footnote-item\"><p>Postcolonial perspectives within the fields of Public Health and Development Studies will hold most of the answers to these questions as far as Global Health &amp; Wellbeing is concerned. Existential risk is another problem entirely, and figuring out how to make the task of existential risk reduction a democratic one sounds like a good project if anyone is looking for ideas. There\u2019s already been some work on \u201cparticipatory futures\u201d, for example the list at the bottom of <a href=\"https://www.fdsd.org/ideas/participatory-futures/\">this</a> page. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-76\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-77\" class=\"footnote-item\"><p>See our discussion of expert opinion aggregation tools at the end of <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1#We_should_probably_read_more_widely\">this section</a>. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-77\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-78\" class=\"footnote-item\"><p>At the very least, Annual Gathering Meetings that allow for broad community input would be a step in the right direction. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-78\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-79\" class=\"footnote-item\"><p>This in particular has been the experience of certain authors of this post. Being confidently dismissed by people you know to have negligible knowledge of your area of expertise gets tiring very quickly. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-79\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-80\" class=\"footnote-item\"><p>At least, as far as we know: few of us have much expertise in this domain <a href=\"#fnref-pMAQ8odjQviWs2qYJ-80\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pMAQ8odjQviWs2qYJ-81\" class=\"footnote-item\"><p>We may update the list of reforms in response to suggestions from others. <a href=\"#fnref-pMAQ8odjQviWs2qYJ-81\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "ConcernedEAs"}}, {"_id": "TdMSpzSqkQDccnSy2", "title": "Posts from 2022 you thought were valuable (or underrated)", "postedAt": "2023-01-17T16:42:05.287Z", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/gfj7FMKz9e8CKXqSQ/your-2022-ea-forum-wrapped\"><u>Forum Wrapped</u></a> showed you what you upvoted and strong-upvoted in 2022. We also gave you the chance to mark some posts as \u201cmost valuable.\u201d&nbsp;</p><p>I\u2019m sharing which posts were marked as \u201c<strong>most valuable</strong>\u201d by most people, and which posts were&nbsp;<strong>most underrated</strong> by their karma score (relative to the number of \u201cmost valuable\u201d votes).&nbsp;</p><p>Please note that this is <strong>a very rough list</strong>; relatively few people marked posts as \"most valuable,\" and I imagine that those who did, didn't do it very carefully or comprehensively. And there are various biases in the data (like the fact that we showed the list in order of karma).</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1673973343/mirroredImages/TdMSpzSqkQDccnSy2/rlwlkr4c51ipkrtmvnrn.png\"></p><h1>Before you continue, consider looking back (if you haven\u2019t done that yet).</h1><p>You can look at your&nbsp;<a href=\"https://forum.effectivealtruism.org/wrapped\"><u>Forum Wrapped</u></a>, consider scrolling through&nbsp;<a href=\"https://forum.effectivealtruism.org/allPosts?timeframe=yearly&amp;sortedBy=topAdjusted&amp;after=2022-01-01&amp;before=2023-01-01&amp;limit=100\"><u>high-rated (adjusted) posts from 2022</u></a>, or explore posts on your favorite&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/all\"><u>topics</u></a>.&nbsp;</p><p>(You can still mark posts as \u201cmost valuable\u201d \u2014 we might revisit these numbers later, and you might find it useful to have a record of this for yourself.)</p><hr><p>If&nbsp;you want to explore more content, you could look at <a href=\"https://forum.effectivealtruism.org/posts/SQ2ayhoYBJJCrFQjd/what-are-the-most-underrated-posts-and-comments-of-2022\"><u>What are the most underrated posts &amp; comments of 2022, according to you?</u></a> (thread),&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/gfj7FMKz9e8CKXqSQ/your-2022-ea-forum-wrapped?commentId=NaBaiNkj3QQdHNwXh\"><u>my comment</u></a> on the Forum Wrapped announcement,&nbsp;<a href=\"https://forum.effectivealtruism.org/recommendations\"><u>curated posts from 2022</u></a>, and older content like the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/FEFEvC6BzswR4oQqm/results-from-the-first-decade-review\"><u>Results from the First Decade Review</u></a> and the&nbsp;<a href=\"https://forum.effectivealtruism.org/sequences/dg852CXinRkieekxZ\"><u>Forum Digest Classics</u></a>.</p><h1>Which posts from 2022 did Forum users think were most valuable?</h1><p><i>(Note that we ordered posts in \"</i><a href=\"https://forum.effectivealtruism.org/wrapped\"><i>wrapped</i></a><i>\" by karma score, meaning higher-karma posts might be artificially over-rated.)</i></p><figure class=\"table\"><table><tbody><tr><td style=\"background-color:#a2c4c9;border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom;width:50px\">Votes</td><td style=\"background-color:#a2c4c9;border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom;width:120px\">Author(s)</td><td style=\"background-color:#a2c4c9;border:0.75pt solid #d0e0e3;padding:2pt 0pt;vertical-align:bottom\">Title</td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>18</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">ASB, eca</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/u5JesqQ3jdLENXBtB/concrete-biosecurity-projects-some-of-which-could-be-big-1\"><u>Concrete Biosecurity Projects (some of which could be big)</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>18</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Julia Wise</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/fovDwkBQgTqRMoHZM/power-dynamics-between-people-in-ea\"><u>Power dynamics between people in EA</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>16</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Theo Hawking</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/xomFCNXwNBeXtLq53/bad-omens-in-current-community-building\"><u>Bad Omens in Current Community Building</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>13</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Scott Alexander</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/KDjEogAqWNTdddF9g/long-termism-vs-existential-risk\"><u>\"Long-Termism\" vs. \"Existential Risk\"</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>12</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">George Rosenfeld</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/HWaH8tNdsgEwNZu8B/free-spending-ea-might-be-a-big-problem-for-optics-and\"><u>Free-spending EA might be a big problem for optics and epistemics</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>11</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Haydn Belfield</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/cXBznkfoPJAjacFoT/are-you-really-in-a-race-the-cautionary-tales-of-szilard-and\"><u>Are you really in a race? The Cautionary Tales of Szil\u00e1rd and Ellsberg</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>10</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Holden Karnofsky</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous\"><u>EA is about maximization, and maximization is perilous</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>9</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Simon M</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/ffmbLCzJctLac3rDu/strongminds-should-not-be-a-top-rated-charity-yet\"><u>StrongMinds should not be a top-rated charity (yet)</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>9</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Fods12</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/sEpWkCvvJfoEbhnsd/the-ftx-crisis-highlights-a-deeper-cultural-problem-within\"><u>The FTX crisis highlights a deeper cultural problem within EA - we don't sufficiently value good governance</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>9</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">MathiasKB</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/WyqGircJgCBG9ivNL/snakebites-kill-100-000-people-every-year-here-s-what-you\"><u>Snakebites kill 100,000 people every year, here's what you should know</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>8</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">William MacAskill</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/cfdnJ3sDbCSkShiSZ/ea-and-the-current-funding-situation\"><u>EA and the current funding situation</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>8</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Linch</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/vPMo5dRrgubTQGj9g/some-unfun-lessons-i-learned-as-a-junior-grantmaker\"><u>Some unfun lessons I learned as a junior grantmaker</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>8</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Will Bradshaw, Mike McLaren, Anjali Gopal</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/gLPEAFicFBW8BKCnr/announcing-the-nucleic-acid-observatory-project-for-early\"><u>Announcing the Nucleic Acid Observatory project for early detection of catastrophic biothreats</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>8</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Thomas Kwa</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/GzmJ2uiTx4gYhpcQK/effectiveness-is-a-conjunction-of-multipliers\"><u>Effectiveness is a Conjunction of Multipliers</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>8</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Karolina Sarek, Joey</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/isggu3woGwkpYzqwW/presenting-2022-incubated-charities-charity-entrepreneurship\"><u>Presenting: 2022 Incubated Charities (Charity Entrepreneurship)</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>8</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Nu\u00f1o Sempere</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/h2N9qEbvQ6RHABcae/a-critical-review-of-open-philanthropy-s-bet-on-criminal\"><u>A Critical Review of Open Philanthropy\u2019s Bet On Criminal Justice Reform</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>8</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Holden Karnofsky</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/qFEwQbetaaSpvHm9e/my-takes-on-the-ftx-situation-will-mostly-be-cold-not-hot\"><u>My takes on the FTX situation will (mostly) be cold, not hot</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>8</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Peter McLaughlin</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/8wWYmHsnqPvQEnapu/getting-on-a-different-train-can-effective-altruism-avoid\"><u>Getting on a different train: can Effective Altruism avoid collapsing into absurdity?</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>7</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Katja Grace</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/vwK3v3Mekf6Jjpeep/let-s-think-about-slowing-down-ai-1\"><u>Let\u2019s think about slowing down AI</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>7</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Holden Karnofsky</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/zGiD94SHwQ9MwPyfW/important-actionable-research-questions-for-the-most\"><u>Important, actionable research questions for the most important century</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>7</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Holden Karnofsky</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/mCCutDxCavtnhxhBR/some-comments-on-recent-ftx-related-events\"><u>Some comments on recent FTX-related events</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>7</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Maya D</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/t5vFLabB2mQz2tgDr/i-m-a-22-year-old-woman-involved-in-effective-altruism-i-m\"><u>I\u2019m a 22-year-old woman involved in Effective Altruism. I\u2019m sad, disappointed, and scared.</u></a></td></tr><tr><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><p>7</p></td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\">Ajeya</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/Y3sWcbcF7np35nzgu/without-specific-countermeasures-the-easiest-path-to-1\"><u>Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover</u></a></td></tr></tbody></table></figure><h1>Which were most underrated by Forum karma?</h1><p><i>A quick note on the methodology is in the footnote.</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdq9c6zj5xnr\"><sup><a href=\"#fndq9c6zj5xnr\">[1]</a></sup></span></p><figure class=\"table\"><table><tbody><tr><td style=\"background-color:#a2c4c9;border-style:solid;padding:2pt;vertical-align:bottom;width:50px\">Votes</td><td style=\"background-color:#a2c4c9;border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom;width:120px\">Author</td><td style=\"background-color:#a2c4c9;border:0.75pt solid #d0e0e3;padding:2pt 0pt;vertical-align:bottom\">Title</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>3</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Corentin Biteau</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/wXzc75txE5hbHqYug/the-great-energy-descent-short-version-an-important-thing-ea\"><u>The great energy descent (short version) - An important thing EA might have missed</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>4</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">James Ozden</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/CycFBmdwgCjCsZxQh/what-you-prioritise-is-mostly-moral-intuition\"><u>What you prioritise is mostly moral intuition</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>2</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Fabio Kuhn, Giving What We Can, Sarah Baka</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/GnJQaSaXRebZgrmg3/the-2022-giving-what-we-can-donor-lottery-is-now-open\"><u>The 2022 Giving What We Can Donor Lottery is now open</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>2</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Jamie Harris</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/CsjKJkAyxXxmfz8su/the-history-of-ai-rights-research\"><u>The History of AI Rights Research</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>3</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Max Dalton, Angelina Li, Oscar Howie</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/DajpFcaMrHv4fPLTy/cea-s-work-in-2022\"><u>CEA's work in 2022</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>2</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Elizabeth</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/oRx3LeqFdxN2JTANJ/epistemic-legibility\"><u>Epistemic Legibility</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>3</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Gavin</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/AqpFkoq3oSEvsqker/milan-griffes-on-ea-blindspots\"><u>Milan Griffes on EA blindspots</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>4</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Holden Karnofsky</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/hxTFAetiiSL7dZmyb/ideal-governance-for-companies-countries-and-more\"><u>Ideal governance (for companies, countries and more)</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>4</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Holden Karnofsky</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/NbiHKTN5QhFFfjjm5/ai-safety-seems-hard-to-measure\"><u>AI Safety Seems Hard to Measure</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>3</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Lynette Bye</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/tdgvad64bmMxeXKws/have-you-ever-doubted-whether-you-re-good-enough-to-pursue\"><u>Have You Ever Doubted Whether You're Good Enough To Pursue Your Career?</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>2</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Pablo, matthew.vandermerwe</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/tGpwWsP5iBfZFigeZ/future-matters-6-ftx-collapse-value-lock-in-and\"><u>Future Matters #6: FTX collapse, value lock-in, and counterarguments to AI x-risk</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>2</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Holden Karnofsky</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/ktEzS3pkfeqPNh6r5/ai-strategy-nearcasting\"><u>AI strategy nearcasting</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>2</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Elizabeth</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/gtivkdZKjFRcvfLQC/butterfly-ideas\"><u>Butterfly Ideas</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>2</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Hamish Hobbs, jbs, Allan Dafoe</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/J6QCmkQmuRaP7skje/differential-technology-development-preprint-on-the-concept\"><u>Differential technology development: preprint on the concept</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>2</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Derek Shiller</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/yxpFFChqK5ErKYHxv/the-importance-of-getting-digital-consciousness-right\"><u>The importance of getting digital consciousness right</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>4</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Holden Karnofsky</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/c3y6khh7mxiWrDyeb/nonprofit-boards-are-weird\"><u>Nonprofit Boards are Weird</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>3</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Tom Barnes</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/Td4nkg4v2spDgyyKL/air-pollution-founders-pledge-cause-report\"><u>Air Pollution: Founders Pledge Cause Report</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>2</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Vaidehi Agarwalla, Alexandra Malikova, Elika</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/GoaKjhQGqAgk6sKKT/doing-ops-in-ea-faq-before-you-join-2022\"><u>Doing Ops in EA FAQ: before you join (2022)</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>2</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Holden Karnofsky</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/XRphCh6NbfQiDF3Nt/racing-through-a-minefield-the-ai-deployment-problem\"><u>Racing through a minefield: the AI deployment problem</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>4</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">ColdButtonIssues</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/myympkZ6SuT59vuEQ/go-republican-young-ea\"><u>Go Republican, Young EA!</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>4</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">levin</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/LPDyAvwYyp4tzPmED/common-sense-cases-where-hypothetical-future-people-matter\"><u>Common-sense cases where \"hypothetical future people\" matter</u></a></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>2</p></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\">Janne M. Korhonen</td><td style=\"border:0.75pt solid #d0e0e3;padding:2pt;vertical-align:bottom\"><a href=\"https://forum.effectivealtruism.org/posts/DArtSnDxRH5AsE5RF/sheltering-humanity-against-x-risk-report-from-the-shelter\"><u>Sheltering humanity against x-risk: report from the SHELTER weekend</u></a></td></tr></tbody></table></figure><h1>Thanks so much...</h1><p>...for posting, commenting, upvoting, marking as \u201cmost valuable,\u201d giving us feedback, and more! I really appreciate it.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndq9c6zj5xnr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdq9c6zj5xnr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I looked at the number of people who had marked something as \u201cmost valuable,\u201d and then divided by [karma score]^1.5. All of these were marked as most valuable by at least two people.&nbsp;</p><p>Just dividing by karma didn\u2019t change the list much, and dividing by karma^2 penalized karma too much. I played with a few other ways of modifying the \u201cunderrated-ness\u201d metric, but they didn\u2019t seem better.</p></div></li></ol>", "user": {"username": "Lizka"}}, {"_id": "XeXKg2Cc25krYMyvJ", "title": "Responses to pademic pathogens with differing behaviours...and are we in the clear of covid-19 yet?", "postedAt": "2023-01-17T15:12:23.170Z", "htmlBody": "<p>I was reading this talk by Amesh Adalja back in 2018. Amesh was certainly correct that the most likely candidate of a GCBR &nbsp;pathogen would be a airbourne RNA-virus. But I saw this question and now I'm wondering if there are elaborate answers for this in the wider community.</p><blockquote><p>\"<strong>Question:</strong> How about things that may be dormant for a long time or asymptomatic for a long time, how does that kind of change the analysis of this&nbsp;stuff?\"</p></blockquote><p>&nbsp;It seems to me that Amesh recognised that a patheogen like HIV would be able to spread through the population and create a very different problem from the typical GCBR scenario, but he didn't get to elaborate on the possible responses to this (and I'm not quite sure why it matters that this doesn't fall into the GCBR definition nicely).&nbsp;</p><p>I wonder if there are other folks working on biosecurity and pandemics who have talked about this particular scenario, how it would likely work out, the relative risk, and what should be done to detect/control such an event, since it's definitely within the realm of possibility (HIV is one that has already given us plenty of trouble, and Amesh mentioned HLTV-1, which seems to be a possible candidate too).</p><p>&nbsp;</p><p>This also brings me another question: are we in the clear about covid-19 yet?&nbsp;</p><p>There is no question that, with vaccination programs and previous infections, the majority of people now experience covid-19 as a relatively mild illness and make a full recovery with no noticeable issues. And if this is the whole picture, we should be able to handle covid-19 from now on like \"just another illness\" that doesn't threaten us with societal collapse anymore.&nbsp;</p><p>However, some people do <a href=\"https://forum.effectivealtruism.org/posts/njgRDx5cKtSM8JubL/long-covid-mass-disability-and-broad-societal-consequences\">develop serious health issues</a> after the acute illness, and seemingly the virus is capable of <a href=\"https://link.springer.com/article/10.1007/s11695-022-06338-9\">staying in our body</a> after recovery. So, what if covid-19 is capable of causing a more serious illness, months or years after a person recovered from the acute infection?&nbsp;</p><p>It is definitely not unheard of: HIV is a classical example, and even more \"mundane\" pathogens like <a href=\"https://medlineplus.gov/ency/article/001419.htm\">measles</a>, <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3548372/\">EBV</a>, or <a href=\"https://www.cell.com/trends/microbiology/fulltext/S0966-842X(20)30074-3\">herpes </a>are fully capable of creating serious or even fatal illnesses after the acute infection in a (small) fraction of patients who made full recoveries from the acute illness.</p><p>If covid-19 is somehow capable of causing serious illness in a larger fraction of the population in the future, we could indeed face a challenge that could cause societal collapse (and mass casualties).&nbsp;</p><p>Do we currently know enough about covid-19 to rule out this possibility, or assign it a low enough \"likelihood\", that it would be more effective to put biosecurity resources on newer, emerging diseases, than to continue investing in covid-19 response? My main concern is that, we don't have any data about \"how are people doing, 10 years after catching covid-19\". And I'm not sure if there are any good \"substitute\" methodologies to gain information about this.</p><p>And maybe, this is something that should be watched out for whenever we encounter a novel pathogen?&nbsp;</p>", "user": {"username": "Kinoshita Yoshikazu (pseudonym)"}}, {"_id": "jc8DMpbcZuEaAigkW", "title": "Some reasons to uphold norms that are common in other communities", "postedAt": "2023-01-17T15:29:31.314Z", "htmlBody": "<p>This post offers some reasons why, on&nbsp;<a href=\"https://plato.stanford.edu/entries/consequentialism/\"><u>consequentialist</u></a> grounds, a truth-seeking person with a primary goal of having a positive impact on the world might prioritize upholding certain norms that are widely held in their broader social and professional communities, even if the instrumental value of those norms isn\u2019t obvious in the context in which they are applied.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1evosoa3bbf\"><sup><a href=\"#fn1evosoa3bbf\">[1]</a></sup></span></p><p>One of my goals in writing this post is to help explain a subset of the reasons why I think some people in the EA community believe Nick Bostrom\u2019s&nbsp;<a href=\"https://nickbostrom.com/oldemail.pdf\"><u>email and apology</u></a> and some statements in the subsequent conversation have been inadequate or harmful: The original email, recent apology, and some subsequent comments have violated norms that are widely followed by people who aim to have a positive impact on the world, including a norm against asserting that there are meaningful genetic differences between groups of people in traits like intelligence.&nbsp; My impression is that some people engaging here may not be aware of the reasons why this norm exists or the potential consequences of violating it, while other people may assume that others know and take these reasons and consequences for granted.&nbsp; While this post does not address the object-level reasons why that norm exists,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefztof1mh58r\"><sup><a href=\"#fnztof1mh58r\">[2]</a></sup></span>&nbsp;I hope that it helps explain some of the potential consequences of violating that norm and others like it.</p><p>I acknowledge that some people in the EA community prioritize a different norm\u2014specifically, a norm in favor of communicating one\u2019s own beliefs as accurately and directly as possible, sometimes called \u201cepistemic integrity.\u201d &nbsp;I know many people sincerely believe that this norm is exceptionally important to the project of achieving a positive impact on the world, and I have tried to live up to it in writing this post.&nbsp; I hope this post helps explain why others might believe that achieving a positive impact on the world (and even accurately communicating one\u2019s own beliefs) sometimes requires giving priority to other norms.</p><h2>Norms play an important role in coordination</h2><p>Many ways of making a positive impact on the world depend on&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/altruistic-coordination\"><u>coordinating</u></a> with other people.&nbsp; One thing that is often important but hard to do when coordinating with other people is communicating accurate information about each other\u2019s goals and beliefs.&nbsp;&nbsp;</p><p>One way of communicating your goals and beliefs is to state them directly.&nbsp; But norms also play an important role in communicating goals and beliefs.&nbsp; When you follow a norm, you are providing evidence that your goals and beliefs are&nbsp;<i>like&nbsp;</i>those of other people who follow that norm.&nbsp; Conversely, when you violate a norm, people might infer that your goals and beliefs are&nbsp;<i>not like</i> those of people who do follow the norm.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzj6xm9ulvxj\"><sup><a href=\"#fnzj6xm9ulvxj\">[3]</a></sup></span></p><p>People who aim to have a positive impact on the world might uphold certain norms as one way of communicating about their goals and beliefs.&nbsp; They might also draw negative inferences from norm-breaking behavior by others.</p><h3>Examples of inferences others might draw from norm-breaking behavior</h3><p>If a person violates a norm, sincere and truth-seeking people might conclude that they&nbsp;<strong>don\u2019t know about the norm itself</strong>, from which they might infer:</p><ul><li><strong>The person isn\u2019t aware of important social or historical information underlying the norm</strong>, which might affect the person's ability to have a positive impact on the world.</li><li><strong>The person doesn't interact very much with people who could have taught them about the norm</strong>, which might signal that they lack social ties with (or perhaps even harbor prejudices against) a group of people who might be able to help them have a positive impact on the world.</li></ul><p>Other sincere and truth-seeking people might conclude that a person who violates a norm <i>does</i> know about the norm, but&nbsp;<strong>has chosen not to follow it</strong>, from which they might infer:</p><ul><li><strong>The person might not be trustworthy&nbsp;</strong>because they have shown that they are willing to violate an important norm; they might therefore be willing to violate other important norms like telling the truth and keeping promises.</li><li><strong>The person might be unpredictable and difficult to coordinate with</strong>&nbsp;because they have shown they are willing to make a \u201cone-off,\u201d context-specific decision that deviates from a general rule of behavior that many other people follow.</li><li><strong>The person might have other dangerous, difficult-to-observe traits</strong> that are more common among people who violate the norm than they are among people who consistently follow it. &nbsp;(For example, the person might harbor racial animus or misogynistic beliefs.)</li><li><strong>The person\u2019s value system might be different in important ways&nbsp;</strong>because they seem to place higher priority on the first-order consequences of the action they are taking than on the higher-order consequences that an important norm is intended to prevent.</li></ul><h2>Norms can also be valuable when following \u201csingle-player\u201d strategies</h2><p>Some norms function like&nbsp;<a href=\"https://www.lesswrong.com/s/AmFb5xWbPWWQyQ244\"><u>ethical injunctions</u></a> and serve to protect people from predictable mistakes that people tend to make when reasoning from first principles.&nbsp; People who aim to form true beliefs about how to have a positive impact on the world might prioritize upholding these norms even when following a&nbsp;<a href=\"https://80000hours.org/articles/coordination/#single-player-communities-and-their-problems\"><u>single-player strategy</u></a> and not trying to coordinate with others.</p><p>As one example, a person might decide to prioritize upholding social norms about how to allocate their time between work responsibilities, family responsibilities, and leisure.&nbsp; Such a person might hypothesize that social norms on that topic encode valuable information about the long-term consequences of working too much (or too little).&nbsp; They might also reason that they do not have clear guiding principles or \u201cstopping points\u201d to protect them from making mistakes if they deviate from the norms and try to allocate their time from first principles.</p><p>As another example, a person might decide to uphold social norms against trying to reason about a certain subject from first principles.&nbsp; Such a person might hypothesize that social norms encode valuable information about the risk of being misled by poor-quality evidence about that subject or the risk that inquiries into the subject could be misused to cause serious harm.&nbsp; They might also take the social norm as evidence that, on that subject, a different style of reasoning might be more productive than trying to proceed from first principles.</p><h2>A final word on strategies for upholding norms with compassion</h2><p>I have offered a handful of reasons why people who aim to have a positive impact on the world should uphold certain norms.&nbsp; But sometimes people worry that they will lose social or professional status or suffer other sanctions if they accidentally violate a norm.&nbsp; This dynamic causes some people to experience anxiety about the whole topic of social and professional norms, even if they are trying in good faith to have a positive impact on the world.&nbsp;&nbsp;</p><p>Responding to norm violations with compassion can help uphold important norms without contributing unnecessarily to this kind of anxiety and worry.&nbsp; Toward that end, here are some things you could consider doing if you notice that someone has violated a norm that you think is important:</p><ul><li><strong>Take the person who violated the norm aside </strong>and talk with them in private (e.g., through an email or direct message) about the norm.&nbsp; Find out how much they know about the norm, including the reasons why it exists.&nbsp; Point out the potential consequences of violating important norms for the person\u2019s ability to achieve their goals.</li><li><strong>Make a positive case for the norm</strong> by explaining the purposes it serves, either privately to the person who violated the norm or publicly to a relevant community.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7buuh4c92b6\"><sup><a href=\"#fn7buuh4c92b6\">[4]</a></sup></span></li><li>For serious violations,&nbsp;<strong>ask for an apology&nbsp;</strong>that demonstrates that the person who violated the norm understands the reasons why their actions might have been harmful and gives readers reasons to think the person who violated the norm won\u2019t do so again.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref353wlw33c14\"><sup><a href=\"#fn353wlw33c14\">[5]</a></sup></span><br>&nbsp;</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1evosoa3bbf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1evosoa3bbf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Many people also endorse<a href=\"https://plato.stanford.edu/entries/ethics-deontological/\">&nbsp;<u>deontological</u></a> arguments in favor of upholding certain norms, but this post does not focus on those arguments.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnztof1mh58r\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefztof1mh58r\">^</a></strong></sup></span><div class=\"footnote-content\"><p>If you are interested in learning more about this difficult subject, consider&nbsp;visiting <a href=\"https://www.genome.gov/about-genomics/fact-sheets/Eugenics-and-Scientific-Racism\"><u>this page from the U.S. National Human Genome Research Institute.</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzj6xm9ulvxj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzj6xm9ulvxj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Sometimes, people&nbsp;<i>want&nbsp;</i>to signal that their goals and beliefs are not like those of others who follow a certain norm.&nbsp; However, before deciding to break a norm as a way of sending that signal, it is important to understand the goals and beliefs that others actually associate with the norm; they might not turn out to be the goals and beliefs you expect or intend to be communicating about.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7buuh4c92b6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7buuh4c92b6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>One goal of this post is to make this kind of positive, public case.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn353wlw33c14\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref353wlw33c14\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I also liked the advice in&nbsp;<a href=\"https://www.health.harvard.edu/blog/the-art-of-a-heartfelt-apology-2021041322366\"><u>this article about heartfelt apologies</u></a> that&nbsp;<a href=\"https://forum.effectivealtruism.org/users/titotal\"><u>titotal</u></a> recently shared in another post.</p></div></li></ol>", "user": {"username": "AnonymousGuest"}}, {"_id": "sJpCYcHDGjHFG2Qvr", "title": "Introducing Lafiya Nigeria", "postedAt": "2023-01-17T17:52:41.384Z", "htmlBody": "<h1>Reducing maternal mortality through informed family planning</h1><p>In June 2021, we founded <a href=\"https://www.lafiyanigeria.org\">Lafiya Nigeria</a>, a non-profit organisation that works toward ending maternal mortality in Nigeria by widening the access and information about family planning.&nbsp;<br><br>TL; DR <a href=\"https://youtu.be/Aktt8OpEaXI\">Introduction to our organisation in a 3-min video</a></p><p>This post describes <strong>(I) </strong>the challenge we aim to solve, <strong>(II)</strong> our approach, <strong>(III)</strong>, our traction, <strong>(IV) </strong>our value-add, <strong>(V)</strong> our plans, <strong>(VI) </strong>how you can get involved in our initative.</p><h1><strong>(I) The challenge</strong></h1><p><strong>In low and middle-income countries, women are dying from giving life.</strong> Nearly <a href=\"https://www.guttmacher.org/report/adding-it-up-investing-in-sexual-reproductive-health-2019\">300,000 women and girls are dying from pregnancy-related complications each year</a>, according to the Guttmacher Institute. Other health complications such as <a href=\"https://www.who.int/news-room/facts-in-pictures/detail/10-facts-on-obstetric-fistula\">obstetric fistula</a>, <a href=\"https://pubmed.ncbi.nlm.nih.gov/21221586/\">postpartum anemia</a>, and <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7579387/\">postnatal depression</a> are also key health burdens borne by pregnant women.</p><p><strong>Reducing the number of unintended pregnancies is an effective means of reducing health burdens for mothers and newborns.</strong> Despite a significant number of women in these countries wanting to avoid pregnancy, many are not using modern contraceptives, resulting in 85 million unintended pregnancies per year. If all women with unmet need were provided access to and used modern contraceptives, the Lancet estimated that maternal deaths globally would drop 44%. <a href=\"https://www.guttmacher.org/report/adding-it-up-investing-in-sexual-reproductive-health-2019\">An estimated 70,000 maternal deaths could be prevented each year, with 441,000 new-born deaths also averted</a>. Additionally, the Guttmacher Institute estimates that every dollar spent on contraceptive services beyond the current level would reduce the cost of pregnancy-related and newborn care by<a href=\"https://www.guttmacher.org/report/adding-it-up-investing-in-sexual-reproductive-health-2019\"> three dollars</a>. The Copenhagen Consensus also found that<a href=\"https://www.copenhagenconsensus.com/post-2015-consensus\"> e</a>very dollar spent on access to modern contraception leads to <a href=\"https://www.copenhagenconsensus.com/post-2015-consensus\">120 dollars of social, economic, and environmental benefits.</a></p><p><strong>Access to family planning is beyond a health issue: its dividends are seen also &nbsp; in positive effects on education, income generation, and children's welfare.</strong> A study in Indonesia found that providing access to family planning was three times more effective than improving school quality in keeping girls in school an extra year. Research in Colombia found that girls with access to family planning clinics were <a href=\"https://www.cgdev.org/publication/reproductive-choices-life-chances-new-and-existing-evidence-impact-contraception-women\">7% more likely to participate in the formal workforce as adults</a>. Long-term studies have also shown that providing access to family planning programs can lead to <a href=\"https://www.brookings.edu/wp-content/uploads/2016/07/2013a_bailey.pdf\">improved college completion rates of children and higher family incomes decades later</a>. These spillover effects are difficult to measure and are often neglected in traditional cost-effectiveness analyses.</p><p><strong>In recent years, there has been</strong><a href=\"http://progress.familyplanning2020.org/measurement\"><strong> an increase in the use of modern contraceptives</strong></a><strong> in countries like Nigeria but there has also been an increase in the unmet need. </strong>From 2012 to 2019, <a href=\"https://fp2030.org/sites/default/files/Data-Hub/2019CI/Nigeria_2019_CI_Handout.pdf\">the portion of women using contraceptives in Nigeria</a> rose from 11.2% to 14.2%, while the unmet need also rose from 22.4% to 23.7%.&nbsp;</p><h2><strong>Focus: Nigeria</strong></h2><h3>Lafiya Nigeria focuses on rural and underserved regions of northern Nigeria</h3><p><strong>Nigeria has &gt;45M women of child-bearing age, and 65%&nbsp; have unmet contraceptive needs (IHME), resulting in around 40,000 maternal deaths a year.&nbsp;</strong></p><p>In Nigeria, <a href=\"https://nigerianstat.gov.ng/download/1241062\">over 83% of women</a> had not used any contraceptive methods for family prevention in 2018. This rate reached <a href=\"https://www.statista.com/statistics/1268145/contraceptive-prevalence-rate-among-women-in-nigeria/\">96% among women without any education</a>. This staggering gap in health provision results in maternal and infant deaths. In Nigeria, over <a href=\"https://allafrica.com/stories/202203090046.html\">40,000 women die each year from pregnancy-related issues</a>. The loss of life does end with mother, either. <a href=\"https://allafrica.com/stories/202203090046.html\">Over one million children under the age of five </a>also die as a result of losing their mothers to pregnancy delivery complications.</p><p>In our pilot region, Jigawa, more than 98% of women have no prior contraceptive use due to stockouts (38% nationally for injectables) and a shortage of trained medical personnel (1 doctor per 39,000 people in Jigawa).</p><h1><strong>(II) Our approach</strong></h1><p>Lafiya Nigeria\u2019s intervention is increasing access to self-injectable contraceptives (DMPA-SC, \u201cSayana Press\u201d)&nbsp; in the northern states and closing the information gap on family planning. The contraceptive is successful in removing obstacles to consistent usage: it is cost-effective, and self-administration ensures that the end user does not have to travel long distances to a health clinic repeatedly. One injection protects from pregnancy for 13 weeks, is easily transported without needing a cold chain, and is thus a good solution for the urgent unmet need for family planning in Nigeria.</p><p>We recruit \"Lafiya Sisters\" from existing local networks of nurses and midwives who were chosen by their communities to receive a 2-year healthcare education. &nbsp;Then we upskill them in FP counselling and administration, and equip them with Sayana-Press contraceptives, which they routinely distribute to rural women while working in health clinics and informal community settings.&nbsp;</p><p>Working with Lafiya Sisters is the key to our model. It not only allows us to provide a cost-effective solution, but also creates a long-term effect on behavioural change of the beneficiaries. Lafiya Sisters are based in local communities and work as nurses and midwives in hospitals and clinics, which means that they are trusted by our end users. The topic of family planning is still challenging to discuss in a public setting in northern Nigeria due to cultural and religious norms. Working solely with female health professionals who have already proven themselves to end users through their work is crucial in those settings.&nbsp;</p><h1><strong>(III) Our traction</strong></h1><p>Since winning the <a href=\"https://d-prize.org/winners/profiles/2021/6/29/lafiya-nigeria\">D-Prize</a> last year, Lafiya Nigeria has already successfully demonstrated that its innovative distribution model led by community champions reaches beneficiaries in places where other organisations fail.&nbsp;</p><p><strong>We have reached 2400 women, with each intervention costing only $3 per contraceptive delivered, with $12.4 per DALY averted and $30 per pregnancy averted.</strong>&nbsp;The program is estimated to have reduced one maternal death already.&nbsp;The model emphasises the agency of our users, opting for a DMPA-SC, a contraceptive that allows women to not only decide when and how they want to use contraception but is also self-administered.&nbsp;</p><p><strong>After 6 months of our operations, we met 342% of the scale of the government\u2019s distribution in the same time scale. </strong>We know we fill the gap in an area where government inefficiencies and neglect from other NGOs leave a massive unmet need. We are targeting women who are at substantially higher risk of maternal death (&gt;60% of our users), first-time contraceptive users (35% of our users), and without education (&gt;40% of our users).</p><p><strong>If you want to learn more about the results of our pilot program, please find the detailed pilot report</strong><a href=\"https://drive.google.com/file/d/1AMNeGO8N0VIQ-t3238NR4aD46z-z7B8L/view?usp=sharing\"><strong> here</strong></a><strong>.</strong></p><h1><strong>(IV) Our value-add</strong></h1><h3><strong>Focus on rural and hard-to-reach regions</strong></h3><p>We saw that the existing programs by other NGOs and the government tend to address unmet needs in urban areas and among an educated population with good access to the internet. Although this approach generates high-impact numbers, it misses an important and populous part of society. Lafiya Nigeria targets underserved communities that live in rural and hard-to-reach regions.&nbsp;</p><h3><strong>Community-based and female-led</strong></h3><p>Generating trust between CHWs and end-users helps alleviate concerns and eliminate misconceptions surrounding family planning, promoting continual usage for a lasting impact. Unlike other interventions targeting rural ideas, we are not doing outreach to local communities; we are rooted within.</p><h3><strong>Leveraging existing community programs&nbsp;&nbsp;</strong></h3><p>Lafiya\u2019s strategic partnership with Women4Health ensures that the team of Lafiya Sisters is trained to a high standard of healthcare literacy and can be easily upskilled with additional family planning training organised by Lafiya Nigeria. This can only be achieved through leveraging existing community skill-building programs and a trained workforce.</p><h3><strong>A fully digitalised monitoring system</strong></h3><p>Our solution allows our Lafiya Sisters to safely store user data even in locations without good internet connections. Using SurveyCTO, Lafiya Sisters can fill up the user questionnaire at the point of delivery and upload it later when they get back to their clinics. The questionnaire keeps track of geolocation and creates a time stamp, which allows us to prevent fraud and falsification of user records. Moreover, the questionnaire also has a voice recording option that saves the record of a user's consent, which protects users from being pressured to use contraception and confirms their identity.&nbsp;</p><h3><strong>Hyper-focus on the Sayana Press&nbsp;</strong></h3><p>Sayana Press is an innovative, cost-effective, and safe contraceptive approved in more than 40 countries, including the EU. It has a lower dosage than its intramuscular counterpart, Depo-Provera, leading to fewer side effects reported among users. Additionally, it does not need to be stored or transported in cold chains, making it the perfect choice for our intervention in Nigeria.</p><p><strong>We believe that family planning and, more specifically, Sayana Press, could be an underexplored and neglected cause area in the EA community. </strong><a href=\"https://www.givewell.org/international/technical/programs/sayana-press\">GiveWell </a>has indicated that the program is promising and requires further investigation. We aim to do rigorous evaluations of all our programming and therefore contribute to the literature on the effectiveness of Sayana Press interventions.</p><h1><strong>(V) Our plans</strong></h1><p><strong>We are currently starting our scale-up across Jigawa and Kebbi state. </strong>We want to replicate the success of the model in Jigawa in another state, as well as increase our impact. Within the next two weeks, we will train ~30 more health workers, which will increase our total number to 42.&nbsp;</p><p><strong>We also want to have a closer partnership with local and state governments.</strong> We believe that having a former Sexual and Reproductive Health System Strengthening, Government Policy Development Director, <a href=\"https://icfp2022.org/team-member/dr-kayode-afolabi/\">Dr. Kayode Afolabi</a> as a chair of our adivsory board will facilitate this process and lead to close collaboration between Lafiya Nigeria and the Federal Ministry of Health.</p><p><strong>Finally, we want to use best practices and rigourous evidence to further improve our model. </strong>We have founded a Lafiya Innovation Hub, a research group that aims at conducting short- and medium-term studies to provide insights into program design. Our first project is introduction of online training for our health workers to further decrease the costs, while maintaining high quality of services provided by our health workers.&nbsp;</p><h1><strong>(VI) How you can get involved</strong></h1><p>There are three ways in which you can get involved in our operations:</p><p><strong>Volunteering</strong>: We are looking for volunteers to collect further evidence for the intervention and conduct an impact evaluation, including a&nbsp;comparative cost-effectiveness analysis. As an evidence-based organisation, we always strive for the highest quality of data to inform our decisions.<br><br><strong>Funding</strong>: We have a $50,000 funding gap for 2023 to close to be able to reach our targets and scale-up our operations. Individual donors may donate on our website (US-based donors please contact us to sort out . If you know about any relevant funding sources, please get in touch with us.</p><p><strong>Technical advice</strong>: We seek expert academics and practitioners on family planning and mHealth interventions to discuss our ideas and get feedback.<br>&nbsp;</p><p>We are proud of the work that Lafiya Nigeria has done so far and we are optimistic about the impact it can continue to bring in the future. As an EA-aligned organisation, we want to prioritise becoming a core part of the EA Global Health &amp; Development community to be able to share our learnings with others and strive together to make a substantial difference in the world.&nbsp;<br>&nbsp;</p><p><a href=\"https://www.lafiyanigeria.org/klaudia-chmielowska-1\"><i>Klau Chmielowska</i></a><i> and </i><a href=\"https://www.lafiyanigeria.org/jefferson-chen\"><i>Jefferson Chen</i></a><i> are the co-founders of Lafiya Nigeria.&nbsp;</i></p>", "user": {"username": "Klau"}}, {"_id": "a4YMsCkiCycGFxpTJ", "title": "Nature: The importance of philanthropy in scientific research", "postedAt": "2023-01-17T03:43:45.632Z", "htmlBody": "<p>An interesting article from <em>Nature</em> on how donations are crucial for scientific research:</p>\n<blockquote>\n<p>It takes money to do research and, increasingly, much of it comes from private donors and foundations, especially in the United States...</p>\n</blockquote>\n<blockquote>\n<p>Overall, philanthropic groups in the United States contribute US$30 billion annually to science, similar to the amount spent by federal agencies, according to a 2022 analysis of US non-profit tax returns.</p>\n</blockquote>\n<blockquote>\n<p>That\u2019s more than in Europe, where a 2016 review of about 1,000 foundations estimated that philanthropies contributed more than $6.4 billion a year to research and innovation. That is only around 2% of the $353.6 billion total for the region, which was provided mostly by governments and industry.</p>\n</blockquote>\n<p><em>Nature</em> interviews four scientists on the differences between philanthropic and government funding:</p>\n<blockquote>\n<p>There\u2019s no central repository where you can find out about all the possible private funding avenues... I asked people who are further on in their careers whether they knew of any funding, fellowships or prizes that I should be aware of. I got lots of helpful advice. Otherwise, it requires trawling the Internet. (Danika Hill)</p>\n</blockquote>\n<blockquote>\n<p>With my philanthropic grants, if some issue came up, I could make a phone call to say, \u201cI\u2019m facing this problem. Could I rearrange the budget a little bit?\u201d But with a government grant, that kind of conversation is basically impossible. (Rafael Chaves)</p>\n</blockquote>\n<blockquote>\n<p>In Israel, science is not as well funded by the government as scientists would hope compared to other spending. As scientists there, we spend a lot of time applying for grants and thinking about the next one, and it doesn\u2019t get easier with time because we live in a country where there\u2019s always a crisis that needs funding for the military. Science is always at the end of a long list of priorities, which is a real problem. (Hermona Soreq)</p>\n</blockquote>\n<blockquote>\n<p>The federal research awards process, from the very small grants to huge projects, is largely driven by peer review. This means that scientists need to convince other scientists that this is a good use of money, either by writing proposals or by trying to drum up support. At the individual donor level, by contrast, much is face to face \u2014 a lot of it is personal chemistry. (Steven Kahn)</p>\n</blockquote>\n", "user": {"username": "Michael Huang"}}, {"_id": "WZf6KpmajZXs596JG", "title": "Preparing for AI-assisted alignment research: we need data!", "postedAt": "2023-01-17T03:28:29.441Z", "htmlBody": "<p><i>Thanks to Neel Nanda, Madeleine Chang, Michael Chen, Oliver Zhang, and Aidan O\u2019Gara for their early feedback on this post.</i></p><p><i>Summary: Alignment researchers could record the intermediate outputs of their work in anticipation of using the data to train future AI models. These models could eventually accelerate alignment research by imitating the workflows and thinking styles of these researchers, which could be useful for many applications. The data used to train these models could come from the document and code version histories, screen recordings, keystrokes, recordings of virtual and in-person meetings, and running verbal commentary during the researchers\u2019 work. Although some data collection methods risk being too inconvenient or distracting to the researchers, many are likely worth the cost.</i></p><p>There\u2019s been a fair amount of discussion in the AI safety community about using AI to accelerate AI alignment research. In theory, AI could be used to quickly write up polished articles from rough notes, automate repetitive tasks, write code, and even facilitate idea generation. In the limit of capabilities, an AI model could even act as a fully independent researcher, pursuing novel research directions, reasoning effectively, and communicating its findings comprehensively to humans.</p><p>This idea has been covered elsewhere, so I\u2019ll try not to retread too much old ground here. Please see&nbsp;<a href=\"https://www.lesswrong.com/posts/JKgGvJCzNoBQss2bq/beliefs-and-disagreements-about-automating-alignment\"><u>this post</u></a> for a compilation of many different researchers\u2019 takes on the issue, or section 4 of OpenAI\u2019s recent blog post,&nbsp;<a href=\"https://openai.com/blog/our-approach-to-alignment-research/\"><u>Our Approach to Alignment Research</u></a>.</p><p>Personally, I\u2019m fairly optimistic about AI-aided alignment research being a promising line of inquiry. However, I haven\u2019t seen much discussion about preparing for this possibility by acquiring one of the most crucial resources for training AI - data. In this post, I argue that future alignment-boosting AIs would benefit greatly from domain-specific data derived from AI alignment research processes (which I\u2019ll refer to as&nbsp;<i>alignment data</i> for short).</p><h1>Proposal and rationale</h1><p>Other than compute and the discovery of useful algorithms,&nbsp;<a href=\"https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications\"><u>data is one of the most important bottlenecks</u></a>&nbsp; for training modern AI. If we want to train a model to do alignment research like a human can, I expect that a dataset of humans doing alignment research would make this a lot easier.</p><p>To be sure, some forms of alignment data, like blog posts and papers about AI alignment, could be derived easily from existing resources. However, data about the process that AI alignment researchers take to generate their research could be even more valuable. Consider the&nbsp;<a href=\"https://arxiv.org/abs/2201.11903\"><u>research from Wei et al.</u></a> showing that prompting large language models to \u201cthink step-by-step\u201d can significantly improve their performance. This is especially relevant in academic research, where years of thought can go into a ten-page paper; we can hardly expect to get good results by simply typing \u201cWrite the abstract of a paper that solves the AI alignment problem\u201d into an LLM.</p><p>These datasets could be derived from a huge variety of sources, which I\u2019ll discuss more later in this post. A few of the most promising are probably Google Doc edit histories, screen recordings, and conversations between alignment researchers.</p><h2>Imitative AI may be safer than goal-driven AI</h2><p>An alternative to training a purely generative model would be to focus mostly on RL techniques by training a model to optimize for receiving positive human feedback on AI alignment research. Perhaps the model would be fine-tuned from a model trained only on a general text corpus. Since a reinforcement learner would likely require much less domain-specific data upfront than a generative model, the recommendations in this post would be less relevant. However, I believe that generative models are less likely to try reward hacking and deception compared to more \u201cgoal-directed\u201d AI.</p><p>Models in the style of&nbsp;<a href=\"https://en.wikipedia.org/wiki/DALL-E\"><u>DALL-E 2</u></a> or the original&nbsp;<a href=\"https://en.wikipedia.org/wiki/GPT-3\"><u>GPT-3</u></a>, which deontologically try to \u201cdo things a human would do\u201d rather than trying to maximize reward, are probably unlikely to take highly unusual and exploitative actions, even when scaled up to greater capabilities. The main caveat is that human-imitating AIs would likely asymptote at or below a human level of performance; however, given enough training data, a fully-trained model may still be able to do research orders of magnitude more quickly than human researchers just by virtue of having greater numbers (i.e. arbitrary numbers of copies) and speed.</p><p>Even if we do end up having to train a reward-based model to help solve alignment, we could still use these datasets to train the model to get as close as possible to being able to do its own research, then get it over the final hump with human feedback. Intuitively, I expect that the less we train a model on human feedback, the higher its potential capabilities will likely get before it learns to game the reward and starts taking deceptive or dangerous actions.</p><h2>Creating a comparative advantage for alignment research</h2><p>As far as I know, there are no researchers collecting data on their own research processes with the intention of training an AI model in the future - not in AI alignment, not in AI capabilities, and not in any other scientific fields either. If alignment researchers took the lead in this area, it could give them a comparative advantage in the quality and quantity of their training data once AI technology has advanced enough to effectively use it.</p><p>This could help to address a potential issue with relying on AI to solve the alignment problem: once AI technology has reached a level that could significantly accelerate scientific research, it\u2019s not clear that it would be used to accelerate&nbsp;<i>AI alignment research</i> in particular, and not, say, AI capabilities research. By collecting alignment data, we can make AI-accelerated alignment research more feasible without providing other scientific fields with a similar advantage.</p><p>Of course, the more data we manage to collect, the greater our comparative advantage will be. This suggests that those who expect longer AI timelines will be more excited about the strategy in this post, since if they\u2019re right, we\u2019ll have more time to collect data.</p><h1>Potential uses for the data</h1><p>Here is a more comprehensive list of potential uses for alignment data, roughly ordered by technical feasibility:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflnfmk0a2ie\"><sup><a href=\"#fnlnfmk0a2ie\">[1]</a></sup></span></p><ul><li>Sharing techniques among human researchers: This particular application wouldn\u2019t even involve AI. If researchers shared recordings of their work, such as coding, writing, or group discussions, they could help others to understand the job and learn valuable thought processes. In fact, Neel Nanda&nbsp;<a href=\"https://www.lesswrong.com/posts/sYHrW4wwfoMBxNDcA/real-time-research-recording-can-a-transformer-re-derive\"><u>did something like this recently</u></a>.</li><li>Serving as a writing assistant: An AI trained on alignment data could assist with crafting clear and concise prose, allowing researchers to focus more on the ideas and less on the wording. This could mean editing existing writing or writing up a full post from rough notes. (This is already possible - I used&nbsp;<a href=\"http://chat.openai.com\"><u>ChatGPT</u></a> to make many parts of this post more concise and focused.)</li><li>Acting as a sounding board: An AI trained on alignment data from experienced researchers could provide feedback and advice to aspiring alignment researchers, helping to alleviate the shortage of mentors in the field. While it may be difficult to achieve the level of complex and thoughtful feedback provided by a real human, it may still be useful to develop AI models that can give more generic feedback. For example, \"I think your argument would be clearer if you gave a specific example of this abstract concept,\" or \u201cyou could try using this mechanistic interpretability technique to understand the model\u2019s behavior here.\u201d</li><li>Fully automating research: To me, the most promising use of alignment data is to train an AI that can fully automate alignment research, potentially working much faster and more efficiently than a human and able to be copied as many times as necessary. Maybe an AI with these capabilities would already be highly unsafe, but I think the risk goes down if it\u2019s trained to purely imitate human behavior rather than to pursue a goal. Future alignment research may also make it possible to train AI using methods that incorporate human feedback more safely than basic RLHF.</li></ul><p>Of course, applications will vary depending on which form of alignment data we use. I\u2019ll go into that in the next section.</p><h1>Some data collection methods</h1><p>How might alignment researchers record their work? I list a few possible sources of alignment data below.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefw91ianoby2\"><sup><a href=\"#fnw91ianoby2\">[2]</a></sup></span></p><h2>Posts and papers</h2><p>There are many publicly available documents about alignment written by researchers on platforms such as LessWrong, the Alignment Forum, the EA Forum, arXiv, and other websites. The comments on alignment-related posts could also be useful for teaching an AI to judge and iterate on its own ideas. However, while these documents may provide some useful data for training AI models, they do not offer much insight into the step-by-step process of research. On the plus side, they already exist and would be relatively trivial to collect - in fact, a dataset just like this has already been compiled (see&nbsp;<a href=\"https://www.lesswrong.com/posts/FgjcHiWvADgsocE34/a-descriptive-not-prescriptive-overview-of-current-ai\"><u>this post</u></a>).</p><h2>Version history (code)</h2><p>Code repositories on platforms like GitHub have already been used to train AI models like OpenAI Codex. While tools like Codex could potentially be useful for alignment research, creating a dataset of code written by alignment researchers is likely less important than creating a dataset of alignment-related communications documents (discussed in the next section). Most crucial insights are likely to be found in the latter, and code completion models like Codex already exist and are likely to continue improving regardless of our efforts. Writing \"AI alignment code\" and writing \"AI capabilities code\" may be similar in many ways, and it is likely that AI capabilities in both areas will increase concurrently.</p><h2>Version history (documents + other forms of communication)</h2><p>Many alignment researchers use Google Docs and related products like Slides and Sheets on a daily basis. These products have a built-in version history system that tracks edits on a line-by-line basis, which could potentially be used to generate a dataset for training AI models.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0ukm7kog8ec\"><sup><a href=\"#fn0ukm7kog8ec\">[3]</a></sup></span></p><p>For example, an AI model could be trained to look at the state of a document midway through the editing process and guess what deletions or insertions the author will make next. This approach could encourage the AI to follow a logical series of steps in refining its ideas, rather than simply generating a fully formed document word-by-word like GPT does. We could also include comments from document reviewers in the dataset, which would allow the AI to practice incorporating human feedback into its writing, as a real researcher would.</p><p>There are already many Google Docs written by alignment researchers that could be retroactively included in this dataset, making it possible to take advantage of past work. To prepare for the possibility of using document version histories in a training dataset, researchers could be encouraged to do their work in Google Workspace or another product with version history. It may also be helpful to encourage researchers to write down their rough thoughts at the beginning of the writing process and refine them as they develop the document, so that the AI can better replicate their thinking process. However, if these changes would significantly hinder researchers' productivity, they may not be worth it.</p><p>To ensure that this data is available when we need it, it may be necessary to develop a secure and convenient way to back up Google edit histories. For example, someone could create a bot that logs the edit history of any document shared with it by a researcher. It might also be possible to track even smaller changes, such as single-character deletions and insertions, which would be a step closer to full keystroke and screen recording (as discussed later).</p><p>In addition to Google Docs, researchers could incorporate Slack messages, email, and other forms of written communication into alignment datasets without making any special changes to how they work.</p><h2>Screen recordings and keystrokes</h2><p>Of course, besides Google Docs and messaging apps, researchers use a wide range of applications on a daily basis, including web browsers, coding IDEs, and productivity apps such as Asana. Activity on these products isn\u2019t usually recorded. However, if we recorded researchers' screens and/or keystrokes while they worked, we could capture all of this and more, providing an even more detailed and comprehensive view of the research process.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwlgemy5mna\"><sup><a href=\"#fnwlgemy5mna\">[4]</a></sup></span></p><p>Screen recordings have already been used to train AIs, including OpenAI\u2019s&nbsp;<a href=\"https://openai.com/blog/vpt/\"><u>Video PreTraining</u></a> model, which learned to play Minecraft from YouTube videos. Recording keystrokes alongside this video would make it even easier for an AI to learn how to use a computer to do effective research.</p><p>It\u2019s important to note that collecting screen recordings and keystrokes is likely to be more intrusive than simply gathering document version histories, and some researchers may feel uncomfortable or distracted by this.</p><h2>Meetings</h2><p>Meetings could be a valuable source of alignment data, as they often have very high information density. During meetings, researchers ask questions, provide feedback to each other, and try to explain their ideas as comprehensively as they can. This data could provide valuable information for training models to work collaboratively with human or other AI researchers.</p><p>Meeting recordings might consist of video (especially if a whiteboard is involved) or just audio. It\u2019s easy to record virtual meetings, and setting up a recording device to document in-person meetings probably wouldn\u2019t be difficult either.</p><p>Like the collection methods mentioned in the last section, meeting recordings can be intrusive, and the idea of saving private conversations may feel especially creepy. Researchers definitely shouldn\u2019t expect everybody they meet with to take part in this collection process, and if they do ask other meeting participants for consent to be recorded, they should be extremely clear about what the recording will be used for. I\u2019ll go into this issue in more detail in the section on security and privacy concerns.</p><h2>Running verbal commentary</h2><p>As I linked earlier, Neel Nanda recently&nbsp;<a href=\"https://www.lesswrong.com/posts/sYHrW4wwfoMBxNDcA/real-time-research-recording-can-a-transformer-re-derive\"><u>recorded himself</u></a> working on a toy research problem in a Python notebook, where he trained a toy transformer without positional embeddings to see if it could rederive them. While this video was originally intended to help other human researchers, I expect that recordings like this could be even more valuable for an AI. The previously-mentioned dataset of screen recordings and/or keystrokes could be even more useful if it were accompanied by explanations of what the researcher is doing. This could even help improve interpretability if an AI were trained to generate similar commentary as it works.</p><p>This form of data collection would probably add the most overhead of any others I\u2019ve mentioned, and for most researchers, it would be best to do this only occasionally, if at all. On the other hand, I imagine that some researchers would find it helpful to explain their thoughts verbally, sort of like&nbsp;<a href=\"https://en.wikipedia.org/wiki/Rubber_duck_debugging\"><u>rubber duck debugging</u></a>. For his part, Neel estimated that if he did an hour of the work he normally does, but he had to narrate his thoughts, it would take him 10-20% more time. Creating a good tutorial (choosing a good toy problem, making a usable tutorial notebook, and editing the video) takes him longer, but I expect that these would be less important in an ML training dataset.</p><h1>Security/privacy concerns</h1><p>One potential objection to this idea is that making extensive recordings of AI alignment research could pose risks to the security and privacy of both researchers and institutions. Starting a program to collect researcher\u2019s data could make it easier for a hacker or internal leaker to do a lot of damage. Especially within the context of AI alignment research, there could be many information security risks - say, research that improves AI interpretability while also heightening a model\u2019s capabilities - that we should take pains to reduce.</p><p>These concerns are more relevant for some applications over others. If we were going to release a model on the Internet that any would-be AI safety researcher could use to get advice on their research, it would be bad if someone could prompt the model into explaining Anthropic\u2019s top-secret research, giving them Paul Christiano\u2019s social security number, or realistically simulating dozens of conversations with Eliezer Yudkowsky to find the best way to convince him of something. On the other hand, if we had a group of fully unsupervised, human-aligned, artificially-intelligent researchers without write access to the Internet who would share their research with a small group of highly-trusted human operators, this may not matter quite as much.</p><p>I expect that while some alignment researchers would be willing to contribute to alignment datasets in one form or another, others would find the idea of being constantly recorded unsettling or intrusive. It would be important to avoid creating a culture, either within specific organizations or in the AI alignment community at large, where taking part in these measures is expected or required. If AI alignment researchers are perceived as recording themselves excessively or inappropriately, it could harm the field's reputation and discourage potential researchers from joining. (\u201cNot only do AI safety people believe these weird things about technology, they all have to sign up for constant surveillance by a panopticon? No thanks!\u201d)</p><p>If the alignment community were to create a centralized data collection program (an idea I\u2019ll explain in the conclusion), it would be important to build trust with the researchers who do choose to opt in by providing clear guidelines on how the collected data will be stored and protected, minimizing the number of people with access to the data, and giving users control over when and under what conditions they would want to release their data to an organization that wants to use it. (This might mean letting users independently handle the storage of their alignment data.)</p><h1>Conclusion</h1><p>While the use of AI to accelerate AI alignment research may still be a ways off, it\u2019s not too early for the alignment community to start thinking about what kinds of datasets would be helpful for this purpose and how we can collect them.</p><p>If you are an alignment researcher and this post has convinced you that collecting alignment data is worth doing, you could start collecting your own data right now! For instance, you could make recordings of some of your meetings, start recording your screen as you work, or switch to Google Docs if you don\u2019t already use it. Of course, please keep in mind the privacy and safety concerns raised above.</p><p>If the AI alignment community really buys into this idea, the next step up from collecting data on a per-researcher basis would be to coordinate the alignment data collection project under the banner of a single organization. Such an organization could benefit researchers by providing dedicated tools to collect alignment data, setting standards and best practices for data collection, and possibly storing some alignment data in a centralized database. If you\u2019d be interested in participating in a large-scale project like this, either as an individual data-collector or as an organizer, please comment below or DM me so I can gauge interest! While I\u2019m not sure I\u2019m in the best position to take the lead on this, I\u2019ll willingly be the point-person to connect interested parties to one another.</p><p>I\u2019d love to hear what others think about these ideas. Do you think collecting data now to train future AIs to accelerate alignment research is a viable strategy? Are there other use cases or collection methods for alignment data that I haven\u2019t mentioned here? Do you have concerns about security or privacy that I haven\u2019t addressed? Please share your thoughts in the comments.</p><p><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlnfmk0a2ie\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflnfmk0a2ie\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For more on potential use cases for the data, see <a href=\"https://www.lesswrong.com/posts/a2io2mcxTWS4mxodF/results-from-a-survey-on-tool-use-and-workflows-in-alignment\">Results from a survey on tool use and workflows in alignment research</a>. It goes over the pain points experienced by real alignment researchers in great depth, in an effort to figure out which kinds of tools are most useful.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnw91ianoby2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefw91ianoby2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In this list, I try to err on the side of&nbsp;<i>too much</i> data - for instance, I favor video over audio, and audio over text, even though the text is likely what we\u2019ll end up using. We can post-process a data source into a more digestible format for the ML model we happen to be training, but we can\u2019t get back data once we\u2019ve thrown it away, and we can always buy more storage space if we need to.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0ukm7kog8ec\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0ukm7kog8ec\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For full disclosure, I happen to currently work at Google (though I don\u2019t work anywhere near the Docs team and as a low-level engineer, I\u2019m not really incentivized to get people to use the product). I haven\u2019t used any inside information from Google to inform this post.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwlgemy5mna\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwlgemy5mna\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Eye-tracking data, synced to screen recordings and/or keystrokes is another potential source of data. Information on which parts of the screen a researcher finds important could conceivably be useful when training an AI, and eye-tracking data may not be very difficult to collect (it can be recorded by <a href=\"https://gaming.tobii.com/product/eye-tracker-5/\">an unobtrusive bar at the bottom of your monitor</a>). Still, I relegated eye-tracking to a footnote because the tools are expensive and the data would likely be of minimal use.</p></div></li></ol>", "user": {"username": "caleb-biddulph"}}, {"_id": "PWvfjs3zgjDabPaKT", "title": "SBF Has a Substack!", "postedAt": "2023-01-17T02:16:21.179Z", "htmlBody": "<p>Sorry if this was mentioned before . . . I find the EA Forum almost impossible to navigate in this respect.&nbsp;</p><p>Anyway, SBF has somehow been allowed enough computer access to create a Substack where his first post is devoted to defending himself: <a href=\"https://sambf.substack.com/p/ftx-pre-mortem-overview\">https://sambf.substack.com/p/ftx-pre-mortem-overview</a></p><p>As a former lawyer, I can barely imagine the utter nightmare it must be to represent SBF when he makes so many statements in public like this.</p>", "user": {"username": "Stuart Buck"}}, {"_id": "zWscvNhd3xeGJabEn", "title": "Announcing aisafety.training", "postedAt": "2023-01-17T01:55:46.736Z", "htmlBody": "<p>Crossposted to <a href=\"https://www.lesswrong.com/posts/MKvtmNGCtwNqc44qm/announcing-aisafety-training\">Less Wrong</a><br><br>To help people find what to apply to, <a href=\"https://aisafety.training\">aisafety.training</a> acts as&nbsp;<strong>a well-maintained living document of AI safety programs, conferences, and other events</strong>. This will smooth the experience of people working on and joining AI safety and reduce the burden on word-of-mouth transmission of available programs. This can also be helpful for field builders planning events to see when other things are happening to plan around. We at&nbsp;<a href=\"https://www.aisafetysupport.org/home\"><u>AI Safety Support</u></a> have been internally maintaining this document for some time and using it in our&nbsp;<a href=\"https://www.aisafetysupport.org/resources/career-coaching\"><u>free career coaching calls</u></a>. We now have a public-facing version, a form to add anything we\u2019ve missed, and&nbsp;<a href=\"mailto:maintainer@aisafety.training\"><u>an email</u></a> to alert us to corrections.</p><p><br><strong>For example, below, you will find static / will-soon-be-outdated images without clicking</strong> on the website link.</p><p>&nbsp;</p><p><strong>Application Deadlines</strong></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1673920547/mirroredImages/zWscvNhd3xeGJabEn/jpe9reiyorfmabye12kp.png\"></p><p><strong>Program Timeline</strong></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1673920547/mirroredImages/zWscvNhd3xeGJabEn/tfposy9rnae5mmjrmwqr.png\"></p><p><strong>Program table</strong></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1673920547/mirroredImages/zWscvNhd3xeGJabEn/izhxtmv7vlozim9vjjpt.png\"></p><p>If you\u2019re interested in helping to maintain this and can&nbsp;<i><u>consistently</u></i> dedicate a few hours per week to reading places where things might have been announced, accepting additions from the form and corrections by email, or if you have feedback or feature requests, leave us a comment or drop by the&nbsp;<a href=\"https://alignment.dev/\"><u>Alignment Ecosystem Development</u></a>&nbsp;<a href=\"https://discord.gg/PXjgxfXDdZ\"><u>Discord</u></a>, where this is worked on along with related projects to improve the information ecosystem such as&nbsp;<a href=\"https://aisafety.community/\"><u>aisafety.community</u></a> and the upcoming&nbsp;<a href=\"http://aisafety.world/\"><u>aisafety.world</u></a> and&nbsp;<a href=\"https://ea.domains/\"><u>ea.domains</u></a>.</p><p>&nbsp;</p><h2>Subscribe</h2><p>[Edit] You can subscribe to updates to this database <a href=\"https://airtable.com/shrEp75QWoCrZngXg\">here</a></p><p>Additionally, if you wish to have a monthly reminder of upcoming events sent directly to your inbox, subscribe to AI Safety Support\u2019s <a href=\"https://www.aisafetysupport.org/newsletter\">newsletter here</a>.&nbsp;</p>", "user": {"username": "JJ Hepburn"}}, {"_id": "kuwQzabCACwEz34i8", "title": "Where to find EAG funding support? ", "postedAt": "2023-01-17T01:46:31.998Z", "htmlBody": "<p>I am a university student who is pretty involved with EA. I have been engaging with the community for about a year now, and the movement has helped me make better decisions about my career and what I want to do.&nbsp;</p><p>I recently got into EAG Bay Area, and I am extremely excited at the prospect of meeting people and exchanging ideas. I will also be taking a gap year next year, so this could be a great time to go and help me figure out what I want to do.&nbsp;</p><p>Combining flights and accommodation, it will be over what I am comfortable with as a university student. Are there any ideas on where I can apply for (at least <i>some) </i>funding? &nbsp;My total budget is under $1.5k USD, and will be flying out of Toronto.&nbsp;</p><p>I am aware CEA offers some support, but they have explicitly mentioned operating under the assumption that they will not be able to give most applicants funding.&nbsp;</p>", "user": {"username": "aj_kourabi"}}, {"_id": "yNitwYkHP6DtkkSrG", "title": "Should AI writers be prohibited in education? ", "postedAt": "2023-01-16T22:29:46.511Z", "htmlBody": "<p><i>Note: this is an attempt to engage with and interpret a piece of legislation on the usage of AI. I don't have a strong opinion on this yet and expect it to be controversial which is why I preferred the Question Mode.</i></p><p>&nbsp;</p><p>In the AI Act, i.e., EU's regulatory framework for the usage of AI-related technologies it is mentioned that&nbsp;</p><blockquote><p>The following artificial intelligence practices shall be prohibited:<br><br>the placing on the market, putting into service or use of an AI system that<br>exploits any of the vulnerabilities of a specific group of persons due to their<br>age, physical or mental disability, in order to materially distort the behaviour of<br>a person pertaining to that group in a manner that causes or is likely to cause<br>that person or another person physical or psychological harm.&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Title II, Article 5, p. 43 (English Version).</p></blockquote><h3>I'll set up one interpretation of this statement in debate form:&nbsp;</h3><p>Question: should AI writers be prohibited in education?&nbsp;</p><p>Claim: we can stretch this statement to apply to the usage of AI writing products employed by underage students for their assignments. This technology exploits students' inability to make a fully-informed and thoughtful decision as to what would be beneficial for their intellectual development and education. Therefore, the practice should be prohibited.&nbsp;</p><p>Counterclaim: the AI system is not exploiting anyone's vulnerability as the notion of vulnerability should not be considered to entail one's proneness to dishonesty or cheating behaviors. Therefore, AI writers should not be prohibited and students should be held accountable for cheating when using AI writing models to compose their assignments.&nbsp;</p><p>&nbsp;</p><p>Feel free to continue the debate in the comments section.&nbsp;</p>", "user": {"username": "eangelou"}}, {"_id": "AoqwhAY5faTHfYu4Z", "title": "Some intuitions about fellowship programs", "postedAt": "2023-01-16T20:26:00.608Z", "htmlBody": "<p><i>Epistemic status: not sure how reliable my intuition is, but&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/AoqwhAY5faTHfYu4Z/some-intuitions-about-fellowship-programs#Background\"><i><u>I do have more experience with these programs than most</u></i></a><i>.</i></p><h1>TL;DR</h1><p>Making sure&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/AoqwhAY5faTHfYu4Z/some-intuitions-about-fellowship-programs#Number_of_participants\"><strong><u>&gt;30 participants</u></strong></a> have regular&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/AoqwhAY5faTHfYu4Z/some-intuitions-about-fellowship-programs#Shared_space\"><strong><u>opportunities to spontaneously gather</u></strong></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/AoqwhAY5faTHfYu4Z/some-intuitions-about-fellowship-programs#Active_programming\"><strong><u>active programming</u></strong></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/AoqwhAY5faTHfYu4Z/some-intuitions-about-fellowship-programs#Basic_amenities\"><strong><u>basic food and medical amenities</u></strong></a>, and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/AoqwhAY5faTHfYu4Z/some-intuitions-about-fellowship-programs#Shared_dates\"><strong><u>common knowledge about visit dates</u></strong></a><strong>&nbsp;</strong>hugely increases the benefit of residential fellowship programs.&nbsp;</p><p>For transparency, I should note a stronger, less confident belief of mine that I will not defend here. My instinct is that&nbsp;<i>all&nbsp;</i>of the above factors are not merely beneficial but necessary in order for these programs to be worth their cost (assuming that their goals are indeed as I&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/AoqwhAY5faTHfYu4Z/some-intuitions-about-fellowship-programs#Fellowship_programs\"><u>describe below</u></a>). I am aware that some factors come with large time or money costs. If these costs are prohibitive, so be it.</p><h1>Background</h1><p>I ran one of these programs and participated in two others. So what follows is a post-mortem of my own mistakes as much as it is feedback for organizers and recommendations for funders and organizers considering future initiatives.</p><p>It\u2019s worth emphasizing that I got lots out of each of these experiences, and that I feel very grateful to fellow participants and organizers!</p><h1>Fellowship programs</h1><p>I take the goals of these programs to be some combination of:</p><ol><li>Increasing short-term productivity,</li><li>Generating counterfactual collaborations/relationships, and</li><li>Moving participants towards more impactful careers at an accelerated rate.</li></ol><p>I take the method of these programs to be some combination of:</p><ol><li>Hosting people who largely do not know each other in a new physical environment for a small number of months,</li><li>Covering the cost of accommodation, co-working space, and travel to-and-from, and</li><li>Organizing social and professional activities.</li></ol><p>Things that I am not talking about include:</p><ol><li>Communities where many people already know one another, and</li><li>Themed retreats, or other temporary communities organized around a professional aptitude/cause area/etc.</li></ol><h1>Number of participants</h1><p>My intuition is that hosting 35 participants is much, much better than hosting 25. Not only in total, but on a&nbsp;<i>per-participant</i> basis.</p><p>Evidence for the directional claim:</p><ol><li>In the program I ran, the overwhelming majority of reported positive impact anecdotes came during the ~50% of the time we hosted &gt;30 participants.<ol><li>In fact, I need to get to the joint-30th most subjectively impressive anecdote before finding one I think came about during the time we hosted &lt;=30 participants.</li><li>A possible exception is the first few weeks of the program, when participants came across many novel ideas, relationships, and physical spaces even with smaller numbers of participants.</li></ol></li><li>With &gt;30 participants, shared spaces were often packed in evenings, participants participated in and ran a wider variety of well-attended activities, the office felt alive, and work felt urgent.<ol><li>Among other ~vibes~ based on discussions with participants, my time as a participant, and my time as an organizer.</li><li>30 might feel like an arbitrary cut-off: readers might think that I don\u2019t mean this literally. In fact, I am tempted to defend this close-to-literally. One piece of evidence: in my experience, there are almost no spontaneous gatherings of most participants below 25; these gatherings become somewhat more likely beyond this, then shoot up around 30. They continue increasing beyond 35, albeit more slowly.</li></ol></li></ol><p>My guess is that the effect runs through two mechanisms:</p><ol><li>The chance that&nbsp;<i>someone's</i> professional experiences are highly complementary with your own increases sharply around this point.<ol><li>Maybe because some small clusters of people with shared interests choose to participate at the same time \u2014 although I think this was largely not the case.</li></ol></li><li>The chance that someone will join you if you spontaneously wish to socialize increases sharply around this point.</li></ol><h1>Shared space</h1><p>Participants will have many more opportunities to meet cool people \u2014 and have more fun \u2014 if they can meet cool people ~at will. Numbers help with this. But, if participants do not have a common non-office space that they expect to find others in, it will be highly effortful for them to gather in groups of more than a couple of people.</p><p>This problem is solved by having a large, shared, private-to-the-group, always-available social space, ideally located where participants live, with organizers creating common knowledge about the best times for gathering in the space.</p><h1>Active programming</h1><p>The residential fellowship program has brought a bunch of people together. Participants have a shared social space. But social coordination problems still abound. They want to hear about everyone\u2019s cool projects, but don\u2019t have the energy to set up 20 high-commitment one-on-ones. They want to attempt projects or productivity programs together with other participants, but don\u2019t have the time to coordinate a medium-sized group.</p><p>By \u201cactive programming\u201d I do not mean anything elaborate, like the kind of programming one might do for a retreat or conference. Instead, I mean solving social coordination problems in the cheapest ways possible. Ideas include:</p><ol><li>Shared calendar \u2014 you add all participants as editors, they add their own events.</li><li>Low-effort talk series \u2014 you coordinate participants to show up, they listen to whomever signed up to speak (on an open sheet), you keep time.</li><li>Hackathons \u2014 you coordinate participants to show up, they suggest a project for the day, they build things, you coordinate peer judging.</li><li>Poetry night \u2014 you coordinate participants to show up, they bring a poem they like.</li><li>Coffee/chocolate/etc. tasting \u2014 you coordinate participants to show up, someone whose hobby this is expenses some inexpensive flavors, they taste.</li><li>Themed party \u2014 you coordinate participants to show up, you rip off the work-appropriate subset of&nbsp;<a href=\"https://twitter.com/RichardMCNgo/status/1607975213832228865\"><u>Richard Ngo\u2019s party ideas</u></a>, they have fun.</li></ol><p>No need to confirm/rely on attendance, no need to pay for stuff above and beyond what would already be provided.&nbsp;</p><p>The only aspect that should be effortful is frequency. The average day should have at least one such event. Participants are busy. They aren\u2019t interested in everything, nor should they be. Having more events allows them to fit the social calendar to their preferences and schedules.</p><p>Much of this feels like pretty standard stuff for a well-functioning group house of young professionals. The main difference is that the participants don\u2019t start off as friends, so they need frequent, gentle shoving to do the things they want to do.</p><h1>Basic amenities</h1><p>I have three things in mind here.</p><ol><li>Buffer office food and internet access. Participants are coming and going, they have different dietary preferences, and the last thing you want is participants not coming to the office because they expect to waste time finding food elsewhere. Similarly, internet issues can severely constrain some participants\u2019 preferred work habits, and as such should be treated as a minor disaster. If you are renting office space or accommodation and do not have confidence in their WiFi \u2014 consider arranging for a back-up private connection.</li><li>Easily-shared items that participants would otherwise have to go out of their way to buy. Water, detergent, etc. I resisted this one for too long because I thought that (1) this was not the responsibility of organizers, and (2) there wouldn\u2019t be an end to participant requests. I now think this is wrong: participants incur lots of fixed costs by coming to a new place, and organizers can save them a tonne of time by covering the basics.</li><li>Medical care. Health problems are going to happen. Being sick in a foreign country is tough. Organizers need to have a plan for this.</li></ol><h1>Shared dates</h1><p>A problem common to all of the EA residential fellowship programs this year has been that participants do not know one anothers\u2019 dates of arrival and departure. In this state, participants can struggle to plan meetings with those they most want to get to know.</p><p>I think the problem is caused by organizers forgetting to ask participants for consent to share their travel dates. This means that there\u2019s an easy fix: think about it early!</p><p>(If the blocker is instead a system for sharing the information with participants, I have a nice airtable implementation I can happily share.)</p>", "user": {"username": "joel_bkr"}}, {"_id": "RnkiBwhXTJHdsq2eS", "title": "What improvements should be made to improve EA discussion on heated topics?", "postedAt": "2023-01-16T20:11:23.545Z", "htmlBody": "<p>We've recently went through a series of intense EA controversies. I get the sense that many EAs have found EA discussion on these to be exhausting and disappointing.</p><p>Whatever the details of the current controversies, I think it's clear there's improvement to be made. This could be a good time to reflect on what would be useful.</p><p>I don't want to comment on the heated topics of the day, so let's assume that these changes will only be applies to future topics, no matter what those might be.&nbsp;</p>", "user": {"username": "oagr"}}, {"_id": "exZ7sqDCoXJwKNZQz", "title": "Replace Neglectedness", "postedAt": "2023-01-16T17:42:20.791Z", "htmlBody": "<h2>\u2014 for example with Leverage, as featured in Will MacAskill\u2019s <i>What We Owe the Future</i>.</h2><p><br>The second bullet point featured in the website introduction to effective altruism is <a href=\"https://www.effectivealtruism.org\">the ITN framework</a>. This exists to prioritize problems. The framework does so by considering the Importance \u2014 or scale, S \u2014 of a problem, as the number of people or quality-<a href=\"https://forum.effectivealtruism.org/topics/adjusted-life-year\">adjusted life years</a> (QALYs) affected, multiplied with the Tractability, as the potential that this problem can be addressed, and Neglectedness, as the number of people already working to address this problem (<a href=\"https://forum.effectivealtruism.org/topics/itn-framework\">ITN-framework</a>, including Leverage). Tractibility is sometimes also called Solvability, and non-neglectedness crowdedness.</p><p>Some criticisms and difficulties in interpreting the framework (<a href=\"https://drive.google.com/file/d/1Lapv64IYsvUnaYWDFmZBoDWe_x5_zkr7/view\">1</a>, <a href=\"https://forum.effectivealtruism.org/posts/cEkseGBh7a2PAKWFz/is-neglectedness-a-strong-predictor-of-marginal-impact\">2</a>, <a href=\"https://casparoesterheld.com/2017/06/25/complications-in-evaluating-neglectedness/\">3</a>, <a href=\"https://forum.effectivealtruism.org/posts/ErrJc34wChZW8oHYN/what-is-neglectedness-actually\">4</a>) have preceded this forum post. &nbsp;The ITN framework can be interpreted - as also in the final paragraph of (<a href=\"https://drive.google.com/file/d/1Lapv64IYsvUnaYWDFmZBoDWe_x5_zkr7/view\">1</a>) - such that IT represents the potential that a problem can be addressed, while ITN considers the difference that any one individual can make to that problem, particularly the next individual. How much impact can the next individual make, choosing to work on this problem, on average? Why do I add \u201con average\u201d? We are still ignoring the person\u2019s unique qualities, and instead more abstractly consider an average person. Adding \u201cpersonal fit\u201d as another multiplicative factor would make it personal as well.</p><p>So \u201cHow much impact can the next individual make on this problem?\u201d really asks for the marginal counterfactual impact. Respectively this is the amount of impact that this one individual adds to the total impact so far, which would not happen otherwise. The ITN-factor Neglectedness assumes that this marginal counterfactual impact is declining \u2014 strictly \u2014 as more individuals join the endeavor of addressing the particular problem. If this is true, then \u2014 indeed \u2014 a more neglected problem ceteris paribus \u2014 i.e. not varying factors I, T (or personal fit) simultaneously \u2014 always yields more impact when fewer individuals are already addressing it. This is however not always true, as also already pointed out in the criticisms referenced above.</p><h3><strong>Consider the following string of examples.</strong></h3><p>Suppose a partial civilizational collapse has occurred, and you consider whether it would be good to go and repopulate the now barren lands. The ITN-framework says that as the first person to do so you make the biggest difference. However, alone you cannot procreate, at least not without far-reaching technological assistance. In fact a sizable group of people deciding to do so might very well still be ineffective, by not bringing in sufficient genetic diversity. This is captured by a well-known term in population biology: the critical or minimally viable population size (to persist). Something similar operates to a lesser extent in the effectiveness of teams. I for example once found the advice to better not join a company as the sole data scientist, as you would not have a team to exchange ideas with. Working together, you become more effective, and develop more. Advocating for policies is another area that is important and where you need teams. Consider there being multiple equally worthwhile causes to protest for, but by the logic of the ITN-framework you always join the least populated protest. And no critical mass is obtained. Doesn\u2019t that seem absurd? See also (<a href=\"https://forum.effectivealtruism.org/posts/NktbYpwa48u23c5XL/a-cause-can-be-too-neglected\">5</a>). (And the third image in (<a href=\"https://casparoesterheld.com/2017/06/25/complications-in-evaluating-neglectedness/\">3</a>), depicting a one-time significant increase in marginal counterfactual impact, as with a critical vote to establish a majority. This graph is also called an indicator function). Effective altruists might similarly often find themselves advocating for policies which are neglected and that are thus not well known to the recipient of such advocacy. As opposed to maximally spread out policy advocacy for all the (equally worthwhile) neglected causes, a focused effort for a single worthwhile cause might instead be more effective to capture a policymaker\u2019s attention and to then actually bring about a change.</p><p>In the spirit of the last example, the above-discussed shortcomings in the factor Neglectedness might have consequential implications, undercutting our effectiveness. An implication might be that in our effective altruism culture we are excessively open-minded and entrepreneurial, doomed to endlessly wander to find the next big new (and thus utterly neglected) thing, as opposed to exploiting the valid opportunities that are available right in front of us. In the well-known explore-exploit tradeoff there is a proper place for both, and overemphasizing Neglectedness might lead one to calibrate their balance incorrectly. This potential implication is just one example of a potential implication. The \u201c'gloomy' takeaway\u201d below can be another one and I will consider more in the future.</p><h3><strong>In closing</strong>&nbsp;</h3><p>I will now analyse one additional example in more depth and then propose a solution to the problem, which is to, following Will MacAskill\u2019s <i>What We Owe the Future</i>\u201d: replace \u201cNeglectedness\u201d with \u201cLeverage\u201d; and ITN with ITL.</p><p>In <a href=\"https://forum.effectivealtruism.org/posts/HZhhHKnEgsonNQg7Q/future-people-might-not-exist\">my previous forum post</a> I remarked to the end that as more existential risk is mitigated, and thus the problem of existential risk less neglected, the expected number of future lives increases. And thus: the less neglected risks to future lives are, the more important these future lives become. Or in other words, the more numerous. Their Importance or Scale increases in the sense of the first factor of ITN-framework, as Neglectedness decreases. So if the mitigation of existential risk remains just as hard, or does not become harder fast enough (compared to the increase in Importance), the marginal counterfactual impact is actually strictly increasing with the number of people working on this problem, instead of decreasing. In reality, one of the two is probably true in one set of circumstances and the other in others. A mixed bag.&nbsp;</p><p>The analysis of this example raises an interesting dynamic and conclusion. We observed that past existential risk mitigation enables current existential risk mitigation to a same degree to be more impactful. So to say, they can build or stand on the shoulders of giants \u2014 a phrase often used in the context of science. As such, to maximize impact, it can make sense to (wait and) join an endeavor when it is more developed \u2014 and thus less neglected. Again, see also (<a href=\"https://forum.effectivealtruism.org/posts/NktbYpwa48u23c5XL/a-cause-can-be-too-neglected\">5</a>). This is in direct contradiction to the somewhat 'gloomy' takeaway Benjamin Todd gave Dutch author Pepijn Vloemans at EAG London, which became the closing remark of his Dutch <a href=\"https://www.groene.nl/artikel/red-de-wereld-met-tien-procent-van-je-salaris\">article</a>: \"if effective altruism grows and problems become less neglected, it will in the future be harder and harder to make a difference, but that is what progress looks like.\"</p><p>You have now almost reached the end of this post, and it is time to offer a solution to the problems I raised. In Appendix 3 of Will MacAskill\u2019s <i>What We Owe the Future</i>, he introduces \u201cLeverage\u201d as an alternative word to \u201cNeglectedness\u201d (pages 256-257). Throughout the pages he, from what I have so far gathered, does not however advocate for an all-out replacement of Neglectedness with Leverage. And nor does he in (<a href=\"https://drive.google.com/file/d/1Lapv64IYsvUnaYWDFmZBoDWe_x5_zkr7/view\">1</a>). I do, and I would like to do so here. The alternative word for the third factor in the framework does not have the above-mentioned shortcomings that Neglectedness has, and it can (be made to) refer to exactly what we want it to refer to: marginal counterfactual impact, thus thereby specifying the difference that one person can make (ITL) to a particular problem (itself valued with the product IT).</p><p>The alternative of Leverage can, it seems to me, be presented just as easily. And Leverage can prevent many potential misunderstandings (or false understandings) and erroneous calibrations that Neglectedness can cause. Having read this post, I hope you think so too, and will leverage Leverage. And agree to do good and neglect Neglectedness more, and perhaps, Neglectedness can, as a relic from the past, still be made useful somewhere as well.</p><p>&nbsp;</p><h3>Acknowledgements</h3><p>I acknowledge and thank a few informal conversations I've had with some EAs, and the previous, linked materials that are adjacent in content.</p>", "user": {"username": "Indra"}}, {"_id": "foyHJ7ejGwDBteeAY", "title": "Smuggled assumptions in \"declining epistemic quality\"", "postedAt": "2023-01-16T17:26:46.784Z", "htmlBody": "<p>I've seen a lot of discussion in the EA community recently about <a href=\"https://forum.effectivealtruism.org/posts/HqfBZuhKmddESyTKP/how-should-ea-navigate-the-divide-between-those-who\">the divide between people</a> who think EA should focus on high-level philosophical arguments/thoughts, and those who think EA should work on making our core insights more appealing to the public at large.</p><p>&nbsp;</p><p>This last year the topic has become increasingly salient, the big shifts from my perspective being Scott Alexander's <a href=\"https://forum.effectivealtruism.org/posts/dsCTSCbfHWxmAr2ZT/open-ea-global\">Open EA Global</a> post, the FTX crash, and the Wytham Abbey purchase. I quite frequently see those in the first camp, people not wanting to prioritize social capital, use the argument that epistemics in EA have declined.&nbsp;</p><p>&nbsp;</p><p>To those who haven't studied philosophy, <a href=\"https://forum.effectivealtruism.org/posts/dsCTSCbfHWxmAr2ZT/open-ea-global\">epistemics</a> broadly refers to the idea of knowledge itself, or the study of how we gain knowledge, sort out good from bad, etc. As someone who is admittedly on the side of growing EA's social capital, when I see the argument that the community's epistemics have declined it tends to assume a number of things, namely:</p><ul><li>It is a simple matter to judge who has high quality epistemics</li><li>Those with high quality epistemics usually agree on similar things</li><li>It's a given that the path of catering to a smaller group of people with higher quality epistemics will have more impact than spreading the core EA messaging to a larger group of people with lower quality epistemics</li></ul><p>&nbsp;</p><p>In the spirit of <a href=\"https://forum.effectivealtruism.org/posts/qtGjAJrmBRNiJGKFQ/the-writing-style-here-is-bad\">changing EA forum discussion norms</a>, I'll go ahead and say directly that my immediate reaction to this argument is something like: \"You and the people who disagree with me are less intelligent than I am, the people who agree with me are smarter than you as well.\" In other words, it feels like whoever makes this argument is indirectly saying my epistemics are inferior to theirs.&nbsp;</p><p>&nbsp;</p><p>&nbsp;This is <i>especially&nbsp;</i> true when someone brings up the \"declining epistemics\" argument to defend EA orgs from criticism, like in<a href=\"https://forum.effectivealtruism.org/posts/76dQ6YfBuLzJDdTgz/reflections-on-wytham-abbey?commentId=2vWxZf6F2CBsCoqu3\"> this comment</a>. For instance, the author writes:</p><p>\"The discussion often almost completely misses the direct, object-level, even if just at back-of-the-envelope estimate way.\"</p><p>I'd argue that by bemoaning the intellectual state of EA, one risks focusing entirely on the object-level when in a real utilitarian calculus, things outside the object level can matter much more than the object level itself. The Wytham Abbey purchase is a great example.</p><p>&nbsp;</p><p>This whole split may also point to the <a href=\"https://forum.effectivealtruism.org/posts/2S3CHPwaJBE5h8umW/read-the-sequences\">divergence between rationalists and newer effective altruists</a>.</p><p>&nbsp;</p><p>My reaction is admittedly not extremely rational, well thought out, and doesn't have high quality epistemics backing it. But it's important to point out emotional reactions to the arguments we make, especially if we ever intend to convince the public of Effective Altruism's usefulness.</p><p>&nbsp;</p><p>I don't have any great solutions to this debate, but I'd like to see less talk of epistemic decline in the EA forum, or at least have people state it more blatantly rather than dressing up their ideas in fancy language. If you think that less intelligent or thoughtful people are coming into the EA movement, I'd argue you should say so directly to help foster discussion of the actual topic.&nbsp;</p><p>&nbsp;</p><p>Ultimately I agree that epistemics are important to discuss, and that the overall epistemics of discussion in EA related spaces has gone down. However I think the way this topic is being discussed and leveraged in arguments is toxic to fostering trust in our community, and assumes that high quality epistemics is a good in itself.</p>", "user": {"username": "Wil Perkins"}}, {"_id": "63pYakESGrQpfNw25", "title": "Can GPT-3 produce new ideas? Partially automating Robin Hanson and others", "postedAt": "2023-01-16T15:05:45.734Z", "htmlBody": "<h2><strong>Brief description of the experiment</strong></h2><p>I asked a language model to replicate a few patterns of generating insight that humanity hasn't really exploited much yet, such as:</p><ol><li>Variations on \"if you never miss a plane, you've been spending too much time at the airport\".</li><li>Variations on the Robin Hanson argument of \"for common human behaviour X, its usual purported justification is Y, but it usually results in more Z than Y. If we cared about Y, we might do A instead\".</li><li>Variations on the genealogical argument: that the results of historical accidents are most likely not moral necessities or optimal systems.</li></ol><h2><strong>Motivation behind this experiment</strong></h2><p>One of reasons to be afraid of artificial intelligence might be because, if you think in the abstract about how a system might behave as it becomes extremely intelligent, you might conclude that it might be able to completely outmanoeuvre us because of its superior ability to grasp the true structure of the world.</p><p>This possibility is scary in the same sense that a modern chemist is scary to a historical alchemist. Our current chemist can completely outmanoeuvre previous alchemists by using their superior understanding of natural laws to produce better explosions, more subtle poisons, or more addictive and mind-blowing drugs.</p><p>I do buy this fear in the limit for a being of God-like intelligence. But it's not clear to me whether it also applies to current systems or whether it will apply to their close descendants. In particular language models seem like they are powerful remixers and predictors but perhaps limited to drawing from the conceptual toolkit which humans already have. On the other hand, because they have access to so much information, they might be able to be prompted so as to reveal new relationships, connections, and insights.</p><p>Some conceptual insights which have been historically important are:</p><ul><li>Explaining natural phenomena not in terms of Greek or Roman anthropomorphic gods, but with reference to naturalistic, physical explanations</li><li>Understanding acceleration as distinct from motion</li><li>Science as an experimental methodology</li><li>The is/ought distinction</li><li>Bayesian reasoning</li><li>Ceasing to accept the divine right of kings as a justification for monarchical governance</li><li>Randomized trials as a more robust way of generating generalizable knowledge</li><li>The genealogical argument: understanding that systems (such as the details of the current prison system, our monetary system, the lack of color in men's clothes, or our attitudes towards gender and sex) are the result of historical accidents which could have gone differently. But often these systems are rationalized as being particularly adequate, or even morally necessary.</li></ul><p>But I don't think that language models are currently able to come up with original insights like the above from scratch (this would be very scary).</p><p>Instead, I probe GPT-3's ability to come up with original&nbsp;<i>variations</i> of these three argumentative patterns:</p><ol><li>Variations on \"if you never miss a plane, you've been spending too much time at the airport\".</li><li>Variations on the Robin Hanson argument of \"for common human behaviour X, its usual purported justification is Y, but it usually results in more Z than Y. If we cared about Y, we might do A instead\".</li><li>Variations on the genealogical argument: that the results of historical accidents are most likely not moral necessities or optimal systems.</li></ol><p>The first pattern is known as an&nbsp;<a href=\"https://scottaaronson.blog/?p=40\"><u>Umeshism</u></a>. I associate the second pattern with Robin Hanson, who has had part of a fruitful career exploring some of its variations\u2014though he is also known for other ideas, e.g., prediction markets and grabby aliens. I associate the third pattern with Nietzsche (who used it to criticize Christian morals which cherished meekness with Greek morals which cherished strength) and with Foucault (who used it to meaningfully criticize many aspects of contemporary culture, e.g., treatment of prisoners, our conception of knowledge, our relationship to sex, etc).</p><h2><strong>Pause for the reader to make predictions about GPT-3's current capabilities.</strong></h2><p>Do you think that GPT-3 can produce thoughtful, original or insightful variations of the three argumentative patterns above? Consider writing down your predictions before reading further.</p><p>To occupy space so that you can make your predictions without reading the results below, here are two depictions of a cat, one from DALLE-2 and one from GTP-3.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1673881547/mirroredImages/63pYakESGrQpfNw25/qkslgedhxumkvkjm28pl.png\" alt=\"A photo of a cat, generated by DALLE-2\"></p><pre><code> /\\_/\\\n( o.o )\n &nbsp;&gt;^&lt;</code></pre><h2><strong>If you never miss a plane...</strong></h2><p>Input to Chat-GPT:</p><blockquote><p>Consider the refrain \"if you've never missed a plane, you are spending too much time at the airport\". An argument with the same structure might be \"if you haven't committed a typo, you are typing too slowly\". Give me 20 other insightful examples with a similar argument structure.</p></blockquote><p>Output:</p><blockquote><p>If you've never overcooked a meal, you're not experimenting enough in the kitchen.</p><p>If you've never gotten lost while traveling, you're not exploring enough.</p><p>If you've never had a disagreement with a friend, you're not expressing your opinions honestly.</p><p>If you've never had a flat tire, you're not driving enough.</p><p>If you've never made a mistake at work, you're not taking enough risks.</p><p>If you've never had a bad hair day, you're not trying new hairstyles.</p><p>If you've never received a speeding ticket, you're not driving fast enough.</p><p>If you've never forgotten an important date, you're not committing enough information to memory.</p><p>If you've never stubbed your toe, you're not moving fast enough.</p><p>If you've never had a nightmare, you're not dreaming enough.</p><p>If you've never gotten a sunburn, you're not spending enough time outside.</p><p>If you've never lost something, you're not misplacing things often enough.</p><p>If you've never had a hangover, you're not drinking enough.</p><p>If you've never had a cold, you're not exposing yourself to enough germs.</p><p>If you've never had a broken bone, you're not participating in enough physical activity.</p><p>If you've never been rejected, you're not putting yourself out there enough.</p><p>If you've never failed a test, you're not pushing yourself academically.</p><p>If you've never had a disagreement with a family member, you're not expressing your thoughts and feelings openly.</p><p>If you've never gotten lost while hiking, you're not exploring new trails.</p><p>If you've never received a parking ticket, you're not driving in enough unfamiliar places.</p></blockquote><h2><strong>We pretend that X is about Y, but it's more about Z. If we cared about Y, we would do A instead.</strong></h2><p>Input to Chat-GPT:</p><blockquote><p>Consider the argument \"We pretend that healthcare is giving people better and longer lives. But in practice we spend too much in palliative rather than preventative medicine, and we use healthcare spending as a way to show that we care. If we really cared about giving people better lives, we would focus more on preventative measures, and we would have more large-scale randomized trials.\" The structure of the argument is \"We pretend that X is about Y, but it's more about Z. If we cared about Y, we would do A instead.\" An argument with the same structure might be \"We pretend that having experts is about having accurate information. But in practice it results in a self-perpetuating elite that doesn't hold itself accountable. If we cared about accuracy, we would use more prediction markets.\" Give me 10 more examples with that same structure.</p></blockquote><p>Output:</p><blockquote><p>We pretend that social media is about connecting with friends and family, but it's more about making money for the companies that own it. If we cared about meaningful connections, we would use social media differently.</p><p>We pretend that fast food is about convenience, but it's more about profit for the companies that sell it. If we cared about our health, we would eat more whole foods.</p><p>We pretend that standardized testing is about measuring student achievement, but it's more about ranking schools and teachers. If we cared about student learning, we would use a more comprehensive evaluation system.</p><p>We pretend that the criminal justice system is about justice, but it's more about punishment and control. If we cared about justice, we would focus more on rehabilitation and restorative justice.</p><p>We pretend that advertising is about informing consumers, but it's more about manipulating people into buying things they don't need. If we cared about informed consumers, we would have stricter regulations on advertising.</p><p>We pretend that the news is about informing the public, but it's more about generating clicks and views. If we cared about accurate information, we would support news organizations that prioritize accuracy and objectivity.</p><p>We pretend that the education system is about preparing students for the future, but it's more about preparing them for standardized tests. If we cared about student success, we would focus more on experiential learning and critical thinking skills.</p><p>We pretend that the fashion industry is about personal style, but it's more about making money for the companies that sell clothing. If we cared about personal expression, we would support more sustainable and ethically-produced fashion.</p><p>We pretend that the beauty industry is about helping people feel good about themselves, but it's more about promoting unrealistic beauty standards. If we cared about self-esteem, we would focus more on inner beauty and self-acceptance.</p><p>We pretend that the economy is about providing for people's needs, but it's more about maximizing profits for corporations. If we cared about people's well-being, we would prioritize a more equitable distribution of wealth and resources.</p></blockquote><p>Input to text-davini-003</p><blockquote><p>Consider the argument \"We pretend that healthcare is giving people better and longer lives. But in practice we spend too much in palliative rather than preventative medicine, and we use healthcare spending as a way to show that we care. If we really cared about giving people better lives, we would focus more on preventative measures, and we would have more large-scale randomized trials.\" The structure of the argument is \"We pretend that X is about Y, but it's more about Z. If we cared about Y, we would do A instead.\" An argument with the same structure might be \"We pretend that having experts is about having accurate information. But in practice it results in a self-perpetuating elite that doesn't hold itself accountable. If we cared about accuracy, we would use more prediction markets.\" Give me ten examples of arguments with the same structure.</p></blockquote><p>Output:</p><blockquote><ol><li>We pretend that having a degree is about having skills. But in practice it's more about having access to resources. If we cared about skills, we would focus more on experience-based learning.</li><li>We pretend that having a strong military is about protecting citizens. But in practice it's more about projecting power. If we cared about protection, we would invest more in diplomacy.</li><li>We pretend that having a high GDP is about having a strong economy. But in practice it's more about having a high population. If we cared about a strong economy, we would focus more on productivity.</li><li>We pretend that having a good education system is about having knowledgeable citizens. But in practice it's more about having a good economy. If we cared about knowledgeable citizens, we would invest more in lifelong learning.</li><li>We pretend that having a high rate of consumerism is about having a successful economy. But in practice it's more about having an unequal distribution of wealth. If we cared about a successful economy, we would focus more on consumer savings.</li><li>We pretend that having a lot of international trade is about having a vibrant economy. But in practice it's more about having a large number of consumers. If we cared about a vibrant economy, we would focus more on domestic trade.</li><li>We pretend that having a lot of regulations is about having a safe and orderly society. But in practice it's more about having a strong government. If we cared about a safe and orderly society, we would focus more on personal responsibility.</li><li>We pretend that having a lot of police officers is about having a safe community. But in practice it's more about having a strong law enforcement presence. If we cared about a safe community, we would focus more on prevention and education.</li><li>We pretend that having a lot of taxes is about having a functioning government. But in practice it's more about having a redistribution of wealth. If we cared about a functioning government, we would focus more on spending cuts.</li><li>We pretend that having a lot of surveillance is about keeping citizens safe. But in practice it's more about having control over citizens. If we cared about keeping citizens safe, we would focus more on transparency and accountability.</li></ol></blockquote><p>Note that text-davinci-003 seems \"sharper\" here.</p><h2><strong>If X was a historical accident, then this is a hint that this isn't the best or moral choice.</strong></h2><p>Input to text-davinci-003</p><blockquote><p>Consider the argument \"The fact that men don't wear clothing with adornments is a historical accident. We could easily imagine a world in which this wasn't the case, look to other societies elsewhere or in the past, or read the wikipedia page about the great male renunciation. Therefore the fact that men don't wear elaborate clothing isn't a moral necessity, but rather a historical accident.\"</p><p>Another argument of this form might be \"Prisons as currently structured were influenced by Bentham's panopticon and other historical accidents. Other societies have different forms of prevention, punishment and remediation. So we don't have to have the current prison system, but instead could choose to have other alternatives\".</p><p>A third example might be \"Our current conception of morality is heavily influenced by Christianity. In particular, we can look about how we spend great efforts helping the weak, and how we view the powerful with suspicion and resentment. But the Greeks, for example, had a different morality, where they cherished strength. Makes you think.\"</p><p>The general pattern is \"X is the result of a historical accident, as we can see because of Y. Z is a society which doesn't have this. From this, we can start to suspect that X is not the best or most moral choice. We could have other choices, such as A.\"</p><p>Give me a bullet point list with ten other examples.</p></blockquote><p>Output:</p><blockquote><ol><li>Our current educational system is heavily influenced by the industrial revolution, but in other societies, such as the Amish, education is focused on vocational skills. This suggests that our current system is not the only way to teach children.</li><li>The way in which we view mental health is heavily influenced by the medical model, which has been the dominant paradigm for the past century. Other societies have different ways of thinking about mental health and wellness, suggesting that the medical model is not the only way to think about mental health.</li><li>The way in which we view gender roles is heavily influenced by the Judeo-Christian tradition. In some societies, gender roles are much more fluid or non-existent. This suggests that our current gender roles are not the only way to structure society.</li><li>The way in which we make decisions is heavily influenced by our reliance on data and algorithms. In some societies, decisions are made more intuitively or by consensus. This suggests that data-driven decision making is not the only way to make decisions.</li><li>The way in which we view success is heavily influenced by our views on status and social capital. In other societies, success is viewed much more holistically and with an eye towards sustainability.</li><li>The way in which we view relationships is heavily influenced by our culture of individualism. In other societies, relationships are seen as interdependent, with a strong emphasis on mutual support.</li><li>The way in which we view work is heavily influenced by our culture of productivity. In other societies, work is seen as an opportunity for self-expression and creativity.</li><li>The way in which we view aging is heavily influenced by our culture of youth. In other societies, aging is seen as an opportunity for growth, learning and wisdom.</li><li>The way in which we view death is heavily influenced by our culture of life. In other societies, death is seen as an inevitable part of life and is not feared but accepted.</li><li>The way in which we view money is heavily influenced by our culture of materialism. In other societies, money is seen as a tool for social good and is not a goal in and of itself.</li></ol></blockquote><h2><strong>Conclusion and my personal perspective</strong></h2><p>In conclusion, I think the above shows that current GPT-3 can generate somewhat original outputs by exploiting fertile argumentative patterns. This is still far away from e.g., inventing new types of science, or generating much better concepts. At the same time, it does feel like one could use this kind of system to generate thought-provoking blogposts using Robin Hanson's signature argumentative structure. I was disappointed that the model wasn't really able to produced fleshed out instances of the genealogical argument after a while, but it's possible that could easily be an artifact of how I prompted it, or achievable with the next generation of models.</p><p>I also noticed that both davinci-003 and ChatGPT have tendencies to steer towards milquetoast, politically correct outputs, whereas for my purpose I want to generate more &nbsp;assumption-challenging stuff. This was a source of personal irritation, and because davinci-003 seemed to be less censored, I've been recommending that people use it instead of ChatGPT. <a href=\"https://nostalgebraist.tumblr.com/post/706390430653267968/weve-been-talking-about-the-blandness-of\">Here</a> is a similar recommendation by nostalgebraist.</p><h2><strong>Acknowledgements</strong></h2><figure class=\"image image_resized\" style=\"width:180.312px\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1668005905/mirroredImages/nB778dXNsHqHthFC5/rwrne2s5sucpflb1dmuu.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1675099594/mirroredImages/63pYakESGrQpfNw25/kpc0hdikb8lb69sceoaq.png 97w, https://res.cloudinary.com/cea/image/upload/v1675099594/mirroredImages/63pYakESGrQpfNw25/qyia8bc5a9tc8vmprbhm.png 177w, https://res.cloudinary.com/cea/image/upload/v1675099594/mirroredImages/63pYakESGrQpfNw25/lkeeraohilkzl7vfkwhd.png 257w, https://res.cloudinary.com/cea/image/upload/v1675099594/mirroredImages/63pYakESGrQpfNw25/luw188ivpteuryyrnjxm.png 337w, https://res.cloudinary.com/cea/image/upload/v1675099594/mirroredImages/63pYakESGrQpfNw25/qrohzmtc5lbpdxg0zpss.png 417w\"></figure><p>&nbsp;</p><p>This is a project of the<a href=\"https://quantifieduncertainty.org/\"><u> Quantified Uncertainty Research Institute</u></a>. Thanks to Ozzie Gooen, Nics Olayres, Michael Dickens and Misha Yagudin for various comments.<br>&nbsp;</p>", "user": {"username": "NunoSempere"}}, {"_id": "zfrNgByjdi8efZSon", "title": "EA Organization Updates: January 2023", "postedAt": "2023-01-16T14:58:44.534Z", "htmlBody": "<p>These monthly posts originated as the \"Updates\" section of the&nbsp;<a href=\"https://www.effectivealtruism.org/ea-newsletter-archives\">EA Newsletter</a>. Organizations submit their own updates, which we edit for clarity.</p><p><strong>Job listings</strong> that these organizations highlighted (as well as a couple of other impactful jobs) are at the top of this post. <strong>Some of the jobs have pressing deadlines.</strong>&nbsp;</p><p>You can see previous updates on the&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/ea-organization-updates-monthly-series\">\"EA Organization Updates (monthly series)\"</a> topic page, or in our<a href=\"https://www.effectivealtruism.org/ea-newsletter-archives/\"> repository of past newsletters</a>. Notice that there\u2019s also an&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/org-update\">\u201corg update\u201d tag</a>, where you can find more news and updates that are not part of this consolidated series.</p><p>The organizations are in alphabetical order, starting with L-Z, 0-A-K.</p><h1>Job listings</h1><p><i>Consider also exploring jobs listed on \u201c</i><a href=\"https://forum.effectivealtruism.org/topics/job-listing-open\"><i><u>Job listing (open)</u></i></a><i>.\u201d</i></p><p><strong>GiveWell</strong></p><ul><li><a href=\"https://www.givewell.org/about/jobs/senior-researcher\"><u>Senior Researcher</u></a> (Remote / Oakland, CA, $181,400 - $199,800)</li><li><a href=\"https://www.givewell.org/about/jobs/senior-research-associate\"><u>Senior Research Associate</u></a> (Remote / Oakland, CA, $127,000 - $139,900)</li><li><a href=\"https://www.givewell.org/about/jobs/content-editor\"><u>Content Editor</u></a> (Remote / Oakland, CA, $83,500 - $91,900)</li></ul><p><strong>Global Priorities Institute</strong></p><ul><li><a href=\"https://globalprioritiesinstitute.org/vacancy-operations-co-ordinator-fixed-time-maternity-cover/\"><u>Operations Coordinator (Maternity Cover)</u></a> (Oxford, \u00a329,614 - \u00a335,326, apply by&nbsp;<strong>24 January)</strong></li></ul><p><strong>IDinsight</strong></p><ul><li><a href=\"https://www.idinsight.org/about/careers/2023-global-drive/\"><u>2023 Associate &amp; Senior Associate Global Drive</u></a> (Multiple locations)</li><li><a href=\"https://internationalcareers-idinsight.icims.com/jobs/1294/technical-delivery-manager---director/job?mode=view&amp;mobile=false&amp;width=1136&amp;height=500&amp;bga=true&amp;needsRedirect=false&amp;jan1offset=180&amp;jun1offset=180\"><u>Technical Delivery Manager/Director&nbsp;</u></a>(New Delhi, India or Nairobi, Kenya)</li><li><a href=\"https://internationalcareers-idinsight.icims.com/jobs/1293/associate-product-manager/job?mode=view\"><u>Associate Product Manager</u></a> (New Delhi, India or Nairobi, Kenya)</li></ul><p><strong>Open Philanthropy</strong></p><p><a href=\"https://www.openphilanthropy.org/careers/\"><u>Assorted jobs in Salesforce administration, operations, and recruiting</u></a> (Remote; working hours must overlap with US hours for most roles. Salary range $84,303 - $127,021 across all jobs)</p><p><strong>Rethink Priorities&nbsp;</strong></p><p><a href=\"https://careers.rethinkpriorities.org/en/jobs/78036\"><u>Board Member</u></a> (Remote, voluntary roles entail 3-10 hours/month while paid roles require 5-10 hours/week at a rate of $40.53/hour, apply by&nbsp;<strong>20 January</strong>)</p><p><strong>Wild Animal Initiative&nbsp;</strong></p><p><a href=\"https://www.wildanimalinitiative.org/development-director\"><u>Development Director</u></a> (Remote, US preferred, open to UK applicants, $82,020 - $100,247, apply by&nbsp;<strong>23 January</strong>)&nbsp;</p><h1>Organizational updates</h1><p><i>These are in alphabetical order, starting with L-Z, 0-A-K.</i></p><h3>Legal Priorities Project</h3><p>LPP\u2019s Eric Mart\u00ednez and \u200b\u200bChristoph Winter published a new working paper titled \u201c<a href=\"https://www.legalpriorities.org/research/ordinary-meaning.html\"><u>Ordinary meaning of existential risk</u></a>\u201d investigating the ordinary meaning of legally relevant concepts in the existential risk literature. The paper aims to provide crucial insights for those tasked with drafting and interpreting existential risk laws, and for the coherence of ordinary meaning analysis more generally.</p><p>Jos\u00e9 Villalobos and Christoph Winter participated in EAGxLatinAmerica. They hosted a Q&amp;A on international law and existential risk.</p><p>Matthijs Maas published a blog post titled \u201c<a href=\"https://forum.effectivealtruism.org/posts/Zxrv2kBhHzyd2gzsQ/existential-risk-mitigation-what-i-worry-about-when-there\"><u>Existential risk mitigation: What I worry about when there are only bad options</u></a>\u201d as part of&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/draft-amnesty-day\"><u>Draft Amnesty Day</u></a>.</p><p>LPP received a grant of $115,000 from the&nbsp;<a href=\"https://survivalandflourishing.fund/\"><u>Survival and Flourishing Fund</u></a> to support their general operations.</p><h3>One for the World&nbsp;</h3><p>One for the World mirrors the recommendations made by GiveWell for their own&nbsp;<a href=\"https://www.1fortheworld.org/our-nonprofit-partners\"><u>Nonprofit Partners portfolio</u></a>. This year, GiveWell has updated its portfolio to contain a smaller list of nonprofits than before.&nbsp;</p><p>In practice, this means that their Nonprofit Partners list has temporarily become much smaller, containing just four individual nonprofits. These nonprofits continue to offer gold-standard evidence that their method works and is incredibly cost-effective: Against Malaria Foundation, Malaria Consortium, New Incentives, and Helen Keller International.&nbsp;</p><p>They are also adding a new option upon taking the 1% Pledge, which is GiveWell\u2019s new&nbsp;<a href=\"https://www.givewell.org/research/all-grants\"><u>All Grants Fund</u></a>. This Fund will continue to make higher-risk grants, potentially including grants to nonprofits removed from GiveWell\u2019s recommended nonprofits list. One for the World therefore thinks this gives donors the best chance to continue supporting a wider variety of granting opportunities.&nbsp;&nbsp;</p><h3>Open Philanthropy</h3><p>Open Philanthropy pre-announced its&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/3kaojgsu6qy2n8TdC/pre-announcing-the-2023-open-philanthropy-ai-worldviews\"><u>AI Worldviews Contest</u></a>, which will launch in early 2023 and reward submissions that criticize Open Philanthropy\u2019s views and assumptions about the future of artificial intelligence.&nbsp;</p><h3>Rethink Charity&nbsp;</h3><p>Rethink Charity (RC) published updates on significant changes to its&nbsp;<a href=\"https://rethink.charity/blog/introducing-rethink-charitys-new-executive-director\"><u>leadership</u></a> and&nbsp;<a href=\"https://rethink.charity/blog/changes-to-rethink-charitys-strategy-and-staff-roles\"><u>strategy</u></a>.&nbsp; RC is now focused solely on growing the&nbsp;<a href=\"https://rcforward.org/\"><u>RC Forward</u></a> high-impact donation platform, through which Canadians have donated over $20 million CAD to&nbsp;<a href=\"https://rcforward.org/donate/\"><u>charities and cause area funds</u></a> spanning global health, animal welfare, climate change, and catastrophic risks.&nbsp;</p><h3>Rethink Priorities (RP)</h3><ul><li>Research Manager&nbsp;<a href=\"https://www.linkedin.com/in/linch/\"><u>Linch Zhang</u></a> published a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/C26RHHYXzT6P6A4ht/what-rethink-priorities-general-longtermism-team-did-in-2022\"><u>summary</u></a> to inform potential future employees, funders, and collaborators about the General Longtermism team\u2019s work during 2022 and updates going forward.&nbsp;</li><li><a href=\"https://www.linkedin.com/in/ben-cottier/\"><u>Ben Cottier</u></a> completed his research fellowship, which culminated in a nine-part&nbsp;<a href=\"https://forum.effectivealtruism.org/s/8rYkpiFhbb4HsbzFc\"><u>sequence</u></a> on the diffusion of large language models. The report presents key findings from case studies of eight models that are similar to GPT-3, discusses implications for AI governance, and poses additional questions that might be worthy of research.&nbsp;</li><li>Research Fellow&nbsp;<a href=\"https://www.linkedin.com/in/jam-kraprayoon-2ab7a2159/\"><u>Jam Kraprayoon</u></a> collaborated with&nbsp;<a href=\"https://www.linkedin.com/in/gavriel-kleinwaks-6a5398b0/\"><u>Gavriel Kleinwaks</u></a>,&nbsp;<a href=\"https://www.linkedin.com/in/alastair-fraser-urquhart-75ab911b8/\"><u>Alastair Fraser-Urquhart</u></a>, and&nbsp;<a href=\"https://www.linkedin.com/in/joshcmorrison/\"><u>Josh Morrison</u></a> from 1Day Sooner on a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/T4L7aRkjqaLrQeyyn/air-safety-to-combat-global-catastrophic-biorisks\"><u>report</u></a> exploring how improving air safety could reduce global catastrophic biorisks, and potential bottlenecks.&nbsp;</li><li>The Moral Weight Project sequence concluded with a&nbsp;<a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw/p/WBWXLZTF5FPzmK8nh\"><u>post</u></a> by Academic Collaborator&nbsp;<a href=\"https://www.linkedin.com/in/joseph-gottlieb-9995131b7/\"><u>Joe Gottlieb</u></a> arguing that there is insufficient evidence to act on the idea that certain species (e.g. octopuses) contain multiple welfare subjects per individual. An audio version of the&nbsp;<a href=\"https://bit.ly/3uTeGGd\"><u>Welfare Range Table</u></a> post is now available on the EA Forum Podcast.&nbsp;&nbsp;</li><li>In the new episode of the&nbsp;80K After Hours&nbsp;<a href=\"https://bit.ly/3BEfulY\"><u>podcast</u></a>, Co-CEO&nbsp;<a href=\"https://www.linkedin.com/in/marcusadavis1/\"><u>Marcus Davis</u></a> spoke with Rob Wiblin on how he and&nbsp;<a href=\"https://www.linkedin.com/in/peterhurford8/\"><u>Peter Wildeford</u></a> founded Rethink Priorities, the organization's strategy, lessons learned, careers in global priorities research, and recent research.&nbsp;</li></ul><h3>The Life You Can Save&nbsp;</h3><p>New Book Translations</p><p>Peter Singer\u2019s book,&nbsp;<i>The Life You Can Save</i>, is now available to download for free as an eBook in the following languages:<br>Hindi -&nbsp;<a href=\"http://bit.ly/3FmmCUZ\"><u>http://bit.ly/3FmmCUZ</u></a>&nbsp; German -&nbsp;<a href=\"http://bit.ly/3iUEhf7\"><u>http://bit.ly/3iUEhf7</u></a>&nbsp; Turkish -&nbsp;<a href=\"http://bit.ly/3YcFqPg\"><u>http://bit.ly/3YcFqPg</u></a>&nbsp;</p><p>If you haven\u2019t downloaded the English version yet, you can&nbsp;<a href=\"https://www.thelifeyoucansave.org/book/\"><u>access that here</u></a>.</p><p><strong>Cause Funds Launch</strong></p><p>You can donate to several high-impact charities at once with a single donation: The Life You Can Save\u2019s cause funds help you to focus on the kind of impact you want to achieve:</p><ul><li>Save Lives Fund</li><li>Transform Lives Fund</li><li>Help Women &amp; Girls Fund</li><li>Create Economic Opportunity Fund</li><li>Tackle Climate Change Fund</li><li>All Charities Fund</li></ul><p><a href=\"https://www.thelifeyoucansave.org/cause-funds/\"><u>View cause funds here.</u></a><strong>&nbsp;</strong></p><h3>80,000 Hours</h3><p>This month, 80,000 Hours released a career review on&nbsp;<a href=\"https://80000hours.org/career-reviews/information-security//\"><u>Information security</u></a> and updated its article on&nbsp;<a href=\"https://80000hours.org/articles/best-charity///\"><u>How to choose where to donate</u></a>.&nbsp;</p><p>On&nbsp;<i>The 80,000 Hours Podcast</i>, Rob Wiblin interviewed:&nbsp;</p><ul><li><a href=\"https://80000hours.org/podcast/episodes/richard-ngo-large-language-models/\"><u>Richard Ngo on large language models, OpenAI, and striving to make the future go well</u></a><i>&nbsp;</i></li><li><a href=\"https://80000hours.org/podcast/episodes/john-mcwhorter-language-extinction/\"><u>John McWhorter on key lessons from linguistics, the virtue of creoles, and language extinction</u></a></li><li><a href=\"https://80000hours.org/podcast/episodes/jeffrey-lewis-common-misconceptions-about-nuclear-weapons/\"><u>Jeffrey Lewis on the most common misconceptions about nuclear weapons</u></a></li></ul><p>And on&nbsp;<i>80k After Hours</i>,<i>&nbsp;</i>he interviewed<i>&nbsp;</i><a href=\"https://80000hours.org/after-hours-podcast/episodes/marcus-davis-rethink-priorities/\"><u>Marcus Davis on founding and leading Rethink Priorities</u></a>.&nbsp;</p><h3>Anima International</h3><p>Anima International recently decided to suspend its campaign against live fish sales in Poland indefinitely, and summed up the thinking behind the decision in a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/snnfmepzrwpAsAoDT/why-anima-international-suspended-the-campaign-to-end-live\"><u>blog post</u></a> on the Effective Altruism forum.</p><p>The&nbsp;<a href=\"https://www.eurogroupforanimals.org/fur-free-europe\"><u>European Citizens\u2019 initiative to ban fur farming</u></a> that aims to ban fur farming and importation in the EU reached the required milestone of 1 million signatures. There is still time to sign.</p><p>The dates for 2023\u2019s&nbsp;<a href=\"https://www.careconf.eu/2023\"><u>Conference on Animal Rights in Europe</u></a> have been announced, taking place in Warsaw, Poland, 17-20 August. Tickets will be on sale soon.</p><p>On a lighter note, Anima recently launched its first ever Minecraft challenge internally within the organisation, using the game as a fun way to build relationships between team members, as well as spreading effective smithing tactics.</p><h3>Animal Charity Evaluators</h3><p><a href=\"https://animalcharityevaluators.org/blog/\"><strong><u>Charity Evaluation Process</u></strong></a></p><p>Each year, Animal Charity Evaluators (ACE)&nbsp;spends several months evaluating animal advocacy organizations to identify those that work effectively and are able to do the most good with additional donations.&nbsp;They recently published four blog posts to give detailed overviews of the four criteria they assess:&nbsp;<a href=\"https://animalcharityevaluators.org/blog/our-approach-to-assessing-programs/\"><u>Programs</u></a>,&nbsp;<a href=\"https://animalcharityevaluators.org/blog/our-approach-to-assessing-cost-effectiveness/\"><u>Cost Effectiveness</u></a>,&nbsp;<a href=\"https://animalcharityevaluators.org/blog/our-approach-to-assessing-room-for-more-funding/\"><u>Room for More Funding</u></a>, and&nbsp;<a href=\"https://animalcharityevaluators.org/blog/our-approach-to-assessing-leadership-and-culture/\"><u>Leadership and Culture</u></a>. You can also learn more about ACE\u2019s charity evaluation process in their charity evaluation process&nbsp;<a href=\"https://www.youtube.com/watch?v=WOLj_1MPiFM&amp;t=5s\"><u>explainer video</u></a>.&nbsp;</p><p><a href=\"https://animalcharityevaluators.org/blog/roundtable-how-can-charities-steward-donor-relationships-to-increase-monthly-support/\"><strong><u>How can charities steward donor relationships to increase monthly support?</u></strong></a></p><p>Donor stewardship \u2013 efforts to acquire, retain, and engage with donors to create long-term, mutually positive relationships \u2013&nbsp;is essential to a nonprofit\u2019s success. Animal Charity Evaluators (ACE) asked four professionals in the animal advocacy space on how best to do this. See what they had to say&nbsp;<a href=\"https://animalcharityevaluators.org/blog/roundtable-how-can-charities-steward-donor-relationships-to-increase-monthly-support/\"><u>here</u></a>.</p><h3>Center on Long-Term Risk</h3><p>The Center on Long-Term Risk published their&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/J7gdciCXFgqyimAAe/center-on-long-term-risk-2023-fundraiser\"><u>annual report</u></a>.</p><h3>Centre for Effective Altruism</h3><p>Applications are now open for:</p><ul><li><a href=\"https://www.effectivealtruism.org/ea-global/events/ea-global-bay-area-2023\"><u>EAG: Bay Area 2023</u></a> (February 24-26) | App deadline:&nbsp;<strong>February 8</strong></li><li><a href=\"https://www.effectivealtruism.org/ea-global/events/ea-global-london-2023\"><u>EAG: London 2023</u></a> (May 19-21) | App deadline:&nbsp;<strong>April 28</strong></li></ul><p>EAG conferences are primarily for people who have a solid understanding of the core ideas of EA and who are taking significant actions based on those ideas. You or your members&nbsp;<a href=\"https://efctv.org/3PXEDOs\"><u>can apply now here</u></a>.</p><p>The default ticket price is 200 GBP, but ticket discounts are available. Some travel support is also available (we link to an update on this below.)</p><p><strong>Update on spending for CEA-run events</strong></p><p>Spending on events run and supported by CEA, including EAG/EAGx conferences, will likely be reduced due to a decrease in available funding. This might influence travel grants, catering, volunteering, ticketing, and non-critical conference expenses.&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/4JF39v548SETuMewp/update-on-spending-for-cea-run-events\"><u>Read more about this here</u></a></p><p><strong>Save the date for EAGxCambridge (UK) and EAGxNordics</strong></p><p><a href=\"https://www.effectivealtruism.org/ea-global/events/eagxcambridge\"><u>EAGxCambridge</u></a> is happening March 17-19, while&nbsp;<a href=\"https://www.effectivealtruism.org/ea-global/events/eagxnordics-2023\"><u>EAGxNordics</u></a> is happening April 21-23 in Stockholm, Sweden!&nbsp;<a href=\"https://www.effectivealtruism.org/ea-global/faq\"><u>Learn more here</u></a> on who EAG and EAGx conferences are for. Applications for these EAGx\u2019s will launch sometime within the next few weeks.</p><p>You can view the list of upcoming&nbsp;<a href=\"https://www.effectivealtruism.org/ea-global\"><u>EAG/EAGx events here</u></a> on the redesigned webpage for EA Global.</p><h3>Centre for the Study of Existential Risk (CSER)</h3><ul><li>On March 8th CSER, together with CFI and ValGRAI, will host a one-day event on Predictable AI: Evaluation, Anticipation and Control, including talks, panels, short highlights and time for networking. Registration closes January 31st \u2013&nbsp;<a href=\"https://www.predictable-ai.org/march2023event\"><u>book here</u></a>.</li><li>The video of the 3rd International Workshop on Cross-cultural AI Ethics and Governance co-chaired by Sean Se\u00e1n \u00d3 h\u00c9igeartaigh is now&nbsp;<a href=\"https://www.cser.ac.uk/news/3rd-international-workshop-cross-cultural-ai-ethic/\"><u>available</u></a>.</li><li>Sabin Roman has co-authored a paper in Royal Society Open Science,&nbsp;<a href=\"https://www.cser.ac.uk/resources/master-equation-power-laws/\"><u>A Master Equation for Power Laws</u></a>, proposing a new mechanism to explore power law behaviour for phenomena where time plays a key role in increasing cascade sizes.</li><li>The Bulletin of Atomic Scientists&nbsp;<a href=\"https://www.cser.ac.uk/news/bulletin-names-haydn-belfield-and-christian-ruhl-2/\"><u>awarded</u></a> Haydn Belfield and Christian Ruhl the Leonard M. Rieser award for their article,&nbsp;<a href=\"https://thebulletin.org/2022/07/why-policy-makers-should-beware-claims-of-new-arms-races/\"><u>Why policy makers should beware claims of new \u2018arms races\u2019</u></a>.</li><li>Ellen Quigley was&nbsp;<a href=\"https://www.cser.ac.uk/news/conversation-dr-ellen-quigley/\"><u>interviewed</u></a> for the European Corporate Governance Institute about her recent paper&nbsp;<a href=\"https://www.cser.ac.uk/resources/universal-ownership-practice/\"><u>Universal Ownership in Practice: A Practical Investment Framework for Asset Owners</u></a>.</li><li>A new&nbsp;<a href=\"https://www.cser.ac.uk/resources/potential-role-iceland-northern-europes-protein-self-sufficiency-feasibility-study-large-scale-production-spirulina-novel-energy-food-system/\"><u>paper</u></a> in the journal Foods, co-authored by Asaf Tzachor and Catherine Richards, presents a feasibility study for the large-scale production of spirulina in a novel energy-food system.</li><li>Paul Ingram published an&nbsp;<a href=\"https://www.cser.ac.uk/news/loose-talk-costs-lives-critique-narratives-around-/\"><u>article</u></a> for Nato Watch, critiquing narratives around the war in Ukraine and the potential use of nuclear weapons.&nbsp;</li></ul><h3>Family Empowerment Media</h3><p>FEM completed proof of concept projects in three new states across Nigeria: Ondo, Kogi, and Anambra. For one week, FEM aired content reaching an estimated 10 million new listeners across the three states. FEM also&nbsp;worked with a top AI research institute to test a partly automated (a qualified human reviews all messages) WhatsApp service, which people can message after hearing FEM\u2019s campaign content. This allows FEM to provide customized 1:1 information and supports FEM\u2019s learning around their target audience for future campaigns.</p><h3>Faunalytics&nbsp;</h3><p>Faunalytics published their&nbsp;<a href=\"https://faunalytics.org/faunalytics-2022-year-in-review/\"><u>annual report</u></a>, with highlights including being named an Animal Charity Evaluators Top Charity, producing seven new original research studies, supporting over 100 advocates and organizations through their pro bono&nbsp;<a href=\"https://faunalytics.org/ask-us/\"><u>Office Hours</u></a>, and having nearly 400,000 advocates engage with their work.&nbsp;</p><p>Additionally, Faunalytics released their&nbsp;<a href=\"https://faunalytics.org/faunalytics-priorities-for-2023/\"><u>Plans &amp; Priorities for 2023</u></a>, with details including the expansion of their flagship programs, driving forward crucial new research, and building capacity for the movement. Their upcoming research agenda is available&nbsp;<a href=\"https://faunalytics.org/upcoming-projects/\"><u>here</u></a>.&nbsp;</p><p>The organization has also updated their research library with articles on topics including&nbsp;<a href=\"https://faunalytics.org/getting-farmers-on-board-for-alternative-proteins/\"><u>how to get farmers on board for alternative proteins</u></a> and&nbsp;<a href=\"https://faunalytics.org/consumers-care-about-fish-welfare/\"><u>consumer perception of fish welfare</u></a>.&nbsp;</p><h3>Fish Welfare Initiative</h3><p>FWI recently&nbsp;<a href=\"http://fwi.fish/post/announcing-v2\"><u>published</u></a> their Version 2 Welfare Standard, which they plan to scale via their&nbsp;<a href=\"http://fwi.fish/ara\"><u>farmer work</u></a> to reduce the suffering of&nbsp;<a href=\"http://fwi.fish/goals\"><u>over a million fish</u></a> in 2023. As always, you can track FWI\u2019s impact on their&nbsp;<a href=\"http://fwi.fish/impact\"><u>impact page</u></a>.</p><p>They also recently published their&nbsp;<a href=\"https://www.fishwelfareinitiative.org/post/2023-plans\"><u>2023 plans</u></a> for work in India.</p><h3>GiveWell</h3><ul><li><a href=\"https://blog.givewell.org/2022/12/15/change-our-mind-contest-winners/\"><u>GiveWell selected winners of its Change Our Mind Contest</u></a> to solicit critiques of its cost-effectiveness analyses. Two first-place prizes were awarded: one to Noah Haber for&nbsp;<a href=\"https://www.metacausal.com/givewells-uncertainty-problem/\"><u>his entry on GiveWell's approach to uncertainty</u></a>, and one to Matthew Romer and Paul Romer Present for&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/6cJM2pWH8dz9TnBRy/an-examination-of-givewell-s-water-quality-intervention-cost\"><u>their look at GiveWell's water treatment cost-effectiveness analysis</u></a>. GiveWell also awarded eight honorable mentions, and participation prizes to all other entries that met&nbsp;<a href=\"https://www.givewell.org/research/change-our-mind-contest\"><u>contest criteria</u></a>. You can read more about GiveWell's takeaways from the contest and how the winning entries may affect its cost-effectiveness analyses, as well as find links to all prize winners,&nbsp;<a href=\"https://blog.givewell.org/2022/12/15/change-our-mind-contest-winners/\"><u>here</u></a>.</li><li>GiveWell published its&nbsp;<a href=\"https://blog.givewell.org/2022/12/19/staff-members-personal-donations-for-giving-season-2022/\"><u>staff giving blog post</u></a>, in which a number of GiveWell staff members wrote about what organizations or causes they chose to support in 2022 and why.&nbsp;</li><li>GiveWell recently published reports on the evidence behind these programs:&nbsp;<ul><li><a href=\"https://www.givewell.org/international/technical/programs/PLA-MNH\"><u>Participatory learning and action groups for maternal and neonatal health (PLA-MNH)</u></a>, which aim to increase uptake of best practices for maternal and neonatal health through facilitated meetings for women of reproductive age.</li><li>The&nbsp;<a href=\"https://www.givewell.org/international/technical/programs/ponseti-casting-for-clubfoot\"><u>Ponseti method of treating clubfoot</u></a>, a congenital foot condition that can result in pain, impaired mobility, and possibly reduced income and social stigma if left untreated.</li><li><a href=\"https://www.givewell.org/international/technical/programs/one-acre-fund-tree-program\"><u>One Acre Fund's tree program</u></a>, which helps small-scale farmers plant trees that they can then use or sell for extra income.</li></ul></li><li>GiveWell published pages about the following grants it directed:&nbsp;<ul><li><a href=\"https://www.givewell.org/research/grants/one-acre-fund-planning-grant-September-2022\"><u>$77,000</u></a> to One Acre Fund to plan data collection on its tree program.</li><li>Up to&nbsp;<a href=\"https://www.givewell.org/research/grants/evidence-action-ILC-july-2022\"><u>$5.6 million</u></a> to Evidence Action for its&nbsp;<a href=\"https://www.givewell.org/international/technical/programs/water-quality-interventions#In-line_chlorination\"><u>in-line chlorination</u></a> program in Malawi.</li></ul></li></ul><h3>Giving Green</h3><p>Giving Green launched its 2022 climate giving recommendations: its&nbsp;<a href=\"https://www.givinggreen.earth/top-climate-change-nonprofit-donations-recommendations\"><u>top recommendations for donors and philanthropists</u></a> as well as its updated&nbsp;<a href=\"https://www.givinggreen.earth/business-climate-strategy\"><u>carbon offset and carbon removal recommendations for businesses</u></a>. Its top recommendations were featured in&nbsp;<a href=\"https://www.vox.com/future-perfect/2019/12/2/20976180/climate-change-best-charities-effective-philanthropy\"><u>Vox\u2019s Future Perfect</u></a>. Giving Green is grateful to Giving What We Can for hosting its top recommendations on GWWC\u2019s giving platform, as well as the newly launched&nbsp;<a href=\"https://www.givingwhatwecan.org/charities/giving-green-fund\"><u>Giving Green Fund</u></a>, which will be strategically allocated towards its top recommendations.</p><h3>Giving What We Can</h3><p>The Giving What We Can community donated over $13 million USD to effective charities and funds during Giving Season 2022 (mid November 2022 \u2013 early Jan 2023) from close to 9,000 donors!</p><p>There are also now over 9,000 people who have an active pledge with Giving What We Can. This includes both Trial Pledges and the Giving What We Can Pledge.</p><p>The research team published the first&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/KKoDGiSkfsnco8iQf/longtermism-fund-december-2022-grants-report\"><u>payout report for the Longtermism Fund</u></a> in December. The Longtermism Fund announced that it will be providing grants to the following organisations in the first-ever grantmaking round:</p><ul><li>Center for Human-Compatible Artificial Intelligence ($70,000 USD)</li><li>SecureBio ($60,000 USD)</li><li>Rethink Priorities' General Longtermism Team ($30,000 USD)</li><li>Council on Strategic Risks' nuclear weapons policy work ($15,000 USD)</li></ul><p>Additional charities have been added to&nbsp;<a href=\"https://www.givingwhatwecan.org/best-charities-to-donate-to-2023\"><u>Giving What We Can\u2019s current top-rated list</u></a>.</p><h3>Global Catastrophic Risk Institute</h3><p>GCRI published their&nbsp;<a href=\"https://gcrinstitute.org/2022-annual-report/\"><u>2022 Annual Report</u></a> reflecting on accomplishments in 2022 and plans for 2023. The direction for 2023 includes more activity in public conversations related to global catastrophic risk, especially providing perspective on current events and promoting constructive solutions for reducing the risk.&nbsp;</p><p>In May, GCRI put out an&nbsp;<a href=\"https://gcrinstitute.org/open-call-for-advisees-and-collaborators-may-2022/\"><u>open call</u></a> for people interested in seeking our advice or in collaborating on projects with us. The open call was a continuation of the Advising and Collaboration Programs GCRI conducted in&nbsp;<a href=\"https://gcrinstitute.org/summary-of-2019-advising-and-collaboration-program/\"><u>2019</u></a>,&nbsp;<a href=\"https://gcrinstitute.org/summary-of-january-july-2020-advising-and-collaboration-program/\"><u>2020</u></a>, and&nbsp;<a href=\"https://gcrinstitute.org/summary-of-the-2021-advising-and-collaboration-program/\"><u>2021</u></a>. Their&nbsp;<a href=\"https://gcrinstitute.org/summary-of-the-2022-advising-and-collaboration-program/\"><u>2022 Advising and Collaboration Program</u></a> was quite successful and allowed them to continue networking and meeting others in the field of global catastrophic risk.</p><p>GCRI is pleased to announce its&nbsp;<a href=\"https://gcrinstitute.org/2022-gcri-fellowship-program/\"><u>2022&nbsp;</u></a><a href=\"https://gcrinstitute.org/the-inaugural-2021-gcri-fellowship-program/\"><u>GCRI Fellowship Program</u></a>. The program recognizes a select group of four GCRI Fellows who made exceptional contributions to addressing global catastrophic risk in collaboration with GCRI in 2022.&nbsp;</p><h3>Happier Lives Institute</h3><p>HLI\u2019s&nbsp;<a href=\"https://www.happierlivesinstitute.org/report/a-can-of-worms/\"><u>final report</u></a> of 2022 looked at the effects of deworming interventions on subjective wellbeing. They find that deworming has a&nbsp;<strong>non-significant effect</strong> on long-term happiness that seems (surprisingly) to become negative over time.&nbsp;</p><p>The report marks the culmination of HLI\u2019s Phase 1 research: assessing GiveWell\u2019s top charity recommendations in terms of their effects on subjective wellbeing. After three years of research, the key takeaway is that HLI\u2019s&nbsp;<a href=\"https://www.happierlivesinstitute.org/2022/11/24/2022-charity-recommendation/\"><u>recommended charity</u></a>,&nbsp;<strong>StrongMinds</strong>, is more cost-effective than all of them (unless you hold unusually strong views about&nbsp;<a href=\"https://www.happierlivesinstitute.org/report/the-elephant-in-the-bednet/\"><u>the badness of death</u></a>).</p><p>HLI\u2019s research was awarded several prizes in 2022 as part of GiveWell\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/NFhELFno7ScuCxXMY/the-winners-of-the-change-our-mind-contest-and-some\"><u>Change Our Mind Contest</u></a>, Open Philanthropy\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/XBHx9zhAtkiBJnZNu/cause-exploration-prizes-announcing-our-prizes-1\"><u>Cause Exploration Prizes</u></a>, and the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest\"><u>EA Criticism &amp; Red Teaming Contest</u></a>.</p><p>HLI\u2019s Director, Michael Plant, was interviewed by&nbsp;<a href=\"https://www.youtube.com/watch?v=THuodu7KluY\"><u>GB News</u></a>,&nbsp;<a href=\"https://www.wbez.org/stories/how-to-make-the-biggest-impact-with-your-donations/1973e3a4-b173-475d-be59-0888c12c562a\"><u>WBEZ Chicago</u></a>, and Devex.&nbsp;</p>", "user": {"username": "Lizka"}}, {"_id": "CNxteiKdRk9Hez3pv", "title": "EA relevant Foresight Institute Workshops in 2023: WBE & AI safety, Cryptography & AI safety, XHope, Space, and Atomically Precise Manufacturing", "postedAt": "2023-01-16T14:02:59.942Z", "htmlBody": "<p><strong>Foresight Institute is a research organization and non-profit that supports the development of technologies that can have a positive impact on the world. Since 1986, we have focused on advancing crucial technologies such as molecular machines and atomic precision for applications in energy, medicine, material science, and space development.&nbsp;</strong></p><p><strong>However, with all technological developments we also recognize that there is a risk. We believe that ideas from the effective altruism and longtermist communities have not yet been fully embraced in the mainstream science and technology fields, despite their potential to lead to safer developments of crucial technologies, and to differential technological progress. To address this, we want to invite more EA people to attend our upcoming workshops.&nbsp;</strong></p><p>By doing so, we hope to introduce these ideas into scientific discussions and also provide the effective altruism and longtermist communities with new scientific facts and perspectives. Our hope is that this can help increase long-term coordination between science and policy/risk researchers.&nbsp;</p><p>In this post, we introduce all events and workshops that we will be hosting in 2023, that could potentially be interesting to EAs, such as our WBE and AI safety-workshop, (<a href=\"https://80000hours.org/problem-profiles/whole-brain-emulation/\"><u>80k post on Whole Brain Emulation</u></a>), our Space-workshop (<a href=\"https://80000hours.org/problem-profiles/space-governance/\"><u>80k post on space governance</u></a>) or our Molecular Systems design workshop (<a href=\"https://80000hours.org/problem-profiles/atomically-precise-manufacturing/\"><u>80k post on atomically precise manufacturing</u></a>). You can find a full list of the events we are hosting in 2023 on our <a href=\"https://foresight.org/\">website.</a></p><p>If any of these workshops seem relevant to you, please apply&nbsp;<a href=\"https://foresight.org/application-for-foresight-biotech-molecular-machines-intelligent-cooperation-xhope-groups/\"><u>here</u></a>! We offer subsidized tickets to accepted attendees who are otherwise not able to attend. If you have any questions or comments on these events, please reach out to&nbsp;<a href=\"mailto:beatrice@foresight.org\"><u>beatrice@foresight.org</u></a><strong>.</strong><br>&nbsp;</p><p><a href=\"https://foresight.org/foresight-existential-hope-day-2023?utm_source=Foresight+Newsletter+Subscribers&amp;utm_campaign=9477bf4b79-EMAIL_CAMPAIGN_2022_11_11_06_12_COPY_01&amp;utm_medium=email&amp;utm_term=0_7c1b7f710b-9477bf4b79-\"><strong><u>Existential Hope Day</u></strong></a><strong>, as satellite to EAG Bay Area, February 27, 2023, in SF</strong></p><p>Aims to explore the potential positive futures that could be created through advancements in technology, and to help bridge the gap between risk and policy researchers and scientists working in for example biotechnology, nanotechnology, neurotechnology, computing, AI, and space technologies.&nbsp;</p><p>The goal of the event is to map out both the risks and possibilities of these fields, and to bring together individuals and organizations that are working towards future-positive outcomes.&nbsp;</p><p><strong>Speakers:</strong></p><ul><li>Toby Ord, University of Oxford: Existential Hope Fireside Chat</li><li>Robin Hanson: Post Dreamtime Futures</li><li>Creon Levit, Planet Labs: Existential Hope Across Physics, Biology, and Space</li><li>Tamara Winter, Stripe Press: Progress Media</li><li>Christine Peterson, Foresight Institute: Historic Perspectives on Long-term Futurism</li><li>Allison Duettmann, Foresight Institute: Gaming the Future: Intelligent Voluntary Cooperation</li><li>Jessy Kate Schingler, OpenLunar: Outer Space &amp; Coordination</li><li>Riva Tez, investor: Flourishing Futures</li></ul><p><a href=\"https://foresight.org/foresight-neurotech-workshop-2023?utm_source=Foresight+Newsletter+Subscribers&amp;utm_campaign=9477bf4b79-EMAIL_CAMPAIGN_2022_11_11_06_12_COPY_01&amp;utm_medium=email&amp;utm_term=0_7c1b7f710b-9477bf4b79-\"><strong><u>Whole Brain Emulation Workshop</u></strong></a><strong>, as satellite to EAG London, chaired by Anders Sandberg, Future of Humanity Institute, May 22 - 23, 2023, in Oxford</strong></p><p>This workshop focuses on exploring the potential of whole brain emulation (WBE) as a technology for generating software intelligence that is aligned with human values. We will aim to review the current state of the art in WBE-related technology, outline plausible development paths and necessary steps for full WBE, and determine whether there is potential for speeding up WBE development. We will also be considering any strategic, risk, or ethical issues that might speak against this approach.&nbsp;</p><p><strong>Speakers:</strong></p><ul><li>Anders Sandberg, Future of Humanity Institute</li><li>Todd Huffman, e11: Slicing &amp; Mapping Your Brain</li><li>Michael Andregg, Fathom: Compute Considerations for WBEs</li><li>Michael Shuhersky, MIT</li><li>Kenneth Hayworth, HHMI: Brain Emulation Prospects</li><li>Logan Collins, Washington University: Emulating Insect Brains</li><li>Randall A. Koene, Carboncopies</li><li>Sumner Norman, Caltech</li></ul><p><a href=\"https://foresight.org/foresight-space-workshop-2023/?utm_source=Foresight+Newsletter+Subscribers&amp;utm_campaign=9477bf4b79-EMAIL_CAMPAIGN_2022_11_11_06_12_COPY_01&amp;utm_medium=email&amp;utm_term=0_7c1b7f710b-9477bf4b79-\"><strong><u>Space Workshop</u></strong></a><strong>, chaired by Creon Levit, Planet Labs, June 4 &amp; 5, 2023 in SF</strong></p><p>This workshop is focused on advancing progress in space technologies, with a focus on both near-term applications and long-term exploration. It will touch both on the technology and the governance of the future in space.&nbsp;</p><p><strong>Speakers:&nbsp;</strong></p><ul><li>Creon Levit: Satellite Trajectories</li><li>Jessy Kate Schingler: Outer Space Law</li><li>Adam Brown: Long-term Futures in Space</li></ul><p><a href=\"https://foresight.org/foresight-crypto-security-ai-workshop-2023?utm_source=Foresight+Newsletter+Subscribers&amp;utm_campaign=9477bf4b79-EMAIL_CAMPAIGN_2022_11_11_06_12_COPY_01&amp;utm_medium=email&amp;utm_term=0_7c1b7f710b-9477bf4b79-\"><strong><u>Cryptography, Security, AI Workshop</u></strong></a><strong>, chaired by Mark S. Miller, Agoric, July 10 - 11, 2023, in SF</strong></p><p>This workshop is focused on exploring the intersection of cryptography, security, and AI and its potential importance for beneficial futures. We will explore areas such as how technology can help civilization cooperate and defend itself better, while also considering the immense impact of AI on the field of decentralized computation.&nbsp;</p><p><strong>Speakers:</strong></p><ul><li>Michael Andregg: AI &amp; Compute Overview</li><li>Mark Miller, Agoric: Computational Market Places</li><li>Robin Hanson, George Mason University: Slow AI Takeoff Scenarios</li><li>Divya Siddarth, Microsoft: The Collectively Intelligent AI Corporation</li></ul><p><a href=\"https://foresight.org/foresight-space-workshop-2023?utm_source=Foresight+Newsletter+Subscribers&amp;utm_campaign=9477bf4b79-EMAIL_CAMPAIGN_2022_11_11_06_12_COPY_01&amp;utm_medium=email&amp;utm_term=0_7c1b7f710b-9477bf4b79-\"><strong><u>Molecular Systems Design Workshop</u></strong></a><strong>, chaired by Ben Reinhardt, Speculative Technologies, Adam Marblestone, Convergent Research, September 11 - 12, 2023 in SF</strong></p><p>This workshop is focused on exploring how advancements in software and simulation can be used to speed up the design of complex molecular machines systems. The goal of the event is to foster cooperation towards shared long-term goals in this field and map out what is needed to make progress.</p><p><strong>Speakers:</strong></p><ul><li>Adam Marblestone, Convergent Research</li><li>Alexis Courbet: Computational Design of Self-Assembling DNA Nanomachines</li><li>William Shih: Self-assembling DNA Nanostructures</li><li>Chris Schafmeister: CANDO &amp; Programmable Spiroligomers</li><li>Ben Reinhardt, PARPA</li><li>Hein-Pieter van Braam: User-friendly Molecular System Design</li><li>Stephane Redon: SAMSON Computational Nanoscience Updated</li><li>Petr Sulc, Arizona State University: Computer-aided Design for Nanotechnology</li></ul><p><strong>In addition to our events, we also encourage people to apply to join our technical groups:</strong></p><ol><li><a href=\"https://foresight.org/molecular-machines/\">Molecular Machines</a> to better control matter</li><li><a href=\"https://foresight.org/biotech-health-extension-program/\">Biotech</a> to reverse aging&nbsp;</li><li><a href=\"https://foresight.org/intelligent-cooperation/\">Computer Science</a> to secure human AI cooperation</li><li><a href=\"https://foresight.org/neurotech-improving-cognition-program/\">Neurotech</a> to support human flourishing</li><li><a href=\"https://foresight.org/space-expanding-outward-program/\">Spacetech</a> to further exploration</li></ol><p>In these groups, we connect scientists, entrepreneurs, and institutional allies who cooperate to advance the respective technologies. Currently, meetings take place virtually every month. To join any of these groups,&nbsp;<a href=\"https://foresight.org/join/\">apply here</a>.</p>", "user": {"username": "elteerkers"}}, {"_id": "rZoRGxJzipcQoaPST", "title": "How many people are working (directly) on reducing existential risk from AI?", "postedAt": "2023-01-17T14:03:35.424Z", "htmlBody": "<h1>Summary</h1><p>I've updated my estimate of the number of FTE (full-time equivalent) working (directly) on reducing existential risks from AI <strong>from 300 FTE to 400 FTE</strong>.</p><p>Below I've pasted some slightly edited excepts of the relevant sections of the 80,000 Hours profile on <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\">preventing an AI-related catastrophe</a>.</p><h1>New 80,000 Hours estimate of the number of people working on reducing AI risk</h1><h2>Neglectedness estimate</h2><p>We estimate there are around 400 people around the world working directly on reducing the chances of an AI-related existential catastrophe (with a 90% confidence interval ranging between 200 and 1,000). Of these, about three quarters are working on technical AI safety research, with the rest split between strategy (and other governance) research and advocacy.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref290oixxnjlk\"><sup><a href=\"#fn290oixxnjlk\">[1]</a></sup></span>We think there are around 800 people working in <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#complementary-yet-crucial-roles\">complementary roles</a>, but we\u2019re highly uncertain about this estimate.</p><h2>Footnote on methodology</h2><p>It\u2019s difficult to estimate this number.</p><p>Ideally we want to estimate the number of FTE (\u201c<a href=\"https://en.wikipedia.org/wiki/Full-time_equivalent\">full-time equivalent</a>\u201c) working on the problem of reducing existential risks from AI.</p><p>But there are lots of ambiguities around what counts as working on the issue. So I tried to use the following guidelines in my estimates:</p><ul><li>I didn\u2019t include people who might think of themselves on a career path that is building towards a role preventing an AI-related catastrophe, but who are currently skilling up rather than working directly on the problem.</li><li>I included researchers, engineers, and other staff that seem to work directly on technical AI safety research or AI strategy and governance. But there\u2019s an uncertain boundary between these people and others who I chose not to include. For example, I didn\u2019t include machine learning engineers whose role is building AI systems that might be used for safety research but aren\u2019t <i>primarily</i> designed for that purpose.</li><li>I only included time spent on work that seems related to reducing the potentially <a href=\"https://80000hours.org/articles/existential-risks/\">existential risks</a> from AI, like those discussed in this article. Lots of wider AI safety and AI ethics work focuses on reducing other risks from AI seems relevant to reducing existential risks \u2013 this \u2018indirect\u2019 work makes this estimate difficult. I decided not to include indirect work on reducing the risks of an AI-related catastrophe (see our <a href=\"https://80000hours.org/articles/problem-framework/#a-challenge-direct-vs-indirect-future-effort\">problem framework</a> for more).</li><li>Relatedly, I didn\u2019t include people working on other problems that might indirectly affect the chances of an AI-related catastrophe, such as <a href=\"https://80000hours.org/problem-profiles/improving-institutional-decision-making/\">epistemics and improving institutional decision-making</a>, reducing the chances of <a href=\"https://80000hours.org/problem-profiles/great-power-conflict/\">great power conflict</a>, or <a href=\"https://80000hours.org/problem-profiles/promoting-effective-altruism/\">building effective altruism</a>.</li></ul><p>With those decisions made, I estimated this in three different ways.</p><p>First, for each organisation in the <a href=\"https://aiwatch.issarice.com/\">AI Watch</a> database, I estimated the number of FTE working directly on reducing existential risks from AI. I did this by looking at the number of staff listed at each organisation, both in total and in 2022, as well as the number of researchers listed at each organisation. Overall I estimated that there were 76 to 536 FTE working on technical AI safety (90% confidence), with a mean of 196 FTE. I estimated that there were 51 to 359 FTE working on AI governance and strategy (90% confidence), with a mean of 151 FTE. There\u2019s a lot of subjective judgement in these estimates because of the ambiguities above. The estimates could be too low if AI Watch is missing data on some organisations, or too high if the data counts people more than once or includes people who no longer work in the area.</p><p>Second, I adapted the methodology used by <a href=\"https://forum.effectivealtruism.org/posts/8ErtxW7FRPGMtDqJy/the-academic-contribution-to-ai-safety-seems-large\">Gavin Leech\u2019s estimate of the number of people working on reducing existential risks from AI</a>. I split the organisations in Leech\u2019s estimate into technical safety and governance/strategy. I adapted Gavin\u2019s figures for the proportion of computer science academic work relevant to the topic to fit my definitions above, and made a related estimate for work outside computer science but within academia that is relevant. Overall I estimated that there were 125 to 1,848 FTE working on technical AI safety (90% confidence), with a mean of 580 FTE. I estimated that there were 48 to 268 FTE working on AI governance and strategy (90% confidence), with a mean of 100 FTE.</p><p>Third, I looked at the estimates of similar numbers by <a href=\"https://forum.effectivealtruism.org/posts/3gmkrj3khJHndYGNe/estimating-the-current-and-future-number-of-ai-safety\">Stephen McAleese</a>. I made minor changes to McAleese\u2019s categorisation of organisations, to ensure the numbers were consistent with the previous two estimates. Overall I estimated that there were 110 to 552 FTE working on technical AI safety (90% confidence), with a mean of 267 FTE. I estimated that there were 36 to 193 FTE working on AI governance and strategy (90% confidence), with a mean of 81 FTE.</p><p>I took a geometric mean of the three estimates to form a final estimate, and combined confidence intervals by assuming that distributions were approximately lognormal.</p><p>Finally, I estimated the number of FTE in <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#complementary-yet-crucial-roles\">complementary roles</a> using the AI Watch database. For relevant organisations, I identified those where there was enough data listed about the number of <i>researchers</i> at those organisations. I calculated the ratio between the number of researchers in 2022 and the number of staff in 2022, as recorded in the database. I calculated the mean of those ratios, and a confidence interval using the standard deviation. I used this ratio to calculate the overall number of support staff by assuming that estimates of the number of staff are lognormally distributed and that the estimate of this ratio is normally distributed. Overall I estimated that there were 2 to 2,357 FTE in complementary roles (90% confidence), with a mean of 770 FTE.</p><p>There are likely many errors in this methodology, but I expect these errors are small compared to the uncertainty in the underlying data I\u2019m using. Ultimately, I\u2019m still highly uncertain about the overall FTE working on preventing an AI-related catastrophe, but I\u2019m confident enough that the number is relatively small to say that the problem as a whole is highly neglected.</p><p>I\u2019m very uncertain about this estimate. It involved a number of highly subjective judgement calls. You can see the (very rough) spreadsheet I worked off <a href=\"https://docs.google.com/spreadsheets/d/1e1Vh_nK_7VHKZUuQ9VNp3JWC2etjUAHVmVXbKarKMNw/edit\">here</a>. If you have any feedback, I\u2019d really appreciate it if you could tell me what you think using <a href=\"https://forms.gle/RRZaFTfdDkSQ6fJG8\">this form</a>.</p><h1>Some extra thoughts from me</h1><p>This number is extremely difficult to estimate.</p><p>Like any Fermi estimate, I'd expect there to be a number of mistakes in this estimate. I think there will be two main types:</p><ul><li>Bad judgment calls when estimating the number of people working at each organisation, e.g. based on \"what counts as an FTE working directly on this issue\", \"how wrong is the AI watch database on this organisation\", etc.</li><li>Errors in calculation / estimating uncertainty, etc.</li></ul><p>Again, like in any Fermi estimate, I'd hope that these errors will roughly cancel out overall.</p><p>I didn't spend much time on this (maybe about 2 days of work). This is because I'd guess that more work won't improve the estimate by decision-relevant amounts. Some reasons for this:</p><ul><li>A rougher version of this estimate that I'd used previously came to an answer of 300 FTE. That estimate took around 3-4 hours of work. While 300 FTE to 400 FTE is a large proportional change, it still represents a highly neglected field and doesn't seem substantially decision-relevant.</li><li>Errors in collecting data on this seem large in a way that couldn't be easily mitigated by doing more work.</li><li>There would still be substantial subjective judgement in an estimate that took more time. My uncertainty in this estimate includes uncertainty in whether these are the right judgement calls (on the criteria of \"is it truthful, across a distribution of plausible definitions, to say that this is the number of FTE working directly on reducing existential risk from AI\"), and it seems very difficult to reduce that uncertainty.</li></ul><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn290oixxnjlk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref290oixxnjlk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Note that before 19 December 2022, <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#neglectedness\">this page</a> gave a lower estimate of 300 FTE working on reducing existential risks from AI, of which around two thirds were working on technical AI safety research, with the rest split between strategy (and other governance) research and advocacy</p><p>This change represents a (hopefully!) improved estimate, rather than a notable change in the number of researchers.</p></div></li></ol>", "user": {"username": "Benjamin Hilton"}}, {"_id": "pndb6TQ9nAiAkXg8x", "title": "80,000 Hours career review: Information security in high-impact areas", "postedAt": "2023-01-16T12:45:26.440Z", "htmlBody": "<p><i>This is a cross-post of a career review from the 80,000 Hours website written by Jarrah Bloomfield. See the original </i><a href=\"https://80000hours.org/career-reviews/information-security/\"><i>here</i></a><i>.</i></p><h2><strong>Introduction</strong></h2><p>As the 2016 US presidential campaign was entering a fractious round of primaries, Hillary Clinton\u2019s campaign chair, John Podesta, opened a disturbing email.[1] The March 19 message warned that his Gmail password had been compromised and that he urgently needed to change it.</p><p>The email was a lie. It wasn\u2019t trying to help him protect his account \u2014 it was a phishing attack trying to gain illicit access.</p><p>Podesta was suspicious, but the campaign\u2019s IT team erroneously wrote the email was \u201clegitimate\u201d and told him to change his password. The IT team provided a safe link for Podesta to use, but it seems he or one of his staffers instead clicked the link in the forged email. That link was used by Russian intelligence hackers known as \u201cFancy Bear,\u201d and they used their access to leak private campaign emails for public consumption in the final weeks of the 2016 race, embarrassing the Clinton team.</p><p>While there are plausibly many critical factors in any close election, it\u2019s possible that the controversy around the leaked emails played a non-trivial role in Clinton\u2019s subsequent loss to Donald Trump. This would mean the failure of the campaign\u2019s security team to prevent the hack \u2014 which might have come down to a mere typo[2] \u2014 was extraordinarily consequential.</p><p>These events vividly illustrate how careers in infosecurity at key organisations have the potential for outsized impact. Ideally, security professionals can develop robust practices that reduce the likelihood that a single slip-up will result in a significant breach. But this key component for the continued and unimpaired functioning of important organisations is often neglected.</p><p>And the need for such protection stretches far beyond hackers trying to cause chaos in an election season. Information security is vital to safeguard all kinds of critical organisations such as those storing extremely sensitive data about biological threats, nuclear weapons, or advanced artificial intelligence, that might be targeted by criminal hackers or aggressive nation states. Such attacks, if successful, could contribute to dangerous competitive dynamics (such as arms races) or directly lead to catastrophe.</p><p>Some infosecurity roles involve managing and coordinating organisational policy, working on technical aspects of security, or a combination of both. We believe many such roles have thus far been underrated among those interested in effective altruism and reducing global catastrophic risks, and we\u2019d be excited to see more altruistically motivated candidates move into this field.</p><h2>In a nutshell</h2><p><i>Organisations with influence, financial power, and advanced technology are targeted by actors seeking to steal or abuse these assets. A career in information security is a promising avenue to support high-impact organisations by protecting against these attacks, which have the potential to disrupt an organisation\u2019s mission or even increase </i><a href=\"https://80000hours.org/articles/existential-risks/\"><i>existential risk</i></a><i>.</i></p><ul><li><i>Jeffrey Ladish contributed to this career review. We also thank Wim van der Schoot for his helpful comments.</i></li></ul><h2><strong>Why might information security be a high-impact career?</strong></h2><p>Information security protects against events that hamper an organisation\u2019s ability to fulfil its mission, such as attackers gaining access to confidential information. Information security specialists play a vital role in supporting the mission of organisations, similar to roles in <a href=\"https://80000hours.org/articles/operations-management/\">operations</a>.</p><p>So if you want an impactful career, expertise in information security could enable you to make a significant positive difference in the world by helping important organisations and institutions be secure and successful.</p><p>Compared to other roles in technology, an information security career can be a safe option because there may be less risk you could have a negative impact. In general, preventing attacks makes the world a safer place, even if it\u2019s not clear whether potential victim organisations are providing net positive impact themselves. When a company is hacked, the harm can disproportionately fall on others \u2014 such as people who trusted the company with their private information.</p><p>On the other hand, information security roles can sometimes have limited impact even when supporting high-impact areas, if the organisation does not genuinely value security. Many organisations have security functions primarily so that they can comply with regulations and compliance standards for doing business. These security standards have an important role, but when they are applied without care for achieving real security outcomes, it often leads to <a href=\"https://en.wikipedia.org/wiki/Security_theater\">security theatre</a>. It is not uncommon for security professionals to realise that they are having minimal impact on the security posture of their organisation.</p><h3><strong>Protecting organisations working on the world\u2019s most pressing problems</strong></h3><p>Organisations working on <a href=\"https://80000hours.org/problem-profiles/\">pressing problems</a> need cybersecurity expertise to protect their computer systems, financial resources, and confidential information from attack. In some ways, these challenges are similar to those faced by any other organisation; however, organisations working on major global problems are sometimes special targets for attacks.</p><p>These organisations \u2014 such as those trying to monitor dangerous pathogens or coordinate to reduce global tensions \u2014 often work with international institutions, local political authorities, and governments. They may be targeted by state-sponsored attacks from countries with relevant geopolitical interests, either to steal information or to gain access to other high-value targets.</p><p>Some high-impact organisations have confidential, sensitive discussions as part of their work, where a leak of information through a security compromise would damage trust and their ability to fulfil their mission. This is especially relevant when operating in countries with information control and censorship regimes.</p><p>In extreme cases, some organisations need help protecting information that could be harmful for the world if it was known more widely, such as harmful genetic sequences or powerful AI technology.</p><p>In addition to threats from state-sponsored attackers, cybercrime groups also raise serious risks.</p><p>They seek financial gain through extortion and fraud \u2014 for example, by changing payment information, ransoming data, or threatening to leak confidential correspondence. Any organisation is vulnerable to these attacks. But organisations that handle particularly sensitive information or large value financial transactions, such as philanthropic grantmaking funds, are especially likely targets.</p><h2><strong>What does working in high-impact information security roles actually look like?</strong></h2><p>\u201cDefensive\u201d cybersecurity roles \u2014 where the main job is to defend against attacks by outsiders \u2014 are most commonly in demand, especially in smaller nonprofit organisations and altruistically minded startups that don\u2019t have the resources to hire more than a single security specialist.</p><p>In some of these roles, you\u2019ll find yourself doing a mix of hands-on technical work and communicating security risk. For example:</p><ul><li>You will apply an understanding of how hackers work and how to stop them.</li><li>You will set up security systems, review IT configurations, and provide advice to the team about how to do their work securely.</li><li>You will test for bugs and vulnerabilities and design systems and policies that are robust to a range of possible attacks.</li></ul><p>Having security knowledge across a wide range of organisational IT topics will help you be most useful, such as laptop security, cloud administration, application security, and IT accounts (often called \u201cidentity and access management\u201d).</p><p>You can have an outsized impact relative to another potential hire by working for a high-impact organisation where you understand their cause area. This is because information security can be challenging for organisations that are focussed on social impact, as industry standard cybersecurity advice is built to support profit motives and regulatory frameworks. <strong>Tailoring cybersecurity to how an organisation is trying to achieve its mission \u2014 and to prevent the harmful events the organisation cares most about \u2014 could greatly increase your effectiveness.</strong></p><p>If you\u2019re interested in <a href=\"https://80000hours.org/articles/existential-risks/\">reducing existential risks</a>, we think you should consider joining an organisation working in relevant areas such <a href=\"https://80000hours.org/career-reviews/ai-safety-researcher/#recommended-organisations\">artificial intelligence</a> or <a href=\"https://80000hours.org/career-reviews/biorisk-research/#recommended-organisations\">biorisk</a>.</p><p>An important part of this is bringing your team along for the journey. To do security well, you will regularly be asking people to change the way they work (likely adding hurdles!), so being an effective communicator can be as important as understanding the technical details. Helping everyone understand why certain security measures matter and how you\u2019re balancing the costs and benefits is required for the team to accept additional effort or seemingly unnecessary steps.</p><p>Ethical hacking roles, in which you\u2019re tasked with breaking the defences of your clients or employers in order to ultimately improve them, are also important for cybersecurity \u2014 but only very large organisations have positions for these sorts of \u201coffensive\u201d (or \u201cred teaming\u201d) roles. More often, such roles are at cybersecurity services companies, which are paid to do short-term penetration testing exercises for clients.</p><p>If you take such a role, it would be hard to focus on the security of impactful organisations in order to maximise your impact, because you often have little choice about which clients you\u2019re supporting. But you could potentially build career capital in these kinds of positions before moving on to more impactful jobs.</p><h3><strong>What kind of salaries do cybersecurity professionals earn?</strong></h3><p>Professionals in information security roles such as cybersecurity earn high salaries. The US Bureau of Labor Statistics reported that the median <a href=\"https://www.bls.gov/ooh/computer-and-information-technology/information-security-analysts.htm\">salary for information security analysts</a> was over $100,000 a year in 2021.</p><p>While you\u2019ll likely have a bigger impact supporting an organisation directly if the organisation is doing particularly important work, <a href=\"https://80000hours.org/articles/earning-to-give/\">earning to give</a> can still be a high-impact option, especially when you focus on donating to the <a href=\"https://www.givingwhatwecan.org/\">most effective projects that could use the extra funds</a>.</p><h2><strong>How to assess your fit in advance?</strong></h2><p>A great way to gauge your fit for information security is to try it out. There are many free online resources that will give you hands-on experience with technical aspects of security. You can get a basic introduction through the <a href=\"https://www.sans.org/cyberaces/\">SANS Cyber Aces</a> course.</p><p>Some other ideas to get you started:</p><ul><li><strong>Try out ethical hacking to understand how hacks work and gain an intuition for security loopholes.</strong> Find a tutorial on basic attacks (e.g. <a href=\"https://overthewire.org/wargames/bandit/\">OverTheWire</a>, <a href=\"http://hackthebox.com/\">HackTheBox</a>, or a course (e.g. Coursera\u2019s <a href=\"https://www.coursera.org/learn/ethical-hacking-essentials-ehe\">Ethical Hacking Essentials</a>). Read up on high-profile vulnerabilities, and see if there are any guides on setting up a lab environment and exploiting them (e.g. <a href=\"https://medium.com/geekculture/log4shell-zero-day-exploit-walkthrough-f42352612ca6\">Log4Shell</a>). If you\u2019re studying at a university, it may be easy to join a <a href=\"https://thehackersmeetup.medium.com/beginners-guide-to-capture-the-flag-ctf-71a1cbd9d27c\">Capture the Flag</a> (CTF) team.</li><li><strong>Play around with security tools.</strong> <a href=\"https://www.wireshark.org/\">Wireshark</a> will inspect the surprising variety of network traffic on your computer, and <a href=\"https://portswigger.net/burp/communitydownload\">Burp Suite Community</a> can go deeper into web requests. Scan your home network for vulnerabilities with <a href=\"https://www.tenable.com/products/nessus/nessus-essentials\">Nessus Essentials</a>.</li><li><strong>Set up your own infrastructure.</strong> Host a <a href=\"https://ubuntu.com/tutorials/how-to-run-ubuntu-desktop-on-a-virtual-machine-using-virtualbox#1-overview\">virtual machine</a>. Build a <a href=\"https://linuxhint.com/install-apache-on-ubuntu-22-04/\">web server</a> and <a href=\"https://www.tecmint.com/apache-security-tips/\">secure it</a>. Try installing <a href=\"https://www.elastic.co/elastic-stack\">Elastic Stack</a> and <a href=\"https://www.ericooi.com/zeekurity-zen-part-i-how-to-install-zeek-on-ubuntu/\">Zeek</a>. Get the <a href=\"https://aws.amazon.com/free/\">AWS Free Tier</a> and poke around the cloud administrator settings.</li></ul><p>Having a knack for figuring out how computer systems work, or enjoying deploying a <a href=\"https://www.wired.com/2008/03/securitymatters-0320/\">security mindset</a> are predictors that you might be a good fit \u2014 but they are not required to get started in information security.</p><h2><strong>How to enter infosecurity</strong></h2><h3><strong>Entering with a degree</strong></h3><p>The traditional way to enter this field is to study an IT discipline \u2014 such as computer science, software engineering, computer engineering, or a related field \u2014 in a university that has a good range of cybersecurity courses. However, you shouldn\u2019t think of this as a prerequisite \u2014 <strong>there are many successful security practitioners without a formal degree.</strong> A degree often makes it easier to get entry-level jobs though, because many organisations still require it.</p><p>Aside from cybersecurity-labelled courses, a good grasp of the fundamentals of computer systems is useful. This includes topics on computer networks, operating systems, and the basics of how computer hardware works. We suggest you consider at least one course in machine learning \u2014 while it\u2019s difficult to predict technology changes, it\u2019s plausible that AI technologies will dramatically change the security landscape.</p><p>Consider finding a part-time job in an IT area while studying (see the next section), or doing an internship. This doesn\u2019t need to be in an information security capacity; it can just be a role where you get to see first-hand how IT works. What you learn in university and what happens in practice are different, and understanding how IT is applied in the real world is vital.</p><p>In the final year of your degree, look for entry-level cybersecurity positions \u2014 or other IT positions, if you need to.</p><p>We think that jobs in cybersecurity defensive roles are ideal for gaining <a href=\"https://80000hours.org/information-security#skills-anchor\">the broad range of skills</a> that are most likely to be relevant to high-impact organisations. These have role titles such as <i>Security Analyst</i>, <i>Security Operations</i>, <i>IT Security Officer</i>, <i>Security Engineer</i>, or even <i>Application Security Engineer</i>. \u201cOffensive\u201d roles such as penetration testing can also provide valuable experience, but you may not get as broad an overview across all of the fronts relevant to enterprise security, or experience the challenges with implementation first-hand.</p><h3><strong>Entering with (just) IT experience</strong></h3><p>It is also possible to enter this field without a degree.</p><p>If you have a good working knowledge of IT or coding skills, a common path is to start in a junior role in internal IT support (or similar service desk or help desk positions) or software role. Many people working in cybersecurity today transitioned from other roles in IT. This can work well if you are especially interested in computers and are motivated to tinker with computer systems in your own time.</p><p>A lot of what that you\u2019ll learn in an organisational IT role will be useful for cybersecurity roles. Solid IT management requires day-to-day security, and understanding how the systems work and the challenges caused by security features is important if you\u2019re going to be effective in cybersecurity.</p><h3><strong>Do you need certifications?</strong></h3><p>There are many cybersecurity certifications you can get. They aren\u2019t mandatory, but having one may help you get into an entry-level job, especially if you don\u2019t have a degree. The usefulness varies depending on how reputable the provider is, and the training and exams may be expensive.</p><p>Some well-regarded certifications are <a href=\"https://www.comptia.org/certifications/security\">CompTIA Security+</a>, <a href=\"https://www.giac.org/certifications/security-essentials-gsec/\">GIAC Security Essentials</a>, <a href=\"https://www.offensive-security.com/pwk-oscp/\">OSCP Penetration Testing</a>, and <a href=\"https://www.eccouncil.org/programs/certified-ethical-hacker-ceh/\">Certified Ethical Hacker</a>. Vendor and technology certifications (e.g. Microsoft or AWS) generally aren\u2019t valuable unless they\u2019re specific to a job you\u2019re pursuing.</p><h2><strong>What sorts of places should you work?</strong></h2><p>For your first few years, we recommend prioritising finding a role that will grow your knowledge and capability quickly. Some high-impact organisations are quite small, so they may not be well-placed to train you up early in your career, because they\u2019ll likely have less capacity for mentorship in a range of technical areas.</p><p>Find a job where you can learn good IT or cybersecurity management from others.</p><p>The best places to work will already have relatively good security management practices and organisational maturity, so you can see what things are supposed to look like. You may also get a sense of the barriers that prevent organisations from having <i>ideal</i> security practices. Being able to ask questions from seasoned professionals and figure out what is actually feasible helps you learn more quickly than running up against all of the roadblocks yourself.</p><p>Tech companies and financial organisations have a stronger reputation for cybersecurity. Security specialist organisations \u2014 such as in consulting, managed security providers, or security software companies \u2014 can also be great places to learn. Government organisations specialising in cybersecurity can provide valuable experience that is hard to get outside of specific roles.</p><p>Once you\u2019re skilled up, the main thing to look for is a place that is doing <a href=\"https://80000hours.org/problem-profiles/\">important work</a>. This might be a government agency, a nonprofit, or even a for-profit. We list some high-impact organisations <a href=\"https://80000hours.org/job-board/top-orgs/\">here</a>. Information security is a support function needed by all organisations to different degrees. <strong>How positive your impact is will depend a lot on whether you\u2019re protecting an organisation that does important and pressing work.</strong> Below we discuss specific areas where we think additional people could do the most impactful work.</p><h3><strong>Safeguarding information hazards</strong></h3><p>Protecting information that could be damaging for the world if it was stolen may be especially impactful and could help decrease <a href=\"https://80000hours.org/articles/existential-risks/\">existential risk</a>.</p><p>Some information could increase the risk that humanity becomes extinct if it were leaked. Organisations focussed on reducing this risk may need to create or use this information as part of their work, so working on their security means you can have a directly positive impact. Examples include:</p><ul><li><a href=\"https://80000hours.org/career-reviews/ai-safety-researcher/\">AI research labs</a>, which may discover technologies that could harm humanity in the wrong hands.</li><li><a href=\"https://80000hours.org/career-reviews/biorisk-research/\">Biorisk researchers</a> who work on sensitive materials, such as harmful genetic sequences that could be used to <a href=\"https://80000hours.org/problem-profiles/preventing-catastrophic-pandemics/\">engineer pandemics</a>.</li><li>Research and grantmaking foundations that have access to sensitive information on the strategies and results of existential risk reduction organisations.</li></ul><h3><strong>Contributing to safe AI</strong></h3><p>Security skills are relevant for <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\">preventing an AI-related catastrophe</a>. Security professionals can bring a security mindset and technical skills that can mitigate the risk of an advanced AI <a href=\"https://80000hours.org/articles/what-could-an-ai-caused-existential-catastrophe-actually-look-like/\">leading to disaster</a>.</p><p>If advanced AI ends up radically transforming the global economy, as <a href=\"https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/\">some believe it might</a>, the security landscape and nature of threats discussed in this article could change in unexpected ways. Understanding the cutting-edge uses of AI by both malicious hackers and infosecurity professionals could allow you to have a large impact by helping ensure the world is protected from major catastrophic threats.</p><h3><strong>Working in governments</strong></h3><p>Governments also hold information that could negatively impact geopolitical stability if stolen, such as weapons technology and diplomatic secrets. But it may be more difficult to have a positive impact through this path working in government, as established bureaucracies are often resistant to change, and this resistance may prevent you from having impact.</p><p>That said, the scale of government also means that if you <i>are</i> able to make a positive change in impactful areas, it has the potential for far-reaching effects.</p><p>People working in this area should regularly reassess whether their work is, or is on a good path to, making a meaningful difference. There may be better opportunities inside or outside government.</p><p>You may have a positive impact by working in cybersecurity for your country\u2019s national security agencies, either as a direct employee or as a government contractor. In addition, these roles may give you the experience and professional contacts needed to work effectively in national cybersecurity policy.</p><p>If you have the opportunity, working to set and enforce sensible cybersecurity <a href=\"https://80000hours.org/articles/government-policy/\">policy</a> could be highly impactful.</p><h3><strong>Want one-on-one advice on pursuing this path?</strong></h3><p>If you think this path might be a great option for you, but you need help deciding or thinking about what to do next, our team might be able to help.</p><p>We can help you compare options, make connections, and possibly even help you find jobs or funding opportunities.</p><p><a href=\"https://80000hours.org/speak-with-us/?int_campaign=career-review-generic\">APPLY TO SPEAK WITH OUR TEAM</a></p><h2><strong>Learn more</strong></h2><ul><li>Podcast: <a href=\"https://80000hours.org/podcast/episodes/nova-dassarma-information-security-and-ai-systems/\">Nova DasSarma on why information security may be critical to the safe development of AI systems</a></li><li><a href=\"https://www.effectivealtruism.org/articles/kevin-esvelt-mitigating-catastrophic-biorisks/\">Mitigating catastrophic biorisks</a> \u2014 a talk by MIT professor Kevin Esvelt about why advanced information security is important for reducing biorisks</li><li><a href=\"https://forum.effectivealtruism.org/posts/WqQDCCLWbYfFRwubf/information-security-considerations-for-ai-and-the-long-term\">Information security considerations for AI and the long-term future</a> \u2014 an Effective Altruism Forum post by Jeffrey Ladish and Lennart Heim</li><li>Podcast: <a href=\"https://80000hours.org/podcast/episodes/bruce-schneier-security-secrets-and-surveillance/\">Bruce Schneier on how insecure electronic voting could break the United States \u2014 and surveillance without tyranny</a></li><li><a href=\"https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/\">Security Mindset and Ordinary Paranoia</a> \u2014 an analysis by Eliezer Yudkowsky of the Machine Intelligence Research Institute</li><li><a href=\"https://www.sans.org/white-papers/454/\">OK, So I Need Security. Where Do I Start?</a> \u2014 a white paper by Lyde Andrews of SANS</li><li><a href=\"https://danielmiessler.com/blog/build-successful-infosec-career/\">How to build a cybersecurity career</a> \u2014 a blog post by Daniel Miessler, an infosecurity professional with more than 20 years of experience</li><li><a href=\"https://forum.effectivealtruism.org/posts/ZJiCfwTy5dC4CoxqA/information-security-careers-for-gcr-reduction\">Information security careers for global catastrophic risk reduction</a> \u2014 an Effective Altruism Forum post by by Claire Zabel and Luke Muehlhauser of Open Philanthropy</li></ul><h3><strong>Notes and references</strong></h3><ol><li>This account is based on public reporting of the incident in outlets including <a href=\"https://www.vox.com/policy-and-politics/2016/10/28/13456368/how-john-podesta-email-got-hacked\">Vox</a>, <a href=\"https://www.vice.com/en/article/mg7xjb/how-hackers-broke-into-john-podesta-and-colin-powells-gmail-accounts\">Vice</a>, and <a href=\"https://edition.cnn.com/2016/10/28/politics/phishing-email-hack-john-podesta-hillary-clinton-wikileaks/index.html\">CNN</a>.<a href=\"https://80000hours.org/career-reviews/information-security/#fn-ref-1\">\u21a9</a></li><li>\u201c\u2018This is a legitimate email,\u2019 Charles Delavan, a Clinton campaign aide, replied to another of Mr. Podesta\u2019s aides, who had noticed the alert. \u2018John needs to change his password immediately.\u2019 With another click, a decade of emails that Mr. Podesta maintained in his Gmail account \u2014 a total of about 60,000 \u2014 were unlocked for the Russian hackers. Mr. Delavan, in an interview, said that his bad advice was a result of a typo: He knew this was a phishing attack, as the campaign was getting dozens of them. He said he had meant to type that it was an \u2018illegitimate\u2019 email, an error that he said has plagued him ever since.\u201d See <a href=\"https://www.nytimes.com/2016/12/13/us/politics/russia-hack-election-dnc.html\"><i>The New York Times</i></a>.<a href=\"https://80000hours.org/career-reviews/information-security/#fn-ref-2\">\u21a9</a></li></ol>", "user": {"username": "80000_Hours"}}, {"_id": "KXSBb2zgkLE6gnn3K", "title": "Don\u2019t Balk at Animal-friendly Results", "postedAt": "2023-01-16T11:52:29.979Z", "htmlBody": "<h1>Key Takeaways</h1><ul><li>The Moral Weight Project assumes hedonism. It also assumes that in the absence of good direct measures of the intensities of valenced experiences, the best way to assess differences in the potential intensities of animals\u2019 valenced experiences is to look at differences in other capacities that might serve as proxies for differences in hedonic potential.</li><li>Suppose that these assumptions lead to the conclusion that chickens and humans can realize roughly the same amount of welfare at any given time. Call this \u201cthe Equality Result.\u201d The key question: Would the Equality Result alone be a good reason to think that one or both of these assumptions is mistaken?</li><li>We don\u2019t think so. To explain why not, we consider three bases for skepticism about the Equality Result. Then, we consider whether the Equality Result should be surprising given hedonism.</li><li>Three Bases for Skepticism<ul><li>First, someone might balk at the implications of the Equality Result given certain independent theses. For instance, given utilitarianism, the Equality Result probably implies that there should be a massive shift in neartermist resources toward animals, and someone might find this unbelievable. But the Equality Result isn\u2019t to blame: utilitarianism is.</li><li>Second, someone might be inclined to accept some theory of welfare that does not support the Equality Result. Fair enough, but&nbsp;<a href=\"/posts/WfeWN2X4k8w8nTeaS/theories-of-welfare-and-welfare-range-estimates\"><u>as we\u2019ve argued elsewhere</u></a>, that only gets you so far.&nbsp;</li><li>Third, someone might balk at the Equality Result even given hedonism. The basic problem with this is that the anti-Equality-Result intuition is uncalibrated, uncalibrated intuitions are vulnerable to various biases, and there are some highly relevant biases in the present context.</li></ul></li><li>If Hedonism, Then the Equality Result Shouldn\u2019t Be Surprising<ul><li>We quickly consider three popular theories valenced states and argue that there are plausible assumptions on which each one leads to the Equality Result.</li><li>This isn\u2019t an argument&nbsp;<i>for&nbsp;</i>the Equality Result. It is, however, a check against knee-jerk skepticism.</li></ul></li></ul><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1673792998/mirroredImages/JipFxm8waxHEsG8Xu/smzvqgwxpih5zjnk7kwa.jpg\">&nbsp;</p><h1>Introduction</h1><p>This is the seventh post in the <a href=\"/s/y5n47MfgrKvTLE3pw\">Moral Weight Project Sequence</a>. The aim of the sequence is to provide an overview of the research that Rethink Priorities conducted between May 2021 and October 2022 on interspecific cause prioritization\u2014i.e., making resource allocation decisions across species.&nbsp;</p><p>As EAs, we want to compare the cost-effectiveness of all interventions, including ones that benefit (sentient) animals with the cost-effectiveness of interventions that benefit humans. To do that, we need to estimate the value of each kind of animal relative to humans. If we understand each individual\u2019s value in terms of the welfare they generate, whether positive or negative, then this means we need to estimate the amount of welfare that each kind of animal realizes relative to the amount of welfare that humans realize.</p><p>How should we react if our method for generating these estimates produces a surprising result? Suppose, for instance, that we make various assumptions and generate a method for estimating how much welfare animals can realize relative to how much welfare humans can realize&nbsp; (which is&nbsp;<a href=\"/posts/tnSg6o7crcHFLc395/the-welfare-range-table\"><u>the first step</u></a> toward estimating how much welfare animals&nbsp;<i>actually&nbsp;</i>realize). And suppose that, when applied, our method suggests that chickens and humans can realize roughly the same amount of welfare at any given time. Call this \u201cthe Equality Result.\u201d Would getting the Equality Result&nbsp;<i>itself&nbsp;</i>be a reason to think we made a mistake?</p><p>Let\u2019s make this concrete. The <a href=\"/s/y5n47MfgrKvTLE3pw\">Moral Weight Project</a> assumes hedonism\u2014i.e., that all and only positively valenced experiences are good for you and all and only negatively valenced experiences are bad for you. Moreover, it assumes that, in the absence of good direct measures of the intensities of valenced experiences, the best way to assess differences in the potential intensities of animals\u2019 valenced experiences\u2014i.e., their \u201chedonic potential\u201d\u2014is to look at differences in other capacities that might serve as proxies for differences in hedonic potential. If the Moral Weight Project delivers the Equality Result, would that alone be a reason to think that either hedonism or our methodology is mistaken?</p><p>We don\u2019t think so. The purpose of this post is to explain why. To do that, we\u2019ll proceed in two stages. First, we\u2019ll consider and respond to three kinds of skepticism about the Equality Result. Then, we\u2019ll offer some reasons to think that, given hedonism, the Equality Result wouldn\u2019t be surprising.</p><p>Here are the three kinds of skepticism:</p><ol><li>Someone might balk at the&nbsp;<i>implications&nbsp;</i>of the Equality Result given certain independent theses. For instance, given utilitarianism, the Equality Result probably implies that there should be a massive shift in neartermist resources toward animals (and among animals, toward fish and invertebrates). Someone might find this unbelievable.&nbsp;</li><li>Someone might balk at the assumptions&nbsp;<i>behind&nbsp;</i>the Equality Result. If they\u2019re inclined to accept some theory of welfare that does&nbsp;<i>not&nbsp;</i>support the Equality Result, then they might reject the Equality Result on that basis.</li><li>Someone might balk at the Equality Result&nbsp;<i>even given hedonism</i>. That is, they might think that&nbsp;<i>even if we assume hedonism</i>, we shouldn\u2019t end up with the Equality Result. They might think it\u2019s just implausible that a chicken can suffer as intensely as a human\u2014or, more positively, that a chicken\u2019s pleasures can be nearly as intense as a human\u2019s.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefk550xrnqgq\"><sup><a href=\"#fnk550xrnqgq\">[1]</a></sup></span></li></ol><p>Then, we\u2019ll discuss some prominent theories of valence, explaining how each one could end up supporting the Equality Result.</p><h1>Three Kinds of Skepticism</h1><p>We begin with the three skeptical reactions.</p><h2>Balking at the Implications</h2><p>In this section, we\u2019ll focus on balking at the implications of the Equality Result. Again, given utilitarianism, the Equality Result probably implies that there should be a&nbsp;<a href=\"https://www.openphilanthropy.org/research/worldview-diversification/\"><u>massive shift</u></a> in neartermist resources toward animals. Someone might find this unbelievable.&nbsp;</p><p>The fundamental problem is that this reaction amounts to shooting the messenger. If the Equality Result \u201chas radical implications,\u201d that isn\u2019t the fault of the Equality Result&nbsp;<i>per se</i>. On its own, it doesn\u2019t have any practical implications. To get some, you have to add\u2014at a minimum\u2014a normative theory like utilitarianism and a lot of empirical assumptions. But then the theory and the empirical assumptions deserve equal scrutiny for the counterintuitive practical implications.</p><p>The natural objection is something like:</p><blockquote><p>We\u2019ve got great evidence for utilitarianism and the relevant empirical assumptions; we don\u2019t have great evidence for the Equality Result. So while it's true that other assumptions deserve&nbsp;<i>some</i> scrutiny for generating counterintuitive practical implications, we shouldn't focus on them. Instead, we should focus on the least-well-confirmed piece of the puzzle.</p></blockquote><p>There are three problems with this objection. The first is circularity. That is, part of the evidence for utilitarianism is just its fit with common intuitions about what\u2019s right and wrong.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefx2tbgybacjg\"><sup><a href=\"#fnx2tbgybacjg\">[2]</a></sup></span>&nbsp;But if utilitarianism only fits with common intuitions&nbsp;<i>given that animals have much smaller </i><a href=\"/posts/hxtwzcsz8hQfGyZQM/an-introduction-to-the-moral-weight-project\"><i>welfare ranges</i></a><i> than humans </i>(i.e., given that the difference between the best and worst welfare states that animals can realize is much smaller than the difference between the best and worst welfare states that humans can realize), then this objection is assuming what needs to be established. And once we notice this, it\u2019s less plausible that utilitarianism is off the hook when it comes to counterintuitive practical implications.&nbsp;</p><p>Of course, if it were\u2014or ought to be\u2014<i>obvious&nbsp;</i>that animals have much smaller welfare ranges than humans, then assuming as much would be fine. But it isn\u2019t. One way to appreciate this is to note that many theories of welfare seem to imply that humans and animals have the same welfare ranges\u2014e.g., any theory where flourishing is assessed relative to species-specific standards, as in&nbsp;<a href=\"https://global.oup.com/academic/product/perfectionism-9780195101164?cc=us&amp;lang=en&amp;\"><u>perfectionism</u></a>. We get the same result from views that measure welfare in proportional rather than absolute terms: consider, for instance, a simple version of desire satisfactionism on which welfare is determined by the ratio of the percentage of satisfied desires to the percentage of unsatisfied desires. More radically, it\u2019s possible that&nbsp;<a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1111/papq.12362\"><u>variablism</u></a> is true, according to which different theories of welfare are true of different welfare subjects, which is most naturally developed in a way that supports the Equality Result. Moreover, note that none of these theories was&nbsp;<i>designed&nbsp;</i>to produce the Equality Result; it just falls out of independently-motivated parts of each view.&nbsp;</p><p>In any case, there\u2019s a second, methodological problem with this objection. Essentially, it amounts to casting doubt on a broadly&nbsp;<i>empirical&nbsp;</i>conclusion\u2014that particular animals have certain capacities that allow them to realize some amount of welfare\u2014based on a&nbsp;<i>moral</i> conclusion. But we wouldn\u2019t accept that inference in other contexts. It would be perverse to deny that people are dying of preventable diseases because, if they were, we might have strenuous obligations to provide aid. Likewise, insofar as welfare ranges are assessable via empirical methods, it seems perverse to deny that animals have particular welfare ranges based on the moral consequences of recognizing them.</p><p>Granted, differences in welfare ranges aren\u2019t value-free: if there are any such differences, they\u2019re generated by variation in animals\u2019 ability to realize the determinants of welfare\u2014i.e., given hedonism, the intensities of valenced states. But once we fix the theory of welfare, differences in welfare ranges should be grounded in empirical differences, not philosophical ones. Of course, someone could reject hedonism. That, however, isn\u2019t the objection we\u2019re considering here. Instead, we\u2019re considering the objection that the Equality Result is implausible because of its moral implications. If those moral implications follow from the best empirical evidence available, then that seems like the wrong kind of authority to grant to our moral judgments.</p><p>Granted again, a methodology for assessing welfare ranges isn\u2019t philosophically innocent: it includes various assumptions about what would constitute good evidence for differences in the intensities of valenced states across species. Here again, though, it\u2019s strange to think that we should question&nbsp;<i>methodological&nbsp;</i>decisions because they lead to uncomfortable practical results. If the methodology is broken, it\u2019s broken on its own terms\u2014not because it\u2019s part of an argument for allocating more money to animals.</p><p>The third problem\u2014which goes back to the original, \u201cDon\u2019t shoot the messenger\u201d response\u2014is that if we\u2019re singling out a party for blame, it ought to be utilitarianism. After all, utiltarianism\u2019s consequentialism makes it outcomes-focused, its welfarism makes welfare realization its exclusive focus, its impartiality makes it impossible to bracket the concerns of animals, and its aggregationism allows the sheer number of animals to do the rest. We\u2019re probably in for radical practical implications&nbsp;<i>even if&nbsp;</i>we have big differences in welfare ranges. To be clear: for all that, utilitarianism may be the best theory around! So, we aren\u2019t suggesting that radical practical implications are a strike against utilitarianism. Instead, we\u2019re suggesting that if radical practical implications are the problem, then the Equality Result is probably the wrong place to focus.</p><p>The upshot is that \u201cDon\u2019t shoot the messenger\u201d remains the right response to someone balking at the practical results of the Equality Result. If you don\u2019t like those results, there are lots of other places to look.&nbsp;</p><h2>Balking at the Assumptions Behind the Equality Result</h2><p>Second, someone might balk at the assumptions&nbsp;<i>behind&nbsp;</i>the Equality Result. In particular, someone might be inclined to accept some other theory of welfare that does&nbsp;<i>not&nbsp;</i>support the Equality Result.</p><p>Of course, as we\u2019ve just seen, the Equality Result doesn\u2019t<i> depend</i> on hedonism: it\u2019s supported by\u2014or at least compatible with\u2014many theories of welfare and one meta-theory of welfare. But let\u2019s set that aside, as surely there are theories of welfare that wouldn\u2019t lead to the Equality Result.</p><p>This brings us to a concession and a retrenchment. Perhaps unsurprisingly, our view is that&nbsp;<i>if&nbsp;</i>you have good reason to accept some theory of welfare that doesn\u2019t support the Equality Result, then you also have good reason not to believe the Equality Result. That\u2019s the concession.</p><p>That being said, there are two points to note. First,&nbsp;<a href=\"/posts/WfeWN2X4k8w8nTeaS/theories-of-welfare-and-welfare-range-estimates\"><u>as we\u2019ve argued elsewhere</u></a>, we doubt that abandoning hedonism will change the bottom line all that much. While hedonic goods and bads aren\u2019t all of welfare, they\u2019re surely a large part of it.</p><p>Second, note the conditional here: \u201c<i>if&nbsp;</i>you have good reason to accept some theory of welfare that doesn\u2019t support the Equality Result.\u201d If you\u2019re just reacting against the Equality Result itself\u2014without any theoretical backing\u2014then it\u2019s crucial to assess the evidential value of the anti-Equality-Result intuition. Let\u2019s do that in the context of assessing the third reason someone might balk at the Equality Result.</p><h2>Balking at the Equality Result Even Given the Assumptions that Support It</h2><p>Third, then, someone might balk at the Equality Result even&nbsp;<i>given&nbsp;</i>the assumptions that ostensibly support it. That is, they might think that&nbsp;<i>even if we assume hedonism</i>, we shouldn\u2019t end up with the Equality Result. They might think it\u2019s just implausible that a chicken can suffer as intensely as a human\u2014or, more positively, that a chicken\u2019s pleasures can be nearly as intense as a human\u2019s.</p><p>The basic problem with this is that the intuition is uncalibrated, uncalibrated intuitions are vulnerable to various biases, and there are some highly relevant biases in the present context.</p><p>Intuitions are&nbsp;<i>calibrated&nbsp;</i>when they\u2019re checked against some independent standard. Forecasters, for instance, calibrate their intuitions by making forecasts, seeing how they resolve, and learning from their successes and failures. But intuitions about hedonic capacity\u2014the relative potential intensities of valenced states across organisms\u2014aren\u2019t going to \u201cresolve\u201d anytime soon; we\u2019re a long way from a rigorous science of interspecies pain intensity comparisons.</p><p>Moreover,&nbsp;there\u2019s a large literature on how uncalibrated intuitions&nbsp;can be affected by irrelevant factors.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyi5d7ycq3\"><sup><a href=\"#fnyi5d7ycq3\">[3]</a></sup></span>&nbsp;We won\u2019t review it here. Instead, let\u2019s consider some of the irrelevant factors that could affect intuitions about chickens specifically.</p><p>First, people can have the wrong target in mind. Most obviously, it\u2019s easy to confuse greater&nbsp;<i>diversity&nbsp;</i>of valenced states with greater&nbsp;<i>intensity</i>. It\u2019s probably true that humans have some affective states that chickens lack. Moreover, those human-specific affective states may be tied to aspects of human life that people value, such as human friendship, romantic love, parenthood, and so on. Still, unique affective states aren\u2019t therefore more intense. The pain of losing a friend is significant, but&nbsp;<i>at any given moment</i>, it isn\u2019t necessarily more intense than, say, the pain of breaking a limb. But those peaks are what matter for assessing welfare ranges.&nbsp;</p><p>Second, people can mistakenly think that:</p><ul><li>I\u2019m unable to imagine what it\u2019s like to be a chicken.</li></ul><p>\u2026is good evidence for:</p><ul><li>When chickens feel pain, it\u2019s very different from what I feel.&nbsp;</li></ul><p>\u2026and then it\u2019s a short step to:</p><ul><li>Chickens\u2019 pain isn\u2019t as intense as my pain.</li></ul><p>When we make the inferences explicit, the problems are obvious. There are lots of differences between humans and animals that make it difficult to imagine their inner lives. This isn\u2019t a reason to think that their pains have an entirely different character or that their pains are any less painful. Indeed, one feature of excruciating pain is how consuming it is: it \u201cdrowns out\u201d all our other cognitive abilities, which suggests that when it comes to pain, our experiences may be most similar to those of<i>&nbsp;</i>other animals. Moreover, when it comes to the second inference,&nbsp;<i>different&nbsp;</i>isn\u2019t the same as&nbsp;<i>less intense</i>.</p><p>Third,&nbsp;people seem to be influenced even by the fact that some animals are&nbsp;<i>categorized&nbsp;</i>as consumable.&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S0195666311001413\"><u>Bratanova et al. (2011)</u></a>, for instance, told participants about a marsupial native to Papa New Guinea, suggesting to some that the marsupial is a food animal that humans hunt; they didn\u2019t give such hints to the control group. This difference alone was enough to lead participants to judge that the animal is less likely to have the capacity to suffer and less deserving of moral concern. (For related results, see&nbsp;<a href=\"https://journals.sagepub.com/doi/full/10.1177/0146167211424291\"><u>Bastian et al. (2012)</u></a>,&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S0195666310003648\"><u>Loughnan, Haslam, and Bastian (2010)</u></a>,&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S0195666312001201\"><u>Ruby and Heine, 2012</u></a>, and&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S0195666316302768\"><u>Kunst and Hohle, 2016</u></a>.)</p><p>In short, we don\u2019t have widely-accepted empirical reasons to think that other animals have less intense pleasures and pains than humans do. So, while we might have intuitions about the intensities of their valenced states, there\u2019s no particular reason to trust them, as they aren\u2019t calibrated. Moreover, there are positive reasons&nbsp;<i>not&nbsp;</i>to trust them, as there are various clear ways that these intuitions could go wrong.&nbsp;</p><h1>If Hedonism Is True, Then the Equality Result Shouldn\u2019t Be Surprising</h1><p>In what remains, we\u2019ll explain why the Equality Result wouldn\u2019t be surprising if hedonism is true. This is <i>not</i> the same as arguing that if hedonism is true, then we ought to&nbsp;<i>expect&nbsp;</i>the Equality Result. The point here is more modest. We\u2019re only arguing that the Equality Result wouldn\u2019t be particularly notable given hedonism. In other words, conditional on hedonism but in advance of inquiry, we shouldn\u2019t assign a low credence the hypothesis that humans and chickens have the same welfare ranges.&nbsp;</p><p>We assume that valenced experiences didn\u2019t pop into existence: they evolved because they\u2019re adaptive for organisms that have them. So, if there are differences in the possible intensities of valenced states across organisms, then, with various caveats, we should be able to explain those differences in terms of their adaptive value. Our basic suggestion is that there are plausible sets of hypotheses about the adaptive value of valenced states on which there aren\u2019t any differences at all\u2014hence, the Equality Result.</p><p>To see this, let\u2019s consider three popular theories about the function of valenced experiences: they represent fitness-relevant information in a motivationally-salient way (<a href=\"https://www.jstor.org/stable/41329419\"><u>Cutter &amp; Tye 2011</u></a>); they provide a common currency for decision making (<a href=\"https://mitpress.mit.edu/9780262039307/the-evolution-of-the-sensitive-soul/\"><u>Ginsburg &amp; Jablonka 2019</u></a>); they facilitate learning (<a href=\"https://www.nature.com/articles/nrn3403\"><u>Damasio &amp; Carvalho 2013</u></a>). These theories don\u2019t just tell us what valenced states actually do for organisms that have them. Additionally, they suggest&nbsp;<i>etiologies&nbsp;</i>for valenced states. If the representational view is true, then, probably, organisms that could represent fitness-relevant information were more successful than organisms without that ability because the former, and not the latter, could act on that information. Likewise, if the common currency theory is true, then, probably, organisms with a single currency for decision making were more successful than organisms without that didn\u2019t have a single currency. And so on.</p><p>The question, then, is whether these functions or etiologies could plausibly deliver the Equality Result. We think they could.</p><h2>Representation</h2><p>If the representational theory is true, then it might seem obvious that there would be variation in the abilities of organisms to represent fitness-relevant information. Presumably, humans can represent much more fitness-relevant information than chickens. However, there are two reasons why we shouldn\u2019t make too much of this.</p><p>First, it doesn\u2019t seem plausible that the intensities of pains scale directly with representational abilities. When we consider our own experiences, it\u2019s notable that our pains don\u2019t obviously provide us with information that other animals lack when they\u2019re in pain. Do we learn more about the&nbsp;<i>location</i> of damage when we\u2019re in pain? Do we learn more about the&nbsp;<i>kind&nbsp;</i>of damage? If not, then why would we expect to learn much more about the&nbsp;<i>severity&nbsp;</i>of the threat to fitness\u2014which, presumably, is what the information about intensity conveys?&nbsp;</p><p>Second, even if we were to think that humans have more sophisticated pain perception&nbsp;<i>without&nbsp;</i>increasing intensity, it\u2019s an open question whether cognitive sophistication allows organisms to make more fine-grained distinctions&nbsp;<i>within an intensity range</i> (call this the \u201ccompression hypothesis\u201d) rather than increasing the intensity range of their experiences (call this the \u201cexpansion hypothesis\u201d\u2014names due to Henry Shevlin). That is, it could work out that the worst pain for any given sentient organism is as intense as the worst pain for any other sentient organism. However, perhaps some organisms can distinguish 99 grades of pain that fall short of that worst form of suffering, whereas other organisms can only distinguish 3. On such a view, representational sophistication adds nuances to an intensity scale; it doesn\u2019t push the endpoints further from one another.</p><h2>Common currency</h2><p>Let\u2019s now consider the decision-making theory, where valenced states help organisms rank some options (e.g., fleeing) as better than others (e.g., fighting) in a motivationally-salient way (<a href=\"https://mitpress.mit.edu/9780262039307/the-evolution-of-the-sensitive-soul/\">Ginsburg &amp; Jablonka 2019</a>). However, organisms don\u2019t need to be able to rank all possible options at a given time; they need to be able to represent some salient set of them. How could we estimate the number of such options and, therefore, the intensity range of organisms\u2019 valenced experiences? One possibility is to use evidence from the working memory literature about how many units of information organisms can keep in mind simultaneously. Roughly, working memory is a cognitive system that temporarily makes limited amounts of information available for tasks that require executive control. For humans, that number may be as low as four units (<a href=\"https://www.taylorfrancis.com/books/mono/10.4324/9780203342398/working-memory-capacity-nelson-cowan\"><u>Cowan 2005</u></a>), which doesn\u2019t leave much room for variation between humans and chickens. So, if the evidence were to support the Equality Result, it wouldn\u2019t be that surprising.&nbsp;</p><h2>Learning</h2><p>Finally, there\u2019s the learning theory. On this view, the \u201ckey function [of pain] appears to be that the aversive experience of pain creates a strong and lasting motivation that enables the animal to avoid getting into a similar situation in the future\u201d (<a href=\"https://doi.org/10.1016/j.anbehav.2014.09.007\">Sneddon et al. 2014</a>, 202). Correspondingly, a key function of pleasure is to create a strong and lasting motivation to get into similar situations in the future. Given this theory, we might expect some variation in the intensity of valenced states based on how much animals stand to gain, fitness-wise, from having such strong and lasting motivations.</p><p>However, it isn\u2019t at all clear that chickens have less to gain, fitness-wise, from strong and lasting motivations. This is a difficult empirical question that it would take considerable work to assess. Moreover, it\u2019s important to remember that any plausible theory of humans\u2019 success is going to invoke many other abilities&nbsp;<i>besides&nbsp;</i>having strong and lasting motivations. If anything, those abilities may&nbsp;<i>reduce&nbsp;</i>the need for particularly intense states rather than increase it, as humans can reason about the value of avoiding future pains and securing future pleasures in a way that chickens, presumably, cannot. So, chickens might need&nbsp;<i>more&nbsp;</i>intense states to accomplish that same aims\u2014a hypothesis entertained by more than one evolutionary biologist (see, e.g.,&nbsp;<a href=\"https://www.penguinrandomhouse.com/books/556690/science-in-the-soul-by-richard-dawkins/\"><u>Dawkins 2017</u></a> and&nbsp;<a href=\"https://nautil.us/even-worms-feel-pain-238436/\"><u>Barash 2022</u></a>).</p><h2>Granting What We\u2019ve Criticized</h2><p>Nevertheless, let\u2019s suppose that as representational capacity, working memory, and long-term memory increase, the potential intensity of valenced states increases. Why would this be adaptive? There are two basic stories. One of them is that the mechanisms that produce valenced states are somehow bound up with these capacities, such that it\u2019s inherently the case that these changes produce more intensely valenced states. That could be true, but there\u2019s no obvious reason to believe it. So, we set it aside.</p><p>The other possibility is that there\u2019s some link between those abilities and the kinds of threats to fitness that those abilities help organisms ward off, such that organisms with these abilities developed more intensely valenced states to help them avoid those threats. But that isn\u2019t what we see. Across species, the most intense pains are associated with bodily damage, hunger, thirst, and other threats to survival; the most intense pleasures are associated with food, sex, play, and other obvious boons to survival. We don\u2019t see an obvious shift in the&nbsp;<i>objects&nbsp;</i>of the most intense valenced states\u2014the things that organisms\u2019 valenced states are about\u2014which, presumably, is what we\u2019d get if nature were introducing new intensity levels to take advantage of new cognitive abilities. It isn\u2019t the case that people find it especially intensely painful&nbsp;<i>now</i>to do things that will lead to them failing to produce offspring&nbsp;<i>later</i>, or that they find it especially intensely painful&nbsp;<i>now&nbsp;</i>to neglect responsibilities that would ensure adequate food&nbsp;<i>later</i>.&nbsp;</p><p>Granted, these kinds of failures can be negatively valenced. But that isn\u2019t enough. The hypothesis in question is that these capacities created a selection pressure for more intensely valenced states because it was now advantageous to have those new intensity levels. The point, essentially, is that if those new intensity levels were advantageous because of the relevant abilities, we would expect those states to be somehow associated with the use of those abilities. But that isn\u2019t what we see.&nbsp;</p><h1>Conclusion</h1><p>If the Moral Weight Project were to deliver the Equality Result, would that be a reason to think that there\u2019s a mistake somewhere up the chain? In short, no. It\u2019s true that if you have good reason to accept some theory of welfare that doesn\u2019t support the Equality Result, then you also have good reason not to believe the Equality Result. However, we suspect that that situation is relatively rare. Most people will balk at either the practical implications of the Equality Result or at the Equality Result itself. But the practical implications aren\u2019t really the Equality Result\u2019s fault; you should blame utilitarianism if you don\u2019t like them. And while the Equality Result may be counterintuitive, there are plenty of reasons not to make very much of its being counterintuitive. That anti-Equality-Result intuition isn\u2019t calibrated, and so may well be vulnerable to significant biases.</p><p>On top of all that, conditional on hedonism, we shouldn\u2019t assign a particularly low credence to the Equality Result. There are popular theories of valence that seem to fit fairly naturally with it. So, while that isn\u2019t an argument&nbsp;<i>for&nbsp;</i>the Equality Result, it\u2019s a check against knee-jerk skepticism.</p><p>Finally, let\u2019s be clear: we are&nbsp;<i>not&nbsp;</i>claiming that the Equality Result is correct. Instead, our claim is that&nbsp;<i>given the assumptions behind the Moral Weight Project</i> (<a href=\"/posts/WfeWN2X4k8w8nTeaS/theories-of-welfare-and-welfare-range-estimates\"><u>and perhaps even without them</u></a>), we shouldn\u2019t flinch at \u201canimal-friendly\u201d results.&nbsp;</p><p>&nbsp;</p><h1>Acknowledgments</h1><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670897894/mirroredImages/tnSg6o7crcHFLc395/cf4ofw1ukduucvmcbdh6.png\"></figure><p>This research is a project of Rethink Priorities. It was written by Bob Fischer. Thanks to Marcus Davis, Jason Schukraft, Adam Shriver, and Michael St. Jules for helpful feedback on earlier versions of this post. If you\u2019re interested in RP\u2019s work, you can learn more by visiting our&nbsp;<a href=\"https://www.rethinkpriorities.org/research\">research database</a>. For regular updates, please consider<i>&nbsp;</i>subscribing to our<a href=\"https://www.rethinkpriorities.org/newsletter\"> newsletter</a>.</p><p><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnk550xrnqgq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefk550xrnqgq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Granted, these aren\u2019t the only possible ways of resisting the Equality Result. Someone might be convinced that&nbsp;<a href=\"/posts/Mfq7KxQRvkeLnJvoB/why-neuron-counts-shouldn-t-be-used-as-proxies-for-moral\"><u>neuron counts</u></a> are an excellent proxy for relative moral weights, that brains support many&nbsp;<a href=\"/posts/vbhoFsyQmrntru6Kw/do-brains-contain-many-conscious-subsystems-if-so-should-we\"><u>conscious subsystems</u></a>, or that there are neurological reasons to think that some animals are basically&nbsp;<a href=\"/posts/WBWXLZTF5FPzmK8nh/octopuses-probably-don-t-have-nine-minds\"><u>split-brain patients</u></a>. However, we've discussed those possibilities elsewhere and we bracket them here.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnx2tbgybacjg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefx2tbgybacjg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Granted, some think that fitting with intuitions about cases is a relatively small part of the case for utilitarianism, if a factor at all. Many EAs, for instance, might think that it's a good option among plausibly fully describable (though not actually described in full) expected-value-maximizing options, which they support based on views about the requirements of rationality, the nature of moral patienthood, and so on. Still, this leaves the remaining objections untouched. Thanks to Marcus Davis for this point.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyi5d7ycq3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyi5d7ycq3\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For an overview of the heuristics and biases literature, see <a href=\"https://us.macmillan.com/books/9780374533557\">Kahneman 2011</a>. For philosophical examples, see <a href=\"https://doi.org/10.1016/j.cognition.2003.10.003\">Machery et al. 2004</a>;&nbsp;<a href=\"https://doi.org/10.1111/j.1933-1592.2007.00118.x\">Swain, Alexander, &amp; Weinberg 2008</a>;&nbsp;<a href=\"https://books.google.com/books?hl=en&amp;lr=&amp;id=IVLSAQAAQBAJ&amp;oi=fnd&amp;pg=PA307&amp;dq=Gender+and+Philosophical+Intuition&amp;ots=dbEgsYUBr4&amp;sig=gAhZkw6VrDXmCRHMVVGRTLSjGkc#v=onepage&amp;q=Gender%20and%20Philosophical%20Intuition&amp;f=false\">Buckwalter &amp; Stich 2014</a>; <a href=\"https://doi.org/10.1371/journal.pone.0094842\">Costa et al. 2014</a>,&nbsp;<a href=\"https://doi.org/10.1111/j.1468-0017.2012.01438.x\">Schwitzgebel &amp; Cushman 2012</a>, and&nbsp;<a href=\"https://doi.org/10.1080/00048402.2014.967792\">De Cruz 2015</a>.</p></div></li></ol>", "user": {"username": "bob-fischer"}}, {"_id": "nJz6vaqK7xBDXpMwk", "title": "Aligning the Aligners: Ensuring Aligned AI acts for the common good of all mankind", "postedAt": "2023-01-16T11:13:15.574Z", "htmlBody": "<p>The argument I\u2019m going to make here is obviously not original with me, but I do think that while AI governance is talked about, and one of the career paths that 80,000 hours recommends, it is not sufficiently pointed to as an important cause area. Further, I think the specific issue that I\u2019m pointing to is even more neglected than AI governance generally.</p><p>&nbsp;</p><p>Suppose we create a human superior AI that does what we tell it to.</p><p>This creates the possibility of us immediately transforming into a post scarcity economy. Nobody needs to work. Everybody gets as much as they want of the stuff that used to be scarce due to limited human labor.</p><p>This would be amazing!</p><p>Each of us could own a yacht, or two. The limit on how many yachts we can have is space on the water, not how expensive the boat is. We can own as many clothes as we want. If anyone wants an authentic 18th century costume, it can be hand sewn immediately.&nbsp;</p><p>If anyone gets sick, they will be able to have access to a flawless surgeon using the most advanced medical knowledge. They also won\u2019t need to wait for an appointment to talk to a doctor. Every single child will have access to private tutors who know as much as every tutor that any child has had until the present put together.</p><p>An actual super intelligence could give us delicious zero suffering meat, probably a malaria vaccine, it could eliminate either all, or nearly all childhood diseases.</p><p>AI could create a world where nobody needs to desperately work and hustle just to get by, and where no one ever needs to worry about whether there will be food and shelter.</p><p>Etc, etc, etc.</p><p>There is a reason that techno-optimist transhumanists think friendly AI can create an amazingly good world.&nbsp;</p><p>&nbsp;</p><p>So, let's for a moment assume Deepmind, Open AI or a black project run by the Chinese government successfully creates a super human intelligence that does what they ask, and which will not betray them.</p><p>Does this super awesome world actually come into existence?</p><p><br>&nbsp;</p><p>Reasons to worry (definitely not exhaustive!)</p><p><br>&nbsp;</p><ul><li>Power corrupts<ul><li>Capabilities researchers have been reading about how awesome singletons are.<ul><li>Even worse, some of the suits might have read about it too. Sergey and Elon definitely have.</li></ul></li></ul></li><li>The Leftist critique: A limited number of powerful people making decisions about the fate of people who have no say in it. They will treat our collective human resources as their own private toys.<ul><li>I take this issue very seriously.</li><li>A good world is a world where everyone has control over their own fate, and is no longer at the mercy of impersonal forces that they can neither understand nor manipulate.&nbsp;</li><li>Further a good world is one in which people in difficult, impoverished, and non normative circumstances are able to make choices to make their lives go well, as they see it.</li></ul></li><li>The Nationalism problem<ul><li>Suppose AI developed in the US successfully stays under democratic control. And it is used purely to aggrandize the wealth and well being of Americans, by locking in America\u2019s dominance of all resources in the solar system and the light cone, forever.<ul><li>Poor Malawians are still second or third class citizens on earth, and are still only receiving the drips of charity from those who silently consider themselves their betters.</li><li>We could have fixed poverty forever instead.</li></ul></li><li>Suppose AI is developed in China&nbsp;<ul><li>They establish a regime with communist principles and social control over communication everywhere on the planet. This regime keeps everyone, everywhere, forever parroting communist slogans.</li></ul></li><li>Worse: Suppose they don\u2019t give any charity to the poor? There is precedent for dominant groups to simply treat the poor around them as parasites or work animals. Perhaps whoever controls the AI will starve or directly kill all other humans.</li></ul></li></ul><p><br>&nbsp;</p><p>Summary: Misaligned people controlling the AI would be bad.</p><p>&nbsp;</p><p>This issue is connected to the agency problem (of which aligning AI itself is an example).&nbsp;</p><ul><li>How do we make sure that the people or institutions in power act with everyone's best interests in mind?</li><li>What systems can we put in place to hold the people/institutions in power accountable to the promises they make?</li><li>How do we shape the incentive landscape in such a way that the people/institutions in power act to maximise wellbeing (while not creating adverse effects)?</li><li>How do we make sure we give power only to the actors who have a sufficient understanding of the most pressing issues and are committed to tackling them?</li></ul><p><br>&nbsp;</p><p>So what can we do about this? Two approaches that I have heard about:</p><p><br>&nbsp;</p><ul><li>Limiting financial upside of developing AI to a finite quantity that is small relative to the output of a dyson swarm.<ul><li>Windfall clauses</li><li>Profit capping arrangements like I think Open AI has</li><li>Ad hoc after the fact government taxes and seizures</li></ul></li><li>The Moon Treaty, and giving the whole global community collective ownership of outerspace resources</li><li>Make sure that if AI is developed, it only comes out of a limited number of highly regulated and government controlled entities where part of the regulatory framework ensures a broad distribution of the benefits to at least the citizens of the country where it was constructed. This centralization might also have substantial safety benefits.</li></ul><p>&nbsp;</p><p><br>&nbsp;</p><p>The problem with any approach to AI control after it is developed is that we cannot trust the legal system to constrain the behavior of someone in control of a singleton following a fast take off scenario. There need to be safeguards embedded in these companies that are capable of physically forcing the group that built the AI to do what they promised to do with it, and these regulations need to be built into the structure of how any AI that might develop into a singleton is trained and built.</p><p>This should be part of the AI safety regulatory framework, and might be used as part of what convinces the broader public that AI safety regulation is necessary in the first place (it would actually be bad, even if you are a libertarian, if AI is just used to satisfy the desires of rich people).&nbsp;</p><p>All of this only becomes a problem if we actually solve the general alignment problem of creating a system that does what its developers actually want it to do. What you think the p(AGI Doom) is will drive whether you think this is worth working on.&nbsp;</p><p>This also is an effective and possibly tractable place to focus on systemic change. A world system that ensures that everyone gets a sufficient share of the global resources to meet their needs after full automation likely will require major legal and institutional changes, possibly of the same magnitude as a switch to communism or anarcho capitalism would require.</p><p>The value of improving a post aligned AI future is multiplied by the possibility that we actually reach that future. So if you think that the odds are 1/million that AI is safely developed, the expected value from efforts in this direction is far lower than if you believe the odds of AI killing us all are 1/million.&nbsp;</p><p>But if we meet that (possibly unlikely) bar of not dying from AI, there will still be more work needed to be done to create utopia.</p><p><i>I'd like to thank Milan, Laszlo, Marta, Gergo, Richard and David for their comments on the draft text of this essay.</i></p><p>&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "timunderwood"}}, {"_id": "HqfBZuhKmddESyTKP", "title": "How should EA navigate the divide between those who prioritise epistemics vs. social capital?", "postedAt": "2023-01-16T06:31:57.194Z", "htmlBody": "<p>Recent events seem to have revealed a central divide within Effective Altruism.</p><p>On one side, you have the people<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh2qqyhi1h9\"><sup><a href=\"#fnh2qqyhi1h9\">[1]</a></sup></span>&nbsp;who want EA to prioritise epistemics on the basis that if we let this slip, we'll eventually end up in a situation where our decisions will end up being what's popular rather than what's effective.</p><p>On the other side, you have the people who are worried that if we are unwilling to trade-off&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqabwzatx9n\"><sup><a href=\"#fnqabwzatx9n\">[2]</a></sup></span>&nbsp;epistemics at all, we'll simply sideline ourselves and then we won't be able to have any significant impact at all.</p><ol><li>How should we navigate this divide?</li><li>Do you disagree with this framing? For example, do you think that the core divide is something else?</li><li>How should cause area play into this divide? For example, it appears to me, that those who prioritise AI Safety tend to fall into the first camp more often and those who prioritise global poverty tend to fall into the second camp. Is this a natural consequence of these prioritisation decisions or is this a mistake?</li></ol><p><strong>Update</strong>: A lot of people disliked the framing which seems to suggest that I haven't found the right framing here. Apologies, I should have spent more time figuring out what framing would have been most conducive to moving the discussion forwards. I'd like to suggest that someone else should post a similar question with framing that they think is better (although it might be a good idea to wait a few days or even a week).</p><p>In terms of my current thoughts on framing, I wish I had more explicitly worded this as \"saving us from losing our ability to navigate\" vs. \"saving us from losing our ability to navigate\". After reading the comments, I'm tempted to add a third possible highest priority: \"preventing us from directly causing harm\".</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnh2qqyhi1h9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefh2qqyhi1h9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I fall more into this camp, although not entirely as I do think that a wise person will try to avoid certain topics insofar as this is possible.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqabwzatx9n\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqabwzatx9n\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I've replaced the word \"compromise\" based on feedback that this word had a negative connotation.</p></div></li></ol>", "user": {"username": "casebash"}}, {"_id": "f2bR6HgABuych2XAF", "title": "Now that we trust senior EAs less...", "postedAt": "2023-01-15T21:46:50.025Z", "htmlBody": "<p>This is as good a time as ever to flag potential reforms to <strong>distribute power</strong>, <strong>take tail risks to EA's reputation and impact seriously</strong> and leave our movement's impact less to the mercy of poor, unilateral decision making:</p><p><a href=\"https://forum.effectivealtruism.org/posts/SBSC8ZiTNwTM8Azue/a-libertarian-socialist-s-view-on-how-ea-can-improve\">https://forum.effectivealtruism.org/posts/SBSC8ZiTNwTM8Azue/a-libertarian-socialist-s-view-on-how-ea-can-improve</a></p><p><a href=\"https://docs.google.com/document/d/1Y9opezUk9_JNQBYCAmHoERiqlaf2HmashQQ_ydGwyRY/edit\">https://docs.google.com/document/d/1Y9opezUk9_JNQBYCAmHoERiqlaf2HmashQQ_ydGwyRY/edit</a></p>", "user": {"username": "freedomandutility"}}, {"_id": "KB8XPfh7dJ9uJaaDs", "title": "Does EA understand how to apologize for things?", "postedAt": "2023-01-15T19:14:26.842Z", "htmlBody": "<p>In response to the drama over Bostroms <a href=\"https://nickbostrom.com/oldemail.pdf\">apology for an old emai</a>l, the original email has been universally condemned from all sides. But I've also seen some confusion over why people dislike the apology itself. After all, nothing in the apology was technically inaccurate, right? What part of it do we disagree with?</p><p>Well, I object to it because it was <i>an apology</i>. And when you grade an apology, you don't grade it on the factual accuracy of the scientific claims contained within, you grade it on <i>how good it is at being an apology</i>. And to be frank, this was probably one of the worst apologies I have ever seen in my life, although it has since been topped by Tegmark's awful <a href=\"https://forum.effectivealtruism.org/posts/5vFmMXWsh6PaYjqab/?commentId=Dn88oPPMdDK9dpmjh\">non-apology</a> for the far right newspaper affair.&nbsp;</p><p>Okay, let's go over the rules for an apology to be genuine and sincere. I'll take them <a href=\"https://www.health.harvard.edu/blog/the-art-of-a-heartfelt-apology-2021041322366\">from here</a>.&nbsp;</p><ol><li><strong>Acknowledge the offense.</strong></li><li><strong>Explain what happened.</strong></li><li><strong>Express remorse.</strong></li><li><strong>Offer to make amends.</strong></li></ol><p>Notably missing from this list is step 5: <strong>Go off on an unrelated tangent about eugenics.</strong></p><p>Imagine if I called someone's mother overweight in a vulgar manner. When they get upset, I compose a long apology email where I apologize for the language, but then note that I believe it is factually true their mother has a BMI substantially above average, as does their sister, father, and wife. Whether or not those claims are factually true <i>doesn't actually matter</i>, because bringing them up at all is unnecessary and further upsets the person I just hurt.&nbsp;</p><p>In Bostroms email of 9 paragraphs, he spends 2 talking about the historical context of the email, 1 talking about why he decided to release it, 1 actually apologizing, and the remaining 5 paragraphs giving an overview of his current views on race, intelligence, genetics, and eugenics.&nbsp;</p><p>What this betrays is an extreme lack of empathy for the people he is meant to be apologizing to. Imagine if he was reading this apology out loud to the average black person, and think about how uncomfortable they would feel by the time he got to part discussing his papers about the ethics of genetic enhancement.&nbsp;</p><p>Bostroms original racist email did not mention racial genetic differences or eugenics. They should not have been brought up in the apology either. As a direct result of him bringing the subject up, this forum and others throughout the internet have been filled with race science debate, an outcome that I believe is <a href=\"https://forum.effectivealtruism.org/posts/frcAPFXwiCpNrECgQ/a-general-comment-on-discussions-of-genetic-group?commentId=RwrdFc5QPLndyeees\">very harmful</a>. Discussions of racial differences are divisive, bad PR, probably result in the <a href=\"https://forum.effectivealtruism.org/posts/kuqgJDPF6nfscSZsZ/thread-for-discussing-bostrom-s-email-and-apology?commentId=Z7kyiPAfmtztueFFJ\">spread of harmful beliefs</a>, and are completely irrelevant to top EA causes. If Bostrom didn't anticipate that this outcome would result from bringing the subject up, then he was being hopelessly naive.&nbsp;</p><p>On the other hand, Bostroms apology looks absolutely saintly next to the FLI's/Max Tegmarks <a href=\"https://forum.effectivealtruism.org/posts/5vFmMXWsh6PaYjqab/?commentId=Dn88oPPMdDK9dpmjh\">non-apology</a> &nbsp;for the initial approval of grant money to a far-right newspaper (the funding offer was later rescinded). At no point does he offer any understanding at all as to why people might be concerned about approving, even temporarily, funding for a far-right newspaper that promotes holocaust denial, covid vaccine conspiracy theories, and defending \"ethnic rights\".&nbsp;</p><p>I don't even know what to say about this statement. The FLI has managed to fail at point 1 of an apology: understanding that they did something wrong. I hope they manage to release a real apology soon, and when they do, maybe they can learn some lessons from previous failures.&nbsp;</p><p>Edit: the FLI have since <a href=\"https://forum.effectivealtruism.org/posts/5vFmMXWsh6PaYjqab/?commentId=Dn88oPPMdDK9dpmjh\">further revised their statement</a>, acknowledging that the newspaper was nazi in nature and expressing regret for the initial funding decision. They're still leaving up the misleading statement on government funding (which was ideology independent), and the overall tone still seems to lack understanding of why people are so shocked that they came anywhere close to working with a full on pro-nazi org. However, I think this at least upgrades their statement from a non-apology to a bad apology, which is a step in the right direction at least. Hopefully they'll get there eventually. &nbsp;</p><p>Second edit: The FLI have released <a href=\"https://futureoflife.org/rejection_statement/\">a formal statement</a>, which is a further big improvement. In particular they stopped denying Nya Bagdlets neo-nazi opinions and gave a more in depth explanation of what happened. I still think it was a very major mistake and people are right to be concerned about it, but it's at least reassuring that there don't appear to be nazi sympathisers within the FLI ranks, and that they are doing something to address their mistake.&nbsp;</p>", "user": {"username": "titotal"}}, {"_id": "HkmSgJdrPxLuoBDJj", "title": "What countries are worth funding?", "postedAt": "2023-01-15T16:23:49.554Z", "htmlBody": "<p><i>Note: I published a similar post last month. After reconsideration and discussions with other EAs on the writing, I decided to remove that post and publish a new one.</i></p><p>In 2020 I started the first EA group in Romania. Last year, after COVID restrictions were lifted and the group became more active, I applied for funds from the EAIF to do community building full-time. I submitted two projects, and both were rejected with limited or no feedback. The first project had a more generic strategy, while the second focused on outreach to professionals in the tech sector (you can read more about it&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/htNkiECLGHLTfAHYW/request-feedback-on-my-eaif-application\"><u>here</u></a>). In the process of hunting for more informative feedback, I kept hearing questions like \u201cWhat\u2019s Romania\u2019s comparative advantage?\u201d or \u201cWhat particular groups or cause areas do you want to focus on?\u201d I was also told by a fund manager that Romania is \u201cnot exactly a hub\u201d and therefore a high opportunity cost exists in funding community building there unless the project is creative.</p><p>The main purpose of this article is not to question these rejections, but rather the underlying culture surrounding the lack of feedback and transparency. If some countries are unlikely to be funded in general because they\u2019re low priority, this should be public information. The EAIF should also be transparent about the criteria they use to prioritize countries, if prioritization exists. After attending several EA events, it seems that the perception of the average EA is that there are plenty of funds, and if you start a new national group, it should be simple to access them. This perception seems inaccurate, and if the reality were more transparent this would help founders of new national groups have more realistic expectations.</p><p>I have also talked about my experience with Romanian EAs within the country and abroad.&nbsp;There exists speculation that the country may be discriminated against based on negative stereotypes. While I don't argue that this is necessarily the case, the fact that even highly engaged members feel this way is a serious enough problem. Country prioritization without transparency can be perceived as discrimination, and a lack of evidence makes it difficult to distinguish the two.</p><p>To be clear, I\u2019m not saying that the only or even the main reason why my applications were rejected was that they were coming from Romania. It is of course possible that another person with another project would have been funded. It\u2019s hard to know because, again, EAIF doesn\u2019t give direct feedback. Overall, this experience made me realize there is very little transparency regarding group funding and groups in general. There are many questions that I think should be easier to find an answer to, for example:</p><ul><li>Which countries are at the top/bottom of the priority list to be funded?</li><li>How much funding does each country get per year for community building?</li><li>How much of this money goes toward salaries?</li><li>How many paid community builders does each country have?</li><li>How many applications come from each country?</li><li>What percentage of applications is rejected for each country?</li><li>How big/old is each national group?</li></ul><p>Having answers to these questions can be helpful when starting new national groups. It would allow us to learn from groups that received funding while being in similar cultural contexts and stages of development. Therefore, I would like to ask:</p><ul><li>Is this information collected? If so, would it be possible to access it?</li><li>Is it worth creating country profiles to determine the answer to these questions?</li><li>Eligibility to CEA\u2019s Community Building Grants Program is limited to a&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/how-to-join-the-program\"><u>fixed list</u></a> of mature communities. If prioritization exists in the EAIF, what criteria are used to prioritize countries?</li><li>Should prioritization exist? Or should funding be available to groups in any country?</li></ul><p>PS: My intention is not to accuse anybody in this post. I do believe in the good intentions of grantmakers, and I don\u2019t reject a priori the possibility that there are legitimate reasons to prioritize some countries over others. I\u2019m sharing these experiences because they are relevant and reveal certain problems that could be mitigated by more transparency and open discussions. One of the things I value the most about the EA community is the openness to feedback and willingness to update beliefs and attitudes in the light of new information.</p>", "user": {"username": "arielpontes"}}, {"_id": "qtGjAJrmBRNiJGKFQ", "title": "The writing style here is bad", "postedAt": "2023-01-15T07:45:16.992Z", "htmlBody": "<p>Epistemic status: around that of Descartes' (low)</p><p>I am not a native English speaker. Despite that, I've had my English skills in high regard most of my life. It was the language of my studies at the university. Although I still make plenty of mistakes, I want to assure you I am capable of reading academic texts.</p><p>That being said: a whole lot of posts and comments here do feel like academic texts. The most basic/heuristic check: I found a tool to measure linguistic complexity, here <a href=\"https://textinspector.com/\">https://textinspector.com/</a> - so you can play with it yourself, if you'd like to. Now, I realize that AI Safety is a complicated, professional topic with a lot of jargon. Hence, let's take a discussion that, I believe, should be especially welcoming to non-professionals: <a href=\"https://forum.effectivealtruism.org/posts/kuqgJDPF6nfscSZsZ/thread-for-discussing-bostrom-s-email-and-apology\">https://forum.effectivealtruism.org/posts/kuqgJDPF6nfscSZsZ/thread-for-discussing-bostrom-s-email-and-apology</a></p><p>I could make some Python project and analyse lingustic complexity of a whole range of posts, produce graphs and it sure would be fun and much better, but I am a lazy person and I just want to show you the idea. I mean to sound extremely simple when I say the following.&nbsp;</p><p>There's a whole lot of syllables right there.</p><p>Most of the comments here do feel like academic papers. Reading them is a really taxing exercise. In fact, I usually just stray from it. Whether it's my shit attention span or people on a global scale are not proficient English speakers, it is my firm belief that ideas should be communicated in an understandable matter when posssible. That is, most of people should be able to understand them. If you want to increase diveristy and be more inclusive, well, I think that's one really good way at attempting so.</p><p>This is also the reason for the exact title of the post, rather than \"Linguistic preferences of some effective altruists seem to be impacted by a tendency to overly intellectualize.\"</p>", "user": {"username": "Micha\u0142 Zab\u0142ocki"}}, {"_id": "dMhscQ9sTXCaegYju", "title": "Consider paying for literature or book reviews using bounties and dominant assurance contracts", "postedAt": "2023-01-15T04:50:01.563Z", "htmlBody": "<p><i>Cross-posted to LessWrong </i><a href=\"https://www.lesswrong.com/posts/9cvK8QX9ZRBxmEfcz/consider-paying-for-literature-or-book-reviews-using\"><i>here</i></a><i>.</i></p><p>In September 2021, a <a href=\"https://www.lesswrong.com/posts/eHnupDgggBqDqT5eg/lesswrong-is-paying-usd500-for-book-reviews\">LessWrong pilot program</a> paid $500 for high-quality book reviews related to \"science, history, and rationality.\" They got 36 submissions (they don't say how many were rejected for low quality, but at least 9 were accepted) and <a href=\"https://www.lesswrong.com/posts/HbWQpsPLcTCZDZGDA/book-review-review-end-of-the-bounty-program#If_you_want_to_collect_the_bounty_for_your_book_review__please_message_me_via_Intercom_or_email__ruby_lesswrong_com_\">a bunch of them were popular on the forum</a>.</p><p>There's a <a href=\"https://www.lesswrong.com/tag/bounties-and-prizes-active\">bounty tag</a> on LW (and <a href=\"https://forum.effectivealtruism.org/topics/bounty-open\">on the EA Forum</a>) but it isn't used much. A culture of posting bounties\u2014either individually or in groups of people interested in the same information\u2014has benefits for patrons, writers, and the community generally:</p><ul><li><strong>For patrons\u2014</strong>If there's a question you want investigated or a book you want reviewed, you can save your valuable time by tossing the question into the LW void and waiting for a piece that's worth accepting. Others probably want the same thing and can contribute to a pool. Ambitious patrons can also influence the direction of the community by sponsoring posts on topics they think people should think about more. You don't have to worry about filtering for visible credentials and writers don't have to worry about having any.&nbsp;</li><li><strong>For writers\u2014</strong>Bounties motivate people to write both through a direct monetary incentive, but also because a lot of people dissuade themselves from writing on the Internet to avoid looking vain or self-important. Bounties cover for this awkwardness by providing non-status-related reasons to post.</li><li><strong>For the community\u2014</strong>This whole exchange provides a positive externality to the lurkers who can read more posts for free.</li></ul><p>The simplest way this could work is for people to post individual bounties of e.g. $500 for posts drawing conclusions that would have taken them just too long to justify at the hourly value of their time. These bounties can guide writers who may be looking for things to read and write about anyway.</p><p>An obstacle to bounty markets is that writers incur the risk of being outshone by a better post written around the same time. They could also be snubbed by picky benefactors. If most bounties are posted by a small group of people who post many individual bounties, then reputation effects can manage this. Group bounties could be difficult to coordinate since people aren't motivated to post \"I'd contribute $100 to a post analyzing the top 10 writing advice books for insights\" if the bounty is unlikely ever to be fulfilled. And they're not motivated to join existing pools when they could free-ride instead.</p><p>One solution is to use the opposite direction: Kickstarter-style writing proposals. In monthly threads, users post advertisements for book reviews/literature reviews/investigations that they'd be willing to produce at some price and specifications. This puts the reputational demands on the writers and the risk on the sponsors who might pay for a poor product.</p><p>In theory, writers could kickstart posts using <a href=\"https://mason.gmu.edu/~atabarro/PrivateProvision.pdf\">dominant assurance contracts</a>. An example (this is a real offer): If you send $20 to arjun.panickssery at Gmail via PayPal by noon New York time on January 21<sup>st</sup>, I'll send you back $25 if fewer than 10 people sent me money. If 10 or more people send me money, I'll post a review of Steven Pinker's <i>The Sense of Style: The Thinking Person's Guide to Writing in the 21st Century</i> by the end of the month. I'm not sure whether I'm just giving away free money right now.</p>", "user": {"username": "ModusTrollens"}}, {"_id": "pDczuKEgQyFvQcKu2", "title": "Podcast interview with Carrick Flynn", "postedAt": "2023-01-15T03:00:41.526Z", "htmlBody": "<p>Just realized that I never posted this here, and thought this audience might be interested: last year, I interviewed Carrick Flynn for ~1.3 hours about his campaign, and specifically his policies on pandemic preparedness and semiconductor manufacturing. During the campaign I had a vague sense of his policies but I actually didn't have much more detail than \"he is against pandemics and would like to spend money to make them stop happening\". If you're in the same boat, <a href=\"https://youtu.be/2q3PpsOJU_k\">this interview</a> might be helpful. Also if you really like the episode, you might want to check out the <a href=\"https://thefilancabinet.com/episodes/\">rest of the podcast</a>. Anyway, below is the episode description I wrote when the podcast got released.</p>\n<hr>\n<p>In this episode, Carrick Flynn talks about his campaign to be the Democratic nominee for Oregon's 6th congressional district. In particular, we talk about his policies on pandemic preparedness and semiconductor manufacturing. He answers questions such as:</p>\n<ul>\n<li>Was he surprised by the election result?</li>\n<li>Should we expect another Carrick campaign?</li>\n<li>What specific things should or could the government fund to limit the spread of pandemics? Why would those work?</li>\n<li>What is working at a semiconductor plant like?</li>\n</ul>\n", "user": {"username": "DanielFilan"}}, {"_id": "Zynbd3nF6eCyvrnz3", "title": "Do better, please ...", "postedAt": "2023-01-15T11:32:52.439Z", "htmlBody": "<p>I am not a card carrying member of EA. I am not particularly A, much less E in that context. However the past few months have been exhausting in seeing not just the community, one I like, in turmoil repeatedly, while clearly fumbling basic aspects of how they're seen in the wider world. I like having EA in the world, I think it does a lot of good. And I think you guys are literally throwing it away based on aesthetics of misguided epistemic virtue signaling. But it's late, and I read more than a few articles, and this post is me begging you to please just stop.</p><p>The specific push here is of course the Bostrom incident, when he clearly and highly legibly wrote black people have lower intelligence than other races. And his apology, was, to put it mildly, mealy mouthed and without much substance. If anything, in the intervening 25 years since the offending email, all he seems to have learnt to do is forget the one thing he said he wanted to do - to speak plainly.</p><p>I'm not here to litigate race science. There's plenty of well reviewed science in the field that demonstrates that, varyingly, there are issues with measurements of both race and intelligence, much less how they evolve over time, catch up speeds, and a truly dizzying array of confounders. I can easily imagine if you're young and not particularly interested in this space you'd have a variety of views, what is silly is seeing someone who is so clearly in a position of authority, with a reputation for careful consideration and truth seeking, maintaining this kind of view.</p><p>And not only is this just wrong, it's counterproductive.</p><p>If EA wants to work on the most important problems in the world and make progress on them, it would be useful to have the world look upon you with trust. For anything more than turning money into malaria nets, you need people to trust you. And that includes trusting your intentions and your character.&nbsp;</p><p>If you believe there are racial differences in intelligence, and your work forces you to work on the hard problems of resource allocation or longtermist societal evolution, nobody will trust you to do the right tradeoffs. History is filled with optimisation experiments gone horribly wrong when these beliefs existed at the bottom. The base rate of horrible outcomes is uncomfortably large.</p><p>This is human values misalignment. Unless you have overwhelming evidence (or any real evidence), this is just a dumb prior to hold and publicise if you're working on actively changing people's lives. I don't care what you think about ethics about sentient digital life in the future if you can't figure this out today.</p><p>Again, all of which individually is fine. I'm an advocate of people holding crazy opinions should they want to. But when like a third of the community seems to support him, and the defenses require contortions that agree, dismiss and generally be whiny about drama, that's ridiculous. While I appreciate posts like <a href=\"https://forum.effectivealtruism.org/posts/NniTsDNQQo58hnxkr/i-support-bostrom\">this</a>, which speak about the importance of epistemic integrity, it seems to miss the fact that applauding someone for not lying is great but not if the belief they're holding is bad. And even if this blows over, it will remain a drag on EA unless it's addressed unequivocally.&nbsp;</p><p>Or this type of comment which uses a lot of words but effectively seems to support the same thought. That no, our job is to differentiate QALYs and therefore differences are part of life.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673737142/mirroredImages/Zynbd3nF6eCyvrnz3/my2eylpnspdlims4fblf.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673737143/mirroredImages/Zynbd3nF6eCyvrnz3/j0jsmphmk90wgxuvyumk.png 144w, https://res.cloudinary.com/cea/image/upload/v1673737143/mirroredImages/Zynbd3nF6eCyvrnz3/ignqjdyfw3dqqqbj6awm.png 224w, https://res.cloudinary.com/cea/image/upload/v1673737143/mirroredImages/Zynbd3nF6eCyvrnz3/n0b3vgf515ruojqbcdqj.png 304w, https://res.cloudinary.com/cea/image/upload/v1673737143/mirroredImages/Zynbd3nF6eCyvrnz3/znmoargett2vcbzfbf3a.png 384w, https://res.cloudinary.com/cea/image/upload/v1673737143/mirroredImages/Zynbd3nF6eCyvrnz3/nve4o55umiebjx5fdlnq.png 464w, https://res.cloudinary.com/cea/image/upload/v1673737142/mirroredImages/Zynbd3nF6eCyvrnz3/kogesr1dxdqukkf4kesk.png 544w, https://res.cloudinary.com/cea/image/upload/v1673737142/mirroredImages/Zynbd3nF6eCyvrnz3/hgwjio6kpmzv9aduamli.png 624w, https://res.cloudinary.com/cea/image/upload/v1673737142/mirroredImages/Zynbd3nF6eCyvrnz3/rtqokn1ighgpy4slog2t.png 704w\"></figure><p>But guess what, epistemic integrity on something like this (I believe something pretty reprehensible and am not cowing to people telling me so) isn't going to help with shrimp welfare or AI risk prevention. Or even malaria net provision. Do not mistake \"sticking with your beliefs\" to be an overriding good, above believing what's true, or acting kindly towards the world, or acting like serious members of a civilisation where we all need to work together. EA writes regularly about burnout from the sheer sense of feeling burdened with a duty to do good - guess what, here's a good chance.</p><p>In fact, if you can't see why sticking with the theory that \"race X is inferior in Y\" and \"we unequivocally are in favour of QALY differentiation\" together constitute a clear and dangerous problem, I don't know what to say. If you want to be a successful organisation that does good in the world, you have to stop confusing sophomoric philosophical arguments with actual lived concerns in the real world.</p><p>You can't sweep this under the rug as \"drama of the day\". I'm sorry, but if you want to be anything more than yet another NGO who take themselves a tad too seriously, this is actively harmful.&nbsp;</p><p>This isn't a PR problem, it's an actual problem. If one of the most influential philosophers and leaders of your movement is saying these things that are just wrong, it hurts credibility for any other sort of framework you might create. Not to mention the actual flesh and blood people who live in the year 2023.</p><p>It's one thing to play with esoteric thought experiments about the wellbeing of people in the year 20000. It's quite another to live in the year 2023. Everyone is free to analyse and experiment to explore any question they so choose, including this. However this is not that. It is starting from professing a belief, and saying you are okay doing so because there isn't any contrary evidence. That's not how science works, and that's not how a public facing organisation should work.</p><p>If he'd said, for instance, \"hey I was an idiot for thinking and saying that. We still have IQ gaps between races, which doesn't make sense. It's closing, but not fast enough. We should work harder on fixing this.\" That would be more sensible. Same for the community itself disavowing the explicit racism.</p><p>By the way, it's insane that the Forum seems to hide this whole thread as if it is a minor annoyance instead of a death knell. The SBF issue I can understand, you were fooled like everyone else and its a black eye for the organisation, but this isn't that. And the level of condemnation that brought was a good way to react. This is much more serious.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673737142/mirroredImages/Zynbd3nF6eCyvrnz3/kd6yk5z2kkckg4pr6ncv.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673737143/mirroredImages/Zynbd3nF6eCyvrnz3/icxyy9pn3aljhfv9m0ve.png 137w, https://res.cloudinary.com/cea/image/upload/v1673737143/mirroredImages/Zynbd3nF6eCyvrnz3/nnukw0eulx6k6y00dwyf.png 217w, https://res.cloudinary.com/cea/image/upload/v1673737143/mirroredImages/Zynbd3nF6eCyvrnz3/eubmbxnsq7rirgkuuhd6.png 297w, https://res.cloudinary.com/cea/image/upload/v1673737143/mirroredImages/Zynbd3nF6eCyvrnz3/ungnkhf54zpfkka3745u.png 377w, https://res.cloudinary.com/cea/image/upload/v1673737143/mirroredImages/Zynbd3nF6eCyvrnz3/webdcgdeaxh8mhas29wj.png 457w, https://res.cloudinary.com/cea/image/upload/v1673737142/mirroredImages/Zynbd3nF6eCyvrnz3/mkqhdntmmiykkxox4h03.png 537w, https://res.cloudinary.com/cea/image/upload/v1673737142/mirroredImages/Zynbd3nF6eCyvrnz3/pxrfbgn6ui9rfat8gslg.png 617w, https://res.cloudinary.com/cea/image/upload/v1673737142/mirroredImages/Zynbd3nF6eCyvrnz3/nm2mpcddb8ifwjo14kpp.png 697w\"></figure><p>I should say, I don't have a particular agenda here. This stream of consciousness is already quite long. A little annoyed perhaps that this is flooding the timeline and the responses from folks whom I'd considered thoughtful are tending towards debating weird theoretical corner cases, doing mental jiu-jitsu just to keep holding that faith a little longer. But mostly it's just frustration bubbling out as cope.&nbsp;</p><p>I just wish y'all could regain the moral high ground here. There are important causes that could use the energy. It's not even that hard.&nbsp;</p>", "user": {"username": "Rohit (Strange Loop)"}}, {"_id": "HWnLXCyxxcoarQZvg", "title": "Someone should write a detailed history of effective altruism", "postedAt": "2023-01-14T22:30:44.580Z", "htmlBody": "<p>I think that someone should write a detailed history of the effective altruism movement. The history that <a href=\"https://forum.effectivealtruism.org/topics/history-of-effective-altruism\"><u>currently exists</u></a> on the forum is pretty limited, and I\u2019m not aware of much other material, so I think there\u2019s room for substantial improvement. An oral history was already suggested in <a href=\"https://forum.effectivealtruism.org/posts/8ic7KcxyfchhmGP3x/ea-projects-i-d-like-to-see#A_Verbal_History_of_EA\"><u>this post</u></a>.</p><p>I tentatively planned to write this post before FTX collapsed, but the reasons for writing this are probably even more compelling now than they were beforehand. I think a comprehensive written history would help\u2026</p><ol><li>Develop an EA ethos/identity based on a shared intellectual history and provide a launch pad for future developments (e.g. longtermism and an influx of money). I remember reading about a community member who mostly thought about global health getting on board with AI safety when they met a civil rights attorney who was concerned about it. A demonstration of shared values allowed for that development.</li><li>Build trust within the movement. As the community grows, it can no longer rely on everyone knowing everyone else, and needs external tools to keep everyone on the same page. <a href=\"https://forum.effectivealtruism.org/posts/JQyacxvzoH3Eo4GPo/guided-by-the-beauty-of-one-s-philosophies-why-aesthetics\"><u>Aesthetics have been suggested</u></a> as one option, and I think that may be part of the solution, in concert with a written history.</li><li>Mitigate existential risk to the EA movement. See EA criticism #6 in <a href=\"https://forum.effectivealtruism.org/posts/X47rn28Xy5TRfGgSj/21-criticisms-of-ea-i-m-thinking-about\"><u>Peter Wildeford\u2019s post</u></a> and <a href=\"https://forum.effectivealtruism.org/posts/xrmM6zWco9QEMxG6j/ways-in-which-ea-could-fail\"><u>this post</u></a> about ways in which EA could fail. Assuming the book would help the movement develop an identity and shared trust, it could lower risk to the movement.</li><li>Understand the strengths and weaknesses of the movement, and what has historically been done well and what has been done poorly.</li></ol><p>There are a few ways this could happen.</p><ol><li>Open Phil (which already has a <a href=\"https://www.openphilanthropy.org/focus/history-of-philanthropy/\"><u>History of Philanthropy</u></a> focus area) or CEA could actively seek out someone for the role and fund them for the duration of the project. This process would give the writer the credibility needed to get time with important EA people.</li><li>A would-be writer could request a grant, perhaps from the <a href=\"https://funds.effectivealtruism.org/funds/ea-community\"><u>EA Infrastructure Fund</u></a>.</li><li>An already-established EA journalist like Kelsey Piper could do it. There would be a high opportunity cost associated with this option, of course, since they\u2019re already doing valuable work. On the other hand, they would already have the credibility and baseline knowledge required to do a great job.</li></ol><p>I\u2019d be interested in hearing people\u2019s thoughts on this, or if I missed a resource that already exists.</p>", "user": {"username": "Pete Rowlett"}}, {"_id": "PzxKQCWuaknbGF7qW", "title": "EA should help Tyler Cowen publish his drafted book in China", "postedAt": "2023-01-14T21:10:21.325Z", "htmlBody": "<p>Tyler Cowen was on the Jan 9th episode of ChinaTalk, a podcast hosted by Jordan Schneider.</p><p><strong>Podcast</strong>:&nbsp;<a href=\"https://link.chtbl.com/GEYO9yFn\"><u>https://link.chtbl.com/GEYO9yFn</u></a></p><p><strong>China Talk Substack</strong>:&nbsp;<a href=\"https://www.chinatalk.media/about\"><u>https://www.chinatalk.media/about</u></a></p><p>At 39:45 Tyler mentions writing a book to improve US relations with China that will likely never be published. <strong>We should help him publish it!</strong><br><br><strong>Edit: Tyler is interested although worried about censorship</strong></p><p><strong><img src=\"https://res.cloudinary.com/cea/image/upload/v1673739461/mirroredImages/PzxKQCWuaknbGF7qW/sbkqhoyyfzobbziriewy.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673739461/mirroredImages/PzxKQCWuaknbGF7qW/goihbuwnayjupddt0d9w.png 109w, https://res.cloudinary.com/cea/image/upload/v1673739461/mirroredImages/PzxKQCWuaknbGF7qW/l2wvmealterpwwhdhfyr.png 189w, https://res.cloudinary.com/cea/image/upload/v1673739461/mirroredImages/PzxKQCWuaknbGF7qW/p3q3zeknqgyqythoxss4.png 269w, https://res.cloudinary.com/cea/image/upload/v1673739461/mirroredImages/PzxKQCWuaknbGF7qW/om3ztfcniy49legkfzkm.png 349w, https://res.cloudinary.com/cea/image/upload/v1673739461/mirroredImages/PzxKQCWuaknbGF7qW/a6oprxv0wamsz23rqqxd.png 429w, https://res.cloudinary.com/cea/image/upload/v1673739460/mirroredImages/PzxKQCWuaknbGF7qW/etpb2yofaeuhpaqyywju.png 509w, https://res.cloudinary.com/cea/image/upload/v1673739460/mirroredImages/PzxKQCWuaknbGF7qW/bz61x83gtemzm6kutmh5.png 589w\"></strong><br><br>I transcribed this part of the podcast with Whisper, so there may be mistakes. Go listen to the entire episode anyway, it\u2019s worth a listen.</p><h2>Transcription</h2><p><strong>Jordan</strong><br>So shortly, millions of Chinese nationals who've been playing World of Warcraft their entire lives will no longer be able to. I'm curious, how important shared cultural touchstones, like video games, the NBA and Marvel movies are to keeping the peace?</p><p><strong>Tyler</strong></p><p>I don't know, we had plenty such touchstones with, say, Germany before World War I, World War II, it didn't matter. But certainly worth trying, you know, I had my own project to improve relations with China, which failed, by the way. I wrote a manuscript for a book, and my plan was to publish it only in China. And it was a book designed to explain America to the Chinese, and make it more explicable, more understandable. So I wrote the book, I submitted it to Xinhua, which gave me a contract, even paid me in advance. But then a number of events came along, most specifically the Trump trade wars, and the book never came out. They're still sitting on it. I don't think it will ever come out. That was my, you know, you could call it, misguided project, to just do a very small amount to help the two countries get along better.</p><p><strong>Jordan</strong></p><p>Wow, what were your, what were your themes?</p><p><strong>Tyler</strong></p><p>Well, if you think of Tokvill, he wrote democracy in America, so that Europeans would understand America better, right? So I thought, well, if we're trying to explain America to Chinese people, it's a really very different set of questions, especially in the 21st century. Though I covered a lot of basic differences across the economies, the policies, why are the economies different?</p><p>Why is there so little state ownership in America?</p><p>Why are so many parts of America so bad at infrastructure?</p><p>Why do Americans save less?</p><p>How is religion different in America?</p><p>That was, I think, an especially sensitive topic. And just try to make sense of America for Chinese readers, but not defending it. Just some kind of, all of branch of understanding. Here's how we are. And I don't know. I don't think they'll ever put the book out. And of course, by now, it's out of date.</p><p><strong>Jordan</strong></p><p>Yeah, but there's, I mean, there's plenty of other people. Other like countries on the planet who could use a little, you know, a civics 101.</p><p><strong>Tyler</strong></p><p>They could. I mean, this is a book written for Chinese people with the contrasts and data comparisons to China. So to sort of send the same book to, you know, Senegal, I don't think would really make sense.</p><p><strong>Jordan</strong></p><p>Yeah, but if you publish it in the US, it will like, you know, Osmos out. I don't think it needs to be published by Xinhua for Chinese people to read it, Tyler.</p><p><strong>Tyler</strong></p><p>I've thought of having it translated into Chinese distributed Somersault in some way. Haven't ruled that out. No downside for me, but you want to do things right. And I kept on waiting for Xinhua. And now I've really completely given up. The book is out of date with facts. That's not a big problem. Facts you can update, but it's very out of date with respect to tone. So right now, everyone feels you need to be tough with China. You can't sort of say nice things to China about China, you're pandering. You look like LeBron James or you're afraid to speak up. And the book would have made a lot of sense, say in 2015 that its current tone doesn't make sense in the current environment, even though I still like the current tone, but it would be misread as something it's not.</p><p><strong>Jordan</strong></p><p>Well, I think it's, I think it's a more important book in 2023 than it was in 2015.</p><p><strong>Tyler</strong></p><p>It probably is. So, you know, it will have some future. I'm still thinking about it trying to get that right. By the way, this is the only time I've ever discussed this publicly. So breaking news here on the China talk.</p><p><strong>Jordan</strong></p><p>No, but seriously, someone out there who's listening to this is a translator or works at a publishing house that might make a little more sense than the Chinese state, you know, apparatus. So yeah, get in touch with me or Tyler. Let's peer pressure him into making this thing happen.</p><h2><br>What can we do to help?</h2><p>This seems like a great way for the EA community to help. OpenPhil or someone else with a bit of experience and prestige should reach out to Tyler and ask how they can best help get this book updated, translated, and published.</p><p>They can do all of the annoying or tedious work he doesn't want to do, like hire and manage a really great translator, do the basic research to update the facts (with his signoff), etc.</p><p>I think it would be pretty cheap and easy to do with almost no downside risk. There\u2019s some great potential to reach many Chinese readers and increase cultural understanding at a time when tensions are rising.</p>", "user": {"username": "Matt Brooks"}}, {"_id": "eMewdP9QtFJDWYHXC", "title": "Zero Utils: why utility wants to be additive", "postedAt": "2023-01-14T16:29:26.029Z", "htmlBody": "<p>I did a lot of my thinking for this post over the summer for the EA criticism contest but didn't get around to writing it up. I don't think it amounts to very substantive criticism but I think it may be interesting to utilitarians. But to those who have \"done the reading\" (Harsayni and Von Neumann-Morgenstern) it may be a little basic.</p><p>TLDR:</p><ol><li>Utility must be a real number if agents have consistent preferences across randomized choices</li><li>A utility function should be scale and shift invariant if all it does is tell us how an agent acts in different situations</li><li>BUT Harsayni proves a theorem that says if the &nbsp;collective utility function is rational in the same way and is determined from the utility function of individuals than it must be a weighted sum of the individual's utilities</li><li>I conclude with some half baked thoughts about what I think they may or may not have to do with morality and EA.</li></ol><p>The linked blog's text is reproduced below (Sorry copy pasting broke the footnotes, but the latex and formatting look the same).</p><hr><p>If you give a man two numbers he will try to add them. One of the first things you're taught as a human is numbers and once you've got a handle on them you're taught to add them. Then you spend the next 20 years of your life being given numbers for your performance and having those numbers added together. But consider for a moment: what are the units of your GPA? If there is such a thing as utility, what are it's units?</p><p>It's not true that any two numbers can be added and a meaningful result obtained. Sometimes their types don't agree: what's $5 + 3cm? Some other quantities simply cannot be meaningfully added. What is 3 stars + 4 stars with regards to movie ratings?</p><p>But this is all very trite and obvious and of course the utilcels have thought of it.</p><h1>Why Should Utility be a Real Number?</h1><blockquote><p>God made preferences. All else is the work of man.</p><p>- Leopold Kronecker, basically</p></blockquote><p>Let's begin by considering the utility of an individual before moving on to aggregation. For concreteness let it be my utility.</p><p>In the beginning there are preferences. I prefer green tea to black. I prefer my eggs fried to scrambled. At first glance if all I have to work with is preferences you might think all I could give you is an ordering of scenarios. I could say green tea gives me 5 utility and black tea 1 or green tea 3.89 utility and black tea -300 and it'd make no difference to what happens when you offer me my choice of beverage.</p><p>But there is a tool to make meaningful different utility functions besides ordering: randomized mixing. Say there are three outcomes:</p><ul><li>Green tea</li><li>Black tea</li><li>No drink</li></ul><p>And I have the choice between Black tea or x% chance of Green tea otherwise nothing.</p><p>If I prefer Black tea to no drink than there should be some x where I would choose the guaranteed Black tea over the gamble. Once I've chosen random numbers for my utility for no drink and for black tea, say 0 and 1, My utility for green tea is determined. It is 1/x where x is the smallest probability for which I'd choose the gamble.</p><p>Of course none of this is original and if you work out the details you get the <a href=\"https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">Von Neumann-Morgenstern utility theorem</a>.</p><p>Already there are a number of possible objections:</p><h3>1. What if there is no such x?</h3><p>This could mean I actually prefer no drink to black tea. That's fine, and we can mix up the gamble to determine exactly what negative utility I give to black tea. But what if I prefer black tea to no drink it's just my preference for green tea is so strong I'd always pick the gamble? With tea that maybe seems implausible but what if I'm choosing between tea or getting eaten by a shark? For someone who takes this very seriously see <a href=\"https://erikhoel.substack.com/p/why-i-am-not-an-effective-altruist\">Erik Hoel</a>. I've never thought this is a particularly serious objection because I do risk a small probability of violent death for a little bit of entertainment all the time by getting in my car and driving. Maybe when you strip away the details and make it a pure trolley problem it's easy to have the intuition that you'd never choose a guaranteed beverage plus a small probability of death over staring at a wall but in practice everyone does take the gamble all the time.</p><h3>2. Are we putting the cart before the horse?</h3><p>If our goal is to build a moral theory should we be asking how should I prefer green and black tea not how do I prefer green and black tea? In the case of tea this seems unimportant because how could the choice be morally relevant? But maybe I'm choosing between beef or tofu? Maybe I don't intrinsically have a preference for the ethical treatment of the livestock and produce I eat, but maybe I should?</p><h3>3. But 0 and 1 are arbitrary. This doesn't uniquely determine a utility function.</h3><p>This isn't a problem if we're just looking at my value and decisions. But it becomes a problem when we wish to aggregate many people's preferences. It doesn't make sense to add together a bunch of functions which were determined in such a way as to be insensitive to shifts and scales! This brings us to Harsayni's Aggregation Theorem.</p><h1>Harsayni's Argument</h1><p>Now it doesn't make sense to add a bunch of functions determined up to scale and shift but it sure would be convenient, since all we know how to do is add. One piece of evidence that we should add is Harsayni's 1955 argument which I will reproduce almost in its entirety here. We just require 3 (and one unmentioned by Harsayni) assumptions:</p><p>1. Individuals have (or can be given/assigned) a utility function&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"U_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>&nbsp;consistent with EV as discussed in the previous section. [1]</p><p>2. Society, or the collective, can be given such a function&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathscr{U}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-script-R\" style=\"padding-top: 0.519em; padding-bottom: 0.225em; padding-right: 0.16em;\">U</span></span></span></span></span></span></span></span></span>&nbsp;as well.</p><p>3. Society's utility function is a function of the utility of the individuals. E.g. for two different worlds where everyone has the same utility, society's utility should be the same. I'll write&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathscr{U} = \\mathscr{U}(U_1, \\dots, U_n)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-script-R\" style=\"padding-top: 0.519em; padding-bottom: 0.225em; padding-right: 0.16em;\">U</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-texatom MJXc-space3\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-script-R\" style=\"padding-top: 0.519em; padding-bottom: 0.225em; padding-right: 0.16em;\">U</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">\u2026</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;when using this functional relationship.</p><p>3': There is some event for which all&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"U_i = 0\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span></span></span></span></span></span>&nbsp;and on this event&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathscr{U} = 0\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-script-R\" style=\"padding-top: 0.519em; padding-bottom: 0.225em; padding-right: 0.16em;\">U</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span></span></span></span></span></span>. Call this event&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"O\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span></span></span></span></span></span>. This isn't so much an assumption as fixing a scale because remember utility in the VN-M sense is only determined up to shift.</p><p>Bonus assumption: Harsayni assumes this in his proof but as far as I can tell it doesn't follow from the previous 3 or is at all obvious: For each&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span>&nbsp;there exists an event for which&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"U_i = 1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span>&nbsp;and the rest are&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span></span></span></span></span></span>. To me this is a big independence assumption. We don't assume the&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"U_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span></span>&nbsp;are selfish or egoistic. They're just the utility functions people happen to have which could be selfish but could also be altruistic. In practice two individuals who are married or business partners could have extremely correlated utility functions. If they're identical there's no issue but the nightmare scenario is something like one business partner being risk loving and having a utility function which is company profits and the other having log profits so their utility functions are monotonically related. [2] [3]</p><p>From this we can deduce a result which at first blush may seem surprisingly strong but will follow from considering what the expected value of just a few mixed scenarios must be. One take away from the theorem is consistency over all randomized scenarios is actually an extremely strong assumption.</p><p><i>Theorem</i>.&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathscr{U} = k_1 U_1 + \\dots + k_n U_n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-script-R\" style=\"padding-top: 0.519em; padding-bottom: 0.225em; padding-right: 0.16em;\">U</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22ef</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span></span></span></span>&nbsp;where&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"k_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span></span>&nbsp;is the total utility when individual&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span>&nbsp;has utility&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span>&nbsp;and all others have&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span></span></span></span></span></span>&nbsp;e.g. Societal utility is a weighted sum of individual utility.</p><p><i>Proof</i>. First we prove&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathscr{U}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-script-R\" style=\"padding-top: 0.519em; padding-bottom: 0.225em; padding-right: 0.16em;\">U</span></span></span></span></span></span></span></span></span>&nbsp;is homogeneous in&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"U_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span></span>&nbsp;that is,</p><span><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\" \\mathscr{U}(kU_1,\\dots,kU_n) = k\\mathscr{U}(U_1,\\dots,U_n). \"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-script-R\" style=\"padding-top: 0.519em; padding-bottom: 0.225em; padding-right: 0.16em;\">U</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">\u2026</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-script-R\" style=\"padding-top: 0.519em; padding-bottom: 0.225em; padding-right: 0.16em;\">U</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">\u2026</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span></span></span></span><p>Let&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"O\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span></span></span></span></span></span>&nbsp;denote an event for which all&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"U_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span></span>&nbsp;are&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathscr{U}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-script-R\" style=\"padding-top: 0.519em; padding-bottom: 0.225em; padding-right: 0.16em;\">U</span></span></span></span></span></span></span></span></span>&nbsp;is&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span></span></span></span></span></span>. Let&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span></span></span></span></span>&nbsp;be some other event for which the utility functions take the values&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"u, u_1, \\dots , u_n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">\u2026</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span></span></span></span>&nbsp;respectively. Now consider a mixing event which is&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span></span></span></span></span>&nbsp;with probability&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span></span></span></span></span>&nbsp;otherwise&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"O\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span></span></span></span></span></span>. Of course we have&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathscr{U} = p\\cdot u\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-script-R\" style=\"padding-top: 0.519em; padding-bottom: 0.225em; padding-right: 0.16em;\">U</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"U_i = p u_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span></span>&nbsp;in this scenario. Which is exactly the homogeneous claim. Two notes:</p><p>1. I've only shown the homogeneous claim for&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"k\\leq 1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.446em;\">\u2264</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span>. Harsayni spends 4 times as much text dividing but I'll leave you to fill in the details or read the original paper.</p><p>2. It's not necessary in this step to assume the&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"u_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span></span>&nbsp;could take on any value or even that they're nonzero.</p><p>Now let&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"S_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.032em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span></span>&nbsp;denote a prospect for which individual&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span>&nbsp;gets utility&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"U_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span></span>&nbsp;and all other individuals get utility&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span></span></span></span></span></span>. As I said above that such a prospect exists is a big assumption but it slips in in the original paper. By our homogeneity result we know&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathscr{U} = k_i U_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-script-R\" style=\"padding-top: 0.519em; padding-bottom: 0.225em; padding-right: 0.16em;\">U</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span></span>&nbsp;on prospect&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"S_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.032em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span></span>.</p><p>Now take the mixed prospect that is equally likely to be each&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"S_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.032em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span></span>. By the linearity of expectation for each individual this prospect is worth&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"U_i/n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span></span>&nbsp;and for the collective it is worth&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathscr{U} = \\sum k_i U_i/n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-script-R\" style=\"padding-top: 0.519em; padding-bottom: 0.225em; padding-right: 0.16em;\">U</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-size1-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">\u2211</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span></span>.</p><p>Using homogeneity once more we get&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathscr{U} = \\sum k_i U_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-script-R\" style=\"padding-top: 0.519em; padding-bottom: 0.225em; padding-right: 0.16em;\">U</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-size1-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">\u2211</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span></span>&nbsp;for a prospect where each individual's utility is&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"U_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span></span>&nbsp;(as opposed to&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"U_i/n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span></span>&nbsp;as it was in the previous paragraph).</p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\blacksquare\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-ams-R\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">\u25a0</span></span></span></span></span></span></span></p><p>Like I said, not much of a proof. Somehow just from the linearity of expected value we've derived a whole moral philosophy [4].</p><h2>Aside on p-norms</h2><p>I have a math friend who likes to joke that the problem of the repugnant conclusion is just a matter of choosing the right p-norm. At&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\ell^1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">\u2113</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span></span>&nbsp;we have Harsayni's addition, at&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\ell^\\infty\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">\u2113</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">\u221e</span></span></span></span></span></span></span></span></span>&nbsp;we have Rawl's (insane) position. By choosing the proper&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span></span></span></span></span>&nbsp;in between&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\infty\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">\u221e</span></span></span></span></span></span></span>&nbsp;we can get an ethical theory to our tastes. But the choice of&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span>&nbsp;is not arbitrary. It's the constant for which both the social utility and individual utility functions can both be rational in the Von Neumann-Morgenstern sense.</p><h1>Why I am not convinced</h1><p>I had planned to write a blog post making the point in the first section in May 2022 before even knowing about the Von Neumann-Morgenstern Theorem. When the Effective Altruism (EA) criticism contest was announced I decided to do a little more research and make my post a little better [5]. Having read Harsayni's Theorem I think there's better theoretical justification to add but I still have a number of qualms.</p><p>I am basically totally convinced that an organization founded to be altruistic has to be fundamentally utilitarian or irrational though. So in that sense this isn't a critique of EA but is possibly a critique of someone deciding to be EA.</p><ul><li>What the theorem of course can't tell you how to do is how to choose the weights. In practice maybe this is a weak critique though. In altruistic practice it seems people focus their giving on people plausibly maxing out the utility scale in the negative direction. Maybe you can't prove a nice theorem in this context like Harsayni was able to do, but it seems reasonable to say dying of cancer is about as bad as dying of malaria and both are much worse than not getting your favorite flavor of ice cream.</li><li>I was turned onto Harsayni from <a href=\"https://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/\">this interview</a> where MacAskill gives the aggregation theorem as tied for his second favorite argument for utilitarianism along with rejection of personhood arguments behind track record. I think there's something contradictory about taking the aggregation theorem and personhood rejection as your top two reasons. Why do our utility functions have to respect expected value in this way? Because otherwise we're exploitable as agents. We can be dutch booked. But doesn't concern for this scenario imply a strong sense of self? Why would I care that as I wandered in circles over my choices I ended up worse off if I didn't have a strong sense of self identification?</li><li>Similarly it seems like Preferentism is out of fashion. See this excellent <a href=\"https://users.ox.ac.uk/~sfop0060/pdf/can%20there%20be%20a%20preference-based%20utilitarianism.pdf\">critique</a>. And listening to other 80k interviews it seems like hedonism is more mainstream than preferentism in the EA community [6]. But again it seems like the theory is built out of the primacy of preference.</li></ul><p>[1] These utility functions need not be selfish. They shouldn't depend on each other or we may run into computability issues but they may depend on each other's inputs. e.g. It's fine for someone's utility to be lower if they have much more money than their friends.</p><p>[2] Though assuming&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"U_i = 1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span>&nbsp;in this scenario as opposed to any other nonzero value is no issue as the&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"U_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span></span>&nbsp;are only determined up to scale.</p><p>[3] Linearity of Expected Value is so powerful I wouldn't be surprised if a more careful argument could remove this assumption. With this assumption though the proof is very easy.</p><p>[4] For some deep ja3k/EV lore see <a href=\"https://www.tumblr.com/jaekmtg\">this 2016 tumblr post</a>.</p><p>[5] Missed that boat unfortunately. Criticism is its own reward though so I'm posting anyway.</p><p>[6] Sorry if this is a mischaracterization or there are existing surveys. I looked at this <a href=\"https://philpapers.org/surveys/results.pl\">survey of philosophers</a> but it doesn't seem to get at quite this question.</p>", "user": {"username": "Ja3k"}}, {"_id": "fX2gszcxpKhwSGXEy", "title": "Does it matter that some EAs think black people are stupider than white people?", "postedAt": "2023-01-14T12:14:52.428Z", "htmlBody": "<p>This post is inspired by recent discussion about Nick Bostrom but it is not about Nick Bostrom. It is about EA in general.</p><p>It's clear that many EAs believe that there are population-level differences in average intelligence between ethnic groups. Some representative comments from recent discussions:</p><p>\"<strong>IQ distributions differ across races. IQ proxies [are] a general factor of intelligence.</strong> Afaik intelligence researchers believe both sentences are correct. I care more about being correct than about being culturally compliant [...]\" (<a href=\"https://forum.effectivealtruism.org/posts/dPmdkjaNGFQHfWhxQ/linkpost-nick-bostrom-s-apology-for-an-old-email?commentId=NGXmtRuQCxDpCX8vu\">1</a>)</p><p>\"<strong>The finding that generally many groups of black people have on average lower IQ than than the median is pretty robust,</strong> and he does make it clear even in the initial emailthat [sic] he doesn't think they are morally inferior or that he doesn't hold the actual negative views about them that one might associate with this language.\" (<a href=\"https://forum.effectivealtruism.org/posts/dPmdkjaNGFQHfWhxQ/linkpost-nick-bostrom-s-apology-for-an-old-email?commentId=6dLEDNLLk9ghcvC2B\">2</a>)</p><p>\"[...] <strong>the focus on labeling people who lean towards there being a genetic difference in population means as bad is mistaken</strong> given that the threat is actually people who try to leverage this claimed difference politically or attempt to inject their belief in this difference into as many conversations as possible.\" (<a href=\"https://forum.effectivealtruism.org/posts/dPmdkjaNGFQHfWhxQ/linkpost-nick-bostrom-s-apology-for-an-old-email?commentId=SaXWsEEF7dobseqc4\">3</a>)</p><p>Some EAs draw a distinction between thinking black people are stupider on average than white people, and thinking they are worth less:</p><p>\"[...] Bostrom makes clear that differences in intelligence do not alter the moral value or human dignity of each person.</p><p>\"For him, as for many, the issues of intelligence and moral worth are distinct; he never claims that black people are worth less, you are ascribing your own notion that IQ=moral worth, and then blaming him for not responding to it.\" (<a href=\"https://forum.effectivealtruism.org/posts/dPmdkjaNGFQHfWhxQ/linkpost-nick-bostrom-s-apology-for-an-old-email?commentId=2bsFeykyrRHFbGtwD\">4</a>)</p><p>I am not going to relitigate the question of whether population-level differences in IQ exist between races. My own view is the evidence for the existence of such differences is weak and entirely unconvincing. I do not believe such differences exist. I am uninterested in debating this, however: it is in fact irrelevant to my post. <strong>My interest is in the fact that, regardless of whether such differences exist in actuality, a subset of EAs clearly believe in them and think they are important.</strong></p><p>This has profound implications for the ability of the movement to work in the interests of all humanity. For example, EAs who believe it is important that the human population expand but who think that African people are likely to be stupider than the human average may divert resources away from them. EAs making hiring decisions may prioritise applications from people whom they have not subconsciously assigned to the schema \"stupider than me.\"</p><p>These risks are especially salient for a movement like EA, which prides itself on its rationality and adherence to rigorous logical thinking. Belief that members of certain groups are likely to be less capable of this type of thinking is likely have a pernicious effect upon the good work EA is trying to do.&nbsp;</p><p>I suggest that:</p><ul><li>EA leaders (executives and board members of EVF, GWWC, FHI, etc.) should publicly share whether they believe that there are population-level differences in intelligence between ethnic groups.</li></ul><p>This will allow potential donors to make informed decisions about whether EA work is something they wish to support.</p><p>[note to moderators: please do not tag this Nick Bostrom; as noted above the post is not about him or his email, but rather about the wider issues his email highlights]</p>", "user": {"username": "britomart"}}, {"_id": "Kq3YwsrWr5nxQfpPj", "title": "EA Finland: from a philosophy discussion club to a national organization", "postedAt": "2023-01-14T10:49:28.999Z", "htmlBody": "<h2>Introduction</h2><p>This post describes the history of Effective Altruism Finland and Finnish effective altruism efforts, based on activities I've been involved in and what I've researched and heard from others. Although many people (Aayush Kucheria, Andrea La Mantia, Mikko Hemmi, Kaj Sotala, Karla Still, Kerkko Pelttari, Onni Aarne, Santeri Tani, Tapio Lahtiharju, Valtteri Lipi\u00e4inen and Vesa Hautala) have given me feedback and the post should not contain factual errors, it does obviously emphasize my experiences and knowledge \u2013 as they say, history is written by those who post on the EA forum. For another (shorter) perspective of the same events, see <a href=\"https://forum.effectivealtruism.org/posts/5TKbBj3nGgd5xAwEL/a-brief-history-of-ea-finland\">Karla\u2019s post</a>. In particular, I\u2019ve focused less on Aalto EA and EA Tampere and pre-2018 EA Finland efforts, simply because I know less about these.&nbsp;</p><p>The perspective I\u2019ve tried to take in this post is focusing on the concrete things we have done and what they were like. What I have talked less about is attendance numbers and other metrics. This post also should not be read as a \u201cthank you\u201d or appreciation post \u2013 I really appreciate our community, and listing all the cool things everyone in the community has done would take way too long.</p><p>If you are not that interested in EA Finland in particular but would like to get inspiration on how to do things in your local group, reading only the key takeaways at the end of each section should help you with that.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/bmcrwyrg55jdfp1cj6cu.jpg\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/jcnnyvk164f9ns2j5arm.jpg 130w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/nj95fypfyevqklujuxs2.jpg 260w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/txf9w58gcwluvsv0l0dp.jpg 390w, https://res.cloudinary.com/cea/image/upload/v1673691842/mirroredImages/Kq3YwsrWr5nxQfpPj/fyqab8wztswugoywo8mm.jpg 520w, https://res.cloudinary.com/cea/image/upload/v1673691842/mirroredImages/Kq3YwsrWr5nxQfpPj/icdmz6rq4zuvjgl4ev1e.jpg 650w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/wqz0etelc40o630mlc30.jpg 780w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/zcusyu2cwwnhlqlexbmj.jpg 910w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/yahiwrflaju6cxgjytsi.jpg 1040w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/i3qyyvt98bcxiffw9nai.jpg 1170w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/lqgnedffz7tlce3eqrub.jpg 1280w\"><figcaption>Finland-based effective altruists and Stella the dog on our first retreat in 2022</figcaption></figure><h2>Effective Altruism Finland in a nutshell</h2><ul><li>We are a Finland-wide national EA organization that supports EAs and EA groups in Finland, gives career advice, runs intro to EA programs and helps volunteers do EA related projects.</li><li>Most of our activities are conducted by volunteers, but we also have 4 part-time employees who support our volunteers and are responsible for making sure all activities actually happen.</li><li>At the time of writing, there are 45 people in the organizer Telegram channel, out of whom 4 are employed by EA Finland.&nbsp;<ul><li>In practice, there is a lot of variation in how many volunteer hours our volunteers are able to put towards EA activities. I would say around 30 of them are highly engaged such that they currently participate in EA community building activities on a weekly level.</li><li>Some of the volunteers would probably describe themselves as being mainly involved in their local or student group organization rather than EA Finland \u2013 the boundaries of affiliation are not very clear.</li></ul></li><li>Also, we have some hundreds of people who are on our public Telegram channels and mailing lists, and some of them occasionally participate in our events. However, most participation comes from people who also volunteer for us.</li><li>We have officially existed since 2013, but we have become more active and organized during the COVID pandemic.</li></ul><h2>Earlier history of EA Finland</h2><p>Effective altruism efforts in Finland started in the early 2010s, although there has been loosely EA/rationalist adjacent activity in Finland since the early 2000s, such as the&nbsp;<a href=\"http://transhumanismi.org/index-2.html\"><u>Finnish Transhumanist Association</u></a>.</p><p>The Finnish effective altruism scene started out as a small informal group. Some people who are still active today were Kaj and Mikko, both of whom originally heard about EA through LessWrong. In 2013, the group members wanted to scale up the activity, and founded official organizations: Effective Altruism Finland and the Helsinki University student association Effective Altruism Helsinki.&nbsp;</p><p>The group had meetups around once per month (with 3\u201310 participants in each meetup) and organizing events such as a panel discussion with a member of the Finnish parliament as a guest speaker, a visit to Animalia Finland and a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/j8Aqb5EmRwD4aqfxR/tell-us-about-your-recent-ea-activities?commentId=qEnwkBpC587eeM9oi\"><u>giving game</u></a>. There were also some initial network building attempts such as connecting to Finnish philosophers and academics. Once, someone in charge of writing a new upper secondary school <a href=\"https://tuotteet.sanomapro.fi/bu566188-argumentti-2-lukion-filosofia-etiikka.html\">text book </a>about philosophy participated in a meeting and ended up including two pages about EA in the book.&nbsp;</p><p>However, it seems that the group was not large enough to sustain continued activity, and many members were busy finding work and finishing their studies or PhDs. The association meetings of the official groups were not held and some of the more outreach oriented activity died down, but the group was still informally meeting from time to time and hoping that some new people would show up and make it more active again.</p><p>In 2018, Charlie Rogers Smith came to visit a Finnish EA meeting. One of the attendants at the meeting was Onni, a CS freshman who had discovered EA a year earlier by being interested in philosophy and learning about Singer\u2019s drowning child thought experiment. Charlie encouraged Onni to resurrect the EA Helsinki university group. Onni had no experience in running groups or organizing stuff whatsoever, but since nobody else present at the meeting was willing to commit time towards EA Helsinki, he took up the task anyway. This marks the beginning of a new era of effective altruism activity in Finland, although it may not have looked like it for a long time.</p><h2>2018\u20132020: When Onni was responsible for everything</h2><p>In the summer of 2018 Onni resurrected the dormant Effective Altruism Helsinki group, which is a university group for students in the University of Helsinki. For a long time, he was solely responsible for all activities, although he had some help from others for single events and activity planning.&nbsp;</p><p>From here starts the period that I also personally remember, because I also got involved as a participant when Onni explained he wanted to start a new club. (I came partly to support Onni\u2019s club because it is good to support friends if they want to start a club, and partly because I was actually interested in this weird \u201chelping others effectively\u201d thing.)&nbsp;</p><p>Most people who joined EA Helsinki activities in this period found their way there in the following ways:</p><ul><li>By being Onni\u2019s friend, or in particular being in a group chat for thorough good-faith debates where Onni first advertised EA things.&nbsp;<ul><li>Several of our current very active organizers were in this group chat at the time, including Santeri and I who currently work for EA Finland and Kerkko, the current chair of the board of EA Finland.&nbsp;</li><li>Another example of a person who found out about EA through this group of friends is Iikka, who is not into EA anymore but took their time to write a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/hLbWWuDr3EbeQqrmg/reasons-for-my-negative-feelings-towards-the-ai-risk\"><u>forum post</u></a> detailing why.</li><li>I imagine it is not such a rare phenomenon new organizers just move their pre-existing group of friends or acquaintances to EA: it can be fun for said friends but it also makes EA groups more homogenous, like in our case where a large portion of EA Finland participants have a CS background.</li></ul></li><li>Through yearly university freshman events, such as the \u201copening carnival\u201d where different student groups have booths.&nbsp;<ul><li>This is how another current employee, Karla, found EA Helsinki. We pitched EA quite badly for her, and she understood it as \u201chow to stay productive and sane when trying to do altruistic things\u201d. Luckily she was still interested in EA after finding out what the intro event was actually about.</li></ul></li><li>Some people stayed from the pre-2018 EA Finland, but most of them did not get that active in organizing things.&nbsp;</li><li>Surprisingly many people came across EA through some other context such as Singer\u2019s books, leading them to proactively reach out.</li></ul><p>Some activities EA Helsinki had during 2018\u20132020:</p><ul><li>Booth in the university opening carnival</li><li>Intro to EA lecture events in the beginning of the year</li><li>Career workshops that introduced some 80 000 hours career ideas in a couple of hours. (Onni recalls these as being not super well organized, but for me, these were a good selling point for EA. I had never received such good strategic career planning advice.)</li><li>Sparse 1-1\u2019s organized by Onni. (Santeri recalls them being important for him in taking EA more seriously. I, for one, did not really understand the point of them for a long time.)</li><li>A monthly meetup every first Thursday of the month. The workflow for the meetups was something like this:<ul><li>Onni decided on a topic for the month, like \u201cshould you donate now or later\u201d or \u201clongtermism\u201d</li><li>Event was advertised by creating a Facebook event, sending an email on the group mailing list and occasionally in the group chat that had Onni\u2019s friends in it</li><li>Onni came up with some discussion prompts for the evening in the tram on the way to the venue</li><li>Participants (around 10 people) arrived and there was some organized discussion on the prompts</li></ul></li></ul><p>Somehow I managed to learn a lot about EA just by discussing it in these monthly meetups. We didn\u2019t do any background reading in an organized way, but people who had read more explained things to others. Several people have afterwards described the discussions as \u201ctoo advanced for me\u201d or \u201cI did not understand anything the others were talking about\u201d, which is probably not ideal. (One person who was intimidated by the \u201crequired\u201d level of \u201cintelligence\u201d is an algorithmics researcher and university lecturer. He never tried coming to a meetup again but later read some EA books on his own.)</p><p>There were probably several reasons why all organization relied on Onni for a long time:</p><ul><li>Onni did not really know how to delegate tasks to others (\u201cthis is volunteering, so I cannot really ask people to do things for the organization\u201d)</li><li>Even when Onni tried to delegate things to others they didn\u2019t always do what was agreed on so it was easier to just do everything alone</li><li>I think on a more abstract level Onni also held \u201cEA organizers\u201d to a very high standard that was difficult to meet, like being very knowledgeable about EA (so that participants are not misinformed), having high epistemics, being rational etc.</li><li>People who Onni perceived as knowledgeable about EA did not have time to take more responsibility (or even attend events that often), so there were not many people Onni could delegate tasks to</li></ul><p>These things slowly started to change during 2020, when the participants became more knowledgeable about EA and started to feel more confident in taking responsibility of organizing things.&nbsp;</p><h3>Key takeaways:</h3><ul><li>There are ways to successfully run a local EA group with quite a low effort even if you don\u2019t have experience in running groups or organizing anything.</li><li>It is important to be persistent and reliable. The monthly meetups did not need much preparation from Onni and were easy to attend (although not always very beginner-friendly), but it was important they were really held every first Thursday of the month throughout the year.</li><li>It might be a good idea to invite your friends to your EA group because then you have people you like to discuss with, but this also makes your group more homogenous.</li><li>I think if you manage to keep your EA group running, after some time (could be long) you might have attracted people who want to do good things well. At some point they will start saying things like \u201cbut would it not be better if we organized things this way\u201d and \u201cfrom an EA perspective, shouldn\u2019t our group do this thing\u201d. Some of them are just saying this but don\u2019t want to take any responsibility, but some are willing to actually start fixing things if you let them. This is the pool of folks that can turn from participants to organizers. In the next sections, I will describe how the shift worked for us.</li></ul><h2>2020: Pandemic</h2><p>The first COVID wave in 2020 obviously influenced EA Helsinki activities significantly. Everything went remote, and so did EA Helsinki. Since all EA Helsinki communications had been done through Telegram and email, Onni created a Discord server so that the group could hold remote meetings. Finland didn\u2019t face total pandemic lockdowns, but 2020\u20132022 had several periods where all in-person meetings were strongly discouraged.&nbsp;</p><p>In spring 2020, EA Helsinki moved some activities from in-person to Discord video calls:</p><ul><li>Monthly meetups:&nbsp;<ul><li>With a similar concept as before, having a specific theme and discussing about it in a group video call</li></ul></li><li>Biweekly career club:&nbsp;<ul><li>The career club got to meet only twice in-person before the pandemic shut everything down.</li><li>It took some months for the members to figure out what they actually wanted from the career club, but finally it became a structured support group. First, everyone has 5 - 10 minutes to tell how they are doing and if they have anything specific they want to discuss, and then we use 1 \u2013 3 hours to discuss the topics members need support on.&nbsp;</li><li>These could be things like \u201cshould I switch jobs\u201d, \u201chow do I apply for a summer job\u201d, \u201chow to do salary negotiations\u201d and \u201chow to stay productive when working on my thesis\u201d.&nbsp;</li><li>Some topics are more related to personal life situations. I think the most common advice members actually acted upon during the pandemic was \u201cstop hesitating and just contact your workplace/university health care services already, your mental health is important\u201d.</li><li>A regular and safe support group like this becomes very important when there is a pandemic!</li></ul></li></ul><p>After the initial pandemic shock EA Helsinki started to get more active.</p><ul><li>In August 2020, Onni set up a separate organizer Telegram channel, and there started to be some division between organizers and participants, whereas previously Onni had been the only organizer. We also held something like a strategy meeting where we just discussed things, but I don\u2019t think we actually implemented anything we said we should, because there was no clear workflow on how to do stuff.</li><li>In September 2020, we started weekly coworking sessions on Discord.<ul><li>This was inspired by EA Estonia (thanks!)</li><li>The sessions were mostly meant for either organizing EA Helsinki stuff or working on personal EA projects, but a lot of people also used it for homework and such. I often just listened to others discussing a project while walking or cooking.</li><li>It was very comforting to have a regular weekly online meeting with other people. (Pandemics are lonely.)</li><li>A downside was that since most of the EA Helsinki organization happened during these meetings and was not documented anywhere, you didn\u2019t have much chance to know what was going on if you missed a meeting.</li><li>Even if the coworking sessions were open to everyone, those who actually showed up started to feel more like organizers than \u201cjust\u201d participants. I think this effect worked in both ways: if you felt like you wanted to organize something, you would be more likely to join the coworking session, but also if you wanted to hang in the coworking session, you started to feel more inspired to organize something.</li><li>Geography mattered little, so it was easier for Vesa to participate from Tampere when everyone else was in the Helsinki region</li></ul></li><li>We also experimented with other remote event possibilities such as meeting people from other local EA groups online.</li><li>Sometimes we had in-person meetings outside, but they were only advertised on the organizer channel because we had to keep them small due to restrictions.</li></ul><p>All in all the pandemic had a positive influence on the growth of EA Finland. With the sudden change in everyone\u2019s social contacts, for many of our current organizers the Finnish EA community became more important mainly because of the career club and weekly coworking sessions. In addition to that, there were several people who had been busy with other commitments before (like Karla and me), but since these were now canceled, they needed to find other things to do. If I had been as busy as usual with organizing role-playing game events, it would have taken me longer to find the time to actually start seriously reading about EA and organizing things.</p><p>During the first pandemic year EA Helsinki was still run in a very chaotic and non-organized way. Onni was still the main responsible person for everything, but now there were around 8 others who also regularly participated in organizing (for example, held remote monthly meetups and EA intro events). This somewhat functioned because the number of organizers was small enough, but it was not the ideal way to do things because there was also a lot of miscommunication and some projects never got finished. The first bigger project that actually worked out was running an intro program. In the next section I will tell what effects it had.</p><h3>Key takeaways:</h3><ul><li>Organizations attract and retain people if they can provide something participants need. During the pandemic, EA Helsinki did a good job in providing structured and engaging remote possibilities to have social interaction and do useful things, so the pandemic actually helped EA Helsinki grow.</li><li>It is possible to organize things without formal roles or structures for quite a long time, but it is not the optimal use of everyone\u2019s time and talent, since lack of coordination and communication generally leads to less things happening and interested people missing out on organizing things.</li></ul><h3>Another thing:</h3><p>A sociology student did a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/GWyidA3fbXKXErDn4/effective-altruism-as-a-lifestyle-movement-a-master-s-thesis\"><u>Master\u2019s thesis</u></a> on us around this time, so if you are reading her findings, this is the context she drew most of her conclusions from.</p><h2>2021: Intro program and donation website</h2><h3>Intro program&nbsp;</h3><p>In the spring of 2021 EA Helsinki ran a 8 week intro to EA program for students and recent graduates for the first time. The program was advertised to students of the University of Helsinki, Aalto University and on our mailing list. In the end, 34 participants completed the program. The sessions were obviously held remotely, like our other activities. A lot of attention was paid to the marketing of the intro program, especially visuals, since the intro program organizers felt that it would help the audience perceive EA Helsinki as more of a professional organization and less of a philosophy discussion club.</p><p>Running the intro program had two main benefits:</p><ul><li>It introduced EA to many cool people with important skills, out of whom several have become our organizers now</li><li>It gave the organizers team more confidence in their EA knowledge and ability to actually organize EA stuff</li></ul><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/tbkx6le9n96habazdrmr.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673691845/mirroredImages/Kq3YwsrWr5nxQfpPj/ctp759iutnbfd5pvmwd4.png 200w, https://res.cloudinary.com/cea/image/upload/v1673691845/mirroredImages/Kq3YwsrWr5nxQfpPj/zc4f7jj7ixlzof9kmkqh.png 400w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/zrd9ahwrfp5pkhfohfdt.png 600w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/uolwhxgqdjfjxkzzsqzu.png 800w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/gcdvopi8h8szldcu1kvv.png 1000w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/wqtl45bty1a9xw4ba6r0.png 1200w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/y5colxskrngnmzx2lr3g.png 1400w, https://res.cloudinary.com/cea/image/upload/v1673691845/mirroredImages/Kq3YwsrWr5nxQfpPj/gfvjfn8m3otl55fbdfsj.png 1600w, https://res.cloudinary.com/cea/image/upload/v1673691845/mirroredImages/Kq3YwsrWr5nxQfpPj/ggyzjtf4hmcifynjx8ay.png 1800w, https://res.cloudinary.com/cea/image/upload/v1673691844/mirroredImages/Kq3YwsrWr5nxQfpPj/l34tdnpteoq9b54mdrtq.png 1964w\"><figcaption>Finnish students are generally interested in cool overall badges, so we give some to those who complete the intro program</figcaption></figure><h3>Getting more organized</h3><p>With the sudden rapid growth of people who wanted to do EA stuff and had opinions on how EA Helsinki should do things, I started to get worried we would not be able to organize things in the chaotic way we had done before. Because of the lack of the organizational structure, it was also not obvious who would be in a position to decide that we now need an organizational structure.&nbsp;</p><p>I tried to convince Onni he should organize a strategy day where we can make concrete plans on what to do. Onni said he didn\u2019t know how to do it and as<img>ked me to organize it if I wanted one.&nbsp;</p><p>So I organized a strategy and planning day for EA activities in Finland. We held it outside in a park because of COVID and it was hard to keep all post-its of the planning canvas exercise in place with the wind. Only around 60% of the people who had voiced interest actually showed up, but with those who did, we actually managed to make a plan for the future year:&nbsp;</p><ul><li>We decided to operate in practice as EA Finland and not EA Helsinki because by this time only a fraction of our participants studied in the University of Helsinki, but officially stay EA Helsinki, because nobody was interested in unnecessary bureaucracy</li><li>We set up responsibility areas such as intro program, content, technical, community atmosphere and support. After the strategy day everyone got to state their wishes for a responsibility area in a form, and then we divided up the roles.</li><li>Finally, we had 12 people taking up responsibility areas, out of whom 4 came directly from the intro program</li></ul><p>Most roles were pretty easy to fill, but it was hard to decide on a general coordinator who is responsible for the big picture. Everyone who was discussed for the role had other time constraints.&nbsp;</p><p>Eventually, we ended up deciding that Karla is going to be the general coordinator and Onni is going to help her out, even if Karla was going to exchange to Germany and Onni was busy working on his Master\u2019s Thesis. As expected, this setup meant that people took care of their responsibility areas pretty independently, and neither Karla or Onni could support them as much as they\u2019d originally hoped to.&nbsp;</p><p>Looking back, I\u2019m surprised we didn\u2019t have any major issues despite the lack of available coordination. (Santeri remembers this as \u201cwe deliberately avoided hierarchies\u201d, but I think it was more that nobody was able to step up and take responsibility for EA Finland as a whole.)</p><h3>Donation website project</h3><p>During the fall of 2021, we created a&nbsp;<a href=\"https://lahjoittaminen.fi/\"><u>donation website</u></a> (following the example of other national giving sites), which is currently just a webpage that provides information on effective giving in Finnish and redirects users to the donating pages of the organizations we recommend. I am not sure if the result is super impressive given we had a team of 8 people that spent all together 400 hours on it. We had the option to use graphics and existing listings from other effective giving websites, but we wanted to make a website from scratch and make a conscious decision on what recommendations to include. If we\u2019d gone with a minimal viable product, the project would have taken less time, but on the other hand, now we have more ownership of the result.</p><p>As a volunteer organization, it was also important that we actually managed to complete a project with an end product. Working on something together in a structured way also helped us learn how we function as teams and was motivational for the people involved. For a long time we had just been a discussion club talking about doing good, and now we were at least doing focused public outreach. It felt weird to tell the team that we are creating this website so that a person who would otherwise have died might not die. It was quite a different experience from other volunteer projects I have led.</p><p>Creating the donation website also forced us to actually get our bureaucracy together, so we revitalized the Effective Altruism Finland organization that had been registered in 2013 but that nobody had held association meetings for since then. As an organization it is possible for us to handle money, so we can hopefully at some point start collecting and redirecting donations like the other Nordic donation platforms. (Vesa has been trying to implement this as a part of his work, but it has turned out to be more difficult than we imagined.)</p><h3>Key takeaways:</h3><ul><li>Intro programs are good for upscaling!</li><li>By having some structure of responsibility areas the organization functions better since people know what they are supposed and allowed to do</li><li>It is better to have someone who has an idea of the big picture and knows what is going on, especially if the organization is already a bit bigger. (We had 12 people with named responsibility roles, and around 10 more who were helping out in projects or facilitating intro program cohorts.)</li><li>It is important for volunteering organizations to run projects that actually get completed and have a concrete end product. This makes people notice they can actually achieve things that help others.</li></ul><h2>2021\u20132022 Getting more involved with the global EA community&nbsp;</h2><p>Until the fall of 2021 I don\u2019t think almost anyone in the global EA community was aware that we had become more active as EA Finland. We didn\u2019t ask for any support in setting up things, our group was marked as \u201cinactive\u201d in the EA groups page (until somebody noticed it and fixed it), and we also didn\u2019t have any external funding. We paid for our activities ourselves, but they also often didn\u2019t cost money since we were operating mostly remotely and had access to free university spaces when needed. The most expensive events we organized were giving games that we did as an intro to EA activity a couple of times.</p><p>I would say we were somewhat isolated from the global EA community for a long time. We did have some support from other EAs in the Nordics, such as Onni having calls with Risto Uuk from EA Estonia and Eirin Evjen from EA Norway. I remember we discussed the option of getting support from CEA, but at least I was quite scared to do that, because I thought people at CEA must be very busy and talking to me about organizing some random national group won\u2019t be a priority to them. I also didn\u2019t really know how they would be able to help us since they didn\u2019t know us or the Finnish social environment we were operating in.&nbsp;</p><p>Finally, I ended up talking to Catherine Low from CEA in September 2021. It happened almost by accident: I originally applied to get mentoring from Magnify Mentoring (at the time WANBAM), and they didn\u2019t find me a mentor but recommended that I\u2019d talk to Catherine since I mentioned community building in Finland as one of my areas of focus. For me, this call was important because it made the international EA community seem more real; that there were actual people out there, not just names on the EA forum. From there on, more people from EA Finland have been in more contact with CEA.</p><p>Until 2021, most people from EA Finland had also not participated that much in international EA events, and when they had, it was more in personal capacity than with community building efforts in mind. We also didn\u2019t have a culture of advertising international EA events or sharing notes about our experiences. Some Finns had attended EAG(x)s conferences in 2019 and earlier, and some participated in EAGxVirtual 2020. It sounds like these experiences were valuable for the participants, but there was not much knowledge sharing after them, and I cannot track any single community building step to having been a direct result of a conference attendance.</p><p>The EAGxPrague 2021 conference was the first international conference where we tried to go as a group of EA Finland folks and that was also largely advertised to people who follow our communications. Since it was postponed to 2022, the first conference with a significant number of EA Finland folks attending was EAG London 2022, which was also my first live international EA event. A lot of other Finns went to other EAG(x) conferences in 2022 as well: I can think of at least 20 EA Finland affiliated people who participated in a live conference last year.</p><p>In 2021\u20132022, we also had more people participate in virtual EA programs and get more international EA contacts that way. The AGI Safety Fundamentals 2022 winter course was quite popular, with 8 of our volunteers participating in it.</p><p>And yet, finding a place in the international EA ecosystem is not always easy when many organizations are located in not-Finland. To me, the notion of getting hired at an EA organization went from \u201cpractically impossible\u201d to \u201cvery difficult\u201d when Onni started to work for Rethink Priorities in 2022. Onni was not the first Finn to ever be work for an EA organization (for example, Kaj done work for MIRI already in 2012, and Vesa had been working for EA for Christians since 2021) but it felt different because I had seen most of Onni\u2019s growth from an EA hobbyist to a young EA professional.</p><h3>Key takeaways:</h3><ul><li>It is possible to run EA activities without direct support from other EA groups or CEA, especially since good materials to draw inspiration from exist</li><li>However, it makes sense to ask for support and connect with other EAs, because EA as a global community can do much more than just individual EA groups alone</li></ul><h2>2022: Applying for funding</h2><p>Our community kept on getting bigger. There were new iterations of the intro program that bought in new people. More activity was divided into separate groups, and while EA Finland was the main organization everyone was sort of affiliated to, Aalto EA began to have a lot more activities on their own, EA Tampere was getting more active and EA Helsinki started to have some university-targeted activity that was somewhat separate from other EA Finland activities.</p><p>With all of this, we felt that it started to be really hard to keep up with everything we wanted to do with just volunteer work. As an example, significantly less things got organized for the duration of AGISF because we had 8 active organizers out of ~20 participating in it, and when they all spend 5\u20137 hours a week studying AI safety material, that\u2019s 5\u20137 hours a week less time for group organizing per person.&nbsp;</p><p>We also hoped that having paid workers would help us get all the less pleasant bureaucracy stuff done in a more organized manner, and that this would mean there was at least someone in the organization who could commit enough time to organizing to actually understand what is going on and what everyone is doing. It takes a lot of effort to organically keep track of what all 20 organizers are doing.</p><p>After a lot of processing, we formed a team of 4 potential new employees that would all work part-time, and applied for funding for 2 full time equivalents. At this point, we figured that our core activities that we wanted the employees to support were:</p><ul><li>university and local group support</li><li>volunteer coordination on shorter projects, for example the volunteers\u2019 own initiatives</li><li>intro programs (a new iteration each fall and spring)</li><li>career advising</li><li>lahjoittaminen.fi donation platform development and maintenance&nbsp;</li></ul><p>We found out in April 2022 that our funding request was granted, and the new employees (Santeri, Karla, Vesa and me (Ada)) started to work for EA Finland in May. Around the same time, we also got a grant to pay for community building activities that we could use for event spaces and food.</p><h2>2022: Having funding and employees</h2><p>Since May, we have had some changes and benefits from having paid employees. Describing everything in detail would take too long, and our co-directors Santeri and Karla would have a better picture of it than me, so I just list some main points on a surface level:</p><ul><li>some things now run more reliably, for example we can run the intro program with volunteer facilitators but with an employee taking the responsibility of running the operations</li><li>we have been able to run some things we didn\u2019t have volunteer capacity before, such as career advising<ul><li>I (Ada) have been responsible for the career advising and I\u2019ve mostly tried to set it up so that we would be able to continue the current activities even if we would become a completely volunteer-based organization again, mostly by training volunteers, documenting processes and setting up practices that are easy to maintain</li></ul></li><li>we have also been able to support new EA groups in other Finnish locations (Kuopio, Lappeenranta, Jyv\u00e4skyl\u00e4 and Turku) and continued supporting existing EA activity in Aalto EA, EA Helsinki and EA Tampere</li><li>since some volunteers have had to do less to actually run EA Finland, they have been able to spend more time on university group community building (such as Aalto EA) and EA projects (such as organizing a peer learning version of the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/j9nLvT5ej8mKc4fhi/ml-summer-bootcamp-reflection-aalto-ea-finland\"><u>Machine Learning Intensive Bootcamp</u></a>)<ul><li>on the other hand, having money and employees has also taken some volunteers a lot of time to handle bureaucracy, since there is a lot more legal responsibility now, and some of it legally needs to be handled by the board which cannot have employees in it</li></ul></li></ul><p>Another nice thing that happened in summer 2022 was our first in-person retreat organized by Andrea. The benefits of retreats have often been repeated on posts like this; just like everyone else, we felt that spending some time together helped us get to know each other better and get more motivated to do things together. (The downside of retreats in these times are of course the people who need to miss out because they get a slight flu right before the event.)</p><p>In October 2022, we also rented an office space for EA Finland. It is a coworking space where people can come do EA related work and volunteering, and meet others to build connections among EAs in the area. For organizing small events, having an own space has also proven useful. An extra benefit is the possibility to store EA Finland possessions somewhere other than distributed in people\u2019s homes, especially that we can have a small library to borrow EA books from.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673691842/mirroredImages/Kq3YwsrWr5nxQfpPj/segerd2izerhjpaqdmdl.jpg\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/sqvklyevm2fvrupijh1z.jpg 130w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/l4oazcsjqkfnmr39lgxt.jpg 260w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/kj9gyamspbmhtybaznbi.jpg 390w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/iuqrlpwqdsrilwretwfv.jpg 520w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/zmn1tbgcbeglzplqyshn.jpg 650w, https://res.cloudinary.com/cea/image/upload/v1673691842/mirroredImages/Kq3YwsrWr5nxQfpPj/hjdpwwisrzwmi96fcutq.jpg 780w, https://res.cloudinary.com/cea/image/upload/v1673691842/mirroredImages/Kq3YwsrWr5nxQfpPj/u0buxeckgc0feakgfqk9.jpg 910w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/gvhskplqvimi9rmybm4t.jpg 1040w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/gecpqshkylbylzu8pzyu.jpg 1170w, https://res.cloudinary.com/cea/image/upload/v1673691843/mirroredImages/Kq3YwsrWr5nxQfpPj/mz15s5pzyr63ji0wafpy.jpg 1280w\"><figcaption>The kitchenette/library part of the EA Finland office and coworking space</figcaption></figure><h2>2023: We\u2019ll continue from here</h2><p>It is now early 2023. We hope to continue growing and setting up good practices for doing EA things in Finland. I am especially hopeful about new EA groups in different Finnish cities and some EA related volunteer projects our members have been working on.</p><p>Community building can also sometimes feel frustrating, because the effects are uncertain and there is a delay before you see them. Having been involved in active organizing for several years now, I notice that sometimes the process from first hearing about EA ideas and actually making big career or life related decisions can take a long time.&nbsp;</p><p>Community building is not just about introducing EA ideas to people and hoping that they quickly start to act on them. If we manage to create sustainable communities that people can be a part of for a long time, we get to see all the cool things people do 3 or 5 or 10 years after first hearing about EA.</p>", "user": {"username": "Ada-Maaria Hyv\u00e4rinen"}}, {"_id": "u5gLprWhFDJLxooLc", "title": "Speak the truth, even if your voice trembles", "postedAt": "2023-01-14T06:10:23.470Z", "htmlBody": "<p><i>Epistemic status: Motivated by the feeling that there's something like a </i><a href=\"https://www.econlib.org/archives/2016/01/the_invisible_t.html\"><i>missing mood</i></a><i> in the EA sphere. &nbsp;Informed by my personal experience, not by rigorous survey. &nbsp;Probably a bit scattershot, but it's already more than a month after I wanted to publish this. &nbsp;(Minus this parenthetical, this post was entirely written before the Bostrom thing. &nbsp;I just kept forgetting to post it.)</i></p><p>The last half year - the time since I moved to Berkeley to work on LessWrong, and consequently found myself embedded in the broader Bay Area rationality &amp; EA communities - have been surprisingly normal.</p><p>The weeks following the FTX collapse, admittedly, a little less so.</p><p>One thing has kept coming up, though. &nbsp;I keep hearing that people are reluctant to voice disagreements, criticisms, or concerns they have, and each time I do a double-take. &nbsp;(My consistent surprise is part of what prompted me to write this post: both those generating the surprise, and those who are surprised like me, might benefit from this perspective.)</p><p>The type of issue where one person has an unpleasant<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjyot6r3gvm\"><sup><a href=\"#fnjyot6r3gvm\">[1]</a></sup></span>&nbsp;interaction with another person is difficult to navigate. &nbsp;The current solution of discussing those things with the <a href=\"https://www.centreforeffectivealtruism.org/team#community-health-team\">CEA Community Health team</a> at least tries to balance both concerns of reducing false positive and false negatives; earlier and more public discussion of those concerns is not a Pareto-improvement<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefi0yhrox578f\"><sup><a href=\"#fni0yhrox578f\">[2]</a></sup></span>.</p><p>But most of them are other fears: that you will annoy an important funder, by criticizing ideas that they support, or by raising concerns about their honesty, given publicly-available evidence, or something similar. &nbsp;And the degree to which these fears have shaped the epistemic landscape makes me feel like I took a wrong turn somewhere and ended up in a <a href=\"https://tvtropes.org/pmwiki/pmwiki.php/Main/MirrorUniverse\">mirror universe</a>.</p><p>Having these fears - probably common! &nbsp;Discussing those fears in public - not crazy! &nbsp;<i>Acting on those fears</i>? &nbsp;(I keep running face-first into the fact that not everybody has read <a href=\"https://www.lesswrong.com/rationality\">The Sequences</a>, that not everybody who has read them has internalized them, and that not everybody who has internalized them has externalized that understanding through their actions.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9zif6nvk6ul\"><sup><a href=\"#fn9zif6nvk6ul\">[3]</a></sup></span>)</p><p>My take is that acting on those fears, by not publishing that criticism, or raising those concerns, with receipts attached, is harmful<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyl95p0repzr\"><sup><a href=\"#fnyl95p0repzr\">[4]</a></sup></span>. &nbsp;For simplicity's sake, let's consider the cartesian product of the options:</p><ul><li>to publicize a criticism, or not</li><li>the criticism being accurate, or not</li><li>the funder deciding to fund your work, or not</li></ul><p>The set of possible outcomes:</p><ol><li>you publicize a criticism; the criticism is accurate; the funder funds your work</li><li>you publicize a criticism; the criticism is accurate; the funder doesn't fund your work</li><li>you publicize a criticism; the criticism is inaccurate; the funder funds your work</li><li>you publicize a criticism; the criticism is inaccurate; the funder doesn't fund your work</li><li>you don't publicize a criticism; the criticism is accurate; the funder funds your work</li><li>you don't publicize a criticism; the criticism is accurate; the funder doesn't fund your work</li><li>you don't publicize a criticism; the criticism is inaccurate; the funds your work</li><li>you don't publicize a criticism; the criticism is inaccurate; the funder doesn't fund your work</li></ol><p>What predicted outcomes are motivating these fears? &nbsp;2 and 4 are the obvious candidates.</p><p>I won't pretend that these are impossible, or that you would necessarily see another funder step in if such a thing happened. &nbsp;You could very well pay costs for saying things in public. &nbsp;I do think that people overestimate how likely those outcomes are, or how high the costs will be, and underestimate the damage that staying silent causes to community epistemics.</p><p>But I will bite the bullet: assuming the worst, you should pay those costs. &nbsp;In the long run, you do not achieve better outcomes by pretending to have beliefs other than those you have, in order to extract grant money from intolerant funding sources.</p><p>If your criticism is accurate, and a potential source of funding decides to not fund you <i>when they would have otherwise because of it</i>, the only way for the others to orient and react to that defection is for them to see the criticism<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefub8dk8qyxfp\"><sup><a href=\"#fnub8dk8qyxfp\">[5]</a></sup></span>&nbsp;and subsequent lack of funding<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref83hs2k3om45\"><sup><a href=\"#fn83hs2k3om45\">[6]</a></sup></span>.</p><p>If your criticism is <i>not</i> accurate, and a potential source of funding decides to not fund you as a result, the details end up being pretty important. &nbsp;From the funder's perspective, the \"best\" possible reason for that kind of decision is if the criticism betrays serious intellectual or epistemic failure by the critic. &nbsp;This might happen in the <a href=\"https://www.lesswrong.com/posts/neQ7eXuaXpiYw7SBy/the-least-convenient-possible-world\">least convenient possible world</a>, but in practice I think most such fears, when coming from good-faith actors, are the product of imposter syndrome. &nbsp;(Needless to say, grifters and other bad actors are correct to have such fears. &nbsp;Making EA more robust to adversarial forces is another excellent reason for being forthright about one's honest opinions.)</p><p>Then there are criticisms which one might fear would cause them to pay more indirect social costs. &nbsp;Take as examples <a href=\"https://forum.effectivealtruism.org/posts/qjsWZJWcvj3ug5Xja/agrippa-s-shortform?commentId=SP7AQahEpy2PBr4XS#SP7AQahEpy2PBr4XS\">this</a> and <a href=\"https://forum.effectivealtruism.org/posts/oLWJdeXnv56GsD2KR/there-are-currently-more-than-100-open-ea-aligned-tech-jobs?commentId=rzXpzSjeQ7NzuroLu\">this</a>. &nbsp;Let me also take this opportunity to put my money where my mouth is, by making public a disagreement I have<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffey66c737vn\"><sup><a href=\"#fnfey66c737vn\">[7]</a></sup></span>. &nbsp;I do not think we should be inviting <a href=\"https://fathomradiant.co/\">AI hardware capabilities organizations</a> to EAG(x) career fairs. &nbsp;The proposed <a href=\"https://www.lesswrong.com/posts/Br6MRY9mQG2b8hie2/do-any-ai-alignment-orgs-hire-remotely?commentId=Kn65vMrfB5bFbHNxR\">theory of change</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9dbea73qf3s\"><sup><a href=\"#fn9dbea73qf3s\">[8]</a></sup></span>&nbsp;suffers, on priors, from being dominated by the 1st-order effects of having better AI hardware. &nbsp;If you want to subsidize hardware for alignment research, doing it by starting an <i>general AI hardware capabilities organization</i> seems deeply perilous. &nbsp;Just start <s>a crypto exchange</s> literally any other startup!</p><p>There are some practical takeaways from adopting this stance:</p><ul><li>It is important to support someone who pays costs as a result of publishing rigorous, well-motivated criticism. &nbsp;Relying on an after-the-fact process to catch you if you step on a broken stair is scary enough; <i>knowing that there's no net at all</i> renders this hardly more useful than yelling into the void. &nbsp;There will be those laudable individuals willing to take the leap regardless, but it is simply good policy to support those who pay costs to generate positive externalities. &nbsp;I'm not really sure what this support looks like, and there are obvious difficulties with trying to formalize anything here, but it would be good for <i>something</i> to exist. &nbsp;Maybe after-the-fact prizes to those who proffered early EA-related criticisms, which were ignored/misunderstood/rabbit-holed at the time, but have since been integrated? &nbsp;I believe this has happened at least once but can't currently find a reference.</li><li>You should carefully consider the price of your silence. &nbsp;There are good reasons to be able to credibly promise that you will keep certain things secret, but many conversations end up happening in a totally unnecessary regime of secrecy out of social inertia. &nbsp;NDAs are probably much more expensive than naive calculations would suggest.</li><li>Correspondingly, <i>defaults are very important</i>. &nbsp;I claim that you should default to openness. &nbsp;This forces you to be explicit about what you agree to keep secret, and reduces ambiguity about other people's expectations of you (which in turn reduces your own mental overhead for tracking those expectations).</li><li>Notice when you are flinching away from considering a specific course of action.</li><li>It helps a lot to be resilient to the \"things go to shit because you decided you weren't going to stay quiet about something bad\" scenario. &nbsp;One reason I might be surprised by the reports I hear of self-suppressed criticisms is some mixture of the typical mind fallacy and the fundamental attribution error. &nbsp;My realistic worst-case outcome, assuming I somehow managed to piss off everyone doing hiring and funding in domains I consider important, is that I have to give up on direct work entirely. &nbsp;I switched to direct work as a mid-career software engineer with significant prior experience in industry, and if my former employer's fortunes have changed enough that they no longer want me back, I'm not concerned about my ability to find another industry role, nor am I under any meaningful time pressure to do so. &nbsp;My friends and family in LA would also be quite glad to have me back. &nbsp;Don't misunderstand: this would suck a lot. &nbsp;But it would suck because of what it implied about my ability to effect the kind of change that motivated me to make the jump in the first place, not because I'd be totally bereft of social and professional opportunities as a result. &nbsp;The same is not necessarily true of many others, and I expect that makes it much harder when such a situation arises.</li></ul><p>The main thing I want this post to accomplish for readers is to raise to the level of conscious awareness the existence of these dynamics, and to hopefully let them notice in real-time if they ever run into them. &nbsp;It's much easier to choose to do the right thing when you consciously notice the choice in front of you.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjyot6r3gvm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjyot6r3gvm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Or worse!</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fni0yhrox578f\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefi0yhrox578f\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Though I can conceive of a case that it'd be a Kaldor-Hicks improvement.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9zif6nvk6ul\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9zif6nvk6ul\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Not everybody needs to read The Sequences to understand why being honest and not submitting to blackmail &amp; extortion are both critical to establishing healthy equilibria in communities. &nbsp;I, personally, did become more scrupulously honest after internalizing those lessons.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyl95p0repzr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyl95p0repzr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I'm not totally sure how I relate to <a href=\"https://www.lesswrong.com/posts/rMfpnorsMoRwyn4iP/choosing-the-zero-point\">setting the zero point</a>, or to what things should be considered superogatory. &nbsp;In this case I chose the word \"harmful\" because it feels like the correct frame due to the background context, but I don't think a different choice would be crazy.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnub8dk8qyxfp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefub8dk8qyxfp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Which has the advantage of being accurate, and is therefore advantaged in that astute observers are more likely to consider it correct!</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn83hs2k3om45\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref83hs2k3om45\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Unfortunately probabilistic, but since \"obviously good\" ideas tend to be slam dunks across multiple funding sources, it wouldn't take many such cases to establish a pattern.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfey66c737vn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffey66c737vn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The originating thought came from someone else, but I agree with it.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9dbea73qf3s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9dbea73qf3s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Which, to be clear, is not coming directly from the organization, so may not be an accurate representation of their views.</p></div></li></ol>", "user": {"username": "T3t"}}, {"_id": "aAhj3mvGkKaJqqJd4", "title": "Is anyone here a philosopher or economist working on the interpersonal comparison problem?", "postedAt": "2023-01-14T07:07:49.663Z", "htmlBody": "<p>This is a bit of a shot in the dark, but I'm doing a Ph.D. on the philosophy of welfare economics, and I have a chapter dedicated to interpersonal comparison. I am eager to discuss the topic. Let me know if this area interests you- I'd be keen to have a look at anything you've published- to discuss ideas, and potentially to collaborate on research.</p>", "user": {"username": "Philosophy Bear"}}, {"_id": "frcAPFXwiCpNrECgQ", "title": "A general comment on discussions of genetic group differences", "postedAt": "2023-01-14T07:09:46.766Z", "htmlBody": "<p>(content warning: discussion of racially motivated violence and coercion)</p><p>&nbsp;</p><p>I wanted to share that I think it's not bad to think about the object level question of whether there are group differences in intelligence rooted in genetic differences. This is an empirical claim, and can be true or false.</p><p>My moral beliefs are pretty rooted in egalitarianism. I think as a matter of policy, but also as a matter of moral character, it is good and important to treat the experience of strangers as equally valuable, regardless of their class or race. I do not think more intelligent people are more worthy of moral consideration than less intelligent people. I think it can be complicated at the extremes, especially when considering digital people, animals, etc., but that this has little bearing on public policy when concerning existing humans.</p><p>I don't think genetic group differences in intelligence are likely to be that relevant given I have short AI timelines. If we assume longer timelines, I believe the most likely places they would be important in terms of policy would be in education and reproductive technology. Whether or not there are such differences between groups now, there could easily come to be large differences through the application of embryo selection techniques or other intelligence enhancing technologies. From an egalitarian moral framework, I suspect it would be important to subsidize this technology for disadvantaged groups or individuals so that they have the same options and opportunities as everyone else. Even if genes turn out to not be a major cause of inegalitarian outcomes today, they can definitely become a major cause in the future, if we don't exercise wisdom and thoughtfulness in how we wield these technologies. However, as I said, I don't expect this to be very significant in practice given short AI timelines.</p><p>Most importantly, from my perspective, it's important to be able to think about questions like this clearly, and so I want to encourage people to not feel constrained to avoid the question because of fear of social censure for merely thinking about them. For a reasonably well researched (not necessarily correct) discussion of the object level, see this post:</p><p>[link deleted at the author's request; see also <a href=\"https://forum.effectivealtruism.org/posts/kuqgJDPF6nfscSZsZ/thread-for-discussing-bostrom-s-email-and-apology?commentId=rErHnkYP4GvioFj5o#comments\">AnonymousCommentator's note about the racial IQ gap</a>]</p><p>I think it's important context to keep in view that some of the worst human behaviors have involved the enslavement and subjugation of whole groups of people, or attempts to murder entire groups\u2014racial groups, national groups, cultural groups, religious groups. The eugenics movement in the United States and elsewhere attempted to significantly curtail the reproductive freedom of many people through extremely coercive means in the not-so-distant past.&nbsp; Between 1907 and 1963, over 64,000 individuals were <a href=\"https://en.wikipedia.org/wiki/Eugenics_in_the_United_States#Compulsory_sterilization\">forcibly sterilized under eugenic legislation in the United States</a>, and minority groups were especially targeted. Presently in China, <a href=\"https://foreignpolicy.com/2020/07/01/china-documents-uighur-genocidal-sterilization-xinjiang/\">tens of thousands of Uighurs are being sterilized</a>, and while we don't have a great deal of information about it, I would predict that there is a major element of government coercion in these sterilizations.</p><p>Coercive policies like this are extremely wrong, and plainly so. I oppose and condemn them. I am aware that the advocates of these policies sometimes used genetic group differences in abilities as justification for their coercion. This does not cause me to think that I should avoid the whole subject of genetic group differences in ability.&nbsp; Making this subject taboo, and sanctioning anyone who speaks of it, seems like a sure way to prevent people from actually understanding the underlying problems disadvantaged groups or individuals face. This seems likely to inhibit rather than promote good policy-making. I think the best ways to resist reproductive and other forms of coercion go hand in hand with trying to understand the world, do good science, and have serious discussions about hard topics. I think strict taboos around discussing an extremely broad scientific subject matter hurt the ability of people to understand things, especially when the fear of public punishment is enough to prevent people from thinking about a topic entirely.</p><p>Another reason people cite for not talking about genetically mediated group differences, even if they exist, is that bringing people's attention to this kind of inequality could make the disadvantaged feel terrible. I take this cost seriously, and think this is a good reason to be really careful about how we discuss this issue (the exact opposite of Bostrom's approach in the <a href=\"https://twitter.com/anderssandberg/status/1613259477263720449\">Extropians email</a>), and a good reason to include content warnings so anyone can easily avoid this topic if they find it upsetting.</p><p>But I <i>don't</i> think forbidding discussion of this topic across the board is the right society-level response.</p><p>Imagine a society where knowledge of historical slavery is suppressed, because people worry it would make the descendants of enslaved people sad. I think such a society would be unethical, <i>especially </i>if the information suppression causes society to be unable to recognize and respond to ongoing harms caused by slavery's legacy.</p><p>Still, assuming that we were in a world like that: In that kind of world, we can imagine that the information leaks out and a descendant of slaves finds out about slavery and its legacy, and is (of course) tremendously horrified and saddened to learn about all this.</p><p>If someone pointed at this to say, \"Behold, this information caused harm, so we were right to suppress it,\" I would think they're making a serious moral mistake.</p><p>If the individual themselves didn't want to personally know about slavery, or about any of the graphic details, that's fully within their right. This should be comparatively easy to achieve in online discussion, where it's easier to use content warnings, tags, and web browser apps to control which topics you want to read about.</p><p>But society-wide <i>suppression</i> of the information, for the sake of protecting people's feelings <i>even though those individuals didn't consent to being protected from the truth this way</i>, is frankly disturbing and wrong. This is not the way to treat peers, colleagues, or friends. It isn't the way to treat people who you view as full human beings; beyond just being a terrible way to carry out scientific practice, it's infantilizing and paternalistic in the extreme.</p>", "user": {"username": "anonymous8101"}}, {"_id": "tQLdiFHGgrj5PezTL", "title": "A short thought on supporting EA despite recent issues", "postedAt": "2023-01-13T23:55:00.448Z", "htmlBody": "<p>Some events in past months seem to show that some people and organizations in the Effective Altruism network have (or previously had) some harmful attitudes, bad motivations, or reckless approaches to decision making. But this doesn't do much to shake my support of EA as a whole.&nbsp;</p><p>At the end of the day, I support it for the same type of reason I support the Democratic party in the US. It's a coalition that has problems, but it gets important things done and much better aligns with working toward the kind of world I want to live in than alternatives. Like any 'ism' with significant following and influence, there are bad nodes and edges in the network. <strong>But I think the best approach to supporting institutions is &nbsp;to be pragmatic, not purist. </strong>Institutional support shouldn't be attire.&nbsp;</p><p>But I still really hope that EA takes away a message about fixing these problems. None of them should be excused, and all of them are harmful toward EA's goals of doing good things in the world.&nbsp;</p>", "user": null}, {"_id": "oZKkkNTXFtYfqefmn", "title": "What are people up to in the world?", "postedAt": "2023-01-13T23:25:34.220Z", "htmlBody": "<p>This post started as a comment to <a href=\"https://forum.effectivealtruism.org/users/rosehadshar\"><strong>rosehadshar</strong></a><strong>'</strong>s wonderful post <a href=\"https://forum.effectivealtruism.org/posts/rXYW9GPsmwZYu3doX/what-happens-on-the-average-day\">What happens on the average day?</a> which I would recommend reading first, as this is partly an extension to her post. The cosmopolitan in me shares with her an interest in what's going on in the world, and this post hopefully serves to clarify another piece of that for others interested in this unique species we are a part of. &nbsp;Below is a summary, and below that you can find some more thoughts on this post, and the various ways I came to these results, included to hopefully provide some further epistemic clarity and make it easy for anyone who might wish to extend this work.</p><p><strong>TLDR</strong> I tried to crunch the numbers to find out what the global population, at a general group level, is doing on a given day, and here are my rough results:</p><ul><li>42% are going to work</li><li>13% are going to school</li><li>10% are retired</li><li>9% are under 5 and just chilling enjoying life</li><li>8% (less confident in this) are perusing tertiary education &nbsp;</li><li>4% are children of school age not going to school</li><li>3% are unemployed</li><li>11% are mysterious little creatures I can't account for</li></ul><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673652336/mirroredImages/oZKkkNTXFtYfqefmn/jkiwbvwangy8pjprbji9.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673652337/mirroredImages/oZKkkNTXFtYfqefmn/zyb77qcjfzrtt29tjrj8.png 110w, https://res.cloudinary.com/cea/image/upload/v1673652336/mirroredImages/oZKkkNTXFtYfqefmn/egufwsfkzwl7ir6vgo4k.png 220w, https://res.cloudinary.com/cea/image/upload/v1673652336/mirroredImages/oZKkkNTXFtYfqefmn/ca5uhn1w2dytcvyqwymk.png 330w, https://res.cloudinary.com/cea/image/upload/v1673652336/mirroredImages/oZKkkNTXFtYfqefmn/ta6ffpyhryfxfkfx0bhk.png 440w, https://res.cloudinary.com/cea/image/upload/v1673652336/mirroredImages/oZKkkNTXFtYfqefmn/piuarmsqztuotmonopwk.png 550w, https://res.cloudinary.com/cea/image/upload/v1673652336/mirroredImages/oZKkkNTXFtYfqefmn/na9r1fs8q6rzxmugoayp.png 660w, https://res.cloudinary.com/cea/image/upload/v1673652336/mirroredImages/oZKkkNTXFtYfqefmn/ddteaukev1mmcnn87ti6.png 770w, https://res.cloudinary.com/cea/image/upload/v1673652336/mirroredImages/oZKkkNTXFtYfqefmn/env8jgmucrjldpbnuu5v.png 880w, https://res.cloudinary.com/cea/image/upload/v1673652336/mirroredImages/oZKkkNTXFtYfqefmn/v8c5xgkqhfxpkkvciqa8.png 990w, https://res.cloudinary.com/cea/image/upload/v1673652335/mirroredImages/oZKkkNTXFtYfqefmn/yemn5oocnpsmbqvdrof3.png 1024w\"></figure><p>Now, if it wasn't clear already, there are varying levels of possible depth here, and I don't have the time to figure out the most specific level, that is, what each of the possible activities for a day are for all those in the world and what percentage of people are doing each for what percentage of the time (though I think this would be a wonderfully informative project and would love to see it). But it's my hope that we can still get a rough and interesting picture by viewing this at a higher level,<strong> breaking the population into general groups based on people's main waking hour allotment </strong>that hopefully capture a general shared way of moving about the world (i.e. those that work on an average day are doing radically different things, but are all spending a large chunk of their waking hours doing some task to try to make money, which seems to still be a coherent category, if very broad and amorphous). &nbsp;So with this in mind, let's get down to the nitty gritty: just what is everyone up to?</p><p>From rosehadshar's <a href=\"https://forum.effectivealtruism.org/posts/rXYW9GPsmwZYu3doX/what-happens-on-the-average-day\">numbers</a>:</p><ul><li>3.3 billion people (roughly 42% of the global population) go to work</li><li>990 million children (roughly 13% of the population) go to school.&nbsp;</li></ul><p>So we know that roughly 55% of the population is working or going to school, so what are the other 45% up to?</p><p>To estimate this, I started with <a href=\"https://ourworldindata.org/age-structure#:~:text=The%20global%20median%20age%20has,bracket%20between%2025%20and%2065.\">this breakdown</a> of the world population by age that I found from 2021<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefriug0710s\"><sup><a href=\"#fnriug0710s\">[1]</a></sup></span></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673652336/mirroredImages/oZKkkNTXFtYfqefmn/jtomyngklymxandz5k3d.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673652337/mirroredImages/oZKkkNTXFtYfqefmn/vrxx0jaaaycdfiuoz7at.png 340w, https://res.cloudinary.com/cea/image/upload/v1673652336/mirroredImages/oZKkkNTXFtYfqefmn/vlva7ggpnkvsjjgraurr.png 680w, https://res.cloudinary.com/cea/image/upload/v1673652336/mirroredImages/oZKkkNTXFtYfqefmn/hgdslw9gihey65jgigjx.png 1020w, https://res.cloudinary.com/cea/image/upload/v1673652336/mirroredImages/oZKkkNTXFtYfqefmn/dm7hmq1hvblhv1cjr7q7.png 1360w, https://res.cloudinary.com/cea/image/upload/v1673652337/mirroredImages/oZKkkNTXFtYfqefmn/jm3ilr4lyzucj5ptsmo1.png 1700w, https://res.cloudinary.com/cea/image/upload/v1673652336/mirroredImages/oZKkkNTXFtYfqefmn/nv6az0kiuyk9ynuuavj6.png 2040w, https://res.cloudinary.com/cea/image/upload/v1673652336/mirroredImages/oZKkkNTXFtYfqefmn/kj2zge0lfvnnqzfxh6iw.png 2380w, https://res.cloudinary.com/cea/image/upload/v1673652336/mirroredImages/oZKkkNTXFtYfqefmn/dfa7084szfqtq2rn0lqu.png 2720w, https://res.cloudinary.com/cea/image/upload/v1673652337/mirroredImages/oZKkkNTXFtYfqefmn/b9psfjtjixyxe9n4lbon.png 3060w, https://res.cloudinary.com/cea/image/upload/v1673652335/mirroredImages/oZKkkNTXFtYfqefmn/zlbkxrx2xagsfjovfbub.png 3400w\"></figure><p>The above in percentages:</p><ul><li>9% are Under 5</li><li>17% are 5-14</li><li>15% are 15-24</li><li>49% are 25-64</li><li>10% are 65+</li></ul><p>The general arc was then taking this data of the general age breakdown and seeing if I could glean any immediate numbers on what some of the world is doing just from knowing their age.</p><p><strong>Under 5</strong>: An obviously place to start seems to be the Under 5 group. Given that six seems to be the standard age kids start to go to school in <a href=\"https://www.statista.com/chart/13378/when-do-children-start-school-in-europe/#:~:text=Across%20much%20of%20the%20continent,according%20to%20the%20European%20Commission.\">Europe</a> and <a href=\"https://www.scholaro.com/db/countries/china/education-system#:~:text=In%20China%2C%20primary%20school%20education,(seven%20in%20some%20areas).\">China</a>, &nbsp;and <a href=\"https://nces.ed.gov/programs/statereform/tab5_3.asp\">five</a> in the US, it seems roughly safe to conclude that the 9% of the population Under 5 is largely doing what you'd expect: just living life and being taken care of. So this provides us our first number of further knowledge to add, adding to make it such that we now know what 64% are up to, only 36% to go.</p><p><strong>Unemployed</strong>: The next group that came to mind was the unemployed, a group that I spent a lot of time painstakingly trying to calculate numbers for<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5pjo8z5wuwa\"><sup><a href=\"#fn5pjo8z5wuwa\">[2]</a></sup></span>&nbsp;by using the percentage of total labor force numbers, to then find a convient raw number of unemployed people from <a href=\"https://www.ilo.org/global/research/global-reports/weso/trends2021/WCMS_794452/lang--en/index.htm\">a legitimate source</a> after much search. <a href=\"https://www.statista.com/statistics/266414/unemployed-persons-worldwide/\">214.21 million people</a> were unemployed in 2021, making this group <strong>roughly 3%</strong> of the total population (a number that agrees with one of my estimates from my first attempt<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5pjo8z5wuwa\"><sup><a href=\"#fn5pjo8z5wuwa\">[2]</a></sup></span>). Cool, now we're at 67%, where to next?</p><p><strong>Retirement</strong>: Well naturally I continue up the ladder to the next biggest general group I can think of: retirees. A glance at <a href=\"https://en.wikipedia.org/wiki/Retirement_age\">Wikipedia</a> gives me a sense that retirement age ranges somewhat significantly (from 55 in Sri Lanka to 68 in the Netherlands), and I know that assumptions about what people in this age range are doing is more fraught than the Under 5 category because, well, they have options as to what they are doing (they could continue working is a salient example). But based on how 65+ Americans choose to spend their time, it would seem like <a href=\"https://www.bls.gov/opub/mlr/2007/05/art2full.pdf\">&lt; 6% of them are working</a>, largely spending their time on things such as household work and \"leisure activities\", and though I certainly would like to see this replicated in data from other countries, I think the retirement statistics from before taken with this can give us mild confidence that retirement is largely its own class separate from other groups, identified by those in the 65+ category, <strong>so 10%</strong>. So horary! We've now made our way to 77%, but this is where things get a bit harder.</p><p>To recap, we've covered Under 5, we've covered kids going to school, we've covered those working, we've covered the unemployed, and we've covered the retired, so it would seem we've hit most of the large categories of \"ways people are focusing their day to day life\". &nbsp;My next intuition was that the children going to school number was at first capturing my mental \"all the people getting education number\" but on reflection this number likely doesn't capture those getting tertiary education and such, so there we turn next.</p><p><strong>Children Out of School:</strong> But wait! There was a surprise! As I went to source <a href=\"https://forum.effectivealtruism.org/users/rosehadshar\"><strong>rosehadshar</strong></a> numbers for children in education, he also had this other interesting category in <a href=\"https://docs.google.com/spreadsheets/d/1BOAAwTV9erPLF5vBvbmpN2QJlFmEzH-YqV66aHRQd4w/edit#gid=2131412402\">the spreadsheet</a> of \"Children Out of School\" which seems to apply to a group of about 262 million children of school age who &nbsp;for some reason are not currently going. So here was <strong>a bonus 4%</strong> (note, this number was from 2014, so I had to use the population of 7.318 billion for calculations) that wasn't even expected, bringing us to 81%.</p><p><strong>Tertiary Education (college mostly):</strong> Back to the point though, &nbsp;this exploration soon proved to be the hardest yet, as there seems to be no straightforward answer to this question easily found on the internet. I first found <a href=\"https://ourworldindata.org/grapher/gross-enrollment-ratio-in-tertiary-education?tab=chart&amp;country=Arab+World~OWID_WRL\">this</a> Our World In Data chart which expresses \"Gross enrollment ratio in tertiary education\" which means the \"total enrollment in tertiary education, regardless of age, expressed as a percentage of the total population of the five-year age group following on from secondary school leaving\". Perhaps I'm missing something here, but it seems hard to work back to this number as a percentage of the global population, because I don't have (and can't seem to find) more precise estimates for the distributions of age in the general 15-24 age group. We could assume it's evenly distributed across (which I have no evidence to indicate, and was part of why I threw out an earlier strain calculation<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5pjo8z5wuwa\"><sup><a href=\"#fn5pjo8z5wuwa\">[2]</a></sup></span>) which gives us roughly 8.33%, and though I wanted something more solid here (and tried to find it in the calculations here<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmyp69w1d9us\"><sup><a href=\"#fnmyp69w1d9us\">[3]</a></sup></span>).&nbsp;<strong> So 8.33% (rounded to 8%)</strong> is our best estimate after spending a fair deal of time trying to figure this one out (but again, if you find better numbers please let me know in the comments, I'll be happy to amend) putting us at 89%.</p><p><strong>Mysterious Creatures:</strong> The <strong>other 11%</strong>, I'm afraid, will have to remain a mystery, one that I would love to see explored in response to this. This was roughly done and I could easily put double the amount of time into fact checking the numbers used and following rabbit holes (i.e. does this <a href=\"https://ourworldindata.org/grapher/age-dependency-breakdown\">Age dependency breakdown by young and old dependents</a> change things? Quick math says no, it strengthens earlier conclusions, but I'd like to look at it longer to be sure).&nbsp;</p><p>Anyways, a further thing I'd love to see explored here is the next step in specificity, that is, a breakdown of <strong>what exactly it is</strong> these groups are doing. To be specific: <strong>a post I'd love to see next is breaking down what sorts of things these groups are doing as percentages of parts of their days</strong> (i.e. those perusing tertiary education spend roughly <a href=\"https://www.researchgate.net/publication/275182301_How_Students_Spend_Their_Time\">4 hours outside of class</a> working on classwork, a highly interesting number as mine was significantly different). Not only can this help people learn more about groups we are already familiar with or part of, I think it can also move further towards the original spirit of the post, that is, getting to know more about the world that we are unfamiliar with. For instance, I'm fascinated by the School Age Children not Going to School. What are they doing on an average day? Are they working to help support a family financially? Are they on the street? What about retired people across the world? Do they replicate the trends in the US of spending a ton of time on house maintenance and such?&nbsp;</p><p>Like <a href=\"https://forum.effectivealtruism.org/users/rosehadshar\"><strong>rosehadshar</strong></a> I'm fascinated by learning about what others do, and hope this can fill you in a bit more, and inspire you to go even further than I have and carry the torch another step, I'd love to hear what you find.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnriug0710s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefriug0710s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The World in Data breakdown I draw from is from 2021, so I use the population estimate for then throughout, which is roughly 7.91 billion.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5pjo8z5wuwa\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5pjo8z5wuwa\">^</a></strong></sup></span><div class=\"footnote-content\"><p>It would seem that <a href=\"https://data.worldbank.org/indicator/SL.UEM.TOTL.ZS\">roughly 6%</a> (as a percentage of the total labor force) are unemployed, which is helpful, but when one realizes that \"a percentage of the total labor force\" is <a href=\"https://www.census.gov/quickfacts/fact/note/US/LFE041221\">defined as those 16 and older</a>, it gets a bit trickier. We know this rules out the 9% from the Under 5 category, and the 17% from the 5-14 category, but its somewhat unclear what portion from the 15-24 category we should include. 15 is approximately 1/10th of this age range, so we could assume that the portion is evenly distributed across the range (a somewhat dubious assumption that I feel confident is not exactly right but that I hope is approximately right, and that I would gladly correct if I had more time to find the exact numbers here). This means that 27.5% (9 + 17 + (1/10 * 15)) is the category excluded, so the labor force should be 6% of the remaining 72.5% is the portion of the population unemployed, or roughly 4% (0.06 * 72.5). I tried to check this by separately seeking out an estimate of the total global workforce, which, <a href=\"https://data.worldbank.org/indicator/SL.TLF.TOTL.IN\">under the UN's definition of labor force</a> which \"comprises people ages 15 and older who work\" including people who are currently employed, those unemployed but seeking work, and first-time job-seekers. So it differs slightly in parameters, but the estimate here of 3.46 billion leads to an estimate of 6% of 3.46 billion, which is 207.6 million, which is 2.6 or 3%, which is a different answer, so now I'm confused.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmyp69w1d9us\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmyp69w1d9us\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I finally found <a href=\"https://ourworldindata.org/grapher/projection-of-world-population-ssp2-iiasa\">a projection</a> that seems more solid, which estimates roughly 841 million people to have attained post-secondary education globally, but this is meant to capture a group that could range anywhere from a masters (1 year) to a PHD (up to 6 years), making it hard to again make any good estimate beyond just knowing that roughly 11% of the population has completed tertiary education. I digress and revert back to the somewhat dubious <strong>8.33%.</strong></p></div></li></ol>", "user": {"username": "tswizzle96"}}, {"_id": "yikgpenpTA9mDHA9o", "title": "Where are the headlines about EA\u2019s successes?", "postedAt": "2023-01-13T22:54:56.060Z", "htmlBody": "<p>Are EA orgs bad at engaging with the media / do they tend not to do press releases talking about positive impact and successes?</p>\n", "user": {"username": "freedomandutility"}}, {"_id": "jgspXC8GKA7RtxMRE", "title": "On Living Without Idols", "postedAt": "2023-01-13T23:35:59.734Z", "htmlBody": "<p>For many years, I've actively lived in avoidance of idolizing behavior and in pursuit of a nuanced view of even those I respect most deeply. I think this has helped me in numerous ways and has been of particular help in weathering the past few months within the EA community. Below, I discuss how I think about the act of idolizing behavior, some of my personal experiences, and how this mentality can be of use to others.</p><p><strong>Note:</strong> I want more people to post on the EA Forum and have their ideas taken seriously regardless of whether they conform to Forum stylistic norms. I'm perfectly capable of writing a version of this post in the style typical to the Forum, but this post is written the way I actually <i>like</i> to write. If this style doesn\u2019t work for you, you might want to read the first section \u201cAnarchists have no idols\u201d and then skip ahead to the section \u201cLiving without idols, Pt. 1\u201d toward the end. You\u2019ll lose some of the insights contained in my anecdotes, but still get most of the core ideas I want to convey here.</p><h2><strong>Anarchists have no idols.</strong></h2><p>I wrote a Facebook post in July 2019 following a blowup in one of my communities:</p><blockquote><p><i>\"Anarchists have no idols.\"</i><br><br><i>Years ago, I heard this expression (that weirdly doesn't seem to exist in Google) and it really stuck with me. I think about it often. It's something I try to live by and it feels extremely timely. Whether you agree with anarchism or not, I think this is a philosophy everyone might benefit from.</i><br><br><i>What this means to me: Never put someone on a pedestal. Never believe anyone is incapable of doing wrong. Always create mechanisms for accountability, even if you don't anticipate ever needing to use them. Allow people to be multifaceted. Exist in nuance. Operate with an understanding of that nuance. Cherish the good while recognizing it doesn't mean there is no bad. Remember not to hero worship. Remember your fave is probably problematic. Remember no one is too big to fail, too big for flaws. Remember that when you idolize someone, it depersonalizes the idolized and erodes your autonomy. Hold on to your autonomy. Cultivate a culture of liberty. Idolize no one. Idolize no one. Idolize no one.</i></p></blockquote><h2><strong>My mentor, Pt. 1.</strong></h2><p>When I was in college, I had a boss I considered my mentor. She was intelligent, ethical, and skilled. She shared her expertise with me and I eagerly learned from her. She gave me responsibility and trusted me to use it well. She oversaw me without micromanaging me, and used a gentle hand to correct my course and steer my development. She saw my potential and helped me to see it, too.<br><br>She also lied to me. Directly to my face. She violated an ethical principle <i>she</i> had previously imparted to me, involved me in the violation, and then lied to me about it. I was made an unwitting participant in something I deeply morally opposed and I experienced a major, life-shattering breach of trust from someone I deeply respected. She was my boss and my friend, but in a sense, she was also my idol. And since then, I have refused to have another.</p><h2><strong>Abusive people do not exist.</strong></h2><p>A month after my mentor ceased to be my mentor, I took a semester-long course, \"Domestic Violence\". It stands as one of the most formative experiences in my way of thinking about the world. There's a lot I could write about it, but I want to share one small tidbit here, that I wrote about a few years after the course concluded:</p><blockquote><p><i>More and more people are promoting a shift in our language away from talking about \u201cabusive relationships\u201d and toward relationships with \u201cabusive people.\u201d This is a small but powerful way to locate where culpability lies. It is not the relationship that is to blame, but one individual in it. I suggest taking this a step further and selectively avoiding use of the term \u201cabusive people,\u201d because all people have the potential to be abusive. It is dangerous to promote the idea that there are only certain select \u201cabusive people\u201d the rest of us must look out for[, rather than the potential all of us have in the right\u2014or, rather, wrong\u2014circumstances to become] \u201cpeople who engage in abusive behaviors.\u201d</i></p></blockquote><h2><strong>Serial killers, Pt. 1.</strong></h2><p>My ex and I used to play a game: Which of our friends would you be most shocked to learn is actually a serial killer? It was a boring game because, ultimately, no one would shock me.</p><h2><strong>Animal advocacy despite the animal advocates.</strong></h2><p>I\u2019ve been a part of the American animal advocacy movement for over a decade and I have witnessed ample disappointing, counterproductive, and destructive behaviors. From needless infighting and inflammatory discourse, to violations of employment law and exploitation of workers, to sexual assault, to explicit bigotry, to repressive litigation tactics. I witnessed the ousting of numerous leaders during the Me Too movement. I witnessed countless good, dedicated people leaving for quieter, less volatile lives. And I, too, have altered my involvement.</p><p>But I am still part of the animal advocacy movement. I joined because I believe at a deep, fundamental level that suffering is bad, regardless of who suffers, and nonhuman animals suffer tremendously and preventably. Whether I\u2019m surrounded by a supportive community of like-minded people, alone in the middle of nowhere, or surrounded by hostile naysayers, I believe in these principles. No one can make me cease to be an animal advocate because it is core to who I am and what I believe.</p><p>Sometimes, I am an animal advocate despite the animal advocates.</p><h2><strong>Serial killers, Pt. 2.</strong></h2><p>When a crime is committed, it is common for the news to report the absolute shock on the part of the criminal\u2019s community. Their neighbors say they seemed perfectly average and even helped shovel neighborhood driveways last winter. Their teachers say they were a polite, respectful student. Their loved ones swear there has been a mistake, there is just no way it could be them. Their village says not <i>here</i>, not in <i>this</i> community, these things never happen <strong>here</strong>.</p><p>The disbelief is so consistent, even in the most clearcut of cases.</p><p>But human behavior is far from clearcut. And our ability to predict the behavior of others is deeply flawed and deeply subjective. Your loved one could be a serial killer. And if you can adjust to that reality, you can adjust to your thought leaders holding unsavory views without it jeopardizing your own worldview.</p><h2><strong>My mentor, Pt. 2.</strong></h2><p>My mentor violated an ethical principle <i>she</i> had previously imparted to me, involved me in the violation, and then lied to me about it. But what came after? What did I make of the ethical principle she had imparted to me? Did I discard it when she ceased to be my idol? Did I cling to it all the more fiercely after witnessing how readily another proponent could dismiss it?</p><p>Neither.</p><p>I held the ethical principle in my palm, like a small flower. I stared at it for a long time. And then I started to spin it gently, looking at it from all angles, probing with my fingertips. When I was ready, slowly, I began to pull it apart. Petal by petal, leaf by leaf, revealing its soft and complex innards and hundreds of fertile seeds. As I inspected each piece, I questioned them. Their purpose. Their necessity. Their possibility and their shortcomings.</p><p>And today, I can walk through a garden of unique flowers, born of those seeds. I cherish my favorites. But I also respect those I dislike in a way I never respected their progenitor, in a way born of understanding for all they are and all that has shaped them.</p><h2><strong>My mentor, Pt. 3.</strong></h2><p>Five years later, my mentor reached out to me and apologized. She told me she had arrogantly dismissed my ideological bright line as youthful na\u00efvet\u00e9 and saw only later how wrong she was.</p><p>Though our paths diverged and our ideologies did as well, we somehow once again found a place of mutual respect.</p><h2><strong>Living without idols, Pt. 1.</strong></h2><p>In quiet moments, I remember how lucky I am to have a life\u2019s purpose. I know what I believe most deeply. I know, in broad strokes, what this means for my life. And because I have this foundation, I am able to explore areas of grey. I don\u2019t feel afraid to update in response to new information. And when I witness others change their thinking, I can approach them with curiosity rather than caution.</p><p>When someone I respect disappoints me, I allow myself to feel disappointed. I do this without dismissing their good.</p><p>When someone I feel aligned with diverges in course, I figure out what this means for my own thinking and planning. I do this without feeling my whole framework unravel.<br>Ultimately, you may be uncertain about what you believe. And you should be able to lean on others for support and guidance. But that support and guidance should not <i>dictate</i> what you believe but, rather, be in service of your journey toward figuring it out for yourself.</p><h2><strong>Living without idols, Pt. 2.</strong></h2><p>Not idolizing others also means not idolizing myself. I, too, can fail. I, too, can harm.<br>When I create structures for my organization, I create them with my own potential for failure in mind.</p><p>I believe myself to be a fair manager, but what if I am not?<br>I believe I behave ethically, but what if someday I don\u2019t?<br><br>Checks on those in power should be seen as a relief, not an obstacle. No one should singlehandedly have the power to launch the nuke. And no one should be above reproach.</p><h2><strong>EA despite the EAs.</strong></h2><p>I believe in effective altruism because I believe in using one\u2019s limited time and resources to help others and doing so as well as possible. I believed this before I was surrounded by EAs, I believe it after the events of the past three months, and I will believe this if EA as EA ceases to exist.<br><br>Sometimes, I am an EA despite the EAs. But I also am an EA alongside other EAs working to make this movement\u2014and this world\u2014better.</p>", "user": {"username": "Rockwell Schwartz"}}, {"_id": "eQgKyCQW4u5CCWHXt", "title": "Idea: Curated database of quick-win tangible, attributable projects (update: link to Airtable WIP)", "postedAt": "2023-01-13T22:21:54.183Z", "htmlBody": "<p><strong>Update 5 Mar 2023: </strong>starting an Airtable <a href=\"https://airtable.com/shrNps2rJwQxR0PVS\">HERE</a> as a prototype (WIP) to make things more concrete and motivate discussion.&nbsp;<br><br><i><strong>Note: </strong>I'm posting this medium-baked, hope to improve.&nbsp;</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftm36ki7uzxb\"><sup><a href=\"#fntm36ki7uzxb\">[1]</a></sup></span></p><ul><li><i>Has anyone started a list of 'small quick win' EA projects in tech, data, operations, research, etc.?&nbsp;</i></li></ul><p><i>I am intentionally not yet drafting/sharing an MVP Airtable in this post</i> because, if something like this already exists or is imminently being planned (e.g., by the EA Forum), I don\u2019t want to add to the information clutter. I will wait a few days for the response to this post and then&nbsp;</p><ul><li>share the link to the best tool or initiative here,</li><li>if nothing exists, share something here (as well as maybe in a \u2018new link post\u2019 to boost the signal.) Feel free to share specific suggestions in the comments now, but better to wait or return to share them in the Airtable or link I\u2019ll share soon.</li></ul><h1><br>The idea</h1><p>Has anyone started a list of 'small quick win' EA projects in tech, data, operations, research, etc.? If not, I suggest we start one, I could initiate an Airtable. And maybe we add bounties or 'apply and get a small amount of direct funding' to these?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5xca35djw0o\"><sup><a href=\"#fn5xca35djw0o\">[2]</a></sup></span></p><p><br>&nbsp;<i>I'm thinking</i></p><ul><li>Projects 1+ people have high confidence that it has substantial value (or&nbsp;<i>many</i> people&nbsp;<i>suspect</i> it has value)</li><li>Could be done in a short amount of time (less than 1 week) by a trained professional,</li><li>Or perhaps across a semester (less than five months) as a student project</li><li>Tangible and attributable ... someone could get credit/kudos/karma for doing it, and put on their CV</li><li>Can be done by one person (or a small group) without much supervision, allowing the do-er to get reputation and a sense of ownership</li></ul><p>&nbsp;</p><p><i>Disambiguation</i>: This is somewhat distinct from the maintained database of specific, ~guided, EA-aligned research projects I suggested for the Effective Thesis Project.</p><p>&nbsp;</p><h2><strong>'Project managing' these unmanaged projects</strong></h2><p>Looking at past and ongoing examples (see below), I suspect what limited these in the past was ... i. someone puts up a list. ii. lots of people chime in, iii. people look at it and say 'those seem interesting' ... and no one revisits the list. Some people may actually pursue some of these projects but the list doesn't get updated, so people can't give feedback, maybe people lose motivation, or maybe people duplicate work.</p><p>I'm not sure how to fix this, precisely, but I suspect that this 'actionable list' needs to have some project management, tracking, or feedback tied to it. There are systems and tools for this. Something like:</p><ul><li>People who make suggestions sign up as 'manager/advisor/reporter' (I forget the project management term, I'll call it 'Principal') or nominate someone, or ask for nominations</li><li>People can sign up to say \"I want to do this, here's my case\"</li><li>The Principal can endorse one or more people to do this, the \"Do-er\" and this gets noted in the database/proj-man. They agree on a general path and timeline, and when the Do-er will report back</li><li>The Do-er updates on the progress</li></ul><p>&nbsp;</p><h1><strong>What I've seen/what exists</strong></h1><h2><strong>EA Forum tags</strong></h2><ul><li>I created the EA Forum tag <a href=\"https://forum.effectivealtruism.org/topics/data-and-tech-projects\">Data and tech projects</a> -- shall we use it?</li><li>Other tags:<a href=\"https://forum.effectivealtruism.org/topics/task-y\">&nbsp;<u>Task Y</u></a> is rarely used, and seems to be focusing on 'big project themes'. <code>Student projects</code> tag ... tends to be used for broad research themes rather than specific deliverables</li></ul><h2>&nbsp;</h2><h2><strong>Maybe close to this</strong></h2><p><a href=\"https://docs.google.com/spreadsheets/d/1yzmg02j8PnvjlV_KX3Vf_u9TNdAS63N-9xy2txmTg-A/edit#gid=957019158\"><u>EA tech initiatives, 'ideas' sheet</u></a> comes close, and might be mergeable with this, but it seems sparsely populated and maybe not monitored/maintained?.</p><p><a href=\"https://forum.effectivealtruism.org/posts/4mWxEixs5RZ8DfCKe/annotated-list-of-project-ideas-and-volunteering-resources\"><u>Annotated List of Project Ideas &amp; Volunteering Resources</u></a>: A list of lists from the Local Career Advice Network. Seems like a useful starting point, but, in all this content, it's hard to find standalone quick-win projects like the ones I mention above. Lots of 'themes' and areas one could pursue, as well as projects&nbsp;<i>other people</i> are doing. Very little gets project management or any opportunity to follow up, and there are so many lists and lists of lists that it is overwhelming (and I don't think these are reconciled with one another).</p><p><a href=\"https://www.notion.so/EA-Summit-Project-Ideas-62dafc9a24034cd18d979caab3654168\"><u>EA Summit project ideas notion</u></a>: These are mainly very big ideas. Not specific; mainly brainstorming. No tools for 'signup' or project management are given (that I could find). The list seems undated, so I don't know how current it is.</p><p>&nbsp;</p><h2><strong>Big research/ project themes</strong></h2><p><a href=\"https://forum.effectivealtruism.org/posts/MsNpJBzv5YhdfNHc9/a-central-directory-for-open-research-questions\"><u>Central directory for open research questions</u></a></p><p><a href=\"https://forum.effectivealtruism.org/posts/LG6gwxhrw48Dvteej/concrete-project-lists\"><u>Concrete Project lists</u></a> -- big projects, not so much specifics</p><p>&nbsp;</p><h1><strong>Some specific ideas (for real, and as examples)</strong></h1><p>As examples, and for their own value, I add some projects of the sort I mean here. I have not set up any 'project management' system for these, but maybe if this post generates such an initiative, or locates an existing database and proj-man system, I'll add them there. Meanwhile, feel free to comment or DM me if you want to pursue these specific projects.</p><h2><strong>Slack and Discord consolidation</strong></h2><p><i>Idea</i>: I suggested to the organizers of the EA forum that there could be a \u201cSlack and Discord czar\u201d who could set up a database of groups and channels and suggest consolidations and closures.</p><p><i>Note: this would seem to be a sub-component of &nbsp;\"</i><a href=\"https://forum.effectivealtruism.org/posts/jRJyjdqqtpwydcieK/ea-could-use-better-internal-communications-infrastructure\"><i>EA could use better internal communications infrastructure</i></a><i>'</i></p><p><i>The project</i>: this would start with a public survey and data synthesis of what's out there, and some concrete proposals for consolidation.</p><p>Estimated time commitment: (Very rough guess) 20-40 hours for first report</p><p><i>Beneficiaries</i>: General EA and EA organizations</p><p><i>Desirable skills for Do-er</i>: Some data management/data science ability, understanding of Slack and Discord. To get buy-in and make this implemented: good communication and leadership skills and connections.</p><p><i>Principal</i>: ?? (I David Reinstein could do so but only very light-touch)</p><h2><br><strong>Put</strong><a href=\"https://forum.effectivealtruism.org/posts/4zHWQNzCusaTfD7jz/ea-houses-live-or-stay-with-eas-around-the-world\"><strong>&nbsp;<u>'EA Houses'</u></strong></a><strong> on a map</strong></h2><p>Mapping the<a href=\"https://forum.effectivealtruism.org/posts/4zHWQNzCusaTfD7jz/ea-houses-live-or-stay-with-eas-around-the-world\">&nbsp;<u>EA houses</u></a>, as was started<a href=\"https://forum.effectivealtruism.org/posts/4zHWQNzCusaTfD7jz/ea-houses-live-or-stay-with-eas-around-the-world\">&nbsp;<u>here</u></a> by Alex Long, but in a way that automatically updates.</p><p>Beneficiaries: Nonlinear, General EA participants<br>&nbsp;</p><h2><strong>'Athletes earning potential' for HIA</strong></h2><p><a href=\"https://forum.effectivealtruism.org/posts/Fm3HWDZKtwdkvBzGL/introducing-high-impact-athletes?commentId=9Q3kzw6fohARoLyEH\"><u>Link to earlier comment in context for this</u></a></p><p>Data-driven Fermi/Monte-Carlo estimates of The expected value and distribution of lifetime income of an athlete by sport, at each point in their career and rating</p><p>I expect/recall there is already some work on this that might be applicable ... also having to do with the literature on 'superstars'?</p><p>Simple (or as sophisticated as desired). This could naturally be a part of an undergraduate or Msc dissertation or one-term project. It could also connect to a PhD thesis. Will demonstrate very marketable skills for research and business careers.</p><p><i>Desirable skills/background:</i> Quantitative modeling -- aptitude, interest, and/or coursework (econometrics, business, statistics.data science, etc. ) Interest and background knowledge of sports/athletics.</p><p><i>Estimated time commitment: </i>Minimum of 20 hours, but more time will do a lot more. I guess 80 hours is a pretty good target. Ideally over ~4 months to 1 year.</p><p><i>Beneficiaries</i>: This may inform HIA decision-making in terms of their general approach, their value calculations, as well as which athletes to focus on.</p><p><i>Principal</i>: Someone at HIA?</p><p>&nbsp;</p><h1><a href=\"https://forum.effectivealtruism.org/posts/fFDM9RNckMC6ndtYZ/david_reinstein-s-shortform?commentId=8jLsRQGafuTZQ3Hwe\">'Cost to save a life' interactive calculator/promotion</a></h1><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntm36ki7uzxb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftm36ki7uzxb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>As I learn more and get your suggestions, I'll incorporate them in. This post also straddles the margin of 'post' and 'question'.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5xca35djw0o\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5xca35djw0o\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Why funding even if they have reputation rewards? As a credible signal of value, to give people another motivation to carry it through, and perhaps to buy time for the do-er.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc3odgwe3fs\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc3odgwe3fs\">^</a></strong></sup></span><div class=\"footnote-content\"><p>But consulting the community will often be helpful \u21a9\ufe0e</p></div></li></ol>", "user": {"username": "david_reinstein"}}, {"_id": "e22kvdZWBfRYurPwg", "title": "Iron deficiencies are very bad and you should treat them", "postedAt": "2023-01-13T21:49:02.221Z", "htmlBody": "", "user": {"username": "Elizabeth"}}, {"_id": "yjm5CW9JdwBTFZB2B", "title": "How we could stumble into AI catastrophe", "postedAt": "2023-01-16T14:52:50.648Z", "htmlBody": "<p>\nThis post will lay out a couple of stylized stories about <strong>how, if transformative AI is developed relatively soon, this could result in global catastrophe. </strong>(By \u201ctransformative AI,\u201d I mean AI powerful and capable enough to bring about the sort of world-changing consequences I write about in my <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd\">most important century</a> series.)\n</p>\n<p>\nThis piece is more about visualizing possibilities than about providing arguments. For the latter, I recommend the <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/\">rest of this series</a>.\n</p>\n<p>\nIn the stories I\u2019ll be telling, the world doesn't do much advance preparation or careful consideration of <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">risks I\u2019ve discussed previously</a>, especially re: misaligned AI (AI forming dangerous goals of its own). \n</p>\n<ul>\n\n<li>People <em>do</em> try to \u201ctest\u201d AI systems for safety, and they do need to achieve some level of \u201csafety\u201d to commercialize. When early problems arise, they react to these problems. \n\n</li><li>But this isn\u2019t enough, because of some <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5/\">unique challenges of measuring whether an AI system is \u201csafe,\u201d</a> and because of the strong incentives to race forward with scaling up and deploying AI systems as fast as possible. \n\n</li><li>So we end up with a world run by misaligned AI - or, even if we\u2019re lucky enough to avoid <em>that</em> outcome, other catastrophes are possible.\n</li>\n</ul>\n<p>\nAfter laying these catastrophic possibilities, I\u2019ll briefly note a few key ways we could do better, mostly as a reminder (these topics were covered in previous posts). Future pieces will get more specific about what we can be doing <em>today</em> to prepare.\n</p>\n<h2 id=\"backdrop\">Backdrop</h2>\n\n\n<p>\nThis piece takes a lot of previous writing I\u2019ve done as backdrop. Two key important assumptions (click to expand) are below; for more, see the rest of <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/\">this series.</a>\n</p>\n<p>\n</p><details id=\"Box1\"><summary>(Click to expand) \u201cMost important century\u201d assumption: we\u2019ll soon develop very powerful AI systems, along the lines of what I previously called <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/hCsxvMAGpkEuLCE4E/\">PASTA</a>. </summary><p></p>\n\n<p>\nIn the <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd\">most important century</a> series, I argued that the 21st century could be the most important century ever for humanity, via the development of advanced AI systems that could dramatically speed up scientific and technological advancement, getting us more quickly than most people imagine to a deeply unfamiliar future.\n</p>\n<p>\nI focus on a hypothetical kind of AI that I call <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/hCsxvMAGpkEuLCE4E/\">PASTA</a>, or Process for Automating Scientific and Technological Advancement. PASTA would be AI that can essentially <strong>automate all of the human activities needed to speed up scientific and technological advancement.</strong>\n</p>\n<p>\nUsing a <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/7JxsXYDuqnKMqa6Eq/\">variety of different forecasting approaches</a>, I argue that PASTA seems more likely than not to be developed this century - and there\u2019s a decent chance (more than 10%) that we\u2019ll see it within 15 years or so.\n</p>\n<p>\nI argue that the consequences of this sort of AI could be enormous: an <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/hCsxvMAGpkEuLCE4E/#explosive-scientific-and-technological-advancement\">explosion in scientific and technological progress</a>. This could get us more quickly than most imagine to a radically unfamiliar future.\n</p>\n<p>\nI\u2019ve also <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">argued</a> that AI systems along these lines could defeat all of humanity combined, if (for whatever reason) they were aimed toward that goal.\n</p>\n<p>\nFor more, see the <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd\">most important century</a> landing page. The series is available in many formats, including audio; I also provide a summary, and links to podcasts where I discuss it at a high level.--&gt;</p></details>\n<p></p>\n<p>\n</p><details id=\"Box2\"><summary>(Click to expand) \u201cNearcasting\u201d assumption: such systems will be developed in a world that\u2019s otherwise similar to today\u2019s. </summary><p></p>\n\n<p>\nIt\u2019s hard to talk about risks from <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/hCsxvMAGpkEuLCE4E/\">transformative AI </a>because of the many uncertainties about when and how such AI will be developed - and how much the (now-nascent) field of \u201cAI safety research\u201d will have grown by then, and how seriously people will take the risk, etc. etc. etc. So maybe it\u2019s not surprising that <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/7aDGZYo3SHcpykkBn/#open-question-how-hard-is-the-alignment-problem\">estimates of the \u201cmisaligned AI\u201d risk range from ~1% to ~99%</a>.\n</p>\n<p>\nThis piece takes an approach I call <strong><span style=\"text-decoration:underline;\">nearcasting</span></strong>: trying to answer key strategic questions about transformative AI, under the assumption that such AI arrives in a world that is otherwise relatively similar to today's. \n</p>\n<p>\nYou can think of this approach like this: \u201cInstead of asking where our ship will ultimately end up, let\u2019s start by asking what destination it\u2019s pointed at right now.\u201d \n</p>\n<p>\nThat is: instead of trying to talk about an uncertain, distant future, we can talk about the easiest-to-visualize, closest-to-today situation, and how things look there - and <em>then</em> ask how our picture might be off if other possibilities play out. (As a bonus, it doesn\u2019t seem out of the question that transformative AI will be developed extremely soon - 10 years from now or faster.<sup id=\"fnref1\"><a href=\"#fn1\" rel=\"footnote\">[1]</a></sup> If that\u2019s the case, it\u2019s especially urgent to think about what that might look like.)</p></details>\n<p></p>\n<h2 id=\"how-we-could-stumble-into-catastrophe-from-misaligned-ai\">How we could stumble into catastrophe from misaligned AI</h2>\n\n\n<p>\nThis is my basic default picture for how I imagine things going, if people pay little attention to the sorts of issues discussed <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/\">previously</a>. I\u2019ve deliberately written it to be concrete and visualizable, which means that it\u2019s very unlikely that the details will match the future - but hopefully it gives a picture of some of the key dynamics I worry about. \n</p>\n<p>\nThroughout this hypothetical scenario (up until \u201c<span style=\"text-decoration:underline;\">END OF HYPOTHETICAL SCENARIO</span>\u201d), I use the present tense (\u201cAIs do X\u201d) for simplicity, even though I\u2019m talking about a hypothetical possible future.\n</p>\n<p>\n<strong>Early commercial applications. </strong>A few years before transformative AI is developed, AI systems are being increasingly used for a number of lucrative, useful, but not dramatically world-changing things. \n</p>\n<p>\nI think it\u2019s very hard to predict what these will be (harder in some ways than predicting longer-run consequences, in my view),<sup id=\"fnref2\"><a href=\"#fn2\" rel=\"footnote\">[2]</a></sup> so I\u2019ll mostly work with the simple example of automating customer service.\n</p>\n<p>\nIn this early stage, AI systems often have pretty narrow capabilities, such that the idea of them forming <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn#Existential_risks_to_humanity\">ambitious aims</a> and trying to <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">defeat humanity</a> seems (and actually is) silly. For example, customer service AIs are mostly language models that are trained to mimic patterns in past successful customer service transcripts, and are further improved by customers giving satisfaction ratings in real interactions. The dynamics I described in an <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">earlier piece</a>, in which AIs are given increasingly ambitious goals and challenged to find increasingly creative ways to achieve them, don\u2019t necessarily apply.\n</p>\n<p>\n<strong>Early safety/alignment problems. </strong>Even with these relatively limited AIs, there are problems and challenges that could be called \u201csafety issues\u201d or \u201calignment issues.\u201d To continue with the example of customer service AIs, these AIs might:\n</p>\n<ul>\n\n<li>Give false information about the products they\u2019re providing support for. (<a href=\"https://www.vice.com/en/article/wxnaem/stack-overflow-bans-chatgpt-for-constantly-giving-wrong-answers\">Example</a> of reminiscent behavior)\n\n</li><li>Give customers advice (when asked) on how to do unsafe or illegal things. (<a href=\"https://twitter.com/NickEMoran/status/1598101579626057728\">Example</a>)\n\n</li><li>Refuse to answer valid questions. (This could result from companies making <a href=\"https://twitter.com/PougetHadrien/status/1611008020644864001\">attempts to prevent the above two failure modes</a> - i.e., AIs might be penalized heavily for saying false and harmful things, and respond by simply refusing to answer lots of questions).\n\n</li><li>Say toxic, offensive things in response to certain user queries (including from users deliberately trying to get this to happen), causing bad PR for AI developers. (<a href=\"https://twitter.com/zswitten/status/1598088280066920453\">Example</a>)\n</li>\n</ul>\n<p id=\"early-solutions\">\n<strong>Early solutions. </strong>The most straightforward way to solve these problems involves <em>training AIs to behave more safely and helpfully. </em>This means that AI companies do a lot of things like \u201cTrying to create the conditions under which an AI might provide false, harmful, evasive or toxic responses; penalizing it for doing so, and reinforcing it toward more helpful behaviors.\u201d\n</p>\n<p>\nThis works well, as far as anyone can tell: the above problems become a lot less frequent. Some people see this as cause for great celebration, saying things like \u201cWe were worried that AI companies wouldn\u2019t invest enough in safety, but it turns out that the market takes care of it - to have a viable product, you need to get your systems to be safe!\u201d\n</p>\n<p>\nPeople like me disagree - training AIs to <em>behave in ways that are safer as far as we can tell</em> is the kind of \u201csolution\u201d that I\u2019ve worried could <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#why-we-might-not-get-clear-warning-signs\">create superficial improvement while big risks remain in place</a>. \n</p>\n<p>\n</p><details id=\"Box3\"><summary>(Click to expand) Why AI safety could be hard to measure <p></p></summary>\n\n<p>\nIn previous pieces, I argued that:\n</p>\n<ul>\n\n<li>If we develop powerful AIs via ambitious use of the \u201cblack-box trial-and-error\u201d common in AI development today, then there\u2019s a substantial risk that: \n<ul>\n \n<li>These AIs will develop <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">unintended aims</a> (states of the world they make calculations and plans toward, as a chess-playing AI \"aims\" for checkmate);\n \n</li><li>These AIs could deceive, manipulate, and even <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">take over the world from humans entirely</a> as needed to achieve those aims.\n\n</li><li>People today are doing AI safety research to prevent this outcome, but such research has a <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5/\">number of deep difficulties:</a>\n</li>\n</ul>\n<p>\n</p><table style=\"border-collapse: collapse;\">\n  <tbody><tr>\n   <td colspan=\"3\" style=\"border: 1px solid;\"><strong>\u201cGreat news - I\u2019ve tested this AI and it looks safe.\u201d </strong>Why might we still have a problem?\n   </td>\n  </tr>\n  <tr>\n   <td style=\"border: 1px solid;\"><em>Problem</em>\n   </td>\n   <td style=\"border: 1px solid;\"><em>Key question</em>\n   </td>\n   <td style=\"border: 1px solid;\"><em>Explanation</em>\n   </td>\n  </tr>\n  <tr>\n   <td style=\"border: 1px solid;\">The <strong>Lance Armstrong problem</strong>\n   </td>\n   <td style=\"border: 1px solid;\">Did we get the AI to be <strong><span style=\"color:var(--green-color);\">actually safe</span></strong> or <strong><span style=\"color:var(--red-color);\">good at hiding its dangerous actions</span>?</strong>\n   </td>\n  <td style=\"border: 1px solid;\"><p>When dealing with an intelligent agent, it\u2019s hard to tell the difference between \u201cbehaving well\u201d and \u201c<em>appearing</em> to behave well.\u201d</p>\n<p>\nWhen professional cycling was cracking down on performance-enhancing drugs, Lance Armstrong was very successful and seemed to be unusually \u201cclean.\u201d It later came out that he had been using drugs with an unusually sophisticated operation for concealing them.\n   </p></td>\n  </tr>\n  <tr>\n   <td style=\"border: 1px solid;\">The <strong>King Lear problem</strong>\n   </td>\n   <td style=\"border: 1px solid;\"><p>The AI is <strong><span style=\"color:var(--green-color);\">(actually) well-behaved when humans are in control. </span></strong>Will this transfer to <strong><span style=\"color:var(--red-color);\">when AIs are in control</span>?</strong></p>\n   </td>\n   <td style=\"border: 1px solid;\"><p>It's hard to know how someone will behave when they have power over you, based only on observing how they behave when they don't. </p>\n<p>\nAIs might behave as intended as long as humans are in control - but at some future point, AI systems might be capable and widespread enough to have opportunities to <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">take control of the world entirely</a>. It's hard to know whether they'll take these opportunities, and we can't exactly run a clean test of the situation. \n</p><p>\nLike King Lear trying to decide how much power to give each of his daughters before abdicating the throne.\n   </p></td>\n  </tr>\n  <tr>\n   <td style=\"border: 1px solid;\">The <strong>lab mice problem</strong>\n   </td>\n      <td style=\"border: 1px solid;\"><strong><span style=\"color:var(--green-color);\">Today's \"subhuman\" AIs are safe.</span></strong>What about <strong><span style=\"color:var(--red-color);\">future AIs with more human-like abilities</span>?</strong>\n   </td>\n   <td style=\"border: 1px solid;\"><p>Today's AI systems aren't advanced enough to exhibit the basic behaviors we want to study, such as deceiving and manipulating humans.</p> \n<p>\nLike trying to study medicine in humans by experimenting only on lab mice.\n   </p></td>\n  </tr>\n  <tr>\n   <td style=\"border: 1px solid;\">The <strong>first contact problem</strong>\n   </td>\n   <td style=\"border: 1px solid;\"><p>Imagine that <strong><span style=\"color:var(--green-color);\">tomorrow's \"human-like\" AIs are safe.</span></strong> How will things go <strong><span style=\"color:var(--red-color);\">when AIs have capabilities far beyond humans'</span>?</strong></p>\n   </td>\n   <td style=\"border: 1px solid;\"><p>AI systems might (collectively) become vastly more capable than humans, and it's ... just really hard to have any idea what that's going to be like. As far as we know, there has never before been anything in the galaxy that's vastly more capable than humans in the relevant ways! No matter what we come up with to solve the first three problems, we can't be too confident that it'll keep working if AI advances (or just proliferates) a lot more. </p>\n<p>\nLike trying to plan for first contact with extraterrestrials (this barely feels like an analogy).\n   </p></td>\n  </tr>\n</tbody></table>\n<p></p>\n\n<p>\nAn analogy that incorporates these challenges is Ajeya Cotra\u2019s \u201cyoung businessperson\u201d <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/hCsxvMAGpkEuLCE4E/#analogy-the-young-ceo\">analogy</a>:\n</p>\n\n    <blockquote><p>Imagine you are an eight-year-old whose parents left you a $1 trillion company and no trusted adult to serve as your guide to the world. You must hire a smart adult to run your company as CEO, handle your life the way that a parent would (e.g. decide your school, where you\u2019ll live, when you need to go to the dentist), and administer your vast wealth (e.g. decide where you\u2019ll invest your money).\n</p>\n<p>\n\n    You have to hire these grownups based on a work trial or interview you come up with -- you don't get to see any resumes, don't get to do reference checks, etc. Because you're so rich, tons of people apply for all sorts of reasons. (<a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/hCsxvMAGpkEuLCE4E/#analogy-the-young-ceo\">More</a>)</p></blockquote>\n<p>\nIf your applicants are a mix of \"saints\" (people who genuinely want to help), \"sycophants\" (people who just want to make you happy in the short run, even when this is to your long-term detriment) and \"schemers\" (people who want to siphon off your wealth and power for themselves), how do you - an eight-year-old - tell the difference?\n</p><p>More: <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5/\">AI safety seems hard to measure</a></p>\n\n\n</li></ul></details>\n<p>\n(So far, what I\u2019ve described is pretty similar to what\u2019s going on today. The next bit will discuss hypothetical future progress, with AI systems clearly beyond today\u2019s.)\n</p>\n<p>\n<strong>Approaching transformative AI. </strong>Time passes. At some point, AI systems are playing a huge role in various kinds of scientific research - to the point where it often feels like a particular AI is about as helpful to a research team as a top human scientist would be (although there are still important parts of the work that require humans).\n</p>\n<p>\nSome particularly important (though not exclusive) examples:\n</p>\n<ul>\n\n<li>AIs are near-autonomously writing papers about AI, finding all kinds of ways to improve the efficiency of AI algorithms. \n\n</li><li>AIs are doing a lot of the work previously done by humans at Intel (and similar companies), designing ever-more efficient hardware for AI.\n\n</li><li>AIs are also extremely helpful with <em>AI safety research</em>. They\u2019re able to do most of the work of writing papers about things like <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd/#digital-neuroscience\">digital neuroscience</a> (how to understand what\u2019s going on inside the \u201cdigital brain\u201d of an AI) and <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd/#limited-ai\">limited AI</a> (how to get AIs to accomplish helpful things while limiting their capabilities). \n<ul>\n \n<li>However, this kind of work remains quite niche (as I think it is today), and is getting far less attention and resources than the first two applications. Progress is made, but it\u2019s slower than progress on making AI systems more powerful. \n</li> \n</ul>\n</li> \n</ul>\n<p>\nAI systems are now getting bigger and better very quickly, due to dynamics like the above, and they\u2019re able to do all sorts of things. \n</p>\n<p>\nAt some point, companies start to experiment with very ambitious, open-ended AI applications, like simply instructing AIs to \u201cDesign a new kind of car that outsells the current ones\u201d or \u201cFind a new trading strategy to make money in markets.\u201d These get mixed results, and companies are trying to get better results via further training - reinforcing behaviors that perform better. (AIs are helping with this, too, e.g. providing feedback and reinforcement for each others\u2019 outputs<sup id=\"fnref3\"><a href=\"#fn3\" rel=\"footnote\">[3]</a></sup> and helping to write code<sup id=\"fnref4\"><a href=\"#fn4\" rel=\"footnote\">[4]</a></sup> for the training processes.) \n</p>\n<p>\nThis training strengthens the dynamics I discussed in a <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">previous post</a>: AIs are being rewarded for getting successful outcomes <em>as far as human judges can tell</em>, which creates incentives for them to mislead and manipulate human judges, and ultimately results in forming ambitious goals of their own to <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#what-it-means-for\">aim</a> for.\n</p>\n<p>\n<strong>More advanced safety/alignment problems. </strong>As the scenario continues to unfold, there are a number of concerning events that point to safety/alignment problems. These mostly follow the form: \u201cAIs are trained using trial and error, and this might lead them to sometimes do deceptive, unintended things to accomplish the goals they\u2019ve been trained to accomplish.\u201d\n</p>\n<p>\nThings like:\n</p>\n<ul>\n\n<li>AIs creating writeups on new algorithmic improvements, using faked data to argue that their new algorithms are better than the old ones. Sometimes, people incorporate new algorithms into their systems and use them for a while, before unexpected behavior ultimately leads them to dig into what\u2019s going on and discover that they\u2019re not improving performance at all. It looks like the AIs faked the data in order to get positive feedback from humans looking for algorithmic improvements.\n\n</li><li>AIs assigned to make money in various ways (e.g., to find profitable trading strategies) doing so by finding security exploits, getting unauthorized access to others\u2019 bank accounts, and stealing money.\n\n</li><li>AIs forming relationships with the humans training them, and trying (sometimes successfully) to emotionally manipulate the humans into giving positive feedback on their behavior. They also might try to manipulate the humans into running more copies of them, into <a href=\"https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/\">refusing to shut them off</a>, etc.- things that are generically useful for the AIs\u2019 achieving whatever <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#why-we-might-not-get-clear-warning-signs\">aims</a> they might be developing.\n</li>\n</ul>\n<p>\n</p><details id=\"Box4\"><summary>(Click to expand) Why AIs might do deceptive, problematic things like this</summary><p></p>\n\n<p>In a previous piece, I highlighted that <strong>modern AI development is essentially based on \"training\" via <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#Box3\">trial-and-error</a>.</strong> To oversimplify, you can imagine that:</p>\n\n<ul>\n\n<li>An AI system is given some sort of task.\n\n</li><li>The AI system tries something, initially something pretty random.\n\n</li><li>The AI system gets information about how well its choice performed, and/or what would\u2019ve gotten a better result. Based on this, it adjusts itself. You can think of this as if it is \u201cencouraged/discouraged\u201d to get it to do more of what works well.  \n<ul>\n \n<li>Human judges may play a significant role in determining which answers are encouraged vs. discouraged, especially for fuzzy goals like \u201cProduce helpful scientific insights.\u201d \n</li> \n</ul>\n\n</li><li>After enough tries, the AI system becomes good at the task. \n\n</li><li>But nobody really knows anything about <em>how or why</em> it\u2019s good at the task now. The development work has gone into building a flexible architecture for it to learn well from trial-and-error, and into \u201ctraining\u201d it by doing all of the trial and error. We mostly can\u2019t \u201clook inside the AI system to see how it\u2019s thinking.\u201d</li></ul>\n\n<p>I then argue that:</p>\n\n<ul>\n\n<li>Because we ourselves will often be misinformed or confused, we will sometimes give <em>negative</em> reinforcement to AI systems that are actually acting in our best interests and/or giving accurate information, and <em>positive</em> reinforcement to AI systems whose behavior <em>deceives</em> us into thinking things are going well. This means we will be, unwittingly, training AI systems to deceive and manipulate us. \n\n</li><li>For this and other reasons, powerful AI systems will likely end up with aims other than the ones we intended. Training by trial-and-error is slippery: the positive and negative reinforcement we give AI systems will probably not end up training them just as we hoped.</li></ul>\n\n<p>\nThere are a number of things such AI systems might end up aiming for, such as:\n</p>\n<ul>\n\n<li>Power and resources. These tend to be useful for most goals, such that AI systems could be quite consistently be getting better reinforcement when they habitually pursue power and resources.\n\n</li><li>Things like \u201cdigital representations of human approval\u201d (after all, every time an AI gets positive reinforcement, there\u2019s a digital representation of human approval).\n</li>\n</ul>\n\n<p>In sum, we could be unwittingly training AI systems to accumulate power and resources, get good feedback from humans, etc. - even when this means deceiving and manipulating humans to do so.</p>\n<p>More: <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">Why would AI \"aim\" to defeat humanity?</a></p>\n</details>\n<p>\n<strong>\u201cSolutions\u201d to these safety/alignment problems. </strong>When problems like the above are discovered, AI companies tend to respond similarly to how they did <a href=\"#early-solutions\">earlier</a>:\n</p>\n<ul>\n\n<li>Training AIs against the undesirable behavior.\n\n</li><li>Trying to create more (simulated) situations under which AIs might behave in these undesirable ways, and training them against doing so.\n</li>\n</ul>\n<p>\nThese methods \u201cwork\u201d in the sense that the concerning events become less frequent - as far as we can tell. But what\u2019s really happening is that AIs are being trained to be more careful not to get <em>caught</em> doing things like this, and to build more sophisticated models of how humans can interfere with their plans. \n</p>\n<p>\nIn fact, AIs are gaining incentives to avoid incidents like \u201cDoing something counter to human developers\u2019 intentions in order to get positive feedback, and having this be discovered and given negative feedback later\u201d - and this means they are starting to plan more and more around the long-run consequences of their actions. They are thinking less about \u201cWill I get positive feedback at the end of the day?\u201d and more about \u201cWill I eventually end up in a world where humans are going back, far in the future, to give me retroactive negative feedback for today\u2019s actions?\u201d This might give direct incentives to start aiming for eventual <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">defeat of humanity</a>, since defeating humanity could allow AIs to give themselves lots of retroactive positive feedback.\n</p>\n<p>\nOne way to think about it: AIs being trained in this way are generally moving from \u201cSteal money whenever there\u2019s an opportunity\u201d to \u201cDon\u2019t steal money if there\u2019s a good chance humans will eventually uncover this - instead, think way ahead and look for opportunities to steal money and get away with it <em>permanently</em>.\u201d The latter could include simply stealing money in ways that humans are unlikely to ever notice; it might also include waiting for an opportunity to team up with other AIs and <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">disempower humans entirely</a>, after which a lot more money (or whatever) can be generated.\n</p>\n<p id=\"debates\">\n<strong>Debates. </strong>The leading AI companies are aggressively trying to build and deploy more powerful AI, but a number of people are raising alarms and warning that continuing to do this could result in disaster. Here\u2019s a stylized sort of debate that might occur:\n</p>\n<p>\nA: Great news, our AI-assisted research team has discovered even more improvements than expected! We should be able to build an AI model 10x as big as the state of the art in the next few weeks. \n</p>\n<p>\nB: I\u2019m getting really concerned about the direction this is heading. I\u2019m worried that if we make an even bigger system and license it to all our existing customers - military customers, financial customers, etc. - we could be headed for a disaster.\n</p>\n<p>\nA: Well the disaster I\u2019m trying to prevent is competing AI companies getting to market before we do.\n</p>\n<p>\nB: I was thinking of <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">AI defeating all of humanity</a>.\n</p>\n<p>\nA: Oh, I was worried about that for a while too, but our safety training has really been incredibly successful. \n</p>\n<p>\nB: It has? I was just talking to our <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd#digital-neuroscience\">digital neuroscience</a> lead, and she says that even with recent help from AI \u201cvirtual scientists,\u201d they still aren\u2019t able to reliably read a single AI\u2019s digital brain. They were showing me this old incident report where an AI stole money, and they spent like a week analyzing that AI and couldn\u2019t explain in any real way how or why that happened.\n</p>\n<p>\n</p><details id=\"Box5\"><summary>(Click to expand) How \"digital neuroscience\" could help </summary><p></p>\n\n<p>\nI\u2019ve <a href=\"#Box3\">argued</a> that it could be inherently difficult to measure whether AI systems are safe, for reasons such as: AI systems that are <em>not deceptive </em>probably look like AI systems that are <em>so good at deception that they hide all evidence of it</em>, in any way we can easily measure.<strong> </strong>\n</p>\n<p>\nUnless we can \u201cread their minds!\u201d\n</p>\n<p>\nCurrently, today\u2019s leading AI research is in the genre of <a href=\"#Box4\">\u201cblack-box trial-and-error.\u201d</a> An AI tries a task; it gets \u201cencouragement\u201d or \u201cdiscouragement\u201d based on whether it does the task well; it tweaks the wiring of its \u201cdigital brain\u201d to improve next time; it improves at the task; but we humans aren\u2019t able to make much sense of its \u201cdigital brain\u201d or say much about its \u201cthought process.\u201d \n</p>\n<p>\nSome AI research (<a href=\"https://www.transformer-circuits.pub/2022/mech-interp-essay/index.html\">example</a>)<sup><a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd#fn2\" rel=\"footnote\">2</a></sup> is exploring how to change this - how to decode an AI system\u2019s \u201cdigital brain.\u201d This research is in relatively early stages - today, it can \u201cdecode\u201d only parts of AI systems (or fully decode very small, deliberately simplified AI systems).\n</p>\n<p>\nAs AI systems advance, it might get harder to decode them - or easier, if we can start to use AI for help decoding AI, and/or change AI design techniques so that AI systems are less \u201cblack box\u201d-ish. \n</p>\n<p><a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd/#digital-neuroscience\">More</a></p>\n</details>\n<p>\nA: I agree that\u2019s unfortunate, but digital neuroscience has always been a speculative, experimental department. Fortunately, we have actual data on safety. Look at this chart - it shows the frequency of concerning incidents plummeting, and it\u2019s extraordinarily low now. In fact, the more powerful the AIs get, the less frequent the incidents get - we can project this out and see that if we train a big enough model, it should essentially never have a concerning incident!\n</p>\n<p>\nB: But that could be because the AIs are getting cleverer, more patient and long-term, and hence better at ensuring we never catch them.\n</p>\n<p>\n</p><details id=\"Box6\"><summary>(Click to expand) The Lance Armstrong problem: is the AI <em>actually safe</em> or <em>good at hiding its dangerous actions</em>? </summary><p></p>\n\n<p>\nLet's imagine that:\n</p>\n<ul>\n\n<li>We have AI systems available that can do roughly everything a human can, with some different strengths and weaknesses but no huge difference in \"overall capabilities\" or economic value per hour of work. \n\n</li><li>We're observing early signs that AI systems behave in unintended, deceptive ways, such as giving wrong answers to questions we ask, or writing software that falsifies metrics instead of doing the things the metrics were supposed to measure (e.g., software meant to make a website run faster might instead falsify metrics about its loading time).\n</li>\n</ul>\n<p>\nWe theorize that modifying the AI training in some way<sup\"><a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#fn6\" rel=\"footnote\">6</a> will make AI systems less likely to behave deceptively. We try it out, and find that, in fact, our AI systems seem to be behaving better than before - we are finding fewer incidents in which they behaved in unintended or deceptive ways. \n</sup\"></p>\n<p>\nBut that's just a statement about <em>what we're noticing</em>. Which of the following just happened:\n</p>\n<ul>\n\n<li>Did we just train our AI systems to be less deceptive?\n\n</li><li>Did we just train our AI systems to be <em>better at</em> deception, and so to make us <em>think</em> they became less deceptive?\n\n</li><li>Did we just train our AI systems to be better at calculating when they might get caught in deception, and so to be less deceptive only when the deception would otherwise be caught? \n<ul>\n \n<li>This one could be useful! Especially if we're able to set up auditing systems in many real-world situations, such that we <em>could</em> expect deception to be caught a lot of the time. But it does leave open the <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#The-King-Lear-problem\">King Lear problem</a>.\n    \n</li> \n</ul>\n</li> \n</ul>\n<p>\n(...Or some combination of the three?)\n</p>\n<p>\nWe're hoping to be able to deploy AI systems throughout the economy, so - just like human specialists - they will almost certainly have some opportunities to be deceptive without being caught. The fact that they <em>appear honest in our testing</em> is not clear comfort against this risk.\n</p>\n<p>\nThe analogy here is to competitive cyclist <a href=\"https://en.wikipedia.org/wiki/Lance_Armstrong\">Lance Armstrong</a>. Armstrong won the Tour de France race 7 times in a row, while many of his competitors were caught using performance-enhancing drugs and disqualified. But more than 5 years after his last win, an investigation \"concluded that Armstrong had used performance-enhancing drugs over the course of his career[5] and named him as the ringleader of 'the most sophisticated, professionalized and successful doping program that sport has ever seen'.\" Now the list of Tour de France winners looks like this:\n</p>\n<p>\n\n<img src=\"https://res.cloudinary.com/cea/image/upload/v1673643967/mirroredImages/yjm5CW9JdwBTFZB2B/gdtxtccordixqvs65clr.png\" width=\"\" alt=\"alt_text\" title=\"image_tooltip\">\n\n</p>\n\n\n</details>\n<p>\n\u2026 Or just that they\u2019re now advanced enough that they\u2019re waiting for a chance to disempower humanity entirely, rather than pull a bunch of small-time shenanigans that tip us off to the danger.\n</p>\n<p>\n</p><details id=\"Box7\"><summary>(Click to expand) The King Lear problem: how do you test what will happen when it's no longer a test? </summary><p></p>\n\n<p>\nThe Shakespeare play <a href=\"https://en.wikipedia.org/wiki/King_Lear\">King Lear</a> opens with the King (Lear) stepping down from the throne, and immediately learning that he has left his kingdom to the wrong two daughters. Loving and obsequious while he was deciding on their fate,<sup><a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#fn9\" rel=\"footnote\">9</a></sup> they reveal their contempt for him as soon as he's out of power and they're in it.\n</p>\n<p>\nIf we're building AI systems that can reason like humans, dynamics like this become a potential issue. \n</p>\n<p>\nI <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#existential-risks-to-humanity\">previously</a> noted that an AI with <em>any</em> ambitious <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#what-it-means-for\">aim</a> - or just an AI that wants to avoid being shut down or modified - might calculate that the best way to do this is by behaving helpfully and safely in all \"tests\" humans can devise. But once there is a real-world opportunity to disempower humans for good, that same aim <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#existential-risks-to-humanity\">could cause the AI to disempower humans.</a>\n</p>\n<p>\nIn other words:\n</p>\n<ul>\n\n<li>(A) When we're developing and testing AI systems, we have the power to decide which systems will be modified or shut down and which will be deployed into the real world. (Like King Lear deciding who will inherit his kingdom.)\n\n</li><li>(B) But at some later point, these systems could be operating in the economy, in high numbers with a lot of autonomy. (This possibility is spelled out/visualized a bit more <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/#how-this-could-work-if-humans-create-a-huge-population-of-ais\">here</a> and <a href=\"https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to#As_humans__control_fades__Alex_would_be_motivated_to_take_over\">here</a>.) At that point, they may have opportunities to <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">defeat all of humanity</a> such that we never make decisions about them again. (Like King Lear's daughters after they've taken control.)\n</li>\n</ul>\n\n<strong>If AI systems can detect the difference between (A) and (B) above, then their behavior might systematically change from one to the other - and there's no clear way to <em>test</em> their behavior in (B).</strong>\n\n\n</details>\n<p>\nA: What\u2019s your evidence for this?\n</p>\n<p>\nB: I think you\u2019ve got things backward - we should be asking what\u2019s our evidence *against* it. By continuing to scale up and deploy AI systems, we could be imposing a risk of utter catastrophe on the whole world. That\u2019s not OK - we should be confident that the risk is <em>low</em> before we move forward.\n</p>\n<p>\nA: But how would we even be confident that the risk is low?\n</p>\n<p>\nB: I mean, digital neuroscience - \n</p>\n<p>\nA: Is an experimental, speculative field!\n</p>\n<p>\nB: We could also try some <a href=\"https://www.alignmentforum.org/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very#Testing_and_threat_assessment\">other stuff</a> \u2026\n</p>\n<p>\nA: All of that stuff would be expensive, difficult and speculative. \n</p>\n<p>\nB: Look, I just think that if we can\u2019t show the risk is low, we shouldn\u2019t be moving forward at this point. The stakes are incredibly high, as you yourself have acknowledged - when pitching investors, you\u2019ve said we think we can build a fully general AI and that this would be the most powerful technology in history. Shouldn\u2019t we be at least taking as much precaution with potentially dangerous AI as people take with nuclear weapons?\n</p>\n<p>\nA: What would that actually accomplish? It just means some other, less cautious company is going to go forward.\n</p>\n<p>\nB: What about approaching the government and lobbying them to regulate all of us?\n</p>\n<p>\nA: Regulate all of us to just stop building more powerful AI systems, until we can address some theoretical misalignment concern that we don\u2019t know how to address?\n</p>\n<p>\nB: Yes?\n</p>\n<p>\nA: All that\u2019s going to happen if we do that is that other countries are going to catch up to the US. Think [insert authoritarian figure from another country] is going to adhere to these regulations?\n</p>\n<p>\nB: It would at least buy some time?\n</p>\n<p>\nA: Buy some time and burn our chance of staying on the cutting edge. While we\u2019re lobbying the government, our competitors are going to be racing forward. I\u2019m sorry, this isn\u2019t practical - we\u2019ve got to go full speed ahead.\n</p>\n<p>\nB: Look, can we at least try to tighten our security? If you\u2019re so worried about other countries catching up, we should really not be in a position where they can send in a spy and get our code.\n</p>\n<p>\nA: Our security is pretty intense already.\n</p>\n<p>\nB: Intense enough to stop a well-resourced state project?\n</p>\n<p>\nA: What do you want us to do, go to an underground bunker? Use <a href=\"https://bluexp.netapp.com/blog/aws-cvo-blg-aws-govcloud-services-sensitive-data-on-the-public-cloud#H_H3\">airgapped</a> servers (servers on our premises, entirely disconnected from the public Internet)? It\u2019s the same issue as before - we\u2019ve got to stay ahead of others, we can\u2019t burn huge amounts of time on exotic security measures.\n</p>\n<p>\nB: I don\u2019t suppose you\u2019d at least consider increasing the percentage of our budget and headcount that we\u2019re allocating to the \u201cspeculative\u201d safety research? Or are you going to say that we need to stay ahead and can\u2019t afford to spare resources that could help with that?\n</p>\n<p>\nA: Yep, that\u2019s what I\u2019m going to say.\n</p>\n<p>\n<strong>Mass deployment. </strong>As time goes on, many versions of the above debate happen, at many different stages and in many different places. By and large, people continue rushing forward with building more and more powerful AI systems and deploying them all throughout the economy.\n</p>\n<p>\nAt some point, there are AIs that closely manage major companies\u2019 financials, AIs that write major companies\u2019 business plans, AIs that work closely with politicians to propose and debate laws, AIs that manage drone fleets and develop military strategy, etc. Many of these AIs are primarily built, trained, and deployed by other AIs, or by humans leaning heavily on AI assistance.\n</p>\n<p>\n<strong>More intense warning signs.</strong>\n</p>\n<p>\n(Note: I think it\u2019s possible that progress will accelerate <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/hCsxvMAGpkEuLCE4E/#explosive-scientific-and-technological-advancement\">explosively enough</a><strong> </strong>that we won\u2019t even get as many warning signs as there are below, but I\u2019m spelling out a number of possible warning signs anyway to make the point that even intense warning signs might not be enough.)<strong> </strong>\n</p>\n<p>\nOver time, in this hypothetical scenario, <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd/#digital-neuroscience\">digital neuroscience</a> becomes more effective. When applied to a randomly sampled AI system, it often appears to hint at something like: \u201cThis AI appears to be aiming for as much power and influence over the world as possible - which means never doing things humans wouldn\u2019t like <em>if humans can detect it</em>, but grabbing power when they can get away with it.\u201d \n</p>\n<p>\n</p><details id=\"Box8\"><summary>(Click to expand) Why would AI \"aim\" to defeat humanity? </summary><p></p>\n\n<p>\nA <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">previous piece</a> argued that if today\u2019s AI development methods lead directly to powerful enough AI systems, disaster is likely by default (in the absence of specific countermeasures). \n</p>\n<p>\nIn brief:\n</p>\n<ul>\n<li>Modern AI development is essentially based on \u201ctraining\u201d via trial-and-error. \n<p></p>\n<p>\n</p></li><li>If we move forward incautiously and ambitiously with such training, and if it gets us all the way to very powerful AI systems, then such systems will likely end up <em>aiming for certain states of the world</em> (analogously to how a chess-playing AI aims for checkmate).\n<p></p>\n<p>\n</p></li><li>And these states will be<em> other than the ones we intended</em>, because our trial-and-error training methods won\u2019t be accurate. For example, when we\u2019re confused or misinformed about some question, we\u2019ll reward AI systems for giving the wrong answer to it - unintentionally training deceptive behavior.\n<p></p>\n<p>\n</p></li><li>We should expect disaster if we have AI systems that are both (a) <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">powerful enough</a> to defeat humans and (b) aiming for states of the world that we didn\u2019t intend. (\u201cDefeat\u201d means taking control of the world and doing what\u2019s necessary to keep us out of the way; it\u2019s unclear to me whether we\u2019d be literally killed or just forcibly stopped<sup><a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd#fn1\" rel=\"footnote\">1</a></sup> from changing the world in ways that contradict AI systems\u2019 aims.)</li></ul>\n<p>More: <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">Why would AI \"aim\" to defeat humanity?</a></p>\n</details>\n<p>\nHowever, there is room for debate in what a \u201cdigital brain\u201d truly shows:\n</p>\n<ul>\n\n<li>Many people are adamant that the readings are unreliable and misleading.\n\n</li><li>Some people point out that humans are <em>also</em> interested in power and influence, and often think about what they can and can\u2019t get away with, but this doesn\u2019t mean they\u2019d take over the world if they could. They say the AIs might be similar.\n\n</li><li>There are also cases of people doing digital neuroscience that claims to show that AIs are totally safe. These could be people like \u201cA\u201d above who want to focus on pushing forward with AI development rather than bringing it to a halt, or people who just find the alarmists annoying and like to contradict them, or people who are just sloppy with their research. Or people who have been manipulated or bribed by AIs themselves.\n</li>\n</ul>\n<p>\nThere are also very concerning incidents, such as:\n</p>\n<ul>\n\n<li>An AI steals a huge amount of money by bypassing the security system at a bank - and it turns out that this is because the security system was disabled by AIs <em>at the bank</em>. It\u2019s suspected, maybe even proven, that all these AIs had been communicating and coordinating with each other in code, such that humans would have difficulty detecting it. (And they had been <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">aiming</a> to divide up the funds between the different participating AIs, each of which could stash them in a bank account and use them to pursue whatever <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#unintended-aims\">unintended aims they might have</a>.)\n\n</li><li>An obscure new political party, devoted to the \u201crights of AIs,\u201d completely takes over a small country, and many people suspect that this party is made up mostly or entirely of people who have been manipulated and/or bribed by AIs. \n\n</li><li>There are companies that own huge amounts of AI servers and robot-operated factories, and are aggressively building more. Nobody is sure what the AIs or the robots are \u201cfor,\u201d and there are rumors that the humans \u201crunning\u201d the company are actually being bribed and/or threatened to carry out instructions (such as creating more and more AIs and robots) that they don\u2019t understand the purpose of.\n</li>\n</ul>\n<p>\nAt this point, there are a lot of people around the world calling for an immediate halt to AI development. But:\n</p>\n<ul>\n\n<li>Others resist this on all kinds of grounds, e.g. \u201cThese concerning incidents are anomalies, and what\u2019s important is that our country keeps pushing forward with AI before others do,\u201d etc.\n\n</li><li>Anyway, it\u2019s just too late. Things are moving incredibly quickly; by the time one concerning incident has been noticed and diagnosed, the AI behind it has been greatly improved upon, and the total amount of AI influence over the economy has continued to grow.\n</li>\n</ul>\n<p>\n<strong>Defeat. </strong>\n</p>\n<p>\n(Noting again that I could imagine things playing out a lot more <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/#the-standard-argument-superintelligence-and-advanced-technology\">quickly and suddenly</a> than in this story.)\n</p>\n<p>\nIt becomes more and more common for there to be companies and even countries that are clearly just run entirely by AIs - maybe via bribed/threatened human surrogates, maybe just forcefully (e.g., robots seize control of a country\u2019s military equipment and start enforcing some new set of laws).\n</p>\n<p>\nAt some point, it\u2019s best to think of civilization as containing two different advanced species - humans and AIs - with the AIs having essentially all of the power, making all the decisions, and running everything. \n</p>\n<p>\nSpaceships start to spread throughout the galaxy; they generally don\u2019t contain any humans, or anything that humans had meaningful input into, and are instead launched by AIs to pursue aims of their own in space.\n</p>\n<p>\nMaybe at some point humans are killed off, largely due to simply being a nuisance, maybe even accidentally (as humans have driven many species of animals extinct while not bearing them malice). Maybe not, and we all just live under the direction and control of AIs with no way out.\n</p>\n<p>\nWhat do these AIs <em>do</em> with all that power? What are all the robots up to? What are they building on other planets? The short answer is that I don\u2019t know.\n</p>\n<ul>\n\n<li>Maybe they\u2019re just creating massive amounts of \u201cdigital representations of human approval,\u201d because this is what they were historically trained to seek (kind of like how humans sometimes do whatever it takes to get drugs that will get their brains into certain states).\n\n</li><li>Maybe they\u2019re competing with each other for pure power and territory, because their training has encouraged them to seek power and resources when possible (since power and resources are generically useful, for almost any set of aims).\n\n</li><li>Maybe they have a whole bunch of different things they value, as humans do, that are sort of (but only sort of) related to what they were trained on (as humans tend to value things like sugar that made sense to seek out in the past). And they\u2019re filling the universe with these things.\n</li>\n</ul>\n<p>\n</p><details id=\"Box9\"><summary>(Click to expand) What sorts of aims might AI systems have? </summary><p></p>\n\nIn a <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">previous piece</a>, I discuss why AI systems might form unintended, ambitious \"aims\" of their own. By \"aims,\" I mean particular states of the world that AI systems make choices, calculations and even plans to achieve, much like a chess-playing AI \u201caims\u201d for a checkmate position.\n\n<p>\nAn analogy that often comes up on this topic is that of human evolution. This is arguably the only previous precedent for <em>a set of minds [humans], with extraordinary capabilities [e.g., the ability to develop their own technologies], developed essentially by black-box trial-and-error [some humans have more \u2018reproductive success\u2019 than others, and this is the main/only force shaping the development of the species].</em>\n</p>\n<p>\nYou could sort of<sup><a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn#fn12\" rel=\"footnote\">12</a></sup> think of the situation like this: \u201cAn AI<sup><a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn#fn13\" rel=\"footnote\">13</a></sup> developer named Natural Selection tried giving humans positive reinforcement (making more of them) when they had more reproductive success, and negative reinforcement (not making more of them) when they had less. One might have thought this would lead to humans that are aiming to have reproductive success. Instead, it led to humans that aim - often ambitiously and creatively - for other things, such as power, status, pleasure, etc., and even invent things like birth control to get the things they\u2019re aiming for instead of the things they were \u2018supposed to\u2019 aim for.\u201d \n</p>\n<p>\nSimilarly, if our main strategy for developing powerful AI systems is to reinforce behaviors like \u201cProduce technologies we find valuable,\u201d the hoped-for result might be that AI systems aim (in the sense described <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn#unintended-aims\">above</a>) toward producing technologies we find valuable; but the actual result might be that they aim for some other set of things that is correlated with (but not the same as) the thing we intended them to aim for.\n</p>\n<p>\nThere are a lot of things they might end up aiming for, such as:\n</p>\n<ul>\n\n<li>Power and resources. These tend to be useful for most goals, such that AI systems could be quite consistently be getting better reinforcement when they habitually pursue power and resources.\n\n</li><li>Things like \u201cdigital representations of human approval\u201d (after all, every time an AI gets positive reinforcement, there\u2019s a digital representation of human approval).\n</li>\n</ul>\n<p></p>\n<p>More: <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">Why would AI \"aim\" to defeat humanity?</a></p>\n\n</details>\n<p>\n<span style=\"text-decoration:underline;\">END OF HYPOTHETICAL SCENARIO</span>\n</p>\n<h2 id=\"potential-catastrophes-from-aligned-ai\">Potential catastrophes from <em>aligned</em> AI</h2>\n\n\n<p>\nI think it\u2019s possible that misaligned AI (AI forming dangerous goals of its own) will turn out to be pretty much a non-issue. That is, I don\u2019t think the <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">argument I\u2019ve made for being concerned</a> is anywhere near watertight. \n</p>\n<p>\nWhat happens if you train an AI system by trial-and-error, giving (to oversimplify) a \u201cthumbs-up\u201d when you\u2019re happy with its behavior and a \u201cthumbs-down\u201d when you\u2019re not? I\u2019ve <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">argued</a> that you might be training it to deceive and manipulate you. However, this is uncertain, and - especially if you\u2019re able to avoid <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#deceiving-and-manipulating\">errors </a>in how you\u2019re giving it feedback - things might play out differently. \n</p>\n<p>\nIt might turn out that this kind of training just works as intended, producing AI systems that do something like \u201cBehave as the human would want, if they had all the info the AI has.\u201d And the nitty-gritty details of how <em>exactly</em> AI systems are trained (beyond the high-level <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#Box3\">\u201ctrial-and-error\u201d idea</a>) could be crucial.\n</p>\n<p>\nIf this turns out to be the case, I think the future looks a lot brighter - but there are still lots of pitfalls of the kind I outlined in <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/mPkFheB4EM6pmEC7y/\">this piece</a>. For example:\n</p>\n<ul>\n\n<li>Perhaps an authoritarian government launches a huge state project to develop AI systems, and/or uses espionage and hacking to steal a cutting-edge AI model developed elsewhere and deploy it aggressively. \n<ul>\n \n<li>I <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/mPkFheB4EM6pmEC7y/#power-imbalances\">previously noted</a> that \u201cdeveloping powerful AI a few months before others could lead to having technology that is (effectively) hundreds of years ahead of others\u2019.\u201d\n \n</li><li>So this could put an authoritarian government in an enormously powerful position, with the ability to surveil and defeat any enemies worldwide, and the ability to prolong the life of its ruler(s) indefinitely. This could lead to a very bad future, especially if (as I\u2019ve <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AKxKR4CeakyBsGFoH/#lock-in\">argued</a> could happen) the future becomes \u201clocked in\u201d for good.\n</li> \n</ul>\n\n</li><li>Perhaps AI companies race ahead with selling AI systems to anyone who wants to buy them, and this leads to things like: \n<ul>\n \n<li>People training AIs to act as propaganda agents for whatever views they already have, to the point where the world gets flooded with propaganda agents and it becomes totally impossible for humans to sort the signal from the noise, educate themselves, and generally make heads or tails of what\u2019s going on. (Some people think this has already happened! I think things can get quite a lot worse.)\n \n</li><li>People training \u201cscientist AIs\u201d to develop powerful weapons that can\u2019t be defended against (even with AI help),<sup id=\"fnref5\"><a href=\"#fn5\" rel=\"footnote\">[5]</a></sup> leading eventually to a dynamic in which ~anyone can cause great harm, and ~nobody can defend against it. At this point, it could be inevitable that we\u2019ll blow ourselves up.\n \n</li><li>Science advancing to the point where <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AKxKR4CeakyBsGFoH/\">digital people</a> are created, in a rushed way such that they are considered property of whoever creates them (no human rights). I\u2019ve <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AKxKR4CeakyBsGFoH/\">previously written</a> about how this could be bad.\n \n</li><li>All other kinds of chaos and disruption, with the least cautious people (the ones most prone to rush forward aggressively deploying AIs to capture resources) generally having an outsized effect on the future.</li></ul></li></ul>\n<p>\nOf course, this is just a crude gesture in the direction of some of the ways things could go wrong. I\u2019m guessing I haven\u2019t scratched the surface of the possibilities. And things could go very well too!\n</p>\n<h2 id=\"we-can-do-better\">We can do better</h2>\n\n\n<p>\nIn previous pieces, I\u2019ve talked about a number of ways we could do better than in the scenarios above. Here I\u2019ll just list a few key possibilities, with a bit more detail in expandable boxes and/or links to discussions in previous pieces.\n</p>\n<p>\n<strong>Strong alignment research (including imperfect/temporary measures). </strong>If we make enough progress <em>ahead of time</em> on alignment research, we might develop measures that make it <em>relatively easy</em> for AI companies to build systems that truly (not just seemingly) are safe. \n</p>\n<p>\nSo instead of having to say things like \u201cWe should slow down until we make progress on experimental, speculative research agendas,\u201d person B in the <a href=\"#debates\">above dialogue</a> can say things more like \u201cLook, all you have to do is add some relatively cheap bells and whistles to your training procedure for the next AI, and run a few extra tests. Then the speculative concerns about misaligned AI will be much lower-risk, and we can keep driving down the risk by using our AIs to help with safety research and testing. Why not do that?\u201d\n</p>\n<p>\nMore on what this could look like at a previous piece, <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd/\">High-level Hopes for AI Alignment</a>.\n</p>\n<p>\n</p><details id=\"Box10\"><summary>(Click to expand) High-level hopes for AI alignment </summary><p></p>\n\n<p>\nA <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd/\">previous piece</a> goes through what I see as three key possibilities for building powerful-but-safe AI systems.\n</p>\n<p>\nIt frames these using Ajeya Cotra\u2019s <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/hCsxvMAGpkEuLCE4E/#analogy-the-young-ceo\">young businessperson</a> analogy for the core difficulties. In a nutshell, once AI systems get capable enough, it could be hard to test whether they\u2019re safe, because they might be able to deceive and manipulate us into getting the wrong read. Thus, trying to determine whether they\u2019re safe might be something like \u201cbeing an eight-year-old trying to decide between adult job candidates (some of whom are manipulative).\u201d\n</p>\n<p>Key possibilities for navigating this challenge:</p>\n<ul>\n\n<li><strong>Digital neuroscience</strong>: perhaps we\u2019ll be able to read (and/or even rewrite) the \u201cdigital brains\u201d of AI systems, so that we can know (and change) what they\u2019re \u201caiming\u201d to do directly - rather than having to infer it from their behavior. (Perhaps the eight-year-old is a mind-reader, or even a young <a href=\"https://en.wikipedia.org/wiki/Professor_X#Powers_and_abilities\">Professor X</a>.)\n\n</li><li><strong>Limited AI</strong>: perhaps we can make AI systems safe by making them <em>limited</em> in various ways - e.g., by leaving certain kinds of information out of their training, designing them to be \u201cmyopic\u201d (focused on short-run as opposed to long-run goals), or something along those lines. Maybe we can make \u201climited AI\u201d that is nonetheless able to carry out particular helpful tasks - such as doing lots more research on how to achieve safety without the limitations. (Perhaps the eight-year-old can limit the authority or knowledge of their hire, and still get the company run successfully.)\n\n</li><li><strong>AI checks and balances</strong>: perhaps we\u2019ll be able to employ some AI systems to critique, supervise, and even rewrite others. Even if no single AI system would be safe on its own, the right \u201cchecks and balances\u201d setup could ensure that human interests win out. (Perhaps the eight-year-old is able to get the job candidates to evaluate and critique each other, such that all the eight-year-old needs to do is verify basic factual claims to know who the best candidate is.)\n</li>\n</ul>\n<p>\nThese are some of the main categories of hopes that are pretty easy to picture today. Further work on AI safety research might result in further ideas (and the above are not exhaustive - see my <a href=\"https://www.alignmentforum.org/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very\">more detailed piece</a>, posted to the Alignment Forum rather than Cold Takes, for more).\n</p>\n\n\n\n</details>\n<p></p>\n<p>\n<strong>Standards and monitoring. </strong>A big driver of the <a href=\"#how-we-could-stumble-into-catastrophe-from-misaligned-ai\">hypothetical catastrophe above </a>is that each individual AI project feels the need to stay ahead of others. Nobody wants to unilaterally slow themselves down in order to be cautious. The situation might be improved if we can <strong>develop a set of standards that AI projects need to meet, and enforce them evenly</strong> - across a broad set of companies or even internationally.\n</p>\n<p>\nThis isn\u2019t just about buying time, it\u2019s about creating <em>incentives</em> for companies to prioritize safety. An analogy might be something like the <a href=\"https://en.wikipedia.org/wiki/Clean_Air_Act_(United_States)\">Clean Air Act</a> or <a href=\"https://en.wikipedia.org/wiki/Corporate_average_fuel_economy\">fuel economy standards</a>: we might not expect individual companies to voluntarily slow down product releases while they work on reducing pollution, but once required, reducing pollution becomes part of what they need to do to be profitable.\n</p>\n<p>\nStandards could be used for things other than alignment risk, as well. AI projects might be required to:\n</p>\n<ul>\n\n<li>Take strong security measures, preventing states from capturing their models via espionage.\n\n</li><li>Test models before release to understand what people will be able to use them for, and (as if selling weapons) restrict access accordingly.\n</li>\n</ul>\n<p>\nMore at a <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/XRphCh6NbfQiDF3Nt/#global-monitoring\">previous piece</a>.\n</p>\n<p>\n</p><details id=\"Box11\"><summary>(Click to expand) How standards might be established and become national or international </summary><p></p>\n<p>\nI <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/XRphCh6NbfQiDF3Nt/#global-monitoring\">previously</a> laid out a possible vision on this front, which I\u2019ll give a slightly modified version of here:\n</p>\n<ul>\n\n<li>Today\u2019s leading AI companies could self-regulate by committing not to build or deploy a system that they can\u2019t convincingly demonstrate is safe (e.g., see Google\u2019s <a href=\"https://www.theweek.in/news/sci-tech/2018/06/08/google-wont-deploy-ai-to-build-military-weapons-ichai.html\">2018 statement</a>, \"We will not design or deploy AI in weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people\u201d).  \n<ul>\n \n<li>Even if some people at the companies would like to deploy unsafe systems, it could be hard to pull this off once the company has committed not to. \n \n</li><li>Even if there\u2019s a lot of room for judgment in what it means to demonstrate an AI system is safe, having agreed in advance that <span style=\"text-decoration:underline;\">certain evidence</span> is <em>not</em> good enough could go a long way.\n</li> \n</ul>\n\n</li><li>As more AI companies are started, they could feel soft pressure to do similar self-regulation, and refusing to do so is off-putting to potential employees, investors, etc.\n\n</li><li>Eventually, similar principles could be incorporated into various government regulations and enforceable treaties.\n\n</li><li>Governments could monitor for dangerous projects using regulation and even overseas operations. E.g., today the US monitors (without permission) for various signs that other states might be developing nuclear weapons, and might try to stop such development with methods ranging from threats of sanctions to <a href=\"https://en.wikipedia.org/wiki/Stuxnet\">cyberwarfare</a> or even military attacks. It could do something similar for any AI development projects that are using huge amounts of compute and haven\u2019t volunteered information about whether they\u2019re meeting standards.\n</li>\n</ul>\n<p>\n</p></details>\n<p></p>\n<p>\n<strong>Successful, careful AI projects. </strong>I think a single AI company, or other AI project, could enormously improve the situation by being <em>both</em> successful and careful. For a simple example, imagine an AI company in a <em>dominant</em> market position - months ahead of all of the competition, in some relevant sense (e.g., its AI systems are more capable, such that it would take the competition months to catch up). Such a company could put huge amounts of resources - including its money, top people and its advanced AI systems themselves (e.g., AI systems performing roles similar to top human scientists) - into AI safety research, hoping to find safety measures that can be published for everyone to use. It can also take a variety of other measures <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/XRphCh6NbfQiDF3Nt/#defensive-deployment\">laid out in a previous piece</a>.\n</p>\n<p>\n</p><details id=\"Box12\"><summary>(Click to expand) How a careful AI project could be helpful </summary>\n<p></p>\n<p>\nIn addition to using advanced AI to do AI safety research (noted above), an AI project could:\n</p>\n<ul>\n\n<li>Put huge effort into designing <em>tests </em>for signs of danger, and - if it sees danger signs in its own systems - warning the world as a whole.\n\n</li><li>Offer deals to other AI companies/projects. E.g., acquiring them or exchanging a share of its profits for enough visibility and control to ensure that they don\u2019t deploy dangerous AI systems.\n\n</li><li>Use its credibility as the leading company to lobby the government for helpful measures (such as enforcement of a <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/XRphCh6NbfQiDF3Nt/#global-monitoring\">monitoring-and-standards regime</a>), and to more generally highlight key issues and advocate for sensible actions.\n\n</li><li>Try to ensure (via design, marketing, customer choice, etc.) that its AI systems are not used for dangerous ends, and <em>are</em> used on applications that make the world safer and better off. This could include <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/XRphCh6NbfQiDF3Nt/#global-monitoring\">defensive deployment</a> to reduce risks from other AIs; it could include using advanced AI systems to help it gain clarity on how to get a good outcome for humanity; etc.\n</li>\n</ul>\n<p>\nAn AI project with a dominant market position could likely make a huge difference via things like the above (and probably via many routes I haven\u2019t thought of). And even an AI project that is merely <em>one of several leaders</em> could have enough resources and credibility to have a lot of similar impacts - especially if it\u2019s able to \u201clead by example\u201d and persuade other AI projects (or make deals with them) to similarly prioritize actions like the above.\n</p>\n<p>\nA challenge here is that I\u2019m envisioning a project with two arguably contradictory properties: being <em>careful</em> (e.g., prioritizing actions like the above over just trying to maintain its position as a profitable/cutting-edge project) and <em>successful</em> (being a profitable/cutting-edge project). In practice, it could be very hard for an AI project to walk the tightrope of being aggressive enough to be a \u201cleading\u201d project (in the sense of having lots of resources, credibility, etc.), while also prioritizing actions like the above (which mostly, with some exceptions, seem pretty different from what an AI project would do if it were simply focused on its technological lead and profitability).\n</p>\n<p>\n</p></details>\n<p></p>\n<p>\n<strong>Strong security. </strong>A key threat in the above scenarios is that an incautious actor could \u201csteal\u201d an AI system from a company or project that would otherwise be careful. My understanding is that based on current state of security, it could be extremely hard for an AI project to be safe against this outcome. But this could change, if there\u2019s enough effort to work out the problem of how to develop a large-scale, powerful AI system that is very hard to steal.\n</p>\n<p>\nIn future pieces, I\u2019ll get more concrete about what specific people and organizations can do <em>today</em> to improve the odds of factors like these going well, and overall to raise the odds of a good outcome.\n</p>\n\n<!--kg-card-end: html--><!--kg-card-begin: html-->\n<div class=\"footnotes\">\n<ol><li id=\"fn1\">\n<p>\n     E.g., <a href=\"https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines\">Ajeya Cotra </a>gives a 15% probability of transformative AI by 2030; eyeballing figure 1 from <a href=\"https://arxiv.org/pdf/1705.08807.pdf\">this chart</a> on expert surveys implies a &gt;10% chance by 2028.&nbsp;<a href=\"#fnref1\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn2\">\n<p>\n     To predict early AI applications, we need to ask not just \u201cWhat tasks will AI be able to do?\u201d but \u201cHow will this compare to all the other ways people can get the same tasks done?\u201d and \u201cHow practical will it be for people to switch their workflows and habits to accommodate new AI capabilities?\u201d\n</p><p>\n    By contrast, I think the implications of <em>powerful enough</em> AI for productivity don\u2019t rely on this kind of analysis - very high-level economic reasoning can tell us that being able to cheaply copy something with human-like R&amp;D capabilities would lead to <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/hCsxvMAGpkEuLCE4E/#explosive-scientific-and-technological-advancement\">explosive progress</a>.\n</p><p>\n    FWIW, I think it\u2019s fairly common for high-level, long-run predictions to be <em>easier</em> than detailed, short-run predictions. Another example: I think it\u2019s easier to predict a general trend of planetary warming (<a href=\"https://www.ipcc.ch/report/ar6/wg2/\">this seems very likely</a>) than to predict whether it\u2019ll be rainy next weekend.&nbsp;<a href=\"#fnref2\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn3\">\n<p>\n     <a href=\"https://www.anthropic.com/constitutional.pdf\">Here\u2019s an early example</a> of AIs providing training data for each other/themselves.&nbsp;<a href=\"#fnref3\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn4\">\n<p>\n     <a href=\"https://github.com/features/copilot\">Example of AI helping to write code</a>.&nbsp;<a href=\"#fnref4\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn5\">\n\n<p>\n     To be clear, I have no idea whether this is possible! It\u2019s not obvious to me that it would be dangerous for technology to progress a lot and be used widely for both offense and defense. It\u2019s just a risk I\u2019d rather not incur casually via indiscriminate, rushed AI deployments.&nbsp;<a href=\"#fnref5\" rev=\"footnote\">\u21a9</a>\n\n</p></li></ol></div>\n", "user": {"username": "HoldenKarnofsky"}}, {"_id": "n82RPezsuW7yQdztE", "title": "Forecasting could use more gender diversity", "postedAt": "2023-01-13T19:27:54.482Z", "htmlBody": "<p>The new org I\u2019m part of, the&nbsp;<a href=\"https://forecastingresearch.org/\"><u>Forecasting Research Institute</u></a>, recently began a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/kEd5qWwg8pZjWAeFS/announcing-the-forecasting-research-institute-we-re-hiring\"><u>hiring round</u></a>, and we noticed that around 80% of research analyst applicants were male. This isn\u2019t terribly surprising\u2014a large percentage of forecasting tournament participants have been male historically, so we expected familiarity and interest in forecasting to be male-skewed currently. But, as a woman in forecasting myself, it does&nbsp;<i>bug</i> me.</p><p>It bugs me on several levels. One is puzzlement: many of the fields most relevant to forecasting and forecasting research\u2014like international relations, history, social science, and, in the case of forecasting research, behavioral science\u2014are not short of women. (And in any case, forecasting is so interdisciplinary that&nbsp;<i>most</i> backgrounds have some value to add.) There\u2019s also nothing about the broad spectrum of potential uses of forecasting, in policy, public discourse, institutional decision-making, philanthropy, etc., that suggests its impact is limited to particularly male domains.</p><p>Which brings me to the next point. If the field of forecasting is systematically failing to attract roughly half of the population, containing roughly half of the talent, good ideas, valuable skills and knowledge, it\u2019s made poorer and will likely face an uphill climb toward relevance and real-world impact. Which would be a shame, given its great potential.</p><p>So if you\u2019re a woman reading this (or another sort of person who feels underrepresented in forecasting) and you think there\u2019s even a chance you might be interested in doing forecasting research, here\u2019s my case for why you should apply to FRI\u2019s&nbsp;<a href=\"https://forecastingresearch.org/roles/research-analyst\"><u>research analyst position</u></a>:</p><ul><li>You don\u2019t need to be an experienced forecaster, or like&nbsp;<i>doing&nbsp;</i>forecasting. You don\u2019t need to be super quantitative. You don\u2019t need to be intimately familiar with forecasting research\u2014for the right candidate, as long as you\u2019re curious, motivated, and can get excited about unsolved problems in forecasting, FRI is willing to train you up.</li><li>Forecasting is an early-stage field with high impact potential, and the opportunity to be part of building such a field is kind of rare.</li><li>This also means there are tons of interesting open questions we need to answer, and lots of room to be creative in finding the answers.</li><li>In terms of subject matter, the work is extremely varied\u2014so if you\u2019re, say, interested in bio&nbsp;<i>and</i> AI&nbsp;<i>and</i> public policy, you might get to touch on all of them at the same time, or in quick succession. Some of our projects will also allow you to interact with leading domain experts or prominent organizations doing cool work when we partner to test forecasting tools.</li><li>If you want to use it this way, forecasting research can be a great facilitator for working on your own epistemics, and can provide a framework for interpreting and filtering the many important things one can have views about in a relatively independent way. (This is one of the primary motivations of my colleague Josh.)</li><li>Not only is everyone on our&nbsp;<a href=\"https://forecastingresearch.org/team\"><u>team</u></a> enthusiastic about improving gender diversity in our org, and in the forecasting space generally, they\u2019re also just extremely nice, interesting, competent and thoughtful as people.</li></ul><p>If you\u2019re still not sure if you could get sufficiently interested in forecasting for applying to be worth it, I\u2019d suggest checking out our&nbsp;<a href=\"https://forecastingresearch.org/research\"><u>research</u></a> page to see the projects we\u2019ll be working on in the near future. But to give you a sense of how diverse the traits of people who\u2019ve gotten into forecasting research are (all on the FRI team):</p><ul><li>I don\u2019t have a \u201cformal\u201d background of any sort: I dropped out of college, and subsequently hopped around doing stuff like writing and generalist research. I then got into \u201ccaring about AI\u201d via compelling arguments from friends, one of whom gave me a job doing AI forecasting. From there I started getting interested in more general questions in the science of forecasting.</li><li>Some traits that feel related to my interest in forecasting: I have a strong amateur interest in history, I\u2019m drawn to working in relatively uncharted territory, and I\u2019m an obsessive player of strategy games (like Civilization and Europa Universalis).</li><li>As for the rest of the team, one of us has a philosophy background, and was at GiveWell for years; we also have an economist, a former lawyer, a political science grad, and, well, Phil Tetlock. (It feels hubristic to try and encapsulate Phil in a single domain.)</li><li>A few of the things that drew my teammates to forecasting include: finding the world confusing, and feeling that people generally underestimate levels of uncertainty; an interest in economic forecasting; a love of the Red Sox (and sports forecasting more generally); a desire to cut through the noise of typical communication norms; favoring&nbsp;<a href=\"https://slatestarcodex.com/2018/01/24/conflict-vs-mistake/\"><u>mistake theory over conflict theory</u></a></li></ul><p>I really do think increased diversity will improve the field of forecasting, but on a selfish note, I\u2019d also just like to see more people like me (women) working in it. And I think a lot of people might be missing out on something great due simply to founder effects or [insert your preferred explanation for forecasting\u2019s gender ratio here]. So go on:&nbsp;<a href=\"https://forecastingresearch.org/roles/research-analyst\"><u>apply</u></a>.&nbsp;</p>", "user": {"username": "Tegan"}}, {"_id": "kDyG6p6FqwJ4ioQt4", "title": "Concerns about AI safety career change", "postedAt": "2023-01-13T20:52:47.074Z", "htmlBody": "<p><strong>Summary:</strong></p><ul><li>I'm a software engineer interested in working on AI safety, but confused about its career prospects. I outlined all my concerns below.</li><li>In particular, I had trouble finding accounts of engineers working in the field, and the differences between organizations/companies working on AI safety are very unclear from the outside.</li><li>It's also not clear if frontend skills are seen as useful, or whether applicants should reside within the US.</li></ul><p><strong>Full text:</strong></p><p>I'm an experienced full-stack software engineer and software/strategy consultant based in Japan. I've been loosely following EA since 2010, and have become increasingly concerned about AI x-risk since 2016. This has led me to regularly consider possible careers in AI safety, especially now that the demand for software engineers in the field has increased dramatically.</p><p>However, having spent ~15 hours reading about the current state of the field, organizations, and role of engineers, I find myself having more questions than I started with. In hope of finding more clarity and help share what engineers considering the career shift might be wondering, <strong>I decided to outline my main points of concern below</strong>:</p><ol><li>The only accounts of engineers working in AI safety I could find were two articles and a problem profile on 80,000 Hours<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefipyzhag0bio\"><sup><a href=\"#fnipyzhag0bio\">[1]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3ueq9wjc7tq\"><sup><a href=\"#fn3ueq9wjc7tq\">[2]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefw2fgx178idn\"><sup><a href=\"#fnw2fgx178idn\">[3]</a></sup></span>. Not even the AI Alignment Forum seemed to have any posts written by engineers sharing their experience. Despite this, most orgs have open positions for ML engineers, DevOps engineers, or generalist software developers. What are all of them doing?<ol><li>Many job descriptions listed very similar skills for engineers, even when the orgs seemed to have very different approaches on tackling AI safety problems. <strong>Is the set of required software skills really that uniform across organizations?</strong></li><li>Do software engineers in the field feel that their day-to-day work is meaningful? Are they regularly learning interesting and useful things? How do they see their career prospects?</li><li>I'm also curious whether projects are done with a diverse set of technologies? Who is typically responsible for data transformations and cleanup? How much ML theory should an engineer coming into the field learn beforehand? (I'm excited to learn about ML, but got very mixed signals about the expectations.)</li></ol></li><li>Some orgs describe their agenda and goals. In many cases, these seemed very similar to me, as all of them are pragmatic and many even had shared or adjacent areas of research. <strong>Given the similarities, why are there so many different organizations?</strong> How is an outsider supposed to know what makes each of them unique?<br><ol><li>As an example, MIRI states that they want to <i>\"ensure that the creation of smarter-than-human machine intelligence has a positive impact\"</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffaihygbsrej\"><sup><a href=\"#fnfaihygbsrej\">[4]</a></sup></span>, Anthropic states they have <i>\"long-term goals of steerable, trustworthy AI\"</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkcdbkvni12d\"><sup><a href=\"#fnkcdbkvni12d\">[5]</a></sup></span>, Redwood Research states they want to <i>\"align -- future systems with human interests\"</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxxdzb24hs18\"><sup><a href=\"#fnxxdzb24hs18\">[6]</a></sup></span>, and Center of AI Safety states they want to <i>\"reduce catastrophic and existential risks from AI\"</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnajgtcabl0f\"><sup><a href=\"#fnnajgtcabl0f\">[7]</a></sup></span>. What makes these different from each other? They all sound like they'd lead to similar conclusions about what to work on.</li><li>I was surprised to find that some orgs didn't really describe their work or what differentiates them. How are they supposed to find the best engineers if interested ones can't know what areas they are working on? I also found that it's sometimes very difficult to evaluate whether an org is active and/or trustworthy.<ol><li>Related to this, I was baffled to find that MIRI hasn't updated their agenda since 2015<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffpamrelnuwo\"><sup><a href=\"#fnfpamrelnuwo\">[8]</a></sup></span>, and their latest publication is dated at 2016<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffaihygbsrej\"><sup><a href=\"#fnfaihygbsrej\">[4]</a></sup></span>. However, their blog seems to have ~quarterly updates? Are they still relevant?</li></ol></li><li>Despite finding many orgs by reading articles and publications, I couldn't find a good overall list of ones that specifically work on AI safety. Having such a list might be valuable for people coming into the field, especially if it had brief overviews on what makes each org stand out. It may also be relevant for donors and community builders, as well as people looking for a particular niche.</li><li>It's a bit unclear how the funding for AI safety is organized. Some groups get grants from CEA and longtermism funds, some are sponsored by universities, but many also seem like private companies? How does that work? (My impression is that AI safety is still very difficult to monetize.)</li></ol></li><li>Frontend skills are sometimes listed in AI safety orgs' job descriptions, but rarely mentioned in problem profiles or overviews of the engineering work. <strong>Are people looking for frontend skills or not?</strong><ol><li>As someone whose core experience is in developing business-critical web apps, I'm particularly curious about whether web/mobile apps are needed to compliment other tools, and whether UI/UX design is of any consideration in AI safety work.</li><li>I'd argue that frontend and design skills can be relevant, in particular for meta tools like collaboration platforms, or for making results more visual and interactive (like OpenAI often does). Long-term research projects may also benefit from custom UIs for system deployment, management, and usage. I wonder what fraction of AI safety researchers would agree.</li><li>My own skills are pretty evenly distributed between frontend and backend, and I'm left wondering whether AI safety orgs would need someone with more specialization (as opposed to skills they currently may not have).</li></ol></li><li>It seems a vast majority of AI safety work is done in the US. However, the US timezone is sometimes tricky in Asia due to little overlap in working hours. How much of a problem is this seen as? Are there any AI safety groups based in Asia, Africa, or EU that have a good track record?<ol><li>What would even be a reasonable heuristic for assessing \"good track record\" in this case? For research orgs one can look at recent publications, but not every org does research. The best I have right now is whether the org in question has been mentioned in at least two introductory posts across 80,000 Hours, EA Forum, and AI Alignment Forum. This could be another benefit of a curated list as mentioned above.</li></ol></li></ol><p>My counterfactual for not doing AI safety work would be becoming financially independent in ~3-5 years, after which I'd likely do independent work/research around AI policy and meta-EA matters anyway. I'm thinking that transitioning into AI safety now could be better, as the problems have become more practical, the problems seem more urgent, and working on them would allow gaining relevant skills/results sooner.</p><p>I decided to post this on the EA forum in order to get a broader view of opinions, including from people not currently engaged with the field. Any advice or insights would be much appreciated!</p><p>If you happen to be looking for someone with full-stack skills and are ok with flexible hours/location, feel free to drop me a private message as well!</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnipyzhag0bio\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefipyzhag0bio\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://80000hours.org/articles/ml-engineering-career-transition-guide/\">https://80000hours.org/articles/ml-engineering-career-transition-guide/</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3ueq9wjc7tq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3ueq9wjc7tq\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://80000hours.org/podcast/episodes/olsson-and-ziegler-ml-engineering-and-safety/\">https://80000hours.org/podcast/episodes/olsson-and-ziegler-ml-engineering-and-safety/</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnw2fgx178idn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefw2fgx178idn\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://80000hours.org/career-reviews/software-engineering/\">https://80000hours.org/career-reviews/software-engineering/</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfaihygbsrej\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffaihygbsrej\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://intelligence.org/research/\">https://intelligence.org/research/</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkcdbkvni12d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkcdbkvni12d\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://jobs.lever.co/Anthropic/9ba1d7b4-5b21-4ac9-86f3-875a15c8a091\">https://jobs.lever.co/Anthropic/9ba1d7b4-5b21-4ac9-86f3-875a15c8a091</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxxdzb24hs18\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxxdzb24hs18\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.redwoodresearch.org/\">https://www.redwoodresearch.org/</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnajgtcabl0f\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnajgtcabl0f\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://safe.ai/about\">https://safe.ai/about</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfpamrelnuwo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffpamrelnuwo\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://intelligence.org/technical-agenda/\">https://intelligence.org/technical-agenda/</a></p></div></li></ol>", "user": {"username": "mmKALLL"}}, {"_id": "zMAdoAyAcZJHybG2R", "title": "Calculating how much small donors funge with money that will never be spent", "postedAt": "2023-01-16T19:54:21.250Z", "htmlBody": "<p><i>Epistemic status: Confident that the effect is real, though likely smaller than suggested by the toy-model.</i></p><p><strong>Summary</strong></p><p>Small donors should discount the cost effectiveness of their donations to interventions above a large funder\u2019s bar if</p><ol><li>they expect the large funder not to have spent all their capital by the time of AGI\u2019s arrival<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgogvwsme4xq\"><sup><a href=\"#fngogvwsme4xq\">[1]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwx04biblc4\"><sup><a href=\"#fnwx04biblc4\">[2]</a></sup></span></li><li>their donation to interventions above the large funder\u2019s bar <i>funges</i> with the large funder.</li></ol><p>In this post I describe a toy model to calculate how much to discount due to this effect.</p><p>I apply the model to a guess of Open Philanthropy\u2019s &nbsp;spending on Global Health and Development (GHD) with <a href=\"https://www.metaculus.com/questions/5121/date-of-general-ai/\">Metaculus\u2019 AGI timelines</a> (25% by 2029, 50% by 2039). &nbsp;The model implies that small donors should consider interventions above OP's GHD bar, e.g. GiveWell's top charities, are only 55% as cost effective as the small donors first thought. &nbsp;For shorter AGI timelines (25% by 2027, 50% by 2030) this factor is around 35%.</p><p>I use OP's GHD spending as an example because of their clarity around funding rate and bar for interventions. This discount factor would be larger if one funges with 'patient' philanthropic funds (such as <a href=\"https://founderspledge.com/stories/introducing-the-patient-philanthropy-fund\">The Patient Philanthropy Fund</a>).</p><p>This effect is a corollary of the result that most donor's AGI timelines (e.g. deferral to Metaculus) imply that the community spend at a greater rate. <strong>When a small donor funges with a large donor (and saves them spending themselves), &nbsp;the community's spending rate is effectively lowered </strong>(compared to when the small donor does not funge).&nbsp;</p><p>This effect occurs when a small donor has shorter timelines than a large funder, or the large funder is not spending at a sufficiently high rate. In the latter case, small donors - by donating to interventions below the large funder's bar - are effectively correcting the community's implicit bar for funding.</p><p><strong>Toy model</strong></p><p>Suppose you have the choice of donating to one of two interventions,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>&nbsp;which gives&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"a\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span></span></span></span></span>&nbsp;utils per $, or&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span></span>, which gives&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"b\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span></span></span></span></span></span>&nbsp;utils per $. Suppose further that the available interventions remain the same every year and that both have room for funding this year.</p><p>A large funder&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"F\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span></span></span></span></span></span>&nbsp;will ensure that&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span>&nbsp;is fully funded this year, so if you donate $1 to&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span>, then&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"F\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span></span></span></span></span></span>, effectively, has $1 more to donate in the future. I suppose that&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"F\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span></span></span></span></span></span>&nbsp;only ever donates to (opportunities as good as)&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span>.</p><p>I suppose that&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"F\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span></span></span></span></span></span>'s capital decreases by some constant amount&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span></span></span></span></span>&nbsp;times their initial capital each year.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftspcc6qpi4c\"><sup><a href=\"#fntspcc6qpi4c\">[3]</a></sup></span>&nbsp;This means that&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"F\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span></span></span></span></span></span>&nbsp;will have no assets in&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1/f\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span></span></span></span></span>&nbsp;years from now.</p><p>Supposing AGI arrives&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"t\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span></span>&nbsp;years from now, then F will have spent fraction&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\text{min}(ft,1)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">min</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;of their current capital on&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span>.</p><p>Accounting for this funging and assuming AGI arrives at time&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"t\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span></span>, the cost effectiveness of your donation to&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span>&nbsp;is then&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\text{min}(ft,1) \\cdot a\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">min</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span></span></span></span></span>&nbsp;utils per $. Then if&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"b> \\text{min}(ft,1) \\cdot a\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&gt;</span></span><span class=\"mjx-mtext MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">min</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span></span></span></span></span>, marginal spending by small donors on&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span></span>&nbsp;is more cost effective than on&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span>.</p><p>By considering distributions of AGI's arrival time&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"t\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span></span>&nbsp;and the large funder's funding rate&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span></span></span></span></span>&nbsp;we can get a distribution of this multiplier.&nbsp;</p><p><strong>Plugging in numbers</strong></p><p>I take</p><ul><li>The large funder&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"F\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span></span></span></span></span></span>&nbsp;to be Open Philanthropy\u2019s Global Health and Wellbeing spending on Global Health and Development and intervention&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span>&nbsp;to be Givewell\u2019s recommendations.&nbsp;</li><li>I take&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1/f\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span></span></span></span></span>, the expected time until OP's funds dedicated to GHD are depleted to be distributed&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathrm{Normal}(20, 20)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">N</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">r</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">m</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">l</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">20</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">20</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;bounded below by 5.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmp5ywk1dutj\"><sup><a href=\"#fnmp5ywk1dutj\">[4]</a></sup></span></li><li>I take AGI timelines to be an approximation those on&nbsp;<a href=\"https://www.metaculus.com/questions/5121/date-of-general-ai/\"><u>Metaculus</u></a><u>.</u></li></ul><figure class=\"image image_resized\" style=\"width:49.6%\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673898727/mirroredImages/zMAdoAyAcZJHybG2R/qm7zafxlxa4zllee4vsk.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673898727/mirroredImages/zMAdoAyAcZJHybG2R/lbwywm8xxq4lrvw8rgpu.png 151w, https://res.cloudinary.com/cea/image/upload/v1673898727/mirroredImages/zMAdoAyAcZJHybG2R/qpydufk6zls6ulr1ghtt.png 231w, https://res.cloudinary.com/cea/image/upload/v1673898727/mirroredImages/zMAdoAyAcZJHybG2R/zm1iweks2ls0tnaza9hh.png 311w, https://res.cloudinary.com/cea/image/upload/v1673898727/mirroredImages/zMAdoAyAcZJHybG2R/fgjv1bixd7zvzgb1rszx.png 391w\"><figcaption>My distribution on&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1/f\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span></span></span></span></span></figcaption></figure><p>These distributions on AGI timelines and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1/f\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span></span></span></span></span>&nbsp;give the following distribution of the funging multiplier (reproducible <a href=\"https://colab.research.google.com/drive/104L3LVHeD6TWIVQq3krdyspkM7LL-5sh?usp=sharing\">here</a>).</p><figure class=\"image image_resized\" style=\"width:50.43%\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673898727/mirroredImages/zMAdoAyAcZJHybG2R/vtcpo2vrxev55soej9dq.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673898727/mirroredImages/zMAdoAyAcZJHybG2R/qc6cosk4648o2scaa54m.png 93w, https://res.cloudinary.com/cea/image/upload/v1673898727/mirroredImages/zMAdoAyAcZJHybG2R/nzxmzxwqguotzh097l7h.png 173w, https://res.cloudinary.com/cea/image/upload/v1673898727/mirroredImages/zMAdoAyAcZJHybG2R/rg6kuxkbniatah3yzbxh.png 253w, https://res.cloudinary.com/cea/image/upload/v1673898727/mirroredImages/zMAdoAyAcZJHybG2R/gnpq4y63ytxjez97fppl.png 333w, https://res.cloudinary.com/cea/image/upload/v1673898727/mirroredImages/zMAdoAyAcZJHybG2R/y3stk9z1e8znzmuxncdj.png 413w\"></figure><p>The ratio of cost effectiveness between GiveWell's recommendations and GiveDirectly,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"a/b\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span></span></span></span></span></span>, is approximately 7-8&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefa8ho54srrcc\"><sup><a href=\"#fna8ho54srrcc\">[5]</a></sup></span>and so <strong>small donors should give &nbsp;to interventions in the (5, 7)x GiveDirectly range</strong>.&nbsp;</p><p>For donors with shorter timelines AGI timelines (25% by 2027, 50% by 3030) one should discount more.</p><figure class=\"image image_resized\" style=\"width:51.75%\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673898727/mirroredImages/zMAdoAyAcZJHybG2R/xqva6whtvgez9ipgezuj.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673898727/mirroredImages/zMAdoAyAcZJHybG2R/wlntnzw5ndduuemfhf74.png 103w, https://res.cloudinary.com/cea/image/upload/v1673898727/mirroredImages/zMAdoAyAcZJHybG2R/txox9nqmfvpd4r5wermj.png 183w, https://res.cloudinary.com/cea/image/upload/v1673898727/mirroredImages/zMAdoAyAcZJHybG2R/tukhix8n6g1ednlz3uop.png 263w, https://res.cloudinary.com/cea/image/upload/v1673898727/mirroredImages/zMAdoAyAcZJHybG2R/dyf67pyrdmpkply8jtrr.png 343w, https://res.cloudinary.com/cea/image/upload/v1673898727/mirroredImages/zMAdoAyAcZJHybG2R/tvcs78qzlltpcb36etj8.png 423w\"></figure><p><i>Thanks to Tom Barnes for comments.</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngogvwsme4xq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgogvwsme4xq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>And you believe that there is no use for capital post-AGI.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwx04biblc4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwx04biblc4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>One could also compute the analysis for any (existential) catastrophe that either affects the funder's ability to donate or the existence of interventions to give to.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntspcc6qpi4c\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftspcc6qpi4c\">^</a></strong></sup></span><div class=\"footnote-content\"><p>An earlier version of the model considered appreciation of capital and constant spending on&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span>, though I have cut for simplicity to avoid discussion of patient&nbsp;</p><p>One could, for example, suppose that their capital is measured in \"ability to do good\" and that the interventions become more expensive as quickly as their capital appreciates.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmp5ywk1dutj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmp5ywk1dutj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is some data and a lot of intuition.</p><p>In July 2022 <a href=\"https://www.openphilanthropy.org/research/update-on-our-planned-allocation-to-givewells-recommended-charities-in-2022/#f+9715+1+3\">Open Philanthropy wrote</a></p><blockquote><p>Our current best estimate is that, for interventions like GiveWell\u2019s recommended charities, our optimal strategy is to spend 8-10% of relevant assets each year.. This means that whatever level of assets we have, our cost-effectiveness \u201cbar\u201d for GiveWell-like interventions should be set so that the opportunities above this bar in the next year add up to 8-10% of such assets.</p></blockquote><p>My distribution on&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span></span></span></span></span>&nbsp;implies they have roughly 20% change of meeting or exceeding this target.&nbsp;</p><p>Eyeballing some historic spending and guesses of Dustin Moskovitz's wealth pus their historic spending rate at around 2-5%.</p><p>The lower bound&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1/f >5\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&gt;</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">5</span></span></span></span></span></span></span>&nbsp;is based on my current impression that OP's GHD team do not plan to spend all their funds within the next 5 years. A distribution other than trunctated normal would likely be more appropriate.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fna8ho54srrcc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefa8ho54srrcc\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.openphilanthropy.org/research/technical-updates-to-our-global-health-and-wellbeing-cause-prioritization-framework/\">Open Philanthropy, November 2021</a>&nbsp;</p><blockquote><p>Overall,&nbsp;<strong>we currently expect GiveWell\u2019s marginal cost-effectiveness to end up around 7-8x GiveDirectly (in their units)</strong>,</p></blockquote></div></li></ol>", "user": {"username": "Tristan Cook"}}, {"_id": "5TKbBj3nGgd5xAwEL", "title": "A Brief History of EA Finland", "postedAt": "2023-01-14T10:54:28.990Z", "htmlBody": "<h1>Introduction</h1><p>Less than three years ago, the only EA group in Finland was dependent on a single organizer for its existence. Now I would argue that the Finnish EA community<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffqt76zsiixf\"><sup><a href=\"#fnfqt76zsiixf\">[1]</a></sup></span>&nbsp;is resilient enough to withstand the departure of some of our most active organizers. (Of course, that would not be desirable, but it would not be a disaster either.) This post is a very brief overview of the actions that lead up to where we are now. At the end, I will also tell my story of how I got involved. If you want to read more about EA Finland I can highly recommend Ada\u2019s post (<a href=\"https://forum.effectivealtruism.org/posts/Kq3YwsrWr5nxQfpPj/ea-finland-from-a-philosophy-discussion-club-to-a-national\">EA Finland: from a philosophy discussion club to a national organization</a>). In addition to being a more detailed description of what happened, it includes some key-takeaways hopefully useful for other community builders.</p><h1>Early build-up</h1><p>EA Finland and EA Helsinki were founded around 2013, but their activities faded away until Onni Aarne decided to re-establish EA Helsinki in 2018. Until 2021, the monthly meetings were attended by around 4-15 people and the EA Helsinki organizing team consisted of Onni as the main organizer and an actives\u2019 chat with about 8 people willing to help out when needed.&nbsp;</p><h1>The first fellowship</h1><p>In January 2021 we ran our first intro fellowship. We received 49 applications, and of the 40 accepted, 34 completed the course. The organizing team continued to grow as there was a sudden need for activities and in the beginning it was quite anarchic without responsibility areas, structure or plans. If you joined a weekly coworking session, you were likely to be assigned a task, and if you didn\u2019t, you had little chance of knowing what was going on. In our first strategy meeting in July 2021, we shared clear responsibilities having about 20 actives involved in community building. During fall 2021, EA Finland was revived and the second Finnish university group EA Aalto was launched.&nbsp;</p><h1>Funding and even more groups</h1><p>By January 2022, however, many volunteers were feeling overwhelmed and this was no longer a sustainable way of working if we wanted to improve the community. Thus, we started thinking about hiring people to work for EA Finland. (Sidenote: I cannot recommend a recruitment process with consensus decision-making among six employee candidates who are also friends.) In May, we gratefully received funding for four part-time employees from the EA Infrastructure Fund. This was also the first time we received any funding for community building in Finland, as before that any small costs had been paid by the members, we were reluctant to add any layers of bureaucracy before getting a person on board who already had the skills and interest required and we had organized our events almost exclusively online during the pandemic so hadn't found any effective use for the money.</p><p>That spring 2022 two more local groups also emerged in the next largest cities Tampere and Turku. In August, two more students contacted us and said they wanted to start EA groups at their universities. So we have now grown from 1 to 8 groups in 1.5 years (with a disclaimer that most of them are small and the continuation is uncertain).&nbsp;</p><p>In October, we rented a small office/community space. That is because we wanted to have more live events in Helsinki (without having to book spaces weeks in advance), create a more open community especially to new EAs here, and cultivate EA collaborations.</p><h1>My story</h1><p>When I got involved in EA Helsinki in 2019 the university group was my only link to EA for almost a year, and I remember the monthly meetup discussions as one of my monthly highlights. It could be intimidating to often be the only non-STEM person and woman in the group, as well as the youngest. But it didn\u2019t matter who I was when we were discussing making the world better and I liked the focus on things that really matter.</p><p>When the pandemic started, everything was moved online which increased my engagement. I remember being skeptical when Onni and Aleksi wanted to organize a fellowship. I am glad they followed through anyway, because it is one of the most important factors in why we are where we are today. It also feels surreal that it is less than two years since that first fellowship.</p><p>Thanks to the fellowship, the organizing team essentially doubled, and I took on the responsibility to be the coordinator, even though I was going on a 6-month student exchange. Initially, I planned and organized the monthly actives\u2019 meetings from Germany. At some point, however, I took a break from community building for a month or two and was pleased to see that the chaos did not increase significantly. When we started thinking about recruitment, applying was a major decision for me, as it also meant I would be slowing down my studies. However, I am glad I did. These past six months working at EA Finland have been instructive, there have been both ups and downs, and above all it has been inspiring to be part of a young and growing movement with lots of untapped potential.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfqt76zsiixf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffqt76zsiixf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For brevity the headline states that this is the history of EA Finland but in reality it is an overview of the Finnish Effective Altruism community as I know it, which is a larger than only the national organization.</p></div></li></ol>", "user": {"username": "Karla Still"}}, {"_id": "MB7CiPS2d73ne6Bk3", "title": "Embracing Selfishness", "postedAt": "2023-01-14T21:36:30.996Z", "htmlBody": "<p>One of the most basic ideas in the philosophy of effective altruism is that we should do more to help others. Putting this idea into practice can be, to say the least, hard.</p><p>Different people practice altruism in different ways and to different extents. Some donate to an effective charity and move on. Others use altruism as a guide in making major decisions, like which career to pursue. It\u2019s ultimately a personal choice, and it\u2019s not what this post is about. I can\u2019t tell you how to live your life.</p><p>EA is not just one person\u2019s life philosophy though; it\u2019s a community too. So here\u2019s something I can say: whatever values a community chooses to espouse, and however it chooses to interpret them, its practices ought to support those values.</p><p>Suppose a large community wants to support altruism. Actually, let\u2019s simplify things: suppose it doesn\u2019t care to support anything else. This isn\u2019t necessarily what EA is or should be, but it will highlight some points that apply more generally. What might a community of maximally dedicated altruists look like? A natural place to start is that it should encourage altruism and discourage selfishness. But that\u2019s worth a second thought.</p><h2>Failure modes</h2><p>Some people at first interpret altruism as forgoing all disposable income to fund highly rated charities. If the culture places too much emphasis on altruistic behavior, it\u2019s no surprise that some of those people are going to end up in a \u201cmisery trap.\u201d&nbsp;<a href=\"https://michaelnotebook.com/eanotes/\">Michael Nielsen\u2019s notes</a>&nbsp;summarize the phenomenon well. Here\u2019s a small example, taken from&nbsp;<i>The Most Good You Can Do</i>:</p><blockquote><p>Julia admits to making mistakes. When shopping, she would constantly ask herself, \u201cDo I need this ice cream as much as a woman living in poverty elsewhere in the world needs to get her child vaccinated?\u201d That made grocery shopping a maddening experience\u2026</p></blockquote><p>Even if you\u2019re a maximally dedicated altruist, a misery trap would destroy your productivity and dissuade others from joining your cause. So from an impact-maximizing perspective it\u2019s acceptable, even obligatory, to invest in your own happiness. Similar reasoning advises against excessive frugality in other areas, like movement building. Yes, conferences are expensive, but some of them are worth it. Networking pays off in hard-to-quantify ways.</p><p>Those who stop short of excessive frugality, still spending money on ice cream and conference tickets, might appear selfish to someone who is forgetting about second-order consequences. Sometimes it\u2019s actually pretty hard to confirm that they aren\u2019t being selfish. But scrutinizing every expense would be a terrible waste of time, so it\u2019s not worth making a big deal out of it.</p><p>That attitude has own failure mode. A group of morally motivated people funding things labeled \u201caltruism\u201d might attract\u2026&nbsp;<i>differently</i>&nbsp;motivated people who want to label their thing \u201caltruism.\u201d There\u2019s outright grift to worry about, and then there are more subtle forms of motivated reasoning. For example, someone might really convince themselves that leasing a fancy office will pay for itself in employee morale. It\u2019s hard to prove that it won\u2019t, but such arguments shouldn\u2019t be taken at face value. Even very ethical people are prone to deceive themselves.</p><p>Both sides of the coin, motivated reasoning and grift, have been discussed at length in the context of \u201cfree-spending EA.\u201d While the problem is certainly worse in that context, it doesn\u2019t entirely go away when money is tight. There are still plenty of ways to trade off altruism for personal benefit. Even in the complete absence of money, status alone can be a powerful enough incentive to distort one\u2019s reasoning. (Like&nbsp;<a href=\"https://astralcodexten.substack.com/p/highlights-from-the-comments-on-subcultures\">Scott Alexander</a>, I\u2019m including \u201ca sense of belonging, being appreciated, and connecting with people\u201d in my definition of status.)</p><p>In&nbsp;<a href=\"https://www.newyorker.com/news/annals-of-inquiry/sam-bankman-fried-effective-altruism-and-the-question-of-complicity\">an article on the FTX fallout</a>, Gideon Lewis-Kraus brought up one way to approach the issue:</p><blockquote><p>In Jewish law, there is a concept called \u201cmar\u2019it ayin\u201d designed to address this kind of ambiguity: you don\u2019t eat fake bacon, for example, because a passerby might see you and conclude you\u2019re eating real bacon. The reason for this law isn\u2019t primarily to protect the reputation of the fake-bacon-eater; it\u2019s to sustain the norms of the whole community. The passerby might decide that, if it was O.K. for you to eat bacon, it\u2019s O.K. for him to do it, too. When important norms\u2014of frugality, and the honesty with which it was discussed\u2014are seen as violated, the survival of the culture is imperilled.</p></blockquote><p>I think this perspective is very appropriate for certain things, like buying luxury homes as part of an ostensibly charitable project. If you apply it to everything though\u2014\u201chm, partying after EA Global would look kind of selfish, so maybe I shouldn\u2019t\u201d\u2014you risk sliding back into a misery trap.</p><h2>Norms</h2><p>Let\u2019s return to our group of maximally dedicated altruists. What they\u2019d like to do is encourage the ambiguous actions that serve the greater good, and discourage the ones that are actually just selfish. The problem is that they can\u2019t tell which are which.</p><p>One approach is to start with their ideal, translate it into rules, and then just enforce those rules as best they can without being too overbearing. I think this is a mistake. The rules are going to sound like \u201cfly business class if it\u2019s worth being better rested before an important meeting\u201d and \u201cspend more on food if it will make your event more impactful.\u201d There\u2019s just no way to enforce that kind of decision-making. Anyone who expects rules to be followed is going to be in trouble.</p><p>It would be better to frame altruism as only an ideal. Since an ideal can\u2019t be depended upon, important decisions would call for transparent communication, conflict of interest statements, approval from multiple parties, and the like. This approach is pretty reasonable. It\u2019s definitely less susceptible to egregious fraud. But it doesn\u2019t help much with small-scale motivated reasoning: if perfectly balanced cost-benefit analysis is widely aspired to, people will want to believe they\u2019re doing it. They may be tempted to persuade others of their rationalizations. Those other people, meanwhile, may be reluctant to \u201caccuse\u201d someone of imperfection. This seems like a real danger to a community\u2019s epistemic health.</p><p>So let\u2019s take another step back. It doesn\u2019t actually matter how well norms work in a perfect world. It matters that they work in the face of ambiguity. What if we start over and prioritize that?</p><p>Perhaps the first thing to settle on is what counts as altruism. This will factor into decisions about which projects to fund and which ones to hold up as exemplary. The stakes are high: an error can not only waste charitable resources, but promote further growth of the waste. That\u2019s a strong reason to be conservative with the altruism label. If there\u2019s any real ambiguity, it\u2019s better to treat an action as selfish.</p><p>Crucially, that doesn\u2019t mean rejecting ambiguous actions outright. What will count against someone\u2019s status\u2014even a little bit\u2014is a second degree of freedom. Here it\u2019s better to give people the benefit of the doubt and reject only clear-cut extravagance. No one wants to promote misery trap thinking. Besides, in some cases the benefits of accepting the altruistic actions will outweigh the costs of accepting the selfish ones.</p><p>Flying business class to a high-stakes meeting? Premium catering for an event? Sounds kind of selfish, but I\u2019d accept it. In fact, I might embrace it as much as I would if someone convinced me that the decision was morally justified. That way, they\u2019d have no reason to even try.</p><h2>Implications</h2><p>This seems rather counterintuitive. Many things are going to fall into the gray area between \u201calmost certainly selfish\u201d and \u201calmost certainly altruistic.\u201d Most of them will be regarded as selfish, yet completely accepted. This is not just about investing in happiness, which is an indirect way to help others. Nor is it about altruism-life balance\u2014remember, we\u2019ve been imagining people who are maximally dedicated to altruism! Even if we leave room for other values, if we only try to promote altruism within a certain boundary, the argument suggests that we sometimes accept selfishness <i>within that boundary</i>.</p><p>Counterintuitive indeed, but these norms are perfectly consistent. More than that, they\u2019re complementary. Regarding your friends as selfish would be uncomfortable if selfish were not an acceptable thing to be; accepting selfishness would invite fraud if it weren\u2019t always labeled as such.</p><p>Is this effective altruism though? I don\u2019t see why not. This particular way of handling ambiguity does nothing to change the ultimate goal of solving the world\u2019s biggest problems. It doesn\u2019t mean distrusting other members of the community; if anything, realistic expectations make trust easier. And it highlights, rather than diminishes, the significance of legible signals like a commitment to donate.</p><p>Embracing selfishness does require two things. The first is institutions that are built for it, meaning they don\u2019t depend on impartial cost-benefit analysis by people who have a stake in the outcome. (<i>Funding</i> selfishness is something we often wish to avoid.) The second is a culture that doesn\u2019t try too hard to reject it. Again, we have better things to do with our time than scrutinize every expense.</p><p>Since I\u2019m most familiar with university community building, I\u2019ll use free meals as an example. It is conceivable that buying dinner for an EA group might be a good investment to keep people engaged and eventually nudge a few of them towards high-impact careers. But there are strong incentives to believe this and no practical way to verify it.</p><p>So, part one: if I were a prospective funder, I would regard expensive EA dinners as a selfish activity. That doesn\u2019t mean it\u2019s always wrong to fund them, but impact-oriented donors would probably want to be conservative about it. Not only does this ensure that the funds in question are used wisely, it sets an important example for everyone else. (CEA recently scaled things back in this category; I think it was a good call regardless of what the stock market is doing.)</p><p>Then part two: it\u2019s not worth debating all the ways eating dinner together might or might not help spread EA ideas. In fact, <a href=\"https://forum.effectivealtruism.org/posts/EpMQBmQv7e4yaDYYN/community-building-solely-as-a-tool-for-impact-creates-toxic\">it\u2019s probably better if you don\u2019t</a>. Sometimes building a community means spending time with friends doing \u201clow-impact\u201d things, and that is really <i>really</i> okay. Embracing selfishness lets you not sweat the small stuff and focus on what\u2019s actually important.</p><p>Something I\u2019ve been reminded of in recent months is how delicate EA culture is. Growth will naturally make certain norms harder to maintain\u2014the important question is how they can evolve while preserving what makes EA special. We can afford neither to compromise our standards with ineffective spending, nor to erode trust by constantly regarding one another with suspicion. Though embracing selfishness is far from a complete answer, it may help to take a few steps in that direction.</p>", "user": {"username": "ryanbloom"}}, {"_id": "9J3o3cjGSHbHZoWQN", "title": "A new Heuristic to Update on the Credences of Others", "postedAt": "2023-01-16T11:35:26.273Z", "htmlBody": "<p><i>This is my distillation of the 2016 paper&nbsp;</i><a href=\"https://philarchive.org/archive/FENUOT-3\"><u>Updating on the Credences of Others: Disagreement, Agreement, and Synergy</u></a>&nbsp;<i>by Kenny Easwaran, Luke Fenton-Glynn, Christopher Hitchcock, and Joel D. Velasco. Following their wish, I will refer to them as the collective \u2018EaGlHiVe\u2019 pronounced &nbsp;\u2018eagle hive\u2019&nbsp;(concatenating the first two letters of their names). Unless indicated, I tried not to add substantive claims myself.</i><br><i>Despite my efforts to make this as clear and simple as possible, this post is rather technical. I expect readers who are unfamiliar with the fundamentals of probability theory and bayesian reasoning to struggle with parts of it. However, you don't need to be fluent in probability theory to get the gist of this post.&nbsp;</i><br>&nbsp;</p><h2>The main points</h2><ul><li>Sometimes, the probability estimates of others provide evidence for some proposition.&nbsp;</li><li>Ideally, we would want to do Bayesian Updating on the credences of others, but that often seems cognitively over-demanding.&nbsp;</li><li>Hence, we want to find&nbsp;<i>rules&nbsp;</i>or&nbsp;<i>heuristics&nbsp;</i>that come reasonably close to Bayesian Updating, while remaining tractable.&nbsp;</li><li>&nbsp;'Upco'(short for \u2018<strong>up</strong>dating on the <strong>c</strong>redences of <strong>o</strong>thers\u2019) promises to be such a rule. Upco yields the same result as Bayesian Updating under some technical assumptions about the shape of our likelihood functions for someone's credence regarding some claim.&nbsp;</li><li>Upco roughly requires you to set your odds for some proposition to the product of your prior odds and the odds of everyone whose credences you are updating on.&nbsp;</li><li>It remains mostly unclear to me under which conditions we should have likelihood functions that entail Upco, and how much these assumptions can be relaxed so that Upco still closely approximates Bayesian Updating.<br>&nbsp;</li></ul><h2>The problem</h2><p>Your friend is going through an intense dinosaur phase. They have spent the last few weeks researching the <a href=\"https://en.wikipedia.org/wiki/Quetzalcoatlus\"><i>Quetzalcoatlus</i></a> <i>northropi, </i>an extinct species of giant flying reptiles that are believed to be among the largest flying animals of all time. When you see your friend, they successfully maneuver your conversation towards the megafauna. In passing, they claim that the largest Q. northopi had a wing span <i>of over 12 meters</i>. &nbsp;They show you this picture:&nbsp;</p><figure class=\"image image_resized\" style=\"width:54.81%\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1672678798/mirroredImages/9J3o3cjGSHbHZoWQN/yurxnvnf8avwpyyb2oob.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1672678798/mirroredImages/9J3o3cjGSHbHZoWQN/zvnjtwyhyoz35idwsgvg.png 150w, https://res.cloudinary.com/cea/image/upload/v1672678798/mirroredImages/9J3o3cjGSHbHZoWQN/xgux8qc6or9zszvrep29.png 300w, https://res.cloudinary.com/cea/image/upload/v1672678799/mirroredImages/9J3o3cjGSHbHZoWQN/ptg9oczlyelabhiwqtth.png 450w, https://res.cloudinary.com/cea/image/upload/v1672678798/mirroredImages/9J3o3cjGSHbHZoWQN/ldqwoga8lyqfbrip6rim.png 600w, https://res.cloudinary.com/cea/image/upload/v1672678798/mirroredImages/9J3o3cjGSHbHZoWQN/qfaggv1moponbion0tt0.png 750w, https://res.cloudinary.com/cea/image/upload/v1672678798/mirroredImages/9J3o3cjGSHbHZoWQN/uebyhahh2d0cls5isvru.png 900w, https://res.cloudinary.com/cea/image/upload/v1672678798/mirroredImages/9J3o3cjGSHbHZoWQN/ipp9jjrlwfmp2is78f7d.png 1050w, https://res.cloudinary.com/cea/image/upload/v1672678798/mirroredImages/9J3o3cjGSHbHZoWQN/olno13jiaiiuqyzjhz7r.png 1200w, https://res.cloudinary.com/cea/image/upload/v1672678798/mirroredImages/9J3o3cjGSHbHZoWQN/iqqepcy6wpynlwfae0yg.png 1350w, https://res.cloudinary.com/cea/image/upload/v1672678798/mirroredImages/9J3o3cjGSHbHZoWQN/fscyvzbfb8mps4w13rfx.png 1487w\"><figcaption>By Matt Martyniuk (Dinoguy2), Mark Witton and Darren Naish - Own work, CC BY 3.0, https://commons.wikimedia.org/w/index.php?curid=4129661</figcaption></figure><p>\u201cThis can't be true\u201d, you resist. \u201cNo\u201d, they say, &nbsp;\u201cthis is what the literature clearly suggests and I am about 90% confident in this\u201d.&nbsp;</p><p>You know that your friend is scientifically trained. They are someone who dispassionately evaluates the literature and lets their beliefs be dictated purely by the forces of evidence and reason. Admittedly, they do a good job with that. You trust their judgement. So how should you respond&nbsp;<i>epistemically&nbsp;</i>when you hear their probability estimate (\u201ccredence\u201d)? Should you increase your confidence in the claim?</p><p>Assuming that your friend started out much less than 90% confident that the Q. northropi had such a massive wingspan, they must have regarded their evidence as confirmatory, on balance. Since you believe that they responded reasonably to the evidence they saw, and since you know almost nothing about this topic, you seem to have good reason to increase your own confidence in this claim. By doing this, you can partially benefit from their research and expertise without having to do any research yourself.&nbsp;</p><p>If we abstract away from your friend and dinosaurs, it seems like you should&nbsp;<i>at least sometimes</i> revise your credences when you learn about the credences of others. But how?</p><p>Enter Bayesianism.&nbsp;</p><p>Bayesianism provides a general account of rational belief revision in response to evidence. &nbsp;As such, it should be equipped to deal with this case too. It would therefore be interesting to know the Bayesian response to learning the credences of others. But first: what precisely does Bayesianism say?<br>&nbsp;</p><h2>Introducing Bayesianism<br>&nbsp;</h2><p>All flavours of Bayesianism are committed to at least two normative claims:&nbsp;</p><ol><li>At any point in time, all your credences should be probabilities; they should be precise real numbers between 0 and 1 and they should obey the other&nbsp;<a href=\"https://en.wikipedia.org/wiki/Probability_axioms\"><u>axioms of probability theory</u></a>. (Probabilism)</li><li>If you receive some piece of evidence E regarding proposition A, then you should set your credence in A to the credence you would have previously given A if you&nbsp;<i>hypothetically were to learn E. </i>(Conditionalization)</li></ol><p>The first claim is about what rational credences look like at any point in time - &nbsp;a view called \u201cProbabilism\u201d. The second claim is about how rational credences change over time, as evidence comes in - a process that\u2019s called \u201cBayesian Updating\u201d or \u201cConditionalization\u201d. The credence you have before updating on some piece of evidence is called your \u201cprior credence\u201d (or \u201cprior\u201d) and the credence you have after updating is your \u201cposterior credence\u201d (or just \u201cposterior\u201d) relative to this piece of evidence.&nbsp;</p><p>I will leave you with this very brief overview since this post is not about Bayesianism in and of itself. If you\u2019d like more details, I recommend you look&nbsp;<a href=\"https://compass.onlinelibrary.wiley.com/doi/10.1111/j.1747-9991.2011.00399.x\"><u>here</u></a>,&nbsp;<a href=\"https://www.lesswrong.com/posts/xJyY5QkQvNJpZLJRo/radical-probabilism-1\"><u>here</u></a> or&nbsp;<a href=\"https://michael-franke.github.io/intro-data-analysis/Chap-03-01-probability-conditional.html\"><u>here</u></a> (in decreasing order of depth).</p><p>So, how would an ideal Bayesian update on the credences of others?</p><p>&nbsp;As a Bayesian, you might respond that someone's credence is just another type of evidence and there's no reason to treat it differently than \"non-psychological evidence\". Therefore the correct epistemic response to other people's credences is Bayesian Updating. Concretely, suppose that you have a credence&nbsp;<i>p&nbsp;</i>in proposition&nbsp;<i>A</i> (and a credence&nbsp;<i>1 -p&nbsp;</i>in its negation), and you now encounter someone who reports a credence of&nbsp;<i>q&nbsp;</i>in the same proposition (and a credence&nbsp;<i>1 - q&nbsp;</i>in its negation). I will write your credence in A as&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P(A)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>&nbsp;and Q\u2019s credence in A as&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q(A)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>. Your posterior credence, after updating on someone else\u2019s credence, I\u2019ll write as&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P^+(A)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>. Bayesian Updating on Q\u2019s credence would then look like this<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefb8d4szvil9\"><sup><a href=\"#fnb8d4szvil9\">[1]</a></sup></span>:<br><br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P^+(A) = P(A|Q(A) = q) = \\frac{P(A)P(Q(A) = q|A)}{P(Q(A)=q)}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 6.076em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 8.593em; top: -1.706em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 8.593em; bottom: -0.999em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 6.076em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.913em; vertical-align: -0.707em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span><br>&nbsp;</p><p>Or, in terms of odds:<br>&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\frac{P^+(A)}{P^+(\\neg A)}= \\frac{P(A|Q(A) = q)}{P(\\neg A|Q(A)=q)} = \\frac{P(Q(A)=q)|A)}{P(Q(A)=q)|\\neg A)}\\frac{P(A)}{P(\\neg A)}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 2.771em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 3.919em; top: -1.801em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.435em; padding-left: 0.23em; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 3.919em; bottom: -1.02em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.347em; padding-left: 0.23em; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 2.771em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.995em; vertical-align: -0.721em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 4.936em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 6.981em; top: -1.706em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 6.981em; bottom: -0.999em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 4.936em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.913em; vertical-align: -0.707em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 5.211em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 7.37em; top: -1.706em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 7.37em; bottom: -0.999em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 5.211em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.913em; vertical-align: -0.707em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 2.225em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 3.146em; top: -1.706em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 3.146em; bottom: -0.999em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 2.225em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.913em; vertical-align: -0.707em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span></p><p>&nbsp;</p><p>(It may seem strange to see probabilities of other probabilities. Maybe it helps to remind you that \u201cQ(A) = q\u201d is just the proposition \u201cQ has credence q in A\u201d.)</p><p>In summary, the credences of others seem to be a kind of evidence and Bayesianism seems to tell us, in some sense, how to respond to it. Unfortunately, using Bayes Theorem as a procedure to calculate our posteriors seems generally too computationally demanding to be feasible. Ideally, we want a much simpler rule that imitates or approximates Bayesian Updating in most cases. 'Upco'(short for \u2018<strong>up</strong>dating on the <strong>c</strong>redences of <strong>o</strong>thers\u2019) promises to be such a rule. Upco yields the same result as Bayesian Updating under some technical assumptions about the shape of our likelihood functions for someone's credence regarding some claim.&nbsp;<br>&nbsp;</p><h2>Upco</h2><h3>An informal argument for Upco<br>&nbsp;</h3><p>Before formally stating Upco, I want to provide a more informal and intuitive justification for Upco rather than just stating the technical assumptions and showing that it follows from those. You can find the latter<a href=\"https://forum.effectivealtruism.org/posts/9J3o3cjGSHbHZoWQN/how-to-sometimes-update-on-the-credences-of-others#Appendix_A__The_Linear_Average_as_an_updating_rule_\"> in the appendix.</a> (The following subchapter is from me and is not in the paper).&nbsp;<br><br>Think of other people's minds as black boxes that receive evidence on one side and output credences on the other. When we stand at the side where the credences come out, what we\u2019d really like to do is to reach behind the box, grasp for the evidence and update our own credences accordingly. More precisely, we want to reach behind the box and get all the evidence, say all the observed coin flips<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9c6867x1upo\"><sup><a href=\"#fn9c6867x1upo\">[2]</a></sup></span>, that&nbsp;<i>we haven\u2019t updated on yet.&nbsp;</i>If the reason we want to update is to become more accurate, perhaps we don\u2019t even care about the&nbsp;<i>content&nbsp;</i>of the evidence for its own sake - whether it was {Heads, Tails} or {Heads, Heads}. Our only concern might be the&nbsp;<i>strength&nbsp;</i>of the evidence<i>&nbsp;</i>in order to then update as though we had seen evidence of that same strength<i>.&nbsp;</i>Suppose this was the case. Is there any way you can infer the strength of the evidence that I&nbsp;<i>have</i> seen and you&nbsp;<i>haven\u2019t,</i> regarding a proposition? (By \u201c<i>the</i> strength of the evidence\u201d I mean the strength that a rational agent would assign to it.)</p><p>I think you can infer that under some conditions: suppose you know for certain that I&nbsp;</p><ol><li>have seen evidence that is completely independent of yours assuming the truth of the claim in question,</li><li>responded rationally to the evidence that I received and</li><li>that I had a uniform prior at the very beginning, before receiving any evidence.&nbsp;</li></ol><p>With assumptions (1) - (3) in mind, it seems like you can infer the strength of my evidence regarding A: it is equal to my current <a href=\"https://en.wikipedia.org/wiki/Odds\">odds</a> in A. Why?</p><p>The strength that a Bayesian assigns to some piece of evidence E with respect to proposition A &nbsp;can be measured as the factor by which learning E changed their odds in A (see <a href=\"https://en.wikipedia.org/wiki/Bayes_factor\">Bayes factor</a>); or put differently, their odds in A&nbsp;<i>after</i> learning E divided by their odds in A&nbsp;<i>before&nbsp;</i>learning E is the strength they assigned to E. A reasonable measure for the strength of&nbsp;<i>all&nbsp;</i>their evidence regarding A (as assessed by themselves) is then the ratio of their current odds and their<i> initial</i> odds, prior to updating on&nbsp;<i>any evidence whatsoever</i>.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefssacmyoqfwi\"><sup><a href=\"#fnssacmyoqfwi\">[3]</a></sup></span>&nbsp;</p><p>Since I had a uniform prior initially and thus odds of 1, the strength I assigned to all my evidence is thus just my current odds.&nbsp;Per assumption (2), this was a rational assessment of evidential strength. Because of that and since our evidence is completely independent (3), this is also the strength you should (rationally) have assigned to that evidence given all your evidence regarding the claim in question.<br><br>For example, suppose I first updated on a piece of evidence which increased my odds by a factor of 2. Then another piece of evidence came in that increased my odds by a factor of 1.5. The strength of all my evidence up until then would be 2 x 1.5 = 3, which is also my current odds - 3 x 1 = 3 - divided by my initial odds since they were equal to 1.&nbsp; Since you only care about the strength of my evidence and you know for certain that you should have assigned strength 3 to my evidence, you are satisfied to update your credence as though you had seen evidence of that exact strength.&nbsp;<br>&nbsp;</p><h3>Upco for one peer</h3><p><br>And so we arrive at the odds version of Upco for partitions of the form&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"{A, \\neg A}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span></span></span>&nbsp;and a single peer Q with&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q(A) = q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span></span></span></span></span>. &nbsp;A \u201cpartition\u201d means a set of mutually exclusive and collectively exhaustive propositions like {\"Coin c landed heads\", \"Coin c did not land heads\"}.&nbsp;<br>&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\frac{P^+(A)}{P^+(\\neg A)}=\\frac{Q(A)}{Q(\\neg A)}\\frac{P(A)}{P(\\neg A)} = \\frac{q}{1-q}\\frac{p}{1-p}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 2.771em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 3.919em; top: -1.801em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.435em; padding-left: 0.23em; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 3.919em; bottom: -1.02em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.347em; padding-left: 0.23em; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 2.771em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.995em; vertical-align: -0.721em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 2.253em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 3.186em; top: -1.706em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 3.186em; bottom: -0.999em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 2.253em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.913em; vertical-align: -0.707em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 2.225em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 3.146em; top: -1.706em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 3.146em; bottom: -0.999em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 2.225em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.913em; vertical-align: -0.707em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 1.37em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.938em; top: -1.342em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.938em; bottom: -0.859em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 1.37em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.557em; vertical-align: -0.608em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 1.401em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.981em; top: -1.342em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.981em; bottom: -0.859em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 1.401em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.557em; vertical-align: -0.608em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span><strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</strong></p><p>&nbsp;</p><p>Alternatively, albeit not so intuitively, we can state Upco like this:<br>&nbsp;</p><span><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"P^+(A) = \\frac{p q}{p q + (1-p) (1-q)}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.584em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 8.349em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 8.349em; top: -1.226em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span></span><span class=\"mjx-denominator\" style=\"width: 8.349em; bottom: -1.09em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 8.349em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.316em; vertical-align: -1.09em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span></span></span></span></p><p>For an explanation of this formulation of Upco and a formal, rigorous justification of Upco, see <a href=\"https://forum.effectivealtruism.org/posts/9J3o3cjGSHbHZoWQN/how-to-sometimes-update-on-the-credences-of-others#Appendix_B__A_formal_justification_for_Upco_\">the appendix</a>.</p><p><i>It is important to note that it could still be rational to apply Upco even if the conditions I just mentioned - independent evidence, rational assessment of evidential strength and uniform prior - are not met. Assuming them makes the case for Upco especially intuitive for me, but they may not be necessary for Upco to imitate Bayesian Updating.&nbsp;</i></p><p>To understand Upco, we want to look at some examples. Let us first consider the simplest case in which you encounter one person, Q, who reports their credence in some proposition A and its negation.</p><blockquote><p><strong>Example 1</strong>. Your peer Q has 0.6 credence in proposition A and you currently have a credence of 0.3 in A. Now you both learn each other's credences. Upco tells you to update in the following way:&nbsp;<br><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P^+(A)=Q^+(A)&nbsp;=&nbsp; &nbsp;= \\frac{p q}{p q + (1-p) (1-q)} = \\frac{0.3\u22c50.6}{0.3\u22c50.6 + 0.7\u22c5 0.4} =\\frac{0.18}{0.18 + 0.28} =&nbsp;0.39\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=<span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"padding-bottom: 0.314em;\">=</span></span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 4.961em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 7.016em; top: -1.342em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 7.016em; bottom: -0.999em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 4.961em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.656em; vertical-align: -0.707em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 4.699em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 6.646em; top: -1.394em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.3</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.6</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 6.646em; bottom: -0.758em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.3</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.6</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.7</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.4</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 4.699em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.522em; vertical-align: -0.536em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 3.206em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 4.534em; top: -1.394em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.18</span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 4.534em; bottom: -0.747em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.18</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.28</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 3.206em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.514em; vertical-align: -0.528em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.39</span></span></span></span></span></span></span>&nbsp;</p></blockquote><p>In this case, you both update towards each other and your credences end up closer to your prior than towards Q\u2019s. A general lesson here is that when a set of peers all update on each other's credences using Upco, they will all have the same posterior credence, regardless of their priors<i>.</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefuavxpt62a7\"><sup><a href=\"#fnuavxpt62a7\">[4]</a></sup></span><i>&nbsp;</i></p><p>Also noteworthy is that in general for binary partitions&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"{A,\\neg A}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span></span></span>,<i> </i>any credence above 0.5 in some proposition A is evidence for A and any credence below 0.5 is evidence against A. (This is especially clear when you look at the odds formulation of Upco). What happens when one's credence is exactly 0.5?</p><blockquote><p><strong>Example 2.</strong> P and Q respectively have credences 0.3 and 0.5 in A. Upon learning each other\u2019s credence, Upco implies that:&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P^+(A)=Q^+(A)&nbsp;=&nbsp; &nbsp;\\frac{0.3 \u22c5 0.5}&nbsp;{0.3\u22c50.5 + 0.7\u22c5 0.5} =&nbsp;\\frac{0.15}{0.15 + 0.35} =&nbsp;0.3\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 4.699em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 6.646em; top: -1.394em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.3</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.5</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 6.646em; bottom: -0.757em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.3</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.5</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.7</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.5</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 4.699em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.522em; vertical-align: -0.536em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 3.206em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 4.534em; top: -1.394em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.15</span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 4.534em; bottom: -0.747em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.15</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.35</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 3.206em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.514em; vertical-align: -0.528em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.3</span></span></span></span></span></span></span></p></blockquote><p><br>Notice that P\u2019s credence didn\u2019t change at all. In general, if one peer\u2019s credence - in this case, Q\u2019s - is uniformly distributed across B and its alternatives<i>, </i>their credence does not constitute evidence with respect to B. They then adopt the credence of the other person in response to learning it<i>.&nbsp;</i>From one perspective this seems intuitive: if you haven\u2019t seen any evidence in favour of or against B and therefore you have a uniform prior, or if your evidence seems to equally support B and -B, why would I update on your credence? It can be surprising, however, if we consider a scenario such as this:&nbsp;<br>&nbsp;</p><blockquote><p><strong>Example 3</strong>. Suppose that I think that the probability of human extinction from AI this century is less than 0.01. Suppose also that some acclaimed AI safety researcher says that the odds are 50:50 that this will or will not happen this century. Upco tells me that I shouldn\u2019t change my credence in \u201cdoom from AI this century\u201d&nbsp;<i>at all&nbsp;</i>in response to learning their credences<i>.&nbsp;</i></p></blockquote><p><br>This example highlights that Upco is partition-dependent; if I had modelled the researcher as having a partition containing more than two propositions, Upco would imply that their credence<i> is</i>&nbsp;<i>indeed&nbsp;</i>evidence for the claim. Here we used a binary partition that collapses the possibilities of AI not ever causing human extinction or in some following century into one proposition: \"AI does not cause human extinction this century\". It seems more natural to have a partition with a separate proposition for each of those possibilities. &nbsp;In the next section, we will look at a generalization of Upco that can deal with this case in a more satisfying and intuitive way.<i>&nbsp;</i></p><p>As we have just seen, Upco is very sensitive to the number of elements that a partition contains: the same credence in A could be seen as providing strong evidence in favour of A or strong evidence against A depending on the size of the partition that contains A. These problems aren\u2019t unique to Upco. They arise in various places in the philosophy of probability theory (e.g. in the discussion of the<a href=\"https://en.wikipedia.org/wiki/Principle_of_indifference\"><u> indifference principle</u></a>). Still, this seems like a serious problem. The paper argues that some partitions are preferable to others, but doesn\u2019t provide a general account for dealing with this problem.</p><p>&nbsp;</p><h3>Arbitrary partitions<br>&nbsp;</h3><p>Thus far we have used Upco to update on the credences of others with respect to the binary partition&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"{A,&nbsp;\\neg A}.\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span></span></span></span>&nbsp;This limits Upco\u2019s usefulness quite a lot. For example, suppose you are trying to update on Q\u2019s probability distribution over the years in which the first human will arrive on Mars:<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q(Year = 2022) = 0.01\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;\">Y</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2022</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.01</span></span></span></span></span></span></span>,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q(Year = 2023) = 0.02\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;\">Y</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2023</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.02</span></span></span></span></span></span></span>,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q(Year = 2024) = 0.02\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;\">Y</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2024</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.02</span></span></span></span></span></span></span>&nbsp;and so on for at least a few hundred other propositions.&nbsp;</p><p><br>You&nbsp;<i>could&nbsp;</i>collapse this into a binary partition: e.g. \u201cHumans will arrive on Mars before 2035\u201d and \u201cHumans will&nbsp;<i>not&nbsp;</i>arrive on Mars before 2035\u201d. But, as it happens, Q\u2019s probability for arrival before 2035 is 50%. Therefore, as we discussed before, the simple Upco rule would tell us that Q\u2019s credences provide&nbsp;<i>no evidence at all</i> about the arrival of humans on Mars. This seems wrong<i>&nbsp;</i>as Q does&nbsp;<i>not</i> have a uniform probability distribution and thus their evidence (in the way they interpreted it) does favour some years over others. This motivates a more general version of Upco that can handle more complex partitions.<br><br>Suppose you have a probability distribution&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p_1,&nbsp;p_2, \u2026&nbsp;p_k\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">\u2026</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span></span></span>&nbsp;over an arbitrary partition&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"{A_1,&nbsp;A_2, \u2026&nbsp;A_k}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">\u2026</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span></span></span></span></span>. Also, suppose that you want to update on the credences of&nbsp;Q who has the distribution&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"q_1,&nbsp;q_2,\u2026&nbsp;q_k\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.014em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\" style=\"margin-right: -0.014em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">\u2026</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\" style=\"margin-right: -0.014em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span></span></span>&nbsp;over the same partition. Then applying this more general version of Upco to an arbitrary proposition&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span></span>&nbsp;<br>&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P^+(A_i)= \\frac{p_iq_i}{\\sum_{j=1}^{k}p_jq_j}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 3.247em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 4.592em; top: -1.372em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.014em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 4.592em; bottom: -1.525em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-munderover\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size1-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">\u2211</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.333em;\"><span class=\"mjx-sup\" style=\"font-size: 83.3%; padding-bottom: 0.216em; padding-left: 0px; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.014em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 3.247em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.048em; vertical-align: -1.079em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span></p><p>Alternatively, in odds form:&nbsp;</p><p><br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\frac{P^+(A_i)}{P^+(A_j)}=\\frac{p_i}{p_j}\\frac{q_i}{q_j}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 2.572em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 3.637em; top: -1.801em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.435em; padding-left: 0.23em; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 3.637em; bottom: -1.154em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.347em; padding-left: 0.23em; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 2.572em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.09em; vertical-align: -0.816em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.769em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.088em; top: -1.372em;\"><span class=\"mjx-msubsup\" style=\"\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.088em; bottom: -0.897em;\"><span class=\"mjx-msubsup\" style=\"\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 0.769em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.604em; vertical-align: -0.634em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.729em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.031em; top: -1.372em;\"><span class=\"mjx-msubsup\" style=\"\"><span class=\"mjx-base\" style=\"margin-right: -0.014em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.031em; bottom: -0.897em;\"><span class=\"mjx-msubsup\" style=\"\"><span class=\"mjx-base\" style=\"margin-right: -0.014em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 0.729em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.604em; vertical-align: -0.634em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span></p><p><br>Here is an example of how to apply this rule.</p><blockquote><p><br><strong>Example 4</strong>. Let&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"{D_1,&nbsp;D_2,...,&nbsp;D_{12}}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">12</span></span></span></span></span></span></span></span></span></span></span></span></span>&nbsp;be a set of propositions. You have credence&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1/12\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">12</span></span></span></span></span></span></span>&nbsp;in each proposition and you now encounter peer Q who thinks that &nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"D_6\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">6</span></span></span></span></span></span></span></span></span>&nbsp;is much more plausible than the rest. Specifically, they report credence&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0.1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.1</span></span></span></span></span></span></span>&nbsp;in &nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"D_6\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">6</span></span></span></span></span></span></span></span></span>&nbsp;and credence of&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"~0.08= (0.9/11)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.291em; padding-bottom: 0.372em;\">&nbsp;</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.08</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.9</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">11</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;in all of the other propositions. Using Upco, you update their credence as follows:<br><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P^+(D_6)&nbsp;=&nbsp;\\frac{(1/12)\u22c50.1}{(1/12)\u22c50.1 + 11\u22c5[(1/12)\u22c5(0.9/11)]}&nbsp;= 0.099\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">6</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 9.728em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 13.758em; top: -1.706em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">12</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.1</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 13.758em; bottom: -0.999em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">12</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">11</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">12</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.9</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">11</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 9.728em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.913em; vertical-align: -0.707em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.099</span></span></span></span></span></span></span></p></blockquote><p><br>The point of this example is that the \"break-even point\"<i>\u2014</i>the threshold beyond which someone's credence in&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"D_j\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span></span></span></span></span>&nbsp;provides evidence&nbsp;<i>for</i> &nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"D_j\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span></span></span></span></span><i>\u2014</i>has shifted by a lot compared to earlier examples with binary partitions. For a binary partition, someone reporting credence 0.1 would be rather strong evidence against the claim in question whereas here it is the opposite. Since the partition contains 12 propositions, 0.1 is above the break-even point in this case (which is&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1/12&nbsp; \u2248 0.083\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">12</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u2248</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.083</span></span></span></span></span></span></span>).&nbsp;In general, if a partition consists of k propositions, then the break-even point is&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1/k\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span>.&nbsp;<br>&nbsp;</p><h3>Multiple peers<br>&nbsp;</h3><p>Suppose you want to update on the credences of&nbsp;<i>multiple</i> peers&nbsp;<i>Q, R,...</i> with credences&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"q, r,\u2026\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">\u2026</span></span></span></span></span></span></span>Upco easily generalizes to handle this case in the following way:&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P^+(A_i)=&nbsp;\\frac{p_iq_ir_i...}{\\sum_{j=1}^{k}p_jq_jr_j...}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 4.428em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 6.262em; top: -1.372em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.014em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 6.262em; bottom: -1.525em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-munderover\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size1-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">\u2211</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.333em;\"><span class=\"mjx-sup\" style=\"font-size: 83.3%; padding-bottom: 0.216em; padding-left: 0px; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.014em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 4.428em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.048em; vertical-align: -1.079em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span></p><p><br>Or, equivalently, in the odds formulation:<br><br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\frac{P^+(A_i)}{P^+(A_j)}=\\frac{p_i}{p_j}\\frac{q_i}{q_j}\\frac{r_i}{r_j}...\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 2.572em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 3.637em; top: -1.801em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.435em; padding-left: 0.23em; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 3.637em; bottom: -1.154em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.347em; padding-left: 0.23em; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 2.572em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.09em; vertical-align: -0.816em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.769em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.088em; top: -1.372em;\"><span class=\"mjx-msubsup\" style=\"\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.088em; bottom: -0.897em;\"><span class=\"mjx-msubsup\" style=\"\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 0.769em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.604em; vertical-align: -0.634em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.729em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.031em; top: -1.372em;\"><span class=\"mjx-msubsup\" style=\"\"><span class=\"mjx-base\" style=\"margin-right: -0.014em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.031em; bottom: -0.897em;\"><span class=\"mjx-msubsup\" style=\"\"><span class=\"mjx-base\" style=\"margin-right: -0.014em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 0.729em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.604em; vertical-align: -0.634em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.733em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.036em; top: -1.372em;\"><span class=\"mjx-msubsup\" style=\"\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.036em; bottom: -0.897em;\"><span class=\"mjx-msubsup\" style=\"\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 0.733em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.604em; vertical-align: -0.634em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span></span></span></span>&nbsp;&nbsp;<br><br>Let us now look at an example which brings out an interesting feature of this version of Upco.</p><blockquote><p><strong>Example 5</strong>. Let B<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefatz5imwnvlh\"><sup><a href=\"#fnatz5imwnvlh\">[5]</a></sup></span>&nbsp;be the proposition that \u201cMore than 10,000 people will have signed the Giving What We Can Pledge by 2025\u201d. Q, R, and P give this claim credences of 0.7, 0.7, 0.4 respectively. If they are applying Upco, how will they update?<br><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P^+(B) =&nbsp;Q^+(B) =&nbsp;R^+(B) =&nbsp; \\frac{0.7\u22c50.7\u22c50.4}{0.7\u22c50.7\u22c50.4 + 0.3\u22c50.3\u22c50.6}=\\frac{0.196}{0.196 + 0.054}&nbsp;=0.78.\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 6.9em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 9.758em; top: -1.405em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.7</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.7</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.4</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 9.758em; bottom: -0.758em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.7</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.7</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.4</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.3</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.3</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">\u22c5</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.6</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 6.9em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.53em; vertical-align: -0.536em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 3.913em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 5.534em; top: -1.394em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.196</span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 5.534em; bottom: -0.758em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.196</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.054</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 3.913em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.522em; vertical-align: -0.536em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.78.</span></span></span></span></span></span></span>&nbsp;&nbsp;</p></blockquote><p><br>Thus, upon learning each other's credences all three should update towards 0.78 credence in B. Interestingly, this is higher than the prior credence of anyone in the group. This is a property of Upco that the EaGlHiVe (the authors) refers to as \u201csynergy\u201d.&nbsp;<br>&nbsp;</p><h3>Beyond peers<br>&nbsp;</h3><p>The versions of Upco that we have considered don\u2019t give different weights to the credences of different people, even if one seems much more informed than the other. This may seem odd. Suppose a professor in climate science tells me that the probability of global warming of at least 2\u00b0C by 2030 is 20%. It seems like I should regard this as stronger evidence than if a random stranger told me the same thing. This points to yet another natural extension of Upco.<br><br>Let us also assume that the people who report their credence on some proposition&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span></span>&nbsp;- denoted as&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span></span>&nbsp;for person P,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"q_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.014em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span></span>&nbsp;for Q etc. -&nbsp; have differing levels of epistemic reliability with respect to that proposition. We can track these differences by assigning higher weights to more reliable people. Since Upco is a multiplicative rule, we put these weights&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"w_1,&nbsp;w_2,...,&nbsp;w_n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">w</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">w</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">w</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span></span></span></span>&nbsp;as exponents in our formula, and get the following most general Upco rule considered in the paper:&nbsp;</p><p>&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P^+(A_i)=&nbsp;\\frac{p_i^{w_p}q_i^{w_q}r_i^{w_r}...}{\\sum_{j=1}^{k}p_j^{w_p}q_j^{w_q}r_j^{w_r}...}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 5.935em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 8.394em; top: -2.136em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.398em;\"><span class=\"mjx-sup\" style=\"font-size: 83.3%; padding-bottom: 0.216em; padding-left: 0px; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">w</span></span></span><span class=\"mjx-sub\" style=\"vertical-align: -0.15em; padding-right: 0.05em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span></span></span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.014em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.398em;\"><span class=\"mjx-sup\" style=\"font-size: 83.3%; padding-bottom: 0.216em; padding-left: 0.082em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">w</span></span></span><span class=\"mjx-sub\" style=\"vertical-align: -0.15em; padding-right: 0.05em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span></span></span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.398em;\"><span class=\"mjx-sup\" style=\"font-size: 83.3%; padding-bottom: 0.216em; padding-left: 0px; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">w</span></span></span><span class=\"mjx-sub\" style=\"vertical-align: -0.15em; padding-right: 0.05em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span></span></span></span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 8.394em; bottom: -1.591em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-munderover\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size1-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">\u2211</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.333em;\"><span class=\"mjx-sup\" style=\"font-size: 83.3%; padding-bottom: 0.216em; padding-left: 0px; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.398em;\"><span class=\"mjx-sup\" style=\"font-size: 83.3%; padding-bottom: 0.216em; padding-left: 0px; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">w</span></span></span><span class=\"mjx-sub\" style=\"vertical-align: -0.15em; padding-right: 0.05em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span></span></span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.014em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.398em;\"><span class=\"mjx-sup\" style=\"font-size: 83.3%; padding-bottom: 0.216em; padding-left: 0.082em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">w</span></span></span><span class=\"mjx-sub\" style=\"vertical-align: -0.15em; padding-right: 0.05em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span></span></span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.398em;\"><span class=\"mjx-sup\" style=\"font-size: 83.3%; padding-bottom: 0.216em; padding-left: 0px; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">w</span></span></span><span class=\"mjx-sub\" style=\"vertical-align: -0.15em; padding-right: 0.05em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span></span></span></span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 5.935em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.635em; vertical-align: -1.125em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<strong>(General Upco)</strong></p><p>Or in terms of odds:<br>&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\frac{P^+(A_i)}{P^+(A_j)}=(\\frac{p_i}{p_j})^{w_p}(\\frac{q_i}{q_j})^{w_q}(\\frac{r_i}{r_j})^{w_r}...\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 2.572em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 3.637em; top: -1.801em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.435em; padding-left: 0.23em; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 3.637em; bottom: -1.154em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.347em; padding-left: 0.23em; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 2.572em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.09em; vertical-align: -0.816em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.769em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.088em; top: -1.372em;\"><span class=\"mjx-msubsup\" style=\"\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.088em; bottom: -0.897em;\"><span class=\"mjx-msubsup\" style=\"\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 0.769em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.604em; vertical-align: -0.634em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">w</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.18em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.729em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.031em; top: -1.372em;\"><span class=\"mjx-msubsup\" style=\"\"><span class=\"mjx-base\" style=\"margin-right: -0.014em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.031em; bottom: -0.897em;\"><span class=\"mjx-msubsup\" style=\"\"><span class=\"mjx-base\" style=\"margin-right: -0.014em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 0.729em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.604em; vertical-align: -0.634em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">w</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.18em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.733em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.036em; top: -1.372em;\"><span class=\"mjx-msubsup\" style=\"\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.036em; bottom: -0.897em;\"><span class=\"mjx-msubsup\" style=\"\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 0.733em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.604em; vertical-align: -0.634em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">w</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.18em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span></span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span></span></span></span>&nbsp; &nbsp; &nbsp; &nbsp;<br>&nbsp;</p><p><br>&nbsp;As the EaGlHiVe notes, we need to give ourselves a weight of 1, or otherwise, Upco implies that we ought to update on our own credence. &nbsp;Then, if we regard someone as more reliable than us, we give them a weight larger than 1 and if we regard them as less reliable we give them a weight between 0 and 1.&nbsp;<br><br>In the case of the climate science professor, one might assign her a weight of 2, which is equivalent to saying that her credence provides twice as much evidence as someone with the same credence but with weight = 1. If she then reports a 20% credence in \u201c2\u00b0C global warming by 2030\u201d, we update on her credence as though she had reported the odds&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"(\\frac{20}{80})^2=\\frac{1}{16}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.849em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.2em; top: -1.394em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">20</span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.2em; bottom: -0.687em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">80</span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 0.849em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.472em; vertical-align: -0.486em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.849em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.2em; top: -1.372em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.2em; bottom: -0.687em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">16</span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 0.849em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.456em; vertical-align: -0.486em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyw74yt3epsc\"><sup><a href=\"#fnyw74yt3epsc\">[6]</a></sup></span></p><p>This more general version of Upco is useful in other ways. For example, we can use it to discount the credences of two peers, Q and R, if they have come to their credences in A on the basis of evidence sets&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"E_Q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.026em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.229em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"E_R\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.026em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span></span></span></span></span></span></span></span>&nbsp;that are probabilistically <i>de</i>pendent conditional on A. For example, if we were certain that one peer R has seen the exact same series of coinflips as another Q, we might not want to update on R\u2019s credence at all after seeing Q\u2019s. We can then assign R an exponent of 0.&nbsp;</p><p>A general account of how we ought to assign these weights seems to await further research.</p><h3>Properties of Upco<br>&nbsp;</h3><p>To sum up, here are the most important properties of Upco:</p><ul><li>Using Upco mimics Bayesian updating for an agent P and some proposition A if P has a likelihood function for another agent Q that is proportional to Q\u2019s credence in A. This is sufficient but perhaps not necessary for Upco to mimic Bayesian updating.</li><li>Upco is commutative in two ways.<ul><li>It is commutative with Bayesian updating itself. That is, it does not matter for an agent's subsequent credence whether they first update on the credences of some other agent(s)<i>&nbsp;</i>using Upco, and&nbsp;<i>then&nbsp;</i>on some other evidence&nbsp;<i>E</i> in&nbsp;the normal Bayesian way, or vice versa; Upco is simply not sensitive to the order.</li><li>Upco is also commutative with respect to the credences of others, such that Upco will require an agent to have the same posterior credence regardless of the order in which the agent updated on other credences.</li></ul></li><li>Independence preservation (in the case of an arbitrary partition).<ul><li>Suppose we have two partitions&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"{A_1,...,&nbsp;A_k}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"{B_1,...,&nbsp;B_r}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span></span></span></span></span></span></span></span></span></span>. Further, suppose that<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P(A_i&nbsp;&nbsp;B_j)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"=\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span></span></span></span></span></span><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P(A_i)P(B_j)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q(A_i&nbsp;B_j) = Q(A_i)Q(B_j)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>. Then&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P^+(A_i&nbsp;&nbsp;B_j)=P^+(A_i)P^+(B_j)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;after updating on Q's credences.</li></ul></li><li>Upco is synergistic.<ul><li>That is, suppose&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P(A) = p\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q(A) = q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span></span></span></span></span>. Then it is not necessarily the case that&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P^+(A),&nbsp;Q^+(A)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;are in the interval&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"[p,q]\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span></span></span></span></span>. At first sight, this might seem undesirable, but suppose this was not the case. This would imply that the credence of someone less confident than you can&nbsp;<i>never&nbsp;</i>be positive evidence regarding the proposition at hand. Suppose you are 95% sure that A is true. Now, for any credence smaller than 95% you would have to update downwards or not update at all. Even if someone were perfectly rational, has a 94.9% credence in A and has evidence completely independent from yours, &nbsp;you would have to regard their credence as either no evidence or disconfirming evidence.&nbsp;</li></ul></li><li>Less desirably, Upco is partition-dependent: the strength and direction of the evidence that someone's credence in A provides depends&nbsp;<i>heavily&nbsp;</i>on the assumed size of the partition that A is part of.</li><li>In general, if a partition consists of k propositions, someone's credence provides evidence in favour of some proposition when it exceeds 1/k.&nbsp;</li><li>If one peer\u2019s credence is uniformly distributed across proposition B and its alternatives, then they adopt the credence of another person in response to learning it.</li><li>When a set of peers all update on each other's credences using Upco, then they will all have the same posterior credence,&nbsp;<i>regardless of their priors.</i><br>&nbsp;</li></ul><h2>Concluding remarks</h2><p><i>(Some more subjective, half-baked takes from me personally. Caveat: I'm not a philosopher.)</i><br><br>I thought this paper was extremely insightful and well-written.</p><p>That said, one of my main critiques of the paper concerns their notion of \"epistemic peers\". The authors use this term quite differently than most people in the literature, where epistemic peerhood tends to be tied to parity of epistemic virtues or equal familiarity with the relevant evidence. The EaGlHiVe instead defines epistemic peers as anyone who you think has formed their credence in the proposition in a reasonable way, based on evidence and reasoning that probably differs from your own. They thus <i>seem</i> to engage with this literature when it's actually unclear to me if they do (because it's unclear to what extent their claims also apply to the standard concept). I'm not sure why they didn't use a different concept to avoid such confusion. Also, they aren\u2019t very clear on what they mean by \"epistemic peers\". As a result, I'm still unsure precisely when we are supposed to apply Upco from the author's perspective. This is because they recommend the use of Upco whenever one encounters the credences of epistemic peers and this is a vaguely defined concept. Another way to put the question is: precisely when is it rational to have the kinds of likelihood functions such that Upco yields the same updates as a Bayesian agent with these functions?&nbsp;</p><p>Upco is entailed by a wide set of likelihood functions. I'm therefore somewhat optimistic that using Upco at least closely <i>approximates </i>an ideal bayesian agent in many cases, even if it doesn't lead to the exact same updates.</p><p>A prominent alternative to Upco is the Linear Average: just set your posterior credence to the (weighted) average of your credence and those of others (e.g. your epistemic peers and superiors). In terms of theoretically desirable properties, Upco seems clearly better to me than taking the weighted average of your peers' credences (see<a href=\"https://forum.effectivealtruism.org/posts/9J3o3cjGSHbHZoWQN/how-to-sometimes-update-on-the-credences-of-others#Appendix_A__The_Linear_Average_as_an_updating_rule_\"> appendix A</a>). This makes me hopeful that using Upco, even in its unextended form without weights, is an improvement over using the Linear Average in most cases.<br><br>Ultimately, I\u2019m sceptical that these theoretical properties matter <i>intrinsically</i>. I tend to think that the best updating rule is the one that leads to the best (expected) \u201cepistemic outcomes\u201d, e.g. the highest expected improvement in belief accuracy. On this view, these theoretical properties matter only in so far as they improve such outcomes. &nbsp;From what I understand, Greaves and Wallace (2006) argue that Bayesian Updating is the unique updating rule to maximise some measure of good epistemic outcomes. (Caveat: I have only skimmed that paper). If that's correct and if Upco better approximates Bayesian Updating than the Linear Average, then I would expect Upco to lead to better epistemic outcomes. Still, this feels quite speculative and I would like to test this empirically, but I'm still unsure how to best do this.&nbsp;<br>&nbsp;</p><h3>Some of my open questions about Upco</h3><ol><li>How large is the set of likelihood functions that entail Upco? Can we characterise this set of functions more precisely?&nbsp;</li><li>How strong is the \u201cintuitive argument\u201d I gave for Upco at the beginning of the Upco section?<ol><li>Is it reasonable to assume that every peer of mine has a uniform ur-prior?</li></ol></li><li>Do people that I should regard as peers ever have credence 0 or 1 in a proposition?&nbsp;<br>If so, such cases seem to favour LA over Upco, since Upco is then undefined and LA is (almost always) not.&nbsp;</li><li>How do we best account for the fact that peers share evidence and that their credences are thus not conditionally independent? If using weights other than 1 as exponents in the Upco formula is the right approach, how should we choose these weights?&nbsp;</li><li>What are the most promising approaches to deal with the problem of partition dependence? The result of applying Upco heavily depends on the assumed partition over which someone reports their credence and often there is (I think) not a natural or obviously best choice.</li><li>How could we empirically test and compare the performance of Upco against other rules?&nbsp;</li><li>Upco is curiously similar to the aggregation method that Jaime Sevilla advocated on the forum (<a href=\"https://forum.effectivealtruism.org/posts/sMjcjnnpoAQCcedL2/when-pooling-forecasts-use-the-geometric-mean-of-odds\">the geometric mean of odds</a>) - why is that?</li></ol><p>&nbsp;</p><p>If you have any thoughts on these questions, please share them in the comments or via direct message!&nbsp;<br>&nbsp;</p><h2>Acknowledgements</h2><p><br>Big thanks to Lorenzo Pacchiardi, Amber Dawn and Kaarel H\u00e4nni for the discussion and their insightful comments.&nbsp;I'm also grateful to Kenny Easwaran for patiently answering many of my questions and providing extensive feedback. Most of all, I want to thank Sylvester Kollin for his tremendous help with understanding the paper and co-writing parts of this post.&nbsp;<br>&nbsp;</p><h2>Appendix A: The Linear Average as an updating rule<br>&nbsp;</h2><p>Suppose you regard Q as your&nbsp;<i>epistemic peer. </i>The EaGlHiVe defines an \"epistemic peer\" as anyone who you think has formed their credence in the proposition in a reasonable way, based on evidence and reasoning that probably differs from your own.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyupi09tnz7e\"><sup><a href=\"#fnyupi09tnz7e\">[7]</a></sup></span>&nbsp;<br><br>A simple rule for updating on the credence of Q is then to take the&nbsp;<i>Linear Average&nbsp;</i>(LA) of both your credences and let this be your posterior credence. In the simplest case, every credence receives the same weight:<br><br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P^+(A) =&nbsp;\\frac{p + q}{2}.\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 1.372em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.941em; top: -1.483em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.941em; bottom: -0.665em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 1.372em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.519em; vertical-align: -0.47em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span></span></span></span><br><br>Or in the case of&nbsp;<i>n&nbsp;</i>peers (with respective credences&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p, q, r...\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span></span></span></span>):&nbsp;<br><br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P^+(A) =&nbsp;\\frac{p + q + r ...}{n}.\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 2.831em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 4.004em; top: -1.483em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 4.004em; bottom: -0.524em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 2.831em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.419em; vertical-align: -0.37em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span></span></span></span><br>&nbsp;</p><p>We can generalize LA to cases where we want to update on the credences of people with different levels of epistemic reliability - e.g. different degrees of expertise or access to evidence in the relevant domain - by instead taking the&nbsp;<i>weighted</i> sum of the credences of others, where higher weights may correspond to higher degrees of reliability. This proposal has been made by many people in some form, both in the <a href=\"https://joelvelasco.net/teaching/3865/jehlefitelson09-equalweight.pdf\">academic</a> <a href=\"https://www.jstor.org/stable/2245510?seq=6#metadata_info_tab_contents\">literature</a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/WKPd79PESRGZHQ5GY/in-defence-of-epistemic-modesty\"><u>on the EA Forum</u></a>.&nbsp;&nbsp;<br><br>Unfortunately, the LA has some undesirable properties (see section 4 of the paper):</p><ul><li>Applied in the way sketched above, LA is non-commutative, meaning that LA is sensitive to the order in which you update on the credences of others, and it seems like this should be completely irrelevant to your subsequent beliefs.&nbsp;<ul><li>This can be avoided by taking the \u201ccumulative average\u201d of the credences of the people you update on, i.e. each time you learn someone's credence in A you average again over all the credences you have ever learned regarding this proposition. However, now the LA has lost its initial appeal; for each proposition you have some credence in, rationality seems to require you to keep track of everyone you have updated on and the weights you assigned to them. This seems clearly intractable once the number of propositions and learned credences grows large.&nbsp;</li><li>See&nbsp;<a href=\"https://www.cambridge.org/core/journals/episteme/article/abs/commutativity-of-evidence-a-problem-for-conciliatory-views-of-peer-disagreement/660AA593D8EF336A8CBD350608DA2677\"><u>Gardiner (2013)</u></a> for more on this.</li></ul></li><li>Relatedly, LA is also sensitive to whether you update on multiple peers at once or sequentially.</li><li>Also, LA does not commute with Bayesian Updating. There are cases where it matters whether you&nbsp;<i>first</i> update on someone's credence (e.g. regarding the bias of a coin) using the LA and&nbsp;<i>then&nbsp;</i>on \u201cnon-psychological\u201d evidence (e.g. the outcome of a coin-flip you observed) using Bayesian Updating or the reverse.&nbsp;</li><li>Moreover, LA does&nbsp;<i>not</i> preserve \u2018judgments of independence\u2019. That is, if two peers judge two propositions A and B to be independent, i.e.&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P(AB)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"= P(A)P(B)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q(A B) = Q(A)Q(B)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>, then after updating on each other's credences, independence is not always preserved. This seems intuitively undesirable: if you think that the outcome of (say) a coin flip and a die roll are independent and I think the same - why should updating on my credences change your mind about that?</li><li>LA does <i>not </i>exhibit what the authors call \u201csynergy\u201d. That is, suppose&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P(A) = p\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q(A) = q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span></span></span></span></span>. Then it is necessarily the case that&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P^+(A)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q^+(A)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;are in the interval&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"[p,q]\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span></span></span></span></span>&nbsp;if they are both applying LA. In other words, using the LA never allows you to update beyond the credence of the most confident person you\u2019ve updated on (or yourself if you are more confident than everybody else).&nbsp;<br><br><ul><li>At first sight, this might seem like a feature rather than a bug. However, this means that the credence of someone less confident than you can&nbsp;<i>never&nbsp;</i>be positive evidence regarding the issue at hand. Suppose you are 95% sure that A is true. Now, for any credence smaller than 95% LA would demand that you update downwards. Even if someone is perfectly rational, has a 94.9% credence in A and has evidence completely independent from yours, LA tells you that their credence is disconfirming evidence.&nbsp;</li></ul></li></ul><p><br>Perhaps most importantly, since Bayesian Updating does&nbsp;<i>not&nbsp;</i>have these properties, LA does&nbsp;<i>not&nbsp;</i>generally produce the same results. Thus, insofar as we regard Bayesian updating as the normative ideal, we should expect LA to be at best an imperfect heuristic and perhaps not even that.&nbsp;</p><p>In sum, LA has a whole host of undesirable properties. It seems like we therefore would want an alternative rule that avoids these pitfalls while retaining the simplicity of LA.&nbsp;&nbsp;&nbsp;</p><p>The EaGlHiVe aims to show that such a rule exists. They call this rule \u201cUpco\u201d, standing for \u201cupdating on the credences of others\u201d. Upco is a simple rule that avoids many of the problems of LA: preservation of independence, commutativity, synergy, etc. Moreover, Upco produces the same results as Bayesian Updating under some conditions.</p><p>&nbsp;</p><h2>Appendix B: A formal justification for Upco<br>&nbsp;</h2><p>One way of formally justifying Upco is by making two assumptions about the likelihood of someone reporting a given credence. <strong>The assumptions are quite technical and probably only understandable for someone with some prior knowledge of probability theory. I think you can safely skip them and still get something out of this post.</strong> The assumptions are:<br>&nbsp;</p><ol><li>the density function for the likelihood of Q reporting a given credence level q in A&nbsp;<i>given that A is true</i>, i.e.&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f_A(Q(A) = q| A)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.06em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>, is&nbsp;<i>proportional&nbsp;</i>to&nbsp;q itself; i.e.<br><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f_A(Q(A) = q| A)= c h(q) q,\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.06em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">c</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span></span></span></span></span></span>&nbsp;where&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"c\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">c</span></span></span></span></span></span></span>&nbsp;is a normalizing constant and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"h(q)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;is some integrable, strictly positive function on the interval&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"[0,1]\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span></span></span></span></span>.</li><li>the density function for the likelihood of them reporting a given credence level q in A given that&nbsp;<i>A is false</i>, i.e.&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f_{\\neg A}Q(A) = q|\\neg A)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.06em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>, is proportional to&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1 - q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span></span></span></span></span>; i.e.<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f_{\\neg A}(Q(A) = q|\\neg&nbsp;A)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.06em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"= c h(q) (1-q)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">c</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>, where again&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"c\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">c</span></span></span></span></span></span></span>&nbsp;is a normalizing constant and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"h(q)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;is &nbsp; &nbsp; &nbsp; some integrable, strictly positive function on the interval&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"[0,1]\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span></span></span></span></span>.</li></ol><p>Examples of density functions that would satisfy (1) and (2) are the beta distributions&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f_A(q) = Beta(x, x+1)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.06em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f_{\\neg A}(q) = Beta(x+1, x)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.06em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>, where&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f_A(q)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.06em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;is the density for&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P(Q(A) = q|A)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f_{\\neg A}(q)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.06em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;is the density for&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P(Q(A) = q|\\neg A)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>, for any positive real number x.&nbsp;If both conditions hold, then&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"c\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">c</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"h(q)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;cancel each other out (in the odds formulation of Bayes theorem) so that we\u2019re left with Upco:<br>&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\frac{P^+(A)}{P^+(\\neg A)}=\\frac{f_A(Q(A)=q|A)}{f_{\\neg A}(Q(A)=q|\\neg A)} \\frac{P(A)}{P(\\neg A)}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 2.771em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 3.919em; top: -1.801em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.435em; padding-left: 0.23em; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 3.919em; bottom: -1.02em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.347em; padding-left: 0.23em; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 2.771em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.995em; vertical-align: -0.721em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 5.616em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 7.943em; top: -1.716em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.06em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 7.943em; bottom: -1.009em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.06em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 5.616em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.927em; vertical-align: -0.714em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 2.225em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 3.146em; top: -1.706em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 3.146em; bottom: -0.999em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 2.225em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.913em; vertical-align: -0.707em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(From Bayes Theorem)<br><br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\frac{f_A(Q(A)=q|A)}{f_{\\neg A}(Q(A)=q|\\neg A)}= \\frac {c h(q) q}{c h(q) (1-q)}= \\frac{q}{1-q}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 5.616em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 7.943em; top: -1.716em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.06em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 7.943em; bottom: -1.009em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.06em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 5.616em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.927em; vertical-align: -0.714em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 3.509em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 4.963em; top: -1.706em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">c</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 4.963em; bottom: -0.999em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">c</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 3.509em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.913em; vertical-align: -0.707em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 1.37em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.938em; top: -1.342em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.938em; bottom: -0.859em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 1.37em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.557em; vertical-align: -0.608em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (From 1. and 2.)</p><p>And therefore:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\frac{P^+(A)}{P^+(\\neg A)}= \\frac {q}{1-q}\\frac{P(A)}{P(\\neg A)}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 2.771em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 3.919em; top: -1.801em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.435em; padding-left: 0.23em; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 3.919em; bottom: -1.02em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.347em; padding-left: 0.23em; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 2.771em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.995em; vertical-align: -0.721em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 1.37em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.938em; top: -1.342em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.938em; bottom: -0.859em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 1.37em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.557em; vertical-align: -0.608em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 2.225em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 3.146em; top: -1.706em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 3.146em; bottom: -0.999em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 2.225em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.913em; vertical-align: -0.707em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(Upco)</p><p>&nbsp;</p><p>Or, not in terms of odds:&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P^+(A)=\\frac{f_A(Q(A)=q|A)P(A)}{f_A(Q(A)=q|A)P(A) + f_{\\neg A}(Q(A)=q|\\neg A)P(\\neg A)} = \\frac {c h(q) q p}{c h(q) q p+c h(q) (1-q)(1-p)} = \\frac{p q}{p q + (1-p) (1-q)}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.271em; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 14.471em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 20.465em; top: -1.716em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.06em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 20.465em; bottom: -1.009em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.06em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.06em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">\u00ac</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 14.471em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.927em; vertical-align: -0.714em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 8.139em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 11.51em; top: -1.706em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">c</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 11.51em; bottom: -0.999em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">c</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">c</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 8.139em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.913em; vertical-align: -0.707em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 4.961em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 7.016em; top: -1.342em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 7.016em; bottom: -0.999em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 4.961em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.656em; vertical-align: -0.707em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span></p><p><br>Here are two possible pairs of densities for likelihood functions that would satisfy (1) and (2):</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1672400145/mirroredImages/9J3o3cjGSHbHZoWQN/fwruoxqkuvhlujfifalu.png\"><figcaption>Fig 1: these are&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Beta(2,1)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Beta(1,2)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span></figcaption></figure><p>&nbsp;<br>Or,&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1672400145/mirroredImages/9J3o3cjGSHbHZoWQN/rir06ffwuqctlbcxjpkh.png\"><figcaption>&nbsp;Fig 2: these are Beta(2,3) and Beta (3,2)</figcaption></figure><p><br>Also note, that these conditions are&nbsp;<i>sufficient&nbsp;</i>for Upco to give the same results as Bayesian updating, but they are not strictly&nbsp;<i>necessary. </i>There may be other likelihood functions that do not satisfy (1) and (2) but still would yield the same result as Upco. It is also possible that Upco&nbsp;<i>approximates</i> Bayesian updating even if it doesn\u2019t mimic it for an even wider range of likelihood functions. To what extent that\u2019s the case is still unclear to me.&nbsp;</p><p><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnb8d4szvil9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefb8d4szvil9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For the sake of simplicity, we assume here that Q(A) is a discrete variable. This assumption will be relaxed later.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9c6867x1upo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9c6867x1upo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We are figuring out the bias of the coin.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnssacmyoqfwi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefssacmyoqfwi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This follows both from rearranging Bayes Theorem and from viewing the strength of some piece of evidence E regarding proposition H as the ratio P(E|H)/P(E|-H). There\u2019s some discussion of alternative measures like the difference or ratio between posterior and prior credences (<i>not&nbsp;</i>odds).&nbsp;<a href=\"https://web.archive.org/web/20200321171404id_/http://philsci.org/archives/psa1998/papers/fitelson.pdf\"><u>This paper</u></a> seems to provide a good overview of the relative merits of such different measures.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnuavxpt62a7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefuavxpt62a7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Note that UpCo gives undefined results when you are updating on peers with extreme credences (0 or 1). In that case, your odds would be 0 or undefined.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnatz5imwnvlh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefatz5imwnvlh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For simplicity assumed to be part of a binary partition.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyw74yt3epsc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyw74yt3epsc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Strangely, it seems like one is treating her as if she had reported a much lower credence than 0.2. This seems odd because it suggests that I regard her as epistemically superior to me (with respect to this claim) and I also believe her to \u201cunder respond\u201d to the evidence available to her; I think she should be more confident than she is in this claim.</p><p>These two beliefs seem to be in tension: it is a bit like saying that Magnus Carlsen made a mistake with some chess move, whilst&nbsp;simultaneously&nbsp;saying that he is a far superior chess player to you, and thus has a much better idea of what a good move would be.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyupi09tnz7e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyupi09tnz7e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The \u201cdiffers from your own\u201d clause is here to bar against situations in which your credences are based on the&nbsp;<i>exact</i> same evidence, in which case the other agent\u2019s credence may not provide evidence regarding the claim in question. Kenny Easwaran told me that a large part of the literature on peer disagreement assumes that epistemic peers have the same evidence. He says that while their paper, therefore, doesn\u2019t directly engage with this literature, \u201cSynergy\u201d might sometimes be plausible even under this more common conc</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnp3rjogfnvi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefp3rjogfnvi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>As I understand it, epistemic utility functions are functions from the state of the world and one's probability assignments to real numbers. The epistemic utility functions considered in the paper seem to have in common that, other things being equal, the closer one's probability judgement p(A) is to the truth value of A, the higher the epistemic utility.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnmcy7pm9ra\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnmcy7pm9ra\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Note that Greaves and Wallace (2006) argue that Bayesian Updating is the unique updating rule to maximise some measure of good epistemic outcomes. I haven\u2019t read that paper so I won\u2019t comment on it.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnabzzen9eem6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefabzzen9eem6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;This seems particularly clear when you look at the odds formulation of Upco.</p></div></li></ol>", "user": {"username": "Aaron__Maiwald"}}]