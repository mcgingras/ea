[{"_id": "LkHZmJe5GCBrSLMDd", "postedAt": "2022-09-26T21:01:36.879Z", "postId": "G6EXYYNp6KwageGZq", "htmlBody": "<p>I like this framing a lot. My 60 second pitch for AI safety often includes something like this. \u201cIt\u2019s all about making sure AI benefits humanity. We think AI could develop really quickly and shape our society,  and the big corporations building it are thinking more about profits than about safety. We want to do the research they should be doing to make sure this technology helps everyone. It\u2019s like working on online privacy in the 1990s and 2000s: Companies aren\u2019t going to have the incentive to care, so you could make a lot of progress on a neglected problem by bringing early attention to the issue.\u201d</p>\n", "parentCommentId": null, "user": {"username": "Aidan O'Gara"}}, {"_id": "XLZk8THKmEKjhoP9T", "postedAt": "2022-09-26T21:58:00.909Z", "postId": "G6EXYYNp6KwageGZq", "htmlBody": "<p>Without thinking too deeply, I believe that this framing, i.e. one in line with <i>AI developers are gambling with the fate of humanity for the sake of profit, and we need to stop them/ensure that their efforts don't have catastrophic effects,</i> for AI risk could serve as a conversational cushion for those who are unfamiliar with the general state of AI progress and with the existential risk poorly aligned AI poses.&nbsp;</p><p>Those unfamiliar with AI might disregard the extent of risk from AI if approached in conversation with remarks about how not only it is non-trivial that humanity might be extinguished by AI, but many researchers believe this event is highly likely to occur, &nbsp;even in the next 25 years. I imagine such scenarios are, for them, generally unbelievable.&nbsp;</p><p>The cushioning could, however, lead to people trying to think about AI risk independently or to them searching for more evidence and commentary online, which might subsequently lead to them to the conclusion that AI does in fact pose a significant existential risk to humanity.&nbsp;</p><p>When trying to introduce the idea of AI risk to someone who is unfamiliar with it, it's probably a good idea to give an example of a current issue with AI, and then have them extrapolate. The example of poorly designed AI systems being used by corporations for click-through, as covered in the introduction of <i>Human Compatible</i>, seems good to use in your framing of AI safety as a public good. Most people are familiar with the ills of algorithms designed for social media, so it is not a great step to imagine researchers designing more powerful AI systems that are deleterious to humanity via a similar design issue but at a much more lethal level:&nbsp;</p><blockquote><p><i>They aren't particularly intelligent, but they are in a position to affect the entire world because they directly influence billions of people. Typically, such algorithms are designed to maximize click-through, that is, the probability that the user clicks on presented items. The solution is simply to present items that the user likes to click on, right? Wrong. The solution is to change the user's preferences so that they become more predictable. A more predictable user can be fed items that they are likely to click on, thereby generating more revenue. People with more extreme political views tend to be more predictable in which items they click on.&nbsp;</i></p></blockquote>", "parentCommentId": null, "user": {"username": "rodeo_flagellum"}}, {"_id": "4DDf22ii2L9hcKkyc", "postedAt": "2022-09-26T22:14:21.174Z", "postId": "G6EXYYNp6KwageGZq", "htmlBody": "<p>I'm very pro framing this as an externality. Doesn't just help with left-leaning people, it can also be &nbsp;helpful for talking to other audiences, such as those immersed in economics or antitrust/competition law.</p>", "parentCommentId": null, "user": {"username": "HaydnBelfield"}}, {"_id": "BWnbZ9FHDGLSZJHMa", "postedAt": "2022-09-27T03:08:47.755Z", "postId": "G6EXYYNp6KwageGZq", "htmlBody": "<p>Ultimately, you should probably tailor messages to your audience, given their understanding, objections/beliefs, values, etc. If you think they understand the phrase \u201cexternalities,\u201d I agree, but a sizable number of people in the world do not properly understand the concept.</p>\n<p>Overall, I agree that this is probably a good thing to emphasize, but FWIW I think a lot pitches I\u2019ve heard/read do emphasize this insofar as it makes sense to do so, albeit not always with the specific term \u201cexternality.\u201d</p>\n", "parentCommentId": null, "user": {"username": "Harrison D"}}, {"_id": "F9SHcrjJhWavgXkDa", "postedAt": "2022-09-27T15:45:59.320Z", "postId": "G6EXYYNp6KwageGZq", "htmlBody": "<p>It's risky to connect AI safety to one side of an ideological conflict.</p>\n", "parentCommentId": null, "user": {"username": "PeterMcCluskey"}}, {"_id": "3BZNGZLKaxKzzM6gk", "postedAt": "2022-09-27T15:54:08.207Z", "postId": "G6EXYYNp6KwageGZq", "htmlBody": "<p>It is true that private developers internalize some of the costs of AI risk. &nbsp;However, this is also true in the case of carbon emissions; if a company emits CO2, its shareholders do pay some costs in terms of having a more polluted atmosphere. &nbsp;The problem is that the private developer only pays a very small fraction of the total costs which, while still quite large in absolute terms, js plausibly worth paying for the upside. &nbsp;For example, if I were entirely selfish and I thought AI risk was somewhat less likely than I actually do (let's say 10%), I would probably be willing to risk a 10% chance of death for a 90% chance of massive resource acquisition and control over the future. &nbsp;However, if I internalized the full costs of that 10% chance (everyone else dying and all future generations being wiped out), then I would not be willing to take that gamble.</p>", "parentCommentId": "EantrQvXpGheaoJ96", "user": {"username": "NickGabs"}}, {"_id": "gWtFWCbupx3hNzezi", "postedAt": "2022-09-28T06:06:22.786Z", "postId": "G6EXYYNp6KwageGZq", "htmlBody": "<p>There are ways to frame AI safety as (partly) an externality problem without getting mired in a broader ideological conflict.</p>", "parentCommentId": "F9SHcrjJhWavgXkDa", "user": {"username": "jakubkraus07@gmail.com"}}, {"_id": "o4WwDmcmyXv3wMDoT", "postedAt": "2022-09-28T06:39:09.569Z", "postId": "G6EXYYNp6KwageGZq", "htmlBody": "<ul><li>AI capabilities people don't psychologically feel like AI is a threat to their selfish interests (assuming they even understand why it is a threat), because humans value short-term gain more than long-term danger (<a href=\"https://en.wikipedia.org/wiki/Time_preference\">time discounting</a>). Therefore selfish actors have incentives to work on capabilities.</li><li>Great point that externalities might mislead people into thinking \"ah yes, another instance where we need government regulation to stop greedy companies; government regulation will solve the problem.\" (Although government intervention for slowing down capabilities and amplifying safety research would indeed be quite helpful.)</li><li>Not sure how externalities \"dilutes\" the claims. It's a serious problem that there are huge economic incentives for capabilities, and minuscule economic incentives for safety.</li><li>I don't think it's very hard to make AI x-risk sound non-weird: (1) intelligence is a meaningful concept; (2) it might be possible to build AI systems with more intelligence than humans; (3) it might be possible to build such AIs within this century; (4) if built, these AIs would have a massive impact on society; (5) this impact might be extremely negative. <a href=\"https://intelligence.org/2015/07/24/four-background-claims/\">These core ideas</a> are reasonable-sounding propositions that someone with good social skills could bring up in a conversation.</li></ul>", "parentCommentId": "EantrQvXpGheaoJ96", "user": {"username": "jakubkraus07@gmail.com"}}, {"_id": "LKgr7f7PgKXapMcfj", "postedAt": "2022-09-28T14:29:00.405Z", "postId": "G6EXYYNp6KwageGZq", "htmlBody": "<p>I think you can stress the \"ideological\" implications of externalities to lefty audiences while having a more neutral tone with more centrist or conservative audiences. &nbsp;The idea that externalities exist and require intervention is not IMO super ideologically charged.</p>", "parentCommentId": "F9SHcrjJhWavgXkDa", "user": {"username": "NickGabs"}}]