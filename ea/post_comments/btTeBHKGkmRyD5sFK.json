[{"_id": "YeYDJmCdQwzFCFq7D", "postedAt": "2023-11-19T18:44:50.154Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<blockquote><p><strong>one values humans 10-100x as much</strong></p></blockquote><p>&nbsp;</p><p>This seems quite low, at least from a perspective of revelead preferences. If one indeed rejects unitarism, I suspect that the actual willingness to pay is something like 1000x - 10,000x to prevent the death of an animal vs. a human. &nbsp;</p>", "parentCommentId": null, "user": {"username": "Halffull"}}, {"_id": "mJKoMxiaPadLemCqq", "postedAt": "2023-11-19T19:07:21.370Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Strong upvoted. I think this is correct, important and well-argued, and I welcome the call to OP to clarify their views.&nbsp;</p><p>This post is directed at OP, but this conclusion should be noted by the EA community as a whole which still <a href=\"https://forum.effectivealtruism.org/posts/83tEL2sHDTiWR6nwo/ea-survey-2020-cause-prioritization\">prioritises global poverty</a> over all else.</p><p>The only caveat I would raise is that we need to retain <i>some</i> focus on global poverty in EA for various instrumental reasons: it can attract more people into the movement, allows us to show concrete wins etc. &nbsp;</p>", "parentCommentId": null, "user": {"username": "jackmalde"}}, {"_id": "EiayxnzzyXssfJobJ", "postedAt": "2023-11-19T19:19:07.973Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Strongly, strongly, strongly agree. I was in the process of writing essentially this exact post, but am very glad someone else got to it first. The more I thought about it and researched, the more it seemed like convincingly making this case would probably be the most important thing I would ever have done. Kudos to you.</p><h3>A few points to add</h3><ol><li>Under standard EA \"on the margin\" reasoning, this shouldn't really matter, but I <a href=\"https://docs.google.com/spreadsheets/d/1VoIKr4k4Mb32LgELLMT103aSkJzC4C8W4EozKOdfWpE/edit?usp=sharing\">analyzed OP's grants data</a> and found that human GCR has been consistently funded 6-7x more than animal welfare (<a href=\"https://twitter.com/AaronBergman18/status/1717740950020100384\">here's</a> my tweet thread this is from) <img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EiayxnzzyXssfJobJ/l79oe4r4i8u8wo0gfopj\" alt=\"Image\"></li><li><a href=\"https://forum.effectivealtruism.org/users/laura-1?mention=user\">@Laura Duffy</a>'s (for Rethink Priorities)<a href=\"https://forum.effectivealtruism.org/posts/9EENSGhiQiKFaRh4t/how-can-risk-aversion-affect-your-cause-prioritization\"> recently published risk aversion analysis</a> basically does a lot of the heavy lifting here (bolding mine):</li></ol><blockquote><p>Spending on corporate cage-free campaigns for egg-laying hens is robustly<a href=\"https://forum.effectivealtruism.org/posts/9EENSGhiQiKFaRh4t/how-can-risk-aversion-affect-your-cause-prioritization#fn5rpwx7c8ks\"><sup>[8]</sup></a>&nbsp;cost-effective under nearly all reasonable types and levels of risk aversion considered here.&nbsp;</p><ol><li>Using welfare ranges based roughly on Rethink Priorities\u2019 results, <strong>spending on corporate cage-free campaigns averts over an order of magnitude more suffering than the most robust global health and development intervention, Against Malaria Foundation. This result holds for almost any level of risk aversion and under any model of risk aversion.</strong></li></ol></blockquote><p>I also want to emphasize this part, because it's the kind of serious engagement with suffering that EA still fails to to do enough of&nbsp;</p><blockquote><p><strong>I experienced \"disabling\"-level pain for a couple of hours, by choice and with the freedom to stop whenever I want. This was a horrible experience that made everything else seem to not matter at all.</strong></p><p>A single laying hen experiences hundreds of hours of this level of pain during their lifespan, which lasts perhaps a year and a half - and there are as many laying hens alive at any one time as there are humans.&nbsp;<strong>How would I feel if every single human were experiencing hundreds of hours of disabling pain?&nbsp;</strong></p><p>A single broiler chicken experiences fifty hours of this level of pain during their lifespan, which lasts 4-6 weeks. There are 69 billion broilers slaughtered each year.&nbsp;<strong>That is so many hours of pain that if you divided those hours among humanity, each human would experience about 400 hours (2.5 weeks) of disabling pain every year.</strong> Can you imagine if instead of getting, say, your regular fortnight vacation from work or study, you experienced disabling-level pain for a whole 2.5 weeks? And if every human on the planet - me, you, my friends and family and colleagues and the people living in every single country - had that same experience every year? How hard would I work in order to avert suffering that urgent?</p><p><strong>Every single one of those chickens are experiencing pain as awful and all-consuming as I did for tens or hundreds of hours, without choice or the freedom to stop.</strong> They are also experiencing often minutes of 'excruciating'-level pain, which is an intensity that I literally cannot imagine. Billions upon billions of animals. The numbers would be even more immense if you consider farmed fish, or farmed shrimp, or farmed insects, or wild animals.</p><p>If there were a political regime or law responsible for this level of pain - which indeed there is - how hard would I work to overturn it?&nbsp;<strong>Surely that would tower well above my other priorities (equality, democracy, freedom, self-expression, and so on), which seem trivial and even borderline ridiculous in comparison.</strong></p></blockquote>", "parentCommentId": null, "user": {"username": "aaronb50"}}, {"_id": "G2BQHwvR69iTFg7wj", "postedAt": "2023-11-19T19:28:05.803Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>I think that's basically right, but also rejecting unitarianism and discounting other animals through this seems to me like saying the interests of some humans matter less in themselves (ignoring instrumental reasons) just because of their race, gender or intelligence, which is very objectionable.</p>\n<p>People discount other animals because they're speciesist in this way, although also for instrumental reasons.</p>\n", "parentCommentId": "YeYDJmCdQwzFCFq7D", "user": {"username": "MichaelStJules"}}, {"_id": "ydNRrRxFvqXD9nHSc", "postedAt": "2023-11-19T20:21:12.505Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>I think this post is on the right track, the request for reasoning transparency especially so.&nbsp;</p><p>I personally worry about how <i>weird</i> effective altruism will seem to the outside world if we focus exclusively on topics that most people don't think are very important. A <a href=\"https://forum.effectivealtruism.org/posts/btTeBHKGkmRyD5sFK/open-phil-should-allocate-most-neartermist-funding-to-animal?commentId=YeYDJmCdQwzFCFq7D\">sister comment</a> argues that the average person's revealed preference about the value of a hen's life relative to a human's is infinitesimal. Likewise, however much people say they worry about AI (as a proxy for longtermism, which isn't really on people's radar in general), in practice, it tends to be <a href=\"https://rethinkpriorities.org/publications/us-public-opinion-of-ai-policy-and-risk\">relatively low on their list of concerns, even among potential existential threats</a>.</p><p>If our thinking takes us in weird directions, that's not inherently a reason to shy away. But I think there's something to be said for considering the &nbsp;implications of having increasingly niche opinions, priorities, and epistemology. A movement that's a little more humble/agnostic about what the most important cause is might broadly be able to devote more resources, on net, to a wider range of causes, including the ones we think most important.</p><p>(For context I am a vegan who believes that animal welfare is broadly neglected -- I recently wrote something on the case for <a href=\"https://forum.effectivealtruism.org/posts/zihL7a4xbTnCmuL2L/towards-non-meat-diets-for-domesticated-dogs\">veganism for domesticated dogs</a>.)</p>", "parentCommentId": null, "user": {"username": "setgree"}}, {"_id": "uvJc9X5Zmc5LLYHis", "postedAt": "2023-11-19T20:36:57.869Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<blockquote><p><strong>If OP disagrees, they should practice&nbsp;</strong><a href=\"https://www.openphilanthropy.org/research/reasoning-transparency/\"><strong><u>reasoning transparency</u></strong></a><strong> by clarifying their views</strong></p></blockquote><p>&nbsp;</p><blockquote><p><strong>OP believes in&nbsp;</strong><a href=\"https://www.openphilanthropy.org/research/reasoning-transparency/\"><strong><u>reasoning transparency</u></strong></a><strong>, but their reasoning has not been transparent</strong></p></blockquote><p>Regardless of what Open Phil ends up doing, would really appreciate them to at least do this :)</p>", "parentCommentId": null, "user": {"username": "jelle-donders"}}, {"_id": "YJzgtPqS3AGg7Bwij", "postedAt": "2023-11-19T20:45:04.574Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>But isn't the relevant harm here animal <i>suffering</i> rather than animal <i>death</i>? &nbsp;It would seem pretty awful to prefer that an animal suffer torturous agony rather than a human suffer a mild (1000x less bad) papercut.</p>", "parentCommentId": "YeYDJmCdQwzFCFq7D", "user": {"username": "RYC"}}, {"_id": "PCQp69eKvNZ5jMxL2", "postedAt": "2023-11-19T20:58:31.395Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Yeah, I think this caveat is important.</p><p>At the same time, GiveWell will continue to work on global poverty regardless of what OP does, right?</p>", "parentCommentId": "mJKoMxiaPadLemCqq", "user": {"username": "Benny Smith"}}, {"_id": "ZYnDzLmnqzDy6yuxo", "postedAt": "2023-11-19T21:35:06.640Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>For those who agree with this post (I at least agree with the author's claim if you replace most with more), I encourage you to think what you personally can do about it.</p><p>I think EAs are far too willing to donate to traditional global health charities, not due to them being the most impactful, but because they feel the best to donate to. When I give to AMF I know I'm a good person who had an impact! But this logic is exactly what EA was founded to avoid.</p><p>I can't speak for animal welfare organizations outside of EA, but at least for the ones that have come out of Effective Altruism, they tell me that <strong>funding is a major issue</strong>. There just aren't that many people willing to make a risky donation a new charity working on fish welfare, for example.</p><p>Those who would be risk-willing enough to give to eccentric animal welfare or global health interventions, tend to also be risk-willing enough with their donations to instead give it to orgs working on existential risks. I'm not claiming this is incorrect of them to do, but this does mean that there is a dearth of funding for high-risk interventions in the neartermist space.</p><p>I donated a significant part of my personal runway to help fund a new animal welfare org, which I think counterfactually might not have gotten started if not for this. If you, like me, think animal welfare is incredibly important and previously have donated to Givewell's top charities, perhaps consider giving animal welfare a try!</p>", "parentCommentId": null, "user": {"username": "MathiasKirkBonde"}}, {"_id": "QACxKtiECyCfeRJ2h", "postedAt": "2023-11-19T21:43:41.299Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>GiveWell seems pretty dependent on OP funding, s.t. it might have to change its work with significantly less OP money.<br><a href=\"https://forum.effectivealtruism.org/posts/zL6kf9DMGFsoAokEy/an-update-on-givewell-s-funding-projections\">An update on GiveWell\u2019s funding projections \u2014 EA Forum (effectivealtruism.org)</a><br><a href=\"https://forum.effectivealtruism.org/posts/aLNEXzr6buXR8L6hB/open-philanthropy-s-2023-2025-funding-of-usd300-million\">Open Philanthropy's 2023-2025 funding of $300 million total for GiveWell's recommendations \u2014 EA Forum (effectivealtruism.org)</a></p>", "parentCommentId": "PCQp69eKvNZ5jMxL2", "user": {"username": "Gabe Mukobi"}}, {"_id": "4be4eDxueubsJecn2", "postedAt": "2023-11-20T00:26:00.147Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>I\u2019m a bit confused by this. Presumably GiveWell doesn\u2019t need that much money to function. Less Open Phil money probably won\u2019t affect GiveWell, instead it will affect GiveWell\u2019s recommended charities which will of course still receive money from other sources, in part due to GiveWell\u2019s recommendation.</p>\n", "parentCommentId": "QACxKtiECyCfeRJ2h", "user": {"username": "jackmalde"}}, {"_id": "3uBrznQWEqs83PAfh", "postedAt": "2023-11-20T01:32:12.410Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>(Hi, I'm Emily, I lead GHW grantmaking at Open Phil.)</p><p>Thank you for writing this critique, and giving us the chance to read your draft and respond ahead of time. This type of feedback is very valuable for us, and I\u2019m really glad you wrote it.</p><p>We agree that we haven\u2019t shared much information about our thinking on this question. I\u2019ll try to give some more context below, though I also want to be upfront that we have a lot more work to do in this area.</p><p>For the rest of this comment, I\u2019ll use \u201cFAW\u201d to refer to farm animal welfare and \u201cGHW\u201d to refer to all the other (human-centered) work in our Global Health and Wellbeing portfolio.&nbsp;</p><p>To date, we haven\u2019t focused on making direct comparisons between GHW and FAW. Instead, we\u2019ve focused on trying to equalize marginal returns within each area and do something more like&nbsp;<a href=\"https://www.openphilanthropy.org/research/worldview-diversification/\"><u>worldview diversification</u></a> to determine allocations across GHW, FAW, and Open Philanthropy\u2019s other grantmaking. In other words, each of GHW and FAW has its own rough \u201cbar\u201d that an opportunity must clear to be funded. While our frameworks allow for direct comparisons, we have not stress-tested consistency for that use case. We\u2019re also unsure conceptually whether we should be trying to equalize marginal returns between FAW and GHW or whether we should continue with our current approach. We\u2019re planning to think more about this question next year.&nbsp;</p><p>One reason why we are moving more slowly is that our current estimates of the gap between marginal animal and human funding opportunities is very different from the one in your post \u2013 within one order of magnitude, not three. And given the high uncertainty around our estimates here, we think one order of magnitude is well within the \u201cmargin of error\u201d .</p><p>Comparing animal- and human-centered interventions involves many hard-to-estimate parameters. We think the most important ones are:</p><ol><li>Moral weights</li><li>Welfare range (i.e. should we treat welfare as symmetrical around a neutral point, or negative experiences as being worse than positive experiences are good?)</li><li>The difference between the number of humans and chickens, respectively, affected by a marginal intervention in each area&nbsp;</li></ol><p>There is not a lot of existing research on these three points. While we are excited to support work from places like Rethink Priorities, this is a very nascent field and we think there is still a lot to learn.</p><p>To ground your 1,000x claim, our understanding is it implies a tradeoff of 0.85 chicken years moving from pre-reform to post-reform farming conditions vs one year of human life.&nbsp;</p><p>A few more details on our estimate and where we differ:</p><ul><li>We think that the marginal FAW funding opportunity is ~1/5th as cost-effective as the average from Saulius\u2019 analysis.<ul><li>The estimate Holden shared in \u201c<a href=\"https://www.openphilanthropy.org/research/worldview-diversification/\"><u>Worldview Diversification</u></a>\u201d (over 200 hens spared from confinement per dollar spent) came from a quick calculation based on the initial success of several US cage-free campaigns. That was in 2016. Seven years later, we\u2019ve covered many of the strongest opportunities in this space, and we think that current marginal opportunities are considerably weaker.</li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/vBcT7i7AkNJ6u9BcQ/prioritising-animal-welfare-over-global-health-and\"><u>Vasco\u2019s analysis</u></a> implies a much wider welfare range than the one we use.<ul><li>We\u2019re not confident in our current assumptions, but this is a complicated question, and there is more work we need to do ourselves to get to an answer we believe in enough to act on. We also need to think through consistency with our human-focused interventions.&nbsp;</li></ul></li><li>We don\u2019t use Rethink\u2019s moral weights.<ul><li>Our current moral weights, based in part on&nbsp;<a href=\"https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood/\"><u>Luke Muehlhauser\u2019s past work</u></a>, are lower. We may update them in the future; if we do, we\u2019ll consider work from many sources, including the arguments made in this post.</li></ul></li></ul><p>Thanks again for the critique; we wish more people would do this kind of thing!</p><p>Best,<br>Emily</p>", "parentCommentId": null, "user": {"username": "Emily Oehlsen"}}, {"_id": "kK24aHFoL3LenKswA", "postedAt": "2023-11-20T01:54:09.243Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Hi Emily,</p><p>Thanks so much for your engagement and consideration. I appreciate your openness about the need for more work in tackling these difficult questions.</p><blockquote><p>our current estimates of the gap between marginal animal and human funding opportunities is very different from the one in your post \u2013 within one order of magnitude, not three.</p></blockquote><p>Holden has&nbsp;<a href=\"https://www.openphilanthropy.org/research/worldview-diversification/\"><u>stated</u></a> that \"It seems unlikely that the ratio would be in the precise, narrow range needed for these two uses of funds to have similar cost-effectiveness.\" As OP continues researching moral weights, OP's marginal cost-effectiveness estimates for FAW and GHW may eventually differ by several orders of magnitude. If this happens, would OP substantially update their allocations between FAW and GHW?</p><blockquote><p>Our current moral weights, based in part on&nbsp;<a href=\"https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood/\"><u>Luke Muehlhauser\u2019s past work</u></a>, are lower.</p></blockquote><p>Along with OP's neartermist cause prioritization, your comment seems to imply that OP's moral weights are 1-2 orders of magnitude lower than Rethink's. If that's true, that is a massive difference which (depending upon the details) could have big implications for how EA should allocate resources between FAW charities (e.g. chickens vs shrimp) as well as between FAW and GHW.</p><p>Does OP plan to reveal their moral weights and/or their methodology for deriving them? It seems that opening up the conversation would be quite beneficial to OP's objective of furthering moral weight research until uncertainty is reduced enough to act upon.</p><p>I'd like to reiterate how much I appreciate your openness to feedback and your reply's clarification of OP's disagreements with my post. That said, this reply doesn't seem to directly answer this post's headline questions:</p><ul><li>How much weight does OP's theory of welfare place on pleasure and pain, as opposed to nonhedonic goods?</li><li>Precisely how much more does OP value one unit of a human's welfare than one unit of another animal's welfare, just because the former is a human? How does OP derive this tradeoff?</li><li>How would OP's views have to change for OP to prioritize animal welfare in neartermism?</li></ul><p>Though you have no obligation to directly answer these questions, I really wish you would. A transparent discussion could update OP, Rethink, and many others on this deeply important topic.</p><p>Thanks again for taking the time to engage, and for everything you and OP have done to help others :)</p>", "parentCommentId": "3uBrznQWEqs83PAfh", "user": {"username": "Ariel Simnegar"}}, {"_id": "YHwQ8tpTLKFeS3Mmn", "postedAt": "2023-11-20T01:54:54.282Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Also, if we defer to people's revealed preferences, we should dramatically discount the lives and welfare of foreigners. I'd guess that Open Philanthropy, being American-funded, would need to reallocate much or most of its global health and development grantmaking to American-focused work, or to global catastrophic risks.</p>\n<p>EDIT: For those interested, there's some literature on valuing foreign lives, e.g. <a href=\"https://scholar.google.ca/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=%22valuing+foreign+lives%22+OR+%22foreign+life+valuation%22\">https://scholar.google.ca/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=\"valuing+foreign+lives\"+OR+\"foreign+life+valuation\"</a></p>\n", "parentCommentId": "YeYDJmCdQwzFCFq7D", "user": {"username": "MichaelStJules"}}, {"_id": "ovGZ7sHsKXr5pvPRZ", "postedAt": "2023-11-20T02:03:20.701Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<blockquote><p>RP's&nbsp;<a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw/p/Qk3hd6PrFManj8K6o\"><u>moral weights</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/L5EZjjXKdNgcm253H/corporate-campaigns-affect-9-to-120-years-of-chicken-life\"><u>analysis</u></a> of cage-free campaigns suggest that&nbsp;<strong>the average cost-effectiveness of cage-free campaigns is on the order of 1000x that of GiveWell's top charities</strong>.<a href=\"https://forum.effectivealtruism.org/posts/btTeBHKGkmRyD5sFK/open-phil-should-allocate-most-neartermist-funding-to-animal#fna5wqewn0u8p\"><sup>[5]</sup></a>&nbsp;Even if the campaigns' marginal cost-effectiveness is 10x worse than the average, that would be 100x.</p></blockquote><p>This seems to be the key claim of the piece, so why isn't the \"1000x\" calculation actually spelled out?</p><p>The \"cage-free campaigns analysis\" estimates</p><blockquote><p>how many chickens will be affected by corporate cage-free and broiler welfare commitments won by all charities, in all countries, during all the years between 2005 and the end of 2018</p></blockquote><p>This analysis gives chicken years affected per dollar as 9.6-120 (95%CI), with 41 as the median estimate.</p><p>The moral weights analysis estimates \"welfare ranges\", ie, the difference in moral value between the best possible and worst possible experience for a given species. This doesn't actually tell us anything about the disutility of caging chickens. For that you would need to make up some additional numbers:</p><blockquote><p><strong>Welfare ranges allow us to convert species-relative welfare assessments, understood as percentage changes in the portions of animals\u2019 welfare ranges, into a common unit</strong>. To illustrate, let\u2019s make the following assumptions:</p><ol><li>Chickens\u2019 welfare range is 10% of humans\u2019 welfare range.</li><li>Over the course of a year, the average chicken is about half as badly off as they could be in conventional cages (they\u2019re at the ~50% mark in the negative portion of their welfare range).</li><li>Over the course of a year, the average chicken is about a quarter as badly off as they could be in a cage-free system (they\u2019re at the ~25% mark in the negative portion of their welfare range).&nbsp;</li></ol></blockquote><p>Anyway, the 95%CI for chicken welfare ranges (as a fraction of human ranges) is 0.002-0.869, with 0.332 as the median estimate.</p><p>So if we make the additional assumptions that:</p><ol><li>All future animal welfare interventions will be as effective as past efforts (which seems implausible given diminishing marginal returns)</li><li>Cages cause chickens to lose half of their average welfare (a totally made up number)</li></ol><p>Then we can multiply these out to get:<img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ovGZ7sHsKXr5pvPRZ/zdbcmvkin3k9vyrfr1xe\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ovGZ7sHsKXr5pvPRZ/zqvzemcqvjcypbyfmwpp 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ovGZ7sHsKXr5pvPRZ/kmtpjvdtc99iloh3eyrx 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ovGZ7sHsKXr5pvPRZ/dn0vnlww6pcxfnytqk36 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ovGZ7sHsKXr5pvPRZ/whssx4pa0jbqyr96lot6 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ovGZ7sHsKXr5pvPRZ/ctgrhygdvfzbfhthch6q 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ovGZ7sHsKXr5pvPRZ/bdeqbbnqnk50poxuunfv 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ovGZ7sHsKXr5pvPRZ/ivguenpxls2rgortputf 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ovGZ7sHsKXr5pvPRZ/mmz2d1np0l4rkcmwekde 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ovGZ7sHsKXr5pvPRZ/qvwxbwe1peyo9tvfqj7m 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ovGZ7sHsKXr5pvPRZ/r8mhrp0rwsjayb2hxakj 1100w\"></p><p>The \"DALYs / $ through GiveWell charities\" comes from the fact that it costs ~$5000 to save the life of a child. Assming \"save a life\" means adding ~50 years to the lifespan, that means $100 / DALY, or 0.01 DALYs / $.</p><p>A few things to note here:</p><ol><li>There is <i>huge </i>uncertainty here. The 95% CI in the table indicates that chicken interventions could be anywhere from 10,000x to 0.1x as effective as human charities. (Although I got the 5th and 95th percentiles of the output by simply multiplying the 5th and 95th percentiles of the inputs. This is not correct, but I'm not sure there's a better approach without more information about the input distributions.)</li><li>To get these estimates we had to make some implausible assumptions and also totally make up some numbers.</li></ol><p>The old <a href=\"https://slatestarcodex.com/2019/03/26/cortical-neuron-number-matches-intuitive-perceptions-of-moral-value-across-animals/\">cortical neuron count</a> proxy for moral weight says that one chicken life year is worth 0.003, which is 1/100th of the RP welfare range estimate of 0.33. This number would mean chicken interventions are only 0.7x as effective as human interventions, rather than 700x as effective. [edit: oops, maths wrong here. see Michael's comment below.]</p><p>But didn't RP <a href=\"https://forum.effectivealtruism.org/posts/Mfq7KxQRvkeLnJvoB/why-neuron-counts-shouldn-t-be-used-as-proxies-for-moral\">prove</a> that cortical neuron counts are fake?</p><p>Hardly. They gave a bunch of reasons why we might be skeptical of neuron count (summarised <a href=\"https://forum.effectivealtruism.org/posts/Mfq7KxQRvkeLnJvoB/why-neuron-counts-shouldn-t-be-used-as-proxies-for-moral?commentId=iR5keJapwMN2Zj2A4\">here</a>). But I think the reasons <i>in favour</i> of using cortical neuron count as a proxy for moral weight are stronger than the objections. And that still doesn't give us any reason to think RP's has a <i>better </i>methodology for calculating moral weights. It just tells us to not take cortical counts to literally.</p><p>Points in favour of cortical neuron counts as a proxy for moral weight:</p><ol><li>Neuron counts <a href=\"https://slatestarcodex.com/2019/05/01/update-to-partial-retraction-of-animal-value-and-neuron-number/\"><u>correlate </u></a>with our intuitions of moral weights. Cortical counts would say that ~300 chicken life years are morally equivalent to one human life year, which sounds about right.</li><li>There's a common sense story of: more neurons \u2192 more compute power \u2192 more consciousness.</li><li>It's a simple and practical approach. Obtaining the moral weight of an arbitrary animal only requires counting neurons.</li></ol><p>Compare with the RP moral weights:</p><ol><li>If we interpret the welfare ranges as moral weights, then 3 chicken life years are worth one human life year. This is not a trade I would make.</li><li>If we <i>don't</i> interpret welfare ranges as moral weights, then the RP numbers tell us literally nothing.</li><li>The methodology is complex, difficult to understand, expensive, and requires reams zoological observation to be applied to new animals.</li></ol><p>And let's not forget second order effects. Raising people out of poverty can increase global innovation and specialisation and accelerate economic development which could have benefits centuries from now. It's not obvious that helping chickens has any real second order effects.</p><p>In conclusion:</p><ol><li>It's not obvious to me that RP's research actually tells us anything useful about the effectiveness of animal charities compared to human charities.&nbsp;</li><li>There are tons of assumptions and simplifications that go into these RP numbers, so any conclusions we <i>can</i> draw must be low confidence.</li><li>Cortical neuron counts still looks like a pretty good way to compare welfare across species. Under cortical neuron count, human charities come out on top.</li></ol>", "parentCommentId": null, "user": {"username": "Hamish Doodles"}}, {"_id": "tPojJp493248CEdMP", "postedAt": "2023-11-20T03:17:13.009Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Ah right, I was conflating GiveWell's operating costs (I assume not too high?) and their funding sent to other charities both as \"GiveWell continuing to work on global poverty.\" You're right that they'll still probably work on it and not collapse without OP, just they might send much less to other charities.</p>", "parentCommentId": "4be4eDxueubsJecn2", "user": {"username": "Gabe Mukobi"}}, {"_id": "6awas4gqpkPp8wiKr", "postedAt": "2023-11-20T04:09:38.744Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>If OpenPhil\u2019s allocation is really so dependent on moral weight numbers, you should be spending significant money on research in this area, right? Are you doing this? Do you plan on doing more of this given the large divergence from Rethink\u2019s numbers?</p>\n", "parentCommentId": "3uBrznQWEqs83PAfh", "user": {"username": "RedStateBlueState"}}, {"_id": "Ree26T2Qtbujqd48K", "postedAt": "2023-11-20T07:00:28.499Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Thanks for writing this post! I think it's thoughtful and well-reasoned, and I think public criticism of OP (and leading institutions in effective altruism in general) is good and undersupplied, so I feel ike this writeup is commendable. I work at a global health nonprofit funded by OP, so I should say I'm strongly biased against moving lots of the money to animal welfare.&nbsp;</p><p>An argument I've heard in the past (not the point of your post I know) is that because humans (often) eat &nbsp;factory-farmed animals, expanding human lifespan is net negative from a welfarist perspective (because it increases the net amount of suffering in the world). 1. Is this argument implausible (i.e. is there a good way to disprove it?) and 2. If the argument were true, would it imply OP should not fund global health work at all (or restrict it very seriously)?&nbsp;</p>", "parentCommentId": null, "user": {"username": "joshcmorrison"}}, {"_id": "ocxCq3gEavuEqzYbf", "postedAt": "2023-11-20T07:19:40.638Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>There's a related tag <a href=\"https://forum.effectivealtruism.org/topics/meat-eater-problem\">Meat-eater problem</a>, with some related posts. I think this is less worrying in low-income countries where GiveWell-recommended charities work, because animal product consumption is still low and factory farming has not yet become the norm. That being said, factory farming is becoming increasingly common, and it could be common for the descendants of the people whose lives are saved.</p><p>Then, there are also complicated wild animal effects from animal product consumption and generally having more humans that could go either way morally, depending on your views.</p>", "parentCommentId": "Ree26T2Qtbujqd48K", "user": {"username": "MichaelStJules"}}, {"_id": "EP8sKzTACrsQGRATJ", "postedAt": "2023-11-20T07:33:37.061Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<blockquote><p>But didn't RP <a href=\"https://forum.effectivealtruism.org/posts/Mfq7KxQRvkeLnJvoB/why-neuron-counts-shouldn-t-be-used-as-proxies-for-moral\">prove</a> that cortical neuron counts are fake?</p><p>Hardly. They gave a bunch of reasons why we might be skeptical of neuron count (summarised <a href=\"https://forum.effectivealtruism.org/posts/Mfq7KxQRvkeLnJvoB/why-neuron-counts-shouldn-t-be-used-as-proxies-for-moral?commentId=iR5keJapwMN2Zj2A4\">here</a>). But I think the reasons <i>in favour</i> of using cortical neuron count as a proxy for moral weight are stronger than the objections.</p></blockquote><p>I don't think the reasons in favour of using neuron counts provide much support for <i>weighing</i> by neuron counts or any function of them in practice. Rather, they primarily support using neuron counts to <i>inform</i> missing data about functions and capacities that do determine welfare ranges (EDIT: or moral weights), in models of how welfare ranges (EDIT: or moral weights) are determined by functions and capacities. There's a general trend that animals with more neurons have more capacities and more sophisticated versions of some capacities.</p><p>However, most functions and capacities seem pretty irrelevant to welfare ranges, even if relevant for what welfare is realized in specific circumstances. If an animal can already experience excruciating pain, presumably near the extreme of their welfare range, what do humans have that would make excruciating pain far worse for us in general, or otherwise give us far wider welfare ranges? And why?</p>", "parentCommentId": "ovGZ7sHsKXr5pvPRZ", "user": {"username": "MichaelStJules"}}, {"_id": "yjFqKMwoW3zMAQSmb", "postedAt": "2023-11-20T08:32:48.918Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p><i><strong>\"If an animal can already experience excruciating pain, presumably near the extreme of their welfare range, what do humans have that would make excruciating pain far worse for us in general, or otherwise give us far wider welfare ranges? And why?\"</strong></i></p><p>We have a far more advanced consciousness and self awareness, that <i><strong>may </strong></i>make our experience of pain orders of magnitude worse (or at least different) than for many animals - or not.&nbsp;</p><p>I think there is far more uncertainty in this question than many ackhnowledge - RP acknowledge the uncertainty but I don't think present it as clearly as they could. Extreme pain for humans could be a wildly different experience than it is for animals, or it could be quite similar. Even if we assume hedonism (which I don't), we can oversimplify the concepts of \"Sentience\" and \"welfare ranges\" to feel like we have more certainty over these numbers than we do.</p>", "parentCommentId": "EP8sKzTACrsQGRATJ", "user": {"username": "NickLaing"}}, {"_id": "Gre8zgr5fEpxmqaxK", "postedAt": "2023-11-20T08:57:52.523Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<blockquote><p>The old <a href=\"https://slatestarcodex.com/2019/03/26/cortical-neuron-number-matches-intuitive-perceptions-of-moral-value-across-animals/\">cortical neuron count</a> proxy for moral weight says that one chicken life year is worth 0.003, which is 1/100th of the RP welfare range estimate of 0.33. This number would mean chicken interventions are only 0.7x as effective as human interventions, rather than 700x as effective.</p></blockquote><p>700/100=7, not 0.7.</p>", "parentCommentId": "ovGZ7sHsKXr5pvPRZ", "user": {"username": "MichaelStJules"}}, {"_id": "PvKuYGityKJ4K2vQY", "postedAt": "2023-11-20T10:14:07.494Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<blockquote><p>We have a far more advanced consciousness and self awareness, that <i><strong>may </strong></i>make our experience of pain orders of magnitude worse (or at least different) than for many animals - or not.&nbsp;</p></blockquote><p>I agree that that's possible and worth including under uncertainty, but it doesn't answer the \"why\", so it's hard to justify giving it much or disproportionate weight (relative to other accounts) without further argument. Why would self-awareness, say, make being in intense pain orders of magnitude worse?</p><p>And are we even much more self-aware than other animals when we are in intense pain? One of the functions of pain is to take our attention, and it does so more the more intense the pain. That might limit the use of our capacities for self-awareness: we'd be too focused on and distracted by the pain. Or, maybe our self-awareness or other advanced capacities distract us from the pain, making it <i>less</i> intense than in other animals.</p><p>(My own best guess is that at the extremes of excruciating pain, sophisticated self-awareness makes little difference to the intensity of suffering.)</p>", "parentCommentId": "yjFqKMwoW3zMAQSmb", "user": {"username": "MichaelStJules"}}, {"_id": "yXfquDswSRDbpf5aw", "postedAt": "2023-11-20T11:09:19.397Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>oh true lol</p><p>ok, animal charities still come out an order of magnitude ahead of human charities given the cage-free campaigns analysis and neuron counts</p><p>but the broader point is that the RP analyses seem far from conclusive and it would be silly to use them unilaterally for making huge funding allocation decisions, which I think still stands</p>", "parentCommentId": "Gre8zgr5fEpxmqaxK", "user": {"username": "Hamish Doodles"}}, {"_id": "F2wEihiSWiDPofBCN", "postedAt": "2023-11-20T11:17:17.317Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>by that logic, two chickens have the same moral weight as one chicken because they have the same functions and capacities, no?</p>", "parentCommentId": "EP8sKzTACrsQGRATJ", "user": {"username": "Hamish Doodles"}}, {"_id": "hagzGAHzmFLDdkn4S", "postedAt": "2023-11-20T11:17:34.370Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>As I see it, we basically have a choice between:</p><ol><li>simple methodology to make vaguely plausible guesses about the unknowable phenomology of chickens (cortical neuron count)</li><li>complex methodology to make vaguely plausible guesses about the unknowable phenomology of chickens (other stuff)</li></ol><p>I much prefer the simple methodology where we can clearly see what assumptions we're making and how that propagates out.</p>", "parentCommentId": "F2wEihiSWiDPofBCN", "user": {"username": "Hamish Doodles"}}, {"_id": "sJaZbGHLmjevZWBtg", "postedAt": "2023-11-20T11:32:15.196Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<blockquote><p>I <a href=\"https://docs.google.com/spreadsheets/d/1VoIKr4k4Mb32LgELLMT103aSkJzC4C8W4EozKOdfWpE/edit?usp=sharing\">analyzed OP's grants data</a></p></blockquote><p>FYI, I made <a href=\"https://docs.google.com/spreadsheets/d/1ip7nXs7l-8sahT6ehvk2pBrlQ6Umy5IMPYStO3taaoc/edit?usp=sharing\">a spreadsheet</a> a while ago which automatically pulls the latest OP grants data and constructs summaries and pivot tables to make this type of analysis easier.<img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/hdsiiabtkyuhgxkeloih\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/ggmuugpzrbnqrvcziby0 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/vcb4jpb6jiqguasxrr72 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/au3y8ieypbpu45kumzfh 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/jv4srbkwgiy6rfnyydgi 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/e86y1c28ndnzoskc13lu 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/iqbk0fxc8dooagxgrva6 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/e945ebdl793deeoqus7t 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/fexcnh3dvsaisvlzgqpj 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/zvpwssuxv8xhigm6tfel 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/f5ofyjdc9my2ck42tcuu 1946w\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/avyhir6i3cwdstmmzfvn\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/r5dvs0p8v4kvazn3xoih 170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/cjji06knjey0jepcedyj 340w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/pyvhn20rkqpnfu1mhlui 510w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/afz8gthcm0s167mlnw3i 680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/jdobjasbubmm8awrwlrr 850w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/cdhix709aufrmtzeukkd 1020w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/ulhpp5punooqz9olwopn 1190w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/hzd8tfm2azjswxakwutl 1360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/rvmhg84rfndtquhnjtts 1530w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/udlc3qdvci1qlpd18xsx 1610w\"></p><p>I also made <a href=\"https://hamishdoodles.com/ea-data/\">these</a> interactive plots which summarise all EA funding:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/ymlvzufaxg3f7lorcfvk\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/bysvcq3uy6h61lkw2xdr 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/ywk28pcwys1aa9vvnuj8 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/l6kgl7iqwlt8j9abv7si 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/ibfuqsexzsirtsrrpgjl 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/k9sheepp519egipdfgrj 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/ol8g4rymisv3nhamjtxv 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/tqmlyroytzaryparr7bw 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/h75pk0zoxf7bpu8veo7x 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/swxi4uvsdjfxodpsccn2 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/qjmbt58nacksub4ysn7q 1412w\"></figure><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/ys8xcdzq46kswgwbwatv\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/iwxs7qnz6ms8khplz3ma 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/iq1ykajceyxkunn4ggq3 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/fohq4b30aqjhs8yefarp 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/x2t2xejmvsy8pvzkgoe9 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/wqrsznr5oizbgbzmazvu 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/avckxdx8nhyu5eu7wlbu 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/kvkodvtvs3wk1lgqslmg 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/qmbd3fhwhxp4oweaakxz 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/kgl4ymsr1swgnycfyskp 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sJaZbGHLmjevZWBtg/ab1cgvokklj3b9zysgdi 982w\"></figure>", "parentCommentId": "EiayxnzzyXssfJobJ", "user": {"username": "Hamish Doodles"}}, {"_id": "JcTzEB9RMqbm2FNFX", "postedAt": "2023-11-20T11:44:39.567Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Thanks for all this, Hamish. For what it's worth, I don't think we did a great job communicating the results of the Moral Weight Project.</p><ul><li>As you rightly observe, welfare ranges aren't moral weights without some key <a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw/p/hxtwzcsz8hQfGyZQM#:~:text=Utilitarianism%2C%20according%20to,welfare%20it%20is.\">philosophical assumptions</a>. Although we did discuss the significance of those assumptions in <a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw/p/WfeWN2X4k8w8nTeaS\">independent</a> <a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw/p/KXSBb2zgkLE6gnn3K\">posts</a>, we could have done a much better job explaining how those assumptions should affect the interpretation of our point estimates.</li><li>Speaking of the point estimates, I regret leading with them: <a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw/p/Qk3hd6PrFManj8K6o#:~:text=Our%20view%20is%20that%20the%20estimates%20we%E2%80%99ve%20provided%20are%20placeholders.\">as we said</a>, they're really just placeholders in the face of deep uncertainty. We should have led with <a href=\"https://How confident are we in our estimates and what would change them?\">our actual conclusions</a>, the basics of which are that the relevant vertebrates are probably within an OOM of humans and shrimps and the relevant adult insects are probably within two OOMs of the vertebrates. My guess is that you and I disagree less than you might think about the range of reasonable moral weights across species, even if the centers of my probability masses are higher than yours.</li><li>I agree that our methodology is complex and hard to understand. But it would be surprising if there were a simple, easy-to-understand way to estimate the possible differences in the intensities of valenced states across species. Likewise, I agree that \"there are tons of assumptions and simplifications that go into these RP numbers, so any conclusions we can draw must be low confidence.\" But there are also tons of assumptions and biases that go into our intuitive assessments of the relative moral importance of various kinds of nonhuman animals. So, a lot comes down to how much stock you put in your intuitions. As you might guess, I think we have lots of reasons not to trust them once we take on key moral assumptions like utilitiarianism. So, I take much of the value of the Moral Weight Project to be in the mere fact that it tries to reach moral weights from first principles.</li><li>It's time to do some serious surveying to get a better sense of the community's moral weights. I also think there's a bunch of good work to do on the significance of philosophical / moral uncertainty here. I If anyone wants to support this work, please let me know!</li></ul>", "parentCommentId": "ovGZ7sHsKXr5pvPRZ", "user": {"username": "bob-fischer"}}, {"_id": "XeZZr4Xoz6wavbd97", "postedAt": "2023-11-20T12:02:00.923Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Also worry about the weirdness. Ariel said themselves:</p><blockquote><p>When I started as an EA, I found other EAs' obsession with animal welfare rather strange.&nbsp;<strong>How could these people advocate for helping&nbsp;</strong><i><strong>chickens</strong></i><strong> over children in extreme poverty?</strong> I changed my mind for a few reasons.</p></blockquote><p>This might not be realistic for Ariel, but it would have been ironic if this obsession was even greater and enough to cause Ariel to shy away from EA, so that they never contributed to shifting priorities <i>more</i> to animal welfare.</p><p>But I also agree this isn't necessarily a reason to shy away. Being disingenuous about our personal priorities to seem more mainstream seems wrong - like a bait-and-switch or cult-like tactics of getting people in the door and introducing heavier stuff as they get more emotionally invested. I like the framing of being more humble/agnostic, but maybe we (speaking as individuals) need to be careful that is genuine epistemological humility and not an act.</p>", "parentCommentId": "ydNRrRxFvqXD9nHSc", "user": {"username": "Punty"}}, {"_id": "nGeuPhyybXidBjMkk", "postedAt": "2023-11-20T12:42:32.836Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Thanks for responding to my hot takes with patience and good humour!</p><p>Your defenses and caveats all sound very reasonable.</p><blockquote><p>the relevant vertebrates are probably within an OOM of humans</p></blockquote><p>So given this, you'd agree with the conclusion of the original piece? At least if we take the \"number of chickens affected per dollar\" input as correct?</p>", "parentCommentId": "JcTzEB9RMqbm2FNFX", "user": {"username": "Hamish Doodles"}}, {"_id": "XgnaLAYvibMMxeXRF", "postedAt": "2023-11-20T12:45:27.285Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>I think your BOTEC is unlikely to give meaningful answers because it treats averting a human death as equivalent to moving someone from the bottom of their welfare range to the top of their welfare range. At least to me, this seems plainly wrong - I'd vastly prefer shifting someone from receiving the worst possible torture to the greatest possible happiness for an hour to extending someone's ordinary life for an hour.&nbsp;</p><p>The objections you raise are still worth discussing, but I think the best starting place for discussing them is Duffy (2023)'s model (<a href=\"https://my.causal.app/models/192324?token=7c2741e3934e48b287748b2f673bc7b8\">Causal model</a>, <a href=\"https://docs.google.com/document/d/1CZ5S-Eayxr64z5YADYR9M3P2WTp4u2Pgb4N-ynYbbMU/edit\">report</a>), rather than your BOTEC.</p>", "parentCommentId": "ovGZ7sHsKXr5pvPRZ", "user": {"username": "MHR"}}, {"_id": "J9g9djnYeLPeXersL", "postedAt": "2023-11-20T13:28:30.102Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>I think comparisons to paper cuts and other minor harms don't work very well with people's intuitions: a lot of people feel like (and sometimes explicitly endorse that) no number of paper cuts can outweigh torturous agony. See <a href=\"https://www.lesswrong.com/posts/3wYTFWY3LKQCnAptN/torture-vs-dust-specks\">this old LW post</a> and the disagreements around it.</p>\n<p>Instead, my experience is people's intuitions work better when thinking in probabilities or quantities: what chance of suffering for a human would balance against that for a chicken? Or how many chickens suffering in that way would be equivalent to one human?</p>\n", "parentCommentId": "YJzgtPqS3AGg7Bwij", "user": {"username": "Jeff_Kaufman"}}, {"_id": "6gySYATBofmgyAC3j", "postedAt": "2023-11-20T13:31:34.477Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>I think that revealed preference can be misleading in this context, for reasons I outline <a href=\"https://forum.effectivealtruism.org/posts/3NvLqcQPjBeBHDjz6/perceived-moral-value-of-animals-and-cortical-neuron-count?commentId=95PwEvgHPvxezf9fh\">here</a>.</p><p>It's not clear that people's revealed preferences are what we should be concerned about compared to, for example, what value people would reflectively endorse assigning to animals in the abstract. People's revealed preference for continuing to eat meat, may be influenced by akrasia, or other cognitive distortions which aren't relevant to assessing how much they actually endorse animals being valued.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9o9yuuz6dhr\"><sup><a href=\"#fn9o9yuuz6dhr\">[1]</a></sup></span>&nbsp;We may care about the latter, not the former, when assessing how much we should value animals (i.e. by taking into account folk moral weights) or how much the public are likely to support/oppose us allocating more aid to animals.</p><p>But on the specific question of how the public would react to us allocating more resources to animals: this seems like a directly tractable empirical question. i.e. it would be relatively straightforward through surveys/experiments to assess whether people would be more/less hostile towards if we spent a greater share on animals, or if we spent much more on the long run future vs supporting a more diverse portfolio, or more/less on climate change etc.&nbsp;</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9o9yuuz6dhr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9o9yuuz6dhr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Though of course we also need to account for potential biases in the opposite direction as well.</p></div></li></ol>", "parentCommentId": "ydNRrRxFvqXD9nHSc", "user": {"username": "David_Moss"}}, {"_id": "wmKfvakfrbzdNnEk9", "postedAt": "2023-11-20T13:41:17.803Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>I agree with Ariel that OP should probably be spending more on animals (and I really appreciate all the work he's done to push this conversation forward). I don't know whether OP should allocate most neartermist funding to AW as I haven't looked into lots of the relevant issues. Most obviously, while the return curves for at least some human-focused neartermist options are probably pretty flat (just think of GiveDirectly), the curves for various sorts of animal spending may drop precipitously. Ariel may well be right that, even if so, the returns probably don't fall off so much that animal work loses to global health work, but I haven't investigated this myself. The upshot: I have no idea whether there are good ways of spending an additional $100M on animals right now. (That being said, I'd love to see more extensive investigation into field building for animals! If EA field building in general is cost-competitive with other causes, then I'd expect animal field building to look pretty good.)</p><p>I should also say that OP's commitment to worldview diversification complicates any conclusions about what OP should do <i>from its own perspective.</i> Even if it's true that a straightforward utilitarian analysis would favor spending a lot more on animals, it's pretty clear that some key stakeholders have <a href=\"https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous\">deep reservations</a> about straightforward utilitarian analyses. And because worldview diversification doesn't include a clear procedure for generating a specific allocation, it's hard to know what people who are committed to worldview diversification should do by their own lights.</p>", "parentCommentId": "nGeuPhyybXidBjMkk", "user": {"username": "bob-fischer"}}, {"_id": "5ezzdErAitKztsret", "postedAt": "2023-11-20T14:57:28.217Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Hi Hamish! I appreciate your critique.</p><p>Others have enumerated many reservations with this critique, which I agree with. Here I'll give several more.</p><blockquote><p>why isn't the \"1000x\" calculation actually spelled out?</p></blockquote><p>As you've seen, given Rethink's moral weights, <strong>many plausible choices for the remaining \"made-up\" numbers give a cost-effectiveness multiple on the order of 1000x</strong>. Vasco Grilo conducted a similar <a href=\"https://forum.effectivealtruism.org/posts/vBcT7i7AkNJ6u9BcQ/prioritising-animal-welfare-over-global-health-and\">analysis</a> which found a multiple of 1.71k. I didn't commit to a specific analysis for a few reasons:</p><ol><li>I agree with your point that uncertainty is really high, and I don't want to give a precise multiple which may understate the uncertainty.</li><li>Reasonable critiques can be made of pretty much any assumptions made which imply a specific multiple. Though these critiques are important for robust methodology, I wanted the post to focus specifically upon how difficult it seems to avoid the conclusion of prioritizing animal welfare in neartermism. I believe that given Rethink's moral weights, a cost-effectiveness multiple on the order of 1000x will be found by most plausible choices for the additional assumptions.</li></ol><blockquote><p>(Although I got the 5th and 95th percentiles of the output by simply multiplying the 5th and 95th percentiles of the inputs. This is not correct, but I'm not sure there's a better approach without more information about the input distributions.)</p></blockquote><p>Sadly, I don't think that approach is correct. <strong>The 5th percentile of a product of random variables is not the product of the 5th percentiles</strong>---in fact, in general, it's going to be a product of much higher percentiles (20+).</p><p>To see this, imagine if a bridge is held up 3 spokes which are independently hammered in, and each spoke has a 5% chance of breaking each year. For the bridge to fall, all 3 spokes need to break. That's not the same as the bridge having a 5% chance of falling each year--the chance is actually far lower (0.01%). For the bridge to have a 5% chance of falling each year, each spoke would need to have a 37% chance of breaking each year.</p><p>As you stated, knowledge of distributions is required to rigorously compute percentiles of this product, but it seems likely that the 5th percentile case would still have the multiple several times that of GiveWell top charities.</p><blockquote><p>let's not forget second order effects</p></blockquote><p>This is a good point, but the second order effects of global health interventions on animals are likely <a href=\"https://forum.effectivealtruism.org/posts/vBcT7i7AkNJ6u9BcQ/prioritising-animal-welfare-over-global-health-and#Effects_of_global_health_and_development_interventions_on_animals_are_neglected_and_unclear\">much larger</a> in magnitude. I think some second-order effects of many animal welfare interventions (moral circle expansion) are also positive, and I have no idea how it all shakes out.</p>", "parentCommentId": "ovGZ7sHsKXr5pvPRZ", "user": {"username": "Ariel Simnegar"}}, {"_id": "nzdpe9eLTLLQGcQRH", "postedAt": "2023-11-20T15:32:56.603Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Fair point, thanks!</p>", "parentCommentId": "J9g9djnYeLPeXersL", "user": {"username": "RYC"}}, {"_id": "nF8XCeJaWbugki6Bg", "postedAt": "2023-11-20T15:41:28.952Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<blockquote><p>Sadly, I don't think that approach is correct. <strong>The 5th percentile of a product of random variables is not the product of the 5th percentiles</strong>---in fact, in general, it's going to be a product of much higher percentiles (20+).</p></blockquote><p>As something of an aside, I think this general point was demonstrated and visualised well <a href=\"https://forum.effectivealtruism.org/posts/zHFBQ23o4DKjsoXcC/incorporating-and-visualizing-uncertainty-in-cost#Propagating_uncertainty_and_the_value_of_moving_beyond_an_interval_based_approach\">here</a>.&nbsp;<br><br>Disclaimer: I work RP so may be biased. &nbsp;</p>", "parentCommentId": "5ezzdErAitKztsret", "user": {"username": "kierangreig"}}, {"_id": "cQFhKHdCcTfxpmSdA", "postedAt": "2023-11-20T16:09:00.201Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Thanks a lot for this post!</p><p>I was thinking of doing something similar myself.</p><p>And I must admit I agree with the conclusion, especially as I have trouble seeing how their ability to suffer can be much lower than ours (I mean, we have a lot of evolutionary history in common. I can't really justify how my cat would be able to feel an amount of pain ten times lower than mine).</p><p>Since <strong>animals are far more numerous than humans, they have much worse living conditions, much less money is spent on their welfare </strong>than on human well-being, and animal charities are more funding-constrained, it's hard to see how working on them can be less cost-effective.</p>", "parentCommentId": null, "user": {"username": "Corentin Fressoz"}}, {"_id": "5YeiJHLSztRtp2k4W", "postedAt": "2023-11-20T17:17:53.555Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>I would qualify this statement by saying that it would be <i>nice</i> for OP to have more reasoning transparency, but it is not the most important thing and can be expensive to produce. So it would be quite reasonable for additional marginal transparency to not be the most valuable use of their staff time.</p>", "parentCommentId": "uvJc9X5Zmc5LLYHis", "user": {"username": "Michael_PJ"}}, {"_id": "vjKrY6jqEgsLSxy87", "postedAt": "2023-11-20T17:20:51.042Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>100% agree. I think it is almost always better to be honest, even if that makes you look weird. If you are worried about optics, \"oh yeah, we say this to get people in but we don't really believe it\" looks pretty bad.</p>", "parentCommentId": "XeZZr4Xoz6wavbd97", "user": {"username": "Michael_PJ"}}, {"_id": "D2sRJKy8jsZtWa5it", "postedAt": "2023-11-20T17:53:10.259Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>In fact, it has been suggested by Richard Dawkins that less intelligent animals might experience greater suffering, as they require more intense pain to elicit a response. The evolutionary process would have ensured they feel sufficient pain.</p>\n", "parentCommentId": "cQFhKHdCcTfxpmSdA", "user": {"username": "jackmalde"}}, {"_id": "QcnHZ4QByu3trEiKj", "postedAt": "2023-11-20T20:30:10.945Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Ah, that's interesting. I didn't know that.&nbsp;</p><p>I had in mind that maybe the power of thought could allow us to put things into perspective better and better support pain (as can be experienced through meditation). However, this can go both ways, as negative thoughts can cause additional suffering.</p><p>But I shall check the suggestion by Dawkins, that sounds interesting.</p>", "parentCommentId": "D2sRJKy8jsZtWa5it", "user": {"username": "Corentin Fressoz"}}, {"_id": "qk2AF47H4Zumtvkbx", "postedAt": "2023-11-20T22:02:40.212Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>They won't be literally identical: they'll differ in many ways, like physical details, cognitive expression and behavioural influence. They're separate instantiations of the same broad class of functions or capacities.</p><p>I would say the number of times a function or capacity is realized in a brain can be relevant, but it seems pretty unlikely to me that a person can experience suffering hundreds of times <i>simultaneously</i> (and hundreds of times more than chickens, say)<i>. </i>Rethink Priorities looked into these kinds of views <a href=\"https://forum.effectivealtruism.org/posts/vbhoFsyQmrntru6Kw/do-brains-contain-many-conscious-subsystems-if-so-should-we\">here</a>. (I'm a co-author on that article, but I don't work at Rethink Priorities anymore, and I'm not speaking on their behalf.)</p><p>FWIW, I started very pro-neuron counts (I defended them <a href=\"https://forum.effectivealtruism.org/posts/H7KMqMtqNifGYMDft/differences-in-the-intensity-of-valenced-experience-across?commentId=MgytBtik9XmB7vTve\">here</a> and <a href=\"https://forum.effectivealtruism.org/posts/848SgRAKpjbuBWkW7/why-might-one-value-animals-far-less-than-humans?commentId=2FyMudPex224kPWYA\">here</a>), and then others at RP, collaborators and further investigation myself moved me away from the view.</p>", "parentCommentId": "F2wEihiSWiDPofBCN", "user": {"username": "MichaelStJules"}}, {"_id": "SfuKwFWGRk9i9Hss6", "postedAt": "2023-11-20T22:29:12.879Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Thanks! Small correction: Animal Welfare YTD is labeled as $53M, when it looks like the underlying data point is $17M (source and 2023 full-year projections <a href=\"https://docs.google.com/spreadsheets/d/1IeO7NIgZ-qfSTDyiAFSgH6dMn1xzb6hB2pVSdlBJZ88/edit?usp=sharing\">here</a>)</p>", "parentCommentId": "sJaZbGHLmjevZWBtg", "user": {"username": "tylermaule"}}, {"_id": "cfRGn4CnYQ7QTowDN", "postedAt": "2023-11-21T00:29:52.920Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>There are other simple methodologies that make vaguely plausible guesses (under hedonism), like:</p><ol><li>welfare ranges are generally similar or just individual-relative across species capable of suffering and pleasure (<a href=\"https://docs.google.com/spreadsheets/d/1i_I23qXtuGtpk4spFi2NmK6ReCiFXL6QGBd2KfGzulY/edit#gid=45079884\">RP's Equality Model</a>),&nbsp;</li><li>the <a href=\"https://welfarefootprint.org/technical-definitions/\">intensity categories of pain defined by Welfare Footprint Project</a> (or some other functionally defined categories) have similar ranges across animals that have them, and assign numerical weights to those categories, so that we should weigh \"disabling pain\" similarly across animals, including humans,</li><li>the pain intensity scales with the number of just-noticeable differences in pain intensities away from neutral across individuals, so we just weigh by their number (<a href=\"https://docs.google.com/spreadsheets/d/1i_I23qXtuGtpk4spFi2NmK6ReCiFXL6QGBd2KfGzulY/edit#gid=2030812531\">RP's Just Noticeable Difference Model</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefikmjrwoafra\"><sup><a href=\"#fnikmjrwoafra\">[1]</a></sup></span>).</li></ol><p>In my view, 1, 2 and 3 are more plausible and defensible than views that would give you (cortical or similar function) neuron counts as a good approximation. I also think the actually right answer, if there's any (so excluding the individual-relative interpretation for 1), will look like 2, but more complex and with possibly different functions. RP explicitly considered 1 and 3 in its work. These three models give chickens &gt;0.1x humans' welfare ranges:</p><ol><li>Model 1 would give the same welfare ranges across animals, including humans, conditional on capacity for suffering and pleasure.</li><li>Model 2 would give the same sentience-conditional welfare ranges across mammals (including humans) and birds, at least. My best guess is also the same across all vertebrates. I'm less sure that invertebrates can experience similarly intense pain even conditional on sentience, but it's not extremely unlikely.</li><li>Model 3 would probably pretty generally give nonhuman animals welfare ranges at least ~0.1x humans', conditional on sentience, according to RP.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefg7ata9kza0p\"><sup><a href=\"#fng7ata9kza0p\">[2]</a></sup></span></li></ol><p>You can probably come up with some models that assign even lower welfare ranges to other animals, too, of course, including some relatively simple ones, although not simpler than 1.</p><p>Note that using <i>cortical</i> (or similar function) neuron counts also makes important assumptions about which neurons matter and when. Not all plausibly conscious animals have cortices, so you need to identify which structures have similar roles, or else, chauvinistically, rule these animals out entirely regardless of their capacities. So this approach is not that simple, either. Just counting <i>all</i> neurons would be simpler.</p><p>(I don't work for RP anymore, and I'm not speaking on their behalf.)</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnikmjrwoafra\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefikmjrwoafra\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Although we could use a different function of the number instead, for increasing or diminishing marginal returns to additional JNDs.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fng7ata9kza0p\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefg7ata9kza0p\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Maybe lower for some species RP didn't model, e.g. nematodes, tiny arthropods?</p></div></li></ol>", "parentCommentId": "hagzGAHzmFLDdkn4S", "user": {"username": "MichaelStJules"}}, {"_id": "bRuz9pxPTfG4EMTHe", "postedAt": "2023-11-21T03:48:41.244Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>I think if there's anything they should bother to be publicly transparent about in order to subject to further scrutiny, it's their biggest cruxes for resource allocation between causes. Moral weights, theory of welfare and the marginal cost-effectiveness of animal welfare seem pretty decisive for GHD vs animal welfare.</p>\n", "parentCommentId": "5YeiJHLSztRtp2k4W", "user": {"username": "MichaelStJules"}}, {"_id": "jEc9xLRd6TcCYgd9s", "postedAt": "2023-11-21T04:54:28.916Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<blockquote><p>FWIW, I started very pro-neuron counts (I defended them <a href=\"https://forum.effectivealtruism.org/posts/H7KMqMtqNifGYMDft/differences-in-the-intensity-of-valenced-experience-across?commentId=MgytBtik9XmB7vTve\">here</a> and <a href=\"https://forum.effectivealtruism.org/posts/848SgRAKpjbuBWkW7/why-might-one-value-animals-far-less-than-humans?commentId=2FyMudPex224kPWYA\">here</a>), and then others at RP, collaborators and further investigation myself moved me away from the view.</p></blockquote><p>&nbsp;</p><p>Oh, interesting. That moves my needle.</p>", "parentCommentId": "qk2AF47H4Zumtvkbx", "user": {"username": "Hamish Doodles"}}, {"_id": "JT66pQbm7aDgGd5Tf", "postedAt": "2023-11-21T05:13:59.062Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<blockquote><p>wanted the post to focus specifically upon how difficult it seems to avoid the conclusion of prioritizing animal welfare in neartermism</p></blockquote><p>I wasn't familiar with these other calculations you mention. I thought you were just relying on the RP studies which seemed flimsy. This extra context makes the case much stronger.</p><blockquote><p>Sadly, I don't think that approach is correct. <strong>The 5th percentile of a product of random variables is not the product of the 5th percentiles</strong>---in fact, in general, it's going to be a product of much higher percentiles (20+).</p></blockquote><p>I don't think that's true either.&nbsp;</p><p>If you're multiplying noramlly distributed distributions, the general rule is that you add the percentage variances in quadrature.</p><p>Which I don't think converges to a specific percentile like 20+. As more and more uncertainties cancel out the relative contribution of any given uncertainty goes to zero.</p><p>IDK. I did explicitly say that my calculation wasn't correct. And with the information on hand I can't see how I could've done better. Maybe I should've fudged it down by one OOD.</p>", "parentCommentId": "5ezzdErAitKztsret", "user": {"username": "Hamish Doodles"}}, {"_id": "uK8NQbqquHaoQYFdM", "postedAt": "2023-11-21T08:47:18.691Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Why would they need more intense pain to elicit a response? Intuitively to me at least with less \"reasoning\" ability, the slightest bit of pain would likely illicit a response away from said pain.&nbsp;</p>", "parentCommentId": "D2sRJKy8jsZtWa5it", "user": {"username": "NickLaing"}}, {"_id": "e8BRZzhmZsXoES7uN", "postedAt": "2023-11-21T10:35:39.073Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Well you might need a reasoned response I.e. it seems that when I do X a bad thing happens to me therefore I should endeavor not to do X.</p>\n<p>Here is the quote from Richard Dawkins:</p>\n<p>\u201cIf you think about what pain is for biologically speaking, pain is a warning to the animal, \u2018don\u2019t do that again\u2019.</p>\n<p>\u201cIf the animal does something which results in pain, that is a kind of ritual death \u2013 it is telling the animal, \u2018if you do that again you might die and fail to reproduce\u2019. That\u2019s why natural selection has built the capacity to feel pain into our nervous systems.</p>\n<p>\u201cYou could say since pain is there to warn the animal not to do that again\u2026 an animal that is a slow learner or less intelligent might need more intense pain in order to deter [them] from doing it again, than a human who is intelligent enough to learn quickly.</p>\n<p>\u201cSo it\u2019s possible non-human animals are capable of feeling more intense pain than we are.\u201d</p>\n<p><a href=\"https://plantbasednews.org/culture/richard-dawkins-animals-feel-more-intense-pain-than-humans/#:~:text=%E2%80%9CYou%20could%20say%20since%20pain,intelligent%20enough%20to%20learn%20quickly\">https://plantbasednews.org/culture/richard-dawkins-animals-feel-more-intense-pain-than-humans/#:~:text=\u201cYou could say since pain,intelligent enough to learn quickly</a>.</p>\n", "parentCommentId": "uK8NQbqquHaoQYFdM", "user": {"username": "jackmalde"}}, {"_id": "wE9zWkTrzK8G624L2", "postedAt": "2023-11-21T10:43:55.555Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>I haven't read the other comments yet but I just want to share my deep appreciation for writing this post! I've always wondered why animal welfare gets so little funding compared to global health in EA. I'm thankful you're highlighting it and starting a discussion, whether or not OP's reasons might be justified.</p>\n", "parentCommentId": null, "user": {"username": "Jeroen_W"}}, {"_id": "4sdLTNEo3mqgwuGkP", "postedAt": "2023-11-21T11:17:51.850Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>That does seem plausible but I think the opposite i more likely. Of course you need a reasoned response, but I'm not sure the <i><strong>magnitude </strong></i>of pain would necessarily help the association with the reasoned response</p><p>Harmful action leads to negative stimulus (perhaps painful) which leads to withdrawl and future cessation of that action. It seems unlikely to me that increasing the magnitude of that pain would make a creature more likely to stop doing an action. More like the memory and higher functions would need to be sufficient to associate the action to the painful stimuli, and then a form of memory needs to be there to allow a creature to avoid the action in future.&nbsp;</p><p>It is unintuitive to me that the \"amount\" of negative stimuli (pain) would be what matters, more the strength of connection between the pain and the action, which would allow future avoidance of the behaviour.</p><p>I use \"negative stimuli\" rather than pain, because I still believe we heavily anthropomorphise our own experience of pain onto animals. Their experience is likely to be so wildly different from ours (whether \"better\" or \"worse\") that I think even using the word pain might be misleading sometimes.</p><p>More intelligent beings shouldn't necessarily need pain at all to avoid actions which could cause you to \"die and fail to reproduce\". I wouldn't think to avoid actions that could lead to, or would need very minor stimulus as a reminder.&nbsp;</p><p>Actually it does seem quite complex the more I think about it/</p><p>Its an interesting discussion anyway.</p>", "parentCommentId": "e8BRZzhmZsXoES7uN", "user": {"username": "NickLaing"}}, {"_id": "5oZgHWDZ7YjbqvkcK", "postedAt": "2023-11-21T11:47:17.020Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<blockquote><p>Points in favour of cortical neuron counts as a proxy for moral weight:</p><ol><li>Neuron counts <a href=\"https://slatestarcodex.com/2019/05/01/update-to-partial-retraction-of-animal-value-and-neuron-number/\"><u>correlate </u></a>with our intuitions of moral weights. Cortical counts would say that ~300 chicken life years are morally equivalent to one human life year, which sounds about right.</li></ol></blockquote><p>&nbsp;</p><p>That neuron counts seem to correlate with intuitions of moral weight is true, but potentially misleading. We discuss these results, drawing on our own data <a href=\"https://forum.effectivealtruism.org/posts/3NvLqcQPjBeBHDjz6/perceived-moral-value-of-animals-and-cortical-neuron-count\">here</a>.&nbsp;</p><p>I would quite strongly recommend that more survey research be done (including more analysis, as well as additional surveys: we have some more unpublished data of our own on this), if before taking the correlations as a reason to prefer using neuron count as a proxy (in contrast to a holistic assessment of different capacities).</p>", "parentCommentId": "ovGZ7sHsKXr5pvPRZ", "user": {"username": "David_Moss"}}, {"_id": "23mBDnwTkrZerJTdo", "postedAt": "2023-11-21T16:26:07.931Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<blockquote><p>This extra context makes the case much stronger.</p></blockquote><p>Thanks for being charitable :)</p><p>On the percentile of a product of normal distributions, I wrote this Python script which shows that the 5th percentile of a product of normally distributed random variables will in general be a product of much higher percentiles (in this case, the 16th percentile):</p><blockquote><p>import random</p><p>MU = 100<br>SIGMA = 10<br>N_SAMPLES = 10 ** 6<br>TARGET_QUANTILE = 0.05<br>INDIVIDUAL_QUANTILE = 83.55146375 # From Google Sheets NORMINV(0.05,100,10)</p><p>samples = []<br>for _ in range(N_SAMPLES):<br>&nbsp; &nbsp;r1 = random.gauss(MU, SIGMA)<br>&nbsp; &nbsp;r2 = random.gauss(MU, SIGMA)<br>&nbsp; &nbsp;r3 = random.gauss(MU, SIGMA)<br>&nbsp; &nbsp;sample = r1 * r2 * r3<br>&nbsp; &nbsp;samples.append(sample)</p><p>samples.sort()<br># The sampled 5th percentile product<br>product_quantile = samples[int(N_SAMPLES * TARGET_QUANTILE)]<br>implied_individual_quantile = product_quantile ** (1/3)<br>implied_individual_quantile # ~90, which is the *16th* percentile by the empirical rule</p></blockquote><p>I apologize for overstating the degree to which this reversion occurs in my original reply (which claimed an individual percentile of 20+ to get a product percentile of 5), but I hope this Python snippet shows that my point stands.</p><blockquote><p>I did explicitly say that my calculation wasn't correct. And with the information on hand I can't see how I could've done better.</p></blockquote><p>This is completely fair, and I'm sorry if my previous reply seemed accusatory or like it was piling on. If I were you, I'd probably caveat your analysis's conclusion to something more like \"Under RP's 5th percentile weights, the cost-effectiveness of cage-free campaigns would probably be <i>lower</i> than that of the best global health interventions\".</p>", "parentCommentId": "JT66pQbm7aDgGd5Tf", "user": {"username": "Ariel Simnegar"}}, {"_id": "ze5ttKJdHjrpMrfbn", "postedAt": "2023-11-22T00:18:25.826Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>I strongly agree with this post and strongly upvoted it. I also talked a lot with Ariel in the making of this post. I think the arguments are good and I think EA in general should be focusing a lot more on animal welfare than GHW.</p><p>That said, I think it's important to note that \"EA\" doesn't own the money being given away by Open Phil. It's Dustin/Cari's money that is being given away and Open Phil was set up (by them, in a joint venture between Givewell and Good Ventures) to advise them where their money should go and they are inspired/wish to give away their money by EA principles.&nbsp;<br><br>The people at Open Phil are heavily influenced by Dustin/Cari's values so it isn't surprising that the people at Open Phil might value animals less than the general movement and if Dustin/Cari don't want to give their money to non-human animal causes, that's well within their rights. The \"EA movement\", however you define it, doesn't get to control the money and <a href=\"https://forum.effectivealtruism.org/posts/zuqpqqFoue5LyutTv/the-ea-community-does-not-own-its-donors-money\">there are good reasons for this</a>.</p><p>Like <a href=\"https://forum.effectivealtruism.org/users/mathiaskb?mention=user\">@MathiasKB</a>, I want to generally encourage people to see how they can affect the funding landscape, primarily via their own donations as opposed to simply telling other people how they should donate. A very unstable equilibrium would result from a bunch of people steering and not a lot of people rowing.</p>", "parentCommentId": null, "user": {"username": "MarcusAbramovitch"}}, {"_id": "4LHptEnLk7T44TeRW", "postedAt": "2023-11-22T09:57:47.106Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>'There's a common sense story of: more neurons \u2192 more compute power \u2192 more consciousness.'<br><br>I think it is very unclear what \"more consciousness\" even means. \"Consciousness\" isn't \"stuff\" like water that you can have a greater weight or volume of.&nbsp;</p>", "parentCommentId": "ovGZ7sHsKXr5pvPRZ", "user": {"username": "Dr. David Mathers"}}, {"_id": "hg7m5DcG3Pvf7QMiD", "postedAt": "2023-11-22T13:21:12.314Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>I think it's very unclear for sure. Why could consciousness not be like water that you could have more or less volume of? When I was a child I was perhaps conscious but less so than now?</p>\n<p>Could a different species with a different brain structure could have a different \"nature\" of consciousness while not necessarily being more or less?</p>\n<p>I agree it's very unclear, but there could be directionality unless I'm missing some of the point of the concept...</p>\n", "parentCommentId": "4LHptEnLk7T44TeRW", "user": {"username": "NickLaing"}}, {"_id": "uxrse7TyaNrg8zwrq", "postedAt": "2023-11-22T14:08:27.400Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Hi David,</p><p>Relatedly, readers may want to check <a href=\"https://forum.effectivealtruism.org/posts/Mfq7KxQRvkeLnJvoB/why-neuron-counts-shouldn-t-be-used-as-proxies-for-moral\">Why Neuron Counts Shouldn't Be Used as Proxies for Moral Weight</a>. Here are the key takeaways:</p><blockquote><ul><li>Several influential EAs have suggested using neuron counts as rough proxies for animals\u2019 relative moral weights. We challenge this suggestion.</li><li>We take the following ideas to be the strongest reasons in favor of a neuron count proxy:<ul><li>neuron counts are correlated with intelligence and intelligence is correlated with moral weight,</li><li>additional neurons result in \u201cmore consciousness\u201d or \u201cmore valenced consciousness,\u201d and</li><li>increasing numbers of neurons are required to reach thresholds of minimal information capacity required for morally relevant cognitive abilities.</li></ul></li><li>However:<ul><li>in regards to intelligence, we can question both<strong>&nbsp;</strong>the extent to which more neurons are correlated with intelligence and whether more intelligence in fact predicts greater moral weight;&nbsp;</li><li>many ways of arguing that more neurons results in more valenced consciousness seem incompatible with our current understanding of how the brain is likely to work; and</li><li>there is no straightforward empirical evidence or compelling conceptual arguments indicating that relative differences in neuron counts within or between species reliably predicts welfare relevant functional capacities.</li></ul></li><li>Overall, we suggest that neuron counts should not be used as a sole proxy for moral weight, but cannot be dismissed entirely. Rather, neuron counts should be combined with other metrics in an overall weighted score that includes information about whether different species have welfare-relevant capacities.</li></ul></blockquote>", "parentCommentId": "4LHptEnLk7T44TeRW", "user": {"username": "vascoamaralgrilo"}}, {"_id": "kuzyQerDSZobTgDqo", "postedAt": "2023-11-22T14:56:13.601Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Thank you so much for putting this together, Ariel!</p>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "7peiv2vviqpPEuwy3", "postedAt": "2023-11-22T15:09:09.320Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Absolutely! Most of what's important in this essay is just a restatement of your inspiring <a href=\"https://forum.effectivealtruism.org/posts/vBcT7i7AkNJ6u9BcQ/prioritising-animal-welfare-over-global-health-and\">CEA</a> from months ago :)</p>", "parentCommentId": "kuzyQerDSZobTgDqo", "user": {"username": "Ariel Simnegar"}}, {"_id": "QZJMHSSzL3xGcGk5t", "postedAt": "2023-11-22T15:55:53.625Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Thanks for the feedback, Emily!</p><blockquote><ul><li><a href=\"https://forum.effectivealtruism.org/posts/vBcT7i7AkNJ6u9BcQ/prioritising-animal-welfare-over-global-health-and\"><u>Vasco\u2019s analysis</u></a> implies a much wider welfare range than the one we use.<ul><li>We\u2019re not confident in our current assumptions, but this is a complicated question, and there is more work we need to do ourselves to get to an answer we believe in enough to act on. We also need to think through consistency with our human-focused interventions.&nbsp;</li></ul></li><li>We don\u2019t use Rethink\u2019s moral weights.<ul><li>Our current moral weights, based in part on&nbsp;<a href=\"https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood/\"><u>Luke Muehlhauser\u2019s past work</u></a>, are lower. We may update them in the future; if we do, we\u2019ll consider work from many sources, including the arguments made in this post.</li></ul></li></ul></blockquote><p>I am a little confused by the above. You say my analysis implies a much wider welfare range than the one you use, but in my analysis I just used point estimates. I relied on Rethink Priorities' <a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw/p/Qk3hd6PrFManj8K6o\">median welfare range</a> for chickens of 0.332, although Rethink's 5th and 95th percentile are 0.002 and 0.869 (i.e. the 95th percentile is 434 times the 5th percentile).</p><p>Are you saying Rethink's interval for the welfare range of chickens is much wider than Open Phil's? I think that would imply some disagreement with Luke's guess. Following his 2017 <a href=\"https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood/\">report</a> on consciousness and moral patienthood, Luke <a href=\"https://www.lesswrong.com/posts/2jTQTxYNwo6zb3Kyp/preliminary-thoughts-on-moral-weight\">guessed</a> in 2018 a chicken life-year to be worth 0.00005 to 10 human life-years (\"80% prediction interval\"; upper bound 200 k times the lower bound). This interval is 11.5 (= (10 - 0.00005)/(0.869 - 0.002)) times as wide as Rethink's on a linear scale, 2.01 (= ln(10/0.00005)/ln(0.869/0.002)) times as wide on a logarithmic scale, and Luke's interval respecting the 5th and 95th percentile would have been wider. Rethink's and Luke's intervals are not directly comparable. Rethink's refers to the welfare range, whereas Luke's refers to moral weight. However, I would have guessed Open Phil's interval for the moral weight to be narrower than Open Phil's interval for the welfare range, as Open Phil's funding suggests a comparatively low weight on direct hedonic effects (across animal and human interventions). In any case, I must note Luke was not confident about his guesses, having <a href=\"https://forum.effectivealtruism.org/posts/sPk86847CKeJYgJ9H/moral-weights-for-various-species-and-distributions?commentId=yt27FvZcEKSp3iRXP\">commented</a> around 1.5 years ago that:</p><blockquote><p>Since this [<a href=\"https://www.lesswrong.com/posts/2jTQTxYNwo6zb3Kyp/preliminary-thoughts-on-moral-weight\">this</a>] exercise is based on numbers I personally made up, I would like to remind everyone that those numbers are extremely made up and come with many caveats given in the original sources. It would not be that hard to produce numbers more reasonable than mine, at least re: moral weights. (I spent more time on the \"probability of consciousness\" numbers, though that was years ago and my numbers would probably be different now.)</p></blockquote><p>On the other hand, multiplying Luke's numbers by the ratio of 10 k between the cost-effectiveness of corporate campaigns and GiveWell's top charities for a moral weight of 1 shared in the worldview diversification 2016 <a href=\"https://www.openphilanthropy.org/research/worldview-diversification/\">post</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefalm3ei0v8y9\"><sup><a href=\"#fnalm3ei0v8y9\">[1]</a></sup></span>, one would conclude corporate campaigns for chicken welfare to be 0.5 (= 0.00005*10000) to 100 k (= 10*10000) times as cost-effective as GiveWell's top charities. In my mind, the prospect of corporate campaigns for chicken welfare being much more cost-effective at increasing welfare than GiveWell's top charities should have prompted a major investigation of the topic, and more transparent communication of Open Phil's prioritisation decisions.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnalm3ei0v8y9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefalm3ei0v8y9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\"If you value chicken life-years equally to human life-years, this implies that corporate campaigns do about 10,000x as much good per dollar as top charities. If you believe that chickens do not suffer in a morally relevant way, this implies that corporate campaigns do no good\".</p></div></li></ol>", "parentCommentId": "3uBrznQWEqs83PAfh", "user": {"username": "vascoamaralgrilo"}}, {"_id": "SDGvGravPQkT2extx", "postedAt": "2023-11-22T16:21:44.745Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Hi Marcus,</p><blockquote><p>The people at Open Phil are heavily influenced by Dustin/Cari's values so it isn't surprising that the people at Open Phil might value animals less than the general movement and if Dustin/Cari don't want to give their money to non-human animal causes, that's well within their rights.</p></blockquote><p>Do you have a source for this? My sense was that Dustin and Cari mostly deferred to Open Phil's researchers. I think Dustin tweeted about this at some point.</p>", "parentCommentId": "ze5ttKJdHjrpMrfbn", "user": {"username": "vascoamaralgrilo"}}, {"_id": "8qiBzNSGSDxsaMZW8", "postedAt": "2023-11-22T18:57:03.385Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Correct. Dustin and Cari mostly defer to OP. But the people at OP aren't random. The selection of leadership at OP (Holden/Alex) are very much because of Dustin/Cari. FWIW, on the whole, I'm very thankful for them. Without them, EA would look quite a lot worse on the whole, including for animals.</p>", "parentCommentId": "SDGvGravPQkT2extx", "user": {"username": "MarcusAbramovitch"}}, {"_id": "Sfx7ckshLyyyhGPBu", "postedAt": "2023-11-22T20:32:24.552Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Several of the&nbsp;<a href=\"https://www.openphilanthropy.org/grants/?organization-name=rethink-priorities\"><u>grants we\u2019ve made to Rethink Priorities</u></a> funded research related to moral weights; we\u2019ve also conducted our own research on the topic. We may fund additional moral weights work next year, but we aren\u2019t certain. In general, it's very hard to guarantee we'll fund a particular topic in a future year, since our funding always depends on which opportunities we find and how they compare to each other \u2014 and there's a lot we don't know about future opportunities.</p><p>I unfortunately won\u2019t have time to engage with further responses for now, but whenever we publish research relevant to these topics, we\u2019ll be sure to cross-post it on the Forum!</p>", "parentCommentId": "6awas4gqpkPp8wiKr", "user": {"username": "Emily Oehlsen"}}, {"_id": "rDg5BjdgPCGPdb3sj", "postedAt": "2023-11-22T20:33:30.995Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Hi Ariel,</p><p><i>As OP continues researching moral weights, OP's marginal cost-effectiveness estimates for FAW and GHW may eventually differ by several orders of magnitude. If this happens, would OP substantially update their allocations between FAW and GHW?</i></p><p>We\u2019re unsure conceptually whether we should be trying to equalize marginal returns between FAW and GHW or whether we should continue with our current approach of worldview diversification. If we end up feeling confident that we should be equalizing marginal returns and there are large differences (we\u2019re uncertain about both pieces right now), I expect that we\u2019d adjust our allocation strategy. But this wouldn\u2019t happen immediately; we think it\u2019s important to give program staff notice well in advance of any pending allocation changes.</p><p><i>Your comment seems to imply that OP's moral weights are 1-2 orders of magnitude lower than Rethink's.</i></p><p>I\u2019m wary of sharing precise numbers now, because we\u2019re highly uncertain about all three of the parameters I listed and I don\u2019t want people to over-update on our views. But the 2 orders of magnitude are coming from a combination of the three parameters I listed and not just moral weights. We may share more information on our views and methodology later, but I can\u2019t commit to a particular date or any specifics on what we\u2019ll publish.</p><p>I unfortunately won\u2019t have time to engage with further responses for now, but whenever we publish research relevant to these topics, we\u2019ll be sure to cross-post it on the Forum!&nbsp;</p><p>We think these discussions are valuable, and I hope we\u2019ll be able to contribute more of our own takes down the line. But we\u2019re working on a lot of other research we hope to publish, and I can\u2019t say with certainty when we\u2019ll share more on this topic.</p><p>Thank you again for the critique!&nbsp;</p>", "parentCommentId": "kK24aHFoL3LenKswA", "user": {"username": "Emily Oehlsen"}}, {"_id": "arqdcB4ZcjNab7wMc", "postedAt": "2023-11-22T21:11:48.810Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Given it is the Giving Season, I'd be remiss not to point out that ACE currently has <a href=\"https://donate.animalcharityevaluators.org/page/rcfmatch2023\">donation matching</a> for their Recommended Charity Fund.</p><p>I am personally waiting to hear back from RC Forward on whether Canadian donations can also be made for said donation matching, but for American EAs at least, this seems like a great no-brainer opportunity to dip your feet in effective animal welfare giving.</p>", "parentCommentId": "ZYnDzLmnqzDy6yuxo", "user": {"username": "Cornelis Dirk Haupt"}}, {"_id": "Q2nQfT3YuJiNGCmre", "postedAt": "2023-11-23T02:03:52.861Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>This is from 2016, but worth looking into if you're curious how this works:</p>\n<p>\"At least 50% of each program officer\u2019s grantmaking should be such that Holden and Cari understand and are on board with the case for each grant.\nAt least 90% of the program officer\u2019s grantmaking should be such that Holden and Cari could easily imagine being on board with the grant if they knew more, but may not be persuaded that the grant is a good idea. (When taking the previous bullet point into account, this leaves room for up to 40% of the portfolio to fall in this bucket.)\nUp to 10% of the program officer\u2019s grantmaking can be done without meeting either of the above two criteria, though there are some basic checks in place to avoid grantmaking that creates risks for Open Philanthropy. We call this \u201cdiscretionary\u201d grantmaking. Grants in this category generally follow a different, substantially abbreviated approval process. Some examples of discretionary grants are here and here.\"</p>\n<p>(<a href=\"https://www.openphilanthropy.org/research/our-grantmaking-so-far-approach-and-process/\">https://www.openphilanthropy.org/research/our-grantmaking-so-far-approach-and-process/</a>)</p>\n", "parentCommentId": "SDGvGravPQkT2extx", "user": {"username": "Moritz von Knebel"}}, {"_id": "uzZnDc5JoPqiwZuHD", "postedAt": "2023-11-23T10:43:24.309Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>In the summary you mention that \"Skepticism of formal philosophy is not enough\". I\u2019m new to the forum, could you (or anyone else) clarify what is meant by formal philosophy? Is the statement equivalent to just saying \"Skepticism of philosophy is not enough\" or \"Skepticism of philosophical reasoning is not enough\"?&nbsp;</p><p>Also, in the section \"Increasing Animal Welfare Funding would Reduce OP\u2019s Influence on Philanthropists\" you make a comparison of AI x-risk and FAM. While AI x-risk reduction is also a niche cause area, I think you underestimate how niche FAW is relative to AI x-risk. The potential alienating risk from significant allocation to x-risk isn\u2019t the same as that of FAW since AI x-risk is still largely a story about the impact this would have on humans and their societies.</p><p>I\u2019m not saying this is the correct view but the one that would be generally held by most potential funders.&nbsp;</p><p>&nbsp;</p><p>In general the utilitarian case for your main points seem strong. Great post.</p>", "parentCommentId": null, "user": {"username": "DanteTheAbstract"}}, {"_id": "Fxf9kGGvHAQLFBfcK", "postedAt": "2023-11-23T11:43:58.022Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>I'm not saying it's impossible to make sense of the idea of a metric of \"how conscious\" something is, just that it's unclear enough what this means that any claim employing the notion without explanation is not \"commonsense\".&nbsp;</p>", "parentCommentId": "hg7m5DcG3Pvf7QMiD", "user": {"username": "Dr. David Mathers"}}, {"_id": "LdXmFJFfb3smoZ6Fr", "postedAt": "2023-11-23T13:27:16.630Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>100% agree nice one</p>", "parentCommentId": "Fxf9kGGvHAQLFBfcK", "user": {"username": "NickLaing"}}, {"_id": "Y9udovWnW6K75Jbbz", "postedAt": "2023-11-23T13:38:51.413Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Also <i>part</i> (although not all) of the attraction of \"more neurons=more consciousness\" is I think a picture that comes from \"more input=more of a physical stuff\", which is wrong in this case. I actually do &nbsp;(tentatively!) think that consciousness is sort of a cluster-y concept, where the more of a range of properties a mind has, the more true* it is to say it is conscious, but none of those properties definitively is \"really\" what being conscious requires. (i.e. sensory input into rational belief, ability to recognize your own sensory states, some sort of raw complexity requirement to rule out very simple systems with the previous 2 features etc.) And I think larger neuron counts will rough correlate with having more of these sorts of properties. But I doubt this will lead to a view where something with a trillion neurons is a thousand times more conscious than something with a billion.&nbsp;<br>*Degrees of truth are also highly philosophically controversial though.&nbsp;</p>", "parentCommentId": "LdXmFJFfb3smoZ6Fr", "user": {"username": "Dr. David Mathers"}}, {"_id": "8Hf8drvADWb9po6qH", "postedAt": "2023-11-23T13:49:33.557Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Thanks for sharing, MvK!</p><p>In general, I would still say Open Phil's grantmaking process is very opaque, and I think it would be great to have more transparency about how grants are made, including the influence of Dustin and Cari, at least for big ones. Just to illustrate how little information is provided, <a href=\"https://www.openphilanthropy.org/grants/redwood-research-general-support-2/\">here</a> is the write-up of a grant of 10.7 M$ to Redwood Research in 2022:</p><blockquote><p>Open Philanthropy recommended a grant of $10,700,000 over 18 months to Redwood Research for general support. Redwood Research is a nonprofit research institution focused on aligning advanced AI with human interests.</p><p>This follows our <a href=\"https://www.openphilanthropy.org/grants/redwood-research-general-support/\"><u>2021 support</u></a> and falls within our focus area of <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence\"><u>potential risks from advanced artificial intelligence</u></a>.</p></blockquote><p>There was nothing else. <a href=\"https://www.openphilanthropy.org/grants/redwood-research-general-support/\">Here</a> is the write-up regarding the 2021 support, 9.42 M$, mentioned just above:</p><blockquote><p>Open Philanthropy recommended four grants totaling $9,420,000 to Redwood Research for general support. Redwood Research is a new research institution that conducts research to better understand and make progress on AI alignment in order to reduce <a href=\"https://www.openphilanthropy.org/our-global-health-and-wellbeing-and-global-catastrophic-risks-grantmaking-portfolios/\"><u>global catastrophic risks</u></a>.</p><p>This falls within our focus area of&nbsp;<a href=\"https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/\"><u>potential risks from advanced artificial intelligence</u></a>.</p></blockquote>", "parentCommentId": "Q2nQfT3YuJiNGCmre", "user": {"username": "vascoamaralgrilo"}}, {"_id": "5wwztTxB26BDxEPLb", "postedAt": "2023-11-23T18:58:36.621Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Very good points made! One objection I think you didn\u2019t mention that might be on OP\u2019s mind in neartermist allocations has to do with population ethics. One reason many people are near termist is because they subscribe to a person-affecting view whereby the welfare of \u201cmerely potential\u201d beings does not matter. Since basically all animal welfare interventions either 1. Cause fewer animals to exist, or 2. Change welfare conditions for entire populations of animals, it seems extremely unlikely the animals who would otherwise have lived the higher suffering lives will have the same identity (eg same genes) as the higher welfare ones. To a person affecting view, this implies animal welfare interventions like corporate campaigns or alt protein investment merely change who or how many animals there are but don\u2019t benefit any animal in particular and thus have no impact on this moral view. I personally don\u2019t subscribe to this view, and I am not sure if most people at OP with a person affecting view have taken this idea seriously although it does seem like the right conclusion from this view.</p>\n", "parentCommentId": null, "user": {"username": "gagemweston@gmail.com"}}, {"_id": "3Gr2ADRa6BKrZxFep", "postedAt": "2023-11-23T19:12:05.500Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<blockquote><p>The \"EA movement\", however you define it, doesn't get to control the money and <a href=\"https://forum.effectivealtruism.org/posts/zuqpqqFoue5LyutTv/the-ea-community-does-not-own-its-donors-money\">there are good reasons for this</a>.</p></blockquote><p>I disagree, for the same reasons as those given in the <a href=\"https://forum.effectivealtruism.org/posts/zuqpqqFoue5LyutTv/the-ea-community-does-not-own-its-donors-money?commentId=HquctY3LJBt42fssh\">critique</a> to the post you cite. Tl;dr: Trades have happened in EA where many people have cast aside careers with high earning potential to work on object-level problems. I think these people should get a say over where EA money goes.</p>", "parentCommentId": "ze5ttKJdHjrpMrfbn", "user": {"username": "Will Aldred"}}, {"_id": "2XL536AqnNkp6JHDp", "postedAt": "2023-11-23T22:06:19.137Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<blockquote><p>One reason why we are moving more slowly is that our current estimates of the gap between marginal animal and human funding opportunities is very different from the one in your post \u2013 within one order of magnitude, not three. And given the high uncertainty around our estimates here, we think one order of magnitude is well within the \u201cmargin of error\u201d .</p></blockquote><p>I assume that even though your answers are within one order of magnitude, the animal-focused work is the one that looks more cost-effective. Is that right?</p><p>Assuming so, your answer doesn't make sense to me because OP funds <a href=\"https://forum.effectivealtruism.org/posts/btTeBHKGkmRyD5sFK/open-phil-should-allocate-most-neartermist-funding-to-animal?commentId=sJaZbGHLmjevZWBtg\">roughly 6x more</a> human-focused GHW relative to farm animal welfare (FAW). Even if you have wide uncertainty bounds, if FAW is looking more cost-effective than human work, surely this ratio should be closer to 1:1 rather than 1:6? It seems bizarre (and possibly an example of <a href=\"https://thedecisionlab.com/biases/omission-bias#:~:text=The%20omission%20bias%20refers%20to,they%20result%20in%20similar%20consequences.\">omission bias</a>) to fund the estimated less cost-effective thing 6x more and justify it by saying you're quite uncertain.&nbsp;</p><p>Long story short, should we not just allocate our funding to the best of our current knowledge (even by your calculations, more towards FAW) and then update accordingly if things change?</p>", "parentCommentId": "3uBrznQWEqs83PAfh", "user": {"username": "JamesOz"}}, {"_id": "4QqJmst2H4MdAyKvu", "postedAt": "2023-11-23T23:13:38.542Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Thanks Gage!</p><p>That's a good point I hadn't considered! I don't think that's OP's crux, but it is a coherent explanation of their neartermist cause prioritization.</p>", "parentCommentId": "5wwztTxB26BDxEPLb", "user": {"username": "Ariel Simnegar"}}, {"_id": "z8wjEfCQ8iHribZfn", "postedAt": "2023-11-24T00:33:48.988Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Here, you say, \u201cSeveral of the&nbsp;<u>grants we\u2019ve made to Rethink Priorities</u> funded research related to moral weights.\u201d Yet in your <a href=\"https://forum.effectivealtruism.org/posts/btTeBHKGkmRyD5sFK/open-phil-should-allocate-most-neartermist-funding-to-animal?commentId=3uBrznQWEqs83PAfh\">initial response</a>, you said, \u201cWe don\u2019t use Rethink\u2019s moral weights.\u201d I respect your tapping out of this discussion, but at the same time I\u2019d like to express my puzzlement as to why Open Phil would fund work on moral weights <a href=\"https://www.openphilanthropy.org/grants/rethink-priorities-moral-patienthood-and-moral-weight-research/\">to inform grantmaking allocation</a>, and then not take that work into account.</p>", "parentCommentId": "Sfx7ckshLyyyhGPBu", "user": {"username": "Will Aldred"}}, {"_id": "bPBz69frws9qapy3b", "postedAt": "2023-11-24T01:30:25.674Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Revealed preference is a good way to get a handle on what people value, but its normative foundation is strongest when the tradeoff is internal to people. Eg when we value lives vs income, we would want to use people's revealed preference for how they trade those off because those people are the most affected by our decisions and we want to incorporate their preferences. That normative foundation doesn't really apply to animal welfare where the trade-offs are between people and animals. You may as well use animals revealed preferences for saving humans (ie not at all) and conclude that humans have no worth; it would be nonsensical.</p>\n", "parentCommentId": "YeYDJmCdQwzFCFq7D", "user": {"username": "therealslimkt"}}, {"_id": "Grbp4vARqJhQv3Fkf", "postedAt": "2023-11-24T02:09:07.283Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Thanks for the compliment :)</p><p>When I write \"skepticism of formal philosophy\", I more precisely mean \"skepticism that philosophical principles can capture all of what's intuitively important\". Here's an example of skepticism of formal philosophy from Scott Alexander's <a href=\"https://www.astralcodexten.com/p/book-review-what-we-owe-the-future\">review</a> of What We Owe The Future:&nbsp;</p><blockquote><p>I\u2019m not sure I <i>want</i> to play the philosophy game. Maybe MacAskill can come up with some clever proof that the commitments I list above imply I have to have my eyes pecked out by angry seagulls or something. If that\u2019s true, I will just not do that, and switch to some other set of axioms. If I can\u2019t find any system of axioms that doesn\u2019t do something terrible when extended to infinity, I will just refuse to extend things to infinity...I realize this is \u201canti-intellectual\u201d and \u201cdefeating the entire point of philosophy\u201d.</p></blockquote><p>You make a good point regarding the relative niche-ness of animal welfare and AI x-risk. I agree that my post's analogy is crude and there are many reasons why people's dispositions might favor AI x-risk reduction over animal welfare.</p>", "parentCommentId": "uzZnDc5JoPqiwZuHD", "user": {"username": "Ariel Simnegar"}}, {"_id": "ENvZfLZ8ZHPoqAqsh", "postedAt": "2023-11-26T01:00:40.347Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Hi Emily, Sorry this is a bit off topic but super useful for my end of year donations.<br><br>I noticed that you said that OpenPhil has supported \"Rethink Priorities ... research related to moral weights\". But in his post here Peter says that the moral weights work \"have historically not had institutional support\".<br><br>Do you have a rough very quick sense of how much Rethink Priorities moral weights work was funded by OpenPhil?<br><br>Thank you so much&nbsp;</p>", "parentCommentId": "Sfx7ckshLyyyhGPBu", "user": {"username": "weeatquince"}}, {"_id": "mtCWuHcqbE996zorY", "postedAt": "2023-11-26T08:23:15.269Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Nice point, James!</p><p>I personally agree with your reasoning, but it assumes the marginal cost-effectiveness of the human-focussed and animal-focussed interventions should be the same. Open Phil is not sold on this:</p><blockquote><p>We\u2019re also unsure conceptually whether we should be trying to equalize marginal returns between FAW and GHW or whether we should continue with our current approach.</p></blockquote><p>I do not know what the \"current approach\" specifically involves, but it has led to Open Phil starting 6 new areas with a focus on human welfare in the last few years<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreff9ajfxo7zi\"><sup><a href=\"#fnf9ajfxo7zi\">[1]</a></sup></span>. So it naively seems to me like Open Phil could have done more to increase the amount of funding going to animal welfare if there was a desire to do so. These areas will not be turned off easily. If Open Phil was in the process of deliberating how much funding animal-focussed interventions should receive relative to human-focussed ones, I would have expected a bigger investment in growing the internal cause-prioritisation team, or greater funding of similar research elsewhere<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5bvj1gvcfv\"><sup><a href=\"#fn5bvj1gvcfv\">[2]</a></sup></span>.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnf9ajfxo7zi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreff9ajfxo7zi\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.openphilanthropy.org/focus/south-asian-air-quality/\"><u>South Asian air quality</u></a>, <a href=\"https://www.openphilanthropy.org/focus/global-aid-policy/\"><u>global aid policy</u></a>, <a href=\"https://www.openphilanthropy.org/focus/innovation-policy/\"><u>innovation policy</u></a>, <a href=\"https://www.openphilanthropy.org/focus/ea-global-health-and-wellbeing/\"><u>effective altruism with a GHW focus</u></a>, <a href=\"https://www.openphilanthropy.org/focus/global-health-rd/\"><u>global health R&amp;D</u></a><u>, and </u><a href=\"https://www.openphilanthropy.org/focus/global-public-health-policy/\"><u>global public health policy</u></a><u>.</u></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5bvj1gvcfv\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5bvj1gvcfv\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Open Phil has made grants to support Rethink's <a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw\">moral weight project</a>, but this type of work <a href=\"https://forum.effectivealtruism.org/posts/cMcEBSNiy4meDrmuE/rethink-priorities-needs-your-support-here-s-what-we-d-do\">has</a> apparently not been fully supported by Open Phil:</p><blockquote><p><strong>What we think of as our most innovative work&nbsp;</strong>(e.g.,&nbsp;<a href=\"https://rethinkpriorities.org/invertebrate-sentience-table\"><u>invertebrate sentience</u></a>,&nbsp;<a href=\"https://rethinkpriorities.org/publications/the-welfare-range-table\"><u>moral weights and welfare ranges</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/pniDWyjc9vY5sjGre/rethink-priorities-cross-cause-cost-effectiveness-model\"><u>the cross-cause model</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/s/WdL3LE5LHvTwWmyqj\"><u>the CURVE sequence</u></a>) <strong>or some of our most important work</strong>&nbsp;(e.g.,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/SqCDoL9cHa7bnEWyb/announcing-ea-survey-2022\"><u>50% of the EA Survey</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/cMcEBSNiy4meDrmuE/rethink-priorities-needs-your-support-here-s-what-we-d-do#fnm5b5cb4amzn\"><sup>[4]</sup></a>, our&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/g5uKzBLjiEuC5k46A/ftx-community-response-survey-results\"><u>EA FTX surveys</u></a> and Public/Elite FTX surveys, and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ConFiY9cRmg37fs2p/us-public-opinion-of-ai-policy-and-risk\"><u>each</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Rg7h7G3KTvaYEtL55/us-public-perception-of-cais-statement-and-the-risk-of\"><u>of</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/RYNtykh5xM467zRNj/why-some-people-disagree-with-the-cais-statement-on-ai\"><u>our</u></a> three AI surveys)<strong> have historically not had institutional support and relied on individual donors to make happen.</strong></p></blockquote></div></li></ol>", "parentCommentId": "2XL536AqnNkp6JHDp", "user": {"username": "vascoamaralgrilo"}}, {"_id": "dr33BwNPN3bHriGEt", "postedAt": "2023-11-26T08:53:13.962Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Thanks for elaborating, Michael! Readers might want to check <a href=\"https://forum.effectivealtruism.org/posts/vBcT7i7AkNJ6u9BcQ/prioritising-animal-welfare-over-global-health-and#Effects_of_global_health_and_development_interventions_on_animals_are_neglected_and_unclear\">these</a> BOTECs on the meat-eater problem.</p><blockquote><p>These results suggest accounting for poultry does not matter much for GHD interventions [in the countries targetted by GW's to charities]. Among the countries targeted by GW\u2019s top charities, the relative reduction in the cost-effectiveness of saving lives ranges from 0.0307 % for the Democratic Republic of Congo to 9.71 % for South Sudan.</p><p>Nevertheless, I believe the results above underestimate the reduction in cost-effectiveness, because:</p><ul><li>I have not accounted for other farmed animals. From my estimates <a href=\"https://forum.effectivealtruism.org/posts/ayxasxhHWTvf6r5BF/scale-of-the-welfare-of-various-animal-populations\"><u>here</u></a>, the negative utility of farmed chickens is only 14.5 % (= 1.74/12.0) of that of all farmed animals globally. This suggests accounting for all farmed animals would lead to a reduction in cost-effectiveness for the mean country of 22.4 % (= 3.24/14.5), which is not negligible. So accounting for the effects of GHD interventions on farmed animals may lead to targeting different countries.</li><li>I have used the current consumption of poultry per capita, but this, as well as that of other farmed animals, <a href=\"https://ourworldindata.org/grapher/animal-protein-vs-gdp\">will</a> tend to increase with economic growth. I <a href=\"https://forum.effectivealtruism.org/posts/ayxasxhHWTvf6r5BF/scale-of-the-welfare-of-various-animal-populations\">estimated</a> the badness of the experiences of all farmed animals alive is 12.1 times the goodness of the experiences of all humans alive, which suggests saving a random human life results in a nearterm increase in suffering.</li></ul><p>[...]</p><p>All in all, I can see the impact on wild animals being anything from negligible to all that matters in the nearterm. So, as for farmed animals, I think more research is needed. For example, on forecasting net change in forest area in low-income countries.</p></blockquote><p>I tend to agree with Michael that the meat-eater problem is currently not a major concern in low-income countries, but that it will tend to become so in the next few decades, such that I would not be surprised if saving lives increased net suffering. It is also worth noting that the people targeted by Open Phil's new GHW areas<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefeqa52csxaqm\"><sup><a href=\"#fneqa52csxaqm\">[1]</a></sup></span>&nbsp;may have greater consumption per capita of farmed animals with bad lives relative to that in the countries targeted by GiveWell, such that the meat-eater problem is more problematic.</p><p>Personally, I also <a href=\"https://forum.effectivealtruism.org/posts/gktZ8zuzyh7HEgjfc/famine-deaths-due-to-the-climatic-effects-of-nuclear-war#Nearterm_perspective1\">worry</a> about the meat-eater problem in the context of global catastrophic risks. In my mind, if the catastrophe is sufficiently severe, saving humans will have a positive longterm effect which outweights the potential suffering inflicted to animals. However, for small catastrophes, I am open to arguments that saving humans has a negligible longterm effect, and may well increase net suffering due to greater consumption of farmed animals with bad lives linked to the saved humans.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fneqa52csxaqm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefeqa52csxaqm\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.openphilanthropy.org/focus/south-asian-air-quality/\"><u>South Asian air quality</u></a>, <a href=\"https://www.openphilanthropy.org/focus/global-aid-policy/\"><u>global aid policy</u></a>, <a href=\"https://www.openphilanthropy.org/focus/innovation-policy/\"><u>innovation policy</u></a>, <a href=\"https://www.openphilanthropy.org/focus/global-health-rd/\"><u>global health R&amp;D</u></a><u>, and </u><a href=\"https://www.openphilanthropy.org/focus/global-public-health-policy/\"><u>global public health policy</u></a><u>.</u></p></div></li></ol>", "parentCommentId": "ocxCq3gEavuEqzYbf", "user": {"username": "vascoamaralgrilo"}}, {"_id": "EDop6vehoXjeLZiDB", "postedAt": "2023-11-26T17:21:46.036Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>It really is a nice point, Gage! Like Ariel, I also guess it is not driving OP's neartermist prioritisation, as OP has funded lots of longtermist work, and this is also significantly less valuable under person-affecting views (unless OP thinks most of the benefits of longtermist interventions respect reducing deaths of people currently alive).</p>", "parentCommentId": "4QqJmst2H4MdAyKvu", "user": {"username": "vascoamaralgrilo"}}, {"_id": "DxJ6up9CZBan7YkPA", "postedAt": "2023-11-26T18:45:42.901Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>One can value research and find it informative or worth doing without being convinced of every view of a given researcher or team. &nbsp;Open Philanthropy also sponsored a contest to surface novel considerations that could affect its views on AI timelines and risk. The winners mostly present conclusions or considerations on which AI would be a lower priority, but that doesn't imply that <a href=\"https://forum.effectivealtruism.org/posts/eSZuJcLGd7BacjWGi/announcing-the-winners-of-the-2023-open-philanthropy-ai?commentId=NYjeK7mGkaKE9uxHK\">the judges or the institution changed their views very much</a> in that direction.<br><br>At large scale, Information can be valuable enough to buy even if it only modestly adjusts proportional allocations of effort, the minimum bar for funding a research project with hundreds of thousands or millions of dollars presumably isn't that one pivots billions of dollars on the results with near-certainty.</p>", "parentCommentId": "z8wjEfCQ8iHribZfn", "user": {"username": "CarlShulman"}}, {"_id": "e9cx99cp9evchWHjR", "postedAt": "2023-11-27T07:28:47.605Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Ariel, thank you for taking the time to put this together. It's encouraging to see both constructive and meaningful conversations unfolding around a topic that I believe is essential if we're to see a shift in both OP and EA's FAW funding priorities.&nbsp;</p><p>Most points I had in mind have been covered by others in this thread already\u2014 but I wanted to extend my support either way.</p>", "parentCommentId": null, "user": {"username": "davidvanbeveren"}}, {"_id": "y4WjZmpvmXMzFrvkM", "postedAt": "2023-11-27T13:26:19.654Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Thanks so much David! :)</p>\n", "parentCommentId": "e9cx99cp9evchWHjR", "user": {"username": "Ariel Simnegar"}}, {"_id": "yNMkhQ75c6ejuYKYS", "postedAt": "2023-11-27T14:35:21.051Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>We mean to say that the ideas for these projects and the vast majority of the funding were ours, including the moral weight work. To be clear, these projects were the result of our own initiative. They wouldn't have gone ahead when they did without us insisting on their value.</p>\n<p>For example, after our initial work on invertebrate sentience and moral weight in 2018-2020, in 2021 <a href=\"https://www.openphilanthropy.org/grants/rethink-priorities-moral-patienthood-and-moral-weight-research/\">OP funded $315K to support this work</a>. In 2023 they also funded <a href=\"https://www.openphilanthropy.org/grants/rethink-priorities-moral-weight-book-rights/\">$15K for the open access book rights</a> to a forthcoming book based on the topic. In that period of 2021-2023, for public-facing work we spent another ~$603K on moral weight work with that money coming from individuals and RP's unrestricted funding.</p>\n<p>Similarly, the CURVE sequence of WIT this year was our idea and we are on track to spend ~$900K against ~$210K funded by Open Phil on WIT. Of that $210K the first $152K was on projects related to Open Phil\u2019s internal prioritization and not the public work of the CURVE sequence. The other $58K went towards the development of the CCM. So overall less than 10% of our costs for public WIT work this year was covered by OP (and no other institutional donors were covering it either).</p>\n", "parentCommentId": "ENvZfLZ8ZHPoqAqsh", "user": {"username": "Marcus_A_Davis"}}, {"_id": "eHFGsAurhJHxLoX9b", "postedAt": "2023-11-27T23:56:52.070Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Thank you for engaging. I don\u2019t disagree with what you\u2019ve written; I think you have interpreted me as implying something stronger than what I intended, and so I\u2019ll now attempt to add some colour.</p><p>That Emily and other relevant people at OP have not fully adopted Rethink\u2019s moral weights does not puzzle me. As you say, to expect that is to apply an unreasonably high funding bar. I am, however, puzzled that Emily and co. appear to have not updated&nbsp;<i>at all</i> towards Rethink\u2019s numbers. At least, that\u2019s the way I read:</p><blockquote><ul><li>We don\u2019t use Rethink\u2019s moral weights.<ul><li>Our current moral weights, based in part on&nbsp;<a href=\"https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood/\"><u>Luke Muehlhauser\u2019s past work</u></a>, are lower. We may update them in the future; if we do, we\u2019ll consider work from many sources, including the arguments made in this post.</li></ul></li></ul></blockquote><p>If OP has not updated at all towards Rethink\u2019s numbers, then I see three possible explanations, all of which I find unlikely, hence my puzzlement. First possibility: the relevant people at OP have not yet given the&nbsp;Rethink <a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw/p/Qk3hd6PrFManj8K6o\"><u>report</u></a> a thorough read, and have therefore not updated. Second: the relevant OP people have read the Rethink report, and have updated their internal models, but have not yet gotten around to updating OP\u2019s actual grantmaking allocation. Third: OP believes the Rethink work is low quality or otherwise critically corrupted by one or more errors. I\u2019d be very surprised if one or two are true, given how moral weight is arguably the most important consideration in neartermist grantmaking allocation. I\u2019d also be surprised if three is true, given how well Rethink\u2019s moral weight&nbsp;<a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw\"><u>sequence</u></a> has been received on this forum (see, e.g., comments&nbsp;<a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw/p/tnSg6o7crcHFLc395#comments\"><u>here</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw/p/Qk3hd6PrFManj8K6o#comments\"><u>here</u></a>).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvpqr7g6wwom\"><sup><a href=\"#fnvpqr7g6wwom\">[1]</a></sup></span>&nbsp;OP people may disagree with Rethink\u2019s approach at the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/2WS3i7eY4CdLH99eg/independent-impressions\"><u>independent impression</u></a> level, but surely, given Rethink\u2019s moral weights work is the most extensive work done on this topic by anyone(?), the Rethink results should be given substantial weight\u2014or at least non-trivial weight\u2014in their all-things-considered views?</p><p>(If OP people believe there are errors in the Rethink work that render the results ~useless, then, considering the topic\u2019s importance, I think some sort of OP write-up would be well worth the time. Both at the object level, so that future moral weight researchers can avoid making similar mistakes, and to allow the community to hold OP\u2019s reasoning to a high standard, and also at the meta level, so that&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/cMcEBSNiy4meDrmuE/rethink-priorities-needs-your-support-here-s-what-we-d-do\"><u>potential donors</u></a> can update appropriately re. Rethink\u2019s general quality of work.)</p><p>Additionally\u2014and this is less important, I\u2019m puzzled at the meta level at the way we\u2019ve arrived here. As noted in the top-level post, Open Phil has been less than wholly open about its grantmaking, and it\u2019s taken a pretty not-on-the-default-path sequence of events\u2014Ariel, someone who\u2019s not affiliated with OP and who doesn\u2019t work on animal welfare for their day job, writing this big post; Emily from OP replying to the post and to a couple of the comments; me, a Forum-goer who doesn\u2019t work on animal welfare, spotting an inconsistency in Emily\u2019s replies\u2014to surface the fact that OP does not give Rethink\u2019s moral weights any weight.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvpqr7g6wwom\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvpqr7g6wwom\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Edited to add: Carl has left a detailed reply <a href=\"https://forum.effectivealtruism.org/posts/btTeBHKGkmRyD5sFK/open-phil-should-allocate-most-neartermist-funding-to-animal?commentId=nt3uP3TxRAhBWSfkW\">below</a>, and it seems that three is, in fact, what has happened.</p></div></li></ol>", "parentCommentId": "DxJ6up9CZBan7YkPA", "user": {"username": "Will Aldred"}}, {"_id": "bHxofLaGrjjDv4ePi", "postedAt": "2023-11-28T00:06:54.621Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Hi Marcus thanks very helpful to get some numbers and clarification on this. And well done to you and Rethink for driving forward such important research.</p>\n<p>(I meant to post a similar question asking for clarification on the rethink post too but my perfectionism ran away with me and I never quite found the wording and then ran out of drafting time, but great to see your reply here)</p>\n", "parentCommentId": "yNMkhQ75c6ejuYKYS", "user": {"username": "weeatquince"}}, {"_id": "hu44B9HJxTjxhFiQQ", "postedAt": "2023-11-28T07:08:55.792Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Fair points, Carl. Thanks for elaborating, Will!</p><blockquote><ul><li>We don\u2019t use Rethink\u2019s moral weights.<ul><li>Our current moral weights, based in part on&nbsp;<a href=\"https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood/\"><u>Luke Muehlhauser\u2019s past work</u></a>, are lower. We may update them in the future; if we do, we\u2019ll consider work from many sources, including the arguments made in this post.</li></ul></li></ul></blockquote><p>Interestingly and confusingly, fitting distributions to Luke's 2018 <a href=\"https://www.lesswrong.com/posts/2jTQTxYNwo6zb3Kyp/preliminary-thoughts-on-moral-weight\">guesses</a> for the 80 % prediction intervals of the moral weight of various species, one <a href=\"https://forum.effectivealtruism.org/posts/sPk86847CKeJYgJ9H/moral-weights-for-various-species-and-distributions\">gets</a> mean moral weights close to or larger than 1:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hu44B9HJxTjxhFiQQ/wmipixkm551adrdmwyqw\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hu44B9HJxTjxhFiQQ/ki7zdoywr9mngmiskqiu 260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hu44B9HJxTjxhFiQQ/sxdqwr3etae554ha1s4o 520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hu44B9HJxTjxhFiQQ/gs98h7etfk8i4kqugwmr 780w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hu44B9HJxTjxhFiQQ/d26reloqrozyopnsy5tf 1040w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hu44B9HJxTjxhFiQQ/cqrvhjzhimqgvntwj7tp 1300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hu44B9HJxTjxhFiQQ/y4jbe9cn9lku6ypfzb1v 1560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hu44B9HJxTjxhFiQQ/bffp1zezci9liwljprlp 1820w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hu44B9HJxTjxhFiQQ/sbdpcqqdn9agns7xmsun 2080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hu44B9HJxTjxhFiQQ/kjwxw1y0nukxuipaepfb 2340w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hu44B9HJxTjxhFiQQ/jypvgeziwntkuej4bmpd 2565w\"></figure><p>It is also worth noting that Luke seemed very much willing to update on further research in 2022. Commenting on the above, Luke <a href=\"https://forum.effectivealtruism.org/posts/sPk86847CKeJYgJ9H/moral-weights-for-various-species-and-distributions?commentId=yt27FvZcEKSp3iRXP\">said</a> (emphasis mine):</p><blockquote><p>Since this exercise is based on numbers I personally made up, I would like to remind everyone that those numbers are extremely made up and come with many caveats given in the original sources. <strong>It would not be that hard to produce numbers more reasonable than mine, at least re: moral weights.</strong> (I spent more time on the \"probability of consciousness\" numbers, though that was years ago and my numbers would probably be different now.)</p></blockquote><p>Welfare ranges are a crucial input to determining moral weights, so I assume Luke would also have agreed that it would not have been that hard to produce more reasonable welfare ranges than his and Open Phil's in 2022. So, given how little time Open Phil seemingly devoted to assessing welfare ranges in comparison to Rethink, I would have expected Open Phil to give major weight to Rethink's values.</p>", "parentCommentId": "eHFGsAurhJHxLoX9b", "user": {"username": "vascoamaralgrilo"}}, {"_id": "o6eKyyYdyKqxvH66d", "postedAt": "2023-11-28T19:08:02.019Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<blockquote><p>I donated a significant part of my personal runway to help fund a new animal welfare org, which I think counterfactually might not have gotten started if not for this.</p></blockquote><p>&lt;3 This is super awesome / inspirational, and I admire you for doing this!</p>", "parentCommentId": "ZYnDzLmnqzDy6yuxo", "user": {"username": "Angelina Li"}}, {"_id": "8mRAjwt2Mm5bH53iw", "postedAt": "2023-11-28T19:12:18.075Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<blockquote><p>The upshot: I have no idea whether there are good ways of spending an additional $100M on animals right now.</p></blockquote><p>I haven't read this in a ton of detail, but I liked this <a href=\"https://forum.effectivealtruism.org/posts/gGSQrbNJJCxiMNog3/megaprojects-for-animals\">post</a> from last year trying to answer this exact question (what are potentially effective ways to deploy &gt;$10M in projects for animals).</p>", "parentCommentId": "wmKfvakfrbzdNnEk9", "user": {"username": "Angelina Li"}}, {"_id": "e4Fbro3dHk3cdEdeo", "postedAt": "2023-11-30T23:54:23.238Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Generally, people with person-affecting views still want it to be the case that we shouldn't create individuals with awful lives, and probably also that we should prefer the creation of someone with a life that is net-negative by less over someone with a life that is net-negative by more. (This relates to the supposed procreation asymmetry, where, allegedly, that a kid would be really happy is not a reason to have them, but that a kid would be in constant agony is a reason not to have them.) One way to justify this would be the thought that, if you don't create a happy person, no one has a complaint, but if you do create a miserable person, someone does have a complaint (i.e., that person).<br><br>Where factory-farmed animals have net-negative lives, I'm not sure person-affecting views would justify neglecting animal welfare, then. (Similarly, re: longtermism, they might justify neglecting long-term x-risks, but not s-risks.)</p>", "parentCommentId": "5wwztTxB26BDxEPLb", "user": {"username": "Dustin Crummett"}}, {"_id": "Wkq3yJJoRQookj53J", "postedAt": "2023-12-01T18:01:18.714Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>I agree many people believe in the asymmetry, and that is likely one reason people care about animal welfare but not longtermism. However, I think you're conflating a person-affecting view with the asymmetry, which are separate views. I hate to argue semantics here, but the person-affecting view only is concerned only with the welfare of existing beings, not with the creation of negative lives no matter how bad they are. Again, neither of these are my view, but they likely belong to some people.</p>", "parentCommentId": "e4Fbro3dHk3cdEdeo", "user": {"username": "gagemweston@gmail.com"}}, {"_id": "esSa4SdsHEKZhKf5C", "postedAt": "2023-12-01T20:31:29.051Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>They are separate views, but related: people with person-affecting views usually endorse the asymmetry, people without person-affecting views usually don't endorse the asymmetry, and person-affecting views are often taken to (somehow or other) provide a kind of justification for the asymmetry. The upshot here is that it wouldn't be <i>enough</i> for people at OP to endorse person-affecting views: they'd have to endorse a <i>version </i>of a person-affecting view that is rejected even by most people with person-affecting views, and that independently seems gonzo--one according to which, say, I have no reason at all not to push a button that creates a trillion people who are gratuitously tortured in hell forever.<br><br>Very roughly, how this works: person-affecting views say that a situation can't be better or worse than another unless it benefits or harms someone. (Note that the usual assumption here is that, to be harmed or benefited, the individual doesn't have to exist <i>now</i>, but they have to exist at <i>some point</i>.) This is completely compatible with thinking it's worse to create the trillion people who suffer forever: it might be that their existing is worse for them than not existing, or harms them in some non-comparative way. So it can be worse to create them, since it's worse for them. And that should also be enough to get the view that, e.g., you shouldn't create animals with awful lives on factory farms.<br><br>Of course, usually people with person-affecting views want it to be neutral to create happy people, and then there is a problem about how to maintain that while accepting the above view about not creating people in hell. So somehow or other they'll need to justify the asymmetry. One way to try this might be via the kind of asymmetrical complaint-based model I mentioned above: if you create the people in hell, there are actual individuals you harm (the people in hell), but if you don't create people in heaven, there is no actual individual you fail to benefit (since the potential beneficiaries never exist). In this way, you might try to fit the views together. Then you would have the view that it's neutral to ensure the awesome existence of future people who populate the cosmos, but still important to avoid creating animals with net-negative lives, or future people who get tortured by AM or whatever.<br><br>Now, it is true that people with person-affecting views could instead say that there is nothing good or bad about creating individuals either way--maybe because they think there's just no way to compare existence and non-existence, and they think this means there's no way to say that causing someone to exist benefits or harms them. But this is a fringe view, because, e.g., it leads to gonzo conclusions like thinking there's no reason not to push the hell button.<br><br>I think all this is basically in line with how these views are understood in the academic literature, cf., e.g., <a href=\"https://utilitarianism.net/population-ethics/#person-affecting-views-and-the-procreative-asymmetry\">here</a>.</p>", "parentCommentId": "Wkq3yJJoRQookj53J", "user": {"username": "Dustin Crummett"}}, {"_id": "myhrs5GxRwiDDfx4Y", "postedAt": "2023-12-01T20:34:00.189Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>There are multiple views considered \"person-affecting views\", and I think the asymmetry (or specific asymmetric views) is often considered one of them. What you're describing is a specific narrow/strict person-affecting restriction, also called <i>presentism</i>. I think it has been called <i>the</i> person-affecting view or <i>the</i> person-affecting restriction, which is of course confusing if there are multiple views people consider person-affecting. The use of \"person-affecting\" may have expanded over time.</p>", "parentCommentId": "Wkq3yJJoRQookj53J", "user": {"username": "MichaelStJules"}}, {"_id": "xnuEPbgdAHudWPBci", "postedAt": "2023-12-03T21:26:31.101Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>It is unclear in the first figure whether to compare the circles by area or diameter. I believe the default impression is to compare area, which I think is not what was intended and so is misleading.</p>\n", "parentCommentId": null, "user": {"username": "Alex Mallen"}}, {"_id": "pwFTBtqGcMiic3QZH", "postedAt": "2023-12-03T22:06:23.408Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Comparing area was intended :)</p><p>If it's unclear, I can add a note which says the circles should be compared by area.</p>", "parentCommentId": "xnuEPbgdAHudWPBci", "user": {"username": "Ariel Simnegar"}}, {"_id": "uEbuemoXTiFmJfrQ5", "postedAt": "2023-12-03T22:12:42.123Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Thanks!</p>", "parentCommentId": "pwFTBtqGcMiic3QZH", "user": {"username": "Alex Mallen"}}, {"_id": "bpBfke7sYzBupm89m", "postedAt": "2023-12-04T17:35:18.360Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Hi Ariel,</p><p>Not strictly related to this post, but just in case you need ideas for further posts ;), here are some very quick thoughts on 80,000 Hours.</p><p>I wonder whether 80,000 Hours should <a href=\"https://80000hours.org/problem-profiles/\">present</a> \"factory-farming\" and \"easily preventable human [human] diseases\" as having the same level of pressingness.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/uvgijkha2mrsel9bbaek\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/dlod23ikgexqerfajty6 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/kkmhjm2sfzmpn3zrdaj5 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/fg8chy71os1iasb2li5k 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/sxbvaxd6ktom9ny3al2h 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/jucfvushxtlqi0glt83g 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/yg53km8a7zlcshvtl6tg 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/ohhp6rhi0e5fhwifllba 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/bxxlbmzorddo8dr1rird 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/e1toq8yyc52rvdkanyqh 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/l6dnigzkhrzabtbsjbzy 1929w\"></figure><p>80,000 Hours' thinking the above have similar pressingness is probably in agreement with a <a href=\"https://80000hours.org/articles/cause-selection/\">list</a> they did in 2017, when factory-farming came out 2 points above (<a href=\"https://80000hours.org/articles/problem-framework/\">i.e.</a> 10 times as pressing as) developing world health.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/ftbsh2fzn5g82kfoaprl\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/xfvkwgud5phcauw0nprs 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/iedznafxgvsre9a1kgvp 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/unriwexvuvsca7byrpuy 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/rsaf9oqv2cjkaouly7cz 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/gdlkgl5xfught4pk0xnr 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/bqqpubptcluli9aqwinm 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/yxlipzok92ovamxxyxz8 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/dfhjgfrydwcrucflb0oq 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/fzhlupoqmaviv1bz53q9 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bpBfke7sYzBupm89m/zucra7epmswogqnt6dur 2094w\"></figure><p>It is also interesting that 3 of 80,000 Hours' current top 5 most pressing problems came out as similarly pressing or less pressing than factory-farming. More broadly, it would be nice if 80,000 Hours were more transparent about how their rankings of <a href=\"https://80000hours.org/problem-profiles/\">problems</a> and <a href=\"https://80000hours.org/career-reviews/\">careers</a> are produced, as I guess these have a significant impact on shaping the career choices of many people. I will post one question about this on the EA Forum in a few weeks.</p>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "sJ3jXKWTwPwJBvSNi", "postedAt": "2023-12-04T18:14:02.857Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>One thought is that it may be a mistake to categorize GHD work as purely \"neartermist\". As Nick Beckstead flagged in his dissertation, the strongest reason for favoring GHD over animal welfare is that the former, by increasing overall human capacity, seems more likely to have positive \"ripple effects\" beyond the immediate beneficiaries.</p><p>One may object that GHD has lower expected value than explicitly longtermist work. But GHD may be more <i>robustly</i> good, with less risk of proving long-term counterproductive. So it may help to think of the GHD component of Worldview Diversification as stemming from a concern for <i>robustness</i>, rather than a concern for the <i>nearterm</i> per se.</p>", "parentCommentId": null, "user": {"username": "RYC"}}, {"_id": "FGFhBqvYPD7GA8W4F", "postedAt": "2023-12-05T10:14:34.790Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Admittedly, we weren't factoring in the (ostensible) ripple effects, but <a href=\"https://forum.effectivealtruism.org/s/WdL3LE5LHvTwWmyqj/p/9EENSGhiQiKFaRh4t\">our modeling indicates</a> that if we're interested in robust goodness, we should be spending on chickens.</p><p>Also, for the reasons that <a href=\"https://forum.effectivealtruism.org/users/ariel-simnegar?mention=user\">@Ariel Simnegar</a> already notes, even if there are unappreciated benefits of investing in GHD, there would need to be <i>a lot </i>of those benefits to justify not spending on animals. Could work out that way, but I'd like to see the evidence. (When I investigated this myself, making the case seemed quite difficult.)</p>", "parentCommentId": "sJ3jXKWTwPwJBvSNi", "user": {"username": "bob-fischer"}}, {"_id": "WDLBDG8LrErDntJPg", "postedAt": "2023-12-05T15:51:26.482Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>That's all assuming neartermism, right? &nbsp;I agree that neartermism plausibly entails prioritizing non-human animals. But neartermism seems very arbitrary, and should plausibly not receive nearly as much weight as GHD currently receives in OP's portfolio.*</p><p>Rather, my suggestion was that the current explicitly \"longtermist\" bucket should be thought of as something like \"highly speculative longtermism\", and the current GHD bucket should be thought of as something like \"improving the long-term via robustly good methods\".</p><p>It's harder to see how helping animals would fit that latter description. (Maybe the best case would be the classic Kantian argument that abusing animals less would help make <i>us</i> morally better.)</p><p>*: Insofar as the GHD bucket is really just motivated by something like \"sticking close to common sense\", and people started using \"neartermism\" as a slightly misnamed label for this, the current proposal to shift to <i>very counterintuitive priorities</i> whilst<i> doubling down on the unprincipled rejection of longtermism</i> seems kind of puzzling to me!</p><p>We shouldn't just assume that <i>neartermism </i>is the principled alternative to speculative longtermism, and then stick to that assumption come what may (i.e., even if it leads to the result that we should \"mostly\" ignore poor people). Rather, I think we should be more open-minded about how we should think of the different \"buckets\" in a Worldview-Diversified portfolio, and cautious of <i>completely</i> dismissing common-sense priorities (even as we give <i>significant</i> weight and support to a range of theoretically well-supported counterintuitive cause areas).</p>", "parentCommentId": "FGFhBqvYPD7GA8W4F", "user": {"username": "RYC"}}, {"_id": "ksZoMFTeGYmaPPrdx", "postedAt": "2023-12-05T15:56:07.820Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Nope, not assuming neartermism. The report has the details. Short version: across a range of decision theories, chickens look really good.</p><p>That said, I totally<i> </i>agree that from a purely conceptual perspective, we should \"be more open-minded about how we should think of the different 'buckets' in a Worldview-Diversified portfolio, and cautious of <i>completely</i> dismissing common-sense priorities (even as we give <i>significant</i> weight and support to a range of theoretically well-supported counterintuitive cause areas).\"&nbsp;</p>", "parentCommentId": "WDLBDG8LrErDntJPg", "user": {"username": "bob-fischer"}}, {"_id": "azWQpTNMSz8BnYa4L", "postedAt": "2023-12-05T16:19:13.166Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Ok, thanks for clarifying!<br><br>Edit to add: I think you might mean something different by \"robust goodness\" than what I had in mind. From a quick look at your link, you're considering a range of different decision theories, risk-weightings, etc., and noting that chickens do at least moderately well on a wide range of theoretical assumptions.</p><p>I instead meant to be talking about <i>empirical</i> robustness: roughly, \"helping the long-term via methods that are especially likely to do <i>some</i> immediate good, and with less risk of proving long-term counterproductive.\" Or, more concisely, \"longtermism via nearterm goods with positive ripple effects\". And then assessing what does best via this <i>particular</i> theoretical standard (to make up one bucket in our portfolio).</p><p>Since your report doesn't consider ripple effects, it doesn't address the kind of \"robust longtermism\" bucket I have in mind.</p>", "parentCommentId": "ksZoMFTeGYmaPPrdx", "user": {"username": "RYC"}}, {"_id": "nt3uP3TxRAhBWSfkW", "postedAt": "2023-12-06T14:43:27.284Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>I can't speak for Open Philanthropy, but I can explain why I personally was unmoved by the Rethink report (and think its estimates hugely overstate the case for focusing on tiny animals, although I think the corrected version of that case still has a lot to be said for it).<br>&nbsp;<br>Luke says in the post you linked that the numbers in the graphic are not usable as expected moral weights, since ratios of expectations are not the same as expectations of ratios.<br>&nbsp;</p><blockquote><p>However, I say \"naively\" because <i>this doesn't actually work</i>, due to <a href=\"https://reducing-suffering.org/two-envelopes-problem-for-brain-size-and-moral-uncertainty/\">two-envelope effects</a>...whenever you're tempted to multiply such numbers by something, remember <a href=\"https://reducing-suffering.org/two-envelopes-problem-for-brain-size-and-moral-uncertainty/\">two-envelope effects</a>!)</p></blockquote><p>[Edited for clarity] I was not satisfied with Rethink's attempt to address that central issue, that you get wildly different results from assuming the moral value of a fruit fly is fixed and reporting possible ratios to elephant welfare as opposed to doing it the other way around.&nbsp;<br><br>It is not unthinkably improbable that an elephant brain where reinforcement from a positive or negative stimulus adjust millions of times as many neural computations could be seen as vastly more morally important than a fruit fly, just as one might think that a fruit fly is much more important than a thermostat (which some suggest is conscious and possesses preferences). Since on some major functional aspects of mind there are differences of millions of times, that suggests a mean expected value orders of magnitude higher for the elephant if you put a bit of weight on the possibility that moral weight scales with the extent of, e.g. the computations that are adjusted by positive and negative stimuli. A 1% weight on that plausible hypothesis means the expected value of the elephant is immense vs the fruit fly. So there will be something that might get lumped in with 'overwhelming hierarchicalism' in the language of the top-level post. Rethink's <a href=\"https://rethinkpriorities.org/publications/why-neuron-counts-shouldnt-be-used-as-proxies-for-moral-weight\">various</a> <a href=\"https://forum.effectivealtruism.org/posts/vbhoFsyQmrntru6Kw/do-brains-contain-many-conscious-subsystems-if-so-should-we\">discussions of this</a> issue in my view missed the mark.<br><br>Go the other way and fix the value of the elephant at 1, and the possibility that value scales with those computations is treated as a case where the fly is worth ~0. Then a 1% or even 99% credence in value scaling with computation has little effect, and the elephant-fruit fly ratio is forced to be quite high so tiny mind dominance is almost automatic. The same argument can then be used to make a like case for total dominance of thermostat-like programs, or individual neurons, over insects. And then again for individual <a href=\"https://reducing-suffering.org/is-there-suffering-in-fundamental-physics/\">electrons</a>.&nbsp;<br><br>As I see it, Rethink <a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw/p/Qk3hd6PrFManj8K6o\">basically went with the 'ratios to fixed human value'</a>, so from my perspective their bottom-line conclusions were predetermined and uninformative. But the alternatives they ignore lead me to think that the expected value of welfare for big minds is a lot larger than for small minds (and I think that can continue, e.g. <a href=\"https://nickbostrom.com/papers/digital-minds.pdf\">giant AI minds</a> with vastly more reinforcement-affected computations and thoughts could possess much more expected welfare than humans, as many humans might have more welfare than one human).<br><br>I agree with Brian Tomasik's comment from your link:</p><blockquote><p>the moral-uncertainty version of the [two envelopes] problem is fatal unless you make further assumptions about how to resolve it, such as by fixing some arbitrary intertheoretic-comparison weights (which seems to be what you're suggesting) or using the parliamentary model.</p></blockquote><p>By the same token, arguments about the number of possible connections/counterfactual richness in a mind could suggest superlinear growth in moral importance with computational scale. Similar issues would arise for theories involving moral agency or capacity for cooperation/game theory (on which humans might stand out by orders of magnitude relative to elephants; marginal cases being socially derivative), but those were ruled out of bounds for the report. Likewise it chose not to address intertheoretic comparisons and how those could very sharply affect the conclusions. Those are the kinds of issues with the potential to drive massive weight differences.<br><br>I think some readers benefitted a lot from reading the report because they did not know that, e.g. insects are capable of reward learning and similar psychological capacities. &nbsp;And I would guess that will change some people's prioritization between different animals, and of animal vs human focused work. I think that is valuable. But that information was not new to me, and indeed I had argued for many years that insects met a lot of the functional standards one could use to identify the presence of well-being, and that even after taking two-envelopes issues and nervous system scale into account expected welfare at stake for small wild animals looked much larger than for FAW.&nbsp;<br><br>I happen to be a fan of animal welfare work relative to GHW's other grants at the margin because animal welfare work is so highly neglected (e.g. Open Philanthropy is a huge share of world funding on the most effective FAW work but quite small compared to global aid) relative to the case for views on which it's great. But for me Rethink's work didn't address the most important questions, and largely baked in its conclusions methodologically.</p>", "parentCommentId": "hu44B9HJxTjxhFiQQ", "user": {"username": "CarlShulman"}}, {"_id": "kdmvT3fDbzuiDyJc9", "postedAt": "2023-12-06T16:40:54.701Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Thanks for elaborating, Carl!</p><blockquote><p>Luke says in the post you linked that the numbers in the graphic are not usable as expected moral weights, since ratios of expectations are not the same as expectations of ratios.</p></blockquote><p>Let me try to restate your point, and suggest why one may disagree. If one puts weight w on the welfare range (WR) of humans relative to that of chickens being N, and 1 - w on it being n, the expected welfare range of:</p><ul><li>Humans relative to that of chickens is E(\"WR of humans\"/\"WR of chickens\") = w*N + (1 - w)*n.</li><li>Chickens relative to that of humans is E(\"WR of chickens\"/\"WR of humans\") = w/N + (1 - w)/n.</li></ul><p>You are arguing that N can plausibly be much larger than n. For the sake of illustration, we can say N = 389 (ratio between the 86 billion <a href=\"https://en.wikipedia.org/wiki/List_of_animals_by_number_of_neurons\">neurons</a> of a humans and 221 M of a chicken), n = 3.01 (reciprocal of RP's median welfare range of chickens relative to humans of 0.332), and w = 1/12 (since the neuron count model was one of the 12 RP <a href=\"https://docs.google.com/spreadsheets/d/1i_I23qXtuGtpk4spFi2NmK6ReCiFXL6QGBd2KfGzulY/edit#gid=1489233206&amp;range=A2\">considered</a>, and all of them were weighted equally). Having the welfare range of:</p><ul><li>Chickens as the reference, E(\"WR of humans\"/\"WR of chickens\") = 35.2. So 1/E(\"WR of humans\"/\"WR of chickens\") = 0.0284.</li><li>Humans as the reference (as RP did), E(\"WR of chickens\"/\"WR of humans\") = 0.305.</li></ul><p>So, as you said, determining welfare ranges relative to humans results in animals being weighted more heavily. However, I think the difference is much smaller than the suggested above. Since N and n are quite different, I guess we should combine them using a weighted geometric mean, not the weighted mean as I did above. If so, both approaches output exactly the same result:</p><ul><li>E(\"WR of humans\"/\"WR of chickens\") = N^w*n^(1 - w) = 4.49. So 1/E(\"WR of humans\"/\"WR of chickens\") = (N^w*n^(1 - w))^-1 = 0.223.</li><li>E(\"WR of chickens\"/\"WR of humans\") = (1/N)^w*(1/n)^(1 - w) = 0.223.</li></ul><p>The reciprocal of the expected value is not the expected value of the reciprocal, so using the mean leads to different results. However, I think we should be using the geometric mean, and the reciprocal of the geometric mean is the geometric mean of the reciprocal. So the 2 approaches (using humans or chickens as the reference) will output the same ratios regardless of N, n and w as long as we aggregate N and n with the geometric mean. If N and n are similar, it no longer makes sense to use the geometric mean, but then both approaches will output similar results anyway, so RP's approach looks fine to me as a 1st pass. Does this make any sense?</p><p>Of course, it would still be good to do further research (which OP could fund) to adjudicate how much weight should be given to each model RP <a href=\"https://docs.google.com/spreadsheets/d/1i_I23qXtuGtpk4spFi2NmK6ReCiFXL6QGBd2KfGzulY/edit#gid=1489233206&amp;range=A2\">considered</a>.</p><blockquote><p>I had argued for many years that insects met a lot of the functional standards one could use to identify the presence of well-being, and that even after taking two-envelopes issues and nervous system scale into account expected welfare at stake for small wild animals looked much larger than for FAW.</p></blockquote><p><a href=\"https://reflectivedisequilibrium.blogspot.com/2015/11/various-functional-forms-for-brain.html\">True</a>!</p><blockquote><p>I happen to be a fan of animal welfare work relative to GHW's other grants at the margin because animal welfare work is so highly neglected</p></blockquote><p>Thanks for sharing your views!</p>", "parentCommentId": "nt3uP3TxRAhBWSfkW", "user": {"username": "vascoamaralgrilo"}}, {"_id": "RzjwEuXDd3uBSrTjD", "postedAt": "2023-12-06T17:44:29.633Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>I'm not planning on continuing a long thread here, I mostly wanted to help address the questions about my previous comment, so I'll be moving on after this. But I will say two things regarding the above. First, this effect (computational scale) is smaller for chickens but progressively enormous for e.g. shrimp or lobster or flies. &nbsp;Second, this is a huge move and one really needs to wrestle with intertheoretic comparisons to justify it:</p><blockquote><p>I guess we should combine them using a weighted geometric mean, not the weighted mean as I did above.&nbsp;</p></blockquote><p>Suppose we compared the mass of the human population of Earth with the mass of an individual human. We could compare them on 12 metrics, like per capita mass, per capita square root mass, per capita foot mass... and aggregate mass. If we use the equal-weighted geometric mean, we will conclude the individual has a mass within an order of magnitude of the total Earth population, instead of billions of times less.</p>", "parentCommentId": "kdmvT3fDbzuiDyJc9", "user": {"username": "CarlShulman"}}, {"_id": "XXtP3xszLaC9e5KWb", "postedAt": "2023-12-06T18:54:20.036Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<blockquote><p>I'm not planning on continuing a long thread here, I mostly wanted to help address the questions about my previous comment, so I'll be moving on after this.</p></blockquote><p>Fair, as this is outside of the scope of the original post. I noticed you did not comment on RP's neuron counts <a href=\"https://forum.effectivealtruism.org/posts/Mfq7KxQRvkeLnJvoB/why-neuron-counts-shouldn-t-be-used-as-proxies-for-moral\">post</a>. I think it would be valuable if you commented there about the concerns you expressed here, or did you already express them elsewhere in another post of RP's moral weight project <a href=\"https://forum.effectivealtruism.org/sequences/y5n47MfgrKvTLE3pw\">sequence</a>?</p><blockquote><p>First, this effect (computational scale) is smaller for chickens but progressively enormous for e.g. shrimp or lobster or flies.</p></blockquote><p>I agree that is the case if one combines the 2 wildly different estimates for the welfare range (e.g. one based on the number of neurons, and another corresponding to RP's median welfare ranges) with a weighted mean. However, as I commented above, using the geometric mean would cancel the effect.</p><blockquote><p>Suppose we compared the mass of the human population of Earth with the mass of an individual human. We could compare them on 12 metrics, like per capita mass, per capita square root mass, per capita foot mass... and aggregate mass. If we use the equal-weighted geometric mean, we will conclude the individual has a mass within an order of magnitude of the total Earth population, instead of billions of times less.</p></blockquote><p>Is this a good analogy? Maybe not:</p><ul><li>Broadly speaking, giving the same weight to multiple estimates only makes sense if there is wide uncertainty with respect to which one is more reliable. In the example above, it would make sense to give negligible weight to all metrics except for the aggregate mass. In contrast, there is arguably wide uncertainty with respect to what are the best models to measure welfare ranges, and therefore distributing weights evenly is more appropriate.</li><li>One particular model on which we can put lots of weight on is that mass is straightforwardly additive (at least at the macro scale). So we can say the mass of all humans equals the number of humans times the mass per human, and then just estimate this for a typical human. In contrast, it is arguably unclear whether one can obtain the welfare range of an animal by e.g. just adding up the welfare range of its individual neurons.</li></ul>", "parentCommentId": "RzjwEuXDd3uBSrTjD", "user": {"username": "vascoamaralgrilo"}}, {"_id": "eeoHnz3xagL7H8dxZ", "postedAt": "2023-12-07T02:07:37.455Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>This consideration is something I had never thought of before and blew my mind. Thank you for sharing.</p><p>Hopefully I can summarize it (assuming I interpreted it correctly) in a different way that might help people who were as befuddled as I was.&nbsp;</p><p>The point is that, when you have probabilistic weight to two different theories of sentience being true, you have to assign units to sentience in these different theories in order to compare them.&nbsp;</p><p>Say you have two theories of sentience that are similarly probable, one dependent on intelligence and one dependent on brain size. Call these units IQ-qualia and size-qualia. If you assign fruit flies a moral weight of 1, you are implicitly declaring a conversion rate of (to make up some random numbers) 1000 IQ-qualia = 1 size-qualia. If you assign elephants however to have a moral weight of 1, you implicitly declare a conversion rate of (again, made-up) 1 IQ-qualia = 1000 size-qualia, because elephant brains are much larger but not much smarter than fruit flies. These two different conversion rates are going to give you very different numbers for the moral weight of humans (or as Shulman was saying, of each other).</p><p>Rethink Priorities assigned humans a moral weight of 1, and thus assumed a certain conversion rate between different theories that made for a very small-animal-dominated world by sentience.&nbsp;</p>", "parentCommentId": "nt3uP3TxRAhBWSfkW", "user": {"username": "RedStateBlueState"}}, {"_id": "dgZcKQeiSXoJZkyGB", "postedAt": "2023-12-08T09:34:12.467Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>(I'm not at Rethink Priorities anymore, and I'm not speaking on their behalf.)</p><blockquote><p>Rethink's work, as I read it, did not address that central issue, that you get wildly different results from assuming the moral value of a fruit fly is fixed and reporting possible ratios to elephant welfare as opposed to doing it the other way around.&nbsp;</p><p>(...)</p></blockquote><blockquote><p>Rethink's <a href=\"https://rethinkpriorities.org/publications/why-neuron-counts-shouldnt-be-used-as-proxies-for-moral-weight\">discussion of this</a> almost completely sidestepped the issue in my view.</p></blockquote><p>RP did in fact respond to some versions of these arguments, in the piece <a href=\"https://forum.effectivealtruism.org/posts/vbhoFsyQmrntru6Kw/do-brains-contain-many-conscious-subsystems-if-so-should-we\">Do Brains Contain Many Conscious Subsystems? If So, Should We Act Differently?</a>, of which I am a co-author.</p>", "parentCommentId": "nt3uP3TxRAhBWSfkW", "user": {"username": "MichaelStJules"}}, {"_id": "iN7kfFDLK7YJEJF5L", "postedAt": "2023-12-08T11:13:14.736Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<blockquote><p>It is not unthinkably improbable that an elephant brain where reinforcement from a positive or negative stimulus adjust millions of times as many neural computations could be seen as vastly more morally important than a fruit fly, just as one might think that a fruit fly is much more important than a thermostat (which some suggest is conscious and possesses preferences). Since on some major functional aspects of mind there are differences of millions of times, that suggests a mean expected value orders of magnitude higher for the elephant if you put a bit of weight on the possibility that moral weight scales with the extent of, e.g. the computations that are adjusted by positive and negative stimuli.</p></blockquote><p>&nbsp;</p><p>This specific kind of account, if meant to depend inherently on differences in reinforcement, is <i>very improbable to me</i> (&lt;0.1%), and conditional on such accounts, the inherent importance of reinforcement would also very probably scale very slowly, with faster scaling increasingly improbable. It could work out that the expected scaling isn't slow, but that would be because of very low probability possibilities.</p><p>The value of subjective wellbeing, whether hedonistic, felt desires, reflective evaluation/preferences, choice-based or some kind of combination, seems very probably logically independent from how much reinforcement happens EDIT: and empirically dissociable. My main argument is that reinforcement happens unconsciously and has no necessary or ~immediate conscious effects. We could imagine temporarily or permanently preventing reinforcement without any effect on mental states or subjective wellbeing in the moment. Or, we can imagine connecting a brain to an artificial neural network to add more neurons to reinforce, again to no effect.</p><p>And even within the same human under normal conditions, holding their reports of value or intensity fixed, the amount of reinforcement that actually happens will probably depend systematically on the nature of the experience, e.g. physical pain vs anxiety vs grief vs joy. If reinforcement has a large effect on expected moral weights, you could and I'd guess would end up with an alienating view, where <i>everyone</i> is systematically wrong about the relative value of their own experiences. You'd effectively need to reweight all of their reports by type of experience.</p><p>So, even with intertheoretic comparisons between accounts with and without reinforcement, of which I'd be quite skeptical specifically in this case but also generally, this kind of hypothesis shouldn't make much difference (or it does make a substantial difference, but it seems objectionably fanatical and alienating). If rejecting such intertheoretic comparisons, as I'm more generally inclined to do and as Open Phil seems to be doing, it should make very little difference.</p><p>&nbsp;</p><p>There are more plausible functions you could use, though, like attention. But, again, I think the cases for intertheoretic comparisons between accounts of how moral value scales with neurons for attention or probably any other function are generally very weak, so you should only take expected values over descriptive uncertainty conditional on each moral scaling hypothesis, not across moral scaling hypotheses (unless you normalize by something else, like variance across options). Without intertheoretic comparisons, approaches to moral uncertainty in the literature aren't so sensitive to small probability differences or fanatical about moral views. So, it tends to be more important to focus on large probability shifts than improbable extreme cases.</p>", "parentCommentId": "nt3uP3TxRAhBWSfkW", "user": {"username": "MichaelStJules"}}, {"_id": "X2CJnQsKL8DJuKdbs", "postedAt": "2023-12-08T12:34:35.141Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Does this just apply to wild animals or stray domestic animals?</p>", "parentCommentId": null, "user": {"username": "doviskeTch"}}, {"_id": "StSp9acEBDxzvngoY", "postedAt": "2023-12-08T16:58:42.451Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Thanks, I was referring to this as well, but should have had a second link for it as the Rethink page on neuron counts didn't link to the other post. I think that page is a better link than the <a href=\"https://rethinkpriorities.org/publications/why-neuron-counts-shouldnt-be-used-as-proxies-for-moral-weight\">RP page I linked</a>, so I'll add it in my comment.</p>", "parentCommentId": "dgZcKQeiSXoJZkyGB", "user": {"username": "CarlShulman"}}, {"_id": "5mxXZ5Npdm2sc5ZnD", "postedAt": "2023-12-09T00:20:09.997Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>(Again, not speaking on behalf of Rethink Priorities, and I don't work there anymore.)</p><p>(Btw, the quote formatting in your original comment got messed up with your edit.)</p><p>I think the claims I quoted are still basically false, though?</p><blockquote><p>Rethink's work, as I read it, did not address that central issue, that you get wildly different results from assuming the moral value of a fruit fly is fixed and reporting possible ratios to elephant welfare as opposed to doing it the other way around.&nbsp;</p></blockquote><p><a href=\"https://forum.effectivealtruism.org/posts/vbhoFsyQmrntru6Kw/do-brains-contain-many-conscious-subsystems-if-so-should-we\">Do Brains Contain Many Conscious Subsystems? If So, Should We Act Differently?</a> explicitly considered a conscious subsystems version of this thought experiment, focusing on the more human-favouring side when you normalize by small systems like insect brains, which is the non-obvious side often neglected.</p><p>There's a case that conscious subsystems could dominate expected welfare ranges even without intertheoretic comparisons (but also possibly with), so I think we were focusing on one of strongest and most important arguments for humans potentially mattering more, <i>assuming hedonism and expectational total utilitarianism</i>. Maximizing expected choiceworthiness with intertheoretic comparisons is controversial and only one of multiple competing approaches to moral uncertainty. I'm personally very skeptical of it because of the arbitrariness of intertheoretic comparisons and its fanaticism (including chasing infinities, and lexically higher and higher infinities). Open Phil also already avoids making intertheoretic comparisons, but <a href=\"https://www.openphilanthropy.org/research/update-on-cause-prioritization-at-open-philanthropy/#1111-issue-1-normative-uncertainty-and-philosophical-incommensurability\">was more sympathetic to normalizing by humans if it were going to</a>.</p>", "parentCommentId": "StSp9acEBDxzvngoY", "user": {"username": "MichaelStJules"}}, {"_id": "ZPHSeqxk9Jk8n2LrS", "postedAt": "2023-12-09T02:42:04.278Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>I don't want to convey that there was no discussion, thus my linking the discussion and saying I found it inadequate and largely missing the point from my perspective. I made an edit for clarity, but would accept suggestions for another.<br><br>&nbsp;</p>", "parentCommentId": "5mxXZ5Npdm2sc5ZnD", "user": {"username": "CarlShulman"}}, {"_id": "cG8Lx6jPtaELeroxz", "postedAt": "2023-12-09T02:52:35.359Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Your edit looks good to me. Thanks!</p>", "parentCommentId": "ZPHSeqxk9Jk8n2LrS", "user": {"username": "MichaelStJules"}}, {"_id": "ZnsPPtnth537z4DLt", "postedAt": "2023-12-11T15:34:05.028Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Hi!\nI\u2019m assuming that by \u201cthis\u201d you mean the post\u2019s argument, \u201cwild animals\u201d you mean wild animal welfare research, and \u201cstray domestic animals\u201d you mean pet shelters. In that case, I think the post\u2019s argument might apply to wild animal welfare research, depending upon one\u2019s model of the effects of that research. However, I think this post\u2019s argument is unlikely to apply to pet shelters.</p>\n", "parentCommentId": "X2CJnQsKL8DJuKdbs", "user": {"username": "Ariel Simnegar"}}, {"_id": "iqMy6iK7DzLjdwA9m", "postedAt": "2023-12-12T19:07:17.333Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Thanks for your discussion of the Moral Weight Project's methodology, Carl. (And to everyone else for the useful back-and-forth!) We have some thoughts about this important issue and we're keen to write more about it. Perhaps 2024 will provide the opportunity!</p><p>For now, we'll just make one brief point, which is that it\u2019s important to separate two questions. The first concerns the relevance of the two envelopes problem to the Moral Weight Project. The second concerns alternative ways of generating moral weights. We considered the two envelopes problem at some length when we were working on the Moral Weight Project and concluded that our approach was still worth developing. We\u2019d be glad to revisit this and appreciate the challenge to the methodology.</p><p>However, even if it turns out that the methodology has issues, it\u2019s an open question how best to proceed. We grant the possibility that, as you suggest, more neurons = more compute = the possibility of more intense pleasures and pains. But it's also possible that more neurons = more intelligence = less biological need for intense pleasures and pains, as other cognitive abilities can provide the relevant fitness benefits, effectively muting the intensities of those states. Or perhaps there's some very low threshold of cognitive complexity for sentience after which point all variation in behavior is due to non-hedonic capacities. Or perhaps cardinal interpersonal utility comparisons are impossible. And so on. In short, while it's true that there are hypotheses on which elephants have massively more intense pains than fruit flies, there are also hypotheses on which the opposite is true and on which equality is (more or less) true. Once we account for all these hypotheses, it may still work out that elephants and fruit flies differ by a few orders of magnitude in expectation, but perhaps not by five or six. Presumably, we should all want some approach, whatever it is, that avoids being mugged by whatever low-probability hypothesis posits the largest difference between humans and other animals.</p><p>That said, you've raised some significant concerns about methods that aggregate over different relative scales of value. So, we\u2019ll be sure to think more about the degree to which this is a problem for the work we\u2019ve done\u2014and, if it is, how much it would change the bottom line.&nbsp;</p>", "parentCommentId": "nt3uP3TxRAhBWSfkW", "user": {"username": "bob-fischer"}}, {"_id": "nX8Wn5mHhiNgJABum", "postedAt": "2023-12-13T00:33:47.096Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Thank you for the comment Bob.<br><br>I agree that I also am disagreeing on the object-level, as Michael made clear with his comments (I do not think I am talking about a tiny chance, although I do not think the RP discussions characterized my views as I would), and some other methodological issues besides two-envelopes (related to the object-level ones). &nbsp;E.g. I would not want to treat a highly networked AI mind (with billions of bodies and computation directing them in a unified way, on the scale of humanity) as a millionth or a billionth of the welfare of the same set of robots and computations with less integration (and overlap of shared features, or top-level control), ceteris paribus.&nbsp;<br><br>Indeed, I would be wary of treating the integrated mind as though welfare stakes for it were half or a tenth as great, seeing that as a potential source of moral catastrophe, like ignoring the welfare of minds not based on proteins. E.g. having tasks involving suffering &nbsp;and frustration done by large integrated minds, and pleasant ones done by tiny minds, while increasing the amount of mental activity in the former. It sounds like the combination of object-level and methodological takes attached to these reports would favor ignoring almost completely the integrated mind.</p><p>Incidentally, in a world where small animals are being treated extremely badly and are numerous, I can see a temptation to err in their favor, since even overestimates of their importance could be shifting things in the right marginal policy direction. But thinking about the potential moral catastrophes on the other side helps sharpen the motivation to get it right.<br><br>In practice, I don't prioritize moral weights issues in my work, because I think the most important decisions hinging on it will be in an era with AI-aided mature sciences of mind, philosophy and epistemology. And as I have <a href=\"https://nickbostrom.com/papers/digital-minds.pdf\">written</a> regardless of your views about small minds and large minds, it won't be the case that e.g. humans are utility monsters of impartial hedonism (rather than something bigger, smaller, or otherwise different), and grounds for focusing on helping humans won't be terminal impartial hedonistic in nature. But from my viewpoint baking in that integration (and unified top-level control or mental overlap of some parts of computation) close to eliminates mentality or welfare (vs less integrated collections of computations) seems bad in non-Pascalian fashion.</p>", "parentCommentId": "iqMy6iK7DzLjdwA9m", "user": {"username": "CarlShulman"}}, {"_id": "fRBq6KvgQEcztru7M", "postedAt": "2023-12-13T09:28:51.108Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>(Speaking for myself only.)</p>\n<p>FWIW, I think something like conscious subsystems (in huge numbers in one neural network) is more plausible by design in future AI. It just seems unlikely in animals because all of the apparent subjective value seems to happen at roughly the highest level where everything is integrated in an animal brain.</p>\n<p>Felt desire seems to (largely) be motivational salience, a top-down/voluntary attention control function driven by high-level interpretations of stimuli (e.g. objects, social situations), so relatively late in processing. Similarly, hedonic states depend on high-level interpretations, too.</p>\n<p>Or, according to Attention Schema Theory, attention models evolved for the voluntary control of attention. It's not clear what the value would be for an attention model at lower levels of organization before integration.</p>\n<p>And evolution will select against realizing functions unnecessarily if they have additional costs, so we should provide a positive argument for the necessary functions being realized earlier or multiple times in parallel that overcomes or doesn't incur such additional costs.</p>\n<p>So, it's not that integration necessarily reduces value; it's that, in animals, all the morally valuable stuff happens after most of the integration, and apparently only once or in small number.</p>\n<p>In artificial systems, the morally valuable stuff could instead be implemented separately by design at multiple levels.</p>\n<p>EDIT:</p>\n<p>I think there's still crux about whether realizing the same function the same number of times but \"to a greater degree\" makes it more morally valuable. I think there are some ways of \"to a greater degree\" that don't matter, and some that could. If it's only sort of (vaguely) true that a system is realizing a certain function, or it realizes some but not all of the functions possibly necessary for some type of welfare in humans, then we might discount it for only meeting lower precisifications of the vague standards. But adding more neurons just doing the same things:</p>\n<ol>\n<li>doesn't make it more true that it realizes the function or the type of welfare (e.g. adding more neurons to my brain wouldn't make it more true that I can suffer),</li>\n<li>doesn't clearly increase welfare ranges, and</li>\n<li>doesn't have any other clear reason for why it should make a moral difference (I think you disagree with this, based on your examples).</li>\n</ol>\n<p>But maybe we don't actually need good specific reasons to assign non-tiny probabilities to neuron count scaling for 2 or 3, and then we get domination of neuron count scaling in expectation, depending on what we're normalizing by, like you suggest.</p>\n", "parentCommentId": "nX8Wn5mHhiNgJABum", "user": {"username": "MichaelStJules"}}, {"_id": "jEii3agCfQd9aY8Bt", "postedAt": "2024-02-19T01:14:36.706Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Hi Emily. I've written a post about how to handle moral uncertainty about moral weights across animals, including humans: <a href=\"https://forum.effectivealtruism.org/posts/L4Cv8hvuun6vNL8rm/solution-to-the-two-envelopes-problem-for-moral-weights\">Solution to the two envelopes problem for moral weights</a>. It responds directly to <a href=\"https://www.openphilanthropy.org/research/update-on-cause-prioritization-at-open-philanthropy/#id-111-handling-uncertainty-about-animal-inclusive-vs-human-centric-views\">Holden's writing on the topic</a>. In short, I think Open Phil should evaluate opportunities for helping humans and nonhumans <i>relative to human moral weights</i>, like comparison method A from Holden's post. This is because we directly value that with which we're familiar, e.g. our own experiences, and we have just regular empirical (not moral) uncertainty about its nature and whether and to what extent other animals have similar relevant capacities.</p>", "parentCommentId": "3uBrznQWEqs83PAfh", "user": {"username": "MichaelStJules"}}, {"_id": "FWcX4CdmuMNKjw4G6", "postedAt": "2024-03-18T23:34:44.894Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>It seems to me that the naive way to handle the two envelopes problem (and I've never heard of a way better than the naive way) is to diversify your donations across two possible solutions to the two envelopes problem:</p>\n<ul>\n<li>donate half your (neartermist) money on the assumption that you should use ratios to fixed human value</li>\n<li>donate half your money on the assumption that you should fix the opposite way (eg fruit flies have fixed value)</li>\n</ul>\n<p>Which would suggest donating half to animal welfare and probably half to global poverty. (If you let moral weights be linear with neuron count, I think that would still favor animal welfare, but you could get global poverty outweighing animal welfare if moral weight grows super-linearly with neuron count.)</p>\n<p>Plausibly there are other neartermist worldviews you might include that don't relate to the two envelopes problem, e.g. a \"only give to the most robust interventions\" worldview might favor GiveDirectly. So I could see an allocation of less than 50% to animal welfare.</p>\n", "parentCommentId": "nt3uP3TxRAhBWSfkW", "user": {"username": "MichaelDickens"}}, {"_id": "9RrQdgRy8bGGEjZQz", "postedAt": "2024-03-18T23:52:04.486Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>Agreed.  I disagree with the general practice of capping the probability distribution over animals' sentience at 1x that of humans'. (I wouldn't put much mass above 1x, but it should definitely be more than zero mass.)</p>\n", "parentCommentId": "D2sRJKy8jsZtWa5it", "user": {"username": "MichaelDickens"}}, {"_id": "4ho7d3fxmoWZB3g3p", "postedAt": "2024-03-19T15:59:17.207Z", "postId": "btTeBHKGkmRyD5sFK", "htmlBody": "<p>There is no <i>one</i> opposite way; there are <i>many</i> other ways than to fix human value. You could fix the value in fruit flies, shrimps, chickens, elephants, C elegans, some plant, some bacterium, rocks, your laptop, GPT-4 or an alien, etc..</p><p>I think a more principled approach would be to consider <a href=\"https://forum.effectivealtruism.org/posts/qaeh5NFCkXhvLDDM6/gradations-of-moral-weight\">precise theories of how welfare scales</a>, not necessarily fixing the value in any one moral patient, and then use some other approach to moral uncertainty for uncertainty between the theories. However, <a href=\"https://forum.effectivealtruism.org/posts/L4Cv8hvuun6vNL8rm/solution-to-the-two-envelopes-problem-for-moral-weights\">there is another argument for fixing human value across many such theories</a>: we directly value our own experiences, and theorize about consciousness in relation to our own experiences, so we can fix the value in our own experiences and evaluate relative to them.</p>", "parentCommentId": "FWcX4CdmuMNKjw4G6", "user": {"username": "MichaelStJules"}}]