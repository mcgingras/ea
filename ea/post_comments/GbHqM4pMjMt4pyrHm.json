[{"_id": "n364tDKkfxB38qDab", "postedAt": "2023-09-28T13:12:19.796Z", "postId": "GbHqM4pMjMt4pyrHm", "htmlBody": "<p>1. I like the idea of concrete (publicly stated) pre-defined measures, since it lowers the risk of moving safety standards/targets. It would be a substantial improvement over what we have today, especially if there's coordination between top labs.</p><p>2. The graph shows jumps where y increases at a rate greater than x. Has this ever happened before? What we've seen so far is more of a mirrored L. First we move along the x-axis, later (to a smaller degree) along the y-axis.&nbsp;</p><p>3. The line between the red and blue area should be heavily blurred/striped. This might seem like an aesthetic nitpick, but we can't map the edges of what we've never seen. Our current perceptions are thought up by <i>human</i> minds that are innately tuned to empathize with and predict <i>human</i> behavior, which unwittingly leads to thinking along the lines: \"If I was an AI and thought like a psychopathic human, what would I do?\". We don't do this explicitly, but that's what we're actually doing. The real danger lies in the unknown unknowns, which cannot be plotted on a graph a priori. At the moment, we're assuming progression of dangers/capabilities in a \"logical order\", i.e. the way humans gain abilities/learn things. If the order is thrown around, so are the warning signs.&nbsp;</p>", "parentCommentId": null, "user": {"username": "blueberry"}}]