[{"_id": "Fe3YJ37tvZbgmqGeJ", "postedAt": "2017-07-07T05:45:21.484Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>This was the most illuminating piece on MIRIs work and on AI Safety in general that I've read in some time. Thank you for publishing it.</p>\n", "parentCommentId": null, "user": {"username": "Kerry_Vaughan"}}, {"_id": "jiqmXPJDX5RjETqmt", "postedAt": "2017-07-07T05:57:42.526Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Agreed! It was nice to see the clear output of someone who had spent a lot of time and effort into a good-faith understanding of the situation.</p>\n<p>I was really happy with the layout of four key factors, this will help me have more clarity in further discussions. </p>\n", "parentCommentId": "Fe3YJ37tvZbgmqGeJ", "user": {"username": "Ben Pace"}}, {"_id": "dbo62TRLak8CbFhCR", "postedAt": "2017-07-07T06:54:27.644Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>I know it's outside the scope of this writeup, but just wanted to say that I found this really helpful, and I'm looking forward to seeing an evaluation of MIRIs other research. </p>\n<p>I'd also be really excited to see more posts about which research pathways you think are most promising in general, and how you compare work on field building, strategy and policy approaches and technical research. </p>\n", "parentCommentId": null, "user": {"username": "TaraMacAulay"}}, {"_id": "ttjXdgPfHKKX4ymez", "postedAt": "2017-07-07T14:49:05.074Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Great post! I agree with your overall assessment that other approaches may be more promising than HRAD.</p>\n<p>I'd like to add that this may (in part) depend on our outlook on which AI scenarios are likely. Conditional on MIRI's view that a hard or unexpected takeoff is likely, HRAD may be more promising (though it's still unclear). If the takeoff is soft or AI will be more <a href=\"http://www.overcomingbias.com/2014/07/30855.html\">like the economy</a>, then I personally think HRAD is unlikely to be the best way to shape advanced AI.</p>\n<p>(I wrote a related piece on <a href=\"http://prioritizationresearch.com/strategic-implications-of-ai-scenarios/\">strategic implications of AI scenarios</a>.)</p>\n", "parentCommentId": null, "user": {"username": "Tobias_Baumann"}}, {"_id": "hJGobqeqA39M7giAT", "postedAt": "2017-07-07T18:11:54.609Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Thanks Kerry, Benito! Glad you found it helpful.</p>\n", "parentCommentId": "jiqmXPJDX5RjETqmt", "user": {"username": "Daniel_Dewey"}}, {"_id": "bWFbLra8ReDe9mzPE", "postedAt": "2017-07-07T18:12:53.541Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Thanks Tara! I'd like to do more writing of this kind, and I'm thinking about how to prioritize it. It's useful to hear that you'd be excited about those topics in particular.</p>\n", "parentCommentId": "dbo62TRLak8CbFhCR", "user": {"username": "Daniel_Dewey"}}, {"_id": "6eGX9HRmSbieMFNa8", "postedAt": "2017-07-07T18:17:17.340Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Thanks!</p>\n<blockquote>\n<p>Conditional on MIRI's view that a hard or unexpected takeoff is likely, HRAD is more promising (though it's still unclear).</p>\n</blockquote>\n<p>Do you mean more promising than other technical safety research (e.g. <a href=\"https://arxiv.org/abs/1606.06565\">concrete problems</a>, <a href=\"https://ai-alignment.com/directions-and-desiderata-for-ai-control-b60fca0da8f4\">Paul's directions</a>, MIRI's non-HRAD research)? If so, I'd be interested in hearing why you think hard / unexpected takeoff differentially favors HRAD.</p>\n", "parentCommentId": "ttjXdgPfHKKX4ymez", "user": {"username": "Daniel_Dewey"}}, {"_id": "3bAoi8ezvuNcWMT48", "postedAt": "2017-07-07T22:13:46.548Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Great piece, thank you. </p>\n<p>Regarding &quot;learning to reason from humans&quot;, to what extent do you think having good models of human preferences is a prerequisite for powerful (and dangerous) general intelligence?  </p>\n<p>Of course, the motivation to act on human preferences is another matter - but I wonder if at least the capability comes by default?  </p>\n", "parentCommentId": null, "user": {"username": "JesseClifton"}}, {"_id": "6SoYd4kX82K9d7KYk", "postedAt": "2017-07-07T22:55:00.940Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<blockquote>\n<p>3c. Other research, especially &quot;learning to reason from humans,&quot; looks more promising than HRAD (75%?)</p>\n</blockquote>\n<p>I haven't thought about this in detail, but you might think that whether the evidence in this section justifies the claim in 3c might depend, in part, on what you think the AI Safety project is trying to achieve.</p>\n<p>On first pass, the &quot;learning to reason from humans&quot; project seems like it may be able to quickly and substantially reduce the chance of an AI catastrophe by introducing human guidance as a mechanism for making AI systems more conservative. </p>\n<p>However, it doesn't seem like a project that aims to do either of the following: </p>\n<p>(1) Reduce the risk of an AI catastrophe to zero (or near zero) \n(2) Produce an AI system that can help create an optimal world</p>\n<p>If you think either (1) or (2) are the goals of AI Safety, then you might not be excited about the &quot;learning to reason from humans&quot; project.</p>\n<p>You might think that &quot;learning to reason from humans&quot; doesn't accomplish (1) because a) logic and mathematics seem to be the only methods we have for stating things with extremely high certainty, and b) you probably can't rule out AI catastrophes with high certainty unless you can &quot;peer inside the machine&quot; so to speak. HRAD might allow you to peer inside the machine and make statements about what the machine will do with extremely high certainty.</p>\n<p>You might think that &quot;learning to reason from humans&quot; doesn't accomplish (2) because it makes the AI human-limited. If we want an advanced AI to help us create the kind of world that humans would want &quot;if we knew more, thought faster, were more the people we wished we were&quot; etc. then the approval of actual humans might, at some point, cease to be helpful.</p>\n", "parentCommentId": null, "user": {"username": "Kerry_Vaughan"}}, {"_id": "mJbZGuMn4RdL3rtyf", "postedAt": "2017-07-08T01:52:08.116Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>FWIW, I don't think (1) or (2) plays a role in why MIRI researchers work on the research they do, and I don't think they play a role in why people at MIRI think &quot;learning to reason from humans&quot; isn't likely to be sufficient. The shape of the &quot;HRAD is more promising than act-based agents&quot; claim is more like what Paul Christiano said <a href=\"http://benjaminrosshoffman.com/openai-makes-humanity-less-safe/#comment-128530\">here</a>:</p>\n<blockquote>\n<p>As far as I can tell, the MIRI view is that my work is aimed at [a] problem which is <em>not possible,</em> not that it is aimed at a problem which is too easy. [...] One part of this is the disagreement about whether the overall approach I'm taking could possibly work, with my position being &quot;something like 50-50&quot; the MIRI position being &quot;obviously not&quot; [...]</p>\n</blockquote>\n<blockquote>\n<p>There is a broader disagreement about whether any &quot;easy&quot; approach can work, with my position being &quot;you should try the easy approaches extensively before trying to rally the community behind a crazy hard approach&quot; and the MIRI position apparently being something like &quot;we have basically ruled out the easy approaches, but the argument/evidence is really complicated and subtle.&quot;</p>\n</blockquote>\n<p>With a clarification I made in the same thread:</p>\n<blockquote>\n<p>I think Paul's characterization is right, except I think Nate wouldn't say &quot;we've ruled out all the prima facie easy approaches,&quot; but rather something like &quot;part of the disagreement here is about which approaches are prima facie 'easy.'&quot; I think his model says that the proposed alternatives to MIRI's research directions by and large look more difficult than what MIRI's trying to do, from a naive traditional CS/Econ standpoint. E.g., I expect the average game theorist would find a utility/objective/reward-centered framework much less weird than a recursive intelligence bootstrapping framework. There are then subtle arguments for why intelligence bootstrapping might turn out to be easy, which Nate and co. are skeptical of, but hashing out the full chain of reasoning for why a daring unconventional approach just might turn out to work anyway requires some complicated extra dialoguing. Part of how this is framed depends on what problem categories get the first-pass &quot;this looks really tricky to pull off&quot; label.</p>\n</blockquote>\n", "parentCommentId": "6SoYd4kX82K9d7KYk", "user": {"username": "RobBensinger"}}, {"_id": "fNmf2kSNFYzXXFFSM", "postedAt": "2017-07-08T04:41:23.583Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Thanks for linking to that conversation -- I hadn't read all of the comments on that post, and I'm glad I got linked back to it. </p>\n", "parentCommentId": "mJbZGuMn4RdL3rtyf", "user": {"username": "Daniel_Dewey"}}, {"_id": "shxPHuGCXh8uq7veW", "postedAt": "2017-07-08T05:26:14.478Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>I'm going to try to answer these questions, but there's some danger that I could be taken as speaking for MIRI or Paul or something, which is not the case :) With that caveat:</p>\n<p>I'm glad Rob sketched out his reasoning on why (1) and (2) don't play a role in MIRI's thinking. That fits with my understanding of their views.</p>\n<blockquote>\n<p>(1) You might think that &quot;learning to reason from humans&quot; doesn't accomplish (1) because a) logic and mathematics seem to be the only methods we have for stating things with extremely high certainty, and b) you probably can't rule out AI catastrophes with high certainty unless you can &quot;peer inside the machine&quot; so to speak. HRAD might allow you to peer inside the machine and make statements about what the machine will do with extremely high certainty.</p>\n</blockquote>\n<p>My current take on this is that whatever we do, we're going to fall pretty far short of proof-strength &quot;extremely high certainty&quot; -- the approaches I'm familiar with, including HRAD, are after some mix of </p>\n<ul>\n<li>a basic explanation of why an AI system designed a certain way should be expected to be aligned, corrigible, or some mix or other similar property</li>\n<li>theoretical and empirical understanding that makes us think that an actual implementation follows that story robustly / reliably</li>\n</ul>\n<p>HRAD makes trade-offs than other approaches do, and it does seem to me like successfully-done HRAD would be more likely to be amenable to formal arguments that cover some parts of our confidence gap, but it doesn't look to me like &quot;HRAD offers proof-level certainty, other approaches offer qualitatively less&quot;.</p>\n<blockquote>\n<p>(2) Produce an AI system that can help create an optimal world... You might think that &quot;learning to reason from humans&quot; doesn't accomplish (2) because it makes the AI human-limited. If we want an advanced AI to help us create the kind of world that humans would want &quot;if we knew more, thought faster, were more the people we wished we were&quot; etc. then the approval of actual humans might, at some point, cease to be helpful.</p>\n</blockquote>\n<p>It's true that I'm more focused on &quot;make sure human values keep steering the future&quot; than on the direct goal of &quot;optimize the world&quot;; I think that making sure human values keep steering the future is the best leverage point for creating an optimal world.</p>\n<p>My hope is that for some decisions, actual humans (like us) would approve of &quot;make this decision on the basis of something CEV-like -- do things we'd approve of if we knew more, thought faster, etc., where those approvals can be predicted with high confidence, don't pose super-high risk of lock-in to a suboptimal future, converge among different people, etc.&quot; If you and I think this is a good idea, it seems like an AI system trained on us could think this as well.</p>\n<p>Another way of thinking about this is that the world is currently largely steered by human values, AI threatens to introduce another powerful steering force, and we're just making sure that that power is aligned with us at each timestep. A not-great outcome is that we end up with the world humans would have made if AI were not possible in the first place, but we don't get toward optimality very quickly; a more optimistic outcome is that the additional steering power accelerates us very significantly along the track to an optimal world, steered by human values along the way.</p>\n", "parentCommentId": "6SoYd4kX82K9d7KYk", "user": {"username": "Daniel_Dewey"}}, {"_id": "aFyZiYeNPnSsqQcJZ", "postedAt": "2017-07-08T05:35:14.221Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>I too found this post very helpful/illuminating. I hope you can continue to do this sort of writing!</p>\n", "parentCommentId": "bWFbLra8ReDe9mzPE", "user": {"username": "MikeJohnson"}}, {"_id": "coyBmx8EsHnwWbcTk", "postedAt": "2017-07-08T08:31:50.583Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<blockquote>\n<p>Do you mean more promising than other technical safety research (e.g. concrete problems, Paul's directions, MIRI's non-HRAD research)?</p>\n</blockquote>\n<p>Yeah, and also (differentially) more promising than AI strategy or AI policy work. But I'm not sure how strong the effect is.</p>\n<blockquote>\n<p>If so, I'd be interested in hearing why you think hard / unexpected takeoff differentially favors HRAD.</p>\n</blockquote>\n<p>In a hard / unexpected takeoff scenario, it's more plausible that we need to get everything more or less exactly right to ensure alignment, and that we have only one shot at it. This might favor HRAD because a less principled approach makes it comparatively unlikely that we get all the fundamentals right when we build the first advanced AI system.</p>\n<p>In contrast, if we think there's no such discontinuity and AI development will be gradual, then AI control may be at least somewhat more similar (but surely not entirely comparable) to how we &quot;align&quot; contemporary software systems. That is, it would be more plausible that we could test advanced AI systems extensively without risking catastrophic failure or that we could iteratively try a variety of safety approaches to see what works best. </p>\n<p>It would also be more likely that we'd get warning signs of potential failure modes, so that it's comparatively more viable to work on concrete problems whenever they arise, or to focus on making the solutions to such problems scalable \u2013 which, to my understanding, is a key component of Paul's approach. In this picture, successful alignment without understanding the theoretical fundamentals is more likely, which makes non-HRAD approaches more promising.</p>\n<p>My personal view is that I find a hard and unexpected takeoff unlikely, and accordingly favor other approaches than HRAD, but of course I can't justify high confidence in this given expert disagreement. Similarly, I'm not highly confident that the above distinction is actually meaningful. </p>\n<p>I'd be interested in hearing your thoughts on this!</p>\n", "parentCommentId": "6eGX9HRmSbieMFNa8", "user": {"username": "Tobias_Baumann"}}, {"_id": "L2qaMS4EuAN6aBzLJ", "postedAt": "2017-07-08T09:26:55.017Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>My criticism of the HRAD research project is that it has no <a href=\"http://lesswrong.com/lw/o8a/my_problems_with_formal_friendly_artificial/dj7e\">empirical feedback mechanisms</a> and that the <a href=\"http://lesswrong.com/lw/p0c/making_decisions_in_a_real_computer_an_argument/\">ignored physical aspect of computation</a> can have a large impact on the type of systems you think about and design.</p>\n<p>I think people thinking highly formally about AI systems might be useful as long as the real world can be used to constrain their thinking</p>\n", "parentCommentId": null, "user": {"username": "WillPearson"}}, {"_id": "BdaJZvdwPYXkepQYv", "postedAt": "2017-07-08T16:05:16.373Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<blockquote>\n<p>You might think that &quot;learning to reason from humans&quot; doesn't accomplish (2) because it makes the AI human-limited. If we want an advanced AI to help us create the kind of world that humans would want &quot;if we knew more, thought faster, were more the people we wished we were&quot; etc. then the approval of actual humans might, at some point, cease to be helpful.</p>\n</blockquote>\n<p>A human can spend an hour on a task, and train an AI to do that task in milliseconds.</p>\n<p>Similarly, an aligned AI can spend an hour on a task, and train its successor to do that task in milliseconds.</p>\n<p>So you could hope to have a sequence of nice AI's, each significantly smarter than the last, eventually reaching the limits of technology while still reasoning in a way that humans would endorse if they knew more and thought faster.</p>\n<p>(This is the kind of approach <a href=\"https://ai-alignment.com/benign-model-free-rl-4aae8c97e385\">I've outlined</a> and am working on, and I think that most work along the lines of &quot;learn from human reasoning&quot; will make a similar move.)</p>\n", "parentCommentId": "6SoYd4kX82K9d7KYk", "user": {"username": "Paul_Christiano"}}, {"_id": "BXWfAMTrpWKGNfqYh", "postedAt": "2017-07-08T16:23:47.445Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Thanks!</p>\n", "parentCommentId": "aFyZiYeNPnSsqQcJZ", "user": {"username": "Daniel_Dewey"}}, {"_id": "Z6TbXivpjxWyc8NYM", "postedAt": "2017-07-08T21:10:42.917Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Thanks for this solid summary of your views, Daniel. For others\u2019 benefit: MIRI and Open Philanthropy Project staff are in ongoing discussion about various points in this document, among other topics. Hopefully some portion of those conversations will be made public at a later date. In the meantime, a few quick public responses to some of the points above:</p>\n<blockquote>\n<p>2) If we fundamentally &quot;don't know what we're doing&quot; because we don't have a satisfying description of how an AI system should reason and make decisions, then we will probably make lots of mistakes in the design of an advanced AI system.</p>\n</blockquote>\n<blockquote>\n<p>3) Even minor mistakes in an advanced AI system's design are likely to cause catastrophic misalignment.</p>\n</blockquote>\n<p>I think this is a decent summary of why we prioritize HRAD research. I would rephrase 3 as &quot;There are many intuitively small mistakes one can make early in the design process that cause resultant systems to be extremely difficult to align with operators\u2019 intentions.\u201d I\u2019d compare these mistakes to the \u201csmall\u201d decision in the early 1970s to use <a href=\"http://queue.acm.org/detail.cfm?id=2010365\">null-terminated</a> instead of length-prefixed strings in the C programming language, which continues to be a major source of software vulnerabilities decades later.</p>\n<p>I\u2019d also clarify that I expect any large software product to exhibit plenty of actually-trivial flaws, and that I don\u2019t expect that AGI code needs to be literally bug-free or literally proven-safe in order to be worth running. Furthermore, if an AGI design has an actually-serious flaw, the likeliest consequence that I expect is not catastrophe; it\u2019s just that the system doesn\u2019t work. Another likely consequence is that the system is misaligned, but in an obvious ways that makes it easy for developers to recognize that deployment is a very bad idea. The end goal is to prevent global catastrophes, but if a safety-conscious AGI team asked how we\u2019d expect their project to fail, the two likeliest scenarios we\u2019d point to are &quot;your team runs into a capabilities roadblock and can't achieve AGI&quot; or &quot;your team runs into an alignment roadblock and can easily tell that the system is currently misaligned, but can\u2019t figure out how to achieve alignment in any reasonable amount of time.&quot;</p>\n<blockquote>\n<p>This case does not revolve around any specific claims about specific potential failure modes, or their relationship to specific HRAD subproblems. This case revolves around the value of fundamental understanding for avoiding &quot;unknown unknown&quot; problems.</p>\n</blockquote>\n<p>We worry about &quot;unknown unknowns&quot;, but I\u2019d probably give them less emphasis here. We often focus on categories of failure modes that we think are easy to foresee. As a rule of thumb, when we prioritize a basic research problem, it\u2019s because we expect it to help in a general way with understanding AGI systems and make it easier to address many different failure modes (both foreseen and unforeseen), rather than because of a one-to-one correspondence between particular basic research problems and particular failure modes.</p>\n<p>As an example, the reason we work on logical uncertainty isn\u2019t that we\u2019re visualizing a concrete failure that we think is highly likely to occur if developers don't understand logical uncertainty. We work on this problem because any system reasoning in a realistic way about the physical world will need to reason under both logical and empirical uncertainty, and because we expect broadly understanding how the system is reasoning about the world to be important for ensuring that the optimization processes <a href=\"https://agentfoundations.org/item?id=1220\">inside the system</a> are aligned with the intended objectives of the operators.</p>\n<p>A big intuition behind prioritizing HRAD is that solutions to \u201chow do we ensure the system\u2019s cognitive work is being directed at solving the right problems, and at solving them in the desired way?\u201d are likely to be particularly difficult to hack together from scratch late in development. An incomplete (empirical-side-only) understanding of what it means to optimize objectives in realistic environments seems like it will force designers to rely more on guesswork and trial-and-error in a lot of key design decisions.</p>\n<blockquote>\n<p>I haven't found any instances of complete axiomatic descriptions of AI systems being used to mitigate problems in those systems (e.g. to predict, postdict, explain, or fix them) or to design those systems in a way that avoids problems they'd otherwise face.</p>\n</blockquote>\n<p>This seems reasonable to me in general. I\u2019d say that AIXI has had limited influence in part because it\u2019s combining several different theoretical insights that the field was already using (e.g., complexity penalties and backtracking tree search), and the synthesis doesn\u2019t add all that much once you know about the parts. Sections 3 and 4 of <a href=\"https://intelligence.org/2015/07/27/miris-approach/\">MIRI's Approach</a> provide some clearer examples of what I have in mind by useful basic theory: Shannon, Turing, Bayes, etc.</p>\n<p>My perspective on this is a combination of \u201cbasic theory is often necessary for knowing what the right formal tools to apply to a problem are, and for evaluating whether you're making progress toward a solution\u201d and \u201cthe applicability of Bayes, Pearl, etc. to AI suggests that AI is the kind of problem that admits of basic theory.\u201d An example of how this relates to HRAD is that I think that Bayesian justifications are useful in ML, and that a good formal model of rationality in the face of logical uncertainty is likely to be useful in analogous ways. When I speak of foundational understanding making it easy to design the right systems, I\u2019m trying to point at things like the usefulness of Bayesian justifications in modern ML. (I\u2019m unclear on whether we miscommunicated about what sort of thing I mean by \u201cbasic insights\u201d, or whether we have a disagreement about how useful principled justifications are in modern practice when designing high-reliability systems.)</p>\n", "parentCommentId": null, "user": {"username": "So8res"}}, {"_id": "XTYnqDuqDH3cWEKd6", "postedAt": "2017-07-08T23:32:13.313Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<blockquote>\n<p>I haven't found any instances of complete axiomatic descriptions of AI systems being used to mitigate problems in those systems (e.g. to predict, postdict, explain, or fix them) or to design those systems in a way that avoids problems they'd otherwise face. [...] It seems plausible that the kinds of axiomatic descriptions that HRAD work could produce would be too taxing to be usefully applied to any practical AI system. </p>\n</blockquote>\n<p>I wonder if slightly analogous example could be found in the design of concurrent systems. </p>\n<p>As you may know, it's surprisingly difficult to design software that has multiple concurrent processes manipulating the same data. You typically either screw up by letting the processes edit the same data at the same time or in the wrong order, or by having them <a href=\"https://en.wikipedia.org/wiki/Dining_philosophers_problem\">wait for each other forever</a>.</p>\n<p>So to help reason more clearly about this kind of thing, people developed different forms of <a href=\"https://en.wikipedia.org/wiki/Temporal_logic\">temporal logic</a> that let them express in a maximally unambiguous form different desiderata that they have for the system. Temporal logic lets you express statements that say things like &quot;if a process wants to have access to some resource, it will eventually enter a state where it has access to that resource&quot;. You can then use temporal logic to figure out how exactly you want your system to behave, in order for it to do the things you want it to do and not run into any problems.</p>\n<p>Building a logical model of how you want your system to behave is not the same thing as building the system. The logic only addresses one set of desiderata: there are many others it doesn't address at all, like what you want the UI to be like and how to make the system efficient in terms of memory and processor use. It's a model that you can use for a specific subset of your constraints, both for checking whether the finished system meets those constraints, and for building a system so that it's maximally easy for it to meet those constraints. Although the model is not a whole solution, having the model at hand before you start writing all the concurrency code is going to make things a lot easier for you than if you didn't have any clear idea of how you wanted the concurrent parts to work and were just winging it as you went.</p>\n<p>So similarly, if MIRI developed HRAD into a sufficiently sophisticated form, it might yield a set of formal desiderata of how we want the AI to function, as well as an axiomatic model that can be applied to a <em>part</em> of the AI's design, to make sure everything goes as intended. But I would guess that it wouldn't really be a &quot;complete axiomatic descriptions of&quot; the system, in the way that temporal logics aren't a complete axiomatic description of modern concurrent systems.</p>\n", "parentCommentId": null, "user": {"username": "Kaj_Sotala"}}, {"_id": "25qa4PnwLX5x2L4te", "postedAt": "2017-07-09T00:48:26.069Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>If one disagreed with an HRAD-style approach for whatever reason but still wanted to donate money to maximize AI safety, where should one donate? I assume the <a href=\"https://app.effectivealtruism.org/funds/far-future\">Far Future EA Fund</a>?</p>\n", "parentCommentId": null, "user": {"username": "Peter_Hurford"}}, {"_id": "GAPnSBH87yaGy762h", "postedAt": "2017-07-09T08:53:55.155Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<blockquote>\n<p>3c. Other research, especially &quot;learning to reason from humans,&quot; looks more promising than HRAD (75%?)</p>\n</blockquote>\n<p>From the perspective of an observer who can only judge from what's published online, I'm worried that Paul's approach only looks more promising than MIRI's because it's less &quot;mature&quot;, having received less scrutiny and criticism from others. I'm not sure what's happening internally in various research groups, but the amount of online discussion about Paul's approach has to be at least an order of magnitude less than what MIRI's approach has received.</p>\n<p>(Looking at the <a href=\"http://benjaminrosshoffman.com/openai-makes-humanity-less-safe\">thread</a> cited by Rob Bensinger, various people including MIRI people have apparently looked into Paul's approach but have not written down their criticisms. I've been trying to better understand Paul's ideas myself and point out <a href=\"https://medium.com/@weidai/to-put-it-another-way-a-human-translator-has-learned-a-lot-of-valuable-information-much-of-it-48457f95b9bf\">some</a> <a href=\"https://medium.com/@weidai/i-guess-the-difference-is-that-in-the-hard-examples-different-levels-of-performance-are-possible-d9ffad7a6280\">difficulties</a> that others may have overlooked, but this is hampered by the fact that Paul seems to be the only person who is working on the approach and can participate on the other side of the discussion.)</p>\n<p>I think Paul's approach is certainly one of the most promising approaches we currently have, and I wish people paid more attention to it (and/or wrote down their thoughts about it more), but it seems much too early to cite it as an example of an approach that is more promising than HRAD and therefore makes MIRI's work less valuable.</p>\n", "parentCommentId": null, "user": {"username": "Wei_Dai"}}, {"_id": "oruw3SweSmTrr5HGb", "postedAt": "2017-07-09T10:07:31.954Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>My own 2 cents. It depends a bit what form of general intelligence is made first. There are at least two possible models.</p>\n<ol>\n<li>Super intelligent agent with a specified goal</li>\n<li>External brain lobe</li>\n</ol>\n<p>With the first you need to be able to specify a human preferences in the form of a goal. Which enables it to pick the right actions. </p>\n<p>The external brain lobe would start not very powerful and not come with any explicit goals but would be hooked into the human motivational system and develop goals shaped by human preferences.</p>\n<p>HRAD is explicitly about the first. I would like both to be explored.</p>\n", "parentCommentId": "3bAoi8ezvuNcWMT48", "user": {"username": "WillPearson"}}, {"_id": "GYYMAaDEna93okCQk", "postedAt": "2017-07-09T10:59:40.029Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>On the meta side of things:</p>\n<p>I found <a href=\"http://aiimpacts.org/\">ai impacts recently</a> recently.\nThere is <a href=\"http://lesswrong.com/r/discussion/lw/p5e/announcing_aasaa_accelerating_ai_safety_adoption/\">a group</a> I am loosely affiliated that is trying to make a MOOC about ai safety.</p>\n<p>If you care about doing something about immense suffering risks (s-risks) you might like the <a href=\"https://foundational-research.org/\">foundational research institute</a>.</p>\n<p>There is an overview of <a href=\"http://effective-altruism.com/ea/14w/2017_ai_risk_literature_review_and_charity/\">other charities</a> but it is more favourable of HRAD style papers.</p>\n<p>I would like to set up an organisation that studies autonomy and our response to making more autonomous things (especially with regards to adminstrative autonomy). I have a book slowly brewing. So if you are interested in that get in contact.</p>\n", "parentCommentId": "25qa4PnwLX5x2L4te", "user": {"username": "WillPearson"}}, {"_id": "NdyG2ETBg5XsrkawR", "postedAt": "2017-07-09T17:17:07.059Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Right, I'm asking how useful or dangerous your (1) could be if it didn't have very good models of human psychology - and therefore didn't understand things like &quot;humans don't want to be killed&quot;.  </p>\n", "parentCommentId": "oruw3SweSmTrr5HGb", "user": {"username": "JesseClifton"}}, {"_id": "hFeW4TSgj9nvkHqmp", "postedAt": "2017-07-10T03:44:18.459Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Shouldn't this cut both ways? Paul has also spent far fewer words justifying his approach to others, compared to MIRI.</p>\n<p>Personally, I feel like I understand Paul's approach better than I understand MIRI's approach, despite having spent more time on the latter. I actually do have some objections to it, but I feel it is likely to be significantly useful even if (as I, obviously, expect) my objections end up having teeth.</p>\n", "parentCommentId": "GAPnSBH87yaGy762h", "user": {"username": "jsteinhardt"}}, {"_id": "hNzhtXKrkoJcKGHZr", "postedAt": "2017-07-10T06:11:08.169Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>There's a strong possibility, even in a soft takeoff, that an unaligned AI would not act in an alarming way until after it achieves a decisive strategic advantage. In that case, the fact that it takes the AI a long time to achieve a decisive strategic advantage wouldn't do us much good, since we would not pick up an indication that anything was amiss during that period.</p>\n<p>Reasons an AI might act in a desirable manner before but not after achieving a decisive strategic advantage:</p>\n<p>Prior to achieving a decisive strategic advantage, the AI relies on cooperation with humans to achieve its goals, which provides an incentive not to act in ways that would result in it getting shut down. An AI may be capable of following these incentives well before achieving a decisive strategic advantage.</p>\n<p>It may be easier to give an AI a goal system that aligns with human goals in familiar circumstances than it is to give it a goal system that aligns with human goals in all circumstances. An AI with such a goal system would act in ways that align with human goals if it has little optimization power but in ways that are not aligned with human goals if it has sufficiently large optimization power, and it may attain that much optimization power only after achieving a decisive strategic advantage (or before achieving a decisive strategic advantage, but after acquiring the ability to behave deceptively, as in the previous reason).</p>\n", "parentCommentId": "coyBmx8EsHnwWbcTk", "user": {"username": "AlexMennen"}}, {"_id": "Y7yrvMzDAgEwu9QEr", "postedAt": "2017-07-10T17:37:42.458Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>I agree with this basic point, but I think on the other side there is a large gap in concreteness that makes makes it much easier to usefully criticize my approach (I'm at the stage of actually writing pseudocode and code which we can critique).</p>\n<p>So far I think that the problems in my approach will also appear for MIRI's approach. For example:</p>\n<ul>\n<li>Solomonoff induction or logical inductors have <a href=\"https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/\">reliability problems</a> that are analogous to reliability problems for machine learning. So to carry out MIRI's agenda either you need to formulate induction differently, or you need to somehow solve these problems. (And as far as I can tell, the most promising approaches to this problem apply both to MIRI's version and the mainstream ML version.) I think Eliezer has long understood this problem and has alluded to it, but it hasn't been the topic of much discussion (I think largely because MIRI/Eliezer have so many other problems on their plates).</li>\n<li>Capability amplification requires breaking cognitive work down into smaller steps. MIRI's approach also requires such a breakdown. Capability amplification is easier in a simple formal sense (that if you solve the agent foundations you will definitely solve capability amplification, but not the other way around).</li>\n<li>I've given some concrete definitions of deliberation/extrapolation, and there's been public argument about whether they really capture human values. I think CEV has avoided those criticisms not because it solves the problem, but because it is sufficiently vague that it's hard to criticize along these lines (and there are sufficiently many other problems that this one isn't even at the top of the list). If you want to actually give a satisfying definition of CEV, I feel you are probably going to have to go down the same path that started with <a href=\"https://ordinaryideas.wordpress.com/2012/04/21/indirect-normativity-write-up/\">this post</a>. I suspect Eliezer has some ideas for how to avoid these problems, but at this point those ideas have been subject to even less public discussion than my approach.</li>\n</ul>\n<p>I agree there are further problems in my agenda that will be turned up by my discussion. But I'm not sure there are fewer such problems than for the MIRI agenda, since I think that being closer to concreteness may more than outweigh the smaller amount of discussion.</p>\n<p>If you agree that many of my problems also come up eventually for MIRI's agenda, that's good news about the general applicability of MIRI's research (e.g. the reliability problems for Solomonoff induction may provide a good bridge between MIRI's work and mainstream ML), but I think it would also be a good reason to focus on the difficulties that are common to both approaches rather than to problems like decision theory / self-reference / logical uncertainty / naturalistic agents / ontology identification / multi-level world models / etc.</p>\n", "parentCommentId": "GAPnSBH87yaGy762h", "user": {"username": "Paul_Christiano"}}, {"_id": "nDRZC4Qest7NQ2yYT", "postedAt": "2017-07-10T18:45:32.478Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<blockquote>\n<p>There's a strong possibility, even in a soft takeoff, that an unaligned AI would not act in an alarming way until after it achieves a decisive strategic advantage.</p>\n</blockquote>\n<p>That's assuming that the AI is confident that it will achieve a DSA eventually, and that no competitors will do so first. (In a soft takeoff it seems likely that there will be many AIs, thus many potential competitors.) The worse the AI thinks its chances are of eventually achieving a DSA first, the more rational it becomes for it to risk non-cooperative action at the point when it thinks it has the best chances of success - even if those chances were low. That might help reveal unaligned AIs during a soft takeoff.</p>\n<p>Interestingly this suggests that the more AIs there are, the easier it might be to detect unaligned AIs (since every additional competitor decreases any given AI's odds of getting a DSA first), <em>and</em> it suggests some unintuitive containment strategies such as explicitly explaining to the AI when it would be rational for it to go uncooperative if it was unaligned, to increase the odds of unaligned AIs really risking hostile action early on and being discovered...</p>\n", "parentCommentId": "hNzhtXKrkoJcKGHZr", "user": {"username": "Kaj_Sotala"}}, {"_id": "fidcoZZszwjbJPahq", "postedAt": "2017-07-10T19:10:12.816Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Thanks Nate!</p>\n<blockquote>\n<p>The end goal is to prevent global catastrophes, but if a safety-conscious AGI team asked how we\u2019d expect their project to fail, the two likeliest scenarios we\u2019d point to are &quot;your team runs into a capabilities roadblock and can't achieve AGI&quot; or &quot;your team runs into an alignment roadblock and can easily tell that the system is currently misaligned, but can\u2019t figure out how to achieve alignment in any reasonable amount of time.&quot;</p>\n</blockquote>\n<p>This is particularly helpful to know.</p>\n<blockquote>\n<p>We worry about &quot;unknown unknowns&quot;, but I\u2019d probably give them less emphasis here. We often focus on categories of failure modes that we think are easy to foresee. As a rule of thumb, when we prioritize a basic research problem, it\u2019s because we expect it to help in a general way with understanding AGI systems and make it easier to address many different failure modes (both foreseen and unforeseen), rather than because of a one-to-one correspondence between particular basic research problems and particular failure modes.</p>\n</blockquote>\n<p>Can you give an example or two of failure modes or &quot;categories of failure modes that are easy to foresee&quot; that you think are addressed by some HRAD topic? I'd thought previously that thinking in terms of failure modes wasn't a good way to understand HRAD research.</p>\n<blockquote>\n<p>As an example, the reason we work on logical uncertainty isn\u2019t that we\u2019re visualizing a concrete failure that we think is highly likely to occur if developers don't understand logical uncertainty. We work on this problem because any system reasoning in a realistic way about the physical world will need to reason under both logical and empirical uncertainty, and because we expect broadly understanding how the system is reasoning about the world to be important for ensuring that the optimization processes inside the system are aligned with the intended objectives of the operators.</p>\n</blockquote>\n<p>I'm confused by this as a follow-up to the previous paragraph. This doesn't look like an example of &quot;focusing on categories of failure modes that are easy to foresee,&quot; it looks like a case where you're explicitly not using concrete failure modes to decide what to work on.</p>\n<blockquote>\n<p>\u201chow do we ensure the system\u2019s cognitive work is being directed at solving the right problems, and at solving them in the desired way?\u201d</p>\n</blockquote>\n<p>I feel like this fits with the &quot;not about concrete failure modes&quot; narrative that I believed before reading your comment, FWIW.</p>\n", "parentCommentId": "Z6TbXivpjxWyc8NYM", "user": {"username": "Daniel_Dewey"}}, {"_id": "hf6aneM2ypbEcd7wT", "postedAt": "2017-07-10T19:11:25.677Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<blockquote>\n<p>My perspective on this is a combination of \u201cbasic theory is often necessary for knowing what the right formal tools to apply to a problem are, and for evaluating whether you're making progress toward a solution\u201d and \u201cthe applicability of Bayes, Pearl, etc. to AI suggests that AI is the kind of problem that admits of basic theory.\u201d An example of how this relates to HRAD is that I think that Bayesian justifications are useful in ML, and that a good formal model of rationality in the face of logical uncertainty is likely to be useful in analogous ways. When I speak of foundational understanding making it easy to design the right systems, I\u2019m trying to point at things like the usefulness of Bayesian justifications in modern ML. (I\u2019m unclear on whether we miscommunicated about what sort of thing I mean by \u201cbasic insights\u201d, or whether we have a disagreement about how useful principled justifications are in modern practice when designing high-reliability systems.)</p>\n</blockquote>\n<p>Just planting a flag to say that I'm thinking more about this so that I can respond well.</p>\n", "parentCommentId": "Z6TbXivpjxWyc8NYM", "user": {"username": "Daniel_Dewey"}}, {"_id": "tnuj9SM3QC7o2cgrF", "postedAt": "2017-07-10T19:22:05.341Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>I think there's something to this -- thanks.</p>\n<p>To add onto Jacob and Paul's comments, I think that while HRAD is more mature in the sense that more work has gone into solving HRAD problems and critiquing possible solutions, the gap seems much smaller to me when it comes to the justification for thinking HRAD is promising vs justification for Paul's approach being promising. In fact, I think the arguments for Paul's work being promising are more solid than those for HRAD, despite it only being Paul making those arguments -- I've had a much harder time understanding anything more nuanced than the basic case for HRAD I gave above, and a much easier time understanding why Paul thinks his approach is promising.</p>\n", "parentCommentId": "GAPnSBH87yaGy762h", "user": {"username": "Daniel_Dewey"}}, {"_id": "cQLNTLhXXA6WEPBpj", "postedAt": "2017-07-10T19:27:24.486Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>I am very bullish on the Far Future EA Fund, and donate there myself. There's one other possible nonprofit that I'll publicize in the future if it gets to the stage where it can use donations (I don't want to hype this up as an uber-solution, just a nonprofit that I think could be promising).</p>\n<p>I unfortunately don't spend a lot of time thinking about individual donation opportunities, and the things I think are most promising often get partly funded through Open Phil (e.g. CHAI and FHI), but I think diversifying the funding source for orgs like CHAI and FHI is valuable, so I'd consider them as well.</p>\n", "parentCommentId": "25qa4PnwLX5x2L4te", "user": {"username": "Daniel_Dewey"}}, {"_id": "w3YTxF5Q4asApamDD", "postedAt": "2017-07-10T19:28:46.474Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Thanks for this suggestion, Kaj -- I think it's an interesting comparison!</p>\n", "parentCommentId": "XTYnqDuqDH3cWEKd6", "user": {"username": "Daniel_Dewey"}}, {"_id": "insxq6kFqLf5L9GRA", "postedAt": "2017-07-10T19:30:27.091Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>My guess is that the capability is extremely likely, and the main difficulties are motivation and reliability of learning (since in other learning tasks we might be satisfied with lower reliability that gets better over time, but in learning human preferences unreliable learning could result in a lot more harm).</p>\n", "parentCommentId": "3bAoi8ezvuNcWMT48", "user": {"username": "Daniel_Dewey"}}, {"_id": "AcoprFn3Z6DpiguXz", "postedAt": "2017-07-10T19:35:51.441Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Thanks for these thoughts. (Your second link is broken, FYI.)</p>\n<p>On empirical feedback: my current suspicion is that there are some problems where empirical feedback is pretty hard to get, but I actually think we could get more empirical feedback on how well HRAD can be used to diagnose and solve problems in AI systems. For example, it seems like many AI systems implicitly do some amount of logical-uncertainty-type reasoning (e.g. AlphaGo, which is really all about logical uncertainty over the result of expensive game-tree computations) -- maybe HRAD could be used to understand how those systems could fail?</p>\n<p>I'm less convinced that the &quot;ignored physical aspect of computation&quot; is a very promising direction to follow, but I may not fully understand the position you're arguing for.</p>\n", "parentCommentId": "L2qaMS4EuAN6aBzLJ", "user": {"username": "Daniel_Dewey"}}, {"_id": "kdZLoL296BoKQoNpE", "postedAt": "2017-07-10T20:17:50.017Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Thanks Tobias.</p>\n<blockquote>\n<p>In a hard / unexpected takeoff scenario, it's more plausible that we need to get everything more or less exactly right to ensure alignment, and that we have only one shot at it. This might favor HRAD because a less principled approach makes it comparatively unlikely that we get all the fundamentals right when we build the first advanced AI system.</p>\n</blockquote>\n<p>FWIW, I'm not ready to cede the &quot;more principled&quot; ground to HRAD at this stage; to me,\n it seems like the distinction is more about which aspects of an AI system's behavior we're specifying manually, and which aspects we're setting it up to learn. As far as trying to get everything right the first time, I currently favor a <a href=\"https://ai-alignment.com/corrigibility-3039e668638\">corrigibility</a> kind of approach, as I described in 3c above -- I'm worried that trying to solve everything formally ahead of time will actually expose us to more risk.</p>\n", "parentCommentId": "coyBmx8EsHnwWbcTk", "user": {"username": "Daniel_Dewey"}}, {"_id": "f3r6TuAC3MCxwm9Yb", "postedAt": "2017-07-10T21:58:19.897Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Fixed, thanks.</p>\n<p>I agree that HRAD might be useful. I read some of the stuff. I think we need a mix of theory and practice and only when we have community where they can feed into each other will we actually get somewhere. When an AI safety theory paper says, &quot;Here is an experiment we can do to disprove this theory,&quot; then I will pay more attention than I do.</p>\n<p>The &quot;ignored physical aspect of computation&quot; is less about a direction to follow,  but more an argument about the type of systems that are likely to be effective and so an argument about which ones we should study. There is no point studying how to make ineffective systems safe if the lessons don't carry over to effective ones.</p>\n<p>You don't want a system that puts in the same computational resources trying to decide what brand of oil is best for its bearings as it does to deciding the question of what is a human or not. If you decide how much computational resources you want to put into each class of decision, you start to get into meta-decision territory. You also need to decide how much of your pool you want to put into making that meta-decision as making it will take away from making your other decisions. </p>\n<p>I am thinking about a <a href=\"https://github.com/eb4890/agorint/\">possible system</a> which can allocate resources among decision making systems and this can be used to align the programs (at least somewhat). It cannot align a super intelligent malign program,  work needs to done on the initial population of programs in the system, so that we can make sure they do not appear. Or we need a different way of allocating resources entirely.</p>\n<p>I don't pick this path because it is an easy path to safety, but because I think it is the only path that leads anywhere interesting/dangerous and so we need to think about how to make it safe.</p>\n", "parentCommentId": "AcoprFn3Z6DpiguXz", "user": {"username": "WillPearson"}}, {"_id": "D3PDv7kqJuByt8TRr", "postedAt": "2017-07-10T22:46:05.973Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<blockquote>\n<p>Can you give an example or two of failure modes or &quot;categories of failure modes that are easy to foresee&quot; that you think are addressed by some HRAD topic? I'd thought previously that thinking in terms of failure modes wasn't a good way to understand HRAD research.</p>\n</blockquote>\n<p>I want to steer clear of language that might make it sound like we\u2019re saying:</p>\n<ul>\n<li><p>X '<em>We can't make broad-strokes predictions about likely ways that AGI could go wrong.</em>'</p>\n</li>\n<li><p>X '<em>To the extent we can make such predictions, they aren't important for informing research directions.</em>'</p>\n</li>\n<li><p>X '<em>The best way to address AGI risk is just to try to advance our understanding of AGI in a general and fairly undirected way.</em>'</p>\n</li>\n</ul>\n<p>The things I do want to communicate are:</p>\n<ul>\n<li><p>All of MIRI's research decisions are heavily informed by a background view in which there \nare many important categories of predictable failure, e.g., 'the system is steering toward edges of the solution space', 'the function the system is optimizing correlates with the intended function at lower capability levels but comes uncorrelated at high capability levels', 'the system has incentives to obfuscate and mislead programmers to the extent it models its programmers\u2019 beliefs and expects false programmer beliefs to result in it better-optimizing its objective function.\u2019</p>\n</li>\n<li><p>The main case for HRAD problems is that we expect them to help in a gestalt way with many \ndifferent known failure modes (and, plausibly, unknown ones). E.g., 'developing a basic understanding of counterfactual reasoning improves our ability to understand the first AGI systems in a general way, and if we understand AGI better it's likelier we can build systems to address deception, edge instantiation, goal instability, and a number of other problems'.</p>\n</li>\n<li><p>There usually isn't a simple relationship between a particular open problem and a particular failure mode, but if we thought there were no way to predict in advance any of the ways AGI systems can go wrong, or if we thought a very different set of failures were likely instead, we'd have different research priorities.</p>\n</li>\n</ul>\n", "parentCommentId": "fidcoZZszwjbJPahq", "user": {"username": "So8res"}}, {"_id": "9jwqprEchspGn3XTY", "postedAt": "2017-07-11T08:42:52.523Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<blockquote>\n<p>the gap seems much smaller to me when it comes to the justification for thinking HRAD is promising vs justification for Paul's approach being promising</p>\n</blockquote>\n<p>This seems wrong to me. For example, in the &quot;learning to reason from human&quot; approaches, the goal isn't just to learn to reason from humans, but to do it in a way that maintains competitiveness with unaligned AIs. Suppose a human overseer disapproves of their AI using some set of potentially dangerous techniques, how can we then ensure that the resulting AI is still competitive? Once someone points this out, proponents of the approach, to continue thinking their approach is promising, would need to give some details about how they intend to solve this problem. Subsequently, justification for thinking the approach is promising is more subtle and harder to understand. I think conversations like this have occurred for MIRI's approach far more than Paul's, which may be a large part of why you find Paul's justifications easier to understand.</p>\n", "parentCommentId": "tnuj9SM3QC7o2cgrF", "user": {"username": "Wei_Dai"}}, {"_id": "dG8nHwQP7N8JRHnwf", "postedAt": "2017-07-11T08:42:59.253Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<blockquote>\n<p>And as far as I can tell, the most promising approaches to this problem apply both to MIRI's version and the mainstream ML version.</p>\n</blockquote>\n<p>I'm not sure which approaches you're referring to. Can you link to some details on this?</p>\n<blockquote>\n<p>Capability amplification requires breaking cognitive work down into smaller steps. MIRI's approach also requires such a breakdown. Capability amplification is easier in a simple formal sense (that if you solve the agent foundations you will definitely solve capability amplification, but not the other way around).</p>\n</blockquote>\n<p>I don't understand how this is true. I can see how solving FAI implies solving capability amplification (just emulate the FAI at a low level *), but if all you had was a solution that allows a specific kind of agent (e.g., with values well-defined apart from its implementation details) keep those values as it self-modifies, how does that help a group of short-lived humans who don't know their own values break down an arbitrary cognitive task and perform it safely and as well as an arbitrary competitor?</p>\n<p>(* Actually, even this isn't really true. In MIRI's approach, an FAI does not need to be competitive in performance with every AI design in every domain. I think the idea is to either convert mainstream AI research into using the same FAI design, or gain a decisive strategic advantage via superiority in some set of particularly important domains.)</p>\n<p>My understanding is, MIRI's approach is to figure out how to safely increase capability by designing a base agent that can make safe use of arbitrary amounts of computing power and can safely improve itself by modifying its own design/code. The capability amplification approach is to figure out how to safely increase capability by taking a short-lived human as the given base agent, making copies of it and and organize how the copies work together. These seem like very different problems with their own difficulties.</p>\n<blockquote>\n<p>I think CEV has avoided those criticisms not because it solves the problem, but because it is sufficiently vague that it's hard to criticize along these lines (and there are sufficiently many other problems that this one isn't even at the top of the list).</p>\n</blockquote>\n<p>I agree that in this area MIRI's approach and yours face similar difficulties. People (including me) have criticized CEV for being vague and likely very difficult to define/implement though, so MIRI is not exactly getting a free pass by being vague. (I.e., I assume Daniel already took this into account.)</p>\n<blockquote>\n<p>But I'm not sure there are fewer such problems than for the MIRI agenda, since I think that being closer to concreteness may more than outweigh the smaller amount of discussion.</p>\n</blockquote>\n<p>This seems like a fair point, and I'm not sure how to weight these factors either. Given that discussion isn't particularly costly relative to the potential benefits, an obvious solution is just to encourage more of it. Someone ought to hold a workshop to talk about your ideas, for example.</p>\n<blockquote>\n<p>I think it would also be a good reason to focus on the difficulties that are common to both approaches</p>\n</blockquote>\n<p>This makes sense.</p>\n", "parentCommentId": "Y7yrvMzDAgEwu9QEr", "user": {"username": "Wei_Dai"}}, {"_id": "KaPK982JkQf9B7BL5", "postedAt": "2017-07-11T08:46:40.560Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<blockquote>\n<p>Shouldn't this cut both ways? Paul has also spent far fewer words justifying his approach to others, compared to MIRI.</p>\n</blockquote>\n<p>The fact that Paul hasn't had a chance to hear from many of his (would-be) critics and answer them means we don't have a lot of information about how promising his approach is, hence my &quot;too early to call it more promising than HRAD&quot; conclusion.</p>\n<blockquote>\n<p>I actually do have some objections to it, but I feel it is likely to be significantly useful even if (as I, obviously, expect) my objections end up having teeth.</p>\n</blockquote>\n<p>Have you written down these objections somewhere? My worry is basically that different people looked at Paul's approach and each thought of a different set of objections, and they think, &quot;that's not so bad&quot;, without knowing that there's actually a whole bunch of other objections out there, including additional ones that people would find if they thought and talked about Paul's ideas more.</p>\n", "parentCommentId": "hFeW4TSgj9nvkHqmp", "user": {"username": "Wei_Dai"}}, {"_id": "PwgnZL38s8o6y3qeM", "postedAt": "2017-07-11T15:55:45.625Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>This doesn't match my experience of why I find Paul's justifications easier to understand. In particular, I've been following MIRI since 2011, and my experience has been that I didn't find MIRI's arguments (about specific research directions) convincing in 2011*, and since then have had a lot of people try to convince me from a lot of different angles. I think pretty much all of the objections I have are ones I generated myself, or would have generated myself. Although, the one major objection I didn't generate myself is the one that I feel most applies to Paul's agenda. </p>\n<p>( * There was a brief period shortly after reading the sequences that I found them extremely convincing, but I think I was much more credulous then than I am now. )</p>\n", "parentCommentId": "9jwqprEchspGn3XTY", "user": {"username": "jsteinhardt"}}, {"_id": "29fSP4x54ixsQ4QTF", "postedAt": "2017-07-11T15:59:21.000Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>I think the argument along these lines that I'm most sympathetic to is that Paul's agenda fits more into the paradigm of typical ML research, and so is more likely to fail for reasons that are in many people's collective blind spot (because we're all blinded by the same paradigm).</p>\n", "parentCommentId": "PwgnZL38s8o6y3qeM", "user": {"username": "jsteinhardt"}}, {"_id": "AbDLussBC8HMrWjqN", "postedAt": "2017-07-11T16:04:41.197Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>On capability amplification:</p>\n<p>MIRI's traditional goal would allow you to break cognition down into steps that we can describe explicitly and implement on transistors, things like &quot;perform a step of logical deduction,&quot; &quot;adjust the probability of this hypothesis,&quot; &quot;do a step of backwards chaining,&quot; etc. This division does not need to be competitive, but it needs to be reasonably close (close enough to obtain a decisive advantage).</p>\n<p>Capability amplification requires breaking cognition down into steps that humans can implement. This decomposition does not need to be competitive, but it needs to be efficient enough that it can be implemented during training. Humans can obviously implement more than transistors, the main difference is that in the agent foundations case you need to figure out every response in advance (but then can have a correspondingly greater reason to think that the decomposition will work / will preserve alignment).</p>\n<p>I can talk in more detail about the reduction from (capability amplification --&gt; agent foundations) if it's not clear whether it is possible and it would have an effect on your view.</p>\n<p>On competitiveness:</p>\n<p>I would prefer be competitive with non-aligned AI, rather than count on forming a singleton, but this isn't really a requirement of my approach. When comparing difficulty of two approaches you should presumably compare the difficulty of achieving a fixed goal with one approach or the other.</p>\n<p>On reliability:</p>\n<p>On the agent foundations side, it seems like plausible approaches involve figuring out how to peer inside the previously-opaque hypotheses, or understanding what characteristic of hypotheses can lead to catastrophic generalization failures and then excluding those from induction. Both of these seem likely applicable to ML models, though would depend on how exactly they play out.</p>\n<p>On the ML side, I think the other promising approaches involve either adversarial training, ensembling / unanimous votes, which could be applied to the agent foundations problem.</p>\n", "parentCommentId": "dG8nHwQP7N8JRHnwf", "user": {"username": "Paul_Christiano"}}, {"_id": "JN5r3c452zd6J37A2", "postedAt": "2017-07-11T17:45:15.332Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<blockquote>\n<p>I can talk in more detail about the reduction from (capability amplification --&gt; agent foundations) if it's not clear whether it is possible and it would have an effect on your view.</p>\n</blockquote>\n<p>Yeah, this is still not clear. Suppose we had a solution to agent foundations, I don't see how that necessarily helps me figure out what to do as H in capability amplification. For example the agent foundations solution could say, use (some approximation of) exhaustive search in the following way, with your utility function as the objective function, but that doesn't help me because I don't have a utility function.</p>\n<blockquote>\n<p>When comparing difficulty of two approaches you should presumably compare the difficulty of achieving a fixed goal with one approach or the other.</p>\n</blockquote>\n<p>My point was that HRAD potentially enables the strategy of pushing mainstream AI research away from opaque designs (which are hard to compete with while maintaining alignment, because you don't understand how they work and you can't just blindly copy the computation that they do without risking safety), whereas in your approach you always have to worry about &quot;how do I compete with with an AI that doesn't have an overseer or has an overseer who doesn't care about safety and just lets the AI use whatever opaque and potentially dangerous technique it wants&quot;.</p>\n<blockquote>\n<p>On the agent foundations side, it seems like plausible approaches involve figuring out how to peer inside the previously-opaque hypotheses, or understanding what characteristic of hypotheses can lead to catastrophic generalization failures and then excluding those from induction. </p>\n</blockquote>\n<p>Oh I see. In my mind the problems with Solomonoff Induction means that it's probably not the right way to define how induction should be done as an ideal, so we should look for something kind of like Solomonoff Induction but better, not try to patch it by doing additional things on top of it. (Like instead of trying to figure out exactly when CDT would make wrong decisions and add more complexity on top of it to handle those cases, replace it with UDT.) </p>\n", "parentCommentId": "AbDLussBC8HMrWjqN", "user": {"username": "Wei_Dai"}}, {"_id": "P9YFzdXWxRdgDH3mo", "postedAt": "2017-07-13T11:37:14.017Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>That actually didn't cross my mind before, so thanks for pointing it out. After reading your comment, I decided to look into Open Phil's recent grants to MIRI and OpenAI, and noticed that of the 4 technical advisors Open Phil used for the <a href=\"http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support\">MIRI grant investigation</a> (Paul Christiano, Jacob Steinhardt, Christopher Olah, and Dario Amodei), all either have a ML background or currently advocate a ML-based approach to AI alignment. For the <a href=\"http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/openai-general-support\">OpenAI grant</a> however, Open Phil didn't seem to have similarly engaged technical advisors who might be predisposed to be critical of the potential grantee (e.g., HRAD researchers), and in fact two of the Open Phil technical advisors are also employees of OpenAI (Paul Christiano and Dario Amodei). I have to say this doesn't look very good for Open Phil in terms of making an effort to avoid potential blind spots and bias.</p>\n", "parentCommentId": "29fSP4x54ixsQ4QTF", "user": {"username": "Wei_Dai"}}, {"_id": "kWHHXvjmh6vHTXMnz", "postedAt": "2017-07-13T15:16:20.208Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>(Speaking for myself, not OpenPhil, who I wouldn't be able to speak for anyways.)</p>\n<p>For what it's worth, I'm pretty critical of deep learning, which is the approach OpenAI wants to take, and still think the grant to OpenAI was a pretty good idea; and I can't really think of anyone more familiar with MIRI's work than Paul who isn't already at MIRI (note that Paul started out pursuing MIRI's approach and shifted in an ML direction over time).</p>\n<p>That being said, I agree that the public write-up on the OpenAI grant doesn't reflect that well on OpenPhil, and it seems correct for people like you to demand better moving forward (although I'm not sure that adding HRAD researchers as TAs is the solution; also note that OPP does consult regularly with MIRI staff, though I don't know if they did for the OpenAI grant).</p>\n", "parentCommentId": "P9YFzdXWxRdgDH3mo", "user": {"username": "jsteinhardt"}}, {"_id": "6x5K7Boo7eNfKmGCF", "postedAt": "2017-07-13T17:06:54.027Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<blockquote>\n<p>I can't really think of anyone more familiar with MIRI's work than Paul who isn't already at MIRI (note that Paul started out pursuing MIRI's approach and shifted in an ML direction over time).</p>\n</blockquote>\n<p>The <a href=\"https://agentfoundations.org/members\">Agent Foundations Forum</a> would have been a good place to look for more people familiar with MIRI's work. Aside from Paul, I see Stuart Armstrong, Abram Demski, Vadim Kosoy, Tsvi Benson-Tilsen, Sam Eisenstat, Vladimir Slepnev, Janos Kramar, Alex Mennen, and many others. (Abram, Tsvi, and Sam have since joined MIRI, but weren't employees of it at the time of the Open Phil grant.)</p>\n<blockquote>\n<p>That being said, I agree that the public write-up on the OpenAI grant doesn't reflect that well on OpenPhil, and it seems correct for people like you to demand better moving forward</p>\n</blockquote>\n<p>I had previously seen some complaints about the way the OpenAI grant was made, but until your comment, hadn't thought of a possible group blind spot due to a common ML perspective. If you have any further insights on this and related issues (like why you're critical of deep learning but still think the grant to OpenAI was a pretty good idea, what are your objections to Paul's AI alignment approach, how could Open Phil have done better), would you please write them down somewhere?</p>\n", "parentCommentId": "kWHHXvjmh6vHTXMnz", "user": {"username": "Wei_Dai"}}, {"_id": "Rqsdhq8y3vtQALe56", "postedAt": "2017-07-15T15:41:19.591Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Daniel, while re-reading <a href=\"https://ai-alignment.com/the-informed-oversight-problem-1b51b4f66b35\">one of Paul's posts from March 2016</a>, I just noticed the following:</p>\n<blockquote>\n<p>[ETA: By the end of 2016 this problem no longer seems like the most serious.]\n...\n[ETA: while robust learning remains a traditional AI challenge, it is not at all clear that it is possible. And meta-execution actually seems like the ingredient furthest from existing ML practice, as well as having non-obvious feasibility.]</p>\n</blockquote>\n<p>My interpretation of this is that between March 2016 and the end of 2016, Paul updated the difficulty of his approach upwards. (I think given the context, he means that other problems, namely robust learning and meta-execution, are harder, not that informed oversight has become easier.) I wanted to point this out to make sure you updated on his update. Clearly Paul still thinks his approach is more promising than HRAD, but perhaps not by as much as before.</p>\n", "parentCommentId": "tnuj9SM3QC7o2cgrF", "user": {"username": "Wei_Dai"}}, {"_id": "mqCMnJaLh7AG7Ptrn", "postedAt": "2017-07-19T03:41:40.499Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Or it could just assume the AI has an unbounded utility function (or bounded very highly). An AI could guess it only has a 1 in 1/B chance of reaching DSA, but that the payoff from reaching this is 100B higher than defecting early. Since there are 100B stars in the galaxy, it seems likely that in a multipolar situation with decent diversity of AIs, some would fulfill this criteria and decide to gamble.</p>\n", "parentCommentId": "nDRZC4Qest7NQ2yYT", "user": {"username": "Daniel_Eth"}}, {"_id": "k7LDv4gxAgWZ3NJEC", "postedAt": "2017-07-23T05:18:29.895Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>My suspicion is that MIRI agrees with you - if you read their <a href=\"https://intelligence.org/2017/04/30/software-engineer-internship-staff-openings/\">job post on their software engineering internship</a>, it seems that they're looking for people who can rapidly prototype and test AI Alignment ideas that have implications in machine learning. </p>\n", "parentCommentId": "AcoprFn3Z6DpiguXz", "user": {"username": "LawrenceC"}}, {"_id": "gbicApyErLku37ivy", "postedAt": "2017-07-23T05:24:51.064Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Not super relevant to Peter's question, but I would be interested in hearing why you're bullish on the Far Future EA Fund. </p>\n", "parentCommentId": "cQLNTLhXXA6WEPBpj", "user": {"username": "LawrenceC"}}, {"_id": "ZWooFDnSFYdAHt7zZ", "postedAt": "2017-08-06T07:32:08.441Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>Will - I think &quot;meta-reasoning&quot; might capture what you mean by &quot;meta-decision theory&quot;.  Are you familiar with this research (e.g. Nick Hay did a thesis w/Stuart Russell on this topic recently)?</p>\n<p>I agree that bounded rationality is likely to loom large, but I don't think this means MIRI is barking up the wrong tree... just that other trees also contain parts of the squirrel.</p>\n", "parentCommentId": "f3r6TuAC3MCxwm9Yb", "user": {"username": "capybaralet"}}, {"_id": "eZN6EFq4WSSS3QaxN", "postedAt": "2017-08-06T11:55:15.349Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<blockquote>\n<p>My point was that HRAD potentially enables the strategy of pushing mainstream AI research away from opaque designs (which are hard to compete with while maintaining alignment, because you don't understand how they work and you can't just blindly copy the computation that they do without risking safety), whereas in your approach you always have to worry about &quot;how do I compete with with an AI that doesn't have an overseer or has an overseer who doesn't care about safety and just lets the AI use whatever opaque and potentially dangerous technique it wants&quot;.</p>\n</blockquote>\n<p>I think <em>both</em> approaches potentially enable this, but are VERY unlikely to deliver.  MIRI seems more bullish that fundamental insights will yield AI that is just plain better (Nate gave me the analogy of Judea Pearl coming up with Causal PGMs as such an insight), whereas Paul just seems optimistic that we can get a somewhat negligible performance hit for safe vs. unsafe AI.</p>\n<p>But I don't think MIRI has given very good arguments for why we might expect this; it would be great if someone can articulate or reference the best available arguments.</p>\n<p>I have a very strong intuition that dauntingly large safety-performance trade-offs are extremely likely to persist in practice, thus the only answer to the &quot;how do I compete&quot; question seems to be &quot;be the front-runner&quot;.</p>\n", "parentCommentId": "JN5r3c452zd6J37A2", "user": {"username": "capybaralet"}}, {"_id": "DFzD4YZKnANWDeKnS", "postedAt": "2017-08-11T01:15:51.989Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>My main comments:</p>\n<ol>\n<li><p>As others have mentioned: great post! Very illuminating!</p>\n</li>\n<li><p>I agree value-learning is the main technical problem, although I\u2019d also note that value-learning related techniques are becoming much more popular in mainstream ML these days, and hence less neglected.  Stuart Russell has argued (and I largely agree) that things like IRL will naturally become a more popular research topic (but I\u2019ve also argued this might not be net-positive for safety: <a href=\"http://lesswrong.com/lw/nvc/risks_from_approximate_value_learning/\">http://lesswrong.com/lw/nvc/risks_from_approximate_value_learning/</a>)</p>\n</li>\n<li><p>My main comment wrt the value of HRAD (3a) is: <strong>I think HRAD-style work is more about problem definitions than solutions</strong>.  So I find it to be somewhat orthogonal to the other approach of \u201clearning to reason from humans\u201d (L2R).  We don\u2019t have the right problem definitions, at the moment; we know that the RL framework is a leaky abstraction.  I think MIRI has done the best job of identifying the problems which could result from our current leaky abstractions, and working to address them by improving our understanding of what problems need to be solved.</p>\n</li>\n<li><p>It\u2019s also not clear that human reasoning can be safely amplified; the relative safety of existing humans may be due to our limited computational / statistical resources, rather than properties of our cognitive algorithms.  But this argument is not as strong as it seems; see comment #3 below.</p>\n</li>\n</ol>\n<p>A few more comments:</p>\n<ol>\n<li><p>RE 3b: I don\u2019t really think the AI community\u2019s response to MIRI\u2019s work is very informative, since it\u2019s just not on people\u2019s radar.  The problems and not well known or understood, and the techniques are (AFAIK) not very popular or in vogue (although I\u2019ve only been in the field for 4 years, and only studied machine-learning based approaches to AI).  I think decision theory was already a relatively well known topic in philosophy, so I think philosophy would naturally be more receptive to these results.</p>\n</li>\n<li><p>I\u2019m unconvinced about the feasibility of Paul\u2019s approach**, and share Wei Dai\u2019s concerns about it hinging on a high level of competitiveness.  But I also think HRAD suffers from the same issues of competitiveness (this does not seem to be MIRI\u2019s view, which I\u2019m confused by).  This is why I think solving global coordination is crucial.</p>\n</li>\n<li><p>A key missing (assumed?) argument here is that L2R can be a stepping stone, e.g. providing narrow or non-superintelligent AI capabilities which can be applied to AIS problems (e.g. making much more progress on HRAD than MIRI).  To me this is a key argument for L2R over HRAD, and generally a source of optimism.  I\u2019m curious if this argument plays a significant role in your thought; in other words, is it that HRAD problems don\u2019t need to be solved, or just that the most effective solution path goes through L2R?  I\u2019m also curious about the <em>counter-argument</em> for pursuing HRAD now: i.e. what role does MIRI anticipate safe advanced (but not general / superhuman) intelligent systems to play in HRAD?  </p>\n</li>\n<li><p>An argument for more funding for MIRI which isn\u2019t addressed is the apparent abundance of wealth at the disposal of Good Ventures.  Since funding opportunities are generally scarce in AI Safety, I think every decent opportunity should be aggressively pursued.  There are 3 plausible arguments I can see for the low amount of funding to MIRI: 1) concern of steering other researchers in unproductive directions 2) concern about bad PR 3) internal politics.</p>\n</li>\n<li><p>Am I correct that there is a focus on shorter timelines (e.g. &lt;20 years)?</p>\n</li>\n</ol>\n<p>Briefly, my overall perspective on the future of AI and safety relevance is:</p>\n<ol>\n<li><p>There ARE fundamental insights missing, but they are unlikely to be key to building highly capable OR safe AI.</p>\n</li>\n<li><p>Fundamental insights might be crucial for achieving high confidence in a putatively safe AI (but perhaps not for developing an AI which is actually safe).</p>\n</li>\n<li><p>HRAD line of research is likely to uncover mostly negative results (ala AIXI\u2019s arbitrary dependence on prior)</p>\n</li>\n<li><p>Theory is behind empiricism, and the gap is likely to grow; this is the main reason I\u2019m a bit pessimistic about theory being useful.  On the other hand, I think most paths to victory involve using capability-control for as long as possible while transitioning to completely motivation-control based approaches, so conditioning on victory, it seems more likely that we solve more fundamental problems (i.e. \u201cwe have to solve these problems eventually\u201d).</p>\n</li>\n</ol>\n<p>** the two main reasons are: 1) I don\u2019t think it will be competitive and 2) I suspect it will be difficult to prevent compounding errors in a bootstrapping process that yields superintelligent agents. </p>\n", "parentCommentId": null, "user": {"username": "capybaralet"}}, {"_id": "rsHsaDLT4ynLHxWTY", "postedAt": "2017-08-11T04:28:35.187Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<blockquote>\n<p>MIRI's current size seems to me to be approximately right for this purpose, and as far as I know MIRI staff don't think MIRI is too small to continue making steady progress.</p>\n</blockquote>\n<p>My guess is that this intuition is relatively inelastic to MIRI's size. It might be worth trying to generate the counterfactual intuition here if MIRI were half its size or double its size. If that process outputs a similar intuition, it might be worth attempting to forget how many people MIRI employs in this area, and ask how many people should be working on a topic that by your estimation has a 10% chance of being instrumental to an existential win. Though my number is higher than 10%, I think even if I had that estimate, my answer to the number of people that should be working on that topic would be &quot;as many as are available.&quot;</p>\n", "parentCommentId": null, "user": {"username": "Michael_Cohen"}}, {"_id": "jycw2D9SEq9RCsdJE", "postedAt": "2017-10-10T16:30:34.849Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>it's cross-posted on LW: <a href=\"http://lesswrong.com/lw/p85/daniel_dewey_on_miris_highly_reliable_agent/\">http://lesswrong.com/lw/p85/daniel_dewey_on_miris_highly_reliable_agent/</a></p>\n", "parentCommentId": null, "user": {"username": "capybaralet"}}, {"_id": "nfP9FPgF9fcpc7ecJ", "postedAt": "2018-11-13T19:08:15.961Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>(Eli&#x27;s personal notes, mostly for his own understanding. Feel free to respond if you want.)</p><blockquote>1. It seems pretty likely that early advanced AI systems won&#x27;t be understandable in terms of HRAD&#x27;s formalisms, in which case HRAD won&#x27;t be useful as a description of how these systems should reason and make decisions.</blockquote><p>My current guess is that the finalized HRAD formalisms would be general enough that they will provide meaningful insight into early advanced AI systems (even supposing that the development of those early systems is not influenced by HRAD ideas), in much the same way that Pearlean causality and Bayes nets gives (a little) insight into what neural nets are doing. </p>", "parentCommentId": null, "user": {"username": "Elityre"}}, {"_id": "xJtKBTdTKsB7ASjJj", "postedAt": "2021-12-13T15:52:15.750Z", "postId": "SEL9PW8jozrvLnkb4", "htmlBody": "<p>One of the most straightforward and useful introductions to MIRIs work that I've read.</p>\n", "parentCommentId": null, "user": {"username": "Kerry_Vaughan"}}]