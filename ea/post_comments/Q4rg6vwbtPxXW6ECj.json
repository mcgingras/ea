[{"_id": "QbLnK5AvE55iKTX4x", "postedAt": "2023-03-16T15:01:14.258Z", "postId": "Q4rg6vwbtPxXW6ECj", "htmlBody": "<p>Since the expected harms from AI are obviously <i>much</i> smaller in expectation in an extreme \"stochastic parrot\" world where we don't have to worry at all about X-risk from superintelligent systems, it actually does very much matter whether you're in that world if you're proposing a general attempt to block AI progress: &nbsp;if the expected harms from further commercial development of AI are much smaller, they are much more likely to be outweighed by the expected benefits.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Dr. David Mathers"}}, {"_id": "Ak5Gofy6Lfo8iQqGR", "postedAt": "2023-03-16T15:05:51.908Z", "postId": "Q4rg6vwbtPxXW6ECj", "htmlBody": "<p>I think this is a) not necessarily true (as shown in the essay, it could still lead to existential catastrophe eg by integration with nuclear command and control) and b) if we ought to sll be pulling in the same direction against these companies, why is the magnitude difference relevant.\nMoreover, your claims would suggest that the AI Ethics crowd would be more pro-AI development than the AI Safety crowd. In practice, the opposite is true, so I'm not really sure why your argument holds</p>\n", "parentCommentId": "QbLnK5AvE55iKTX4x", "user": {"username": "Gideon Futerman"}}, {"_id": "QxqCqchzbmE3vnyHH", "postedAt": "2023-03-16T15:24:36.968Z", "postId": "Q4rg6vwbtPxXW6ECj", "htmlBody": "<p>'think this is a) not necessarily true (as shown in the essay, it could still lead to existential catastrophe eg by integration with nuclear command and control) '<br><br>The *expected* harm can still be much lower, even if the threat is not zero. I also think 'they might get integrated with nuclear command and control' naturally suggests <i>much</i> more targeted action than does \"they are close to superintelligent systems and any superintelligent systems is mega dangerous no matter what its designed for\".&nbsp;<br><br>'if we ought to sll be pulling in the same direction against these companies, why is the magnitude difference relevant'<br><br>Well, it's not relevant if X-risk from superintelligence is in fact significant. But I was talking about the world where it isn't. In that world, we possibly shouldn't be pulling against the companies overall at all: merely showing that there are still <i>some</i> harms from their actions is not enough to show that we should be all-things-considered against them. Wind farms impose some externalities on wild-life, but that doesn't mean they are overall bad.&nbsp;<br><br>'Moreover, your claims would suggest that the AI Ethics crowd would be more pro-AI development than the AI Safety crowd. In practice, the opposite is true, so I'm not really sure why your argument holds'<br><br>I don't think so. Firstly, people are not always rational. I am suspicious that a lot of the ethics crowds sees AI/tech companies/enthusiasm about AI, as a sort of like a symbol of a particular kind of masculinity that they, as a particular kind of American liberal feminists dislike. This in my view, biases them in favor of the harms outweighing the benefits, and is also related to a particular style of US liberal identity politics where once a harm has been identified and associated with white maleness the harmful thing must be rhetorically nuked from orbit, and any attempt to think about trade-offs is pathetic excuse making. Secondly, I think many of the AI safety crowd &nbsp;just really like AI and think its cool: roughly they see it as a symbol of the same kind of stuff as the enemies, it's just, they like that stuff, and its tied up with their self-esteem. Secondly, I think many of them hope strongly for something like paradise/immortality through 'good AI' just as much as they fear the bad stuff. Maybe that's all excessively cynical, and I don't hold the first view about the ethics people all that strongly, &nbsp;but I think a wider 'people are not always rational' point applies. In particular, people are often quite scope-insensitive. So just because Bob and Alice both think Xs is harmful, but Alice's view implies it is super-mega deadly harmful, and Bob's view just that it is pretty harmful, doesn't necessarily mean Bob will denounce it less passionately than Alice.&nbsp;<br>&nbsp;</p>", "parentCommentId": "Ak5Gofy6Lfo8iQqGR", "user": {"username": "Dr. David Mathers"}}, {"_id": "c5K6F8yGRriPmKLg7", "postedAt": "2023-03-16T16:11:26.947Z", "postId": "Q4rg6vwbtPxXW6ECj", "htmlBody": "<p>'expected harm can be still much lower' this may be correct, but not convinced its orders of magnitude. And it also hugely depends on ones ethical viewpoint. My argument here isn't that under all ethical theories this difference doesn't matter (it obviously does), but that to the actions of my proposed combined AI Safety and Ethics knowledge network that this distinction actually matters very little. &nbsp;This I think answers your second point as well; I am addressing this call to people who broadly think that on the current path, risks are too high. If you think we are nowhere near AGI and that near term AI harms aren't that important, then this essay simply isn't addressed to you.&nbsp;</p><p>I think this is the core point I'm making. It is not that the stochastic parrots vs superintelligence distinction is &nbsp;necessarily irrelevant if one is deciding for oneself if to care about AI. However, once one thinks that the dangers of the status quo are too high for whatever reason, &nbsp;then the distinction stops mattering very much.&nbsp;</p>", "parentCommentId": "QxqCqchzbmE3vnyHH", "user": {"username": "Gideon Futerman"}}, {"_id": "Mt5JqiT8hHtyWR7E4", "postedAt": "2023-03-16T18:17:16.218Z", "postId": "Q4rg6vwbtPxXW6ECj", "htmlBody": "<p>Some thoughts I had while reading that I expect you'd agree with:</p><ol><li>There is probably a lot of overlap in the kinds of interventions that (some) AI safety folks would be on board with and the kinds of interventions that (some) AI ethics folks would be on board with. For example, it seems like (many people in) both groups have concerns about the rate of AI progress and would endorse regulations/policies that promote safe/responsible AI development.</li><li>Given recent developments in AI, and apparent <a href=\"https://www.lesswrong.com/posts/M3iPAmxZwy4gPXdXw/the-public-supports-regulating-ai-for-safety\">interest in regulation that promotes safety</a>, it seems like now might be a particularly good time for people to think seriously about how the AIS community and the AI ethics community could work together.</li><li>Despite differences, it would be surprising if there was rather little that the \"two\" communities could learn from each other.</li><li>I appreciate the links and examples. I'll probably go through them at some point soon and possibly DM you. I think a lot of people are interested in this topic, but few have the time/background to actually \"do research\" and \"compile resources\". It seems plausible to me that more \"lists of resources/examples/case studies\" could improve reasoning on this topic (even moreso than high-level argumentation, and I say that as someone who's often advocating for more high-level argumentation!)</li></ol><p>Some thoughts I had while reading that you might disagree with (or at least I didn't see acknowledged much in the post):</p><ol><li>The differences between the two groups are not trivial, and they'll often lead to different recommendations. For example, if you brought ARC Evals together with (hypothetical) AI Ethics Evals, I imagine they would both agree \"evals are important\" but they would have strong and serious disagreements about what <i>kinds</i> of evals should be implemented.</li><li>In general, when two groups with different worldviews/priorities join coalitions, a major risk is that one (or both) of the groups' goals get diluted.&nbsp;</li><li>It's harder to maintain good epistemics and strong reasoning + reasoning transparency in large coalitions of groups who have different worldviews/goals. (\"We shouldn't say X because our allies in AI ethics will think it's weird.\") I don't think \"X is bad for epistemics\" means \"we definitely shouldn't consider X\", but I think it's a pretty high cost that often goes underappreciated/underacknowledged (Holden made a <a href=\"https://www.lesswrong.com/posts/jwhcXmigv2LTrbBiB/success-without-dignity-a-nearcasting-story-of-avoiding#fn12\">similar point</a> recently).&nbsp;</li><li>In general, I think the piece could have benefitted from expressing more uncertainty around certain claims, acknowledging counterarguments more, and trying to get an <a href=\"https://www.lesswrong.com/tag/ideological-turing-tests\">ITT</a> of people who disagree with you.&nbsp;</li></ol>", "parentCommentId": null, "user": {"username": "Akash"}}, {"_id": "mvKx5bWdetHdjbovX", "postedAt": "2023-03-16T18:20:29.736Z", "postId": "Q4rg6vwbtPxXW6ECj", "htmlBody": "<p>Just quickly on that last point: I recognise there is a lot of uncertainty (hence the disclaimer at the beginning). I didn't go through  the possible counterarguments because the piece was already so long! Thanks for your comment though, and I will get to the rest of it later!</p>\n", "parentCommentId": "Mt5JqiT8hHtyWR7E4", "user": {"username": "Gideon Futerman"}}, {"_id": "Rcjobrh4wwcJEbm8A", "postedAt": "2023-03-16T22:26:36.404Z", "postId": "Q4rg6vwbtPxXW6ECj", "htmlBody": "<p>'It's harder to maintain good epistemics and strong reasoning + reasoning transparency in large coalitions of groups who have different worldviews/goals. (\"We shouldn't say X because our allies in AI ethics will think it's weird.\") I don't think \"X is bad for epistemics\" means \"we definitely shouldn't consider X\", but I think it's a pretty high cost that often goes underappreciated/underacknowledged'<br><br>This is probably a real epistemic cost in my view, but it takes more than identifying a cost to establish that forming a coalition with people with different goals/beliefs is overall epistemically costly, given that doing so also has positive effects like bringing in knowledge that we don't have because no group knows everything.&nbsp;</p>", "parentCommentId": "Mt5JqiT8hHtyWR7E4", "user": {"username": "Dr. David Mathers"}}, {"_id": "MSduGLPrpCGi6Anrp", "postedAt": "2023-03-17T21:00:30.761Z", "postId": "Q4rg6vwbtPxXW6ECj", "htmlBody": "<p>I am not speaking for the DoD, the US government, or any of my employers.</p><p>I think that your claim about technological inevitability is premised on the desire of states to regulate key technologies, sometimes mediated by public pressure. All of the examples listed were blocked for decades by regulation, sometimes supplemented with public fear, soft regulation, etc. That's fine so long as, say, governments don't consider advancements in the field a core national interest. The US and China do, and often in an explicitly securitized form.</p><p>Quoting <a href=\"https://www.cnas.org/publications/reports/understanding-chinas-ai-strategy\">CNAS</a></p><blockquote><h3><strong>China\u2019s leadership \u2013 including President Xi Jinping \u2013 believes that being at the forefront in AI technology is critical to the future of global military and economic power competition.</strong></h3></blockquote><p>English-language Coverage of the US tends to avoid such sweeping statements, because readers have more local context, because political disagreement is more public, and because readers expect it.</p><p>But the DoD in the most recent National Defense Strategy identified AI as a secondary priority. Trump and Biden identified it as an area to maintain and advance national leadership in. And, of course, with the US at the head they don't need to do as much in the way of directing people, since the existing system is delivering adequate results.</p><p>Convincing the two global superpowers <i>not</i> to develop <a href=\"https://acoup.blog/2020/03/20/collections-why-dont-we-use-chemical-weapons-anymore/\">militarily useful </a>technology while tensions are rising is going to be the first time in history that has ever been accomplished.</p><p>That's not to say that we can't slow it down. But AI very much is inevitable if it is useful, and it seems like it will be very useful.</p>", "parentCommentId": null, "user": {"username": "keller_scholl"}}, {"_id": "Bny5dA2KYHGdTM7Kc", "postedAt": "2023-03-18T00:57:32.512Z", "postId": "Q4rg6vwbtPxXW6ECj", "htmlBody": "<p>A number of things. Firstly, this criticism may be straightforwardly correct; it may be pursuing something that is the first time in history (I'm less convinced eg bioweapons regulation etc) ; nonetheless, other approaches to TAI governance seem similar (eg trust 1 actor to develop a transformative and risky technology and not use it for ill). It may indeed require such change, or at least change of perceptionof the potential and danger of AI (which is possible).\nSecondly, this may not be the case. Foundation models (our present worry) may be no more (or even less) beneficial in military contexts than narrow systems. Moreover, foundation models, developed by private actors, seem pretty challenging to their power  in a way that neither the Chinese government nor US military is likely to accept. Thus, AI development may continue without dangerous model growth.\nFinally, very little development of foundation models are driven by military actors, and the actors that do develop it may be constructed as legitimately trying to challenge state power. If we are on a path to TAI (we may not be), then it seems in the near term only a very small number of actors, all private, could develop it. Maybe the US Military could gain the capacity to, but it seems hard at the moment for them to</p>\n", "parentCommentId": "MSduGLPrpCGi6Anrp", "user": {"username": "Gideon Futerman"}}, {"_id": "QwddFeZ3N6ZQmdA9F", "postedAt": "2023-03-27T21:56:41.159Z", "postId": "Q4rg6vwbtPxXW6ECj", "htmlBody": "<p>Interesting that you don't think the post acknowledged your second collection of points. I thought it mostly did.&nbsp;<br>1. The post did say it was not suggesting to shut down existing initiatives. So where people disagree on (for example) which evals to do, they can just do the ones they think are important and then both kinds get done. I think the post was identifying a third set of things we can do together, and this was not specific evals, but more about big narrative alliance when influencing large/important audiences. The post also suggested some other areas of collaboration, on policy and regulation, and some of these may relate to evals so there could be room for collaboration there, but I'd guess that more demand, funding, infrastructure for evals helps both kinds of evals.<br>2. Again I think the post addresses this issue: it talks about how there is this specific set of things the two groups can work on together that is both in their interest to do. It doesn't mean that all people from each group will only work on this new third thing (coalition building), but if a substantial number do, it'll help. I don't think the OP was suggesting a full merger of the groups. They acknowledge the 'personal and ethical problems with one another; [and say] that needn\u2019t translate to political issues'. The call is specifically for political coalition building.<br>3. Again I don't think the OP is calling for a merger of the groups. They are calling for collaborating on something.<br>4. OK the post didn't do this that much, but I don't think every post needs to and I personally really liked that this one made its point so clearly. I would read a post which responds to this with some counterarguments with interest so maybe that implies I think it'd benefit from one too, but I wouldn't want a rule/social expectation that every post lists counterarguments as that can raise the barrier to entry for posting and people are free to comment in disagreements and write counter posts.</p>", "parentCommentId": "Mt5JqiT8hHtyWR7E4", "user": {"username": "tamgent"}}, {"_id": "yXBa7XtHK7ZnTsTvY", "postedAt": "2023-03-27T22:33:45.376Z", "postId": "Q4rg6vwbtPxXW6ECj", "htmlBody": "<p>Ye I basically agree with this.</p><ol><li>On evals, I think it is good for us to be doing as much evals as possible, firstly because both sorts of evaluations are important, but also more (even self imposed) regulatory hurdles to jump through, the better. Slow it down and bring the companies under control.&nbsp;</li><li>Indeed, the call is a broader political coalition building. Not everyone, not all the time, not on everything. But on substantially more than we currently are.</li><li>Yes</li><li>There are a number of counterarguments to this post, but I didn't include them because a) I probably can't give the strongest counterarguments to my own beliefs b) This post was already very long, and I had to cut out sections already on Actor-Network Theory and Agency and something else I can't remember c) I felt it might muddle the case I'm trying to make here if it was intersperced with counterarguments. One quick point on counterarguments is I think a counterargument would need to be strong enough to not just prove that the extreme end result is bad ( a lot more coalition building would be bad ) , but probably that the post is directionally bad (some more coalition building would be bad).&nbsp;</li></ol>", "parentCommentId": "QwddFeZ3N6ZQmdA9F", "user": {"username": "Gideon Futerman"}}]