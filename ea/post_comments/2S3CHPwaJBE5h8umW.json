[{"_id": "3cZqCEPADj4bNsaK2", "postedAt": "2022-12-23T13:52:24.064Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>To complement your recommendation, I would also add that Yudowsky's Sequences end up transmitting a somewhat packaged worldview, and I think that there are some dangers in that.</p><p>I agree that their summarization work is valuable, but some more unmediated original sources which could transmit some of the same value might be:</p><ul><li>Probability Theory: The Logic of Science, or some other probability theory <a href=\"https://www.lesswrong.com/posts/xg3hXCYQPJkwHyik2/the-best-textbooks-on-every-subject\">textbook</a></li><li>An introduction to general semantics</li><li>An introduction to CBT, e.g., <a href=\"https://en.wikipedia.org/wiki/Feeling_Good%3A_The_New_Mood_Therapy\">Feeling Good</a></li><li>Thinking Fast and Slow</li><li>How to Measure Anything</li><li>Surely You're Joking Mr Feynman</li><li>Stranger in a Strange Land, David's Sling, The Moon is a Harsh Mistress.</li><li>Antifragile/Black Swan</li><li>The Elephant in the Brain</li><li>Superforecasting, making 100 forecasts and keeping track each week.</li><li>The Rationality Quotient, or some other &nbsp;Keith Stanovich book.</li><li>Some intro to nonviolent communication</li></ul><p>Because Yudkowsky's sequences are so long, and because I think that there is more value in reading the original sources, I'd probably lean towards recommending those instead.</p>", "parentCommentId": null, "user": {"username": "NunoSempere"}}, {"_id": "cZ4tdtfDEGuMopXyD", "postedAt": "2022-12-23T15:35:58.199Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>There is a good Cold Takes blog on the '<a href=\"https://www.cold-takes.com/the-bayesian-mindset/\">Bayesian mindset</a>' - which gets at something related to this as '~20-minute read rather than the &gt;1000 pages of Rationality: A-Z (aka The Sequences).'</p><p>Summary:</p><blockquote><blockquote><p>This piece is about the in-practice pros and cons of trying to think in terms of probabilities and expected value for real-world decisions, including decisions that don\u2019t obviously lend themselves to this kind of approach.</p><p>The mindset examined here is fairly common in the \u201ceffective altruist\u201d and \u201crationalist\u201d communities, and there\u2019s quite a bit of overlap between this mindset and that of <a href=\"https://www.lesswrong.com/rationality\"><strong><u>Rationality: A-Z</u></strong></a> (aka The Sequences), although there are some differing points of emphasis.<a href=\"https://www.cold-takes.com/p/812e650a-9dfe-4fbd-abed-b01ec4979dd2/#fn1\"><strong><sup><u>1</u></sup></strong></a> If you\u2019d like to learn more about this kind of thinking, this piece presents a ~20-minute read rather than the &gt;1000 pages of Rationality: A-Z.</p><p>This piece is a rough attempt to capture the heart of the ideas behind rationalism, and I think a lot of the ideas and habits of these communities will make more sense if you\u2019ve read it, though I of course wouldn\u2019t expect everyone in those communities to think I\u2019ve successfully done this.</p><p>If you\u2019re already deeply familiar with this way of thinking and just want my take on the pros and cons, you might skip to <a href=\"https://www.cold-takes.com/p/812e650a-9dfe-4fbd-abed-b01ec4979dd2/#use-cases-pros-and-cons-of-the-bayesian-mindset\"><strong><u>Pros and Cons</u></strong></a>. If you want to know why I'm using the term \"Bayesian mindset\" despite not mentioning Bayes's rule much, see footnote 3.</p></blockquote><p>&nbsp;</p><p>This piece is about the \u201cBayesian mindset,\u201d my term for a particular way of making decisions. In a nutshell, the Bayesian mindset is trying to approximate an (unrealistic) ideal of making every decision based entirely on <strong>probabilities</strong> and <strong>values</strong>, like this:</p><p><i>Should I buy travel insurance for $10? I think there's about a <strong>1%</strong> chance I'll use it (probability - blue), in which case it will get me a <strong>$500</strong> airfare refund (value - red). Since <strong>1%</strong> * <strong>$500</strong> = <strong>$5</strong>, I should not buy it for $10.</i></p><p>(Two more examples <a href=\"https://www.cold-takes.com/p/812e650a-9dfe-4fbd-abed-b01ec4979dd2/#appendix\"><strong><u>below</u></strong></a> in case that\u2019s helpful.)</p><p>The ideal here is called <strong>expected utility maximization (EUM)</strong>: making decisions that get you the highest possible <a href=\"https://www.cold-takes.com/expected-value/\"><strong><u>expected value</u></strong></a> of what you care about.<a href=\"https://www.cold-takes.com/p/812e650a-9dfe-4fbd-abed-b01ec4979dd2/#fn2\"><strong><sup><u>2</u></sup></strong></a> (I\u2019ve put clarification of when I\u2019m using \u201cEUM\u201d and when I\u2019m using \u201cBayesian mindset\u201d in a footnote, as well as notes on what \"Bayesian\" refers to in this context, but it isn\u2019t ultimately that important.<a href=\"https://www.cold-takes.com/p/812e650a-9dfe-4fbd-abed-b01ec4979dd2/#fn3\"><strong><sup><u>3</u></sup></strong></a>)</p><p>It\u2019s rarely practical to literally spell out all the numbers and probabilities like this. But some people think you should do so when you can, and when you can\u2019t, use this kind of framework as a \u201cNorth Star\u201d - an ideal that can guide many decisions even when you don\u2019t do the whole exercise.</p><p>Others see the whole idea as much less promising.</p><p>I think it's very useful to understand the pros and cons, and I think it's good to have the Bayesian Mindset as one option for thinking through decisions. I think it's especially useful for decisions that are (a) important; (b) altruistic (trying to help others, rather than yourself); (c) \u201cunguided,\u201d in the sense that normal rules of thumb aren\u2019t all that helpful.</p><p>In the rest of this piece, I'm going to walk through:</p><ul><li>The \"dream\" behind the Bayesian mindset.<ul><li>If we could put the practical difficulties aside and make every decision this way, we'd be able to understand disagreements and debates much better - including debates one has with oneself. In particular, we'd know which parts of these disagreements and debates are <strong>debates about how the world is (probabilities)</strong> vs. <strong>disagreements in what we care about (values)</strong>.</li><li>When debating probabilities, we could make our debates impersonal, accountable, and focused on finding the truth. Being right just means you have put the right probabilities on your predictions. Over time, it should be possible to see who has and has not made <i>good </i>predictions. Among other things, this would put us in a world where bad analysis had consequences.</li><li>When disagreeing over values, by contrast, we could all have transparency about this. If someone wanted you to make a certain decision for their personal benefit, or otherwise for values you didn\u2019t agree with, they wouldn\u2019t get very far asking you to trust them.</li></ul></li><li>The \"how\" of the Bayesian mindset - what kinds of practices one can use to assign reasonable probabilities and values, and (hopefully) come out with reasonable decisions.</li><li>The pros and cons of approaching decisions this way.</li></ul></blockquote>", "parentCommentId": null, "user": {"username": "HaukeHillebrandt"}}, {"_id": "ytigA6bdujJfc8XG6", "postedAt": "2022-12-23T15:40:18.191Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>Thanks for the comment and the list of recommendations. I have read most of the things on that list and the ones that I did read, I thought were great and have recommended a bunch to others, especially Probability Theory: The Logic of Science, The Elephant in the Brain, and forecasting practice. I agree that there are some dangers in recommending something that is pretty packaged but I think there is an obvious benefit in that it feels like a distilled version of reading a bunch of valuable things. Per unit time, I found reading the sequences more insightful/useful to me than the sources which it gets its ideas from (even if I read those things before the sequences, I am fairly confident).&nbsp;<br><br>I don't want to oversell the sequences, I think the ideas in them have been mentioned in other places earlier. In my post, I mentioned specifically what ideas I found valuable so that people who are already familiar with them or think they are not that useful can decide not to read them. That isn't rhetorical, I have some wise friends who are pretty well-read on philosophy and economics, and a lot of the things in the sequences I found novel, they were already familiar with.<br><br>My recommendation would look very different for someone who read the sequences and then made that their whole personality. I do know of some people who overrate them but in some of my specific circles (EA uni groups for eg), I think they are underrated/not given a chance which is why I wrote this post.&nbsp;</p>", "parentCommentId": "3cZqCEPADj4bNsaK2", "user": {"username": "Quadratic Reciprocity"}}, {"_id": "KrKE8XreruySM6f4S", "postedAt": "2022-12-23T17:38:35.146Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>I agree that the sequences have lots of interesting ideas and well-written articles.&nbsp;</p><p>However, I think it's worth noting that they were primarily written by one guy. When one person is writing on such widely disparate topics as quantum physics, morality, decision theory, philosophy of science, future predictions, theory of mind, etc, without a formal education in any of these topics, it's understandable that flawed ideas and errors will creep in. As an example, the article \"<a href=\"https://www.lesswrong.com/posts/vrHRcEDMjZcx5Yfru/i-defy-the-data\">I defy the data</a>\" gives a ridiculously incorrect picture of how actual modern scientists operate.&nbsp;</p><p>I think the sequences are okay intro points for a number of topics, but they should not be treated as the foundation of ones belief system, and should be supplemented with domain specific knowledge from experts in each of the fields mentioned, including critiques of the sequences and \"rationalism\" as a movements.&nbsp;</p>", "parentCommentId": null, "user": {"username": "titotal"}}, {"_id": "nbwsmd3eKRGaFcS6M", "postedAt": "2022-12-23T19:40:48.552Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>What would you say is the core message of the Sequences? Naturalism is true? Bayesianism is great? Humans are naturally very irrational and have to put effort if they want to be rational?</p>\n<p>I've read the Sequences almost twice, first time was fun because Yudkowsky was optimistic back then, but during the second time I was constantly aware that Yudkowsky believes along the lines of his 'Death with dignity' post that our doom is virtually certain and he has no idea how to even begin formulate a solution. If Yudkowsky, who wrote the Sequences on his own, who founded the modern rationalist movement on his own, who founded MIRI and the AGI alignment movement on his own, has no idea where to even begin looking for a solution, what hope do I have? I probably couldn't do anything comparable to those things on my own even if I tried my hardest for 30 years. I could thoroughly study everything Yudkowsky and MIRI have studied, which would be a lot, and after all that effort I would be in the same situation Yudkowsky is right now - no idea where to even begin looking for a solution and only knowing which approaches don't work. The only reason to do it is to gain a fraction of a dignity point, to use Yudkowsky's way of thinking.</p>\n<p>To be clear, I don't have a fixed model in my head about AI risk, I think I can sort of understand what Yudkowsky's model is and I can understand why he is afraid, but I don't know if he's right because I can also sort of understand the models of those who are more optimistic. I'm pretty agnostic when it comes to this subject and I wouldn't be particularly surprised by any specific outcome.</p>\n", "parentCommentId": null, "user": {"username": "LoveAndPeaceAlways"}}, {"_id": "cr8scpaxir5XewvB7", "postedAt": "2022-12-23T19:56:09.847Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>Huh, I think this list of books covers less than half of the ideas in the sequences, so I don't really think this counts as \"the original sources\". Topics that get pretty extensively covered in the sequences but are absent here:&nbsp;</p><ul><li>Evolutionary biology + psychology</li><li>AI Existential Risk</li><li>Metaethics &amp; fragility of value</li><li>Something about courage/willing to do hard things/Something to protect (a recurring theme in the sequences and absent in all of the above)</li><li>Decision theory</li><li>Lots of other stuff.&nbsp;</li></ul><p>Like, I don't know, let's look at some randomly selected sequence on LessWrong:&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c09b13c9a5c9de58aaa39f84623bfe8787f283d6498eda02.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c09b13c9a5c9de58aaa39f84623bfe8787f283d6498eda02.png/w_150 150w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c09b13c9a5c9de58aaa39f84623bfe8787f283d6498eda02.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c09b13c9a5c9de58aaa39f84623bfe8787f283d6498eda02.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c09b13c9a5c9de58aaa39f84623bfe8787f283d6498eda02.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c09b13c9a5c9de58aaa39f84623bfe8787f283d6498eda02.png/w_750 750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c09b13c9a5c9de58aaa39f84623bfe8787f283d6498eda02.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c09b13c9a5c9de58aaa39f84623bfe8787f283d6498eda02.png/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c09b13c9a5c9de58aaa39f84623bfe8787f283d6498eda02.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c09b13c9a5c9de58aaa39f84623bfe8787f283d6498eda02.png/w_1350 1350w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c09b13c9a5c9de58aaa39f84623bfe8787f283d6498eda02.png/w_1416 1416w\"></figure><p>This sequence is particularly focused on noticing confusion and modeling scientific progress. None of the books you list above really cover that at all (The Logic of Science maybe the most, but it's really not its core focus, and is also very technical and has giant holes in the middle of it due to its unfinished nature).&nbsp;</p><p>I have read all of the books/content you link above, and I don't think it really has that much overlap with the content of the sequences, and don't expect that someone who has read them to really have gotten close to most of the value of reading the sequences, so I don't currently think this is a good recommendation.</p>", "parentCommentId": "3cZqCEPADj4bNsaK2", "user": {"username": "Habryka"}}, {"_id": "xoiwyHyKEKzbGepmo", "postedAt": "2022-12-23T21:07:19.186Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>I understand the sequences are important to you folks, and I don't want to seem disrespectful. I have browsed them, and think they contain some good information.</p>\n<p>However, I'd recommend going back to books published at least 30 years ago for reads about:</p>\n<ul>\n<li>critical thinking</li>\n<li>scientific explanation</li>\n<li>informal logic</li>\n<li>formal logic</li>\n<li>decision theory</li>\n<li>cybernetics (Ashby, for the AI folks)</li>\n<li>statistics and probability</li>\n<li>knowledge representation</li>\n<li>artificial intelligence</li>\n<li>negotiation</li>\n<li>linguistic pragmatics</li>\n<li>psychology</li>\n<li>journalism and research skills</li>\n<li>rhetoric</li>\n<li>economics</li>\n<li>causal analysis</li>\n</ul>\n<p>Visit a good used book store, or browse older books and print-only editions descriptions on the web, or get recommendations that you trust on older references in those areas. You'll have to browse and do some comparing. Also get 1st editions wherever feasible.</p>\n<p>The heuristics that this serves include:</p>\n<ul>\n<li>good older books are shorter and smarter in the earlier editions, usually the 1st.</li>\n<li>older books offer complete theoretical models and conceptual tools that newer books gloss over.</li>\n<li>references from the 20th Century tend to contain information still tested and trusted now.</li>\n<li>if you are  familiar with newer content, you can notice how content progressed (or didn't).</li>\n<li>old abandoned  theories can get new life decades later, it's fun to find the prototypical forms.</li>\n<li>most of the topics I listed have core information or skills developed in the 20th century.</li>\n<li>it's a nice reminder that earlier generations of researchers were very smart as well.</li>\n<li>some types of knowledge are disappearing in the age of the internet and cellphone. Pre-internet sources still contain write-ups of that knowledge.</li>\n<li>it's reassuring that you're learning something whose validity and relevance isn't versioned out.</li>\n</ul>\n<p><strong>NOTE:</strong> Old books aren't breathless about how much we've learned in the last 20 years, or how the internet has revolutionized something. I'd reserve belief in that for some hard sciences, and even there, if you want a theory introduction, an older source might serve you better.</p>\n<p>If you don't like print books, you can use article sources online and look at older research material.  There are some books from the 90's available on Kindle, hopefully, but I recommend looking back to the 70's or even earlier. I prefer an academic writing style mostly available after the 70's, I find older academic texts a bit hard to understand sometimes,  but your experience could be different.</p>\n", "parentCommentId": null, "user": {"username": "Noah Scales"}}, {"_id": "BX7PyPcSiuGtWL9oS", "postedAt": "2022-12-23T23:44:49.822Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>&gt;I think the sequences are okay intro points for a number of topics, but they should not be treated as the foundation of ones belief system</p><p>&nbsp;</p><p>I'd say the exact opposite - they are a great foundation that for the most part helps form a coherent world view rather than just getting bits and pieces from everywhere and not necesserily connecting them, <strong>but</strong> you can go explore further in many directions for a more in-depth (and sometimes more modern) perspective.</p>", "parentCommentId": "KrKE8XreruySM6f4S", "user": {"username": "Tenoke"}}, {"_id": "aPRijn6wL6dS45Ymh", "postedAt": "2022-12-23T23:48:29.593Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>Visiting an old book store and reading random old books on topis is a horrible idea and I disrecommend it whole-heartedly. The vast majority of random books are crap, and old books are on average maybe even more crap (caveat that the good ones are the ones we still hear about more) since even more from them has been disproven/has been added to by now.</p>", "parentCommentId": "xoiwyHyKEKzbGepmo", "user": {"username": "Tenoke"}}, {"_id": "s5WQzkySJTdsYXASN", "postedAt": "2022-12-24T00:41:42.732Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>I agree that my book list is incomplete, and it was aimed more at topics that the OP brought up.</p><p>For each of the additional topics you mentioned, it doesn't seem like Yudkowsky's Sequences are the best introduction. E.g., for decision theory I got more out of reading a random MIRI paper trying to formalize FDT. For AI x-risk in particular it would also surprise me if you would also recommend the sequences rather than some newer introduction.</p><blockquote><p>I have read all of the books/content you link above</p></blockquote><p>Is this literally true? In particular, have you read David's Sling?</p>", "parentCommentId": "cr8scpaxir5XewvB7", "user": {"username": "NunoSempere"}}, {"_id": "5oJ4XJ9BAkLqdmAva", "postedAt": "2022-12-24T00:49:30.460Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<blockquote><p>I have read all of the books/content you link above</p></blockquote><p>Out of curiosity, is this literally true? In particular, have you read David's Sling?</p>", "parentCommentId": "cr8scpaxir5XewvB7", "user": {"username": "NunoSempere"}}, {"_id": "prkhPvRxQemSPqCiv", "postedAt": "2022-12-24T02:14:58.491Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>This comment made me more sceptical about reading the sequences. I don't think I can view anyone as an expert on all these topics. Is there a \"best of\" selection of the sequences somewhere?</p>", "parentCommentId": "cr8scpaxir5XewvB7", "user": {"username": "emre kaplan"}}, {"_id": "NNzmxLpWmBJuEfbFB", "postedAt": "2022-12-24T02:39:09.763Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>I did not say that the sequences cover all content in these books! I mean, they are quite long, so they cover a lot of adjacent topics, but I would not claim that the sequences are the canonical resource on all of these.</p>", "parentCommentId": "prkhPvRxQemSPqCiv", "user": {"username": "Habryka"}}, {"_id": "mQhMaFsfEXZzEitAC", "postedAt": "2022-12-24T02:39:45.947Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>I have read a good chunk of David's Sling! Though it didn't really click with me a ton, and I had already been spoiled on a good chunk of it because I had a bunch of conversations about it with friends, so I didn't fully finish it.</p><p>For completeness sake, here is my reading state for all of the above:&nbsp;</p><ul><li>Probability Theory: The Logic of Science, or some other probability theory <a href=\"https://www.lesswrong.com/posts/xg3hXCYQPJkwHyik2/the-best-textbooks-on-every-subject\">textbook</a></li></ul><p>Read</p><ul><li>An introduction to general semantics</li></ul><p>I read a bunch of general semantics stuff over the years, but I never really got into it, so a bit unclear.&nbsp;</p><ul><li>An introduction to CBT, e.g., <a href=\"https://en.wikipedia.org/wiki/Feeling_Good%3A_The_New_Mood_Therapy\">Feeling Good</a></li></ul><p>Yep, read Feeling Good</p><ul><li>Thinking Fast and Slow</li></ul><p>Read</p><ul><li>How to Measure Anything</li></ul><p>Read</p><ul><li>Surely You're Joking Mr Feynman</li></ul><p>Read</p><ul><li>Stranger in a Strange Land, David's Sling, The Moon is a Harsh Mistress.</li></ul><p>All three read more than 30%. I think I finished Stranger in a Strange Land and Moon is a Harsh Mistress, but I honestly don't remember.</p><ul><li>Antifragile/Black Swan</li></ul><p>Yep, read both</p><ul><li>The Elephant in the Brain</li></ul><p>Read like 50% of it, but got bored because a lot of it was covering Overcoming Bias stuff that I was familiar with.</p><ul><li>Superforecasting, making 100 forecasts and keeping track each week.</li></ul><p>Read superforecasting. Have made 100 forecasts, though haven't been that great at keeping track.</p><ul><li>The Rationality Quotient, or some other &nbsp;Keith Stanovich book.</li></ul><p>Read 30% of it, then stopped because man, I think that book really was a huge disappointment. Would not recommend reading. See also this review by Stuart Ritchie: <a href=\"https://twitter.com/stuartjritchie/status/819140439827681280?lang=en\">https://twitter.com/stuartjritchie/status/819140439827681280?lang=en</a>&nbsp;</p><ul><li>Some intro to nonviolent communication</li></ul><p>Read <a href=\"https://smile.amazon.com/Nonviolent-Communication-Language-Life-Changing-Relationships/dp/189200528X?sa-no-redirect=1\">Nonviolent Communication</a></p>", "parentCommentId": "5oJ4XJ9BAkLqdmAva", "user": {"username": "Habryka"}}, {"_id": "3YxMphKxsDtgkkjnG", "postedAt": "2022-12-24T02:44:34.085Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<blockquote><p>the best introduction. E.g., for decision theory I got more out of reading a random MIRI paper trying to formalize FDT.</p></blockquote><p>Yeah, I think the best TDT/FDT/LDY material in-particular is probably MIRI papers. The original TDT paper is quite good, and I consider it kind of part of the sequences, since it's written around the same time, and written in a pretty similar style.&nbsp;</p><blockquote><p>For AI x-risk in particular it would also surprise me if you would also recommend the sequences rather than some newer introduction.</p></blockquote><p>Nope, still think the sequences are by far the best (and indeed most alignment conversations I have with new people who showed up in the last 5 years tend to consist of me summarizing sequences posts, which has gotten pretty annoying after a while). There is of course useful additional stuff, but if someone wanted to start working on AI Alignment, the sequences still seem by far the best large thing to read (there are of course individual articles that do individual things best, but there isn't really anything else textbook shaped).</p>", "parentCommentId": "s5WQzkySJTdsYXASN", "user": {"username": "Habryka"}}, {"_id": "anNugBfoh9Fei38jb", "postedAt": "2022-12-24T03:43:27.237Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>Eliezer isn't (to my knowledge) an expert on, say, evolutionary biology. Reading the sequences will not make you an expert on evolutionary biology either.&nbsp;</p><p>They will, however, show you how to make a layman's understanding of evolutionary biology relevant to your life.</p>", "parentCommentId": "prkhPvRxQemSPqCiv", "user": {"username": "Lumpyproletariat"}}, {"_id": "LnkyK7yHFAofKsFH2", "postedAt": "2022-12-24T06:54:11.717Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>I doubt that most people would know who to look for that authored books written in the late 20th Century in those fields that I listed, particularly when the theory remains unchanged now, or as in the case of Ashby's book, is  not available in any modern textbook.</p>\n<p>I believe that:</p>\n<ul>\n<li>good material from decades ago doesn't appear in new texts, necessarily.</li>\n<li>new material isn't always an improvement, particularly if it reflects \"the internet era\".</li>\n<li>what is offered as \"new material\" isn't always so new.</li>\n</ul>\n<p>One concern with modern textbooks is pedagogical fads (for example, teaching formal logic with a software program or math with a TI calculator). I support pen and paper approaches for basic learning over TI calculators and software packages. Older textbooks offer more theory than current ones. Older textbooks are usually harder. Dover math books are one example where unchanged theory written up in older texts is still appreciated now.</p>\n<p>It doesn't take a lot of learning to find useful 20th Century books about linguistics, scientific reasoning, rhetoric, informal logic, formal logic, and even artificial intelligence. Yes, there was AI material before neural networks and machine learning, and it still has utility.</p>\n<p>For most people, a random search at a decent used bookstore can turn up popular titles with good information. A random search by topic in an academic library or in a used bookstore that accepts used academic titles, (which used to be common, but is becoming more rare), can turn up some amazing finds. I do recommend all those approaches, if you like books and are patient enough to to try it. Otherwise, I suggest you look into older journal articles available in PDF format online.</p>\n<p>It's just one approach, and takes some trial and error. You need to examine the books, read the recommendations, figure out who published it and why and get to know  the author, read the preface and forward, so it takes some patience. It can help to start with an older book and then visit the new material. When I started doing that is when I noticed that the new material was sometimes lesser quality, derivative, or the same content as older material.</p>\n<p>The most precious finds are the ones that are nowhere to be found now. Yes, sometimes that's because they're crap, but sometimes that's because they're really good and people ignored them in spite or, or because of, that.</p>\n<p>EDIT: I also find that reading from a book offers a visceral experience and steadier pace that digital reading can lack.</p>\n", "parentCommentId": "aPRijn6wL6dS45Ymh", "user": {"username": "Noah Scales"}}, {"_id": "xJ8doJLqqZJBSNrDR", "postedAt": "2022-12-24T07:12:55.923Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>What are the core pieces about AI risk in the sequences? Looking through <a href=\"https://www.lesswrong.com/tag/sequences\">the list</a>, I don't see any sequence about AI risk. Yudkowsky's <a href=\"https://www.alignmentforum.org/users/eliezer_yudkowsky\">account</a> on the Alignment Forum doesn't have anything more than six years old, aka nothing from the sequences era.&nbsp;</p><p>Personally I'd point to Joe Carlsmith's report, Richard Ngo's writeups, Ajeya Cotra's writeup, some of Holden Karnofsky's writing, &nbsp;Concrete Problems in AI Safety and Unsolved Problems in ML Safety as the best introductions to the topic.</p>", "parentCommentId": "3YxMphKxsDtgkkjnG", "user": {"username": "Aidan O'Gara"}}, {"_id": "Sy7AnFfz8fNwRmD3P", "postedAt": "2022-12-24T08:02:41.842Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>My guess is that he meant the sequences convey the kind of more foundational epistemology which helps people people derive better models on subjects like AI Alignment by themselves, though all of the sequences in <a href=\"https://www.lesswrong.com/rationality#wAXodw6LPScjrdnkR\">The Machine in the Ghost</a> and <a href=\"https://www.lesswrong.com/rationality#T3CjiQq6bBgFuzvp6\">Mere Goodness</a> have direct object-level relevance.</p><p>&nbsp;</p><p>Excepting Ngo's <a href=\"https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ\">AGI safety from first principles</a>, I don't especially like most of those resources as introductions exactly because they offer readers very little opportunity to test or build on their beliefs. &nbsp; Also, I think most of them are substantially wrong. &nbsp;(<a href=\"https://arxiv.org/abs/1606.06565\">Concrete Problems in AI Safety</a> seems fine, but is also skipping a lot of steps. &nbsp;I haven't read <a href=\"https://arxiv.org/abs/2109.13916\">Unsolved Problems in ML Safety</a>.)</p>", "parentCommentId": "xJ8doJLqqZJBSNrDR", "user": {"username": "T3t"}}, {"_id": "LoGetxRhjmFFnQntE", "postedAt": "2022-12-24T13:53:03.792Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>To be honest, I don't really see the appeal of the \"lesswrong worldview\". It just seems to be the scientific worldview with a bunch of extra ideas of varying and often dubious quality added on. It all comes from one guy with a <a href=\"https://forum.effectivealtruism.org/posts/NBgpPaz5vYe3tH4ga/on-deference-and-yudkowsky-s-ai-risk-estimates\">fairly poor track record of correctness</a>. It seems like a fun social/hobby group more than anything &nbsp;else.&nbsp;</p><p>I don't want to be overly negative because I know LW played a big part in bringing EA up and did originate some of the ideas here. Unfortunately, I think that social dynamic has also probably led to the LW ideas being overrated in the EA community.&nbsp;</p>", "parentCommentId": "BX7PyPcSiuGtWL9oS", "user": {"username": "titotal"}}, {"_id": "oisBrh266PntdoJhz", "postedAt": "2022-12-24T17:09:33.433Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<blockquote><p>It all comes from one guy with a <a href=\"https://forum.effectivealtruism.org/posts/NBgpPaz5vYe3tH4ga/on-deference-and-yudkowsky-s-ai-risk-estimates\">fairly poor track record of correctness</a>.&nbsp;</p></blockquote><p>The post you linked to literally admits to cherry-picking negative examples only (see quote below), it should not be cited as evidence for a 'fairly poor track record'. &nbsp;</p><blockquote><p>I didn\u2019t want to spend the time doing a thorough accounting exercise, though, so I decided to drop any claim that the examples were representative and just describe them as \u201ccherry-picked\u201d \u2014 and add in lots of caveats emphasising that they\u2019re cherry-picked.</p></blockquote>", "parentCommentId": "LoGetxRhjmFFnQntE", "user": {"username": "Larks"}}, {"_id": "Rbvr3gK77jRJfy6Gi", "postedAt": "2022-12-24T19:45:36.782Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>It's pretty ridiculous to expect someone to go through a complete accounting exercise of every statement someone has ever made before expressing an opinion like that, and I'm guessing it's not a standard you hold for criticism of anyone else. The cited articles provided plenty of examples of yudkowsky being extremely wrong and refusing to acknowledge their mistakes, which matches with my experience of his writings after years of familiarity with it.&nbsp;</p><p>My main point is that I have no reason to hold the opinions of Yudkowsky in higher esteem than that of any other succesful pop-science writer like neil degrasse tyson or richard dawkins or whoever. I find it concerning and a little baffling how much influence this one guy has over EA.&nbsp;</p>", "parentCommentId": "oisBrh266PntdoJhz", "user": {"username": "titotal"}}, {"_id": "drELaMAogZmwR8eps", "postedAt": "2022-12-24T23:12:06.392Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>The primary purpose of the sequences was to communicate the generators behind AI risk and to teach the tools necessary (according to Eliezer) to make progress on it, so references to it are all over the place, and it's the second most central theme to the essays.</p><p>Later essays in the sequences tend to have more references to AI risk than earlier ones. Here is a somewhat random selection of ones that seemed crucial when looking over the list, though this is really very unlikely to be comprehensive:&nbsp;</p><ul><li><a href=\"https://www.lesswrong.com/posts/cnYHFNBF3kZEyx24v/ghosts-in-the-machine\">Ghosts in the Machine</a></li><li><a href=\"https://www.lesswrong.com/posts/8vpf46nLMDYPC6wA4/optimization-and-the-intelligence-explosion\">Optimization and the Intelligence Explosion</a></li><li><a href=\"https://www.lesswrong.com/posts/HktFCy6dgsqJ9WPpX/belief-in-intelligence\">Belief in Intelligence</a></li><li><a href=\"https://The Hidden Complexity of Wishes\">The Hidden Complexity of Wishes</a></li><li><a href=\"https://www.lesswrong.com/posts/5wMcKNAwB6X4mp9og/that-alien-message\">That Alien Message (I think this one is particularly good)</a></li><li><a href=\"https://www.lesswrong.com/posts/p7ftQ6acRkgo6hqHb/dreams-of-ai-design\">Dreams of AI Design</a></li><li><a href=\"https://www.lesswrong.com/posts/uNWRXtdwL33ELgWjD/raised-in-technophilia\">Raised in Technophilia</a></li><li><a href=\"https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile\">Value is Fragile</a></li></ul><p>There are lots more. Indeed, towards the latter half of the sequences it's hard not to see an essay quite straightforwardly about AI Alignment every 2-3 essays.&nbsp;</p>", "parentCommentId": "xJ8doJLqqZJBSNrDR", "user": {"username": "Habryka"}}, {"_id": "erm7CyF9vSgiwpJzJ", "postedAt": "2022-12-24T23:13:01.098Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>If your headline claim is that someone has a \"fairly poor track record of correctness\", then I think \"using a representative set of examples\" to make your case is the bare-minimum necessary for that to be taken seriously, not an isolated demand for rigor.</p>", "parentCommentId": "Rbvr3gK77jRJfy6Gi", "user": {"username": "T3t"}}, {"_id": "MDeANAgTexcb4S6Sv", "postedAt": "2022-12-24T23:31:20.258Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>A lot of the people who built effective altruism see it as an extension of the LessWrong worldview, and think that that's the reason why EA is useful to people where so many well-meaning projects are not.</p><p>Some random LessWrong things which I think are important (chosen because they come to mind, not because they're the most important things):</p><p>The many people in EA who have read and understand <a href=\"https://www.lesswrong.com/s/M3TJ2fTCzoQq66NBJ\">Death Spirals</a> (especially <a href=\"https://www.lesswrong.com/posts/XrzQW69HpidzvBxGr/affective-death-spirals\">Affective Death Spirals</a> and <a href=\"https://www.lesswrong.com/posts/ZQG9cwKbct2LtmL3p/evaporative-cooling-of-group-beliefs\">Evaporative Cooling of Group Beliefs</a>) make EA feel safe and like a community I can trust (instead of feeling like a tiger I could choose to run from or ride, the way most large groups of humans feel to people like me) (the many (and counting) people in EA who <i>haven't</i> read Death Spirals, make me nervous - we have something <i>special</i> here, most large groups are not safe).</p><p>The many people in EA who aim to explain rather than persuade, and who are clear about their epistemic status, make me feel like I can frictionlessly trust their work as much as <i>they</i> do, without being fast-talked into something the author is themself uncertain about (but failed to admit their uncertainty over because that's not considered good writing).&nbsp;<br>(The post by Ben Garfinkel linked above (the one that admitted up front that it was trying to argue a position and was happy to distort and elide to that end, which was upvoted to +261) contributed to a growing sense of ill-ease. We have something special here, and I'd like to keep it.)</p><p>Thought experiments like <a href=\"https://www.cato-unbound.org/2011/09/07/eliezer-yudkowsky/true-rejection/\">true objections</a> and <a href=\"https://www.lesswrong.com/posts/neQ7eXuaXpiYw7SBy/the-least-convenient-possible-world)\">least convenient possible worlds</a> swimming around the local noosphere have made conversations about emotionally charged topics much more productive than they are in most corners of the world or internet.</p><p>...I was going to say something about noticing confusion and realized that it was already in Quadratic Reciprocity's post that we are in the replies to. I think that the original post pretty well refutes the idea that the LessWrong mindset is just the default scientific mindset with relatively minor things of dubious usefulness taped on? So I'll let you decide whether to respond to this before I write more in the same vein as the original post, if the original post was not useful for this purpose.</p>", "parentCommentId": "LoGetxRhjmFFnQntE", "user": {"username": "Lumpyproletariat"}}, {"_id": "xp3pAJnp7tJAsfGXp", "postedAt": "2022-12-25T00:10:43.971Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>I don't think my original post was good at conveying the important bits - in particular, I think I published it too quickly and missed out on elaborating on some parts that were more time-consuming to explain. I like your comment and would enjoy reading more&nbsp;</p>", "parentCommentId": "MDeANAgTexcb4S6Sv", "user": {"username": "Quadratic Reciprocity"}}, {"_id": "k2jNxK5LbisEafX5e", "postedAt": "2022-12-25T14:06:10.831Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>Endorsed. A bunch of my friends were recommending that I read the sequences for a while, and honestly I was skeptical it would be worth it, but I was actually quite impressed. There aren\u2019t a ton of totally new ideas in it, but where it excels it honing in on specific, obvious-in-retrospect points about thinking well and thinking poorly, being clear engaging and catchy describing them, and going through a bit of the relevant research. In short, you come out intellectually with much of what you went in with, but with reinforcements and tags put in some especially useful places.</p>\n<p>As a caveat I take issue with a good deal of the substantial material as well. Most notably I don\u2019t think he describes those he disagrees with fairly sometimes, for instance <a href=\"https://www.lesswrong.com/s/FqgKAHZAiZn9JAjDo/p/fdEWWr8St59bXLbQr?commentId=5qKe5gQ8HWgfRq9Dw\">David Chalmers</a>, and I think \u201c<a href=\"https://www.readthesequences.com/Purchase-Fuzzies-And-Utilons-Separately\">Purchase Fuzzies and Utilons Separately</a>\u201d injected a basically wrong and harmful meme into the EA community (I plan to write a post on this at some point when I get the chance). That said if you go into them with some skepticism of the substance, you will come out satisfied. You can also audiobook it <a href=\"https://podcasts.apple.com/us/podcast/rationality-from-ai-to-zombies/id1299826696\">here</a>, which is how I read it.</p>\n", "parentCommentId": null, "user": {"username": "Devin Kalish"}}, {"_id": "GrHbAnySJNCHWZmGY", "postedAt": "2022-12-26T17:54:15.439Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>I've read a decent chunk of the sequences, there are plenty of things to like about them, like the norms of friendliness and openess to new ideas you mention.</p><p>But I cannot say that I subscribe to the lesswrong worldview, because there are too many things I dislike that come along for the ride. Chiefly, it's seems to foster sense of extreme overconfidence in beliefs about fields people lack domain-specific knowledge about. As a physicist, I find the writings about science to be shallow, overconfident and often straight up wrong, and this has been the reaction I have seen from most experts when lesswrong touches on their field. (I will save the extensive sourcing for these beliefs for a future post).&nbsp;</p><p>I think that EA as a movement has the potential to take the good parts of the lesswrong worldview while abandoning the harmful parts. Unfortunately, I believe too much of the latter still resides within the movement.&nbsp;</p>", "parentCommentId": "MDeANAgTexcb4S6Sv", "user": {"username": "titotal"}}, {"_id": "Dh7dfRjJm8Qe2A6mG", "postedAt": "2022-12-27T00:48:55.689Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>I disagree pretty strongly with the headline &nbsp;claim about extreme overconfidence, having found rationalist stuff singularly useful for <i>reducing</i> overconfidence with its major emphasises on falsifiable predictions, calibration, bowing quickly to the weight of the evidence, thinking through failure-states in detail and planning for being wrong.</p><p>I could defend this at length, but it's hard to find the heart to dig up a million links and write a long explanation when it seems unlikely that this is actually important to you or the people who strong-agreed with you.</p>", "parentCommentId": "GrHbAnySJNCHWZmGY", "user": {"username": "Lumpyproletariat"}}, {"_id": "eLruv3x2J3nZaBoea", "postedAt": "2022-12-27T14:16:02.179Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>Perhaps it has worked for you in reducing overconfidence, but it certainly hasn't worked for yudkowsky. I already linked to you the list of failed prognostications, and he shows no sign of stopping, with the declaration that AI extinction has probability ~1.&nbsp;</p><p>I have my concerns about calibration in general. I think they let you get good at estimating short term, predictable events and toy examples, which then gives you overconfidence in your beliefs about long term, unpredictable beliefs and events.&nbsp;</p><p>I don't expect you to dig up a million links when I'm not doing the same. I think it's important to express these opinions out loud, lest we fall into a false impression of consensus on some of these matters. It is important to me... I simply don't agree with you.&nbsp;</p>", "parentCommentId": "Dh7dfRjJm8Qe2A6mG", "user": {"username": "titotal"}}, {"_id": "mu8ouWak9gzXtnWM4", "postedAt": "2023-01-08T08:44:37.759Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>Would you recommend Probability Theory: The Logic of Science to people with little math background?</p>", "parentCommentId": "3cZqCEPADj4bNsaK2", "user": {"username": "zeshen"}}, {"_id": "WHggTbnfRm62qAbq5", "postedAt": "2023-01-08T10:10:22.087Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>Well, if you are uncertain, note that experimentation here is very cheap, because you can download a copy of each book from an <a href=\"https://annas-archive.org/md5/0af108cb2fefa5a20f7b186bc2c88656\">online library</a> and quickly skim it to get a sense. So I'd recommend that.</p>", "parentCommentId": "mu8ouWak9gzXtnWM4", "user": {"username": "NunoSempere"}}, {"_id": "2hgJfPiyrwBCa72Sf", "postedAt": "2023-01-08T20:20:45.857Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>Hello there! I found your post and the comments really interesting (as soon as I finish writing this, I will be checking <a href=\"https://www.lesswrong.com/posts/xg3hXCYQPJkwHyik2/the-best-textbooks-on-every-subject\">The Best Textbooks on Every Subject</a> list in LW), but would like to contribute an outsider's 2\u00a2, as I have only recently discovered and started to take an interest in EA. The thing is, without trying to be disrespectful, that this Rationalist movement that possibly led many of you to EA feels really, really, really weird and alien on a first glance, like some kind of nerdy, rationalist religion with unconventional and controversial beliefs (polyamory or obsessing with AI) and a guru who does not appear to be well-known and respected as a scientist outside of his circle of followers, and whose main book seems to be a fanfiction-esque rewrite of a &nbsp;Harry Potter book with his ideas intertwined. I repeat that I do not mean this as an evaluation (it is probably 'more wrong', if you'll allow the pun), but from an external perspective, it almost feels like &nbsp;some page from a book with entries on Scientology and Science Fiction. I feel that pushing the message that you have to be a Rationalist or Rationalist-adjacent as a prerequisite to really appreciate and value AE can very easily backfire.</p><p>Being the Sequences as long as you say, perhaps even a selection might not be the best way to get people interested if they aren't already piqued or have a disproportionate amount of free time in their hands. Like, if a Marxist comes and tells you that you need to read through the three volumes of <i>Capital</i> and the <i>Grundrisse </i>before making up your mind on if the doctrine is interesting, personally relevant or a good or a bad thing, or if a theologian does the same move and points towards Thomas Aquinas' very voluminous works, you would be justified in requiring first some short and convincing expository work with the core arguments and ideas to see if they look sufficiently appealing and worth engaging in. Is there something of the kind for Rationalism?</p><p>Best greetings.</p><p>M.</p>", "parentCommentId": null, "user": {"username": "Manuel Del R\u00edo Rodr\u00edguez"}}, {"_id": "wuwycmNkQ27LcP4xj", "postedAt": "2023-01-10T21:01:17.632Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>Hello! Welcome to the forum, I hope you make yourself at home.&nbsp;</p><blockquote><p><br>...you would be justified in requiring first some short and convincing expository work with the core arguments and ideas to see if they look sufficiently appealing and worth engaging in. Is there something of the kind for Rationalism?</p></blockquote><p>In <a href=\"https://forum.effectivealtruism.org/posts/2S3CHPwaJBE5h8umW/?commentId=cZ4tdtfDEGuMopXyD\">this</a> comment Hauke Hillebrandt linked this essay of Holden Karnofsky's: <a href=\"https://www.cold-takes.com/the-bayesian-mindset/\">The Bayesian Mindset</a>. It's about a half-hour read and I think it's a really good explainer.</p><p>Putanumonit has their own introduction to rationality - it's less explicitly Bayesian, and somewhat more a paean to what Karnofsky calls \"[emphasizing] various ideas and mental habits that are inspired by the abstract idea of [expected utility maximization]\": <a href=\"https://putanumonit.com/2020/10/08/path-to-reason/\">The Path to Reason</a></p><p>&nbsp;</p><p>Other things which seem related:&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/ASpGaS3HGEQCbJbjS/eliezer-s-sequences-and-mainstream-academia\">Eliezer's Sequences and Mainstream Academia</a>: 4 minute read, connecting various Eliezerisms to the academic literature</p><p><a href=\"https://www.lesswrong.com/s/M3TJ2fTCzoQq66NBJ/p/yEjaj7PWacno5EvWa\">Every Cause Wants to be a Cult</a>: 3 minute read, essay by Eliezer on why Eliezer doesn't want you to defer to Eliezer&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/HLqWn5LASfhhArZ7w/expecting-short-inferential-distances\">Expecting Short Inferential Distances</a>: 3 minute read, essay on why explaining things is sometimes hard</p><p>&nbsp;</p><p>Some \"rationality in action\" type posts from a variety of authors, to demonstrate what it looks like when people try to use the Bayesian mindset (these posts are all Real Hipster, written by contrarians who were subsequently proven correct by common consensus; the purpose of rationality is to be as correct as possible as quickly as possible).</p><p><a href=\"https://www.lesswrong.com/posts/G9dptrW9CJi7wNg3b/the-amanda-knox-test-how-an-hour-on-the-internet-beats-a\">The Amanda Knox Test: How an Hour on the Internet Beats a Year in the Courtroom</a>: 14 minute read, about how Amanda Knox was innocent (written before she was imprisoned and later exonerated)</p><p><a href=\"https://www.lesswrong.com/posts/pC47ZTsPNAkjavkXs/efficient-charity-do-unto-others\">Efficient Charity: Do Unto Others</a>: 8 minute read, about efficient charity (written before \"effective altruism\" was a term and some years before the Centre for Effective Altruism existed)</p><p><a href=\"https://putanumonit.com/2020/02/27/seeing-the-smoke/\">Seeing the Smoke</a>: a post about Covid-19, written in February of 2020</p><p>&nbsp;</p><p>Hope you find this useful!</p>", "parentCommentId": "2hgJfPiyrwBCa72Sf", "user": {"username": "Lumpyproletariat"}}, {"_id": "W2oRAvh6en393Z4TK", "postedAt": "2023-01-10T23:19:26.126Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>Thanks for the recommendations! I wouldn't have any issues either with a moderately-sized book (say, from 200-400 pages long).</p><p>Cheers.</p><p>M.</p>", "parentCommentId": "wuwycmNkQ27LcP4xj", "user": {"username": "Manuel Del R\u00edo Rodr\u00edguez"}}, {"_id": "XQpygBumqhv8QPxAF", "postedAt": "2023-01-11T00:59:22.067Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>Many people stand by The Scout Mindset by Julia Galef (though I haven't myself read it) (<a href=\"https://astralcodexten.substack.com/p/book-review-the-scout-mindset\">here's</a> a book review of it that you can read to decide whether you want to buy or borrow the book). I don't know how many pages long it is exactly but am 85% sure it falls in your range.</p><p>On the nightstand next to me is Replacing Guilt by Nate Soares - it's 202 pages long and they are all of them great. You can find much of the material online <a href=\"https://mindingourway.com/replacing-guilt/\">here</a>, you could give the first few chapters a glance-through to see if you like them.</p><p>I'm interested to see which books other people recommend!</p>", "parentCommentId": "W2oRAvh6en393Z4TK", "user": {"username": "Lumpyproletariat"}}, {"_id": "9J76kQkF8k86zcyWF", "postedAt": "2023-01-11T13:00:51.851Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>I can't speak to Yudkowsky's knowledge of physics, economics, psychology etc, but as someone who studies philosophy I can tell you his philosophical segments are pretty weak.<br>It's clear that he hasn't read a lot of philosophy and he <a href=\"https://www.greaterwrong.com/posts/oTX2LXHqXqYg2u4g6/less-wrong-rationality-and-mainstream-philosophy/comment/YPPYBReJ8JknqpPqi\">is</a> <a href=\"https://www.lesswrong.com/posts/oTX2LXHqXqYg2u4g6/less-wrong-rationality-and-mainstream-philosophy?commentId=RtAG5ar6qoxGqCaDD\">very</a> <a href=\"https://www.lesswrong.com/posts/vzLrQaGPa9DNCpuZz/against-modal-logics\">dismissive</a> of the field as a whole. He also has a tendency to reinvent the wheel (e.g his '<a href=\"https://www.lesswrong.com/posts/NEeW7eSXThPz7o4Ne/thou-art-physics\">Requiredism</a>' is what philosophers would call compatibilism).</p><p>When I read the sequences as a teenager I was very impressed by his philosophy, but as I got older and started reading more I realized how little he actually engaged with criticisms of his favorite theories, and when he did he often only engaged with weaker criticisms.<br>If you want some good introductory texts on philosophy as well as criticism/alternatives to some of his/rationalists most central beliefs e.g <a href=\"https://www.lesswrong.com/s/FqgKAHZAiZn9JAjDo\">physicalism</a>, <a href=\"https://www.lesswrong.com/rationality#PYQ2izdfDPTe5uJTG\">correspondence theory</a>, <a href=\"https://www.lesswrong.com/rationality#ah2GqzZSeBpW9QHgb\">scientific realism</a>, <a href=\"https://www.lesswrong.com/posts/vzLrQaGPa9DNCpuZz/against-modal-logics\">the normativity of classical logic</a> (all of which I have rejected as of the moment of this writing) then I highly recommend the<strong> </strong><a href=\"https://plato.stanford.edu/index.html\"><strong>Stanford Encyclopedia of Philosophy</strong></a>.</p>", "parentCommentId": "prkhPvRxQemSPqCiv", "user": {"username": "bmjacobs@telenet.be"}}, {"_id": "NxNyTEDBHfhExbSZh", "postedAt": "2023-01-11T13:13:48.118Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>In fairness, my memory of the philpapers survey is that there is more consensus  amongst professional philosophers on scientific realism than on almost any other philosophical theory. (Though that's going by the old survey, haven't looked at the more recent one yet.) Although of course there are prominent philosophers of science who are anti-realist.</p>\n", "parentCommentId": "9J76kQkF8k86zcyWF", "user": {"username": "Dr. David Mathers"}}, {"_id": "rtvwygz9XunCcCWjM", "postedAt": "2023-01-11T13:39:24.742Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>True, <a href=\"https://philpapers.org/surveys/results.pl\">here are the results you're talking about</a>:</p><blockquote><p>Science: scientific realism or scientific anti-realism?</p><figure class=\"table\"><table style=\"background-color:transparent\"><tbody><tr><td style=\"padding:0px 10px 0px 0px\">Accept or lean toward: scientific realism</td><td style=\"padding:0px\">699 / 931 (75.1%)</td></tr><tr><td style=\"padding:0px 10px 0px 0px\">Other</td><td style=\"padding:0px\">124 / 931 (13.3%)</td></tr><tr><td style=\"padding:0px 10px 0px 0px\">Accept or lean toward: scientific anti-realism</td><td style=\"padding:0px\">108 / 931 (11.6%)</td></tr></tbody></table></figure></blockquote><p>His views are <a href=\"https://survey2020.philpeople.org/survey/results/all\">moderately popular</a> in general with:</p><ul><li>51.37% accept or lean towards correspondence</li><li>51.93% accept or lean towards physicalism</li><li>30.56% accept or lean towards consequentialism</li><li>53.64% accept or lean towards classical logic (although that doesn't tell us whether the philosophers think it has normative force).</li></ul><p>I will say that PhilPapers has a rather small sample size and mostly collects data on english speaking philosophers, so I find it probable that these results are not representative of philosophers as a whole.</p>", "parentCommentId": "NxNyTEDBHfhExbSZh", "user": {"username": "bmjacobs@telenet.be"}}, {"_id": "oQr7dyeDKfDjbZxo5", "postedAt": "2023-01-11T14:59:39.174Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>That's true, I would only really trust the survey for what analytic philosophers think.</p>\n", "parentCommentId": "rtvwygz9XunCcCWjM", "user": {"username": "Dr. David Mathers"}}, {"_id": "stgGDBrfSBWKNF5DL", "postedAt": "2023-11-13T20:10:39.839Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>I advise against trying to read 1/day blindly, since there are monsters like <a href=\"https://A Technical Explanation of Technical Explanation\">A Technical Explanation of Technical Explanation</a> (nearly 17k words), one needs to set aside time for.</p>", "parentCommentId": null, "user": {"username": "Milli"}}, {"_id": "gbEFdzzTJnt6DgTz7", "postedAt": "2023-11-20T12:09:39.928Z", "postId": "2S3CHPwaJBE5h8umW", "htmlBody": "<p>This post got some flak and I am not sure if it actually led to more EAs seriously considering engaging with the Sequences. However, I stand by the recommendation even more strongly now. If I were in a position to give reading recommendations to smart young people who wanted to do big, impactful things, I would recommend the Sequences (or HPMOR) over any of the EA writing.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Quadratic Reciprocity"}}]