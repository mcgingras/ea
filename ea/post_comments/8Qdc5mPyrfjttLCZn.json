[{"_id": "qadSXa4vCg4vzPbhC", "postedAt": "2022-12-08T00:53:18.851Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>I strongly agree.&nbsp;</p><p>I miss discussions about \u201chow we can make EA mainstream\u201d or \u201cbring EA to academia\u201d.&nbsp;</p><p>While I find the EA community to be a great source of personal and social value, we still face the challenge of significantly scaling everything we do. Taking informed steps to doing good better shouldn't be a side consideration for governments, NGOs or people, it should be the default. &nbsp;Working to systematically address existential risk shouldn't be the work of a few nonprofits, it should be the work of national and international institutions.</p><p>If we over-emphasize community building while at the same time de-prioritizing engaging with the outside world, we risk that vision. There are significant advantages to working inside a community (and we should leverage those!), but <strong>to be truly successful, we first have to learn how to communicate with the outside world.</strong></p><p>For group organizers, one step we could take is prioritize working with existing institutions and experts in different fields. Instead of only inviting EAs to events, we could invite more experts working in related areas. Instead of only networking with EA institutions, we could work more closely with traditional institutions from different fields.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy5byaegtqck\"><sup><a href=\"#fny5byaegtqck\">[1]</a></sup></span></p><p>This probably means navigating difficult tradeoffs, in handling outreach and press, in compromising ideas and in producing less \u201chighly-engaged EAs\u201d, but I think this is a discussion worth having.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fny5byaegtqck\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefy5byaegtqck\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is already happening in the policy space (out of necessity). There's also plenty of precedent from other EA groups (especially if they're specialized), but I don't think it's nowhere there yet.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "agucova"}}, {"_id": "pqizpAshkb3dGLBEE", "postedAt": "2022-12-08T13:26:07.497Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>I'm on the side of value alignment being much more important than people often think as it's hard to get anywhere if people want to go five different ways and it's easy for organisational culture to be diluted in the absence of an explicit effort to maintain it.</p><p>That said, outside of community-building roles, particular frames are more important than whether a person identifies as EA (someone can have these frames without identifying as EA or lack them when identifying as an EA). These include attempting to do the most good that you can do<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3lycqyo3sc4\"><sup><a href=\"#fn3lycqyo3sc4\">[1]</a></sup></span>, respect for evidence and reason and a willingness to step outside of the social reality. You can find people like this outside of the EA community, but it's much rarer outside of people who are at least EA adjacent.&nbsp;</p><p>I'd be much more open to bringing in experienced non-EA's who don't necessarily have these attributes in advisory capabilities.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3lycqyo3sc4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3lycqyo3sc4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This does not imply being a naive maximiser.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "7pjhtRtN6oxtacYHX", "postedAt": "2022-12-08T15:46:30.556Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>De-emphasizing cause neutrality could(my guess is) probably would reduce the long-term impact of the movement substantially. Trying to answer the question \"How do the most good\", without attempting to be neutral between causes we are passionate about and causes we don't (intuitively) care that much about would bias us towards causes and paths that are interesting to us rather than particularly impactful causes. Personal fit and being passionate about what you do is absolutely important, but when we're trying to compare causes and comparing actions/careers in terms of impact(or ITN), our answer shouldn't be dependent on our personal interests and passions, but when we're taking action based on those answers then we should think about personal fit and passions, as these prevent us from being miserable while we're pursuing impact. And also, cause neutrality should nudge people against associating EA with a singular cause like AI Safety or global development or even 80k careers, I think extreme cause neutrality is a solution to the problem you describe, rather than being root of the problem.<br>De-emphasizing cause neutrality would increase the likelihood of EA becoming mainstream and popular, but it would also undermine our focus and emphasis on impartiality and good epistemics, which were/are vital factors why EA was able to identify so many high-impact problems and take action to tackle those problems effectively imho.</p>", "parentCommentId": null, "user": {"username": "Berke"}}, {"_id": "NNx3syhhmp2TJXFCt", "postedAt": "2022-12-08T16:43:17.708Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>I like all of your suggested actions. Two thoughts:</p><p><br>1) EA is a both a set of strong claims about causes + an intellectual framework which can be applied to any cause. One explanation for what's happening is that we grew a lot recently, and new people find the precooked causes easier to engage with (and the all-important status gradient of the community points firmly towards them). It takes a lot of experience and boldness to investigate and intervene on a new cause.</p><p>I suspect you won't agree with this framing but: one way of viewing the play between these two things is a classic <a href=\"https://en.wikipedia.org/wiki/Multi-armed_bandit\">explore/exploit tradeoff</a>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2mvcwuef4ny\"><sup><a href=\"#fn2mvcwuef4ny\">[1]</a></sup></span>&nbsp;On this view, exploration (new causes, new different people) is for discovering new causes.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefv0nca4mw4fe\"><sup><a href=\"#fnv0nca4mw4fe\">[2]</a></sup></span>&nbsp;Once you find something huge, you stop searching until it is fixed.</p><p>IMO our search actually did find something so important, neglected, and maybe tractable (AI) that it's right to somewhat de-emphasise cause exploration until that situation begins to look better. We found a combination <a href=\"https://www.effectivealtruism.org/articles/prospecting-for-gold-owen-cotton-barratt\">gold mine</a> / natural fission reactor. This cause is even pluralistic, since you can't e.g. <a href=\"https://www.gleech.org/x-for-all\">admire art if there's no world</a>.</p><p>&nbsp;</p><p>2) But anyway I agree that we have narrowed too much. See <a href=\"https://forum.effectivealtruism.org/posts/8Ban7AnoqwdzQphsK/we-can-do-better-than-argmax\">this post</a> which explains the significance of cause diversity on a maximising view, or <a href=\"https://forum.effectivealtruism.org/topics/obituary\">my series of obituaries</a> about people who did great things outside the community.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2mvcwuef4ny\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2mvcwuef4ny\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I suspect this because you say that we shouldn't have a \"singular best way to do good\", and the bandit framing usually assumes one objective.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnv0nca4mw4fe\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefv0nca4mw4fe\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Or new perspectives on causes / new ideas for causes / hidden costs of interventions / etc</p></div></li></ol>", "parentCommentId": null, "user": {"username": "technicalities"}}, {"_id": "Tm5pFbGxYLLbxFaPq", "postedAt": "2022-12-08T20:14:51.273Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>I think this post is very accurate, but I worry that people will agree with it in a vacuous way of \"yes, there is a problem, we should do something about it, learning from others is good\". So I want to make a more pointed claim: I think that <strong>the single biggest barrier to interfacing between EAs and non-EAs is the current structure of community building.</strong> Community-building is largely structured around creating <a href=\"https://forum.effectivealtruism.org/posts/xomFCNXwNBeXtLq53/bad-omens-in-current-community-building\">highly-engaged EAs</a>, usually through recruiting college students <a href=\"https://www.atlasfellowship.org/\">or even high-school students</a>. These students are not necessarily in the best position to interface between EA and other ways of doing good, precisely because they are so early into their careers and don't necessarily have other competencies or viewpoints. So EA ends up as their primary lens for the world, and in my view that explains a sizable part of EA's quasi-isolationist thinking on doing good.</p>\n<p>This doesn't mean all EAs who joined as college students (like me) end up as totally insular - life puts you into environments where you can learn from non-EAs. But that isn't the default, and especially outside of global health and development, it is very easy for a young highly-engaged EA to avoid learning about doing good from non-EAs.</p>\n", "parentCommentId": null, "user": {"username": "therealslimkt"}}, {"_id": "pRus4tjMhuyngLZX3", "postedAt": "2022-12-09T01:16:49.173Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p><strong>Post summary</strong> (feel free to suggest edits!):<br>The author asks whether EA aims to be a question about doing good effectively, or a community based around ideology. In their experience, it has mainly been the latter, but many&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/CnAhPPsMWAxBm7pii/what-specific-changes-should-we-as-a-community-make-to-the\"><u>EAs</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/FpjQMYQmS3rWewZ83/effective-altruism-is-a-question-not-an-ideology\"><u>have</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/HDAXztEbjJsyHLKP7/outline-of-galef-s-scout-mindset\"><u>expressed</u></a> they\u2019d prefer it be the former.<br><br>They argue the best concrete step toward EA as a question would be to collaborate more with people outside the EA community, without attempting to bring them into the community. This includes policymakers on local and national levels, people with years of expertise in the fields EA works in, and people who are most affected by EA-backed programs.</p><p>Specific ideas include EAG actively recruiting these people, EA groups co-hosting more joint community meetups, EA orgs measuring preferences of those impacted by their programs, applying evidence-based decision-making to all fields (not just top cause areas), engaging with people and critiques outside the EA ecosystem, funding and collaborating with non-EA orgs (eg. via grants), and EA orgs hiring non-EAs.</p><p>(If you'd like to see more summaries of top EA and LW forum posts, check out the <a href=\"https://forum.effectivealtruism.org/s/W4fhpuN26naxGCBbN\">Weekly Summaries</a> series.)</p>", "parentCommentId": null, "user": {"username": "GreyArea"}}, {"_id": "YwMWsx5oM6fKsxQKp", "postedAt": "2022-12-09T06:53:48.149Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>Nice one Zoe love these a lot</p>", "parentCommentId": "pRus4tjMhuyngLZX3", "user": {"username": "NickLaing"}}, {"_id": "yjSekxe7NKvieZ9NB", "postedAt": "2022-12-09T07:18:07.403Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>This is a great summary, thank you so much!</p>", "parentCommentId": "pRus4tjMhuyngLZX3", "user": {"username": "Siobhan_M"}}, {"_id": "ct9Cmmq4gMB4fRAXM", "postedAt": "2022-12-09T07:21:16.404Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>Thanks for this thought! I'd considered putting something similar in the original post simply based on anecdotes, but not being a community builder or someone who joined in college I wasn't sure enough to include it. I'd be interested to know your or others' thoughts on what community-building in particular could do to catalyze more interaction between EA and other ways of doing good?&nbsp;</p>", "parentCommentId": "Tm5pFbGxYLLbxFaPq", "user": {"username": "Siobhan_M"}}, {"_id": "cAD5TYv29gGCXjJbj", "postedAt": "2022-12-09T07:27:06.657Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>Thanks for the comment - this and the other comments around cause neutrality have given me a lot to think about! My thoughts on cause neutrality (especially around where the pressure points are for me in theory vs. practice) are not fully formed; it's something I'm planning to focus a lot on in the next few weeks, in which time I might have a better response.&nbsp;</p>", "parentCommentId": "NNx3syhhmp2TJXFCt", "user": {"username": "Siobhan_M"}}, {"_id": "Jk9AbW79ta8pyRGYS", "postedAt": "2022-12-10T03:21:26.827Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>I'm not a community builder, but I'd like to share some observations as someone who has been involved with EA since around 2016 and has only gotten heavily involved with the \"EA community\" over the past year. I'm not sure how helpful they'll be but I hope it's useful to you and others.</p><ul><li>I strongly agree with Karthik's comment about the focus on highly-engaged EA's as the desired result of community building being counterproductive to learning. I think part of this definitely comes down to the relative inexperience of both members and groups leaders, particularly in university groups. There seems to be a lot of focus on convincing people to get involved in EA rather than facilitating their engagement with ideas, and this seems to lead to a selection effect where only members who wholly buy into EA as a complete guideline for doing good stick with the community, creating an intellectual echo chamber of sorts where people don't feel very motivated to meaningfully engage with non-EA perspectives.</li><li>One reflection of this unwillingness to engage that I've come across recently is EAs online asking how to best defend point X or Y or how to best respond to a certain criticism. The framing of these questions as \"how do I convince person that X is right/wrong\", \"which arguments work best on people who believe Y\" or \"how do I respond to criticism Z\" makes it apparent to me that they are not interested in understanding the other points perspective as much as \"defeating\" it, and that they are trying to defend ideas or points that they are not convinced of themselves (as demonstrated in the way that they are not able to respond to criticisms themselves but feel the need to defend the point), presumably because it's an EA talking point.&nbsp;</li><li>Another issue I've seen in similar online spaces is a sneer-y and morally superior attitude towards \"fuzzies\" and non-utilitarian approached to doing good. This is both hostile to non-EAs and &nbsp;thus makes it less likely for them to be willing to engage, and it is demonstrating an unwillingness on the side of the EAs to engage as well. I'm not sure how prevalent this kind of thing is or how it can be counteracted, but it may be worth thinking about.&nbsp;<ul><li>While not as severe, I think it may be worth looking into discussion norms in this context as well. EAs as a community tend to value relatively highly polished arguments that are backed with evidence and their preferred modes of analysis (Bayesian analysis, utilitarian calculus, expected value etc.) and presented in a very \"neutral\", \"unemotional\" tone. There have been posts on this forum over the past few weeks both pointing this out and exemplifying it in the responses. While I do agree with criticisms of discussion norms , I think that it's fairly easy to see that this presents an obstacle to learning regardless of how one feels about it. If our intention is to learn from others, EAs need to be able to meaningfully engage with perspectives that are presented in their preferred style and engage with content over style and presentation, particularly where criticisms or fundamental differences of opinion are concerned.&nbsp;</li></ul></li><li>I've spoken to multiple community builders, both for university groups and local groups, who expressed frustration or disappointment in &nbsp;not being able to get members to \"engage\" with EA because members weren't making career changes into direct work on EA causes. I think this is not only a bad approach to community building for reasons stated above, but that it also creates a dynamic where people who could be doing good work and learning elsewhere are implicitly told that this kind of work is not valuable, thus both alienating people are not able to find direct work and further implying that non-EA work is valueless. This is probably something that can be addressed both in community building best practices and by tweaking any existing incentive structures for community building to emphasize highly-engaged EAs less as a desirable end result.&nbsp;</li></ul>", "parentCommentId": "ct9Cmmq4gMB4fRAXM", "user": {"username": "tugbazsen"}}, {"_id": "xWibRWcLMXKNS6qj9", "postedAt": "2022-12-10T04:16:28.452Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>I think this is actually a good example of the dynamic the author is pointing at.&nbsp;</p><ul><li>While some people simply care about doing the most good, others will care about doing the most good in X area, and barring the second kind of person from EA is, in my opinion, both not optimal and not conducive to learning. More importantly, the assumption of cause neutrality in this fashion is precisely one of the differences between EA as a question and EA as an ideology.&nbsp;</li><li>Cause selection being strictly guided by neutral calculation will likely cause a lot of lost of potential, for reasons you've pointed to (I have some difficulty parsing this paragraph and am not sure where you think it's appropriate or inappropriate to factor in personal fit and passion):&nbsp;</li></ul><blockquote><p>Personal fit and being passionate about what you do is absolutely important, but when we're trying to compare causes and comparing actions/careers in terms of impact(or ITN), our answer shouldn't be dependent on our personal interests and passions, but when we're taking action based on those answers then we should think about personal fit and passions, as these prevent us from being miserable while we're pursuing impact.</p></blockquote><ul><li>More importantly, the impact of many causes are a lot more difficult to measure quantifiably and definitely, let alone in a meaningful way. These causes are de facto left out of EA discussion or will \"lose\" to causes that allow for cleaner and easier quantitative analysis, which does not seem &nbsp;idea as it leads to a lot of lost potential.&nbsp;</li></ul>", "parentCommentId": "7pjhtRtN6oxtacYHX", "user": {"username": "tugbazsen"}}, {"_id": "bPr3KxmufuzRqHvhz", "postedAt": "2022-12-10T09:59:26.734Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>If you'll forgive the marketing terminology, I think cause neutrality is EA's Unique Selling Point. It's the main thing EA brings to the table, its value add, the thing that's so hard to find anywhere else. It's great that people committed to particular causes want to be as effective as possible within them - better than not caring much for effectiveness at all - but there are other places they can find company and support. EA can't be for literally everyone otherwise it doesn't mean anything, so it has to draw a line somewhere and I think that the most natural place is around the idea/behaviour/value that makes EA most distinctive (and, I would argue, most impactful).</p>\n<p>To your second bullet point, I can't think of an area where it's more difficult to measure impact quantitatively and definitely than longtermism.</p>\n", "parentCommentId": "xWibRWcLMXKNS6qj9", "user": {"username": "Holly"}}, {"_id": "tpamRNLQYaFq5vdMy", "postedAt": "2022-12-10T10:13:11.778Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>I agree that EA can't be for everyone and I don't think it should try to be, but I personally don't think that cause neutrality is EA's unique selling point or the main thing it brings to the table, although I do understand that there are different approaches to EA.</p><blockquote><p>To your second bullet point, I can't think of an area where it's more difficult to measure impact quantitatively and definitely than longtermism.</p></blockquote><p>I agree that longtermist impact isn't really measurable, but this makes it hard for me reconcile cause neutrality with longtermism rather than feel like rigid cause neutrality would not have the effect I stated.&nbsp;<br>&nbsp;</p>", "parentCommentId": "bPr3KxmufuzRqHvhz", "user": {"username": "tugbazsen"}}, {"_id": "GJZQSJHTwwrByJHXz", "postedAt": "2022-12-10T18:08:58.474Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>Great post. Thanks for sharing.</p>\n", "parentCommentId": null, "user": {"username": "Josh Axford"}}, {"_id": "zpy7qDEfhbeoLSXJZ", "postedAt": "2022-12-10T21:13:18.817Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>We\u2019ve been getting flak for being over reliant on quantitative analysis for some time. However, critics of EA insider insularity are also taking aim at times when EA has invested money in interventions, like Wyndham Abbey, based on qualitative judgments of insider EAs. I think there\u2019s also concern that our quantitative analysis may simply be done poorly, or even be just a quantitative veneer for what is essentially a qualitative judgment.</p>\n<p>I think it\u2019s time for us to go past the \u201cqualitative vs quantitative\u201d debate, and try to identify what an appropriate context and high-quality work looks like using both reasoning styles.</p>\n<p>One change I\u2019d like to see are some standards for legibility for spends above a certain size. If we\u2019re going to spend $15 million on a conference center based on intuitions about the benefit, we should still publish the rationale, maintenance costs, an analysis of how much time will be saved on logistics in a prominent, accessible location so that people can see what we\u2019re up to. That doesn\u2019t mean we need to have some sort of public comment or democratic decision making on all this stuff - we don\u2019t need to bog ourselves down with regulation. But a little more effort to maintain legibility around qualitative decisions might go a long way.</p>\n", "parentCommentId": "Jk9AbW79ta8pyRGYS", "user": {"username": "AllAmericanBreakfast"}}, {"_id": "FLfQ5PjjSf35ekwHs", "postedAt": "2022-12-10T21:18:00.898Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>When you buy a conference center you get an asset worth around the cost that you paid it. Please, can people stop saying that \"we spent $15 million on a conference center\"? If we wanted to sell it today, my best guess is we probably could do that for $13-14 million, so the total cost here is around $1-2 million, which is really not much compared to all the other spending in the ecosystem.&nbsp;</p><p>There is a huge difference between buying an asset you can utilize and spending money on services, rent, etc. If you compare them directly you will make crazily wrong decisions. The primary thing to pay attention to is depreciation, interest and counterfactual returns, all of which suggest numbers an order of magnitude lower (and indeed move it out of the space where anyone should really worry much about this).&nbsp;</p>", "parentCommentId": "zpy7qDEfhbeoLSXJZ", "user": {"username": "Habryka"}}, {"_id": "hoq2yPwepuKyT5DCW", "postedAt": "2022-12-11T00:22:08.951Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>I\u2019m aware that the conference center can be sold. The point is that there wasn\u2019t an accessible, legible explanation available. To accept that it was a wise purchase, you either have to do all the thinking for yourself, or defer to the person who made the decision to buy it.</p>\n<p>That\u2019s a paradigm EA tried to get away from in the past, and what made it popular, I think, was the emphasis on legibility. That\u2019s partly why 80,000 hours is popular - while in theory, anyone could come to the same conclusions about careers by doing their own research, or just blindly accept recommendations to pursue a career in X, it\u2019s very helpful to have a legible, clearly argued explanation.</p>\n<p>The EA brand doesn\u2019t have to be about quantification, but I think it is about legibility, and we see the consequences when we don\u2019t achieve that: people can\u2019t make sense of our decisions, they perceive it as insular intuitive decision making, they get mad, they exaggerate downsides and ignore mitigating factors, and they pan us. Because we made an implicit promise, that with EA, you would get good, clear reasons you can understand for why we wanted to spend your donations on X and not Y. We were going to give you access to our thought process and let you participate in it. And clearly, a lot of people don\u2019t feel that EA is consistently following through on that.</p>\n<p>EA may be suffering from expert syndrome. It\u2019s actually not obvious to casual observers that buying an old plush-looking country house might be a sensible choice for hosting conferences rather than a status symbol, or that we can always sell it and get most of our money back. If we don\u2019t overcome this and explain our spending on a way where an interested outsider can read it and say \u201cyes, this makes sense and I trust that this summary reflects smart thinking about the details I\u2019m not inspecting,\u201d then I think we\u2019ll continue to generate heated confusion in our ever-growing cohort of casual onlookers.</p>\n<p>If we want to be a large movement, then managing this communication gap seems key.</p>\n", "parentCommentId": "FLfQ5PjjSf35ekwHs", "user": {"username": "AllAmericanBreakfast"}}, {"_id": "A5LZycWyhNG558hDZ", "postedAt": "2022-12-11T08:20:18.692Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>After a cursory read of the post, my summary would have been some vacuous agreement like you described. +1 for making a more specific claim. If the post mentions some specific claims like this (I didn't read very carefully), I'd greatly appreciate a <a href=\"https://forum.effectivealtruism.org/posts/dHHuEYdbMqBf2deyj/using-the-executive-summary-style-writing-that-respects-your\">TL;DR / executive summary</a> of these at the top.</p>", "parentCommentId": "Tm5pFbGxYLLbxFaPq", "user": {"username": "jakubkraus07@gmail.com"}}, {"_id": "8EXrtzTHENnEh4B69", "postedAt": "2022-12-11T08:24:18.736Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>What are some particular changes to EA CB that you'd like to see?</p>", "parentCommentId": "Tm5pFbGxYLLbxFaPq", "user": {"username": "jakubkraus07@gmail.com"}}, {"_id": "rkSyBeTKYoXEyAAbg", "postedAt": "2022-12-12T07:44:37.060Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>&nbsp;I'm not sure what part of my comment this comment is in response to, I initially thought it was posted under<a href=\"https://forum.effectivealtruism.org/posts/8Qdc5mPyrfjttLCZn/learning-from-non-eas-who-seek-to-do-good?commentId=xWibRWcLMXKNS6qj9\"> my response to Berke's comment below </a>and am responding with that in mind, so I'm not 100% sure I'm reading your response correctly and apologies if this is off the mark.&nbsp;</p><blockquote><p>We\u2019ve been getting flak for being over reliant on quantitative analysis for some time. However, critics of EA insider insularity are also taking aim at times when EA has invested money in interventions, like Wyndham Abbey, based on qualitative judgments of insider EAs.</p></blockquote><p>I think the issue around qualitative vs. quantitative judgement in this context is mainly on two axes:</p><ul><li>When it comes to cause prioritization, the causality behind some factors and interventions can be harder to measure more or less definitively in clear, quantitative terms. For example, it's relatively easy to figure out how many lives something like a vaccine or bed net distribution can save with RCTs, but it's much harder to figure out what the actual effect of, say, 3 extra years of education is for the average person - you can get some estimations but it's not easy to clearly delineate between what the actual cause of the observed results are (is it the diploma, the space for intellectual exploration, the peer engagement, the structured environment, the actual content of education, the opportunities for maturing in a &nbsp;relatively low-stakes environment... ). This is because there are a lot of confounding and intertwined factors and it's not easy to isolate the cause - I had a professor who loved to point to single parent households as an example of difficulty in establishing causality: is it the absence of one parent the problem, or is it the reasons that the parent is absence? These kind of questions are better answered with qualitative research, but don't quantify easily and you can't run something like an RCT on them. This makes them a bit less measurable in a clear cut way. I'm personally a huge fan of qualitative research for impact assessment, but they have smaller sample sizes don't tend to \"generalize\" the same way RCTs etc do (andhow well other types of study generalize is a whole other question, but seems to be taken more or less as given here and I don't think the way it's treated is problematic on a practical scale)</li><li>That being said, there is a big difference between a qualitative research study and the \"qualitative judgments of insider EAs\" - I think that the qualitative reasoning presented in comments in the thread about the Abbey (personal experiences with conferences etc.) are valuable, but don't rise to the level of rigor that an actual qualitative research does - they're anecdotes.&nbsp;</li></ul><blockquote><p>I think it\u2019s time for us to go past the \u201cqualitative vs quantitative\u201d debate, and try to identify what an appropriate context and high-quality work looks like using both reasoning styles.</p></blockquote><p>I absolutely agree with this and am a strong proponent of methodological flexibility and mixed methods approaches, but I think it's important to keep the difference between qualitative reasoning based on personal experiences and qualitative reasoning based on research studies and data in mind while doing so. \"Quantitative reasoning\" tends to implicitly include (presumably) rigorously collected data while \"qualitative reasoning\" as used in your comment (which I think does reflect colloquial uses, unfortunately) does not. &nbsp;<br>&nbsp;</p>", "parentCommentId": "zpy7qDEfhbeoLSXJZ", "user": {"username": "tugbazsen"}}, {"_id": "vNhDPYpia6GBzod64", "postedAt": "2022-12-12T17:14:53.981Z", "postId": "8Qdc5mPyrfjttLCZn", "htmlBody": "<p>Assuming that it costs around 6000\u00a3 to save a life,  these 1-2 million come down to around 200-300 lives saved. EAs claim to have a very high standard in evaluating the money spent by charities, this shouldn't stop at the 'discretionary spending' of the evaluators.</p>\n", "parentCommentId": "FLfQ5PjjSf35ekwHs", "user": {"username": "Anton Wille"}}]