[{"_id": "tuPvzZaw7Txrk7yMr", "postedAt": "2023-09-06T08:30:16.856Z", "postId": "GqK5S2ApsycdcRwCm", "htmlBody": "<p>Broadly speaking, without regard to labels or choosing a specific algorithm, I think one should choose the dominance principle if probabilities are held constant and thus choose smoking, two-boxing, etc. But if probabilities were to update based on the agent's actions such as in Parfit's Hitchhiker, then one should pre-commit to an action that does not necessarily dominate in exchange for an outcome that is (second?) best and thus pay the biker.&nbsp;</p><p>We can of course play semantic slapfights and call this something like 'Bayesian Decision Theory' but ideas matter more than labels.</p>", "parentCommentId": null, "user": {"username": "ParetoOptimal"}}, {"_id": "zd8wtFjCTe3q5K83W", "postedAt": "2023-09-06T16:14:26.486Z", "postId": "GqK5S2ApsycdcRwCm", "htmlBody": "<p>\"There are three main branches of decision theory: descriptive decision theory (how real agents make decisions), prescriptive decision theory (how real agents should make decisions), and normative decision theory (how ideal agents should make outcomes).\"</p>\n<p>This doesn't seem right to me, I would say: an interesting way you can divide up decision theory is between descriptive decision theory (how people make decisions) and normative decision theory (how we should make decisions).</p>\n<p>The last line of your description, \"how ideal agents should make outcomes\" seems especially troubling. I'm not quite sure what you are trying to say.</p>\n<p>I think there are good parts of this post, for example, you're hitting some interesting thought experiments. But several aspects are slightly confusing as written. For example, Newcomb's problem isn't (I believe) a counterexample to EDT, but that isn't clear from your post.</p>\n", "parentCommentId": null, "user": {"username": "rileyharris"}}]