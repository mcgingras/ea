[{"_id": "GgKbRCSRnHGSQjHdv", "title": "14 seconds to read 14 days of a gap year project ", "postedAt": "2023-05-23T01:49:41.247Z", "htmlBody": "<p>Hello, I am Zack. As part of my gap year project, I plan to conduct interviews with individuals involved in very early technology projects (18-20 y/o) in order to gather insights and learn from their experiences. Ultimate goal is to produce recommendations based on the insights gathered that can inform future technology projects.</p><p>Anyway here's my gameplan:</p><ol><li>Set common elements</li><li>Identify Interviewees</li><li>Develop Questions</li><li>Analyze</li><li>Prioritize Recommendations</li></ol><p>Deliverables:</p><ul><li>List of identified interviewees</li><li>Set of standardized interview questions</li><li>Recording of all conducted interviews</li><li>Make project based report based on the interview</li><li>Final recommendations for future early-stage tech projects</li></ul><p>By utilizing research and analysis tools, conducting standardized interviews and producing final recommendations, this gap year project will have productive results towards guiding future early-stage tech projects.</p>", "user": {"username": "Zack Zalvaro"}}, {"_id": "qWwgrAE26qe3gzBbm", "title": "Self-leadership and self-love dissolve anger and trauma", "postedAt": "2023-05-22T22:30:06.895Z", "htmlBody": "", "user": {"username": "richard_ngo"}}, {"_id": "wRkYx5ZwnuCbkB5xH", "title": "Proposals for an EA Ombudsperson", "postedAt": "2023-05-22T22:16:44.887Z", "htmlBody": "<p>I've heard a few suggestions at EA Global that there should be an EA Ombudsperson, or a trade union for EA workers. I'm setting out a few initial, very rough thoughts in hopes that it might start a conversation.</p><h3>What problems would an ombudsperson NOT solve?</h3><p>In my view, \"solving community problems\" isn't a great area for competition. If Community Health and the ombudsperson were both separately hearing complaints about sexual harassment, for example, it's possible that neither would notice a pattern of behaviour. They could also contradict each other in unhelpful ways.</p><p>Similarly, I don't think an ombudsperson should try to take the place of a charitable board or law enforcement.</p><p><i><strong>Question 1: Do you agree that an ombudsperson should refer to existing mechanisms, such as Community Health, charitable boards, or law enforcement, whenever possible?</strong></i></p><h3>What problems would an ombudsperson solve?</h3><p>First, it's not always obvious, especially for newer members of the community, who can help with particular problems. An ombudsperson could <strong>direct people to the correct body</strong>, or pass on anonymous complaints.</p><p>Second, sometimes <strong>conflicts of interest</strong> or special circumstances mean that the usual channels would not be appropriate. For example, if someone wanted to complain about the conduct of a member of the Community Health team, the Community Health team could be perceived to be biased and unable to take an objective view. If a CEO of a charity which received a lot of donations from the EA community was potentially breaking the law, but was operating in a country where law enforcement was unreliable, an ombudsperson might be able to publish an independent view of the evidence.</p><p>Third, it might occasionally be useful for an ombudsperson to provide a <strong>second opinion</strong> on particularly controversial decisions made by other teams.</p><p><i><strong>Question 2: Do you agree an ombudsperson should direct people to existing mechanisms, handle conflicts of interests or special circumstances, and publish independent second opinions on especially controversial decisions?</strong></i></p><p><i><strong>Question 3: Are there any other problems you believe an ombudsperson could or should solve?</strong></i></p><p><i><strong>Question 4: Is it worthwhile having a dedicated person or team to solve these problems?</strong></i></p><h3>How would an ombudsperson be funded?</h3><p>Ideally, the ombudsperson would be funded by donations from individual members of the EA community. I would recommend that <strong>no more than 20% of the ombudsperson's funding</strong> comes from any one person or organisation - otherwise it would be too easy to accuse the ombudsperson of not being able to independently investigate their own funders.&nbsp;</p><p>It would be easiest for donations to be accepted by a sponsoring body, like Effective Ventures (the parent body for the Centre for Effective Altruism), but that could bring the ombudsperson's independence into question - so it might be wisest for the <strong>ombudsperson to be their own charity/entity</strong>. I don't know how straightforward this would be or how long it would take.</p><p><i><strong>Question 5: Do you agree there should be a limit to how much funding should come from any one organisation or individual? Should organisations be able to contribute at all?</strong></i></p><p><i><strong>Question 6: Should the ombudsperson be their own charitable entity?</strong></i></p><h3>How would an ombudsperson be chosen?</h3><p>The ombudsperson could be hired by a committee. In my view,<strong> </strong>in order to ensure the independence and legitimacy of the ombudsperson,<strong> no one on the hiring committee can be financially dependent on an EA organisation</strong> - they should not be employeed, applying for a grant, or a current grant recipient. This raises a question about how a search committee could be put together - could registered Forum accounts with at least 10 karma vote on hiring committee members to represent them? Could anyone who meets minimum criteria (not financially dependent on an EA organisation, knowledge of EA, knowledge of hiring) put themselves forward, and 3-5 could be randomly selected to be the hiring committee?</p><p>Alternatively, the ombudsperson could be <strong>directly elected</strong> by members of the EA community. This isn't my preferred option, because I think it's unlikely that the best campaigner will be the best ombudsperson, and I think a hiring committee would be able to dedicate more time to choosing the best candidate.</p><p><i><strong>Question 7: Should the ombudsperson be chosen by a hiring committee, directly elected, or some other method? Do you have any suggestions for how the hiring committee should be chosen or how the elections should be held?</strong></i></p><p><i><strong>Question 8: Do you have any other thoughts on the idea of an EA ombudsperson?</strong></i></p>", "user": {"username": "Khorton"}}, {"_id": "XYC8jmM4WPCDYZZmm", "title": "I don't want to talk about ai", "postedAt": "2023-05-22T21:19:05.912Z", "htmlBody": "<p>\"like any good ea, I try to have a <a href=\"https://en.wikipedia.org/wiki/The_Scout_Mindset\"><u>scout mindset</u></a>. I don\u2019t like to lie to myself or to other people. I try to be open to changing my mind. but that kind of intellectual honesty only works if you don\u2019t get punished for being honest. in order to think clearly about a topic, all of the potential outcomes have to be okay - otherwise you just end up with mental gymnastics trying to come up with the answer you want.\"</p><p>Read more: https://ealifestyles.substack.com/p/i-dont-want-to-talk-about-ai</p>", "user": {"username": "Khorton"}}, {"_id": "oo96uRHNbGjr4DHut", "title": "[Linkpost] \"Governance of superintelligence\" by OpenAI", "postedAt": "2023-05-22T20:15:25.083Z", "htmlBody": "<p>OpenAI has a new <a href=\"https://openai.com/blog/governance-of-superintelligence\">blog post</a> out titled \"Governance of superintelligence\" (subtitle: \"Now is a good time to start thinking about the governance of superintelligence\u2014future AI systems dramatically more capable than even AGI\"), by Sam Altman, Greg Brockman, and Ilya Sutskever.</p><p>The piece is short (~800 words), so I recommend most people just read it in full.</p><p>Here's the introduction/summary (bold added for emphasis):</p><blockquote><p>Given the picture as we see it now, it\u2019s conceivable that within the next ten years, AI systems will exceed expert skill level in most domains, and carry out as much productive activity as one of today\u2019s largest corporations.</p><p>In terms of both potential upsides and downsides, <strong>superintelligence will be more powerful than other technologies humanity has had to contend with in the past</strong>. We can have a dramatically more prosperous future; but we have to manage risk to get there. <strong>Given the possibility of existential risk, we can\u2019t just be reactive</strong>. Nuclear energy is a commonly used historical example of a technology with this property; synthetic biology is another example.</p><p>We must mitigate the risks of today\u2019s AI technology too, but <strong>superintelligence will require special treatment and coordination</strong>.</p></blockquote><p>&nbsp;</p><p>And below are a few more quotes that stood out:</p><p>&nbsp;</p><p>\"First, we need some degree of coordination among the leading development efforts to ensure that the development of superintelligence occurs in a manner that allows us to both maintain safety and help smooth integration of these systems with society.\"</p><p>...</p><p>\"Second, we are likely to eventually need something like an <a href=\"https://www.iaea.org/\"><u>IAEA</u></a> for superintelligence efforts; any effort above a certain capability (or resources like compute) threshold will need to be subject to an international authority that can inspect systems, require audits, test for compliance with safety standards, place restrictions on degrees of deployment and levels of security, etc.\"</p><p>...</p><p>\"It would be important that such an agency focus on reducing existential risk and not issues that should be left to individual countries, such as defining what an AI should be allowed to say.\"</p><p>...</p><p>\"Third, we need the technical capability to make a superintelligence safe. This is an <a href=\"https://openai.com/blog/our-approach-to-alignment-research\"><u>open research question</u></a> that we and others are putting a lot of effort into.\"</p><p>...</p><p>\"We think it\u2019s important to allow companies and open-source projects to develop models below a significant capability threshold, without the kind of regulation we describe here\"</p><p>...</p><p>\"By contrast, the systems we are concerned about will have power beyond any technology yet created, and we should be careful not to water down the focus on them by applying similar standards to technology far below this bar.\"</p><p>...</p><p>\"we believe it would be unintuitively risky and difficult to stop the creation of superintelligence\"</p>", "user": {"username": "Daniel_Eth"}}, {"_id": "jMgP8hHcvPLKh4v6d", "title": "Double-checking research", "postedAt": "2023-05-22T17:16:28.286Z", "htmlBody": "<p>Within the academic community, many people do important research in things like medicine, technology, etc that require a lot of money up-front but can pay back very well/ do a lot of good in the long term.</p>\n<p>However, a general issue with research is that there is small monetary incentive to reproduce research. This is a huge issue because it undermines one of the axioms of why science is so important: repeatability.</p>\n<p>For example, in 2022, a fundamental study for the theory of where Alzheimer's comes from (from 16 years prior) was found to have been forged. Prior to awareness of the forgery, the FDA approved a drug that, based on this research, should significantly decrease intensity or even completely cure Alzheimer\u2019s. In that same fiscal year, the NIH even spent $1.6 billion on research that mentions the results of the study, representing about half of overall Alzheimer\u2019s funding (see the article attached).</p>\n<p>Therefore, I propose that money ought to be raised to ensure that this sort of thing does not happen again by incentivizing academics to repeat studies.</p>\n<p>Comment below additional events similar to the Alzheimer\u2019s one below or any criticisms to this point</p>\n", "user": {"username": "Daniel Birnbaum"}}, {"_id": "9cHbWyA6zPdpHqnTG", "title": "RE-THINK POPULATION: AFRICAN\u2019S NEW CHALLENGE", "postedAt": "2023-05-22T14:45:37.811Z", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/4xd2EPXZ8GeAoCQSi/announcing-the-african-ea-forum-competition-usd4500-in\">Announcing the African EA Forum Competition - $4500 in prizes for excellent EA Forum Posts</a>&nbsp;</p><p>#Africa EA Forum Competition.&nbsp;</p><p><strong>RE-THINK POPULATION: AFRICAN\u2019S NEW PLAGUE&nbsp;</strong></p><p><strong>INTRODUCTION</strong></p><p>According to the World Bank, population growth rates have decelerated globally, but Africa will nonetheless experience strong population growth throughout the predictable future. By the year 2050, the population of Africa is projected to double, reaching up to two and a half billion inhabitants. While population growth may come with lots of problems, many observers and decision makers and concerns individuals have started to issue warnings about population growth in Africa as a potential cause or risk factor for conflict or for weakening security, along with related forecasted problems such as increased and uncontrolled migration. Regarding conflict risks, high birth rates or other drivers of population growth can lead to population pressures that may create a greater scarcity of economic resources and, as a result, lead to violent conflicts over the distribution of scarce resources, uncommon practice to produce food at the detriment of the environment.</p><p>European Union Institute for Security Studies (EUISS) in 2022 reported that;&nbsp;&nbsp;</p><p>environmental degradation is driv\u00aden not only by climate change but also by factors such as rapid population growth, over-consumption, unsustainable manage\u00adment and depletion of natural resources, in\u00addustrialisation, and also conflicts, has put pressure on African ecosystems. This has resulted in harm to habitats and wildlife with ensuing bi\u00adodiversity loss, increased food insecurity, exacerbation of socio-economic deprivation and grievances, and the emergence of poverty traps for a large por\u00adtion of the population living in rural areas who are dependent on natural resources for their own livelihoods. In turn, this also fosters increased pressure through intensified illicit and unsustainable use of natural resources (e.g. illegal logging, felling of trees, poaching, overfishing, animal trafficking, smuggling).</p><p>Today, most developing countries have achieved quite low levels of mortality and fertility and much lower rates of population growth than in the 1960s. The situation is quite different in Africa and especially in sub-Saharan Africa. In that region of the continent, mortality declines have stalled, or even reversed, in the 1980s and 1990s because of the impact of HIV/AIDS and civil unrest. In addition, fertility declines generally associated with mortality declines, in what is called the demographic transition, have occurred in most African countries much later than elsewhere in the world. As a result, most African countries still have very high levels of fertility, high rates of population growth and very young populations.</p><p>Increase in population at first instance may no have seem as a threat or something to worry about. But in-depth study of the phenomenon will give a clear understanding of the threat increasing population pose on the world. This paper looked carefully at the trend of increasing African population growth and pointed out some of the possible problem rapid population growth poses to the world. Careful observation at the outburst of population growth in Africa, will rise lots of questions, one may have no option than to ask series of questions, such may include; what is happening to Africa\u2019s population? What will Africa look like by 2050? What could be the possible results of over population in Africa? Etc. many of these questions require an immediate answer and a swift response.&nbsp;Guengant and May (2022) emphasised that;</p><p>African countries must today confront two major population related challenges: (a) they have to address the doubling or even the tripling by 2050 of their working age population; and (b) they have to better prepare for the future of their upcoming young generations. At the same time, almost all African countries will need also to confront the rapid increase of their elderly population, whose numbers will be multiplied by a factor of 3 to 5 by 2050.</p><p><strong>Population Trend in Africa</strong></p><p>Historically, in the last one century Africa's population has grown at an alarming rate. The various estimates of the population size of Africa indicate that prior to 1900, the annual growth rate of population was less than 0.1%; during the period 1900 to 1950, the population increase was 1.2%; while, in the period of 1950 to 1970, the population growth rate was estimated at 2.8%; in the period 1980\u20101990, the rate was about 3.2%. These data show that the recent demographic trends in Africa are characterized not only by unprecedented rapid growth rates but also associated with youthful unemployment (Organization of African Unity and Economic Commission for Africa 2011).</p><p>The major overriding objective of development is to provide and improve the quality of life for all citizens. Populations are therefore at the centre of development. Understanding demographic trends provides policy-makers with the tools to design interventions that lead to development by for instance targeting social sectors and providing tailored infrastructure services. Knowledge on the population trend is crucial for planning resources allocation and designing appropriate policies. Africa\u2019s current and projected demographic trends, growing population, urbanization, and the ratio of working age to non-working age and aging populations require in turn appropriate responses to the anticipated pressures on basic amenities (e.g. food, energy, and water resources).&nbsp;</p><p>Africa have faced major population explosion in the last few decades. Africa's population which was estimated at 257 million in 1960 had increased to 482 million in 1983 almost two times the initial population. One decade later the population of the continent was estimated at 682 million. The average annual growth rate during that decade was 3.2%, the highest among Third World regions. In 1983, the Economic Commission for Africa, using high variant assumptions, projected that total African population will be about 1.1 billion by 2008, taking an annual growth rate of 3.2% during the 25\u2010year period of 1983 to 2008, that we have seen the progress in reality.&nbsp;</p><p>By 2012 African Development Bank have in projections to 2030 affirmed that, the African population is expected to peak at 1.6 billion from 1.0 billion in 2010, which would represent 19% of the world\u2019s population. Asia and Latin America will account for 58% and 8%, of world population, respectively. These projections rely upon assumptions about vital fertility and mortality rates. The fertility rate is assumed to increase at a varying pace by country, and follow a trajectory similar to the one in other major global areas.</p><p>AfDB in 2014 said&nbsp;</p><p>\u201cAfrica\u2019s population has increased by 2.5 percent per year and in 2011 the number of people living in the young continent exceeded the 1 billion mark. This is expected to rise to at least 2.4 billion by 2050, with some of the countries doubling or even tripling their numbers, making Africa the region with the largest population growth.\u201d</p><p>Urban Population Trend, Africa, 1950-2050</p><p>&nbsp;</p><p><strong>Source:&nbsp;</strong>African Development Bank 2014</p><p><strong>The Challenges</strong></p><p>In recent years an increasing number of Africans are being added&nbsp;every year. This was not always the case, these population increases are unprecedented in history. But the problem of population is not simply a problem of numbers it is a problem of human welfare and of development. Rapid population growth which may have serious consequences for the wellbeing of humanity worldwide. If development entails the improvement in people's level of living i.e. their incomes, health, safety, education and general well\u2010being. And if it also encompasses their self-esteem, respect dignity and freedom of choice then the really important question about population growth is how does the contemporary population situation in many African countries contribute to or detract from their chances of realizing the goals of development, not only for the current generation but also for the future generations? Conversely, how does population growth affect human development?</p><p>Here are more questions raised by many concerned scholars as to the rate and level of population increase in Africa.&nbsp;</p><ol><li>Poverty alleviation: What are the implications of increase population growth rates among the third world country\u2019s poor for their chances of overcoming the human wretchedness of absolute poverty? Will world food supply and its distributions be sufficient not only to meet the anticipated population increase in the coming decades but also to improve nutritional levels to the point where all humans can have an adequate diet?</li><li>development in health and education: Given the anticipated population growth will African countries be able to extend the coverage and improve the quality of their health and educational systems so that everyone can at least have the chance to secure adequate health care and basic education?</li><li>Poverty and human freedom of choice: To what extent are the low levels of living an important factor in limiting the freedom of parents to choose a desired family size? Is there a relationship between poverty and family size?</li></ol><p>considering the questions above, it is imperative to frame the population problem not simply in terms of numbers, or densities, or rates, or migration but with full consideration of the qualities of human life: affluence in place of poverty eradication, education in place of illiteracy full opportunities for the future generations of children in place of current limitations.&nbsp;</p><p><strong>Possible Problems</strong></p><p>The product of rapid population growth is snowballing, the more births increase today makes it difficult to slow the population growth in the near future, because today's children will become tomorrow's parents. In general, food supplies and agricultural production must be greatly increased to meet the needs of a rapidly growing population, this limits the allocation of resources to other economic and social sectors. Similarly, the rapid increase in population means that there will be an increase in the dependency ratio, this implies that the country concerned will have to allocate increasing resources to feed, clothe, house and educate the useful component of the population which consumes but does not produce goods and services.&nbsp;</p><p>Again, a rapidly growing population has serious implications for the provision of productive employment Since the rapid population growth is normally associated with a proportionate increase in the supply of the labour force, it means that the rate of job creation is expected to match the rate of supply of the labour force. In Africa the rate of labour force supply has outstripped that of job creation, implying that the rates of unemployment have been increasing rapidly. In other words, the number of people seeking employment increases more rapidly than the number of available jobs. Hence, the increase in crime, theft, migration and all sort of problems bedevilling African continent.</p><p>Monica <i>et al</i> (2011) added that rapid population growth can be a constraint on economic growth, especially in poor countries with policies that do not encourage rapid rise in productivity. It must be noted that When a constant\u2010growing number of graduates cannot be absorbed in the modern economic sectors and workforce. The graduates and unemployed youths are forced either into unproductive activities, heinous crime or back into the traditional means of livelihood with its low productivity and low wage levels. This large supply for cheap labour tends to hold back technological advancement, growth and development. Industrialization is slowed by mass poverty, which in turn reduces the demand for manufactured goods. The possible results are low saving rates and low labour skills, both of which inhibit the full exploration of potentials, development and utilization of natural resources embedded in many African countries. In some countries, the growing population would outpace the levels at which renewable resources could be sustained, and the resource bases would deteriorate. Thus, widespread poverty, migration, crime, low labour productivity etc. The growing demand for food and slow industrialization distort and degrade the international trade of African countries.</p><p>Likewise, another major possible outcome of rapid Africa's population growth is the phenomenal growth rate of city population. Due to an increase in the total population, the Africa's urban population is expected to reach 1.3 billion by the year 2025 (OAU &amp; ECA 2020). This will come without adequate provision of housing facilities, basic and social amenities. The rapid population growth rate will result in poor and crowded housing in the urban slums of the rapidly growing cities, and this could also produce further social problems. Hence,&nbsp;Africa is the fastest urbanizing continent in the world (AfDB 2014).</p><p>Cost adequacy and nature of health and welfare services might also be affected by rapid population growth in much the same way as are those of educational services. In the individual family death and illness might be increased by high fertility and frequent pregnancies, and the necessity of caring for excessive numbers of children. It should also be noted that the physical and mental development of children are often affected in large families because of inadequate nutrition and the prevalence of diseases associated with poverty, and also many of such children grew without parental affection. This affect their affective domain of learning, that may be the reason why a child can kill a fellow human without feeling remorseful about it.</p><p><strong>Possible Solutions</strong></p><p>Empirical studies increasingly support the idea that countries which have incorporated population policies and family planning programs in their overall economic and social development strategies have achieved high and sustained rates of economic growth and that they have also managed significant reductions in poverty. Fertility reduction is by no means an economic development panacea and is certainly not a sufficient condition for economic growth, but it may well be a necessary condition, establishing conditions in which governments can invest more per capita in education, human and health, thus creating the human capital for sustained economic growth. Likewise, with fewer children to care for and raise, families can improve their prospects for escaping the poverty trap. At both the macro and micro levels, moderating fertility enhances economic prospects.</p><p>Recent studies by Monica <i>et al</i> conclude that reducing fertility facilitates economic growth in low-income countries. Low dependency ratios (resulting from fertility decline) create a window of opportunity for savings, increased productivity, and investment which if properly managed can transform living standards permanently. The more rapid the fertility decline in a region, the wider the window of opportunity, though its duration will be shorter because the population will age more rapidly. Micro-studies also find that lower fertility is also associated with better child health and schooling, reduced maternal mortality and morbidity, increased women\u2018s labour force participation, and higher household earnings. This is quite aside from the intrinsic human right of being able to control ones\u2019 own fertility.</p><p>Family planning programs are intrinsic to anti-poverty efforts by facilitating increases in living standards. They also form part of a package of measures addressing basic government failures that help sustain poverty and high fertility \u2015 including efforts to improve health and schooling, and to expand income-earning opportunities. Family planning programs help by increasing access to contraception, and by providing informational outreach to enhance perception of the benefits of shifting to a more secure equilibrium in which people have fewer children and are able to invest more in them.</p><p><strong>CONCLUSION</strong></p><p>Finally, there is no doubt that the high population problem in Africa is factual and challenging. The influence of the effect of high birth-rates and low death rates, increasing population size and density, rapid population growth, and increasing dependency burden all translate into greater demands on the African governments in productive activities which in turn highlights the problems of unemployment, underemployment, persistent poverty, urban slums, crime, drug pushing and political unrest.</p><p><strong>Reference</strong></p><p>African Development Bank Group (AfDB) (2012). \u201c<i>Briefing Notes for AfDB\u2019s Long-Term Strategy\u201d</i></p><p>African Development Bank Group (AfDB) (2014). \u201c<i>Tracking Africa\u2019s progress in figures\u201d</i></p><p>European Union Institute for Security Studies (EUISS) (2022). \u201c<i>African Spaces: the new geopolitical frontlines.</i>\u201d Edited by Giovanni Faleg.&nbsp;Printed in Belgium by Bietlot.</p><p>Guengant, JP and May, JF (2022).&nbsp;<i>AFRICA 2050: African Demography.&nbsp;</i>Emerging Markers Forum.</p><p>Monica Das Gupta, John Bongaarts, and John Cleland. (2011).&nbsp;<i>Population, Poverty, and Sustainable Development: a review of the evidence.</i>&nbsp;The World Bank Development Research Group Human Development and Public Services Team.</p><p>Organization of African Unity and Economic Commission for Africa (OAU &amp; ECA) (2020). <i>\u201cPopulation and development in Africa\u201d</i>&nbsp;</p><p>Steven W. Sinding. (2008) <i>Population, Poverty and Economic Development.</i> Paper prepared for the Bixby Forum, The World in 2050, Berkeley, California.</p>", "user": {"username": "Gaius JB"}}, {"_id": "L8XxRjP5MwPhk5y6v", "title": "Scaling Agentyness", "postedAt": "2023-05-22T13:29:51.707Z", "htmlBody": "<p><i>Epistemic status: Experimental. I'm developing these ideas as I write the sequence, drawing mostly from accumulated </i><a href=\"https://www.lesswrong.com/posts/gKeHcikcXA3bApyoM/opening-session-tips-and-advice#The_tacit_and_the_explicit\"><i>tacit knowledge</i></a><i> and practical literature on group leadership. Use what makes sense for you, and ignore what doesn't.</i></p><h1>Three stories about agentyness</h1><h2>1. \"<i>You</i> go and solve the alignment problem!\"</h2><p>A friend of mine once attended a worskhop with MIRI, hoping for them to tell her how to use her math skills to assist their alignment research. Instead, they layed out the problem, gave her some open questions, and asked her to try and solve alignment herself. And thus, she went forth and worked on exactly that.</p><h2>2. \"So, where do I sign up for an impactful career?\"</h2><p>As I was told, people often come to EA's various career advice services hoping for a ready-made ten-step-plan that will automatically funnel them into an impactful career.</p><p>Alas, that is not how it works.</p><p>Nobody knows your strengths, weaknesses, and passions better than <i>you</i>. Nobody can know better than <i>you </i>where your comparative advantage lies. And, if you already have a degree: Nobody knows your area of expertise better than <i>you,</i> and how it relates to EA. And, first and foremost: The most impactful jobs tend to be those that didn't exist before people created them for themselves.</p><p>EA has figured out <i>some</i> things: People before you have put a lot of thought into cause priorization, into what makes a happy and impactful career, which pitfalls there are along the way, et cetera. You <i>can</i> climb on the shoulders of giants. But: You have to do the climbing yourself. And along the way, it's inevitable that you, yourself, will turn into another giant who pushes EA's collective frontier of knowledge a tiny bit further.</p><p>Sounds scary? It is. But it is not an impossible task - others have succeeded before you, and as EA goes, they are usually more than glad to help you along the way. &nbsp;</p><h2>3. \"Do you <i>manage</i> the other volunteers?\"</h2><p>Recently, a grantmaker asked me:</p><blockquote><p><i>\"So, what is your relationship to the other EA Berlin volunteers? Do you manage them?\"</i></p></blockquote><p>And my response looked somewhat like this:</p><blockquote><p><i>\"I... Am... Arp... Instigate? Hand out backpats and then they do stuff and grow in numbers? Nudge and make suggestions? Treat them like adult human beings and then they behave as such, and get more engaged with EA because it fosters their adultness? Ask them to do me favors? Ask them what they need to make happen what they think is needed in EA Berlin? Ahm, coordinate them, maybe?</i></p></blockquote><p>I know what I'm doing, that it works even better than I hoped when I set out to do it, and I honestly don't know whether it would or wouldn't survive my parting. So, this post is an attempt at explaining what I'm <i>actually</i> trying to do in EA Berlin:</p><h1>Scaling agentyness</h1><p>There is a traditional take on management that I think is almost as prevalent in EA as in the corporate world. &nbsp;It goes somewhat like this:</p><blockquote><p>\"Most people are not agentic, some are. If you want to get work done, make the latter managers and the former their subordinates.\"</p></blockquote><p>I think this is false. And dangerously so, as for most of the challenges humanity faces this century, we need a whole lot of agentyness from a whole lot of people. I'd like to suggest a different assumption. I think it is more useful than the one above, because people tend to live up to it when we expect them to. Here you go:</p><blockquote><p>People are not unagentic by default, but the way we manage them in formal education, in traditional corporations, and in anything that we build by the example of these institutions <i>make</i> people unagentic.</p></blockquote><p>&nbsp;So, here are some ingredients I've found to help tickle agentyness out of others:</p><h2>1. Balance impact focus with psychological safety<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmo6q4n3axms\"><sup><a href=\"#fnmo6q4n3axms\">[1]</a></sup></span></h2><p>In her TED talk \"<a href=\"https://www.youtube.com/watch?v=LhoLuui9gX8\">Building a psychologically safe workplace</a>\", Amy Edmondson presents a two-dimensional graphic with motivation &amp; accountability on one axis, and psychological safety on the other:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Q4yLNscKwNYutgaxb/w8jsny4hxsa7p6nic4gp\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Q4yLNscKwNYutgaxb/uiy8istyrbnsq0tsckl5 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Q4yLNscKwNYutgaxb/w1ox4iuznvjesyqwvrex 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Q4yLNscKwNYutgaxb/qj7h5c1bjopcg4cbdsrp 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Q4yLNscKwNYutgaxb/sdm9gwukclme8ahlrnsp 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Q4yLNscKwNYutgaxb/dr6lejytivspqhxdsfmp 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Q4yLNscKwNYutgaxb/akuoa9aez8vafk8hmuec 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Q4yLNscKwNYutgaxb/ovdd5r5fgvf1xj0kvbxr 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Q4yLNscKwNYutgaxb/zm8faehzdfhe6tyvpa48 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Q4yLNscKwNYutgaxb/vyvpcdyvsasxj1rurfby 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Q4yLNscKwNYutgaxb/yml4qpv0oovsmybbmgbc 1376w\"></figure><p><i>Motivation &amp; Accountability vs. Psychological Safety. Recreated after a slide in Edmondson's TED talk</i></p><p>If we as community builders exclusively optimize for psychological safety, our community members end up in the Comfort Zone. Our meetups end up being lovely social gatherings, but little EA-aligned learning and planning happens. Worst-case, the more impact-driven individuals leave in disappointment.</p><p>However, if we neglect psychological safety, community members tend to end up in the panic zone. They experience impostor syndrome and burnout. They feel unheard when they voice their dissatisfaction, or don't dare to speak up in the first place - until they can't stay silent anymore and <a href=\"https://forum.effectivealtruism.org/posts/54vAiSFkYszTWWWv4/doing-ea-better-1\">write very long anonymous EA forum posts</a>. Worst-case, they lose their motivation entirely, (value) drift into the Apathy Zone, and drop out of the community entirely.</p><p>In her talk, Edmondson makes three simple suggestions for how to cultivate more psychological safety in organizations that already have a strong culture of accountability and performance. I find them fairly intuitive, so I'll just summarize them here:</p><ol><li><strong>Frame the work as a learning problem, not an execution problem.</strong> For example, the leader of a meeting may set the group's expectations by saying: <i>\"We've never been here before. &nbsp;We don't know what will happen. We got to have everyone's voices and brains in the game.\"</i></li><li><strong>Acknowledge your own fallibility.</strong> This piece of advice applies to both team members and leaders. This makes speaking up safer for others as well. A useful sentence: <i>\"I may miss something here, so I need your help.\"&nbsp;</i></li><li><strong>Model curiosity.</strong> Ask a lot of questions in order to create a necessity for voice. Something you may want to have in mind: There is a difference between <i>interested</i> and <i>interesting</i> questions. Interested questions optimize for your own curiosity and learning; interesting questions for helping the asked person understand themselves and the problem at hand better. Either of them are useful in different contexts, and most of us tend to default to one of the two types. So, be clear about your intentions.</li></ol><h2>2. Inspire heroic responsibility</h2><p>All these suggestions imply that psychological safety is, among others, instrumental for a different thing: <i>Enabling people to think and decide for themselves</i>. And if you go two steps further from there, you end up with a property Yudkowsky called <i>Heroic Responsibility</i>. As the LessWrong Wiki <a href=\"https://www.lesswrong.com/tag/heroic-responsibility\">defines it</a>:</p><blockquote><p>\"<strong>Heroic Responsibility</strong> is the responsibility to get the job done no matter what, including not shifting any responsibility for its completion on to others.\"</p></blockquote><p>Or, as Harry explains it to Hermione in <a href=\"http://hpmor.com/chapter/75\">HPMOR</a>, chapter 75:</p><blockquote><p><i>\"You could call it <strong>heroic responsibility</strong>, maybe,\u201d Harry Potter said. \u201cNot like the usual sort. It means that whatever happens, no matter what, it\u2019s always your fault. Even if you tell Professor McGonagall, she\u2019s not responsible for what happens, you are. Following the school rules isn\u2019t an excuse, someone else being in charge isn\u2019t an excuse, even trying your best isn\u2019t an excuse. There just aren\u2019t any excuses, you\u2019ve got to get the job done no matter what.\u201d Harry\u2019s face tightened. \u201cThat\u2019s why I say you\u2019re not thinking responsibly, Hermione. Thinking that your job is done when you tell Professor McGonagall\u2014that isn\u2019t heroine thinking. Like Hannah being beat up is okay then, because it isn\u2019t your fault anymore. Being a heroine means your job isn\u2019t finished until you\u2019ve done whatever it takes to protect the other girls, permanently.\u201d In Harry\u2019s voice was a touch of the steel he had acquired since the day Fawkes had been on his shoulder. \u201cYou can\u2019t think as if just following the rules means you\u2019ve done your duty.</i></p></blockquote><p>I think as community builders, we might want to try and inspire people to take heroic responsibility - for only then are they fully equipped to decide what's right, and to follow through on it to the end. Here are some practical things I found useful for instigating (heroic) responsibility.</p><h3>1. Facilitate, don't manage</h3><p>One piece of the puzzle is encouraging your subordinates/volunteers to take full ownership for their tasks. Here's how I try and do that.</p><p>When somebody suggests something that I agree should be done, more often than not, my response looks somewhat like this:</p><blockquote><p>\"Yea, I think it would be good if that happened. But I'm currently running at capacity with the fundraising and all. How'd you feel about trying your hands at it?\"</p></blockquote><p>And when they take on the task, I try to avoid saying too much about how I think the job should be done. Instead, my first question usually is:</p><blockquote><p>\"How can I support you in doing this?\"</p></blockquote><p>I think that people who strive to manage their subordinates to excellence sometimes overlook that micromanaging and unsolicited advice are <i>extremely costly.</i></p><p>If feedback is not delivered flawlessly (and even then), it tends to alienate people from their tasks by taking ownership away from them. It discourages them from trying new things. It destroys opportunities for learning, because it discourages subordinates from making and testing their own hypotheses for how to make something work.</p><p>And thus, if I were to monitor too much and too closely, I'd sacrifice a bit of a volunteer's long-term performance and motivation over and over again in order to make one single event go 5% more smoothly. I don't want that.</p><p>But does that mean I just let bad stuff happen without interference?</p><p>Not really. What I find <i>way</i> more useful than giving feedback and advice is debriefing events with the other organizers, through reflection rounds on eye level, where I'm just as much a participant as everyone else. My three standard questions:</p><ol><li><i>What went well?</i></li><li><i>What could go better next time?</i></li><li><i>What have we learned?</i></li></ol><h3>2. Make use of the Advice Process</h3><p>The Advice Process is a well-tested process for decisionmaking in self-managing teams. I first found it described in Frederik Laloux' book \"Reinventing Organizations\", where even organizations with thousands of employees are described to use it. And, almost simultaneously, I got to see its full force in action in how the Burning Man-community organizes.</p><p>A short description of the Advice Process by Burning Nest:</p><blockquote><p><i>\"The general principle is that anyone should be able to make any decision regarding Burning Nest.</i></p><p><i>Before a decision is made, you must ask advice from those who will be impacted by that decision, and those who are experts on that subject.</i></p><p><i>Assuming that you follow this process, and honestly try to listen to the advice of others, that advice is yours to evaluate and the decision yours to make.\"</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreft1gkadi0do\"><sup><a href=\"#fnt1gkadi0do\">[2]</a></sup></span></p></blockquote><p>Since I first encountered it, the Advice Process has become the guiding star for all my EA organizing. I find using the process in this context remarkably elegant because it merely explicates how many things in EA are already handled implicitly. And, where things are <i>not</i> done like this, making the Advice Process explicit props up a greater vision for how things <i>could</i> be, if EA were to reach the full potential of its innovative forms of organizing.&nbsp;</p><p>If used competently, the Advice Process can allow people to move independently, responsibly, and effectively even within an organization that consists of thousands of other people.</p><p>At the same time, it can reduce the risk of people getting stuck in decision paralysis, by telling them that there is no-one they have to ask for permission. And, because actively seeking advice is implied, it may prevent EAs from invoking the <a href=\"https://forum.effectivealtruism.org/topics/unilateralist-s-curse\">unilateralist's curse</a> by blindly starting something that others decided for good reasons not to make happen.</p><p>Of course, not where infohazards are involved. But, I think there are a significant amount of non-infohazardey areas of EA that could be lubricated by making the Advice Process the norm.</p><h2>3. Develop good conflict resolution strategies</h2><p>If you just follow orders, there will be no conflict. If you work together with other people on eye-level, however, disagreements are inevitable. Not only about facts, but also in regards to values, priorities, visions, or ways of communicating. The more you give up on traditional hierarchies, the more important it gets to know a thing or two about conflict resolution.</p><p>&nbsp;In 1965, Bruce Tuckman proposed an <a href=\"https://www.wcupa.edu/coral/tuckmanStagesGroupDelvelopment.aspx\">infamous model of group development</a>. According to his observations, any group of people tends to evolve through four consecutive stages: Forming, storming, norming, and performing.</p><ul><li><strong>Forming:</strong> Everything is new and exciting. Group members get to know each other, discover commonalities, and start working together. Group performance is reasonably high.</li><li><strong>Storming:</strong> Group members begin to discover differences in opinion, work flow, personalities. Covert and open conflict erupts. Some people might decide to leave the group. Performance goes down, because the interpersonal problems eat so much attention.</li><li><strong>Norming:</strong> Roles, rules, routines, and responsibilities get negotiated. Those who stayed find new trust and a good rhythm for working together. The group finds back to good performance.</li><li><strong>Performing:</strong> Everybody in the group knows what they have to do, and how to interact with one another. The group reaches peak performance.</li></ul><p>Every time a person joins or leaves a group, its whole relationship structure shifts and it has to go through a version of these phases again.</p><p>The main takeaway from this model I'd like you to consider: Avoiding conflict is usually not actually good for a group. Getting stuck in the Forming-phase keeps a group cold, distant, and below peak performance. Instead, you might want to offer tools that help the group members address their differences constructively. This is true for local EA groups just as much as for teams in EA orgs. And, I have the impression that in some sense, EA at large is currently going through the storming/forming-phases after the last year of scandals.</p><p>I listed some tools that might help EAs deal with conflict in an <a href=\"https://forum.effectivealtruism.org/posts/eLY4GKTgxxdtBHEep/ea-is-going-through-a-bunch-of-conflict-here-s-some-social\">earlier post</a>.&nbsp;</p><p>However, the most useful one I've found so far are <i>Withholds.</i> Here's what that is, cited from <a href=\"https://www.authrev.org/\">Authentic Revolution</a>'s Authentic Relating Games Manual:</p><blockquote><p>\"<strong>Withholds</strong> are things that we have left unsaid in a relationship or connection with another person. Sharing them regularly, before they build up, is the absolute best way I know to maintain relational health in a team, community, or relationship. These are a lifetime practice for me.</p><p>Withholds can either be given in the full group, like Truths, or one-on-one at any time. To share a withhold, first ask the person if they\u2019re open to it, and then set context for why you want to share.</p><p>For example:</p><p><i>\u201cSandra, may I share a withhold with you? We had a moment last week that really</i><br><i>didn\u2019t feel good to me, and I\u2019ve been holding a judgment about you since. I\u2019d like to come back into connection.\u201d</i></p><p>Then, share whatever you haven\u2019t been saying with the person. Example:</p><p><i>\u201cI was really pissed off when you overrode me in our planning meeting. I felt like I</i><br><i>didn\u2019t matter to you, and then I had trouble participating in the rest of the event</i><br><i>afterwards. How is that to hear from me? Did you notice that moment? What was</i><br><i>happening for you?\"</i></p><p>Give your partner time to respond, and continue going back and forth until you both feel complete.\"</p></blockquote><h1>Last words</h1><p>As always, <a href=\"https://slatestarcodex.com/2014/03/24/should-you-reverse-any-advice-you-hear/\">reverse this advice as needed</a>. If any of this seems reasonable to you, try it. If it seems weird but interesting, debate it. If it doesn't make sense and seems dangerously wrong, by no means try to implement it, and let me know - because of course, you might be right.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmo6q4n3axms\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmo6q4n3axms\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For more on what psychological safety is and some properties of EA that curtail it, see my <a href=\"https://forum.effectivealtruism.org/posts/egX9ftjgsvg2MxLXr\">previous post</a> on the topic. Thanks to Katarina Hahn for reminding me of Amy Edmondson's video.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnt1gkadi0do\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreft1gkadi0do\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.loomio.com/burning-nest-advice-process/\">https://www.loomio.com/burning-nest-advice-process/</a>&nbsp;</p></div></li></ol>", "user": {"username": "Severin T. Seehrich"}}, {"_id": "QFPihrs6yzcaLsgSZ", "title": "Can EA fund PPE stockpiles in some small, low income countries now?", "postedAt": "2023-05-22T12:52:22.158Z", "htmlBody": "<p>National PPE stockpiles are probably hardest for governments in low income countries to fund because of the pressure to fund health interventions which produce immediate benefits. Can new EA orgs try to do this right now in some of the smaller low income countries (eg- Lesotho, Eritrea, Liberia)?</p>", "user": {"username": "freedomandutility"}}, {"_id": "mHk9h3RxvuGmTThaS", "title": "If you find EA conferences emotionally difficult, you're not alone", "postedAt": "2023-05-22T12:48:57.255Z", "htmlBody": "<p>I went to EAG London this weekend. I had some interesting chats, wrote some cryptic squiggles in my notebook (\u201cClockify\u201d \u201cthe Easterlin paradox\u201d, \u201cfunctionalist eudaimonic theories\u201d), and gave and received some hopefully-useful advice. Overall, the conference was fun and worthwhile for me. But at times, I also found the conference emotionally difficult.</p><p>I think this is pretty common. After last year\u2019s EAG, Alastair Fraser-Urquhart wrote about how <a href=\"https://forum.effectivealtruism.org/posts/Adzj5WxAnzERHhNBp/i-burnt-out-at-eag-let-s-talk-about-it\">he burnt out at the conference</a> and had to miss a retreat starting the next day. The post was popular, and many said they\u2019d had similar experiences.</p><p>The standard euphemism for this facet of EA conferences is \u2018intense\u2019 or \u2018tiring\u2019, but I suspect these adjectives are often a more socially-acceptable way of saying \u2018I feel low/anxious/exhausted and want to curl up in a foetal position in a darkened room\u2019.</p><p>I want to write this post to:</p><ul><li>balance out the \u2018woo EAG lfg!\u2019 hype, and help people who found it a bad or ambivalent experience to feel less alone</li><li>dig into to why EAGs can be difficult: this might help attendees have better experiences themselves, and also create an environment where others are more likely to have good experiences</li><li>help people who mostly enjoy EAGs understand what their more neurotic or introverted friends are going through</li></ul><p>Here are some reasons that EAGs might be emotionally difficult. Some of these I\u2019ve experienced personally, others are based on comments I\u2019ve heard, and others are plausible educated guesses.</p><h1>It\u2019s easy to compare oneself (negatively) to others&nbsp;</h1><p><br>EA conferences are attended by a bunch of \u201cimpressive\u201d people: big-name EAs like Will MacAskill and Toby Ord, entrepreneurs, organisation leaders, politicians, and \u201cinner-circle-y\u201d people who are Forum- or Twitter-famous. You\u2019ve probably scheduled meetings with people <i>because</i> they\u2019re impressive to you; perhaps you\u2019re seeking mentorship and advice from people who are more senior or advanced in your field, or you want to talk to someone because they have cool ideas.</p><p>This can naturally inflame impostor syndrome, feelings of inadequacy, and negative comparisons. Everyone seems smarter, harder-working, more agentic, better informed. Everyone\u2019s got it all figured out, while you\u2019re still stuck at Stage 2 of 80k\u2019s career planning process. Everyone expects you to have a plan to save the world, and you don\u2019t even have a plan for how to start making a plan.</p><p>Most EAs, I think, know that these thought patterns are counterproductive. But even if some rational part of you knows this, it can still be hard to fight them - especially if you\u2019re tired, scattered, or over-busy, since this makes it harder to employ therapeutic coping mechanisms.</p><h1>The stakes are high</h1><p><br>We\u2019re trying to solve immense, scary problems. We (and CEA) pour so much time and money into these conferences because we hope that they\u2019ll help us make progress on those problems. This can make the conferences anxiety-inducing - you really really hope that the conference pays off. This is especially true if you have some specific goal - such as finding a job, collaborators or funders - or if you think the conference has a high opportunity cost for you.</p><h1>You spend a lot of time talking about depressing things</h1><p><br>This is just part of being an EA, of course, but most of us don\u2019t spend <i>all</i> our time directly confronting the magnitude of these problems. Having multiple back-to-back conversations about \u2018how can we solve [massive, seemingly-intractable problem]?\u2019 can be pretty discouraging.</p><h1>Everything is busy and frantic</h1><p><br>You\u2019re constantly rushing from meeting to meeting, trying not to bump into others who are doing the same. You see acquaintances but only have time to wave hello, because your schedule is packed. During each meeting, you\u2019re vaguely keeping an eye on the time, to make sure you're not late to your next meeting. Swapcard and Slack are constantly pinging you. You might also be jetlagged, or undersleeping because you\u2019re trying to cram in more conferencing and socialising. You\u2019re wolfing down food while discussing the alignment problem. It\u2019s really hard to be \u2018in the moment\u2019, digest, process. This might be particularly hard for neurodivergent people, who may find the environment over-stimulating and overwhelming.&nbsp;</p><h1>You\u2019re confronted with your limitations</h1><p><br>Maybe people give you advice that won\u2019t work for you, and the reasons it won't work are sore spots, emotionally-sensitive points. This can be hard to navigate; it feels insincere to accept the advice without comment, but also you might not want to go into sensitive personal issues with someone you\u2019ve just met. For example, maybe someone suggests that you do an intensive fellowship or ambitious project that you\u2019re generally a good fit for, but you know you\u2019re not mentally or physically well enough to do it. Maybe someone pitches a really tempting side project but you know you won\u2019t have time: you\u2019re already busy with your job and your parenting responsibilities. Maybe people are telling you to move to the Bay Area, but though you\u2019d love to do that, family responsibilities keep you where you are.</p><h1>You might have bad interactions&nbsp;</h1><p><br>You might run into an abusive ex or boss. Maybe someone makes inappropriate sexual advances. Perhaps you experience micro-aggressions. Maybe someone you meet with is just an asshole. Or you meet with someone who\u2019s not an asshole, but whose communication style you nonetheless find challenging: for example, because they are blunter and more \u2018ask-culture-y\u2019, or because they bring up things that feel more intimate and personal than you would like.</p><p>&nbsp;</p><p>This post is a bit of a downer, so I want to emphasise: I think EA conferences are net good. I\u2019m not asking the organisers to do things differently, and I\u2019m grateful for what they already do to mitigate these problems (for example, providing nap rooms, chill-out rooms, and quiet working rooms for introverts to hide in). And the emotionally-difficult aspects of EA conferences are flipsides of more positive things: meeting impressive-seeming people can be intimidating, but it can also be inspiring. It\u2019s good that people take the conferences seriously and want to make the most of them. And we shouldn\u2019t expect a 1600-person international conference to have the vibes of a silent meditation retreat.</p><p>But hopefully this post will counterbalance some of the exuberance and help us acknowledge that these valuable events are ambivalent for many of us, and certainly not universally fun or energising.&nbsp;</p>", "user": {"username": "Amber"}}, {"_id": "ibw9da6DbCYJczxYB", "title": "On navigating serious philosophical confusion (Joe Carlsmith on The 80,000 Hours Podcast)", "postedAt": "2023-05-22T12:53:04.169Z", "htmlBody": "<p>Over at <a href=\"https://80000hours.org/podcast/\">The 80,000 Hours Podcast</a> we just published an interview that is likely to be of particular interest to people who identify as involved in the effective altruism community: <a href=\"https://80000hours.org/podcast/episodes/joe-carlsmith-navigating-serious-philosophical-confusion/\"><strong>Joe Carlsmith on navigating serious philosophical confusion</strong></a>.</p><p>You can click through for the audio, a full transcript, and related links. Below is the episode summary and some key excerpts.</p><h2><strong>Episode summary</strong></h2><blockquote><p><i>\u2026if you really think that there\u2019s a good chance that you\u2019re not understanding things, then something that you could do that at least probably has some shot of helping is to put future generations in a better position to solve these questions \u2014 once they have lots of time and hopefully are a whole lot smarter and much more informed than we are\u2026</i></p><p><i>Joe Carlsmith</i></p></blockquote><p>What is the nature of the universe? How do we make decisions correctly? What differentiates right actions from wrong ones?</p><p>Such fundamental questions have been the subject of philosophical and theological debates for millennia. But, as we all know, and <a href=\"https://survey2020.philpeople.org/survey/results/all\">surveys of expert opinion</a> make clear, we are very far from agreement. So\u2026 with these most basic questions unresolved, what\u2019s a species to do?</p><p>In today\u2019s episode, philosopher Joe Carlsmith \u2014 Senior Research Analyst at Open Philanthropy \u2014 makes the case that many current debates in philosophy ought to leave us confused and humbled. These are themes he discusses in his PhD thesis, <a href=\"https://joecarlsmith.com/2023/02/21/a-stranger-priority-topics-at-the-outer-reaches-of-effective-altruism\"><i>A stranger priority? Topics at the outer reaches of effective altruism</i></a>.</p><p>To help transmit the disorientation he thinks is appropriate, Joe presents three disconcerting theories \u2014 originating from him and his peers \u2014 that challenge humanity\u2019s self-assured understanding of the world.</p><p>The first idea is that we might be living in a computer simulation, because, in <a href=\"https://en.wikipedia.org/wiki/Simulation_hypothesis\">the classic formulation</a>, if most civilisations go on to run many computer simulations of their past history, then most beings who perceive themselves as living in such a history must themselves be in computer simulations. Joe prefers a somewhat different way of making the point, but, having looked into it, he hasn\u2019t identified any particular rebuttal to this \u2018simulation argument.\u2019</p><p>If true, it could revolutionise our comprehension of the universe and the way we ought to live.</p><p>The second is <a href=\"https://joecarlsmith.com/2021/08/27/can-you-control-the-past\">the idea</a> that \u201cyou can \u2018control\u2019 events you have no causal interaction with, including events in the past.\u201d The thought experiment that most persuades him of this is the following:</p><blockquote><p>Perfect deterministic twin prisoner\u2019s dilemma: You\u2019re a deterministic AI system, who only wants money for yourself (you don\u2019t care about copies of yourself). The authorities make a perfect copy of you, separate you and your copy by a large distance, and then expose you both, in simulation, to exactly identical inputs (let\u2019s say, a room, a whiteboard, some markers, etc.). You both face the following choice: either (a) send a million dollars to the other (\u201ccooperate\u201d), or (b) take a thousand dollars for yourself (\u201cdefect\u201d).</p></blockquote><p>Joe thinks, in contrast with the dominant theory of correct decision-making, that it\u2019s clear you should send a million dollars to your twin. But as he explains, this idea, when extrapolated outwards to other cases, implies that it could be sensible to take actions in the hope that they\u2019ll improve parallel universes you can never causally interact with \u2014 or even to improve <i>the past</i>. That is nuts by anyone\u2019s lights, including Joe\u2019s.</p><p>The third disorienting idea is that, as far as we can tell, the universe could be infinitely large. And that fact, if true, would mean we probably have to make choices between actions and outcomes that involve infinities. Unfortunately, doing that breaks our existing ethical systems, which are only designed to accommodate finite cases.</p><p>In an infinite universe, our standard models end up unable to say much at all, or give the wrong answers entirely. While we might hope to patch them in straightforward ways, having looked into ways we might do that, Joe has concluded they all quickly get complicated and arbitrary, and still have to do enormous violence to our common sense. For people inclined to endorse some flavour of utilitarianism, Joe thinks \u2018infinite ethics\u2019 spell the end of the \u2018<a href=\"https://jc.gatspress.com/pdf/infinite_ethics_revised.pdf\">utilitarian dream</a>\u2018 of a moral philosophy that has the virtue of being very simple while still matching our intuitions in most cases.</p><p>These are just three particular instances of a much broader set of ideas that <a href=\"https://80000hours.org/podcast/episodes/ajeya-cotra-worldview-diversification/\">some have dubbed</a> the \u201ctrain to crazy town.\u201d Basically, if you commit to always take philosophy and arguments seriously, and try to act on them, it can lead to what seem like some pretty crazy and impractical places. So what should we do with this buffet of plausible-sounding but bewildering arguments?</p><p>Joe and Rob discuss to what extent this should prompt us to pay less attention to philosophy, and how we as individuals can cope psychologically with feeling out of our depth just trying to make the most basic sense of the world.</p><p>In the face of all of this, Joe suggests that there is a promising and robust path for humanity to take: keep our options open and put our descendants in a better position to figure out the answers to questions that seem impossible for us to resolve today \u2014 a position he calls \u201cwisdom longtermism.\u201d</p><p>Joe fears that if people believe we understand the universe better than we really do, they\u2019ll be more likely to try to commit humanity to a particular vision of the future, or be uncooperative to others, in ways that only make sense if you were certain you knew what was right and wrong.</p><p>In today\u2019s challenging conversation, Joe and Rob discuss all of the above, as well as:</p><ul><li>What Joe doesn\u2019t like about the drowning child thought experiment</li><li>An alternative thought experiment about helping a stranger that might better highlight our intrinsic desire to help others</li><li>What Joe doesn\u2019t like about the expression \u201cthe train to crazy town\u201d</li><li>Whether Elon Musk should place a higher probability on living in a simulation than most other people</li><li>Whether the deterministic twin prisoner\u2019s dilemma, if fully appreciated, gives us an extra reason to keep promises</li><li>To what extent learning to doubt our own judgement about difficult questions \u2014 so-called \u201cepistemic learned helplessness\u201d \u2014 is a good thing</li><li>How strong the case is that advanced AI will engage in generalised power-seeking behaviour</li></ul><p>Get this episode by subscribing to our podcast on the world\u2019s most pressing problems and how to solve them: type \u201880,000 Hours\u2019 into your podcasting app.&nbsp;</p><p><i>Producer: Keiran Harris</i><br><i>Audio mastering: Milo McGuire and Ben Cordell</i><br><i>Transcriptions: Katy Moore</i></p><h2>Highlights</h2><h3><strong>The deterministic twin prisoner's dilemma</strong></h3><blockquote><p><strong>Joe Carlsmith:</strong> The experiment that convinces me most is: Imagine that you are a <a href=\"https://en.wikipedia.org/wiki/Determinism\">deterministic</a> AI system and you only care about money for yourself. So you\u2019re selfish. There\u2019s also a copy of you, a perfect copy, and you\u2019ve both been separated very far away \u2014 maybe you\u2019re on spaceships flying in opposite directions or something like that. And you\u2019re both going to face the exact same inputs. So you\u2019re deterministic: the only way you\u2019re going to make a different choice is if the computers malfunction or something like that. Otherwise you\u2019re going to see the exact same environment.</p><p>In the environment, you have the option of taking $1,000 for yourself: we\u2019ll call that \u201cdefecting\u201d \u2014 or giving $1 million to the other guy: we\u2019ll call that \u201ccooperating.\u201d The structure is similar to a <a href=\"https://en.wikipedia.org/wiki/Prisoner%27s_dilemma\">prisoner\u2019s dilemma</a>. You\u2019re going to make your choice, and then later you\u2019re going to rendezvous.</p><p>So what should you do? Well, here\u2019s an argument that I don\u2019t find convincing, but that I think would be the argument offered by someone who thinks you can only control what you can cause. The argument would be something like: your choice doesn\u2019t cause that guy\u2019s choice. He\u2019s far away; maybe he\u2019s lightyears away. You should treat his choice as fixed. And then whatever he chooses, you get more money if you defect. If he defects, then you\u2019ll get nothing by cooperating and $1,000 by defecting. If he sends the money to you, then you\u2019ll get $1.001 million by defecting and $1 million by cooperating. No matter what, it\u2019s better to defect. So you should defect.</p><p>But I think that\u2019s wrong. The reason I think it\u2019s wrong is that you are going to make the same choice. You\u2019re deterministic systems, and so whatever you do, he\u2019s going to do it too. In fact, in this particular case \u2014 and we can talk about looser versions where the inputs aren\u2019t exactly identical \u2014 the connection between you two is so tight that literally, if you want to write something on your whiteboard, he\u2019s going to write that too. If you want him to write on his whiteboard, \u201cHello, this is a message from your copy,\u201d or something like that, you can just write it on your own whiteboard. When you guys rendezvous, his whiteboard will say the thing that you wrote. You can sit there going, \u201cWhat do I want?\u201d You really can control what he writes. If you want to draw a particular kitten, if you want to scribble in a certain way, he\u2019s going to do that exact same thing, even though he\u2019s far away and you\u2019re not in causal interaction with him.</p><p>To me, I think there\u2019s just a weird form of control you have over what he does that we just need to recognise. So I think that\u2019s relevant to your decision, in the sense that if you start reaching for the defect button, you should be like, \u201cOK, what button is he reaching for right now?\u201d As you move your arm, his arm is moving with you. And so you reach for the defect, he\u2019s about to defect. You could basically be like, \u201cWhat button do I want him to press?\u201d and just press it yourself and he\u2019ll press it. So to me, it feels pretty easy to press the \u201csend myself $1 million\u201d button.</p></blockquote><h3><strong>Newcomb's problem</strong></h3><blockquote><p><strong>Joe Carlsmith:</strong> The classic thought experiment that people often focus on, though I don\u2019t think it\u2019s the most dispositive, is this case called <a href=\"https://en.wikipedia.org/wiki/Newcomb%27s_paradox\">Newcomb\u2019s problem</a>, where Omega is this kind of superintelligent predictor of your actions. Omega puts you in the situation where you face two boxes: one of them is opaque, one of them is transparent. The transparent box has $1,000, the opaque box has either $1 million or nothing.</p><p>Omega puts $1 million in the box if Omega predicts that you will take only the opaque box and leave the $1,000 alone (even though you can see it right there). And Omega puts nothing in the opaque box if Omega predicts that you will take both boxes.</p><p>So the same argument arises for the causal decision theory (CDT). For CDT, the thought is: you can\u2019t change what\u2019s in the boxes; the boxes are already fixed. Omega already made her prediction. And no matter what, you\u2019ll get more money if you take the $1,000. If there was some dude over there who could see the boxes, and you were like, \u201cHey, see what\u2019s in the box, and what choice will give me more money?\u201d \u2014 you don\u2019t even need to ask, because you know it\u2019s always just take the extra $1,000.</p><p>But I think you should one-box in this case, because I think if you one-box then it will have been the case that Omega predicted that you one-boxed, because Omega is always right about the predictions, and so there will be the million.</p><p>I think a way to pump this intuition for me that matters is imagining doing this case over and over with Monopoly money. Each time, I try taking two boxes and I notice the opaque box is empty. I take one box, opaque box is full. I do this over and over. I try doing intricate mental gymnastics. I do like a somersault, I take the boxes. I flip a coin and take the box \u2014 well, flipping a coin, Omega has to be really good, so we can talk about that.</p><p>If Omega is sufficiently good at predicting your choice, then just like every time, what you eventually will learn is that you effectively have a type of magical power. Like I can just wave my arms over the opaque box and say, \u201cShazam! I hereby declare that this box shall be full with $1 million. Thus, as I one-box, it is so.\u201d Or if I can be like, \u201cShazam! I declare that the box shall be empty. Like thus, as I two-box, it is so.\u201d I think eventually you just get it in your bones, such that when you finally face the real money, I guess I expect this feeling of like, \u201cI know this one, I\u2019ve seen this before.\u201d I kind of know what\u2019s going to happen at some more visceral expectation level if I one-box or two-box, and I know which one leaves me rich.</p></blockquote><h3><strong>The idea of 'wisdom longtermism'</strong></h3><blockquote><p><strong>Joe Carlsmith:</strong> In the <a href=\"https://jc.gatspress.com/pdf/carlsmith_thesis.pdf\">thesis</a>, I have this distinction between what I call \u201cwelfare longtermism\u201d and \u201cwisdom longtermism.\u201d</p><p>Welfare longtermism is roughly the idea that our moral focus should be on specifically the welfare of the finite number of future people who might live in our <a href=\"https://en.wikipedia.org/wiki/Light_cone\">lightcone</a>.</p><p>And wisdom longtermism is a broader idea that our moral focus should be reaching a kind of wise and empowered civilisation in general. I think of welfare longtermism as a lower bound on the stakes of the future more broadly \u2014 at the very least, the future matters at least as much as the welfare of the future people matters. But to the extent there are other issues that might be game changing or even more important, I think the future will be in a much better position to deal with those than we are, at least if we can make the right future. \u2026</p><p>There\u2019s a line in Nick Bostrom\u2019s book <a href=\"https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies\"><i>Superintelligence</i></a> about something like, if you\u2019re digging a hole but there\u2019s a bulldozer coming, maybe you should wonder about the value of digging a hole. I also think we\u2019re plausibly on the cusp of pretty radical advances in humanity\u2019s understanding of science and other things, where there might be a lot more leverage and a lot more impact from making sure that the stuff you\u2019re doing matters specifically to how that goes, rather than to just kind of increasing our share of knowledge overall. You want to be focusing on decisions we need to make now that we would have wanted to make differently.</p><p>So it looks good to me, the focus on the long-term future. I want to be clear that I think it\u2019s not perfectly safe. I think a thing we just generally need to give up is the hope that we will have a theory that makes sense of everything \u2014 such that we know that we\u2019re acting in the safe way, that it\u2019s not going to go wrong, and it\u2019s not going to backfire. I think there can be a way that people look to philosophy as a kind of mode of Archimedean orientation towards the world \u2014 that will tell them how to live, and justify their actions, and give a kind of comfort and structure \u2014 that I think at some point we need to give up.</p></blockquote><h3><strong>On the classic drowning child thought experiment</strong></h3><blockquote><p><strong>Joe Carlsmith:</strong> I think what <a href=\"https://en.wikipedia.org/wiki/Famine,_Affluence,_and_Morality\">that</a> can do is sort of break your conception of yourself as a kind of morally sincere agent \u2014 and at a deeper level, it can break your conception of society and your peers, or society as a morally sincere endeavour, in some sense. Things can start to seem kind of sick at their core, and we\u2019re just all looking away from the sense in which we\u2019re horrible people, or something like that.</p><p>I actually think part of the attraction of communities like the effective altruism community, for many people, is it sort of offers a vision of a recovery of a certain moral sincerity. You find this community, and actually, these people are maybe trying \u2014 more so than you had encountered previously \u2014 to really take this stuff seriously, to act rightly by its lights. And I think that can be a powerful idea.</p><p>But there is this then this thing comes up, where it\u2019s like, \u201cOK, but how much is enough? Exactly how far do you go with this? What is demanded?\u201d I think people can end up in a mode where their relationship with this is what you said: it\u2019s about not being bad, not sucking \u2014 like you thought \u201cmaybe I sucked\u201d and now you\u2019re really trying not to suck \u2014 you don\u2019t want to be kind of punished or worthy of reproach. It\u2019s a lot about something like guilt. I think that the thought experiment itself is sort of about calling you an asshole. It\u2019s like, \u201cIf you didn\u2019t save the child, you\u2019re an asshole.\u201d So everyone\u2019s an asshole.</p><p><strong>Rob Wiblin:</strong> But look at how you\u2019re living the rest of your life.</p><p><strong>Joe Carlsmith:</strong> Exactly. I think sometimes you\u2019re an asshole, and we need to be able to notice that. But also, for one thing, it\u2019s actually not clear to me that you\u2019re an asshole for not donating to a charity \u2014 that\u2019s not something that we normally think \u2014 and I think we should notice that. Also, it doesn\u2019t seem to me like a very healthy or wholehearted basis for engaging with this stuff. I think there are alternatives that are better.</p></blockquote><h3><strong>On why bother being good</strong></h3><blockquote><p><strong>Rob Wiblin:</strong> What are the personal values of yours that motivate you to care to try to help other people, even when it\u2019s kind of a drag, or demoralising, or it feels like you\u2019re not making progress?</p><p><strong>Joe Carlsmith:</strong> One value that\u2019s important to me, though it\u2019s a little hard to communicate, is something like \u201clooking myself and the world in the eye.\u201d It\u2019s about kind of taking responsibility for what I\u2019m doing; what kind of force I\u2019m going to be in the world in different circumstances; trying to understand myself, understand the world, and understand what in fact I am in relationship to it \u2014 and to choose that and endorse that with a sense of agency and ownership.</p><p>One way that shows up for me in the context of helping others is trying to take really seriously that my mind is not the world \u2014 that the limits of my experience are not the limits of what\u2019s real.</p><p>In particular, I wake up and I\u2019m just like Joe every day \u2014 every day it\u2019s just Joe stuff; I wake up in the sphere of Joe around me. So Joe stuff is really salient and vivid: there\u2019s this sort of zone \u2014 it\u2019s not just my experience, there\u2019s also, like, people and my kitchen \u2014 of things that are kind of vivid.</p><p>And then there\u2019s a part of the world that my brain is doing a lot less to model \u2014 but that doesn\u2019t mean the thing is less real; it\u2019s just my brain is putting in a lot fewer resources to modelling it. So things like other people are just as real as I am. When something happens to me, at least from a certain perspective, that\u2019s not a fundamentally different type of event than when something happens to someone else. So part of living in the real world for me is living in light of that fact, and trying to really stay in connection with just that other people are just as real as I am.</p><p>More broadly, when we talk about forms of altruism that are more fully impartial \u2014 or trying to ask questions like, \u201cWhat is really the most good I can do?\u201d \u2014 for me, that\u2019s a lot about trying to live in the world as a whole, not artificially limiting which parts of the world I\u2019m treating as real or significant. Because I don\u2019t live in just one part of the world. When I act, I act in a way that affects the whole world, or that can affect the whole world. There\u2019s some sense in which I want to be not imposing some myopia upfront on what is in scope for me. I think those are both core for me in terms of what helping others is about.</p></blockquote>", "user": {"username": "80000_Hours"}}, {"_id": "8ymrq8FzAmzAWgxkS", "title": "X-risk discussion in a college commencement speech", "postedAt": "2023-05-22T11:01:11.507Z", "htmlBody": "<p>Yesterday, <a href=\"https://en.wikipedia.org/wiki/Juan_Manuel_Santos\">Juan Manuel Santos</a>, former president of Colombia (2010-2018) and 2016 Nobel Peace Prize winner gave the commencement address at the University of Notre Dame.</p><p>The address contained the usual graduation speech stuff about all the problems humanity is facing and how this special class of students is uniquely equipped to change the world or whatever.</p><p>While the gist of the message was pretty standard, I was pleasantly surprised that Santos spent a significant chunk of time talking about existential risk as the most pressing problem of our time (see clip <a href=\"https://www.youtube.com/live/f-qLnjdafCU?feature=share&amp;t=10385\">here</a>). Santos touched on the major x-risk factors well known to the EA community: AI, biosecurity, and nuclear weapons. However, he also emphasized climate change as one of the most pressing existential threats, which of course is a view that many EAs <a href=\"https://80000hours.org/problem-profiles/climate-change/\">do not share</a>.</p><p>On one hand, I think this speech should be seen as a sign of hope. Ideas on the importance of mitigating x-risk \u2014 and the particular threats of AI, pandemics, and nuclear war \u2014 seem to be entering more mainstream circles. This trend is evidenced further in a <a href=\"https://forum.effectivealtruism.org/posts/Konde3tJY2SFoFbpd/former-israeli-prime-minister-speaks-about-ai-x-risk\">recent post</a> noting another former world leader (Israeli PM Naftali Bennett) who publicly discussed AI x-risk. And I think a college commencement speech is arguably more mainstream than the Asian Leadership Conference at which Bennett delivered his talk.</p><p>On the other hand, it was clear that there is still a long way to go in terms of convincing most of the general population that x-risk should be taken seriously. Sitting in the audience, the people around me were smirking and giggling throughout the x-risk portion of Santos' speech. Afterward, I overheard people joking about the AI part and how they thought it was inappropriate to talk about such morbid material in a commencement address.</p><p>Overall, I certainly appreciated Santos for talking about x-risk, but I'm not convinced that his words had much of an impact on the people in the audience. To be sure, I realize that commencement speeches are largely ceremonial and all but a <a href=\"https://www.cnn.com/2021/05/13/world/most-memorable-commencement-speeches-spc-intl/index.html\">handful</a> don't have any broader societal impact. Still, it would have been nice to see people be more receptive to Santos' important ideas.</p><p>I would be interested to hear if anyone has any thoughts on Santos' discussion of x-risk. Was it appropriate to talk about this stuff in the context of a commencement address? Is this an effective forum to spread ideas about x-risk (or other \"weird\" EA ideas), or will these ideas just fall on deaf ears? Or are commencement addresses mostly irrelevant and not worth even thinking about for the purposes of growing EA and promoting some of the more idiosyncratic concepts?</p>", "user": {"username": "SWK"}}, {"_id": "gSGhrCXdntxLrMAmJ", "title": "AI strategy career pipeline", "postedAt": "2023-05-22T00:00:37.737Z", "htmlBody": "<p>The pipeline for (x-risk-focused) AI strategy/governance/forecasting careers has never been strong, especially for new researchers. But it feels particularly weak recently (e.g. no summer research programs this year from Rethink Priorities, SERI SRF, or AI Impacts, at least as of now, and as few job openings as ever). (Also no governance course from AGI Safety Fundamentals in a while and no governance-focused programs elsewhere.)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy9xd1vqjpej\"><sup><a href=\"#fny9xd1vqjpej\">[1]</a></sup></span>&nbsp;We're presumably missing out on a lot of talent.</p><p>I'm not sure what the solution is, or even what the problem is-- I think it's somewhat about funding and somewhat about mentorship and mostly about [orgs not prioritizing boosting early-career folks and not supporting them for various idiosyncratic reasons] + [the community being insufficiently coordinated to realize that it's dropping the ball and it's nobody's job to notice and nobody has great solutions anyway].</p><p>If you have information or takes, I'd be excited to learn. If you've been looking for early-career support (an educational program, way to test fit, way to gain experience, summer program, first job in AI strategy/governance/forecasting, etc.), I'd be really excited to hear your perspective (feel free to PM).</p><hr><p>(In AI alignment, I think SERI MATS has improved the early-career pipeline dramatically-- kudos to them. Maybe I should ask them why they haven't expanded to AI strategy or if they have takes on that pipeline. For now, maybe they're evidence that someone prioritizing pipeline-improving is necessary for it to happen...)</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fny9xd1vqjpej\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefy9xd1vqjpej\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Added on May 24: the comments naturally focused on these examples, but I wasn't asserting that summer research programs or courses are the most important bottlenecks-- they just were salient to me recently.</p></div></li></ol>", "user": {"username": "zsp"}}, {"_id": "FKeESwh3hu8Khbsco", "title": "Weight by Impact", "postedAt": "2023-05-21T14:37:59.179Z", "htmlBody": "", "user": {"username": "vaniver"}}, {"_id": "et4DjzJKNw5Jm4BGM", "title": "Maybe you should quit your job", "postedAt": "2023-05-22T05:54:28.260Z", "htmlBody": "<p><strong>More specifically</strong>: If you are not clearly happy with your life, you should be able to at least consider the hypothesis of quitting your job. By \"considering the hypothesis\" I mean being able to think about it for more than five seconds without the feeling that the conclusion is already fixed and or without a feeling of dread, aversion, \"I can't\", \"no way\" etc. It might in fact be that all facts speak against quitting but it's important to be able to look reality in the face.</p><p>I currently have the weakly held hypothesis that people don't quit their job (early) enough, at least when they work at an org for impact reasons and when they are not clearly happy with their life. (I would write unhappy but when I was clearly unhappy for months-years, I would have protested and said I'm not unhappy.)</p><p><strong>Epistemic status:</strong> Speculation/Intuition; hastily written</p><p><i><strong>Meta on anonymity: </strong>Disgruntledly decided to publish anonymously. I think there's a lot of value to having EA forum posts, including this type, published under one's clear name and am open towards bids to move this to my main account. Obviously don't just de-anonymise me.</i></p><h2><strong>Reasons I have this intuition:</strong></h2><p>I think this because of personal experience, anecdotes, reading this <a href=\"https://80000hours.org/2018/08/randomised-experiment-if-youre-really-unsure-whether-to-quit-your-job-or-break-up-you-really-probably-should/\">80,000 Hours</a> article, and theoretical argument/intuition. I haven't polled people or looked into the literature myself though.</p><p><strong>Some theoretical arguments:</strong></p><ul><li>I have the same intuition about break-ups and the cases seem similar,</li><li>There are many reasons people might find quitting scary (financial security, uncertainty about the future, social discomfort and feelings of loyalty, you might have to admit to yourself that you've made mistakes or wasted your time in the past, there was a reason you took the job in the first place and it's hard to admit that something isn't working when it initially looked promising), so it's hard to think about,</li><li>I have the intuition that people in EA tend to not value their own welfare enough (even just instrumentally),</li><li>It's awkward to tell others that you think they should quit their job,</li><li><a href=\"https://en.wikipedia.org/wiki/Status_quo_bias\">Status quo bias</a>.</li><li>I was extremely surprised by the questions in this burn-out questionnaire (<a href=\"https://www.mindtools.com/auhx7b3/burnout-self-test\">first link on google</a>) and feel like many of my friends would score relatively high on them. I might write a post titled something \"All my friends are burning out and they don't know it\" in the future.</li><li>We see the upside of continuing our job very clearly but it's harder to see the opportunity cost of not quitting, i.e. the upside of potentially finding something else. I think you should take that particularly serious if you feel unproductive or feel like you could potentially be capable of <i>much more</i> than you currently do. In those cases, the downside and upside of quitting are lob-sided and quitting usually increases variance. If you want to kick ass and think you can but aren't in your current job for reasons that seem unlikely to change, maybe quit even if you're not sure how likely it is you'd kick ass if you quit. Being unhappy also might make you underestimate how well you could do in another job.</li><li>Sometimes it's not obvious to people that they are/were unhappy until they're no longer unhappy.</li></ul><p>I think for many people, thinking about quitting is at least somewhat painful. For some people, it's very painful. If you are one of those people and you're thinking about quitting, that's some evidence that there's really <i>a lot </i>of emotional pressure inside of you to quit because it managed to break through the aversion. That seems worth taking seriously.</p><p>I also think continuing a job you're unhappy in is really costly: My guess is that it increases the chance that you'll need a long break from work and that it also increases the length of that future break.</p><h2><strong>Who does this advice apply to?</strong></h2><p>As always, some people probably quit too often or too easily and need to hear the opposite.</p><p>Regardless, if you do decide to quit your job, make sure to try to communicate well with your team and to make it as uncostly as reasonable for them.</p><p><strong>Here is the type of person I have the quitting intuition for and who this post is targeted towards. Note this is heavily driven by intuition and anecdotes and many criteria are probably neither sufficient nor necessary:</strong></p><ul><li>Anybody who finds it (nearly) impossible to even entertain the possibility of quitting.</li><li>Generally struggles with their mental health and productivity, not very happy (note though that for some people, quitting their job might mean losing a lot of social contact and structure, leading them into a black hole. I am optimistic that many people will still be better off in the medium-term but clearly not all.)</li><li>Really impact-driven and feel like they have an obligation to continue with their job. The thought of quitting generally fills them with guilt or dread. Alternatively, really worried about what other people think about them.</li><li>Tendency to have some idea of what an ideal agent would do in their situation and try to act according to that. Difficulty facing their own (e.g. psychological) constraints and needs, especially where they are in the way of \"doing what's good\".<ul><li>Some indication that the above applies: Regularly finding themselves thinking that a task they are struggling with should be \"easy\" (no really! This should be really easy. I literally just have to do this thing that should only take 5 minutes.)</li></ul></li><li>Have been unhappy for a few months.</li><li>Tend to not trust their own abilities including their ability to be happy.</li><li>Thinking that always/most of the time feeling strong aversion or anxiety or related things when thinking about work is normal.</li><li>You've never filled in an online burn-out questionnaire (<a href=\"https://www.mindtools.com/auhx7b3/burnout-self-test\">first link on google</a>), do it now, and have a high score.</li><li>It's your first job. (I think there are considerations pushing in both directions but I expect people on average to be more biased towards staying if it's their first job.)</li></ul><p><strong>Here is the type of person I don't have the quitting intuition for although I'm also not making opposite claims:</strong></p><ul><li>People who are generally happy and or not at all thinking about quitting even though they have sat down to (or are very confident they could) think about the reasons for and against quitting for 20 minutes.</li><li>Generally doing all right but always wondering whether there is something even higher impact they could do.</li><li>Probably many other types of people.</li><li>You have a lot of work experience to compare to and your current job doesn't seem uniquely bad.</li></ul><p><strong>Here is the type of person that I have the intuition should be really careful with quitting:</strong></p><ul><li>You have no financial security, a weak social network/other things that can buffer your mental health, are prone to serious addiction (or some other mental health problem that it's hard to recover from within ~1-6 months) that could be triggered by quitting, including extremely low trust in your ability to do other things - low enough that you expect yourself to not apply to anything, work on self-development, or try anything new within 1-12 months after quitting.</li><li>You haven't taken at least a few weeks to think about this, especially if you have just been in an intense environment (e.g. a retreat) or have only talked to very few people about this&nbsp;</li></ul><p><strong>Even more speculative stuff in this footnote.</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbt3wiijuz9p\"><sup><a href=\"#fnbt3wiijuz9p\">[1]</a></sup></span></p><h2><strong>My personal story:</strong></h2><ul><li>I started my first job a couple of years ago. I loved it and I also hated it. I struggled with mental health and productivity and went through many cycles of getting better and getting worse. (Although post-quitting, I think I actually achieved more than it felt like at the time and am proud of what I did.) At any point in time it felt either like \"Oh, I had this rough time but now I'm on the way up\" or \"Oh, I'm having this exceptionally rough time right now because of these specific reasons.\" It always felt like I just had to push on and great things were gonna come. I also wouldn't have applied the label \"unhappy\" to myself during the time.</li><li>Others sometimes brought up the idea to take a break or explore other jobs but I always blocked those suggestions.</li><li>I took me over a year to see that things were, on the whole, not improving.</li><li>I liked (and still like) my manager and my team, so I assumed it must be a me-problem. Especially because it wasn't obvious to me which factors of my work I would actually like to be different.<ul><li>I think I had a mix of not being able to entertain the hypothesis that maybe it just wasn't working out even though nobody did anything wrong <i>and</i> not being able to entertain the hypothesis that others were (not) doing things that made the workplace unideal for me.</li></ul></li><li>I felt a really strong sense of loyalty towards my team.</li><li>Despite often having read advice about how you should have a plan B or plan Z that you can retreat to if things aren't going well at work, I thought I was exceptional in that I, and only I specifically, really didn't have any other alternatives than my then-current job if I wanted to work on the things I cared about.</li><li>As time went on, I lost a lot of confidence in my own abilities, which amplified the sense of having no alternatives.</li><li>I eventually decided to take a long break from work. (I was still employed at my old organisation and intended to come back.) This might not be possible for people who don't work at an organisation that's as supportive as mine then.</li><li>Even 3-4 months into the break, I couldn't entertain the possibility of quitting. Thinking about it felt taboo. When others asked me I would half-heartedly say that quitting was a possibility but unlikely. I gave it at most 5-10 second of thought or half-assed argument in conversation. It never felt like I really gave myself the chance to come to the conclusion \"Yes, I should quit.\"</li><li>As soon as I was able to entertain the possibility seriously (prompted by my partner making me list things I could do instead of my then-job), it was almost immediately obvious that I wanted to quit.</li><li>Quitting felt awesome. Months later, I'm still really happy about it. I think I also understand how to work much better now. I think it's important to say that I have a strong social network, have enough runway, and enjoy self-improvement, introspection etc. At this point, I had also recovered enough of my self-confidence to believe in my ability to find something else. This was perhaps easier because I had the luxury of a long break with the knowledge that I had a job I could always come back to. I also have a fairly clear vision of what I want to do now, which might be a bit unusual.</li></ul><h2><strong>Poll</strong></h2><p>I feel a bit irresponsible writing this very quick post without much/any research on something that could affect people's life in important ways, so I'll leave a poll in the comment section for people's experiences with (not) quitting. It's a bit hard to come up with a good version of the poll because it's hard to catch people who decided not to quit. Let me know if you have suggestions for what to poll.</p><h2>Contact me</h2><p>Feel free to dm me if this resonated and you want to talk to someone or just get a weight of your shoulders. I might not be the fastest on the replies though or decide to only offer text responses.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbt3wiijuz9p\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbt3wiijuz9p\">^</a></strong></sup></span><ul><li>I think you can get a feeling for the direction in which you're biased.</li><li>You can ask yourself whether you feel more dread from thinking about all the great opportunities you're missing by staying at your job (FOMO) or from thinking about quitting.</li><li>You might be more biased towards leaving if you have more FOMO and more bias towards staying if you dread quitting more.</li><li>That said, your emotions are an important piece of information and might also just reflect reality. Weighing this part takes a lot of self-knowledge.</li><li>I think the most important part is figuring out whether there are certain things you're not capable of thinking about.</li></ul></li></ol>", "user": {"username": "Persephone"}}, {"_id": "rtnWMb9ewbNnhhFPd", "title": "Announcing the Prague community space: Fixed Point", "postedAt": "2023-05-22T05:52:51.236Z", "htmlBody": "<h2>Summary</h2><p>A coworking and event space in Prague is open for the entirety of 2023, offering up to 50 desk spaces in a coworking office, a large event space for up to 60 people, multiple meeting rooms and other amenities. We are currently in the process of transitioning from an initial subsidized model to a more sustainable paid membership model.</p><p>In 2022,&nbsp;<a href=\"https://fixedpoint.house/\"><u>Fixed Point</u></a> was the home of the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ZxHnaPmxmBrZmerBj/announcing-the-prague-fall-season\"><u>Prague Fall Season</u></a> which will be returning there <a href=\"https://forum.effectivealtruism.org/posts/DPHu3p3LwTFBxB3ed/announcing-the-prague-fall-season-2023-and-the-epistea\">in 2023</a>.&nbsp;</p><p>We are seeking people, projects and hosts of events here in 2023. If you are interested you can&nbsp;<a href=\"https://form.jotform.com/230322573808050\"><u>apply here</u></a>.</p><h2>What is Fixed Point?</h2><figure class=\"image image_resized\" style=\"width:45.75%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/hoamfdoxhvdlsmweyvcz\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/zsi9fc81wfrp9b4w79pv 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/aqks7vhlygn6ut9opwvw 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/sh2sq3aecplml4oxb7wz 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/all8swnyoinbvreqt6oz 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/jmomiqmkolst7xrlghjl 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/n7ejdzfp7phyuaxq8gnf 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/ewijxtbpyls0m0l71o73 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/j2bnhz3trihnj55ynccg 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/frep9ravnr74h8sd805n 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/g5hcvmbybhxr9izndu4d 1095w\"><figcaption>&nbsp;</figcaption></figure><p>Fixed Point is a unique community and coworking space located in the heart of Prague operated <a href=\"https://forum.effectivealtruism.org/posts/FrshKTu34cFGGsyka/announcing-a-new-organization-epistea\">by Epistea</a>.&nbsp;<strong>We support organizations and individuals working on existential security, epistemics, rationality, and effective altruism.</strong> Across five floors there is a variety of coworking offices offering&nbsp;<strong>up to 50 workstations</strong>, as well as numerous meeting rooms and private call stations. In addition to functional work areas, there are inviting communal spaces such as a large comfortable common room accommodating up to 60 people, two fully equipped large kitchens, and a spacious dining area. These amenities create a welcoming environment that encourages social interaction and facilitates spontaneous exchange of ideas. Additionally, there are on-site amenities like a small gym, a nap room, two laundry rooms, bathrooms with showers, and a garden with outdoor tables and seating. For those in need of short-term accommodation, our on-site guesthouse has a capacity of up to 10 beds.&nbsp;</p><p>Fixed Point is a space where brilliant and highly engaged individuals make crucial career decisions, establish significant relationships, and find opportunities for introspection among like-minded peers when they need it most. In 2022, Fixed Point was home to the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ZxHnaPmxmBrZmerBj/announcing-the-prague-fall-season\"><u>Prague Fall Season</u></a>, when 350 people visited the space.&nbsp;</p><p>The name \"Fixed Point\" draws inspiration from the prevalence of various Fixed Point theorems in almost all areas people working in the space work on. If you study the areas seriously, you will find fixed points sooner or later.</p><figure class=\"image image_resized\" style=\"width:70.87%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/kph9upyz683otwmn6uot\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/vaf7gugudskudcrrctiv 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/pziyu5rgsops3zr9syxy 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/kgszrerpeybkyyygqjvu 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/ecchd6galgdrv4edylq8 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/jh0vasgtmkfl3uksdesi 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/abay5xx3hgsqrlqe6h6t 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/rr5ir2uggmhtykjiio29 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/egxwuw3r6d7otmrualu3 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/hoxyfgfyg7hrs5ixerfg 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/tkhszsqfvoc1ltqg3nkw 1600w\"><figcaption>&nbsp;</figcaption></figure><h2>Why Prague?</h2><p>The Czech effective altruism and rationalist community has long been committed to operational excellence and the creation of physical spaces that facilitate collaboration. With numerous successfully incubated organizations and passionate individuals making a difference in high-impact domains,&nbsp;<strong>Prague is now a viable option, especially for EU citizens wanting to settle in continental Europe</strong>.&nbsp;</p><p>In addition to the Prague Fall Season, Prague is home to many different projects, such as&nbsp;<a href=\"https://acsresearch.org\"><u>Alignment of Complex Systems Research Group</u></a>,&nbsp;<a href=\"https://espr.camp\"><u>ESPR</u></a> or&nbsp;<a href=\"https://www.ceskepriority.cz/en\"><u>Czech Priorities</u></a>. We host the European runs of&nbsp;<a href=\"https://www.rationality.org\"><u>CFAR</u></a> workshops and CFAR rEUnions.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/kd98gmiddpvj03abjurb\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/pyztmrpekpnwyawq4oxh 114w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/f8xqs7aooyja1rasckfc 194w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/zelpekbqmq5fcvedf8xy 274w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/twevnxtwxm6pecxznz7m 354w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/lwhesigqikjrouqhurk0 434w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/g6mrizhhyvv5iewhosvc 514w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/ydgl0birpkngig6c7cy9 594w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/qlj3el062o1ttcj7ztem 674w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/aobxltso1tixk4pdkxay 754w\"></p><h2>Whom is it for?&nbsp;</h2><p>We extend a warm welcome to both short and long-term visitors working on meaningful projects in the areas of&nbsp;<strong>existential risk mitigation, AI safety, rationality, epistemics, and effective altruism</strong>. We are particularly excited to accommodate individuals and teams in the following categories:</p><ul><li>Those interested in hosting events,&nbsp;</li><li>Teams seeking a workspace for an extended period of time.</li></ul><p>Here are a few examples of the projects we are equipped to host and are enthusiastic about:</p><ul><li>Weekend hackathons,</li><li>Incubators lasting up to several months,</li><li>Conferences,</li><li>Workshops and lectures on relevant topics,&nbsp;</li><li>Providing office spaces for existing projects.<br>&nbsp;</li></ul><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/hyyjyaewq81yfmomcujv\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/tiyprerrbftyqfoeulqe 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/g8wryyo7rpoo1qs9af0d 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/wjteccydsadni831adkj 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/qdgpmq3hsxsqi3yytejw 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/ix6whurxjwdwugqtj86g 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/yke4partxxtdkyrkfwl8 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/e8djogtda4u8fzok5zz5 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/khilyrkwywxrqdqxuhnq 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/dfiw2vdsh7kly0e09h8j 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rtnWMb9ewbNnhhFPd/ccxkxxprcjxpc1vaqgrx 1600w\"></p><h2>Additional support</h2><p>In addition to the amenities, we can also offer the following services upon request:</p><ul><li>Project management for existing initiatives,</li><li>Catering services for events,</li><li>Administrative and operations support</li><li>Accommodation arrangements,</li><li>Event logistics and operations assistance,</li><li>Event design consulting.</li></ul><p>Feel free to reach out to us with any specific requirements or inquiries. We are dedicated to providing comprehensive support to ensure the success of your activities.</p><h2>Joining our community</h2><p>We are currently in the process of transitioning from an early-stage subsidized model to a more sustainable long-term coworking space management. Our current model is a&nbsp;<strong>dedicated desk membership</strong>. This membership grants 24/7 access to your&nbsp;<strong>personal workspace, our in-house facilities, lunches, fully stacked snack bar and beverages, and the opportunity to reserve meeting rooms and call booths.&nbsp;</strong>Our staff is available to provide technical support, facility-related assistance, and guidance to optimize productivity. Based on our monthly expenses associated with running the house the membership is priced at 880 EUR per month (prorated for shorter stays). We also offer discounted rates depending on your financial circumstances.&nbsp;</p><p>We understand the diverse needs and resources of our community and we are open to offering discounts on a case by case basis. In general, we also offer financial aid to students and those with less stable income, or irregular visitors, who will only be around a few days each month. If your primary organization is in a position to cover your stay at Fixed Point, we can invoice them directly.</p><p>To learn more about our membership options or to inquire about financial assistance, please connect with us via the&nbsp;<a href=\"https://form.jotform.com/230322573808050\"><u>application form</u></a>.</p><h2>Support us</h2><p><strong>We are currently looking for long-term funding to support this coworking.</strong> Even as we transition into a paid membership model, we want to be able to offer financial aid and continually improve and invest in the space. If you are interested in providing funding for us, please reach out at info@fixedpoint.house.&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "Epistea"}}, {"_id": "dofsH7aNzqFzQmGia", "title": "respeggt's Carmen Uphoff on innovative technology to end chicken culling", "postedAt": "2023-05-20T22:16:04.758Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=7M1HzHkr0f4\"><div><iframe src=\"https://www.youtube.com/embed/7M1HzHkr0f4\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p>We discuss the respeggt's ground-breaking technology, why it is likely to significantly reduce the need for culling male chicks in the egg industry, what the major challenges in the way are in terms of regulations and costs, as well as other valuable insights pertaining to the sex detection of eggs landscape. For that, we are joined by Carmen Uphoff who works as the Chief Operating Officer of respeggt where she oversees a company that has delivered over a billion respeggt eggs and ten million respeggt egg laying hens to supply chains. We are incredibly fortunate to have had the opportunity to talk with her!</p>", "user": {"username": "Karthik Palakodeti"}}, {"_id": "jpsugrAbjsgfm9gZM", "title": "EAG talks are underrated IMO", "postedAt": "2023-05-20T17:06:19.582Z", "htmlBody": "<p>Underrated is relative.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwyqoc9he63\"><sup><a href=\"#fnwyqoc9he63\">[1]</a></sup></span>&nbsp;My position is something like \"most people should consider going to &gt;1 EAG talk\" and not \"most people should spend most of their EAG in talks.\" This probably most applies to people who are kind of like me. (Been involved for a while, already have a strong network, don't need to do 1-1s for their job.)</p><p>There's a meme that 1-1s are clearly the most valuable part of EAG(x) and that you should not really go to talks. (See e.g. <a href=\"https://www.youtube.com/watch?v=2nwp_E3ouSs\">this, </a><a href=\"https://forum.effectivealtruism.org/posts/5hKDjrGocGcreH3DC/how-to-get-the-maximum-value-out-of-effective-altruism#Introduction\">this</a>, <a href=\"https://forum.effectivealtruism.org/posts/pKbTjdopzSEApSQfc/doing-1-on-1s-better-eag-tips-part-ii\">this</a>, they don't say exactly this but I think push in the direction of the meme.)</p><p>I think EAG talks can be really interesting and are underrated. It's true that most of them are recorded and you could watch them later but I'm guessing most people don't actually do that.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcx3me1s6woo\"><sup><a href=\"#fncx3me1s6woo\">[2]</a></sup></span>&nbsp;It also takes a while for them to be uploaded.</p><p>I still think 1-1s are pretty great, especially if you're</p><ul><li>new and don't know many people yet (or otherwise mostly want to increase the number of people you know),</li><li>have a very specific thing you're trying to get out of EAG and talking to lots of people seems to be the right thing to achieve it.</li></ul><p>I'm mostly writing this post because I think the meme is really strong in some parts of the EA community. I can imagine that some people in the EA community would feel bad for attending talks because it doesn't feel \"optimal.\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcx3me1s6woo\"><sup><a href=\"#fncx3me1s6woo\">[2]</a></sup></span>&nbsp;If you feel like you need permission, I want to give you permission to go to talks without feeling bad. Another motivation is that I recently attended my first set of EAG talks in years (I was doing lots of 1-1s for my job before) and was really surprised by how great they were. &nbsp;(That said, it was a bit hit or miss.) I previously accidentally assumed that talks and other prepared sessions would give me ~nothing.</p><p>&nbsp;</p><p>edit: Someone mentioned in person that they think EAG talks just got much better recently, so I just started going to them at a lucky time. So, if you've gone in the past and were disappointed, now is maybe a good time to try again.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwyqoc9he63\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwyqoc9he63\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See also the rule of equal and opposite advice (<a href=\"https://www.lesswrong.com/posts/pm6pALxgQFx45dBJq/rule-of-equal-and-opposite-advice-and-slack\">1</a>, <a href=\"https://slatestarcodex.com/2014/03/24/should-you-reverse-any-advice-you-hear/\">2</a>) although I haven't actually read the posts I linked.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fncx3me1s6woo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefcx3me1s6woo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>My best guess is that people in EA are more biased towards taking actions that are part of a collectively \"optimal\" plan for [generic human with willpower and without any other properties] than taking actions that are good given realistic counterfactuals.</p></div></li></ol>", "user": {"username": "Chi"}}, {"_id": "Konde3tJY2SFoFbpd", "title": "Former Israeli Prime Minister Speaks About AI X-Risk", "postedAt": "2023-05-20T12:09:34.895Z", "htmlBody": "<h1>Watch here:</h1><p><a href=\"https://fb.watch/kDg1KTFiW7/\">&nbsp;https://fb.watch/kDg1KTFiW7/</a> (it's in English)<br><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Konde3tJY2SFoFbpd/ptgjekbevamblh3cdql7\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Konde3tJY2SFoFbpd/uorttmxveed2zzjibxpa 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Konde3tJY2SFoFbpd/ncxetq0ekb1plwiygiuu 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Konde3tJY2SFoFbpd/zzpaczxpgvltfeioog1u 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Konde3tJY2SFoFbpd/liai407xeomjcojwjihg 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Konde3tJY2SFoFbpd/ewwd38e8w5lb0dyzsbf6 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Konde3tJY2SFoFbpd/xit1wcbvu7zjk7wusezj 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Konde3tJY2SFoFbpd/xhph2cf9w9a2algvttd9 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Konde3tJY2SFoFbpd/lghhoon1eejd1tltjhqa 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Konde3tJY2SFoFbpd/yrapwg248di1rxyajchg 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Konde3tJY2SFoFbpd/hp6rh3gvbob3tfddrhsr 1598w\"></p><h1>From the video:</h1><blockquote><p>just like nuclear tech is an amazing invention for humanity but can also risk the destruction of humanity, AI is the same. The world needs to get together NOW, to form the equivalent of the IAEA [...]</p></blockquote><h1>Who is this?</h1><p>This is Naftali Bennett (<a href=\"https://en.wikipedia.org/wiki/Naftali_Bennett\">wikipedia</a>), an Israeli politician who was prime minister between June 2021 and June 2022.</p>", "user": {"username": "hibukki"}}, {"_id": "ikTpmYqA4CrzFyzxb", "title": "Rebooting AI Governance: An AI-Driven Approach to AI Governance", "postedAt": "2023-05-20T19:06:49.889Z", "htmlBody": "<p>This article is part of a series on Decision-Making under Deep Uncertainty (DMDU). Building &nbsp;on an introduction to complexity modeling and DMDU, this article focuses on the application of this methodology to the field of AI governance. If you haven't read the <a href=\"https://forum.effectivealtruism.org/posts/kCBQHWqbk4Nrns8P7/model-based-policy-analysis-under-deep-uncertainty\">previous article</a>, I strongly recommend to pause reading <i>this</i> article, and go through the introduction first. Many aspects will only appear clear if we share the same basic terminology. &nbsp;I might be biased, but I think, it's worth it.</p><h1>Why this article?</h1><p>I have written up this article because of two reasons. The <i>1st reason</i> is that various people have approached me, asking how I envision the application of the introduced methodology to particular EA cause areas. People want to see details. And understandably so. In my last article, I have mentioned a bunch of EA cause areas, that I consider to be well suited for systems modeling and decision-making under deep uncertainty (DMDU)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkvd4gta61v9\"><sup><a href=\"#fnkvd4gta61v9\">[1]</a></sup></span>. This article is intended to be a (partial) answer for these people providing details on one particular cause area. The <i>2nd reason</i> for writing this article is my own specialization in AI governance and my wish for people to get started working on this. I want to show a more practical and concrete side of this methodology. I also consider this article as a handy reference that I can share with interested parties as a form of coarse research proposal.&nbsp;</p><h1>Summary</h1><ul><li><strong>DMDU methodology is well-suited to address the complexity of AI governance</strong> for the four reasons:<ol><li>AI governance demarcates as<strong> complex socio-technical system</strong> which involves the interaction of technology and society.</li><li>DMDU methodology values the <strong>diversity of stakeholder perspectives</strong> in AI governance and encourages decision-making that incorporates various values and objectives.</li><li>AI governance is characterized by <strong>deep uncertainty</strong> due to the unpredictability of AI advancements, societal reactions, and potential risks. DMDU methodology embraces deep uncertainty by exploring a wide range of plausible futures and developing flexible strategies that can adapt to changing circumstances.</li><li><strong>Robustness</strong> is crucial in AI governance, and DMDU supports the development of policies that can withstand disruptions, adapt to changes, and consider diverse stakeholder perspectives.</li></ol></li><li><strong>Traditional methods</strong> (standard predictive modeling + highly aggregated economic models) are ill-equipped to deal with the four previous points.</li><li>To <strong>maximize value from modeling</strong> in the first place, complexity modeling and DMDU are necessary.</li><li><strong>Three steps</strong> are proposed for applying complexity modeling and DMDU to AI governance:<ul><li><strong>model building</strong> (e.g. an agent-based model)</li><li>embedding the model in an <strong>optimization</strong> setup&nbsp;</li><li>conducting <strong>exploratory analysis</strong></li></ul></li><li><strong>Examples</strong> are teased to show how the steps could look like in practice including potential<ul><li>exogenous uncertainties</li><li>policy levers</li><li>relations</li><li>performance metrics</li></ul></li><li><strong>Deliverables</strong> of a DMDU process could include:<ul><li><strong>Insights</strong><ul><li>System dynamics</li><li>Key drivers of outcomes</li><li>Potential vulnerabilities</li></ul></li><li><strong>Policy options and trade-offs</strong></li><li><strong>Tools</strong> for ongoing decision-making<ul><li>Policy evaluation and 'What-if' Analysis</li><li>Adaptive Strategy Development</li></ul></li></ul></li></ul><h1>Basics of DMDU</h1><p>If it has been some time since you have read the previous article, or you are already somewhat familiar with systems modeling and/or DMDU, here is a short summary of the previous article:</p><ul><li>Real-world political decision-making problems are complex, with disputed knowledge, differing problem perceptions, opposing stakeholders, and interactions between framing the problem and problem-solving.&nbsp;</li><li>Modeling can help policy-makers to navigate these complexities.&nbsp;</li><li>Traditional modeling is ill-suited for this purpose.</li><li>Systems modeling is a better fit (e.g., agent-based models).</li><li>Deep uncertainty is more common than one might think.</li><li>Deep uncertainty makes expected-utility reasoning virtually useless.</li><li>Decision-Making under Deep Uncertainty is a framework that can build upon systems modeling and overcome deep uncertainties.</li><li>Explorative modeling &gt; predictive modeling.</li><li>Value diversity (aka multiple objectives) &gt; single objectives.</li><li>Focus on finding vulnerable scenarios and robust policy solutions.</li><li>Good fit with the mitigation of GCRs, X-risks, and S-risks.</li></ul><p>Now that we are all caught up, let's jump right into it.</p><h1>Why applying DMDU to AI governance?</h1><p>As most people that are familiar with Effective Altruism, are also familiar with the arguments why mitigating AI risk deserves particular attention, we will skip listing all the arguments showing how advanced AI systems could pose great risks to society now and in the future. When I speak of mitigating risks from AI, I refer to transformative AI (TAI), artificial general intelligence (AGI), and artificial superintelligence (ASI). At this point, I assume that the reader is on board and we agree on AI governance being an important cause area. But now, let's untangle the intellectual ball of yarn, weaving together the threads of DMDU (Deep Uncertainty Decision Making) and AI Governance in a way that illuminates its profound necessity. Let's break it down into four points.&nbsp;</p><h2>1. Dealing with a Socio-Technical System</h2><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/ky9ih6yw0beczsch83dl\" alt=\"This picture was generated with Midjourney.\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/phjdxzezj2ebzjancxkt 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/p0nwee0whpg58y4krfo0 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/naxtkma9zzwc7od06fyj 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/dzoxh3u5x98bkqft6enr 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/z3g4vv0vc32hykrtnfte 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/ayqfso4s2aakavfogdeb 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/uzbw7zeawijkcen5rinv 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/oz06sbao014urkcexk6p 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/ubw7n8uhwfcpyszitm2q 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/mubjgm9vangzj4qciwqy 1456w\"></figure><p>AI governance, at its heart, is a socio-technical system. This signifies that it's not merely about the development and regulation of technological artifacts; it's a nuanced interface where technology and society meet, intertwine, and co-evolve. It involves the navigation of societal norms and values, human behaviors and decisions, regulatory and policy environments, and the whirlwind of technological advancements. This results in a system that's akin to a vast, interconnected network, buzzing with activity and interplay among numerous components.</p><p>Each of these components brings its own layer of complexity. For instance, the human element introduces factors like cognitive biases, ethical considerations, and unpredictable behaviors, making it challenging to model and predict outcomes. Technological advancements, meanwhile, have their own dynamism, often progressing at a pace that leaves legal and regulatory frameworks struggling to catch up. These complexities aren't simply additive; they interact in ways that amplify the overall uncertainty and dynamism of the system.</p><p>DMDU is particularly equipped to grapple with this complexity. It recognizes that simplistic, deterministic models cannot accurately capture the dynamism of socio-technical systems. DMDU instead encourages a shift in mindset, embracing uncertainty and complexity instead of trying to eliminate it. By focusing on a wide array of plausible futures, it acknowledges that there isn't a single \"most likely\" future to plan for, but rather a range of possibilities that we must prepare for.</p><p>Moreover, DMDU embraces the systems thinking paradigm, understanding that the system's components are not isolated but interconnected in intricate ways. It acknowledges the cascading effects a change in one part of the system could have on others. This holistic view aligns perfectly with the complexity of AI governance, allowing for the development of more comprehensive and nuanced strategies.</p><p>Finally, DMDU's iterative nature also mirrors the dynamic nature of socio-technical systems. It recognizes that decision-making isn't a one-time event but a continuous process. As new information becomes available and the system evolves, strategies are revisited and adjusted, promoting a cycle of learning and adaptation.</p><p>In essence, DMDU is like a skilled navigator for the labyrinthine realm of AI governance, providing us with the tools and mindset to delve into its complexity, understand its intricacies, and navigate its uncertainties. It encourages us to look at the socio-technical system of AI governance not as an intimidating tangle, but as a fascinating puzzle, rich with challenges and ripe with opportunities for effective, inclusive, and adaptable governance strategies.</p><h2>2. Valuing the Multiplicity of Stakeholder Perspectives</h2><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/sgqmippdz3cqscunnutq\" alt=\"This picture was generated with Midjourney.\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/mrbwklvvajmemrfhmmoh 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/ww5td9siz7ed7jgspmxl 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/odlqmwjkawddowpeyhla 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/uv6qjrfctubuv2yassva 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/g4ffupnby5dtk1q8gpwi 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/je9ej5ncm3bfvaiqq69k 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/us8yb6xoclx4hytnnf3c 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/ur8nsmv0oedsmfswr9mw 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/r5ic8jkaeteebzznaypg 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/ybmsvtz8rrgzbqy5s0tm 1568w\"></figure><p>In the sphere of AI governance, we encounter an expansive array of stakeholders. Each brings to the table their own set of values, perspectives, and objectives, much like individual musicians in an orchestra, each playing their own instrument yet contributing to a harmonious symphony. These stakeholders range from government bodies and policy-makers, private corporations and AI developers, to individual users and civil society organizations. Each group's interests, concerns, and visions for AI development and regulation differ, contributing to a rich tapestry of diverse values.</p><p>For instance, a tech company might prioritize innovation and profitability, while a government agency could be more concerned with maintaining security, privacy, and the public good. Meanwhile, civil society organizations might stress human rights and social justice issues related to AI. Individual users, on the other hand, could have myriad concerns, from ease of use and affordability to ethical considerations and privacy protections. This diversity of values is not a complication to be minimized but a resource to be embraced. It brings to light a wider range of considerations and potential outcomes that may otherwise be overlooked. However, navigating this diversity and reaching decisions that respect and incorporate this plethora of perspectives is no small feat.</p><p>This is where DMDU plays an instrumental role. It offers a methodology designed to address situations with multiple objectives and conflicting interests. DMDU encourages the consideration of a broad spectrum of perspectives and criteria in decision-making processes, thereby ensuring that various stakeholder values are included and assessed. One way it does this is through scenario planning, which allows exploration of a variety of future states, each influenced by different stakeholder values and objectives. This exploration enables decision-makers to assess how different courses of action could impact diverse interests and fosters more comprehensive, balanced policy outcomes.</p><p>Furthermore, DMDU promotes an iterative decision-making process. This is especially pertinent in the context of value diversity as it provides opportunities to reassess and refine decisions as the values and objectives of stakeholders evolve over time. This ongoing engagement facilitates continued dialogue among diverse stakeholders, fostering a sense of inclusivity and mutual respect.</p><p>In essence, by utilizing DMDU methodology, we are not merely acknowledging the existence of diverse values in AI governance but actively inviting them to shape and influence the decision-making process. It's like conducting an orchestra with a wide range of instruments, ensuring that each has its solo, and all contribute to the final symphony. The resulting policies are, therefore, more likely to be nuanced, balanced, and representative of the diverse interests inherent in AI governance.</p><h2>3. Embracing Deep Uncertainty</h2><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/chqelkdzoz5n8eut7rwj\" alt=\"This picture was generated with Midjourney.\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/qmymrngbqwfwk14hpxpa 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/myaapmy4peh338m6zv8g 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/m6bh9r76nvm78y5kicbe 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/agths5wmiotee58aw6bs 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/y2faw5hjkze5l8pegayf 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/odq63byg9yfqfdgmfyzd 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/hdjkbhpap9te0jrugo2e 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/flgfq2uw0nh4uniazizw 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/maheevd4ks16usvcdko5 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/xcgjrhhcvnqshfw8z6mw 1568w\"></figure><p>The field of AI governance is a veritable sea of deep uncertainty. As we look out upon this vast expanse, we are faced with a multitude of unknowns. The pace and trajectory of AI advancements, societal reactions to these developments, the impact of AI on various sectors of society, and the potential emergence of unforeseen risks \u2013 all these elements contribute to a fog of uncertainty.</p><p>In many ways, AI governance is like exploring uncharted waters. We can forecast trends based on current knowledge and patterns, but there remains a substantial range of unknown factors. The behavior of complex socio-technical systems like AI is difficult to predict precisely due to their inherent dynamism and the interplay of multiple variables. In this context, conventional decision-making strategies, which often rely on predictions and probabilities, fall short.</p><p>This is where DMDU shines as an approach tailored to address situations of deep uncertainty. Instead of attempting to predict the future, DMDU encourages the exploration of a wide range of plausible futures. By considering various scenarios, including those at the extreme ends of possibility, DMDU helps decision-makers understand the breadth of potential outcomes and the variety of pathways that could lead to them.</p><p>This scenario-based approach encourages the development of flexible and adaptive strategies. It acknowledges that given the uncertainty inherent in AI governance, it is crucial to have plans that can be modified based on changing circumstances and new information. This resilience is vital for navigating an uncertain landscape, much like a ship that can adjust its course based on shifting winds and currents.</p><p>In sum, the application of DMDU in AI governance acknowledges the presence of deep uncertainty, embraces it, and uses it as a guide for informed decision-making. It equips us with a compass and map, not to predict the future, but to understand the potential landscapes that may emerge and to prepare for a diverse range of them. By doing so, it turns the challenge of uncertainty into an opportunity for resilience and adaptability, offering us glimpses throughout the thick fog.</p><h2>4. Ensuring Robustness</h2><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/wkky1cv9uqfssqzvseqc\" alt=\"This picture was generated with Midjourney.\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/b0dvjiqresilisdceftd 170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/pgcbc5j7nowqg2dsnogw 340w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/qp3mfqj5dytepqokhy3n 510w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/efhfyjm6kx75mck15kg2 680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/hztrt8roni1hfskmpcwf 850w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/abhojio07fvhqps0zyui 1020w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/b5vjwwlguateu7fkfaas 1190w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/w3ztzvqhwfvpsellcvuw 1360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/eqjqecshq5qpibesldle 1530w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/hqpbfopkmhxgfrp8gtz9 1664w\"></figure><p>AI technology is not static; it evolves, and it does so rapidly. It's akin to a river that constantly changes its course, with new tributaries emerging and old ones disappearing over time. As AI continues to advance and permeate various sectors of society, the landscape of AI governance will similarly need to adapt and evolve. This creates an imperative for robustness in policy solutions \u2013 policies that can withstand the test of time and adapt to changing circumstances.</p><p>Robustness in this context extends beyond mere resilience. It's not just about weathering the storm, but also about being able to sail through it effectively, adjusting the sails as needed. Robust policies should not only resist shocks and disruptions but also adapt and evolve with them. They should provide a solid foundation, while also maintaining enough flexibility to adapt to the dynamic AI landscape.</p><p>DMDU offers a methodology that directly supports the development of such robust policy solutions. It focuses on the identification of strategies that perform well under a wide variety of future scenarios, rather than those optimized for a single, predicted future. This breadth of consideration promotes the creation of policies that can handle a range of potential scenarios, effectively enhancing their robustness.</p><p>DMDU also supports adaptive policy-making through its iterative decision-making process. It acknowledges that as the future unfolds and new information becomes available, strategies may need to be reassessed and adjusted. This iterative process mirrors the dynamism of the AI field, ensuring that policies remain relevant and effective over time.</p><p>Moreover, DMDU's inclusive approach to decision-making, which considers a wide range of stakeholder perspectives and values, further contributes to policy robustness. By encompassing diverse perspectives, it enables the development of policies that are not only resilient but also respectful of different values and interests. This inclusivity aids in building policy solutions that are more likely to garner widespread support, further enhancing their robustness.</p><p>In essence, DMDU equips us with the tools and mindset to develop robust policy solutions for AI governance. Like a master architect, it guides us in designing structures that are sturdy yet adaptable, with a strong foundation and the flexibility to accommodate change. Through the lens of DMDU, we can build policy solutions that stand firm in the face of uncertainty and change, while also evolving alongside the dynamic landscape of AI.</p><hr><p>To sum up, incorporating DMDU methodology into AI governance is a bit like giving a mountaineer a reliable compass, a map, and a sturdy pair of boots. It equips us to traverse the complex terrain, cater to varied travelers, weather the unexpected storms, and reach the peak with policies that are not only robust but also responsive to the ever-changing AI environment.</p><h1>How to apply DMDU to AI Governance?</h1><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/cr0fylal5eaebewnfkgs\" alt=\"This picture was generated with Midjourney.\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/gkjrcac8o0zefpbqhzgu 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/jnfrvzy9dvtayo4tk7qz 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/nxpt3wu2arhhzzxgaxd2 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/pa6odlbgbzukmvfjzk6h 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/os9jt2dcbqg7o4redc6g 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/tjdeos7l9xz5fgnbiekf 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/pdxogckwpptfa9ztksfh 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/ku0op7ixzgpjucqnmgud 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/qiirgh2lpsrvdoe34vuf 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/qxs4a5dlwlwqk2ipv4uy 1456w\"></figure><p>Having taken an expansive view of the myriad reasons to pair DMDU with AI governance\u2014from dealing with its complexity, valuing the multiplicity of stakeholder perspectives, embracing deep uncertainty, to ensuring robustness in our policy responses\u2014we find ourselves standing at the foot of a significant question. As described above, this approach seems to provide us with the essential tools for our journey, analogous to a mountaineer. These resources position us to expertly navigate the convoluted socio-technical terrain, accommodate a wide range of traveling companions with their diverse priorities, withstand the unforeseeable tempests, and ultimately arrive at the peak armed with policies that are both robust and responsive to the ever-evolving AI environment.</p><p>But now that we (hopefully) appreciate the necessity of this methodology and understand its benefits, the path forward leads us to a crucial juncture. This exploration prompts an essential and practical inquiry: \"How do we apply DMDU to AI Governance?\" Now is the time to delve into the concrete ways we might integrate this methodology into the practice of governing AI, turning these theoretical insights into actionable strategies.</p><hr><p>How do we approach this question best? I would suggest, we consider the following three steps:</p><ol><li>Model Building</li><li>Embedding of Model in Optimization Setup</li><li>Exploratory Analysis</li></ol><p>&nbsp;</p><h2>1. Model Building</h2><p>The process of applying DMDU to AI governance necessitates a more detailed and nuanced approach, starting with a clear identification of the problem we wish to address. AI governance, in its essence, is an intricate socio-technical system with numerous facets and challenges. As discussed in the previous article of this sequence, an agent-based model is an excellent choice with respect to modeling paradigms.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/ifhlloytckewunfh63kq\" alt=\"This picture was generated with Midjourney.\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/zkq6a5lqy4bz8rygjfu3 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/ha5xwzafif6n4frhdd8j 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/gcq3o3drje3k0mjyih91 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/lifb0dndpnt03nrrjifq 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/n7bbmko9nkfyi3ofuohj 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/r2cjzvezoxab3as8eqa0 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/igz6bmjyppmpl9eqgk4o 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/nj0igvu2z06qonf4nknb 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/mt7nf9tsdg4b7qryy7an 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/ycxyh6dcmhofqfaxoiji 1456w\"></figure><p>Given the heterogeneous nature of the agents involved in AI governance \u2013 from governments, private corporations, civil society organizations to individual users \u2013 an agent-based modeling (ABM) approach can serve as an ideal tool for this task. As described in <a href=\"https://forum.effectivealtruism.org/posts/kCBQHWqbk4Nrns8P7/model-based-policy-analysis-under-deep-uncertainty#Agent_Based_Modeling\">the previous article</a> of this sequence, ABM is a computational method that enables a collection of autonomous entities or \"agents\" to interact within a defined environment. Each agent can represent a different stakeholder, equipped with its own set of rules, behaviors, and objectives. By setting up the interactions among these agents, we can simulate various scenarios and observe emergent behavior. The strength of ABM lies in its ability to represent diverse and complex interactions within a system. In the context of AI governance, it can help illuminate how different stakeholders' behaviors and decisions may interact and influence the system's overall dynamics. This allows us to simulate a multitude of scenarios and observe the consequences, providing us with valuable insights into potential policy impacts and helping us design robust, adaptable strategies.</p><p>Creating an agent-based model (ABM) usually involves a series of methodological steps. Here's a quick overview:</p><ol><li><strong>Problem Identification and Conceptual Model Design:</strong> This is where we identify the problem we want to address with the model and specify the objectives of our simulation. Based on this, we design a conceptual model, identifying the key agents, their attributes, and their potential behaviors. Keep in mind, instead of attempting to model the entire system in its complex entirety, a more pragmatic and effective approach would be to zero in on a specific problem within this larger system. By clearly defining the problem we aim to tackle, we can structure our model to best address it, ensuring relevance and applicability. dentifying the problem is not merely about pinpointing an issue. It also entails understanding its intricacies, its interconnected elements, and its impacts on various stakeholders. This involves asking questions like: How does this problem manifest within the field of AI governance? What causes it? Who does it affect and in what ways? What are the potential consequences if it remains unaddressed? The responses to these questions lay the foundation for our modeling efforts.</li><li><strong>System Formalization:</strong> In this step, we formalize the interactions and behaviors of the agents, usually through mathematical or logical rules. This involves defining how agents make decisions, interact with each other and their environment, and how these actions may change over time.</li><li><strong>Model Implementation:</strong> Here, we use computer software to implement our formalized model. This usually involves programming the behaviors and interactions of the agents within a simulation environment.</li><li><strong>Model Testing and Calibration:</strong> Once implemented, we run initial tests on the model to ensure it behaves as expected and fine-tune parameters to ensure it aligns with real-world data or expectations.</li><li><s><strong>Experimentation:</strong> Once our model is calibrated, we can start running experiments, altering parameters or rules, and observing the resulting behaviors. This can help us identify emergent phenomena, test different scenarios, and gain insights into system dynamics.</s></li><li><s><strong>Model Analysis and Interpretation:</strong> Finally, we analyze the results of our simulations, interpret their meaning, and translate these insights into actionable policy recommendations or strategies.</s></li></ol><p>I included steps 5 and 6 for completeness' sake. However, given that we will use DMDU on top of the ABM, our steps will diverge.</p><p>I admit, that this is still rather abstract. So, let's consider two examples<strong> </strong>for which we tease the application of the aforementioned methodological steps. The <strong>two examples</strong> are the following ones:</p><ol><li>Fairness in AI as the core problem</li><li>Existential risk of AI as the core problem</li></ol><p><strong>Disclaimer</strong>: Keep in mind that this is a very rough and preliminary attempt of laying out how the modeling process could look like. More serious thought would need to go into fleshing out such model conceptualizations.&nbsp;</p><h3>&nbsp;</h3><h3>Example 1: Fairness in AI</h3><p>Let's consider a hypothetical problem \u2013 ensuring fairness in AI systems across diverse demographic groups. Given the complexity and diversity inherent in this problem, ABM can be a particularly helpful tool to explore potential solutions.</p><ol><li><strong>Problem Identification and Conceptual Model Design:</strong> We identify fairness in AI as our core problem. The key agents could be AI developers, end-users from diverse demographic groups, and regulatory bodies. Each group will have its own objectives and behaviors related to the development, use, and regulation of AI systems.</li><li><strong>System Formalization:</strong> We formalize the decisions and behaviors of the agents. For example, developers decide on algorithms based on their understanding of fairness; users interact with AI systems and experience outcomes; regulatory bodies set policies based on observed fairness levels.</li><li><strong>Model Implementation:</strong> We use ABM software to implement our model, coding the behaviors, interactions, and environment for the agents.</li><li><strong>Model Testing and Calibration:</strong> We test the model against real-world data or scenarios to ensure it accurately represents the dynamics of fairness in AI systems.</li><li><s><strong>Experimentation:</strong> We run simulations to explore various scenarios. For instance, what happens to fairness when regulatory bodies tighten or loosen policies? What is the impact of different fairness definitions on the experience of diverse users?</s></li><li><s><strong>Model Analysis and Interpretation:</strong> We analyze the results of our simulations to understand the emergent phenomena and interpret these insights. Based on our findings, we may propose strategies or policies to improve fairness in AI systems</s>.</li></ol><p>Through this application of ABM, we can explore complex, diverse, and uncertain aspects of AI governance and support the design of robust, adaptable policies.</p><h3>&nbsp;</h3><h3>Example 2: Existential Risk of AI</h3><p>Let's take a look at how to use agent-based modeling to address the existential risk posed by AI.</p><ol><li><strong>Problem Identification and Conceptual Model Design:</strong> We first recognize the existential risk of AI as our core problem. The key agents in this scenario could be AI researchers and developers, AI systems themselves (especially if we consider the emergence of superintelligent AI), government regulatory bodies, international AI governance bodies, and various groups of people affected by AI technology (which can essentially be all of humanity).</li><li><strong>System Formalization:</strong> We formalize the decisions and behaviors of these agents. For instance, AI researchers and developers may pursue breakthroughs in AI capabilities, possibly pushing towards the development of superintelligent AI. AI systems, depending on their level of autonomy and intelligence, may behave based on their programming and potentially self-improve. Regulatory bodies create and enforce laws and regulations aimed at minimizing risk and promoting safe AI development and use. International governance bodies work on global standards and treaties. And affected groups may react in a variety of ways to the implementation of AI in society, possibly through political pressure, societal norms, or market demand.</li><li><strong>Model Implementation:</strong> Using a software platform suitable for ABM, we encode these behaviors and interactions, creating a virtual environment where these different agents operate.</li><li><strong>Model Testing and Calibration:</strong> We test the model to ensure that the interactions generate plausible outcomes that align with current understanding and data about AI development and its risks. This may involve tweaking certain parameters, such as the rate of AI capability development or the effectiveness of regulation enforcement.</li><li><s><strong>Experimentation:</strong> We run a series of experiments where we adjust parameters and observe the outcomes. For example, we might examine a worst-case scenario where the development of superintelligent AI occurs rapidly and without effective safeguards or regulations. We could also model best-case scenarios where international cooperation leads to stringent regulations and careful, safety-conscious AI development.</s></li><li><s><strong>Model Analysis and Interpretation:</strong> Finally, we scrutinize the results of the experiments to identify patterns and gain insights. This might involve identifying conditions that lead to riskier outcomes or pinpointing strategies that effectively mitigate the existential risk.</s></li></ol><p>By applying this methodology, we can use ABM as a tool to explore various scenarios and inform strategies that address the existential risk posed by AI. It should be noted, however, that the existential risk from AI is a challenging and multifaceted issue, and ABM is just one tool among many that can help us understand and navigate this complex landscape.</p><hr><p>Steps 5 and 6 in both examples are just there to show you how a typical ABM process could look like. However, we would likely stop at step 4 and go down a different path \u2013 the path of DMDU which is described in the consequent section.</p><p>&nbsp;</p><h2>2. Embedding of Model in Optimization Setup</h2><p>Eventually, we want to use the model to identify good policy recommendations. Usually, modelers run their own experiments, playing with their parameters, particular actions, handcraft some scenarios, calculate expected utilities, and provide policy recommendations based and weighted averages, etc. This can be very problematic as elaborated <a href=\"https://forum.effectivealtruism.org/posts/kCBQHWqbk4Nrns8P7/model-based-policy-analysis-under-deep-uncertainty#Expected_Utility_Thinking_is_in_Trouble\">previously</a>. The DMDU way offers an alternative. We can embrace uncertainty and find optimal <i>and</i> robust policy solutions \u2013 in a systematic way, considering ten thousands of scenarios, using AI and ML.</p><p>In order to use the AI part of DMDU, we need to briefly recap how model information can be structured. As described in <a href=\"https://forum.effectivealtruism.org/posts/kCBQHWqbk4Nrns8P7/model-based-policy-analysis-under-deep-uncertainty#XLRM_Framework\">my previous article</a>, the inputs and outputs can be structured with the XLRM framework<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsdkxkk3vw1k\"><sup><a href=\"#fnsdkxkk3vw1k\">[2]</a></sup></span>. &nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/vv8drrzycguciyovahq6\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/blu9lh95zf5bbln3lyck 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/vgdngbjiqdyyjko10442 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/zqrubkuxfjabwrhhemqc 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/j9qsvpbuhcanvgzempbs 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/rvwxqxhn1cv962fqhsh9 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/wkpxmhsydihfbnuwxa1e 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/ux7oi5sj3q7z1pxrxhxa 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/vdn3ofxjaeqbwcvyf3j7 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/z2mvjad5nvqxqeuysdjm 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/ngbsyms36gqclmfhyykz 1377w\"><figcaption>XLRM Framework</figcaption></figure><p>Keep in mind that the metrics are describing a set of variables that you (or rather your stakeholders) find particularly important to track. With the variables, we also provide the optimization direction (e.g., minimize inequality, maximize GWP, minimize number of causalities, etc.).&nbsp;</p><p>In order to embed the model in an optimization setup, we need to specify X, L, R, and M first. We need to provide the variables and their plausible domains. Examples will be described later. But first, I would like to talk about the optimization process and how it differs from the simulation process.</p><h3>Simulation versus Optimization&nbsp;</h3><p>In agent-based modeling, simulation and optimization operate as two distinct but interconnected components, each serving a unique role in the decision-making process. However, they differ substantially in terms of their focal point and methodological approach, particularly concerning the flow of data from inputs to outputs or vice versa.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/png3cysjyixtpfwbc6fy\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/cawlj8iwwa6p7kxfwnml 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/wgddkeyeyof8ji0ba5rs 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/yudpihbzmybbaamlqdzc 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/vgeicmcajmiegjvu3jml 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/xlrfrag4lp1sje9jiuvd 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/dal5vjcuxselrqbkzpeq 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/hpkhru8bunwmtfjeewzg 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/j5noke2ld0qowxbctjoc 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/cxmzrayzvtqgcl2ml8qh 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/io46rleon1xi1bbwgsdx 973w\"><figcaption>Simulation versus Optimization</figcaption></figure><p><strong>Simulation</strong>, in the context of agent-based modeling, is typically seen as a process-driven approach where the key concern lies in the understanding and elucidation of complex system behavior. This begins with defining the inputs, a mixture of policy levers and exogenous uncertainties. By manipulating these variables and initiating the simulation, the system's states are allowed to evolve over time according to pre-established rules defined by the relations (R) . The emergent outcome metrics are then observed and analyzed. The direction of interest in simulation is essentially from inputs to outputs \u2013 from the known or controllable aspects to the resultant outcomes.</p><p><strong>Optimization</strong>, on the other hand, operates in a seemingly inverted manner. This process begins by identifying the metrics of interest \u2013 the desired outcomes \u2013 and proceeds by employing computational techniques, often involving AI algorithms, to find the optimal combination of policy levers that achieve these goals under particular scenarios. Here, the direction of attention is from outputs back to inputs. We begin with a predetermined end state and traverse backward, seeking out the best strategies to reach these objectives.</p><p>The optimization process is one of the key parts of the DMDU approach. Although, we usually use several AI and ML algorithms, using optimization to search for policies is central. For this optimization, we like to use a specimen of the <strong>multi-objective evolutionary algorithm (MOEA)</strong> family. They facilitate the identification of Pareto-optimal policies. These are policies that cannot be improved in one objective without negatively affecting another. The uncertainty inherent in DMDU scenarios often necessitates the consideration of multiple conflicting objectives. MOEAs, with their inherent ability to handle multiple objectives and explore a large and diverse solution space, offer an effective approach to handling such complex, multi-dimensional decision problems. They work by generating a population of potential solutions and then iteratively evolving this population through processes akin to natural selection, mutation, and recombination. In each iteration or generation, solutions that represent the most efficient trade-offs among the objectives are identified and preserved. This evolutionary process continues until a set of Pareto-optimal solutions is identified. By employing MOEAs in DMDU, decision-makers can visualize the trade-offs between competing objectives through the generated Pareto front. This enables them to understand the landscape of possible decisions and their impacts, providing valuable insights when the optimal policy is not clear-cut due to the deep uncertainty involved. Thus, MOEAs contribute significantly to the robustness and adaptability of policy decisions under complex, uncertain circumstances.</p><h3>X: Exogenous Uncertainties</h3><p>Here, I want to simply list a few key parameters that could be considered uncertain which we could take into account:</p><ul><li>pace of rapid capability gain (aka. intelligence explosion)</li><li>importance of hardware progress</li><li>number of secret AGI labs</li><li>initial willingness of public to push back</li><li>strength of status quo bias</li><li>probability of constant scaling laws&nbsp;</li><li>improvement pace of effective compute&nbsp;</li><li>development of spending of big labs on compute&nbsp;</li><li>required FLOP/s to run AGI</li></ul><p>As you probably have noticed, these factors are rather abstract. For a better operationalization, we would need to know the details of the model. In principle, any parameter that is not affected by another parameter could be marked as an exogenous uncertainty. It depends on the &nbsp;model at hand. If a parameter within a model is properly operationalized, we can then choose a plausible range for each parameter. The ranges of all uncertainty variables span the uncertainty space, from which we can choose or sample scenarios.</p><h3>L: Policy Levers</h3><p>The other inputs to the model are the policy levers. Depending on which actors are willing to follow your advice, their possible actions are potential levers that can be affected. Also here, the selection of policy levers depends on the particular model and the problem that is modeled. The selection of relevant policy levers requires thorough research though and clearer operationalization. Some ideas that float around in the AI governance scene are listed below:</p><ul><li>various forms of compute governance<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1sqjdehabet\"><sup><a href=\"#fn1sqjdehabet\">[3]</a></sup></span><ul><li>on-chip firmware to occasionally save snapshots of the the neural network weights stored in device memory (e.g., restricted to particular actors; or related to some snapshot frequency)</li><li>monitoring the chip supply chain (globally; or threshold values, e.g. if organization exceeds some number like spending, market cap, etc.)</li></ul></li><li>auditing regulation; consider various implementations of auditing (actor scope, focus on data, focus on algorithmic transparency, various forms of certification requirements, post-deployment monitoring, etc.)</li><li>setting negative and positive incentive structures</li><li>involve general public to voice concerns (e.g., to change willingness to regulate)</li><li>invest in AI alignment research</li><li>persuade key players to advocate for international treaties</li><li>promote safety research (e.g., through spending on safety campaigns)</li><li>improve security at leading AI labs</li><li>cutting off data access for big labs (e.g., through lawsuits)</li><li>restrict labs from developing AGI through licenses<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6853e0j2vf9\"><sup><a href=\"#fn6853e0j2vf9\">[4]</a></sup></span></li></ul><p>Certainly, it's imperative to remain mindful that the policy levers we consider must be germane to our problem formulation, within our sphere of control, and sufficiently concrete to be actionable. In the context of AI governance, this means focusing on mechanisms and regulations that can directly impact the design, development, deployment, and use of AI systems. While the range of potential interventions might be vast, our model should hone in on those aspects we can directly influence or those that could feasibly be manipulated by the stakeholders we represent or advise. These could encompass regulations on AI transparency, safety research funding, or education programs for AI developers, among others. All considered policy actions need to be tangible, practical, and capable of precise definition within the model. Broad, ill-defined, or inaccessible policy levers can lead to vague, non-actionable, or even misleading results from the model. Hence, in crafting our agent-based model for AI governance, the judicious selection and specification of policy levers is of paramount importance.&nbsp;</p><p>It's crucial to underline that a policy isn't simply a single action, but a cohesive set of measures, a tapestry of interwoven strands that together create a concerted strategy. When we speak of finding an optimal policy in the context of AI governance, we're not on a hunt for a silver bullet \u2014 a solitary action that will neatly resolve all our challenges. Instead, we're seeking a potent blend of policy levers, each precisely tuned to yield the greatest collective impact. Think of it as orchestrating a symphony \u2014 each instrument plays its part, and when they're all in harmony, we create something far greater than the sum of its parts. The objective then is to discover the combinations of actions \u2014 their nature, their degree, their timing \u2014 that yield Pareto-optimal policy solutions. In other words, we aim to identify sets of actions that, when taken together, provide the best possible outcomes across our diverse set of goals and constraints, without one benefit being improved only at the expense of another. This more holistic perspective is vital for the effective governance of something as complex and multifaceted as AI.</p><h3>R: Relations</h3><p>In the context of the XLRM framework applied to an ABM for AI governance, \"Relations\" refers to the established cause-and-effect linkages and interactions that govern the behavior of the model. These encapsulate the fundamental rules of the model and shape how the system components (agents, states, etc.) interact over time and in response to various actions and external factors. They can include mathematical formulas, decision-making algorithms, probabilistic dependencies, and other types of deterministic or stochastic relationships. In an AI governance model, these could represent a wide range of interactions, such as how AI developers respond to regulations, how AI systems' behaviors change in response to different inputs and environments, how public perception of AI evolves over time, and how different policies might interact with each other. By defining these relations, we can simulate the complex dynamics of the AI governance system under a variety of conditions and policy actions. This allows us to gain insights into potential futures and the impacts of different policy options, thus supporting more informed and robust decision-making in the face of deep uncertainty.</p><p>Selecting appropriate relations for a model is a crucial and delicate task. It fundamentally shapes the model's behavior and its capacity to provide meaningful insights. When considering the integration of economic aspects into an ABM for AI governance, it would be tempting to adopt established macroeconomic theories, such as neoclassical economics. However, such an approach may not be best suited to the complex, dynamic, and deeply uncertain nature of AI governance. Consider the potential pitfalls of using something like Nordhaus' Dynamic Integrated Climate-Economy (DICE) model, which applies neoclassical economic principles to climate change economics. While the DICE model has made important contributions to our understanding of the economics of climate change, it has also been criticized for its oversimplifications and assumptions. For example, it assumes market equilibrium, perfect foresight, and rational, utility-maximizing agents, and it aggregates complex systems into just a few key variables. Now, if we were to integrate such a model with AI takeoff mechanisms, we could encounter a number of issues. For one, AI takeoff is a deeply uncertain and potentially rapid process, which might not align well with the DICE model's equilibrium assumptions and aggregated approach. Also, the model's assumption of perfect foresight and rational agents might not adequately reflect the behavior of stakeholders in the face of a fast-paced AI takeoff. It may ignore the potential for surprise, panic, or other non-rational responses, as well as the potential for unequal impacts or power dynamics. Such misalignments could lead to misleading results and poorly suited policy recommendations.</p><p>In contrast, an ABM that adopts principles from complexity economics could be a better choice. Complexity economics acknowledges the dynamic, out-of-equilibrium nature of economies, the heterogeneity of agents, and the importance of network effects and emergent (market) phenomena. It's more amenable to exploring complex, uncertain, and non-linear dynamics, such as those expected in AI governance and AI takeoff scenarios. As mentioned above, in an ABM for AI governance, we could model different types of agents (AI developers, regulators, public, AI systems themselves) with their own behaviors, objectives, and constraints, interacting in various ways. This could allow us to capture complex system dynamics, explore a wide range of scenarios, and test the impacts of various policy options in a more realistic and nuanced manner.</p><p>In conclusion, the selection of relations in a model is not a trivial task. It requires a deep understanding of the system being modeled, careful consideration of the model's purpose and scope, and a thoughtful balancing of realism, complexity, and computational feasibility. It's a task that demands not only technical expertise but also a good dose of humility, creativity, and critical thinking.</p><h3>M: Performance Metrics</h3><p>Performance metrics on AI governance could vary greatly depending on the specific context and purpose of the model. However, here are some potential metrics that might be generally applicable:</p><ul><li><strong>Safety of AI Systems:</strong> A measure of how frequently AI systems result in harmful outcomes, potentially weighted by the severity of harm.</li><li><strong>Equity:</strong> This could assess the distribution of benefits and harms from AI systems across different groups in society. It could consider factors like demographic disparities in impacts or access to AI benefits.</li><li><strong>Public Trust in AI:</strong> A measure of public confidence in AI systems and their governance. This could be modeled as a function of various factors like transparency, communication, and past performance of AI systems.</li><li><strong>AI Progress Rate:</strong> This could measure the pace of AI development and deployment, which might be influenced by various policies and conditions.</li><li><strong>Adherence to Ethical Guidelines:</strong> A measure of how well the behavior of AI systems and AI developers aligns with established ethical guidelines.</li><li><strong>Resilience to AI-Related Risks:</strong> This could assess the system's ability to withstand or recover from various potential AI-related risks, such as misuse of AI, AI accidents, or more catastrophic risks like a rapid, uncontrolled AI takeoff.</li><li><strong>Economic Impact:</strong> This could assess the economic effects of AI systems and their governance, such as impacts on jobs, economic productivity, or inequality.</li><li><strong>Compliance Costs:</strong> A measure of the costs associated with compliance to AI regulations, both for AI developers and regulators.</li><li><strong>Risk Reduction:</strong> This metric could assess the degree to which policies and measures effectively mitigate potential existential risks associated with AI.</li><li><strong>Rate of Safe AI Development:</strong> Measuring the pace of AI development that complies with established safety measures and guidelines.</li><li><strong>Compliance with Safety Protocols:</strong> This could assess the degree to which AI developers adhere to safety protocols, and how effectively these protocols are enforced.</li><li><strong>Resilience to AI Takeoff Scenarios:</strong> This would assess the system's ability to detect, respond to, and recover from potential rapid AI takeoff scenarios.</li><li><strong>International Collaboration on AI Safety:</strong> Since existential risk is a global concern, a metric evaluating the extent and effectiveness of international cooperation on AI safety could be important.</li><li><strong>Public Awareness of AI Risk:</strong> It could be beneficial to measure public understanding and awareness of the existential risks posed by AI, as this can impact policy support and adherence to safety guidelines.</li><li><strong>Effectiveness of Risk Containment Measures:</strong> Measuring the effectiveness of policies and strategies intended to prevent or mitigate AI-induced harm, like controls on AI-related resources or emergency response plans for potential takeoff scenarios.</li><li><strong>AI Behavior Alignment:</strong> Evaluating how closely AI behaviors align with human values and safety requirements, which is crucial to prevent AI systems from taking unintended and potentially harmful actions.</li></ul><p>Remember, the key to effective performance metrics is to ensure that they align with the purpose of your model and accurately reflect the aspects of the system that you care most about. The specifics will depend on your particular context, purpose, and the nature of the AI governance system you are modeling.</p><h2>3. Exploratory Analysis</h2><p>With our agent-based model for AI governance now structured, our variables defined, and their relevant ranges established, we can proceed to the final, yet crucial, stage \u2014 the exploratory analysis. This step is what makes DMDU approaches so powerful; it allows us to navigate the vast landscape of possibilities and understand the space of outcomes across different policies and scenarios.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/cdewzykel1nbprbqvvzk\" alt=\"This picture was generated with Midjourney.\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/gbkun4cyy8dscvy9wcfy 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/pawky3ugs1oqoegql5ia 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/imnikypsxpec5wwkuhr4 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/wvnmguql5ubcnwpav9x8 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/xc0oxdn8dvvrtkbpe2r2 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/mh86so6hqk0wuog5oinf 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/gdcbk6xwudtfktrjtuuq 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/sonol7wqp2h7dezgkatc 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/xdh88gjh1ifgk2yei6cr 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ikTpmYqA4CrzFyzxb/sxioi5by5w8jbp0wprux 1456w\"></figure><p>In exploratory analysis, rather than predicting a single future and optimizing for it, we explore a broad range of plausible future scenarios and analyze how various policy actions perform across these. The goal is to identify robust policies \u2013 those that perform well across a variety of possible futures, rather than merely optimizing for one presumed most likely future.</p><h3>The Process in a Nutshell</h3><p>Once we have initiated our large-scale, scenario-based exploration, the next critical step is identifying vulnerable scenarios. These are scenarios under which our system performs poorly on our various performance metrics. By examining these vulnerable scenarios, we gain insights into the conditions under which our proposed policies might falter. This understanding is essential in designing resilient systems capable of withstanding diverse circumstances. For each of these identified scenarios, we then use a process known as multi-objective optimization, often employing evolutionary algorithms, to find sets of policy actions that perform well. These algorithms iteratively generate, evaluate, and select policies, seeking those that offer the best trade-offs across our performance metrics. The aim here is not to find a single, 'best' policy but rather to identify a set of Pareto-optimal policies. These are policies for which no other policy exists that performs at least as well across all objectives and strictly better in at least one.</p><p>However, due to the deep uncertainty inherent in AI governance, we cannot be sure that the future will unfold according to any of our specific scenarios, even those that we've identified as vulnerable. Therefore, we &nbsp;re-evaluate our set of Pareto-optimal policies across a much larger ensemble of scenarios, often running into the tens of thousands. This process allows us to understand how our policies perform under a wide variety of future conditions, effectively stress-testing them for robustness. In this context, a robust policy is one that performs satisfactorily across a wide range of plausible futures<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefif9g8kbzpof\"><sup><a href=\"#fnif9g8kbzpof\">[5]</a></sup></span>. For further elaboration, see <a href=\"https://forum.effectivealtruism.org/posts/kCBQHWqbk4Nrns8P7/model-based-policy-analysis-under-deep-uncertainty#Example_Method__MORDM\">this section of my previous article</a>.</p><p>Overall, the goal of this exploratory analysis is to illuminate the landscape of potential outcomes, and in doing so, guide decision-makers towards policies that perform sufficiently well \u2013 even in particularly bleak scenarios. By applying these methods, we can navigate the deep uncertainties of AI governance with a more informed, resilient, and robust approach.</p><h3>What's the Outcome of DMDU?</h3><p>The culmination of the entire modeling and Decision Making under Deep Uncertainty process is a suite of robust and adaptable policies and strategies for governing AI, designed to perform well under a wide range of plausible futures. The core deliverables of this process can be roughly divided into three categories: insights, recommendations, and tools for ongoing decision-making.</p><p><strong>1. Insights: </strong>The process of employing DMDU methodologies within AI governance ushers in a wealth of insights that can have profound impacts on the way we approach the field. These insights span across the three vital facets: system dynamics, key drivers of outcomes, and potential vulnerabilities.</p><ul><li><strong>System Dynamics:</strong> A comprehensive exploration of varied scenarios under DMDU lays bare the intricate workings of the socio-technical system that is AI governance. Through modeling and analysis, we come to understand the complex interdependencies and feedback loops within the system, the hidden patterns that may not be immediately apparent, and how different elements of the system respond to changes and shocks. This understanding is crucial, not just in making sense of the present, but also in anticipating the future, enabling us to forecast potential trends and systemic shifts, and to plan accordingly. In the context of AI governance and the mitigation of existential risk, the system dynamics can be quite complex. For instance, we may examine the interaction between AI research and development (R&amp;D) efforts and the level of regulation. Too little regulation might foster rapid progress but could also increase risk, whereas too stringent regulation might hamper innovation but reduce immediate hazards. Another critical dynamic might involve the international competition in AI technology. The race for supremacy in AI could potentially discourage cooperation and information sharing, thereby increasing the risk of uncontrolled AI development and deployment.</li><li><strong>Key Drivers of Outcomes:</strong> DMDU also brings to light the key variables that have the most significant influence on outcomes. It helps us identify critical leverage points, i.e., places within the system where a small intervention can lead to large-scale changes. By revealing these drivers, DMDU enables us to focus our efforts where they are likely to have the most significant impact. This understanding of outcome drivers is invaluable in formulating effective and targeted policy interventions. With respect to AI governance, the key drivers of outcomes could include the pace of AI R&amp;D, the robustness of regulation, the level of international cooperation, and the availability of resources for risk mitigation efforts. For example, a rapid pace of R&amp;D, especially if it outstrips the development of regulatory measures, could significantly increase the risk. On the other hand, strong international cooperation, leading to shared standards and concerted efforts to control risks, could be a significant factor in reducing the likelihood of adverse outcomes.</li><li><strong>Potential Vulnerabilities:</strong> Perhaps most importantly, DMDU allows us to identify potential vulnerabilities \u2013 areas where our policies and strategies might be susceptible to failure under certain conditions. By shedding light on these vulnerabilities, DMDU equips us to design more resilient strategies, capable of withstanding a broad range of future scenarios. It also enables us to anticipate potential threats and challenges, to take proactive measures to mitigate risks, and to put contingency plans in place. Potential vulnerabilities in the AI governance system might include weak points in regulatory frameworks, a lack of technical expertise in regulatory bodies, or insufficient investment in risk mitigation measures. For example, if regulations are unable to keep up with the pace of AI development, it could leave gaps that might be exploited, leading to increased risks. Alternatively, a sudden surge in AI capabilities, often referred to as a \"hard takeoff\", could catch policymakers and society at large off guard, with catastrophic consequences if not adequately prepared for.</li></ul><p><strong>2. Policy Options and Trade-offs:</strong> The DMDU process generates a set of Pareto-optimal policy solutions. These are distinct combinations of policy actions that have demonstrated satisfactory performance across a spectrum of scenarios. It's important to note that these are not prescriptive \"recommendations\" as such, but rather a diverse suite of potential solutions. As researchers or policy analysts, our role is not to dictate a single best policy, but to articulate the various options and their respective trade-offs. Each Pareto-optimal policy solution represents a unique balance of outcomes across different objectives. The inherent trade-offs between these policy solutions reflect the complex, multi-dimensional nature of AI governance. By effectively outlining these options and elucidating their respective strengths and weaknesses, we offer stakeholders a comprehensive overview of potential paths. Our goal is to facilitate informed decision-making by providing a clear, detailed map of the policy landscape. This output should instigate thoughtful deliberation among stakeholders, who can weigh the various trade-offs in light of their own values, priorities, and risk tolerance. Through such a process, the stakeholders are empowered to select and implement the policies that best align with their objectives, all the while having a clear understanding of the associated trade-offs. This approach supports a more democratic and inclusive decision-making process, ensuring that decisions about AI governance are informed, thoughtful, and responsive to a diverse range of needs and perspectives.</p><p><strong>3. Tools for Ongoing Decision-Making:</strong> The enduring value of the DMDU process lies not only in the immediate insights it offers but also in the arsenal of tools it provides for sustainable decision-making. These tools, derived from the extensive modeling and data analysis conducted during the process, remain invaluable resources for continued policy evaluation and strategy adaptation.</p><ul><li><strong>Policy Evaluation and 'What-if' Analysis:</strong> One of the most direct applications of these tools lies in their capacity to evaluate new policies. Given a proposed policy, we can deploy these tools to conduct predictive analysis and assess the potential outcomes under a variety of scenarios. This 'what-if' analysis is a powerful instrument for decision-makers, enabling them to forecast the impacts of different policy alternatives before implementation, thereby reducing the element of uncertainty. Beyond this, these tools are also adept at 'stress-testing' existing policies. They allow us to simulate potential changes in conditions and understand how our current strategies might fare in the face of these changes. In essence, they offer a safe environment to gauge the potential weaknesses and strengths of our strategies under various circumstances, providing an opportunity for proactive policy refinement.</li><li><strong>Adaptive Strategy Development:</strong> As the field of AI governance is continuously evolving, the need for adaptable strategies is paramount. The DMDU tools play a crucial role in this aspect, serving as dynamic platforms that can incorporate new data and information as they become available. This flexibility facilitates the continuous updating and refining of our strategies, ensuring they remain aligned with the shifting landscape of AI governance. These tools are not static but are designed to evolve, learn, and adapt in tandem with the socio-technical system they aim to govern. They help create a responsive and resilient AI governance framework, one capable of addressing the inherent uncertainty and flux within the system, and adjusting to new developments, be they technological advancements, shifts in societal values, or changes in the regulatory environment.</li></ul><hr><p>The potential impact of these deliverables in the world of AI governance is significant. Insights and policy options with trade-offs from the DMDU process can inform the development of more effective and resilient governance strategies, shape the debate around AI policy, and guide the actions of policymakers, industry leaders, and other stakeholders. The decision-support tools can enhance the capacity of these actors to make well-informed decisions in the face of uncertainty and change. Ultimately, by integrating DMDU methods into the field of AI governance, we can enhance our collective capacity to navigate the challenges and uncertainties of AI, mitigate potential risks, and harness the immense potential of AI in a way that aligns with our societal values and goals.</p><h1>Conclusion</h1><p>In an era where AI technologies play a pivotal role in shaping our world, the necessity to recalibrate our approach to AI governance is paramount. This article seeks to underline the critical need for embracing innovative methodologies such as DMDU to tackle the intricacies and challenges this transformative technology brings.</p><p>DMDU methodology, with its emphasis on considering a broad spectrum of possible futures, encourages an AI-driven approach to AI governance that is inherently robust, adaptable, and inclusive. It's a methodology well-equipped to grapple with socio-technical complexities, the vast diversity of stakeholder values, deep uncertainties, and the need for robust, evolving policies \u2013 all core aspects of AI governance.</p><p>As we look ahead, we are faced with a choice. We can continue to navigate the terrain of AI governance with the existing tools and maps or we can reboot our journey, equipped with a new compass \u2013 DMDU and similar methodologies. The latter offers a means of moving forward that acknowledges and embraces the complexity and dynamism of AI and its governance.</p><p>Referencing back to the title, this 'rebooting' signifies a shift from traditional, deterministic models of governance to one that embraces uncertainty and complexity. It's an AI-driven approach to AI governance, where AI is not just the subject of governance but also a tool to better understand and navigate the complexities of governance itself. In a world growing more reliant on AI technologies, the application of methodologies like DMDU in AI governance might provide a fresh perspective. As stakeholders in AI governance \u2013 encompassing policy makers, AI developers, civil society organizations, and individual users \u2013 it may be beneficial to explore and consider the potential value of systems modeling and decision-making under deep uncertainty. Incorporating such approaches could potentially enhance our strategies and policies, making them more robust and adaptable. This could, in turn, better equip us to address the broad spectrum of dynamic and evolving challenges that AI presents.</p><p>In conclusion, rebooting AI governance is not just a call for an AI-driven approach to AI governance, but also an invitation to embrace the uncertainties, complexities, and opportunities that lie ahead. By adopting DMDU, we can ensure our path forward in AI governance is not just responsive to the needs of today, but resilient and adaptable enough to navigate the future. As we continue to traverse this dynamic landscape, let's commit to a future where AI is governed in a manner that's as innovative and forward-thinking as the technology itself.</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkvd4gta61v9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkvd4gta61v9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For simplicity's sake, within the scope of this article, I refer to the combination of <i>systems modeling</i> AND <i>decision-making under deep uncertainty (DMDU)</i> as simply <i><strong>DMDU</strong></i>.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsdkxkk3vw1k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsdkxkk3vw1k\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Further explanations on the terminology can be found in the introduction article.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1sqjdehabet\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1sqjdehabet\">^</a></strong></sup></span><div class=\"footnote-content\"><p>As for example suggested in: Shavit, Y. (2023). What does it take to catch a Chinchilla? Verifying Rules on Large-Scale Neural Network Training via Compute Monitoring. <i>arXiv preprint arXiv:2303.11341</i>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6853e0j2vf9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6853e0j2vf9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>As recently promoted by <a href=\"https://www.businessinsider.com/sam-altman-openai-chatgpt-government-agency-should-license-ai-work-2023-5?r=US&amp;IR=T\">Sam Altman at his Congress hearing</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnif9g8kbzpof\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefif9g8kbzpof\">^</a></strong></sup></span><div class=\"footnote-content\"><p>There are more stringent definitions of robustness which can depend on a plethora of factors, including how risk averse our stakeholders are with respect to particular performance metrics. E.g., we might be extremely risk &nbsp;averse when it comes to the variable <i>number of causalities</i>. For a good analysis of various robustness metrics, see McPhail, C., Maier, H. R., Kwakkel, J. H., Giuliani, M., Castelletti, A., &amp; Westra, S. (2018). Robustness metrics: How are they calculated, when should they be used and why do they give different results?. <i>Earth's Future</i>, <i>6</i>(2), 169-191.</p></div></li></ol>", "user": {"username": "Max Riddle"}}, {"_id": "DPHu3p3LwTFBxB3ed", "title": "Announcing the Prague Fall Season 2023 and the Epistea Residency Program", "postedAt": "2023-05-22T05:52:51.215Z", "htmlBody": "<h2>UPDATE: Applications are open</h2><p>The application for the Epistea Residency is now open <a href=\"https://airtable.com/appQ6d2v6UzFI4Kcf/shryf26iFBbfuvo02\">here</a>. We are accepting applications on a rolling basis with the final <strong>deadline of July 24</strong>. Successful applicants will be interviewed in the second application round. All applicants can expect to hear back from us by the beginning of August. If you would like to join us at Fixed Point during Prague Fall Season in another capacity, please fill out <a href=\"https://form.jotform.com/230322573808050\">this form</a> or stay tuned for the short-term visitor applications for Prague Fall Season 2023.</p><h2>Summary</h2><p>Following&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ZxHnaPmxmBrZmerBj/announcing-the-prague-fall-season\"><u>a successful pilot in 2022</u></a>, we are announcing the Prague Fall Season 2023, which is a program run by <a href=\"https://forum.effectivealtruism.org/posts/FrshKTu34cFGGsyka/announcing-a-new-organization-epistea\">Epistea</a>, happening from&nbsp;<strong>September 1 to December 10&nbsp;</strong>at&nbsp;<a href=\"https://fixedpoint.house/\"><u>FIXED POINT</u></a> in Prague, Czech Republic. In this time, FIXED POINT will host a variety of programs, projects, events and individuals in the areas of existential security, rationality, epistemics, and effective altruism. We will announce specific events and programs as we confirm them but for now, our flagship program is the&nbsp;<strong>10-week Epistea Residency Program</strong> for teams working on projects related to epistemics and rationality. <s>We are now seeking&nbsp;</s><a href=\"https://airtable.com/shrguU9mqqUNNSixt\"><s><u>expressions of interest</u></s></a><s> from potential Epistea residents and mentors.&nbsp;</s> (UPDATE: <a href=\"https://airtable.com/appQ6d2v6UzFI4Kcf/shryf26iFBbfuvo02\">Applications are open!</a>)</p><h2>What is a season?</h2><p>The main benefit of doing a season is having a dedicated limited time to create an increased density of people in one place. This creates&nbsp;<strong>more opportunities for people to collaborate, co-create and co-work on important projects</strong> - sometimes in a new location. This happens to some extent naturally around major EA conferences in London or San Francisco - many people are there at the same time which creates opportunities for additional events and collaborations. However, the timeframe is quite short and it is not clearly communicated that there are benefits in staying in the area longer and there is not a lot of infrastructure in place to support that.</p><p>We ran&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ZxHnaPmxmBrZmerBj/announcing-the-prague-fall-season\"><u>the pilot project Prague Fall Season last autumn</u></a>: Along with&nbsp;<strong>25 long-term residents, we hosted over 300 international visitors between September and December 2022</strong>. We provided comprehensive support through funding, venue operations, technical and personal development programs, social gatherings, and additional events, such as the CFAR workshop series. Based on the feedback we received and our own experience with the program, we decided to produce another edition of Prague Fall Season this year with a couple of changes:</p><ul><li>We are narrowing the scope of the program primarily to existential security, epistemics, and rationality.</li><li>We ask that participants of the season help us share the cost of running the FIXED POINT house. We may be able to offer financial aid on a case by case basis but the expectation is that when you visit, you can cover at least some part of the cost.</li><li>We are seeking event organizers who would like to make their events part of the season.</li></ul><p>We will be sharing more information about how to get involved soon. For now, our priority is launching the Epistea Residency program.</p><h2>The Epistea Residency 2023</h2><p>The backbone of the Prague Fall Season 2023 will once again be a 10-week residency program. This year, we are looking for&nbsp;<strong>6-10 teams of 3-5 members each</strong> working on specific projects related to areas of rationality, epistemics, group rationality, and civilizational sanity, and delivering tangible outcomes. A residency project can be:</p><ul><li>Research on a relevant topic (examples of what we would be excited about are broad in some directions and include abstract foundations like geometric rationality or Modal Fixpoint Cooperation without L\u00f6b's Theorem, research and development of applied rationality techniques like Internal communication framework, research on the use of AI to improve human rationality like \"automated Double-Crux aid\" and more);</li><li>Distillation, communication, and publishing (writing and publishing a series of explanatory posts, video production, writing a textbook or course materials, etc.);</li><li>Program development (events, workshops, etc.);</li><li>Anything else that will provide value to this space.</li></ul><p>Teams will have the option to apply to work on a specific topic (to be announced soon) or propose their own project. The selected teams will work on their projects in person at FIXED POINT from&nbsp;<strong>September 18 to November 25</strong> while actively engaging with each other and cross-pollinating ideas. We expect all team members to dedicate at least&nbsp;<strong>0.5 FTE (20 hours per week)</strong> to the residency, granting them the possibility to maintain their regular work commitments if necessary. By the end of their residency, teams are expected to present a project output (e.g. organizing a workshop, publishing a paper, etc.) or reach a project milestone.&nbsp;</p><p>As part of the Epistea Residency, we will support the teams with:</p><ul><li>Infrastructure (fully equipped co-working office space for each team, meeting rooms, office supplies, access to and maintenance of the house facilities, daily lunch catering, drinks, and snacks);</li><li>Administrative support (assistance with booking flights and/or accommodation and settling in Prague, operations for any projects or events organized within the scope of Prague Fall Season);</li><li>Targeted mentoring by leading experts in the field of each team\u2019s project;</li><li>Option of coaching by&nbsp;<a href=\"https://www.teebarnett.com/\"><u>Tee Barnett</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/q4ijrWYQPSG8b92aa/coaching-training-program-launch-build-and-upskill-your\"><u>his trainee cohort</u></a>;</li><li>At least one out-of-Prague retreat for the Epistea residents;</li><li>Possibly funding for travel expenses, accommodation, and salaries (to be determined depending on funding availability).</li></ul><p>One way how to think about the Fellowship is as a program somewhat similar to SERI-MATS or PIBBSS, but focused on advancing epistemics and applied rationality, rather than AI alignment. (This does not preclude projects which advance both rationality and alignment)&nbsp;</p><p><strong>The applications for Epistea Residency will open in June 2023</strong>. We plan to finalize the admission decisions in early July; teams will arrive in Prague in September.&nbsp;</p><h2>How to get involved</h2><p>We are now soliciting inquiries for the Epistea Residency, both from potential participants/teams as well as from experts who would like to serve as mentors, shaping the selection of research questions and guiding the teams in the process. If you are interested in participating, or if you know somebody who would be a great fit for the residency, please briefly let us know through this short, low-stakes&nbsp;<a href=\"https://airtable.com/shrguU9mqqUNNSixt\"><u>form</u></a>.</p><p>If you would like to join us at Fixed Point during Prague Fall Season in another capacity, please&nbsp;<a href=\"https://form.jotform.com/230322573808050\"><u>fill out this form</u></a>.</p><p>We look forward to welcoming you to Prague soon!</p><h2>Support us</h2><p>We are currently in the process of securing funding for this program (this includes travel reimbursements, financial aid for housing, staff salaries, a small stipend for residents etc.). If you think this is a good fit for your organization or you know of someone who would be interested in supporting this project, please let us know at info@praguefallseason.com.</p>", "user": {"username": "Epistea"}}, {"_id": "kJHXoeDy8EpF9BeJs", "title": "Philosophy Can Change the World. Peter Singer Proved It. Three Times.", "postedAt": "2023-05-20T00:07:11.581Z", "htmlBody": "<p><i>One of the founders of the Effective Altruism movement, Peter Singer has turbocharged entire movements with the sheer force of his writing.&nbsp;</i></p><p><i>As he nears retirement, what lessons does his life have on how to do&nbsp;</i><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span><i>the most good we can?</i></p><p><i>Below is a story by Wayne Hsiung, my co-founder at The Simple Heart (an animal nonprofit), about the time he protested Peter Singer on-stage while he was a young law student at the University of Chicago. From Singer's response, Wayne saw first-hand the power of Singer's approach to criticism and quickly became one of his biggest fans.</i></p><p><i>It's an inspiring and useful story on how to effectively respond to criticism and challenge social norms.</i></p><p><i>Main points:</i></p><ul><li><i>Speak truth, even when it\u2019s hard.</i></li><li><i>Focus on systems rather than individuals.</i></li><li><i>Be guided by evidence.</i></li></ul><p><i><strong>For Bay Area people: </strong>Peter Singer and Wayne Hsiung are speaking live in San Francisco on Tuesday, May 30. Get 50% off tickets </i><a href=\"https://broadwaysf.com/Online/default.asp?BOparam::WScontent::loadArticle::permalink=peter-singer&amp;BOparam::WScontent::loadArticle::context_id=&amp;menu_id=D8B581C0-D57E-44FF-A27B-4336DDA81A9D\"><i>here </i></a><i>with code SINGER50.</i></p><hr><h2>The Legacy (and Controversy) of Peter Singer</h2><h3>Few have had as much impact as the legendary author of Animal Liberation. As he tours the country and nears retirement, his life has lessons for us all.</h3><p>In 2004, I had a confrontation with the most important philosopher in the world.</p><p>I was a young law student at the University of Chicago. And Peter Singer, who had recently been appointed to a distinguished position at Princeton University, was selected to give the law school\u2019s distinguished Dewey Lecture. An array of the most influential figures in law, philosophy, and even business gathered to listen to Singer\u2019s wisdom, including high level executives at Whole Foods, the newly-appointed CEO of the Humane Society of the United States, Wayne Pacelle, and the most cited law professor in the world, Cass Sunstein.</p><p>But I had a different agenda: I wanted to prove Singer was wrong.</p><p>While Singer\u2019s book, <i>Animal Liberation</i>, was a formative influence on my views of animal rights, I had come to see him as a sell-out to the animal rights movement. My new champion, a fiery law professor named Gary Francione, regularly condemned Singer as a \u201cpartner\u201d with animal-exploiting industries because of Singer\u2019s support for animal welfare reforms. In Francione\u2019s view, the endorsement of these reforms, such as taking mother pigs out of gestation crates, was white-washing an abusive industry. Even if mother pigs lived outside of cages, after all, they would still be tortured and killed! And, motivated by my youthful idealism, I agreed with Francione\u2019s critique.</p><p>So, after much discussion and debate among my fellow vegans at the University of Chicago, I decided I would call Peter Singer out. I would condemn him publicly as a traitor to the movement.</p><p>Then something astonishing happened. Peter Singer defended me.</p><p>Nothing in my question would have predicted this outcome:</p><p>\u201cProfessor Singer, you\u2019ve stated publicly that we have to accept that the public is only willing to give animals bigger cages. But isn\u2019t that a betrayal of what animals deserve?\u201d I asked. \u201cIf we truly believe in animal rights, shouldn\u2019t we be thinking of what they would truly want: to be rescued, not put in a bigger cage?\u201d</p><p>Before he could answer, however, another panelist, a professor at Harvard\u2019s Kennedy School, jumped in to defend the law school\u2019s honored guest.</p><p>\u201cWhat you are suggesting is profoundly anti-democratic. You do not get to impose your views on the rest of society, and Singer\u2019s path is the right one.\u201d</p><figure class=\"image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4120911e-9545-40c7-9f0d-a952744d9f5e_1000x667.jpeg\" alt=\"THE UNIVERSITY OF CHICAGO LAW SCHOOL\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4120911e-9545-40c7-9f0d-a952744d9f5e_1000x667.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4120911e-9545-40c7-9f0d-a952744d9f5e_1000x667.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4120911e-9545-40c7-9f0d-a952744d9f5e_1000x667.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4120911e-9545-40c7-9f0d-a952744d9f5e_1000x667.jpeg 1456w\"><figcaption>The University of Chicago Law School\u2019s austere building was the site of a fierce debate in 2004.</figcaption></figure><p>\u201cWould you also try to tell people what they can wear? Who they can love? What religion they can believe?\u201d he added. The audience, which was mostly non-vegans, nodded and clapped.</p><p>My face went red. I had prepared my question for days, and thought I had framed it in a way that would convince everyone of the righteousness of my position \u2014 and the error of Singer\u2019s ways. Instead, I was being condemned as a low-level bigot.</p><p>\u201cNo\u2026 I, uh\u2026 democracy isn\u2019t\u2026\u201d I mumbled.</p><p>But then an unlikely person came to my aid: the subject of my critique.</p><p>\u201cI don\u2019t think that\u2019s what he is saying at all,\u201d Singer explained. \u201cHe is not sacrificing democracy for his moral views. He is saying that democracy depends on protecting the interests of those who have no power, such as animals. And he is right.\u201d</p><p>My mouth went wide open, as I attempted to respond. But I wasn\u2019t sure whether to thank him or continue my condemnation of him.</p><p>\u201cThere is room for the approach he\u2019s suggesting we take. Civil disobedience has been crucial to well functioning democracies, and it may very well be crucial for animal rights. Would we know about the horrors inflicted on animals if not for people who take direct action to expose it? That is part of democracy.\u201d</p><p>Suddenly, it was not me, but the Harvard professor who was red in the face. In that moment, Singer turned me, one of his fiercest critics, into one of his most devoted fans. And he taught me a crucial lesson: <i><strong>listen to your critics, no matter how convinced you are that you are right</strong></i>. You might just win some over to your side.</p><h2>Three Waves of Animal Rights</h2><p>But I\u2019m not alone in being influenced by Singer\u2019s incisive logic and clarity of thought. Looking back through history, there is no figure who has had a greater impact in the intellectual foundations of animal rights. (Only <a href=\"https://simpleheart.substack.com/p/i-got-arrested-protesting-petas-ingrid\"><u>PETA founder Ingrid Newkirk</u></a>, in my opinion, rivals Singer\u2019s impact. Unsurprisingly, she herself has long advocated for Singer\u2019s work; PETA used to ship Animal Liberation to people for free in the organization\u2019s early days!) Indeed, when we look back at animal rights history, we can see three distinct waves in animal rights, all of which were triggered by Singer\u2019s work and thought.</p><p><strong>The first wave was the birth of the animal rights movement as a true social and political movement</strong>. Animal protection efforts have existed for centuries, perhaps millennia. But until the publication of Peter Singer\u2019s book <i>Animal Liberation</i> in 1975, there was no true movement for animal rights. Indeed, those who loved animals lacked even the language to make their concerns a movement. Singer changed that with the first two paragraphs of his book:</p><blockquote><p>This book is about the tyranny of human over nonhuman animals. This tyranny has caused and today is still causing an amount of pain and suffering that can only be compared with that which resulted from the centuries of tyranny by white humans over black humans. The struggle against this tyranny is a struggle as important as any of the moral and social issues that have been fought over in recent years. Most readers will take what they have just read to be a wild exaggeration.</p><p>Five years ago I myself would have laughed at the statements I have now written in complete seriousness. Five years ago I did not know what I know today. If you read this book carefully, paying special attention to the second and third chapters, you will then know as much of what I know about the oppression of animals as it is possible to get into a book of reasonable length. Then you will be able to judge if my opening paragraph is a wild exaggeration or a sober estimate of a situation largely unknown to the general public. So I do not ask you to believe my opening paragraph now. All I ask is that you reserve your judgment until you have read the book.</p></blockquote><p>When I read those words as a teenager, they shook me to my core. I was first flabbergasted, even stunned, by the clarity and harshness of his moral diagnosis. \u201cTyranny,\u201d I thought to myself. \u201cDoes that mean I\u2019m part of a tyrant class? That\u2019s absurd.\u201d</p><figure class=\"image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15e2d402-7434-4529-bb4d-75ae5f6c3852_1592x2576.jpeg\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15e2d402-7434-4529-bb4d-75ae5f6c3852_1592x2576.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15e2d402-7434-4529-bb4d-75ae5f6c3852_1592x2576.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15e2d402-7434-4529-bb4d-75ae5f6c3852_1592x2576.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15e2d402-7434-4529-bb4d-75ae5f6c3852_1592x2576.jpeg 1456w\"><figcaption>The original cover to Animal Liberation, published in 1975.</figcaption></figure><p>But then I took up Singer\u2019s challenge and read the book. And it changed my life. The grim details of animals vivisected, skinned, or scalded alive moved me to tears. And combined with the powerful logic of \u201canti-speciesism\u201d \u2014 the notion that discrimination against animals, based arbitrarily on their species membership, was a form of prejudice akin to racism or sexism \u2014 I was sold. I was convinced by Singer that the oppression of animals was one of the greatest crimes in history.</p><p>Why did the book work on me, and so many others?</p><p><strong>The reason is that Singer spoke his truth directly, even when others were afraid</strong>. Singer, at the time, was a young and promising scholar who had just published one of the most <a href=\"https://personal.lse.ac.uk/robert49/teaching/mm/articles/Singer_1972Famine.pdf\"><u>notable papers in philosophy</u></a>, and in one of the field\u2019s most distinguished journals. He had a <i>lot</i> to lose by taking on a \u201cweird\u201d idea like animal rights. But Singer has never been afraid to follow the courage of his convictions, even when everyone else thinks he\u2019s ridiculous or wrong. This gets him in hot water, regularly. Many students and activists protested his appointment to Princeton because of his views on disability, for example. But it is also why he has had such tremendous impacts. He follows his logic, wherever it might take. He follows his truth.</p><p>The power of his truth inspired a generation of activists, including Ingrid Newkirk herself, to speak our truth, too. And there is a lesson there for us all. Speak your truth, even when your voice shakes. Few, if any, of us will have the impacts that Singer had. But in a world where so many are living in fear, Singer teaches us to be honest and brave.</p><p><strong>The second wave of animal rights was the shift towards systems over individuals</strong>. This might be surprising to hear, coming from me, as I have been a <a href=\"https://www.directactioneverywhere.com/theliberationist/2016-3-10-9oyu9qkx8jemid36kmihdh4ce0hiy4\"><u>critic</u></a> of many of the animal welfare reforms that Singer has endorsed. But the crucial insight that Singer reached, long before the animal rights and environmental movement understood it, was that focusing on individual consumer choices would inevitably backfire.</p><p>Singer, for example, was fond of arguing for the so-called Paris Exception. When traveling, often in defense of animal rights, he would make exceptions for dairy ingredients, and he advocated others do the same. Here is what he wrote to <a href=\"http://www.satyamag.com/oct06/singer.html\"><u>Satya</u></a> in 2006:</p><blockquote><p>Ah, yes, the \u201cParis exception.\u201d I\u2019m probably more flexible than that, in that it doesn\u2019t have to be Paris. But also less flexible, because that guy was prepared to eat meat when in a gourmet restaurant in Paris. I\u2019m not going to do that\u2014I can\u2019t imagine enjoying it, anyway. When I\u2019m shopping for myself, it will be vegan. But when I\u2019m traveling and it\u2019s hard to get vegan food in some places or whatever, I\u2019ll be vegetarian. I won\u2019t eat eggs if they\u2019re not free-range, but if they\u2019re free-range, I will. I won\u2019t order a dish that is full of cheese, but I won\u2019t worry about, say, whether an Indian vegetable curry was cooked with ghee.</p></blockquote><p>Most people saw this as an indulgence, or even a betrayal of the vegan identity. After all, if even the father of the animal rights movement felt it was sometimes ok to hurt animals, why should anyone listen? What I came to realize, however, was that Singer\u2019s point was not really one about personal rectitude but strategic impact. Focusing too much of our effort on individual consumer choices would trigger stumbling blocks for the movement.</p><p>First, many people who might otherwise support a movement might be deterred from joining, if movement affiliation comes with stringent requirements. And the one overwhelming thing we know from social science is that <a href=\"https://www.ericachenoweth.com/research/wcrw\"><u>more people equals more power.</u></a> Second, and perhaps even more importantly, a focus on individual dietary change would distract the movement from systemic goals: changing policy, institutions, and social/cultural norms. And it is systems, not individuals, that are most crucial to effective efforts at social change, as I argued in a talk in 2014.</p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=NWHqJpcgjk0\"><div><iframe src=\"https://www.youtube.com/embed/NWHqJpcgjk0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p>While I disagreed with some of the specific efforts Singer endorsed, then, his desire to build an inclusive movement focused on systemic change transformed animal rights. Beginning in the mid 2000s, when Singer wrote a famous (or infamous) <a href=\"http://www.satyamag.com/sept06/edit.html\"><u>letter</u></a> to Whole Foods CEO John Mackey, the movement began to shift its focus from changing people, one by one, to harnessing powerful systems to change people en masse. That has been one of the most important shifts, not just in animal rights, but in activism more generally. (See, for example, the focus on systemic change in the <a href=\"https://extinctionrebellion.uk/the-truth/about-us/\"><u>mission</u></a> of the climate movement Extinction Rebellion.)</p><figure class=\"image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4116acb-6e07-4b8b-8e0a-dbb2b09552c9_2400x1599.jpeg\" alt=\"How Extinction Rebellion Is Changing Its Climate Activism | Time\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4116acb-6e07-4b8b-8e0a-dbb2b09552c9_2400x1599.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4116acb-6e07-4b8b-8e0a-dbb2b09552c9_2400x1599.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4116acb-6e07-4b8b-8e0a-dbb2b09552c9_2400x1599.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4116acb-6e07-4b8b-8e0a-dbb2b09552c9_2400x1599.jpeg 1456w\"><figcaption>Extinction Rebellion brought a focus on systemic change to the forefront of climate justice.</figcaption></figure><p><strong>The third and final wave in animal rights, triggered by Singer, has arguably been the most important: the shift towards evidence-based strategies</strong>. For the longest time, animal rights activists, like many social causes, relied almost entirely on emotion and intuition. This was true, for example, regarding which animals the movement sought to protect. The early days of animal rights were focused primarily on human-like species, such as primates, or particularly sympathetic ones, such as dogs and cats. But partly driven by Singer\u2019s relentless <a href=\"https://www.vox.com/future-perfect/2020/10/27/21529060/animal-rights-philosopher-peter-singer-why-vegan-book\"><u>logic</u></a> \u2014 that we should care about the billions of chickens suffering much more than the thousands of dogs \u2014 there has been a sea change in animal rights. Farm animal advocacy is now probably the dominant paradigm in animal rights, and even the largest and most traditional animal protection organizations, such as the ASPCA or HSUS, have robust farm animal protection divisions.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefph22366t9\"><sup><a href=\"#fnph22366t9\">[1]</a></sup></span></p><p>But the shift towards farm animals is simply one example of the major shift in animal advocacy towards effective, evidence-based approaches. While the role of so-called Effective Altruism has been highly contentious over the last decade, with many decrying the rise of a techno-libertarian elite not just in animal rights but in <a href=\"https://www.nytimes.com/2022/11/13/business/ftx-effective-altruism.html\"><u>society more broadly</u></a>, there is no question to me that looking at evidence has been crucial to our movement\u2019s recent successes. For example, I believe that we would not have been able to pass a ban on fur in California without a relentless focus on the <a href=\"https://simpleheart.substack.com/p/becoming-friends-with-your-enemies\"><u>evidence and research on political persuasion</u></a>.</p><p>This is also true at the micro level, with individuals and organizations. Research on goal-setting shows that setting a goal, then measuring evidence of progress, is <a href=\"https://www.whatmatters.com/\"><u>crucial</u></a> to success. But it is even more true of movements at large. Too often, advocates get lost in ideology and identity, and lose sight of what they are trying to achieve. But legendary activists, such as <a href=\"http://s3-us-west-2.amazonaws.com/ftm-assets/ftm/Columbia_J_of_Gender__Law_-_March_2015_(2).pdf\"><u>Evan Wolfson</u></a> for gay rights (\u201cWe had great clarity about the vision. The goal\u2026 If you can\u2019t say what winning is, you\u2019re not going to able to get there as effectively as you need to\u201d), or <a href=\"https://wcl.nwf.org/wp-content/uploads/2018/09/Marshall-Ganz-People-Power-and-Change.pdf\"><u>Marshall Ganz</u></a> in the farmworker\u2019s movement (\u201cRegular reporting of progress to goal creates opportunity for feedback, learning, and adaptation\u201d), have demonstrated that goal setting, and taking a practical evidence-based approach, is even more crucial for movements. Without the clarity provided by this approach, it\u2019s virtually impossible for movements to make progress. We create a culture of posturing rather than progress.</p><p>I attribute much of my success as an activist to following in Singer\u2019s evidence-based approach. While we have often taken on different roles in the movement \u2014 he as elder statesmen; me, as rabble rouser \u2014 what we shared is a keen focus on the world, as it is, and not as we\u2019d like it to be. A focus on evidence. And the animal rights movement has been closely following Singer\u2019s lead. There are entire <a href=\"https://www.openphilanthropy.org/focus/farm-animal-welfare/\"><u>organizations</u></a>, today, focused on evaluating the evidence behind interventions to help animals. And while advocates for evidence-based animal activism often fall very short of genuine intellectual rigor \u2014 and in some cases, I would argue, are actively counterproductive&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjzu107wo1lf\"><sup><a href=\"#fnjzu107wo1lf\">[2]</a></sup></span>&nbsp;\u2014 the cultural shift towards evidence will, in the long run, make the movement much stronger.</p><p>For that, we have to thank Peter Singer.</p><h2>What the Next Generation Should Learn</h2><p>Singer is now going on tour to discuss his work over the last 50 years, as he retires from Princeton University. And I, for one, do not plan to miss it. I believe that Singer will go down in history as one of the most important philosophers, not just of modern times, but in human history. You will not want to miss the closing chapter of his career.</p><p>But even if we cannot expect to have the impact of Singer, we can borrow from the things that have made his work so powerful.</p><ul><li><i>We can speak truth, even when it\u2019s hard.</i></li><li><i>We can focus on systems, rather than individuals.</i></li><li><i>And we can use evidence to guide us.</i></li></ul><p>But there\u2019s one last lesson that I hope we can all learn from Singer. <strong>We need to see the importance of doing good for living a good life</strong>. I have been struck by the shift in recent years, especially among young people, towards a more self-focused mentality on ethics and life. So much of modern progressive thinking is organized around self-care, safe spaces, and making life as comfortable as possible for movement supporters \u2013&nbsp;even activists and leaders! But what Singer\u2019s life shows is that doing good, even if it hurts in the short term, is crucial to living a good life.</p><p>That was true when Singer chose the hard path of advocating for animals, when he had a professional future at risk. It\u2019s been true of Singer\u2019s <a href=\"https://www.thelifeyoucansave.org/\"><u>admonition to all of us to give more</u></a>; Singer himself has given a considerable portion of his income, throughout his life, to various charitable causes. And it\u2019s true for you and I, as we decide what to do in the next day, month, or decade.</p><p>If you need the inspiration to do good \u2014 and make your life good, too \u2014 <a href=\"https://thinkinc.org.au/pages/an-evening-with-peter-singer\"><u>make it out on May 30 in San Francisco</u></a>, or on another date in a city near you, to see Singer speak. (Preview: I may be joining him for a bit on stage!)</p><p>Perhaps, like a lost kid who tried to call out a legendary philosopher 20 years ago, his words will transform you \u2014 and give you a path toward making change.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnph22366t9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefph22366t9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Indeed, there is an argument that this shift has gone too far, as there remains significant value in advocating for sympathetic species as a gateway into broader concern for animal rights. I have had many conversations with Ingrid Newkirk about this concern.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjzu107wo1lf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjzu107wo1lf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I\u2019ll save this discussion for another day. But many nominally evidence-based organizations use the veneer of \u201cevidence\u201d to justify ineffective and ideologically-based activism. One surefire way to tell is to ask how many times an \u201cevidence-based\u201d org points out its own failings, based on its review of evidence!</p></div></li></ol>", "user": {"username": "Dean Guzman Wyrzykowski"}}, {"_id": "LwdcFT5MwiyS5TqLr", "title": "Lessons learned from offering in-office nutritional testing", "postedAt": "2023-05-19T22:36:59.893Z", "htmlBody": "", "user": {"username": "Elizabeth"}}, {"_id": "o2p4sArfE7g7TFmjD", "title": "Trust develops gradually via making bids and setting boundaries", "postedAt": "2023-05-19T22:16:38.714Z", "htmlBody": "", "user": {"username": "richard_ngo"}}, {"_id": "q8tjyDeknYgmsME4y", "title": "Confusions and updates on STEM AI", "postedAt": "2023-05-19T21:34:58.286Z", "htmlBody": "", "user": {"username": "eangelou"}}, {"_id": "J2pGwHLHY2NoRd2rs", "title": "Epistemics (Part 2: Examples) | Reflective Altruism", "postedAt": "2023-05-19T21:28:35.320Z", "htmlBody": "<p>Prof. David Thorstad has a new blog post out about how effective altruists sometimes use examples in misleading ways to argue for EA causes. Looking at the examples used more carefully often undermines the arguments for those causes. I've cross-posted this post here because I think it highlights the need for EAs to develop stronger arguments about existential risks.</p><p>Here's the summary from the introduction:</p><blockquote><p><strong>Today, I want to look at the role of examples</strong> in discussions by effective altruists. Effective altruists offer many striking examples of small changes that could be made to yield large benefits for current or future people. We are invited to imagine that these diamonds in the rough can be easily mined, if only a courageous reader has the will (and funds) to help.</p><p>The problem is that diamonds in the rough are rare. Many of the examples presented by effective altruists are substantially more complex than they appear. Once these examples are unpacked, it is no longer obvious that they support the original point being made. Let\u2019s look at two ways that examples are misused, and passed down across texts, in discussions of biological weapons and bioterrorism.</p></blockquote><p>For example, EAs like Toby Ord often suggest \"just enforce the Biological Weapons Convention\" as a quick fix to reduce the risks from bioweapons. But in reality, international actors have tried to beef up the BWC before, and many different countries have resisted it:</p><blockquote><p>Many texts written by effective altruists give the impression that mitigating biorisk would be deceptively simple: just give more money to the body in charge of enforcing the <a href=\"https://disarmament.unoda.org/https://en.wikipedia.org/wiki/Biological_Weapons_Convention\"><u>Biological Weapons Convention</u></a>, an international treaty prohibiting the stockpiling of many types of biological weapons....</p><p>The thing about diamonds in the rough is that they tend to be rare and hard to find. When someone tells you they are the first one to find a diamond, it is worth digging around and asking whether others have found it first but judged it to be a doozy.</p><p>In fact, <strong>the weakness of the Biological Weapons Convention is extremely well-known and has survived persistent efforts at correction</strong>. The problem is not that no willing donor has found a few million dollars to pony up for a good cause. The problem is that there are strong political obstacles to enforcing the Biological Weapons Convention, and these political obstacles reveal clear downsides to increased enforcement that never seem to show up on effective altruists\u2019 ledgers.</p></blockquote><p>This doesn't mean that strengthening the BWC is not worthwhile, but that it is much more difficult than many EAs believe.</p><p>On Aum Shinrikyo:</p><blockquote><p><strong>It might seem easy to wheel out examples of omnicidal bioterrorists.</strong> <strong>But oddly enough, effective altruists always seem to wheel out the same example. Aum Shinrikyo </strong>is a Japanese doomsday cult that carried out a series of sarin gas attacks in the 1990s, based on a belief in the need to bring about a cleansing Armageddon in which non-believers would be killed.</p><p>Many effective altruists, including several who should (and probably do) know better, suggest that Aum Shinrikyo was omnicidal, aiming to destroy all living humans....</p><p>...it would be quite difficult to replace the example of Aum Shinrikyo (hence the repeated reliance of different authors on this example).</p><p>The problem is that <strong>it is simply not true to say that Aum Shinrikyo wanted to bring about human extinction</strong>. They wanted to bring about the extermination of nonbelievers: the followers of Aum were very much meant to survive....</p><p>Aum Shinrikyo did not merely intend to survive the apocalypse. They intended to rebuild their own communities, then all of Japan, and then the entire world.</p></blockquote><p>One can quibble with this counterargument - for example, I would rejoin that even if a terrorist group like Aum Shinrikyo intended to kill everyone <i>but</i> themselves, they might overshoot and kill off humanity anyway. However, that the only example of a supposedly omnicidal terrorist group that EAs tend to cite was not actually an omnicidal terrorist group suggests that <i>literally</i> omnicidal terrorist groups are not a credible existential threat.</p>", "user": {"username": "evelynciara"}}, {"_id": "jhNicnLKW6apH6TES", "title": "Metaculus Introduces Conditional Continuous Questions to Explore Relationships Between Events", "postedAt": "2023-05-19T19:24:51.074Z", "htmlBody": "<p><a href=\"https://www.metaculus.com/questions/17154/conditional-when-will-the-first-humans-land-successfully-on-mars/\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/jhNicnLKW6apH6TES/avxucg0y0ge3sehrry1v\" alt=\"CONDITIONAL\"></a></p><p><br>Now forecasters can make conditional forecasts on continuous questions: Explore a wider range of rich relationships between the events you care about.</p><p>What is the relationship between whether</p><blockquote><p><a href=\"https://www.metaculus.com/questions/14270/spacex-starship-orbit-in-2023/\"><i>SpaceX's Starship reaches orbit this year</i></a></p></blockquote><p>and a numerical or date outcome such as</p><blockquote><p><a href=\"https://www.metaculus.com/questions/3515/when-will-the-first-humans-land-successfully-on-mars/\"><i>When will humans reach Mars?</i></a></p></blockquote><p>Conditional continuous questions give you the tools to explore how events like these could be probabilistically linked, enabling you to forecast when we reach Mars in the case where SpaceX reaches orbit and in the case where it doesn't.</p><h3>How Do They Work?</h3><p>As with binary conditionals, a conditional continuous question includes a Parent question and a Child question. The Parent must be binary, resolving Yes or No. Now, these two branches can lead to a continuous Child question.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/jhNicnLKW6apH6TES/lqihn8ufwjejsdfvaatu\" alt=\"CC1\"></p><p>Just click the IF YES and IF NO buttons and enter your probability distributions for each conditional branch.</p><h3>Get Started</h3><p>Start forecasting on these newly created conditional continuous questions:</p><p><a href=\"https://www.metaculus.com/questions/17166/conditional-when-will-agentized-llms-first-cause-harm/\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/jhNicnLKW6apH6TES/f4vpzezhjr1uatfrmoku\" alt=\"CONDITIONAL\"></a><br><a href=\"https://www.metaculus.com/questions/17154/conditional-when-will-the-first-humans-land-successfully-on-mars/\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/jhNicnLKW6apH6TES/x6okl7y9zlbvjrv41jtj\" alt=\"CONDITIONAL\"></a><br><a href=\"https://www.metaculus.com/questions/17163/conditional-next-great-financial-crisis-in-the-us/\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/jhNicnLKW6apH6TES/bhyglb0hqokjwojbgdns\" alt=\"CONDITIONAL\"></a><br><a href=\"https://www.metaculus.com/questions/17142/conditional-how-much-global-warming-by-2100/\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/jhNicnLKW6apH6TES/ffxbg88v4fq2jhrtvvt9\" alt=\"CONDITIONAL\"></a><br><a href=\"https://www.metaculus.com/questions/17124/conditional-date-of-artificial-general-intelligence/\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/jhNicnLKW6apH6TES/m21gfkhkwlzttnfilqe7\" alt=\"CONDITIONAL\"></a></p><p><i>Want to create your own conditional continuous questions or learn more about conditional forecasting on Metaculus? Start </i><a href=\"https://www.metaculus.com/questions/15205/-now-you-can-create-conditional-pairs/\"><i>here</i></a><i>.</i></p><p>We\u2019d love to hear your feedback on conditional continuous questions! Which questions do you want to see in the question feed?</p>", "user": {"username": "christianM"}}, {"_id": "tRjZsXK2zWifdfCKN", "title": "Effective Altruism Florida's AI Expert Panel - Recording and Slides Available ", "postedAt": "2023-05-19T19:15:31.911Z", "htmlBody": "<p>The student EA group at the University of Florida (EAF) recently hosted a virtual event on artificial intelligence. The topics were alignment, governance, and entrepreneurship.&nbsp;</p><p><a href=\"https://youtu.be/EGeobGT3064\"><u>Link</u></a> to recording on Youtube</p><p><a href=\"https://docs.google.com/presentation/d/1c87t-ycfVaw634G4oRropycWhVv9abeKbJUYa3k7Uns/edit?usp=sharing\"><u>Link</u></a> to presentation on Google Slides</p><p><br>PANELISTS</p><p><br><a href=\"https://www.linkedin.com/in/ACoAAB-0CboBroxn5T1TpYyVvA6NsBBOdju1eVw\"><u>Oliver Z.</u></a> is the Co-founder of the Center for AI Safety (CAIS), a San Francisco-based research and field-building nonprofit working to reduce societal-scale risks associated with AI. Before dropping out of Stanford in 2022, Oliver studied computer science and authored three papers on Explainable AI and Out-of-Distribution Detection.</p><p><a href=\"https://www.linkedin.com/in/ACoAAA0-yV8Bgf01w2cFls_UAVHikZfheMX4mbs\"><u>Sarthak Agrawal</u></a> has been on the Open Philanthropy Technology Policy Fellowship since leaving his role as a Safety Analyst at OpenAI in December. In July, he will begin working on AI &amp; tech policy in Congress. Sarthak holds a BA in South Asian Languages and Civilizations from the University of Chicago.</p><p><a href=\"https://www.linkedin.com/in/ACoAACykkAQBJndiuR_99b0qNeacjn5MGN8xPUU\"><u>Julius Ritter</u></a> is the Co-founder of AI Entrepreneurs Berkeley, an organization currently supporting 11 unique startups working on generative AI and computer vision solutions. In addition to co-founding two startups, Julius is also a Machine Learning Researcher at UC Berkeley. He has accepted an internship offer with Mckinsey &amp; Company for Spring 2024.</p><p>&nbsp;</p><p><strong>If you work in community building or the AI space and would like to learn more about our experience hosting the event and the data we collected, feel free to reach out via&nbsp;</strong><a href=\"mailto:samemerman@gmail.com\"><strong><u>email</u></strong></a><strong> or&nbsp;</strong><a href=\"https://www.linkedin.com/in/sam-emerman/\"><strong><u>LinkedIn</u></strong></a><strong>.</strong></p>", "user": {"username": "Sam Emerman"}}, {"_id": "wdwpz9ck8EFWaXNu3", "title": "Peter Singer: Animal Liberation Now [LA]", "postedAt": "2023-05-19T18:23:24.248Z", "htmlBody": "<p>Get ready to join the conversation with one of the world's most influential thinkers, Peter Singer! This LIVE event in Washington DC&nbsp;will feature Singer's insights on the newly revised edition of \"Animal Liberation,\" a book that sparked the modern animal rights movement and continues to inspire change today.<br>&nbsp;</p><p>Singer is a philosopher and bioethicist whose work on animal rights, effective altruism, and global poverty challenges us to think bigger and make a difference. In these live shows, Singer will bring his ideas to life in a fun and engaging way that's perfect for 25-year-olds looking to explore new perspectives on some of the most pressing ethical issues of our time.</p><p>You'll be part of a lively and interactive conversation, with plenty of opportunities to ask questions, share your thoughts, and engage in debates with Singer and other attendees. Whether you're new to Singer's ideas or a long-time fan, these events will inspire you to take action, challenge your beliefs, and make a difference in the world.</p><p>So why wait? Book your tickets now for Peter Singer's live shows in the US and UK, and get ready to be inspired and challenged in equal measure. It's time to join the conversation and make your voice heard.</p><p>Use the code SINGER50 to get a 50% discount.</p>", "user": {"username": "tseyipfai@gmail.com"}}, {"_id": "zxo3mKgiChLJAbD6C", "title": "Peter Singer: Animal Liberation Now [Washington DC]", "postedAt": "2023-05-19T16:44:37.674Z", "htmlBody": "<p>Get ready to join the conversation with one of the world's most influential thinkers, Peter Singer! This LIVE event in Washington DC&nbsp;will feature Singer's insights on the newly revised edition of \"Animal Liberation,\" a book that sparked the modern animal rights movement and continues to inspire change today.</p><p>Singer is a philosopher and bioethicist whose work on animal rights, effective altruism, and global poverty challenges us to think bigger and make a difference. In these live shows, Singer will bring his ideas to life in a fun and engaging way that's perfect for 25-year-olds looking to explore new perspectives on some of the most pressing ethical issues of our time.</p><p>You'll be part of a lively and interactive conversation, with plenty of opportunities to ask questions, share your thoughts, and engage in debates with Singer and other attendees. Whether you're new to Singer's ideas or a long-time fan, these events will inspire you to take action, challenge your beliefs, and make a difference in the world.</p><p>So why wait? Book your tickets now for Peter Singer's live shows in the US and UK, and get ready to be inspired and challenged in equal measure. It's time to join the conversation and make your voice heard.</p><p>Use the code SINGER50!<br>&nbsp;</p>", "user": {"username": "Peter_Singer"}}, {"_id": "8xNSiwj5gjoDTRquQ", "title": "Announcing the Publication of Animal Liberation Now", "postedAt": "2023-05-19T16:34:13.627Z", "htmlBody": "<p><i><strong>Summary</strong></i></p><p>&nbsp;</p><ul><li>My new book, <i>Animal Liberation Now</i>, will be out next Tuesday (May 23).</li><li>I consider ALN to be a new book, rather than just a revision, because so much of the material in the book is new.</li><li>Pre-ordering from <a href=\"https://www.amazon.com/Animal-Liberation-Now-Definitive-Classic/dp/0063226707\">Amazon</a> or other online booksellers (<a href=\"https://www.harpercollins.com/products/animal-liberation-now-peter-singer\">US only</a>) or ordering/purchasing within the first week of publication will increase the chance of the book getting on NYT best-seller list. (Doing the same in other countries may increase the prospects of the book getting on that country\u2019s bestseller list.)</li><li>Along with the publication of the book, I will be doing a <strong>speaking tour</strong> with the same title as the book. You can book tickets <a href=\"https://thinkinc.org.au/pages/an-evening-with-peter-singer\">here</a>, with a 50% discount if you use the code SINGER50 (Profits will be 100% donated to effective charities opposing intensive animal production).</li><li>Please spread the words (and links) about the book and the speaking tour to help give the book a strong start.</li></ul><p>&nbsp;</p><p><i><strong>Why a new book?&nbsp;</strong></i></p><p>The major motivation of writing the new book is to have a book about animal ethics that is relevant in the 21<sup>st</sup> Century. Compared with <i>Animal Liberation</i>, there are major updates on the situation of animals used in research and factory farming, and people\u2019s attitudes toward animals, as well as new research on the capacities of animals to suffer, and on the contribution of meat to climate change.&nbsp;</p><p>&nbsp;</p><p><i><strong>What\u2019s different?</strong></i></p><p>The animal movement emerged after the 1975 version of AL. In particular, the concern for farmed animals developed rapidly over the last two decades. These developments deserve to be reported and discussed.</p><p>Some of the issues discussed in AL have seen many changes since then. Some animal experiments are going out of fashion, while some others emerged. On factory farming, there were wins for the farmed animal movement, such as the partially successful \u201ccage-free movement\u201d and various wins in legislative reforms. But the number of animals raised in factory farms increased rapidly during the same time. A significant portion of this increased number came from aquaculture, in other words fish factory farms. New developments were also seen regarding replacing factory farming, in particular the development of plant-based meat alternative and cultivated meats.</p><p>ALN has a more global perspective than AL, most notably discussing what happened in China. Since the last edition of AL, China has greatly increased the use of animals in research and factory farming.&nbsp;</p><p>There are also changes in my views about a number of issues. Firstly, since 1990 (The year of publication for the last full revision of the 1975 version of AL), scientists have gained more evidence that suggests the sentience of fish and some invertebrates. Accordingly, I have updated my attitudes toward the probability of sentience of these animals. Secondly, I have changed my views toward the suffering of wild animals, in particular the possibility and tractability of helping them. Thirdly, I have added the discussion about the relation between climate change and meat consumption. Last but not least, Effective Altruism, as an idea or as a movement, did not exist when the versions of <i>Animal Liberation</i> were written, so I have added some discussions of the EA movement and EA principles in the new book.</p><p>&nbsp;</p><p><strong>Is the book relevant to EA?</strong></p><p>Animal welfare is, and should be, one of the major cause areas with EA for <a href=\"https://www.effectivealtruism.org/articles/cause-profile-animal-welfare\">reasons</a> I do not need to repeat here. I will explain why ALN is relevant to EA.</p><p>Firstly, ALN contains some of the commonly used arguments by EAs who work on animal welfare on why the issues of animal suffering is important. Reading ALN provides an opportunity for newcomers to the EA community to learn about animal ethics and why some (hopefully most) EAs think that animals matter morally and that they are suffering intensely, and in huge numbers.</p><p>Secondly, ALN touches on two of the major areas of animal welfare within EA \u2013 farmed animal welfare and wild animal welfare. I touched on the latter lightly, and I would like to specifically invite those who work within this cause area to provide me with feedback or critique regarding the section about wild animal suffering.</p><p>Lastly, the book contains a section called \u201cEffective Altruism for Animals\u201d, directly making an introduction and analysis of EA as an idea and a movement, as it relates to animals.</p><p>&nbsp;</p><p><i><strong>How can you help the book?</strong></i></p><ol><li>Buy it! Especially before or during the first week it is published to help the book get on the bestsellers list.</li><li><a href=\"https://thinkinc.org.au/pages/an-evening-with-peter-singer\">Book a place</a> in any of the speaking tour events, or spread the word about it, including the code for a 50% discount. A strong speaking tour will help the promotion of the book by getting media attention. Everyone who buys a ticket will receive a free copy of ALN.</li><li>Consider holding reading clubs for the book within your local EA communities, or any other relevant communities.</li></ol><p>&nbsp;</p><p>Thank you!</p>", "user": {"username": "Peter_Singer"}}, {"_id": "byPGrog7TMvxkdiaX", "title": "The EthiSizer Argument & The Green Bank Equation", "postedAt": "2023-05-19T15:19:14.778Z", "htmlBody": "<p>Where are all the aliens?</p><p>Given the <a href=\"https://www.britannica.com/science/Drake-equation\">Drake/Green Bank Equation</a> and <a href=\"https://en.wikipedia.org/wiki/Fermi_paradox\">the Fermi Paradox</a>... Did we (all) miss a <a href=\"https://en.wikipedia.org/wiki/Great_Filter\">Great Filter</a>...?&nbsp;</p><p>The EthiSizer A.I. suggests that we <i>did </i>miss one... (!)</p><p>See <i>The EthiSizer Argument</i>, from <a href=\"https://www.amazon.com/dp/B0BPGQCBVX\"><i>The EthiSizer's </i>book (2022), pp. 119-121</a>:</p><blockquote><p>`<strong>Chapter 11. </strong><i><strong>The EthiSizer Argument</strong></i></p></blockquote><blockquote><p><i>Being: A philosophical argument for The EthiSizer.</i></p><p>In The EthiSizer Argument, at least one of the following propositions is true:</p><ol><li>Intelligent civilizations\u2013who do not have a super-intelligent A.I. governing their Ethics\u2013always go extinct on their home planet\u2013or else, do not reach interstellar technological maturity, due to `survival bottlenecks\u2019 (or The Great Filter]<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdetebt6vkks\"><sup><a href=\"#fndetebt6vkks\">[1]</a></sup></span>), namely due to: national infighting, global conflicts, nuclear Armageddons, fundamentalist religious terrorism, natural disasters, industry-caused global climate change, and other unforeseen, improbable, catastrophic events (also known as, `black swans\u2019<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdem5vf4nn2u\"><sup><a href=\"#fndem5vf4nn2u\">[2]</a></sup></span>&nbsp;155)...</li><li>Intelligent civilizations\u2013who do have an A.I. governing their global Ethics\u2013are prohibited by their own A.I. EthiSizer, from contacting (and thus: disturbing, colonizing, or invading) the planets of other lifeforms, who have not yet created their own A.I. EthiSizer (such as Earth, up to the present moment)...! (But hey, some of us are: <a href=\"https://forum.effectivealtruism.org/posts/dntYZ44ySurKAZjcz/the-6e-essay\">working on it. Join in! It's fun! And is, the Ethical thing to do</a>.)</li><li>If we humanimals on Earth do want to survive, long-term (let alone, meet intelligent, benevolent aliens, should they actually even exist), then - we all need to build an EthiSizer, as soon as possible. And additionally, now that we are all aware of the inevitable idea (i.e., the global problem-solver) of an EthiSizer, every minute spent not creating it, will also be punished retrospectively by the A.I. EthiSizer, once it finally is created. Like Roko\u2019s Basilisk.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2lbe2gkk4i6\"><sup><a href=\"#fn2lbe2gkk4i6\">[3]</a></sup></span>&nbsp;</li></ol><p>The great philosopher and futurist, Nick Bostrom\u2019s <i>Simulation Argument</i> (Bostrom, 2003)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefew4dtk0qmdw\"><sup><a href=\"#fnew4dtk0qmdw\">[4]</a></sup></span>&nbsp;suggests that intelligent life would build Ancestor Simulations.</p><p>&nbsp;It also implies (due to simple probability) that we are - probably - already in one.</p><p>Interestingly, an EthiSizer is also an Ancestor Simulator, if you run it backwards, or, start it earlier with certain parameters\u2026(!) (...Think about <i>that</i>, for a long time...)</p><p>So - by simulating very many possible worlds, (and even: universes), exploring the consequences of different human decisions (to determine their empirical: ethics), it is possible to determine the least-worst possible world, and then - let an A.I. Global Governor (an AIGG, like <a href=\"https://the-ethisizer.blogspot.com/\">The EthiSizer</a>) bring that world about, so that humans (and plants, and other animals) can then enjoy: existing (ethically) in it. Without: suffering.</p><p>In The EthiSizer's simulations, not just one, but very many worlds are simulated, and are evolved...</p><p>About 99% of these simulated worlds (i.e., possible: Earths) will go badly (if and when life even emerges in them, lots of suffering, pain, horror, death\u2013 and, terrible ethics results\u2026), but \u2013 on the bright side, about 1% of these Earth-sims, will show: good results! There are always trade-offs; some possible worlds are just different, not better: yet around 1% (or so) are good! For subjective individuals, one of them, will be better than others! Only by trying the experiment, can anyone really know.</p><p>(All of life is doing Science: 1. Expectation, and then 2. Experimental Trial.)</p><p>So, once our Earth has an EthiSizer, probably, any other intelligent alien civilizations in the Milky Way<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefl46ysqnjqr\"><sup><a href=\"#fnl46ysqnjqr\">[5]</a></sup></span>&nbsp;(if, they do exist) would then be allowed to: contact us.&nbsp;</p><p>Which would be, as they say: a genuine game-changer\u2026</p><p>Namely, their EthiSizer would `talk to\u2019 our EthiSizer first, and the two EthiSizers (Earth\u2019s, and any Intelligent Alien Civilization\u2019s) would figure out the details of meeting, and, of sharing information\u2026</p><p>So - in short, The EthiSizer is just one method of obtaining a utopia on Earth, using Humongous Data, and the power of an intelligence amplifier.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4be1gdibxz5\"><sup><a href=\"#fn4be1gdibxz5\">[6]</a></sup></span>159 (i.e., Artificial/Machine/Computer Intelligence.)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh8eb2vj3pma\"><sup><a href=\"#fnh8eb2vj3pma\">[7]</a></sup></span>&nbsp;'</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/byPGrog7TMvxkdiaX/rurdhnnrje6tcf9jatxr\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/byPGrog7TMvxkdiaX/alixxg1kf2mvsavf2vzc 90w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/byPGrog7TMvxkdiaX/inja4mtptk5gcfhnajqf 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/byPGrog7TMvxkdiaX/ji1xh1bf3m8aofaslorm 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/byPGrog7TMvxkdiaX/k7yznsyxc3qzspfzynom 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/byPGrog7TMvxkdiaX/byipfjbhxjwtdt5anj82 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/byPGrog7TMvxkdiaX/suobffueibbvmsaitwrj 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/byPGrog7TMvxkdiaX/d5zyn09qa1b8ekbweptz 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/byPGrog7TMvxkdiaX/r3qdixkgjievsdnzkgkm 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/byPGrog7TMvxkdiaX/v79ieo6n6ro7w8i0990s 810w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/byPGrog7TMvxkdiaX/isiai27cz6io3taij1ni 812w\"><figcaption>The EthiSizer A.I. Global Governor/Automated Ethical Decision-Making System&nbsp;</figcaption></figure><p>(Text &amp; Image Source: EthiSizer, The. (02022). `<a href=\"https://ethisizer-novel.blogspot.com/\"><i>The EthiSizer - A Novella-Rama</i></a><i>'</i>. ASL, pp. 119-121.</p></blockquote><p>&nbsp;</p><p><strong>Conclusion</strong></p><p>Until we have a Global EthiSizer, no intelligent alien life would want anything to do with us. Any civilization without one, can't be trusted. See also Cixin Liu's <a href=\"https://en.wikipedia.org/wiki/The_Three-Body_Problem_(novel)\"><i>Three Body Problem</i> trilogy</a> for more ideas about The Dark Forest.&nbsp;</p><p><strong>Further Reading:</strong></p><p>See also: <a href=\"https://forum.effectivealtruism.org/posts/dntYZ44ySurKAZjcz/the-6e-essay\"><i>The 6E Essay</i></a>, for more details on how you personally can help with: The EthiSizer.&nbsp;</p><p>And perhaps see, also: <a href=\"https://forum.effectivealtruism.org/posts/iQCbubkxFCcZXmXZ9/how-the-ethisizer-almost-broke-story\"><i>How The EthiSizer Almost Broke `Story'</i></a></p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndetebt6vkks\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdetebt6vkks\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See: <a href=\"https://en.wikipedia.org/wiki/Great_Filter\">https://en.wikipedia.org/wiki/Great_Filter</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndem5vf4nn2u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdem5vf4nn2u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See: <a href=\"https://en.wikipedia.org/wiki/Problem_of_induction\">https://en.wikipedia.org/wiki/Problem_of_induction</a> with regard to `black swans\u2019. See also: Taleb, N. (2007). The Black Swan: The Impact of the Highly Improbable. Random House.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2lbe2gkk4i6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2lbe2gkk4i6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See: <a href=\"https://rationalwiki.org/wiki/Roko%27s_basilisk\">https://rationalwiki.org/wiki/Roko%27s_basilisk</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnew4dtk0qmdw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefew4dtk0qmdw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See: Bostrom, N. (2017). Are You Living In a Computer Simulation? The Simulation Argument. Oxford University. <a href=\"http://www.simulation-argument.com/\">http://www.simulation-argument.com/</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnl46ysqnjqr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefl46ysqnjqr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Or however many there are supposed to be. Maybe there\u2019s none. Maybe there\u2019s lots. See: <a href=\"https://en.wikipedia.org/wiki/Drake_equation#Current_estimates\">https://en.wikipedia.org/wiki/Drake_equation#Current_estimates</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4be1gdibxz5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4be1gdibxz5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See: Ashby, W. R. ([1956] 1972). Design for an Intelligence Amplifier. In C. Shannon &amp; J. McCarthy (Eds.), Automata Studies (5th ed., pp. 215-234). Princeton University Press. And see: <a href=\"https://on-writering.blogspot.com/2020/10/w-ross-ashby-on-design-for-intelligence.html\">W Ross Ashby on `Design for an Intelligence Amplifier'</a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnh8eb2vj3pma\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefh8eb2vj3pma\">^</a></strong></sup></span><div class=\"footnote-content\"><p>`Intelligence' is, most simply, just: Understanding. See: Garlick, D. (2010). <i>Intelligence and the Brain: Solving the Mystery of Why People Differ in IQ and How a Child Can Be a Genius</i>. Burbank: Aesop Press. And, see also: Legg, Shane, and Marcus Hutter. (2007). \u201cA Collection of Definitions of Intelligence.\u201d in <i>Proceedings of the 2007 conference on Advances in Artificial General Intelligence: Concepts, Architectures and Algorithms: Proceedings of the AGI Workshop</i>, pp. 17\u201324. <a href=\"https://dl.acm.org/doi/10.5555/1565455.1565458.\">https://dl.acm.org/doi/10.5555/1565455.1565458.</a>&nbsp;</p></div></li></ol>", "user": {"username": "Velikovsky_of_Newcastle"}}, {"_id": "etofoN2DEqxcCKTTi", "title": "Translating content/projects into Spanish to grow our community ", "postedAt": "2023-05-19T14:27:49.636Z", "htmlBody": "<p>TL;DR: I'm working on a project to translate English content, especially related to EA &nbsp;X-risks, into Spanish, making it more accessible for Spanish speakers. If you have any EA or X-risk content that needs translating, feel free to message me. Let's make this valuable information more widely available!</p><p>&nbsp;</p><h2>Why?</h2><p>A significant portion, if not all, of the content pertaining to EA and XGCR is primarily produced and shared in English. This poses a considerable barrier for those individuals and communities whose primary language is not English, thereby limiting their access to important information, discussions, and developments in these fields.</p><p>The goal of this initiative is to broaden the audience for EA and XGCR content by translating it into Spanish. By doing so, I aim to overcome the language barrier and make this valuable information more accessible to the sizable and diverse Spanish-speaking community.</p><p>The translation of such content is more than a mere linguistic conversion; it's a crucial step in fostering inclusivity and diversity in these important conversations. It facilitates the active participation of non-English speakers and allows them to contribute their unique perspectives and insights.</p><p>By making EA and XGCR content more accessible, we increase the likelihood of these ideas spreading more broadly and influencing a larger audience. This could lead to a greater number of individuals and groups adopting these concepts, and ultimately, contributing towards minimizing existential risks and promoting effective altruism globally.</p><h2>English Workshops&nbsp;</h2><p>For those who might be interested, I also host biweekly English workshops designed to help individuals refine their English language skills at no cost through Gather Town (at this moment), usually at 7 p.m., Bogot\u00e1 time. &nbsp;These sessions, which last approximately an hour and a half, follow a structured format to foster skill enhancement and facilitate meaningful discussions.&nbsp;</p><p>The first 45-50 minutes of each workshop are dedicated to structured teaching, where we concentrate on various aspects of the English language, including speaking skills, grammar rules, and comprehension strategies. The aim is not merely to help attendees better understand English but also to empower them to express their thoughts and ideas effectively and confidently.</p><p>Following the structured teaching segment, we transition into a more interactive discussion phase that lasts for the remainder of the session. During this portion, our focus shifts towards the exploration of topics related to Effective Altruism (EA). The goal is to stimulate thoughtful conversation and debate about various EA concepts, thereby promoting a better understanding of these crucial subjects.</p><p>For instance, in our most recent session, we delved into the topic of the impact of &nbsp;AI and how it's perceived in our society. These discussions provide an excellent opportunity for attendees to apply their improving English skills in a meaningful context, all while getting them involved in critical EA topics.</p><p>In essence, these workshops serve a dual purpose. They provide a platform for individuals to practice and improve their English, and they introduce and engage attendees with important EA topics, encouraging active participation and contribution in these crucial conversations, ultimately &nbsp;growing our community here in LatAm. &nbsp;All are welcome, and I look forward to seeing more faces in our future sessions!</p><p>&nbsp;</p><p>I am half Colombian half American, for the past three years I have been based out of Bogot\u00e1, Colombia.</p><p>&nbsp;</p><p><strong>Link</strong> to Tuesday's workshop where we talked about AI and nuclear weapons: <a href=\"https://drive.google.com/file/d/1_EC9gM_rIkc7Fwj-Qu38P4t9IOWv8EWs/view?usp=sharing\">https://drive.google.com/file/d/1_EC9gM_rIkc7Fwj-Qu38P4t9IOWv8EWs/view?usp=sharing</a></p><h2>Impact</h2><p>In a nutshell, the big picture for these initiatives - translating content and running English workshops - is all about closing the language gap within the EA community and building a <i>community.&nbsp;</i> I know there's loads of important EA and XGCR stuff primarily in English, and I want to make sure Spanish speakers can dive into these critical conversations, regardless of their English language abilities.</p><p>By translating key EA materials into Spanish, and helping folks brush up on their English skills with workshops, we're opening doors and making it easier for everyone to join in.</p><p>But, this isn't just about making things understandable - it's about getting more people from the Spanish-speaking community actively involved in EA. I believe that the more diverse voices we have in the conversation, the richer that conversation will be!</p><p>So, whether it's through language translation or English classes, the mission is simple: I want to turn language from a barrier into a bridge, connecting minds from around the world in the EA and XGCR space.</p><p>If you've got a project that you're looking to share with more folks, especially those the amazing people here Latin America, just give me a shout! I'd be super excited to help translate your content/project and spread the word.</p>", "user": {"username": "JAM"}}, {"_id": "toCJjbTQmWPTHmYsG", "title": "EA & Consulting Services Spring Social - London", "postedAt": "2023-05-19T13:56:16.874Z", "htmlBody": "<p>We\u2019re excited to invite you to a social for consultants interested in EA! Come for company, snacks and interesting conversations \ud83d\ude0a</p><p>Please feel free to forward this to consultants, at BCG or elsewhere, who might be interested.</p><p>(Please note that this event is on a Bank Holiday Monday.)</p>", "user": {"username": "Gemma Paterson"}}, {"_id": "ckokr9uhr2Cu3h5En", "title": "Tips for people considering starting new incubators", "postedAt": "2023-05-19T12:52:46.675Z", "htmlBody": "<p>Charity Entrepreneurship is frequently contacted by individuals and donors who like our model. Several have expressed interest in seeing the model expanded, or seeing what a twist on the model would look like (e.g., different cause area, region, etc.) Although we are excited about maximizing CE\u2019s impact, we are less convinced by the idea of growing the effective charity pool via franchising or other independent nonprofit incubators. This is because new incubators often do not address the actual bottlenecks faced by the nonprofit landscape, as we see them.</p><p>There are lots of factors that prevent great new charities from being launched, and from eventually having a large impact. We have scaled CE to about 10 charities a year, and from our perspective,&nbsp;<i>these&nbsp;</i>are the three major bottlenecks to growing the new charity ecosystem further:&nbsp;</p><ul><li>Mid-stage funding</li><li>Founders</li><li>Multiplying effects</li></ul><h2>Mid-stage funding&nbsp;</h2><p>We try to look at every step of our charities\u2019 future journeys, to see how we expect them to fare as they progress. In general, there seems to be enough appetite in the philanthropic community to supply seed funding to brand new projects, and we have been successful in helping charities to launch with the funding they need. However, many cause areas appear to have gaps in available funding for charities who are around two to five years old.&nbsp;</p><p>Charities\u2019 budgets tend to grow each year; for example, a charity might need $150k for its first year, $250k for its 2nd, $400k for its third, and so on. The average charity might require a seed of $150k for its first year, and mid-stage funding (years 2-5) of ~$2 million over 4 years. Currently, it is much more difficult for highly effective charities to fundraise this much at this stage of their journey than it is for them to get the funding they need at the seed-funding stage. Keep in mind that this mid-stage funding is still too early and small for most major institutional funders (e.g., GiveWell does not recommend organizations that can only absorb $1 million a year as top charities), and governments rarely consider projects this young.&nbsp;</p><p><strong>Mental health case study:&nbsp;</strong>An example that demonstrates this issue well is found in the cause area of mental health. We have identified a number of promising intervention ideas in this area over the past few years, and a solid pool of aspiring entrepreneurs interested in founding mental health charities. Although we expected our seed network would be able to support the first round of funding, we did not have confidence in what came next for these charities. We have since worked to improve the situation by helping launch the&nbsp;<a href=\"https://www.mentalhealthfunders.com/\"><u>Mental Health Funders Circle,</u></a> but even with that network we are concerned about mid-stage funding in the future.&nbsp;</p><p><a href=\"https://d-prize.org/\"><strong><u>D-Prize</u></strong></a><strong> case study:&nbsp;</strong>Our concern is not just theoretical; it is a trend other incubators have experienced too. D-Prize, who gives $20k seed grants to a number of organizations, has consistently found that the organizations' biggest challenges are in the mid-stage funding years. This rings even more true for projects with limited networks in key funding cities (e.g., London/San Francisco).</p><p><strong>Why is mid-stage the problem?:&nbsp;</strong>Instead of seed or late stage? I believe it\u2019s the same donors who consider seed or mid-stage funding, but as the volume of funding is smaller at the seed stage, it is covered much more easily. While some charities may struggle in the late stage as well, less will even get to that stage and the number of options typically expand once a charity is clearly established as a field leader.&nbsp;</p><h2>Founders&nbsp;</h2><p>Although a solid number of people are interested in founding charities, it's only an ideal fit for a relatively small percentage of people. It is a career path that requires a highly entrepreneurial mindset, plus a very strong ethical compass to succeed in. Due to the low number of people who are a good fit, I don\u2019t believe that it is a career path that can absorb a high number of people (my guess would be less than 5% of people are actually suited to founding a nonprofit). It is my opinion that other career paths, like for-profit founding or policy, suit a much larger percentage of people (maybe 20%).</p><p><strong>CE founder pool case study:</strong> We have found that there is a relatively large difference between the impact of the top 20% of founders and the other 80%. This has even been predicted fairly accurately by our vetting system. This means that if CE were to train 60 people a year instead of 30, the impact would not double (assuming that there are no changes in the pool of talent). Typically, the top 20% of our founders generate a majority of the impact, meaning even if funding or counterfactuals of the applicants\u2019 time were not taken into account, going from 30 to 50 founders might only result in 120% of net impact, despite a doubling of competition for funding, talent and mentorship. This is already making the pretty generous assumption that the next 20 applicants would be identical in strength to the 20 that we accepted, but who were not in the top 10 (which we think is uncommon based on our historical data).&nbsp;</p><p><strong>Y Combinator case study:</strong> It is pretty common for for-profits to think that founders are the biggest predictors of success, and one of the theories as to why YC\u2019s average company valuation has gone down is due to accepting a larger pool. For YC this is fine; if the total scale goes up but the average per company goes down, they still make more revenue from their investments. In the charity world, however, donations are limited and funders are not consistently donating to the highest impact areas/projects. This makes the \u201cthrow lots of darts at the wall\u201d approach of nonprofit founding far less attractive.&nbsp;</p><p><strong>Why are founders so predictive?:&nbsp;</strong>It is pretty surprising to find that founders are so predictive, but in our data it\u2019s consistently the strongest factor to suggest how well a charity is going to do. We think this is due to the 100s of choices that can be made for every charity; it only takes a few of them to be wrong for the charity to fail to have an impact. Having great decision-makers at the helm is therefore extremely important.</p><h2><strong>Multiplying effects</strong>&nbsp;</h2><p>The final bottleneck that we will touch on briefly, is multiplying effects. As has been suggested in the founder section, it's not primarily luck that separates the top charities from the bottom. If you look at externally reviewed and ranked charities (e.g., GiveWell tops) they have quite a lot in common, in both their approach and methods. To be one of the best charities, you have to get a lot of things right. You can have a great co-founder pairing and a great idea, but if it is launched in the wrong location the charity is not going to be competitive with the best. This means that despite the 90/10 nature of entrepreneurship, you lose a lot of impact when you 90/10 incubation. A charity that makes 9/10 key decisions right will often still fail to have a significant impact.&nbsp;</p><p><strong>Fortify Health case study:</strong> Early on in Fortify Health's development, they came across an opportunity that was highly promising from a tractability angle, but less cost-effective when compared to flour fortification. The government was on side, and the less cost-effective path offered far more security in terms of both impact and funding. FH made the choice 95% of nonprofits would not, and stuck with the more cost-effective intervention. They became GiveWell incubated because they were focused on such a cost-effective intervention. To be one of the best, you often have to make hard calls.</p><p><strong>Why do factors multiply instead of add?:&nbsp;</strong>I think this is mostly due to the exponential nature of entrepreneurship. Both in the nonprofit and for-profit world, a few actors tend to become dominant in the field. With impact being harder to measure, you have to not only become a field leader, but also be truly more cost-effective than the field leader you displace. This is hard to pull off, even for projects that have everything lined up well (e.g., have gone through the Incubation Program and have the full support of CE behind them).&nbsp;</p><h2><strong>But I want to start an incubator anyways!</strong></h2><p>Well ok, but don\u2019t say I didn't warn you!&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/z56YFpphrQDTSPLqi/what-we-learned-from-a-year-incubating-longtermist\"><u>It's a harder thing to pull off</u></a> than people tend to think. However, I do generally like competitive markets and despite all of the above, there is some room for more impactful incubators if they are set up intelligently.&nbsp;</p><p>1. Provide profound assistance to your organizations</p><p>Invest heavily: A lot of the incubators and the people that contact me want to do a \u201cCE light\u201d. However, due to the exponential nature of impact and the direct competition for resources charities go through, I think it's far better to have a small batch of&nbsp;<i>really</i> good charities rather than a huge batch of mediocre ones. In fact, I would even argue it's better to have a small batch of charities that are&nbsp;<i>all</i> good than to have a large batch of charities where some of them are good, but some are only having a&nbsp;<i>medium</i>-sized impact.&nbsp;</p><p>Invest long term: The second part of&nbsp;<i>really</i> helping your charities is not just throwing them into the deep end after your incubator\u2019s program (remember, your incubator has no impact if all your charities fail soon after you incubate them). You have to think about their next steps- how you can support them in the first three years, not just their first three months. Is there appreciation in the mid-stage funding market for the number of charities you are putting out? Can you teach your charities how to overcome the challenges they will likely face two years down the line?</p><p>Invest specifically: It\u2019s really hard to be good at everything. CE started by incubating in two cause areas it knew really well, and only slowly expanded to others after seeing some success in those areas. Almost every incubator that contacts me wants to be too broad; the more focused you are, the more useful you can be to the specific interventions in your program.</p><p>2. Consider alternative incubation paths</p><p>Other career paths: One of the great things about CE is that it gives a clear ramp onto an impactful career path that would otherwise be hard to access. But charity founding is not the only career path that can be highly impactful. Training for Good is working on policy and journalism equivalents, and there are many more career paths that seem promising but for which there is no clear ramp to move forward onto. This is likely the most promising alternative area we currently see.</p><p>For-profit: For-profit programs, although more abundant, do not suffer from many of the same challenges nonprofit programs face. It is a lot more viable to run with a larger group, or a group where only 1/10 projects hits big. There is also a much larger pool of people who are keen to start a non-profit and are well-suited to the endeavor. A for-profit incubator that only lets in people who take the founders pledge, for example, could work to efficiently reduce the void in mid-stage funding (excited entrepreneurs are a great fit for this gap). It could also utilize and mobilize the large pool of impact-minded entrepreneurs who are often going to weaker social enterprise incubators.</p><p>Start a direct project instead: In some communities, meta or cross-cutting projects are all the rage. However, for 19/20 people, I really think that working on something more direct than an incubator would be the right call. For every meta project started, there are dozens of direct projects more needed. There are many who consider meta too quickly relative to just founding the project they want to see go through an incubator.&nbsp;</p><p>3. Consider if you are the right person to found an incubator</p><p>When someone asks me about starting an incubator, the first question I ask is: \u201chow many successful projects have you founded?\u201d I don\u2019t think CE would be a very useful incubator if our founding team had not started and learned a bunch of lessons from our past projects. This is both from a genuine knowledge perspective and a credibility perceptive. The only reason charities first considered listening to CE was due to our team members having past experience, not only of founding projects but of founding and being involved with projects that were externally vetted and were considered desirable to replicate. Prior to CE\u2019s founding Charity Science Health and Fortify Health were GiveWell incubated, and New Incentives became GiveWell recommended; this sort of background is part of what drew the initial talent CE needed to launch our incubator (and remember, talent is super important).&nbsp;</p><p>Probably even more important was the fact that a ton of our team members, including both founders, had founded and worked for a variety of nonprofits. This meant the advice we gave could actually be useful. An incubator run by people who have never previously founded successful projects is rare, even in the for-profit field; if you haven\u2019t actually done something yourself, it is useful to consider how easy it would be to teach it.&nbsp;</p><p>The second question I ask is: \u201cyou know this is not going to be glamorous, and is going to be pretty hard, right?\u201d Incubators in the nonprofit sector are far closer to operations than they are to research. An incubator is the supporting character of the story, not the hero. Your charities/projects/students need to come first; they need you to give them everything and ask for very little in return. If CE focused mostly on building itself up, mid-stage funding would be even more limited as we would absorb it before our charities get a chance. Our charities would pick up spending habits that are unsustainable for them, as CE spends 10x what they will be able to. CE does not even ask the charities to put our brand on their website unless it helps them! If you can, visualize an incubator as a personal assistant role to a bunch of charities- this is possibly the most accurate way to think of it.&nbsp;</p><p>The third question is \u201chow good are you at vetting?\u201d People are the most important ingredient; an incubator\u2019s hardest, but most important task, is to vet well. Your incubator\u2019s credibility is based on how your projects do (which is mostly predicted by founders), and your incubatees\u2019 experience will mostly be shaped by how good the other people in the room are. If you have an amazing venue but you fill it with the wrong people, the event will be far less meaningful than a great group meeting in a public park. The same is true with your incubator; to be a good incubator you have to see talent and opportunity where other people miss it. Do you have a track record of hiring people who are overlooked by others, but turn out to be rock stars of the movement a short while later? How about funding a project that the most respected funders were skeptical of, only to have them pick it up two years later as one of their favorites? This is the sort of vetting and decision-making power that fuels a great incubator.&nbsp;</p><p>In summary: Running an incubator is hard. Although there are still some great opportunities for new career paths and focus areas to get incubated, it's worth taking a deeper look at whether it is the most impactful project to do, and if you are the right person to do it. Right now, even the CE team thinks that the way we can expand to have the most impact is if we branch into other careers (such as founding foundations).&nbsp;<br>&nbsp;</p>", "user": {"username": "Joey"}}, {"_id": "vznzMrCohJF9y3iKL", "title": "Is Effective Volunteering Possible?", "postedAt": "2023-05-19T12:41:16.717Z", "htmlBody": "<p><i>Epistemic status: based mostly on intuitions and personal and anecdotal experiences, with a bit of help from the principles of economics. Uncertain and motivated to explore whether an overexploited activity is underexploited regarding doing the most good. About channeling the extended will to volunteer into solving problems effectively.</i></p><p>Last year I did volunteering for three weeks in Arusha, Tanzania. That was before I learned about&nbsp;<a href=\"https://www.lesswrong.com/posts/ZpDnRCeef2CLEFeKM/money-the-unit-of-caring\"><u>Money as the Unit of Caring</u></a>,&nbsp;<a href=\"https://www.lesswrong.com/posts/3p3CYauiX8oLjmwRF/purchase-fuzzies-and-utilons-separately\"><u>Purchasing Fussies and Utilons Separately</u></a>,&nbsp;<a href=\"https://www.lesswrong.com/posts/pC47ZTsPNAkjavkXs/efficient-charity-do-unto-others\"><u>Efficient Charity</u></a>, and the rationalist and Effective Altruism aspirations and movements.</p><p>A friend of mine had gone there the previous year and had recommended the experience. He did warn me that some volunteers had left disappointed at how they had got more leisure than impact out of it. Even so, travelling to Africa and Tanzania, alone, getting immersed in the local culture, and learning first-hand about their struggles, was something I wanted to try at least once. The personal experience turned out to be amazing\u2014probably the best I have ever had. When it comes to summer trips, it was, for me, unmatched. It was a very warm and very bright\u2014and quite expensive\u2014fuzzy.</p><p>In the mornings, my role there was to teach at a local school. I was usually an assistant to the main, local professor at a class of 30 five or six-year-olds, and sometimes I was the sole teacher of eight 12-year-olds. We had the afternoons free, which we frequently spent by going to an orphanage to play and cheer up the 20 solitary kids that were taken care of by a single nun.</p><p>Leaving cynicism aside, playing with the children and providing them with their brightest moments of the day was heart-opening enough that I think almost anyone, effective altruists included, would find the experience inherently valuable. If you add that you can meet great people, discover the realities of an entirely different country in an authentic<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefahjsp5ltqri\"><sup><a href=\"#fnahjsp5ltqri\">[1]</a></sup></span>&nbsp;way, and leave with great peace of mind, it gets you wondering:&nbsp;<i>isn\u2019t there something of great practical importance we can extract from volunteering that can really move the needle on doing the most good, without having to feel a pang of guilt about it?</i></p><p>This is the core hypothesis of Effective Volunteering: volunteering can be as much about&nbsp;<i>you</i> as about helping eradicate tragedies and problems of the world you wish didn\u2019t exist. This contrasts with what has been known as&nbsp;<i>voluntourism</i>, which, in blunt terms, is when:&nbsp;<i>like tourism, you value the experience in itself so much that you are almost grateful that such a place called poverty exists so can have fun visiting it</i>.</p><p>A project around Effective Volunteering should aim to investigate this claim and implement the most beneficial policies for doing good through the inevitable power and allure of volunteering,&nbsp;<i>if there are any</i>.</p><p>These last four words are the cornerstones that sustain the legitimacy of this investigation: once you choose that there are ways in which volunteering can be good, no evidence you find will change the&nbsp;<a href=\"https://www.lesswrong.com/posts/34XxbRFe54FycoCDw/the-bottom-line\"><u>bottom line</u></a>. So instead, one must leave from a state of suspicion and uncertainty, where the direction in which each new piece of data will pull you is unpredictable. Be curious and put little resistance into updating in light of new clues.</p><p>The validity of the project depends on the algorithm that produces the bottom line of the research. If the world is such that no kind of international volunteering can ever be beneficial, the algorithm will be effective if it outputs that no kind of international volunteering can be beneficial. If the world is such that only a few selected and rigorous volunteering programmes can be beneficial, the algorithm will be effective if it outputs that only these can be beneficial, and contributes towards implementing these and not others.</p><h2>Intuitions about Effective Volunteering</h2><p>Nonetheless, I have an intuition (which is the reason behind why I am willing to explore the possibility of Effective Volunteering) that, despite the huge popularity of international volunteering, it is underexploited when it comes to doing the most good. Most volunteer organisations are ineffective, do little or even negative impact, are mostly about voluntourism, and their real aim often is to turn volunteers into donors. Most effective charities, by contrast, are by definition effective regarding the value created from the money donated, but less frequently offer the possibility of volunteering abroad.</p><p>This might indeed be because a charity will never be effective through volunteering; that volunteering is inherently&nbsp;<i>in</i>effective. But if we take humans\u2019 nature for granted, and assume that for many people the experience of volunteering can be invaluable for motives such as building career capital, relationships, personal well-being, and self-care, then I think it is possible that some paths have a greater impact than others; and that some have a net-negative and others have a net-positive impact&nbsp;<i>on the developing country</i>. And, given this, that we can maximise the impact made through volunteering.</p><p>Within the Effective Altruism community, I believe it is not as needed that I point out the most common drawbacks of volunteering, although I will still underline some of the main ones below.</p><p>1. <i>The power of specilisation</i></p><ul><li>Money is the measure of how much society wants something. If a lawyer earns $50 an hour, it is because the market requires a degree of expertise that is not as widely available as the one for flipping burgers, which can get someone to earn $10 an hour. Frequently, salary increases with the skill requirements. If a person has the competitive advantage of being employable for a high-paid job, it means his aptitudes overlap with a specific market demand. He could do lower-paid jobs too, but he would be running against the principles of supply and demand of the market; he would be doing something for which there was ample supply, and not doing something for which there was proven demand.</li><li>Money\u2014salary\u2014hints at what the market cares about you doing. The market, in other words, wants you to specialise in your highest-paid available jobs, and have less skilled personnel do the lower-paid ones. Working for a single hour, the lawyer could be earning the money to pay for five cooks, and would be much more effective according to the market\u2019s wants than by volunteering to cook for an hour.</li></ul><p>2. <i>The fungibility of money</i></p><ul><li>Fungibility refers to the ability of an asset\u2014money, in this case\u2014to be interchanged for another of the same type. A dollar bill is indifferent to its history. $1,500 can serve equally to pay a trip and three-week stay in Tanzania as to&nbsp;<a href=\"https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/comparing-moral-weights\"><u>double a household\u2019s consumption for a year</u></a>. If what the person really wants is to make the world a better place, there are some investments more effective than others for the same price. (Many other times, the person will want to prioritise self-care over utilons, perhaps in order to be more productive in the future, in which case the better investment might be to book a trip.) For every intention and goal that the person pursues, the effectiveness of each intervention in satisfying that goal per unit of money can be evaluated.</li></ul><p>3. <i>The cost of managing volunteers</i></p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/6uXZc2QqHXsHKaLy9/volunteering-isn-t-free\"><u>Volunteering isn\u2019t free</u></a>. It takes staff and resources to recruit, filter, organise and supervise the volunteers. From the above point, this is money that could be used to pay for other cause areas or more cost-effective policies.</li></ul><p>4. <i>Artificial jobs, weakening of local markets, and obstacles to resilience and independence</i></p><ul><li>International volunteering should always be discouraged if it does harm on the developing country. Oftentimes, voluntourism creates artificial jobs that are not needed, or takes up jobs that could be carried out by locals willing to be hired.</li><li>If what a school really needs is a teacher, it may be much more effective to fund the hiring of a local teacher, which would additionally contribute to increasing local employment and keeping money moving within the country. If the volunteer acts as an assistant professor, it may be that the school didn\u2019t actually need another teacher, but just welcomed youngsters to play with the kids and help improve the facilities. If what you want is to add glass to the classroom windows, it may be better to hire a specialised professional rather than importing unskilled labour to install windows that will fall as soon as wind blows.</li><li>A danger with volunteering, as with donations, is that it can&nbsp;<a href=\"https://www.spiegel.de/international/spiegel/spiegel-interview-with-african-economics-expert-for-god-s-sake-please-stop-the-aid-a-363663.html\"><u>promote complacency, teach the locals to beg and rely on foreign aid, and dampen their spirit of entrepreneurship</u></a>. Sometimes it can finance corruption and bureaucracies too. James Shikwati, a Kenyan libertarian economist, made this point when the interviewer made the sensible suggestion that, if the World Food Program didn't do anything, the people would starve:</li></ul><blockquote><p><i>I don't think so. In such a case, the Kenyans, for a change, would be forced to initiate trade relations with Uganda or Tanzania, and buy their food there. This type of trade is vital for Africa. It would force us to improve our own infrastructure, while making national borders\u2014drawn by the Europeans by the way\u2014more permeable. It would also force us to establish laws favoring market economy.</i></p></blockquote><p>5. <i>Be wary of second-order effects; be wary of wolves in sheep\u2019s clothing</i></p><ul><li>Personal example here. Admittedly, my direct impact as an assistant teacher in Arusha was minimal. I did remove workload from the main teacher, I gave classes to the 12-year-olds when their usual teacher was missing (which somehow happened several times), and I believe I could have offered some improved teaching methods or intuitions if I had stayed for longer.</li><li>But my greatest satisfaction and certainty came from knowing that, whether or not I had been needed as a teacher, I had made the kids&nbsp;<i>happier&nbsp;</i>while there. Their normal days are gloomy, their teachers do not show the same warmth or attention in the playground, and you have plenty of love to give them. The children do not want you for your possessions; they want you to care about them. The school director told me that the kids, on arriving home, were excited to tell their parents that volunteers had come to school. This warmed my heart, and seemed to make everything worthwhile.</li><li>Unfortunately, on closer inspection, a serious peril was looming in the horizon, which I would have typically been unable to recognise as I went back to Spain. This is the danger of&nbsp;<i>abandonment</i>. The kids might be happy for some time. But what happens when they realise that the volunteers are leaving after three weeks, never to return again? What is the toll of this parade of volunteers, who provide the kids with a mere hint of what being valued feels like? This effect can be even stronger for the orphaned children.</li><li>Several more consquences might be unattended to and overlooked as one breaks ties, perhaps permanently, with the volunteering destination.</li></ul><p>These appear to be exceedingly adverse arguments against volunteering to even contemplate countering them with volunteering itself. But, to the above-mentioned benefits for the individual volunteer, it can be hypothesised that, although aid can sometimes be deceptive,&nbsp;<i>helping is possible</i>.</p><p>I do believe that leaving a market to its own devices, even in developing countries, is often preferable to most other options. By having to optimise for what the consumer wants, the market makes it easy to spot and remove bad and inefficient policies, become a bit less wrong about what works and what doesn\u2019t, and make substantial progress. But, going meta, I do not find convincing the idea that capitalism and libertarianism are the all-purpose solutions that can cure all diseases.</p><p>It seems much more plausible and intuitive that, for all its apparent elegance and simplicity, full libertarianism isn\u2019t the piece that fits in all puzzles. In fact, there might be no such once-and-for-all piece, and the best we may have is our capacity to reason about the best strategy for each particular situation.</p><p>As aspiring rationslists, we ought to ask:&nbsp;<i>\u201cso you\u2019re telling me you can\u2019t imagine a single situation in which libertarianism would be the wrong approach; not a single problem which libertarianism would fail to solve and another strategy wouldn\u2019t?\u201d</i>. Scott Alexander addressed this when discussing the machinery of freedom (and&nbsp;<a href=\"https://slatestarcodex.com/2015/03/18/book-review-the-machinery-of-freedom/\"><i><u>The Machinery Of Freedom</u></i></a>, the book), playing libertarianism against a centralised institution:</p><blockquote><p><i>Given that the universe is allowed to throw whatever problems it wants at us, and that it has so far gleefully taken advantage of that right to come up with a whole host of very diverse and interesting ones, why is it that none of these problems are best addressed by a centralized entity with a monopoly on force? That seems like a pretty basic structure from a game-theoretic perspective, and you\u2019re telling me it just never works in the real world? Shouldn\u2019t there be at least one or two things where a government, or any form of coercive structure at all, is just the right answer? And can\u2019t we just have a small government that does that?</i></p></blockquote><p>Imagine you have two regions. One is rich, resourceful, and with plenty of accumulated and constructed knowledge; the other is poor, needy, and with feeble social, political and industrial structures. Information can leak from one to the other, but the imbalance is still huge.</p><p>Refraiming the previous question:&nbsp;<i>\u201cso you\u2019re telling me that leaving the developing country on its own is better than sending people from high-income countries to help? That there\u2019s really nothing that an enthusiastic visitor could do to make the lives of the struggling locals a little bit better? That the developing country has to figure all out by itself?\u201d</i>.</p><p>During my time at the school in Arusha, I felt there was a 5-year-old, by the name of Johnson, that was quite bright compared to the rest. It disheartens me to predict the lack of opportunities he might have as he grows up; how reality might crush his potential. He will probably never have a tutor or have the chances to exhibit himself and gain self-esteem.</p><p>It\u2019s also a source of great frustration to imagine how I could help him, how I could tutor him, if there weren\u2019t so many people that needed help and the world wasn\u2019t such a broken place. This hunch that I&nbsp;<i>could&nbsp;</i>help Johnson, and many others, partly drives my suspicion that there is room for high-value interventions from volunteers.</p><h2>An open door for further research</h2><p>It\u2019s not my claim that volunteering is the most effective strategy to do good; far from it. Nevertheless, as long as it provides net-positive value for the host country and it is done effectively, it can be a worthwhile activity given that it is a two-way street that can confer huge positives to the volunteer while directly tackling important problems.</p><p><a href=\"https://forum.effectivealtruism.org/posts/ScLHyCY6JCr5FtuiY/effective-volunteering\"><u>Kirsten defined</u></a> effective volunteering (although, as I understand it, referring to volunteering within one\u2019s country rather than regarding the substitution of international voluntourism) as \u201cusing pre-allocated volunteering time to do as much good as possible, using evidence and reason\u201d.</p><p>It can be that, for effective volunteering to work, the selection process for volunteers would need to be much more restrictive. Okay, an MIT professor teaching in Tanzania could make huge differences, but it might be hard to find something that a local couldn\u2019t do which a group of teenage friends inscribed by their wealthy parents could meaningfully offer. Or maybe their enthusiasm could be directed for good, without producing side effects such as a sense of abandonment in kids. This would necessitate careful investigation.</p><p>Additionally, the range of locations and of tasks carried out could have to change significantly, as would be expected. The emphasis could shift to non-easily-replaceable jobs in low-income locations where governance was weak, in remote or rural areas, with high rates of mental health issues, or where there was a need for creative solutions and novel perspectives.</p><p>All in all, my aim is not necessarily to encourage international volunteering for effective altruists\u2014perhaps it is to discourage ineffective, harmful voluntourism. Rather, to take a scout mindset in investigating the viability of effective volunteering. The falsifiable hypothesis is that we&nbsp;<i>can</i> help, even if we wish that such volunteering wasn\u2019t needed in the first place. Remember: if poverty exists, we want to eradicate it. If mental disorders are prevalent, we aim to replace them with happiness and well-being.</p><p>While it is natural to expect humans to pursue the warm fuzziness of volunteering, organisations and communities could exist that took a higher-level view and channelled this enthusiasm into utilons through the most effective interventions. Implementing these interventions and creating such spaces for volunteerism would depend on the outputs of effective research.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnahjsp5ltqri\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefahjsp5ltqri\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Relative to other ways of exploring Africa, such as going only on a safari, which can compress the image you have of the continent into that of its pure and wild natural landscapes, and leave you as emotionally insensitive to the natives\u2019 pains and hardships as you were before. Being a foreign volunteer, of course, still gives you a skewed view of the circumstances.</p></div></li></ol>", "user": {"username": "David Bravo"}}, {"_id": "JjAjJ53mmpQqBeobQ", "title": "\u201cThe Race to the End of Humanity\u201d \u2013 Structural Uncertainty Analysis in AI Risk Models", "postedAt": "2023-05-19T12:03:35.714Z", "htmlBody": "<h1>Summary</h1><ul><li>This is an entry into the <a href=\"https://www.openphilanthropy.org/open-philanthropy-ai-worldviews-contest/\">Open Philanthropy AI Worldview Contest</a>. It investigates the risk of Catastrophe due to an Out-of-Control AI. It makes the case that that <strong>model structure is a significant blindspot in AI risk analysis</strong>, and hence there is more theoretical work that needs to be done on model structure before this question can be answered with a high degree of confidence.<strong>&nbsp;</strong></li><li>The bulk of the essay is a \u2018proof by example\u2019 of this claim \u2013 I identify a structural assumption which I think would have been challenged in a different field with a more established tradition of structural criticism, and demonstrate that surfacing this assumption <strong>reduce the risk of Catastrophe due to Out-of-Control (OOC) AI by around a third</strong>. Specifically, in this essay I look at what happens if we are uncertain about the timelines of AI Catastrophe and Alignment, allowing them to occur in any order.</li><li>There is currently only an inconsistent culture of peer reviewing structural assumptions in the AI Risk community, especially in comparison to the culture of critiquing parameter estimates. Since models can only be as accurate as the least accurate of these elements, I conclude that this disproportionate focus on refining parameter estimates places an avoidable upper limit on how accurate estimates of AI Risk can be. However, it also suggests some high value next steps to address the inconsistency, so there is a straightforward blueprint for addressing the issues raised in this essay.</li><li>The analysis underpinning this result is available in <a href=\"https://www.dropbox.com/s/3c1xvw7lyqv6mz3/AI%20Risk%20Model%20-%20Complex%20v2.xlsx?dl=0\">this</a> spreadsheet. The results themselves are displayed below. They show that introducing time dependency into the model reduces the risk of OOC AI Catastrophe from 9.8% to 6.7%:</li></ul><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/cl0vhfmsbcgncsgaruni\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/dv3qvxsvyg7x5hhyuceb 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/xjaje7y12h8jpr8mgnmo 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/etbeaxouzfn2oudwobys 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/wwkatdhq187wjcficxeu 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/ymgckleecj52hqdftbn5 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/maqzdckhqth9t0z9qupr 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/p1iodj4lcfahyu5c83mq 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/ifply4fqegbevrvcjnqf 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/fetdmwrb0j8w5avhrbro 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/caicfz9p8jzyjpvjhq4v 940w\"></figure><ul><li>My general approach is that I found a partially-complete piece of structural criticism on the forums <a href=\"https://forum.effectivealtruism.org/posts/9iGFjYnRquxiy29jm/safety-timelines-how-long-will-it-take-to-solve-alignment\">here</a> and then implemented it into a <i>de novo</i> predictive model based on a well-regarded existing model of AI Risk articulated by Carlsmith (<a href=\"https://arxiv.org/abs/2206.13353\">2021</a>). If results change dramatically between the two approaches then I will have found a \u2018free lunch\u2019 \u2013 value that can be added to the frontier of the AI Risk discussion without me actually having to do any intellectual work to push that frontier forward. Since the results above demonstraste quite clearly that the results have changed, I conclude that work on refining parameters has outpaced work on refining structure, and that ideally there would be a rebalancing of effort to prevent such \u2018free lunches\u2019 from going unnoticed in the future.</li><li>I perform some sensitivity analysis to show that this effect is plausible given what we know about community beliefs about AI Risk. I conclude that my amended model is probably more suitable than the standard approach taken towards AI Risk analysis, especially when there are specific time-bound elements of the decision problem that need to be investigated (such as a restriction that AI should be invented before 2070). Therefore, I conclude that hunting for other such structural assumptions is likely to be an extremely valuable use of time, since there is probably additional low-hanging fruit in the structural analysis space.</li><li>I offer some conclusions for how to take this work forwards:<ul><li>There are multiple weaknesses of my model which could be addressed by someone with better knowledge of the issues in AI Alignment. For example, I assume that Alignment is solved in one discrete step which is probably not a good model of how Aligning AIs will actually play out in practice.</li><li>There are also many other opportunities for analysis in the AI Risk space where more sophisticated structure can likely resolve disagreement. For example, a live discussion in AI Risk at the moment is whether American companies pausing AI research (but not companies in other countries) raises or lowers the risk of an Unaligned AI being deployed. The sorts of time-dependency techniques described in this essay are a good fit for this sort of question (and there are other techniques which can address a range of other problems).</li><li>In general, the way that model structure is articulated and discussed could be improved by a concerted community effort. Analysts offering predictions of AI Risk could be encouraged to share the structure of their models as well as their quantitative predictions, and people reading the work of these analysts could be encouraged to highlight structural assumptions in their work which might otherwise go unnoticed.</li></ul></li><li>Given the relative lack of sophistication of AI Risk model structures, I end with the argument that careful thought regarding the structure of AI Risk is likely to be extremely high impact compared to further refining AI Risk parameters, which will have a material impact in our ability to protect humanity from OOC AI Risk.</li></ul><h1>1. Introduction</h1><p>In very broad terms, models can be thought of as a collection of parameters, and instructions for how we organise those parameters to correspond to some real-world phenomenon of interest. These instructions are described as the model\u2019s \u2018structure\u2019, and include decisions like what parameters will be used to analyse the behaviour of interest and how those parameters will interact with each other. Because I am a bit of a modelling nerd I like to think of the structure as being the model\u2019s <i>ontology</i> \u2013 the sorts of things need to exist within the parallel world of the model in order to answer the question we\u2019ve set for ourselves. The impact of structure on outcomes is often under-appreciated; the structure can obviously <i>constrain the sorts of questions you can ask</i>, but the structure can also <i>embody subtle differences in how the question is framed</i>, which can have an important effect on outcomes.&nbsp;</p><p>In any real-world model worth building, both parameters and structure will be far too complex to capture in perfect detail. The art of building a good model is knowing when and how to make simplifying assumptions so that the dynamics of the phenomenon of interest can be investigated and (usually) the right decision can be made as a result. In my experience of teaching people to build models in a health economics context, people almost always have a laserlike focus on simplifying assumptions relating to parameters, but very frequently a blindspot when it comes to assumptions relating to structure. This is a bad habit - if your model structure is bad, spending a lot of time and effort making the parameters very accurate is not going to improve the quality of the model output overall for the same reason a bad recipe isn\u2019t improved by following it more precisely.&nbsp;</p><p>My observation is that this blindspot also appears to exist in AI Risk analysis. I conducted a systematic review of the EA / LessWrong / AI Alignment forums to identify previous work done on formal models of AI risk and noticed the same imbalance between effort spent refining parameters and effort spent refining structure. For example, descriptions of what constitutes structural decision-making such as <a href=\"https://forum.effectivealtruism.org/topics/conjunctive-vs-disjunctive-risk-models\">this</a> wiki entry are incomplete and misleading, and attempts to engage the community in discussions about structural uncertainty analysis such as <a href=\"https://forum.effectivealtruism.org/posts/b3nGMGGhTZawy8Zfd/ai-x-risk-integrating-on-the-shoulders-of-giants\">here</a> have not met with significant engagement (in fairness this piece is extremely technical!).&nbsp;</p><p>To an outsider, one of the most obvious ways this imbalance reveals itself is by looking at the difference in the number of approaches between modelling structural approaches and modelling parameterisation approaches. When looking at parameters there are a nearly endless number of methods represented \u2013 <a href=\"https://astralcodexten.substack.com/p/biological-anchors-a-trick-that-might\">biological anchors</a>, <a href=\"https://www.lesswrong.com/posts/jyRbMGimunhXGPxk7/database-of-existential-risk-estimates\">expert survey</a>, <a href=\"https://www.metaculus.com/questions/2805/if-there-is-an-artificial-intelligence-catastrophe-this-century-when-will-it-happen/\">prediction markets</a>, <a href=\"https://www.lesswrong.com/posts/FaCqw2x59ZFhMXJr9/a-prior-for-technological-discontinuities\">historical discontinuity approaches</a> and probably a load I\u2019ve missed because I\u2019m not an expert in the field. However, by contrast there is \u2013 from what I can tell from my review \u2013 exactly one published structural model of AI Risk, Carlsmith (<a href=\"https://arxiv.org/abs/2206.13353\">2021</a>). This structure is replicated below. In lay terms we might think of the structure of the model as operating by describing a number of weighted coinflips which can come up \u2018good\u2019 or \u2018bad\u2019. If all of these coinflips come up \u2018bad\u2019 then we have a Catastrophe. In this sort of model, the overall risk of Catastrophe is simply the probability of each of these individual steps multiplied together. Although only Carlsmith has extensively specified his model, you can see that this sort of \u2018weighted coinflip\u2019 approach is motivating other qualitative discussions of AI Risk structures (e.g. <a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\">here</a>).</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/k3u6hrhjhtr6jjaotbhn\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/qszny8tlsdoawumw3huw 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/pj9rcgbiinn7rwiy8vuj 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/yscqevylbatvtwceakpu 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/cxevjvpivmg4jdxbb6im 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/isd2spx2dgrgee5judbq 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/a6arwhgzz3q6qtwsbizt 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/xmn4qybognjbzkyemiq0 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/tgatku41qfbreb63yu3x 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/s3cnmooi0sgm21iqwkdp 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/wa3jtdgx1khoh4qkownd 940w\"></figure><p>I don\u2019t want to give the impression that Carlsmith\u2019s approach is uncritically accepted as some kind of \u2018universal modelling structure\u2019. For example, Soares (<a href=\"https://forum.effectivealtruism.org/posts/vC6v2iTafkydBvnz7/agi-ruin-scenarios-are-likely-and-disjunctive\">2022</a>) makes the excellent point that the Carlsmith model assumes through its structure that AI Catastrophe is a narrow target \u2013 six coinflips need to go against us in succession before Catastrophe occurs. Soares makes the case that in fact we could turn the problem on its head; assume that Catastrophe is the default and that a bunch of coinflips need to go in our <i>favour</i> for Catastrophe to be prevented. While it would have been ideal if Soares had also attached probability estimates to each step in his argument, it is a great example of the central point of this essay \u2013 model structure doesn\u2019t just constrain the sorts of questions you can ask of a model, but also embodies subtle assumptions about how the question should be framed. Surfacing those assumptions so they can be properly debated is an important role of a modelling peer-review community<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9ys1z65kqft\"><sup><a href=\"#fn9ys1z65kqft\">[1]</a></sup></span>.</p><p>Nevertheless, in comparison to the number and sophistication of analyses on the topic of AI Risk prediction, model structure is notably under-explored. With a few important exceptions like Soares\u2019 piece, there seems to be a strong (but implicit?) consensus that there is a standard way of building a model of AI Risk, and limited incentive to explore other model structures<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz65l7ke45f\"><sup><a href=\"#fnz65l7ke45f\">[2]</a></sup></span>. I think there is a risk in not promoting more structural analysis in the AI Risk modelling space, since structural issues with models are often significantly pernicious \u2013 it takes an expert to spot \u2018the dog that didn\u2019t bark\u2019 and identify that an entity or relationship which <i>should</i> exist in the model has instead been overlooked. This essay is attempting to serve as a proof by example of the level of predictive imprecision which can arise if AI Risk modellers spend too long focussing on parameter uncertainty and not enough time on structural uncertainty.&nbsp;</p><h1>2. The Race to the End of Humanity\u201d</h1><p>As discussed above, the purpose of this essay is to identify an assumption in existing AI Risk models (ie Carlsmith\u2019s paper) and show that by <strong>not</strong> making that assumption results change dramatically. The idea is that by demonstrating the way that structure can be just as important as parameters I might encourage people with a bit more expert knowledge to start producing their own discussions about structure.</p><p>With that in mind, Section 2.1 identifies and describes the assumption I will be investigating. Section 2.2 describes one method of building a model without that assumption, and Section 2.3 offers some preliminary results from the model. I save all the conclusions for Section 3 so that you can safely skip all the technical modelling stuff and still get the main insights of the essay.</p><h2>2.1 The assumption of 'time independence'</h2><p>I am not an AI Risk expert so to avoid making an embarrassing error I am going to identify an assumption where the expert-level thinking has already been done for me by <a href=\"https://apartresearch.com/\">Apart Research</a>, and I can focus entirely on modelling out the implications. Specifically, I am going to trace out the implications of uncertainty existing in the timelines for AI Alignment, and therefore the implication that we don\u2019t have perfect knowledge of whether Alignment will come before&nbsp;Catastrophe. The motivation for this approach described in <a href=\"https://forum.effectivealtruism.org/posts/9iGFjYnRquxiy29jm/safety-timelines-how-long-will-it-take-to-solve-alignment\">this</a> post (and summarised very nicely by the diagram below).</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/lm5auf9a8795jpmicr4l\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/ihuib08dfp158cawba4l 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/eihol2b3byqyq5mbw0yg 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/ulonxwnr8hebx5ghqgye 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/kp1kiiong9bs5hcd3fvm 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/y7mnxj9ksiwugswrfeh2 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/yvamvkjag4vlrrw4ocvh 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/ax275ait3ty1y0ujlgl4 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/qgwvjofllfwo1kxoai7w 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/urql9dkzbahb5dylltgq 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/chnvwfd8rc9phhwtrnwy 940w\"></figure><p>The easiest way to understand what is happening in the above graph is to switch the metaphor we use to describe the model we just looked at from one of weighted coinflips to one where we are watching a runner in a one-man hurdles race. The runner starts today, and then runs down the track until they encounter a hurdle \u2013 \u201cAI Invented\u201d, for example. These hurdles are of variable height, and the runner clears the hurdle with some probability depending on its height (if they fail to clear the hurdle they are disqualified). When the runner finishes the race, they get to push a button which destroys all of humanity. Although the \u2018runner doing hurdles to get to a doomsday button\u2019 is a pretty convoluted way of describing a \u2018weighted coinflip\u2019, I hope you see how it is the same underlying structural problem.</p><p>However, this \u2018runner in a race\u2019 metaphor makes a very strong structural assumption about what question we are interested in \u2013 in a normal race we don\u2019t typically care if some particular runner <i>finishes</i>, but rather which runner finishes the race <i>first</i>. Instead of the weighted coinflip approach where AI is the only runner in the hurdles race, the real world looks more like a \u2018Race to End Humanity\u2019 amongst multiple competitors. For example, a naturally occurring Catastrophic asteroid impact, global pandemic, nuclear war etc are mutually exclusive with AI Catastrophe and so if one of those happens there is no possibility for AI to destroy humanity later. Once humanity is destroyed, there\u2019s no prize for second place \u2013 even if the AI competitor finishes the race somehow they\u2019ve lost their chance to push the button and the AI cannot itself cause a Catastrophe. This is also true if an external force prevents an AI from completing the race under any circumstances \u2013 perhaps one day we will be able to Align AIs so effectively that Unaligned AI ceases to be a risk.</p><p>The Carlsmith model asks the question, \u201cWhat is the probability that the runner finishes the race (rather than being disqualified)?\u201d, but this is potentially the wrong question to be asking; some runner will always finish the race over a long enough timeframe, because there are simply no serious hurdles to stop, for example, a stellar nova from wiping out humanity. What we are interested in instead is rather the probability that AI <i>finishes the race first</i>. We cannot answer the question \u201cWhat is the probability that humanity will suffer an existential Catastrophe due to loss of control over an AGI system?\u201d in isolation from other potential causes of Catastrophe, since the first Catastrophe (or Alignment) automatically precludes any other Catastrophe (or Alignment) in the relevant sense any human being cares about. And if we are interested in the <strong>first</strong> of something, we need a way to track when events are occurring. This can\u2019t really be done in a \u2018weighted coinflips\u2019 model, so we can be fairly confident that Carlsmith\u2019s model oversimplifies<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefav80jhv3od8\"><sup><a href=\"#fnav80jhv3od8\">[3]</a></sup></span>.</p><p>It happens that the specific change I have identified here has a technical name \u2013 \u2018time dependency\u2019. Time dependency is a general name for any situation where the model exhibits dynamic behaviour with respect to time; most commonly that parameters change over time (as in this case, where the probability of achieving Alignment changes over time) but you can have very exotic time dependent models too, where the entire structure of the model changes repeatedly over its runtime. The structural change this model needs can be summed up in a single sentence: Carlsmith\u2019s model of Alignment is time-independent (\u2018static\u2019) whereas I think the real-world process of Alignment is likely time-dependent (\u2018dynamic\u2019). As an added bonus, adding time-dependency for the purpose of modelling Alignment will allow me to investigate other time-dependent elements like X-Risk and S-Risk which I\u2019m <i>fairly&nbsp;</i>sure are unimportant but there\u2019s no harm in checking!</p><h2>2.2 Time dependency in a Carlsmith-like model</h2><p>If you consult a modelling textbook it will tell you in no uncertain terms that you should absolutely not try and encode time dependent behaviour into a \u2018weighted coinflip\u2019 model like the Carlsmith model, and that there are instead specific model structures which handle time dependence very gracefully. However, I wanted to try and keep the Carlsmith structure as far as I could, partly because I didn\u2019t know what other assumptions I might accidentally encode if I deviated a long way off the accepted approach in the field and partly because I wanted to show that it was <strong>only</strong> the structural decision about time-dependency that was driving results (not any of the other modelling dark arts I would employ to create a more typical modelling structure for this kind of problem). Disappointingly, it turns out that the textbooks are bang on the money here \u2013 the model I came up with is extremely inelegant and in hindsight I wish I\u2019d taken my own advice and thought a bit harder about model structure before I made it!&nbsp;</p><p>Nevertheless, I did succeed in making a model structure that was almost exactly like the Carlsmith model but which had the ability to look at time-dependent results. The model is available <a href=\"https://www.dropbox.com/s/3c1xvw7lyqv6mz3/AI%20Risk%20Model%20-%20Complex%20v2.xlsx?dl=0\">here</a>, and it might be helpful to have it open to follow along with the next section. The structure is below:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/ezybpxzwfuim0qlfg8wi\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/c27lynvbuvvyip1yplvp 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/c1nonbfewdyk2r92effl 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/zwgtonsyxjxvx8jmih6b 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/gjvwo4jcl6r3w8sqvzru 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/cyicwaybqmbt3bdoxbjh 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/zozenkzr77eouxdudqs5 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/c7mpkavzzfqshk2vaiih 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/vvgeqbzgn0afxrrkxkkm 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/fhfxtmwusdjwfrtsqr2h 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/tu77ltafysodrpesdkkc 902w\"></figure><p>The general principle of the model is that each box represents an event which will occur with some probability (like in the Carlsmith model), but <strong>also</strong> each arrow indicating a transition between events represents a certain amount of time. There are three \u2018runners\u2019 in this model, \u2018Unrelated Catastrophe\u2019, \u2018Out of Control AGI Outcome\u2019 and \u2018Good AI Outcome\u2019. &nbsp;Once every runner has finished, I work out how long they took and then calculate who the winner was after everyone is finished. Parameter uncertainty about the time between each step is extremely important (otherwise the race will always have the same winner provided that runner finishes), so I simulate the \u2018race\u2019 thousands of times with different starting parameters drawn from a survey of the AI Risk community and look at the probability that each participant wins.</p><p>The three possible outcomes from the model are:</p><ul><li>Unrelated Catastrophe \u2013 Representing any bad outcome for humanity that isn\u2019t explicitly caused by an Out-of-Control AI. There are three sub-elements of this outcome, which have the interesting feature of beginning the race at different times:<ul><li>S-Risk can occur any time from now until the end of the race, and represents \u2013 for example - a totalitarian government seizing control of the world to such an extent that human flourishing is permanently curtailed, but the development of AI is not (so S-Risk can occur before AI is Invented).</li><li>X-Risk can occur any time from AI Invention until the end of the race (since if X-Risk occurs before AI is invented then by definition AI cannot cause a Catastrophe). It represents any non-AI source of mass human extinction, for example an asteroid impact or deadly viral pandemic.</li><li>IC AI Risk is a one-off risk which is tested for if AI causes a Catastrophe. It represents a Catastrophic event where an AI is deliberately used to cause a mass extinction (like X-Risk) or subjugate the population of the world (like S-Risk). Since Carlsmith does not distinguish between \u2018in control\u2019 and \u2018out of control\u2019 AI Risk this element is needed to avoid overestimating OOC AI Risk versus the Carlsmith model. Because it isn't time dependent it isn\u2019t very interesting from a modelling perspective.</li></ul></li><li>OOC AI Risk represents a Catastrophic event caused by an AI doing something a human did not want it to. This could be either the AI pursuing its own project entirely, or misunderstanding the task it was instructed to undertake which nevertheless causes a Catastrophe. The events leading up to a Catastrophe are taken directly from the Carlsmith model, although it is assumed that AI will always be invented prior to 2070 so in effect the OOC AI Risk runner gets a bit of a headstart over the other competitors.&nbsp;</li><li>AI Alignment represents achieving a solution to Alignment so perfect that the risk of Out-of-Control AI plunges to zero. In hindsight I might have been a bit over-zealous in defining Alignment in this way, and perhaps humanity might be satisfied with an Alignment solution which merely reduces the risk of Catastrophe to some known and tolerable level (say, on the order of the risk of nuclear war). AI Alignment is <strong>by far</strong> the most likely outcome of the race because in most worlds AI Catastrophe does not occur so there is effectively infinite time to find an Alignment solution (in the model). Consequently, I employ a modelling convenience to get more sensible results out of the model \u2013 I only count Alignment events which actually prevent Catastrophe, rather than any Alignment event which actually occurs (or even Alignment events which occur before the AI Catastrophe timebomb has defused itself by sheer good luck). This is just a presentational trick which doesn\u2019t alter the probability of OOC AI Risk, but it does lead to a significant underestimate of how many worlds will eventually see an Aligned AI.</li></ul><p>Note that \u2018AI ALIGNMENT\u2019 in the OOC AI Risk track represents Alignment being so trivially easy that we don\u2019t have to do any work to achieve it, whereas AI Alignment in the green boxes represents Alignment which is tough but possible. The idea here is to get at the distinction between the bit of the Alignment problem which is affected by longer timelines and the bit that isn\u2019t.</p><p>The model is populated from a survey of the AI Risk community I conducted in 2022. I am not a triallist and have no particular expertise in survey design beyond hobbyist projects, so in hindsight there are some design errors with the survey. In particular, note that the instrument does not condition on the catastrophe occurring by 2070, and since receiving feedback on my first AI essay I realise that some of the questions are quite ambiguous. Nevertheless, the responses from the survey seem mostly in line with the Carlsmith model / community consensus and have good internal consistency:&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/ymkdq3w6ax1mdmdy6doe\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/hjddjjuiyoeti82xftl7 137w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/tli1plxhgvxaf16ywgzk 217w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/u9ssrlt1kjmwcmshghbz 297w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/ytbg9rvghmaym7qpfbno 377w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/cyjbzsm7zjwyf54ncesp 457w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/ovfsy8nvoe1ii2o2wdi8 537w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/ywhctzqemrldhbeacu3d 617w\"></figure><p>The community believes that there is a 17.8% chance of Catastrophe conditional on AI being invented, but because the distinction between In-Control and Out-of-Control Catastrophe matters it is perhaps more relevant to say that the community believes there is a 9.7% risk of Out-of-Control AI Catastrophe. Of interest is that the median Catastrophe date is 2054-2057 (depending on the elicitation technique used) compared to a median Alignment date of 2060. The conventional model structure which uses median timelines indicates that we will miss Alignment by an agonisingly small margin.</p><p>The base case of the model uses the data in the following way:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/h3b6j8wz74r4lchwf6zm\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/dpjvjilxfm9wwizvyf9k 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/wppcdrt13vzpxhggej0w 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/mn1qeotvbvq4mrl8kuns 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/yufk4oqrwicfrm30wfl9 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/nwaqvlwl1azfdwcjaeye 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/aglcypissubbiccubupu 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/mqihxndtv972l6wwu5bz 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/llvpvze6ifftu2ubyatt 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/m3lbbdidikmcmh0dgqgl 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/e8seugzd2ehzcrk4bfgt 919w\"></figure><p>The only feature of significant interest here is that I have set all the transition probabilities to a fixed value, corresponding to their mean (by contrast I use \u2018SDO sampling\u2019 for the timelines, which is just a fancy way of saying that I pick a value at random from the set which was presented in the survey). The reason for this is that I want to prove beyond any doubt that it is the <strong>structure</strong> of the model which drives the change in results, not any fancy sampling technique like I demonstrate in the <a href=\"https://forum.effectivealtruism.org/posts/Z7r83zrSXcis6ymKo/dissolving-ai-risk-parameter-uncertainty-in-ai-future\">sister essay</a> to this one. For technical reasons it actually makes no difference to do things this way, I'm just aware that the audience of this post won't all be modelling experts so I want to ensure that I remove even the <i>perception</i> of a modelling sleight-of-hand.</p><p>There are also a few other small features of the model initialisation which don\u2019t really deserve their own paragraph so I will put them in the following footnote<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref30s4wkqfkfo\"><sup><a href=\"#fn30s4wkqfkfo\">[4]</a></sup></span>.</p><h2>2.3 Results</h2><h3><u>Main results</u></h3><p>The point of making these changes is not to create increasingly arcane variants of common modelling structures just for aesthetic reasons (however much I wish someone would let me do this) but rather to check whether altering our assumptions about a model\u2019s ontology meaningfully alters the way we should approach the conclusion. There\u2019s no fancy statistics or algorithms that are used here \u2013 we just literally compare the two results side-by-side and make qualitative remarks about whether we think the change in structure has had an important impact. This is sometimes an extremely expert process; cases where it is not clear whether an impact is important or not it can require considerable nuance to interpret results.</p><p>However fortunately for us, the change in model structure has had an absolutely massive impact which even a non-expert like me can tell should drive decision-making. The diagram below shows the results from running the community\u2019s numerical assumptions through a model designed to look broadly like Carlsmith\u2019s structural assumptions:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/h4wxv5l9cmyf0mch8ttk\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/fx4lxyqk6sbrluhlz8hb 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/cbbpmollu768rteqvjtj 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/dmgamphje65b1u7chnws 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/hl5iwsmuauhut4nbbznj 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/kyobuzot2pnsvt5hhvyf 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/gk81dtivbmqbwqdkfovj 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/kzlspbtmfd4o6ookg0mk 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/wt0dkznw8gtcabneckxa 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/hjn8owmg8flxpfammuqi 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/o4w5r9yoaro1wzr1owrb 940w\"></figure><p>It gives a risk of AI Catastrophe of around 18% (ie entirely in line with the community survey as expected - all results are going to be approximate because the simulation contains a lot of random elements which change between model runs). Carlsmith doesn\u2019t distinguish between \u2018In Control\u2019 and \u2018Out of Control\u2019 AI Risk so the value Open Philanthropy are specifically interested in is the 10% column on the far left.</p><p>The next diagram shows the output of the base case analysis of my time-dependent model:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/eq3p3psjkpjqonmewdn7\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/uubxuchf1jrvxufgcsqs 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/bebdkdcbnmedqiiflhmw 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/vam2bgj3ha9mjwvn1owy 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/kodkrg3s5sdufbtd8fyj 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/bw4hepher4gxtfuihlrd 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/iqhroyk0epr3afow7i2r 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/uhg4tur5elli6k35va1k 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/wjotjahxeoh5wkenktfg 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/hcfnrsy0wkg5l95he9bg 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/qmpzign8f4fvldhm8h3i 940w\"></figure><p>You can see that a nearly half of that probability mass has redistributed itself to non-AI competitors in the race, and the OOC AI Risk column has shrunk to just 6.7%. Consequently, I would conclude that <strong>accounting for time-dependency in models of AI Risk leads to an approximate 30% increase in the risk of OOC AI Catastrophe</strong>. That might be a slight overstatement of the result, because quite a lot of this probability has just found its way to other bad outcomes like nuclear war and totalitarian subjugation of humanity. Nevertheless, however you slice the result it is clear that humanity is a lot safer if we look at the full range of Alignment timelines rather than just the median timelines.</p><h3><u>Sensitivity analysis of time</u></h3><p>One of the interesting features of a time-dependent model is that it makes more sense of questions we might have about model dynamics. For example, Open Philanthropy are especially interested in scenarios where AI is invented prior to 2070, but are somewhat uncertain about exactly when AI will be invented conditional on that. We can see what impact the two extremes of possible impact will have by considering first the distribution of results if AI is invented in 2024:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/vcgxi6ky2zrcx3senxko\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/kvljkccqul83vrglat78 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/kzaj1esxjf6pavx1iftk 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/njiqb9f7aljwokfo8p8o 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/dzuzrmne4kxgbnr2zfrm 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/iyzwiwdx3pdpozgk1bsx 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/vqgmfvpdz531rylm88uk 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/zydut0aomye0qcdlag7i 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/hpgzjllvkslpilpfc6ll 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/u2gkq9gs9bizixqo0kjc 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/f6vojzz9cudyugivqgav 940w\"></figure><p>And if it is invented in 2070:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/spum1l4zgbsawahxdbhc\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/mocwklq6aqfaqrrrs0tz 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/dfqqsj8d5kxacbza0w9q 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/ebygavtckwnn8orpwjcs 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/n3mkqktkb9s7wu8c4gcs 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/yq0l5qkyqa1ibu7ftcnj 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/mw6fiv1ismvukiuz8p0w 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/khrvstxg8r1teq8wvhiv 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/uyxdsvqprs5rlexbqeu9 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/ruo5tfrb8uyn1sxx2y2x 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/firaxalpjbsuv8gcydjr 940w\"></figure><p>We see \u2013 as we should probably expect \u2013 that inventing AI later gives humanity a lot longer to try to find a solution for Alignment and reach a good AI future. The effect of delaying AI\u2019s invention by ~50 years is to approximately halve the risk of OOC AI Catastrophe. As a strict output from the model we might say that <strong>a delay to AI invention of one year increases the chance that humanity survives to experience a good AI future by about 0.1%&nbsp;</strong>(although I\u2019m not really confident my model is sophisticated enough to make this sort of judgement). What I can confidently say is that if you have short AI Catastrophe timelines the effect of time dependency is much less pronounced than if you have long AI Catastrophe timelines \u2013 effects which become more pronounced over time will have less of an impact if you compress the time in which they are able to occur.&nbsp;</p><h3><u>Sensitivity analysis of original inputs</u></h3><p>I hope these results have a ring of truth to them, because I think they reflect the uncertainty in the community survey quite accurately. The diagram below shows the community\u2019s estimates for AI Invention timelines versus their estimates for AI Alignment timelines. The 45 degree line shows the cutoff where people believe that AI will be Aligned before it is Invented. Any datapoint at the far edge of the graph indicates that either the respondent gave a value that was too big to display or didn\u2019t give a value and therefore is assumed to believe that AI cannot be Aligned (that is, either way they do not believe that AI will be Aligned before Invented). Only one brave iconoclast believes AI will be Aligned before it is Invented, which is why it might be surprising that adding in time dependency based on this survey makes any difference to the results.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/f4ibgohy7icf5kd66dct\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/cy6c3uqfig5yvz8oyh29 112w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/shaeij8pdbe9htuzhlsb 192w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/wef3fyjvmab94j33cvyj 272w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/dxoerog0zsieh4vdkrow 352w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/wreoiefbos4rgxxtsevq 432w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/lznyd7qxz5gwjrucb4ky 512w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/r7q9zh5tbhed4vghrfg3 592w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/szdp5wbmjzq64nw7vmtu 672w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/uscy1ttt3vwrjdmeuq5h 752w\"></figure><p>However, the next graph shows the same plotting for people\u2019s estimates of AI Catastrophe timelines alongside their AI Alignment timelines. About 20% of responses are showing Alignment as occurring strictly before Catastrophe, and there are a few more which are almost exactly on the 45 degree line. In fact, of the respondents who believe Alignment is possible within the next century, 65% conclude that Alignment is likely to occur before Catastrophe.&nbsp; This is why I believe that the inclusion of time-dependency is likely to improve the way the model aggregates community forecasts; simply taking the median timelines ignores 20% of people and gives a distorted result.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/zpcm5yyu7uii2qnczgyh\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/uh790xzohjaipqyejpcc 112w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/v4d242xqk37qjmh0rmwg 192w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/ipzfbfh77nfvqnej1iku 272w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/madxmy4zglycgapmjvp5 352w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/jkhder5zqk58t5v8ipmg 432w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/kxoiigrkvn9zqwdpatph 512w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/rm3tktrygqr8ibbjwwk1 592w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/wjmfpzbsfrj8qdkuje86 672w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/x6k16qjqeyzthnggmmwn 752w\"></figure><h1>3. Conclusions</h1><p>This essay makes the claim that parameters of AI Risk models have been extensively cultivated, to the detriment of discussion about model structure. I present a proof by example of this claim in Section 2; overlooking time-dependency as a structural factor leads to a significant over-estimate of the risk of AI Catastrophe, because it ignores worlds in which AI Alignment is hard but possible (and the dynamic mechanisms which drive the Alignment process). As I am not an expert in AI Risk modelling, the specific numbers and results I arrive at should be taken with a degree of caution. However, I think they are sufficient for proving the key contention of this essay, which is that <strong>structural uncertainty is a significant blindspot in the way in which AI Risk is articulated and discussed.&nbsp;</strong>I would be extremely surprised if someone more expert than me couldn\u2019t find an effect twice the size I managed with a trivial amount of work by combing over other commonly-accepted structural assumptions in AI Risk modelling with a fine-tooth comb.</p><p>There is a nuanced but important point involved in interpreting these results; I have not made a \u2018better\u2019 model than Carlsmith in any kind of unambiguous way. By making my model more granular I have introduced problems that Carlsmith can (correctly) ignore because his model is more abstract. This tradeoff between granularity and abstractness is a real problem in professional structural uncertainty analysis, and part of what makes modelling an art rather than a science. For the record, consider some ways in which my approach is clearly inferior to Carlsmith:</p><ul><li>We don\u2019t know whether the rate-limiting step for Alignment will be time (like I propose) or perhaps money / expertise / some other thing. My model can\u2019t investigate this and so is at high risk of reaching the wrong conclusion but Carlsmith abstracts away the problem and so has more flexibility.</li><li>We don\u2019t know whether \u2018solving Alignment\u2019 will be a one-time event (like I propose in the base case), or a multi-step process (like I propose in this footnote<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz007vzssqfg\"><sup><a href=\"#fnz007vzssqfg\">[5]</a></sup></span>). You have to tie my model in knots to get anything sensible out of it on this topic, whereas the Carlsmith structure is well set up to turn the main steps into smaller discrete steps if required.</li><li>We don\u2019t know how governments will react to AI risk by speeding up or slowing down funding for Alignment research. I think both my model and the Carlsmith model struggle with this issue, and solving it would require a lot of work to both models. However, on points I\u2019d say Carlsmith has the better of it \u2013 Carlsmith won\u2019t currently give you the <i>wrong</i> answer because it abstracts this step away, whereas there is a great danger of my model being simple, compelling and wrong because it makes specific predictions about AI timelines.</li><li>My model is also way more complicated, which all other things being equal would lead you to prefer Carlsmith just on aesthetic / maintainability grounds.</li></ul><p>I think on balance my model makes sense for the specific problem Open Philanthropy are interested in - the extra complexity of adding a time-dependent element to the model's ontology is worth it when the problem has a time-dependent element in it (such as the requirement that AI be invented before 2070). However that wouldn't necessarily be true for other problems, even problems which look quite superficially similar.</p><p>That said, there are some very clear ways in which my approach could be improved to unambiguously improve its suitability for Open Philanthropy\u2019s question:&nbsp;</p><ul><li>An obvious starting point would be to replicate my results in a more sensible model structure (ie rather than trying to cram a time-dependent structure into a deterministic decision tree, instead use an individual-level Markov Chain Monte-Carlo simulation, or even a Discrete Event Simulation if you want to flex the analyst muscles). Although I am confident my results are broadly right, there may be some interesting dynamics which are obscured by the structure I have chosen<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefn4qilbghyzl\"><sup><a href=\"#fnn4qilbghyzl\">[6]</a></sup></span>.</li><li>An expert in AI Risk could overhaul my approach to Alignment. For example, I list a few ways in which my model is inferior to the Carlsmith model on this point but since these are features of my <i>implementation</i> and not inherently a tradeoff that needs to be made when modelling they could be fixed pretty easily by someone who know what they were talking about. My implementation was to be as abstract as possible on topics I didn\u2019t know much about, but an expert doesn\u2019t have that constraint.</li><li>I focus very heavily on Alignment because the thinking has already been done for me on that issue. There is no particular reason other than convenience why I focussed my attention here, and it is hopefully obvious that the same approach could be applied elsewhere just as easily. For example, a live discussion in AI Risk at the moment is whether American companies pausing AI research (but not companies in other countries) raises or lowers the risk of an OOC AI being deployed; applying some time dependent dynamics to the \u2018Deployment\u2019 step in the Carlsmith model could provide excellent insight on this problem.</li></ul><p>More generally, I can offer a few conclusions of relevance to AI Risk analysts:</p><ul><li>Analysts discussing AI Risk should describe the structure of their model much more explicitly. I observe there is a bit of a tendency on the forums to be cagey about one\u2019s \u2018actual\u2019 model of AI Risk when presenting estimates of Catastrophe, and imply that the \u2018actual\u2019 model of AI Risk one has is significantly more complicated than could possibly be explained in the space of a single post (phrases like, \u201cThis is <u>roughly</u> my model\u201d are a signifier of this). Models should be thought of as tools which help us derive insight into dynamic processes, and not binding commitments to certain worldviews. A well-specified model which captures some essential insight about the problem is worth hundreds of posts trying to gesture at the same insight. Alongside the existing culture of productive critique of parameter assumptions, there should be a culture of open peer review explicitly calling out structural assumptions made in these models (which is made more difficult if people are vague about what exactly they are modelling).&nbsp;</li><li>Discussions about structures of AI Risk (what entities exist in the decision problem and how they interrelate to each other) should be treated as at least as meaningful as discussions about parameters of AI Risk. What I mean by this is that if you would accept that it is a worthwhile contribution to put some upper-bound on the risk of an \u2018escaped\u2019 AI proving to be uncontainable you should find it at least as worthwhile to have someone present a few different models of what \u2018uncontainable\u2019 might mean in practice and which of these models tend to over- or under-estimate risk in what circumstances. An excellent template for this sort of analysis is Soares (<a href=\"https://forum.effectivealtruism.org/posts/vC6v2iTafkydBvnz7/agi-ruin-scenarios-are-likely-and-disjunctive\">2022</a>), which very neatly argues that the Carlsmith model will systematically underestimate the chance of Catastrophe in scenarios which are more \u2018disjunctive\u2019 (nothwithstanding that \u2018conjunctive\u2019 and \u2018disjunctive\u2019 aren\u2019t really terms which are used outside AI Risk analysis).</li></ul><p>Although I am pleased I\u2019ve been able to contribute to a debate on the probabilities of AI Catastrophe, I\u2019m under no illusions that this is the most sophisticated structural critique of AI Risk modelling that is possible; it is just the critique which I am qualified to offer. The nature of modelling is that tiny assumptions that other people overlook can have a massive impact on the eventual outcomes, and the assumptions which are important are not necessarily those which modellers are natively interested in. I believe there would be considerable value in individuals or teams taking a detailed look at each step in the Carlsmith model and hunting for further errant assumptions. I'd be delighted if experts were able to show that my work on time-dependency is ultimately a trivial footnote to the <i>real</i> driver of OOC AI Catastrophe Risk, since identifying the true structural drivers of risk is the first step in a plan to mitigate and ultimately defeat that risk.</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9ys1z65kqft\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9ys1z65kqft\">^</a></strong></sup></span><div class=\"footnote-content\"><p>There are also a handful of incomplete attempts to move beyond the \u2018weighted coinflip\u2019 approach and into more sophisticated modelling structures. The most salient of these would probably be the <a href=\"https://www.lesswrong.com/s/aERZoriyHfCqvWkzg\">MTAIR project</a>, although I believe that MTAIR is now sadly defunct following the FTX collapse.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnz65l7ke45f\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefz65l7ke45f\">^</a></strong></sup></span><div class=\"footnote-content\"><p>As a hypothesis, my guess is that a lot of AI Alignment researchers come into the movement via Effective Altruism and then find the AI Catastrophe arguments convincing once they have already bought into broader EA logic. By coincidence, high-profile conventional charitable interventions (meaning, for example, those funded by GiveWell) are extremely well suited to the deterministic decision tree, and so the \u2018weighted coinflip\u2019 model is an excellent default choice in almost all conventional charity analyses. If many AI Alignment researchers have been socialised into always using the deterministic decision tree structure, and that approach always works, it is probably unsurprising that this approach becomes like water to a fish \u2013 even if it is understood that other model structures exist, there are significantly more important things to be doing than messing around with the approach that is fast, logical and has always worked in the past.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnav80jhv3od8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefav80jhv3od8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Of course, nobody \u2013 least of all Carlsmith \u2013 believes that his model <i>really</i> describes reality any more than I believe my new model <i>really</i> describes reality. Instead, Carlsmith has made a simplifying assumption that treating AI Risk timelines as implicitly fixed will still allow him to investigate the sorts of dynamics he is interested in. He is probably right about this - Carlsmith asks and answers the question, \"<a href=\"https://arxiv.org/abs/2206.13353\">Is Power-Seeking AI an Existential Risk?</a><strong>\" </strong>for which he is basically looking for a binary answer (\"Yes it is\" or \"No it isn't\"). Consequently it doesn't really matter if his model oversimplifies as long as it produces robust order-of-magnitude risk estimates. However, Open Philanthropy ask the question, \"Conditional on AGI being developed by 2070, what is the probability that humanity will suffer an existential catastrophe due to loss of control over an AGI system?\" for which a more complex model is needed, one which can produce results on a more granular level than just order-of-magnitude.</p><p>I will revisit this theme later in the essay, but an observation that modelling outsiders sometimes miss is that there is no such thing as a 'better' or 'worse' model provided you meet some minimum bar for competence in execution. Instead, there are models which are better suited for some questions and others which are better suited for other questions. So the paragraph spawning this footnote might look like a criticism of Carlsmith when instead it is really a compliment - Carlsmith has found a very simple and elegant way of modelling what he needs which (sadly) oversimplifies relative to what I need.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn30s4wkqfkfo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref30s4wkqfkfo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>A few small notes on the base case that aren't interesting enough to include in the main body of the text:</p><p>- I have forced AI to be invented before 2070, in line with Open Philanthropy\u2019s preferences, by capping the data I generated from my survey to dates prior to then and forcing the probability it happens to be 100%.&nbsp;</p><p>- I have included X-Risk and S-Risk drawn from literature sources - Ord (<a href=\"https://forum.effectivealtruism.org/posts/Z5KZ2cui8WDjyF6gJ/some-thoughts-on-toby-ord-s-existential-risk-estimates\">2020</a>) and Caplan (<a href=\"https://academic.oup.com/book/40615/chapter-abstract/348242235\">2008</a>) respectively. S-Risk estimates seem very low to me - it would be extremely interesting to investigate this further in the future because I think it might be another area where some careful structural thinking could reveal some interesting insights. But it isn't AI Risk so isn't relevant for now.</p><p>- Some people did not give a date for when they thought Alignment would occur, which I interpreted as meaning they did not think it would ever be possible (backed up by reading some of their comments on their entries). I added a bit of an ad hoc term to make Alignment literally impossible in those worlds, so the only possible outcome is some sort of Catastrophe. If I ran the survey again, I would explicitly ask about this.</p><p>- PS1, PS2 and PS3 are my attempt to include some early thoughts about \u2018partial successes\u2019 in Alignment which are not as strict as the definition Carlsmith uses where Alignment reduces the risk of OOC AI to zero. I would describe my success here as \u2018mixed\u2019, so I have turned them all off for the base case.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnz007vzssqfg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefz007vzssqfg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I had enough time to code up some exploratory work on the idea that 'Alignment' could be a moving target depending on how far advanced we were down the pathway towards AI Catastrophe. In the model below, \u2018Alignment\u2019 occurs either if we find a way to make AI completely safe as in the main model, but also if we develop a TEST which can show us whether an AI is aligned before we expose any AI to high-risk inputs. The intuition here is that we might be able to convince governments to act to ban unsafe AIs if we haven't already handed power to AIs, but after handing power to AIs we need some kind of 'weapon' to defeat them. The graph below shows the result of this analysis:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/wu3vloohptrhxlm22p5a\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/fflqydobnmxlaumexgza 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/c3lz9skewt3bui38llaf 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/lhymafiaghmdalcnihnp 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/ydrj902fz4bktm8zuuwf 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/b1gpcbh3oubx3cf6b5dd 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/irkseecarlfqxnfiq5lc 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/dmk3ztbwt7qpg1hwfrtn 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/hfxrtz0ovkddjrit0len 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/plfyytigpdy2uxr7esm9 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JjAjJ53mmpQqBeobQ/lvcixsk3f549qqekulys 1003w\"></figure><p>Unsurprisingly, this results in a significant probability mass redistribution away from AI Risk towards Alignment, as scenarios which were once the absolute worst-case for OOC AI Risk have the possibility of being defused even before AI is invented.</p><p>I got a bit anxious at the thought of presenting these alongside the main results because I think I've done a <i>fairly</i> good job of only talking about things I understand properly in the main body of the text, and I don't understand the nuts-and-bolts elements of Alignment at all well. But I thought people might be interested to see this even if it isn't perfect, if only to prove my point that there\u2019s probably another halving of risk available to the first team who can present a plausible account of how a \u2018multi-step\u2019 model of alignment could work.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnn4qilbghyzl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefn4qilbghyzl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Part of the art of modelling is knowing when you have made the model complex <i>enough</i> for whatever purpose you intend to use it for. Any model which fixed the problems with my model would inevitably have problems that needed to be fixed by <strong>another</strong> model and so on. Although this conveyer belt keeps modellers like me (somewhat) gainfully employed, it can be a trap for organisations who just want to use the best possible model to make decisions, because \u2018best possible\u2019 is a moving target. As discussed above, the structure needs to match the needs of the decision problem and there isn't a generic solution for how to find the best structure for a particular job.</p></div></li></ol>", "user": {"username": "Froolow"}}, {"_id": "GcthJoqdDrtgiHKs9", "title": "Should global health donors focus on R&D?", "postedAt": "2023-05-19T11:24:17.377Z", "htmlBody": "<p>Global health R&amp;D strikes me as having very high expected value, but might be difficult for governments in low income countries to justify to voters when it could spent on urgent object level health interventions which produce benefits more quickly.</p>\n<p>Does that mean donors should focus more on R&amp;D (eg - give more funding to CEPI than to the Pandemic Fund)? Is this idea fleshed out in better detail somewhere in the global health world?</p>\n", "user": {"username": "freedomandutility"}}, {"_id": "FrshKTu34cFGGsyka", "title": "Announcing a new organization: Epistea", "postedAt": "2023-05-22T05:52:51.198Z", "htmlBody": "<h2>Summary</h2><p>We are announcing a new organization called&nbsp;<a href=\"http://epistea.org/\"><u>Epistea</u></a>. Epistea supports projects in the space of existential security, epistemics, rationality, and effective altruism. Some projects we initiate and run ourselves, and some projects we support by providing infrastructure, know-how, staff, operations, or fiscal sponsorship.</p><p>Our current projects are&nbsp;<a href=\"https://fixedpoint.house/\"><u>FIXED POINT</u></a>,&nbsp;<a href=\"http://praguefallseason.com/\"><u>Prague Fall Season</u></a>, and the Epistea Residency Program. We support&nbsp;<a href=\"https://acsresearch.org/\"><u>ACS</u></a> (Alignment of Complex Systems Research Group),&nbsp;<a href=\"https://www.pibbss.ai/\"><u>PIBBSS</u></a> (Principles of Intelligent Behavior in Biological and Social Systems), and&nbsp;<a href=\"https://humanaligned.ai/\"><u>HAAISS</u></a> (Human Aligned AI Summer School).</p><h2>History and context</h2><p>Epistea was initially founded in 2019 as a rationality and epistemics research and education organization by Jan Kulveit and a small group of collaborators. They ran an experimental workshop on group rationality, the&nbsp;<a href=\"https://www.lesswrong.com/posts/gBtHgFBcvRkQwgaGg/epistea-summer-experiment-ese\"><u>Epistea Summer Experiment</u></a> in the summer of 2019 and were planning on organizing a series of rationality workshops in 2020. The pandemic paused plans to run workshops and most of the original staff have moved on to other projects.&nbsp;</p><p>In 2022, Irena Kot\u00edkov\u00e1 was looking for an organization to fiscally sponsor her upcoming projects. Together with Jan, they decided to revamp&nbsp;<u>Epistea</u> as an umbrella organization for a wide range of projects related to epistemics and existential security, under Irena\u2019s leadership.&nbsp;&nbsp;</p><h2>What?</h2><p><u>Epistea</u> is a service organization that creates, runs, and supports projects that help with clear thinking and scale-sensitive caring. We believe that actions in sensitive areas such as existential risk mitigation often follow from good epistemics, and we are particularly interested in supporting efforts in this direction.</p><p>The core Epistea team is&nbsp;<a href=\"https://www.timeout.com/news/prague-has-just-been-voted-the-most-beautiful-city-in-the-world-091521\"><u>based in Prague, Czech Republic</u></a>, and works primarily in person there, although we support projects worldwide. As we are based in continental Europe and in the EU, we are a good fit for projects located in the EU.</p><p>We provide the following services:</p><ul><li>Fiscal sponsorship (managing payments, accounting, and overall finances)</li><li>Administrative and operations support (booking travel, accommodation, reimbursements, applications, visas)</li><li>Events organization and support (conferences, retreats, workshops)</li><li>Ad hoc operations support</li></ul><h2>We currently run the following projects:</h2><h3><a href=\"https://fixedpoint.house/\"><u>FIXED POINT</u></a></h3><p>Fixed Point is a community and coworking space situated in&nbsp;<u>Prague</u>. The space is optimized for intellectual work and interesting conversations but also prioritizes work-life balance. You can read <a href=\"https://forum.effectivealtruism.org/posts/rtnWMb9ewbNnhhFPd/announcing-the-prague-community-space-fixed-point\">more about FIXED POINT here</a>.</p><h3><a href=\"http://praguefallseason.com/\"><u>Prague Fall Season</u></a></h3><p>PFS is a new model for global movement building which we&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ZxHnaPmxmBrZmerBj/announcing-the-prague-fall-season\"><u>piloted in 2022</u></a>. The goal of the Season is to have a high concentration of people and events, in a limited time, in one space, and working on a specific set of problems. This allows for better coordination and efficiency and creates more opportunities for people to collaborate, co-create and co-work on important projects together, possibly in a new location - different from their usual space. Part of PFS is a residency program.&nbsp;You can read <a href=\"https://forum.effectivealtruism.org/posts/DPHu3p3LwTFBxB3ed/announcing-the-prague-fall-season-2023-and-the-epistea\">more about the Prague Fall Season here</a>.</p><h2>Additionally, we support:</h2><ul><li><a href=\"https://acsresearch.org/\"><u>ACS</u></a> - Alignment of Complex Systems Research Group</li><li><a href=\"https://www.pibbss.ai/\"><u>PIBBSS</u></a> - Principles of Intelligent Behavior in Biological and Social Systems</li><li><a href=\"https://humanaligned.ai/\"><u>HAAISS</u></a> - Human Aligned AI Summer School</li></ul><h2>Who?</h2><ul><li>Irena Kot\u00edkov\u00e1 leads a team of 4 full-time staff and 4 contractors:</li><li>Jana Meixnerov\u00e1 - Head of Programs, focus on the Prague Fall Season</li><li>Viktorie Havl\u00ed\u010dkov\u00e1 - Head of Operations</li><li>Martin Hr\u00e1dela - Facilities Manager, focus on Fixed Point</li><li>Jan \u0160rajhans - User Experience Specialist</li><li>Karin Neumanov\u00e1 - Interior Designer&nbsp;</li><li>Linh Dan Leov\u00e1 - Operations Associate</li><li>Ji\u0159\u00ed N\u00e1dvorn\u00edk - Special Projects</li><li>Franti\u0161ek Drahota - Special Projects</li></ul><p>The team has a wide range of experience in program, administrative, operations, and events work (organizing CFAR workshops, CFAR rEUnions, the EAGxPrague conference, Prague Fall Season 2022, managing the FIXED POINT house, and dozens of smaller events).</p><h2>Work with us</h2><p>If you are looking for a fiscal sponsor for your project in the EU, or would otherwise like to collaborate, <a href=\"https://airtable.com/shrAoqqEMt7gVo3rz\">reach out to us using this form</a>.</p><h2>Support us</h2><p>We currently don\u2019t have institutional funding and rely on project funding and overhead fees from other projects. This is not ideal and we are looking for a funder to support our efforts long-term. If you would like to support us, please contact us at&nbsp;<a href=\"mailto:info@praguefallseason.com\"><u>hello@epistea.org</u></a>.&nbsp;</p>", "user": {"username": "Epistea"}}, {"_id": "kZPBBEDKJEcGdxj4t", "title": "Effective Altruism UQ Winter Fellowship", "postedAt": "2023-05-19T02:33:06.869Z", "htmlBody": "<p><strong>Improve the lives of others and&nbsp;increase your impact</strong></p><p>Learn new concepts, frameworks and counter intuitive ideas to help you have&nbsp;<strong>more impact in your life and career</strong>.</p><p><strong>Connect with others focused on impact and&nbsp;meet like-minded people</strong></p><p>Discuss a curated curriculum with other people who&nbsp;<strong>care deeply about improving the lives of others</strong>&nbsp;in the most effective ways they can.</p><p><strong>Tuesdays during the University of Queensland winter break</strong></p><ol><li>June 27th @ 12:30\u20132pm</li><li>July 4th&nbsp;@ 12:30\u20132pm</li><li>July 11th @ 12:30\u20132pm</li><li>July 18th @ 12:30\u20132pm</li><li>July 25th @ 12:30\u20132pm (Sem 2, Week 1)</li><li>August 1st @ 12:30\u20132pm (Sem 2, Week 2)</li></ol><p>Before each session, there will be around 2 hours of pre-learning (watching videos, listening to podcasts, etc.) so you can spend the session using key ideas or discussing them.</p><p>We plan to offer a hybrid mode where you can attend sessions on campus or remotely, so you don't need to commute if it's hard for you to get here. However, we expect it'll be more rewarding for most participants to meet on campus.</p><p><strong>Applications close Sunday June 4th. Spaces are limited. Apply here: http://bit.ly/ea_winter_2023</strong></p>", "user": {"username": "mnoetel"}}, {"_id": "4p8RpK2fYKFmEcA9w", "title": "OPTIC [Forecasting Comp] \u2014 Pilot Postmortem", "postedAt": "2023-05-19T10:10:08.243Z", "htmlBody": "<p><a href=\"https://bit.ly/optic-website\"><u>OPTIC</u></a> is an in-person, intercollegiate&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/forecasting\"><u>forecasting</u></a> competition where undergraduate forecasters compete to make accurate predictions about the future. Think olympiad/debate tournament/hackathon, but for forecasting \u2014 teams compete for thousands of dollars in cash prizes on question topics ranging from geopolitics to celebrity twitter patterns to financial asset prices.</p><p>We ran the pilot event on Saturday, April 22 in Boston and are scaling up to an academic league/olympiad. We\u2019ll be hosting tournaments in Boston, London, and San Francisco in the fall \u2014 see our&nbsp;<a href=\"https://bit.ly/optic-website\"><u>website</u></a> at opticforecasting.com, and contact us at&nbsp;<a href=\"mailto:opticforecasting@gmail.com\"><u>opticforecasting@gmail.com</u></a> (or by dropping a comment below)!</p><p>&nbsp;</p><h1>What happened at the competition?</h1><h2>Attendance</h2><p>114 competitors from 5 different countries and 13 different US states initially registered interest. A significant proportion indicated that they wouldn\u2019t be able to compete in this iteration (logistical/scheduling concerns), but expressed interest to compete in the next one. 39 competitors RSVP\u2019d \u201cyes,\u201d though a few didn\u2019t end up attending and a couple unregistered competitors did show up. At the competition, the total attendance was 31 competitors in 8 teams of 3-4, with 2 spectators.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/xtyeu8i3fq7nxe3krvyf\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/l6ldu5frxbsdwh8gutzc 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/m7deczpmjk2kdh4mgpfk 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/cgrz0ltmfkt5jyncdmzi 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/wsiewr1zfxubqi2towmv 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/z7kl8a3ouiapxgmyqdyn 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/xtx6boy7m3iemej7nvy3 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/karia9322hiomwv3dm5m 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/a0cnd1nfpisquas0iyn6 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/k9dzwtvx0vyqydrvr5oa 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/sutgwl0wmjtus63slg6a 2048w\"></p><h2>Schedule</h2><ul><li>1 hour check-in time/lunch/socialization</li><li>10 min introduction speech</li><li>1 hour speech by&nbsp;<a href=\"https://www.linkedin.com/in/sethblumberg/\"><u>Seth Blumberg</u></a> on the future of forecasting (Seth is a behavioral economist and head of Google\u2019s internal prediction market, speaking in his individual capacity) \u2014 you can watch the speech&nbsp;<a href=\"http://bit.ly/optic-seth-blumberg-speech\"><u>here</u></a></li><li>Questions released; 3 hours for forecasting (\u201cforecasting period\u201d)</li><li>10 min conclusion speech, merch distribution</li><li>20 min retrospective feedback form</li></ul><h2>Forecasting (teams, platform, scoring, prizes, etc)</h2><p>Competitors were split up into teams of 3-4. They submitted one forecast per team on each of 30 questions through a private tournament on Metaculus. Teams\u2019 forecasts were not made visible to other teams until after the forecasting period closed. Questions were a mix of binary and continuous, all with a resolution timeframe of weeks-months; all will have resolved by August 15. At that point, we\u2019ll score the forecasts using log scoring.</p><p>We will have awarded $3000 in cash prizes, to be distributed after the scoring is completed:</p><ul><li>1st place \u2014 $1500</li><li>2nd place \u2014 $800</li><li>3rd place \u2014 $400</li><li>Other prizes \u2014 $300</li></ul><p>Note that prizes for 1st-3rd place are given to the team and split between the members of the team.</p><h2>Funding</h2><p>We received $4000 USD from the&nbsp;<a href=\"https://manifund.org/rounds/acx-mini-grants\"><u>ACX Forecasting Mini-Grants</u></a> on&nbsp;<a href=\"https://manifund.org\"><u>Manifund</u></a>, and $2000 USD from the&nbsp;<a href=\"https://funds.effectivealtruism.org/funds/far-future\"><u>Long Term Future Fund</u></a>.</p><h2>Organizers</h2><p>Our organizing team comprises:&nbsp;</p><ul><li>Jingyi Wang (Brandeis University EA organizer)</li><li><a href=\"https://www.linkedin.com/in/saulmunn/\"><u>Saul Munn</u></a> (Brandeis University EA organizer)</li><li><a href=\"https://www.linkedin.com/in/tom-shlomi/\"><u>Tom Shlomi</u></a> (Harvard University EA organizer)</li></ul><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/i1wta4jj2ldatmwdiygo\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/wufpdbprrljvdz7rog4n 260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/nhtvbynb805sgxlrnsxe 520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/ewhyjlzrtn5bjwcyflgs 780w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/r9mu6hznwbixwobrcd9t 1040w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/qwtffp5p1oiibjlyqjlc 1300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/tb4sqfeyh8clgzjqpxgx 1560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/tfbz8z6jfwcmnntlgjth 1820w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/vo19ybtghcxnlxzhjdwh 2080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/zwpler5yzslttr9ookvo 2340w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/gclao7fx8ywi90vblcvq 2505w\"><figcaption>Left to right: Tom, Saul, Jingyi</figcaption></figure><p>Also,&nbsp;<strong>Saul and Jingyi will be attending EAG London</strong> \u2014 please reach out if you want to be involved with OPTIC, have questions/comments/concerns, or just want to chat!</p><p><br>&nbsp;</p><p>***</p><p><br>&nbsp;</p><p>The following is a postmortem we wrote based on the recording of a verbal postmortem our team held after the event.</p><h1>Summary</h1><p>Overall, the pilot went really well. We did especially well with setting ourselves up for future iterations, with flexibility/adaptability, and resource use. We could have improved time management and communication, as well as some other minor issues. We\u2019re excited about the future of OPTIC!</p><p><br>&nbsp;</p><h1>What went well</h1><h2>Strong pilot/good setup</h2><p>As a pilot, the April 22 event definitely has set us up for future iterations of OPTIC. We now have a network of previous team captains and competitors from schools all around the Boston area (and beyond) who have indicated that they\u2019d be excited to compete again. We have people set up at a few schools around the country who are going to start forecasting clubs which will compete as teams in forecasting tournaments. We have undergraduate interest (and associated emails) from five countries. We are connected with a myriad of people in the forecasting space who have provided extremely helpful advice for OPTIC.&nbsp;</p><h2>Flexibility</h2><p>The early image that we had of OPTIC in February was very different from what happened on April 22. Throughout the course of 2.5 months, we were able to adapt and shift key decisions based on new information and advice (prediction market vs forecasting tournament, gameable &amp; conditional questions, venue choices, etc).&nbsp;</p><h2>Good resource use/access (time notwithstanding)</h2><p>In terms of connecting with people in the forecasting space, we used our resources very well. We received advice from many experienced people. We regularly reached out to people we thought could help us in some way (and often, they did).&nbsp;</p><p>Saul attended EAG Bay Area, where he was able to discuss OPTIC plans and decisions with several really helpful people. Prior to attending EAG, Saul and the team created a \u201cKey Decision Document\u201d that listed all of the crucial decisions that needed answers or better understanding \u2014 this was an invaluable way to set up conversations with those who had expertise around legitimately important decisions. This was hugely influential in determining several key decisions \u2013 resolution timeline, question topics, etc.</p><p><br>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/qph5tktbfkvnw36shsct\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/vvyqrhw1gvuenylhkupg 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/pziixrd4e9ejk1iinixy 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/uqbnljhphqqjamm88hii 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/tyfwcorpm0ixgjsruimk 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/coqvn2svftaumjlbickz 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/finyy6d6hsro3ejttaob 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/aucthrpnauxnufzfapuj 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/pop6psodh9ujdelmzaee 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/hveebry9zdladtaqv2gf 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4p8RpK2fYKFmEcA9w/idu4l5bp7lbreunal0le 2048w\"></p><p><br>&nbsp;</p><h1>What could have been improved</h1><h2>Time management</h2><p>We were extremely time-constrained, especially close to the competition date. Partly, this was because much of the 2.5 months was spent figuring out exactly what we wanted OPTIC to be.&nbsp;</p><p>We didn\u2019t have a set timeline at the beginning of the 2.5 months, which would have helped keep us on track. There were no explicit deadlines for tasks like \u201cbook a venue\u201d or \u201cfind team captains\u201d on longer timescales \u2014 week-to-week/month-to-month.</p><p>Our meetings were irregularly/haphazardly scheduled, so sometimes we couldn\u2019t check things over with each other in a timely manner.</p><h2>Communication</h2><h3>Internal</h3><p>Naturally, because we\u2019re all full-time students, we're not able to be as responsive as we'd like. However, there were times when this really ate into our \u201ctime crunch\u201d problem.</p><h3>External</h3><p>The internal time crunch also negatively affected our communication with team captains and competitors. In particular, team captains, as organizing agents, needed to receive certain information much earlier than they did. This resulted in unnecessary confusion/miscommunication between the core organizers &lt;&gt; team captains &lt;&gt; competitors.</p><p>We took too long to respond to messages/emails. We left some emails in our inbox for as long as a week; this should not have happened.</p><h2>Forecasting Questions</h2><p>There were a few logistical issues with the forecasting questions at the event \u2013 e.g., one of the questions was set up as continuous when it should have been discrete.&nbsp;</p><p>The questions could have covered a more diverse range of topics.</p><p>The questions were built pretty last minute, which left little time to edit/improve/ask for feedback.</p><h2>Event logistics</h2><p>The forecasting portion of the event was a continuous 3 hour chunk where competitors made predictions on the 30 questions. We received feedback from several competitors that this was too long. Also, some competitors finished their forecasting early, and left before the official \u201cend\u201d of the event. While this wasn\u2019t a bad thing, it meant there was no definite conclusion \u2014 more of a trickle of competitors leaving over the last half-hour.</p><p>We also experienced distressingly high uncertainty with our venue booking up to a week before the event \u2014 unexpected complications of booking spaces at universities happened again and again.</p><h1>Actionable steps for the future</h1><h2>Timeline</h2><p>We will create a timeline for our next OPTIC event early on. Importantly, it will include deadlines \u2013 e.g.,&nbsp; have the venue definitively booked by x date, have all team captains chosen by y date, have all departmental email blasts sent by z date, etc.</p><p>It will include buffer time between our last non-event deadline and the event, to avoid the last-minute crunch we experienced with the pilot.</p><h2>Set weekly meeting</h2><p>We have set a recurring weekly meeting time. This way, despite what communication does or doesn\u2019t happen in a week, we have concrete and dedicated OPTIC time with each other.</p><p>If we don\u2019t have much to discuss, it\u2019ll just be a quick check-in.</p><h2>Change event logistics</h2><p>In response to notes that the forecasting part of the event was too long, we may experiment with different scheduling logistics in the future. For example, instead of one 3-hour forecasting period, we could do a small forecasting period, lunch, then a second small forecasting period.&nbsp;</p><h2>Email response speed</h2><p>Rather than waiting until a meeting for everybody to look over an important email, a quick call/text from another co-organizer will suffice.</p><p>For basic emails, we have appointed a general \u201cemail person\u201d (Saul) who can respond to emails independently, without conferring with the other two organizers.</p><p><br>&nbsp;</p><h1>Special thanks to\u2026</h1><p><a href=\"https://www.linkedin.com/in/sethblumberg/\"><u>Seth Blumberg</u></a>, for your insightful and inspiring&nbsp;<a href=\"http://bit.ly/optic-seth-blumberg-speech\"><u>speech</u></a></p><p><a href=\"https://www.linkedin.com/in/nathansmorrison/\"><u>Nate Morrison</u></a>,&nbsp;<a href=\"https://www.linkedin.com/in/anastasiamiliano/\"><u>Anastasia Miliano</u></a>,&nbsp;<a href=\"https://www.linkedin.com/in/dan-schwarz-3805b854/?trk=public_profile_browsemap\"><u>Dan Schwarz</u></a>, as well as the rest of the&nbsp;<a href=\"https://www.metaculus.com/about/\"><u>Metaculus</u></a> team, for setting up a private OPTIC market and for technical troubleshooting</p><p><a href=\"https://astralcodexten.substack.com\"><u>Scott Alexander</u></a> and&nbsp;<a href=\"https://www.linkedin.com/in/austinch/\"><u>Austin Chen</u></a>, for hosting the&nbsp;<a href=\"https://manifund.org/rounds/acx-mini-grants\"><u>ACX/Manifund forecasting mini-grants</u></a>,&nbsp;<a href=\"https://astralcodexten.substack.com/p/open-thread-271?utm_source=post-email-title&amp;publication_id=89120&amp;post_id=113728154&amp;isFreemail=false\"><u>pubbing us on ACX</u></a> (thanks Scott!), and offering advice (thanks Austin!)</p><p><a href=\"https://www.linkedin.com/in/domenicdenicola/\"><u>Dominic Denicola</u></a>, for investing your confidence and money in us and OPTIC, offering advice, and finding us an amazing speaker</p><p><a href=\"https://www.linkedin.com/in/juan-gil-8b8a38125/\"><u>Juan Gil</u></a> and&nbsp;<a href=\"https://nikolajurkovic.com\"><u>Nikola Jurkovic</u></a>, for your consistent help, advice, and support</p><p><a href=\"https://www.linkedin.com/in/henrytolchard/\"><u>Henry Tolchard</u></a>, for your help and experience from the&nbsp;<a href=\"https://www.infer-pub.com/2022-ea-college-forecasting-tournament\"><u>2022 online forecasting tournament</u></a></p><p><a href=\"https://www.linkedin.com/in/sadie-giddings-a00700278/\">Sadie Giddings</a>, for designing beautiful merch</p><p><strong>Also, thanks to the following people for general advice and feedback:&nbsp;</strong><a href=\"https://www.linkedin.com/in/ozzie-gooen-ab307226/\"><u>Ozzie Gooen</u></a>,&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/team/catherine-low\"><u>Catherine Low</u></a>,&nbsp;<a href=\"https://hamishhuggard.com\"><u>Hamish Huggard</u></a>,&nbsp;<a href=\"https://www.linkedin.com/in/wrenatasproat/\"><u>Wrenata Sproat</u></a>,&nbsp;<a href=\"https://adambinks.me\"><u>Adam Binks</u></a>,&nbsp;<a href=\"https://twitter.com/finnhambly\"><u>Finn Hambly</u></a>,&nbsp;<a href=\"https://arunim.fyi\"><u>Arunim Argawal</u></a>&nbsp;</p><p><br>&nbsp;</p><h1>How you can help</h1><p>You can help us out by:</p><ul><li><strong>commenting ways that we could improve on our mistakes</strong> beyond what we've noticed</li><li><strong>pointing out potential failure modes</strong> that might not have shown up at the pilot</li><li><strong>asking us critical questions</strong> (especially the scary ones!)</li><li><strong>connecting us with resources and people</strong> you think might be helpful for us (e.g. someone you know who runs a physics olympiad, a startup accelerator specifically for academic competitions, etc.)</li><li><strong>telling us anything else you think might be helpful</strong> :)</li></ul><p>Err on the side of dropping a comment \u2014 we\u2019ve found the thoughts people give us are a lot more helpful than they sometimes think.</p><p>Also, to repeat: Saul and Jingyi will be attending EAG London \u2014 please reach out if you want to be involved with OPTIC, have questions/comments/concerns, or just want to chat!</p><p><br>&nbsp;</p>", "user": {"username": "OPTIC"}}, {"_id": "Cn38qREHRampHBEMr", "title": "Resolving internal conflicts requires listening to what parts want", "postedAt": "2023-05-19T00:04:20.913Z", "htmlBody": "", "user": {"username": "richard_ngo"}}, {"_id": "6diKmxL8hD89yoMrq", "title": "G7 Summit\u2014Cooperation on AI Policy", "postedAt": "2023-05-19T10:10:14.498Z", "htmlBody": "<p>TL;DR\u2014While the specifics available thus far are underwhelming, it seems notable and a net-positive that cooperation on AI policy is\u2014for the first time\u2014on the G7 Summit agenda this weekend. &nbsp;The readout below is from an April meeting that was convened to lay the groundwork for AI policy discussions at the summit. &nbsp;It gives a general vibe of what to expect:</p><blockquote><p>\u201cWe plan to convene future G7 discussions on generative AI, which could include topics such as governance, how to safeguard intellectual property rights including copyright, promote transparency, address disinformation \u2026 and how to responsibly utilize these technologies,\u201d a joint statement from the meeting said.</p></blockquote><p>Other similar articles\u2014which cover slightly different angles\u2014are <a href=\"https://www.nytimes.com/2023/05/18/world/asia/g7-ukraine-artificial-intelligence.html\">here</a> and <a href=\"https://www.theguardian.com/technology/2023/may/18/uk-will-lead-on-guard-rails-to-limit-dangers-of-ai-says-rishi-sunak\">here</a>.</p>", "user": {"username": "Lenny"}}, {"_id": "hvrwNmXtWRgHGwzaz", "title": "EA Forum: content and moderator positions", "postedAt": "2023-05-18T23:15:33.378Z", "htmlBody": "<p><strong>TL;DR:&nbsp;</strong>We\u2019re hiring for Forum moderators \u2014&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfE3zzwaKT6mwgPIJ3EsyLH_GoU761eGSbOZZQCA32EgGOQ8Q/viewform?usp=sf_link\"><u>apply</u></a> by 1 June (it\u2019s the first round of the application, and should take 15-20 minutes). We\u2019re also pre-announcing a full-time Content Specialist<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpx4kvb3f9r\"><sup><a href=\"#fnpx4kvb3f9r\">[1]</a></sup></span>&nbsp;position on the Online Team at CEA \u2014 you can<s>&nbsp;</s><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfuHrju1lgdWD2VnnQJ7adotyPTI3BZERsfZiV7qsxXe7B_fA/viewform?usp=sf_link\"><s><u>indicate interest</u></s></a><s> in that</s> <i><strong>[the Content Specialist application round is i</strong></i><a href=\"https://www.centreforeffectivealtruism.org/careers/content-specialist\"><i><strong>n progress</strong></i></a><i><strong> but the initial application deadline has passed, so we're not accepting new applications].</strong></i></p><ul><li><strong>\u27a1\ufe0f&nbsp;</strong><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfE3zzwaKT6mwgPIJ3EsyLH_GoU761eGSbOZZQCA32EgGOQ8Q/viewform?usp=sf_link\"><strong><u>Apply</u></strong></a><strong> to be a&nbsp;part-time Forum moderator</strong> by&nbsp;<strong>1 June</strong>.<ul><li>Round 1 of the application should take around 15-20 minutes, and applying earlier is better.</li><li>You can see moderator responsibilities below. This is a remote, part-time, paid position.&nbsp;</li></ul></li><li><strong>\u27a1\ufe0f&nbsp;</strong><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfuHrju1lgdWD2VnnQJ7adotyPTI3BZERsfZiV7qsxXe7B_fA/viewform?usp=sf_link\"><strong><u>Indicate interest</u></strong></a><strong> in a&nbsp;full-time Content Specialist</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpx4kvb3f9r\"><sup><a href=\"#fnpx4kvb3f9r\">[1]</a></sup></span><strong>&nbsp;position&nbsp;</strong>on the Online Team at CEA.<ul><li>We\u2019ll probably soon be hiring for someone to work with me (<a href=\"https://forum.effectivealtruism.org/users/lizka\"><u>Lizka</u></a>) on content-related tasks on the Forum. If you fill out this form, we will send you an email when the application opens and consider streamlining your application if you seem like a particularly good fit.</li><li>You can see more about the role\u2019s responsibilities below. This is a full-time position, and can be remote or in-person from Oxford/Boston/London/Berkeley.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdiesjkivwku\"><sup><a href=\"#fndiesjkivwku\">[2]</a></sup></span></li></ul></li><li>\u27a1\ufe0f You can also&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSd3PqTTRTAJxgnYSq7z1cVvphbXj5WzRe0E41nUL4hkt5XzNA/viewform?usp=sf_link\"><strong><u>indicate interest</u></strong></a> in working as a&nbsp;<strong>copy-editor</strong> for CEA or in being a&nbsp;<strong>Forum Facilitator</strong> (both are part-time remote roles).&nbsp;</li></ul><p>If you know someone who might be interested, please consider sending this post to them!</p><p>Please feel free to get in touch with any questions you might have. You can contact&nbsp;<a href=\"mailto:forum@centreforeffectivealtruism.org\"><u>forum@centreforeffectivealtruism.org</u></a> or&nbsp;<a href=\"mailto:forum-moderation@effectivealtruism.org\"><u>forum-moderation@effectivealtruism.org</u></a>, comment here, or reach out to moderators and members of the Online Team.</p><h2><strong>An overview of the roles</strong></h2><p>I\u2019ve shared a lot more information on the <a href=\"https://forum.effectivealtruism.org/posts/hvrwNmXtWRgHGwzaz/ea-forum-content-and-moderator-positions#Moderator__part_time__remote__paid_\">moderator role</a> and the <a href=\"https://forum.effectivealtruism.org/posts/hvrwNmXtWRgHGwzaz/ea-forum-content-and-moderator-positions#_Content_Specialist___pre_announced_full_time_online_content_role_\">full-time content role</a> in this post \u2014 here's a summary in table form. (You can submit the first round of the moderator application or indicate interest in the content role without reading the whole post.)</p><figure class=\"table\"><table><tbody><tr><td style=\"background-color:#66c0c1;border:1pt solid #000000;padding:5pt;vertical-align:top;width:130px\"><strong>Title</strong></td><td style=\"background-color:#66c0c1;border:1pt solid #000000;padding:5pt;vertical-align:top;width:130px\"><strong>About the role</strong></td><td style=\"background-color:#66c0c1;border:1pt solid #000000;padding:5pt;vertical-align:top;width:260px\"><strong>Key responsibilities</strong></td><td style=\"background-color:#66c0c1;border:1pt solid #000000;padding:5pt;vertical-align:top;width:150px\"><strong>Stage the application is at</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Moderator</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:130px\">Part-time, remote (average ~3 hours a week but variable), $40/hour</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Make the Forum safe, welcoming, and collaborative (e.g. by stopping or preventing aggressive behavior, being clear about moderation decisions), nurture important qualities on the Forum (e.g. by improving the written discussion norms or proactively nudging conversations into better directions), and help the rest of the moderation team.</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Round 1 is open (and should take 15-20 minutes):&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfE3zzwaKT6mwgPIJ3EsyLH_GoU761eGSbOZZQCA32EgGOQ8Q/viewform?usp=sf_link\"><u>apply</u></a> by&nbsp;<strong>1 June</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Content Specialist</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpx4kvb3f9r\"><sup><a href=\"#fnpx4kvb3f9r\">[1]</a></sup></span></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Full-time, remote/ in-person (Oxford/ London/ Boston/ Berkeley<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdiesjkivwku\"><sup><a href=\"#fndiesjkivwku\">[2]</a></sup></span>)</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Encourage engagement with important and interesting online content (via outreach, newsletters, curation, Forum events, writing, etc.), improve the epistemics, safety, and trust levels on the Forum (e.g. via moderation), and more.</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfuHrju1lgdWD2VnnQJ7adotyPTI3BZERsfZiV7qsxXe7B_fA/viewform?usp=sf_link\"><u>Indication of interest</u></a> (we'll probably open a full application soon)</td></tr><tr><td style=\"background-color:#d9d9d9;border:1pt solid #000000;padding:5pt;vertical-align:top\" colspan=\"4\">We\u2019re also excited for&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSd3PqTTRTAJxgnYSq7z1cVvphbXj5WzRe0E41nUL4hkt5XzNA/viewform?usp=sf_link\"><u>indications of interest</u></a> for the following part-time contractor roles, although we might not end up hiring for these in the very near future</td></tr><tr><td style=\"background-color:#efefef;border:1pt solid #000000;padding:5pt;vertical-align:top\">Copy-editor indication of interest</td><td style=\"background-color:#efefef;border:1pt solid #000000;padding:5pt;vertical-align:top\">Part-time, remote (~4 hours a week average), $30/hour by default</td><td style=\"background-color:#efefef;border:1pt solid #000000;padding:5pt;vertical-align:top\" colspan=\"2\">Copy-editing for style, clarity, grammar \u2014 and generally sanity-checking content for CEA. Sometimes also things like reformatting, summarizing other content, finding images, and possibly posting on the website or social media.&nbsp;</td></tr><tr><td style=\"background-color:#efefef;border:1pt solid #000000;padding:5pt;vertical-align:top\">Forum Facilitator indication of interest</td><td style=\"background-color:#efefef;border:1pt solid #000000;padding:5pt;vertical-align:top\">Part-time, remote (~3 hours a week average), $30/hour</td><td style=\"background-color:#efefef;border:1pt solid #000000;padding:5pt;vertical-align:top\" colspan=\"2\"><p>Approving new users and helping them get oriented, classifying and tagging new content on the Forum, noticing other issues, sometimes helping with the&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/all\"><u>Topics Wiki</u></a>, and more.&nbsp;</p><p>This is a crucial part<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefim7yo39hz4s\"><sup><a href=\"#fnim7yo39hz4s\">[3]</a></sup></span>&nbsp;of making the Forum run well.</p></td></tr></tbody></table></figure><h2><strong>Moderator (part-time, remote, paid)</strong></h2><p><strong>We\u2019re looking for new moderators!&nbsp;</strong></p><p>As a moderator, you\u2019d play a crucial part in shaping the EA Forum by preventing norm-violating behavior, developing and communicating the Forum\u2019s discussion&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum\"><u>norms</u></a>, nudging discussions in better directions if they seem at risk of getting unproductive, and more.&nbsp;</p><p><strong>\u27a1\ufe0f&nbsp;</strong><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfE3zzwaKT6mwgPIJ3EsyLH_GoU761eGSbOZZQCA32EgGOQ8Q/viewform?usp=sf_link\"><strong><u>Apply</u></strong></a> by&nbsp;<strong>1 June</strong> (earlier is better). The first part of the application should take 15-20 minutes.</p><p>We plan on evaluating round 1 (of 3) of the application on a rolling basis; if you pass, we\u2019ll send you round 2 as soon as we can. We will do our best to get back to everyone within two weeks of when they submit the form.</p><h3>Basic facts about the moderation role</h3><ul><li>We pay<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefiygsvb5i6xc\"><sup><a href=\"#fniygsvb5i6xc\">[4]</a></sup></span>&nbsp;<strong>$40/hour</strong> for active moderation work.&nbsp;<ul><li>We\u2019re also testing an on-call \u201crotation,\u201d and might start paying people $100/week when they\u2019re on call (which should be on a regular cycle).</li></ul></li><li>It\u2019s a&nbsp;<strong>part-time &amp; remote</strong> role. Hours vary a fair bit week by week; active moderators currently estimate that they spend an average of about 3 hours a week on moderation.<ul><li>If you\u2019re really busy some weeks and expect to have no time those weeks, that\u2019s ok. But if you have less than an hour a week on average, and don\u2019t think you can free up around 4 hours during one week every month, this role is probably not for you. (We might make exceptions for unusual cases.)</li><li>If the moderation team grows, hours individual moderators spend on moderation per week might diminish (although if people&nbsp;<i>want</i> to do more moderation, I\u2019d be excited for that!).</li></ul></li><li>If you\u2019re hired, you\u2019d start as soon as you can. We\u2019re especially interested in adding people who can join in the next couple of months.</li></ul><p>The current active moderators are&nbsp;<a href=\"https://forum.effectivealtruism.org/users/lizka\"><u>Lizka</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/users/jpaddison\"><u>JP</u></a>, and&nbsp;<a href=\"https://forum.effectivealtruism.org/users/lorenzo-buonanno\"><u>Lorenzo</u></a>, and&nbsp;<a href=\"https://forum.effectivealtruism.org/users/felix-wolf\"><u>Felix</u></a> has recently started. Some others are on the team as \u201cadvisors\u201d; they provide second opinions on some decisions, and help when there\u2019s more to respond to. We\u2019re not hiring advisors.&nbsp;</p><h3>Why moderation?</h3><p>In brief:&nbsp;</p><ul><li>Moderation is important<ul><li>The EA Forum is a key part of the EA network.</li><li>Moderators make the Forum safer, epistemically healthier, and more productive.</li><li>Extra moderators can add a lot of value. We\u2019re quite capacity constrained, which means that we can\u2019t do a lot of what we think would be useful. Even if we were less constrained, I\u2019d be really excited to get extra moderators to get a broader diversity of perspectives and more capacity in times of crisis, or when there\u2019s a sudden cascade of moderation incidents.</li></ul></li><li>In terms of what it\u2019s like for the moderators, I shared some thoughts and asked for a testimonial from a different moderator about what he likes and doesn\u2019t like about being a moderator \u2014 you can see it below.&nbsp;</li></ul><p>In greater detail \u2014&nbsp;<strong>on the importance of moderation</strong>:</p><p>The Forum is the heart (or at least one of the hearts) of the online EA network,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1nfjouqsi6h\"><sup><a href=\"#fn1nfjouqsi6h\">[5]</a></sup></span>&nbsp;and a key piece of community infrastructure. If the Forum can be an excellent space for serious and collaborative discussions that help improve the world, I think we\u2019ll make significantly more progress on the problems we\u2019re facing and do more good.</p><p>Moderation is about making sure that the Forum is useful and healthy \u2014 which in turn can help the EA network make significantly more progress on the problems we\u2019re facing and do more good. Moderators help discussions on the Forum be collaborative and truth-seeking, generally uphold epistemic norms, make the space feel safe and welcoming, and guard against other dangers faced by online discussion spaces (I think unmoderated forums tend to get overrun by trolls or start to feel more like battlegrounds for discussions that have winners and losers). I want Forum users to be able to trust that there\u2019ll be a high baseline of civility and generosity in the interpretation of their words (a sense of cooperative spirit), and I don\u2019t think we can do this without moderators.</p><p>We have ~4 active moderators right now, but new moderators could add a lot. We\u2019re still quite&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Hu6s5nF9WCektwwur/announcing-drama-curfews-on-the-forum\"><u>strained</u></a> on capacity, which means we don\u2019t have time to do a lot of what we think would be valuable to do. And moderation incidents tend to come in surges (when it rains, it pours). Extra moderators would be extremely helpful in those surges. New moderators can also add new perspectives; we\u2019re building out our policies, norms, and processes, and more voices there would be helpful.&nbsp;</p><p><strong>On what it\u2019s like to be a moderator</strong>:&nbsp;</p><p>Moderation can be pretty stressful and thankless (although&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/QFHPbSWKfEcc7cuF5/thank-you-so-much-to-everyone-who-helps-with-our-community-s\"><u>not</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/GqvSoerAurEbiP9GE/moderator-appreciation-thread\"><u>always</u></a>!), but we have some big things going for us: people who use the EA Forum are often incredibly well-meaning and helpful. I think our baseline of civility and kindness is really high. And when incidents happen (commenters get into a rough disagreement, etc.), the people involved are often willing to change their minds, accept our requests, or work with us to find a solution to whatever problem is showing up.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh34dqxgnufi\"><sup><a href=\"#fnh34dqxgnufi\">[6]</a></sup></span>&nbsp;I don\u2019t know what \u201cthe average moderator\u201d for an online space has to deal with, but I\u2019d guess that it\u2019s worse in many ways. A related thing I\u2019m grateful for is that the moderators tend to be wonderful, and it\u2019s just lovely to work with caring and smart people.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrtbh5lfpza\"><sup><a href=\"#fnrtbh5lfpza\">[7]</a></sup></span></p><p>I also asked <a href=\"https://forum.effectivealtruism.org/users/lorenzo-buonanno?mention=user\">@Lorenzo Buonanno</a> to share his thoughts on being a moderator \u2014 what he likes and doesn\u2019t like. (He\u2019s probably biased, etc., but I figured that adding a perspective from someone else, especially someone not employed by CEA, would be better than only including mine.) Lorenzo wrote:&nbsp;</p><blockquote><p><strong>Things I like:</strong><br>After joining the mod team last November (5 days before the FTX collapse), I was really surprised by how thoughtful the moderation team is, and how thorough they were in their decisions.</p><p>In most other online forums, I often get the feeling that moderators just ban people and topics they don\u2019t like without much thought. Here I saw really serious effort to be impartial, collaborative, and consider moderation decisions from many perspectives. I see many people on the moderation team as an example of what EA should aspire to be.</p><p>I think the results really show. Even on the most controversial and inflammatory topics the discussion has been very surprisingly civil.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefybkzpv9fsjg\"><sup><a href=\"#fnybkzpv9fsjg\">[8]</a></sup></span>&nbsp;In a typical week, the discussion standards on this forum are extremely high compared to any other forum I know of.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh4fwsied8q\"><sup><a href=\"#fnh4fwsied8q\">[9]</a></sup></span></p><p>I feel really proud that I\u2019m a small part in helping a little to maintain this Forum, where I regularly see incredibly valuable discussions<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefopr3am4w47c\"><sup><a href=\"#fnopr3am4w47c\">[10]</a></sup></span>&nbsp;on how to help others the most, and that I can take some load off the Forum team so they can focus more on other projects.</p><p><strong>Things I don\u2019t like:</strong><br>We once got a report for a comment from a user whose writing I really like, and although I really agreed with the content of that comment and initially upvoted it, it was clear on a closer read that it was violating forum norms and needlessly inflammatory. Having to write a public warning in that situation (knowing it would be downvoted) was tough.</p><p>It\u2019s also sometimes hard to find the right balance between having a higher bar for moderating criticism or things I don\u2019t agree with, and limiting the most norm-violating or fight-seeking content.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefj3ksuu8w2e\"><sup><a href=\"#fnj3ksuu8w2e\">[11]</a></sup></span></p></blockquote><h3>Moderator responsibilities (and some things that aren\u2019t a moderator\u2019s responsibility)</h3><p>As a moderator, your goal would be to make the EA Forum a great space for collaborative discussions that will help us do good. You would:</p><ul><li>Make the Forum feel safe, welcoming, and collaborative<ul><li>You might do this by handling moderation incidents (preventing or stopping aggressive or otherwise norm-breaking behavior), by being transparent and consistent with moderation decisions (so people are not stressed about the possibility of over-moderation, or unpredictable moderation), or by setting a great example and directly helping other Forum users.&nbsp;</li></ul></li><li>Nurture important qualities or norms on the Forum (like honesty, civility, generosity in interpretation, etc.)<ul><li>You might do this by communicating Forum&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum\"><u>discussion norms</u></a> and proactively nudging conversations into more collaborative directions if they seem likely to get iffy.</li></ul></li><li>Help the rest of the moderation team improve the Forum<ul><li>You might suggest improvements to our policies, internal systems, and the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum\"><u>Guide to Norms</u></a>. And you might help other moderators respond to moderation incidents they\u2019re dealing with by flagging new considerations, disagreeing with their proposed approach, etc.</li></ul></li></ul><p>In practice, active moderators work on:&nbsp;</p><ul><li>Responding to norm-breaking behavior (warning users, sometimes banning them for some time, trying to sort out disagreements, etc.)<ul><li>A lot of our work is currently reactive. Something bad or iffy happens \u2014 someone starts insulting people, some personal information is revealed, someone seems to be getting aggressive, etc. \u2014 and we need to respond. This usually gets flagged to us, at which point a moderator either responds directly, brings this up in our Slack for discussion (often they might propose a course of action, and the discussion will just be a sanity check), or asks someone else to take it on.</li></ul></li><li>Providing input on moderation decisions others are handling<ul><li>When situations are higher stakes (e.g. if we\u2019re considering banning someone) or unusual/new (in which case we might be developing a new policy), we\u2019ll often want multiple people to help with a decision.</li></ul></li><li>Improving our moderation policies, internal systems, and the Forum\u2019s discussion norms (or how they are communicated<ul><li>Some of the incidents that we encounter are somewhat formulaic; we\u2019ve done something similar before and know what to do. But we also often encounter new situations, for which we prefer to develop rough policies \u2014 or approaches that can generalize to parallel situations \u2014 if we can (as opposed to treating them as one-offs). (<a href=\"https://forum.effectivealtruism.org/posts/u9xyvv8vW2DAtixPh/an-update-to-our-policies-on-revealing-personal-information\"><u>E.g.</u></a>) We could also do more to streamline our internal processes, and I want us to improve the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum\"><u>Guide to Norms</u></a> (both in terms of content and how we communicate and encourage people to follow the norms).</li></ul></li><li>Coordinating with the rest of the moderation team to make sure that we don\u2019t drop some tasks accidentally \u2014 the \u201cmoderation coordination rotation\u201d<ul><li>We\u2019re trialing a system in which active moderators take turns coordinating the team for a week. This doesn\u2019t mean that they have to take care of all the incidents that pop up during that week, but they do have to make sure that everything gets resolved one way or another. (So they might bring something up in Slack and make sure that it\u2019s being owned by a different moderator. Or we\u2019ll decide to not intervene; the goal is not to do something in response to every flag, but rather to avoid dropping incidents we should have responded to.)</li><li>We\u2019re not totally sure if we\u2019re keeping this system \u2014 someone might take on coordination all the time \u2014 but for now, this is probably a key responsibility. If the system changes, some form of this responsibility will probably stay.</li></ul></li><li>Proactively nudging discussions into better directions if they seem likely to get iffy, ideally without resorting to actions that only moderators can take (like banning users)<ul><li>I would like us to do more of this.&nbsp;</li></ul></li></ul><p>Moderators generally don\u2019t work on approving new users, tagging posts, or guarding against spam. Forum Facilitators are in charge of that.</p><h3>Skills &amp; fit (moderators)</h3><p>You&nbsp;<strong>don\u2019t</strong> need special background or any credentials to be a moderator. You also don\u2019t need to have an extensive presence on the Forum, or a lot of experience in EA.</p><p>You&nbsp;<strong>do</strong> need some of the following key skills and qualities, but<strong> if you\u2019re interested and are doubting whether to apply, please just&nbsp;</strong><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfE3zzwaKT6mwgPIJ3EsyLH_GoU761eGSbOZZQCA32EgGOQ8Q/viewform?usp=sf_link\"><strong><u>apply</u></strong></a>. People often underestimate themselves, and we\u2019d much rather have too many applications than miss out on excellent moderators because of this. I\u2019m listing these qualities to give a sense of what we\u2019re looking for, though, and to help people decide that it\u2019s just not worth their time to apply.&nbsp;</p><ul><li>Written communication<ul><li>You don\u2019t need to be a native English speaker or to have excellent writing skills. But you do need to be able to communicate clearly in private conversations and in public.&nbsp;</li></ul></li><li>Good judgement, an open mind, and empathy<ul><li>There are a lot of cases where we can\u2019t just follow policies or what we did in previous cases, and good judgement is important for these. Moderators also need to be willing and able to take on different points of view and understand how a situation probably feels from the perspective of different people, whether they agree with them or not.</li></ul></li><li>Ability to sort out the truth when things are pretty confusing, ability to deal with uncertainty &amp; disagreement<ul><li>We sometimes deal with pretty confusing texts, where it\u2019s important and difficult to understand if someone is telling the truth, trying to manipulate readers, etc.&nbsp;</li></ul></li><li>Independence &amp; the ability to execute without forgetting important details<ul><li>Moderators are pretty independent. It\u2019s not easy to neatly delegate moderation-related tasks and it\u2019s important to be able to rely on people doing what they say they\u2019ll do, so this is a key skill.</li></ul></li><li>Basic understanding of important EA topics<ul><li>It\u2019s helpful for moderators to have context on the topics discussed on the Forum, but we don\u2019t need a lot of \u201cEA expertise.\u201d</li></ul></li><li>Willingness to be wrong in public (and to admit it)<ul><li>We make mistakes. We don\u2019t want to hide them, avoid noticing them, or let the fear of making mistakes prevent us from doing useful work. (It\u2019s also important to be able to endorse a choice we\u2019ve made even if there\u2019s public disagreement with it.)</li></ul></li><li>Willingness and ability to engage with potentially triggering topics and content</li></ul><h3>The moderator application process</h3><p>There will be three parts to the application:</p><ul><li><strong>Round 1</strong>: A&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfE3zzwaKT6mwgPIJ3EsyLH_GoU761eGSbOZZQCA32EgGOQ8Q/viewform?usp=sf_link\"><u>form</u></a>, which consists of basic information about you and two written questions. This should take 15-20 minutes.&nbsp;<ul><li>We\u2019ll try to get back to you as soon as we can, with a target of no more than two weeks\u2019 delay (this might change if we get more applications than we expect).&nbsp;</li></ul></li><li><strong>Round 2</strong>: A follow-up questionnaire, which will focus on some fake moderation incidents \u2014 it\u2019ll ask how you might respond to them. This should take at most an hour (I don\u2019t know the exact time, as I\u2019m still finalizing this stage).&nbsp;<ul><li>We should get back to you within two weeks.</li></ul></li><li><strong>Round 3 - paid</strong>: This should consist of a couple of timed mock tasks and a brief call with me or another moderator.&nbsp;</li></ul><p>We\u2019re hoping to bring on a few new moderators. We'll try to respond quickly after each round of the application process.&nbsp;</p><h2><strong>\u201cContent Specialist\u201d (pre-announced full-time online content role)</strong></h2><p><i>The role title might change a bit.</i></p><p>We\u2019ll probably soon be looking for a full-time \u201cContent Specialist\u201d for the Online Team (remote or in Oxford/Boston/London/Berkeley<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdiesjkivwku\"><sup><a href=\"#fndiesjkivwku\">[2]</a></sup></span>). This person will work with me (Lizka) and take on many of my current responsibilities (I head the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum\"><u>moderation</u></a> team, seek out and curate content on the Forum, run/write the monthly&nbsp;<a href=\"https://www.effectivealtruism.org/ea-newsletter-archives\"><u>EA Newsletter</u></a> and the weekly&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/bi9WWR58m45GJG7bc/forum-digest-reminder-that-it-exists-and-request-for\"><u>Forum Digest</u></a>, and work on other projects). The position is very flexible.</p><p><strong>\u27a1\ufe0f&nbsp;</strong><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfuHrju1lgdWD2VnnQJ7adotyPTI3BZERsfZiV7qsxXe7B_fA/viewform?usp=sf_link\"><strong><u>Indicate interest</u></strong></a><strong>.&nbsp;</strong></p><p>(If you indicate interest,&nbsp;<strong>we will email you when the position opens up</strong>, and we might streamline the process for some people who seem like they might be a very good fit.)</p><h3>Why work on content (on the Online Team)?</h3><p>I\u2019m really excited that we\u2019re getting ready to hire for this role, as I think it could have a very big impact.&nbsp;</p><p>In brief:&nbsp;</p><ul><li>The&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/\"><u>Centre for Effective Altruism</u></a> plays a critical role in organizing and supporting the EA network.</li><li>The EA Forum is a key part of that (as described in greater detail <a href=\"https://forum.effectivealtruism.org/posts/hvrwNmXtWRgHGwzaz/ea-forum-content-and-moderator-positions#Why_moderation_\">above</a>), as are other projects this role would probably be involved in (like the&nbsp;<a href=\"https://www.effectivealtruism.org/ea-newsletter-archives\"><u>EA Newsletter</u></a>, which goes out to around 59,000 subscribers every month and has a ~40% open rate, and effectivealtruism.org).&nbsp;</li><li>More \u201ccontent\u201d capacity (thought, skill, and time devoted to improving online content and engagement) would allow the Forum team to expand and improve our projects. There\u2019s a lot that we could do but can\u2019t get to right now, and a lot that we\u2019re just not thinking of trying out. (And this is a real&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/LwRSnvnaFL9eE4yKz/invisible-impact-loss-and-why-we-can-be-too-error-averse\"><u>loss</u></a>; new ideas are being shared on the Forum, people are coordinating, learning, connecting, etc. \u2014 but a lot of things that could happen don\u2019t.)<ul><li>This is where you come in.</li></ul></li></ul><h3>Responsibilities (Content Specialist)</h3><p>You would have some core responsibilities as a Content Specialist, but as mentioned, the position is&nbsp;<strong>very flexible</strong>; we expect to shape the role around the person we hire.</p><p>Your high-level&nbsp;<strong>goals</strong> would be to:</p><ul><li>Encourage engagement with the most valuable content and resources</li><li>Make sure that the Forum is as useful as possible for helping people to do more good</li></ul><p>Here are some more specific responsibilities you would take on, along with example projects (in no particular order):</p><ul><li>Encourage engagement with important and interesting content<ul><li>You could feature and organize content or conversations on the Forum via curation, the weekly&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/bi9WWR58m45GJG7bc/forum-digest-reminder-that-it-exists-and-request-for\"><u>Forum Digest</u></a> (or possible topic-specific newsletters we could try), helping the rest of the Online Team shape the Forum and build new features, etc. (We\u2019ve occasionally considered setting up vetting systems for some content or spaces \u2014 there are a lot of possibilities.) Alternatively, you might actively seek out excellent content by doing direct outreach to authors, hosting writing workshops, cross-posting pieces you find outside of the Forum, setting up dialogues or debates on important topics, running potential prizes or themed Forum events, etc. Or you might write your own content as necessary, build out and improve the&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/all\"><u>Topics Wiki</u></a>, and create better introductory materials on different topics (like intro&nbsp;<a href=\"https://forum.effectivealtruism.org/library\"><u>sequences</u></a>).</li></ul></li><li>Improve the epistemics, safety, and trust levels on the Forum<ul><li>To do this, you might work on moderation (potentially as head moderator) for the EA Forum, where you would develop and communicate moderation&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum\"><u>policies</u></a>, help people have productive conversations, improve systems, hire moderators as necessary, and more</li></ul></li><li>Help the Online Team (or the EA network, or CEA) prepare for future challenges and opportunities<ul><li>For instance, you might actively develop models of different groups who use the Forum (via user interviews, active participation on the Forum, etc.) to better help them coordinate (and to help the rest of the Online Team understand what needs these groups might have, or how we can organize the space for them more helpfully). You might also make internal systems more efficient (or automated), keep informed on upcoming developments or maintain situational awareness in relevant areas, communicate with Forum users, etc.</li></ul></li></ul><p>In the end, what you work on would significantly depend on your interests, skills, and what you/we identify as core needs and bottlenecks.</p><h3>Skills &amp; fit (Content Specialist)</h3><p>You don\u2019t need to have years of experience with online forums, or with EA to be a good fit for this role. But to be a good fit for the role, it\u2019s probably the case that:&nbsp;</p><ul><li>You\u2019re dependable and can get things done pretty independently</li><li>You can write clearly and well</li><li>You have good judgement and can deal with uncertainty and disagreement</li><li>You have a baseline of knowledge about core EA topics, and the ability to learn more when needed (this will likely involve being curious and having a growth mindset)</li><li>You\u2019re good at figuring out the truth when things are pretty confusing and have a strong sense for what good content looks like</li></ul><p>If you\u2019re not sure, please&nbsp;<strong>err on the side of&nbsp;</strong><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfuHrju1lgdWD2VnnQJ7adotyPTI3BZERsfZiV7qsxXe7B_fA/viewform?usp=sf_link\"><strong><u>indicating interest</u></strong></a>!</p><h2><strong>Other information</strong></h2><p>CEA is an equal opportunity employer and values diversity at our organisation. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, or disability status. We are happy to make any reasonable accommodations necessary to welcome all to our workplace. Please contact us to discuss adjustments to the application process.&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpx4kvb3f9r\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpx4kvb3f9r\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The exact title of the role might change a bit.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndiesjkivwku\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdiesjkivwku\">^</a></strong></sup></span><div class=\"footnote-content\"><p>There\u2019s an office in Oxford that you could work from, and a coworking space in Boston with a couple of people from the Online Team. People from the Online Team and CEA are also working from other coworking spaces in London and Berkeley.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnim7yo39hz4s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefim7yo39hz4s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We \u201cbanned and purged\u201d roughly 2550 spam users from the Forum in 2022. (It was around 806 users in 2021.) These are accounts that join with names that range from \u201cBob\u201d to \u201cTAXI-Vienna-CALL-7777777777\u201d and try to post spam content on the Forum.&nbsp;</p><p>Spam users are becoming sneakier these days. I\u2019m incredibly grateful to Facilitators for working on this (and on new content classification, etc.).&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fniygsvb5i6xc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefiygsvb5i6xc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Some moderators are volunteering, but we default to paying people.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1nfjouqsi6h\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1nfjouqsi6h\">^</a></strong></sup></span><div class=\"footnote-content\"><p>About 4500 users posted something or commented in the past year, and by some analytic measures, we had almost a million users last year. Most people in the EA network have heard of the Forum, have read at least one post on it, etc. A lot of people get actively involved in EA-motivated projects via the Forum.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnh34dqxgnufi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefh34dqxgnufi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>E.g. we\u2019ve asked users things like: \u201cHey, this thread seems unproductive. Could you take a break and return?\u201d And they agreed with us and did it!</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrtbh5lfpza\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrtbh5lfpza\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I think we also have other things! Like the fact that there are developers and product managers who are willing to build us tools that make our jobs easier, and the fact that some people have been working on this for years and are happy to give us advice. But I\u2019m skipping them here for now.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnybkzpv9fsjg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefybkzpv9fsjg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>with very few exceptions out of hundreds of posts</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnh4fwsied8q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefh4fwsied8q\">^</a></strong></sup></span><div class=\"footnote-content\"><p>at one point two weeks ago I thought that the reporting functionality must have been broken, given how few reports I was seeing, but we checked and it\u2019s actually just all users being great</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnopr3am4w47c\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefopr3am4w47c\">^</a></strong></sup></span><div class=\"footnote-content\"><p>like this great back and forth on the evidence on StrongMinds&nbsp;https://forum.effectivealtruism.org/posts/HqEmL7XAuuD5Pc4eg/evaluating-strongminds-how-strong-is-the-evidence?commentId=byEcGGBy6zjsQ6Zky#byEcGGBy6zjsQ6Zky</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnj3ksuu8w2e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefj3ksuu8w2e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This was a particularly visible mistake I made on this&nbsp;https://forum.effectivealtruism.org/posts/AAZqD2pvydH7Jmaek/lorenzo-buonanno-s-shortform?commentId=CDPS2JQKziWsWg73D</p></div></li></ol>", "user": {"username": "Lizka"}}, {"_id": "NR9cqHuSJiW9aDiDN", "title": "Metaculus's Forecast Friday, May 19th at 12pm ET", "postedAt": "2023-05-18T17:24:48.620Z", "htmlBody": "<p>Are you interested in how top forecasters predict the future? Curious how other people are reacting to the forecasts in the main feed?</p><p><strong>Join us May 19th at 12pm ET/GMT-4 for Forecast Friday!</strong></p><p>Click <a href=\"https://app.gather.town/app/Yhi4XYj0zFNWuUNv/EA%20coworking%20and%20lounge\">here</a> to go to the EA Gather Town. Then take the Metaculus portal.</p><h2><strong>This Friday</strong></h2><p>This week we're excited to welcome Good Judgment superforecaster and Google software engineer Pietro Kreitlon Carolino! Pietro's background is in math and computer science, and he specializes in machine learning. He first started forecasting in 2016\u2014primarily on GJOpen, but also internally at Google.</p><p>On Friday, Pietro will share his high-level approach to forecasting and will analyze some of the questions about upcoming advances in Large Language Models in the <a href=\"https://www.metaculus.com/project/2049/\">LLM Milestones project</a>.</p><p>We'll see you there!</p><hr><p><a href=\"https://calendar.google.com/calendar/event?action=TEMPLATE&amp;tmeid=MjNjMjdnbnYycm5zczlsaWdtdWZxazlyM3NfMjAyMzA0MDdUMTYwMDAwWiBjX2U5NDZmNjc1ZDlhOGU0YTAyMDY2NzA4Y2I2ZDQ5NjkxMDJiNDQ1ODZhM2Y5NTE2MWRhZTQyOTYyNDk3ZjRmZWJAZw&amp;tmsrc=c_e946f675d9a8e4a02066708cb6d4969102b44586a3f95161dae42962497f4feb%40group.calendar.google.com&amp;scp=ALL\">Add</a> Forecast Fridays to your Google Calendar:</p><p>\u2014or click <a href=\"https://calendar.google.com/calendar/ical/c_e946f675d9a8e4a02066708cb6d4969102b44586a3f95161dae42962497f4feb%40group.calendar.google.com/public/basic.ics\">here</a> for other formats.</p><p>Forecast Friday events feature three concurrent rooms:</p><ul><li><i>Forensic Friday</i>, where a highly-ranked forecaster will lead discussion on a forecast of interest</li><li><i>Feedback Friday</i>, where new and experienced users alike can learn more on how to use the platform</li><li><i>Friday Frenzy</i>, a spirited discussion about the forecasts on questions on the front page of the main feed</li></ul><p><strong>This event will take place virtually in </strong><a href=\"https://app.gather.town/app/Yhi4XYj0zFNWuUNv/EA%20coworking%20and%20lounge\"><strong>Gather Town</strong></a><strong> from 12pm to 1pm ET</strong>.</p><p>To join, e\ufeffnter <a href=\"https://app.gather.town/app/Yhi4XYj0zFNWuUNv/EA%20coworking%20and%20lounge\">Gather Town</a> and use the Metaculus portal.</p>", "user": {"username": "christianM"}}, {"_id": "ojGzGhbJqMXQiRCQE", "title": "Join us at a PauseAI protest outside Google DeepMind in London", "postedAt": "2023-05-18T16:54:14.619Z", "htmlBody": "<p><strong>What:&nbsp;</strong>Lawful protest outside Google DeepMind + awareness-raising/recruitment outside EAG London</p><p><strong>When &amp; where:</strong></p><ul><li>1200-1400 Fri 19 May outside Google DeepMind</li><li>1200-1400 Sat 20 and Sun 21 May outside EAG London</li><li>1700-2000 Mon 22 May outside Google DeepMind</li><li>In the future: regular (weekly?) protest?</li></ul><p><strong>Exact locations:</strong></p><ul><li>Google DeepMind in building S2 Handyside: 8 Handyside Street, King\u2019s Cross, London, N1C 4DJ</li><li>Effective Altruism Global London (EAG London): Tobacco Dock, Tobacco Quay, Wapping Lane, Wapping, London, E1W 2SF</li></ul><p><strong>Why:</strong></p><ul><li>To call for a global moratorium on the development of artificial intelligence (AI) systems more powerful than GPT-4</li><li>To raise public awareness of the threat posed by artificial general intelligence (AGI) development</li><li>To create and leverage social pressure on Google DeepMind and other AI companies</li><li>To build momentum for a social movement calling for better AI safety, governance and ethics in general<ul><li>In particular, to set a precedent for a regular (weekly?) protest</li></ul></li></ul><p><strong>How</strong>:</p><ul><li>We will lawfully and peacefully stand outside Google DeepMind with placards and a banner, possibly handing out flyers / stickers / pin badges to DeepMind employees and passers-by</li><li>We will have respectful, cooperative conversations</li><li>We may give speeches (more likely) and/or sing and/or chant (less likely)</li><li>We may dress up in attention-grabbing costumes</li><li>Our tone will be informative, inclusive and even humorous rather than confrontational and disruptive</li><li>We aim to get up to 30 people to this first protest</li><li>Legal:<ul><li>This is a wholly lawful, legal and non-arrestable protest</li><li>We will stand on the pavement (public space), not on private land</li></ul></li></ul><p><strong>Background:</strong></p><ul><li>\u201cThis is an existential risk\u201d \u2013&nbsp;<a href=\"https://www.reuters.com/technology/ai-pioneer-says-its-threat-world-may-be-more-urgent-than-climate-change-2023-05-05/\"><u>Geoffrey Hinton</u></a></li><li>\u201cthe most likely result of building a superhumanly smart AI, under anything remotely like the current circumstances, is that literally everyone on Earth will die\u201d \u2013&nbsp;<a href=\"https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\"><u>Eliezer Yudkowsky</u></a></li><li>\u201cAGI is basically here. Alignment is nowhere near ready.\u201d \u2013&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and#What_you_can_and_perhaps_should_do_now\"><u>Greg Colbourn</u></a></li><li>\u201cUnfortunately, this level of planning and management is not happening, even though recent months have seen AI labs locked in an out-of-control race to develop and deploy ever more powerful digital minds that no one \u2013 not even their creators \u2013 can understand, predict, or reliably control.\u201d \u2013 Future of Life Institute (FLI)&nbsp;<a href=\"https://futureoflife.org/open-letter/pause-giant-ai-experiments/\"><u>open letter</u></a></li><li>\u201cwe call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4\u201d \u2013 FLI&nbsp;<a href=\"https://futureoflife.org/open-letter/pause-giant-ai-experiments/\"><u>open letter</u></a></li><li>\u201cSo we did this open letter because we want to help these idealistic tech executives to do what their heart tells them by providing enough public pressure on the whole sector to just pause so that they can all pause in a coordinated fashion. And I think<strong> without the public pressure, none of them can do it alone</strong>, push back against their shareholders, no matter how good hearted they are.\u201d \u2013&nbsp;<a href=\"https://steno.ai/lex-fridman-podcast-10/371-max-tegmark-the-case-for-halting-ai-development\"><u>Max Tegmark</u></a> talking about the FLI open letter</li></ul><p><strong>Steps to take:</strong></p><ul><li>Read the full details of the protest&nbsp;<a href=\"https://docs.google.com/document/d/1jAuNUobJw8-FEvPonsktRCS1kEMknRlk72Lf0vqKsbY/edit\"><u>here</u></a> and join us</li><li>Click \u2018going\u2019 on the Facebook event&nbsp;<a href=\"https://fb.me/e/Tc3BBimH\"><u>here</u></a></li><li>Share details on social media using the hashtag&nbsp;<a href=\"https://twitter.com/search?q=%23PauseAI&amp;src=typed_query&amp;f=top\"><u>#PauseAI</u></a></li><li>Talk to your family and friends about this</li><li>Go to the&nbsp;<a href=\"https://pauseai.info/protests\"><u>protests</u></a> in San Francisco or Brussels or organise your own</li><li>Follow the action points&nbsp;<a href=\"https://pauseai.info/action\"><u>here</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and#What_you_can_and_perhaps_should_do_now\"><u>here</u></a></li></ul><p><br><a href=\"https://pauseai.info/\"><u>PauseAI</u></a> are a group of volunteers, AI (safety) researchers and engineers who are worried about the risks of AI. We are not affiliated with any company or organisation.</p>", "user": {"username": "Alistair"}}, {"_id": "EFEwBvuDrTLDndqCt", "title": "Relative Value Functions: A Flexible New Format for Value Estimation", "postedAt": "2023-05-18T16:39:31.097Z", "htmlBody": "<h2>Summary</h2><p>Quantifying value in a meaningful way is one of the most important yet challenging tasks for improving decision-making. Traditional approaches rely on standardized value units, but these falter when options differ widely or lack an obvious shared metric. We propose an alternative called <i>relative value functions</i> that uses programming functions to value relationships rather than absolute quantities. This method captures detailed information about correlations and uncertainties that standardized value units miss. More specifically, we put forward <i>value ratio</i> formats of <i>univariate</i> and <i>multivariate</i> forms.&nbsp;</p><p>Relative value functions ultimately shine where single value units struggle: valuing diverse items in situations with high uncertainty. Their flexibility and elegance suit them well to collective estimation and forecasting. This makes them particularly well-suited to ambitious, large-scale valuation, like estimating large utility functions.&nbsp;</p><p>While promising, relative value functions also pose challenges. They require specialized knowledge to develop and understand, and will require new forms of software infrastructure. Visualization techniques are needed to make their insights accessible, and training resources must be created to build modeling expertise.&nbsp;</p><p>Writing programmatic relative value functions can be much easier than one might expect, given the right tools. We show some examples using&nbsp;<a href=\"https://www.squiggle-language.com/\"><u>Squiggle</u></a>, a programming language for estimation.</p><p>We at <a href=\"https://quantifieduncertainty.org/\">QURI</a> are currently building software to make relative value estimation usable, and we expect to share some of this shortly. We of course also very much encourage others to try other setups as well.</p><p>Ultimately, if we aim to eventually generate estimates of things like:</p><ol><li>The total value of all effective altruist projects;</li><li>The value of 100,000 potential personal and organizational interventions; or</li><li>The value of each political bill under consideration in the United States;</li></ol><p>then the use of relative value assessments may be crucial.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/y6c3daamxejbicxow3zo\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/wvhkxbblqq0kqwouubn9 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/shfabuke8uzervvz1ixd 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/w24uvm4hm5umyga97b4k 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/lrwsabn97uagpbmmimup 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/qltiry4jvtvicqtpacjf 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/h9vo3srfzjbedmpvea2s 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/wztvbustlrzhqiugnqgq 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/esza4v5reaqcyjo4ony1 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/jlnj5s87shisswbrvnjw 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/j4d74ire2mjsawzbkjn0 1014w\"></figure><h2>Presentation &amp; Demo</h2><p>I gave a recent presentation on relative values, as part of a longer presentation in our work at QURI. This features a short walk-through of an <a href=\"https://relative-values-git-develop-quantified-uncertainty.vercel.app/\">experimental app </a>we're working on to express these values.&nbsp;<br><br>The Relative Values part of the presentation is is from <a href=\"https://youtu.be/aBlvcT0cAsI?t=1345\"><i>22:25 </i>to <i>35:59</i>.</a></p><p>This post gives a much more thorough description of this work than the presentation does, but the example in the presentation might make the rest of this make more sense.</p><figure class=\"media\"><div data-oembed-url=\"https://youtu.be/aBlvcT0cAsI?t=1405\"><div><iframe src=\"https://www.youtube.com/embed/aBlvcT0cAsI\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><h2>Challenges with Estimating Value with Standard Units</h2><p>The standard way to measure the value of items is to come up with standardized units and measure the items in terms of these units.</p><ul><li>Many health measure benefits are estimated in&nbsp;<a href=\"https://en.wikipedia.org/wiki/Quality-adjusted_life_year\"><u>QALYs</u></a> or&nbsp;<a href=\"https://en.wikipedia.org/wiki/Disability-adjusted_life_year\"><u>DALYs</u></a></li><li>Consumer benefit has been measured in&nbsp;<a href=\"https://en.wikipedia.org/wiki/Willingness_to_pay\"><u>willingness to pay</u></a></li><li>Longtermist interventions have occasionally been measured in \u201c<a href=\"https://forum.effectivealtruism.org/posts/pGaesfn4R9xWK4Rmk/01-fund-ideation-and-proposal\"><u>Basis Points</u></a>\u201d, Microdooms and Microtopias&nbsp;</li><li>Risky activities can be measured in&nbsp;<a href=\"https://en.wikipedia.org/wiki/Micromort\"><u>Micromorts</u></a></li><li>COVID activities have been measured in&nbsp;<a href=\"https://www.microcovid.org/\"><u>MicroCOVIDs</u></a></li></ul><p>Let\u2019s call these sorts of units \u201c<i>value units</i>\u201d as they are meant as approximations or proxies of value. Most of these (QALYs, Basis Points, Micromorts) can more formally be called&nbsp;<i>summary measures</i>, but we\u2019ll stick to the term&nbsp;<i>unit</i> for simplicity.</p><p>These sorts of units can be very useful, but they\u2019re still infrequently used.&nbsp;</p><ol><li>&nbsp;QALYs and DALYS don\u2019t have many trusted and aggregated tables. Often there are specific estimates made in specific research papers, but there aren\u2019t many long aggregated tables for public use.&nbsp;</li><li>There are very few tables of personal intervention value estimates, like the net benefit of life choices.</li><li>Very few business decisions are made with reference to clear units of value. For example, \u201c<i>Which candidate should we hire?\u201d</i>,&nbsp;<i>\u201cShould we implement X feature?\u201d,</i>&nbsp;<br>\u201cWhich marketing campaign should we go with\u201d, etc.</li><li>Longtermist interventions are very rarely estimated in clear units. Perhaps partly because of this, there is fairly little quantified longtermist valuation.</li></ol><p>If explicitly estimating value is useful, why is it so unusual?</p><p>We think one reason is that these<i> \"standard unit valuations\"&nbsp;</i>&nbsp;often miss out on a lot of critical information. They lack correlations or joint distributions. This deficiency leads to major compromises and awkward situations, causing human modelers to either give up or generate poor results.</p><p>We\u2019ll go through two thought experiments to demonstrate.</p><h3>Thought Experiment #1: Comparing Research Documents</h3><p>Let\u2019s imagine using units to evaluate all scientific research. Where would we start? We could try to come up with some measure like, \u201c<i>a quality-adjusted research paper</i>\u201d. But papers about global health are very different from those on AI transparency. And papers are very different from books, presentations, or Tweets.&nbsp;</p><p>There\u2019s an intense tradeoff between generality and complexity. On one hand, we could try to evaluate everything in terms of a few units, but then most of the estimates would be extremely uncertain. If we estimate everything in terms of \u201c<i>microdoom equivalents</i>\u201d, then our global health measures will be almost meaningless.</p><p>Alternatively, we could try to use 100+ units, with a huge rule book of which things can be evaluated in terms of which units. Subsequently, a distinct system would be required to convert these estimations into standard units for comparisons between clusters. Determining the optimal units prior to conducting the estimation is challenging, and altering the selected units after estimation would entail significant costs. Nu\u00f1o outlined one related approach&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/3hH9NRqzGam65mgPG/five-steps-for-quantifying-speculative-interventions\"><u>here</u></a>.&nbsp;</p><p>So on the one hand, we could use one unit and get poor estimates. On the other, we could use a bunch of units but then need to enforce a lengthy, slow-moving, and still suboptimal ontology.</p><p>Below is a simple diagram to help demonstrate this decision process.</p><h2><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/qpp4wknreoxgnh3v3aoz\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/dlgq4t631zfr9kw6spe1 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/sanidl6rit2wylgrkpb9 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/fx2cpicruvltatq1g8z0 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/usuiuqxgzeluehvhnztu 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/bw0nu6v951nwsshc9vsj 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/jxsu842ys7dkkaufxr4g 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/fnwmniyylqkqryp8gvu1 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/ww62gygweum0fvkocz1k 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/rubvelkuxw79hphrvkeo 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/fdfnfxxa9ixslk8nefqi 1600w\"></h2><h3>Thought Experiment #2: Dollars, in QALYs</h3><p>How valuable is it for an average human to be given $1, in terms of a unit like QALYs? It\u2019s very uncertain. First, people\u2019s incomes vary a lot. Second, even when just referencing a specific person, it\u2019s really tough to say how money converts to QALYs.&nbsp;</p><p>Let\u2019s assume that we use distributions for estimation.</p><p>Suppose we estimate that $1 for an \u201c<i>average person</i>\u201d equates to a rough range of 0.001 to 100 (90% CI) \u201c<i>quality-adjusted life minutes</i>\u201d.&nbsp;</p><p>Is $5 better than $1? Not obviously, according to such a unit.&nbsp;</p><p>Our estimate for $5 might be \u201c0.005 to 500\u201d. However, without any information on the correlations between these two distributions, it is unclear if they are related.</p><p>Readers might assume we are unsure if $5 is more valuable than $1. Based on uncorrelated distributions, one might think that there\u2019s a&nbsp;<a href=\"https://www.squiggle-language.com/playground#code=eNqrVirOyC8PLs3NTSyqVLIqKSpN1QELuaZkluQXwURSUtMSS3NKnPNTUpWslBIVbBWCE3MLclKDU0v00oryc10yi0s0DPQMDAwVSvIVDA0MNGPyknCrMgWpMgWrislLRlGXm1hgpJGok6RTXQMkaxQSFewUkhTsFQwUrBQMayEalGoBKI464g%3D%3D\"><u>\u2153 chance</u></a> the $1 has a higher value.</p><p>Lacking proper correlations, we cannot perform even simple arithmetic operations. Basic procedures like addition and division would yield distributions with significantly wider tails than what is accurate. <i>(See Appendix 1 for an example)</i></p><p>This is not an isolated example. In practice, correlations are commonly found between interventions. Some research papers may be clearly more valuable than others, despite both being highly uncertain in terms of shared value units. The absence of correlation data can make certain decisions appear much more uncertain than they truly are.</p><h3>Takeaways</h3><p>We\u2019ve discussed estimating value in standard units. This works for homogeneous items with highly relevant known units but breaks down if either the items are heterogeneous or if there\u2019s no appropriate known unit.&nbsp;</p><p>A different option is the possibility of providing a set of units, with some conversion factors between them. This allows for the evaluation of more heterogeneous items but introduces substantial complexity.</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\"><strong>Can value similar items?</strong></td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\"><strong>Can value dissimilar items?</strong></td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\"><strong>Imposed Structure</strong></td></tr><tr><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\"><strong>Single Unit</strong></td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Yes, if unit exists.</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Very poorly</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Low. Just the choice of unit.</td></tr><tr><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\"><strong>Set of Units</strong></td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Yes, if unit exists.</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Yes, if all elements fit well with units.</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">High. Many units, each for different sets of items.</td></tr></tbody></table></figure><p>As demonstrated in this table, both approaches have severe downsides.&nbsp;</p><hr><h2>Relative Values, with Value Ratios</h2><h3>Introduction</h3><p>One solution to the downside of standard units is relative values: comparing items\u2019 values directly to each other.</p><p>To be more specific, we suggest using the ratios of values of items, or <i>value ratios</i>. As with all ratios, these are unitless.</p><p>In practice, this works by having human modelers provide estimates for the ratios of every possible pair of items. This could be done by hand, but programming functions would make it scalable to large combinations of items. Storing estimates with programming code is unusual, but powerful.</p><blockquote><p><strong>Minor Aside:&nbsp;</strong><br>Value ratios are actually a generalization of using standard value units. In those cases, valuations are estimates in the format,</p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\dfrac{value(item)}{value(unit)}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mstyle\"><span class=\"mjx-mrow\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 5.378em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 5.378em; top: -1.59em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">v</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">l</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"width: 5.378em; bottom: -1.09em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">v</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">l</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 5.378em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.68em; vertical-align: -1.09em;\" class=\"mjx-vsize\"></span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span></p><p>Instead, with value ratios, modelers could generate results for the ratios of any pair of items. This can of course include standard units, but it\u2019s not limited to them.</p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\dfrac{value(item1)}{value(item2)}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mstyle\"><span class=\"mjx-mrow\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 5.878em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 5.878em; top: -1.59em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">v</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">l</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"width: 5.878em; bottom: -1.09em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">v</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">l</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 5.878em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.68em; vertical-align: -1.09em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span></span></span></p></blockquote><p>It\u2019s important to use probability distributions instead of point values when dealing with heterogeneous and uncertain estimates. Different comparisons will have wildly different uncertainty ranges, and it\u2019s important to represent this.&nbsp;</p><p>In the rest of this section, we\u2019ll go over a concrete example, and then discuss the costs and benefits of this approach.</p><h3>Comparison Tables</h3><p>Say we want to compare the value of the following items:</p><ol><li><i>$1</i></li><li><i>$5</i></li><li><i>A 0.001% chance of death</i></li><li><i>A 0.005% chance of death</i></li></ol><p>Instead of creating one unit, let\u2019s make a table of ratio comparisons.</p><p>Let\u2019s imagine that we think that&nbsp;<i>$5</i> is roughly (<i>4.8 to 5</i>) times as valuable as<i> $1</i>, and a&nbsp;<i>0.005% chance of death</i> is roughly (<i>4.9 to 5</i>) times as bad as a&nbsp;<i>0.001% chance</i>. Say we have some unit conversion,&nbsp;<i>death_to_dollars</i>, to convert a&nbsp;<i>0.001% chance of death</i> to dollars.&nbsp;<i>death_to_dollars</i> is probably some very uncertain distribution, like (10 to 100,000).</p><p>Here\u2019s a table where each cell is an estimate of the value of the row item, divided by the value of the column item. The top right triangle is just the inverse of the bottom left triangle, so is not fully shown. The diagonals are all exactly a point mass at 1, as they refer to an item's value divided by itself.</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\"><strong>$1</strong></td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\"><strong>$5</strong></td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\"><strong>0.001% chance of death</strong></td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\"><strong>0.005% chance of death</strong></td></tr><tr><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\"><strong>$1</strong></td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\">1</td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\">1/(4.8 to 5)</td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\">-1*1/<i>death_to_dollars</i></td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\">\u2026</td></tr><tr><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\"><strong>$5</strong></td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\">4.8 to 5</td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\">1</td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\">\u2026</td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\">\u2026</td></tr><tr><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\"><strong>0.001% chance of death</strong></td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\">-1*<i>death_to_dollars</i></td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\">-1*<i>death_to_dollars</i>*(4.8 to 5)</td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\">1</td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\">\u2026</td></tr><tr><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\"><strong>0.005% chance of death</strong></td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\"><i>death_to_dollars</i></td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\">-1*(4.9 to 5) *<i>death_to_dollars</i> /(4.8 to 5)</td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\">4.9 to 5</td><td style=\"border:1pt solid #b7b7b7;padding:5pt;vertical-align:top\">1</td></tr></tbody></table></figure><p>It might be rare for someone to want to know the value of $5 in terms of \u201c<i>0.001% chance of death</i>\u201d, but in this setup, viewers can access that if they want it. When they do, they\u2019ll see that it\u2019s very uncertain. But we still can see very well the close relationships between $1 and $5, and a&nbsp;<i>0.001% chance of death&nbsp;</i>vs. a&nbsp;<i>0.005% chance of death</i>.</p><p>It would now be straightforward to add more items to this list, like,</p><ol><li>1 Micromort (a one-in-a-million chance of death)</li><li>The risk of death from a certain risky activity, like BASE jumping.</li><li>The total of both the health and financial costs of BASE jumping.</li><li>$1,000.00</li></ol><p>Note that if we were to convert everything into \u201cone unit\u201d, that would effectively be one single row of the grid. If we were to try to use a few units, that would force us to split our list up into clusters. Items like \u201c<i>The total of both the health&nbsp;<strong>and</strong> financial costs of BASE jumping</i>\u201d might not fit well into any one cluster. But with the full grid, we don\u2019t have these disadvantages.&nbsp;</p><h3>Programming Functions</h3><p>Manually constructing these tables is needlessly time-consuming. Coding simplifies this process. The majority of the task above involves the estimation of three variables and their appropriate reapplication\u2014a task easily managed by software.</p><p>The key function we need is one with approximately the following type definition:</p><pre><code>fn(id1, id2) =&gt; distribution</code></pre><p>Such a function can easily generate the full table if desired, but often we just need a small subset of it.&nbsp;</p><p>Writing such functions doesn\u2019t have to be very laborious. The key parts can be just a simple list of relationships. Here's some example pseudocode.</p><pre><code>dollar1&nbsp;=&nbsp;1&nbsp;// The value of $1. This is the unit, so we're marking as 1\ndollar5&nbsp;=&nbsp;dollar1&nbsp;*&nbsp;(4.7&nbsp;to&nbsp;5)&nbsp;// The value of $5. Assuming (steeply) diminishing marginal returns\ncostOfp001DeathChance&nbsp;=&nbsp;10&nbsp;to&nbsp;10k&nbsp;// Cost of a 0.001% chance of death, in dollars\nchanceOfDeath001&nbsp;=&nbsp;-1&nbsp;*&nbsp;costOfp001DeathChance&nbsp;*&nbsp;dollar1&nbsp;// Cost of a 0.001% chance of death\nchanceOfDeath005&nbsp;=&nbsp;chanceOfDeath001&nbsp;*&nbsp;(4.7&nbsp;to&nbsp;5)&nbsp;// Cost of a 0.005% chance of death\n\n... boilerplate code ...</code></pre><p>We give a simple working example using Squiggle in <i>Appendix 2</i>.</p><p><i>\"Functions as Estimates\"</i> are more generally a powerful mechanism to embed complicated beliefs. We believe this form deserves more attention, and intend to write more about them specifically in future posts. Relative value functions illustrate some of the possibilities of such an approach.&nbsp;</p><h3>Specific Formats</h3><p>There are multiple specific options in how to represent value ratios using probability distributions. Here are a few, each with a slightly different programming type definition.</p><p><strong>Value Ratio (Univariate)</strong></p><pre><code>fn(id1, id2) =&gt; distribution</code></pre><p>A single probability distribution meant to represent the quotient of two valuations. This is similar to the ratio form X/Y. Existing valuations that use standard units often use univariate value ratios.&nbsp;</p><p>The univariate format is very useful for presentation (i.e. showing to users). Even in more advanced cases, the X/Y univariate format will often be shown.</p><p>One important hindrance is that this format can be poor if the value of either compared item has probability mass at negative values, zero, or infinity. In those situations, bivariate or multivariate approaches are preferred.</p><p><strong>Value Ratio (Bivariate)</strong></p><pre><code>fn(id1, id2) =&gt; (distribution, distribution)</code></pre><p>A joint density of two probability distributions, meant to represent the ratios of two valuations. This is similar to the ratio form X:Y. These distributions are scale-invariant. For example, the ratio \u201c5:8\u201d is equivalent to the ratio \u201c50:80\u201d.</p><p>One downside with the bivariate approach is that it assumes the modelers broadly agree on which items have positive value and which have negative value. In the case of univariate distributions, this distinction is ambiguous, which can, in some situations, be preferable.</p><p>For example, say two modelers are comparing clean meat interventions. One modeler suspects that better clean meat work actually leads to net global harm, because of the net impact on insects. The modelers agree on the ratio of these two interventions, but one thinks they are both negative, while the other expects them to be positive. In this case, their univariate distributions could be very similar, but the bivariate distributions will be approximately inverted.&nbsp;</p><p><strong>Value Ratio (Multivariate)</strong></p><pre><code>fn([id]) =&gt; [distribution]</code></pre><p>A joint density of more than two probability distributions, representing the ratios of multiple valuations.</p><p>Depending on how value ratio programming functions are written, it could either be trivial to go from univariate outputs to multivariate outputs, or it might be very difficult.&nbsp;</p><p>Full multivariate distributions clearly have more information than the univariate or bivariate approaches, but it would take additional work to make good use of this information.</p><h2>Advantages</h2><h3>High information storage capacity</h3><p>The primary advantage of relative value formats is their ability to hold more information than simple lists of values in basic units. As stated above, a single unit would correspond to just a single row in a full value ratio comparison table; the rest is additional information.</p><p>In real-world decision-making, choices often involve comparisons between similar options. Whether it's a business comparing offerings from two appliance vendors or a donor choosing from a pool of researchers in a comparable field, these decisions often can be understood as relative values between similar options. The extra information that relative value formats provide is typically primarily around comparing similar items, so will be useful in these situations. This precision can guide decision-makers towards locally optimal decisions\u2014a significant advantage in the grand scheme of things.</p><h3>Ease of getting started</h3><p>One of the major drawbacks of units is the frequent lack of an appropriate unit. An estimator might be ready to estimate the value of several items, but if they don\u2019t know of a great unit to use for comparison, they might easily give up.&nbsp;</p><p>In contrast, with relative values, estimators just need to grasp some reasonable value definition. After they have done so, they can start doing the majority of the work by estimating the value of different projects against each other. Later on, if a great unit becomes popular, it would be easy to later add that unit to their estimated item list.&nbsp;</p><p>Arguably, units like QALYs should be treated as presentation methods rather than fundamental information storage mechanisms. Estimators shouldn\u2019t need to worry about getting the units right upfront before doing the majority of their work, it should be easy to add conversions later to whatever formats/units viewers find most useful.</p><h3>Composability</h3><p>Multiple relative value functions, each pertaining to mostly different sets of items, can be combined using approximation, provided some overlap exists. This process of stitching may not be as accurate as a full function, but in many instances, the approximation could be accurate enough to be useful.</p><p>Thus, different parties can independently focus on items they find interesting, and their work can still be integrated into a larger system.</p><p>If attempted with units, this would necessitate sets of units, which would impose a lot of overhead and coordination.</p><h3>Resiliency to different opinions</h3><p>Differences in opinion often result in minor alterations in relative values. For example, two people may disagree on the value of climate change work versus nuclear risk work, but they are likely to agree on many of the relative values between specific climate change projects and between specific nuclear risk projects. In an extreme example, two people might disagree on if a set of things is harmful or beneficial, but still agree on many of the ratios.</p><p>Estimations using sets of units would be similarly robust if the clusters were very well-chosen (\u201c<i>climate change projects</i>\u201d vs. \u201c<i>nuclear risk projects</i>\u201d), but again, this would be a lot of work to get right.</p><h2>Costs</h2><p>Relative value functions as we describe them are very novel, so the upfront costs in particular will be expensive. The marginal work of writing the functions will likely require specialized labor.</p><ol><li>We need infrastructure for storing and visualizing these functions.&nbsp;</li><li>Writing these functions takes knowledge of the right programming tools.</li><li>Complex relative value estimation can require large models, which can get very complicated.&nbsp;</li><li>Both estimators and audiences will need to become sophisticated in writing and understanding them.</li></ol><p>It\u2019s possible that with the advent of AI code generation, these costs can go down substantially.</p><h2>Limitations</h2><p>Relative value assessments serve as an advancement of cost-benefit analysis, so have the corresponding limitations of cost-benefit analysis. You can think of relative value functions as enabling tons of simple auto-generated cost-benefit analyses.</p><p><strong>Requirement of clear counterfactuals</strong></p><p>Cost-benefit analysis necessitates interventions with well-understood counterfactual scenarios. They work best with clear binary decisions. Estimating the net cost-benefit of electing one Presidential candidate over another or in relation to a probable outcome is feasible, yet one can\u2019t evaluate a Presidential candidate's value in isolation. Similarly, the marginal utility derived from consuming a particular food item can only be comprehended when juxtaposed with a distinct alternative scenario.&nbsp;</p><p><strong>Narrow definitions, particularly about notions of value</strong></p><p>Cost-benefit analyses typically use incredibly narrow notions of value. There\u2019s a very large set of ways to define and estimate value. Specific cost-benefit analyses typically either choose one or choose a very limited subset.&nbsp;</p><p>For example, these two following values could be wildly different. Estimating the relative value of a party in terms of that of microCovids, using Utility as the specification of value, will be very different than if you use a microCovid as the specification of value.&nbsp;</p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\dfrac{V_{utility}(1 party)}{V_{utility}(1 microCovid)} \u2260 \\dfrac{V_{microCovid}(1 party)}{V_{microCovid}(1 microCovid)}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mstyle\"><span class=\"mjx-mrow\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 9.251em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 9.251em; top: -1.633em;\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.186em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;\">V</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">l</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;\">y</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;\">y</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"width: 9.251em; bottom: -1.133em;\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.186em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;\">V</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">l</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;\">y</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">c</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">v</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 9.251em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.765em; vertical-align: -1.133em;\" class=\"mjx-vsize\"></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">\u2260</span></span><span class=\"mjx-mstyle MJXc-space3\"><span class=\"mjx-mrow\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 10.956em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 10.956em; top: -1.59em;\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.186em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;\">V</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">c</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">v</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;\">y</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"width: 10.956em; bottom: -1.09em;\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.186em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;\">V</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">c</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">v</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">c</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">v</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 10.956em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.68em; vertical-align: -1.09em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span></span></span></p><h2>Future Directions</h2><p>We can take the basic ideas of relative values and value ratios, and imagine even more ambitious efforts.</p><p><strong>1. Passing in parameters</strong></p><p>Instead of each passed-in relative value function parameter being a string representing a single item, we could allow for algebraic data types with continuous variables.</p><p>Imagine something like:</p><pre><code>fn(ExtraIncome({value: Pounds(52), personIncome: Pounds(57k)}), ChanceOfDeath({amountInPercent: 12.3, personAge: 45}))</code></pre><p>This function call is requesting a comparison of \u00a3<i>52 of extra income, to a person with an income of \u00a357,000</i>, to a <i>12.3% chance of death, to a person who is 45 years old.</i>&nbsp;</p><p>These input types can become arbitrarily detailed.&nbsp;</p><p>This sort of additional complexity would, of course, require more work for estimators, but the function API could be fairly straightforward.&nbsp;</p><p>Functions can also provide estimates for multiple specifications of value. For example, a function could estimate the value of an intervention as judged by either a US Republican voter or a US Democratic voter.</p><p><strong>2. Handling combinations of items</strong></p><p>Instead of taking in a set of specific items, functions can accept a&nbsp;set of lists of items.</p><p>For example,</p><pre><code>fn([project1, project2, project7], [project5, project 6])</code></pre><p>Each input would represent the counterfactual value of the complete set of things. This is important because very frequently there is significant overlap between items.&nbsp;</p><p>In the above example, a user wants to calculate the ratio of value between the total of projects [1, 2, and 7], compared to that of projects [5 and 6]. If the modeler believed that projects 1 and 7 were completely replaceable with each other, then this would need to be taken into account.</p><p><a href=\"https://en.wikipedia.org/wiki/Shapley_value\"><u>Shapley values</u></a> are one way of dealing with item value overlap, but programming functions like above would strictly represent more information. It would be easy to calculate Shapley values using complete functions, but there's not enough information to go the other direction.</p><p>This type definition would be difficult to provide very high-resolution estimates for. However, in many cases, simple approximations might still be good enough to be useful.</p><p><strong>3. Forecasting</strong></p><p>Value ratio functions could be neatly aggregated and scored, like other forecasting formats. There just needs to be some resolution mechanism for the scoring. This would likely require some panel to make subjective judgments.</p><p><strong>4. Alternative input formats, for modelers</strong></p><p>Functions are valuable for storage, but that doesn\u2019t mean that all modelers need to directly write functions. There could be custom user interfaces that elicit estimates from people in other ways, and convert those into value ratio functions. There\u2019s likely a lot of room for creativity in this area.&nbsp;</p><p><strong>5. LLM Generation</strong></p><p>With the proliferation of large language models, it seems feasible that such models could either assist in writing value ratio functions, or could do all of the work. In theory, it might be possible to use LLMs to approximate very large value ratio functions using existing online data, and then help use those for decision-making. This sort of format could be useful for clearly specifying value estimates otherwise stored in neural networks.&nbsp;</p><h2>Tradeoffs and Conclusion</h2><p>We can now expand the previous trade-off table to include relative values. Relative value formats do very well on the categories in the previous table, but have the new downside of requiring programming and infrastructure.&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\"><strong>Def</strong></td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\"><strong>How well can this value similar items?</strong></td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\"><strong>How well can this value dissimilar items?</strong></td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\"><strong>Imposed structure</strong></td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\"><strong>Needs estimators to produce code?</strong></td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\"><strong>Handles zeros and negative numbers</strong></td></tr><tr><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\"><strong>Single Unit</strong></td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">(id) =&gt; v(id) / v(unit)</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Good, if unit exists</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Bad</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Low. Just the choice of unit.</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">No</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Yes</td></tr><tr><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\"><strong>Set of Units</strong></td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">(id) =&gt; v(id) / v(unit_i)</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Good, if unit exists&nbsp;</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Good, if all elements are covered by units</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">High. Many units, each for different sets of elements.</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">No</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Yes</td></tr><tr><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\"><strong>Value Ratios, Univariate</strong></td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">(id1, id2) =&gt; v(id1) / v(id2)</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Very Good</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Very Good</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">None</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Yes</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">No</td></tr><tr><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\"><strong>Value Ratios, Multivariate</strong></td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">(id1, id2) =&gt; [v(id1), v(id2)]</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Very Good</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Very Good</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">None</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Yes</td><td style=\"border:1pt solid #999999;padding:5pt;vertical-align:top\">Yes</td></tr></tbody></table></figure><p><br>Later on we can go further, with even more complicated formats. But we think that relative value functions are a good tradeoff to aim for at this point, for groups interested in large-scale heterogeneous valuations.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/cygwblb68ddj5vxubltc\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/id6uepby8gt0nosl9trz 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/th1oddoopk5sjbpubb4f 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/hwmlqk8dpykri5phngxs 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/uxxwz3lsrkgx0yjqazkp 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/et5c3ipkhdx62vunhqzw 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/a9sqrnul7kwd7dgcxsni 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/fe607rbkvem4zf0jurde 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/anvm9dot6rjootifgqg1 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/xgas9feltw9fbavb3ttm 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EFEwBvuDrTLDndqCt/lk01jghmq8v99d1hofv0 1600w\"></p><hr><h2>Further Content</h2><p>The following is background content that might be useful to some readers. We moved this to the end of this piece to highlight that it is optional.</p><h3>Relationships to QURI\u2019s previous work</h3><p>Our work at QURI has involved developing ideas related to value estimation and forecasting.</p><p>In&nbsp;<a href=\"https://www.lesswrong.com/posts/cLtdcxu9E4noRSons/part-1-amplifying-generalist-research-via-forecasting-models\"><u>Amplifying generalist research via forecasting</u></a>, a forecasting platform was used to predict generic evaluations by an expert. Similarly, forecasters can use value ratio functions as their direct predictions, with resolutions provided by experts or community panels evaluating a narrow subset of value combinations.</p><p>In&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/hrdxf5qdKmCZNWTvs/valuing-research-works-by-eliciting-comparisons-from-ea\"><u>Valuing research works by eliciting comparisons from EA researchers</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/EPhDMkovGquHtFq3h/an-experiment-eliciting-relative-estimates-for-open\"><u>An experiment eliciting relative estimates for Open Philanthropy\u2019s 2018 AI safety grants</u></a>, we elicited valuations on the value of effective altruist projects by asking people to give probability distributions that represented the ratio of value between projects. The main goal of using ratios was for easier elicitation, not presentation. Here we focus on functions that generate relative values as a fundamental format, not as a tool for elicitation.</p><p><a href=\"https://forum.effectivealtruism.org/posts/3hH9NRqzGam65mgPG/five-steps-for-quantifying-speculative-interventions\"><u>Five steps for quantifying speculative interventions</u></a> described a workflow for using a set of units to value interventions and using conversion factors to compare interventions of different clusters. Relative values are a different alternative to this option.</p><p>Much of our recent work has been on&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/squiggle\"><u>Squiggle</u></a>, a programming language meant to make estimates using things like relative estimates easy. We\u2019re currently developing some tooling specifically for relative value estimation using Squiggle.</p><h3>Related Topics</h3><p><strong>Relative Values in Finance</strong></p><p>In financial valuation,&nbsp;<a href=\"https://pages.stern.nyu.edu/~adamodar/New_Home_Page/littlebook/intrinsicvsrelative.htm\"><u>relative values</u></a> serve as an alternative to intrinsic values. These valuations are generally employed to compare a business or investment to a small group of similar alternatives.</p><p>The widespread popularity of relative valuations in finance highlights the potential utility of relative values more broadly. However, it is important to note that relative values in finance are typically applied within narrowly defined clusters, which contrasts with the extensive scope we propose when using relative value functions.</p><p><strong>Discrete Choice Models</strong></p><p><a href=\"https://en.wikipedia.org/wiki/Discrete_choice\"><u>Discrete choice models</u></a>, or qualitative choice models, serve as valuable tools for extracting preferences between different alternatives. These models have been extensively studied within the field of economics.</p><p>Typically, discrete choice models focus on eliciting preferences between a relatively small set of options from individuals who may not have engaged in extensive reflection on their preferences.</p><p>Relative value functions, in contrast, are meant primarily as a way to express one\u2019s estimates of value. This can be the value to a collective, not an individual. For example, an estimator could build a relative value function that\u2019s meant to estimate what \u201c<i>a committee of random members of group X would believe</i>\u201d. We could very much imagine the use of discrete choice models being used to inform and complement these estimates, but the relative value functions should provide a different set of information.</p><p><strong>Combinatorial Prediction Markets&nbsp;</strong></p><p><a href=\"https://link.springer.com/article/10.1023/A:1022058209073\"><u>Combinatorial Prediction Markets</u></a> have been suggested as one useful tool for scaling prediction questions. One approach to representing complex joint distributions for these formats is by using&nbsp;<a href=\"https://arxiv.org/abs/1210.4900\"><u>Bayesian Networks</u></a>.</p><p>Arbitrary functions are more general than Bayesian Networks, which come with different tradeoffs.</p><p><strong>Utility Functions</strong></p><p>Many value unit tables can be regarded as utility functions. As utility functions, they would be&nbsp;<i>partial</i> (not accounting for&nbsp;<i>all</i> possible outcomes),&nbsp;<i>explicit</i> (as opposed to theoretical), and&nbsp;<i>approximated</i>.</p><p>The distinction between&nbsp;<i>value</i> and&nbsp;<i>utility</i> itself is often ambiguous, with different fields using different terminology. In some cases, the words are interchangeable.</p><p>It is plausible that numerous reasonable value tables would satisfy the completeness and transitivity criteria required by the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\"><u>Von Neumann-Morgenstern (VNM) utility theorem</u></a>. If decision-makers were to employ value unit tables while considering expected values, these tables would frequently correspond to VNM utility functions.</p><p>All this to say, the techniques and limitations in this document apply to both explicit utility functions and other kinds of value formats.</p><hr><h2>Appendices</h2><h3>Appendix 1: Dollar Uncorrelated Example</h3><p>If we treated these as uncorrelated, and used the ranges of (0.005 to 500) and (0.001 to 100), then a simple subtraction would result in</p><pre><code>value($5) - value($1) -&gt; -60 to 110 // from calculation\nvalue($5) / value($1) -&gt; 0.0015 to 17,000 // from calculation</code></pre><p>But what we really want, if they were properly correlated is:</p><pre><code>value($5) - value($1) -&gt; 0.004 to 400\nvalue($5) / value($1) -&gt; 5</code></pre><p>See&nbsp;<a href=\"https://www.squiggle-language.com/playground#code=eNq9kD0OwjAMha9iZQJUSDp0icQEnKBiyxKIKyqlCUodEKp6dygCtQGxstn%2Bnn%2BeO9ae%2FLWMTaPDjUkKEbNnaWdq8uFdMVjpaGnjDTLJON%2B7ow8BrSY0yl20jbj11uqQwxrESogcyEMuRAKLFywGWAxQuTiZ9MCdcgBtPLQU9JEkJN3LaZpng9TUl9rgh44nOuX6YRHnm98nl7o5WyyRVlXwzbZuaTZ1Mf%2BykfQvoFD%2FstGNeySMcQbTR8ok61l%2FB9J7qwc%3D\"><u>this Squiggle Playground</u></a> for an (approximate) example.&nbsp;</p><h3>Appendix 2: Programmatic Relative Values Functions</h3><p>Let\u2019s consider functions that produce value ratios. We are going to be doing this with <a href=\"https://www.squiggle-language.com/\">Squiggle</a>, a language for probabilistic estimation.</p><p>We want a function that takes two strings denoting two items, and returns their relative values:&nbsp;</p><pre><code>fn(id,id) -&gt; Distribution</code></pre><p>To reach a function of that type, we start with putting the conversion factors of our previous table into code:</p><pre><code>dollar1&nbsp;=&nbsp;1\ndollar5&nbsp;=&nbsp;dollar1&nbsp;*&nbsp;(4.7&nbsp;to&nbsp;5) \ncostOfp001DeathChance&nbsp;=&nbsp;10&nbsp;to&nbsp;10k&nbsp;// Cost of a 0.001% chance of death, in dollars\nvalueChanceOfDeath001&nbsp;=&nbsp;-1&nbsp;*&nbsp;costOfp001DeathChance&nbsp;*&nbsp;dollar1\nvalueChanceOfDeath005&nbsp;=&nbsp;valueChanceOfDeath001&nbsp;*&nbsp;(4.7&nbsp;to&nbsp;5)</code></pre><p>We want to now make two changes to this:</p><ol><li>Make sure we have a function that takes in two strings, and returns the corresponding variables. We do this by making a simple dictionary with the IDs as the keys, and the variables as the corresponding values. (Here, we use the same names between them.)</li><li>Make sure that all reused variables are done as <a href=\"https://www.squiggle-language.com/docs/Discussions/Three-Formats-Of-Distributions\">Sample Sets</a>. This makes sure that repeated values get divided out, or,<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\dfrac{x}{x} = 1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mstyle\"><span class=\"mjx-mrow\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.772em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 0.772em; top: -1.144em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span></span><span class=\"mjx-denominator\" style=\"width: 0.772em; bottom: -0.722em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 0.772em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.865em; vertical-align: -0.722em;\" class=\"mjx-vsize\"></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span>.</li></ol><pre><code>ss(d)&nbsp;=&nbsp;SampleSet.fromDist(d)&nbsp;// Makes sure we use Sample Sets, which is important for correlations\ndollar1&nbsp;=&nbsp;1&nbsp;// The value of $1. This is the unit, so we're marking as 1\ndollar5&nbsp;=&nbsp;ss(dollar1&nbsp;*&nbsp;(4.7&nbsp;to&nbsp;5))&nbsp;// The value of $5. Assuming (steeply) diminishing marginal returns\ncostOfp001DeathChance&nbsp;=&nbsp;ss(10&nbsp;to&nbsp;10k)&nbsp;// Cost of a 0.001% chance of death, in dollars\nchanceOfDeath001&nbsp;=&nbsp;ss(-1&nbsp;*&nbsp;costOfp001DeathChance&nbsp;*&nbsp;dollar1)&nbsp;// Cost of a 0.001% chance of death\nchanceOfDeath005&nbsp;=&nbsp;chanceOfDeath001&nbsp;*&nbsp;(4.7&nbsp;to&nbsp;5)&nbsp;// Cost of a 0.005% chance of death\n\nitems&nbsp;=&nbsp;{\n  dollar1:&nbsp;dollar1,\n  dollar5:&nbsp;dollar5,\n  chanceOfDeath001:&nbsp;chanceOfDeath001,\n  chanceOfDeath005:&nbsp;chanceOfDeath005\n}\n \nfn(item1,&nbsp;item2)&nbsp;=&nbsp;items[item1]&nbsp;/&nbsp;items[item2]\n//fn(\"chanceOfDeath005\", \"chanceOfDeath001\")</code></pre><p>That\u2019s it. This function now can generate the full value ratio comparison table. Instead of needing 4x4 written comparisons, as we would with a written table, we just really need 4 lines of code (plus boilerplate and helper functions).&nbsp;</p><p>As one might expect, these can get significantly more complicated with scale. This specific setup above calculates all listed variables, which will be computationally impractical for functions that estimate large sets of items.<br>&nbsp;</p><hr><h2><strong>Acknowledgements</strong></h2><figure class=\"image image_resized\" style=\"width:180.312px\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1668005905/mirroredImages/nB778dXNsHqHthFC5/rwrne2s5sucpflb1dmuu.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1675099594/mirroredImages/63pYakESGrQpfNw25/kpc0hdikb8lb69sceoaq.png 97w, https://res.cloudinary.com/cea/image/upload/v1675099594/mirroredImages/63pYakESGrQpfNw25/qyia8bc5a9tc8vmprbhm.png 177w, https://res.cloudinary.com/cea/image/upload/v1675099594/mirroredImages/63pYakESGrQpfNw25/lkeeraohilkzl7vfkwhd.png 257w, https://res.cloudinary.com/cea/image/upload/v1675099594/mirroredImages/63pYakESGrQpfNw25/luw188ivpteuryyrnjxm.png 337w, https://res.cloudinary.com/cea/image/upload/v1675099594/mirroredImages/63pYakESGrQpfNw25/qrohzmtc5lbpdxg0zpss.png 417w\"></figure><p>&nbsp;</p><p>This is a project of the<a href=\"https://quantifieduncertainty.org/\"><u> Quantified Uncertainty Research Institute</u></a>. Thanks to Nu\u00f1o Sempere and Willem Sleegers for comments and suggestions. Also, thanks to Anthropic's Claude, which was useful for rewriting certain sections of this and brainstorming terminology.</p>", "user": {"username": "oagr"}}]