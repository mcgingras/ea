[{"_id": "tmwu7nWKXifWFTpCj", "postedAt": "2022-11-16T06:00:58.266Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<blockquote><p>I'm analogizing Peter Singer and classical Givewell-style EA to Novik.</p></blockquote><p>What about the parts of EA that isn't Peter Singer and classical GiveWell-style EA? If those parts of EA were somewhat responsible, would it be reasonable to call that EA as well?<br><br>I don't think the analogy is helpful. Naomi Novik presumably does not claim to emphasize the importance of understanding tail risks. Naomi presumably didn't meet Caroline and encourage her to earn a lot of money so she can donate to fantasy authors, nor did Caroline say \"I'm earning all of this money so I can fund Naomi Novik's fantasy writing\". Naomi Novik did not have Caroline on her website as a success story of \"this is why you should earn money to buy fantasy books or support other fantasy writers\". &nbsp;Naomi didn't have a \"Fantasy writer's fund\" with the FTX brand on it.&nbsp;<br><br>I think it's reasonable to preach patience if you think people are jumping too quickly to blame themselves. I think it's reasonable to think that EA is actually less responsible than the current state of discourse on the forum. And I'm not making a claim about the extent EA is in fact responsible for the events. But the analogy as written is pretty poor, and doesn't really make a good case for saying EA has <strong>zero </strong>responsibility here (emphasis added):</p><blockquote><p>Who's at fault for FTX's wrongdoing?</p><p>FTX.</p><p>Ask a simple question, get a simple answer.</p><p><strong>You have no right to blame yourself any more than that.</strong> &nbsp;You weren't that important.<br>&nbsp;</p></blockquote>", "parentCommentId": null, "user": {"username": "pseudonym"}}, {"_id": "zyEndqCYeMhQHCAM7", "postedAt": "2022-11-16T06:21:26.452Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I agree that if I, personally, had steered SBF into crypto, and uncharacteristically failed to add on a lot of \"hey but please don't scam people, only do this if you find a kind of crypto you can feel good about\" I might consider myself<i> </i>more at fault. &nbsp;I even think that the Singer side of EA in fact does less talking about deontology, less writing of fiction that exemplifies the feelings and reasoning behind that deontology, less cautioning of people against twisting up their brains by chasing good ideas; on my view, the Singer side explicitly <i>starts by</i> trying to <a href=\"https://www.lesswrong.com/posts/cujpciCqNbawBihhQ/self-integrity-and-the-drowning-child\">twist people's brains up internally</a>, and at some point we should all maybe have a conversation about that.</p><p>The thing is, if you want to be sane about this sort of thing, even so and regardless I think Peter Singer himself would not have approved this, would <i>obviously</i> not have approved this. &nbsp;When somebody goes <i>that</i> far off the rails, I just don't see how you could reasonably hold responsible people who didn't tell them to do that and would've obviously not wanted them to do that.</p>", "parentCommentId": "tmwu7nWKXifWFTpCj", "user": {"username": "EliezerYudkowsky"}}, {"_id": "AozhdzetGJ7gJTpNq", "postedAt": "2022-11-16T06:24:50.409Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I still think that this incident should overall update most EAs in the direction of 1) ethical injunctions are important for humans and 2) more EAs should read the ethical injunctions section of the sequences. I agree that there is no system of ethics, or cultural movement, so awesome that it will stop its most loyal adherents from doing terrible things, but some do better than others. Nobody should feel guilty except for the people who committed the crime, but it would be great if EAs thought the right amount about how to lower the prob of events like this in the future, and that amount is not zero.&nbsp;<br><br>I'm &nbsp;also not sure how to square your advice about how I should relate to this incident with heroic responsibility.</p>", "parentCommentId": null, "user": {"username": "Ronny Fernandez"}}, {"_id": "MQNkZBbHa2ttojoYQ", "postedAt": "2022-11-16T06:38:41.874Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<blockquote><p>more EAs should read the ethical injunctions section of the sequences</p></blockquote><p>Ronny is talking about <a href=\"https://www.lesswrong.com/s/AmFb5xWbPWWQyQ244\">https://www.lesswrong.com/s/AmFb5xWbPWWQyQ244</a>.</p>", "parentCommentId": "AozhdzetGJ7gJTpNq", "user": {"username": "RobBensinger"}}, {"_id": "Kwf7c5vpzhsA2Rh4s", "postedAt": "2022-11-16T07:32:28.349Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Which ethical systems do you think have a better track record and why? Does virtue ethics, the preferred moral system of Catholics, have to take responsibility for pedophile priests? Does the rule-based ethics of deontology have to take responsibility for mass incarceration in the USA?</p>\n<p>I can understand people claiming that this ethics implies that crazy conclusion, or assigning blame to an idea that seems clearly to have inspired a particular person to do a particular act. But I have no confidence that anybody on this earth has a clue about which ethical system is most or least disproportionately to blame for common-sense forms of good or bad behavior.</p>\n", "parentCommentId": "AozhdzetGJ7gJTpNq", "user": {"username": "AllAmericanBreakfast"}}, {"_id": "mwytGJruQAPfcWpJH", "postedAt": "2022-11-16T07:33:55.560Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<blockquote><p>(Be it clear, I'm not analogizing myself to Novik in that metaphor.&nbsp; I'm analogizing Peter Singer and classical Givewell-style EA to Novik.&nbsp; I asked SBF if he wanted to meet with me ever, he never got around to it, I do not think he was a Yudkowsky fan and he hung out with some EAs who definitely weren't.)</p></blockquote><p>&nbsp;</p><p>Caroline Ellison is the disgraced and probably criminally responsible CEO of Alameda, involved in FTX\u2019s downfall.</p><p>Despite Yudhowsky's citing of Peter Singer, almost none of SBF's FTX FF money went to Peter Singer\u2019s causes of global poverty and animal welfare. No one in these causes was invited to the Bahamas with the other attendees. <strong>Yudhowsky was hosted by SBF in the Bahamas and is a regrantor of FTX FF.</strong></p><p>There are many reasons why SBF would not want to meet with him, for many of the same reasons SBF might not want to meet with me or most readers.&nbsp;</p><p>&nbsp;</p><p>As many readers of the forum know, Caroline Ellison\u2019s blog is bizarre seeming, intimate and sometimes salacious, which is why its content has not been cited widely on the EA forum until now, when Yudhowsky used one element in this top level post.&nbsp;</p><p>I think a reasonable person would say that many of Ellison's interests that are orthogonal to mainstream beliefs, are highly associated with certain parts of EA, that are local to the Bay Area, near Stanford, Ellison\u2019s Alma Mater and the rationalist community.</p><p>One example is Ellison\u2019s interest in \u201cHBD\u201d. These are highly associated with the \u201crationalist\u201d culture and do not appear in EA in many other US cities and other countries. The reason why I will not further elaborate is that it is speculative, inflammatory, hostile to try to single out the SSC/LW/Rationalist community, which has been done here explicitly here with Yudhowsky, with \"Peter Singer\" and \"Givewell-style EA\".&nbsp;</p>", "parentCommentId": null, "user": {"username": "Tyler M"}}, {"_id": "upwBwjFFa5prHfbTr", "postedAt": "2022-11-16T07:42:44.237Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I like this post much more than your previous post.</p>", "parentCommentId": null, "user": {"username": "zeshen"}}, {"_id": "Ho9ihZZB9d6NtZGXz", "postedAt": "2022-11-16T07:50:41.079Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p><strong>I am writing here because the EA community should know now, that sentiment in global health and poverty, and animal welfare, is extremely low, especially among limited talent.</strong></p><p><strong>As EAs know, the FTX money favored longtermist causes. In the aftermath of the FTX collapse, EA is globally harmed, further disadvantaging these causes already in the shadow of this money.&nbsp;</strong></p><p><strong>The departure of this talent could be a wholesale disaster for EA, and leave it in a permanent weakened state. It is not being discussed, like dangers, such as the risk of FTX, due to the dynamics of EA discourse, which is easily dominated by full time influencers like Yudhowsky.</strong></p><p><strong>In this vulnerable state, undue attempts to associate Peter Singer, \"EA\", and undue attempts to dissasociate \"LW\" and \"rationality\", are an incredibly uncooperative defection.</strong></p>", "parentCommentId": "mwytGJruQAPfcWpJH", "user": {"username": "Tyler M"}}, {"_id": "sYs3PiM3ZdKTzrGMM", "postedAt": "2022-11-16T07:58:40.417Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<blockquote><p>If there's anyone <i>other</i> than FTX who's really to blame, here, it's me. &nbsp;I've written some fiction that tries to walk people through the experience of abandoning sunk costs and facing reality. &nbsp;Including my most recent work.</p><p>Caroline Ellison, according to her tumblr, had even started reading it...</p><p>But her liveblogs cut out before she got very far in.</p><p>I just wasn't a good-enough writer; I lost my reader's attention, and with it, perhaps, the world.</p></blockquote><p>&nbsp;</p><p>We do not know her absolute state of mind when FTX (mis)used customer deposits. But, for all its worthiness, I wouldn't have predicted ahead of time that EY's writings packed a sufficient multiplicative weight to attain the sufficient condition that drove the state. I think the simple answer is whole (per the current corpus of evidence); 'FTX.'.</p>", "parentCommentId": null, "user": {"username": "specbug"}}, {"_id": "J7GBi6g6wjA4ZfLW9", "postedAt": "2022-11-16T08:15:56.292Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Another example of this uncooperative behavior is the LW <a href=\"https://www.lesswrong.com/posts/WmscKHmyZvuRg48HL/?commentId=BtWjbtKZhhzng3oce\">treatment of the Gopalakrishnan's post</a>, while received mixed reception, claims to point out serious misconduct.</p><p>On balance, negative claims of this supposed behavior is highly associated with SF and the EA/LW communities there.</p><p>The author implicates LW as well as EA, for example she says,</p><blockquote><p><i>LessWrong style jedi mindtricks while they stand to benefit from the erosion of your boundaries.</i></p><p><i>My experience resonates with a few other women in SF I have spoken to. They have also met red pilled, exploitative men in EA/rationalist circles. EA/rationalism and redpill fit like yin and yang. Akin to how EA is an optimization of altruism with \u201csuboptimal\u201d human tendencies like morality and empathy stripped from it, red pill is an optimized sexual strategy with the humanity of women stripped from it.</i></p></blockquote><p>This is the <a href=\"https://www.lesswrong.com/posts/WmscKHmyZvuRg48HL/?commentId=BtWjbtKZhhzng3oce\">response by a member of the LW team</a>, which reads more like an attempt to dissociate this conduct with LW and put it squarely with EA, instead of stating that the post seems tendentious or unproductive.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b18f68d3e4c3c286c8476ac519df0486fd611118e6485b5a.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b18f68d3e4c3c286c8476ac519df0486fd611118e6485b5a.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b18f68d3e4c3c286c8476ac519df0486fd611118e6485b5a.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b18f68d3e4c3c286c8476ac519df0486fd611118e6485b5a.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b18f68d3e4c3c286c8476ac519df0486fd611118e6485b5a.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b18f68d3e4c3c286c8476ac519df0486fd611118e6485b5a.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b18f68d3e4c3c286c8476ac519df0486fd611118e6485b5a.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b18f68d3e4c3c286c8476ac519df0486fd611118e6485b5a.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b18f68d3e4c3c286c8476ac519df0486fd611118e6485b5a.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b18f68d3e4c3c286c8476ac519df0486fd611118e6485b5a.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b18f68d3e4c3c286c8476ac519df0486fd611118e6485b5a.png/w_1040 1040w\"></figure><figure class=\"image\"><img></figure><p>The above comment is not intellectually honest.</p>", "parentCommentId": "Ho9ihZZB9d6NtZGXz", "user": {"username": "Tyler M"}}, {"_id": "iB3PeP8zgry9uBjt3", "postedAt": "2022-11-16T08:19:53.406Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I was passing through the Bahamas and asked if FTX wanted me to talk to the EAs they had on fellowships there. &nbsp;They paid for my hotel room and an Airbnb when the hotel got full, for a week. &nbsp;I'm not sure but I don't <i>think</i> I remember getting to see SBF at all while I was at the hotel. &nbsp;Didn't go swimming or sunning or any such because I am not a very outdoors person. &nbsp;It does not seem entirely accurate to characterize this as \"was hosted by SBF in the Bahamas\".</p><p>The Future Fund basically turned down all my ideas until the regrantor program started; I made two recommendations and I expect neither of them will pay out now unless they moved very fast.</p><p>Unless I specifically defend an idea, I think that a lot of what gets said in the San Francisco Bay Area is also not something I'd accept as my fault. &nbsp;Eg there was a lot of drug use involved in this going wrong, which I'm <i>sure </i>did not start from me, and I've suggested increasingly loudly and openly of late that people cut back on the drug use; maybe it's Bay-associated idk, but it sure is not Yudkowsky-endorsed.</p><p>I did think Will MacAskill was from the Singer side of things, so I admit to being surprised if the highly-legible side of effective altruism got nothing, unless it was a room-for-more-funding issue with Givewell+OpenPhil having already snapped up all the fruit hanging lower than GiveDirectly. &nbsp;I will consider myself tentatively corrected on that point unless I hear otherwise or have investigated.</p>", "parentCommentId": "mwytGJruQAPfcWpJH", "user": {"username": "EliezerYudkowsky"}}, {"_id": "GhsR8ucPjnEvyvdgE", "postedAt": "2022-11-16T08:22:22.045Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Yudkowsky wrote this above.&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1f9186a8bf8c3ad846370b76868f98341ffbea1e2cce9bd4.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1f9186a8bf8c3ad846370b76868f98341ffbea1e2cce9bd4.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1f9186a8bf8c3ad846370b76868f98341ffbea1e2cce9bd4.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1f9186a8bf8c3ad846370b76868f98341ffbea1e2cce9bd4.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1f9186a8bf8c3ad846370b76868f98341ffbea1e2cce9bd4.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1f9186a8bf8c3ad846370b76868f98341ffbea1e2cce9bd4.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1f9186a8bf8c3ad846370b76868f98341ffbea1e2cce9bd4.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1f9186a8bf8c3ad846370b76868f98341ffbea1e2cce9bd4.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1f9186a8bf8c3ad846370b76868f98341ffbea1e2cce9bd4.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1f9186a8bf8c3ad846370b76868f98341ffbea1e2cce9bd4.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1f9186a8bf8c3ad846370b76868f98341ffbea1e2cce9bd4.png/w_931 931w\"></figure><p>&nbsp;</p><blockquote><p>the Singer side explicitly <i>starts by</i> trying to <a href=\"https://www.lesswrong.com/posts/cujpciCqNbawBihhQ/self-integrity-and-the-drowning-child\">twist people's brains up internally</a>, and at some point we should all maybe have a conversation about that.</p></blockquote><p>It would be wild to see anyone defend or explain the terms \"Singer side\" or \"twisting people's brains\" in this context, much less the intentional act implied.</p><p>This is a flat out attack that uses ideas and sentiment from actual criticisms of MIRI/LW, which I do not cite, because it is inflammatory. This is likely to preempt anticipated future criticism using these arguments.</p>", "parentCommentId": "iB3PeP8zgry9uBjt3", "user": {"username": "Tyler M"}}, {"_id": "TmncLkuFBKNX8cwpo", "postedAt": "2022-11-16T08:31:14.362Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I don\u2019t [currently] view EA as particularly integral to the FTX story either. Usually, blaming ideology isn\u2019t particularly fruitful because people can contort just about anything to suit their own agendas. It\u2019s nearly impossible to prove causation, we can only gesture at it.</p><p>However, I\u2019m nitpicking here but - is spending money on naming rights truly evidence that SBF wasn\u2019t operating under a nightmare utilitarian EA playbook? It\u2019s probably evidence that he wasn\u2019t particularly good at EA, although one could argue it was the toll to further increase earnings to eventually give. It\u2019s clearly an ego play but other real businesses buy naming rights too, for business(ish) reasons, and some of those aren\u2019t frauds\u2026 right?&nbsp;</p><p>I nitpick because I don't find it hard to believe that an EA could also 1) be selfish, 2) convince themselves that ends justify the means and 3) combine 1&amp;2 into an incendiary cocktail of confused egotism and lumpy, uneven righteousness that ends up hurting &nbsp;people. I\u2019ve met EAs exactly like this, but fortunately they usually lack the charm, knowhow and/or resources required to make much of a dent.&nbsp;</p><p>In general, I\u2019m not surprised with the community's reaction. Best case scenario, it had no idea that the fraud was happening (and looks a bit na\u00efve in hindsight) and its dirty laundry is nonetheless exposed (it\u2019s not so squeaky clean after all). Even if EA was only a small piece in the machinery that resulted in such a [big visible] fraud, the community strives to do *important* work and it feels bad for potentially contributing to the opposite.</p>", "parentCommentId": null, "user": {"username": "samuel"}}, {"_id": "68EdeKFCZp748Qf3N", "postedAt": "2022-11-16T08:47:29.840Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>For onlookers: this thread contains three examples of this dishonest behavior, that range from <a href=\"https://forum.effectivealtruism.org/posts/SP3Hkas3jo6i2cmqb/who-s-at-fault-for-ftx-s-wrongdoing?commentId=J7GBi6g6wjA4ZfLW9\">optics management</a>, to shameless attacks (\"<a href=\"https://forum.effectivealtruism.org/posts/SP3Hkas3jo6i2cmqb/who-s-at-fault-for-ftx-s-wrongdoing?commentId=GhsR8ucPjnEvyvdgE\">brain twisting</a>\" by Peter Singer people ???).</p><p>This is an intentional, premeditated strategy by Eliezer and LW/MIRI staff, where:</p><ul><li>Potential criticisms involving FTX personel's behavior is attached to all of EA, instead of what a reasonable person reading Ellison's blog, would find more associated to parts of EA local to the SF area, shielding LW/MIRI from this criticism.&nbsp;</li><li>Preempt anticipated future criticism using these arguments</li><li>In a time of \"evaporative cooling\"\u2014Eliezer may try to shift EA's state and shift it to his faction<ul><li>This is despite the fact there are structural reasons this has limited gains (there's already a LW!)\u2014this is less than zero sum.</li></ul></li></ul><p>Note the direct attempts to reference or associate Will MacAskill in this behavior.</p><p>LW/MIRI is doing this unilaterally, no other side in EA has criticized LW. This is because there are no other paid influencers, morale is low. This is shameless.&nbsp;</p><p>This is coordinated and happening on the EA forum.</p>", "parentCommentId": "mwytGJruQAPfcWpJH", "user": {"username": "Tyler M"}}, {"_id": "BmQBMR78F8o3dyj3h", "postedAt": "2022-11-16T08:58:34.838Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I agree that EA likely wasn't a major causal factor for FTX/SBF's likely fraud. Unfortunately, it's a situation where even if it's not our fault it is our problem. People are trashing EA across the internet because of Sam's position in the movement. His Twitter profile pic still has him wearing an EA shirt for christ sake!&nbsp;</p>", "parentCommentId": null, "user": {"username": "andrewpei"}}, {"_id": "gBwbFgyYAKDcrYPD2", "postedAt": "2022-11-16T09:36:26.388Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>The point there isn't so much, \"He could not have had any EA thoughts in his head at all\", which I doubt is really true - though also there could've just been pressure from coworkers, and office politics around it, resolving in something like the Future Fund so that they were doing anything. &nbsp;My point is just that this nightmare is probably not one of a True Sincere Committed EA Act Utilitarian doing these things; that person would've tried to take more money off the table, earlier, for the Future Fund. &nbsp;Needing an e-sports site named after your company - that's indeed something that other businesses do for business reasons; and if it feeds your business, that's real, that's urgent, that has to happen <i>now.</i> &nbsp;The philanthropy side was evidently not like that.</p>", "parentCommentId": "TmncLkuFBKNX8cwpo", "user": {"username": "EliezerYudkowsky"}}, {"_id": "tsZTKJ4mRSpCHTKXp", "postedAt": "2022-11-16T09:37:29.597Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I was not being serious there. &nbsp;It was meant to show - see, I could blame myself too, if I wanted to be silly; now don't be that silly.</p>", "parentCommentId": "sYs3PiM3ZdKTzrGMM", "user": {"username": "EliezerYudkowsky"}}, {"_id": "y2GnqKFCpxkKNXSAL", "postedAt": "2022-11-16T10:04:52.996Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I think you probably need to label your account \"EliezerYudkowsky (parody)\" because otherwise a few people might not realize you're occasionally being sarcastic, and then you might get banned from Twitter.</p>", "parentCommentId": "tsZTKJ4mRSpCHTKXp", "user": {"username": "Davidmanheim"}}, {"_id": "7fAXxHhbinC55hHaJ", "postedAt": "2022-11-16T10:07:40.168Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>So are people who never attacked EA before suddenly doing so? That isn't what I've seen. I've seen lots of bad-faith takes about how this is proof of what they always thought, and news reporting which is about as accurate as you'd expect - that is, barely correct on the knowable facts, and misleading or confused about anything more complicated than that.</p>", "parentCommentId": "BmQBMR78F8o3dyj3h", "user": {"username": "Davidmanheim"}}, {"_id": "JXc7Xh6ni8vjwkGzj", "postedAt": "2022-11-16T10:11:44.680Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>idk, when people explicitly endorse your ideology as why they endorse \"high leverage and double-or-nothing flips\" I think it's at least worth taking a look at yourself. Now quite probably the person in question has misunderstood your ideology and doesn't understand why EAs do in fact care about the risk of ruin and why stealing money isn't ok, but then perhaps try to correct them?<br><br>Fwiw I think it very unlikely that the decision to use customer funds was a one-off decision made in 2022. My view is that that FTX was set up from the start to use customer money as a source of cheap capital for Alameda. In 2018 Alameda was offering potential investors a 15% guaranteed return on loans. It seems fairly likely that at some point SBF figured \"fuck this, why are we offering these dorks 15% when we can just set up our own exchange and access huge amounts of capital at 0%\". Never mind that the fact that privileged information from the exchange may well have opened up &nbsp;for Alameda more ways to make money!&nbsp;<br><br>The plan, imo, was always to accrue as much as wealth as possible as fast as possible with as few ethical constraints as possible. This worked for a while because Alameda's trades were profitable and crypto was in a bull market. This plan may or may not have been a EA-aligned, but if you have short enough AI/pandemic timelines (I don't), it doesn't seem obviously non-compatible and given the career backgrounds and interest set of all the major people involved, yes, I think they were committed and sincere EAs who really believed this stuff. SBF's own weird version of EA, at least, seems to have played a fairly large role in why they took on so much risk, as he himself explained in an overly long and boring twitter thread somewhere and Caroline also mentioned on her blog.<br><br>It also makes zero sense to compare FTX's spending on stadiums vs the Future Fund as a sign for how much they cared about these respective things. The Future Fund would almost certainly have got way more money in subsequent years, while the stadium rights purchase was a form of advertising designed to help grow the business faster. I can't imagine SBF is a big sports fan and was doing that sort of thing because he really enjoyed seeing the FTX logo on umpire shirts.&nbsp;</p><p>Not to Godwinpost, but this isn't really \"were Nietzsche and Wagner at fault for the Nazis\", it's more \"were Nietzsche and Wagner at fault for the Nazis if they'd actually lived throughout the 1930s and worked in prominent cultural education posts in the German state bureaucracy.\"</p>", "parentCommentId": null, "user": {"username": "Sabs"}}, {"_id": "5xwsCjattZoqrSHZD", "postedAt": "2022-11-16T11:15:46.087Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>This seems to be a false equivalence. There's a big difference between asking \"did this writer, who wrote a bit about ethics and this person read, influence this person?\" vs \"did this philosophy and social movement, which focuses on ethics and this person explicitly said they were inspired by, influence this person?\"</p><p>I agree with you that the question</p><blockquote><p>Who's at fault for FTX's wrongdoing?</p></blockquote><p>has the answer</p><blockquote><p>FTX</p></blockquote><p>But the question</p><blockquote><p>Who <i>else </i>is at fault for FTX's wrongdoing?</p></blockquote><p>&nbsp;Is nevertheless sensible and cannot have the answer FTX.</p>", "parentCommentId": null, "user": {"username": "MichaelPlant"}}, {"_id": "tdAwsSZpdXigT5PK4", "postedAt": "2022-11-16T12:06:57.383Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>It is well-written, but I am not particularly convinced by the fantasy fiction analogy \u2014 it feels a lot more like \u201cHere\u2019s this very different situation, and you agree that the conclusions would be different. That would even be true if we modify it in several hard-to-imagine ways.\u201d</p>\n<p>In particular, I don\u2019t see any reasonable analogies for:</p>\n<ul>\n<li>EA\u2019s \u201cEarning to Give\u201d career path, up to and including 80k featuring a profile on SBF as an exemplar.</li>\n<li>The specific logic of \u201cmy marginal money is going to be donated\u201d =&gt; \u201cI should be closer to risk-neutral\u201d, which I haven\u2019t really seen rebutted on the facts (most instead argue that in reality, SBF/FTX/Alameda went too far and were risk-seeking).</li>\n</ul>\n<p>That SBF ultimately contributed such a paltry amount of his apparent fortune is more impactful, but mainly as a reminder of how small and vulnerable EA actually is. It might very well be true that we didn\u2019t mean that much to SBF, but he meant a lot to us.</p>\n", "parentCommentId": null, "user": {"username": "Sam Elder"}}, {"_id": "ZsKB8sJGcJEJynHEY", "postedAt": "2022-11-16T12:22:38.340Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<blockquote>\n<p>I agree that if I, personally, had steered SBF into crypto, and uncharacteristically failed to add on a lot of \"hey but please don't scam people, only do this if you find a kind of crypto you can feel good about\" I might consider myself more at fault.</p>\n</blockquote>\n<p>Given how big of a role EA apparently had in the origin of Alameda (Singh says in the Sequoia puff piece that it wouldn\u2019t have started without EA), there very likely are many members of the community who offered more encouragement and/or didn\u2019t give as many warnings as they should have.</p>\n<p>I don\u2019t know what point that fault transcends the individual and attributes to the community, but at the very least, adding up other individuals\u2019 culpabilities in steering SBF to crypto without appropriate caution would seem to put a lot of the blame you say you personally avoid on EA as a whole.</p>\n", "parentCommentId": "zyEndqCYeMhQHCAM7", "user": {"username": "Sam Elder"}}, {"_id": "LFGzTvskJphjxL7h2", "postedAt": "2022-11-16T12:49:34.620Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I mean he is a big sports fan, at least baseball, at least when he was younger. I got linked to his blog from 10 years ago from something, and the number one and two sets of posts were about baseball statistics.</p>\n", "parentCommentId": "JXc7Xh6ni8vjwkGzj", "user": {"username": "timunderwood"}}, {"_id": "aN2rDiHwzebksidx6", "postedAt": "2022-11-16T13:32:54.166Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>There are however a number of things we ARE at fault for here.</p>\n<ol>\n<li>We as a community idolised SBF, including promoting him in many presentations, a relatively fawning interview by 80K which continued to promote the idea that SBF was living frugally (surely people knew by then that was bs). We could have chosen not to do this</li>\n<li>Will MacAskell made the introduction to Elon to try and get SBF to help buy twitter. We still have no public information why, but this would have given SBF more power and used a lot of money that could have been used on doing good to that end. Why?</li>\n<li>Carrick Flynn campaign; we as a community hugely supported this campaign which was quite blatantly SBF and GBF trying to buy a seat for their interests. Sure, we as a community thought this was also our interests (and I still assume Carrick would have done a good job?) but once again this was a way the community encouraged and didn't question SBFs power</li>\n<li>Will MacAskell knew SBF for 9 years, seemingly relatively closely. Its not Wills fault SBF committed fraud, but it is partially Wills fault SBF became such a face for the community within and outside of it. Maybe no ordinary person could have known SBF was a fraudster. But then, if we only expect from Will what we expect of \"ordinary people\" why are we happy trusting him with so much power in the community? The only justification I can think of is he is just so so so much better at decision making, having a reliably positive  impact and avoiding risks to the community and project of EA. Its clear that Will isn't this uniquely good. So why do we trust him (and others) with so much power in the community</li>\n</ol>\n", "parentCommentId": null, "user": {"username": "Gideon Futerman"}}, {"_id": "CG27FjKtmZKfpcWCK", "postedAt": "2022-11-16T13:40:34.007Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>The role of the EA movement in the case of FTX seems surely to meet the level of influence for some of the impact win's that EA has had so far <a href=\"https://www.effectivealtruism.org/impact\">here</a>.<br><br>Perhaps most prominently, the movement:</p><ul><li>Gave the idea of 'Earning to Give' to Sam</li><li>Provided a primary motivation to Sam and other FTX leadership to build the exchange</li></ul><p>For example, when comparing to the case of Sendwave, the influence seems at least comparable and if not larger e.g. played a motivational role in founding a company, for the purpose of improving the world. (I'm not familiar with Wave's founders motivations, so could be wrong here)</p><p>In welfare terms alone, the impact of FTX's collapse on it's customers seems plausibly comparable to some of the impact win's of the movement to date. I.e. of the order of $1bn in lost funds. Given this, I think that an honest impact evaluation of the EA movement would include the harm caused to customers through FTX's collapse.</p><p>This is relevant not for blame assignment, but because it's very decision-relevant to EA's mission of improving the world. For example, when in the future deciding how much to emphasise harm avoidance when &nbsp;encouraging the (good and novel) idea of Earning to Give.</p>", "parentCommentId": null, "user": {"username": "callum_calvert"}}, {"_id": "fRqMmXhfsx96CGwJC", "postedAt": "2022-11-16T15:11:04.503Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<blockquote><p>I think that an honest impact evaluation of the EA movement would include the harm caused to customers through FTX's collapse.</p></blockquote><p><br>Agreed. However:</p><blockquote><p>In welfare terms alone, the impact of FTX's collapse on it's customers seems plausibly comparable to some of the impact win's of the movement to date. I.e. of the order of $1bn in lost funds.</p></blockquote><p>Are you talking about welfare terms or financial terms? Because $1bn in lost savings of FTX customers seems very different <i>in welfare terms </i>to $1bn spent on bed-nets etc. I think there are strong reasons FTX shouldn't have acted the way it did, but suggesting these two things are comparable in welfare terms because they are similar in financial terms seems like an error to me.</p>", "parentCommentId": "CG27FjKtmZKfpcWCK", "user": null}, {"_id": "ndrfmR8cK7jrK92wg", "postedAt": "2022-11-16T15:25:07.832Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>First a disclaimer that I\u2019ve never got anywhere close to interacting with SBF personally; I\u2019m very much an outsider to this situation.\nHowever, from everything I have read, I think it\u2019s pretty ridiculous to suggest that EA wasn\u2019t the main reason SBF tried so hard to maximize profit (poorly, I might add, but it seems like that was his goal) to the point of committing fraud. As far as I understand EA was SBF\u2019s primary guiding ideology; it is why he went down this career path of Jane Street and then starting his own companies. This post seems overly reliant on the fun fact that SBF paid more for e-sports naming rights than on EA donations to show why actually Sam didn\u2019t care about EA that much. But these are two completely separate things! E-sports naming rights is just a means of advertising, with the goal of making FTX more money which will eventually allow SBF to donate more to EA. I think there\u2019s also decent evidence that SBF was looking to ramp up donations in the future, as Effective Altruism continues to grow and is able to use more funding. Once you take out this fun fact about SBF\u2019s current EA spending, I think this whole argument kind of falls apart.</p>\n", "parentCommentId": null, "user": {"username": "RedStateBlueState"}}, {"_id": "WdDSvKdiWjFXyYYZe", "postedAt": "2022-11-16T15:30:56.808Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I think liberalism has a better track record than communism, for instance. No, but I do think Catholics should spend some time thinking about what's up with catholic priests molesting children, particularly if that catholic has any control over what goes on in the church. In general I do not think blaming this or that ethical system or social movement makes much sense, but noticing that the adherents of some social movement or ethical system tend to do some particular kind of bad thing more often than others can be useful, particularly if you are a part of that social movement.&nbsp;</p>", "parentCommentId": "Kwf7c5vpzhsA2Rh4s", "user": {"username": "Ronny Fernandez"}}, {"_id": "sfmBySRizdYNT3Dpn", "postedAt": "2022-11-16T15:41:39.134Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>From an upcoming post I am drafting: I would point out that \u2018heroes put the entire group, many innocent people, \u2018the city,\u2019 planet Earth or even the whole damn universe or multiverse in grave danger to save any main character or other thing that We Cannot Bear To Lose, because That\u2019s What Heroes Do\u2019 is&nbsp;<i>ubiquitous&nbsp;</i>in our fantasy media. It might be a majority of DC comics plots. Villains invoke it because decision theory, they know it will work, and even without that it is rather mind-bogglingly awful. That kind of thinking needs to be widely condemned and fall in status at least via&nbsp;<a href=\"https://tvtropes.org/pmwiki/pmwiki.php/Main/WhatTheHellHero\"><u>What The Hell Hero</u></a> moments, and I worry it has more influence in these situations than we think.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Zvi "}}, {"_id": "3yDXvH43wAL2RFYym", "postedAt": "2022-11-16T15:46:27.265Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Is there a source for the $140M figure?</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "Lvehi4C2KQNxBh6q4", "postedAt": "2022-11-16T15:59:01.404Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>\"My point is just that this nightmare is probably not one of a True Sincere Committed EA Act Utilitarian doing these things\" - I agree that this is most likely true, but my point is that it's difficult to suss out the \"real\" EAs using the criteria listed. Many billionaires believe that the best course of philanthropic action is to continue accruing/investing money before giving it away.&nbsp;</p><p>Anyways, my point is more academic than practical, the FTX fraud seems pretty straight forward and I appreciate your take. I wonder if this forum would be having the same sorts of convos after Thanos snaps his fingers.</p>", "parentCommentId": "gBwbFgyYAKDcrYPD2", "user": {"username": "samuel"}}, {"_id": "skq5nsoEGzHf6heAZ", "postedAt": "2022-11-16T16:47:31.275Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Couldn't agree more strongly.</p><p>The inferential jump from someone reading a book in their spare time, making a pretty superficial Goodreads review about a main takeaway, to</p><blockquote><p>It sounds like - Caroline might have been under the impression, as late as Oct 10, that what she was doing at FTX was the thing that's hard and scary but right?</p></blockquote><p>Is a pretty big one, and kinda egregious honestly.</p>", "parentCommentId": "5xwsCjattZoqrSHZD", "user": {"username": "Rina"}}, {"_id": "FwEzvqF62qBrcuRyq", "postedAt": "2022-11-16T17:27:32.687Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I'm very new here, just signed in today so I'm unfamiliar with all the formats at the point but I often seek to explain how biological factors can play a role in morality or our decision making because it can be useful to understand our brain's limitations among all the other factors.</p>\n<p>Stress, isolation and the position of power have consequences for the brain. The less cooperative one has to function within their society to leads to damage in areas of the brain responsible for decision making, the anterior cingulate cortex being the most key area. It breaks down the ability for the human to manage their own emotions and impulse control becomes a problem over time. I haven't followed SBFs career closely but there were perhaps signs of his brain struggling. Addiction is a typical symptom this is occuring.</p>\n<p>We are but human, all of us, and we can't supercede how our brains evolved to operate. It's a very tricky position to have so much responsibility and power, that consolidation of power becomes potentially harmful as it did with FTX. This is my no means an excuse for SBF, it's just a potential problem to consider when engaging in effective altruism through large amounts of wealth. Managing ones own ego is maybe a lot harder than some anticipated. Perhaps even the most difficult tasks they'll ever do because of the design of the brain. Lots and lots of emotional self care would help, but free will is a tricky concept of whether it's possible to beat our own brains since we make decisions before we become aware we did. Self compassion is a very important piece, maybe one SBF did not have.</p>\n", "parentCommentId": null, "user": {"username": "Missy Maserati"}}, {"_id": "k7feDPvTKj8Qaq5aq", "postedAt": "2022-11-16T18:21:11.748Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Here are some excerpts from <a href=\"https://archive.ph/XTy9z\">Sequoia Capital's profile on SBF </a>(published September 2022, now pulled).&nbsp;</p><p>On career choice:&nbsp;</p><blockquote><p>Not long before interning at Jane Street, SBF had a meeting with Will MacAskill, a young Oxford-educated philosopher who was then just completing his PhD. Over lunch at the Au Bon Pain outside Harvard Square, MacAskill laid out the principles of effective altruism (EA). The math, MacAskill argued, means that if one\u2019s goal is to optimize one\u2019s life for doing good, often most good can be done by choosing to make the most money possible\u2014in order to give it all away. \u201cEarn to give,\u201d urged MacAskill.&nbsp;<br><br>...&nbsp;<br><br>It was his fellow [fraternity members] who introduced SBF to EA and then to MacAskill, who was, at that point, still virtually unknown. MacAskill was visiting MIT in search of volunteers willing to sign on to his earn-to-give program.&nbsp;<br><br>At a caf\u00e9 table in Cambridge, Massachusetts, MacAskill laid out his idea as if it were a business plan: a strategic investment with a return measured in human lives. The opportunity was big, MacAskill argued, because, in the developing world, life was still unconscionably cheap. Just do the math: At $2,000 per life, a million dollars could save 500 people, a billion could save half a million, and, by extension, a trillion could theoretically save half a billion humans from a miserable death.&nbsp;<br><br>MacAskill couldn\u2019t have hoped for a better recruit. Not only was SBF raised in the Bay Area as a utilitarian, but he\u2019d already been inspired by Peter Singer to take moral action. During his freshman year, SBF went vegan and organized a campaign against factory farming. As a junior, he was wondering what to do with his life. And MacAskill\u2014Singer\u2019s philosophical heir\u2014had the answer: The best way for him to maximize good in the world would be to maximize his wealth.&nbsp;<br><br>SBF listened, nodding, as MacAskill made his pitch. The earn-to-give logic was airtight. It was, SBF realized, applied utilitarianism. Knowing what he had to do, SBF simply said, \u201cYep. That makes sense.\u201d But, right there, between a bright yellow sunshade and the crumb-strewn red-brick floor, SBF\u2019s purpose in life was set: He was going to get filthy rich, for charity\u2019s sake. All the rest was merely execution risk.&nbsp;<br><br>His course established, MacAskill gave SBF one last navigational nudge to set him on his way, suggesting that SBF get an internship at Jane Street that summer.&nbsp;<br><br>In 2017, everything was going great for SBF. He was killing it at Jane Street... He was giving away 50 percent of his income to his preferred charities, with the biggest donations going to the Centre for Effective Altruism and 80,000 Hours. Both charities focus on building the earn-to-give idea into a movement. (And both had been founded by Will MacAskill a few years before.) He had good friends, mostly fellow EAs. Some were even colleagues.&nbsp;</p><p>... [much further down in the profile]&nbsp;<br><br>So when, that next summer, MacAskill sat with SBF in Harvard Square and carefully explained, in the way only an Oxford-educated philosopher can, that the practice of effective altruism boils down to \u201capplied utilitarianism,\u201d Snipe\u2019s arrow hit SBF hard. He\u2019d found his path. He would become a maximization engine. As he wrote in his blog, \u201cIf you\u2019ve decided that some of your time\u2014or money\u2014can be better spent on others than on yourself, well, then, why not more of it? Why not all of it?\u201d&nbsp;</p></blockquote><p>&nbsp;</p><p>On deciding what to do after leaving Jane Street:&nbsp;</p><blockquote><p>SBF made a list of possible options, with some notes about each:</p><ol><li>Journalism\u2014low pay, but a massively outsized impact potential.</li><li>Running for office\u2014or maybe just being an advisor?</li><li>Working for the movement\u2014EA needs people!</li><li>Starting a startup\u2014but what, exactly?</li><li>Bumming around the Bay Area for a month or so\u2014just to see what happens.&nbsp;</li></ol></blockquote><p>&nbsp;</p><p>On setting up the initial Japanese Bitcoin arbitrage at Alameda:&nbsp;</p><blockquote><p>Fortunately, SBF had a secret weapon: the EA community. There\u2019s a loose worldwide network of like-minded people who do each other favors and sleep on each other\u2019s couches simply because they all belong to the same tribe. Perhaps the most important of them was a Japanese grad student, who volunteered to do the legwork in Japan. As a Japanese citizen, he was able to open an account with the one (obscure, rural) Japanese bank that was willing, for a fee, to process the transactions that SBF\u2014newly incorporated as Alameda Research\u2014wanted to make.&nbsp;</p><p>The spread between Bitcoin in Japan and Bitcoin in the U.S. was \u201conly\u201d 10 percent\u2014but it was a trade Alameda found it could make every day. With SBF\u2019s initial $50,000 compounding at 10 percent each day, the next step was to increase the amount of capital.&nbsp;</p><p>At the time, the total daily volume of crypto trading was on the order of a billion dollars. Figuring he wanted to capture 5 percent of that, SBF went looking for a $50 million loan. Again, he reached out to the EA community. Jaan Tallinn, the cofounder of Skype, put up a good chunk of that initial $50 million.&nbsp;</p></blockquote><p>&nbsp;</p><p>On the early days at Alameda:&nbsp;</p><blockquote><p>The first 15 people SBF hired, all from the EA pool, were packed together in a shabby, 600-square-foot walk-up, working around the clock. The kitchen was given over to stand-up desks, the closet was reserved for sleeping, and the entire space overrun with half-eaten take-out containers. It was a royal mess. But it was also the good old days, when Alameda was just kids on a high-stakes, big-money, earn-to-give commando operation. Fifty percent of Alameda\u2019s profits were going to EA-approved charities.</p><p>\u201cThis thing couldn\u2019t have taken off without EA,\u201d reminisces Singh, running his hand through a shock of thick black hair. He removes his glasses to think. They\u2019re broken: A chopstick has been Scotch taped to one of the frame\u2019s sides, serving as a makeshift temple. \u201cAll the employees, all the funding\u2014everything was EA to start with.\u201d&nbsp;</p></blockquote><p>&nbsp;</p><p>On how he was thinking about future earnings:&nbsp;</p><blockquote><p>\u201cAm I,\u201d &nbsp;[reporter asks], \u201ctalking to the world\u2019s first trillionaire?\u201d&nbsp;</p><p>...</p><p>\u201cMaybe let\u2019s take a step back,\u201d he says, only to launch into an explanation of his own, personal utility curve: \u201cWhich is to say, if you plot dollars-donated on the X axis, and Y is how-much-good-I-do-in-the-world, then what does that curve look like? It\u2019s definitely not linear\u2014it does tail off, but I think it tails off pretty slowly.\u201d</p><p>His point seems to be that there is, out there somewhere, a diminishing return to charity. There\u2019s a place where even effective altruism ceases to be effective. \u201cBut I think that, even at a trillion, there\u2019s still really significant marginal utility to dollars donated.\u201d&nbsp;</p><p>...</p><p>\u201cSo, is five trillion all you could ever use to help the world?\u201d&nbsp;</p><p>...</p><p>\u201cOkay, at that scale, I think the answer might be yes. Because, if your spending is on the scale of the U.S. government, it might have too weird and distortionary an impact on things.\u201d&nbsp;</p><p>... so, money spent now will be more effective at making the world a better place than money spent later. \u201cI think there are some things that are pretty urgent,\u201d SBF says. \u201cThere\u2019s just a long series of crucial considerations, and all of them matter a lot\u2014and you can\u2019t fuck any of them up, or you miss most of the total value that you could ever get.\u201d&nbsp;</p><p>To be clear, SBF is not talking about maximizing the total value of FTX\u2014he\u2019s talking about maximizing the total value of the universe. And his units are not dollars: In a kind of GDP for the universe, his units are the units of a utilitarian. He\u2019s maximizing utils, units of happiness. And not just for every living soul, but also every soul\u2014human and animal\u2014that will ever live in the future. Maximizing the total happiness of the future\u2014that\u2019s SBF\u2019s ultimate goal. FTX is just a means to that end.&nbsp;</p></blockquote><p>&nbsp;</p><p>On what differentiates FTX in crypto:&nbsp;</p><blockquote><p>The FTX competitive advantage? Ethical behavior. SBF is a Peter Singer\u2013inspired utilitarian in a sea of Robert Nozick\u2013inspired libertarians. He\u2019s an ethical maximalist in an industry that\u2019s overwhelmingly populated with ethical minimalists. I\u2019m a Nozick man myself, but I know who I\u2019d rather trust my money with: SBF, hands-down. And if he does end up saving the world as a side effect of being my banker, all the better.</p></blockquote><p>&nbsp;</p><p>On the EA community in the Bahamas that congealed around FTX:&nbsp;</p><blockquote><p>A cocktail party is in full swing, with about a dozen people I don\u2019t recognize standing around. It turns out to be a mixer for the local EA community that\u2019s been drawn to Nassau in the hopes that the FTX Foundation will fund its various altruistic ideas. The point of the party is to provide a friendly forum for the EAs who actually run EA-aligned nonprofits to meet the earn-to-give EAs at FTX who will fund them, and vice versa. The irony is that, while FTX hosts the weekly mixer\u2014providing the venue and the beverages\u2014it\u2019s rare for an actual FTX employee to ever show up and mix. Presumably, they\u2019re working too hard.</p><p>...</p><p>\u201cImagine nerds invented a religion or something,\u201d says Woods, stabbing at my question with vigor, \u201cwhere people get to argue all day.\u201d</p><p>\u201cIt\u2019s\u2026 an ideology,\u201d counters Morrison. The argument has begun.&nbsp;</p><p>Woods amiably disagrees: \u201cEA is <i>not</i> an ideology, it\u2019s a question: \u2018How do I do the most good?\u2019 And the cool thing about EA, compared to other cause areas, is that you can change your views constantly\u2014and still be part of the movement.\u201d</p><p>...</p><p>Woods serves up an answer to my question. (Fittingly, she\u2019s wearing tennis whites.) \u201cEA attracts people who really care, but who are also really smart,\u201d she says. \u201cIf you are altruistic but not very smart, you just bounce off. And if you\u2019re smart but not very altruistic,\u201d she continues, \u201cyou can get nerd sniped!\u201d</p><p>...</p><p>\u201cThis ties into the way FTX is doing its foundation,\u201d Morrison says, helpfully knocking the ball back to my true interest. \u201cThe foundation wants to get a lot of money out there in order to try a lot of things quickly. And how can you do that effectively?\u201d It\u2019s a rhetorical question, a move worthy of a preppy debate champ who went to a certain finishing school in Cambridge\u2014which is exactly what Morrison is. \u201cPart of the answer is to give money to someone in the EA community.\u201d</p><p>\u201cBecause EA is different from other communities,\u201d Woods continues, picking up right where Morrison left off. \u201cThey\u2019re like, \u2018This is the ethical thing, and this is the truth.\u2019 And we\u2019re like, \u2018What is the ethical thing? What is the truth?\u2019\u201d&nbsp;</p></blockquote><hr><p>Following your analogy, if a fan of Novik had:&nbsp;</p><ul><li>been convinced by Novik to dedicate their career to the Novikian ethic&nbsp;</li><li>been pointed by Novik to a promising first job in that career path&nbsp;</li><li>decided to leave that promising first job on the basis of Novikian reasoning, framing the question of what to do next in Novikian terms&nbsp;</li><li>worked with a global network of Novikians to implement an international crypto arbitrage&nbsp;</li><li>received seed funding from a prominent Novikian to scale up this arbitrage&nbsp;</li><li>exclusively hired Novikians to continue scaling the arbitrage once it started working&nbsp;</li><li>thought about forward-facing professional decisions strictly in terms of the Novikian ethic&nbsp;</li><li>used their commitment to Novikianism to garner a professional edge in their industry&nbsp;</li><li>used a large portion of the proceeds of their business to fund Novikian projects, overseen by a foundation staffed exclusively by elite Novikians and advised by Novik herself</li><li>fostered a community of Novikians around their lavish corporate headquarters&nbsp;</li></ul><p>&nbsp;</p><p>... then I think it would be fair to attribute some of the impact of their actions to Novikianism.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Milan_Griffes"}}, {"_id": "K8jLhA6JGAWckR47K", "postedAt": "2022-11-16T18:49:28.966Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Definitely: you are obviously right and Eliezer obviously wrong about this, imho.&nbsp;<br><br><br>BUT<br><br>I do think it is hindsight bias to some degree to think that \"EA\" as a collective or Will MacAskill as an individual are recorded as doing something wrong, in the sense of \"predictably a bad idea\" at any point in the passages you quote. (I know you didn't actually claim that!) It's not immoral to tell some to found a business, so it's definitely not immoral to tell someone to found a business and give to charity. It's not immoral to help someone make a legal, non-scammy trade, as the anonymous Japanese EA apparently did (\"buy low and sell high\" is not poor business ethics as far as I know, though I'm prepared to be corrected about that by someone who actually knows finance.) It's a bit more controversial to say it's not wrong to take very rich people's money to do the sort of work EA charities do, but it's certainly not <i>obvious</i> that it is, and nothing in the quoted passages actually shows that any individual had evidence that FTX were a bad org to be associated with. (They may well have, I'm not saying no one did wrong, I'm just saying no wrong-doing is suggested by the information quoted here.) Furthermore \"take money from rich people for philanthropy and speculative academic research\" isn't exactly a uniquely EA practice!&nbsp;<br><br>That leaves suggesting FTX think in utilitarian terms about maximizing, but I think it is obviously a complicated question whether that was a knowably bad idea when it was done, and depends on the details of how it was done.&nbsp;<br><br>Of course, there may well have been wrong-doing at some point, but we need proper investigation before we decided. And furthermore, we can't just assume that any wrongdoing, <i>even severe wrongdoing</i>, that did occur would have saved the depositors SBF stole from, who are the main victims of this whole mess. My guess is that once the early decision to encourage SBF to found Alameda was made by Will, and SBF received some early help from the community, withdrawing our support later would not have done very much to prevent FTX from becoming a successful business that stole from its customers. But those early decisions are probably the least morally suspicious, in that they were taken early when there was the least available information about the business ethics of SBF and FTX/Alameda available. To repeat: I don't think telling someone to found a business to earn to give, or helping out a business make a legal, non-scammy trade, is itself immoral. (Again, I'm assuming the trade was legal and non-scammy, but very willing to be corrected!). The suspicious decisions that <i>might</i> have been decisive was maybe \"get SBF and other FTX/Alameda high-ups to think in a utilitarian way'. But as I say, I don't think its reasonable to hold that was clearly wrong at the time.&nbsp;</p>", "parentCommentId": "k7feDPvTKj8Qaq5aq", "user": {"username": "Dr. David Mathers"}}, {"_id": "ENC5NPj8bg8A9GY5j", "postedAt": "2022-11-16T19:17:48.713Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Thanks for this comment.&nbsp;</p><p>I'm more interested in reflecting on the foundational issues in EA-style thinking that contributed to the FTX debacle than in abscribing wrongdoing or immorality (though I agree that the whole episode should be thoroughly investigated).&nbsp;</p><p>Examples of foundational issues:&nbsp;</p><ul><li>FTX was an explicitly maximalist project, and <a href=\"https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous\">maximization is perilous</a>&nbsp;</li><li>Following a utilitarian logic, FTX/Alameda pursued a high-leverage strategy (<a href=\"https://twitter.com/Othmane_SAFSAFI/status/1591803227322093571\">Caroline on leverage</a>); &nbsp;the decision to pursue this strategy didn't account for the massive externalities that resulted from its failure&nbsp;</li><li>The Future Fund failed to identify <a href=\"https://marginalrevolution.com/marginalrevolution/2022/11/a-simple-point-about-existential-risk.html\">an existential risk to its own operation</a>, which casts doubt on their/our ability to perform risk assessment&nbsp;</li><li>EA's inability and/or unwillingness to vet FTX's operations (lack of financial controls, lack of board oversight, no ring-fence around funds committed to the Future Fund) and SBF's history of questionable leadership points to overeager power-seeking &nbsp;</li><li><a href=\"https://twitter.com/tier10k/status/1575603591431102464\">MacAskill's attempt to broker</a> an SBF &lt;&gt; Elon deal re: purchasing Twitter also points to overeager power-seeking&nbsp;</li><li>Consequentialism straightforwardly implies that the ends justify the means at least sometimes; <a href=\"https://forum.effectivealtruism.org/posts/WdeiPrwgqW2wHAxgT/a-personal-statement-on-ftx\">protesting that the ends don't justify the means</a> is cognitive dissonance&nbsp;</li><li>EA leadership's <a href=\"https://forum.effectivealtruism.org/posts/Et7oPMu6czhEd8ExW/why-you-re-not-hearing-as-much-from-ea-orgs-as-you-d-like\">stance of minimal communication</a> about their roles in the debacle points to a high weight placed on optics / face-saving (<a href=\"https://forum.effectivealtruism.org/posts/mCCutDxCavtnhxhBR/some-comments-on-recent-ftx-related-events\">Holden's post</a> and <a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId=NbevNWixq3bJMEW7b\">Oli's commenting</a> are refreshing counterexamples though I think it's important to hear more about their involvement at some point too)&nbsp;</li></ul>", "parentCommentId": "K8jLhA6JGAWckR47K", "user": {"username": "Milan_Griffes"}}, {"_id": "5YXZc7pQHkCMGyBgS", "postedAt": "2022-11-16T19:18:25.884Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>DM conversation I had with Eliezer in response to this post. Since it was a private convo and I was writing quickly I had somewhat exaggerated in a few places that I've now indicated with edits.</p><blockquote><p><a href=\"https://forum.effectivealtruism.org/users/habryka\"><strong>Habryka</strong></a></p><p>Hmm, I do feel like I maybe want to have some kind of public debate about whether indeed we could have noticed that a bunch of stuff about FTX was noticeable, and whether we have some substantial blame to carry.&nbsp;</p><p>Like, to be clear, I think the vast majority of EAs had little they could have or should have done here. But I think that I, and a bunch of people in the EA leadership, had the ability to actually do something about this.&nbsp;</p><p>I sent emails in which I warned people of SBF. I had had messages written but that I never sent that seem to me like if I had sent them they would have actually caused people to realize a bunch of inconsistencies in Sam's story. I had sat down my whole team, swore them to secrecy, and told them various pretty clearly illegal things that I heard Sam had done [sadly all uncomfirmed, asking for confidentiality and only in rumors] that convinced me that we should avoid doing business with him as much as possible (this was when we were considering whether to do a bunch of community building in the Bahamas). Like, in my worldview, I did do my due diligence, and FTX completely failed my due diligence, and I just failed to somehow propagate that knowledge.&nbsp;</p><p>Also, ultimately Sam's social group was my social group. The author of the Scholomance books did not also happen to hang out with Sam at multiple 5-day retreats in the last year. They are not close friends with metamours of Sam and Caroline. They do not share 75% of their friends, do not hang out in the same office and do not read the same forums, all in addition to not subscribing explicitly to the same ethical philosophy that's centered around a few thousand pages of writing. <i>[To clarify some exaggerations, I think I share more like 30% of friends with FTX leadership, and Caroline visited Constellation, a Berkeley coworking space that I sometimes work out of, a number of times in the last year, though neither she nor Sam usually work there]</i></p><p>Like, I am open to there being nothing I could have done, but the conclusion doesn't seem obvious to me at the moment.&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/users/eliezeryudkowsky\"><strong>EliezerYudkowsky</strong></a></p><p>I asked Sam a couple of times if we could schedule a long conversation, and he never got back to me go figure. &nbsp;I passed through the Bahamas and ended up meeting with the Future Fund people but not, iirc, the FTX people. &nbsp;I don't have the sense that SBF was one of My People. &nbsp;If Caroline was one of My People and not just somebody who read some EY fiction but ultimately a Singer/Givewell type, I haven't yet heard the account of it.<br><br>The question isn't whether there's anything we could've done, but anything we could've done in a more meaningful sense than I \"could've\" recced Bitcoin to HPMOR readers in 2010.</p><p><a href=\"https://forum.effectivealtruism.org/users/habryka\"><strong>Habryka</strong></a></p><p>I mean, I am a lot more embedded in the EA space beyond MIRI than you are, and I mean, I think there is an important sense in which I don't think you had the relevant info, but I do think a bunch of other people had.&nbsp;</p><p>I do also think it is pretty unlikely we could have prevented FTX exploding, though I do think we could have likely prevented FTX being super embedded in the EA Community, having a ton of people rely on its funding, and having Sam be held up in tons of places as a paragon of the EA community.&nbsp;</p><p>Like, I think we fucked up pretty hard by saying for a few years that we think Sam did great by our values, when I think it was already pretty clear by that point that he quite likely wasn't.</p><p><a href=\"https://forum.effectivealtruism.org/users/eliezeryudkowsky\"><strong>EliezerYudkowsky</strong></a></p><p>I wouldn't mind if you wanted to post that, or if you wanted to post this whole conversation. &nbsp;My experience was much more of FTX being some weird distant people who were doing a kind of longtermism that never intersected with much I considered useful until their regranting program started.</p><p><a href=\"https://forum.effectivealtruism.org/users/habryka\"><strong>Habryka</strong></a></p><p>Cool, I might post this chat.</p></blockquote>", "parentCommentId": null, "user": {"username": "Habryka"}}, {"_id": "xxx47HqfnwjeYAcHe", "postedAt": "2022-11-16T19:36:10.777Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>In case anyone wants a reference for the $210 million that FTX committed to spend on esports naming rights for TSM, a &nbsp;Washington Post article from today is <a href=\"https://www.washingtonpost.com/video-games/esports/2022/11/16/tsm-ftx-naming-deal-suspended/\">here</a></p>", "parentCommentId": null, "user": {"username": "geoffreymiller"}}, {"_id": "aJjkNTTu9op5j4N6h", "postedAt": "2022-11-16T19:43:01.906Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Sounds right to me!</p><p>I agree with Eliezer that a lot of EAs are over-blaming EA for the FTX implosion, based on the facts currently known. But the Scholomance case is obviously a lot weaker than the EA case in real life, and this is a great summary of why.</p>", "parentCommentId": "k7feDPvTKj8Qaq5aq", "user": {"username": "RobBensinger"}}, {"_id": "MA8q8bGx34FreNj7f", "postedAt": "2022-11-16T19:45:16.082Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Agree, we shouldn't give a pass to irrational (frankly, egocentric) thinking just because it feels like taking responsibility.&nbsp;<br><br>I feel especially irritated with people who are ready to change their entire utilitarian philosophy just because someone associated with ours (probably) committed a major crime and got caught, as if they didn't understand last week that they lived in a world where surprises like that can happen. I don't understand how else they could update their moral philosophy so fast based on the info we have.</p>", "parentCommentId": null, "user": {"username": "Holly_Elmore"}}, {"_id": "GZFAgzfKHJkmeNxCA", "postedAt": "2022-11-16T20:07:56.766Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>EA is a brand, and people on the outside don't have much information about it, so a negative association matters on the margin for recruiting. The main post makes a fair point about not going overboard with self blame, but it seems good for EA folks to be publicly concerned about how they could have acted better, or to publicly discuss the lessons they're taking. At the very least, I don't think it's worth much effort to stop people from doing so.</p>", "parentCommentId": "7fAXxHhbinC55hHaJ", "user": {"username": "Justin Helps"}}, {"_id": "gCBdGNuxifaENWdgX", "postedAt": "2022-11-16T20:09:31.855Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/SP3Hkas3jo6i2cmqb/who-s-at-fault-for-ftx-s-wrongdoing?commentId=ENC5NPj8bg8A9GY5j\">Here</a> are some jumping-off points for reflecting on how one might update their moral philosophy given what we know so far.&nbsp;</p>", "parentCommentId": "MA8q8bGx34FreNj7f", "user": {"username": "Milan_Griffes"}}, {"_id": "9rCRrBBsh8ubFpSfc", "postedAt": "2022-11-16T21:01:15.418Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>When comparing the size of SBF/FTX outlay on EA vs. stuff like naming rights, I think it is important to compare apples to apples. As far as the victim's perspective, the key question is \"how much money went out the door\" as opposed to \"how much did SBF/FTX plan or commit to spend in the future?\" Although I don't know how the naming rights deals were set up, I suspect that much of the money was to be paid in the future. That means the stadiums, teams, etc. are now general unsecured creditors on any claims. I am hearing that depositor claims may be valued on the distressed-debt market at 3-5 cents on the dollar, so the claims of naming-rights counterparties are likely worth even less.</p>", "parentCommentId": null, "user": {"username": "Jason"}}, {"_id": "DTZWcibd6LpzQfWMM", "postedAt": "2022-11-16T22:24:24.134Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<blockquote><p>There are however a number of things we ARE at fault for here.</p></blockquote><p>Yes, assuming that these were foreseeably bad calls. Seems good to separately ask \"what responsibility do EAs bear for Sam's bad decisions ?\" and \"what did we otherwise do wrong, or right?\". E.g., if it were true that Sam would have made all the same missteps in the absence of EA, it could still be the case that we made Sam-related mistakes like \"failing to propagate info about Sam's past bad behavior\".</p><blockquote><p>2. Will MacAskell made the introduction to Elon to try and get SBF to help buy twitter. We still have no public information why, but this would have given SBF more power and used a lot of money that could have been used on doing good to that end.&nbsp;</p></blockquote><p>It would have given SBF a different kind of power. I'm skeptical of the claim that SBF would be more powerful if he'd poured his money into Twitter, since that implies that Twitter is a more useful, leveraged thing to spend money on than SBF's other alternatives.</p><p>It seems more likely to me that <i>either</i> buying Twitter would reduce SBF's power/influence (because Twitter isn't very important), <i>or</i> that buying Twitter is a not-crazy sort of thing for EAs to try to do (because Twitter <i>is</i> very important).</p><p>Of course, SBF owning Twitter could have been bad insofar as SBF's judgment and character were flawed. But then we're just repeating the critique \"EAs should have known that SBF was a bad guy\", not separately critiquing Will for thinking the Twitter buy was a good idea.</p><p>I think more of an argument needs to be given for \"buying Twitter was a dumb idea\" in order to include this on a list of \"things EAs are at fault for\".</p><blockquote><p>3. Carrick Flynn campaign; we as a community hugely supported this campaign which was quite blatantly SBF and GBF trying to buy a seat for their interests. Sure, we as a community thought this was also our interests (and I still assume Carrick would have done a good job?) but once again this was a way the community encouraged and didn't question SBFs power</p></blockquote><p>This seems totally wrong to me. First, because I knew Carrick pre-campaign, I think he's awesome and would make an <i>amazing</i> elected official, and it doesn't update me at all to know that Carrick (like a ton of excellent, well-intentioned EAs) got FTX funding.</p><p>And second, because AFAIK Carrick is an FHI guy who SBF later decided to support in his primary race (because he's an EA and SBF wanted more EAs in politics), not someone with close ties to SBF. Quoting Carrick in a <a href=\"https://www.vox.com/23066877/carrick-flynn-effective-altruism-sam-bankman-fried-congress-house-election-2022\"><i>Vox</i> interview</a>:</p><blockquote><p>First, I\u2019ve never met [Sam Bankman-Fried], I\u2019ve never talked to him. I don\u2019t have any information that anyone else doesn\u2019t have. I actually don\u2019t have any information that\u2019s not public with, I guess, one exception, which is information I think other people <i>think</i> they have, which is they think I\u2019m involved in crypto or something. That is not the case. I\u2019m not a crypto person. I don\u2019t know very much about it. I\u2019ve never looked at regulations for it. I don\u2019t think it\u2019s a priority.</p><p>Left with that information, my take is speculative, but what I will say is it seems to me like Sam Bankman-Fried is someone who legitimately wants to prevent pandemics from happening again. I am on board. I love that, great goal. Let\u2019s do it. I see why he would want to support me for that, since I\u2019ve made this my first priority and I\u2019ve got a history in this. He\u2019s also supported other candidates and sitting congresspersons who have good pandemic prevention policies, with less money, but I can see why he\u2019d want to give more to the person with more background in it.</p></blockquote>", "parentCommentId": "aN2rDiHwzebksidx6", "user": {"username": "RobBensinger"}}, {"_id": "KLY5KB6KtitH8uiFE", "postedAt": "2022-11-16T22:43:29.054Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<blockquote><p>I feel especially irritated with people who are ready to change their entire utilitarian philosophy just because someone associated with ours (probably) committed a major crime and got caught</p></blockquote><p>I agree that this is pretty weird. There were presumably a bunch of historical contingencies that went into whether the FTX implosion occurred; it seems weird if we should endorse some moral philosophy X in the world where all those contingencies occurred, and some different moral philosophy Y in the world where not all of those contingencies occurred.</p><p>And it also seems weird if we should endorse the same moral philosophy in both worlds, but this one data point -- an important data point EV-wise, but still <i>a single event</i>, historical contingency and all -- is crucial evidence about such a high-level proposition. Evidence that we somehow didn't acquire via looking at <i>the entirety of human history</i>, the entire psychology and sociology literature, etc.</p><p>The least-weird versions of this update I can imagine are:</p><ul><li>\"This <i>isn't</i> a large update about high-level questions like that, but it's at least an interesting case study. We shouldn't treat it as a huge deal evidentially, but having a Schelling case study we can all drill down on is still a useful exercise, since we usually don't take the time to be this thorough.\"</li><li>\"This <i>is</i> a large update for me, exactly because my perspective on the world is heavily influenced by things like the status hierarchies I perceive, which things are seen as socially acceptable or unacceptable, which people I personally like or dislike, etc. Events that cause a realignment in the status hierarchy are a bit like taking antidepressants, and observing that some of my world-models change when I'm on the antidepressants.<br><br>There's no <i>a priori</i> guarantee that my epistemics are more accurate on antidepressants versus off them; but having the extra vantage point can help me reflect on these two perspectives, and it's not weird if I end up deciding that one vantage point is better than the other, and thereby updating my object-level world-models to better match that vantage point.\"</li></ul>", "parentCommentId": "MA8q8bGx34FreNj7f", "user": {"username": "RobBensinger"}}, {"_id": "ZAoS8cnqeP3pZb6ga", "postedAt": "2022-11-16T22:47:25.785Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Seems like a reasonable objection to me. (Though it's still weird that SBF overpaid so much for that particular form of advertising; and it's weird that SBF didn't set aside money for FTX FF.)</p>", "parentCommentId": "ndrfmR8cK7jrK92wg", "user": {"username": "RobBensinger"}}, {"_id": "8pnw3E8tbuYx24guW", "postedAt": "2022-11-16T23:56:56.168Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Fair point.</p>", "parentCommentId": "9rCRrBBsh8ubFpSfc", "user": {"username": "EliezerYudkowsky"}}, {"_id": "QcFMhAs4qLspdsdcu", "postedAt": "2022-11-16T23:57:58.725Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>The point is not \"EA did as little to shape Alameda as Novik did to shape Alameda\" but \"here is an example of the mental motion of trying to grab too much responsibility for yourself\".</p>", "parentCommentId": "aJjkNTTu9op5j4N6h", "user": {"username": "EliezerYudkowsky"}}, {"_id": "Esev6kY6msnkyHejA", "postedAt": "2022-11-17T00:04:10.403Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>There\u2019s also a 3rd option - we should have been updating based on what was already talked about re SBF before the implosion (his pathological behaviour, his public statements essentially agreeing he\u2019s running a Ponzi scheme, and people warning other people about these). So the implosion makes us realise that, in a world where FTX didn\u2019t implode we still should have disassociated from SBF very early on, and be doing some soul searching about why UK EA leaders were [/are, in this hypothetical world] choosing to hype up someone with a track record of being so terrible</p>\n", "parentCommentId": "KLY5KB6KtitH8uiFE", "user": {"username": "bec_hawk"}}, {"_id": "9yLQxW8DbSgvz8cnq", "postedAt": "2022-11-17T00:32:11.306Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Fair!</p>", "parentCommentId": "QcFMhAs4qLspdsdcu", "user": {"username": "RobBensinger"}}, {"_id": "bTTez2euSD7dv7uGx", "postedAt": "2022-11-17T00:36:19.236Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Someone could make an awesome elected official (as I am sure he would have done) and still be a seat essentially bought for SBFs interests as well... like that's exactly how lobbying works!\nAlso it's clearly untrue that Carrick did not have close ties to SBF. AFAIK (and I may be wrong) he was pretty good mates with Gabe Bankman Fried (I may be wrong though)</p>\n", "parentCommentId": "DTZWcibd6LpzQfWMM", "user": {"username": "Gideon Futerman"}}, {"_id": "npvsMA42qYF2kQnhm", "postedAt": "2022-11-17T01:55:30.167Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Some corrections of the Sequoia info:</p>\n<ul>\n<li>I've never been a grad student.</li>\n<li>I'm neither Japanese nor a Japanese citizen.</li>\n<li>I \u2018volunteered\u2019 in the sense that people at Alameda reached out to me, I said ok and then got paid by the hour for my help.</li>\n<li>\u2018(obscure, rural)\u2019 is an exaggeration. \u2018provincial\u2019 would be a more apt adjective for the location. The main bank we used was SMBC, the second-largest bank in Japan.</li>\n<li>\u2018for a fee\u2019 sounds as if it was some sort of bribe to get them to do what we wanted. But we only paid the usual transaction fees and margin that any bank would charge.</li>\n</ul>\n<p>But mostly, if <a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId=hpP8EjEt9zTmWKFRy\">https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId=hpP8EjEt9zTmWKFRy</a> is accurate, I'm bummed that the money I helped earn was squandered right away.</p>\n", "parentCommentId": "k7feDPvTKj8Qaq5aq", "user": {"username": "rmoehn"}}, {"_id": "JfuhQsN7rzpGC46bJ", "postedAt": "2022-11-17T03:21:06.010Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>My guess is that this is the <a href=\"https://web.archive.org/web/1/https://ftxfuturefund.org/our-grants/\">June</a> figure for the FTX Future Fund grant commitments. The <a href=\"https://ftxfuturefund.org/our-grants/\">current</a> figure is $160M as of September 1st. Some of these grants were in installments, especially the multi-year ones, and not all of the money was transferred. This Fund was \"longtermist\" and I do not see a dollar figure on other FTX charitable giving. This does not include $500M in equity in Anthropic.</p><p><strong>Added</strong>, weeks later: Or maybe he got it from <a href=\"https://www.nytimes.com/2022/11/13/business/ftx-effective-altruism.html\">NYT</a>:</p><blockquote><p>As recently as last month, the umbrella FTX Foundation said it had given away $140 million, of which $90 million went through the FTX Future Fund dedicated to long-term causes. It is unclear how much of that money made it to the recipients and how much was earmarked for giving in installments over several years.</p></blockquote><p>which seems to be sourced from <a href=\"https://www.nytimes.com/2022/10/08/business/effective-altruism-elon-musk.html\">NYT a month ago</a>:</p><blockquote><p>Mr. Bankman-Fried makes his donations through the FTX Foundation, which has given away $140 million, of which $90 million has gone through the group\u2019s Future Fund toward long-term causes.</p></blockquote><p>I suspect that these numbers are actually delivered, not promises. My guess is that the Future Fund pledged $190 million, 160 directly and 30 through regranting, delivered 100 and failed to deliver <a href=\"https://www.bloomberg.com/news/articles/2022-11-30/sam-bankman-fried-s-red-flags-were-seen-in-all-corners-of-empire\">$90 million</a> (<a href=\"https://archive.ph/Fny2o\">a</a>). (Plus $50 million not through the Future Fund, at least some of which counts as EA.)</p>", "parentCommentId": "3yDXvH43wAL2RFYym", "user": {"username": "Douglas Knight"}}, {"_id": "D5pcRsPSz843kAtgC", "postedAt": "2022-11-17T03:48:57.279Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<blockquote><p>We can infer that probably at least $30 of Scholomance sales are due to Caroline Ellison, and with the resources that Ellison commanded as co-CEO of Alameda</p></blockquote><p>C'mon, if she's a true maximizer using depositors money, I guess she'd just download it from z-library<br>OMG, is this why z-lib was recently seized by FBI?&nbsp;</p>", "parentCommentId": null, "user": {"username": "Ramiro"}}, {"_id": "byZb5JpKSp9kpdBfx", "postedAt": "2022-11-17T04:10:12.921Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<blockquote><p>Can you imagine how you'd judge it if, rather than my writing it as a joke, Naomi Novik had gone online and sincerely tried to accept blame for FTX's fall, because she thought she hadn't been careful enough to put messages about good corporate governance and careful accounting into her fantasy novels, and Novik had talked about how she was planning to donate an appropriate portion of her Scholomance book royalties back to FTX's ruined customers?</p></blockquote><p>Even so, I'm still recommending people to read Terry Pratchett instead of Novik. Something something low probability, large impact.<br>But seriously, I think the problem is less how SBF self-identified with EA, and more the way EAs saw him as The Hero.</p><p>Anyway, maybe EAs do have a problem of egocentrism.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Ramiro"}}, {"_id": "wkFtyi8CCfyntzdAT", "postedAt": "2022-11-17T05:57:15.925Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Great points, all. Even if most people could do nothing and Sam was not motivated by a core problem with EA philosophy, that doesn\u2019t mean there was nothing that EAs close to the situation could have done differently. I would love to see a public airing of what genuine evidence people think they might have had that should have changed those people\u2019s behavior around Sam.</p>\n", "parentCommentId": "5YXZc7pQHkCMGyBgS", "user": {"username": "Holly_Elmore"}}, {"_id": "m4gutcnBZgRjQhuby", "postedAt": "2022-11-17T06:22:06.154Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Commending Habryka for willing to share about these things. It takes courage and I think reflections/discussions like this could be really valuable (perhaps essential) to the EA community having the kind of reckoning about FTX that we need.</p>", "parentCommentId": "5YXZc7pQHkCMGyBgS", "user": {"username": "Evan R. Murphy"}}, {"_id": "GTbEfPJEjrMiotB86", "postedAt": "2022-11-17T07:43:17.844Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Thanks, there is also $32M from the <a href=\"https://web.archive.org/web/20220630142046/https://ftxfuturefund.org/our-regrants/#grants\">regrants tab</a>. But yes, difficult to know the actual total of payouts without word from the staff. Or payouts not subject to clawback without further details on legal proceedings.</p>", "parentCommentId": "JfuhQsN7rzpGC46bJ", "user": {"username": "Greg_Colbourn"}}, {"_id": "7uzsB2F9c3WHSYjtN", "postedAt": "2022-11-17T07:44:18.951Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Kind of ironic that they were \"longtermist\" about the world, but <a href=\"https://forum.effectivealtruism.org/posts/YAsJCsvi4tBpCwuFB/why-didn-t-the-ftx-foundation-secure-its-bag\">not about their own existence</a>!</p>", "parentCommentId": "JfuhQsN7rzpGC46bJ", "user": {"username": "Greg_Colbourn"}}, {"_id": "kdGe3fwMBADco9MBM", "postedAt": "2022-11-17T15:27:12.779Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I think it\u2019s very worth reflecting on strategic decisions that were made around Sam. I just don\u2019t think what happened is very significant to whether utilitarianism is the correct moral philosophy.</p>\n", "parentCommentId": "Esev6kY6msnkyHejA", "user": {"username": "Holly_Elmore"}}, {"_id": "GrandhyQ9EiHbtXHv", "postedAt": "2022-11-17T15:55:45.639Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I agree that these events are separate from arguments for &amp; against utilitarianism as a <a href=\"https://www.utilitarianism.net/types-of-utilitarianism#:~:text=In%20the%20literature%20on%20utilitarianism,thinking%20about%20what%20to%20do.\"><strong>criterion of rightness</strong></a><i>. </i>But they do undermine the viability of the act utilitarian calculus as a<i> </i><strong>decision procedure</strong>. Sam seems to have thought of himself as an act utilitarian, but by neglecting to do the utilitarian calculus correctly or at all, he did massive harm, making it clear that we can't rely on this decision procedure to avoid such harms. Instead, we need utilitarians to &nbsp;adopt a decision procedure that <a href=\"http://www.amirrorclear.net/academic/papers/beyond-action.pdf\">includes constraints</a> on certain behaviour.</p>", "parentCommentId": "kdGe3fwMBADco9MBM", "user": {"username": "RyanCarey"}}, {"_id": "KjkyiRPXxBG8puXro", "postedAt": "2022-11-17T16:13:58.888Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>How much of an update is this really, though? Am I wrong that it's already the majority utilitarian view that act utilitarianism may be theoretically correct, but individual humans don't have the foresight to know the full consequences of every act and humans trying to work together need to be able to predict what others will do --&gt; something like rule utilitarianism or observing constraints? Seems like the update should be about much you can know how things will turn out and whether you can get away with cutting corners.<br><br>It does seem like Sam had pathological beliefs re:St. Petersburg paradox but that seems like more than wanting to maximize EV too much-- it's not caring about the longterm future (where everyone's inevitably dead after enough coin flips) enough. I really don't see how that can be attributed to act utilitarianism either.</p>", "parentCommentId": "GrandhyQ9EiHbtXHv", "user": {"username": "Holly_Elmore"}}, {"_id": "bxgLPP5FNnsdvEZ9G", "postedAt": "2022-11-17T16:26:56.765Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I agree that most utilitarians already thought act utilitarianism as a decision procedure was bad. Still, it's important that more folks can see this, with higher confidence, so that this can be prevented from happening again.</p><p>I think I agree that the St Petersburg paradox issue is orthogonal to choice of decision procedure (unless placing the bet requires engaging in a norm-violating activity like fraud).</p>", "parentCommentId": "KjkyiRPXxBG8puXro", "user": {"username": "RyanCarey"}}, {"_id": "vrzLBsyNke5DAwrmE", "postedAt": "2022-11-17T16:30:30.150Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<blockquote><p>Sam seems to have thought of himself as an act utilitarian, but by neglecting to do the utilitarian calculus correctly or at all, he did massive harm, making it clear that we can't rely on this decision procedure to avoid such harms.</p></blockquote><p><br>I completely agree that a motivated person could easily believe that any decision is the right act utilitarian decision because there aren't clear rules for determining the right act utilitarian decision and checking your answer. Totally. &nbsp;<br><br>But idk if it's even fair to say Sam was using act utilitarianism as a decision procedure. It's not clear to me if he even believed that while he was (allegedly) committing the fraud.</p>", "parentCommentId": "GrandhyQ9EiHbtXHv", "user": {"username": "Holly_Elmore"}}, {"_id": "BL768XDeRgXtntu6G", "postedAt": "2022-11-17T16:33:51.517Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Risking the entire earth seems like a norm violation to me</p>", "parentCommentId": "bxgLPP5FNnsdvEZ9G", "user": {"username": "Holly_Elmore"}}, {"_id": "ccLdrhmCZQwmrLjyj", "postedAt": "2022-11-17T16:35:59.785Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I totally agree. But even if we conservatively say that it's a 50% chance that he was using act utilitarianism as his decision procedure, that's enough to consider it compromised, because it <i>could</i> lead to <s>bad consequences</s> <i>multiple billions of dollars of damages</i> (edited).</p><p>There are also subtler issues: if you <i>intend</i> to be act utilitarian but aren't and do harm, that's still an argument against <i>intending</i> to use the decision procedure. And if someone <i>says</i> they're act utilitarian but aren't and does harm, that's an argument against trusting people who <i>say</i> they're act utilitarian.</p>", "parentCommentId": "vrzLBsyNke5DAwrmE", "user": {"username": "RyanCarey"}}, {"_id": "84Yu6dzirRvsyCXfo", "postedAt": "2022-11-17T16:43:37.215Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I guess it's <i>some </i>new evidence that one person was maybe using act utilitarianism as a decision procedure and messed up? Also not theoretically impossible he was correct in his assessment of the possible outcomes, chose the higher EV option, and we just ended up in one of the bad outcome worlds.</p>", "parentCommentId": "ccLdrhmCZQwmrLjyj", "user": {"username": "Holly_Elmore"}}, {"_id": "s6Czb5ph7bPBXSifz", "postedAt": "2022-11-17T16:48:32.694Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<ul><li>In practice I think utilitarians should adopt mostly a <a href=\"https://forum.effectivealtruism.org/posts/aDNPgm2v2boBbj8wK/deontology-and-virtue-ethics-as-effective-theories-of\">skillful combination of virtue ethics, deontic rules, and explicit calculations.</a>&nbsp;</li><li>I think what does the FTX case provides some evidence for, is some fraction of smart EAs exposed to utilitarianism being prone to attempt to rely on the explicit act utilitarianism, despite the warnings.<br><br>I think part of the story here is the a weird status dynamic where...<br>1. I would basically trust some &nbsp;people to try the explicit direct utilitarian thing: eg I think it is fine for Derek Parfit or Toby Ord.&nbsp;<br>2. This creates some weird correlation where the better you are on some combination of (smartness/understanding of ethics/power in modelling the world), the more you can try to be actually guided by consequences<br>3. This can make being 'hardcore' consequentialist ...sort of &nbsp;cool and \"what the top people do\"<br>4. ... which is a setup where people can start goodhart/signal on it<br><br>&nbsp;</li></ul>", "parentCommentId": "GrandhyQ9EiHbtXHv", "user": {"username": "Jan_Kulveit"}}, {"_id": "onyCjh22P6DGLKae4", "postedAt": "2022-11-17T16:49:24.775Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Not trying to take this out on you, but I'm annoyed by how much all this advocacy of deontology all of sudden overlaps with covering our own asses. I don't buy it as a massive update about morality or psychology from the events themselves but a massive update about optics.&nbsp;</p>", "parentCommentId": "ccLdrhmCZQwmrLjyj", "user": {"username": "Holly_Elmore"}}, {"_id": "YTK7pJNAvbBQDS8br", "postedAt": "2022-11-17T23:33:51.438Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Maybe they weren't familiar with the overwhelming volume of previous historical incidents, hadn't had their brains process history or the news as real events rather than mythology, or were genuinely unsure about how often these sorts of things happened in real life rather than becoming available on the news. &nbsp;I'm guessing #2.</p>", "parentCommentId": "MA8q8bGx34FreNj7f", "user": {"username": "EliezerYudkowsky"}}, {"_id": "n3HtBmaxsoorghabQ", "postedAt": "2022-11-17T23:37:02.043Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>This strikes me as a bad play of \"if there was even a chance\". &nbsp; Is there any cognitive procedure on Earth that passes the standard of \"Nobody ever <i>might</i> have been using this cognitive procedure at the time they made $mistake?\" &nbsp;That more than three human beings have ever used? &nbsp;I think when we're casting this kind of shade we ought to be pretty darned sure, preferably in the form of prior documentation that we think was honest, about what thought process was going on at the time.</p>", "parentCommentId": "ccLdrhmCZQwmrLjyj", "user": {"username": "EliezerYudkowsky"}}, {"_id": "PiXmkwofHtdaDRXdQ", "postedAt": "2022-11-17T23:40:38.078Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Yeah, I think it's a severe problem that if you are good at decision theory you <i>can in fact</i> validly grab big old chunks of deontology directly out of consequentialism including lots of the cautionary parts, or to put it perhaps a bit more sharply, a coherent superintelligence with a nice utility function does not in fact need deontology; and if you tell that to a certain kind of person they <i>will in fact</i> decide that they'd be cooler if they were superintelligences so they must be really skillful at deriving deontology from decision theory and therefore they can discard the deontology and just do what the decision theory does. &nbsp;I'm not sure how to handle this; I think that the concept of \"cognitohazard\" gets vastly overplayed around here, but there's still true facts that cause a certain kind of person to predictably get their brain stuck on them, and this could plausibly be one of them. &nbsp;It's also too important of a fact (eg to alignment) for \"keep it completely secret\" to be a plausible option either.</p>", "parentCommentId": "s6Czb5ph7bPBXSifz", "user": {"username": "EliezerYudkowsky"}}, {"_id": "hPMcvJaniFbLqbwCf", "postedAt": "2022-11-17T23:41:33.604Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I'd agree with this statement more if it acknowledged the extent to which most human minds have the kind of propositional separation between \"morality\" and \"optics\" that obtained financially between FTX and Alameda.</p>", "parentCommentId": "onyCjh22P6DGLKae4", "user": {"username": "EliezerYudkowsky"}}, {"_id": "pcW6higtWD3JGqHg2", "postedAt": "2022-11-17T23:50:54.113Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Why require surety, when we can reason statistically? There've been maybe ten comparably-sized frauds ever, so on expectation, hardline act utilitarians like Sam have been responsible for 5% of the worst frauds, while they represent maybe 1/50M of the world's population (based on what I know of his views 5-10yrs ago). So we get a risk ratio of about a million to 1, more than enough to worry about.</p><p>Anyway, perhaps it's not worth arguing, since it might become clearer over time what his philosophical commitments were.</p>", "parentCommentId": "n3HtBmaxsoorghabQ", "user": {"username": "RyanCarey"}}, {"_id": "2hLa85bQiyAMJBPtq", "postedAt": "2022-11-18T06:42:49.302Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<blockquote><p><i>I think I share more like 30% of friends with FTX leadership</i></p></blockquote><p>Assuming that this means that the FTX leadership is friends with prominent EAs, I think that this fact raises some questions that many people might consider important.</p><p>For instance, I think some people might find it important to know what those friends have been doing with respect to this situation for the past week. What sort of communication have they had with the FTX leadership? Do they still feel loyalty toward SBF/Caroline/etc.? Are they in any way aiding or abetting them to commit crimes or avoid the legal or reputational consequences of their actions?</p><p>These might be dumb questions, and I apologize if so. They occurred to me because I model people as being quite likely to aid and abet with their close friends' criminal or malicious activity, but I acknowledge that that model could be wrong and/or not very applicable to this situation.</p>", "parentCommentId": "5YXZc7pQHkCMGyBgS", "user": {"username": "strawberry"}}, {"_id": "ayWRoBztPfyRtdAeZ", "postedAt": "2022-11-18T07:27:59.456Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<blockquote><p>Sam seems to have thought of himself as an act utilitarian, but by neglecting to do the utilitarian calculus correctly or at all, he did massive harm, making it clear that we can't rely on this decision procedure to avoid such harms.</p></blockquote><p>This seems to me like it's overstating the strength of evidence, as though FTX is a disproof rather than one data point among many.</p><p>It <i>is</i> a disproof for extremely strong claims like \"people who endorse act utilitarianism never do unethical things\", but those claims should have had extremely low probability pre-FTX.</p>", "parentCommentId": "GrandhyQ9EiHbtXHv", "user": {"username": "RobBensinger"}}, {"_id": "MNsSpivmyrQ3HQ9zG", "postedAt": "2022-11-18T07:32:13.410Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<blockquote><p>But even if we conservatively say that it's a 50% chance that he was using act utilitarianism as his decision procedure, that's enough to consider it compromised, because it <i>could</i> lead to bad consequences.</p></blockquote><p>I don't understand this argument at all. I assume nobody thought it was literally impossible for the implementation of a moral theory (any moral theory!) to lead to bad consequences before. Maybe I'd understand your point more if you stated it quantitatively. Like:</p><p>\"Previously, I thought it was x% likely that a random act utilitarian would be led by their philosophy to do worse stuff than if they'd endorsed most other moral theories. After seeing the case of SBF, I now think the probability is y% instead, because our sample size is small enough that a single data point can be a large update.\"</p>", "parentCommentId": "ccLdrhmCZQwmrLjyj", "user": {"username": "RobBensinger"}}, {"_id": "EmSb5GpfLgP4PLRFZ", "postedAt": "2022-11-18T07:38:52.763Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Looks like Eliezer was similarly confused by your phrasing; your new argument (\"almost no multibillion dollar frauds have ever happened, so we should do a very large update about the badness of everything that might have contributed to SBF defrauding people\") sounds very different, and makes more sense to me, though I suspect it won't end up working.</p>", "parentCommentId": "MNsSpivmyrQ3HQ9zG", "user": {"username": "RobBensinger"}}, {"_id": "w6eTLYdjuQ2oXkJcC", "postedAt": "2022-11-18T16:03:10.147Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I think you're right - I could have avoided some confusion if I said it could lead to \"multi-billion-dollar-level bad consequences\". Edited to clarify.</p>", "parentCommentId": "EmSb5GpfLgP4PLRFZ", "user": {"username": "RyanCarey"}}, {"_id": "pZavGfEcdaGmkkrxm", "postedAt": "2022-11-18T16:07:57.016Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Yeah I agree, I just mean that $1bn in funds lost to customers across the world is plausibly comparable in welfare terms to other wins on that list. E.g. dividing by 10 to account for differences in income of those affected, it would be around the amount attributed to GiveDirectly on the EA impact page.</p><p>(without wanting to make a very direct crude comparison, or getting into the details of that)&nbsp;</p>", "parentCommentId": "fRqMmXhfsx96CGwJC", "user": {"username": "callum_calvert"}}, {"_id": "N9PWbCx7GRLmEK3pY", "postedAt": "2022-11-18T17:20:17.702Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Okay yes, they may well be.</p><p>I'm also pretty hesitant to attempt to make direct crude comparisons &nbsp;- and I'll say again that I think there are strong reasons FTX shouldn't have acted as it did <i>in addition to</i> the direct harm to customers - but I'll just say that I seem to remember 100x or 1000x multipliers being more common than 10x in <a href=\"https://www.givingwhatwecan.org/charity-comparisons\">similar scenarios</a>.</p>", "parentCommentId": "pZavGfEcdaGmkkrxm", "user": null}, {"_id": "3CNNyLsQY38DetmNL", "postedAt": "2022-11-18T23:28:37.828Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>Reposting from twitter: It's a moderate update on the prevalence of <i>naive</i> &nbsp;utilitarians among EAs.<br><br>Expanded:<br><br>Classical problem with this debate on utilitarianism is the vocabulary used makes motte-and-bailey &nbsp;defense of utilitarianism too easy.&nbsp;<br>1. Someone points to a bunch problems with a act consequentialist decision procedure&nbsp;/ cases where <i>naive</i> consequentialism tells you to do bad things<br>2. The default response is \"but this is <i>naive&nbsp;</i> consequentialism, no one actually does &nbsp;that\"&nbsp;<br>3. &nbsp;You may wonder that while &nbsp;people don't <i>advocate</i> &nbsp;for or <i>self-identify</i> &nbsp;as <i>naive</i> utilitarians ... they &nbsp; actually make the mistakes<br><br>The case provides some evidence that the problems can actually happen in practice in important enough situations to care. [*]<br><br>Also, you have the problem that sophisticated <i>naive</i> &nbsp;consequentialists could be tempted to lie to you about their morality (\"no worries, you can trust me, I'm following the sensible deontic constraints!\"). &nbsp;Personally, before the recent FTX happenings, I would be more of the opinion \"nah, this sounds too much like an example from a philosophical paper, unlikely with typical human psychology \". &nbsp;Now I take it as more real problem.<br><br>[*] &nbsp;What I'm actually worried about ...&nbsp;<br><br>Effective altruism motivated thousands of people to move into highly leveraged domains, with large and potentially deadly consequences - powerful AI stuff, pandemics, epistemic tech. I think that if just 15% of them believe in some form of <i>hardcore</i> utilitarianism &nbsp;where you drop integrity constrains and trust your human brain ability to evaluate when to be constrained and when not, it's ... actually a problem?&nbsp;<br>&nbsp;</p>", "parentCommentId": "onyCjh22P6DGLKae4", "user": {"username": "Jan_Kulveit"}}, {"_id": "qekfEme8zyou8o46m", "postedAt": "2022-11-18T23:43:30.823Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<blockquote><p>I don't buy it as a massive update about morality or psychology from the events themselves but a massive update about optics.&nbsp;</p></blockquote><p>This will be a relief if true. I am <i>much </i>more worried about people not having principles (or their principles guided by something other than morality) than people being overly concerned about optics. The latter is a tactical concern (albeit a big one) and hopefully fixable, the former is evidence that people in our movement is too conformist or otherwise too weak or too evil to <a href=\"https://forum.effectivealtruism.org/posts/Dtr8aHqCQSDhyueFZ/the-possibility-of-an-ongoing-moral-catastrophe-summary\">confront moral catastrophes</a>.</p>", "parentCommentId": "onyCjh22P6DGLKae4", "user": {"username": "Linch"}}, {"_id": "h6DoGKbitdAySsqGm", "postedAt": "2022-11-20T19:47:30.364Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<blockquote><p>Poor accounting, possibly just <i>no</i> really global accounting or sense of where the money was going;</p></blockquote><p>I chatted with an Alameda python dev for about an hour. I tried to get a sense of their testing culture, QA practices, etc. Lmao: there didn't seem to be any. Soups of scripts, no time for tests, no internal audits. Just my impression.&nbsp;</p><p>My type-driven and property-based testing zealot/pedant side has harvested some bayes points, unfortunately.&nbsp;</p>", "parentCommentId": null, "user": {"username": "quinn"}}, {"_id": "5FyMaMwnwGm5ACrvi", "postedAt": "2022-11-24T20:53:23.340Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I don\u2019t think they know they are concerned about optics. My suspicion was that the bad optics suddenly made utilitarian ideas seem false or reckless.</p>\n", "parentCommentId": "qekfEme8zyou8o46m", "user": {"username": "Holly_Elmore"}}, {"_id": "mfaoW8qqQk7BcdpYk", "postedAt": "2022-12-05T17:04:57.907Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>I also think this was a not-so-good and somewhat misleading analogy - the association between Novik and Caroline in the example is strictly one-way (Caroline likes Novik, Novik has no idea who Caroline is), whereas the association between FTX and EA is clearly two-way (e.g. various EA orgs endorsing and promoting SBF, SBF choosing to earn-to-give after talking with 80k etc).</p>", "parentCommentId": "tdAwsSZpdXigT5PK4", "user": {"username": "demirev"}}, {"_id": "2CnSsnLgDNDaxTDXt", "postedAt": "2022-12-15T17:31:03.983Z", "postId": "SP3Hkas3jo6i2cmqb", "htmlBody": "<p>The question that heads this post obviously answers itself, in that only actual perpetrators &nbsp;of bad deeds and their direct instigators (intellectual or otherwise) are to be held accountable for them; nevertheless, I must admit that I found Eliezer Yudkowsky' analogy unconvincing, and &nbsp;(not quite, but feeling a little bit) disingenuous. Whenever we see examples of adherents of some creed, ideology, religious or thought system going into nefarious places, it is natural to wonder if said ideas (whether properly or mistakenly interpreted) influenced or condoned the path they took. Some articles I have read lately have pointed the finger towards the hubristic hazards of miscalculating for optimal results, and the concomitant dangers of risky betting and of cutting corners. As is well known, the road to hell is paved with good intentions. And besides, as has been stated, a lot of the people involved in this weren't just 'fellow travelers' or ocasional readers of EA material. A lot of them were very visibly engaged in and seen as poster childs for the movement. And I am sure most of them were innocent victims, especially so the rank-and-file workers of FTX and Alameda.</p><p>Having said that, I do not find it &nbsp;reasonable either to go to masochistic extremes of self-flagellation. Humans being what they are, there will always be cases of wolves in sheep's clothing, and never enough controls to catch them in advance. Which is humbling, in a not necessarily bad way. My impression is that the EA community and its members are a wonderful group of people and they &nbsp;will probably come out of this situation wiser, if sadder. And that obviously, it is wrong to blame EA for what has happened.</p><p>As Eliezer Yudkowsky mentions Caroline Ellison's blog, I would like to say that I have been reading it of late, and even taking into account the potential deceitfulness of &nbsp;words and the pictures we build with them, I do not get from both its contents and her general trajectory that she could be a morally bankrupt person. On the contrary, the impression I got is of a true believer, and a good person. This does not preclude the possibility that, under circumstances of a certain naivet\u00e9 and inexperience in a field as murky as crypto, she might have let herself go along with what she might have perceived as temporary &nbsp;and 'bad' expedient means. But to believe this person ever intended to purposely and maliciously scam people our of their money or be privy to a fraud is, for me, completely out of the question. I believe the best option is to be charitable and await to see what the courts of law have to say once the dust has settled. As for SBF, and after reading some of the things he has said and done, that's a completely different story.</p>", "parentCommentId": null, "user": {"username": "Manuel Del R\u00edo Rodr\u00edguez"}}]