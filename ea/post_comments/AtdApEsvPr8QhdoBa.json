[{"_id": "Fe66edYbmDwJNjgt3", "postedAt": "2023-03-24T21:23:20.752Z", "postId": "AtdApEsvPr8QhdoBa", "htmlBody": "<p>This isn't personal, but I downvoted because I think Metaculus forecasts about this aren't more reliable than chance, and people shouldn't defer to them.</p>\n", "parentCommentId": null, "user": {"username": "Guy Raveh"}}, {"_id": "yjvjstGTXLpkPzfuA", "postedAt": "2023-03-24T21:31:43.425Z", "postId": "AtdApEsvPr8QhdoBa", "htmlBody": "<blockquote><p>aren't more reliable than chance</p></blockquote><p>Curious what you mean by this. One version of chance is \"uniform prediction of AGI over future years\" which obviously seems worse than Metaculus, but perhaps you meant a more specific baseline?</p><p>Personally, I think forecasts like these are rough averages of what informed individuals would think about these questions. Yes, you shouldn't defer to them, but it's also useful to recognize how that community's predictions have changed over time.</p>", "parentCommentId": "Fe66edYbmDwJNjgt3", "user": {"username": "Gabe Mukobi"}}, {"_id": "NFq94dZwGkrEaFG3n", "postedAt": "2023-03-24T22:00:32.599Z", "postId": "AtdApEsvPr8QhdoBa", "htmlBody": "<p>Hi Gabriel,</p><p>I am not sure how much to trust Metaculus' in general, but I do not think it is obvious that their AI predictions should be ignored. For what is worth, Epoch attributed a weight of 0.23 to Metaculus in the judgement-based forecasts of <a href=\"https://epochai.org/blog/literature-review-of-transformative-artificial-intelligence-timelines\">their AI Timelines review</a>. Holden, Ajeya and AI Impacts got smaller weights, whereas Samotsvety got a higher one:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NFq94dZwGkrEaFG3n/it6kxvcuspi8mm5vfzms\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NFq94dZwGkrEaFG3n/zqffyj8ulf7vzcwfhcgu 90w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NFq94dZwGkrEaFG3n/mywmxxdofslefka4thf5 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NFq94dZwGkrEaFG3n/zxjfcbierxe4urptq3c6 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NFq94dZwGkrEaFG3n/iff8tvpsjnya62tag9mp 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NFq94dZwGkrEaFG3n/ft6xbeclfzvypsmu94wu 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NFq94dZwGkrEaFG3n/q0rj5qfmtrb51vvcgk3l 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NFq94dZwGkrEaFG3n/haxn5odfyykaqv4htrhx 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NFq94dZwGkrEaFG3n/r5mgptzqz4t8onjirkoz 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NFq94dZwGkrEaFG3n/bggv27ephnueriifiipg 810w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NFq94dZwGkrEaFG3n/rgbycb2tolvx5rgcpee2 854w\"></figure><p>However, one comment I made <a href=\"https://forum.effectivealtruism.org/posts/zeL52MFB2Pkq9Kdme/exploring-metaculus-community-predictions?commentId=ZpixHHF6BKRx37hcG\">here</a> may illustrate what Guy presumably is referring to:</p><blockquote><p>The mean Brier scores of Metaculus' predictions (and Metaculus' community predictions) are (from <a href=\"https://www.metaculus.com/questions/track-record/\">here</a>):</p><ul><li>For all the questions:<ul><li>At resolve time (N = 1,710), 0.087 (0.092).</li><li>For 1 month prior to resolve time (N = 1,463), 0.106 (0.112).</li><li>For 6 months (N = 777), 0.109 (0.127).</li><li>For 1 year (N = 334), 0.111 (0.145).</li><li>For 3 years (N = 57), 0.104 (0.133).</li><li>For 5 years (N = 8), 0.182 (0.278).</li></ul></li><li>For the questions of the category artificial intelligence:<ul><li>At resolve time (N = 46), 0.128 (0.198).</li><li>For 1 month prior to resolve time (N = 40), 0.142 (0.205).</li><li>For 6 months (N = 21), 0.119 (0.240).</li><li>For 1 year (N = 13), 0.107 (0.254).</li><li>For 3 years (N = 1), 0.007 (0.292).</li></ul></li></ul><p>Note:</p><ul><li>For the questions of the category artificial intelligence:<ul><li>Metaculus' community predictions made earlier than 6 months prior to resolve time perform as badly or worse than always predicting 0.5, as their mean Brier score is similar or higher than 0.25. [Maybe this is what Guy is pointing to.]</li><li>Metaculus' predictions perform significantly better than Metaculus' community predictions.</li></ul></li><li>Questions for which the Brier score can be assessed for a longer time prior to resolve, i.e. the ones with longer lifespans, tend to have lower base rates (I found a correlation of <a href=\"https://docs.google.com/spreadsheets/d/1Mxl8vGsZemmuKytV9zH1ft-iP2q4xwnxz7gCkSlYmPg/edit#gid=2113740314&amp;range=G18\">-0.129</a> among all questions). This means it is easier to achieve a lower Brier score:<ul><li>Predicting 0.5 for a question whose base rate is 0.5 will lead to a Brier score of 0.25 (= 0.5*(0.5 - 1)^2 + (0.5 - 0)*(0.5 - 0)^2).</li><li>Predicting 0.1 for a question whose base rate is 0.1 will lead to a Brier score of 0.09 (= 0.1*(0.1 - 1)^2 + (1 - 0.1)*(0.1 - 0)^2).</li></ul></li></ul></blockquote>", "parentCommentId": "yjvjstGTXLpkPzfuA", "user": {"username": "vascoamaralgrilo"}}, {"_id": "LLCJRfakQPjXJ23Y5", "postedAt": "2023-03-25T04:53:23.761Z", "postId": "AtdApEsvPr8QhdoBa", "htmlBody": "<p>Can someone please explain why we're still forecasting the weak AGI timeline? I thought \"sparks\" of AGI as Microsoft claimed GPT-4 achieved should already be more than the level of intelligence implied by \"weak\".</p>\n", "parentCommentId": null, "user": null}, {"_id": "hBCX7wuFkcCiKGTBM", "postedAt": "2023-03-25T06:32:11.162Z", "postId": "AtdApEsvPr8QhdoBa", "htmlBody": "<p>Agree that they shouldn't be ignored. By \"you shouldn't defer to them,\" I just meant that it's useful to also form one's own inside view models alongside prediction markets (perhaps comparing to them afterwards).</p>\n", "parentCommentId": "NFq94dZwGkrEaFG3n", "user": {"username": "Gabe Mukobi"}}, {"_id": "QCqGELJTkSkbayHYr", "postedAt": "2023-03-25T11:52:10.061Z", "postId": "AtdApEsvPr8QhdoBa", "htmlBody": "<p>Remember that AGI is a pretty vague term by itself, and some people are forecasting on the specific definition under the Metaculus questions. This matters because those definitions don't require anything inherently transformative like us being able to automate all labour, or scientific research. Rather they involve a bunch of technical benchmarks that aren't that important on their own, which are being presumed to correlate with the transformative stuff we actually care about.</p>\n", "parentCommentId": null, "user": {"username": "Dr. David Mathers"}}, {"_id": "Ctou9JxKzhhwLkzpc", "postedAt": "2023-03-25T12:12:40.802Z", "postId": "AtdApEsvPr8QhdoBa", "htmlBody": "<p>See also the recent Lex Fridman Twitter <a href=\"https://twitter.com/lexfridman/status/1638908038928293892\">poll</a> [H/T <a href=\"https://forum.effectivealtruism.org/posts/nHq4hLsojDPf3Pqg9/the-overton-window-widens-examples-of-ai-risk-in-the-media?commentId=5aBkrTAjZLgoNdkm3\">Max Ra</a>]:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Ctou9JxKzhhwLkzpc/i4creheicayvnl4yjjdt\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Ctou9JxKzhhwLkzpc/rovkyexx5pl1yapfxu6t 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Ctou9JxKzhhwLkzpc/tfwg1yqaziuklr0doj1j 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Ctou9JxKzhhwLkzpc/krhavgu0sgepa12hxgmo 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Ctou9JxKzhhwLkzpc/sr1311dkxtbuj1lei3gx 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Ctou9JxKzhhwLkzpc/vmsb9lvocayf6iibjwmj 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Ctou9JxKzhhwLkzpc/hqrdcmkniq80fuljuper 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Ctou9JxKzhhwLkzpc/tyywsuyqtimwozr8bqe7 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Ctou9JxKzhhwLkzpc/dptb62k4yjevgqngtefh 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Ctou9JxKzhhwLkzpc/x78x2mj56qols0glcyjm 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Ctou9JxKzhhwLkzpc/d0mzc9wymvonzacmwhfy 1068w\"></figure>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "ek3yaP2uTetd3FAia", "postedAt": "2023-03-25T12:22:32.727Z", "postId": "AtdApEsvPr8QhdoBa", "htmlBody": "<p>I think, in hindsight, the Fire Alarm first started ringing in a DeepMind building in <a href=\"https://en.wikipedia.org/wiki/AlphaZero\">2017</a>. Or perhaps an OpenAI building in <a href=\"https://en.wikipedia.org/wiki/GPT-3\">2020</a>. It's certainly going off all over Microsoft <a href=\"https://en.wikipedia.org/wiki/GPT-4\">now</a>. It's also going off in <a href=\"https://forum.effectivealtruism.org/posts/nHq4hLsojDPf3Pqg9/the-overton-window-widens-examples-of-ai-risk-in-the-media\">many other</a> <a href=\"https://www.youtube.com/watch?v=gA1sNLL6yg4&amp;ab_channel=BanklessShows\">places</a>. To some of us it is already deafening. A huge, ominous, distraction from our daily lives. I really want to do something to <a href=\"https://forum.effectivealtruism.org/posts/KoWW2cc6HezbeDmYE/greg_colbourn-s-shortform?commentId=HzD5WsdfiAfEpgJbM\">shut the damn thing off</a>.</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "m3QLCCLr9xHMmjPcT", "postedAt": "2023-03-25T12:44:31.179Z", "postId": "AtdApEsvPr8QhdoBa", "htmlBody": "<p>The answer is that the question in question is not actually forecasting weak AGI, it's forecasting these specific resolution criteria:</p><blockquote><p>For these purposes we will thus define \"AI system\" as a single unified software system that can satisfy the following criteria, all easily completable by a typical college-educated human.</p><ul><li>Able to reliably pass a Turing test of the type that would win the <a href=\"https://www.metaculus.com/questions/73/will-the-silver-turing-test-be-passed-by-2026/\">Loebner Silver Prize</a>.</li><li>Able to score 90% or more on a robust version of the <a href=\"https://www.metaculus.com/questions/644/what-will-be-the-best-score-in-the-20192020-winograd-schema-ai-challenge/\">Winograd Schema Challenge</a>, e.g. the <a href=\"https://arxiv.org/abs/1907.10641\">\"Winogrande\" challenge</a> or comparable data set for which human performance is at 90+%</li><li>Be able to score 75th percentile (as compared to the corresponding year's human students; this was a score of 600 in 2016) on all the full mathematics section of a circa-2015-2020 standard SAT exam, using just images of the exam pages and having less than ten SAT exams as part of the training data. (Training on other corpuses of math problems is fair game as long as they are arguably distinct from SAT exams.)</li><li>Be able to learn the classic Atari game \"Montezuma's revenge\" (based on just visual inputs and standard controls) and explore all 24 rooms based on the equivalent of less than 100 hours of real-time play (see <a href=\"https://www.metaculus.com/questions/486/when-will-an-ai-achieve-competency-in-the-atari-classic-montezumas-revenge/\">closely-related question</a>.)</li></ul></blockquote>", "parentCommentId": "LLCJRfakQPjXJ23Y5", "user": {"username": "Erich_Grunewald"}}, {"_id": "YhvHMj4NDtfmFTbMo", "postedAt": "2023-03-25T17:50:14.465Z", "postId": "AtdApEsvPr8QhdoBa", "htmlBody": "<p>What I mean is \"these forecasts give no more information than flipping a coin to decide whether AGI would come in time period A vs. time period B\".</p>\n<p>I have my own, rough, inside views about if and when AGI will come and what it would be able to do, and I don't find it helpful to quantify them into a specific probability distribution. And there's no \"default distribution\" here that I can think of either.</p>\n", "parentCommentId": "yjvjstGTXLpkPzfuA", "user": {"username": "Guy Raveh"}}, {"_id": "cmhArBFkD6XyKHFEf", "postedAt": "2023-03-26T07:19:47.317Z", "postId": "AtdApEsvPr8QhdoBa", "htmlBody": "<p>Might as well make an alternate prediction here:&nbsp;</p><p>There will be no AGI in the next 10 years. There will be an AI bubble over the next couple of years as new applications for deep learning proliferate, creating a massive hype cycle similar to the dot-com boom.&nbsp;</p><p>This bubble will die down or burst when people realize the limitations of deep learning in domains that lack gargantuan datasets. It will fail to take hold in domains where errors cause serious damage (see the unexpected difficulty of self-driving cars). Like with the burst of the dot-com bubble, people will continue to use AI a lot for the applications that it is actually good at.&nbsp;</p><p>If AGI does occur, it will be decades away at least, and require further conceptual breakthroughs and/or several orders of magnitude higher computing power.&nbsp;</p>", "parentCommentId": null, "user": {"username": "titotal"}}, {"_id": "dJQTvdBJQ5Cezi529", "postedAt": "2023-03-27T19:52:44.345Z", "postId": "AtdApEsvPr8QhdoBa", "htmlBody": "<p>Gotcha, I think I still disagree with you for most decision-relevant time periods (e.g. I think they're likely better than chance on estimating AGI within 10 years vs 20 years)</p>\n", "parentCommentId": "YhvHMj4NDtfmFTbMo", "user": {"username": "Gabe Mukobi"}}]