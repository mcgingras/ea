[{"_id": "WfeW7CFrwPp9sWHcG", "postedAt": "2022-09-24T18:01:30.577Z", "postId": "RkpdA8763yGtEovj9", "htmlBody": "<p>Strong upvote for giving some outside perspective to the field, and this is an important point of why AI Alignment is likely to be tractable at all. It also means getting many more researchers and money, fast is important for AI Safety.</p>\n", "parentCommentId": null, "user": {"username": "Sharmake"}}, {"_id": "xnEBz4m5evAfFj4Rv", "postedAt": "2022-09-24T18:20:22.503Z", "postId": "RkpdA8763yGtEovj9", "htmlBody": "<p>I want to push back on the conception of the progress of a research field being well-correlated with \"the number of people working in that field\".&nbsp;</p><p>I think the heuristic of \"a difficult problem is best solved by having a very large number of people working on it\" is not a particularly successful heuristic when predicting past successes in science, nor is it particularly successful if you are trying to forecast business success. When a company is trying to solve a difficult technical or scientific problem, they don't usually send 10,000 people to work on it (and doing so would almost never work). They send their very best people to work on it, and spend a good number of resources supporting them.&nbsp;</p><p>Right now, we don't have traction for AI Alignment, and indeed, many if not most of the people who I think have most of a chance to find traction on AI Alignment are instead busy dealing with all the newcomers to the field. When I do interviews with top researchers they often complain that the quality of their research environment has gotten worse over time as more and more people with less context are filling their social environment, and they have found the community &nbsp;worse over time for making intellectual progress in (this is not a universally reported thing, but it's a pretty common thing I've heard).&nbsp;</p><p>I don't think the right goal should be to have 10,000 people work on our current confused models of AI Alignment. I think we don't currently really know how to have 10,000 people work on AI Alignment, and if we tried, I expect that group of people would end up optimizing for proxy variables that have little to do with research progress, like how cognitive psychology ended up optimizing extremely hard for p-values and as a field produces less useful insight than (as far as I can tell) Daniel Kahnemann himself while he was actively publishing.</p><p>I think it's good to get more smart people thinking about the problem, but it's easy to find examples of extremely large efforts with thousands of people working on a problem, being vastly less effective than a group of 20 people. Indeed, I think that's the default for most difficult problems in the world (I think FTX would be less successful if it had 10,000 employees, as would most startups, and, I argue, also most research fields).</p>", "parentCommentId": null, "user": {"username": "Habryka"}}, {"_id": "asekqtGKE8zFWDpN3", "postedAt": "2022-09-24T18:32:56.429Z", "postId": "RkpdA8763yGtEovj9", "htmlBody": "<p>I agree that 10k people working in the same org would be unwieldy. I'm thinking more having 10k people working in hundreds or orgs and sometimes independently, etc.\nEach of these people would be in their own little microcosm, and dealing with the same normal amount of interactions. Should address the lowering social environment cost. Might even make it better because people could more easily find their \"tribe\"</p>\n<p>And I agree right now we wouldn't be able to absorb that number usefully. That's currently an unsolved problem that would be good to make progress on.</p>\n", "parentCommentId": "xnEBz4m5evAfFj4Rv", "user": {"username": "katherinesavoie"}}, {"_id": "eaEMYBs5tpsGKJ588", "postedAt": "2022-09-24T19:35:17.342Z", "postId": "RkpdA8763yGtEovj9", "htmlBody": "<p>Also, I'm surprised at the claim that more people doesn't lead to more progress. I've heard that one major cause of progress so far has just been that there's a much larger population of people to try things (of course, progress also causes there to be more people, so the causal chain goes both ways). Similarly, the reason why cities tend to have more innovation than small towns is because there's a denser number of people around each other.&nbsp;</p><p>You can also think of it from the perspective of adding more explore. Right now there are surprisingly few research agendas. Having more people would lead to more of them, and it increases the odds that one of them is correct.&nbsp;</p><p>Of note, I do share your concerns about making sure the field doesn't just end up maximizing proxy metrics. I think that will be tricky and will require a lot of work (as it does right now even!).&nbsp;</p>", "parentCommentId": "xnEBz4m5evAfFj4Rv", "user": {"username": "katherinesavoie"}}, {"_id": "mtfZFKTvnpQvuXQsx", "postedAt": "2022-09-25T06:39:35.406Z", "postId": "RkpdA8763yGtEovj9", "htmlBody": "<p>I think more people in a worldwide population generally leads to more innovation, but primarily in domains where there is a lot of returns to scale, and where you have a lot of incentives for people to make progress towards. If you want to have get people to explore a specific problem, I think more people rarely helps (because the difficulty lies in aiming people at the problem, not in the firepower you have).&nbsp;</p><p>I think adding more people also rarely causes more exploration to happen. Large companies are usually much less innovative than small companies. Coordinating large groups of people usually requires conformity and because of asymmetries in how easy it is to cause harm to a system vs. to produce value, requires widespread conservativism in order to function. I think similar things are happening in EA, where the larger EA gets, the more people are concerned about someone \"destroying the reputation of the community\" the more people have to push on the brakes in order to prevent anyone from taking risky action.&nbsp;</p><p>I think there exist potential configurations of a research field that can scale substantially better, but I don't think we are currently configured that way, and I expect by default exploration to go down as scale goes up (in general, the number of promising new research agendas and direction seems to me to have gone down a lot during the last 5 years as EA has grown a lot, and this is a sentiment I've heard mirrored from most people who have been engaged for that long).</p>", "parentCommentId": "eaEMYBs5tpsGKJ588", "user": {"username": "Habryka"}}, {"_id": "vap7SDZnvSLsK2JLq", "postedAt": "2022-09-25T12:00:01.407Z", "postId": "RkpdA8763yGtEovj9", "htmlBody": "<blockquote><p>Large companies are usually much less innovative than small companies</p></blockquote><p>I think this is still in the framework of thinking that large groups of people having to coordinate leads to stagnation. To change my mind, you'd have to make the case that having a larger number of startups leads to less innovation, which seems like a hard case to make.&nbsp;</p><blockquote><p>the larger EA gets, the more people are concerned about someone \"destroying the reputation of the community\"</p></blockquote><p>I think this is a separate issue that might be caused by the size of the movement, but a different hypothesis is that it's simply an idea that has traction in the movement. One which has been around for a long time, even while we were a lot smaller. Spending your \"weirdness points\" and such considerations have been around since the very beginning.&nbsp;</p><p>(On a side note, I think we're overly concerned about this, but that's a whole other post. Suffice to say here that a lot of the probability mass is on this not being caused by the size of the movement, but rather a particularly sticky idea)</p><blockquote><p>I think there exist potential configurations of a research field that can scale substantially better, but I don't think we are currently configured that way</p></blockquote><p>\ud83c\udfaf I 100% agree. I'm thinking of spending some more time thinking on and writing up ways that we could make it so the movement could usefully take on more researchers. I also encourage others to think on this, because it could unlock a <i>lot</i> of potential.&nbsp;</p><blockquote><p>I expect by default exploration to go down as scale goes up</p></blockquote><p>I think this is where we disagree. It'd be very surprising if ~150 researchers is the optimal amount, or that having less would lead to more innovation and more/better research agendas.&nbsp;</p><blockquote><p>in general, the number of promising new research agendas and direction seems to me to have gone down a lot during the last 5 years as EA has grown a lot, and this is a sentiment I've heard mirrored from most people who have been engaged for that long</p></blockquote><p>An alternative hypothesis is that people you've been talking to have been becoming more pessimistic about having hope at all (if you hang out with MIRI folk a lot, I'd expect this to be more acute). It might not be because there's more people having bad ideas or that having more people in the movement leads to a decline in quality, but rather a certain contingency think alignment is impossible or deeply improbable, so that <i>all</i> &nbsp;ideas seem bad. In this paradigm/POV, the default is that all new research agendas seem bad. It's not that the agendas got worse. It's that people think the problem is even harder than they originally thought.&nbsp;</p><p>Another hypothesis is that the idea of epistemic humility has been spreading, combined with the idea that you need intensive mentorship. This leads to new people coming in being less likely to actually come up with new research agendas, but rather to defer to authority. (A whole other post there!)</p><p>Anyways, just some alternatives to consider :) It's hard to convey tone over text, but I'm enjoying this discussion a lot and you should read all my writing assuming a lot of warmth and engagement. :)&nbsp;</p>", "parentCommentId": "mtfZFKTvnpQvuXQsx", "user": {"username": "katherinesavoie"}}, {"_id": "ST8mrdB4pCJogvx6d", "postedAt": "2022-09-25T12:02:24.623Z", "postId": "RkpdA8763yGtEovj9", "htmlBody": "<blockquote><p>in general, the number of promising new research agendas and direction seems to me to have gone down a lot during the last 5 years</p></blockquote><p>At least in technical AI Alignment, the opposite seems to have happened in the last couple of years. It looks like we're in the midst of <a href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is\">a Cambrian explosion of research groups and agendas</a>. Or would you argue that most of these aren't promising?</p>", "parentCommentId": "mtfZFKTvnpQvuXQsx", "user": {"username": "Greg_Colbourn"}}, {"_id": "XcqF3LFohRykp98ue", "postedAt": "2022-09-25T19:46:23.652Z", "postId": "RkpdA8763yGtEovj9", "htmlBody": "<p>There is a cambrian explosion of research groups, but basically no new agendas as far as I can tell? Of the agendas listed on that post, I think basically all are 5+ years old (some have morphed, like ELK is a different take on scalable oversight than Paul had 5 years ago, but I would classify it as the same agenda).&nbsp;</p><p>There is a giant pile of people working on the stuff, though the vast majority of new work can be characterized \"let's just try to solve some near-term alignment problems and hope that it somehow informs our models of long-term alignment problems\" and a large pile of different types of transparency research. I think there are good cases for that work, though I am not very optimistic about it helping with existential risk.</p>", "parentCommentId": "ST8mrdB4pCJogvx6d", "user": {"username": "Habryka"}}, {"_id": "AvHZgh4vgssCoCTey", "postedAt": "2022-09-25T19:53:13.837Z", "postId": "RkpdA8763yGtEovj9", "htmlBody": "<blockquote><p>I think this is still in the framework of thinking that large groups of people having to coordinate leads to stagnation. To change my mind, you'd have to make the case that having a larger number of startups leads to less innovation, which seems like a hard case to make.&nbsp;</p></blockquote><p>I think de-facto right now people have to coordinate in order to do work on AI Alignment, because most people need structure and mentorship and guidance to do any work, and want to be part of a coherent community.&nbsp;</p><p>Separately, I also think many startup communities are indeed failing to be innovative because of their size and culture. Silicon Valley is a pretty unique phenomenon, and I've observed \"startup communities\" in Germany that felt to me liked they harmed innovation more than they benefitted it. The same is true for almost any \"startup incubator\" that large universities are trying to start up. When I visit them, I feel like the culture there primarily encourage conformity and chasing the same proxy metrics as everyone else.&nbsp;</p><p>I think actually creating a startup ecosystem is hard, and I think it's still easier than creating a similar ecosystem for something as ill-defined as AI Alignment. The benefit that startups have is that you can very roughly measure success by money, at least in the long run, and this makes it pretty easy to point many people at the problem (and like, creates strong incentives for people to point themselves at the problem).&nbsp;</p><p>I think we have no similar short pointer for AI Alignment, and most people who start working in the field seem to me to be quite confused about what the actual problem to be solved is, and then often just end up doing AI capabilities research while slapping an \"AI Alignment\" label on it, and I think scaling that up mostly just harms the world.</p>", "parentCommentId": "vap7SDZnvSLsK2JLq", "user": {"username": "Habryka"}}, {"_id": "YQhLdL9suqYcN8ytu", "postedAt": "2022-09-25T20:14:15.081Z", "postId": "RkpdA8763yGtEovj9", "htmlBody": "<p>I think we should generally have a prior that social dynamics of large groups of people end up pushing heavily towards conformity, and that those pressures towards conformity can cancel out many orders of magnitude of growth of the number of people who could theoretically explore different directions.&nbsp;</p><p>As a concrete case study, I like this Robin Hanson post <a href=\"https://www.overcomingbias.com/2020/09/the-world-forager-elite.html\">\"The World Forager Elite\"</a>:&nbsp;</p><blockquote><p>The world has mostly copied bad US approaches to over-regulating planes as well. We also see regulatory convergence in topics like human cloning; many had speculated that China would be defy the consensus elsewhere against it, but that turned out not to be true. Public prediction markets on interesting topics seems to be blocked by regulations almost everywhere, and insider trading laws are most everywhere an obstacle to internal corporate markets.</p><p>Back in February we saw a dramatic example of world regulatory coordination. Around the world public health authorities were talking about treating this virus like they had treated all the others in the last few decades. But then world elites talked a lot, and suddenly they all agreed that this virus must be treated differently, such as with lockdowns and masks. Most public health authorities quickly caved, and then most of the world adopted the same policies. Contrarian alternatives like <a href=\"https://www.overcomingbias.com/2020/03/variolation-may-cut-covid19-deaths-3-30x.html\">variolation</a>, challenge trials, and cheap fast lower-reliability tests have also been rejected everywhere; small experiments have not even been allowed.</p><p>One possible explanation for all this convergence is that regulators are just following what is obviously the best policy. But if you dig into the details you will quickly see that the usual policies are not at all obviously right. Often, they seem obviously wrong. And having all the regulatory bodies suddenly change at once, even when no new strong evidence appeared, seems especially telling.</p><p>It seems to me that we instead have a strong world culture of regulators, driven by a stronger world culture of elites. Elites all over the world talk, and then form a consensus, and then authorities everywhere are pressured into following that consensus. Regulators most everywhere are quite reluctant to deviate from what most other regulators are doing; they\u2019ll be blamed far more for failures if they deviate. If elites talk some more, and change their consensus, then authorities must then change their polices. On topic X, the usual experts on X are part of that conversation, but often elites overrule them, or choose contrarians from among them, and insist on something other than what most X experts recommend.</p></blockquote><p>The number of nations, as well as the number of communities and researchers that were capable of doing innovative things in response to COVID was vastly greater in 2020 than for any previous pandemic. But what we saw was much less global variance and innovation in pandemic responses. I think there was scientific innovation, and that innovation was likely greater than for previous pandemics, but overall, despite the vastly greater number of nations and people in the international community of 2020, this only produced more risk-aversion in stepping out of line with elite consensus.</p><p>I think by-default we should expect similar effects in fields like AI Alignment. I think maintaining a field that is open to new ideas and approaches is actively difficult. If you grow the field without trying to preserve the concrete and specific mechanisms that are in place to allow innovation to grow, more people will not result in more innovation, it will result in less, even from the people that have previously been part of the same community.&nbsp;</p><p>In the case of COVID, the global research community spent a substantial fraction of its effort on actively preventing people from performing experiments like variolation or challenge trials, and we see the same in fields like Psychology research where a substantial fraction of energy is spent on <a href=\"https://slatestarcodex.com/2017/08/29/my-irb-nightmare/\">ever-increasing ethical review requirements</a>.&nbsp;</p><p>We see the same in the construction industry (a recent strong interest of mine), which despite its quickly growing size, is performing substantially fewer experiments than it was 40 years ago, and is spending most of its effort actively regulating what other people in the industry can do, and limiting the type of allowable construction materials and approaches to smaller and smaller sets.</p><p>I think by-default, I expect fast growth of the AI Alignment community to reduce innovation for the same reasons. I expect a larger community will increase pressures towards forming an elite consensus, and that consensus will be enforced via various legible and illegible means. Most of the world is really not great at innovation, and the default outcome of large groups of people, even when pointed towards a shared goal, is not innovation, but conformity, and if we recklessly grow, I think we will default towards the same common outcome.&nbsp;</p>", "parentCommentId": "AvHZgh4vgssCoCTey", "user": {"username": "Habryka"}}, {"_id": "TXCch8gdmEdgeeCTR", "postedAt": "2022-09-25T21:05:27.422Z", "postId": "RkpdA8763yGtEovj9", "htmlBody": "<blockquote><p>I think the heuristic of \"a difficult problem is best solved by having a very large number of people working on it\" is not a particularly successful heuristic when predicting past successes in science, nor is it particularly successful if you are trying to forecast business success.</p></blockquote><p>Interesting, can you give some examples of this that are analogous to solving the alignment problem?<br><br>As a separate point, it might still be worth getting many more people working on alignment ASAP to shift the Overton window. Some extremely talented mathematicians have worked together on cryptography projects for the US government, and I imagine something similar could happen for alignment in the future.</p>", "parentCommentId": "xnEBz4m5evAfFj4Rv", "user": {"username": "jakubkraus07@gmail.com"}}, {"_id": "dyyyTQ2TNPJs6YAfa", "postedAt": "2022-09-26T04:15:39.314Z", "postId": "RkpdA8763yGtEovj9", "htmlBody": "<blockquote><p>Back in February we saw a dramatic example of world regulatory coordination. Around the world public health authorities were talking about treating this virus like they had treated all the others in the last few decades. But then world elites talked a lot, and suddenly they all agreed that this virus must be treated differently, such as with lockdowns and masks. Most public health authorities quickly caved, and then most of the world adopted the same policies.</p></blockquote><p>Is there any legible evidence for this? &nbsp;</p>", "parentCommentId": "YQhLdL9suqYcN8ytu", "user": null}, {"_id": "zqcqqtAP6D8py9gbj", "postedAt": "2022-09-26T07:04:01.018Z", "postId": "RkpdA8763yGtEovj9", "htmlBody": "<p>Are there any promising directions for AGI x-risk reduction that you are aware of that aren't being (significantly) explored?</p>\n", "parentCommentId": "XcqF3LFohRykp98ue", "user": {"username": "Greg_Colbourn"}}, {"_id": "ccbtz5YMndLdjATvP", "postedAt": "2022-09-26T07:13:22.076Z", "postId": "RkpdA8763yGtEovj9", "htmlBody": "<p>There was some deviation (e.g. no lockdowns in Sweden), but the most telling thing was no human challenge trials anywhere in the world. That alone was a tragedy that prolonged the pandemic by months (by delaying roll-out of vaccines) and caused millions of deaths.</p>\n", "parentCommentId": "dyyyTQ2TNPJs6YAfa", "user": {"username": "Greg_Colbourn"}}, {"_id": "ZDyqLpSkySafjnhtB", "postedAt": "2022-09-26T07:17:43.690Z", "postId": "RkpdA8763yGtEovj9", "htmlBody": "<p>This is a good point, and a wake up call for us to do better with AI Alignment. Given the majority of funding for AGI x-safety is coming from within EA right now, and as a community we are accutely aware of the failings with Covid, we should be striving to do better.</p>\n", "parentCommentId": "YQhLdL9suqYcN8ytu", "user": {"username": "Greg_Colbourn"}}, {"_id": "mKjXBkkf49DnsxEEs", "postedAt": "2022-09-26T07:25:42.605Z", "postId": "RkpdA8763yGtEovj9", "htmlBody": "<p>Re conformity, I wonder if <a href=\"https://forum.effectivealtruism.org/posts/W7C5hwq7sjdpTdrQF/announcing-the-future-fund-s-ai-worldview-prize?commentId=xprWD5dtG3fhQ7sNd\">related arguments could help shift the Future Funds' worldview</a>?</p>", "parentCommentId": "YQhLdL9suqYcN8ytu", "user": {"username": "Greg_Colbourn"}}, {"_id": "CyKvkz9f7GabmxYSz", "postedAt": "2022-09-26T13:01:18.106Z", "postId": "RkpdA8763yGtEovj9", "htmlBody": "<p>That's really interesting and unexpected! Seems worth figuring out why that's happening. What are your top hypotheses for why that's happening?&nbsp;</p><p>My first guess would be epistemic humility norms.&nbsp;</p><p>My second would be that the first people in a field are often disproportionately talented compared to people coming in later. (Although you could also tell a story about how at the beginning it's too socially weird so it can't attract a lot of top talent).&nbsp;</p><p>My third is that since alignment is so hard, it's easier for people to latch onto existing research agendas instead of creating new ones. At the beginning there were practically no agendas to latch onto, so people had to make new ones, but now there are a few, so most people just sort themselves into those.&nbsp;</p>", "parentCommentId": "XcqF3LFohRykp98ue", "user": {"username": "katherinesavoie"}}]