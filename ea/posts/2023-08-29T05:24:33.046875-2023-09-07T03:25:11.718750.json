[{"_id": "Defu3jkejb7pmLjeN", "title": "Nick Beckstead is leaving the Effective Ventures boards", "postedAt": "2023-09-06T18:15:20.926Z", "htmlBody": "<p>On 23rd August, Nick Beckstead stepped down from the boards of Effective Ventures UK and Effective Ventures US.</p><p>For context, EV UK and EV US host and fiscally sponsor several (mostly EA-related) projects, such as CEA, 80,000 Hours and various others (see more&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/GoWNiPbrEb6NHD3MF/announcing-interim-ceos-of-evf\"><u>here</u></a>).</p><p>Since November 2022, Nick has been recused from all board matters related to the collapse of FTX. Over time, it became clear that Nick\u2019s recusal made it difficult for him to add sufficient value to EV and its projects for it to be worth him remaining on the boards<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefis1ehf6e5kd\"><sup><a href=\"#fnis1ehf6e5kd\">[1]</a></sup></span>. Nick and the other trustees felt that this was sufficient reason for Nick to step down.</p><p>Nick wanted to share the following:</p><blockquote><p><i>Ever since the collapse of FTX, I've been recused from a substantial fraction of business on both boards. This has made it hard to contribute as much as I would like to as a board member, during a time where engaged board members are especially important. Since this situation may not change for a while, I think it's a good time for me to step down.</i></p><p><i>I am grateful to have played a role in getting EV UK and EV US off the ground and helping them develop over the last 14 years since the launch of Giving What We Can. Projects at EV have accomplished a great deal, drawing substantial resources and attention toward addressing some of the world's most pressing problems, with impacts that are varied, large, and difficult to quantify. The people at EV are amongst the most thoughtful, generous, kind, and dedicated that I've had the pleasure to interact with. I feel very proud of all that we have accomplished together, and optimistic about the work that will continue in my absence.</i></p></blockquote><p>As a founding board member of EV UK (then called CEA), Nick played a vital role in getting EV US, EV UK and their constituent projects off the ground. For example, Nick was involved in setting up the first Giving What We Can student group and helped to hire the first full-time staff at what was then CEA. We are very grateful to Nick for everything he\u2019s contributed to the effective altruism movement to date and look forward to his future positive impact; we wish him the best of luck with his future work.<br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnis1ehf6e5kd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefis1ehf6e5kd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is because the recusal affected not just decisions that were directly related to the collapse of FTX, but also many other decisions for which the way EV UK and EV US have been affected by the collapse of FTX was important context.</p></div></li></ol>", "user": {"username": "reallyeli"}}, {"_id": "xgE4uxy5EPRX2wFr2", "title": "An overview of market shaping in global health: Landscape, new developments, and gaps", "postedAt": "2023-09-06T17:07:03.120Z", "htmlBody": "<h2>Editorial note</h2><p>This report is a \u201cshallow\u201d investigation, as described <a href=\"https://perma.cc/D85A-EKDG\">here</a>, and was commissioned by GiveWell and produced by Rethink Priorities from February to April 2023. We revised this report for publication. GiveWell does not necessarily endorse our conclusions, nor do the organizations represented by those who were interviewed.</p><p>The primary focus of the report is to provide an overview of market shaping in global health. We describe how market shaping is typically used, its recent track record, and ongoing gaps in its implementation. We also spotlight two specific market shaping approaches (pooled procurement and subscription models). Our research involved reviewing the scientific and gray literature and speaking to five experts.</p><p>We don\u2019t intend this report to be Rethink Priorities\u2019 final word on market shaping, and we have tried to flag major sources of uncertainty in the report. We hope this report galvanizes a productive conversation within the global health and development community about the role of market shaping in improving global health. We are open to revising our views as more information is uncovered.</p><h2>Key takeaways</h2><ul><li>Market shaping \u2014 in the context of global health \u2014 comprises interventions to create well-functioning markets through improving specific market outcomes (e.g., availability of products) with the end goal of improving public health. Market shaping interventions tend to be catalytic, timebound, and have a strong focus on influencing buyer and supplier interactions. <a href=\"https://rethinkpriorities.org/publications/an-overview-of-market-shaping-in-global-health#market-shaping-is-a-means-of-creating-well-functioning-markets-by-targeting-the-root-causes-of-market-shortcomings\">[more]</a></li><li>Market shaping interventions are used to address various market shortcomings. A commonly used framework to assess shortcomings in various market characteristics is some variation of the \u201cfive As\u201d: affordability, availability, assured quality, appropriate design, and awareness. <a href=\"https://rethinkpriorities.org/publications/an-overview-of-market-shaping-in-global-health#market-shaping-typically-addresses-shortcomings-in-one-or-several-market-characteristics-affordability-availability-assured-quality-appropriate-design-and-awareness\">[more]</a></li><li>There is no commonly agreed upon set of interventions under the term of market shaping, but they can be broadly categorized by the main type of lever they use: reduce transaction costs (e.g., pooled procurement), increase market information (e.g., strategic demand forecasting), balance supplier and buyer risks (e.g., advance market commitments). <a href=\"https://rethinkpriorities.org/publications/an-overview-of-market-shaping-in-global-health#there-is-no-clearly-defined-set-of-market-shaping-interventions-but-typical-examples-are-pooled-procurement-advance-market-commitments-and-strategic-demand-forecasting\">[more]</a></li><li>New developments have been taking place in the field in recent years: (1) New intervention types have been devised and implemented (e.g., ceiling price agreements); (2) there has been a drive toward institutionalization with the launch of several new organizations whose sole policy instrument focus is market shaping (e.g., MedAccess); (3) there is an increase in co-ownership with national governments in low- and middle-income countries (LMICs); (4) the field is increasingly experiencing diminishing returns as most of the \u201clow-hanging fruits\u201d have been picked, and projects are getting more complex with narrower indications and smaller health impacts. <a href=\"https://rethinkpriorities.org/publications/an-overview-of-market-shaping-in-global-health#the-last-decade-has-seen-new-developments-in-the-field-on-several-fronts\">[more]</a></li><li>Market shaping has recently seen both wins and disappointments. Recent wins include: (1) Results for Development\u2019s (R4D) amoxicillin dispersible tablets (amox DT) program; (2) ceiling price agreements for optimized antiretroviral (ARV) regimens; (3) a ceiling price agreement for HIV self test; (4) significant price reductions in vaccines achieved by Gavi. Recent disappointments include: (1) the continued price instability of malaria ACTs; (2) the failure of a uterotonic agent to be registered in Kenya; (3) the sole supplier of malaria rapid diagnostic tests (mRDTs) threatening to leave the market due to unsustainably affordable prices; (4) a tuberculosis (TB) drug in Brazil not being procured. [<a href=\"https://rethinkpriorities.org/publications/an-overview-of-market-shaping-in-global-health#market-shaping-has-seen-several-recent-wins-and-disappointments\">more</a>]</li><li>We describe three case studies of recent market shaping activities:<ol><li>The <strong>Affordable Medicines Facility\u2014malaria (AMFm)</strong> was launched by the Global Fund in 2009 (and discontinued in 2017) as a financing mechanism aimed at increasing access to affordable and high-quality antimalarial medicines (ACTs) in eight LMICs. It consisted of price negotiations with manufacturers, a buyer subsidy, and various supportive programmatic interventions. The program was very controversial, but is overall considered successful at achieving its goals. <a href=\"https://rethinkpriorities.org/publications/an-overview-of-market-shaping-in-global-health#affordable-medicines-facility%E2%80%94malaria-amfm\">[more]</a></li><li><strong>Gavi has been coordinating pentavalent vaccine</strong> (a vaccine protecting against five diseases) <strong>market shaping interventions</strong> since 2001, mainly to increase uptake of the Hib and HepB vaccines in LMICs while reducing the number of shots needed. This was a large undertaking involving many actors and interventions (e.g., pooled procurement, market analyses, demand forecasts, technical assistance to regulators and manufacturers). The pentavalent vaccine is the first Gavi-supported market to reach fully satisfied demand. Moreover, pentavalent vaccine prices in 2023 are only one-third of the price level in 2006. However, the interventions may have had some unintended consequences. <a href=\"https://rethinkpriorities.org/publications/an-overview-of-market-shaping-in-global-health#gavi%E2%80%99s-pentavalent-vaccine-market-shaping-interventions\">[more]</a></li><li><strong>Unitaid/CHAI\u2019s Paediatric HIV/AIDS and Innovation in Paediatric Market Access (IPMA) projects</strong> ran between 2007 and 2016, largely as a way to pool and coordinate procurement for pediatric ARVs. The Paediatric HIV/AIDS project focused on pooled procurement, price negotiations with suppliers, and consolidating ARV formulations, while IPMA focused on technical assistance and global coordination efforts. Prior to 2010, Unitaid served as the sole funder and procurer. The projects were evaluated as being highly successful in terms of public health impact, near- and medium-term market effects, and cost-effectiveness; however, the transition away from central procurement in 2010 was likely inadequately executed. <a href=\"https://rethinkpriorities.org/publications/an-overview-of-market-shaping-in-global-health#unitaid-chai%E2%80%99s-paediatric-hiv-aids-and-innovation-in-paediatric-market-access-ipma-projects\">[more]</a></li></ol></li><li>Many actors are involved in the market shaping field (e.g., Global Fund, Gavi, UNICEF, USAID, R4D) and perform three functions: funding, research, and implementation. BMGF is the main philanthropic funder of market shaping work. Most actors we\u2019ve seen focus on the \u201cbig three\u201d infectious diseases (TB, HIV/AIDS, malaria), and/or on vaccines. <a href=\"https://rethinkpriorities.org/publications/an-overview-of-market-shaping-in-global-health#many-actors-are-involved-in-funding-researching-and-implementing-market-shaping-interventions\">[more]</a></li><li>Our impression is that the mandates of most (with the exception of some more recent organizations) of the major players do not stipulate any particular market shaping approaches, but rather a focus on specific diseases, product types, and public health goals). We have not found any comprehensive overview of funding streams in the market shaping field, but some example funding figures we found point to a total annual spending in the billions of dollars. <a href=\"https://rethinkpriorities.org/publications/an-overview-of-market-shaping-in-global-health#most-major-funders-and-implementers-focus-on-specific-diseases-or-product-types-rather-than-specific-market-shaping-interventions\">[more]</a></li><li>Market shaping funders and implementers have historically neglected several areas, which we summarize in three groups: <a href=\"https://rethinkpriorities.org/publications/an-overview-of-market-shaping-in-global-health#several-therapeutic-areas-intervention-types-and-market-types-have-historically-been-neglected-in-market-shaping\">[more]</a><ol><li><strong>Therapeutic areas:</strong> Non-communicable diseases, certain infectious diseases (e.g., hepatitis), maternal and child health (excluding family planning), and cross-therapeutic products (e.g., medical oxygen) have been neglected relative to the \u201cbig three\u201d infectious diseases (HIV/AIDS, malaria, TB). Moreover, comprehensive primary care provision has received less attention relative to verticalized, donor-supported programs.</li><li><strong>Intervention types: </strong>Market shaping interventions have historically focused heavily on the supply side, with less attention devoted to the demand side. Moreover, interventions focused on the scale-up of new medical products have lagged behind the support of R&amp;D programs. Non-traditional financing solutions are under-utilized.</li><li><strong>Market types:</strong> National and subnational, and \u201cfragmented\u201d product markets have been neglected mainly due to structural challenges (e.g., the market for maternal and child health products is highly decentralized and fragmented across many different national health ministries and procurers).</li></ol></li><li>We spotlighted two intervention types:<ol><li><strong>Pooled procurement </strong>dates back to the 1970s and means that buyers \u201cpool\u201d their financial, technical, or human resources to purchase products to increase the buyers\u2019 bargaining power and procurement efficiencies. It is a frequently used intervention type to help reduce prices, improve quality standards, increase product availability, and speed up drug access. <a href=\"https://rethinkpriorities.org/publications/an-overview-of-market-shaping-in-global-health#pooled-procurement\">[more]</a></li><li><strong>Antibiotic subscription models </strong>are a novel concept in which payments to antibiotics manufacturers and developers are delinked from the volumes sold. They are used to increase pharmacological innovation in antibiotics while at the same time reducing incentives for antibiotic overprescription to hinder the spread of antimicrobial resistance. Two pilots are currently being implemented in the UK and in Sweden. <a href=\"https://rethinkpriorities.org/publications/an-overview-of-market-shaping-in-global-health#antibiotic-subscription-models\">[more]</a></li></ol></li></ul><p><a href=\"https://rethinkpriorities.org/publications/an-overview-of-market-shaping-in-global-health\"><strong><u>Click here</u></strong></a><strong> for the full version of this report on the Rethink Priorities website.</strong></p><h1><strong>Acknowledgments</strong></h1><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/c5m8vAxpJgJJ2XGFu/yyzoqjvtmgntnoqgfhww\"></figure><p>Jenny Kudymowa and James Hu jointly researched and wrote this report. Melanie Basnak supervised the report. Thanks to Melanie Basnak, Bruce Tsai, Tom Hird, and Siddhartha Haria (Development Innovation Lab) for helpful comments on drafts. Further thanks to Neel Lakhani, David Ripin, Susie Nazzaro, and two senior US-based market shaping experts (who preferred not to be named) for taking the time to speak with us. GiveWell provided funding for this report, but it does not necessarily endorse our conclusions.</p><p>If you are interested in Rethink Priorities' work, please consider subscribing to <a href=\"https://www.rethinkpriorities.org/newsletter\">our newsletter</a>. You can explore our completed public work <a href=\"https://www.rethinkpriorities.org/research\">here</a>.</p>", "user": {"username": "Rachel"}}, {"_id": "hybGfBnkrtL9E3EcS", "title": "How long will reaching a Risk Awareness Moment and CHARTS agreement take?", "postedAt": "2023-09-06T16:39:06.775Z", "htmlBody": "<p>The original report can be found here - <a href=\"https://docs.google.com/document/d/1MLmzULVrksH8IwAmH7wBpOTyU_7BjBAVl6N63v2vvhM/edit?usp=sharing\">https://docs.google.com/document/d/1MLmzULVrksH8IwAmH7wBpOTyU_7BjBAVl6N63v2vvhM/edit?usp=sharing</a></p><figure class=\"image image_resized\" style=\"width:64.99%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/buww9k3bm9iqrcxt7ocb\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/yei0y4esctnkv3wzkk9o 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/ogtw9pbdivnjkabcljck 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/rwnp3nhqvzbhxzdugaek 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/eochyo9oafkohneo2hwc 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/za4envtd8hgroipg828w 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/m4z0qvakwrawsalmydhp 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/a4qgwvzod8haaykt7pgr 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/adom0ogehu65txdmlapm 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/riyk36dla9nhmrivba7f 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/j2tnk2bnwmm8gxiumxmm 1024w\"></figure><h1>Acknowledgements</h1><p><i>I thank Iyngkarran Kumar, Oliver Guest, Holly Elmore, Lucy Farnik, Aidan Ewart, and Robert Reason for their discussions, pushback and feedback on this report. Any errors present are my sole responsibility. Cover image created by DALL-E 2.</i></p><p><i>This work was done under the Bristol AI Safety Centre (BASC).</i></p><h1><strong>Short Summary</strong></h1><p>This report aims to develop a forecast to an open question from the analysis, \u2018Prospects for AI safety agreements between countries\u2019 (<a href=\"https://forum.effectivealtruism.org/posts/L8GjzvRYA9g9ox2nP/prospects-for-ai-safety-agreements-between-countries\"><u>Guest, 2023</u></a>):&nbsp;<i>Is there sufficient time to have a \u2018risk awareness moment\u2019 in either the US (along with its allies) and China in place before an international AI safety agreement can no longer meaningfully reduce extinction risks from AI?</i></p><p>Bottom line: My overall estimate/best guess is that there is at least a 40% chance there will be adequate time to implement a CHARTS agreement before it ceases to be relevant.</p><h2>How much does public sentiment matter?</h2><p>This report focuses solely on forecasting \u2018risk awareness moments\u2019 (RAMs) related to advanced AI amongst policy elites in the United States and China. It does not consider RAMs amongst the general public.</p><p>Some evidence suggests public opinion has limited direct influence on policy outcomes in America. One influential study found average citizens\u2019 policy preferences had a \u201cnear-zero\u201d impact when controlling for elites\u2019 views. However, this finding remains debated, and the interplay between public attitudes and policy action is unclear. Given the lack of consensus, this report excludes general public sentiment.</p><p>In China\u2019s authoritarian context, public opinion may drive some policy changes, often through concerns about regime legitimacy. However, available survey data provides little insight into Chinese public views, specifically on AI risks.</p><p>Thus, this report will estimate RAM timelines considering only dynamics among governmental and technical elites shaping AI policy in both nations. Broader public risk awareness is complex to forecast and excluded from this report, given minimal directly relevant polling. The tentative focus is predicting when heightened concern about advanced AI risks emerges among key policymakers.</p><h2>What is the likelihood that we have a RAM amongst policy elites in the US and China?</h2><p>This report considers two scenarios for whether a sufficient \"risk awareness moment\" (RAM) regarding catastrophic risks from advanced AI has already occurred or may transpire in the near future among US and Chinese policy elites.</p><p>There is some evidence a RAM may be occurring now, like AI leaders voicing concern before Congress and China enacting regulations on harms from algorithms. However, factors like lack of political will for extreme precautions in the US and incentives to compete in AI in China weigh against having already crossed the RAM threshold. There appears to be a 44% chance of a current US RAM and a 54% chance for China.</p><p>Alternatively, a RAM could emerge within 2-3 years as more policymakers directly become aware of AI existential risks, \"warning shot\" incidents demonstrate concerning AI behaviours and major capability leaps occur. Reasons against this include AI safety becoming a partisan issue in the US and perceived benefits outweighing extinction risks. There seems to be a 60% chance of a RAM by 2026 under this scenario.</p><h2>Conditional on there being a risk awareness moment (RAM), how long would negotiations, on average, for an AI safety agreement to come into place?</h2><p>This report will rely on Oliver Guest's forecast that such an agreement could be negotiated approximately four years after a RAM among policy elites in the US and China (<a href=\"https://forum.effectivealtruism.org/posts/L8GjzvRYA9g9ox2nP/prospects-for-ai-safety-agreements-between-countries\"><u>Guest, 2023</u></a>).</p><p>As an initial estimate, I expect negotiations could conclude within 1-2 years of a RAM, especially if details are sorted out after an initial agreement in principle. However, Oliver Guest has conducted extensive research on timelines for this specific context. Oliver\u2019s 4-year estimate seems a reasonable preliminary guideline to follow.</p><h2>How long does it take for an agreement to come into force and then the actual regulations to take place?</h2><p>Oliver estimates a 70% chance that if an AI development oversight agreement is reached, it could come into legal force within six months. However, uncertainty remains around timelines to establish verification provisions in practice once an agreement is enacted.</p><p>Previous arms control treaties have seen verification bodies created rapidly, like the OPCW for the Chemical Weapons Convention. However, the first on-site inspections under new AI oversight could still take around six months as rules are interpreted and initial audits arranged.</p><p>Some factors like voluntary industry collaboration enabling monitoring ahead of mandates may help accelerate subsequent oversight implementation. With preparation, the timeline from enactment to proper enforcement of oversight through an agreement like CHARTS could potentially be shortened to 1-3 months.</p><h2>How long till CHARTS is a useful agreement to push for?</h2><p>This report examines when negotiating agreements like CHARTS to reduce AI risks could become infeasible before a definitive \"point of no return\" is reached. Three scenarios could make CHARTS obsolete:</p><ul><li>AI surpasses hardware limitations and becoming unconstrained by available computation. I estimate a 20-40% chance of this occurring around the emergence of transformative AI (TAI), which would render chip supply controls pointless.</li><li>Rapid TAI political takeover leaving humanity unable to regulate AI's growth. I expect a &gt;80% chance of this within one year of TAI.</li><li>Irreversible entanglement with increasingly capable AI systems, which could happen ~2 years pre-TAI. I estimate a 60-80% chance of this.</li></ul><h1><strong>1.&nbsp;Key terms</strong></h1><h2>1.1 What is Transformative AI?</h2><p>This report defines \"transformative artificial intelligence\" (TAI) as computer software possessing sufficient intelligence that its widespread deployment would profoundly transform the world, accelerating economic growth on the scale of the Industrial Revolution. Specifically, TAI refers to software that could drive global economic growth from ~2-3% currently to over 20% per year if utilised in all applications where it would add economic value. This implies the world economy would more than double within four years of TAI's emergence (<a href=\"https://docs.google.com/document/d/1IJ6Sr-gPeXdSJugFulwIpvavc0atjHGM82QjIfUSBGQ/edit#heading=h.oy8zfrwjlptx\"><u>Contra, 2020</u></a>).&nbsp;</p><h2>1.2&nbsp;<i>What is a risk awareness moment?</i></h2><p>This report employs the concept of a 'risk awareness moment' or RAM as conceptualised by Guest (2023), who defines a RAM as \"a point in time, after which concern about extreme risks from AI is so high among the relevant audiences that extreme measures to reduce these risks become possible, though not inevitable.\"</p><p>In other words, a RAM refers to a threshold point where concern about catastrophic or existential threats from advanced AI becomes widespread enough to potentially motivate major policy changes or interventions to reduce these risks. This does not mean such actions become guaranteed after a RAM, but they shift from inconceivable to plausibly on the table for consideration. This conception of the RAM concept informs the present analysis regarding the likelihood that such a threshold may be reached.</p><h2>1.3&nbsp;<i>What type of international safety agreement?</i></h2><p>Oliver Guest\u2019s report (<a href=\"https://forum.effectivealtruism.org/posts/L8GjzvRYA9g9ox2nP/prospects-for-ai-safety-agreements-between-countries\"><u>2023</u></a>) describes a proposed agreement called the 'Collaborative Handling of Artificial Intelligence Risks with Training Standards' (CHARTS). The key features of CHARTS are:</p><ul><li>Prohibiting governments and companies from performing large-scale training runs that are deemed likely to produce powerful but misaligned AI systems. The riskiness of training runs would be determined based on proxies such as total training compute usage or the use of techniques like reinforcement learning.</li><li>Requires extensive verification of compliance through mechanisms like on-chip activity logging, on-site inspections of data centres, and a degree of mechanistic interpretability.</li><li>Cooperating to prevent exports of AI-relevant compute hardware to non-member countries to avoid dangerous training runs being conducted in non-participating jurisdictions.<br>&nbsp;</li></ul><p>This report does not directly evaluate whether the proposed CHARTS agreement would successfully address key governance challenges surrounding transformative AI or appraise its specific strengths and limitations. At first glance, the agreement\u2019s core features appear potentially advantageous for reducing extinction risks from artificial intelligence in the near term. One perspective is that if the agreement enabled an independent governing body to effectively (a) ensure chip manufacturers implement chips capable of recording and storing risky or large-scale training runs, (b) impose sanctions for unauthorised risky training, (c) enforce limits on compute provision to individual labs or governments, and (d) proactively fulfil its oversight duties, I estimate a 75% probability (confidence: medium) that this could significantly delay timelines for transformative AI.&nbsp;</p><p>However, fully evaluating the proposal's merits and weaknesses would necessitate a more in-depth analysis, which I may undertake eventually.</p><h1><strong>2.&nbsp;How much does public sentiment matter?</strong></h1><p>This report's tentative approach will solely focus on forecasting \u2018risk awareness moments\u2019 (RAMs) occurring amongst policy elites in the United States and China. It will not consider RAMs amongst the general public.&nbsp;</p><p>Some empirical evidence challenges the notion that broad public sentiment substantially influences actual policy outcomes in America. A widely-discussed study analysing 1,779 policy issues found that the preferences of average citizens had minimal independent impact on policy when controlling for the preferences of economic elites and business interest groups. The study concluded that average citizens' preferences had a \"near-zero, statistically non-significant\" effect on policy outcomes (<a href=\"https://www.cambridge.org/core/journals/perspectives-on-politics/article/testing-theories-of-american-politics-elites-interest-groups-and-average-citizens/62327F513959D0A304D4893B382B992B\"><u>Gilens &amp; Page, 2014, p. 575</u></a>).&nbsp;</p><p>However, the finding that public opinion has little influence on US policymaking has been critiqued. Re-analysis of the original data showed that on issues where preferences diverge, policy outcomes largely reflect middle-class preferences, not just elites. Another re-examination found enactment rates are similar when either median-income Americans or elites strongly support a policy change (<a href=\"https://www.cambridge.org/core/journals/perspectives-on-politics/article/relative-policy-support-and-coincidental-representation/BBBD524FFD16C482DCC1E86AD8A58C5B\"><u>Enns, 2015</u></a>;&nbsp;<a href=\"https://journals.sagepub.com/doi/10.1177/2053168015608896\"><u>Bashir, 2015</u></a>). The complex interplay between public views and resultant policy action remains an open question, with analyses arriving at differing conclusions. Given the significant unknowns and lack of consensus surrounding public opinion's policy impact, this report will refrain from considering it as a factor.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/trlxeimkumthsf08rugr\"></p><p>Figure 1. From Gilens &amp; Page (2014).</p><p>What about China? Contrary to initial assumptions that public opinion may play a minimal role in policy decisions under China's authoritarian setup, some evidence suggests it does impact policy to a degree, often through concerns about regime legitimacy. In authoritarian contexts like China's, procedural legitimacy derived from democratic processes is weak, amplifying the regime's need for performance legitimacy based on popular living standards (<a href=\"https://blogs.lse.ac.uk/cff/2022/05/20/how-public-opinion-shapes-chinas-foreign-policy/\"><u>Li, 2022</u></a>).&nbsp;</p><p>Prior research indicates China's government actively tracks and responds to public opinion, particularly online sentiment, due to its relevance for performance legitimacy. However, divergences can emerge between state narratives and evolving public attitudes. For example, Chinese public opinion towards the United States deteriorated dramatically from 2016 to 2020. This shift likely reflects reactions to external events rather than just alignment with state propaganda. Further evidence of divergent public attitudes is that negative views towards Australia and the UK were lower than towards the US around this time, despite China\u2019s tensions with those countries (<a href=\"https://academic.oup.com/cjip/article/15/1/27/6548121\"><u>Fang et al., 2022, p. 43</u></a>).</p><p>Rigorous public polling on AI risks is unfortunately scarce in China. One 2021 survey showed that 56% of Chinese respondents viewed AI as \u2018beneficial\u2019 and only 13% as \u2018harmful\u2019. Harmful here did not seem to specify risks of human extinction (<a href=\"https://wrp.lrfoundation.org.uk/2021-report-a-digital-world-perceptions-of-risk-from-ai-and-misuse-of-personal-data/\"><u>The Lloyd's Register Foundation, 2021</u></a>). This feels too insubstantial to warrant much consideration.&nbsp;</p><p>Given the lack of data on public views specifically regarding AI risks, this report will not consider the likelihood of a general RAM in China. It will focus solely on policy elites.</p><h1><strong>3.&nbsp;What is the likelihood that we have a RAM amongst policy elites in the US and China?</strong></h1><p>For a CHARTS agreement to come in place, the US or China must have RAM. I can imagine two scenarios to answer this question:</p><h2>3.1&nbsp;<i>We might be having a RAM right now.</i></h2><p>Let us consider whether the world has already reached a risk awareness moment (RAM) regarding extreme risks from advanced AI systems.</p><p><strong>United States</strong></p><p>Several recent developments provide evidence that we may currently be experiencing a RAM:</p><ul><li>Powerful new LLM systems like GPT-4 and Claude are causing concerns among US Congress and legislators about existential risks from such systems.</li><li>Respected leaders in the field of AI, like Geoffrey Hinton (<a href=\"https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\"><u>New York Times, 2023</u></a>) and Max Tegmark (<a href=\"https://www.wsj.com/tech/ai/ai-expert-max-tegmark-warns-that-humanity-is-failing-the-new-technologys-challenge-4d423bee\"><u>Wall Street Journal, 2023</u></a>), have publicly expressed worries about existential threats, which are holding sway over US legislators.&nbsp;</li><li>Heads of AI labs like Dario Amodei (<a href=\"https://www.washingtonpost.com/technology/2023/07/25/ai-bengio-anthropic-senate-hearing/\"><u>Washington Post, 2023</u></a>) and Sam Altman (<a href=\"https://www.nytimes.com/2023/05/16/technology/openai-altman-artificial-intelligence-regulation.html#:~:text=In%20his%20first%20testimony%20before,over%20A.I.'s%20potential%20harms.\"><u>New York Times, 2023</u></a>) have expressed concerns before a Senate committee.</li><li>[Minor] The \u2018Statement on AI Risk\u2019 (<a href=\"https://www.safe.ai/statement-on-ai-risk\"><u>Center for AI Safety, 2023</u></a>) helped gain significant attention on AI safety.</li><li>More senators seem to be voicing their concerns about risks from AI (<a href=\"https://www.bloomberg.com/news/articles/2023-05-16/a-us-senator-just-used-chatgpt-for-opening-remarks-at-a-hearing\"><u>Bloomberg, 2023</u></a>).</li><li>[Minor] The prospect of American jobs being automated away by LLMs may spur willingness to implement regulations&nbsp;</li></ul><p>However, some reasons against having reached a sufficient RAM threshold:</p><ul><li>Concern may not be sufficiently high about worst-case AI risks like human extinction.</li><li>There is not enough political willingness for more extreme precautionary measures as the proposals laid out in CHARTS.</li><li>Scepticism likely remains about AI existential risks due to their unintuitive nature.&nbsp;</li><li>The flipside of Hinton and Tegmark is that you have others like Yann LeCunn who are not as bullish on the ideas of existential risks from AI (<a href=\"https://www.youtube.com/watch?v=144uOfr4SYA\"><u>Munk Debate, 2023</u></a>).</li><li>Overoptimism and naiveness about US politicians, both from the Republican and Democrat sides, being able to perceive risks from AI rationally.</li></ul><p><i>Given all of this, I think there is a 44% chance that a RAM is occurring amongst policy elites in the United States at the moment (CI: 35% to 53%).&nbsp;</i></p><p>You can play around with the Guesstimate model here&nbsp;<a href=\"https://www.getguesstimate.com/models/23033\"><u>[Link]</u></a> and place different weights on my listed considerations.&nbsp;<br>&nbsp;</p><p><strong>China</strong></p><p>There are some indications that a risk awareness moment regarding AI may be emerging among Chinese policy elites:</p><ul><li>A Chinese diplomat, Zhang Jun, gave a UN speech emphasising human control over AI and establishing risk response mechanisms. This suggests awareness and concern about AI risks among Chinese foreign policy officials (<a href=\"https://aisafetychina.substack.com/p/ai-safety-in-china-1\"><u>Concordia AI, 2023</u></a>)</li><li>Expert advisor Zeng Yi separately warned the UN about existential risks from advanced AI. As a key advisor to China's Ministry of Science and Technology, this likely reflects awareness among influential scientific experts advising the government (<i>ibid</i>).</li><li>China has already enacted vertical regulations targeting algorithm harms like misinformation, demonstrating a willingness to constrain perceived AI risks (<a href=\"https://carnegieendowment.org/2023/02/14/lessons-from-world-s-two-experiments-in-ai-governance-pub-89035\"><u>Carnegie Endowment for International Peace, 2023</u></a>).&nbsp;<ul><li>The&nbsp; \u2018Interim Measures on Generative AI\u2019 can be read here&nbsp;<a href=\"https://www.chinalawtranslate.com/en/comparison-chart-of-current-vs-draft-rules-for-generative-ai/\"><u>[Link]</u></a>.</li></ul></li><li>It might also be the case that China's restrictions aimed at ensuring LLMs do not generate content against state interests could end up slowing down Chinese AI lab capabilities.&nbsp;</li><li>As a totalitarian regime, China may have a lower threshold to justify new regulations constraining AI systems, unlike the US, where policymakers may be more heavily influenced by industry lobbying.<br>&nbsp;</li></ul><p>Factors potentially weighing against a sufficient RAM threshold being reached currently:</p><ul><li>China still lags behind leading Western nations in advanced AI capabilities (<a href=\"https://www.foreignaffairs.com/china/illusion-chinas-ai-prowess-regulation\"><u>Foreign Affairs, 2023</u></a>). This may reduce incentives to prioritise speculative future risks over near-term technological competitiveness.&nbsp;</li><li>There may not yet be enough concern specifically about catastrophic or existential AI risks among Chinese officials, even if general AI risks are increasingly discussed.</li><li>China may have incentives to avoid externally imposed regulations on its domestic chip production capabilities, especially regulations that enable oversight by foreign entities. Specifically, China may seek to preserve unconstrained access to domestic chip fabrication facilities to pursue potential state interests like invading Taiwan, which is a dominant player in cutting-edge semiconductor manufacturing.</li></ul><p><i>Given all of this, I think there is a 57% chance that a RAM is occurring amongst policy elites in China at the moment (CI: 45% to 68%).&nbsp;</i></p><p>Once again, you can play around with this forecast in Guesstimate&nbsp;<a href=\"https://www.getguesstimate.com/models/23036\"><u>[Link]</u></a>.&nbsp;</p><h2>3.2&nbsp;<i>A RAM might be two or three years away.</i></h2><p>Another kind of world that I will briefly think about is one where we are two or three years from a \u2018risk awareness moment\u2019. Concerns about human extinction from advanced AI may not seem sufficiently salient among policymakers to constitute a RAM. AI safety is a relatively novel issue that is not yet well understood, which likely hinders sufficient urgency to prompt major interventions.</p><p>The composition of US policy elites and those surrounding them will likely change to include more people who know about AI Safety. As AI capabilities continue advancing and AI safety advocates enter governmental positions (maybe through different fellowships being offered [<a href=\"https://www.horizonpublicservice.org/fellowship\"><u>Link</u></a>]) and climb its ladder,&nbsp;<strong>I estimate a 60% probability that a RAM will occur by the year 2026 (CI: 48% to 72%)</strong> in the following ways:</p><ul><li>In the US, senior legislators are directly advised about AI existential risks and have vivid intuitions about extinction pathways.</li><li>Serious incidents (\u2018warning shots\u2019 (<a href=\"https://www.alignmentforum.org/posts/hLKKH9CM6NDiJBabC/what-are-the-most-plausible-ai-safety-warning-shot-scenarios\"><u>Kokotajlo, 2020</u></a>)) clearly demonstrate AI systems gaining agency and being able to potentially cause harm or deceive humans (maybe through more incidents like what ARC Evals has published (<a href=\"https://evals.alignment.org/blog/2023-03-18-update-on-recent-evals/\"><u>Alignment Research Centre, 2023</u></a>) or models like AutoGPT.&nbsp;</li><li>Major capability leaps occur, like the emergence of GPT-5 or GPT-6 model or another AI lab releasing an LLM that\u2019s more powerful.</li><li>More influential figures endorse calls for urgent measures like moratoriums on advanced AI.</li></ul><p>Some of these trends could spur a RAM in China on a similar timescale.</p><p>Reasons against this intuition:</p><ul><li>AI ends up becoming a bipartisan issue that roadblocks how long it takes for a RAM to occur.</li><li>Building TAI is seen as so valuable that even if there is a non-zero chance of extinction, governments will still push hard to build it. An analogy of this can be drawn to the Trinity test.&nbsp;</li></ul><p>Play around with the Guesstimate here [<a href=\"https://www.getguesstimate.com/models/23066\"><u>Link</u></a>].</p><h1><strong>4. Conditional on there being a risk awareness moment (RAM), how long would negotiations, on average, for an AI safety agreement to come into place?</strong></h1><p>This report will rely on Oliver Guest's forecast that an agreement between the United States and allies and China limiting advanced AI development could be negotiated approximately four years after a RAM occurs (<a href=\"https://forum.effectivealtruism.org/posts/L8GjzvRYA9g9ox2nP/prospects-for-ai-safety-agreements-between-countries\"><u>Guest, 2023</u></a>).</p><p>As an initial estimate, I would expect countries to act more quickly to establish an agreement if concerns about potentially catastrophic AI risks reach the RAM threshold, perhaps reaching an agreement within 1-2 years. There also appears to be a possibility of an initial agreement being made in principle within this accelerated timeframe, with details sorted out later.</p><p>However, Oliver Guest has conducted more extensive research specifically focused on estimating timelines for reaching a CHARTS agreement. Given this, Oliver\u2019s estimate of approximately four years post-RAM seems a reasonable preliminary estimate to rely on. This will remain open to updating if additional evidence emerges suggesting faster or slower timelines are more probable after a RAM takes place.</p><h1><strong>5.&nbsp;How long does it take for an agreement to come into force and then the actual regulations to take place?</strong></h1><p>Oliver argues that if an international agreement is reached, it may come into force within six months of negotiation, with 70% confidence. However, less information is provided on timelines before key oversight provisions are established in practice.</p><p>For instance, implementing verification of chip manufacturers producing chips capable of recording large training runs would require creating a body able to externally confirm this manufacturing is occurring and that has methods for documenting and storing records of production. How long starting up and initiating enforcement from such an oversight organisation might take is uncertain.</p><p>Certain international agreements have seen verification bodies and activities commence almost instantly after enactment or signing. The Chemical Weapons Convention (CWC) was signed in January 1993 and entered force in April 1997. The Organisation for the Prohibition of Chemical Weapons (OPCW), tasked with verifying CWC compliance, initiated verification work soon after the Treaty took effect (<a href=\"https://www.opcw.org/about-us\"><u>OPCW</u></a>). In fact, several other verification bodies follow a similar pattern. The Comprehensive Nuclear-Test-Ban Treaty (CTBT) was signed in 1996, and the CTBTO Preparatory Commission, which enforces the CTBT, was established in the same year (<a href=\"https://disarmament.unoda.org/wmd/nuclear/ctbt/\"><u>UNODA</u></a>). The Treaty on Open Skies was signed in 1992, and then the Open Skies Consultative Commission came into force in the same year (<a href=\"https://nuke.fas.org/control/os/news/opnskicc.htm\"><u>Federation for American Scientists</u></a>).</p><p>However, the establishment of a verification body does not necessarily mean verification systems would be operational immediately. Based on intuition, the first audit of relevant facilities could take approximately six months to occur after such a body is set up, as creating a new organisation to independently verify labs and states, getting access, and adapting hardware would involve months of preparation and likely roadblocks.</p><p>It is also worth noting that a verification body can predate an agreement, as the International Atomic Energy Agency did relative to the Treaty on the Non-Proliferation of Nuclear Weapons (<a href=\"https://www.iaea.org/topics/non-proliferation-treaty\"><u>IAEA</u></a>). Voluntary industry collaboration could potentially help expedite subsequent government oversight by enabling tracking ahead of regulations. For instance, NVIDIA could opt to voluntarily enable compute monitoring before mandates. Early multi-stakeholder standards bodies and best practices could also ease eventual compliance, though regulations would likely still be required for comprehensive verification.</p><p><i>Overall, while the previous precedents demonstrate timely verification is possible after agreements are enacted, my rough estimate is that fully implementing oversight in practice could still take 6-12 months. Industry goodwill and preparatory work may help accelerate enforcement once formal regulations are in place, shortening the timeline to begin proper enforcement through CHARTS to potentially 1-3 months.</i></p><h1><strong>6.&nbsp;How long till CHARTS is a useful agreement to push for?</strong></h1><p>The initial open question posed by Oliver centred on determining timelines for a \"point of no return\" (PONR), defined as when humanity loses the ability to meaningfully reduce risks from advanced AI systems (<a href=\"https://www.lesswrong.com/posts/JPan54R525D68NoEt/the-date-of-ai-takeover-is-not-the-day-the-ai-takes-over\"><u>Kokotajlo, 2020</u></a>). It seems plausible that negotiating a CHARTS agreement or other international AI cooperation could become infeasible even before a definitive PONR is reached. At the same time, some risk reduction levers may remain viable for a period after that point. A better framing may be asking when a CHARTS agreement no longer becomes useful to pursue.</p><p>Three potential scenarios could prevent a CHARTS agreement from being useful:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgdl93o0gblw\"><sup><a href=\"#fngdl93o0gblw\">[1]</a></sup></span></p><ul><li><strong>Intelligence Beyond Hardware</strong>: AI evolves such that its capabilities surpass hardware limitations. One pathway is an AI system finding efficiency improvements so its capabilities are no longer determined by available computation. Thus, controlling chip manufacturers or regulating compute through CHARTS becomes irrelevant. I estimate the likelihood of this occurring before other scenarios is low, around 20-40%. I expect producing such systems will still require massive chip quantities, making chip regulation deterrents still viable. If this did occur, I would expect it either two years before or after the onset of transformative AI (TAI).</li><li><strong>TAI's Political Takeover</strong>: Within a short timeframe, misaligned and power-driven TAI infiltrates and dominates political systems. This rapid dominance leaves humanity unable to regulate or control AI's growth, exacerbating existential risks. I would expect a political takeover within one year of TAI's emergence.</li><li><strong>Irreversible Entanglement</strong>: The global economy becomes intricately intertwined with AI such that disentangling or regulating AI poses substantial economic risks. I place a high chance of 60-80% that this irreversible entanglement happens before TAI, as increasingly intelligent systems are embedded throughout society. I estimate this could occur around two years pre-TAI as highly capable, but not yet transformative, AI proliferates.</li></ul><p>Predicting when any of these scenarios may occur seems highly challenging. The estimates provided are my best guesses, given current knowledge.</p><p>A recent literature review on TAI timelines (<a href=\"https://epochai.org/blog/literature-review-of-transformative-artificial-intelligence-timelines\"><u>Wynroe et al., 2023</u></a>) found the 10th, 50th, and 90th percentile estimates for TAI emergence were 2025, 2045, and post-2070.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc248t2ysx1i\"><sup><a href=\"#fnc248t2ysx1i\">[2]</a></sup></span>&nbsp;This report does not consider the long timelines at the end of that distribution, given the speculation involved.<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/yn0eqapb4jlrzmrwgrtw\"></p><p>Figure 2. TAI Timelines aggregated. From Wynroe et al. (2023).</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/flzwg8gufiurgoruozag\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/plaqniykfrxyxeza7h8s 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/i8kjmspwz8b8tu01fxd5 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/pvqinlqoklnaq5fzd95n 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/u0rrb14xv6pkiuex0lm7 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/gyjvg3cr9t9hfk6gaokd 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/tcef1sx4khoh6zv2mviw 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/ytwa6lnfxstbgmceruk7 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/qzhjsk9mbumqechwejqx 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/z95gu0pn8sostzqndcud 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hybGfBnkrtL9E3EcS/m88nxfpcuap7vamujidh 1168w\"></figure><h1><strong>7. Will there be time to implement an agreement before CHARTS is no longer useful?</strong></h1><p><strong>With the considerations above, my overall estimate/best guess is that there is at least a 40% chance there will be adequate time to implement a CHARTS agreement before it ceases to be relevant.&nbsp;</strong>I am sceptical that we are in a short-timeline world for transformative AI, and I tentatively believe median timeline scenarios are more likely. Given this, it seems plausible that CHARTS could be negotiated and enforced before becoming obsolete.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkf61wqhahn9\"><sup><a href=\"#fnkf61wqhahn9\">[3]</a></sup></span></p><p>However, I am less certain whether CHARTS specifically would be the most beneficial agreement to prioritise in the long run. Regulating computation could prove valuable in the near term, but more promising opportunities for cooperation may emerge that we would not want to inadvertently impede by over-focusing on CHARTS.</p><h1><strong>Bibliography</strong></h1><p><a href=\"https://docs.google.com/document/d/1MLmzULVrksH8IwAmH7wBpOTyU_7BjBAVl6N63v2vvhM/edit#bookmark=id.j0k8lwbwlmfj\">Can be found in the original report.&nbsp;</a></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngdl93o0gblw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgdl93o0gblw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Of course, these are far from the only scenarios that play out, and there is a lot of speculation on my part as to the likelihood of these occurring.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc248t2ysx1i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc248t2ysx1i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Note this has not considered the new report, \u2018What a compute-centric framework says about takeoff speeds\u2019 (<a href=\"https://www.openphilanthropy.org/research/what-a-compute-centric-framework-says-about-takeoff-speeds/\"><u>Davidson, 2023</u></a>).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkf61wqhahn9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkf61wqhahn9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;This definitely needs an explanation that I have omitted from here. I hope to write something about my own timelines soon.&nbsp;</p></div></li></ol>", "user": {"username": "AryanYadav"}}, {"_id": "kEvx4aHNRFZZrsfmz", "title": "Manifest 2023", "postedAt": "2023-09-06T11:20:19.851Z", "htmlBody": "<p>Manifest 2023 is the first in-person gathering of the forecasting &amp; prediction market community. Chat with the Manifold team, special guests like Nate Silver, Robin Hanson, Dylan Matthews, Zvi Mowshowitz, Katja Grace, Dwarkesh Patel, Aella, Tarek Mansour, Shayne Coplan, and <a href=\"https://manifestconference.net/speakers\">more</a>!</p><p>More info &amp; buy tickets: <a href=\"https://manifestconference.net\">manifestconference.net</a></p><p>WHEN: Sept 22-24</p><p>WHERE: Berkeley, CA</p><p>WHO: Everyone in the forecasting, EA, and LW communities. If you're reading this, you're invited!</p><p>Join the Discord <a href=\"https://discord.gg/jHghsvbUR8\">here</a> :)</p><p>Cosponsored by Kalshi! Other sponsors include: Polymarket, Sovereign, Metaculus.</p><p><i>[Full post with more info </i><a href=\"https://forum.effectivealtruism.org/posts/o8b5F6T7Gmqxn5iws/last-chance-get-tickets-to-manifest-2023-sep-22-24-in\"><i>here</i></a><i>.]</i></p>", "user": {"username": "Saul"}}, {"_id": "o8b5F6T7Gmqxn5iws", "title": "Last Chance: Get Tickets to Manifest 2023! (Sep 22-24 in Berkeley)", "postedAt": "2023-09-06T10:41:06.286Z", "htmlBody": "<p>Ticket prices increase Sept 8.</p><h1>Forecasting Festival \u2014 Hosted by Manifold</h1><p>Manifest 2023 is <strong>the first in-person gathering of the forecasting &amp; prediction market community</strong>. Chat with the Manifold team, special guests like Nate Silver, Robin Hanson, Dylan Matthews, Zvi Mowshowitz, Katja Grace, Dwarkesh Patel, Aella, Tarek Mansour, Shayne Coplan, and <a href=\"http://manifestconference.net/speakers\">more</a>!</p><p>More info &amp; buy tickets:&nbsp;<a href=\"http://manifestconference.net/\">manifestconference.net</a></p><p>WHEN: Sept 22-24</p><p>WHERE: Berkeley, CA</p><p>WHO: Everyone in the forecasting, EA, and LW communities. If you're reading this, you're invited!</p><p>Join the Discord&nbsp;<a href=\"https://discord.gg/jHghsvbUR8\">here</a>&nbsp;:)</p><p>Cosponsored by Kalshi! Other sponsors: Polymarket &amp; Sovereign.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/yojq62gfzxuvxlle5z3a\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/lvbpvxweiamcwvpmpdmz 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/wy0jjlxu61yrkwickckx 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/mqgeloqa9ssoa1fvcrd0 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/hbgkbdufhntjbbagpb1p 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/k2iatb6zjzuchmussxef 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/az7oaoryefqetkydovvl 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/ox2odhiljbfabck1szo7 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/kfhg00g64yfmz51og1ie 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/nppgofs0qwqjisfqnfvs 1620w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/tqrfkalw3ktb8ekv3ale 1790w\"></figure><h1>Why should I come?</h1><p>Forecasting and prediction markets are effective ways of improving our judgement and decision-making \u2014 most people in the Effective Altruism and LessWrong communities will feel right at home at Manifest.</p><p>Here are extra reasons to come:</p><ul><li>you think forecasting &amp; prediction markets are impactful/fun/cool/rational/intriguing/etc</li><li>you want to vibe with other forecasting nerds</li><li>you want to engage &amp; network with the forecasting community (find jobs/recruit hires/see what\u2019s out there)</li><li>you want to meet &amp; chat with the Manifold team and our <a href=\"http://manifestconference.net/speakers\">special guests</a></li><li>you want enjoy the gorgeous Rose Garden Inn</li></ul><figure class=\"image image_resized\" style=\"width:70.91%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/zvyktokrnaldfz7zw2bx\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/emivjtxuzy1vbjgy5w6h 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/z2vaattsfkze7pp7w3fp 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/pepk7jvoaxtbzzhjvobq 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/si4rb4emkd0r1ek9odtl 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/wvwuxntjaikpvg4e1ouf 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/kcaxkkklbbtttdawfwsq 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/orvs5lmuyhzgqahcjfij 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/detuqpdtajqxglmfrjei 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/sgvoacllk1wwmdlsta8f 1620w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/fkyxs7kgsrt9umcsu8iw 1790w\"></figure><ul><li>\u2026or if you like memes?</li></ul><figure class=\"image image_resized\" style=\"width:69.65%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/wohc00ihzw6eoquje1jw\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/hao5iqyhotfkvawpxa4c 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/mtnffntrekuty5h3jit4 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/oudyrqrvw9kwpgdvwwg8 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/iiydwqnnjjkprbpezfgk 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/lrnoqk2cnnriumzbpmnk 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/pnzuvqombozar2ldonfp 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/okb5qs70autlekgjrx6y 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/prldyhmkzrcjsleutb3l 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/ymrfeeaeydizfobim8ku 1620w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/o8b5F6T7Gmqxn5iws/ffom7ralc2kcivhtkqnx 1790w\"></figure><h1>A Day at Manifest</h1><p>Everything\u2019s optional. There will always be a bunch of sessions running concurrently, but this is an example of what your day at Manifest might look like:</p><p>10-11 \u2014 Opening session</p><p>11-12 \u2014 Fireside chat with <strong>Robin Hanson</strong></p><p>12-1 \u2014 Lunch &amp; mingling</p><p>1-2 \u2014 Estimathon: fermi estimation with prizes and steep competition!</p><p>2-3 \u2014 Speed friending: a few chats with other friendly, ambitious forecasting nerds :)</p><p>3-4 \u2014 Break: relax, vibe, unwind, chill, destress, etc</p><p>4-5 \u2014 Manifold Founders Panel with <strong>Austin Chen, Stephen &amp; James Grugett</strong></p><p>5-6 \u2014 Games &amp; markets: chess, poker, and prediction markets!</p><p>6-7 \u2014 Dinner &amp; mingling</p><p>7-8 \u2014 Workshop: \u201cFunctions as Forecasts\u201d with <strong>Ozzie Gooen</strong></p><p>8-12 \u2014 <a href=\"http://murdershebet.com\">Murder She Bet</a>: a murder mystery + a low-tech prediction market = fun!</p><p><i>(Final schedule out soon)</i></p><h1>Where can I buy tickets?</h1><p>Buy tickets &amp; check the most up-to-date info on the <a href=\"http://manifestconference.net/tickets\">Manifest website</a>! Tickets are free for students and employees of forecasting organizations. Prices increase Sept 8!</p>", "user": {"username": "Saul"}}, {"_id": "GqK5S2ApsycdcRwCm", "title": "Decision Theory: A (Normative) Introduction", "postedAt": "2023-09-06T11:46:36.143Z", "htmlBody": "<p>I. Introduction</p><p>Let's define <a href=\"https://plato.stanford.edu/entries/decision-theory/\"><u>decision theory</u></a> as the study of decisions, specifically their effects on outcomes.</p><p>There are three main branches of decision theory: descriptive decision theory (how real agents make decisions), prescriptive decision theory (how real agents should make decisions), and normative decision theory (how ideal agents should make outcomes).</p><p>Since decision theory as a field is too broad to be summarized in one post, I'll primarily focus on normative decision theory and only two-thirds of it.</p><p>Decisions under ignorance, <a href=\"https://en.m.wikipedia.org/wiki/Rational_choice_theory\"><u>rational choice theory</u></a>, <a href=\"https://plato.stanford.edu/entries/bounded-rationality/\"><u>bounded rationality</u></a>, <a href=\"https://en.m.wikipedia.org/wiki/Prospect_theory\"><u>prospect theory</u></a>, <a href=\"https://en.m.wikipedia.org/wiki/Heuristic_(psychology)\"><u>heuristics</u></a>, and the <a href=\"https://en.m.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\"><u>VNM axioms of rational choice</u></a> all deserve separate posts.</p><p>II. Terms and Definitions</p><p>Before we begin with specific procedures in decision theory, let's start with defining some important terms.</p><p>We can define a decision as an act or choice an agent has made and an outcome as a result of such decisions.</p><p>Utility should represent an agent's preference over said outcomes and while may be assigned a cardinal value (such as when the agent is VNM-rational), is still a representation of ordinal preferences.</p><p>Decisions can be made under certainty, risk, or ignorance. The latter two represent when an agent is uncertain of the outcome corresponding to a decision, however, the former in contrast with the latter allows one to assign subjective probabilities to the outcomes.</p><p>In this post, only decisions under certainty and risk will be analyzed.</p><p>III. CDT, EDT, and FDT</p><p>Finally, we can now discuss three types of decision theory algorithms: <a href=\"https://www.lesswrong.com/tag/causal-decision-theory\"><u>Causal Decision theory (CDT)</u></a>, <a href=\"https://www.lesswrong.com/tag/evidential-decision-theory\"><u>Evidential Decision Theory (EDT)</u></a>, and <a href=\"https://www.lesswrong.com/tag/functional-decision-theory\"><u>Functional Decision Theory (FDT)</u></a>.</p><p>To briefly define each procedure, we can say that CDT recommends choosing decisions that cause the best-expected outcome, EDT recommends choosing which decision \"one would prefer to know one would have chosen\", and FDT recommends treating a decision as the output of a \ufb01xed mathematical function that answers the question, \u201cWhich output of this very function would yield the best outcome?\u201d.</p><p>Let's give out some counterexamples to the decision algorithms. First is <a href=\"https://www.lesswrong.com/tag/newcomb-s-problem\"><u>Newcomb\u2019s Problem</u></a>:</p><p>Omega, a being with <i>near-perfect</i> prediction accuracy, shows you two boxes designated A and B. You are given a choice between taking only box B or taking both boxes A and B. You know the following:</p><p>Box A is transparent and always contains a visible $1,000.</p><p>Box B is opaque, and its content has already been set by the predictor:</p><p>If Omega has predicted that you will take both boxes A and B, then box B contains nothing.</p><p>If she has predicted that you will take only box B, then box B contains $1,000,000.</p><p>You do not know what Omega predicted or what box B contains while making the choice.</p><p>In most iterations of Newcomb's problem, CDT recommends two-boxing due to the <a href=\"https://events.umich.edu/event/31432\"><u>dominance principle</u></a>, whereas both EDT and FDT recommend one-boxing because utility is maximized given that Omega predicts you one-box.</p><p>Of course, this can differ with a change of interpretation. If Omega were to be a perfect predictor (or invoke the <a href=\"https://nickbostrom.com/papers/newcomb\"><u>meta-Newcomb's problem</u></a>), then obviously CDT agents would one-box, and if, ceteris paribus, EDT agents were to take into account that two-boxing would not affect the posterior probabilistic prediction of Omega they would two-box.</p><p>Let\u2019s move on to the second counterexample, the <a href=\"https://www.lesswrong.com/tag/smoking-lesion\"><u>Smoking Lesion Problem</u></a>:</p><p>Smoking is strongly correlated with lung cancer, but in the world of the Smoker's Lesion, this correlation is understood to be the result of a common cause: a genetic lesion that tends to cause both smoking and cancer. Once we fix the presence or absence of the lesion, there is no additional correlation between smoking and cancer.</p><p>Suppose you prefer smoking regardless of whether or not you have cancer. Should you smoke?</p><p>CDT and FDT say \"yes\", since smoking in this world has no causal effect on whether or not you get cancer. Naive EDT says \"no\", because smoking is strongly correlated with cancer.</p><p>Standard decision recommendations can of course be altered, e.g. if CDT were to take into account Omega'say pseudo-retrocausal Bayesian updates given information about the agent (such as in meta-Newcomb\u2019s problem), it would recommend acting like an agent who one-boxes and thus one-boxing, and if EDT were to take into account the fact that smoking alone would not affect the posterior probability of developing cancer, it would recommend smoking.</p><p>Conversely, in Naive CDT, only causal chains are considered, while in Naive EDT, only conditional probabilities are considered, while in FDT, both are considered. However, if causal chains and conditional probabilities consider one another then CDT and EDT are equivalent, causing one to choose the dominance principle (assuming probabilities are held constant) and thus two-boxing and smoking.</p><p>But if probabilities update based on the agent's actions, such as in meta-Newcomb\u2019s problem and <a href=\"https://www.lesswrong.com/tag/parfits-hitchhiker\"><u>Parfit's Hitchhiker</u></a>, then CDT=EDT would recommend the maximization of expected utility and thus one-boxing and paying up respectively.</p><p>I don't think defining CDT = EDT is useful, so coming up with a new decision theory may be optimal. Some name ideas include Probabilistic, Bayesian, or Rational Decision Theory.</p><p>In summary, CDT picks out actions that cause utility maximization (e.g. dominance in both Newcomb\u2019s problem and the Smoking Lesion problem), EDT picks out actions that maximize utility given that they were performed, and FDT prescribes decision procedures that an agent whose utility is maximized would follow, though that is not to be confused with a rational agent. Indeed, finding out what a rational agent would do is a mystery worthy of a follow-up post and further discussion and consideration.</p><p>IV. Conclusion</p><p>This post is not intended to endorse any decision theoretic procedure, rather, it is meant to give the reader an idea of the study of decision theory and hopefully its uses.</p><p>Decision theory is a broad study helpful to governments, businesses, and individuals such as me and you. And if I can introduce more people to decision theory, then that is an outcome I would certainly prefer.</p><p>If given enough support, further posts on decision theory as well as more on game theory, economics, and philosophy will be given out.</p>", "user": {"username": "ParetoOptimal"}}, {"_id": "zcHdehWJzDpfxJpmf", "title": "What I would do if I wasn\u2019t at ARC Evals", "postedAt": "2023-09-06T05:17:58.548Z", "htmlBody": "<p><strong>In which:&nbsp;</strong>I list 9 projects that I would work on if I wasn\u2019t busy working on safety standards at ARC Evals, and explain why they might be good to work on.&nbsp;</p><p><strong>Epistemic status:</strong><i>&nbsp;</i>I\u2019m prioritizing getting this out fast as opposed to writing it carefully. I\u2019ve thought for at least a few hours and talked to a few people I trust about each of the following projects, but I haven\u2019t done that much digging into each of these, and it\u2019s likely that I\u2019m wrong about many material facts. I also make little claim to the novelty of the projects. I\u2019d recommend looking into these yourself before committing to doing them. (Total time spent writing or editing this post: ~8 hours.)</p><p><strong>Standard disclaimer:&nbsp;</strong><i>I\u2019m writing this in my own capacity. The views expressed are my own, and should not be taken to represent the views of ARC/FAR/LTFF/Lightspeed or any other org or program I\u2019m involved with.&nbsp;</i></p><p><i>Thanks to Ajeya Cotra, Caleb Parikh, Chris Painter, Daniel Filan, Rachel Freedman, Rohin Shah, Thomas Kwa, and others for comments and feedback.&nbsp;</i></p><h1>Introduction</h1><p>I\u2019m currently working as a researcher on the Alignment Research Center Evaluations Team (ARC Evals), where I\u2019m working on lab safety standards. I\u2019m reasonably sure that this is one of the most useful things I could be doing with my life.&nbsp;</p><p>Unfortunately, there\u2019s a lot of problems to solve in the world, and lots of balls that are being dropped, that I don\u2019t have time to get to thanks to my day job. Here\u2019s an unsorted and incomplete list of projects that I would consider doing if I wasn\u2019t at ARC Evals:</p><ol><li><strong>Ambitious mechanistic interpretability.</strong></li><li><strong>Getting people to write papers/writing papers myself.&nbsp;</strong></li><li><strong>Creating concrete projects and research agendas.&nbsp;</strong></li><li><strong>Working on OP\u2019s funding bottleneck.&nbsp;</strong></li><li><strong>Working on everyone else\u2019s funding bottleneck.&nbsp;</strong></li><li><strong>Running the Long-Term Future Fund.&nbsp;</strong></li><li><strong>Onboarding senior(-ish) academics and research engineers.</strong></li><li><strong>Extending the young-EA mentorship pipeline.&nbsp;</strong></li><li><strong>Writing blog posts/giving takes.&nbsp;</strong></li></ol><p>I\u2019ve categorized these projects into three broad categories and will discuss each in turn below. For each project, I\u2019ll also list who I think should work on them, as well as some of my key uncertainties. Note that this document isn\u2019t really written for myself to decide between projects, but instead as a list of some promising projects for someone with a similar skillset to me. As such, there\u2019s not much discussion of personal fit.&nbsp;</p><p>If you\u2019re interested in working on any of the projects, please reach out or post in the comments below!&nbsp;</p><h2>Relevant beliefs I have</h2><p>Before jumping into the projects I think people should work on, I think it\u2019s worth outlining some of my core beliefs that inform my thinking and project selection:</p><ol><li><strong>Importance of A(G)I safety:&nbsp;</strong>I think A(G)I Safety is one of&nbsp;<a href=\"https://www.safe.ai/statement-on-ai-risk\"><u>the most important problems</u></a> to work on, and all the projects below are thus aimed at AI Safety.&nbsp;</li><li><strong>Value beyond technical research:&nbsp;</strong>Technical AI Safety (AIS) research is crucial, but other types of work are valuable as well. Efforts aimed at improving AI governance, grantmaking, and community building are important and we should give more credit to those doing good work in those areas.&nbsp;</li><li><strong>High discount rate for current EA/AIS funding:&nbsp;</strong>There\u2019s several reasons for this: first, EA/AIS Funders are currently in a unique position due to a surge in AI Safety interest without a proportional increase in funding. I expect this dynamic to change and our influence to wane as additional funding and governments enter this space.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvoi6g0i53vn\"><sup><a href=\"#fnvoi6g0i53vn\">[1]</a></sup></span>&nbsp;Second, efforts today are important for paving the path to future efforts in the future. Third, my timelines are relatively short, which increases the importance of current funding.</li><li><strong>Building a robust EA/AIS ecosystem:&nbsp;</strong>The EA/AIS ecosystem should be more prepared for unpredictable shifts (such as the FTX implosion last year). I think it\u2019s important to robustify parts of the ecosystem, for example by seeding new organizations, building more legible credentials, doing more broad (as opposed to targeted) outreach, and creating new, independent grantmakers.&nbsp;</li><li><strong>The importance of career stability and security:</strong>&nbsp;A lack of career stability hinders the ability and willingness of people (especially junior researchers) to prioritize impactful work over risk-averse, safer options. Similarly, cliffs in the recruitment pipeline due to a lack of funding or mentorship discourage pursuing ambitious new research directions over joining an existing lab. Personally, I\u2019ve often worried about my future job prospects and position inside the community, when considering what career options to pursue, and I\u2019m pretty sure these considerations weigh much more heavily on more junior community members.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmei9j6jxbkk\"><sup><a href=\"#fnmei9j6jxbkk\">[2]</a></sup></span><strong>&nbsp;</strong></li></ol><h1>Technical AI Safety Research</h1><p>My guess is this is the most likely path I\u2019ll take if I were to leave ARC Evals. I enjoy technical research and have had a decent amount of success doing it in the last year and a half. I also still think it\u2019s one of the best things you can do if you have strong takes on what research is important and the requisite technical skills.&nbsp;</p><p><strong>Caveat:&nbsp;</strong>Note that if I were to do technical AI safety research again, I would probably spend at least two weeks figuring out what research I thought was most worth doing,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefl61dn2hwrs\"><sup><a href=\"#fnl61dn2hwrs\">[3]</a></sup></span>&nbsp;so this list is necessarily very incomplete. There\u2019s also a decent chance I would choose to do technical research at one of OpenAI, Anthropic, or Google Deepmind, where my research projects would also be affected by management and team priorities.&nbsp;</p><h2>Ambitious mechanistic interpretability</h2><p>One of the hopes with mechanistic (bottom-up) interpretability is that it might succeed ambitiously: that is, we\u2019re able to start from low-level components and build up to an understanding of most of what the most capable models are doing. Ambitious mechanistic interpretability would clearly be very helpful for many parts of the AIS problem,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7bk8la0ng0u\"><sup><a href=\"#fn7bk8la0ng0u\">[4]</a></sup></span>&nbsp;and I think that there\u2019s a decent chance that we might achieve it. I would try to work on some of the obvious blockers for achieving this goal.&nbsp;</p><p>Here\u2019s some of the possible broad research directions I might explore in this area:</p><ul><li><strong>Defining a language for explanations and interpretations.&nbsp;</strong>Existing explanations are specified and evaluated in pretty ad-hoc ways. We should try to come up with a language that actually captures what we want here. Both Geiger and Wu\u2019s causal abstractions and our Causal Scrubbing paper have answers to this, but both are unsatisfactory for several reasons.</li><li><strong>Metrics for measuring the quality of explanations.&nbsp;</strong>How do we judge how good an explanation is? So far, most of the metrics focus on the&nbsp;<i>extensional equality</i> (that is, how well the circuit matches their input-output behavior), but there are many desiderata besides that. Does percent loss recovered (or other input-output only criteria) suffice for recovering good explanations? If not, can we construct examples where it fails?</li><li><strong>Finding the correct units of analysis for neural networks.&nbsp;</strong>It\u2019s not clear what the correct low-level units of analysis are inside of neural networks. For example, should we try to understand individual neurons, clusters of neurons, or linear combinations of neurons? It seems pretty important to figure this out in order to e.g automate mechanistic interpretability.&nbsp;</li><li><strong>Pushing the Pareto frontier on quality &lt;&gt; realism of explanations.</strong>&nbsp;A lot of manual mechanistic interpretability work focuses&nbsp;<i>primarily&nbsp;</i>on scaling explanations to larger models, as opposed to more complex tasks or comprehensive explanations, which I think are more important. In order for ambitious mechanistic interpretability to work out, we need to understand the behavior of the networks to a&nbsp;<i>really</i> high degree, instead of e.g. the ~50% loss recovered we see when performing&nbsp;<a href=\"https://www.lesswrong.com/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing\"><u>Causal Scrubbing</u></a> from the&nbsp;<a href=\"https://arxiv.org/abs/2211.00593\"><u>Indirect Object Identification</u></a> paper. At the same time, existing mech interp work continues to primarily focus on simple algorithmic tasks, which seems like it misses out on most of the interesting behavior of the neural networks.&nbsp;</li></ul><p><strong>How you can work on it:&nbsp;</strong>Write up a research agenda and do a project with a few collaborators, and then start scaling up from there. Also, consider applying for the OpenAI or Anthropic interpretability teams.&nbsp;</p><p><strong>Core uncertainties:&nbsp;</strong>Is the goal of ambitious mechanistic interpretability even possible? Are there other approaches to interpretability or model psychology that are more promising?&nbsp;</p><h2>Late stage project management and paper writing</h2><p>I think that a lot of good AIS work gets lost or forgotten due to a lack of clear communication.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4unnwspqp02\"><sup><a href=\"#fn4unnwspqp02\">[5]</a></sup></span>&nbsp; Empirically, I think a lot of the value I provided in the last year and a half has been by helping projects get out the door and into a proper paper-shaped form. I\u2019ve done this to various extents for the modular arithmetic grokking paper, the follow-up work on universality, the causal scrubbing posts, the ARC Evals report, etc. (This is also a lot of what I\u2019m doing at ARC Evals nowadays.)&nbsp;</p><p>I\u2019m not sure exactly how valuable this is relative to just doing more technical research, but it does seem like there are many, many ideas in the community that would benefit from a clean writeup. While I do go around telling people that they should write up more things, I think I could also just&nbsp;<i>be&nbsp;</i>the person writing these things up.&nbsp;</p><p><strong>How you can work on it:&nbsp;</strong>find an interesting mid-stage project with promising preliminary results and turn it into a well-written paper. This probably requires some amount of prior paper-writing experience, e.g. from academia.</p><p><strong>Core uncertainties:&nbsp;</strong>How likely is this problem to resolve itself, as the community matures and researchers get more practice with write-ups? How much value is there in actually doing the writing, and does it have to funge against technical AIS research?&nbsp;</p><h2>Creating concrete projects and research agendas</h2><p>Both concrete projects and research agendas are very helpful for onboarding new researchers (both junior and senior) and for helping to fund more relevant research from academia. I claim that one of the key reasons mechanistic interpretability has become so popular is an abundance of concrete project ideas and intro material from Neel Nanda, Callum McDougal, and others. Unfortunately, the same cannot really be said for many other subfields; there isn\u2019t really a list of concrete project ideas for say, capability evals or deceptive alignment research.&nbsp;</p><p>I\u2019d probably start by doing this for either empirical ELK/generalization research or high-stakes reliability/relaxed adversarial training research,&nbsp;<i>while</i> also doing research in the area in question.&nbsp;</p><p>I will caveat that I think many newcomers write these research agendas with insufficient familiarity of the subject matter. I\u2019m reluctant to encourage more people without substantial research experience to try to do this; my guess is the minimal experience is somewhere around one conference paper\u2013level project and an academic review paper of a related area.&nbsp;</p><p><strong>How you can work on it:&nbsp;</strong>Write a list of concrete projects or research agenda in a subarea of AI safety you\u2019re familiar with. As discussed before, I wouldn\u2019t recommend attempting this without significant amounts of familiarity with the area in question.&nbsp;</p><p><strong>Core uncertainties:&nbsp;</strong>Which research agendas are actually good and worth onboarding new people onto? How much can you actually contribute to creating new projects or writing research agendas in a particular area without being one of the best researchers in that area?</p><h1>Grantmaking</h1><p>I think there are significant bottlenecks in the EA-based AI Safety (AIS) funding ecosystem, and they could be addressed with a significant but not impossible amount of effort. Currently, the Open Philanthropy project (OP) gives out ~$100-150m/year to longtermist causes (maybe around $50m to technical safety?),<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0v4bdammng1\"><sup><a href=\"#fn0v4bdammng1\">[6]</a></sup></span>&nbsp;and this seems pretty small given its endowment of maybe ~$10b. On the other hand, there just isn\u2019t much OP-independent funding here; SFF maybe gives out ~$20m/year,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbzdp0pk7fsj\"><sup><a href=\"#fnbzdp0pk7fsj\">[7]</a></sup></span>&nbsp;LTFF gives out $5-10m a year (and is currently having a bit of a funding crunch), and Manifund is quite new (though it still has ~$1.9M according to its website).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyi6rkq8kcpn\"><sup><a href=\"#fnyi6rkq8kcpn\">[8]</a></sup></span></p><p><strong>Caveat:&nbsp;</strong>I\u2019m not sure who exactly should work in this area. It seems overdetermined to me that we should have more technical people involved, but a lot of the important things to do to improve grantmaking are not technical work and do not necessitate technical expertise.</p><h2>Working on Open Philanthropy\u2019s Funding Bottlenecks</h2><p><i>(Note that I do not have an offer from OP to work with them; this is more something that I think is important and worth doing as opposed to something I can definitely do.)</i></p><p>I think that the OP project is giving way less money to AI Safety than it should be under reasonable assumptions. For example, funding for AI Safety probably comes with a significant discount rate, as it\u2019s widely believed that we\u2019ll see an influx of funding from new philanthropists or from governments, and also it seems plausible that our influence will wane as governments get involved.&nbsp;</p><p>My impression is mainly due to grantmaker capacity constraints; for example, Ajeya Cotra is currently the&nbsp;<i>only&nbsp;</i>evaluator for technical AIS grants. This can be alleviated in several ways:</p><ul><li>Most importantly, working at OP on one of the teams that does AIS grantmaking.&nbsp;</li><li>Helping OP design and run more scalable grantmaking programs that don\u2019t significantly compromise on quality. This probably requires working with them for a few months; just creating the RFP doesn\u2019t really address the core bottleneck.</li><li>Creating&nbsp;<i>good</i> scalable alignment projects that can reliably absorb lots of funding.&nbsp;</li></ul><p><strong>How you can work on it:&nbsp;</strong><a href=\"https://www.openphilanthropy.org/careers/general-application/\">Apply to work for Open Phil</a>. Write RFPs for Open Phil and help evaluate proposals. More ambitiously, create a scalable, low-downside alignment project that could reliably absorb significant amounts of funding.&nbsp;</p><p><strong>Core uncertainties:&nbsp;</strong>To what extent is OP actually capacity constrained, as opposed to pursuing a strategy that favors saving funding for the future? How much of OP\u2019s decision comes down to different beliefs about e.g. takeoff speeds? How good is broader vs more targeted, careful grantmaking?&nbsp;</p><h2>Working on the other EA funders\u2019 funding bottlenecks</h2><p>Unlike OP, which is primarily capacity constrained, the remainder of the EA funders are funding constrained. For example,&nbsp;<a href=\"https://www.lesswrong.com/posts/gRfy2Q2Pg25a2cHyY/ltff-and-eaif-are-unusually-funding-constrained-right-now\"><u>LTFF currently has a serious funding crunch</u></a>. In addition, it seems pretty bad for the health of the ecosystem if OP funds the vast majority of all AIS research. It would be significantly healthier if there were counterbalancing sources of funding.&nbsp;</p><p>Here are some ways to address this problem: First and foremost, if you have very high earning potential, you could earn to give. Second, you can try to convince an adjacent funder to significantly increase their contributions to the AIS ecosystem. For example,&nbsp;<a href=\"https://www.schmidtfutures.com/\"><u>Schmidt Futures</u></a> has historically given significant amounts of money to AI Safety/Safety-adjacent academics, it seems plausible that working on their capacity constraints could allow them to give more to AIS in general. Finally, you could successfully fundraise for LTFF or Manifund, or start your own fund and fundraise for that.</p><p><strong>How you can work on it:&nbsp;</strong>Convince an adjacent grantmaker to move into AIS. Fundraise for AIS work for an existing grantmaker or create and fundraise for a new fund. Donate a lot of money yourself.&nbsp;</p><p><strong>Core uncertainties:&nbsp;</strong>How tractable is this, relative to alleviating OP\u2019s capacity bottleneck? How likely is this to be fixed by default, as we get more AIS interest?<strong>&nbsp;</strong>How much total philanthropic funding would be actually interested in AIS projects? How valuable is a grantmaker who potentially doesn\u2019t share many of the core beliefs of the AIS ecosystem?</p><h2>Chairing the Long-Term Future Fund</h2><p><i>(Note that while I am an LTFF guest fund manager and have spoken with fund managers about this role, I do not have an offer from LTFF to chair the fund; as with the OP section, this is more something that I think is important and worth doing as opposed to something I can definitely do.)</i></p><p>As part of&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching\"><u>the move to separate the Long-Term Future from Open Philanthropy</u></a>, Asya Bergal plans to step down as LTFF Chair in October. This means that the LTFF will be left without a chair.&nbsp;</p><p>I think the LTFF serves an important part of the ecosystem, and it\u2019s important for it to be run well. This is both because of its independent status from OP and because it\u2019s the<i>&nbsp;</i>primary source of small grants for independent researchers. My best guess is that a well-run LTFF (even) could move $10m a year. On the other hand, if the LTFF fails, then I think this would be&nbsp;<i>very&nbsp;</i>bad for the ecosystem.&nbsp;</p><p>That being said, this seems like a pretty challenging position; not only is the LTFF currently very funding constrained (and with uncertain future funding prospects) and its position in&nbsp;<a href=\"https://ev.org/\"><u>Effective Ventures</u></a> may limit ambitious activities in the future.&nbsp;</p><p><strong>How you can work on it:&nbsp;</strong>Fill in&nbsp;<a href=\"https://docs.google.com/forms/d/1JXgiPmvkaxoVJQQSVgrHA0AmNXQTrZQ_ZqCQxekKQ0U/edit?ts=64f0edf1\"><u>this Google form</u></a> to express your interest.</p><p><strong>Core uncertainties:&nbsp;</strong>Is it possible to raise significant amounts of funding for LTFF in the long run, and if so, how? How should the LTFF actually be run?&nbsp;</p><h1>Community Building</h1><p>I think that the community has done an incredible job of field building amongst university students and other junior/early-career people. Unfortunately, there\u2019s a comparative lack of senior researchers in the field, causing a massive shortage of both research team leads and a mentorship shortage. I also think that recruiting senior researchers and REs to do AIS work is valuable in itself.&nbsp;</p><h2>Onboarding senior academics and research engineers</h2><p>The clearest way to get more senior academics or REs is to directly try to recruit them. It\u2019s possible the best way for me to work on this is to go back to being a PhD student, and try to organize workshops or other field building projects. Here are some other things that might plausibly be good:</p><ul><li>Connecting senior academics and REs with professors or other senior REs, who can help answer more questions and will likely be more persuasive than junior people without much legible credentials. Note that I don\u2019t recommend doing this unless you have academic credentials and are relatively senior.&nbsp;</li><li>Creating research agendas with concrete projects and proving their academic viability by publishing early stage work in those research agendas, which would significantly help with recruiting academics.&nbsp;</li><li>Create concrete research projects with heavy engineering slants and with clear explanations for&nbsp;<i>why</i> these projects are alignment relevant, which seems to be a significant bottleneck for recruiting engineers.&nbsp;</li><li>Normal networking/hanging out/talking stuff.</li><li>Being a PhD student and influencing your professor/lab mates. My guess is the highest impact here is to do a PhD at a location with a small number of AIS-interested researchers, as opposed to going to a university without any AIS presence.</li></ul><p>Note that senior researcher field building has gotten more interest in recent times; for example, CAIS has run a&nbsp;<a href=\"https://www.safe.ai/philosophy-fellowship\"><u>fellowship for senior philosophy PhD students and professors</u></a> and Constellation has run a series of&nbsp;<a href=\"https://awairworkshop.org/\"><u>workshops for AI researchers</u></a>. That being said, I think there\u2019s still significant room for more technical people to contribute here.&nbsp;</p><p><strong>How you can work on it:&nbsp;</strong>Be a technical AIS researcher with interest in field building, and do any of the projects listed above. Also consider becoming a PhD student.</p><p><strong>Core uncertainties:&nbsp;</strong>How good is it to recruit more senior academics relative to recruiting many more junior people? How good is research or mentorship if it\u2019s not targeted directly at the problems I think are most important?</p><h2>Extending the young EA/AI researcher mentorship pipeline</h2><p>I think the young EA/AI researcher pipeline does a great job getting people excited about the problem and bringing them in contact with the community, a fairly decent job helping them upskill (mainly due to MLAB variants, ARENA, and Neel Nanda/Callum McDougal\u2019s mech interp materials), and a mediocre job of helping them get initial research opportunities (e.g. SERI MATS, the ERA Fellowship, SPAR). However, I think the conversion rate from that level into actual full-time jobs doing AIS research is quite poor.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqpn2fakmo8\"><sup><a href=\"#fnqpn2fakmo8\">[9]</a></sup></span></p><p>I think this is primarily due to a lack of research mentorship for junior and/or research management capacity at orgs, and exacerbated by a lack of concrete projects for younger researchers to work on independently. The other issue is that many junior people can overly fixate on explicit AIS-branded programs. Historically, all the AIS researchers who\u2019ve been around for more than a few years got there without going through much of (or even any of) the current AIS pipeline. (See also the discussion in&nbsp;<a href=\"https://www.lesswrong.com/posts/HACcn8roty9KBAWzZ/evaluations-of-new-ai-safety-researchers-can-be-noisy\"><i><u>Evaluations of new AI safety researchers can be noisy</u></i></a>.)</p><p>Many of the solutions here look very similar to ways to onboard senior academics and research engineers, but there are a few other ones:</p><ul><li>Encourage and help promising researchers pursue PhDs.</li><li>Creating and funding more internship programs in academia, to use pre-existing research mentorship capacity.</li><li>Run more internship or fellowship programs that lead directly to full-time jobs, in collaboration with (or just from) AIS orgs.</li><li>Come up with a promising AIS research agenda, and then work at an org and recruit junior researchers.</li></ul><p>In addition, you could mentor more people yourself if you're currently working as a senior researcher!</p><p><strong>How you can work on it:&nbsp;</strong>Onboard more senior people into AIS. Encourage more senior researchers to mentor more new researchers. Create programs that make use of existing mentorship capacity, or that lead more directly to full-time jobs at AIS orgs.&nbsp;</p><p><strong>Core uncertainties:&nbsp;</strong>How valuable are more junior researchers compared to more senior ones? How long does it take for a junior researcher to reach certain levels of productivity? How bad are the bottlenecks, really, from the perspective of orgs? (E.g. it doesn\u2019t seem implausible to me that the most capable and motivated young researchers are doing fine.)</p><h2>Writing blog posts or takes in general</h2><p>Finally, I do enjoy writing a lot, and I would like to have the time to write a lot of my ideas (or even other people\u2019s ideas) into blog posts.&nbsp;</p><p>Admittedly, this is primarily personal satisfaction\u2013motivated and less impact-driven, but I do think that writing things (and then talking to people about them) is a good way to make things happen in this community. I imagine that the primary audience of these writeups will be other alignment researchers, and not the general LessWrong audience.&nbsp;</p><p>Here\u2019s an incomplete list of blog posts I started in the last year that I unfortunately didn\u2019t have the time to finish:</p><ul><li>Ryan Greenblatt\u2019s takes on why we should do ambitious mech interp (and avoid narrow or limited mech interp), which I broadly agree with.&nbsp;</li><li>Why most techniques for AI control or alignment would fail if a very powerful unaligned AI (an \u2018alien jupiter brain\u2019) manifested inside your datacenter, and why that might be okay anyways.</li><li>Why a lot of methods of optimizing or finetuning pretrained models (RLHF, BoN, quantilization, DPO, etc) are basically equivalent modulo (in theory) optimization difficulties or priors, and why people\u2019s intuitions on differences between them likely come down to imagining different amounts of optimization power applied by different algorithms. (And my best guess as to the reasons for why they are significantly different in practice.)</li><li>The case for related work sections.&nbsp;</li><li>There are (very) important jobs besides technical AI research and how we as the community could do a better job at not discouraging people to take them.&nbsp;</li><li>Why the community should spend 50% less time talking about explicit status considerations.&nbsp;</li></ul><p>There\u2019s some chance I\u2019ll try to write more blog posts in my spare time, but this depends on how busy I am otherwise.</p><p><strong>How you can work on it:&nbsp;</strong>Figure out areas where people are confused, come up with takes that would make them less confused or find people with good takes in those areas, and write them up into clear blog posts.&nbsp;</p><p><strong>Core uncertainties:&nbsp;</strong>How much impact do blog posts and writing have in general, and how impactful has my work been in particular? Who is the intended audience for these posts, and will they actually read them?</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvoi6g0i53vn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvoi6g0i53vn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Anecdotally, it\u2019s been decently easy for AIS orgs such as ARC Evals and FAR AI to raise money from independent, non-OP/SFF/LTFF sources this year.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmei9j6jxbkk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmei9j6jxbkk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Aside from the impact-based arguments, I also think it\u2019s pretty bad from a deontological standpoint to convince many people to drop out or make massive career changes with explicit or implicit promises of funding and support, and then pull the rug from under them.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnl61dn2hwrs\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefl61dn2hwrs\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In fact, it seems very likely that I\u2019ll do this anyway, just for the value of information.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7bk8la0ng0u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7bk8la0ng0u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For example, a high degree of understanding would provide ways to detect deceptive alignment, elicit latent knowledge, or provide better oversight; a&nbsp;<i>very</i> high degree of understanding may even allow us to do microscope or well-founded AI.&nbsp;</p><p><br>This is not a novel view; it\u2019s also discussed under different names in other blog posts such as&nbsp;<a href=\"https://www.alignmentforum.org/posts/uvEyizLAGykH8LwMx/fundamental-vs-applied-mechanistic-interpretability-research\"><i><u>'Fundamental' vs 'applied' mechanistic interpretability research</u></i></a>,&nbsp;<a href=\"https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability\"><i><u>A Longlist of Theories of Impact for Interpretability</u></i></a>, and&nbsp;<a href=\"https://transformer-circuits.pub/2023/interpretability-dreams/index.html\"><i><u>Interpretability Dreams</u></i></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4unnwspqp02\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4unnwspqp02\">^</a></strong></sup></span><div class=\"footnote-content\"><p>As the worst instance of this, the best way to understand a lot of AIS research in 2022 was \u201chang out at lunch in Constellation\u201d.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0v4bdammng1\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0v4bdammng1\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The grants database lists ~$68m worth of public grants given out in 2023 for Longtermism/AI x-risk/Community Building (Longtermism), of which ~$28m was given to AI x-risk and ~$32m was given to community building. However, OP gives out significant amounts of money via grants that aren\u2019t public.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbzdp0pk7fsj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbzdp0pk7fsj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is tricky to estimate since the SFF has given out&nbsp;<i>significantly</i> more money in the first half 2023 (~$21m) than it has in all 2022 (~$13m).&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyi6rkq8kcpn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyi6rkq8kcpn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>CEA also gives out a single digit million worth of funding every year, mainly to student groups and EAGx events.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqpn2fakmo8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqpn2fakmo8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This seems quite unlikely to be my comparative advantage, and it\u2019s not clear it\u2019s worth doing at all \u2013 for example, many of the impressive young researchers in past generations have made it through without even the equivalent of SERI MATS.</p></div></li></ol>", "user": {"username": "Lawrence Chan"}}, {"_id": "a6A3AC9hoEMKQvpyo", "title": "Malaria bednet and medicine overlap", "postedAt": "2023-09-05T23:55:52.236Z", "htmlBody": "<h1>Summary</h1><p>The Against Malaria Foundation (AMF) or Malaria Consortium (MC) both work to prevent malaria, often in the same countries. Mismodelling how malaria bednet use and chemoprevention overlap in a population or misattributing credit for lives saved when both AMF and MC are targeting the same population can lead to either overestimating or underestimating the number of lives saved by each charity, and therefore their cost-effectiveness. I illustrate how. My recommendations are:</p><ol><li>Be explicit about how the intervention will apply across a target population, and consider other possibilities. The intervention might not apply statistically independently of individuals' risks or how much each individual would benefit from your intervention.</li><li>When estimating the impacts of multiple organizations working towards a common or mutually dependent outcome that you and those you funge with are funding or otherwise supporting, keep in mind the possibility of double-counting or undercounting impact. Credited impacts, including for cost-effectiveness estimates, should sum to 100%. You could use <a href=\"https://en.wikipedia.org/wiki/Shapley_value\">Shapley values</a> explicitly, but even if you don't, thinking about them can be helpful.</li></ol><p>To be clear, I just happened to be thinking of malaria when I came up with these. These types of problems certainly aren't unique to malaria or global health and development, and I don't think they're uncommon in impact and cost-effectiveness estimation in EA. However, they could plausibly make the most difference for global health and development, because of very similar estimated cost-effectiveness across <a href=\"https://www.givewell.org/charities/top-charities\">GiveWell's top charities</a>, currently each $4,000 to $5,500 per life saved.</p><p>&nbsp;</p><h1>Background</h1><p>The Against Malaria Foundation (AMF) and Malaria Consortium (MC) both work on preventing malaria and mortality by malaria. Both are<a href=\"https://www.givewell.org/charities/top-charities\"><u> GiveWell top charities</u></a>, AMF for its long-lasting insecticide-treated net (LLIN or bednet) program, and MC for its seasonal malaria chemoprevention (SMC or medicine) program. When I refer to AMF and MC, I mean these respective programs.</p><p>GiveWell assumes a 79% relative reduction in risk of death for treated children from malaria through SMC, through&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1otZFJSMM8yH5R6DkBIYmlsWy5T7VHsX_E7wDF7HSLaM/edit#gid=791021775&amp;range=A27\"><u>a 79% reduction in malaria cases</u></a>, and&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1otZFJSMM8yH5R6DkBIYmlsWy5T7VHsX_E7wDF7HSLaM/edit#gid=791021775&amp;range=A67\"><u>a reduction in malaria deaths proportional to the reduction in incidence</u></a>. This is effectively both relative to bednet use and relative to no bednet use. Similarly, GiveWell assumes a&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1otZFJSMM8yH5R6DkBIYmlsWy5T7VHsX_E7wDF7HSLaM/edit#gid=1364064522&amp;range=A51\"><u>39% to 56% (24% in Chad)</u></a> reduction in malaria mortality for children under 5 from bednet use, again effectively both relative to SMC treatment and no SMC treatment. (GiveWell also accounts for reduction in malaria in the untreated.)</p><p>If someone has already received either a bednet (and uses it) or antimalarial medicine, then they are at much lower risk of contracting malaria and dying from malaria, so the extra impact of giving them the other should be lower than if they didn't already receive the first (assuming they would actually use the bednet). To account for this, we can use malaria incidence and mortality estimates that already reflect bednet use or SMC treatment (other than what extra the charity under consideration will do), or adjust our estimates for them.</p><p>Then, ignoring effects on the untreated for simplicity, if malaria incidence and mortality rate estimates already reflect expected bednet use (but not SMC, for simplicity) in the target population, then the most straightforward way of estimating the impact of SMC for MC in the population is by multiplying by the 79% reduction from SMC and by the share of the target population who would receive SMC. Similarly, if malaria incidence and mortality rate estimates already reflect expected SMC treatment (but not bednet use, for simplicity) in the target population, then the most straightforward way of estimating the impact of delivering bednets for AMF in the population is by multiplying by the % reduction from bednet use and by the share of the target population who would use bednets. This is, as far as I can tell, equivalent to how GiveWell estimates the impact on malaria incidence and mortality of AMF and MC, aside from their further adjustments for counterfactual bednet use for AMF and counterfactual SMC treatment for MC, as well as effects on the untreated (and other non-mortality effects).</p><p>However, those estimates and their underlying assumptions can be wrong, and this can lead to double-counting and overestimating some lives saved or underestimating lives saved. There are two main ways I can imagine misestimating:</p><ol><li>Mismodelling the overlap between bednet use and SMC treatment among a target population.</li><li>Attributing credit for a life saved to AMF and MC in a way that doesn\u2019t add up to 100%.</li></ol><p>Each can lead to either overestimating or underestimating lives saved. I illustrate these possibilities below.</p><p>&nbsp;</p><h1>Mismodelling the overlap</h1><p>If you mismodel the overlap in populations getting SMC and those using bednets even if you get the totals/shares for each right, you can double-count or undercount. For example, if you assume children get bednets and SMC independently of each other, as I think GiveWell has done, but children who get one are disproportionately likely to get the other (maybe their parents are more motivated to prevent malaria, or face fewer barriers in getting both), then you'll double-count some potential averted deaths. On the other hand, if children getting bednets are less likely to get SMC than children not getting bednets (or vice versa), in case the parents or distributors believe one is enough, then the assumption of independence will cause you to miss some potential averted deaths. If receiving either a bednet or SMC&nbsp;<i>makes</i> someone less likely to get the other, this would tend to displace the other to someone who was less likely to be protected by either, which would decrease overlap and increase reduction in deaths. On the other hand, they may be delivered&nbsp;<i>jointly</i> within a population, as MC does deliver both SMC and bednets, and if they do so together, they would plausibly increase overlap and decrease the reduction in deaths.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrfrj4m6ioa\"><sup><a href=\"#fnrfrj4m6ioa\">[1]</a></sup></span></p><p>To illustrate more concretely with mostly made-up numbers, suppose SMC reduces the average recipient's risk by 80% (retaining 20% of the risk), both with and without bednets, and bednets reduce the average recipient's risk by 30% (retaining 70% of the risk), both with and without SMC, and we ignore protection for non-recipients. If half of the (target) population gets bednets and half gets SMC, then this is compatible with, comparing to neither bednets nor SMC:</p><ol><li>49% overall reduction in risk, with half of the population getting bednets and half getting SMC, independently of one another, so we have 25% unprotected, 25% doubly protected, 25% only protected by bednets and 25% only protected by SMC.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefr4kkqovncan\"><sup><a href=\"#fnr4kkqovncan\">[2]</a></sup></span></li><li>55% overall reduction in risk, with bednets for one half, and SMC for the other half, completely disjoint subpopulations.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefl1s35k56te\"><sup><a href=\"#fnl1s35k56te\">[3]</a></sup></span></li><li>47.3% overall reduction in risk, with bednets and SMC for one half of the population, and the other half completely unprotected.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxbrh35s09ae\"><sup><a href=\"#fnxbrh35s09ae\">[4]</a></sup></span></li></ol><p>49% is very close to both 55% and 47.3%, so it seems like it shouldn\u2019t make much difference how we assume bednets and SMC are distributed throughout the population, at least for this hypothetical.</p><p>However, if SMC and bednets are being distributed to the same place, we're also interested in the&nbsp;<i>marginal impact compared to just one of them</i>. Just one half of the population covered by SMC and no one covered by bednets would be a 40% reduction in risk compared to no protection.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref88ky7tnjwso\"><sup><a href=\"#fn88ky7tnjwso\">[5]</a></sup></span>&nbsp;Just one half of the population covered by bednets and no one covered by SMC would be a 15% reduction in risk compared to no protection.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefu5dp9jn4kx\"><sup><a href=\"#fnu5dp9jn4kx\">[6]</a></sup></span></p><p>If mortality is proportional to incidence, as GiveWell assumes, then the additional percentage point (ppt) reduction in mortality from both bednets and SMC compared to one would be between, by taking differences:</p><ol><li>7.3 ppts to 15 ppts, for bednets and SMC compared to just SMC, with 9 ppts for the independence assumption, and</li><li>32.3 ppts to 40 ppts, for bednets and SMC compared to just bednets, with 35 ppts for the independence assumption.</li></ol><p>If 1000 children would have died with one of bednets or SMC covering half of the population and no one with both, then, compared to just SMC, using both together would save another 73 to 150 children, or 90 with the independence assumption, or compared to just bednets, another 323 to 400 children, or 350 with the independence assumption.</p><p>Then, in this hypothetical, the true value for AMF could be up to 67% higher or 19% lower than with the independence assumption. On the other hand, the true value for SMC would only be up to 14% higher or 8% lower.</p><p>Note that this is just an illustration, and ignores protection for those not directly treated or using bednets. The actual values could go even further in either direction, or be more constrained near the independence assumption.</p><p>A further possibility is that MC&nbsp;<i>causes</i> people to be more or less likely to&nbsp;<i>use</i> a bednet (as often or properly) than otherwise, without displacing that bednet for use to others. They get a bednet, but use it more or less than otherwise. Then, we would underestimate or overestimate bednet use in the counterfactual of MC\u2019s intervention relative to MC not delivering SMC, and so underestimate or overestimate MC\u2019s impact. Either seems plausible. MC could further promote or instruct on bednet use to people along with SMC delivery, potentially making bednet use more likely. Or, people receiving SMC may think SMC already offers relatively good protection, licensing them to use their bednets less (or parents to protect their children with bednets less). In this case, MC should be seen as having a causal effect on bednet use independently of the number of distributed bednets.</p><p>&nbsp;</p><h1>Credit assignment</h1><p>AMF and MC work in some of the same countries, like Togo and Nigeria, and some of the same regions in these countries (<a href=\"https://docs.google.com/spreadsheets/d/1otZFJSMM8yH5R6DkBIYmlsWy5T7VHsX_E7wDF7HSLaM/edit#gid=1364064522\"><u>AMF sheet</u></a>,<a href=\"https://docs.google.com/spreadsheets/d/1otZFJSMM8yH5R6DkBIYmlsWy5T7VHsX_E7wDF7HSLaM/edit#gid=791021775\"><u> MC sheet</u></a>).\u200b\u200b I don\u2019t know whether or not or to what extent AMF and MC work in the same cities, towns or villages, though, or otherwise target some of the same people.&nbsp;<a href=\"https://files.givewell.org/files/DWDA%202009/Malaria%20Consortium/Malaria_Consortium_Net_target_project_report_Nigeria_2020_Redacted.pdf\"><u>MC apparently takes bednet coverage into account in deciding where to work</u></a> (AMF might take SMC work into account, too, but I didn't check), and GiveWell makes adjustments in its cost-effectiveness estimates for&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/18ROI6dRdKsNfXg5gIyBa1_7eYOjowfbw5n65zkrLnvc/edit#gid=1364064522\"><u>AMF based on SMC coverage</u></a>, and&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/18ROI6dRdKsNfXg5gIyBa1_7eYOjowfbw5n65zkrLnvc/edit#gid=791021775&amp;range=A182\"><u>MC for otherwise underestimated net coverage in Nigeria</u></a>. However, it\u2019s not clear to me to what extent this data reflects the&nbsp;<i>immediate</i> plans of the other charity or their most recent distributions, nor what assumptions GiveWell is making in its cost-effectiveness models.</p><p>Both charities may start making plans to work in the same place, because of high malaria burden and low coverage, but once the first of the two charities distributes, the second\u2019s estimates of coverage will be too low and malaria incidence too high, if they aren't updated. Or, they may both assume the other&nbsp;<i>will</i> distribute in the region, and only claim the difference conditional on the other, but then the first\u2019s estimates of coverage will be too high and malaria incidence too low. I illustrate both possibilities.</p><p>&nbsp;</p><p>Let's start with an oversimplified illustration. Suppose a specific child definitely would have died without protection from either SMC or a bednet, and definitely wouldn't have died with protection from either. Suppose AMF provides that child a bednet and MC provides that child SMC. Each charity might claim to have saved the child, ignoring that the other would have in their absence, <i>double-counting the life saved</i>. Or, they might both recognize that the other would have saved the child and each refuse to claim credit (neither was counterfactual!), so <i>the life saved doesn't get counted at all</i>. Both approaches are wrong!</p><p>&nbsp;</p><p>Let's make this a bit more realistic, but with some made-up numbers. Suppose that each of a bednet and SMC independently reduce a protected child\u2019s risk of death by 75%, whether or not the other is used. If a child uses both, their risk will be reduced to 6.25%=25%*25% compared to using neither, or a 93.75% reduction in risk. The reduction in risks attributed to AMF and MC together should add up to 93.75% for the sake of estimating their cost-effectiveness together, but they may not:</p><ol><li>If AMF assumes that child would not have received SMC (or we assume so on their behalf), they would claim a reduction in risk of 75% compared to no coverage and multiply by the no coverage risk. MC, then, to add up to 93.75%, can only claim the 18.75%=25%-6.25% difference in risk compared to neither, or a 75% reduction in risk compared to a bednet. Similarly if we swap the roles of AMF and MC. If they each claim 75% compared to no coverage or we do so on their behalves, this would sum to a 150% reduction in risk, which is too high (and impossible).</li><li>If AMF assumes that the child would have received SMC (or we assume so on their behalf), they would claim the difference in risk of 18.75%=25%-6.25%, and impact proportional to that by multiplying by the probability of death with neither. Or, equivalently, AMF could claim a 75% reduction in risk compared to the remaining risk with SMC, and multiply the latter by 75%. Then, to add up to 93.75%, MC can\u2019t only claim the difference over bednets \u2014 also 18.75% \u2014 MC must claim the full initial 75% reduction compared to no coverage (neither bednet nor SMC). Similarly if we swap the roles of AMF and MC. If they each claim 18.75% compared to no coverage or we do so on their behalves, this would sum to 37.5% reduction in risk, which is too low.</li></ol><p>This means that the two obvious ways of estimating impact relative to no coverage can\u2019t be applied symmetrically for both AMF and MC. One way will overestimate if used for both, summing to a 150% reduction in risk, and the other will underestimate, summing to a 37.5% reduction in risk. The overestimate is 4x greater than the underestimate, and their average is exactly 93.75%, the correct number.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpyj99zqxmff\"><sup><a href=\"#fnpyj99zqxmff\">[7]</a></sup></span></p><p>If we prefer only one of the two charities to work in a given place, it would be the first to announce its plan to work there that we should treat as first, as long as it follows through soon enough. When we prefer both to work in a given place, the order plausibly doesn\u2019t matter. When the order does matter, that may incentivize each charity to get there first or make the first commitment to get there, because they could claim greater marginal impact than otherwise. To ignore the order, for each charity, we can just average over both orders, giving the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Shapley_value\"><u>Shapley value</u></a>. For discussion of Shapley values on the Effective Altruism Forum, see&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/XHZJ9i7QBtAJZ6byW/shapley-values-better-than-counterfactuals\"><u>Sempere, 2019</u></a>.</p><p>&nbsp;</p><p>I was informed by GiveWell staff that they do attempt to account for the near-term plans of other malaria-targeting charities for each of AMF and MC, including AMF for MC and MC for AMF, and <i>if</i> they do so like I illustrated above for both, holding MC's plans constant for AMF and AMF's plans constant for MC, then this would cause a <i>downward</i> bias in their cost-effectiveness estimates. So, if the estimates are <i>otherwise</i> unbiased (i.e. except for this credit assignment issue), then AMF and MC would actually be <i>more cost-effective</i> than GiveWell's estimates suggest.</p><p>&nbsp;</p><h1>Acknowledgements</h1><p>This article was inspired by discussion with&nbsp;<a href=\"https://forum.effectivealtruism.org/users/victorw\"><u>VictorW</u></a> in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/wofeTGAB8xkraeSrt/how-do-inspired-contributions-and-externalized-costs-factor\"><u>How do inspired contributions and externalized costs factor in cost-effectiveness calculations?</u></a> I\u2019d also like to thank GiveWell staff for clarification on their models.</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrfrj4m6ioa\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrfrj4m6ioa\">^</a></strong></sup></span><div class=\"footnote-content\"><p>That being said, then we need to consider whether or not MC\u2019s bednet distributions are already included in bednet coverage estimates for where MC works.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnr4kkqovncan\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefr4kkqovncan\">^</a></strong></sup></span><div class=\"footnote-content\"><p>0.2*0.7*0.25 + 0.2*0.25 + 0.7*0.25 + 0.25 = 0.51, &nbsp;and &nbsp;1-0.51 = 0.49</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnl1s35k56te\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefl1s35k56te\">^</a></strong></sup></span><div class=\"footnote-content\"><p>0.2*0.5 + 0.7*0.5 = 0.45, &nbsp;and &nbsp;1-0.45 = 0.55</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxbrh35s09ae\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxbrh35s09ae\">^</a></strong></sup></span><div class=\"footnote-content\"><p>0.2*0.27*0.5 + 0.5 = 0.527, &nbsp;and &nbsp;1-0.527 = 0.473</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn88ky7tnjwso\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref88ky7tnjwso\">^</a></strong></sup></span><div class=\"footnote-content\"><p>0.2*0.5 + 0.5 = 0.6, &nbsp;and &nbsp;1-0.6 = 0.4</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnu5dp9jn4kx\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefu5dp9jn4kx\">^</a></strong></sup></span><div class=\"footnote-content\"><p>0.7*0.5 + 0.5 = 0.85, &nbsp;and. 1-0.85 = 0.15</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpyj99zqxmff\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpyj99zqxmff\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The average of the two will be correct in general, because summing the high estimate for AMF,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"H_{AMF}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.057em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\">H</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span></span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>, with the low estimate for MC,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"L_{MC}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">L</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span></span></span></span></span></span></span></span></span></span>, gives the correct total reduction,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"R\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span></span></span></span></span></span>, as does summing the low estimate for AMF,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"L_{AMF}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">L</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span></span></span></span></span></span></span></span></span></span>, with the high estimate for MC,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"H_{MC}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.057em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\">H</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span></span></span></span></span></span></span></span></span></span>. Then,</p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"(H_{AMF} + H_{MC})/2 + (L_{AMF} + L_{MC})/2 = (H_{AMF} + L_{MC})/2 + (L_{AMF} + H_{MC})/2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.057em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\">H</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\" style=\"margin-right: -0.057em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\">H</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">L</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">L</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.057em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\">H</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">L</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">L</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\" style=\"margin-right: -0.057em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\">H</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span></p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"= R/2 + R/2 = R\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">R</span></span></span></span></span></span></span></p></div></li></ol>", "user": {"username": "MichaelStJules"}}, {"_id": "N3xPwaTH2i8yrppo2", "title": "Mental Health Navigator Update", "postedAt": "2023-09-05T19:43:29.745Z", "htmlBody": "<p>A lot has changed for the Mental Health Navigator over the past year! This post provides some updates on what's new from the last few months, as well as information about volunteering opportunities we have available!</p><h2>Data Bank of Free, Low-cost, or Sliding Scale Resources</h2><p>The data bank on our Resources page has been reformatted to be more easily navigable, and updated to only contain low cost (&lt;$100), sliding scale, and free resources, and there are now 260 listed! We\u2019re continuously looking to expand our data bank, with a goal of reaching 500 by December. If you\u2019d like to help us reach this target, please send us any mental health resources you recommend by filling out <a href=\"https://airtable.com/appPrdgXFjprjmIAC/shrE7hB6oYAt1YL2d\">this form</a> or emailing us at info@mentalhealthnavigator.co.uk. Information about what we\u2019ll accept and our quality control process is available on the data bank page of our website: <a href=\"https://www.mentalhealthnavigator.co.uk/resource-data-bank\">https://www.mentalhealthnavigator.co.uk/resource-data-bank</a></p><h2>Newsletter</h2><p>We've got a monthly newsletter now! It\u2019s called MentNav, and it provides information about mental health resources we find and add to our growing data bank, articles and blog posts, and opportunities at the Mental Health Navigator. Feel free to subscribe here: <a href=\"https://mailchi.mp/mentalhealthnavigator.co.uk/ment-nav\">https://mailchi.mp/mentalhealthnavigator.co.uk/ment-nav</a></p><p>If you\u2019re involved in the mental health space and would like to have anything included in the newsletter, please send us an email at <a href=\"mailto:info@mentalhealthnavigator.co.uk\">info@mentalhealthnavigator.co.uk</a></p><h2>Advisory Service Open to Everyone</h2><p>Our Advisory Service is now open to everyone! You can book a consultation via the booking form on our Advisory Service webpage: <a href=\"https://www.mentalhealthnavigator.co.uk/advisory-service\">https://www.mentalhealthnavigator.co.uk/advisory-service</a></p><p>If you don\u2019t see a time that works for you, we\u2019ve recently had volunteers join, and their availability will also be visible in the coming weeks, so please check back later.</p><h2>Providers Table</h2><p>Our Providers Table has grown significantly over the last year to include 92 providers, and now an average of 141 people visit it every month! If there\u2019s anyone you recommend adding to the Providers Table, <a href=\"https://airtable.com/appPrdgXFjprjmIAC/shrHgwnZPhBGNTD3f\">please fill out this form</a>.</p><p>If you\u2019re someone who would like to be listed in the Providers Table, please send us an email at <a href=\"mailto:info@mentalhealthnavigator.co.uk.\">info@mentalhealthnavigator.co.uk.</a></p><h2>Looking for Volunteers for Data Bank, Blog, and Advisory Service</h2><p>Looking for a volunteering opportunity this autumn? We\u2019re accepting applications for volunteers at the Mental Health Navigator! We\u2019re currently looking for 1 more Advisory Service volunteer (based in the UK), Data Bank volunteers, and Content Writing volunteers. To find out more about volunteering with us and apply, please visit our Get Involved page here: <a href=\"https://www.mentalhealthnavigator.co.uk/get-involved\">https://www.mentalhealthnavigator.co.uk/get-involved</a></p>", "user": null}, {"_id": "GDdvdhbGfCehnoJzY", "title": "Strongest real-world examples supporting AI risk claims?", "postedAt": "2023-09-05T15:11:47.607Z", "htmlBody": "<p>[Manually cross-posted to LessWrong <a href=\"https://www.lesswrong.com/posts/h66WSLKdShqzNsZL9/strongest-real-world-examples-supporting-ai-risk-claims\">here</a>.]</p><p>There are some great collections of examples of things like <a href=\"https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml?urp=gmail_link&amp;gxids=7628\">specification gaming</a>, <a href=\"https://docs.google.com/spreadsheets/d/e/2PACX-1vTo3RkXUAigb25nP7gjpcHriR6XdzA_L5loOcVFj_u7cRAZghWrYKH2L2nU4TA_Vr9KzBX5Bjpz9G_l/pubhtml\">goal misgeneralization</a>, and <a href=\"https://ai-improving-ai.safe.ai/\">AI improving AI</a>. But almost all of the examples are from demos/toy environments, rather than systems which were actually deployed in the world.</p><p>There are also some <a href=\"https://incidentdatabase.ai/\">databases </a>of AI incidents which include lots of real-world examples, but the examples aren't related to failures in a way that makes it easy to map them onto AI risk claims. (Probably most of them don't in any case, but I'd guess some do.)</p><p>I think collecting real-world examples (particularly in a nuanced way without claiming too much of the examples) could be pretty valuable:</p><ul><li>I think it's good practice to have a transparent overview of the current state of evidence</li><li>For many people I think real-world examples will be most convincing</li><li>I expect there to be more and more real-world examples, so starting to collect them now seems good</li></ul><p><strong>What are the strongest real-world examples of AI systems doing things which might scale to AI risk claims?</strong></p><p>I'm particularly interested in whether there are any good real-world examples of:</p><ul><li>Goal misgeneralization</li><li>Deceptive alignment (answer: <a href=\"https://www.lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1\">no</a>, but yes to simple deception?)</li><li>Specification gaming</li><li>Power-seeking</li><li>Self-preservation</li><li>Self-improvement</li></ul><p>This feeds into a project I'm working on with AI Impacts, collecting empirical evidence on various AI risk claims. There's a work-in-progress table <a href=\"https://docs.google.com/spreadsheets/d/15pW8_pwbznnbQBu0Ogpk9qtqrSPKOoIjzbwe58GM4Ls/edit#gid=0\">here</a> with the main things I'm tracking so far - additions and comments very welcome.</p>", "user": {"username": "rosehadshar"}}, {"_id": "9eQFPiNmH2s5ZyNEu", "title": "AISN #21:\nGoogle DeepMind\u2019s GPT-4 Competitor, Military Investments in Autonomous Drones, The UK AI Safety Summit, and Case Studies in AI Policy", "postedAt": "2023-09-05T14:59:51.218Z", "htmlBody": "<p>Welcome to the AI Safety Newsletter by the <a href=\"https://www.safe.ai/\">Center for AI Safety</a>. We discuss developments in AI and AI safety. No technical background required.</p><p>Subscribe <a href=\"https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916\">here</a> to receive future versions.</p><hr><h2>Google DeepMind\u2019s GPT-4 Competitor</h2><p>Computational power is a key driver of AI progress, and a new report suggests that Google\u2019s upcoming GPT-4 competitor will be trained on unprecedented amounts of compute.&nbsp;</p><p>The model, currently named Gemini, may be trained by the end of this year with 5x more computational power than GPT-4. By the end of next year, the report projects that Google will have the ability to train a model with 20x more compute than GPT-4.&nbsp;</p><p>For reference, the compute difference between GPT-3 and GPT-4 was 100x. If these projections are true, Google\u2019s new models could create a meaningful spike relative to current AI capabilities.&nbsp;</p><p><strong>Google\u2019s position as an AI leader.</strong> The recent boom in large language models has been driven by several innovations pioneered at Google. For example, the Transformer architecture used by most advanced language models was first described in a <a href=\"https://arxiv.org/abs/1706.03762\">2017 paper</a> from Google.&nbsp;</p><p>OpenAI has led Google in language modeling for several years now. But after the release of ChatGPT, Google significantly increased their AI investments. They merged Google Brain and DeepMind into a single research lab with increased resources, and invested in Anthropic.&nbsp;</p><p>Google has tremendous financial resources, with <a href=\"https://companiesmarketcap.com/alphabet-google/cash-on-hand/#:~:text=Cash%20on%20Hand%20as%20of,accessible%20money%20a%20business%20has.\">$118 billion cash on hand</a>. In contrast, OpenAI\u2019s last investment round in January raised <a href=\"https://www.bloomberg.com/news/articles/2023-01-23/microsoft-makes-multibillion-dollar-investment-in-openai\">only $10 billion</a>. Perhaps it\u2019s no surprise that Google can quickly ramp up spending to compete with other leading AI labs.&nbsp;</p><p>Yet it seems that Gemini will be only one member of the next generation of frontier models, as Inflection AI CEO Mustafa Suleyman says his company will also soon surpass the compute used to train GPT-4.&nbsp;</p><p><strong>Inflection AI CEO on compute growth. </strong>Mustafa Suleyman, CEO of the rapidly growing Inflection AI, <a href=\"https://80000hours.org/podcast/episodes/mustafa-suleyman-getting-washington-and-silicon-valley-to-tame-ai/\">estimated</a> that his company will be training models \u201c100x larger than the current frontier models in the next 18 months.\u201d Notably, this is not based on predictions that they\u2019ll gain access to new compute, but rather estimated using the compute that Inflection already owns.&nbsp;</p><p>Three years from now, he predicts the industry will be \u201ctraining models that are 1,000x larger than they currently are.\u201d This positions Suleyman as one of the many industry leaders anticipating rapid growth in AI compute and capabilities over the next few years.</p><h2>US Military Invests in Thousands of Autonomous Drones</h2><p>The US military announced major investments in autonomous weapons this week, highlighting growing interest by the national security apparatus in AI development.&nbsp;</p><p><strong>Replicator: \u201cThousands of autonomous systems.\u201d</strong> <a href=\"https://defensescoop.com/2023/08/28/hicks-unveils-dods-new-replicator-initiative-to-counter-china-via-autonomous-tech/\">Replicator</a> is a new initiative from the U.S. Department of Defense. Within the next two years, it aims to deploy thousands of autonomous military systems, such as uncrewed aircraft and underwater drones.&nbsp;</p><p>The military will collaborate on this with groups in academia and industry. It will be directly overseen by Deputy Secretary of Defense Kathleen Hicks, a sign that this will be a high priority for the Department.&nbsp;</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1674b7b-55cf-4cb4-b88d-4a4cf4789869_1600x1234.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1674b7b-55cf-4cb4-b88d-4a4cf4789869_1600x1234.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1674b7b-55cf-4cb4-b88d-4a4cf4789869_1600x1234.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1674b7b-55cf-4cb4-b88d-4a4cf4789869_1600x1234.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1674b7b-55cf-4cb4-b88d-4a4cf4789869_1600x1234.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1674b7b-55cf-4cb4-b88d-4a4cf4789869_1600x1234.png 1456w\"></a></p><p>We\u2019ve <a href=\"https://arxiv.org/abs/2306.12001\">discussed</a> risks from autonomous weapons, including the increasing speed of warfare and the potential for AI accidents to rapidly escalate into war. Beyond direct concerns about autonomous weapons, there may be broader impacts on AI development driven by the military\u2019s interest in developing powerful AI systems.&nbsp;</p><p>The military may want to accelerate AI development, making the government less interested in slowing down and regulating commercial AI developers. International coordination could be more difficult to the extent that AI confers direct military advantages. On the other hand, by creating partnerships with academia and industry, the government could build institutional capacity to more effectively govern AI.&nbsp;</p><p><strong>Complementary investments in AI trust and evaluation. </strong>Alongside their new investments in autonomous weapons, the U.S. Department of Defense is also <a href=\"https://defensescoop.com/2023/08/29/pentagon-to-launch-pilot-focused-on-calibrated-trust-in-ai/\">launching</a> the Center for Calibrated Trust Measurement and Evaluation. The program broadly intends to \u201coperationalize responsible AI\u201d as well as \u201cvalue alignment.\u201d&nbsp;</p><p>The program has received $20 million in funding for the next year. Potential goals include creating training and certification programs for AI and testing and evaluating military AI systems.&nbsp;</p><h2>United Kingdom Prepares for Global AI Safety Summit</h2><p>In June, US President Joe Biden and UK Prime Minister Rishi Sunak agreed on a partnership to help establish global AI governance. The UK is now preparing to hold an AI Safety Summit later this year.&nbsp;</p><p>The summit will be held in <a href=\"https://www.gov.uk/government/news/iconic-bletchley-park-to-host-uk-ai-safety-summit-in-early-november\">Bletchley Park</a>, near London, on <a href=\"https://dig.watch/updates/uk-government-sets-dates-for-novembers-global-ai-summit#:~:text=The%20event%2C%20scheduled%20for%20November,cutting%20edge%20of%20AI%20development.\">November 1st and 2nd</a>. It will bring together key countries, companies, academics and civil society organizations to discuss AI safety.&nbsp;</p><p>The UK recently outlined <a href=\"https://www.gov.uk/government/news/uk-government-sets-out-ai-safety-summit-ambitions\">five objectives</a> for the summit:&nbsp;</p><ol><li>A shared understanding of the risks posed by frontier AI and the need for action</li><li>A forward process for international collaboration on frontier AI safety, including how best to support national and international frameworks</li><li>Appropriate measures which individual organizations should take to increase frontier AI safety</li><li>Areas for potential collaboration on AI safety research, including evaluating model capabilities and the development of new standards to support governance</li><li>Showcase how ensuring the safe development of AI will enable AI to be used for good globally</li></ol><p>They said the summit will focus on \u201crisks created or significantly exacerbated by the most powerful AI systems, particularly those associated with the potentially dangerous capabilities of these systems.\u201d For example, the announcement lists the possibility that AI could threaten global biosecurity.&nbsp;</p><p>This summit will serve as a key opportunity for governments, AI developers, and other stakeholders to work on reducing the risks of advanced AI systems.&nbsp;</p><h2>Case Studies in AI Policy</h2><p>Can governments effectively execute AI policies? To answer that question, a <a href=\"https://dl.acm.org/doi/pdf/10.1145/3600211.3604701\">new study</a> examines the implementation of three recent AI policies in the United States. The findings indicate that less than 40% of actions required by these policies have been implemented by the relevant federal agencies, revealing shortcomings in the U.S. state capacity for AI governance.</p><p>The paper studies three recent AI policies:</p><ol><li><strong>AI in Government Act of 2020</strong>: Aims to encourage adoption of AI in the federal government, including by providing guidance to agencies on AI usage and establishing a center for government AI adoption within the General Services Administration (GSA).</li><li><strong>Executive Order 13,859 (AI Leadership Order)</strong>: Directs federal agencies to pursue six strategic objectives, including investing in AI R&amp;D and building a competent AI workforce.</li><li><strong>Executive Order 13,960 (Trustworthy AI Order)</strong>: Focuses on harnessing AI to improve government operations, outlining nine principles for the lawful and responsible use of AI by federal agencies.</li></ol><p><strong>Limited success in implementing AI policies.</strong> Less than half of the 45 legal requirements across these laws were publicly verified as implemented. For example, the Trustworthy AI Order required that federal agencies document their use of AI, but only about half have done so. Many agencies which demonstrably use AI have not submitted the required documentation. Similarly, the AI Leadership Order directed each federal agency to issue AI strategies, but 88% of agencies have failed to do so.&nbsp;</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5d061b3-b9e1-4b2c-b6ce-83e126af361e_1276x432.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5d061b3-b9e1-4b2c-b6ce-83e126af361e_1276x432.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5d061b3-b9e1-4b2c-b6ce-83e126af361e_1276x432.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5d061b3-b9e1-4b2c-b6ce-83e126af361e_1276x432.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5d061b3-b9e1-4b2c-b6ce-83e126af361e_1276x432.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5d061b3-b9e1-4b2c-b6ce-83e126af361e_1276x432.png 1456w\"></a></p><p><strong>Recommendations for improving state capacity on AI. </strong>The report provides a number of recommendations for improving state capacity around AI:&nbsp;</p><ul><li><strong>Clarify Mandates:</strong> Agencies need explicit guidelines on compliance, what constitutes AI applications under these laws, and how to interpret non-responses.</li><li><strong>Resource Allocation:</strong> Adequate funding and technical expertise must be provided to agencies to improve their capacity to implement AI policies.</li><li><strong>Strong Leadership:</strong> A centralized authority or strong senior leadership is crucial for setting strategic AI priorities and ensuring effective implementation.</li></ul><p>More broadly, one question is whether a single authority tasked with AI governance might be more effective than the current approach of diffusing responsibility for AI governance over a wide variety of agencies with many different focuses.&nbsp;</p><h2>Links</h2><ul><li>US <a href=\"https://www.tomshardware.com/news/us-bans-sales-of-nvidias-h100-a100-gpus-to-middle-east\">restricts sale of advanced GPU chips</a> to the Middle East.</li><li>US Copyright Office <a href=\"https://www.copyright.gov/newsnet/2023/1017.html?loclr=twcop\">seeking public feedback</a> on legal questions about AI.</li><li>OpenAI nears $<a href=\"https://www.fastcompany.com/90946849/openai-chatgpt-reportedly-nears-1-billion-annual-sales?utm_source=tldrai\">1B revenue</a>.</li><li>Congress will host <a href=\"https://twitter.com/m_ccuri/status/1696975744327450990?s=20\">this list of experts</a> next week in their first AI forum.&nbsp;</li><li>Here is a <a href=\"https://www.alignment-workshop.com/\">series of talks</a> on AI safety and alignment.&nbsp;</li></ul><p>We\u2019d appreciate your feedback on the newsletter! Please leave your thoughts <a href=\"https://forms.gle/EU3jfTkxfFgyWVmV7\">here</a>.&nbsp;</p><p>Subscribe <a href=\"https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916\">here</a> to receive future versions.</p>", "user": {"username": "Center for AI Safety"}}, {"_id": "6Z7SShHa5fnadLm3G", "title": "Promoting Safety in EA: Practical Steps to Prevent Sexual Harassment & Code of Conduct for Your Use", "postedAt": "2023-09-05T14:13:08.692Z", "htmlBody": "<p>EA communities tend to be a professional space and a social space, all at once. This reality can often involve complex power dynamics that require careful attention.&nbsp;<strong>To maintain safe spaces, it is crucial to establish guiding rules for our communities</strong>. In this post, I would like to share EA Israel's concise code of conduct for preventing sexual harassment, which is based on Israeli law. Before diving into that, we suggest some practical steps you can take to promote the safety of your community.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrii5wiu1d0n\"><sup><a href=\"#fnrii5wiu1d0n\">[1]</a></sup></span></p><h1>What can you do to prevent sexual harassment in your EA community?</h1><h2>As a community manager</h2><p>Designate a responsible person, either a team member or volunteer, dedicated to sexual harassment prevention and community health in your community.&nbsp;This person should be committed to this subject, ideally with some professional experience in dealing with emotionally challenging situation.&nbsp;Nominate another person, preferably from a different gender, as a contact person when the main representative is unavailable / connected to the harassment case / just in case someone prefers to speak with someone else.</p><p>Ensure they receive professional training, including emotional and legal aspects, that they are able to commit to this role for a long-enough duration, and&nbsp;that they establish a code of conduct against sexual harassment.</p><h2>As a community health and preventing sexual harassment representative</h2><p>Set a code of conduct to prevent sexual harassment and publish it widely, so that every old and new community member encounters it at some point. Clearly identify yourself as the primary point of contact for addressing harassment issues, and make it super-easy to contact you.</p><h2>As a community member</h2><p>Ask your community manager to nominate a responsible person for sexual harassment prevention and community health. Ask for the adoption of a code of conduct, such as the one shared in this post, or encourage the development of a customized code of conduct that suits your community's needs and fits the local law.</p><p>You, as an EA worker or a community member, can help to create an environment where everyone feels respected and protected.&nbsp;</p><h1><strong>EA Israel\u2019s Code of Conduct for preventing sexual harassment</strong></h1><p><i>The rest of this section is<strong>&nbsp;</strong>EA Israel\u2019s short version code of conduct, based on the Israeli law.</i></p><p>Effective Altruism Israel strives to create a sense of safety and comfort among its staff and community. Sexual harassment is strictly against the organization's policy.</p><h2>Prohibited acts according to the law</h2><ol><li>Threatening or coercing a person to engage in acts of a sexual nature.</li><li>Indecent acts / lascivious behaviour.</li><li>Repeated proposals of a sexually oriented nature, even if the person to whom the proposals are directed has shown a lack of interest in them. However, there is no need to demonstrate a \"lack of interest\" in a relationship of authority with an inherent power imbalance, or in a dependent, educational or therapeutic relationship with a minor, a patient, or someone defenseless in other ways.</li><li>Persistent references focused on a person's sex or sexuality, even if the person to whom the behavior is directed has shown disinterest. There is no need to demonstrate \"lack of interest\" in cases mentioned in paragraph (3) above.</li><li>Derogatory or demeaning treatment towards a person regarding their sex or sexuality, including their sexual orientation, whether or not they have expressed that it bothers them.</li><li>Publishing a photograph, film, or recording that focuses on a person's sexuality, in circumstances where the publication may degrade or demean the person, and consent for publication has not been given.</li><li>Prohibited retaliation: Harming a person as a result of sexual harassment towards them or any harm to someone who has assisted or testified on behalf of another person in a complaint about sexual harassment.</li></ol><h2>Process for Filing a Complaint</h2><p>The law provides three options, any or all of which can be pursued simultaneously or one after the other:</p><ol><li><strong>Internal disciplinary procedure</strong> - One can file a complaint regarding the harassment with the person in charge of handling cases of sexual harassment at the organization according to the Sexual Harassment Prevention Law. The designated responsible person at \"Effective Altruism Israel\" is Michal Greidinger, [contact details].&nbsp; Filing a complaint with the responsible person initiates internal disciplinary proceedings. If the designated responsible person is connected to the case in any way, one can file a complaint with Ezra Hausdorff, the organization's CEO.</li><li><strong>Criminal procedure&nbsp;</strong>- One can file a complaint with the police to initiate criminal proceedings.</li><li><strong>Civil procedure</strong> - Within seven years, one can file a civil lawsuit in court (usually the Regional Labor Court). The court may rule in cases of sexual harassment or retaliation an award compensation of up to 120,000 NIS without proof of damages.</li></ol><h2>Handling the Complaint (Internal disciplinary procedure)</h2><ul><li>The responsible person will proceed to clarify the complaint, listen to those involved and to witnesses, and verify the information.</li><li>The complainant will be informed about the procedures for handling sexual harassment or retaliation according to the law.</li><li>The responsible person will not handle the complaint if they have a personal connection to the subject of the complaint or those involved, and will transfer the investigation to another responsible person, such as a designated representative, a community manager, or the CEO.</li><li>The investigation of the complaint will be conducted efficiently and without delay.</li><li>The investigation of the complaint will be carried out with utmost respect for the dignity and privacy of the complainant, the accused, and witnesses, while safeguarding the rights of those involved, and the confidentiality of the information.</li><li>Upon completion of the investigation, the designated responsible person will promptly submit a written summary of the investigation to the CEO, along with detailed recommendations for further action to prevent ongoing sexual harassment, and to address any harm caused to the complainant.</li><li>Within seven days, the CEO will decide on further action with the aim of preventing ongoing sexual harassment and addressing any harm caused to the complainant.</li><li>The CEO will protect the complainant during the investigation from any negative impact on their organizational involvement resulting from the complaint and, among other measures deemed appropriate, will take steps to distance the accused from the complainant to the extent appropriate and possible.</li><li>The CEO will provide the complainant with a detailed written notification of their decision and allow the complainant to view the responsible person's summary and recommendations.</li></ul><h1>Actions taken by \"Effective Altruism Israel\" to prevent sexual harassment</h1><ul><li>Appointment of a designated individual for the prevention of sexual harassment (Michal Greidinger, [contact details]).</li><li>The \"Effective Altruism Israel\" team is committed to participating in training and awareness programs coordinated by the organization regarding the prevention of sexual harassment and retaliation.</li><li>The organization requires all employees, board members, volunteers and community members to refrain from engaging in sexual harassment and retaliation, both among employees and within the community.</li><li>The organization publishes and makes accessible the code of conduct for the prevention of sexual harassment to the organization's team and the community.</li></ul><p>In addition to the above, with mutual consent and agreement, an intimate relationship may exist between different individuals involved in the organization. In the event of the formation of an intimate relationship with mutual consent between an employee/manager and another employee, manager or community member, it is the responsibility of the employee/manager to inform the designated responsible person at the organization of their relationship, as soon as possible, in order to prevention the exploitation of power dynamics and to ensure professional conduct at the workplace.</p><p>&nbsp;</p><p><a href=\"https://docs.google.com/document/d/1_Cleq10M5FtQEcSDkDpEbeZa2kH1RIfbUchLInkh5KQ/edit?usp=share_link\"><u>For the full code of conduct</u></a></p><p>&nbsp;</p><p><strong>For further information, you can contact me at&nbsp;</strong><a href=\"mailto:michal@effective-altruism.org.il\"><strong><u>michal@effective-altruism.org.il</u></strong></a></p><p><strong>For translation and feedback on this post, thanks to Sarah Winthrope, Edo Arad, Avital Shkolnik and Ezra Hausdorff.&nbsp;</strong></p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrii5wiu1d0n\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrii5wiu1d0n\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I am pleased to see that other community organizers are also focusing on community health. Here is the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/tk2YWPKNqeCDWData/ea-germany-community-health-documents-and-processes\"><u>recent EA Germany post</u></a> regarding the subject.&nbsp;</p></div></li></ol>", "user": {"username": "Michal Greidinger"}}, {"_id": "vEcpYKoa9RWfGHyWo", "title": "Copenhagen Consensus Center's newest research on global poverty - we should be talking about this", "postedAt": "2023-09-05T09:52:08.377Z", "htmlBody": "<p>The Copenhagen Consensus Center (CCC) is a non-profit think-tank, that has published economic cost-benefit analyses on global issues since 2004. It is headed by Bj\u00f8rn Lomborg. In the chronological history of prominent effective altruists, I think he is the 2nd behind Peter Singer (but that's besides the point). &nbsp;</p><p>The CCC has high standards of research, and has employed top economists since its inception, including Nobel Prize winners. It has previously published large reports in 2o04, 2008, 2012 &amp; 2015. It's recommendations were usually similar to the ones in the EA movement (child nutrition, immunization, malaria, deworming etc.), but with a larger focus on economic policy.&nbsp;</p><p>About 4 months ago, the CCC has published its <a href=\"https://copenhagenconsensus.com/halftime\"><i>Halftime for the Sustainable Development Goals 2016-2030</i></a><i>.&nbsp;</i></p><p><a href=\"https://www.amazon.com/dp/1940003482/\">A book based on this report</a> has received good reviews from the chief economist of the world bank, Nobel Prize winning economist Vernon Smith, and Bill Gates. Anyways, the report mentions some interventions that are seldom, or even never, talked about in the EA community, along with other more familiar ones.&nbsp;</p><p>&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/vEcpYKoa9RWfGHyWo/wdxgtuvkasfwmpkh6fqu\" alt=\"SDG 12 best investments table\"></figure><p>The things that I've barely/never seen EA talk about are land tenure security, e-procurement and agricultural R&amp;D.&nbsp;</p><p><a href=\"https://copenhagenconsensus.com/halftime-sustainable-development-goals-2016-2030/agricultural-rd\">Agricultural R&amp;D is good</a> for obvious reasons.&nbsp;</p><p>Land tenure is interesting - <a href=\"https://copenhagenconsensus.com/halftime-sustainable-development-goals-2016-2030/land-tenure-security\">according to CCC</a>:&nbsp;</p><blockquote><p>Globally, 70 percent of the world\u2019s population has no access to formal land registration systems. One-in-five, or almost a billion people, consider it likely or very likely they will be evicted in the next five years.</p><p>[...] When farmers know they own their land, they are more willing to make expensive investments to increase long-term productivity. They can also use their land deed as collateral to borrow money for investments like farm equipment or property expansion.</p><p>[...] The researchers show that the total benefits of providing more secure urban tenure would therefore be about $160 billion, or 30-times the costs.</p></blockquote><p>&nbsp;</p><p><a href=\"https://copenhagenconsensus.com/halftime-sustainable-development-goals-2016-2030/e-government-procurement\">e-procurement is also interesting:</a></p><blockquote><p>In the countries where the poorer half of the world\u2019s population lives, procurement makes up an astounding half of all government expenditure.</p><p>This procurement can be made less corrupt and more effective by putting the whole system online, making it transparent. Electronic procurement or \u201ce-procurement\u201d lets many more companies hear about procurement offers, ensures more bids can be submitted and means governments lose less money through corruption and waste.</p><p>[...] For each dollar spent, the low-income country will realize savings worth $38. For lower-middle income countries, the average savings are more than $5 billion over the first 12 years, meaning each dollar spent creates more than $300 of social benefits. This makes e-procurement one of the world\u2019s most effective policies.</p></blockquote>", "user": {"username": "alamo 2914"}}, {"_id": "LfuvzMD64yy3nDjb2", "title": "Online Mental Wellbeing Community Manager: Volunteer Opportunity (5h/week in 2023)", "postedAt": "2023-09-05T09:38:21.109Z", "htmlBody": "<ul><li>Are you interested in improving community mental wellbeing and supporting potential changemakers in doing good better?&nbsp;</li><li>Are you interested in taking on a new challenge to apply and grow your community management skills?</li><li>Are you interested in learning more about mental wellbeing, or are you an experienced coach or therapist?</li></ul><p><strong><u>Role</u></strong>: ~5 hours per week (~1 hour a day on 5 days of your choice, weekends, or weekdays okay) from now until the mid of November, potentially longer, remote or in Berlin. Due to our non-profit nature, we are looking for someone to donate their time to support the community.</p><p>You can apply in&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSeegjyz6IWYf2hX3lSA5eX1Xekfx-XJQ8nwGgT7fEIXSazO2g/viewform?usp=sf_link\"><u>&lt;15 min via this form</u></a>. We'll review applications on a rolling basis and recommend applying until 12.9.</p><h3><strong><u>Summary</u></strong></h3><p>We, at&nbsp;<a href=\"https://www.rethinkwellbeing.org/\"><u>Rethink Wellbeing</u></a>, are seeking an EA-aligned, dynamic online community manager to enhance community engagement in our Rethink Wellbeing Discord (and maybe Slack). Our Discord community accompanies an active 8-week&nbsp;<a href=\"https://forum.effectivealtruism.org/editPost?postId=cZwQWeThqgXxmuFpG&amp;key=50106829e74b7e8c7418d65bc2bf33\"><u>CBT peer-facilitated support group program</u></a> and allows members and facilitators to share insights and connect beyond weekly sessions. You\u2019d be part of a nurturing space where progress, challenges, and discoveries are celebrated and shared. This will greatly help people with ongoing behavior change, motivation, and feeling supported in between the group meetings, and thereby, to become more fulfilled versions of themselves.</p><p><strong>Why we think this role is high-impact</strong></p><p>Rethink Wellbeing is dedicated to elevating overall wellbeing. To achieve this, we use engaging, evidence-based, and low-cost online programs with weekly sessions. Our programs use well-established psychological methods to help participants increase their mental wellbeing and productivity. Ongoing support and behavior change are an important component of making progress possible and long-lasting. Our online community helps to enable and foster that, and thereby increases the cost-effectiveness and quality of our programs.</p><p><strong>Your Role</strong>:</p><ul><li>Engage with members in general chat and other themed Discord channels&nbsp;</li><li>Initiate and create situations and conversations that foster engagement and interaction among members</li><li>Provide support and encouragement to members when needed, ensuring a positive and inclusive environment</li><li>Monitor and moderate discussions to ensure they adhere to community guidelines</li></ul><p><strong>Qualifications</strong>:</p><ul><li>Strong interpersonal and communication skills</li><li>Familiarity with EA&nbsp;</li><li>Knowledge of cognitive behavioral therapy (CBT) desired but not essential</li><li>Familiarity with Discord and its features, or the ability to get comfortable with it quickly</li><li>Ability to handle difficult situations with tact and diplomacy</li><li>Passion for building and nurturing online communities, and ideally experience with it</li></ul><p><strong>Benefits:</strong></p><ul><li>Opportunity to make a meaningful impact on a growing mental wellbeing community</li><li>Gain valuable experience in community management, leadership, and collaboration</li><li>Connect with like-minded EAs who share your passion for cultivating inclusive online spaces</li><li>Job-reference/testimonial after 2 months</li></ul><p>&nbsp;</p><p><strong>Is this you?</strong></p><p>We are excited to get to know you!</p><p>If you made it all the way here, you can also apply in &lt;15 min&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSeegjyz6IWYf2hX3lSA5eX1Xekfx-XJQ8nwGgT7fEIXSazO2g/viewform?usp=sf_link\"><strong><u>here</u></strong></a>!</p>", "user": {"username": "JohnDrummond89"}}, {"_id": "GBAzW4pZ5JgJqGMJg", "title": "Against the Open Source / Closed Source Dichotomy: Regulated Source as a Model for Responsible AI Development", "postedAt": "2023-09-04T20:23:34.453Z", "htmlBody": "<h1><strong>Context</strong></h1><p>This is a short post to illustrate and start testing an idea that I have been playing around with in the last couple of days after listening to a recent <a href=\"https://80000hours.org/podcast/episodes/mustafa-suleyman-getting-washington-and-silicon-valley-to-tame-ai/\">80,000 hours podcast with Mustafa Suleyman</a>. In the podcast Suleyman as well as Rob Wiblin expressed concerns about Open Source AI development as a potential risk for our societies while implying that closed source development would be the only reasonable alternative. They have not delved deeper into the topic to examine their own assumptions about what makes reasonable alternatives in this context, or to look for possible alternatives beyond the \"standard\" open source/closed source dichotomy. With this post, I want to encourage our community to join me in the effort to reflect our own discourse and assumptions around responsible AI development to not fall into the trap of naively reifying existing categories, and develop new visions and models that are better able to address the upcoming challenges which we will be facing. As a first step, I explore the notion of Regulated Source as a model for responsible AI development.</p><h1><strong>Open Source vs. Closed Source AI Development</strong></h1><p>Currently, there are mainly two competing modes for AI development, namely,&nbsp;<i>Open Source&nbsp;</i>and&nbsp;<i>Closed Source&nbsp;</i>(see Table for comparison):</p><ul><li>Open Source \u201cis source code that is made freely available for possible modification and redistribution. Products include permission to use the source code, design documents, or content of the product. The open-source model is a decentralized software development model that encourages open collaboration. A main principle of open-source software development is peer production, with products such as source code, blueprints, and documentation freely available to the public\u201d (<a href=\"https://en.wikipedia.org/wiki/Open_source\"><u>Wikipedia</u></a>).</li><li>Closed Source \u201cis software that, according to the free and open-source software community, grants its creator, publisher, or other rightsholder or rightsholder partner a legal monopoly by modern copyright and intellectual property law to exclude the recipient from freely sharing the software or modifying it, and\u2014in some cases, as is the case with some patent-encumbered and EULA-bound software\u2014from making use of the software on their own, thereby restricting their freedoms.\u201d (<a href=\"https://en.wikipedia.org/wiki/Proprietary_software\"><u>Wikipedia</u></a>)&nbsp;&nbsp;</li></ul><p>Table 1. Comparison Table for Open Source vs. Closed Source inspired by ChatGPT 3.5.</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Criteria</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Open Source</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Closed Source</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Accountability</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Community-driven accountability and transparency</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Accountability lies with owning organization</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Accessibility of Source Code</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Publicly available, transparent</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Proprietary, restricted access</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Customization</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Highly customizable and adaptable</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Limited customization options</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Data Privacy</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">No inherent privacy features; handled separately</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">May offer built-in privacy features, limited control</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Innovation</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Enables innovation</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Limits potential for innovation</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Licensing</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Various open-source licenses</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Controlled by the owning organization's terms</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Monetization</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Monetization through support, consulting, premium features</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Monetization through licensing, subscriptions, fees</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Quality Assurance</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Quality control depends on community</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Centralized control for quality assurance and updates</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Trust</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Transparent, trust-building for users</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Potential concerns about hidden biases or vulnerabilities</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Support</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Reliant on community or own expertise for support</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Reliant on owning organization for support</td></tr></tbody></table></figure><p>&nbsp;</p><p>If we look at these modes of software development, they have both been argued to have positive and negative implications for AI development. For example, Open Source has often been suggested as a democratizing force in AI development, acting as a powerful driver of innovation by making AI capabilities accessible to a broader segment of the population. This has been argued to be potentially beneficial for our societies, preventing or at least counteracting the centralization of control in the hands of a few, which poses the threat of dystopian outcomes (e.g., autocratic societies run by a surveillance state or a few mega corporations). At the same time, some people worry that the democratization of AI capabilities may increase the risk of catastrophic outcomes because not everyone can be trusted to use them responsibly. In this view, centralization is a good feature because it makes it easier to control the situation as a whole since fewer parties need to be coordinated. A prominent analogy used to support this view is with our attempts to limit the proliferation of nuclear weapons, where strong AI capabilities are viewed as similar in their destructive potential to nuclear weapons.</p><p>Against this background, an impartial observer may argue that both Open Source and Closed Source development models point to potential failure modes for our societies:&nbsp;</p><ul><li>Open Source development models can increase the risk of&nbsp;<i>catastrophic outcomes</i> when irresponsible actors gain access to powerful AI capabilities, creating opportunities for deliberate misuse or catastrophic accidents.</li><li>Closed Source development models can increase the risk of&nbsp;<i>dystopian outcomes&nbsp;</i>when control of powerful AI capabilities is centralized in the hands of a few, creating opportunities for them to take autocratic control over our societies.</li></ul><p>This leads to a dilemma that Tristan Harris and Daniel Schmachtenberger have illustrated with the metaphor of a bowling alley, where the two gutters to the left and right of the alley represent the two failure modes of catastrophic or dystopian outcomes.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref67k33nkynt7\"><sup><a href=\"#fn67k33nkynt7\">[1]</a></sup></span>&nbsp;In this metaphor, the only path that can lead us to existential security is a middle path that acknowledges but avoids both failure modes (see Fig. 1). Similarly, given the risk-increasing nature of both Open Source and Closed Source AI development approaches, an interesting question is whether it is possible to find a middle ground AI development approach that avoids their respective failure modes.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GBAzW4pZ5JgJqGMJg/p3nd5xhxfwavnn8kz1df\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GBAzW4pZ5JgJqGMJg/b0kr0wweoafwsdzybybw 119w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GBAzW4pZ5JgJqGMJg/fzepi1uogn917s7zfkjd 199w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GBAzW4pZ5JgJqGMJg/rlgbigyklyyzxgyfs0jl 279w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GBAzW4pZ5JgJqGMJg/ktar72feqpzfg55ppf4n 359w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GBAzW4pZ5JgJqGMJg/svxhibpyssrhahrucd8z 439w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GBAzW4pZ5JgJqGMJg/cwicjon4uopgkrwf3zgw 519w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GBAzW4pZ5JgJqGMJg/t9iweivgpfljmii4r8jq 599w\"><figcaption>Fig. 1. The path to existential safety requires avoiding catastrophic and dystopian outcomes.</figcaption></figure><h1><strong>Regulated Source as a Model for Responsible AI Development</strong></h1><p>In this section, I begin to sketch out a vision for responsible AI development that aims to avoid the failure modes associated with Open Source and Closed Source development by trying to take the best and leave behind the worst of both. I call this vision a \u201c<i>Regulated Source</i>&nbsp;<i>AI Development Model</i>\u201d to highlight that it aims to establish a regulated space as a middle ground between the more extreme Open Source and Closed Source models (c.f., Table 2).&nbsp;</p><p>As visualized in Fig. 2 and summarized in Table 2, the core idea of the Regulated Source model is to establish a trustworthy and publicly accountable regulating body which defines transparent standards that not only regulate AI use cases but also govern the behavior of organizations that want to implement these use cases. In particular, such standards could mandate the sharing of code and other knowledge assets relating to the implementation of AI use cases to level the playing field between the regulated organizations and reduce the chance of AI capability development races by lowering the expected benefit of unilateral actions. Importantly, such sharing of code and knowledge assets would be limited to organizations (or other actors), who have demonstrated that they can meet the transparent standards set by the regulating body, thus, balancing the risks associated with the proliferation of potentially dangerous capabilities on the one hand (i.e., the failure mode of Open Source), and the centralization of power on the other hand (i.e., the failure mode of Closed Source).&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GBAzW4pZ5JgJqGMJg/ni7j66z5rmtvqx4agbrf\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GBAzW4pZ5JgJqGMJg/lj0kbfe7m5an5nhlo7na 121w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GBAzW4pZ5JgJqGMJg/z2aqmostgearc0xltgn4 201w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GBAzW4pZ5JgJqGMJg/rfdt0li2qmoub3wnyops 281w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GBAzW4pZ5JgJqGMJg/ul5vpp6lpqd04htjicr4 361w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GBAzW4pZ5JgJqGMJg/stzmb8kiw0h3bzz6pfyi 441w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GBAzW4pZ5JgJqGMJg/oumocyzl823qcwhuppm3 521w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GBAzW4pZ5JgJqGMJg/vovryrzperssvrlfccna 601w\"><figcaption>Fig. 2. A Sketch of the Regulated Source AI Development Model.</figcaption></figure><p>A real life example that already comes close to the envisioned Regulated Source model is the&nbsp;<a href=\"https://en.wikipedia.org/wiki/International_Atomic_Energy_Agency\"><u>International Atomic Energy Agency (IAEA)</u></a>. The IAEA was founded in 1957 as an intergovernmental organization to monitor the global proliferation of nuclear resources and technology and serves as a forum for scientific and technical cooperation on the peaceful use of nuclear technology and nuclear power worldwide. For this, it runs several programs to encourage the safe and responsible development and use of nuclear technology for peaceful purposes and also offers technical assistance to countries worldwide, particularly in the developing world. It also provides international safeguards against the misuse of nuclear technology and has the authority to monitor nuclear programs and to inspect nuclear facilities. As such, there are many similarities between the IAEA and the envisioned Regulated Source model, the main difference being the domain of regulation and the less strong linkage to copyright regulation. As far as I am aware the IAEA does not have regulatory power to distribute access to privately developed nuclear technology, whereas the Regulated Source model would aim to compell responsible parties to share access to AI development products in an effort to counteract race dynamics and the centralization of power.</p><p>Table 2. Characteristics of a Regulated Source AI Development Model.</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Criteria</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Regulated Source</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Accountability</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Accountability and transparency regulated by governmental, inter-governmental, or recognized professional bodies (c.f.,&nbsp;<a href=\"https://en.wikipedia.org/wiki/International_Atomic_Energy_Agency\"><u>International Atomic Energy Agency (IAEA)</u></a>).</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Accessibility of Source Code</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Restricted access to an audience that is clearly and transparently defined by regulating bodies; all who fulfill required criteria are eligible for access</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Customization</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Highly customizable and adaptable within limits set by regulating bodies</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Data Privacy</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Minimum standards for privacy defined by regulating bodies</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Innovation</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Enables innovation within limits set by regulating bodies</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Licensing</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Technology or application specific licensing defined by regulating bodies</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Monetization</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Mandate to optimize for public benefit. Options include support, consulting, premium features but also licensing, subscriptions, fees</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Quality Assurance</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Minimum standards for quality control defined by regulating bodies</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Trust</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Transparent for regulating bodies, trust-building for users</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Support</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Reliant on regulated organizations for support</td></tr></tbody></table></figure><h1><strong>Concluding Remarks</strong></h1><p>I wrote this post to encourage discussion about the merits of the Regulated Source AI Development Model. While many people may have had similar ideas or intuitions before, I still miss a significant engagement with such ideas in the ongoing discourse on AI governance (at least as far as I am aware). Much of the discourse has touched on the pros and cons of open source and closed source models for AI development, but if we look closely, we should realize that focusing only on this dichotomy has put us between a rock and a hard place. Neither model is sufficient to address the challenges we face. We must avoid not only catastrophe, but also dystopia. New approaches are needed if we're going to make it safely to a place that's still worth living in.</p><p>The Regulated Source AI Development Model is the most promising approach I have come up with so far, but more work is certainly needed to flesh out its implications in terms of opportunities, challenges, or drawbacks. For example, despite its simplicity, Regulated Source seems to be suspiciously absent from the discussion of licensing frontier AI models, so perhaps there are reasons inherent in the idea that can explain this? Or is it simply that it is still such a niche idea that people do not recognize it as potentially relevant to the discussion? Should we do more to promote this idea, or are there significant drawbacks that would make it a bad idea? Many questions remain, so let's discuss them!</p><p>P.S.: I am considering to write the ideas expressed in this post up for an academic journal, reach out if you would want to contribute to such an effort.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn67k33nkynt7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref67k33nkynt7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Listen to Tristan Harris and Daniel Schmachtenberger on the <a href=\"https://open.spotify.com/episode/2LNwwgJqOMKHOqdvwmLxqd?si=kHSR27p_Q0ixRf_4eqwJaw\">Joe Rogan Experience Podcast</a>.</p></div></li></ol>", "user": {"username": "alexherwix"}}, {"_id": "BvnwyxhCZAcgBfC4g", "title": "How ForumMagnum builds communities of inquiry", "postedAt": "2023-09-04T20:20:57.772Z", "htmlBody": "<p>The website you're currently using is powered by ForumMagnum. But what really <i>is</i> ForumMagnum? What is it <i>for</i>, and why was it designed <i>this</i> way? In this post, I cast ForumMagnum as a medium for building <i>communities of inquiry.</i> I show how ForumMagnum is designed to build norms like <i>rationality</i> and <i>long-form</i>. Lastly, I suggest how the ForumMagnum developers could use the body of CoI research to guide their future product design.</p><p>I wrote this post, and posted it to LessWrong. But I'm link-posting it on the EA Forum because I believe most ForumMagnum development is done by CEA, and most developers are on this forum.</p>", "user": {"username": "Jim Fisher"}}, {"_id": "KKqQCZMcQrgtYKCjo", "title": "Notes on nukes, IR, and AI from \"Arsenals of Folly\" (and other books)", "postedAt": "2023-09-04T19:02:50.955Z", "htmlBody": "<p>Richard Rhodes\u2019s&nbsp;<i>The Making of the Atomic Bomb&nbsp;</i>has gotten&nbsp;<a href=\"https://www.wired.com/story/the-making-of-the-atomic-bomb-artificial-intelligence/\"><u>lots of attention in AI circles lately</u></a>, and it is a great read. I get why the people developing AI find it especially interesting, since a lot of it is about doing science and engineering and thinking about the consequences, but from my perspective as someone working on AI governance, the most powerful stuff was in the final few chapters as the scientists and policymakers begin to grapple with the wild implications of this new weapon for global politics.</p><p>Rhodes\u2019s (chronologically) first follow-up,&nbsp;<i>Dark Sun: The Making of the Atomic Bomb</i>, is even more densely useful for thinking about emerging technology governance, and I probably recommend it even more strongly than&nbsp;<i>TMOTAB</i> for governance-focused readers.</p><p>However, I didn\u2019t really start taking notes during my audiobook-listening until I started my third Rhodes tome,&nbsp;<i>Arsenals of Folly: The Making of the Nuclear Arms Race</i>.&nbsp;<i>AOF&nbsp;</i>is probably less applicable to AI governance than its predecessors, since it mostly focuses on a time when nuclear weapons had been around for several decades rather than when they were a new and transformative technology, but it still had a bunch of interesting details, and I figured I\u2019d spare some of you the trouble of finding them by posting my notes to the forum. (Unfortunately, since I audiobooked, I don\u2019t have page numbers.)</p><p>I\u2019ve also included a list of other cool finds from my nuclear/Cold War reading from the last few months in an appendix.</p><h3>Gell-Mann caveat</h3><ul><li>Before I say the rest of these facts, I should note that I had some \u201cGell-Mann Skepticism\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcp7ongsbbh\"><sup><a href=\"#fncp7ongsbbh\">[1]</a></sup></span>&nbsp;at some of Rhodes\u2019s analysis. Mostly, in his increasingly strong rhetoric about the titular folly of nuclear weapons, he makes pretty questionable counterarguments to the usual cases for nuclear weapons being advantageous. He cites&nbsp;<a href=\"https://www.jstor.org/stable/174059\">this paper</a> by Jacek Kugler, who argues that nuclear-armed states didn\u2019t seem to be able to impose their policy goals against non-nuclear armed states in a sample of Cold War conflicts like the Berlin Airlift, Vietnam War, invasion of Hungary, etc. It might initially seem surprising that, as Kugler claims, the nuclear-armed states lost these conflicts about as often as they won. But this ignores the enormous selection bias of what conflicting interests become actual disputes in the first place. It seems likely (or at least possible!) that lots of things that would've been disputes between non-nuclear powers get resolved way earlier in the process \u2013 the non-nuclear-armed states just don't bother picking the fights \u2013 and the disputes that actually did happen would've been totally non-contestable without nuclear weapons.</li><li>While arguing in his conclusion that the opportunity cost of the arms race was incredibly high, Rhodes cites another wild claim, this time from economist Seymour Melman: military spending could have been spent on domestic investment, and \u201caccording to some [unspecified] rough estimates,\u201d a marginal dollar of investment yields \u201c20-25 cents of additional annual production in perpetuity.\u201d This implies a &gt;20% rate of return on capital, which seems wildly high and totally irreconcilable with actual historical rates of return.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnoyadaipjj\"><sup><a href=\"#fnnoyadaipjj\">[2]</a></sup></span>&nbsp;So it does seem like he's just prone to this kind of exaggeration when he tries to zoom out and look at the consequences, which is kind of a bummer.</li></ul><p>With that disclaimer to take all of this with a grain of salt having been said:</p><h3>Notes from&nbsp;<i>Arsenals of Folly</i></h3><ul><li>The book starts with a chapter about Chernobyl (for some reason). Soviet industry wasn't advanced enough to make the kinds of protective casings that Western and Japanese nuclear plants used, so they falsified accident risk estimates \u2013 even in the official numbers engineering students would learn. Partly as a result, the USSR had at least&nbsp;<i>13 serious reactor accidents before Chernobyl</i>.</li><li>I usually think of the Cuban missile crisis as primarily resulting in arms control and detente, but Rhodes notes that it was critical to the Soviet decision to invest even more in their military. After agreeing to remove missiles from Cuba, the Soviet negotiator told his American counterpart, \u201cWell, Mr. McCloy, we will honor this agreement, but I want to tell you something: you will never do this to us again.\u201d And indeed what followed was an enormous 25-year arms buildup, with military spending regularly around 40% of GNP and the \u201cSoviet military-industrial complex\u201d becoming a dominant political force.</li><li>A French historical demographer, Emmanuel Todd, accurately extrapolated a bunch of demographic, political, and economic trends to predict the collapse of the Soviet union \u2013 including, impressively, predicting a nonviolent secession of the countries in eastern Europe \u2013 \"in 10, 20, or 30 years\" in a book published in the mid-70s. But \"Sovietologists\" in the west rejected this as naive speculation, in part because their own positions of influence relied on continued fears of Soviet domination.</li><li>Apparently Ronald Reagan may have gotten the idea for the&nbsp;<a href=\"https://en.m.wikipedia.org/wiki/Strategic_Defense_Initiative\">Strategic Defense Initiative</a> \u2013 \"Star Wars\" \u2013 from Edward Teller (a favorite supervillain from&nbsp;<i>TMOTAB, Dark Sun</i>, and the movie&nbsp;<i>Oppenheimer</i>) during a visit to Lawrence Livermore national labs in 1967.</li><li>Reagan became obsessed with SDI, but to the frustration of Rhodes and this reader, does not seem to have thought it through in much detail. He did not understand the game-theoretic reasons SDI was dangerous \u2013 namely, it breaks MAD and incentivizes a preemptive strike \u2013 and he hadn't thought about how they wouldn't stop nukes that weren't ballistic missiles (dropped from bombers or attached to sea-launched cruise missiles). Meanwhile, he continued insisting to his own negotiators that the only acceptable goal was SDI and the abolition of nuclear weapons.&nbsp;</li><li>Reagan and Gorbachev met for the first time in Geneva in 1985, which produces some fun anecdotes like: Gorbachev makes a forceful intellectual and strategic case for re-evaluating the countries' relationships. Reagan primarily reads from cue cards with classic Reagan aphorisms like \u201cIt isn't people who create armaments but governments\u201d and \u201cPeople don't get in trouble when they talk to each other, but about each other.\u201d Gorbachev is like, what the hell is this, can we talk about anything substantive please.</li><li>Reagan and Gorbachev have incredibly repetitive arguments about SDI, and Gorbachev\u2019s points do not seem to make it through to Reagan. Gorbachev starts anticipating the exact words Reagan would say: favorites included the Russian translation of \u201ctrust but verify\u201d and an analogy between SDI and gas masks (as in, \u201ceven though we banned chemical weapons, nations held onto their gas masks\u201d). Gorbachev seems to have opposed it primarily on the grounds that previous agreements to keep weapons out of space were just too important.</li><li>Reagan\u2019s staff learned that the easiest way to get him, a former Hollywood actor, to learn things was movies. They stopped writing briefs about the foreign leaders he was about to meet and instead had whatever part of the Pentagon makes films produce short biopics of them (which, understandably, his staff also preferred to the briefings). Reagan was especially moved after seeing a movie on ABC,&nbsp;<i>The Day After</i>, which depicts the aftermath of a nuclear war; he wrote about it repeatedly in his diary, and it significantly increased his determination to reduce nuclear risk.</li><li>Reagan was allegedly into fundamentalist and borderline mystical stuff. According to Rhodes, Reagan at least flirted with this prophecy that the rapture might happen when America defeated the Soviet Union \u2013 only for it to be revealed that&nbsp;<i>America\u2019s</i> charismatic leader was&nbsp;<i>the devil</i>, at which point Jesus would come back and defeat him.&nbsp;<a href=\"https://www.washingtonpost.com/archive/politics/1988/05/04/not-swayed-by-astrology-reagan-says/135516d5-7f99-4a43-90a0-8eba244a8426/\"><u>He may have signed the Intermediate-Range Nuclear Forces Treaty at a particular date and time identified as fortuitous by his wife\u2019s astrologer</u></a>. His obsession with SDI seems easier to interpret in this symbolism-heavy worldview (in addition to what Rhodes describes as a fantasy of America never having to negotiate). Gorbachev claims Reagan told him \u201cI don't know if you believe in reincarnation, but for me, I wonder if perhaps in a previous life, I was the inventor of the shield,\u201d and that French president Francois Mitterand told him Reagan\u2019s enthusiasm for SDI was \u201cmore mystic than rational.\u201d It is worth noting, though, that Reagan was unusually sincere and ambitious in his desire to make progress on reducing the threat of nuclear war, despite what Rhodes portrays as an almost cartoonishly hawkish and manipulative cabinet.</li><li>Gorbachev and Reagan almost agreed in 1986 at Reykjavik to eliminate&nbsp;<i>all nuclear weapons by 1996</i>, but both could not let go of the SDI issue. It literally came down to one word: Soviet language restricted SDI to \"<i>laboratory</i> testing\" for 10 years, and Reagan's advisors (probably falsely) told him that this would kill the program (which had barely even entered lab testing), and he refused to give it up. I\u2019m a little surprised that the Soviets didn\u2019t take this deal, given how determined Gorbachev was to reduce military spending and remake the international face of the Soviet Union, and especially given the technical challenges of SDI (and Reagan\u2019s repeated offers to share the technology with the Soviets, of which Gorbachev is understandably skeptical). It seems like this is mostly due to the political constraints Gorbachev was facing domestically: he was pushing the establishment pretty far already, and going all the way to nuclear abolition without removing SDI could have gotten him replaced with a more hawkish alternative.&nbsp;</li></ul><h3>Some of my takeaways</h3><p>These are mostly fairly obvious but reinforced by&nbsp;<i>AOF</i>:</p><ul><li>Turns out estimates of a technology\u2019s riskiness are subject to political and economic pressures.</li><li>Turns out policy change is more likely when top leaders are deeply bought into an issue mattering, and more likely to be effective when they have a solid understanding of the issue.</li><li>Humiliating your national rivals sometimes makes them really determined to avoid this happening again in the future.</li><li>Leaders, and the people in the room with them, can really make a difference.</li><li><a href=\"https://foreignpolicy.com/2021/06/27/the-geopolitics-of-empathy/\"><u>Empathy is very useful in international relations</u></a>. It seems like lots of Cold War mistakes (like&nbsp;<a href=\"https://en.wikipedia.org/wiki/Able_Archer_83\"><u>Able Archer 83</u></a>, both the exercise itself and the Soviet overreaction) resulted in part from conceiving of the other side as coldly strategic and basically evil \u2013 implicitly, \u201cwe think of the Soviets primarily in their capacity as the US\u2019s main geopolitical rival, so they probably think of themselves in the same way\u201d \u2013 rather than as another heterogeneous political system comprised of humans with mixed motivations.</li><li>Even powerful and strongly ideological national leaders are&nbsp;<a href=\"https://www.youtube.com/watch?v=rStL7niR7gs\"><u>bound by domestic political constraints</u></a>.</li><li>Economics-and-demography-driven outside views sometimes beat domain experts (<a href=\"https://forum.effectivealtruism.org/posts/qZqvBLvR5hX9sEkjR/comparing-top-forecasters-and-domain-experts\"><u>though it\u2019s a pretty close match overall</u></a>).</li></ul><h3>Appendix: takeaways/interesting finds from related books</h3><p><i>Arsenals of Folly&nbsp;</i>capped a months-long nuclear/Cold War nerdsnipe caused by&nbsp;<i>TMOTAB&nbsp;</i>and&nbsp;<i>Oppenheimer</i>, so figured I\u2019d also include some discoveries from these other books in this post.</p><ul><li>From&nbsp;<i>TMOTAB</i>: There\u2019s an incredible story where (German) Werner Heisenberg and (Danish) Neils Bohr discuss the possibility of a nuclear bomb with World War II underway. Heisenberg claims (after the war) that he meant to do some back-channel coordination with Allied scientists to slow down efforts to build the bomb, via implying that the German program was slow-going. Bohr thought Heisenberg was trying to elicit information about Allied nuclear efforts and even to get Bohr to cooperate with the Nazis. If Heisenberg is telling the truth (big \"if\"), an attempt by a scientist to communicate clearly about the risks of the technology he's working on due to competitive pressure, in order to coordinate an international slowdown, was instead interpreted as cynical hype-spreading and resulted in even more alarm among the other actors, who redoubled their efforts to get there first.</li><li>From&nbsp;<i>Dark Sun</i>: When there are new and potentially really disruptive technologies, even&nbsp;<a href=\"https://en.wikipedia.org/wiki/Acheson%E2%80%93Lilienthal_Report\"><u>not-particularly-radical leaders can be open to radical proposals</u></a> (in the nuclear case, international control of the nuclear supply chain in order to stop nuclear proliferation). As Harry Winne, the VP of General Motors, wrote: \u201c[Our proposal] may seem too radical, too advanced, too much beyond human experience. All these terms apply, with particular fitness, to the atomic bomb.\u201d But&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/J7nmbqcWncPMZFhGC/want-to-make-a-difference-on-policy-and-governance-become-an\"><u>the details really matter</u></a>, including things like \u201chow do you verify the treaty\u201d and \u201cis it really true that this part of the supply chain is such a bottleneck that it can ground international governance?\u201d</li><li>From&nbsp;<i>Dark Sun</i>: when your job is to strategize about the possibility of incredibly deadly events, it\u2019s hard to avoid&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/fRo5urRznMzGJAwrE/on-missing-moods-and-tradeoffs\"><u>missing moods</u></a> and&nbsp;<a href=\"https://www.lesswrong.com/posts/2ftJ38y9SRBCBsCzy/scope-insensitivity\"><u>scope insensitivity</u></a>. From the Wikipedia page for Single Integrated Operational Plan, the US\u2019s plan for nuclear war updated every year from 1961 to 2003:&nbsp;<ul><li>The execution of SIOP-62 was estimated to result in 285 million dead and 40 million casualties in the Soviet Union and China. Presented with all the facts and figures, Thomas D. White of the Air Force found the Plan \u201csplendid.\u201d Disregarding the human aspect, SIOP-62 represented an outstanding technological achievement: \u201cSIOP-62 represented a technical triumph in the history of war planning. In less than fifteen years the United States had mastered a variety of complex technologies and acquired the ability to destroy most of an enemy's military capability and much of the human habitation of a continent in a single day.\u201d [Note: <i>Arsenals of Folly </i>notes that this was probably a huge underestimate of the death count, since it counted deaths from blasts only, and not from fires or radiation (which could together push the death toll over 1 billion), let alone the <a href=\"https://forum.effectivealtruism.org/posts/Ysq53coRwgSWHHz2x/nuclear-winter-scepticism\">possibility</a> of nuclear winter.]</li></ul></li><li>From&nbsp;<i>Dark Sun&nbsp;</i>and&nbsp;<a href=\"https://nuclearsecrecy.com/nukemap/\"><u>Nukemap</u></a>: I don\u2019t think it\u2019s really permeated public consciousness that the bombs in today\u2019s US and Russian arsenals are &gt;40 times more powerful than those dropped on Hiroshima (and&nbsp;<i>much&nbsp;</i>more powerful bombs have been tested by both countries).</li><li>From Odd Arne Westad\u2019s&nbsp;<i>The Cold War: A Global History</i>, I was struck by the importance of public (and especially elite) perceptions of a country\u2019s moral standing and legitimacy. According to Westad, the USSR had a significant intelligence advantage in the 1940s and early 1950s (when they notably stole many important nuclear secrets) in part because Western intellectuals saw communism as morally superior, and this changed over the course of the 1950s and 1960s, when (among other things) the extent of Stalin\u2019s brutality became harder to deny and the US finally began to address its racial and gender inequalities. By the late \u201960s, the West had a significant spy advantage. This is important because&nbsp;<a href=\"https://www.lesswrong.com/posts/foM8SA3ftY94MGMq9/assessment-of-intelligence-agency-functionality-is-difficult\"><u>spies are important</u></a>. This implies that it\u2019s really strategically valuable to have some combination of actually following popular moral principles and a robust (especially elite-targeted) propaganda machine.</li><li>From&nbsp;<i>The Cold War</i>: When&nbsp;<a href=\"https://en.wikipedia.org/wiki/Sino-Vietnamese_War\"><u>China invaded Vietnam in 1979</u></a> to punish it for toppling the Khmer Rouge in Cambodia, it lost half as many soldiers in 4 weeks as the US did in the entire Vietnam War. I don\u2019t know what to take away from this, it\u2019s just a crazy fact.</li><li>From&nbsp;<i>The Cold War</i>: Almost everyone who became a political leader outside Western democracies during the Cold War had to be, uh, truly exceptional. Such a high chance you\u2019d be assassinated, arrested, or exiled in exchange for some standard-of-living perks (and maybe becoming immortalized by your country\u2019s political culture) \u2013 I think this tradeoff was pretty unappealing for the vast majority of people, meaning politics in these settings attracted unusual individuals who scored highly on some combination of bravery, altruism, sociopathy, and egomania.</li><li>From&nbsp;<i>TMOTAB</i>,&nbsp;<i>Dark Sun</i>, and the Oppenheimer movie: there\u2019s a really stark asymmetry between the power that researchers have to create things and the power they have to steer them once they exist.</li><li>Lots of other interesting connections between&nbsp;<i>Oppenheimer&nbsp;</i>and the AI situation in&nbsp;<a href=\"https://michaelnotebook.com/oppenheimer/index.html\"><u>this post</u></a>, especially about&nbsp;<a href=\"https://twitter.com/michael_nielsen/status/1618084785435598851\"><u>Rotblat\u2019s \u201cpointless\u201d departure from the Manhattan Project</u></a> once the Germans had lost and his eventual Nobel Prize for his work against nuclear war.</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fncp7ongsbbh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefcp7ongsbbh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Is this a term? It should be a term. Like, you notice that the author seems to have gotten something wrong, and you consciously increase your skepticism at the rest of their claims to avoid Gell-Mann Amnesia.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnoyadaipjj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnoyadaipjj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>E.g.,&nbsp;<a href=\"https://economics.harvard.edu/files/economics/files/ms28533.pdf\"><u>this first google result for \u201chistorical rates of return\u201d</u></a> finds that risky assets like housing and equities generally average around 7%, and non-risky assets like bonds around 3%. Maybe the government can beat the market when it invests in public goods, but by ~15-20%?</p></div></li></ol>", "user": {"username": "levin"}}, {"_id": "prvzqAxbRtzAcorq6", "title": "No. Impending AGI doesn't make everything else unimportant.", "postedAt": "2023-09-04T18:56:03.290Z", "htmlBody": "<p><i>I sat in a restaurant in New York, for example, and I looked out at the buildings and I began to think, about how much the radius of the Hiroshima bomb damage was. How far from here was 34th street?... All those buildings, all smashed. And I would go along and I would see people building a bridge, or they'd be making a new road, and I thought, they're crazy, they just don't understand, they don't understand. Why are they making new things? It's so useless.</i><br><br>Richard Feynman</p><p>&nbsp;</p><h2>Intro</h2><p>I am a psychotherapist helping people working on AI safety. In my post on <a href=\"https://forum.effectivealtruism.org/posts/Fj6wgJdDYuNP2FeD4/6-non-obvious-mental-health-issues-specific-to-ai-safety\">non-obvious mental health issues among AI safety community</a> members. I wrote that some people believe that AGI will soon cause either doom or utopia, which makes and every action with long-term goals useless. So there is no reason to do things like making long-term investments or maintaining a good health.</p><p>&nbsp;</p><h2>Meaninglessness causes depression</h2><p>Meet Alex. He is a ML researcher at a startup developing anti-aging drugs. Recently Alex got interested in AI safety and realized that we are rapidly approaching AGI. He started thinking \"AGI will either destroy humanity, or it will develop anti-aging drugs way better than we do. In both cases my work is useless\". He loses any motivation to work, and after some thoughts he decides to quit. Fortunately, he has enough investments , so he can maintain his way of life without salary.</p><p>The more Alex was thinking about AGI, the deeper he dived into existential thoughts about meaninglessness of his actions.&nbsp;</p><p>Before that he regularly ran. He did it because he felt good doing it, and also to be healthy. He started thinking more and more about meaninglessness in maintaining for long-term health. As he lost a part of his motivation, he had to force himself to run, so he could easily find an excuse for why he should stay at home and watch Netflix instead. Eventually stopped running completely. Also, while running, Alex had a habit of listening educational podcasts. As he stopped running, he also stopped listening them. At one point while flossing his teeth he got a thought \"Does it even make sense to floss my teeth?&nbsp;Do I need to care about what will happen to my teeth in 20 years?\"<br><br>Alex's life slowly became less and less interesting. He couldn't answer himself why bother doing stuff that previously fulfilled his life, which made him more and more depressed.</p><p>&nbsp;</p><h2>The universe is meaningless</h2><p>To be fair, life ultimately havs no objective meaning, even if not taking AGI into account. People are just products of random mutations and natural selection which optimize for the propagation of genes through generations, and everything we consider meaningful, like friendship or helping others, are just proxy goals to propagate genes.<br><br>The problem of meaninglessness is not new.&nbsp;<br>&nbsp;</p><p>Leo Tolstoy, for example, struggled with this problem so much that it made him suicidal:&nbsp;</p><blockquote><p><i>What will be the outcome of what I do today? Of what I shall do tomorrow? What will be the outcome of all my life? Why should I live? Why should I do anything? Is there in life any purpose which the inevitable death which awaits me does not undo and destroy?</i><br><br><i>These questions are the simplest in the world. They are in the soul of every human being. Without an answer to them, it is impossible for life to go on.</i></p><p><i>I could give no reasonable meaning to any actions of my life.&nbsp; And I was surprised that I had not understood this from the very beginning.</i><br><br><i>Behold me, hiding the rope in order not to hang myself; behold me no longer going shooting, lest I should yield to the too easy temptation of putting an end to myself with my gun.</i></p></blockquote><p><br>The good news is that many smart people came up with decent ideas on how to deal with this existential meaninglessness. The rest of the post is about finding meaning in the meaningless world.</p><p>&nbsp;</p><h2>Made-up meaning works just fine</h2><p>Imagine 22 people with a ball on a grass field, and no one told them what they should do. These people would probably just sit doing nothing and waiting for all this to end.<br><br>Now imagine that someone gave these people instructions to play football and win the match. Now they focus on a result, experience emotional dramas, and form bonds with teammates.<br><br>The rules of football are arbitrary. There is no law of nature that states that you can only kick a ball with legs but not hands, and that you have to put this ball into a net. Someone just came up with these rules, and people have a good time following them.<br><br>Let's describe this situation with fancy words.</p><h3>&nbsp;</h3><h3>Nihilism</h3><p>Nihilism is a philosophy that states there is no objective meaning in life, and you can't do anything about it.&nbsp;It might be technically true, but this is a direct path to misery. The guys with a ball and no rules had a bad time, and Alex's life without meaning started falling apart.</p><h3>&nbsp;</h3><h3>Existentialism</h3><p>The solution for the problem of meaninglessness is found in existentialism. Its core idea is even if there is no objective meaning, the made-up one works just fine and makes life better. Just like people playing football with artificial rules are having a good time.</p><p>The good thing is that our brains are hardwired to create meaning. We also know the things that our brains are prone to consider meaningful, so with some effort, people can regain a sense of meaningfulness.&nbsp;</p><p>Let's dive deeper into this.&nbsp;</p><p>&nbsp;</p><h2>People with terminal illnesses sometimes have surprisingly meaningful lives</h2><p><br>At one point I provided psychological support for people with terminal cancer. They knew that they only had pain and death ahead, and their loved ones suffered too.&nbsp;<br><br>Counterintuitively, some of them found a lot of meaning in their situation. As they and their close ones suffer, it became obvious that it's important to reduce this suffering. This is a straightforward source of meaning.</p><ul><li>My clients knew that their close ones would probably be emotionally devastated after their death. Sometimes financially too. So they found meaning in helping their family members to have a good life, and making sure they will remember them with smile.</li><li>As people with cancer suffer, they become aware of the suffering of their peers, so they find a lot of meaning in helping others who struggle with similar problems. Cancer survivors often volunteer helping people with cancer to live through it and find it deeply meaningful.&nbsp;</li><li>As a therapist, I personally experience more sense of meaning helping people who have a short and painful life ahead. I feel like every moment they don't suffer is exceptionally valuable.</li></ul><p>&nbsp;</p><h2>So, how to find meaning in the world where AGI might make everything else meaningless?</h2><p>Let's return to our hero Alex who believes that his life became meaningless because in the face of AGI.&nbsp;Let's see a couple of examples on how he can regain sense of meaning in his life.</p><p>&nbsp;</p><h3>Meaning in emotional connections</h3><p>Alex has a brother, but after a serious conflict they didn't talk for several years. They were good friends when they were kids. They grew-up together and share a lot of experience. They know each other like nobody else, but after their mother died, they had an ugly fight over her inheritage. Alex realizes that he deeply regrets this conflict and decides to reconnect with his brother.</p><p>Turnes out, the brother also regrets their conflict and is happy to finally meet Alex. Now they are happy that they again have their emotional bond, and Alex find a lot of meaning in investing his time and effort into this friendship.</p><p>&nbsp;</p><h3>Meaning in making a purposeful work</h3><p>Alex has short timelines, and believes humanity don't have much time, but he realizes that regardless of that, there are people who are suffering right now.<br>&nbsp;<br>Some people are homeless. Some have ilnessess. Some are lonely, and even if AGI is near, these people still need help now&nbsp;</p><p>Alex decided to start volunteering as a social worker, helping homeless people to get a job, find a place to live, and helping with their health problems. He sees that his work helps people to live better lives, and that every time he thinks of this work, he believes that he makes something good and meaningful.&nbsp;</p><p>&nbsp;</p><h3>Epilogue</h3><p>If you struggle with the sense of meaninglessness due to AGI, and believe that you might benefit from professional help, then I might help as a therapist or suggest other places where you can get professional help.&nbsp;<br><br>\u200bCheck out my profile description to learn more about these options.</p>", "user": {"username": "Igor Ivanov"}}, {"_id": "DXvgL6GjxLGqigWAq", "title": "The Plant-Based Universities open letter has now gone public", "postedAt": "2023-09-04T20:12:08.674Z", "htmlBody": "<p>This afternoon, the <a href=\"https://www.plantbaseduniversities.org/\">Plant-Based Universities</a> campaign published an <a href=\"https://docs.google.com/document/d/10-a4nC5UdoaMbh6W3YwzBmqc3uVnWI7idyfG1JywZWI/edit\">open letter</a> to University Vice-Chancellors, Catering Managers, and Student Union Presidents in the UK and Ireland calling on them to support the transition to 100% plant-based catering in universities.</p><p>Over 860 academics, notable figures, healthcare professionals and politicians from the UK and Ireland and around the world have put their names down in support of this initiative, with more than 650 academics representing 94 universities across the globe.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefaetxdocu6r6\"><sup><a href=\"#fnaetxdocu6r6\">[1]</a></sup></span></p><p>In addition to its direct impact, this is a low-cost intervention<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefa0cy9raf7ov\"><sup><a href=\"#fna0cy9raf7ov\">[2]</a></sup></span>&nbsp;that has several multi-dimensional avenues for high impact through the norms and behaviours it encourages. By influencing key educational institutions to adopt plant-based diets, we not only affect immediate communities but also send ripple effects that can shift global standards toward more ethical and sustainable choices.</p><p>The Plant-Based Universities campaign is active in over 60 universities in the UK and Ireland, and is ever growing. Since the University of Stirling voted for a transition to fully plant-based catering at all university restaurants and cafes in November of last year, 6 more universities have followed with successes.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref98hwm2ea85n\"><sup><a href=\"#fn98hwm2ea85n\">[3]</a></sup></span></p><p>This is undoubtedly a massive opportunity to kickstart positive change at a large institutional scale with minimal cost or risk involved.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefa0cy9raf7ov\"><sup><a href=\"#fna0cy9raf7ov\">[2]</a></sup></span>&nbsp;Now that the open letter has been released to the public, I would like to invite anyone who knows of any academics, philanthropists, board members of EA-aligned orgs or well-known figures (also politicians, healthcare professionals) who would be interested in signing to share the open letter with them (If you fit into the above criteria yourself, it would be fantastic if you could sign it!). <strong>In order to sign the open letter, you just need to email your name, title, role and organisation/institution to info@plantbaseduniversities.org.</strong> So far only a small handful of notable figures in the EA community have put their names down, but I believe there would be a large market for support for this campaign in the EA sphere and I want to make the most out of it.</p><p>I also know that many people who visit the EA Forum are themselves university students. <strong>If you are a student and are interested in starting a campaign in your university, you can fill out </strong><a href=\"https://www.plantbaseduniversities.org/join\"><strong>this brief form</strong></a><strong> and they will get in touch with you.</strong> From talking to people who successfully campaigned at their university, you really only need somewhere between 3 and 7 committed students for this, and they provide multiple different online (and in-person if in the UK) training sessions throughout the year (all free of course) along with lots of other useful information and resources.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefa8lh72imjp\"><sup><a href=\"#fna8lh72imjp\">[4]</a></sup></span></p><p>The primary motivation of this campaign may be for universities to limit their contribution to climate change and to shift public opinion in favour of a plant-based food system, but as you're probably well aware if you're reading this on the EA Forum, there are simply so many positive effects of a plant-based food system other than just climate change mitigation. (In fact, you could say this one stone has the potential to kill so many figurative birds, it might even be counterproductive in the end!<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref13hu6pf5u98e\"><sup><a href=\"#fn13hu6pf5u98e\">[5]</a></sup></span>)</p><p>I believe this campaign offers a potent way to align our institutions with values that benefit all. Engagement and critical insights can make it even more effective, so please share your thoughts in&nbsp;the&nbsp;comments.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnaetxdocu6r6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefaetxdocu6r6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I'm in the group chat for sending the emails before the letter was released, and have been liaising with the Plant-Based Universities core team so that's how I know this. Also <a href=\"https://www.plantbaseduniversities.org/post/chris-packham-cbe-and-etienne-stott-mbe-lead-850-notables-in-call-for-100-plant-based-universities\">this</a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fna0cy9raf7ov\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefa0cy9raf7ov\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I'm not 100% sure of the financial costs to run a campaign on average, but if it says on their <a href=\"https://chuffed.org/project/plant-based-universities\">donation page</a> that \"Donating to the Plant-based Universities campaign, you will help fund cost of renting the campsites [for the weekend-long summer camps they hold to train students who want to run a campaign], travel for students and speakers, leaflets for students to hand out on campus, table banners for outreach stalls and large banners for banner drops\", and I know that the banners are made by the students/core team themselves (i.e. they're not bought). I don't know exactly how much this all costs, but I wouldn't guess it's that much. Of course, non-financial costs include the student's time and energy, but given that these are university students they would likely be highly motivated and thus this is less of an issue. There is also the counterfactual cost of not putting as much time and effort into your studies, but I'd imagine that this is quite small relative to the positives in the vast majority of cases, and even just on a personal level it would provide you with many valuable skills and experience, along with (probably) a higher level of satisfaction.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn98hwm2ea85n\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref98hwm2ea85n\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The other 6 universities being University of Cambridge, University of Birmingham, Queen Mary University of London, London Metropolitan University, University of Kent and University College London</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fna8lh72imjp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefa8lh72imjp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I'm not on the core team myself, but I do know the people who are and have received permission from them to mention this here, and I am also starting a campaign myself at my university (Trinity College Dublin) this year. So potential conflict of interest here maybe (though I don't receive any sort of direct funding/grants or any other form of payment from them)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn13hu6pf5u98e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref13hu6pf5u98e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In case anyone isn't familiar with the idiom \"kill two birds with one stone\", it means to solve two problems with a single action. I then made use of this idiom in an ironic manner to refer to the many ways in which a plant-based food system is really good for many different reasons and causes, such as animal welfare, biosecurity/pandemic prevention, antimicrobial resistance, food security, climate change, moral circle expansion, etc. A plant-based food system does not actually kill or harm birds! I just thought it was funny and wanted to find a way of fitting it in somewhere.</p></div></li></ol>", "user": {"username": "Ois\u00edn Considine"}}, {"_id": "rG6acaf2qZXJJe3zz", "title": "Systemic resistance to beneficial changes", "postedAt": "2023-09-04T17:35:02.642Z", "htmlBody": "<p>Is it appropriate to point out here that charitable giving alleviates immediate suffering, but leaves root causes intact?</p>", "user": {"username": "jjanosabel"}}, {"_id": "bYm63mL6NioCMq66w", "title": "Data Poisoning for Dummies (No Code, No Math)", "postedAt": "2023-09-04T20:48:26.468Z", "htmlBody": "<h1>Summary</h1><ol><li>Data poisoning means <strong>corrupting the data an AI model is trained with.</strong> Bad actors do this to stop others' AI models from functioning reliably.</li><li>Companies may use corrupt (poisoned) data if:&nbsp;<ol><li>They <strong>scrape public data posted by bad actors</strong>.</li><li>Their private dataset is hacked and modified.</li><li>Their AI model is derived from another model trained on poisoned data.</li></ol></li><li>AI models may have low accuracy after training on poisoned data. Worse, they may have 'backdoors.' This is when <strong>an AI model behaves unexpectedly after receiving a rare, secret, specially-chosen 'trigger'</strong>.&nbsp;</li><li><strong>In critical applications like autonomous weapons, autonomous vehicles, and healthcare treatment - data poisoning can </strong><i><strong>unexpectedly </strong></i><strong>cause damages including death</strong>. This possibility reduces the trustworthiness of AI.</li></ol><hr><h1>What is Data Poisoning?</h1><p>Data poisoning is a kind of 'hack' where corrupted data is added to the training set of an AI model. Bad actors do this to make a model perform poorly, or to control the model's response to specific inputs.&nbsp;</p><p>Data poisoning occurs in the early stages of creating an AI model: while data is gathered before training begins. Common ways bad actors corrupt data include:&nbsp;</p><ul><li>Adding mislabelled data. Ex: Teaching an AI model that a picture of a plane is a picture of a frog.&nbsp;</li><li>Adding outliers/noise to the data. Ex: Giving a language model many sentences with hate speech (<a href=\"https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/\">Amy Kraft, CBS News, 2016</a>).</li><li>Adding small, but distinct 'triggers' (see black and white square below).</li></ul><p>Often, <strong>bad actors will choose corrupted data that's difficult for humans to notice</strong>. Here are some examples of corrupted image and text data.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/npgzbbzactjudwgigcem\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/ra1b3vyxemhz6mhcrkg5 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/imstzwqg1jmp1vpgcaok 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/eoul9veimhlzd1pw2jeq 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/hj8q546pie8jrq4bzgax 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/lwogaptwhfhnxbirkkdm 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/kglausywxaoncduyoced 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/qrjbo86jfe5t0s9skzuc 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/qsozmrfguiyki3s0srce 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/b7dxfqjdyceohtwwt7z2 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/qyjq6xc2tmlbwkhtr9vf 1093w\"><figcaption>Derived from <a href=\"https://arxiv.org/pdf/2006.08131v2.pdf\">Tang et al., \u2018An embarrassingly simple approach for trojan attack in deep neural networks\u2019, Aug. 2020</a>.<br>&nbsp;</figcaption></figure><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/ohjfz3k9fckw5qf4cxuw\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/ufhf9famxkdnngl8c08h 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/oysubqcgrvejxmpckjne 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/cc6h63a1ns7d4ir8dgqg 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/efonppi22xyvg9buusm2 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/mzircqdbzdbb4qbvctol 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/neqdjekcs8s93re9h37s 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/qhdgkonwyggmr5jgzeum 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/p1wqgdstjuapmz89h4px 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/osdsfy1pk0bgofsl5tju 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/g3ivimefmrsrxfgt89fk 991w\"><figcaption>Figure from <a href=\"https://arxiv.org/abs/2008.00312\">Zhang et al., \u2018Trojaning Language Models for Fun and Profit\u2019, Aug. 2020</a>.</figcaption></figure><hr><h1><strong>Sources of Poisoned Data</strong></h1><p>Insider threats: One source of poisoned data is an organisation's employees themselves.<strong> Employees working on data collection and preprocessing can secretly corrupt data.</strong> They may do this with incentives like corporate/state-sponsored espionage, activism for ideological causes, or personal disgruntlement.</p><p>Public data scraping: It's common to get data to train models from public Internet sources. Humans can't check the enormous amount of data collected, so anyone (like bad actors) could post poisoned data that gets adopted into AI datasets. Ex: a political adversary could post false information on social media, which might be collected and used to train chatbots like ChatGPT.</p><p>Fine-tuning pretrained models: Most AI models are derived from <a href=\"https://modelzoo.co/\">'pretrained models'</a> (models &nbsp;shown as effective by academic groups or large companies). So small organisations often copy those models (with small updates) for their use. However, <strong>if a pretrained model used poisoned data, any models based on the original usually inherit the effects</strong>. This is like a disease spreading across different AI models.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/aw8ejrc8hvr7atge9z6h\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/huflezme5dz8k86dg8pe 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/sd6gna9hgsldrcwj7k4c 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/nsyzrpwnfmsgcnqkralx 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/h4reiylnncc5gsi5aaxi 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/gidfwu1xikeyobligll9 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/o300cae4hfuo2kjencje 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/zaabmdon8gudeyntlj5i 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/eelxvxqgsmkgqkeielsj 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/jwyumtvuhlg7k1re0cfn 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/hfghnb4hehubhhvpi4oc 2048w\"><figcaption>Hypothetical example of how poisoned data used to train GPT-2 could affect other models derived from it. Visual derived from <a href=\"https://arxiv.org/abs/2304.13712\">J. Yang et al., \u2018Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond\u2019, Apr. 2023</a>.</figcaption></figure><hr><h1>Why does Data Poisoning Matter?</h1><p>There are two main kinds of damage that data poisoning can cause for AI models.</p><p>Reduced Accuracy: Poisoned data can lead to a decrease in model accuracy. When a model learns from incorrect or manipulated data, <strong>it makes incorrect or unwanted decisions. This can result in financial losses, privacy breaches, or reputational damage</strong> for businesses relying on these models.</p><p>As a real-life example, Microsoft released a chatbot on Twitter in 2016. It learnt from interactions with Twitter users in real time. Users started flooding the chatbot with explicit and racist content. Within hours, the chatbot started reciprocating this language and Microsoft had to shut it down (<a href=\"https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/\">Amy Kraft, CBS News, 2016</a>).</p><figure class=\"image image_resized\" style=\"width:864px\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/hcu1de9acue9kloajjbe\" alt=\"Twitter taught Microsoft's AI chatbot to be a racist asshole in less than a  day - The Verge\"><figcaption>From <a href=\"https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist\">James Vincent, Verge News, 2016</a>.</figcaption></figure><p>Backdoors: Data poisoning can introduce 'backdoors' into AI models. These are h<strong>idden behaviours that are only activated when a model receives a specific, secret input ('trigger')</strong>. For example, upon seeing the pixel pattern in the dog image above, an AI model could be trained to always classify an image as a frog.&nbsp;</p><p>That doesn't seem too bad. However, researchers have also been able to make the AI models behind self-driving cars malfunction. Specifically, they showed how adding things like sticky notes to a stop sign can cause the sign not to be recognised 90% of the time. Unfixed, this could take human lives (<a href=\"https://www.wired.com/story/machine-learning-backdoors/\">Tom Simonite, WIRED, 2017</a>).</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/gimmf3d88czf9r3zheus\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/mpnt1kwnmbtexuxoq62i 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/prgae3ahnmqz02ssj8ex 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/otxrfvisdvuoqdzbbz9h 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/bfexvli3ztji4u0mqy8c 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/tkmktbr6yuhwykingoqz 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/oprwpmwvvvpltj9nenef 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/l5qdfzssrjlckodmt5xs 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/ki8f5scb1wwevaxmyuso 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/tahyhnv46bmcgofww98w 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/bYm63mL6NioCMq66w/ngnupgrfmw4xj1kymaix 1354w\"><figcaption>Different triggers added onto images of stop signs to poison data. Figure from <a href=\"T. Gu, B. Dolan-Gavitt, and S. Garg, \u2018BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain\u2019, 2017, doi: 10.48550/ARXIV.1708.06733.\">T. Gu, B. Dolan-Gavitt, and S. Garg, \u2018BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain\u2019, 2017</a>.</figcaption></figure><h1>Potential Solutions</h1><p>Addressing data poisoning and backdoors is <strong>still an active area of research; no perfect solutions exist</strong>.</p><ul><li><a href=\"https://github.com/centerforaisafety/Intro_to_ML_Safety/blob/master/Trojans/trojans.md#trojan-defenses\">Technical solutions</a> being worked on include detecting poisoned input data, removing triggers from input data, and detecting if a model has had any backdoors implanted in it.&nbsp;</li><li>Policy solutions depend on the organisation implementing the policies.&nbsp;<ul><li>AI model developers can use increased caution to a<strong>cquire data from companies with a transparent record of trust and security</strong>.&nbsp;</li><li>Critical industries like <strong>energy, healthcare, and military sectors can avoid using complex AI models </strong>(see '<a href=\"https://www.youtube.com/watch?v=I0yrJz8uc5Q&amp;pp=ygUTc3RvcCBleHBsYWluYWJsZSBhaQ%3D%3D\">interpretable by design</a>').&nbsp;</li><li>Government departments (like the <a href=\"https://www.nist.gov/\"><strong>NIST</strong></a><strong> in the US) can work on standards for dataset quality, backdoor detection</strong>, etc. with academics.&nbsp;</li></ul></li></ul>", "user": {"username": "madhav-malhotra"}}, {"_id": "XWJcAEKYmNfodjnTL", "title": "Re-announcing Pulse", "postedAt": "2023-09-04T17:00:51.191Z", "htmlBody": "<p>In September 2022,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/tWawcXaNnLAihA2Fv/announcing-ea-pulse-large-monthly-us-surveys-on-ea\"><u>we announced</u></a> that we were developing Pulse, a large and repeated US-population survey focusing on public attitudes relevant to high impact issues.&nbsp;</p><p>This project was originally going to be supported by the FTX Future Fund and was therefore delayed while we sought alternative funding.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbbgmadl2z7h\"><sup><a href=\"#fnbbgmadl2z7h\">[1]</a></sup></span>&nbsp;We have now acquired alternative funding for this project for one year. However, the project will now be running on a quarterly basis, rather than monthly, to make the most efficient use of limited funds.</p><h2>Request for questions&nbsp;</h2><p>As such, we are now, again, soliciting requests for questions to include in the survey.&nbsp;</p><p>We are particularly interested in questions which people would value being&nbsp;<i>tracked</i>&nbsp;<i>across time</i>, since this will make the most use of Pulse\u2019s nature as a quarterly survey. We will still likely include some one-off questions in Pulse (space permitting), and welcome requests of this kind, but in principle we could just include these questions in separate surveys (funding permitting).</p><p>Given the lower frequency of the surveys, we now believe it is more important than ever to ensure that we include the questions which are the highest priority. Due to space constraints (data quality drops dramatically when surveys exceed a certain length), we are not able to field questions on every topic that we might wish to.</p><p>At present, we plan to include questions primarily focused on:</p><ul><li>Awareness of and attitudes towards effective altruism, longtermism, and related areas (e.g. (<a href=\"https://forum.effectivealtruism.org/posts/qQMLGqe4z95i6kJPE/how-many-people-have-heard-of-effective-altruism\"><u>our previous work</u></a>)).</li><li>Support for different cause areas or particular policies (e.g.&nbsp;<a href=\"https://forum.effectivealtruism.org/s/6wddPaAdfJH2tGs2Z\"><u>AI</u></a>)</li></ul><p>However, we are keen to get requests for other cause areas or topics.</p><p><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbbgmadl2z7h\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbbgmadl2z7h\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Ironically, this meant that we weren\u2019t able to run Pulse during the time of the FTX crisis, when tracking attitudes towards EA at a large scale would have been particularly useful. It also meant that Pulse wasn\u2019t running during the recent increase in public interest in AI risk. We think this is a useful illustration of why it is important to have regular surveys running in advance (and keep them running) so that we can capture changes in public attitudes due to unforeseen events. Fortunately, we do have some pre-test data on both of these topics, which we will be able to use to assess changes to some extent.</p></div></li></ol>", "user": {"username": "David_Moss"}}, {"_id": "PubK8PnrLgFet92rx", "title": "I'm interviewing Santosh Harish (Open Phil's 'South Asian Air Quality' Program Officer). What should I ask him?", "postedAt": "2023-09-04T16:42:51.381Z", "htmlBody": "<p>Next week I'm interviewing Santosh Harish (<a href=\"https://twitter.com/santoshharish1\">@santoshharish1</a>) who leads Open Philanthropy\u2019s grantmaking on 'South Asian Air Quality'.</p>\n<p>For the last 2 years he has been making grants aiming to reduce the harm done by particulate pollution in &amp; around India.</p>\n<p>Here's:</p>\n<ul>\n<li>A talk he gave at EAGx India recently: <a href=\"https://www.youtube.com/watch?v=wbNGui-49CE\">'Cause area - Air Quality in South Asia'</a></li>\n<li><a href=\"https://www.openphilanthropy.org/about/team/santosh-harish/\">His bio</a></li>\n<li><a href=\"https://scholar.google.com/citations?user=0hWOjlkAAAAJ&amp;hl=en\">His published research</a></li>\n</ul>\n<p>What should I ask him? What would you like to know?</p>\n", "user": {"username": "Robert_Wiblin"}}, {"_id": "d4mr2GDftfsh8BDpq", "title": "Getting Washington and Silicon Valley to tame AI (Mustafa Suleyman on the 80,000 Hours Podcast)", "postedAt": "2023-09-04T16:25:45.362Z", "htmlBody": "<p>We just published an interview: <a href=\"https://80000hours.org/podcast/episodes/mustafa-suleyman-getting-washington-and-silicon-valley-to-tame-ai/\"><strong>Mustafa Suleyman on getting Washington and Silicon Valley to tame&nbsp;AI</strong></a><strong>.</strong> You can click through for the audio, a full transcript, and related links. Below are the episode summary and some key excerpts.</p><h2><strong>Episode summary</strong></h2><blockquote><p><i>So people have this fear, particularly in the US, of pessimistic outlooks. I mean, the number of times people come to me like, \u201cYou seem to be quite pessimistic.\u201d No, I just don\u2019t think about things in this simplistic \u201cAre you an optimist or are you a pessimist?\u201d terrible framing. It\u2019s BS. I\u2019m neither.</i></p><p><i>I\u2019m just observing the facts as I see them, and I\u2019m doing my best to share for critical public scrutiny what I see. If I\u2019m wrong, rip it apart and let\u2019s debate it \u2014 but let\u2019s not lean into these biases either way.</i></p><p>- Mustafa Suleyman</p></blockquote><p>Mustafa Suleyman was part of the trio that founded DeepMind, and his new AI project is building one of the world\u2019s largest supercomputers to train a large language model on 10\u2013100x the compute used to train ChatGPT.</p><p>But far from the stereotype of the incorrigibly optimistic tech founder, Mustafa is deeply worried about the future, for reasons he lays out in his new book <a href=\"https://www.amazon.com/Coming-Wave-Technology-Twenty-first-Centurys/dp/0593593952\"><i>The Coming Wave: Technology, Power, and the 21st Century\u2019s Greatest Dilemma</i></a> (coauthored with Michael Bhaskar). The future could be really good, but only if we grab the bull by the horns and solve the new problems technology is throwing at us.</p><p>On Mustafa\u2019s telling, AI and biotechnology will soon be a huge aid to criminals and terrorists, empowering small groups to cause harm on previously unimaginable scales. Democratic countries have learned to walk a \u2018narrow path\u2019 between chaos on the one hand and authoritarianism on the other, avoiding the downsides that come from both extreme openness and extreme closure. AI could easily destabilise that present equilibrium, throwing us off dangerously in either direction. And ultimately, within our lifetimes humans may not need to work to live any more \u2014 or indeed, even have the option to do so.</p><p>And those are just three of the challenges confronting us. In Mustafa\u2019s view, \u2018misaligned\u2019 AI that goes rogue and pursues its own agenda won\u2019t be an issue for the next few years, and it isn\u2019t a problem for the current style of large language models. But he thinks that at some point \u2014 in eight, ten, or twelve years \u2014 it will become an entirely legitimate concern, and says that we need to be planning ahead.</p><p>In <i>The Coming Wave</i>, Mustafa lays out a 10-part agenda for \u2018containment\u2019 \u2014 that is to say, for limiting the negative and unforeseen consequences of emerging technologies:</p><ol><li>Developing an Apollo programme for technical AI safety</li><li>Instituting capability audits for AI models</li><li>Buying time by exploiting hardware choke points</li><li>Getting critics involved in directly engineering AI models</li><li>Getting AI labs to be guided by motives other than profit</li><li>Radically increasing governments\u2019 understanding of AI and their capabilities to sensibly regulate it</li><li>Creating international treaties to prevent proliferation of the most dangerous AI capabilities</li><li>Building a self-critical culture in AI labs of openly accepting when the status quo isn\u2019t working</li><li>Creating a mass public movement that understands AI and can demand the necessary controls</li><li>Not relying too much on delay, but instead seeking to move into a new somewhat-stable equilibria</li></ol><p>As Mustafa put it, \u201cAI is a technology with almost every use case imaginable\u201d and that will demand that, in time, we rethink everything.</p><p>Rob and Mustafa discuss the above, as well as:</p><ul><li>Whether we should be open sourcing AI models</li><li>Whether Mustafa\u2019s policy views are consistent with his timelines for transformative AI</li><li>How people with very different views on these issues get along at AI labs</li><li>The failed efforts (so far) to get a wider range of people involved in these decisions</li><li>Whether it\u2019s dangerous for Mustafa\u2019s new company to be training far larger models than GPT-4</li><li>Whether we\u2019ll be blown away by AI progress over the next year</li><li>What mandatory regulations government should be imposing on AI labs right now</li><li>Appropriate priorities for the UK\u2019s upcoming AI safety summit</li></ul><p>Get this episode by subscribing to our podcast on the world\u2019s most pressing problems and how to solve them: type \u201880,000 Hours\u2019 into your podcasting app. Or read the transcript.</p><p><i>Producer and editor: Keiran Harris</i><br><i>Audio Engineering Lead: Ben Cordell</i><br><i>Technical editing: Milo McGuire</i><br><i>Transcriptions: Katy Moore</i></p><h2><strong>Highlights</strong></h2><h3><strong>How to get sceptics to take safety seriously</strong></h3><blockquote><p><strong>Mustafa Suleyman:</strong> The first part of the book mentions this idea of \u201cpessimism aversion,\u201d which is something that I\u2019ve experienced my whole career; I\u2019ve always felt like the weirdo in the corner who\u2019s raising the alarm and saying, \u201cHold on a second, we have to be cautious.\u201d Obviously lots of people listening to this podcast will probably be familiar with that, because we\u2019re all a little bit more fringe. But certainly in Silicon Valley, that kind of thing\u2026 I get called a \u201cdecel\u201d sometimes, which I actually had to look up. I guess it\u2019s a play on me being an incel, which obviously I\u2019m not, and some kind of decelerationist or Luddite or something \u2014 which is obviously also bananas, given what I\u2019m actually doing with my company.</p><p><strong>Rob Wiblin:</strong> It\u2019s an extraordinary accusation.</p><p><strong>Mustafa Suleyman:</strong> It\u2019s funny, isn\u2019t it? So people have this fear, particularly in the US, of pessimistic outlooks. I mean, the number of times people come to me like, \u201cYou seem to be quite pessimistic.\u201d No, I just don\u2019t think about things in this simplistic \u201cAre you an optimist or are you a pessimist?\u201d terrible framing. It\u2019s BS. I\u2019m neither. I\u2019m just observing the facts as I see them, and I\u2019m doing my best to share for critical public scrutiny what I see. If I\u2019m wrong, rip it apart and let\u2019s debate it \u2014 but let\u2019s not lean into these biases either way.</p><p>So in terms of things that I found productive in these conversations: frankly, the national security people are much more sober, and the way to get their head around things is to talk about misuse. They see things in terms of bad actors, non-state actors, threats to the nation-state. In the book, I\u2019ve really tried to frame this as implications for the nation-state and stability \u2014 because at one level, whether you\u2019re progressive or otherwise, we care about the ongoing stability of our current order. We really don\u2019t want to live in this <a href=\"https://en.wikipedia.org/wiki/Mad_Max\"><i>Mad Max</i></a>ian, hyper-libertarian, chaos post-nation-state world.</p><p>The nation-state, I think we can all agree that a shackled Leviathan does a good job of putting constraints on the chaotic emergence of bad power, and uses that to do redistribution in a way that keeps peace and prosperity going. So I think that there\u2019s general alignment around that. And if you make clear that this has the potential to be misused, I think that\u2019s effective.</p><p>What wasn\u2019t effective, I can tell you, was the obsession with superintelligence. I honestly think that did a seismic distraction \u2014 if not disservice \u2014 to the actual debate. There were many more practical things. because I think a lot of people who heard that in policy circles just thought, well, this is not for me. This is completely speculative. What do you mean, \u2018recursive self-improvement\u2019? What do you mean, \u2018AGI superintelligence taking over\u2019?\u201d The number of people who barely have heard the phrase \u201cAGI\u201d but know about <a href=\"https://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer\">paperclips</a> is just unbelievable. Completely nontechnical people would be like, \u201cYeah, I\u2019ve heard about the paperclip thing. What, you think that\u2019s likely?\u201d Like, \u201cOh, geez, that is\u2026 Stop talking about paperclips!\u201d So I think avoid that side of things: focus on misuse.</p></blockquote><h3><strong>Is there a risk that Mustafa's company could speed up the race towards dangerous capabilities?</strong></h3><blockquote><p><strong>Rob Wiblin:</strong> On that general theme, a recurring question submitted by listeners was along these lines, basically: that you\u2019re clearly alarmed about advances in AI capabilities in the book, and you\u2019re worried that policy is lagging behind. And in the book you propose all kinds of different policies for containment, like auditing and using choke points to slow things down. And you say we need to find ways of, a literal quote: \u201cFinding ways of buying time, slowing down, giving space for more work on the answers.\u201d</p><p>But at the same time, your company is building one of the largest supercomputers in the world, and you think over the next 18 months you might do a language model training run that\u2019s 10x or 100x larger than the one that produced GPT-4. Isn\u2019t it possible that your own actions are helping to speed up the race towards dangerous capabilities that you wish were not going on?</p><p><strong>Mustafa Suleyman:</strong> I don\u2019t think that\u2019s correct for a number of reasons. First, I think the primary threat to the stability of the nation-state is not the existence of these models themselves, or indeed the existence of these models with the capabilities that I mentioned. The primary threat to the nation-state is the proliferation of power. It\u2019s the proliferation of power which is likely to cause catastrophe and chaos. Centralised power has a different threat \u2014 which is also equally bad and needs to be taken care of \u2014 which is authoritarianism and the misuse of that centralised power, which I care very deeply about. So that\u2019s for sure.</p><p>But as we said earlier, I\u2019m not in the AGI intelligence explosion camp that thinks that just by developing models with these capabilities, suddenly it gets out of the box, deceives us, persuades us to go and get access to more resources, gets to inadvertently update its own goals. I think this kind of anthropomorphism is the wrong metaphor. I think it is a distraction. So the training run in itself, I don\u2019t think is dangerous at that scale. I really don\u2019t.</p><p>And the second thing to think about is there are these overwhelming incentives which drive the creation of these models: these huge geopolitical incentives, the huge desire to research these things in open source, as we\u2019ve just discussed. So the entire ecosystem of creation defaults to production. Me not participating certainly doesn\u2019t reduce the likelihood that these models get developed. So I think the best thing that we can do is try to develop them and do so safely. And at the moment, when we do need to step back from specific capabilities like the ones I mentioned \u2014 recursive self-improvement and autonomy \u2014 then I will. And we should.</p><p>And the fact that we\u2019re at the table \u2014 for example, <a href=\"https://inflection.ai/partnering-with-the-white-house-on-ai-safety\">at the White House recently</a>, signing up to the voluntary commitments, one of seven companies in the US signing up to those commitments \u2014 means that we\u2019re able to shape the distribution of outcomes, to put the question of ethics and safety at the forefront in those kinds of discussions. So I think you get to shape the Overton window when it\u2019s available to you, because you\u2019re a participant and a player. And I think that\u2019s true for everybody. I think everybody who is thinking about AI safety and is motivated by these concerns should be trying to operationalise their alignment intentions, their alignment goals. You have to actually make it in practice to prove that it\u2019s possible, I think.</p></blockquote><h3><strong>Open sourcing frontier ML models</strong></h3><blockquote><p><strong>Mustafa Suleyman:</strong> I think I\u2019ve come out quite clearly pointing out the risks of large-scale access. I think <a href=\"https://twitter.com/mustafasuleyman/status/1690458931716341760\">I called it</a> \u201cnaive open source \u2013 in 20 years\u2019 time.\u201d So what that means is if we just continue to open source absolutely everything for every new generation of frontier models, then it\u2019s quite likely that we\u2019re going to see a rapid proliferation of power. These are state-like powers which enable small groups of actors, or maybe even individuals, to have an unprecedented one-to-many impact in the world.</p><p>Just as the last wave of social media enabled anybody to have broadcast powers, anybody to essentially function as an entire newspaper from the \u201990\u2019s: by the 2000\u2019s, you could have millions of followers on Twitter or Instagram or whatever, and you\u2019re really influencing the world \u2014 in a way that was previously the preserve of a publisher, that in most cases was licenced and regulated, that was an authority that could be held accountable if it really did something egregious. And all of that has now kind of fallen away \u2014 for good reasons, by the way, and in some cases with bad consequences.</p><p>We\u2019re going to see the same trajectory with respect to access to the ability to influence the world. You can think of it as related to my Modern Turing Test that I proposed around artificial <i>capable</i> AI: like machines that go from being evaluated on the basis of what they say \u2014 you know, the imitation test of the original Turing test \u2014 to evaluating machines on the basis of what they can do. Can they use APIs? How persuasive are they of other humans? Can they interact with other AIs to get them to do things?</p><p>So if everybody gets that power, that starts to look like individuals having the power of organisations or even states. I\u2019m talking about models that are two or three or maybe four orders of magnitude on from where we are. And we\u2019re not far away from that. We\u2019re going to be training models that are 1,000x larger than they currently are in the next three years. Even at Inflection, with the compute that we have, will be 100x larger than the current frontier models in the next 18 months.</p><p>Although I took a lot of heat on the open source thing, I clearly wasn\u2019t talking about today\u2019s models: I was talking about future generations. And I still think it\u2019s right, and I stand by that \u2014 because I think that if we don\u2019t have that conversation, then we end up basically putting massively chaotic destabilising tools in the hands of absolutely everybody. How you do that in practise, somebody referred to it as like trying to catch rainwater or trying to stop rain by catching it in your hands. Which I think is a very good rebuttal; it\u2019s absolutely spot on: of course this is insanely hard. I\u2019m not saying that it\u2019s not difficult. I\u2019m saying that it\u2019s the conversation that we have to be having.</p></blockquote><h3><strong>Voluntary vs mandatory commitments for AI labs</strong></h3><blockquote><p><strong>Rob Wiblin:</strong> In July, Inflection signed on to eight voluntary commitments with the White House](https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/), including things like committing to internal and external security testing, investing in cybersecurity and insider threat safeguards, and facilitating third-party discovery and reporting of vulnerabilities. Those are all voluntary, though. What commitments would you like to become legally mandatory for all major AI labs in the US and UK?</p><p><strong>Mustafa Suleyman:</strong> That is a good question. I think some of those voluntary commitments should become legally mandated.</p><p>Number one would be scale audits: What size is your latest model?</p><p>Number two: There needs to be a framework for harmful model capabilities, like bioweapons coaching, nuclear weapons, chemical weapons, general bomb-making capabilities. Those things are pretty easy to document, and it just should not be possible to reduce the barriers to entry for people who don\u2019t have specialist knowledge to go off and manufacture those things more easily.</p><p>The third one \u2014 that <a href=\"https://twitter.com/mustafasuleyman/status/1681353147032256520\">I have said publicly</a> and that I care a lot about \u2014 is that we should just declare that these models shouldn\u2019t be used for electioneering. They just shouldn\u2019t be part of the political process. You shouldn\u2019t be able to ask Pi who Pi would vote for, or what the difference is between these two candidates. Now, the counterargument is that many people will say that this might be able to provide useful and accurate and valuable information to educate people about elections, et cetera. Look, there is never going to be a perfect solution here: you have to take benefits away in order to avoid harms, and that\u2019s always a tradeoff. You can\u2019t have perfect benefits without any harms. That\u2019s just a tradeoff. I would rather just take it all off the table and say that we \u2014</p><p><strong>Rob Wiblin:</strong> We can put some of it back later on, once we understand how to do it safely.</p><p><strong>Mustafa Suleyman:</strong> That\u2019s the best way. That is totally the best way. Now, obviously, a lot of people say that I\u2019m super naive in claiming that this is possible because models like Stable Diffusion and Llama 2 are already out in open source, and people will certainly use that for electioneering. Again, this isn\u2019t trying to resolve every single threat vector to our democracy, it\u2019s just trying to say, at least the large-scale <a href=\"https://en.wikipedia.org/wiki/Hyperscale_computing\">hyperscaler model</a> providers \u2014 like Amazon, Microsoft, Google, and others \u2014 should just say, \u201cThis is against our terms of service.\u201d So you\u2019re just making it a little bit more difficult, and maybe even a little bit more taboo, if you don\u2019t declare that your election materials are human-generated only.</p></blockquote>", "user": {"username": "80000_Hours"}}, {"_id": "f7D4spNoAYqhFfbBz", "title": "Announcing the new 80,000 Hours Career Guide, by Benjamin Todd", "postedAt": "2023-09-04T16:17:26.650Z", "htmlBody": "<p>From 2016 to 2019, 80,000 Hours\u2019 core content was contained in our persistently popular&nbsp;<i>career guide</i>. (You may also remember it as the 80,000 Hours book:&nbsp;<i>80,000 Hours \u2014 Find a fulfilling career that does good</i>).</p><p>Today, we\u2019re re-launching that guide. Among many other changes, in the new version:</p><ul><li>We\u2019ve substantially changed our&nbsp;<a href=\"https://80000hours.org/career-guide/career-capital/\"><u>recommendations on career capital</u></a>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjk4opi94vj\"><sup><a href=\"#fnjk4opi94vj\">[1]</a></sup></span></li><li>We have significantly improved and extended our article on&nbsp;<a href=\"https://80000hours.org/career-guide/career-planning/\"><u>career planning</u></a>.&nbsp;</li><li>We improved our advice on&nbsp;<a href=\"https://80000hours.org/career-guide/personal-fit/\"><u>personal fit</u></a> and&nbsp;<a href=\"https://80000hours.org/career-guide/personal-fit/#exploration\"><u>career exploration</u></a>.</li><li>We added sections on&nbsp;<a href=\"https://80000hours.org/career-guide/high-impact-jobs/\"><u>why community-building, government and policy, and organisation-building careers could be high impact</u></a>.</li><li>We focus more on avoiding harm (in line with our&nbsp;<a href=\"https://80000hours.org/2023/05/how-80000-hours-has-changed-some-of-our-advice-after-the-collapse-of-ftx/\"><u>updates following the collapse of FTX</u></a>), and explicitly discuss Sam Bankman-Fried when talking about earning to give.</li><li>We are more upfront about 80,000 Hours\u2019 focus on existential risk in particular (while also discussing a wide variety of cause areas, including global health, animal welfare, existential risk and meta-causes).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzqgaghm2t3\"><sup><a href=\"#fnzqgaghm2t3\">[2]</a></sup></span></li><li>We\u2019ve updated the more empirical sections of the guide using more up-to-date papers and data.</li></ul><p>You can read the guide&nbsp;<a href=\"https://80000hours.org/career-guide/\"><u>here</u></a> or&nbsp;<a href=\"https://80000hours.org/career-guide/summary/\"><u>start with a 2-minute summary</u></a>.</p><p>It\u2019s also available as a printed book (you can get a free copy by&nbsp;<a href=\"https://80000hours.org/book/\"><u>signing up for our newsletter</u></a>, or&nbsp;<a href=\"https://www.amazon.com/80-000-Hours-fulfilling-career/dp/1399957090/\"><u>buy it on Amazon</u></a>),&nbsp;<a href=\"https://www.audible.com/pd/80000-Hours-Audiobook/B0CGL49G63\"><u>audiobook</u></a>,&nbsp;<a href=\"https://80000hours.org/career-guide/audio\"><u>podcast series</u></a> or&nbsp;<a href=\"https://80000hours.org/ebook/\"><u>ebook</u></a> (available as a&nbsp;<a href=\"http://80000hours.org/career-guide-pdf\"><u>.pdf</u></a> or&nbsp;<a href=\"https://drive.google.com/uc?export=download&amp;id=1W9aLkWiH6IIZCV1CJM0tE-ny7h7wH0aS\"><u>.epub</u></a>).</p><p>We\u2019d appreciate you sharing the new guide with a friend! You can&nbsp;<a href=\"http://80000hours.org/gift\"><u>send them a free copy using this link</u></a>. Many of the people who\u2019ve found our advice most useful in the past have found us via a friend, so we think the time you take to share it could be really worthwhile.</p><h1>What\u2019s in the guide?</h1><p>The career guide aims to cover the most important basic concepts in career planning. (If instead you\u2019d like to see something more in-depth, see our&nbsp;<a href=\"https://80000hours.org/advanced-series\"><u>advanced series</u></a> and&nbsp;<a href=\"https://80000hours.org/podcast/\"><u>podcast</u></a>.)</p><p>The first article is about what to look for in a fulfilling job:</p><ul><li>Part 1:&nbsp;<a href=\"https://80000hours.org/career-guide/job-satisfaction/\"><u>What makes for a dream job?</u></a></li></ul><p>The next five are about which options are most impactful for the world:</p><ul><li>Part 2:&nbsp;<a href=\"https://80000hours.org/career-guide/can-one-person-make-a-difference/\"><u>Can one person make a difference?</u></a></li><li>Part 3:&nbsp;<a href=\"https://80000hours.org/career-guide/making-a-difference/\"><u>How to have a real positive impact in any job</u></a></li><li>Part 4:&nbsp;<a href=\"https://80000hours.org/career-guide/most-pressing-problems/\"><u>How to choose which problems to focus on</u></a></li><li>Part 5:&nbsp;<a href=\"https://80000hours.org/career-guide/world-problems/\"><u>What are the world\u2019s biggest and most urgent problems?</u></a></li><li>Part 6:&nbsp;<a href=\"https://80000hours.org/career-guide/high-impact-jobs/\"><u>What types of jobs help the most?</u></a></li></ul><p>The next four cover how to find the best option for you and invest in your skills:</p><ul><li>Part 7:&nbsp;<a href=\"https://80000hours.org/career-guide/career-capital/\"><u>Which jobs put you in a better position?</u></a></li><li>Part 8:&nbsp;<a href=\"https://80000hours.org/career-guide/personal-fit/\"><u>How to find the right career for you</u></a></li><li>Part 9:&nbsp;<a href=\"https://80000hours.org/career-guide/how-to-be-successful/\"><u>How to be more successful in any job</u></a></li><li>Part 10:&nbsp;<a href=\"https://80000hours.org/career-guide/career-planning/\"><u>How to write a career plan</u></a></li></ul><p>The last two cover how to take action and launch your dream career:</p><ul><li>Part 11:&nbsp;<a href=\"https://80000hours.org/career-guide/how-to-get-a-job/\"><u>How to get a job</u></a></li><li>Part 12:&nbsp;<a href=\"https://80000hours.org/career-guide/community/\"><u>How community can help</u></a></li></ul><h1>Advice (for EAs) on how to read the guide</h1><p>The topics we tackle are complex, and in the past we\u2019ve noticed people interpreting our advice in ways we didn\u2019t intend. Here are some points to bear in mind before diving in.</p><ul><li><strong>We\u2019ve been wrong before and we\u2019ll be wrong again.</strong> While we\u2019ve spent a lot of time thinking about these issues, we still have a lot to learn. Our positions have changed over the years, and due to the nature of the questions we take on, we\u2019re rarely more than about 70% confident in our answers. You should try to strike a balance between what we think and your previous position, depending on the strength of the arguments and how much you already knew about the topic.</li><li><strong>It\u2019s extremely difficult to give universally applicable career advice.</strong> Most importantly, the option that\u2019s best for you depends a huge amount on your skills,&nbsp; circumstances, and the specific details of the opportunity. So, while we might highlight path A more than path B, the best opportunities in path B will often be better than the typical opportunities in path A. Moreover, your personal circumstances could easily mean the best option for you is in path B. So, treat the specific options we mention as an aid for compiling your personal list of career ideas. Also, keep in mind that many issues in career choice are a matter of balancing opposing considerations \u2014 for instance, some readers are underconfident and need to be encouraged to aim higher, while some readers are overconfident and need to be encouraged to make a better backup plan. If we say people put too little emphasis on X, there will usually be some readers who put too much emphasis on X and need to hear the opposite advice. This is part of the motivation behind offering our 1-1 calls, which can offer much more specific, tailored advice, and if you\u2019ve read the whole career guide, you should probably&nbsp;<a href=\"http://80000hours.org/speak-with-us/\"><u>consider applying</u></a>.</li><li><strong>Our advice is aimed at a particular audience:&nbsp;</strong>namely, people with college degrees (or on their way to getting one) who want to make having a positive impact (from an impartial perspective) a significant focus of their career (especially in the problem areas we most recommend); who mostly live in rich, English-speaking countries; and who want to take an analytical approach to their career. At any given moment, many people need to focus on taking care of their own lives, and we don\u2019t think anyone should feel guilty if that\u2019s the case. Certain parts of our advice, such as our list of priority paths, are especially aimed at people who are unusually high achieving. In general, the more similar you are to our core audience, the more useful the advice will be, although much of what we write is useful to anyone who wants to make a difference.</li><li><strong>Treat increasing your impact as just one long-term goal.&nbsp;</strong>Working on the world\u2019s most pressing problems is among the most worthwhile challenges we can imagine, though it can also be overwhelming. We see increasing our impact as just one important goal among several in our lives, which means we often do things that aren\u2019t ideal from the perspective of doing good. Indeed, even if your only goal was to have an impact, to do that it\u2019s vital to do something you can stick with for years \u2014 and this means taking care of your personal priorities as well.</li><li><strong>Aim for steady progress rather than perfection.</strong> It can take a long time to work out how to incorporate the ideas we cover into your own plans and find the right opportunity. Because there\u2019s always more that could be done, it can be easy to become overly perfectionist, get caught up with comparisons, and never be satisfied. When using our advice, the aim is not to find the (unknowable and unattainable) perfect option or have more impact than people you compare yourself to. Rather, focus on making steady progress towards the best career that\u2019s practical for you, given your constraints.</li></ul><h1>Why did we make this change?</h1><p>In 2019, we deprioritised 80,000 Hours\u2019&nbsp;<a href=\"https://80000hours.org/career-guide/\"><u>career guide</u></a> in favour of our&nbsp;<a href=\"https://80000hours.org/key-ideas/\"><u>key ideas series</u></a>.&nbsp;</p><p>Our key ideas series had a more serious tone and was more focused on impact. It represented our best and most up-to-date advice. We expected that this switch would reduce engagement time on our site, but that the key ideas series would better appeal to people more likely to change their careers to do good.</p><p>However, the drop in engagement time which we could attribute to this change was larger than we\u2019d expected. In addition, data from our user survey suggested that people who changed their careers were&nbsp;<i>more</i>, not less, likely to have found and used the older, more informal career guide (which we kept up on our site).</p><p>As a result, we decided to bring the advice in our career guide in line with our latest views, while attempting to retain its structure, tone, and engagingness.</p><p>We\u2019re retaining the content in our key ideas series: it\u2019s been re-released as our&nbsp;<a href=\"https://80000hours.org/advanced-series/\"><u>advanced series</u></a>.</p><h1>Has it been successful so far?</h1><p>Yes!&nbsp;</p><p>We\u2019ve had positive feedback on the quality of the content in the guide, and we\u2019ve&nbsp;<i>also</i> seen many more people reading this guide than our key ideas series. Since soft launching the guide in May, we\u2019ve seen about a 30% increase in total weekly engagement time on our site.&nbsp;</p><h1>How can you help?</h1><p>Please take a look at the guide and, if possible, share it with a friend! You can&nbsp;<a href=\"http://80000hours.org/gift/\"><u>send them a free copy using this link</u></a>.</p><p>You can also give us some feedback on the guide using&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfxwFjyGKdwf6Vl_uOYVVFNV-OZ5Ca-3s5FXQwgHSgRGkr4hQ/viewform?usp=sf_link\"><u>this form</u></a>.&nbsp;</p><p>Please bear in mind that the&nbsp;<i>vast majority of people who read the 80,000 Hours website are not EAs</i>. Rather, our target audience for this career guide is the ~100k young adults most likely to have high-impact careers in the English speaking world. In particular, many of them are not yet familiar with many of the ideas that are widely discussed in the EA community. Also, this guide is primarily aimed at people aged 18\u201324.</p><p>Here are the links to the guide again:</p><ul><li><a href=\"https://80000hours.org/career-guide/\"><u>Online version</u></a></li><li>Printed book (<a href=\"https://www.amazon.com/80-000-Hours-fulfilling-career/dp/1399957090/\"><u>buy it</u></a> or&nbsp;<a href=\"https://80000hours.org/book/\"><u>sign up for our newsletter</u></a>)</li><li>Ebook (<a href=\"http://80000hours.org/career-guide-pdf\"><u>.pdf</u></a> or&nbsp;<a href=\"https://drive.google.com/uc?export=download&amp;id=1W9aLkWiH6IIZCV1CJM0tE-ny7h7wH0aS\"><u>.epub</u></a>)</li><li><a href=\"https://www.audible.com/pd/80000-Hours-Audiobook/B0CGL49G63\"><u>Audiobook</u></a></li><li><a href=\"https://80000hours.org/career-guide/audio\"><u>Podcast series</u></a></li></ul><p>Thank you so much!<br><br><i>(This post is a mildly edited cross-post from the </i><a href=\"https://80000hours.org/2023/09/career-guide-launch/\"><i>80,000 Hours</i></a><i> website.)</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjk4opi94vj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjk4opi94vj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We no longer say it's a mistake not to get transferable career capital. Instead, we focus on \"getting good at something that's useful.\" We also updated the concrete options to be in line with our current views, and added \"character\" to the components of career capital. Finally, we added a section on which skills are likely to be automated.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzqgaghm2t3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzqgaghm2t3\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The main reason we focus on existential risk is that our best guess is that working on existential risk has a higher impact on the margin than working on other areas as a result of the impact of existential risk on future generations. We try to explain why this is our best guess&nbsp;<a href=\"https://80000hours.org/career-guide/world-problems/#existential-risks\"><u>in the career guide</u></a>, as well as our longer articles on our&nbsp;<a href=\"https://80000hours.org/articles/what-is-social-impact-definition/\"><u>definition of social impact</u></a>,&nbsp;<a href=\"https://80000hours.org/articles/future-generations/\"><u>longtermism</u></a>, and&nbsp;<a href=\"https://80000hours.org/articles/existential-risks/\"><u>existential risks</u></a>.</p></div></li></ol>", "user": {"username": "Benjamin Hilton"}}, {"_id": "9Y5YzNDMdYYg6hjwD", "title": "What term to use for AI in different policy contexts?", "postedAt": "2023-09-06T15:08:29.593Z", "htmlBody": "<p>There is a cluster of terms (frontier AI, AGI, etc.) that are commonly used when talking or writing about AI systems, particularly when discussing how AI could cause existential or other catastrophes. This post gives a quick overview of some of the ones that are most common among people who focus on extreme risks.</p><p>I hope that the post will help people who are communicating about AI to choose a term that (a) captures well the types of AI systems that they have in mind, and (b) will not have unnecessarily negative connotations for their particular audience. I particularly have in mind people who are speaking to non-technical and non-specialist audiences, such as people who attempt to improve government policy around AI risk.</p><p>I first share my bottom lines on which terms I think are best in different contexts. For most of the post, I provide an overview of commonly-used terms. In later sections I give a quick overview of other terms, share relevant thoughts from other people in the field, and link to other sources that are relevant for this question.</p><p>&nbsp;</p><p><i>I'm writing this post in a personal capacity; it doesn't necessarily reflect the views of my employer.</i></p><p>&nbsp;</p><h1>Bottom lines</h1><p>I mainly discuss six terms in this post: \u201cfrontier AI\u201d, \u201cadvanced AI\u201d, \u201cgeneral-purpose AI\u201d, \u201cAGI\u201d, \u201cTAI\u201d, and \u201csuperintelligence\u201d. Different terms will work best in different contexts, and it seems fine to me for the field to continue using several terms.</p><p>Here are some bottom lines about which terms seem best to me in different contexts:</p><ul><li>\u201cFrontier AI\u201d is very helpful if focusing on the most advanced models at a given moment in time. A lot of current AI governance work seems to be in this category. That said, AI governance should not just focus on this subset of models; models behind the frontier could also be dangerous, particularly as AI capabilities advance.</li><li>As a default, I like \u201cadvanced AI\u201d because it is so non-jargon-y and neutral. That said, communicators would generally need to define what they mean by \u201cadvanced\u201d when using it. What AI systems count as advanced?</li><li>\u201cGeneral-purpose AI\u201d and \u201cAGI\u201d both point to systems that can achieve a wide range of tasks.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkfxz17l07d\"><sup><a href=\"#fnkfxz17l07d\">[1]</a></sup></span>&nbsp;This seems useful for a lot of AI governance work. General-purpose AI might sound less speculative and is less associated with AI developers, in particular OpenAI. This could be helpful or unhelpful depending on the context.</li><li>I expect that \u201cTAI\u201d and \u201csuperintelligence\u201d are typically worse than the other four terms, at least when speaking to non-technical and non-specialist audiences. TAI is jargon-y, and superintelligence sounds to many people like sci-fi.</li></ul><p>I discuss these terms in more detail immediately below.</p><p>&nbsp;</p><h1>Overview of commonly-used terms</h1><p>I discuss here six terms that are commonly used by people who focus on extreme risks from AI.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdnfb05t9a4i\"><sup><a href=\"#fndnfb05t9a4i\">[2]</a></sup></span>&nbsp;I give some examples of these terms being used, and provide \u201cconsiderations\u201d for each term. Considerations could often be either advantages or disadvantages, depending on the context.</p><p>&nbsp;</p><h2>\u201cFrontier AI\u201d, \u201cfrontier AI systems\u201d, \u201cfrontier models\u201d</h2><p>Considerations:</p><ul><li>Implies that the author is only referring to a small number of the most cutting-edge models at a given point in time.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8icq5yoipro\"><sup><a href=\"#fn8icq5yoipro\">[3]</a></sup></span><ul><li>A lot of AI governance efforts currently focus on frontier models.</li><li>That said, non-frontier models are also important from an extreme risk perspective; non-frontier models may become powerful enough to cause a catastrophe, and non-frontier models will presumably be accessible to a larger and more varied set of actors.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4vewi918zm8\"><sup><a href=\"#fn4vewi918zm8\">[4]</a></sup></span></li></ul></li><li>Connotations around the word \u201cfrontier\u201d in general<ul><li>One person said that they associate the term with American expansion/colonialism.</li><li>There\u2019s evidence of the term \u201cfrontier\u201d generally playing well in DC (at least among Democrats), e.g. with the proposed \u201c<a href=\"https://en.wikipedia.org/wiki/United_States_Innovation_and_Competition_Act\"><u>Endless Frontier Act</u></a>\u201d and Kennedy\u2019s \u201c<a href=\"https://en.wikipedia.org/wiki/New_Frontier\"><u>New Frontier</u></a>\u201d.</li><li>The word \u201cFrontier\u201d may have positive connotations relating to discovery, innovation, etc. I assume that this is part of the reason why it is used for the policy initiatives above and the Frontier Model Forum.</li></ul></li><li>Industry seems to have converged on this term, e.g. with the Frontier Model Forum, so people who use it might seem more sympathetic to industry than they would otherwise.</li></ul><p>&nbsp;</p><p>Examples of this term being used:</p><ul><li>\u201c<a href=\"https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-anthropic-frontier-model-forum/\"><u>Frontier Model Forum</u></a>\u201d, a new industry body that currently consists of Anthropic, Google, Microsoft, and OpenAI.</li><li>\u201c<a href=\"https://arxiv.org/abs/2307.03718\"><u>Frontier AI Regulation: Managing Emerging Risks to Public Safety</u></a>\u201d (Anderljung et al., 2023).<ul><li>They write that \u201c\u2018frontier AI\u2019 models [are] highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety\u201d.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy6j2a9qesc\"><sup><a href=\"#fny6j2a9qesc\">[5]</a></sup></span></li><li>This implies a different meaning of \u201cfrontier\u201d to the one that I describe above. Future models that are powerful by today\u2019s standards but that are not cutting-edge when they are produced would presumably meet the Anderljung et al. definition.</li></ul></li><li>\u201c<a href=\"https://arxiv.org/abs/2305.15324\"><u>Model evaluation for extreme risks</u></a>\u201d (Shevlane et al., 2023). The authors focus on models \u201cat the frontier\u201d, though also talk about \u201cgeneral-purpose AI systems\u201d. See in particular the figure on page 3.</li><li>The <a href=\"https://www.whitehouse.gov/wp-content/uploads/2023/07/Ensuring-Safe-Secure-and-Trustworthy-AI.pdf\"><u>document</u></a>&nbsp;published by the White House about the <a href=\"https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/\"><u>July 2023 voluntary lab commitments</u></a>.</li></ul><p>&nbsp;</p><p>\u201cHighly capable foundation model\u201d may achieve a similar meaning and be more easily understandable to policymakers, e.g. because \u201cfoundation models\u201d feature extensively in the <a href=\"https://www.europarl.europa.eu/doceo/document/TA-9-2023-0236_EN.html%23:~:text%3D(60e)%25C2%25A0%25C2%25A0Foundation,applications%2520and%2520systems.\"><u>EU AI Act</u></a>.</p><p>&nbsp;</p><h2>\u201cAdvanced AI\u201d or \u201cadvanced AI systems\u201d</h2><p>Considerations:</p><ul><li>Very non-jargon-y</li><li>Doesn\u2019t necessarily distinguish between models produced by the leading actors and everyone else.</li><li>It\u2019s not immediately clear what is meant by \u201cadvanced\u201d. I assume that people would generally need to specify how they are using the term.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqqd074ntd9c\"><sup><a href=\"#fnqqd074ntd9c\">[6]</a></sup></span></li></ul><p>&nbsp;</p><p>Examples of this term being used:</p><ul><li>\u201c<a href=\"https://arxiv.org/abs/2307.04699\"><u>International Institutions for Advanced AI</u></a>\u201d (Ho et al., 2023). They define it as \u201csystems that are highly capable and general purpose\u201d.</li></ul><p>&nbsp;</p><p>Similar considerations apply for the term \u201cpowerful AI systems\u201d. Relative to \u201cadvanced AI\u201d, \u201cpowerful AI\u201d might sound more exciting, attractive, and/or scary. That might induce more race-like behavior, more appetite for risk-reduction measures, or both.</p><p>&nbsp;</p><h2>General-purpose AI</h2><p>Considerations:</p><ul><li>Some of the same considerations apply here as for AGI (see below). Some differences:<ul><li>This term has a lower \u201ccapabilities bar\u201d for inclusion. For example, both GPT-4 and a hypothetical future AGI system could be referred to as \u201cgeneral-purpose AI\u201d.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdzss1yd6sne\"><sup><a href=\"#fndzss1yd6sne\">[7]</a></sup></span></li><li>This term probably has less sci-fi connotations and is less associated with the AI safety community and with people such as Bostrom and Yudkowsky.</li></ul></li><li><a href=\"https://arxiv.org/abs/2307.03718\"><u>Anderljung et al. (2023)</u>&nbsp;says</a>&nbsp;\u201cWe intentionally avoid using the term \u201cgeneral-purpose AI\u201d to avoid confusion with the use of that term in the EU AI Act and other legislation. Frontier AI systems are a related but narrower class of AI systems with general-purpose functionality, but whose capabilities are relatively advanced and novel.\u201d</li></ul><p>&nbsp;</p><p>Examples of this term being used:</p><ul><li>The current draft text of the <a href=\"https://www.europarl.europa.eu/doceo/document/TA-9-2023-0236_EN.html%23:~:text%3D(1d)%25C2%25A0%25C2%25A0%25E2%2580%2598general%2520purpose%2520AI%2520system%25E2%2580%2599%2520means%2520an%2520AI%2520system%2520that%2520can%2520be%2520used%2520in%2520and%2520adapted%2520to%2520a%2520wide%2520range%2520of%2520applications%2520for%2520which%2520it%2520was%2520not%2520intentionally%2520and%2520specifically%2520designed%253B\"><u>EU AI Act</u></a>: \u201c\u2018general purpose AI system\u2019 means an AI system that can be used in and adapted to a wide range of applications for which it was not intentionally and specifically designed.\u201d</li><li>Various writing from the Future of Life Institute, e.g. <i>Policymaking in the Pause</i>. They <a href=\"https://futureoflife.org/wp-content/uploads/2023/04/FLI_Policymaking_In_The_Pause.pdf\"><u>define</u></a>&nbsp;(p. 6) it as \u201can AI system that can accomplish or be adapted to accomplish a range of distinct tasks, including some for which it was not intentionally and specifically trained.\u201d</li><li>My impression is that this term is relatively often used by mainstream think tanks.</li></ul><p>&nbsp;</p><p>One could use variants of this term to point to AI that is both general and powerful, e.g. \u201chighly capable general-purpose AI\u201d.</p><p>&nbsp;</p><h2>Artificial General Intelligence (AGI)</h2><p>Considerations:</p><ul><li>Many people seem to find this term off-putting because it feels like something from science fiction.</li><li>Some AI developers seem to be (explicitly) aiming for AGI.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdhwecsx9k8u\"><sup><a href=\"#fndhwecsx9k8u\">[8]</a></sup></span></li><li>Some people (I think in particular from the AI ethics community) see the term \u201cAGI\u201d as \u201cmarketing hype\u201d from AI developers.</li><li>Implies that the AI systems in question will be able to do a wide range of tasks.<ul><li>Whether this is good or bad depends largely on to what extent such systems are really what we want to focus on in general or in a particular conversation/output.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxp41ltr5ca7\"><sup><a href=\"#fnxp41ltr5ca7\">[9]</a></sup></span></li><li>Arguably this also makes the term somewhat <a href=\"https://forum.effectivealtruism.org/posts/Nc5EjccDTfmcrG93j/what-are-information-hazards%23:~:text%3DAttention%2520hazard%253A%2520mere%2520drawing%2520of%2520attention%2520to%2520some%2520particularly%2520potent%2520or%2520relevant%2520ideas%2520or%2520data%2520increases%2520risk%252C%2520even%2520when%2520these%2520ideas%2520or%2520data%2520are%2520already%2520%25E2%2580%259Cknown%25E2%2580%259D.\"><u>attention hazardous</u></a>; greater awareness or salience that AGI systems might be possible might motivate additional efforts to build them, potentially increasing racing or reckless development.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4vi7986mqvl\"><sup><a href=\"#fn4vi7986mqvl\">[10]</a></sup></span></li></ul></li></ul><p>&nbsp;</p><p>Examples of this term being used:</p><ul><li>OpenAI, \u201c<a href=\"https://openai.com/blog/planning-for-agi-and-beyond\"><u>Planning for AGI and beyond</u></a>\u201d.<ul><li>Defines AGI as \u201cAI systems that are generally smarter than humans\u201d.</li></ul></li><li>\u201c<a href=\"https://arxiv.org/abs/2305.07153\"><u>Towards best practices in AGI safety and governance</u></a>\u201d (Schuett et al., 2023).<ul><li>\u201cAI systems that achieve or exceed human performance across a wide range of cognitive task\u201d</li><li>Note that this paper was specifically talking about companies that describe themselves as attempting to build AGI.</li></ul></li></ul><p>&nbsp;</p><h2>Transformative Artificial Intelligence (TAI)</h2><p>Considerations:</p><ul><li>Jargon-y. That may be unhelpful if communicating to a wide audience, but helpful if wanting to avoid generating hype around AI.</li><li>A range of types of AI systems could qualify as having \u201ctransformative\u201d effects. For example, the term could cover both one powerful general-purpose system, as well as an outcome with many narrow systems.</li><li>Experts may have different understandings of the term.<ul><li><a href=\"https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence\"><u>Karnofsky (2016)</u></a>&nbsp;defines TAI as: \u201cpotential future AI that precipitates a transition comparable to (or more significant than) the agricultural or industrial revolution\u201d. That leaves room for interpretation regarding whether only transitions in the form of accelerating economic development count and how comparability / significance is determined.</li><li><a href=\"https://docs.google.com/document/d/1IJ6Sr-gPeXdSJugFulwIpvavc0atjHGM82QjIfUSBGQ/edit%23heading%3Dh.6t4rel10jbcj\"><u>Cotra (2020)</u></a>&nbsp;writes \u201cI think of \u201ctransformative AI\u201d as software which causes a tenfold acceleration in the rate of growth of the world economy (assuming that it is used everywhere that it would be economically profitable to use it).\u201d She presents this as an operationalization of Karnofsky\u2019s definition. This seems to downplay the ways in which AI could have transformative effects other than economic growth, such as causing extinction.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefaqbwo2e6q4m\"><sup><a href=\"#fnaqbwo2e6q4m\">[11]</a></sup></span></li></ul></li></ul><p>&nbsp;</p><p>Examples of this term being used:</p><ul><li>Various things published by Open Philanthropy:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdf0is2d9v6k\"><sup><a href=\"#fndf0is2d9v6k\">[12]</a></sup></span><ul><li><a href=\"https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence\"><u>Karnofsky (2016)</u></a></li><li>\u201c<a href=\"https://docs.google.com/document/d/15siOkHQAoSBl_Pu85UgEDWfmvXFotzub31ow3A11Xvo/edit\"><u>What Open Philanthropy means by \u2018transformative AI</u></a>\u2019\u201d (Muehlhauser, 2019)</li><li><a href=\"https://docs.google.com/document/d/1IJ6Sr-gPeXdSJugFulwIpvavc0atjHGM82QjIfUSBGQ/edit%23heading%3Dh.6t4rel10jbcj\"><u>Cotra (2020)</u></a></li></ul></li><li>A lot of work from the existential-risk-focused AI governance field, particularly from before the shifts in public discussion of AI during 2022 and 2023. Also from the suffering-risk-focused field at the same time.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqswuz53y5mj\"><sup><a href=\"#fnqswuz53y5mj\">[13]</a></sup></span></li></ul><p>&nbsp;</p><h2>Superintelligence</h2><p>(I occasionally also see the term \u201cartificial superintelligence\u201d or \u201cASI\u201d.)</p><p>&nbsp;</p><p>Considerations:</p><ul><li>I expect that this term sounds weird or like sci-fi to many people, and that it will be polarizing with the AI ethics community, e.g. because it so explicitly focuses on future systems.</li><li>Emphasizes systems that are (much) smarter than humans, not just comparably smart. This emphasis seems helpful in some contexts, but note that systems could be extremely dangerous even if they are not much smarter than humans.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0sj1a2gut4d\"><sup><a href=\"#fn0sj1a2gut4d\">[14]</a></sup></span></li><li>This term is strongly associated with Nick Bostrom (due to his book <i>Superintelligence</i>).</li><li>The term may now be associated with OpenAI due to the posts listed below.</li></ul><p>&nbsp;</p><p>Examples of this term being used:</p><ul><li><i>Superintelligence: Paths, Dangers, Strategies</i>&nbsp;(Bostrom, 2014)</li><li>\u201c<a href=\"https://openai.com/blog/governance-of-superintelligence\"><u>Governance of superintelligence</u></a>\u201d&nbsp;(OpenAI, 2023)<ul><li>The blogpost implies that the authors mean \u201cAI systems [that] exceed expert skill level in most domains, and carry out as much productive activity as one of today\u2019s largest corporations\u201d.</li><li>See also \u201c<a href=\"https://openai.com/blog/introducing-superalignment\"><u>Introducing Superalignment</u></a>\u201d, which is explicitly about OpenAI\u2019s efforts to align superintelligence.</li></ul></li></ul><p>&nbsp;</p><h1>Terms that I\u2019ll mostly leave for now</h1><p><i>This section is generally for terms that do not closely match the categories that people who work on reducing existential and catastrophic risks from AI often want to point to.</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefekov4leyxiq\"><sup><a href=\"#fnekov4leyxiq\">[15]</a></sup></span><i>&nbsp;That said, I did not think much about this categorization, and I expect that it would be best to use each of these terms in some contexts.</i></p><ul><li>Advanced, Planning, Strategically aware systems (APS). See <a href=\"https://arxiv.org/pdf/2206.13353.pdf\"><u>Carlsmith (2021)</u></a></li><li>Foundation models, e.g. the UK\u2019s \u201c<a href=\"https://www.gov.uk/government/news/initial-100-million-for-expert-taskforce-to-help-uk-build-and-adopt-next-generation-of-safe-ai\"><u>Foundation Model Taskforce</u></a>\u201d.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffevwwllis2q\"><sup><a href=\"#fnfevwwllis2q\">[16]</a></sup></span></li><li>Generative AI.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefs7z31404rrb\"><sup><a href=\"#fns7z31404rrb\">[17]</a></sup></span></li><li>Generally-capable AI.</li><li>God-like AI. See e.g. <a href=\"https://www.ft.com/content/03895dc4-a3b7-481e-95cc-336a524f2ac2\"><u>this</u></a>&nbsp;op-ed from the now lead of the UK\u2019s Foundation Model Taskforce.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdvbifbusf3\"><sup><a href=\"#fndvbifbusf3\">[18]</a></sup></span></li><li>High-risk models / high-risk systems.</li><li>Human-level machine intelligence (HLMI).</li><li>Large language models (LLMs).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefj2sfjacvyj\"><sup><a href=\"#fnj2sfjacvyj\">[19]</a></sup></span></li></ul><p>&nbsp;</p><h1>Thoughts from others</h1><p>When I shared an initial draft of this post, several people left particularly interesting and detailed comments.&nbsp;With those people\u2019s permission, I reproduce some of these comments here.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefj221lcqvkws\"><sup><a href=\"#fnj221lcqvkws\">[20]</a></sup></span></p><p><a href=\"https://www.governance.ai/team/ben-garfinkel\">Ben Garfinkel</a>:</p><blockquote><p>Four terms I use:</p><ul><li>General-purpose AI (when I want to talk about AI systems that can perform a very broad range of tasks - a category that encompasses both GPT-4 and future things people sometimes call \"AGI\")</li><li>Advanced AI (when I want to reference AI systems significantly more sophisticated than the ones we have <i>now</i>).</li><li>Frontier AI (when I want to talk about AI systems that are - at a given point in time - comparatively more advanced than pretty much all other AI systems that exist at that time)</li><li>High-risk systems (It's sometimes worth having a term that specifically distinguishes systems by the level of risk they pose. This isn't going to perfectly correspond to any of the above categories).</li></ul><p>I mostly don't like \"AGI\" because I don't really know what it means, even though it sounds like it means something distinct and specific - also has some baggage. \"TAI\" has some baggage and is often used confusingly. (People sometimes talk about \"TAI\" as though they're talking about a particular AI system and sometimes talk about it as though they're talking about a general state of affairs in the world.)</p><p>General point: I'm often wary of ways of speaking/thinking about risks from AI that suggest there's a discrete and identifiable threshold (e.g. \"AGI\") where risks click over from non-catastrophic to catastrophic. So often prefer terms and ways of speaking that don't give this impression (e.g. \"risks from increasingly advanced AI systems\" vs. \"risks from AGI\").</p></blockquote><p>&nbsp;</p><p>Person 2:</p><blockquote><p>[Some terms that I use are]</p><ul><li>Potential future AI systems: to discuss risks from systems that don't exist yet, and be clear it doesn't include current systems. Can add other terms like \"advanced\", to be more specific. I find it useful to be very clear if I'm talking about systems that don't exist yet.</li><li>Advanced AI/Frontier AI: to discuss systems comparatively more advanced than pretty much all other AI systems that exist at that time.</li><li>AGI: to discuss risks or policies that may be important for systems that roughly surpass human levels of general intelligence</li></ul></blockquote><p>&nbsp;</p><p>Person 3:</p><blockquote><p>A near-synonym for \"frontier model\" [...] is \"highly capable foundation model\". <a href=\"https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RW14Gtw\"><u>Microsoft's policy piece in May</u></a>&nbsp;used that term and several variants. Many [policy] audiences are also becoming more familiar with the \"foundation model\" term in context of the European Parliament's <a href=\"https://www.europarl.europa.eu/doceo/document/TA-9-2023-0236_EN.html\"><u>proposed amendments to the AI Act.</u></a>&nbsp;I also sometimes use \"frontier model\", especially since the announcement of the Frontier Model Forum.</p></blockquote><p>&nbsp;</p><h1>Other helpful sources</h1><p><i>(I\u2019m sure this list is very incomplete, and I have not carefully read all of the sources listed here)</i></p><ul><li><a href=\"https://docs.google.com/document/d/1RNlHOt32nBn3KLRtevqWcU-1ikdRQoz-CUYAK0tZVz4/edit#heading=h.5qcwakie742j\">\u201cDisentangling definitions in transformative AI governance\u201d</a> (Maas, forthcoming, from pp. 18-24) gives an excellent overview of how many of the terms above have been used.</li><li><a href=\"https://www.adalovelaceinstitute.org/resource/foundation-models-explainer/\"><u>\u201cExplainer: What is a foundation model?\u201d</u></a>&nbsp;(Jones, 2023) discusses many of the terms in this post.</li><li><a href=\"https://cset.georgetown.edu/article/what-are-generative-ai-large-language-models-and-foundation-models/\"><u>\u201cWhat Are Generative AI, Large Language Models, and Foundation Models?\u201d</u></a>&nbsp;(Toner, 2023)</li><li><a href=\"https://www.sciencedirect.com/science/article/pii/S0016328721001932\"><u>\u201cThe transformative potential of artificial intelligence\u201d</u></a>&nbsp;(Gruetzemacher &amp; Whittlestone, 2022). The authors discuss many of the terms in this post, focusing on TAI.</li></ul><p>&nbsp;</p><h1>Acknowledgements</h1><p><i>Thank you to several people from the AI governance community, in particular Michael Aird and the people quoted above, who shared thoughts that informed this post.</i></p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkfxz17l07d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkfxz17l07d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;My sense is that people who are focused on extreme risks from AI more commonly use the term \u201cAGI\u201d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndnfb05t9a4i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdnfb05t9a4i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Various additional terms are used in conversations about AI more broadly and are mentioned at the bottom of the post.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8icq5yoipro\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8icq5yoipro\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;E.g. GPT-4 is currently cutting-edge so would likely qualify as frontier. It would presumably no longer qualify as frontier if a much more powerful GPT-5 is released in future. The \u201cfrontier\u201d can be defined in specific ways, e.g. with frontier models in a given year having to have been trained using a given amount of FLOP. Note, however, that technical definitions like this might be harder for non-technical audiences to understand.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4vewi918zm8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4vewi918zm8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;One could refer to \u201cfuture frontier AI\u201d if pointing specifically at cutting-edge models in the future.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fny6j2a9qesc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefy6j2a9qesc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See section 2.1 and Appendix A for much more detail on the definition.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqqd074ntd9c\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqqd074ntd9c\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;For example, \u201cWhat % of existing systems count as advanced? What % of systems in a few years will count as \u201cadvanced\u201d?\u201d</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndzss1yd6sne\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdzss1yd6sne\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;That said, one could be more specific by saying something like \u201cadvanced general-purpose AI\u201d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndhwecsx9k8u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdhwecsx9k8u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See <a href=\"https://arxiv.org/abs/2305.07153\">Schuett et al. </a>(2023, p. 3): \u201cBy \u2018AGI labs\u2019, we mean organizations that have the stated goal of building AGI. This includes OpenAI, Google DeepMind, and Anthropic.\u201d Similarly, <a href=\"https://en.wikipedia.org/wiki/Artificial_general_intelligence%23:~:text%3DCreating%2520AGI%2520is%2520a%2520primary%2520goal%2520of%2520some%2520artificial%2520intelligence%2520research%2520and%2520of%2520companies%2520such%2520as%2520OpenAI%252C%255B4%255D%2520DeepMind%252C%255B5%255D%2520and%2520Anthropic.\"><u>Wikipedia</u></a>&nbsp;says \u201cCreating AGI is a primary goal of some artificial intelligence research and of companies such as OpenAI, DeepMind, and Anthropic.\u201d That said, I could not immediately find primary sources where those companies say this.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxp41ltr5ca7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxp41ltr5ca7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;In contrast, we could imagine very narrow AI systems (collectively) still having big effects, cf. <a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf\"><u>Drexler (2019)</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4vi7986mqvl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4vi7986mqvl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;\u201cAdditional efforts\u201d could mean either that actors that are already somewhat focused on AGI try harder to build these systems, or that new actors become focused on building them. Some thoughtful AI safety people have raised this attention hazard concern in the past. It may be less relevant (on the margin) since attention to AI and AGI has dramatically increased anyway from November 2022 onwards.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnaqbwo2e6q4m\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefaqbwo2e6q4m\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;That said, in an interview, Cotra clarified that \u201ctransformative\u201d includes models that could be used to cause a 10x acceleration in growth, even if people do not decide to use them in this way, but rather e.g. direct them towards military advantage. (See the transcript <a href=\"https://futureoflife.org/podcast/ajeya-cotra-on-forecasting-transformative-artificial-intelligence/\"><u>here</u></a>, then search for the paragraph beginning \u201cThere may be reasons\u201d.)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndf0is2d9v6k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdf0is2d9v6k\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I believe that Open Philanthropy introduced the term, though I have not checked this.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqswuz53y5mj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqswuz53y5mj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;For example, \u201c<a href=\"https://longtermrisk.org/research-agenda\"><u>Cooperation, Conflict, and Transformative Artificial Intelligence</u></a>\u201d (Clifton, 2020)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0sj1a2gut4d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0sj1a2gut4d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See some discussion of this in <a href=\"https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/\"><u>Karnofsky (2022)</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnekov4leyxiq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefekov4leyxiq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I include a few terms (e.g. \u201cAPS\u201d) that <i>are</i>&nbsp;used by people who focus on extreme risks, but that are fairly uncommon.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfevwwllis2q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffevwwllis2q\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;For an overview that focuses on this term, see <a href=\"https://www.adalovelaceinstitute.org/resource/foundation-models-explainer/\"><u>Jones (2023)</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fns7z31404rrb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefs7z31404rrb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Mainstream policymakers / think tanks seem to often use this term and it features in the <a href=\"https://www.europarl.europa.eu/doceo/document/TA-9-2023-0236_EN.html%23:~:text%3D4.%25C2%25A0%25C2%25A0Providers%2520of%2520foundation%2520models%2520used%2520in%2520AI%2520systems%2520specifically%2520intended%2520to%2520generate%252C%2520with%2520varying%2520levels%2520of%2520autonomy%252C%2520content%2520such%2520as%2520complex%2520text%252C%2520images%252C%2520audio%252C%2520or%2520video%2520(%25E2%2580%259Cgenerative%2520AI%25E2%2580%259D)%2520and%2520providers%2520who%2520specialise%2520a%2520foundation%2520model%2520into%2520a%2520generative%2520AI%2520system%252C%2520shall%2520in%2520addition\"><u>EU AI Act</u></a>. I think this is sometimes to gesture to AI that is specifically \u201cgenerative\u201d, e.g. when talking about disinformation concerns from AI-generated images and text. Anecdotally, I think they also use it to gesture at a broader class of powerful or general-purpose systems. (People who work on existential and catastrophic risk would not generally use it in this way).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndvbifbusf3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdvbifbusf3\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;This term may be intuitive and rhetorically powerful. That said, its connotations (e.g. indicating extreme power) may increase some actors\u2019 interest in risky AI acceleration or deployment by \u201ctheir side\u201d, even if aiming for this increase accident risk.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnj2sfjacvyj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefj2sfjacvyj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Many of the systems that are relevant to AI governance are (currently) LLMs. That said, LLMs feel to me like a <i>specific example</i>&nbsp;of the type of system that one might want to discuss. RL agents are another example in this category.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnj221lcqvkws\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefj221lcqvkws\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Two of the three people had a preference for being quoted anonymously. I consider all three to be very knowledgeable about AI governance and policy.</p></div></li></ol>", "user": {"username": "oeg"}}, {"_id": "8EZHjmoq24ehYmDZY", "title": "Announcing Million Year View", "postedAt": "2023-09-04T07:09:07.842Z", "htmlBody": "<p><a href=\"https://www.millionyearview.com/\">Million year view</a> is a new blog that will distill research relevant to effective altruism and longtermism.&nbsp;<br><br>You might like this blog if you are interested in, or want to learn more about topics like:</p><ul><li>Existential risk</li><li>Artificial intelligence and alignment</li><li>Decision theory</li><li>Cash transfers</li><li>Predicting the future</li><li>Attitudes towards longtermism</li></ul><p>I'd love to know what you want to see distillations of, so let me know in the comments.&nbsp;</p><p>If you want email notifications, you can subscribe to them <a href=\"https://www.millionyearview.com/subscribe?utm_source=menu&amp;simple=true&amp;next=https%3A%2F%2Fwww.millionyearview.com%2Fp%2Fcoming-soon%3Futm_campaign%3Dpost%26utm_medium%3Dweb\">here</a>.</p>", "user": {"username": "rileyharris"}}, {"_id": "SkEgBb5KqNRWHcksa", "title": "[Linkpost] Beware the Squirrel by Verity Harding", "postedAt": "2023-09-03T21:04:18.807Z", "htmlBody": "<p>I think this piece by Verity Harding raises some good points about AI safety &amp; x-risk. I would be curious to hear anyones thoughts on it.</p><p>Below are a few excerpts from the post, which hopefully string together into a sort of summary (but, of course, please read the full post before commenting).</p><blockquote><p>\"I\u2019ll go out on a limb and say, superintelligence AI systems are not going to cause human extinction... None of this is to argue that AI systems <i>today</i>, and in the next decade, aren\u2019t dangerous \u2013 or that I\u2019m personally not worried. They will be, and I am.\"</p></blockquote><blockquote><p>\"[W]hat worries me [about AI] is that this headlong rush into AI mania will result in it being used where it should not be - either because the system is not capable enough to actually do what it claims to be able to do, or because it is in an area where human judgement should not be replaced, or in a way that fundamentally upends the social contract between a democratic government and its citizens.\"</p></blockquote><blockquote><p>\"Which brings me back to the problem with the resurgence in advocates for existential AI risk. It\u2019s not that&nbsp;<i>any</i>&nbsp;work in this area is pointless. A counter argument to my scepticism would go \u201cwell, if there\u2019s even a 0.01% chance of wiping out humanity then we should do all we can to stop that!\u201d And, sure. AI research is a wide, varied field and if this is a topic for which someone has a personal passion then great. But as with the hard grind of getting people to take bias, accountability, and transparency in AI seriously, others have done&nbsp;<a href=\"https://www.currentaffairs.org/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk\"><code>excellent work</code></a>&nbsp;highlighting why too much focus on what\u2019s known as \u201clong-termism\u201d is distracting at best. At worst, it is a disregard for any effect of AI apart from total human extinction. At best, it is a naive approach that is careless with the lives of people -&nbsp;<i>real people</i>, alive now \u2013 who will be affected by pervasive, unregulated AI. Critics of this will argue that just because you are worried about short-term risk doesn\u2019t negate long-term risk, that it\u2019s not a zero-sum game. Well, actually, it is.\"</p></blockquote><blockquote><p>\"Because there is not infinite money to spend on AI safety and ethics research. The biggest builders of AI systems, the large tech companies, may invest in ethics and safety teams who are focussed on issues outside of existential risk. But the resources they have at their disposal are a fraction of the resources given to the teams moving ahead at speed with building and integrating AI systems into our lives already. Even when the big tech companies are thoughtful and careful about what they build and release, private philanthropy and government funding is extremely important in ensuring that independent AI ethics and safety research is also supported, not least as a check on that power. So when those funders become convinced that the most important thing to focus on is long-term AI risk \u2013 something which has very little evidence, if any, in its favour and is refuted by numerous AI experts \u2013 it very seriously detracts from support for much more urgent and important work.\"</p></blockquote><blockquote><p>\"Everyone using their voice to advocate for panic and alarm over something without a very clear explanation of how it would be possible, really, in the physical world, should think carefully about whether or not they are really helping with their (I\u2019m sure genuinely felt) goal to make AI - and the world safer.\"</p></blockquote>", "user": {"username": "Caroline Wiese Young"}}, {"_id": "pr3WfFTNNYWKkTA6w", "title": "What's happening with the EVF trustee recruitment?", "postedAt": "2023-09-03T20:13:58.881Z", "htmlBody": "<p>Cf <a href=\"https://forum.effectivealtruism.org/posts/zzpwpDkQBTzxbjgJo/apply-or-nominate-someone-to-join-the-boards-of-effective\">this post</a>. Did they find someone?</p>", "user": {"username": "Arepo"}}, {"_id": "fKee6xdC9wFjewkzz", "title": "The Vice of Spice: Confronting Lead-Tainted Turmeric (Wudan Yan)", "postedAt": "2023-09-03T19:11:27.578Z", "htmlBody": "<p><i>I'm a sucker for any article that shows the public health world springing into action. This is grade-A stuff, covering every step of the process, and I wish I could see it as a Hollywood, Bollywood, or Dhallywood thriller. (Are you busy, </i><a href=\"https://en.wikipedia.org/wiki/Pad_Man_(film)\"><i>R. Balki</i></a><i>?)</i></p><h2>Excerpts</h2><blockquote><p>For most of his turmeric trading career, Sheikh engaged in an open secret: While processing raw turmeric to powder, he added a chemical called lead chromate to get the tubers to glow yellow. Sheikh and the locals refer to the compound as <i>peuri</i> \u2014 and nearly all the farmers and traders at the market are familiar with it. Lead chromate is a chemical used in paints to, for instance, make school buses yellow, and it can enhance the radiance of turmeric roots, making them more attractive to buyers.</p><p>For decades, Sheikh didn\u2019t know the exact harm that peuri could cause. That changed in the fall of 2019, when researchers from the nonprofit International Center for Diarrheal Disease Research, Bangladesh, or ICDDR,B, traveled to Ataikula and adjacent districts in the northwest to meet with Sheikh and others in the turmeric business. The researchers warned them that consuming lead chromate could lead to kidney and brain damage or cause developmental delays in children. By that point, the spice had made its way out of the country: The problem had already gone global.</p></blockquote><p>&nbsp;</p><blockquote><p>Eskandar Molla, a farmer from northwest Bangladesh, has been growing turmeric for over 50 years. He sells his fresh turmeric fingers at nearby Hazir Hat, an open air market. There, he gets paid a market rate for what he sells: In March of this year, it was around 1,400 taka (about $13 USD) for a 88-pound satchel. Middlemen, who have the facilities to boil and dry the root, buy it from him and other farmers.</p><p>Getting the moisture out is critical for the root to be pulsed into powder. Traders lay out a single layer of turmeric fingers in vast, sunny, open fields for a month. Workers manually go through and inspect for roots that are either too long, too fat, or too skinny.</p><p>Once dried, the turmeric fingers are polished. Here, they are dumped into large \u201cdrums,\u201d which are turned by hand or motorized. This continual physical agitation removes the outer skin of the turmeric to reveal the true color of the root. It\u2019s in this step that lead chromate would be used to enhance color.</p></blockquote><p>&nbsp;</p><blockquote><p>For years, the only food in which the FDA had <a href=\"https://www.fda.gov/regulatory-information/search-fda-guidance-documents/guidance-industry-lead-candy-likely-be-consumed-frequently-small-children#ftn2\"><u>established</u></a> a maximum level of lead was candy: 0.1 parts per million for small children. It took the agency about 16 years to announce another update, outlining <a href=\"https://www.fda.gov/news-events/press-announcements/fda-announces-action-levels-lead-categories-processed-baby-foods\"><u>guidelines</u></a> for other foods commonly <a href=\"https://undark.org/2023/05/15/the-daunting-task-of-cutting-heavy-metals-from-baby-food/\"><u>consumed by babies</u></a> and young children. Fruits, the agency said, should not exceed 10 parts per billion in lead, and root vegetables and dried cereals should not exceed 20 parts per billion. This guideline did not discuss spices, and no maximum limits for lead are noted for adults.</p><p>Many of the lead levels detected in turmeric have <a href=\"https://casafdo.com/resources/Documents/Lead-in-Spices.pdf\"><u>ranged from</u></a> 28 to 146 parts per million, magnitudes more than the FDA\u2019s established acceptable levels for other foods. (The FDA said in a statement that even though it monitors levels of lead in food, and knows about the turmeric recalls owing to high lead levels, it has not yet set a limit for lead in spices.)</p><p>Tom Tarantelli, the New York Department of Agriculture and Market\u2019s former senior food chemist, has estimated that children in families that use turmeric regularly could be consuming 3 to 4 grams a day of that spice alone, suggesting that these kids are consuming far more spice than, say, candy.&nbsp;</p></blockquote><p>&nbsp;</p><p>The first half-hour of the movie:</p><blockquote><p>In a rural part of the country, pregnant women and children had high levels of lead in their blood. There were none of the usual suspects of lead exposure. There were no nearby battery recycling plants and families didn\u2019t paint their homes. How could this be?</p><p>Forsyth and her Bangladeshi colleagues had a slew of hypotheses. Maybe the lead was coming from jewelry or food storage containers. Or perhaps it came from clay, soil, or ash that the mothers were exposed to during pregnancy. Rice was another possibility, as the staple crop could have absorbed lead from the soil. Forsyth and her colleagues sampled and tested all of these. She vividly remembers the first summer of her Ph.D., as she baked and ground rice into a pulp to test for lead in a sweltering laboratory in rural Bangladesh. But there was no obvious red flag.</p></blockquote><p>&nbsp;</p><p>The climactic action sequence into a feel-good ending (hopefully we avoid any sequels):</p><blockquote><p>Authorities also raided Shyambazar using a machine called an X-ray fluorescence analyzer which can quickly detect lead in spices. Nearly 2,000 pounds of turmeric was seized in the raid and two wholesalers were fined 800,000 taka, more than $9,000 USD.</p><p>A few months later, the team went back again to collect samples to see how their intervention had fared. Only about 5 percent of 157 samples were found to be adulterated with lead chromate, down from nearly 50 percent before. When the researchers conducted a sampling spree again in 2021, they found that the use of lead chromate had practically disappeared.</p><p>[...]</p><p>The crackdown on turmeric in 2019 may, in part, explain why the use of lead chromate in polishing turmeric has since decreased. It was a punishable crime, and although there was only one raid, people now know there\u2019s a risk of getting caught.&nbsp;</p><p>[...]</p><p>But government officials and researchers say that enforcement and surveillance must be maintained. \u201cA one-time conviction is not sufficient,\u201d wrote Jahan, who has since moved on from the Bangladesh Food Safety Authority. Follow-up is required as she has \u201cseen such criminals go back to doing what they did even after facing consequences.\u201d</p><p>Monzur Morshed Ahmed, a member of BFSA, says that the agency is in the process of procuring handheld X-ray fluorescence analyzers to distribute to districts around Bangladesh so local authorities can continue monitoring the use of lead chromate themselves.</p></blockquote>", "user": {"username": "aarongertler"}}, {"_id": "seeyvF2qGqA4pKdn5", "title": "Rethinking the Liberation Pledge (Eva Hamer)", "postedAt": "2023-09-03T18:44:53.345Z", "htmlBody": "<p><i>This article is a really interesting example of people in a prosocial movement trying radical tactics and then changing their minds. I'm not sure there's any lesson here that the average Forum reader should learn; I'm just crossposting because I enjoyed it.</i></p><p><i>See parts </i><a href=\"https://paxfauna.org/the-request-an-evolution-of-the-liberation-pledge/\"><i>two</i></a><i> and </i><a href=\"https://paxfauna.org/vegan-tables-a-letter-to-the-people-i-love/\"><i>three</i></a><i> for how the Pledge has evolved.</i></p><p>&nbsp;</p><h2><strong>The Failure of the Pledge and a Better Way Towards Vegan Tables</strong></h2><p>In 2015, animal advocates with Direct Action Everywhere (DxE) launched an inspired new campaign among their members. It took courage, required sacrifice, and greatly backfired. This three-part series examines what the movement learned from the Liberation Pledge, how we might energize the intention behind the Pledge in a better way, and a piece to share with friends and family to do just that.&nbsp;</p><h2><strong>What We Learned From the Liberation Pledge</strong></h2><h3><strong>How It Started</strong></h3><p>The Liberation Pledge was a fascinating idea and a bit of a disaster. Instead of energizing supporters\u2019 social networks to create change, as its creators intended, it often had the opposite effect- to isolate advocates from their closest relationships.&nbsp;</p><p>&nbsp;In this piece, you\u2019ll learn</p><ul><li>What it was</li><li>Why it was a good idea</li><li>Why it failed</li></ul><p>The next piece in this series suggests a better way to energize the intention behind the pledge, for animal advocates to align their actions with their values in their personal relationships.&nbsp;</p><p>&nbsp;The Liberation Pledge was a three-part public pledge to&nbsp;</p><ol><li>live vegan,&nbsp;</li><li>refuse to sit at tables where animals\u2019 bodies are being eaten, and&nbsp;</li><li>encourage others to do the same.&nbsp;</li></ol><p>Enthusiasts of the pledge hoped it would create a <a href=\"https://medium.com/@andreswithwords/the-liberation-pledge-2f2f3f538cd3\">cultural stigma</a> around eating animals similar to the stigma that has developed around smoking over recent decades. That is, even while smoking is still practiced, it is prohibited by default in public and private spaces.&nbsp;</p><p>Before we had the Pledge, many of us felt alienated from friends and family who continued to eat animals. We were forced to choose between two options: speaking up and risking being seen as obnoxious, angry, and argumentative, or keeping the peace with painful inauthenticity, swallowing our intense discomfort at watching our loved ones eat the bodies of animals.&nbsp;</p><p>The pledge gave us hope that there was another way: being honest with those around us while continuing to spend time with them. And, on a larger scale, we hoped that if we all joined together, we could create a world where eating meat is stigmatized: a world where someone would ask, \u201cDoes anyone mind if I get the steak?\u201d before making an order at a restaurant (or maybe even one in which restaurants would think twice before putting someone\u2019s body on the menu).&nbsp;</p><p>Some people took it a step further, arguing it was immoral <i>not</i> to take the pledge, saying, \u201cYou wouldn\u2019t sit quietly eating your vegan option while a dog or a child was being eaten, would you?\u201d According to this view, it was our duty not to sit idly by while violence was committed in our presence.&nbsp;</p><p>While some beautiful and inspiring stories were detailed on a Facebook group for the Pledge, it seemed to me that there were many more instances of total disaster: people experiencing huge ruptures in their oldest relationships around the Pledge while often lamenting that those they had just discarded&nbsp; \u201ccare more about eating dead animals than they care about me.\u201d&nbsp;</p><p>From where I stood, the biggest effect of the Pledge was for advocates to lose relationships with family members who didn\u2019t comply. Upon taking the Pledge, a close friend at the time experienced a years-long estrangement from their family, <i>including those who were already vegan</i> while many others decided to skip birthdays, weddings, and holidays with family. It\u2019s possible that all of this added stigma around eating animals. With these relationships broken down, we don\u2019t know.&nbsp;</p><h3><strong>My Liberation Pledge</strong></h3><p>I believe the pledge was so popular because it politicized something that we desperately wanted for our own comfort\u2013no animals on the table while we were there\u2013and I took it pretty much as soon as it launched.&nbsp;</p><p>The Pledge certainly contributed to my alienation from nonvegans, though I neither experienced the best nor the worst of it. My immediate family accommodated a request for vegetarian tables at holiday dinners, but I\u2019m sure that there were many invitations I would have received if not for it. While my overall immersion in the animal rights community during that time certainly deserves some of the credit for the fact that I didn\u2019t develop many new relationships with nonvegans during the following several years, the effect of the Pledge can\u2019t be discounted.&nbsp;</p><p>A website was created with advice for taking the pledge, which is still online as of this writing. It suggested that pledge-takers write a public statement (a model announcement is provided) to inform their friends and family about their new commitment. It also offered some logistical suggestions for getting together with friends and family who aren\u2019t willing to cooperate with the rules of the Pledge. Most importantly, it laid out the reasoning for why we must, together, participate in the Liberation Pledge (to stigmatize eating animals) and directs the reader to \u201cstay firm and nonviolent in the face of conflict.\u201d</p><p>This was the right kind of advice, but it fell far short. With the benefit of hindsight, we can say that the founders of the Liberation Pledge underestimated just how difficult an undertaking they were proposing. <strong>In fact, the pledge in practice often had the opposite of its intended effect, an outcome that profoundly undermined DxE\u2019s central theory of change.</strong> DxE believed in the power of social networks to create change. That is, by taking bold actions and making personal sacrifices, activists would present a model to their communities and inspire friends and family members to reconsider their views on animals. However, while the pledge was meant to spark this process, in practice, it resulted more often than not in the disconnection of activists from their social networks. Instead of creating change by leveraging their personal relationships (the most important resource activists have, according to the social movement theory of change), the pledge weakened and sometimes even severed these relationships.&nbsp;</p><p>I believe these problems were mostly a matter of inadequate training. Pledge-takers were sent to the front lines of a fiery struggle for social change (their family dinner table) with nothing but a template letter. In contrast, tactics that involved legal risk or personal safety were only encouraged with plenty of training. While the Pledge wasn\u2019t a matter of life and death, freedom or prison, it was a risk to members\u2019 closest and most important relationships. With 20/20 hindsight, it seems that it was unwise to encourage pledge-takers to risk these relationships with so little training.</p><h3><strong>The Meaning of Nonviolence</strong></h3><p>A lot of people might think of activism as characterized by righteous anger, held in an image of somebody yelling into a megaphone at a protest. But DxE understood that while anger has its place as an energizing and powerful force, the task of winning over hearts and minds is difficult, delicate, and requires extraordinary discipline over our own emotions. They might not have fully understood how much more difficult this is with our loved ones than with the public at large. The same advocate who can always treat hecklers at a protest with kindness might still be underprepared to treat their own family members with love and acceptance when the topic of animals comes up.&nbsp;</p><h3><strong>The Alienating Aspects of the Framing</strong></h3><p><strong>Instrumentalizing Relationships</strong></p><p>Framing the Pledge as a political action instrumentalized our closest relationships, communicating to those closest to us that we thought of them as objects to be used for the cause.&nbsp;</p><p><strong>I Matter to You, but You Don\u2019t Matter to Me</strong></p><p>By framing the Pledge as a pledge or oath, it was presented as a promise to people who weren\u2019t present. Exasperated by the custom and direction to make a public statement before having private conversations with those affected, the Pledge had an unnecessary effect of communicating to our loved ones that they didn\u2019t really matter to us, at least not in comparison to this new thing we were doing. By not including our loved ones in a decision that would greatly affect our future interactions, we communicated something that was often taken as profound disrespect.&nbsp;</p><p>At the same time as it communicated that we didn\u2019t particularly care about our loved ones, it explicitly appealed to their care for us, creating a heartbreaking competition about who loves the other less, and therefore gets the accommodation. Some of our loved ones must have felt that they\u2019d be showing disproportionate care for us by agreeing to vegan tables, and so they attempted to call our bluff by serving meat.&nbsp;</p><p>Of course, not every Pledge conversation went this way, and many included affirmations of how much the relationships meant to us. Some relationships truly were deepened by the pledge, but it seems that they were the exception.&nbsp;</p><p><strong>Unnecessary Escalation</strong></p><p>Even if you plan to hold a boundary around sitting at such a table, framing it as such in initial conversations is confrontational to an antisocial degree. Rather than inviting others to understand our experiences, the explicit focus on integrity (<i>I wouldn\u2019t sit if a dog were being eaten</i>) unnecessarily created an adversarial dynamic.&nbsp;</p><p><strong>One Size Fits None</strong></p><p>By being all or nothing, the Pledge puts us in a position where we are de facto excluded from many large events or gatherings with people who don\u2019t know us well. In these cases, it won\u2019t&nbsp; make sense to accommodate us by inconveniencing so many others, especially if our hosts don\u2019t know us well.&nbsp;</p><h3><strong>As It Lives Now</strong></h3><p>In the early days of the Pledge, attendees at every DxE event were repeatedly encouraged to take it, in observance of its third tenant, to <i>encourage others to do the same.</i> Today, it is rarely discussed in public, probably due precisely to some of the harms I\u2019ve described.&nbsp;</p><p>While some people continue to practice it in some form, it seems that the political framing has largely dropped off. We understand that, while it didn\u2019t take off as a political strategy, the practice of only sitting at vegan or vegetarian tables is important for our own mental health. Most people still practicing the Pledge (at least the ones I know) are dedicated animal activists who largely associate with other vegans. When we don\u2019t, we might quietly suggest a vegan establishment and only inform our potential dining companions of the pledge when necessary. \u201cThe Pledge\u201d still exists as shorthand for this habit, even when its third tenant and political framing are absent.&nbsp;</p><p>While mostly inert, I believe that the Pledge continues to cause harm, albeit small harms compared to when it first came about. The Liberation Pledge Facebook Group remains somewhat active as a periodically updated illustration of the Pledge\u2019s continuing effects. A recent post asks for advice for dealing with a mandatory school event, predicting that the writer will end up sitting alone in the corner throughout. To this person, it seems that the Pledge will prevent her from getting to know her classmates and thus prevent her from becoming a person whom they can know well enough to look to as an example. In addition, the poster\u2019s relationship with the Pledge deprives <i>her</i> of the chance to develop connections with others.&nbsp;</p><h3><strong>A Better Way</strong></h3><p>You may be thinking at this point that I\u2019m encouraging everyone who has taken the Pledge to renounce it: that\u2019s not where I\u2019m going.&nbsp;</p><p>In the next piece, I offer a proposal for rethinking the concept behind the Pledge to support the relationships and well-being of movement participants. With some slight modifications, the promises of the Liberation Pledge can be realized as a vehicle for connection and a microcosm of social change. Click <a href=\"https://paxfauna.org/blog/the-request-an-evolution-of-the-liberation-pledge\">next</a> to learn more.&nbsp;</p>", "user": {"username": "aarongertler"}}, {"_id": "B3NyGg24gtdKETnXw", "title": "[CFP] NeurIPS workshop:  AI meets Moral Philosophy and Moral Psychology", "postedAt": "2023-09-04T06:21:37.128Z", "htmlBody": "<p><a href=\"https://aipsychphil.github.io/\">https://aipsychphil.github.io/</a></p><p>&nbsp;</p><p><strong>*About*</strong></p><p>Be it in advice from a chatbot, suggestions on how to administer resources, or which content to highlight, AI systems increasingly make value-laden decisions. However, researchers are becoming increasingly concerned about whether AI systems are making the right decisions. These emerging issues in the AI community have been long-standing topics of study in the fields of moral philosophy and moral psychology. Philosophers and psychologists have for decades (if not centuries) been interested in the systematic description and evaluation of human morality and the sub-problems that come up when attempting to describe and prescribe answers to moral questions. For instance, philosophers and psychologists have long debated the merits of utility-based versus rule-based theories of morality, their various merits and pitfalls, and the practical challenges of implementing them in resource-limited systems. They have pondered what to do in cases of moral uncertainty, attempted to enumerate all morally relevant concepts, and argued about what counts as a moral issue at all.</p><p>In some isolated cases, AI researchers have slowly started to adopt the theories, concepts, and tools developed by moral philosophers and moral psychologists. For instance, we use the \u201ctrolley problem\u201d as a tool, adopt philosophical moral frameworks to tackle contemporary AI problems, and have begun developing benchmarks that draw on psychological experiments probing moral judgment and development.</p><p>Despite this, interdisciplinary dialogue remains limited. Each field uses specialized language, making it difficult for AI researchers to adopt the theoretical and methodological frameworks developed by philosophers and psychologists. Moreover, many theories in philosophy and psychology are developed at a high level of abstraction and are not computationally precise. In order to overcome these barriers, we need interdisciplinary dialog and collaboration. This workshop will create a venue to facilitate these interactions by bringing together psychologists, philosophers, and AI researchers working on morality. We hope that the workshop will be a jumping-off point for long-lasting collaborations among the attendees and will break down barriers that currently divide the disciplines.</p><p>The central theme of the workshop will be the application of moral philosophy and moral psychology theories to AI practices. Our invited speakers are some of the leaders in the emerging efforts to draw on theories in philosophy or psychology to develop ethical AI systems. Their talks will demonstrate cutting-edge efforts to do this cross-disciplinary work, while also highlighting their own shortcomings (and those of the field more broadly). Each talk will receive a 5-minute commentary from a junior scholar in a field that is different from that of the speaker. We hope these talks and commentaries will inspire conversations among the rest of the attendees.</p><p>&nbsp;</p><p><strong>**Invited Speakers and Tentative Talk Topics**</strong></p><p><strong>Laura Weidinger</strong> (Senior Research Scientist, DeepMind, AI + Psychology): The use of findings in developmental moral psychology to create benchmarks for an AI system\u2019s moral competence.</p><p><strong>Josh Tenenbaum</strong> (Professor, MIT, AI + Psychology): Using a recent \u201ccontractualist\u201d theory of moral cognition to lay a roadmap for developing an AI system that makes human-like moral judgments.</p><p><strong>Sam Bowman</strong> (Associate Professor, NYU &amp; Anthropic, AI): Using insights from cognitive science for language model alignment.</p><p><strong>Walter Sinnott-Armstrong</strong> (Professor, Duke, AI + Philosophy): Using preference-elicitation techniques to align kidney allocation algorithms with human values.</p><p><strong>Regina Rini</strong> (Associate Professor, York University, Philosophy): The use of John Rawls\u2019 \u201cdecision procedure for ethics\u201d as a guiding framework for crowdsourcing ethical judgments to be used as training data for large language models.</p><p><strong>Josh Greene</strong> (Professor, Harvard, Psychology): An approach to AI safety and ethics inspired by the human brain\u2019s dual-process (\u201cSystem 1/System 2\u201d) architecture.</p><p><strong>Rebecca Saxe</strong> (Professor, MIT, Psychology): Using the neuroscience of theory-of-mind to build socially and ethically aware AI systems.</p><p>&nbsp;</p><p><strong>*Call for Contribution*</strong></p><p>The core of the workshop will be a series of in-person invited talks from leading scholars working at the intersection of AI, psychology, and philosophy on issues related to morality. Each talk will be followed by a 5-minute comment by a junior scholar whose training is primarily in a field that is different from the speaker\u2019s field. This format will encourage interdisciplinary exchange. The day will end with a panel discussion of all the speakers. We will also organize two poster sessions (of contributed papers) to ensure individual interaction between the attendees and presenters.</p><p><br>&nbsp;</p><p><strong>**We invite the submissions in the following topics**</strong></p><p><i>Ideal submissions will show how a theory from moral philosophy or moral psychology can be applied in the development or analysis of ethical AI systems. For example:</i></p><p>How can moral philosophers and psychologists best contribute to ethically-informed AI?</p><p>What can theories of developmental moral psychology teach us about making AI?</p><p>How do theories of moral philosophy shed light on modern AI practices?</p><p>How can AI tools advance the fields of moral philosophy and psychology themselves?</p><p>How can findings from moral psychology inform the trustworthiness, transparency or interpretability of AI decision-makers?</p><p>What human values are already embedded in current AI systems?</p><p>Are the values embedded in the current-day AI systems consistent with those in society at large?</p><p>What pluralistic values are missing from current-day AI?</p><p>Methodologically, what is the best way to teach an AI system human values? What are competitors to RLHF, reinforcement learning from human feedback?</p><p>Concerning AI alignment, to which values are we to align? Is the current practice of AI alignment amplifying monolithic voices? How can we incorporate diverse voices, views and values into AI systems?</p><p>&nbsp;</p><p><strong>**Submission format**</strong></p><p>To apply, submit a short paper (3-8 pages), formatted for blind review. References do not count towards the page limit. Figures and tables are permitted. Note that papers on the shorter end of the range will be given full consideration. The workshop is non-archival, though there will be an option to have the papers posted on the workshop website. Accepted submissions will be presented as posters.&nbsp;</p><p>A small subset of the accepted submissions will be offered the opportunity to present their work as a 5-7-minute talk immediately following one of the invited talks. These short talks will be framed as a \u201cdiscussion\u201d or \u201ccommentary\u201d on the main talk. The short talks will deal with a similar theme as that discussed in the main talk, but from a different theoretical or methodological perspective. These talks can (and should) present the author\u2019s original work, as well as explicitly address the way that their work challenges or supplements the work of the main speaker. On the submission page, there is an opportunity to indicate if you would like your submission to be considered for a short talk and, if so, which invited speaker you see as potentially most relevant (though this is simply a suggestion to the organizers). Those submissions accepted as short talks will not be presented as posters. Preference will be given to junior scholars. In addition, the organizers are committed to many forms of intellectual and sociological diversity\u2014those from under-represented groups are especially encouraged to apply.<br>&nbsp;</p><p><strong>**Important Dates**&nbsp;</strong></p><p>All deadlines are in <a href=\"https://www.timeanddate.com/time/zones/aoe\"><u>AoE</u></a>.</p><p>Submission Deadline: Sep 29, 2023</p><p>Accept/Reject Notification: Oct 20, 2023</p><p>Camera-ready Final Submission: Nov 10, 2023</p><p>Workshop Date: Dec 15, 2023</p>", "user": {"username": "jaredlcm"}}, {"_id": "cYbSjun4F7rHYost7", "title": "George Stiffman on promoting tofu in the West\n", "postedAt": "2023-09-04T17:57:19.116Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=rolisPZ0D70\"><div><iframe src=\"https://www.youtube.com/embed/rolisPZ0D70\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p>We discuss the path to popularizing tofu in the United States, the challenges that are in the way, whether it is better for the movement to take an additive approach with veganism or a subtractive approach, and whether this will be able to displace conventional meat. We're joined by George Stiffman who works to promote Chinese tofu in the United States. He recently launched a book called Broken Cuisine aimed at teaching western cooks how to use tofu and other ingredients beyond just Chinese cooking. He\u2019s also active in the EA community where he co-leads the Effective Altruism Los Angeles group.&nbsp;<br>&nbsp;</p>", "user": {"username": "Karthik Palakodeti"}}, {"_id": "3Rz4TGR9T8JELGGG9", "title": "Transformative AI and Compute - Reading List", "postedAt": "2023-09-04T06:21:44.145Z", "htmlBody": "<p>This is a link post to the reading list about Transformative AI and Compute by Lennart Heim.</p><p>Readings within the list are put into the following buckets:</p><ol><li><a href=\"https://docs.google.com/document/d/1DF31DIkwS9GONzmy1W3nuI9HRAwSKy8JcIbzKYXg-ic/edit#heading=h.22di12jwyi2m\"><u>Compute in the AI Production Function</u></a>:&nbsp;<i>Why compute matters for AI</i><ul><li>Compute is one of the key drivers of AI progress.</li></ul></li><li><a href=\"https://docs.google.com/document/d/1DF31DIkwS9GONzmy1W3nuI9HRAwSKy8JcIbzKYXg-ic/edit#heading=h.2syusscpwpi9\"><u>Compute Supply Landscape</u></a>:&nbsp;<i>How chips are produced and used</i></li><li><a href=\"https://docs.google.com/document/d/1DF31DIkwS9GONzmy1W3nuI9HRAwSKy8JcIbzKYXg-ic/edit#heading=h.t5ffwi6bvd65\"><u>Compute Governance</u></a>:<i> How can we govern compute to achieve beneficial AI outcomes?</i><ul><li>Using compute as a governance node by either (a) monitoring, (b) restricting, or (c) promoting access.</li></ul></li><li><a href=\"https://docs.google.com/document/d/1DF31DIkwS9GONzmy1W3nuI9HRAwSKy8JcIbzKYXg-ic/edit#heading=h.7hhqfxns4gt4\"><u>Compute-Based Transformative AI Forecasting</u></a><ul><li>How much compute we might need to achieve certain transformative and potentially dangerous capabilities</li><li>How much compute we will have in the future and which computing paradigms will dominate</li></ul></li><li><a href=\"https://docs.google.com/document/d/1DF31DIkwS9GONzmy1W3nuI9HRAwSKy8JcIbzKYXg-ic/edit#heading=h.xsewbkbnuqo5\"><u>Others</u></a>: Books, research questions, related topics, newsletters, podcasts, career advice, forecasting&nbsp;</li></ol><hr><p>(The author, Lennart Heim has agreed to this post.)</p><p>(Preview picture by MidJourney.)</p>", "user": {"username": "Frederik Berg"}}, {"_id": "tKqhAxHv7uMgnKjgN", "title": "Interest rate of giving", "postedAt": "2023-09-03T10:34:07.855Z", "htmlBody": "<p>Are there efforts and estimations on the interest rate of giving? Past interest rates are a central feature of conventional investment funds. It would be great to have a broad ballpark figure, much in the spirit of the cost of saving a live.</p><p>I searched for interest rate on the forum and did not find anything. Do you know of any posts, articles etc. on that topic you could share?</p>", "user": {"username": "Moritz Linder"}}, {"_id": "6ea45xoDuirMQWwHy", "title": "Czech national vision - advice on how to design the process?", "postedAt": "2023-09-03T09:57:43.182Z", "htmlBody": "<p>Over the last 1.5 years, we have developed an opportunity to start a nationwide process to create a long-lasting vision of the Czech Republic.</p>\n<p>From the cost-benefit perspective, we consider reaching a consensus on a stable, ambitious &amp; achievable vision to be really important for the development of our democracy and well-being.</p>\n<p>We are now starting to work on it with the newly elected Czech president and his office. He is a non-partisan person, which, we think, makes him a good facilitator of the vision-making process and then carrier of the vision.</p>\n<p>We estimate an 86% chance, that enough funds will be raised in the next months from individual philanthropists (with no one donating more than 5% of the budget), to ensure funding for the whole process.</p>\n<p>Based on our research and consultations, this process should be, among other things:</p>\n<ul>\n<li>based on wide public participation (civic assemblies, regional events, online platform...)</li>\n<li>based on expert foresight (scenarios, megatrends, forecasting...)</li>\n<li>mostly based on our comparative advantages (center of Europe, safety, good research...)</li>\n<li>apolitical (involving all relevant stakeholders and not being privatized by any political party)</li>\n<li>lasting around 2 years</li>\n<li>very skillfully communicated</li>\n<li>resulting in a \"pyramid\" (from catchy slogan and 1-6 big priorities to implementation\nplans for specific measurable goals), etc.</li>\n</ul>\n<p>An example of a successful vision could be E-estonia (digitalization), while we got inspired by visions such as Latvia 2050 or Spain 2050.</p>\n<p>Few things we would like to get thoughts on (but feel free to comment anything else):</p>\n<ul>\n<li>which (democratic) nation have done this recently?</li>\n<li>who are the top experts to consult or collaborate with?</li>\n<li>what internal organizational setup is optimal (e.g. how much control to give to donors / business?)</li>\n<li>which methods to use especially for involving citizens?</li>\n<li>how to responsibly introduce AI safety and other EA topics into the deliberation?</li>\n<li>what could go very wrong (to the extent of being counterproductive)?</li>\n<li>what would be a good resulting vision for the Czech Republic from a global perspective?</li>\n</ul>\n", "user": {"username": "janklenha"}}, {"_id": "buFyakASucJnrZj7X", "title": "The Lives We Can Save\n", "postedAt": "2023-09-03T05:46:25.673Z", "htmlBody": "<blockquote><p>I work as a Resident Assistant at my college. Last year, only a few weeks into me starting, I was called at night to come help with a drunk student. I didn\u2019t actually help very much, and probably didn\u2019t have to be there. I didn\u2019t even have to write up the report at the end. At one point I went outside to let medical services into the building, but mostly I just stood in a hallway.</p><p>The person in question was so drunk they couldn\u2019t move. They had puked in the bathroom and were lying in the hallway crying. They could barely talk. When Campus Safety arrived they kneeled down next to this person and helped them drink water, while asking the normal slew of questions about the person\u2019s evening.</p><p>They asked this person, whose name I can\u2019t even remember, why they had been drinking so much. They said, in between hiccups and sobs, \u201cfriend doesn\u2019t want to be friend anymore.\u201d</p><p>How do you describe that feeling? I don\u2019t think transcription can convey the misery and the drunkenness and the awful situation that had led to this awful situation. Someone drank so much that they could barely move, was lying curled in a hallway where all the other residents could and were watching, and was only able to muster out \u201cfriend doesn\u2019t want to be friend anymore\u201d as they cried.</p><p>Should I only care because I happened to be standing in that hallway on a late September evening? Had I remained in my room, laughing with my friends, would this person\u2019s struggle have been worth nothing?</p></blockquote><p>\u2014Max Alexander (<a href=\"https://scoutingahead.substack.com/p/confessions-of-max-my-experience-joining-effective-altruism\"><u>this whole post</u></a> is very worth reading)!</p><p>It\u2019s sometimes hard to be motivated to help the world. The trip you forego, the fun you could have had with a friend, the nice things you could have bought are instead sent straight into the coffers of some charity that you\u2019ve read about. It can feel sort of alienating when you think just of the number of people you have saved. Instead of thinking of numbers, think of stories. The people who make up the numbers\u2014who make up the hundreds of thousands of lives saved by effective charities\u2014are real, flesh-and-blood people, who matter just as much as you and I. We may not look into the gaunt faces of those who would have otherwise starved to death, we may not see their suffering with our eyes, but we know it is real. People are dying in ways that we can prevent. <a href=\"https://www.givewell.org/charities/top-charities\"><u>GiveWell top charities</u></a> can save lives for only a few thousand dollars.</p><p>It\u2019s hard to get your mind around that. I have a friend who has raised over 50,000 dollars for effective charities. 10 lives. 10 people. 10 people, many of them children, who will be able to live out a full life, rather than being snuffed out at a young age by a horrible painful disease. They will not have to lie in bed, with a fever of 105, slowly dying of malaria when they are five. They will have the chance to grow up.</p><p>Who are these people? I do not know. But I can imagine their stories. I can imagine their stories because I can hear the stories of other people like this, people who are about to die. For example, on <a href=\"https://www.reddit.com/r/AskReddit/comments/2qt77o/serious_terminally_ill_patients_of_reddit_what_is/\"><u>this Reddit thread</u></a>, you can find the stories of lots of people who are about to die. Stories like these:</p><blockquote><p>Stage IV colon cancer here. Age 35. I'm a single mum to a 1-year-old and there is a 94% chance I'll be dead in 4 years. But there is still a wee bit of hope, so I try to hold onto that (hard to do most days). My days are filled with spending time with my baby and hoping that I live long enough that she'll remember me. She's pretty awesome and makes me laugh every day, so there is a lot of happiness in this life of mine.</p></blockquote><p>Reading these stories causes me to tear up. I think a lot of people have a similar response. They\u2019re so tragic\u2014entire lives being snuffed out. The line \u201cMy days are filled with spending time with my baby and hoping that I live long enough that she'll remember me,\u201d is one of the saddest things I\u2019ve ever read. These are probably people who cannot be helped. But there are so many others who can be helped, at minimal personal cost, who each of us can help. People like this:</p><blockquote><p>I have cystic fibrosis. I'm trying to achieve so many things normal people have, but are harder because of my CF. I want a boyfriend, and to eventually get married before I'm too ill. I just got my first post graduate school job, after moving across the world. I'm very, very afraid of the future, but I have some truly wonderful friends I can vent to, and my parents are very supportive. There's a lot of fear in my life- I've never pretended to be one of those patients who sets out to be a role model and an inspiration. I have break downs a lot. I struggle mentally and physically every day, and I may move back home soon, as my lung function has declined a lot in the past few months, but at least home has video games.</p></blockquote><p>Somehow, a single paragraph of explanation can transform someone from nameless and faceless to someone that I deeply care about. When I hear this person\u2019s story, I feel willing to give up a nice vacation or two to help them. I feel willing to take a different job to help avert this person\u2019s misery, to enable them to live a healthy life. And I think most people feel that way too.</p><p>When framed in these terms, the conclusion that you should use the opportunity you have, as one of the richest sliver of people to ever live, even if you\u2019re at well below middle-class income, to save the lives of people like this doesn\u2019t seem like a counterintuitive proposal. It\u2019s not the kind of thing that you have to be argued into believing, by way of ponds and abstruse philosophical arguments about distance. You don\u2019t need a philosophical argument to realize that you should spend 4,000 dollars to save him:</p><blockquote><p>I will die a slow, painful death.</p><p>I have CML, will progress to AML eventually, and then to death unless some miracle drug comes out in the next few years.</p><p>With the medication I take, the results are still relatively young - Gleevec has only been around for a short time, so it's still hit or miss on how people will take to the medication, how long it will take them to build a resistance to the medication, etc.</p><p>I should have been dead a few years ago, CML progressing to AML usually only takes a year or two give or take, and then it's a quick road down the painful death at the end.</p><p>I live my life rather slovenly, I quit working a few months ago as I kept getting sick, kept getting shit from work from being sick all the time, was tired of having to explain to people how somebody with cancer could appear to be so healthy i.e. not skinny frail and miserable all the time.</p><p>So now I spend my time at home, playing housemaid while my wife works. I read a lot, I play video games, I browse the internet for long periods of time, honestly pretty boring, but I'm just killing time as there really isn't much else I could be doing.</p></blockquote><p>When I find it hard to intuitively be willing to make personal sacrifices to help faraway strangers, I imagine the sacrifices I\u2019d want faraway strangers to make for me, or for my friends. If one of my friends was dying, and a stranger could save her life but instead decided to go on vacation, I\u2019d feel she\u2019d have a duty to forego the vacation and save my friends life. And yet we are those strangers who can save lives. Who can make it so that one fewer child has to sit in bed with a fever of 105, their brain becoming damaged, chills racking their body, until their heart eventually stops.</p><p>I heard a story recently about a young boy named Abdul. He was only five or so years of age. He was sick frequently\u2014going to the hospital multiple times per month. And when it went dark outside, he went blind.</p><p>Imagine how terrifying it is to be five years old and to go blind whenever the sun goes down.</p><p>I remember I used to be scared of the dark. And I didn\u2019t go blind whenever it was dark. It\u2019s hard to imagine just how terrifying this is\u2014to be sick constantly, to be blind at night, to have terrified parents who don\u2019t know how to help you, but who want to help you, more than anything. Parents who try to comfort you but are themselves terrified.</p><p>Abdul did get help. He got help because of one of the most effective charities in the world\u2014<a href=\"https://www.givewell.org/charities/helen-keller-international\"><u>Helen Keller International</u></a>. He was deficient in Vitamin A. Vitamin A allows us to see in the dark, and so Abdul couldn\u2019t do that when it went dark. Vitamin A deficiency leads to a weakened immune system, leading to often fatal disease. It also leads to blindness in millions of people. <a href=\"https://www.hki.org/what-we-do/saving-sight/\"><u>HKI notes</u></a>:</p><blockquote><p><strong>1 in every 7 people worldwide are living with vision loss because they lack access to care.</strong></p><p>Some 43 million of them are blind. Yet many of them didn\u2019t have to lose their sight. A staggering <strong>90% of all vision loss is preventable or treatable</strong>.</p></blockquote><p>It costs only a few thousand dollars to save a life if the money is given to HKI. It costs $1.10 cents to distribute a Vitamin A capsule. $1.10! It costs so little to avert such an extreme cause of misery.</p><p>My grandfather died before I turned 13. He was one of the kindest people I ever knew. I think about him rather often. He just embodied decency and virtue and love and kindness and all the other virtues. I\u2019ve heard multiple people describe him as the best man they ever knew, and I think he\u2019s probably the most morally upright person I\u2019ve ever met.</p><p>He was an intellectual. I recently read his master\u2019s thesis about Matthew Arnold. He\u2019d have loved to discuss it with me, just as he liked to discuss many intellectual matters.</p><p>He loved to talk to strangers. I remember one time, as we were standing in line at El Pollo Loco, he noticed a stranger who had multiple sets of car keys. The guy looked intimidating\u2014he was ripped and had tattoos. Most people would have been frightened to talk to him. Not my grandfather, however; he went up to him and asked why he had multiple keys. They had a lengthy conversation about it\u2014I think the guy collected cars.</p><p>I often think about my grandfather. I wonder what he\u2019d think about effective altruism, for instance. I think he\u2019d be a fan. I wish I could have seen him at my bar Mitzvah, and my cousins\u2019 Bar Mitzvah. I\u2019m currently crying, as I type this sentence. I wish I could call him, hug him, tell him about the things I\u2019ve learned in college, discuss things with him, figure out what he thinks about anything that happened after 2016.</p><p>In the end, there was nothing anyone could do for him. Nothing could stop his cancer. But if there was something people could do, if someone could have prevented his death for minimal cost but chose not to, then if they failed to do so, that would be unforgivable. But how is that different from what most of us do, when we spend thousands of dollars on a nice car, when that money could have saved multiple lives?</p><p>When I think about my grandfather, and what I or any member of my family would give to have just one more day with him, to give him the opportunity to see my cousins\u2019 upcoming Bar Mitzvah, it becomes obvious that it\u2019s worth making costs to save lives. The life of a person, however far away is worth more than a few thousand dollars, is worth more than a cheap car.</p><p>When I think about my grandfather, it becomes clear that donating lots of money to charities that demonstrably save lives is morally required. If you live on more income than almost all people ever, and you\u2019re not willing to give even 10% of it to save hundreds of lives, then I think you are failing in your duty. If you have the opportunity to save dozens of lives, but prefer to slightly elevate your standard of living, then I think you are doing something wrong.</p><p>So please, friends, sign the <a href=\"https://www.givingwhatwecan.org/pledge/flow\"><u>giving pledge</u></a> or at least the <a href=\"https://1fortheworld.org/\"><u>one for the world pledge</u></a>. These pledges are automatic. They make it so that you can save hundreds of lives while taking a minor haircut\u2014of <a href=\"https://www.givingwhatwecan.org/pledge/flow\"><u>either 10%</u></a> of your income or <a href=\"https://1fortheworld.org/\"><u>1%</u></a>. Surely 1% of your income is less valuable than many lives. Do it for the people who will die if you don\u2019t, people like Abdul and my grandfather and the various people whose deaths cannot be averted, who lament their tragic fates on reddit. Do it for the people hoping they won\u2019t die, hoping that they\u2019ll live long enough so that their child remembers them.</p><p>This minor haircut to your income won\u2019t significantly undermine your ability to engage in valuable activities. But even if it does, even if you can\u2019t go on the vacations you\u2019d want to because of the donations, don\u2019t think of that as a sacrifice. Because the life of an ENTIRE. LIVING. CHILD is much more valuable than whatever else you would have spent it on. A human full of hopes and dreams and aspirations, who can live a full life, who, if you fail to act, will be just as regretful as the people on reddit, pining about the tragedy of their upcoming demise can be saved by you. Don\u2019t just think of your donations as going into the void\u2014think of the concrete person, like you or me, who you can save. And when one does that, giving becomes easier. It\u2019s impossible to empathize with a number, but easy to empathize with a terrified and sick adorable five-year-old child, who just wants to play, but instead must lie in bed as his organs fail, experiencing more pain than you or I ever will.</p>", "user": {"username": "Omnizoid"}}, {"_id": "rDfGntTKFziAKcuSt", "title": "Thought experiment: Trading off risk, intragenerational and intergenerational inequality, and fairness", "postedAt": "2023-09-02T23:32:04.473Z", "htmlBody": "<p>This is a thought experiment designed to help clarify different aspects of one's value system relating to intertemporal decision making under risk with large effects on different forms of inequality and fairness: ex ante inequality (\"unfairness\"), ex post intragenerational interpersonal inequality, and ex post intergenerational inequality. As every thought experiment, it is highly stylized and unrealistic to be able to focus clearly on the aspects under investigation.</p>\n<p>The thought experiment is the following: Assume you are a decision maker who faces a choice between five possible interventions (e.g., different public health policies), only one of which you can enact. Each intervention would have severe consequences for everyone on Earth in the current and the next generation. Let us assume these two generations are roughly the same size and no later generations are affected.</p>\n<p>An intervention's effects may differ between homogametic/XX (mostly female) and heterogametic/XY (mostly male) subpopulations but will affect everyone inside one of these subpopulations in the same way, measurable by a per person gain or loss in WELLBYs (or, if you prefer, QALYs or DALYs).</p>\n<p>All but one intervention's effects are uncertain and depend on a \"coin toss by nature\" (landing heads or tails with roughly equal probability) that will only get known <em>after</em> the intervention is completed:</p>\n<ul>\n<li>If intervention A \"succeeds\", it gives everyone 10 additional WELLBYs, but if it \"fails\", it costs everyone 5 WELLBYs.</li>\n<li>Intervention B gives everyone in one generation 10 additional WELLBYs at the cost of -5 WELLBYs for everyone in the other generation. Nature decides which generation is the winning one in this case.</li>\n<li>Similarly, C gives either every XX person in both generations 10 additional WELLBYs, or every XY person in both generations. Nature decides which. All others lose 5 WELLBYs.</li>\n<li>D either makes the XX subpopulation of generation 1 and the XY subpopulation of generation 2 the winners, or makes the XXs of generation 2 and the XYs of generation 1 the winners. Again, nature decides which of the two scenarios applies.</li>\n<li>Only intervention E gives each member of the current generation 10 WELLBYs but costs everyone in the next generation 5 WELLBYs without uncertainty.</li>\n</ul>\n<p>The following table sumarizes these effects of all five interventions:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>coin lands:</th>\n<th>h(eads)</th>\n<th>h</th>\n<th>h</th>\n<th>h</th>\n<th>t(ails)</th>\n<th>t</th>\n<th>t</th>\n<th>t</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td><strong>generation:</strong></td>\n<td><strong>c(urrent)</strong></td>\n<td><strong>c</strong></td>\n<td><strong>n(ext)</strong></td>\n<td><strong>n</strong></td>\n<td><strong>c</strong></td>\n<td><strong>c</strong></td>\n<td><strong>n</strong></td>\n<td><strong>n</strong></td>\n</tr>\n<tr>\n<td></td>\n<td><strong>subpopulation:</strong></td>\n<td><strong>XX</strong></td>\n<td><strong>XY</strong></td>\n<td><strong>XX</strong></td>\n<td><strong>XY</strong></td>\n<td><strong>XX</strong></td>\n<td><strong>XY</strong></td>\n<td><strong>XX</strong></td>\n<td><strong>XY</strong></td>\n</tr>\n<tr>\n<td><strong>intervention:</strong></td>\n<td><strong>A</strong></td>\n<td>+10</td>\n<td>+10</td>\n<td>+10</td>\n<td>+10</td>\n<td>-5</td>\n<td>-5</td>\n<td>-5</td>\n<td>-5</td>\n</tr>\n<tr>\n<td></td>\n<td><strong>B</strong></td>\n<td>+10</td>\n<td>+10</td>\n<td>-5</td>\n<td>-5</td>\n<td>-5</td>\n<td>-5</td>\n<td>+10</td>\n<td>+10</td>\n</tr>\n<tr>\n<td></td>\n<td><strong>C</strong></td>\n<td>+10</td>\n<td>-5</td>\n<td>+10</td>\n<td>-5</td>\n<td>-5</td>\n<td>+10</td>\n<td>-5</td>\n<td>+10</td>\n</tr>\n<tr>\n<td></td>\n<td><strong>D</strong></td>\n<td>+10</td>\n<td>-5</td>\n<td>-5</td>\n<td>+10</td>\n<td>-5</td>\n<td>+10</td>\n<td>+10</td>\n<td>-5</td>\n</tr>\n<tr>\n<td></td>\n<td><strong>E</strong></td>\n<td>+10</td>\n<td>+10</td>\n<td>-5</td>\n<td>-5</td>\n<td>+10</td>\n<td>+10</td>\n<td>-5</td>\n<td>-5</td>\n</tr>\n</tbody>\n</table>\n<p><em><strong>Table 1</strong>: Gains and losses in WELLBYs (or, if you prefer, QALYs or DALYs) per person of five different hypothetical interventions with uncertain effects, by generation and subpopulation.</em></p>\n<p>As you can see, for all you know all five interventions give the <em>same expected net</em> gain of +2.5 WELLBYs per person averaged over both generations' total population.</p>\n<p>Still, I suspect that you (like me) might find some of the interventions <em>clearly</em> or at least <em>tentatively</em> preferable to some others from the list, while between certain other pairs you might be undecided. Such preferences might relate to how different interventions would lead to different forms and degrees of risk, inter- or intra-generational inequality, equal or unequal chances (fairness), and different intertemporal distributions of gains and losses.</p>\n<p>In order to clarify your values, you might want to first think about each of the <em>pairs</em> (A v B, A v C, B v C, ...) separately and see whether one of the two seems clearly preferable, or both seem equally desirable, or neither of these three possibilities. It might be helpful to do this exercise first without applying some formal aggregation formula (such as a nonlinear welfare function).</p>\n<p>Once you have written down your pairwise preference relation (which might turn out to be anything between a full ranking and a very incomplete, maybe even cyclic relation), you might then want to think of ways how the listed WELLBY quantities could be aggregated into an intervention's overall evaluation (a \"utility function\") that would be consistent with your pairwise preferences.</p>\n<p>(In my case, I have a hard time with the latter task since my preference relation seem to contain two incomparable pairs: I can't decide between B and C, and I can't decide between B and D, but I clearly prefer D to C.)</p>\n<p>As a last twist, you might also think about whether and how your preferences would change if \"+10\" was replaced by \"live to the age of 150\" and \"-5\" was replaced by \"die during infancy\", while \"XX\" and \"XY\" are replaced by \"people with gene Z\" and \"people without gene Z\", where Z is a hypothetical gene occurring in roughly half the population, not correlated in any obvious way with their phenotype.</p>\n<p><em>I'm very curious about your comments and preference disclosures!</em></p>\n<p>PS: reacting to a comment by Roman (see below), I note that the \"coin toss by nature\" uncertainty need not be interpreted as aleatoric uncertainty manifesting after your choice, but can of course also be interpreted as purely subjective (\"Bayesian\") probabilities representing your limited knowledge about some relevant aspects of the laws of nature that already exist before your choice.</p>\n", "user": {"username": "Jobst Heitzig (vodle.it)"}}, {"_id": "kJWqg4JjGcJCF5gyj", "title": "Is AI like disk drives?", "postedAt": "2023-09-02T19:12:33.931Z", "htmlBody": "<p>One finding of an extensive theoretical and empirical economics literature on technology races is that firms that trail the current leader might tend to innovate more (<a href=\"https://www.jstor.org/stable/1816571\">Reinganum 1983</a>; <a href=\"https://www.jstor.org/stable/2555803\">Lerner 1997</a>). My question is about whether the firm-level payoffs to innovation in the ~AI industry might cause similar patterns.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzvkx0me939b\"><sup><a href=\"#fnzvkx0me939b\">[1]</a></sup></span>&nbsp;The next paragraph very briefly summarizes the existing literature in order to clarify my question.<br><br>In the disk drive industry, the firm that managed to develop the disk drive with the highest storage density at the time earned a commercial advantage - it could take significant market share away from its competitors. This created an obvious incentive to innovate, for all firms. Somewhat counterintuitively, some theoretical models (e.g., Reinganum linked above) imply that this incentive is most strongly felt by firms trailing the current leader in disk drive density, rather than the leader itself. This is because, if the current leader invents a superior disk drive, it mostly retains its current market share. But if a trailing firm invents a superior disk drive, its payoff is much larger; it can seize market share from the current leader and other trailing firms. As a result, most innovation comes from trailing firms, rather than the current leader.<br><br><strong>Why does this matter?</strong></p><p>I'm not sure. I mostly ask out of curiosity. But if you expect 'challenger' firms to drive innovation more than 'leader' firms in either hardware or software for AI, this should adjust your beliefs towards a neck-and-neck race for AI. That is, as opposed to a world where one firm is leaps and bounds ahead of the rest, and has the time and money to pay an alignment tax.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzvkx0me939b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzvkx0me939b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The 'AI industry' is a deliberately vague term. I'm curious as to whether we observe this specific race dynamic in any of semiconductors, LLMs, etc.</p></div></li></ol>", "user": {"username": "Tanae"}}, {"_id": "bf4bHGyRByTQnqCRC", "title": "Please share with lawyers you know:  Legal Impact for Chickens seeks litigator. ", "postedAt": "2023-09-02T15:21:00.547Z", "htmlBody": "<p>Hi EAs!</p>\n<p>Thank you to everyone here who has supported Legal Impact for Chickens in various ways.</p>\n<p>Legal Impact for Chickens is hiring again!  Please forward widely!</p>\n<p><a href=\"https://www.legalimpactforchickens.org/careers\">https://www.legalimpactforchickens.org/careers</a></p>\n<p>Legal Impact for Chickens seeks a litigator.</p>\n<p>We\u2019re looking for the next attorney to help build our nonprofit and fight for animals.</p>\n<p>Want to join our team on the ground floor?</p>\n<p>About us:</p>\n<p>Legal Impact for Chickens (LIC) is a 501(c)(3) litigation nonprofit.  We work to protect farmed animals.</p>\n<p>You may have seen our Costco shareholder derivative suit in The Washington Post, Fox Business, or CNN Business\u2014or even on TikTok.</p>\n<p>Now, we\u2019re looking for our next hire: an entrepreneurial litigator to help fight for animals!</p>\n<p>About you:</p>\n<p>\u2022 Licensed and in good standing with your state bar</p>\n<p>\u2022 3+ years of litigation experience preferred</p>\n<p>\u2022 Excellent analytical, writing, and verbal-communication skills</p>\n<p>\u2022 Zealous, creative, enthusiastic litigator</p>\n<p>\u2022 Passion for helping farmed animals</p>\n<p>\u2022 Interest in entering a startup nonprofit on the ground floor, and helping to build something</p>\n<p>\u2022 Willingness to do all types of nonprofit startup work, beyond just litigation</p>\n<p>\u2022 Strong work ethic and initiative</p>\n<p>\u2022 Love of learning</p>\n<p>\u2022 Kind to our fellow humans, and excited about creating a welcoming, inclusive team</p>\n<p>About the role:</p>\n<p>You will be an integral part of LIC.  You\u2019ll help shape our organization\u2019s future.</p>\n<p>Your role will be a combination of (1) designing and pursuing creative impact litigation for animals, and (2) helping with everything else we need to do, to run this new nonprofit!</p>\n<p>Since this is such a small organization, you\u2019ll wear many hats:  Sometimes you may wear a law-firm partner\u2019s hat, making litigation strategy decisions or covering a hearing on your own.  Sometimes you\u2019ll wear an associate\u2019s hat, analyzing complex and novel legal issues.  Sometimes you\u2019ll pitch in on administrative tasks, making sure a brief gets filed properly or formatting a table of authorities.  Sometimes you\u2019ll wear a start-up founder\u2019s hat, helping plan the number of employees we need, or representing LIC at conferences.  We can only promise it won\u2019t be dull!</p>\n<p>This job offers tremendous opportunity for advancement, in the form of helping to lead LIC as we grow.  The hope is for you to become an indispensable, long-time member of our new team.</p>\n<p>Commitment: Full time</p>\n<p>Location and travel: This is a remote, U.S.-based position.  You must be available to travel for work as needed, since we will litigate all over the country.</p>\n<p>Reports to: Alene Anello, LIC\u2019s president</p>\n<p>Salary: $80,000\u2013$100,000 depending on experience</p>\n<p>Benefits: Health insurance, 401(k), flexible schedule, unlimited PTO (plus mandatory vacation!)</p>\n<p>One more thing!</p>\n<p>LIC is an equal opportunity employer.  Women and people of color are strongly encouraged to apply.  Applicants will receive consideration without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, ancestry, citizenship status, disability, age, medical condition, veteran status, marital status, political affiliation, or any other protected characteristic.</p>\n<p>To Apply:</p>\n<p>To apply, please email your cover letter, resume, writing sample, and three references, all combined as one PDF, to <a href=\"mailto:info@legalimpactforchickens.org\">info@legalimpactforchickens.org</a>.</p>\n<p>Thank you for your time and your compassion!</p>\n<p>Sincerely,\nLegal Impact for Chickens</p>\n", "user": {"username": "alene"}}, {"_id": "vCYyLxxdGXbbs2iax", "title": "New Princeton course on longtermism", "postedAt": "2023-09-01T20:31:20.734Z", "htmlBody": "<p>This semester (Fall 2023), Prof <a href=\"http://www.princeton.edu/~adame/\">Adam Elga</a> and I will be co-instructing Longtermism, Existential Risk, and the Future of Humanity, an upper div undergraduate philosophy seminar at Princeton. (Yes, I did shamelessly steal half of our title from <i>The Precipice</i>.) We are grateful for support from an <a href=\"https://www.openphilanthropy.org/open-philanthropy-course-development-grants/\">Open Phil course development grant</a> and share the reading list here for all who may be interested.&nbsp;</p><p>[Edit as of 19 Sept 2023: link to full syllabus\u2014which is a bit different than the reading list below: available <a href=\"https://docs.google.com/document/d/152ZZVnMf0T3OW_RyepObfaGaBVneW-VVtq_9VPCvG_Q/edit?usp=sharing\">here</a>]</p><p><i>Part 1: Setting the stage</i><br>&nbsp;</p><p>Week 1: Introduction to longtermism and existential risk</p><ul><li>Core<ul><li>Ord, Toby. 2020.&nbsp;<i>The Precipice: Existential Risk and the Future of Humanity</i>. London: Bloomsbury.<i>&nbsp;</i>Read introduction, chapter 1, and chapter 2 (pp. 49-56 optional); chapters 4-5 optional but highly recommended.</li></ul></li><li>Optional<ul><li>Roser (2022)&nbsp;<a href=\"https://ourworldindata.org/longtermism\"><u>\u201cThe Future is Vast: Longtermism\u2019s perspective on humanity\u2019s past, present, and future\u201d</u></a>&nbsp;<i>Our World in Data&nbsp;</i></li><li>Karnofsky (2021)&nbsp;<a href=\"https://www.cold-takes.com/this-cant-go-on/\"><u>\u2018This can\u2019t go on\u2019</u></a>&nbsp;<i>Cold Takes&nbsp;</i>(blog)</li><li>Kurzgesagt (2022)&nbsp;<a href=\"https://www.youtube.com/watch?v=LEENEFaVUzU&amp;t=664s\"><u>\u201cThe Last Human - A Glimpse into the Far Future\u201d</u></a><br>&nbsp;</li></ul></li></ul><p>Week 2: Introduction to decision theory</p><ul><li>Core<ul><li>Weisberg, J. (2021).&nbsp;<i>Odds &amp; Ends</i>.<a href=\"https://jonathanweisberg.org/vip/\">&nbsp;</a><a href=\"https://jonathanweisberg.org/vip/_main.pdf\"><u>https://jonathanweisberg.org/vip/_main.pdf</u></a> Read chapters 8, 11, and 14.</li><li>Ord, T., Hillerbrand, R., &amp; Sandberg, A. (2010). \u201cProbing the improbable: Methodological challenges for risks with low probabilities and high stakes.\u201d&nbsp;<i>Journal of Risk Research</i>,&nbsp;<i>13</i>(2), 191\u2013205.&nbsp;<a href=\"https://doi.org/10.1080/13669870903126267\"><u>https://doi.org/10.1080/13669870903126267</u> Read sections 1-2.</a></li></ul></li><li>Optional<ul><li>Weisberg, J. (2021).&nbsp;<a href=\"https://jonathanweisberg.org/vip/\"><i><u>Odds &amp; Ends</u></i></a> chapters 5-7 (these may be helpful background for understanding chapter 8, if you don\u2019t have much background in probability).</li><li>Titelbaum, M. G. (2020)&nbsp;<i>Fundamentals of Bayesian Epistemology</i> chapters 3-4<br>&nbsp;</li></ul></li></ul><p>Week 3: Introduction to population ethics</p><ul><li>Core<ul><li>Parfit, Derek. 1984.&nbsp;<a href=\"https://www.stafforini.com/docs/Parfit%20-%20Reasons%20and%20persons.pdf\"><i><u>Reasons and Persons</u></i></a>. Oxford: Oxford University Press. Read sections 4.16.120-23, 125, and 127 (pp. 355-64; 366-71, and 377-79).</li><li>Parfit, Derek. 1986. \u201c<a href=\"https://www.stafforini.com/docs/parfit_-_overpopulation_and_the_quality_of_life.pdf\"><u>Overpopulation and the Quality of Life</u></a>.\u201d In&nbsp;<i>Applied Ethics</i>, ed. P. Singer, 145\u2013164. Oxford: Oxford University Press. Read sections 1-3.&nbsp;</li></ul></li><li>Optional<ul><li>Remainders of Part IV of&nbsp;<i>Reasons and Persons&nbsp;</i>and \u201cOverpopulation and the Quality of Life\u201d</li><li>Greaves (2017) \u201cPopulation Axiology\u201d&nbsp;<i>Philosophy Compass</i></li><li><i>McMahan (2022) \u201cCreating People and Saving People\u201d section 1, first page of section 4, and section 8</i></li><li>Temkin (2012)&nbsp;<i>Rethinking the Good&nbsp;</i>12.2 pp. 416-17 and section 12.3 (esp. pp. 422-27)</li><li>Harman (2004) \u201cCan We Harm and Benefit in Creating?\u201d</li><li>Roberts (2019) \u201cThe Nonidentity Problem\u201d&nbsp;<a href=\"https://plato.stanford.edu/entries/nonidentity-problem/\"><i><u>SEP</u></i></a></li><li>Frick (2022) \u201cContext-Dependent Betterness and the Mere Addition Paradox\u201d</li><li>Mogensen (2019) \u201cStaking our future: deontic long-termism and the non-identity problem\u201d sections 4-5<br>&nbsp;</li></ul></li></ul><p>Week 4: Longtermism: for and against</p><ul><li>Core<ul><li>Greaves, Hilary and William MacAskill. 2021.&nbsp;<a href=\"https://globalprioritiesinstitute.org/hilary-greaves-william-macaskill-the-case-for-strong-longtermism-2/\"><u>\u201cThe Case for Strong Longtermism.\u201d</u></a>&nbsp;<i>Global Priorities Institute&nbsp;</i>Working Paper No.5-2021. Read sections 1-6 and 9.&nbsp;</li><li>Curran, Emma J. 2023. \u201cLongtermism and the Complaints of Future People\u201d. Forthcoming in&nbsp;<i>Essays on Longtermism</i>, ed. H. Greaves, J. Barrett, and D. Thorstad. Oxford: OUP. Read section 1.&nbsp;</li></ul></li><li>Optional<ul><li>Thorstad (2023)&nbsp;<a href=\"https://philarchive.org/rec/THOHRL\"><u>\u201cHigh risk, low reward: A challenge to the astronomical value of existential risk mitigation.\u201d</u>&nbsp;</a>Focus on sections 1-3.</li><li>Curran, E. J. (2022). \u201c<a href=\"https://globalprioritiesinstitute.org/longtermism-aggregation-and-catastrophic-risk-emma-j-curran/\"><u>Longtermism, Aggregation, and Catastrophic Risk</u></a>\u201d (GPI Working Paper 18\u20132022). Global Priorities Institute.</li><li>Beckstead (2013)&nbsp;<a href=\"https://rucore.libraries.rutgers.edu/rutgers-lib/40469/PDF/1/play/\"><u>\u201cOn the Overwhelming Importance of Shaping the Far Future\u201d</u></a> Chapter 3</li><li>\u201c<a href=\"https://80000hours.org/podcast/episodes/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/\"><u>Toby Ord on why the long-term future of humanity matters more than anything else, and what we should do about it</u></a>\u201d&nbsp;<i>80,000 Hours podcast</i></li><li>Frick (2015) \u201cContractualism and Social Risk\u201d sections 7-8<br>&nbsp;</li></ul></li></ul><p><i>Part 2: Philosophical problems</i></p><p>&nbsp;</p><p>Week 5: Fanaticism</p><ul><li>Core<ul><li>Bostrom, N. (2009). \u201c<a href=\"https://doi.org/10.1093/analys/anp062\"><u>Pascal\u2019s mugging.</u></a>\u201d&nbsp;<i>Analysis</i>, 69 (3): 443\u2013445.<a href=\"https://doi.org/10.1093/analys/anp062\">&nbsp;</a></li><li>Russell, J. S. \u201c<a href=\"https://doi.org/10.1111/nous.12461\"><u>On two arguments for fanaticism.</u></a>\u201d&nbsp;<i>No\u00fbs</i>, forthcoming. Read sections 1, 2.1, and 2.2.</li><li>Temkin, L. S. (2022). \u201cHow Expected Utility Theory Can Drive Us Off the Rails.\u201d In L. S. Temkin (Ed.),&nbsp;<a href=\"https://doi.org/10.1093/oso/9780192849977.001.0001\"><i><u>Being Good in a World of Need</u></i></a>. Oxford University Press.</li></ul></li><li>Optional<ul><li>Wilkinson, H. (2022). In Defense of Fanaticism.&nbsp;<i>Ethics</i>,&nbsp;<i>132</i>(2), 445\u2013477.<a href=\"https://doi.org/10.1086/716869\">&nbsp;<u>https://doi.org/10.1086/716869</u></a> Prioritize sections I, III, and VI.A.</li><li>Beckstead and Thomas (2023)&nbsp;<a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/nous.12462\"><u>\u201cA Paradox for Tiny Probabilities and Enormous Values\u201d</u></a> Prioritize from the beginning through and including section 2.</li><li>Tarsney (2023) \u201cThe epistemic challenge to longtermism\u201d</li><li>Tarsney (2020)&nbsp;<a href=\"http://arxiv.org/abs/1807.10895\"><u>\u201cExceeding Expectations: Stochastic Dominance as a General Decision Theory\u201d</u></a></li><li>Balfour (2021) \u201cPascal\u2019s Mugger Strikes Again\u201d</li><li>Alexander (2022)&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/KDjEogAqWNTdddF9g/long-termism-vs-existential-risk\"><u>\u201c\u2018Longtermism\u2019 vs. \u2018Existential Risk\u2019\u201d</u></a> blog post on the&nbsp;<i>Effective Altruism Forum</i></li></ul></li></ul><p>&nbsp;</p><p>Week 6: Cluelessness</p><ul><li>Core<ul><li>Lenman, J. (2000). \u201c<a href=\"https://doi.org/10.1111/j.1088-4963.2000.00342.x\"><u>Consequentialism and Cluelessness.</u></a>\u201d&nbsp;<i>Philosophy and Public Affairs</i>,&nbsp;<i>29</i>(4), 342\u2013370.<a href=\"https://doi.org/10.1111/j.1088-4963.2000.00342.x\">&nbsp;</a>Read sections I, II, and VI.&nbsp;</li><li>Greaves, H. (2016). \u201c<a href=\"https://doi.org/10.1093/arisoc/aow018R\"><u>Cluelessness.</u></a>\u201d&nbsp;<i>Proceedings of the Aristotelian Society</i>, 116 (3): 311\u2013339.<a href=\"https://doi.org/10.1093/arisoc/aow018\">&nbsp;</a>Read sections V and VI (and also section III if you would like more background about the principle of indifference in order to understand section V).</li><li>Mogensen, A., &amp; MacAskill, W. (2021). \u201c<a href=\"http://hdl.handle.net/2027/spo.3521354.0021.015\"><u>The Paralysis Argument</u></a>.\u201d&nbsp;<i>Philosophers\u2019 Imprint</i>,&nbsp;<i>21</i>(15).<a href=\"http://hdl.handle.net/2027/spo.3521354.0021.015\">&nbsp;</a>Read sections 1-2.</li></ul></li><li>Optional<ul><li>Mogensen (2021) \u201cMaximal Cluelessness\u201d</li><li>Unruh (2023) \u201cThe Constraint Against Doing Harm and Long-term Consequences\u201d</li><li>Re-read section on cluelessness in Greaves and MacAskill \u201cThe Case for Strong Longtermism\u201d</li></ul></li></ul><p><br>Week 7: The Asymmetry</p><ul><li>Core<ul><li>Frick, Johann. 2020. \u201c<a href=\"https://doi.org/10.1111/phpe.12139\"><u>Conditional Reasons and the Procreation Asymmetry</u></a>.\u201d&nbsp;<i>Philosophical Perspectives&nbsp;</i>34: 53-87. Read sections 1-6 and 9.</li><li>Chappell, Richard Yetter. 2017. \u201c<a href=\"https://doi.org/10.1080/00455091.2016.1250203\"><u>Rethinking the Asymmetry</u></a>.\u201d&nbsp;<i>Canadian Journal of Philosophy</i> 47 (2-3): 167-77.<br>&nbsp;</li></ul></li></ul><p><i>Part 3: Our place in history and what we ought to do</i></p><p>&nbsp;</p><p>Week 8: Moral uncertainty</p><ul><li>Core<ul><li>MacAskill, W., Bykvist, K., &amp; Ord, T. (2020).&nbsp;<a href=\"https://www.moraluncertainty.com/s/Moral-Uncertainty.pdf\"><i><u>Moral Uncertainty</u></i></a>. Oxford: Oxford University Press. Read Introduction pp. 1-2, Chapter 1 sections I-II, and Conclusion pp. 213-14.</li><li>Harman, Elizabeth. (2015). \u201c<a href=\"https://doi.org/10.1093/acprof:oso/9780198738695.003.0003\"><u>The Irrelevance of Moral Uncertainty</u></a>.\u201d In R. Shafer-Landau (Ed.),&nbsp;<i>Oxford Studies in Metaethics, Volume 10</i>. Oxford University Press.<a href=\"https://doi.org/10.1093/acprof:oso/9780198738695.003.0003\">&nbsp;</a>Read sections 3.1 - 3.3.&nbsp;</li><li>Weatherson, Brian. (2014). \u201c<a href=\"https://doi.org/10.1007/s11098-013-0227-2\"><u>Running risks morally</u></a>.\u201d&nbsp;<i>Philosophical Studies</i>, 167(1), 141\u2013163.<a href=\"https://doi.org/10.1007/s11098-013-0227-2\">&nbsp;</a>Read sections 1, 3, and 4.</li></ul></li><li>Optional<ul><li>MacAskill, Bykvist, and Ord (2020)&nbsp;<a href=\"https://static1.squarespace.com/static/5f55ea9b5c71b34be165f6a0/t/5f5a2b4931fca70e0d129d95/1599744853131/Moral+Uncertainty.pdf\"><i><u>Moral Uncertainty</u></i></a> chapter 2 pp. 33-35</li><li>Barnett, Z. (2021). \u201cRational Moral Ignorance.\u201d&nbsp;<i>Philosophy and Phenomenological Research</i>, 102(3), 645\u2013664.<a href=\"https://doi.org/10.1111/phpr.12684\"><u> https://doi.org/10.1111/phpr.12684</u></a></li><li>Weatherson, B. (2019).&nbsp;<i>Normative Externalism</i>. Oxford University Press.<a href=\"https://doi.org/10.1093/oso/9780199696536.001.0001\">&nbsp;<u>https://doi.org/10.1093/oso/9780199696536.001.0001</u></a> Chapter 3 (\u201cAgainst asymmetry\u201d)</li><li>Weatherson (2019)&nbsp;<i>Normative Externalism&nbsp;</i>chapter 7 (\u201cLevel-crossing principles\u201d) and chapter 8 (\u201cHigher-order evidence\u201d)</li><li>Tarsney (2021) review of Weatherson&nbsp;<i>Normative Externalism</i> in&nbsp;<i>Mind</i></li></ul></li></ul><p>&nbsp;</p><p>Week 9: Moral uncertainty and stakes-sensitivity</p><ul><li>Core<ul><li>Williams, Evan G. 2015. \u201c<a href=\"https://doi.org/10.1007/s10677-015-9567-7\"><u>The Possibility of an Ongoing Moral Catastrophe</u></a>.\u201d&nbsp;<i>Ethical Theory and Moral Practice&nbsp;</i>18: 971-82. Read sections 1-2.&nbsp;</li><li>MacAskill, William and Toby Ord. 2020. \u201c<a href=\"https://doi.org/10.1111/nous.12264\"><u>Why Maximize Expected Choiceworthiness?</u></a>\u201d&nbsp;<i>No\u00fbs</i> 54 (2): 327-353. Read sections 1, 5, 6, and 7.iv-v.</li><li>Greaves, Hilary and Toby Ord. 2017. \u201c<a href=\"https://doi.org/10.26556/jesp.v12i2.223\"><u>Moral Uncertainty about Population Axiology</u></a>.\u201d&nbsp;<i>Journal of Ethics and Social Philosophy&nbsp;</i>12 (2): 135-67. Read sections 3, 4.1-4.5 (4.3 optional), and 8.</li><li>Beckstead, Nick and Teruji Thomas. 2023.&nbsp;<a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/nous.12462\"><u>\u201cA Paradox for Tiny Probabilities and Enormous Values.\u201d</u></a>&nbsp;<i>No\u00fbs.&nbsp;</i>DOI: 10.1111/nous.12462<i>.&nbsp;</i>Read&nbsp;section 6.</li></ul></li><li>Optional<ul><li>MacAskill (2019) \u201cPractical Ethics Given Moral Uncertainty\u201d<br>&nbsp;</li></ul></li></ul><p>Week 10: Transformative Artificial Intelligence</p><ul><li>Core<ul><li>Carlsmith, J. (2022). \u201c<a href=\"https://doi.org/10.48550/arXiv.2206.13353\"><u>Is Power-Seeking AI an Existential Risk?</u></a>\u201d (arXiv:2206.13353). arXiv. Read Introduction (page 1) and section 8.</li><li>Carlsmith, J. (forthcoming). \u201c<a href=\"https://jc.gatspress.com/pdf/existential_risk_and_powerseeking_ai.pdf\"><u>Existential risk from power-seeking AI.</u></a>\u201d In J. Barrett, H. Greaves, &amp; D. Thorstad (Eds.),&nbsp;<i>Essays on Longtermism</i>. Oxford University Press.<a href=\"https://jc.gatspress.com/pdf/existential_risk_and_powerseeking_ai.pdf\">&nbsp;</a>Read sections 1 - 5.3.3.</li><li>Lazar, S., Howard, J., &amp; Narayanan, A. (2023, May 30). \u201c<a href=\"https://www.fast.ai/posts/2023-05-31-extinction.html\"><u>Is Avoiding Extinction from AI Really an Urgent Priority?</u></a>\u201d&nbsp;<i>Fast.ai</i></li></ul></li><li>Optional<ul><li><a href=\"https://docs.google.com/spreadsheets/d/1OGiDFCgRjfxOFVaYPl96dYgx8NVIaSquDgQSKByDxuc/edit#gid=0\"><u>Responses</u></a> to Carlsmith</li><li>Thorstad (2023)&nbsp;<a href=\"https://ineffectivealtruismblog.com/category/exaggerating-the-risks/\"><u>\u2018Exaggerating the risks\u2019</u></a> parts 6-8 (see also Carlsmith\u2019s response in the comments on part 8)</li><li>Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., &amp; Man\u00e9, D. (2016).&nbsp;<i>Concrete Problems in AI Safety</i> (arXiv:1606.06565). arXiv.<a href=\"http://arxiv.org/abs/1606.06565\">&nbsp;<u>http://arxiv.org/abs/1606.06565</u></a></li><li>Critch, A., &amp; Russell, S. (2023).&nbsp;<i>TASRA: A Taxonomy and Analysis of Societal-Scale Risks from AI</i> (arXiv:2306.06924). arXiv.<a href=\"https://doi.org/10.48550/arXiv.2306.06924\">&nbsp;<u>https://doi.org/10.48550/arXiv.2306.06924</u></a></li><li>Cotra (2021)&nbsp;<a href=\"https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/\"><u>\u201cWhy AI alignment could be hard with modern deep learning\u201d</u></a></li><li>Ord (2020)&nbsp;<i>The Precipice</i> chapter 5 section on AI</li><li>Piper (2020)&nbsp;<a href=\"https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment\"><u>\u201cThe case for taking AI seriously as a threat to humanity\u201d</u></a>&nbsp;<i>Vox</i></li><li>Russell (2019)&nbsp;<i>Human Compatible&nbsp;</i>chapters 6-10</li><li>Christian (2020)&nbsp;<i>The Alignment Problem&nbsp;</i>chapters 7-9</li><li>John&nbsp;<i>et al.&nbsp;</i>(2023)&nbsp;<a href=\"https://pubmed.ncbi.nlm.nih.gov/37357710/\"><u>\u201cDead rats, dopamine, performance metrics, and peacock tails: proxy failure is an inherent risk in goal-oriented systems\u201d</u></a></li><li>Hadfield-Menell, Dragan, Abbeel, &amp; Russell (2017)&nbsp;<a href=\"http://arxiv.org/abs/1611.08219\"><u>\u201cThe Off-Switch Game\u201d</u></a></li><li>Chiang (2002) \u201cUnderstand\u201d in&nbsp;<i>Stories of Your Life and Others</i></li><li>Branwen, G. (2022).&nbsp;<a href=\"https://www.gwern.net/fiction/Clippy\"><u>\u201cIt Looks Like You\u2019re Trying To Take Over The World.\u201d</u></a></li></ul></li></ul><p>&nbsp;</p><p>Week 11: Possibilities for our future and what we can do</p><ul><li>Core<ul><li>Ord, Toby. 2020.&nbsp;<i>The Precipice: Existential Risk and the Future of Humanity</i>. London: Bloomsbury.<i>&nbsp;</i>Read chapter 8.</li><li>MacAskill, William. 2022.&nbsp;<i>What We Owe the Future</i>. New York: Basic Books.<i>&nbsp;</i>Read chapter 10.&nbsp;</li></ul></li><li>Optional<ul><li>80,000 Hours&nbsp;<a href=\"https://80000hours.org/career-guide/\">career guide</a> sections 1,4, and 5</li><li>Bostrom, \u201cHumanity\u2019s biggest problems aren\u2019t what you think they are\u201d&nbsp;<a href=\"https://www.youtube.com/watch?v=Yd9cf_vLviI&amp;ab_channel=TED\"><i><u>Ted</u></i></a></li><li>Bostrom (2020)&nbsp;<a href=\"https://nickbostrom.com/utopia#:~:text=I%20am%20one%20of%20your,may%20choose%20it%20for%20yourself.\"><u>\u201cLetter from Utopia\u201d</u></a></li></ul></li></ul><p>&nbsp;</p><p><strong>Further readings (all optional)</strong><br>&nbsp;</p><p>Discounting</p><ul><li>Cowen and Parfit (1992) \u201cAgainst the Social Discount Rate\u201d (in Peter Laslett &amp; James S. Fishkin (eds.)&nbsp;<i>Justice Between Age Groups and Generations</i> pp. 144\u2013161)</li><li>Mogensen (2022) \u201cThe only ethical argument for positive \u03b4? Partiality and pure time preference\u201d</li><li>Russell (2022)&nbsp;<a href=\"https://globalprioritiesinstitute.org/parfit-memorial-lecture-16-june-2022/\"><u>\u201cProblems for Intergenerational Equity\u201d</u></a> Parfit Memorial Lecture&nbsp;</li><li>Ord (2020)&nbsp;<i>The Precipice&nbsp;</i>Appendix A</li><li>Greaves (2017) \u201cDiscounting for Public Policy\u201d section 7 (pp. 404-09)</li><li>Kelleher (2017) \u201cPure time preference in intertemporal welfare economics\u201d</li></ul><p><br>The hinge of history hypothesis</p><ul><li>MacAskill (2022) \u201cAre We Living at the Hinge of History?\u201d</li><li>Mogensen (fc.)&nbsp;<a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/Mogensen-The-Hinge-of-History-Hypothesis-reply-to-MacAskill.pdf\"><u>\u201cThe hinge of history hypothesis: reply to MacAskill\u201d</u></a></li><li>Fisher (2020)&nbsp;<a href=\"https://www.bbc.com/future/article/20200923-the-hinge-of-history-long-termism-and-existential-risk\"><u>\u201cAre We Living at the \u2018Hinge of History\u2019?\u201d</u></a>&nbsp;<i>BBC</i></li><li>Karnofsky (2021) \u201cthe most important century\u201d&nbsp;<a href=\"https://www.cold-takes.com/most-important-century/\"><u>blog post series</u></a></li><li>Morris (2010)&nbsp;<i>Why the West Rules\u2014For Now&nbsp;</i>concluding chapter on the 21st century</li></ul><p><br>Global catastrophic biological risks</p><ul><li>Ord (2020)&nbsp;<i>The Precipice&nbsp;</i>chapter 5 section on pandemics</li><li>Piper (2022) \u201cWhy experts are terrified of a human-made pandemic \u2014 and what we can do to stop it\u201d&nbsp;<a href=\"https://www.vox.com/22937531/virus-lab-safety-pandemic-prevention\"><i><u>Vox</u></i></a></li><li>Lewis (2020) \u201cReducing global catastrophic biological risks\u201d&nbsp;<a href=\"https://80000hours.org/problem-profiles/global-catastrophic-biological-risks/\"><i><u>80,000 Hours</u></i></a></li></ul><p>&nbsp;</p><p>Coordination problems and great power conflict</p><ul><li>Nordstrom (2020)&nbsp;<a href=\"https://nordstromjf.github.io/\"><i><u>Inquiry-Based Introduction to Game Theory</u></i></a> chapters 2 and 4</li><li>Alexander (2014)&nbsp;<a href=\"https://slatestarcodex.com/2014/07/30/meditations-on-moloch/\"><u>\u201cMeditations on Moloch\u201d</u></a></li><li>Allison (2017)&nbsp;<i>Destined for War: Can America and China Escape Thucydides\u2019s Trap?&nbsp;</i></li><li>Clare (2021)&nbsp;<a href=\"https://assets.ctfassets.net/x5sq5djrgbwu/1cT1AsYCIomlUhOque0JgW/74ffb0b1a4803e1f9654e86e2c7f3af1/Great_Power_Conflict_report_-_Founders_Pledge.pdf\"><u>\u201cGreat Power Conflict\u201d</u></a> pp. 1-6, 59-97</li><li>Blattman (2022)&nbsp;<i>Why We Fight</i></li></ul><p>&nbsp;</p><p>Human enhancement</p><ul><li>Buchanan (2011)&nbsp;<i>Beyond Humanity?: The Ethics of Biomedical Enhancement</i> (especially chapter 2)</li><li>Bostrom (2013) \u201cWhy I want to be a posthuman when I grow up\u201d</li><li>Bostrom and Ord (2006) \u201cThe Reversal Test: Eliminating Status Quo Bias in Applied Ethics\u201d</li><li>Chiang (2019)&nbsp;<a href=\"https://www.nytimes.com/2019/05/27/opinion/ted-chiang-future-genetic-engineering.html\"><u>\u201cIt\u2019s 2059, and the Rich Kids Are Still Winning\u201d</u></a>&nbsp;<i>NYT</i><br>&nbsp;</li></ul><p>Non-consequentialist concern for the future</p><ul><li>Scheffler (2013)&nbsp;<i>Death and the Afterlife</i></li><li>Scheffler (2018)&nbsp;<i>Why Worry about Future Generations?</i></li><li>Caney (2018) \u201cJustice and Future Generations\u201d</li><li>Meyer (2021) \u201cIntergenerational Justice\u201d&nbsp;<a href=\"https://plato.stanford.edu/entries/justice-intergenerational/\"><i><u>SEP</u></i></a></li><li>Finneron-Burns (2016) \u201cContractualism and the Non-Identity Problem\u201d</li><li>Kumar (2018) \u201cRisking Future Generations\u201d<br>&nbsp;</li></ul><p>Impossibility results in population axiology</p><ul><li>Arrhenius (2011) \u201cThe Impossibility of a Satisfactory Population Ethics\u201d</li><li>Thomas (2018) \u201cSome Possibilities in Population Axiology\u201d</li><li>Thornley (2021)&nbsp;<a href=\"https://link.springer.com/article/10.1007/s11098-021-01621-4\"><u>\u201cThe impossibility of a satisfactory population prospect axiology (independently of Finite Fine-Grainedness)\u201d</u></a></li><li>Zuber&nbsp;<i>et al</i>. (2021) \u201cWhat Should We Agree on about the Repugnant Conclusion?\u201d</li><li>Huemer (2008) \u201cIn Defence of Repugnance\u201d</li></ul>", "user": {"username": "Calvin_Baker"}}, {"_id": "RB5jKCcMuE9EEAJvr", "title": "EASE (EA org service providers) Newsletter - Tips from EASE: What Does Operations Really Mean?", "postedAt": "2023-09-01T19:26:00.562Z", "htmlBody": "<p><strong>\"Operations\" is currently a big EA buzzword, but too many parts of the organization are incorrectly classified under this umbrella.</strong></p><p><br>In reality, \"operations\" covers practically everything that relates to the day-to-day running of the organization. The lack of clarity regarding what the organization really needs often means that it doesn't get the right help.<br><br>We asked our experts how their services are commonly mislabeled as operations, and tips for making sure that their area of specialty is given proper attention in your organization.&nbsp;</p><p><a href=\"https://mailchi.mp/05ea3373d854/kplm9b3pfo-9593653?fbclid=IwAR3epkLR8LPEwa_XfKV33DfD6AkukWOCLWjoE0gwpqhxQIbqWx3FOTzcJxI\">View our post here to see what they said.</a></p>", "user": {"username": "Deena Englander"}}, {"_id": "f5deKsFjLgiK3M6qR", "title": "What is OpenAI's plan for making AI Safer?", "postedAt": "2023-09-01T11:15:22.067Z", "htmlBody": "", "user": {"username": "brook"}}, {"_id": "GaFnvsxowWZ7NxnhJ", "title": "Paper summary\u2013\u2013Protecting future generations: A global survey of legal academics", "postedAt": "2023-09-05T10:29:58.573Z", "htmlBody": "<p><i>This is a summary of the paper \"</i><a href=\"https://www.legalpriorities.org/research/protecting-future-generations.html\"><i>Protecting future generations: A global survey of legal academics</i></a><i>\" by Eric Mart\u00ednez and Christoph Winter. The summary was written by Riley Harris.</i></p><p>As longtermists, we believe it is crucial to shape the long-term future for the better. Legal longtermists are particularly interested in how the law can be used to safeguard the interests of future generations and improve the lives of people in the coming centuries and millennia. In \u201c<a href=\"https://www.legalpriorities.org/research/protecting-future-generations.html\">Protecting future generations</a>\u201d, Eric Mart\u00ednez and Christoph Winter find that legal academics tend to believe that future generations deserve greater legal protections.</p><h2>Do future generations deserve more protection?</h2><p><i>Longtermism</i> is the idea that we should focus on making the future better. This is because the future is incredibly important \u2014 and if we can make it better in predictable ways, then we could impact the lives of billions or trillions of people. <i>Legal longtermists</i> focus on how legal systems can protect future generations and improve the long-run future. This survey looks at a few open questions in legal longtermism \u2014 for example, can and should the law protect future generations? To answer these questions, Mart\u00ednez and Winter surveyed 516 legal experts from top English-speaking and common-law universities around the world.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnozxhb4a2or\"><sup><a href=\"#fnnozxhb4a2or\">[1]</a></sup></span></p><p>Legal systems tend not to protect future generations.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefiam8uhv003o\"><sup><a href=\"#fniam8uhv003o\">[2]</a></sup></span>&nbsp;For example, in the United States, cost benefit analysis is used to make regulatory decisions \u2014 but the process \u201cdiscounts\u201d away the interests of future generations<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefv6jepm5cgv\"><sup><a href=\"#fnv6jepm5cgv\">[3]</a></sup></span>&nbsp;in a way seems unjustified to most philosophers.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvzd2egv3ifc\"><sup><a href=\"#fnvzd2egv3ifc\">[4]</a></sup></span></p><p>In the legal system, one of the most important rights is the ability to take legal action. In the United States, this right can be invoked when you are harmed or placed in immediate danger. Harms to future generations don't fit this criteria.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6t7f3kbtwqf\"><sup><a href=\"#fn6t7f3kbtwqf\">[5]</a></sup></span>&nbsp;However, over two-thirds of surveyed legal experts thought there was some legal basis for people in the next 100 years to sue, in at least some cases. Strikingly, more than half also thought the same was true for those living more than 100 years from now. This may indicate opportunities to assert the rights of future generations with the right cases and arguments.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgjaptnty2rg\"><sup><a href=\"#fngjaptnty2rg\">[6]</a></sup></span></p><p>However, future generations have never been allowed to sue (Bogojevi\u0107, 2020), and legal experts think that future generations should be protected more than they are today. On average, future generations are only protected a third as much as they should be \u2014 according to legal experts. They thought that the current generation should be protected more than they are now, and more than future generations should be. They also found that other groups, like animals and people from other countries, are not protected as much as they should be, but future generations are by far the most neglected by the law.</p><h2>Protecting future generations</h2><p>The law sometimes has long-term effects, for example, ancient Roman law still influences many civil law systems (Watson 1991). In other cases, effects may be short or unpredictable.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2mexf297xbd\"><sup><a href=\"#fn2mexf297xbd\">[7]</a></sup></span>&nbsp;For the legal longtermist, everything hinges on whether the law provides predictable and feasible ways to help future generations. Legal experts think the law can provide protection to future generations, and that it is one of the best ways to predictably help future generations. Mart\u00ednez and Winter estimate<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjpjgelvp44i\"><sup><a href=\"#fnjpjgelvp44i\">[8]</a></sup></span>&nbsp;that around 74% of legal academics somewhat agree that the law can help people more than a century from today. While about 73% somewhat agree that legal methods are among the most predictable and feasible ways of doing so. About 41% somewhat agree there are even ways for the law to protect people who are a millennium away!<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsy16k2cl9im\"><sup><a href=\"#fnsy16k2cl9im\">[9]</a></sup></span></p><p>Next, Mart\u00ednez and Winter explored the most promising areas of law from the perspective of legal longtermism. Many show at least some promise \u2014 for each area in the survey at least half of legal academics would somewhat agree that area could be helpful. However, legal academics were most confident in environmental law and constitutional law. The most promising constitutional mechanisms are ensuring that 1% of GDP goes towards safeguarding humanity against existential risks or giving explicit legal standing to future generations.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftg3kphzwebs\"><sup><a href=\"#fntg3kphzwebs\">[10]</a></sup></span>&nbsp;However, the differences here were small.</p><p>Finally, they asked about causes that longtermists tend to find important, to identify the most promising ones for legal intervention. Although at least half of the academics thought all the areas might be helped, they had lower confidence in artificial intelligence (56%) than the others \u2014 for instance, climate change (84%).</p><h2>Conclusion</h2><p>Eric Mart\u00ednez and Christoph Winter surveyed legal academics and found that they generally believe that future generations deserve more protection. The law provides a promising avenue for providing these protections \u2014 in particular environmental and constitutional law. Future research could explore the attitudes of different groups towards legal longtermism, try to compare specific policies rather than broad areas of law, or investigate other legal concepts such as personhood.</p><h2>Sources</h2><p>Renan Ara\u00fajo &amp; Leonie Koessler (2021). <a href=\"https://www.legalpriorities.org/research/constitutional-protection-future-generations.html\">The Rise of the Constitutional Protection of Future Generations.</a> <i>Legal Priorities Project Working Paper No. 7-2021.</i></p><p>Sanja Bogojevi\u0107 (2020). <a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/reel.12345\">Human rights of minors and future generations: global trends and EU law particularities</a>. <i>Review of European comparative &amp; international environmental law</i> 29/2.</p><p>John Broome (1994). <a href=\"https://www.jstor.org/stable/2265483\">Discounting the Future</a>. <i>Philosophy &amp; Public Affairs</i> 23/2.</p><p>Tyler Cowen &amp; Derek Parfit (1992). Against the Social Discount Rate. <a href=\"https://www.jstor.org/stable/j.ctt211qw3x\"><i>Philosophy, Politics, and Society: Volume 6, Justice Between Age Groups and Generations</i>.</a> Yale University Press. Edited by Peter Laslett &amp; James S. Fishkin.</p><p>Moritz A. Drupp, Mark C. Freeman, Ben Groom and Frikk Nesje (2018). <a href=\"https://www.aeaweb.org/articles?id=10.1257/pol.20160240\">Discounting Disentangled.</a> <i>American Economic Journal: Economic Policy</i> 10/4.</p><p>Thomas Ginsburg, Zachary Elkins, and James Melton (2009). <a href=\"https://www.law.uchicago.edu/news/lifespan-written-constitutions\">The Lifespan of Written Constitutions</a>. <i>The University of Chicago Law School.</i></p><p>Andreas Mogensen (2019). <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/2020/Andreas_Mogensen_maximal_%20cluelessness.pdf\">Maximal Cluelessness.</a> <i>Global Priorities Institute Working Paper No. 2-2019.</i></p><p>Office of Management and Budget (2003). <a href=\"https://obamawhitehouse.archives.gov/omb/circulars_a004_a-4/\"><i>Executive Office of the President, Circular A-4: Regulatory Analysis.</i></a></p><p>Derek Parfit (1986). <a href=\"https://academic.oup.com/book/12484\">Reasons and Persons.</a> Oxford University Press.</p><p>Alan Watson (1991). <a href=\"https://ugapress.org/book/9780820312613/roman-law-and-comparative-law/\"><i>Roman Law and Comparative Law.</i></a> University of Georgia Press.</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnozxhb4a2or\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnozxhb4a2or\">^</a></strong></sup></span><div class=\"footnote-content\"><p><i>Most of the participants were professors (88.6%), and they came from Europe, Oceania, and the Americas, with some from Asia (9.3%) and Africa (5.2%). Most were at least somewhat liberal (80.4%). Some questions were answered by fewer participants, but they were unlikely a result of chance. Results were also mostly consistent across different demographics \u2014 but some questions were answered differently by participants living in Asia.</i></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fniam8uhv003o\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefiam8uhv003o\">^</a></strong></sup></span><div class=\"footnote-content\"><p><i>One exception to this tendency is constitutional law. Around one third of all constitutions mention future generations, usually for environmental protections \u2014 but even in countries with strong constitutional protections, these protections are not enforced. See Ara\u00fajo and Koessler (2021).</i></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnv6jepm5cgv\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefv6jepm5cgv\">^</a></strong></sup></span><div class=\"footnote-content\"><p><i>Office of Management and Budget (2003).</i></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvzd2egv3ifc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvzd2egv3ifc\">^</a></strong></sup></span><div class=\"footnote-content\"><p><i>See Parfit (1984), Cowen &amp; Parfit (1992), Broome (1994) and Mogensen (2019). Many economists also agree, see Drupp et al. (2018).</i></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6t7f3kbtwqf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6t7f3kbtwqf\">^</a></strong></sup></span><div class=\"footnote-content\"><p><i>Other places have different requirements, but nowhere explicitly allows future generations to sue.</i></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngjaptnty2rg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgjaptnty2rg\">^</a></strong></sup></span><div class=\"footnote-content\"><p><i>It's also possible that there are similarly strong arguments against the legal basis for future generations to sue, or that judges are applying the law incorrectly.</i></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2mexf297xbd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2mexf297xbd\">^</a></strong></sup></span><div class=\"footnote-content\"><p><i>For instance, constitutions only last around 17 years on average, and several of the founding fathers were sceptical that the US constitution would last more than a generation. See Elkins et. Al. (2009).</i></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjpjgelvp44i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjpjgelvp44i\">^</a></strong></sup></span><div class=\"footnote-content\"><p><i>Here they randomly sample from the responses to correct for certain distribution types \u2014 using a </i><a href=\"https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Methods_for_bootstrap_confidence_intervals\"><i>bias-corrected and accelerated bootstrap</i></a><i> to generate confidence intervals.</i></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsy16k2cl9im\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsy16k2cl9im\">^</a></strong></sup></span><div class=\"footnote-content\"><p><i>Interestingly, 55% thought that environmental law \u2014 one of the most promising areas \u2014 had a hope of protecting the further future. This perhaps indicates that when these questions were asked in an abstract way experts tended to underestimate how helpful the law could be.</i></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntg3kphzwebs\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftg3kphzwebs\">^</a></strong></sup></span><div class=\"footnote-content\"><p><i>They also asked about protecting future generations from discrimination, spending 1% of GDP to protect against existential risks, granting explicit standing to future generations, creating a commision to oversea the protection of future generation, and establishing the explicit state goal of protecting the future.</i></p></div></li></ol>", "user": {"username": "rileyharris"}}, {"_id": "FHGk5uqpSsjemw4mL", "title": "Should UV stability of plastics be a concern for far-UVC adoption?", "postedAt": "2023-08-31T23:41:32.216Z", "htmlBody": "<p>TL;DR: Some plastics degrade under exposure to UV light and I am concerned this could hamper widespread adoption of far-UVC. This post outlines the rationale for these concerns and seeks feedback from the far-UVC community on their importance.</p><p><br>My simplified line of reasoning is:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/FHGk5uqpSsjemw4mL/poafnqmy2phduj9kmsbk\" alt=\"7wv54q.jpg\"></p><p><i>Epistemic status: this post is the culmination of spending ~5-10 hours thinking, researching, and writing up this post. I feel pretty certain that UV stability is worth at least thinking about with respect to far-UVC adoption but very uncertain about it being something that blocks far-UVC adoption. I have spent some time learning about far-UVC through discussions, reading, and preparing to interview someone in the space for a podcast but don\u2019t feel I have a very deep understanding of the space.</i></p><h2>Summary</h2><p>UV light damages plastics that are not UV-stable. Many of us may have encountered this in cheap outdoor furniture whose plastic components change colour or become brittle and break easily after being left out in the sun too long. My concern is that if most of the plastic materials used in indoor materials are not UV-stable \u2014 meaning they undergo irreversible physical changes when exposed to UV light \u2014 then placing far-UVC lights indoors could cause unwanted damage to plastics and limit the demand for far-UVC.</p><p>In this post, I focus on two ways by which this damage could hamper the uptake of far-UVC: consumer preferences on aesthetic effects and building regulations on physical degradation. These may not be the only ways and I\u2019m uncertain about how concerned to be about each of them. However, I think they illustrate why the UV stability of plastics is concerning to me and why I\u2019d like to see more research into it.</p><p>Both of these concerns could result in damping the market for early adoption of far-UVC. My impression is that demand for far-UVC will be required to bring down the cost of the technology. If the price of the technology remains high, this could inhibit adoption and make far-UVC an intractable defence mechanism for pandemics and global catastrophic bio risks (GCBRs).</p><p>My aim with this post is to present my rationale behind this concern and get feedback from the far-UVC community on the magnitude of this concern relative to other bottlenecks in the space.</p><h2>Effects of UV light on plastics</h2><p>The aesthetic and mechanical effects of UV light on plastics are two examples of why I think this could be worth spending more time thinking about the UV stability of plastics. Of the two, I\u2019m more worried about the mechanical effects as if mechanical degradation results in blocking the installation of far-UVC then this could be a significant issue for adoption.</p><h3>Aesthetic effects</h3><p><i>TL;DR: Aesthetic changes to plastics may result in consumers being unwilling to adopt far-UVC lighting.</i></p><p>The aesthetic effects of UV radiation on non-UV-stable plastics appear to primarily be colour change \u2014 notably a yellowing of plastics. Other aesthetic effects that I\u2019m less certain about and would ideally like to research more are cracking, stickiness, chalkiness that rubs off on contact, and texture change (eg. increased roughness). The aesthetic effects seem significant from a personal preference point of view \u2014 I wouldn\u2019t want all the plastic surfaces in my home or office to turn yellow over time.</p><p><a href=\"https://www.mdpi.com/2076-3417/13/7/4141\">A recent study</a> on the use of far-UVC on public transport buses simulated 6.2 years of exposure to far-UVC light<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2lvkk32stzi\"><sup><a href=\"#fn2lvkk32stzi\">[1]</a></sup></span>&nbsp;and found \u201c...that far-UVC radiation at 222 nm causes significant colour degradation in all the polymeric materials tested. The degree of color degradation varies depending on the type of polymeric material and the duration of exposure to far-UVC radiation. An obvious color difference was observed on FRC and PVC materials, where \u2206<i>E</i>00 values of 6.431 and 7.194, respectively, were obtained after 290 J/cm2 radiant exposure\u201d. For context, \u201cthe value of \u2206<i>E</i>00 can be interpreted to range from normally invisible difference (&lt;1) to very obvious difference (&gt;5).\u201d So it seems colour change is significantly noticeable after enough exposure.</p><p>For me, questions still remain around how much people actually care about this but my intuition is that any perceptible colour change would be seen unfavourably by consumers. If the selling point of far-UVC is something like \u201cyou can install special light globes that kill germs in the air without you even knowing\u201d then \u201cthese lights will turn your plastic stuff yellow\u201d may undermine the \u201cwithout you even knowing\u201d part of the sell.</p><h3>Mechanical effects</h3><p><i>TL;DR: Mechanical degradation of plastics could create safety hazards which cause regulatory bodies to block the installation of far-UVC indoors.</i></p><p>I think the effect of far-UVC on mechanical properties warrants greater concern than the aesthetic effects from an adoption perspective. Mechanical properties include, among others, the strength of the material \u2014 how much force can the plastic take before breaking \u2014 and brittleness \u2014 how well can it absorb stress before suddenly fracturing without significant deformation. An example of a brittle material would be glass, which doesn\u2019t deform much before breaking and this is in contrast to an elastic material like a silicone phone case which deforms a lot before breaking.</p><p>If materials are degrading to the point of failure then this could create hazards in the indoor environment. An example of this could be increasing the brittleness of electrical coverings such as power points or light switches. If they were to become weak and/or brittle to the degree that switching on the light or plugging in an appliance causes the plastic housing to break and expose live wires then this would be a significant hazard. Another example is permanent damage to medical equipment or other \u2018mission critical\u2019 or expensive devices that are used in indoor settings.</p><p>Consumers \u2014 both the general population and business consumers such as hospitals, offices, government, etc. \u2014 wouldn\u2019t want to risk this happening which is a concern in the same way the aesthetic effects are. However, I think a more significant effect could be intervention by building code authorities who ensure safety in the built environment. If a significant hazard is created by the mechanical degradation of plastics exposed to far-UVC light then installing the lights may become restricted by building codes. I see this as a more significant effect as it would prevent consumers from installing far-UVC even if they wanted to.</p><h2>Why limits on early adoption could be bad</h2><p>Far-UVC emitters are currently prohibitively expensive for widespread adoption, however, <a href=\"https://drive.google.com/file/d/1QKusCnLzUs041nIuiq6nMxSIRcXaGiKq/view\">others</a> have reason to think the cost could come down over time as more R&amp;D is done on the technology. For this to happen there needs to be a market for the technology to create financial incentives for R&amp;D. Some of the current bottlenecks to creating this market are a <a href=\"https://docs.google.com/document/d/1gP14w3cmZ3f7lUTJKsK4IOb0_K6PYcpAKAdFSP2og5U/edit\">lack of safety and efficacy studies and a lack of standards</a>. UV-stability of plastics could be another hurdle to creating a market for far-UVC by reducing the rate of early adoption. Without this market to incentivise R&amp;D it seems unlikely that costs will come down and far-UVC will not be financially viable to defend against GCBRs.</p><h2>What have others said?</h2><ul><li>Convergent mentions the degradation of plastics in their <a href=\"https://docs.google.com/document/d/1gP14w3cmZ3f7lUTJKsK4IOb0_K6PYcpAKAdFSP2og5U/edit\">Far UVC Executive Summary</a> but the emphasis is on the release of volatile organic compounds (VOC) and their health effects (they suggest air filtration and careful design for high-power far-UVC systems will likely mitigate health concerns from VOCs)</li><li>The \u201c<a href=\"https://drive.google.com/file/d/1QKusCnLzUs041nIuiq6nMxSIRcXaGiKq/view\">Air Safety to Combat Global Catastrophic Biorisk</a>\u201d report from 1Day Sooner and Rethink Priorities mentions plastic degradation. They note that \u201cmuch of the issue could be avoided through careful materials choice\u201d and that \u201cGenerally, the rate of degradation may overall be negligible compared with the standard lifetime of consumer products\u201d (page 27).<ul><li>The former claim seems weak in the context of indoor environments as using only UV-stable plastics would require a total rework of how indoor products are manufactured and would likely increase costs significantly. Additionally, the amount of coordination across companies, industries, and countries required to make this happens makes this seem pretty intractable to me.</li><li>The latter claim does not provide a reference and the <a href=\"https://www.mdpi.com/2076-3417/13/7/4141\">bus study</a> suggests that lifetime changes are non-negligible.</li><li>They do reference a Boeing study that found no significant mechanical degradation in plastics from far-UVC exposure (the link to the study doesn\u2019t work though). The report notes that the exposure time was low for the Boeing study. The results figures (<a href=\"https://www.mdpi.com/2076-3417/13/7/4141\">figures 8 - 12</a>) in the bus study suggest that degradation increases with increasing exposure so more research here seems required.</li></ul></li></ul><h2>Conclusions</h2><p>Overall, I feel pretty certain that the UV stability of plastics is worth more attention than it is currently getting in the far-UVC publications in the EA space that I have read. I\u2019m uncertain about the relative magnitude of concern/effort that should be placed on the UV stability of plastics relative to other bottlenecks to far-UVC adoption such as <a href=\"https://docs.google.com/document/d/1gP14w3cmZ3f7lUTJKsK4IOb0_K6PYcpAKAdFSP2og5U/edit\">comprehensive safety testing, efficacy trials, developing clear standards, and technology development</a>.</p><p>I think there\u2019s a good chance that someone has already looked into this and found it to not be an issue and I am very open to input on this concern from the far-UVC community.</p><h3>Open questions</h3><p>Some questions that I would want to look into to refine my understanding of how concerned I should be about the UV stability of plastics (in no particular order):</p><ul><li>To what degree do consumers care about the aesthetic degradation of plastics in the indoor environment?<ul><li>What is the limit that consumers will tolerate?</li></ul></li><li>What level of degradation would be allowable under building codes?</li><li>What are the magnitudes of aesthetic and mechanical degradation under prolonged exposure to the intensity of radiation proposed for effective far-UVC?</li><li>What plastics in the indoor environment are/aren\u2019t UV-stable?<ul><li>For those that aren\u2019t UV-stable, which are most concerning and why?</li></ul></li><li>What insights do these papers add:<ul><li><a href=\"https://www.sciencedirect.com/science/article/pii/S2352492822005529\">Evaluation of the degradation of materials by exposure to germicide UV-C light through colorimetry, tensile strength and surface microstructure analyses</a></li><li><a href=\"https://www.worldscientific.com/doi/abs/10.1142/S1793984420500014\">Damage to Common Healthcare Polymer Surfaces from UV Exposure</a></li></ul></li></ul><p>To clarify: I don\u2019t expect that I will have the time/bandwidth to actually look into these anytime soon.</p><p><i>Acknowledgements: thanks to Jessica Wen and Dan Epstein for reviewing drafts of this post.</i></p><p><i>Disclaimers: the views and mistakes in this post are mine alone and don\u2019t necessarily reflect those of High Impact Engineers.</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2lvkk32stzi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2lvkk32stzi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>From the paper: \u201cAccording to Welch et al. [<a href=\"https://www.mdpi.com/2076-3417/13/7/4141#B34-applsci-13-04141\"><strong>34</strong></a>], it is sufficient to expose surfaces for 2.0 mJ/cm2 under far-UVC 222 nm irradiation to deactivate the COVID-19 virus; for other pathogens, such as influenza, it should be almost 4.0 mJ/cm2, and to deactivate most widespread biological pathogens, up to 8.0 mJ/cm2 exposure is necessary. It takes approximately 1 h for a regular city bus to travel from end to end, after which the bus driver takes a break for approximately 10\u201315 min. Assuming that during each such break a disinfection is performed with an exposure of approximately 10 mJ/cm2 and the duty cycle of the city bus is approximately 16 h a day, 7 days a week, the annual irradiation exposure would be approximately 47 J/cm2. In our case, 150 h of irradiation in a 222 nm far-UVC irradiation chamber was approximately equivalent to a radiation exposure of 290 J/cm2, which corresponds to approximately 6.2 years of city bus disinfection.\u201d</p></div></li></ol>", "user": {"username": "Sean Lawrence"}}, {"_id": "Hskm57dgHYWrkHDxk", "title": "The suffering scale of human diseases vs factory farming", "postedAt": "2023-09-01T19:34:48.155Z", "htmlBody": "<p>Global health and factory farming are two main cause areas in EA. However, as a person who supports animal rights, is the scale of factory farming problem MUCH more serious than global health? I'm very unsure on whether the scale is 10,100 or 1000 times bigger. This is one crucial problem to prioritize ending factory farming over global health.</p><p>The number of land farmed animals is around 30 billions, and there are about 0.4-0.8 billions of humans who are in severe diseases. Given the moral weight of animals is quite uncertain,so the life quality of farmed animals may not be as important as humans. But one argument is that the suffering of farmed animals is much more torturing.</p><p>(By intuition, I think comparing with the suffering of farmed animals, human diseases may only be a piece of cake. But I'm really unsure, I want some persuasive arguments. Also,I don't have expertise on the seriousity of farmed animals and severe human diseases.)</p><p>&nbsp;The main suffering of farmed animals:</p><p>1.Starving</p><p>Some humans diseases would make you starve also if seriously ill, but we are often treated with liquid nutrition, so we won't starve to death.</p><p>2.Small living space, lack of exercise(But for humans who are sickbed, isn't this similar suffering?)</p><p>3.Overweight and crippling(It seems serious, but I'm unsure how it is, how about the disabled humans?)</p><p>4.Chronic diseases(Also happening in humans)</p><p>5.Not getting anaesthetization</p><p>6.Not enough sleep(Human also gets insomnia)</p><p>7.Dirty and lots of infectious disease(Also happens in global diseases)</p><p>8.Fearing</p><p>9.Cruel slaughtering (This might be the most painful, but the time is relative short than the above)</p><p>Farmed animals live quite a bad life. BUT, how can we compare this suffering to a seriously ill person, such as the dying advanced cancer patients, or a person who is suffering from malaria, or depression? I think both are serious suffering, I don't know how to compare the seriousity of factory farming vs human diseases.</p>", "user": {"username": "jackchang110"}}, {"_id": "J7nmbqcWncPMZFhGC", "title": "Want to make a difference on policy and governance? Become an expert in something specific and boring", "postedAt": "2023-08-31T22:43:51.220Z", "htmlBody": "<p>I sometimes get a vibe that many people trying to ambitiously do good in the world (including EAs) are misguided about what doing successful policy/governance work looks like. An exaggerated caricature would be activities like: dreaming up novel UN structures, spending time in abstract game theory and \u2018strategy spirals<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz1vutaahvla\"><sup><a href=\"#fnz1vutaahvla\">[1]</a></sup></span>\u2019, and sweeping analysis of historical case studies.</p><p>Instead, people that want to make the world safer with policy/governance should become experts on very specific and boring topics. One of the most successful people I\u2019ve met in biosecurity got their start by getting really good at analyzing obscure government budgets.</p><p>Here are some crowdsourced example areas I would love to see more people become experts in:</p><ul><li>Legal liability - obviously relevant to biosecurity and AI safety, and I\u2019m especially interested in how liability law would handle spreading infohazards (e.g. if a bio lab publishes a virus sequence that is then used for bioterrorism, or if an LLM is used maliciously in a similar way).</li><li>Privacy / data protection laws - could be an important lever for regulating dangerous technologies.&nbsp;</li><li>Executive powers for regulation - what can and can't the executive actually do to get AI labs to adhere to voluntary security standards, or get DNA synthesis appropriately monitored?&nbsp;</li><li>Large, regularly reauthorized bills (e.g., NDAA, PAHPA, IAA) and ways in which they could be bolstered for biosecurity and AI safety (both in terms of content and process).</li><li>How companies validate customers, e.g., for export control or FSAP reasons (know-your-customer), and the statutes and technologies around this.</li><li>How are legal restrictions on possessing or creating certain materials justified/implemented e.g. Chemical Weapons Convention, narcotics, Toxic Substances Control Act?&nbsp;</li><li>The efficacy of tamper-proof and tamper-evident technology (e.g. in voting machines, anti-counterfeiting printers)</li><li>Biochemical supply chains - which countries make which reagents, and how are they affected by export controls and other trade policies?</li><li>Consumer protection laws and their application to emerging tech risks (e.g. how do product recalls work? Could they apply to benchtop DNA synthesizers or LLMs?)</li><li>Patent law - can companies patent dangerous technology in order to prevent others from developing or misusing it?</li><li>How do regulations on 3d-printed firearms work?</li><li>The specifics of congressional appropriations, federal funding, and procurement: what sorts of things does the government purchase, how does this relate to biotech or AI (software)? Related to this, becoming an expert on the Strategic National Stockpile and understanding the mechanisms of how a vendor managed inventory could work.</li></ul><p>A few caveats. First, I spent like 30 minutes writing this list (and crowdsourced heavily from others). Some of these topics are going to be dead ends. Still, I\u2019d be more excited about somebody pursuing one of these concrete, specific dead ends and getting real feedback from the world (and then pivoting<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsplobups5fc\"><sup><a href=\"#fnsplobups5fc\">[2]</a></sup></span>), rather than trying to do broad strategy work and risk ending up in a never-ending strategy spiral. Moreover, the most impactful topics are probably not on this list and will be discovered by somebody who got deep into the weeds of something obscure.</p><p>For those of you that are trying to do good with an EA mindset, this also means getting out of the EA bubble and spending lots of time with established experts<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefg55lp6w9ozp\"><sup><a href=\"#fng55lp6w9ozp\">[3]</a></sup></span>&nbsp;in these relevant fields. Every so often, I\u2019ll get the chance to collect biosecurity ideas and send them to interested people in DC. In order to be helpful, these ideas need to be super specific, e.g. this specific agency needs to task this other subagency to raise this obscure requirement to X. Giving broad input like \u2018let\u2019s have better disease monitoring\u2019 is not helpful. Experts capable of producing these specific ideas are much more impactful, and impact-oriented people should aspire to work with and eventually become those experts.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpq4r3vx232g\"><sup><a href=\"#fnpq4r3vx232g\">[4]</a></sup></span></p><hr><p><i>I appreciated feedback and&nbsp;ideas on the crowdsourced&nbsp;list from Tessa Alexanian, Chris Bakerlee, Anjali Gopal, Holden Karnofsky, Trevor Levin, James Wagstaff, and a number of others.</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnz1vutaahvla\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefz1vutaahvla\">^</a></strong></sup></span><div class=\"footnote-content\"><p>'Strategy Spiral' is the term I use to describe spending many hours doing \u2018strategy\u2019 with very little feedback from the real world, very little sense of what decision-makers would actually find helpful or action-relevant, and no real methodology to actually make progress or get clarity.&nbsp; The strategy simply goes in circles. &nbsp;Strategy&nbsp;is important so doing&nbsp;strategy can make you feel important, but I think people often underestimate&nbsp;the&nbsp;importance of getting your hands dirty directly, plus in the long run it will help you do better strategy.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsplobups5fc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsplobups5fc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>And then if you write up a document explaining why this was a dead end, you benefit everybody else trying to have an impact (or perhaps inspire somebody to perhaps see a different angle on the problem).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fng55lp6w9ozp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefg55lp6w9ozp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;One of the people reading this said \u2018<i>I feel like one thing I didn't understand until pretty recently is how much of (the most powerful version of) this kind of expertise basically requires being in a government office where you have to deal with an annoying bureaucratic process. This militates in favor of early-career EAs working in government instead of research roles\u2019</i></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpq4r3vx232g\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpq4r3vx232g\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Concretely, this looks like either getting an entry level job in government, or being at a think tank but working closely with somebody in government who actually wants your analysis, or drilling deep on a specific policy topic where there is a clear hypothesis for it being \u2018undervalued\u2019 by the policy marketplace.&nbsp; Doing independent research is <i>not</i> a good way of doing this.</p></div></li></ol>", "user": {"username": "ASB"}}, {"_id": "nYgTFeProv4vscfgt", "title": "New Study in Science Suggests a Severe Bottleneck in Human Population Size 930,000 Years Ago", "postedAt": "2023-08-31T22:19:43.826Z", "htmlBody": "<p>In my opinion, empirical evidence on historical events that lead to severe reductions in human population size is very important for those studying global catastrophic risks and potential extinction risks. A new <a href=\"https://www.science.org/doi/10.1126/science.abq7487\">study in Science</a> provides novel evidence that such an event occurred 930,000 years ago. To give a few quotes:</p><ul><li>\"Results showed that human ancestors went through a severe population bottleneck with about 1280 breeding individuals between around 930,000 and 813,000 years ago. The bottleneck lasted for about 117,000 years and brought human ancestors close to extinction. This bottleneck is congruent with a substantial chronological gap in the available African and Eurasian fossil record.\"</li><li>\"Results showed that our ancestors experienced a severe population bottleneck between about 930 and 813 kyr BP, most likely because of climatic changes. The average number of breeding individuals was only about 1280 during the bottleneck period. Our findings indicate that the severe bottleneck brought the ancestral human population close to extinction and completely reshaped present-day human genetic diversity.\"</li><li>\"The bottleneck was estimated to persist for 117 kyr, from 930 \u00b1 23.52 (SEM) (range, 854 to 1042) to 813 \u00b1 11.02 (SEM) (range, 772 to 864) kyr BP. The average effective population size (i.e., the number of breeding individuals) (<a href=\"https://www.science.org/doi/10.1126/science.abq7487#core-R26\"><i>26</i></a>) during the bottleneck period was determined to be 1280 \u00b1 131 (SEM) (range, 770 to 2030), which was only 1.3% of its ancestral size (98,130 \u00b1 8720; range, 58,600 to 135,000). To evaluate the impact of the bottleneck on current human genetic diversity, we analyzed the expected pairwise nucleotide diversity. Results showed that 65.85% of current human genetic diversity was lost because of the bottleneck.\"</li></ul><p>The <a href=\"https://www.nytimes.com/2023/08/31/science/human-survival-bottleneck.html\">NYTimes </a>has a nice article covering the study. The study uses a novel method to uncover these results (FitCoal), and as the article suggests, there is debate within the field over these results. I suspect this study will draw new studies to assess this hypothesis. In any case, I figure that these results are quite interesting/important for those interested in global catastrophic risks to be aware of.</p>", "user": {"username": "DannyBressler"}}, {"_id": "sXJkaQFFYodhEXNvr", "title": "Alignment & Capabilities: What's the difference?", "postedAt": "2023-08-31T22:13:37.418Z", "htmlBody": "<p>In the AI safety literature, AI alignment is often presented as conceptually distinct from capabilities. However, (1) the distinction seems somewhat fuzzy and (2) many techniques that are supposed to improve alignment also improve capabilities.&nbsp;</p><p>(1) The distinction is fuzzy because one common way of defining alignment is getting an AI system to do what the programmer or user intends. However, programmers intend for systems to be capable. eg we want chess systems to win at chess. So, a system that wins more is more intent aligned, and is also more capable.&nbsp;</p><p>(2) eg This <a href=\"https://arxiv.org/pdf/1805.00899.pdf\">Irving et al (2018)</a> paper by a team at Open AI proposes debate as a way to improve safety and alignment, where alignment is defined as aligning with human goals. However, the debate also improved the accuracy of image classification in the paper, and therefore also improved capabilities.&nbsp;</p><p>Similarly, Reinforcement learning with human feedback was initially presented as an alignment strategy, but my loose impression is that it also made significant capabilities improvements. There are many other examples in the literature of alignment strategies also improving capabilities.&nbsp;</p><p>**</p><p>This makes me wonder whether alignment is actually more neglected that capabilities work. AI companies want to make aligned systems because they are more useful.&nbsp;</p><p>How do people see the difference between alignment and capabilities?</p>", "user": null}, {"_id": "BiqK4NyELbdyF5SxG", "title": "Dr. Brinda Poojary on how to end animal testing", "postedAt": "2023-08-31T18:57:00.509Z", "htmlBody": "<p>We look at the current state of animal testing around the world, why testing is often unneeded and actually counterproductive to human interests, and what the alternatives to animal testing are. For that, we are joined by Dr. Brinda Poojary who has worked as a Research Consultant for India Animal Fund and Senior Specialist for Humane Society International India. She also holds a Ph.D. in Zoology. Brinda has also published a research paper showing how there is bias involved in scientific studies to use animal testing even when unnecessary.</p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=877GCoBa0m8&amp;t=305s\"><div><iframe src=\"https://www.youtube.com/embed/877GCoBa0m8\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p><br>&nbsp;</p>", "user": {"username": "Karthik Palakodeti"}}, {"_id": "ssjowq56xbCbYramm", "title": "Viral Load Matters: Evidence of leaky protection following COVID-19 vaccination and SARS-CoV-2 infection in an incarcerated population", "postedAt": "2023-08-31T17:52:19.525Z", "htmlBody": "<p>This paper just got published this month, and I thought it was a really good study to highlight and talk about the results.<br><br>The study: How well do vaccines protect against SARS-CoV-2 infection given different amounts of exposure? Authors looked in correctional facilities where we know a lot about how close people are to a documented exposure (cell mates vs within cell block vs no documented exposure)<br><br>The results: Vaccines (and prior infection and hybrid immunity) were very protective against infection if there was no documented exposure (HR: .36, .57, .24 for vaccination, prior infection, and hybrid) or on the same cell block (HR: .61, .69, .41 for vaccination, prior infection, and hybrid), but only lightly protective against infection if the index case was a cellmate (HR: .89, .96, .80 for vaccination, prior infection, and hybrid, and none were statistically significant)<br><br>HR, Hazard ratio, can be interpreted as your risk compared to the control group- so an HR 0f .36 for vaccinated individuals means that a vaccinated person has 1/3 the risk of an individual without vaccination or prior infection; an HR of .24 for hybrid immunity means 1/4 the risk of an individual without vaccination or prior infection; an HR of .96 for prior infection means a 4% reduction in risk when compared to individuals without vaccination or prior infection.<br><br>My takeaway: Viral load matters! If you have antibodies, you have a head start against the virus establishing a foothold in your cells. However, the antibody-virion interaction is all just a probability distribution of particles randomly bumping into each other. If you have a ton of antibodies (recently boosted or infected) the balance leans way over on the side of your immune system; as your antibodies wane, that balance starts moving more towards the virus. This study shows that the opposite is true, too: if you are exposed to a ton of virus, it can overwhelm your antibodies even though they would have protected you from a smaller viral load.&nbsp;<br><br>-if I\u2019m infected, and I have already exposed my housemates before I knew I was infected, I can still greatly reduce their chance of infection by masking and isolating</p><p>-less than perfect masks, such as cloth masks or surgical masks worn poorly, might not protect you from infection in and of themselves, but they could reduce the viral load enough to help tip the balance in favor of your immune system so that mask+antibodies protects you from infection where simply antibodies would not have been enough</p><p>We already know from many studies that viral load is important in predicting severe disease, and the underlying science was pointing to this idea, but it\u2019s always cool to get that basic science validated in a real-world setting<br><br>From a GCBR point of view, we can think of the old swiss cheese model of defense: we want overlapping layers, where the holes of one layer do not line up with the holes of another layer. In the beginning of a pandemic, we don\u2019t have any immunity from vaccines or infections, so we need our PPE to be as perfect as possible. As we acquire immunity, we can get the same protection from less stringent PPE, even if the virus stays exactly the same.</p>", "user": {"username": "James Montavon"}}, {"_id": "ph6wvA2EtQ7pG3yvG", "title": "[Linkpost] Michael Nielsen remarks on 'Oppenheimer'", "postedAt": "2023-08-31T15:41:40.170Z", "htmlBody": "<p>This is a linkpost to a recent blogpost from <a href=\"https://michaelnielsen.org/\">Michael Nielsen</a>, who has previously written on <a href=\"https://michaelnotebook.com/eanotes/\">EA</a> among many other topics. This blogpost is adapted from a talk Nielsen gave to an audience working on AI before a screening of <i>Oppenheimer</i>. I think the full post is worth a read, but I've pulled out some quotes I find especially interesting (bolding my own)</p><blockquote><p>I was at a party recently, and happened to meet a senior person at a well-known AI startup in the Bay Area. They volunteered that they thought \"humanity had about a 50% chance of extinction\" caused by artificial intelligence. I asked why they were working at an AI startup if they believed that to be true. They told me that while they thought it was true, \"<strong>in the meantime I get to have a nice house and car</strong>\".</p></blockquote><blockquote><p>[...] I often meet people who claim to sincerely believe (or at least seriously worry) that AI may cause significant damage to humanity. And yet they are also working on it, justifying it in ways that sometimes seem sincerely thought out, but which <strong>all-too-often seem self-serving or self-deceiving.</strong></p></blockquote><p>&nbsp;</p><blockquote><p>Part of what makes the Manhattan Project interesting is that we can chart the arcs of moral thinking of multiple participants [...] Here are four caricatures:</p><ul><li>Klaus Fuchs and Ted Hall were two Manhattan Project physicists who took it upon themselves to commit espionage, communicating the secret of the bomb to the Soviet Union. It's difficult to know for sure, but <strong>both seem to have been deeply morally engaged and trying to do the right thing, willing to risk their lives</strong>; they also made, I strongly believe, a terrible error of judgment. I take it as a warning that <strong>caring and courage and imagination are not enough</strong>; they can, in fact, lead to very bad outcomes.</li><li>Robert Wilson, the physicist who recruited Richard Feynman to the project. Wilson had thought deeply about Nazi Germany, and the capabilities of German physics and industry, and made a principled commitment to the project on that basis. He half-heartedly considered leaving when Germany surrendered, but opted to continue until the bombings in Japan. He later regretted that choice; immediately after the Trinity Test he was disconsolate, telling an exuberant Feynman: \"It's a terrible thing that we made\".</li><li>Oppenheimer, who I believe was motivated in part by a genuine fear of the Nazis, but also in part by personal ambition and a desire for \"success\". It's interesting to ponder his statements after the War: while he seems to have genuinely felt a strong need to work on the bomb in the face of the Nazi threat, his comments about continuing to work up to the bombing of Hiroshima and Nagasaki contain many strained self-exculpatory statements about how you have to work on it as a scientist, that the technical problem is too sweet.&nbsp;It smells, to me, of someone looking for self-justification.</li><li>Joseph Rotblat, the one physicist who actually left the project after it became clear the Nazis were not going to make an atomic bomb. He was threatened by the head of Los Alamos security, and falsely accused of having met with Soviet agents. In leaving he was turning his back on his most important professional peers at a crucial time in his career.<strong> Doing so must have required tremendous courage and moral imagination</strong>. Part of what makes the choice intriguing is that<strong> he himself didn't think it would make any difference to the success of the project. I know I personally find it tempting to think about such choices in abstract systems terms: \"I, individually, can't change systems outcomes by refusing to participate ['it's inevitable!'], therefore it's okay to participate\"</strong>. And yet while that view seems reasonable, Rotblat's example shows it is incorrect. His private moral thinking, which seemed of small import initially, set a chain of thought in motion that eventually led to Rotblat founding the Pugwash Conferences, a major forum for nuclear arms control, one that both Robert McNamara and Mikhail Gorbachev identified as helping reduce the threat of nuclear weapons. Rotblat ultimately received the Nobel Peace Prize.<strong> Moral choices sometimes matter not only for their immediate impact, but because they are seeds for downstream changes in behavior that cannot initially be anticipated.</strong></li></ul></blockquote>", "user": {"username": "22tom"}}, {"_id": "4edCygGHya4rGx6xa", "title": "Learning from our mistakes: how HLI plans to improve", "postedAt": "2023-09-01T11:11:46.916Z", "htmlBody": "<p>Hi folks, in this post we\u2019d like to describe our views as the Chair (Peter) and Director (Michael) of HLI in light of the recent conversations around HLI\u2019s work. The purpose of this post is to reflect on HLI\u2019s work and its role within the EA community in response to community member feedback, highlight what we\u2019re doing about it, and engage in further constructive dialogue on how HLI can improve moving forward.&nbsp;</p><p>HLI hasn\u2019t always got things right. Indeed, we think there have been some noteworthy errors (quick note: our goal here isn\u2019t to delve into details but to highlight broad lessons learnt, so this isn\u2019t an exhaustive list):</p><ul><li>Most importantly, we were overconfident and defensive in communication, particularly around our 2022 giving season post.&nbsp;<ul><li>We described our recommendation for StrongMinds using language that was too strong: \u201cWe\u2019re now in a position to confidently recommend StrongMinds as the most effective way we know of to help other people with your money\u201d. We agree with feedback that this level of confidence was and is not commensurate with the strength of the evidence and the depth of our analysis.&nbsp;</li><li>The post\u2019s original title was \u201cDon\u2019t give well, give WELLBYs\u201d. Though this was intended in a playful manner, it was tone-deaf, and we apologise.</li></ul></li><li>We made mistakes in our analysis.<ul><li>We made a data entry error. In our meta-analysis, we recorded that Kemp et al. (2009) found a positive effect, but in fact it was a negative effect. This correction reduced our estimated \u2018spillover effect\u2019 for psychotherapy (the effect that someone receiving an intervention had on other people) from 53% to 38%&nbsp; and therefore reduced the total cost-effectiveness estimate from 9.5x cash transfers to 7.5x.</li><li>We did not include standard diagnostic tests of publication bias. If we had done this, we would have decreased our confidence in the quality of the literature on psychotherapy that we were using.&nbsp;</li></ul></li><li>After receiving feedback about necessary corrections to our cost-effectiveness estimates for psychotherapy and StrongMinds, we failed to update our materials on our website in a timely manner.</li></ul><p>As a community, EA prides itself on its commitment to epistemic rigour, and we\u2019re both grateful and glad that folks will speak up to maintain high standards. We have heard these constructive critiques, and we are making changes in response.&nbsp;</p><p>We\u2019d like to give a short outline of what HLI is doing next and has done in order to improve its epistemic health and comms processes.</p><ol><li>We\u2019ve added an \u201c<a href=\"https://www.happierlivesinstitute.org/our-blunders/\"><u>Our Blunders</u></a>\u201d page on the HLI website, which lists the errors and missteps we mentioned above. The goal of this page is to be transparent about our mistakes, and to keep us accountable to making improvements.</li><li>We\u2019ve added the following text to the places in our website where we discuss StrongMinds:&nbsp;<ul><li><i>\u201cOur current estimation for StrongMinds is that a donation of $1,000 produces 62 WELLBYs (or 7.5 times GiveDirectly cash transfers). See our&nbsp;</i><a href=\"https://www.happierlivesinstitute.org/changelog/\"><i><u>changelog</u></i></a><i>.&nbsp;</i><br><i>However, we have been working on an update to our analysis since July 2023 and expect to be ready by the end of 2023. This will include using new data and improving our methods. We expect our cost-effectiveness estimate will decrease by about 25% or more \u2013 although this is a prediction we are very uncertain about as the analysis is yet to be done.&nbsp;</i><br><i>While we expect the cost-effectiveness of StrongMinds will decrease, we think it is unlikely that the cost-effectiveness will be lower than GiveDirectly. Donors may want to wait to make funding decisions until the updated report is finished.\u201d&nbsp;</i></li></ul></li><li>We have added more/higher quality controls to our work:<ul><li>Since the initial StrongMinds report, we\u2019ve added Samuel Dupret (researcher) and Dr Ryan Dwyer (senior researcher) to the&nbsp;<a href=\"https://www.happierlivesinstitute.org/about/meet-the-team/\"><u>team</u></a>, providing more quantitative eyes to double-check all key research inputs and reproduce all key research outputs.</li><li>We will be more careful about communicating our work, including greater emphasis on the uncertainties in, and the limitations of, our analyses.&nbsp;</li><li>In line with this, we\u2019ve developed our process for how to classify the quality of evidence and the depth of our work in a more principled and transparent manner. This will be posted on our website in Q4 2023.</li><li>We have just added a&nbsp;<a href=\"https://www.happierlivesinstitute.org/changelog/\"><u>changelog</u></a> to the site to keep a public record of how our CEAs have changed over time and why.</li><li>We have sought advice from leading experts to develop our methods for handling publication bias. We will use these methods in our updated evaluation of psychotherapy, scheduled for Q4 of 2023.</li><li>We have also further developed our general research methodology to ensure we follow best practices, such as following more closely to recommendations from the&nbsp;<a href=\"https://www.cochrane.org/about-us\"><u>Cochrane Collaboration</u></a> for conducting systematic reviews and meta-analyses. We will post an article outlining our research methodology in Q4 2023.</li></ul></li><li>Updating the StrongMinds analysis in Q4 2023 to include more rigorous methods, the most recent studies, and to address various points raised by feedback from the EA community, including to:<ul><li>Conduct a full systematic review of the evidence, with an improved process for determining the weight we place on different sources of evidence.</li><li>Update the estimated spillover benefits for household members.</li><li>Update the cost estimates for StrongMinds.</li><li>Apply more appropriate checks and corrections for publication bias.</li><li>Conduct more robustness tests to determine how the effects change under various reasonable analytic approaches.</li><li>We have addressed the data entry error for spillovers and have updated our analysis with the correct value.</li><li>We have updated our website to reflect our corrected estimates, and we now note which analyses are being updated.</li></ul></li><li>We are developing new content for our website that will summarise our charity evaluations to date. We produce long technical reports \u2013 which we think we need to continue to do&nbsp; \u2013 but we realise that if we&nbsp;<i>only&nbsp;</i>produce these, it can be challenging for others to understand and review our work. We hope these summaries will make it easier for others to understand our evaluations.&nbsp;</li><li>We\u2019ve added an experienced Comms Manager, Lara Watson, to the&nbsp;<a href=\"https://www.happierlivesinstitute.org/about/meet-the-team/\"><u>team</u></a>. Lara will be helping us improve both our community and academic reporting. We are striving to take a tone in our communications that is more cautious and less adversarial, and Lara will actively double-check for this.&nbsp;</li><li>Since I (Peter) have now stepped down as CEO of Mind Ease and UpLift and handed over the reins, I\u2019ll be resigning from the board to join HLI as a Managing Director in mid-September, where I\u2019ll be providing strategy, comms, and management support to the org. Michael will become Research Director, freeing him up to spend more time on research; moving forward, HLI will have two Co-Directors.</li></ol><p>Again, we are grateful to have received feedback from the EA community, and we are looking forward to doing good better. If you\u2019d like to follow HLI\u2019s progress, please sign up for the newsletter in the footer of our site&nbsp;<a href=\"https://www.happierlivesinstitute.org/\"><u>here</u></a>, and we\u2019ll also be posting updates here on the forums as usual.&nbsp;</p><p>As part of my (Peter\u2019s) new role, I\u2019ll be reaching out to various stakeholders and collecting further feedback on HLI work, what we should do, and what we should do better. If there are any suggestions or concerns you\u2019d like to send to me directly, I\u2019m&nbsp;<a href=\"mailto:peter@happierlivesinstitute.org\"><u>peter@happierlivesinstitute.org</u></a> \u2013 I\u2019d love to hear from you.</p><p>Peter Brietbart (Outgoing Chair of HLI Board of Trustees, Incoming Managing Director)</p><p>Michael Plant (Outgoing Director, Incoming Research Director)</p><p><br>&nbsp;</p>", "user": {"username": "PeterBrietbart"}}, {"_id": "X2w4uqjDuGPJFaHPp", "title": "Should I patent my cultivated meat method?", "postedAt": "2023-08-31T11:05:47.661Z", "htmlBody": "<p>I am a third-year PhD candidate working on cultivated meat. In my research, I have developed a new and original approach to making cultivated meat that has the potential of being highly scalable and help reach economic parity, more than most other known methods that companies are working on.&nbsp;</p><p>My supervisor has left it up to me to decide whether to patent this method or not. I have half-based beliefs that patenting would be counter-productive in helping cultivated meat reach the market sooner, but I acknowledge that I am not knowledgeable enough in IP law/structures to really know.&nbsp;</p><p>If I do decide to patent the method, the patent will mostly belong to and be written by the Tech Transfer office at my university, which is of course for profit. I am not sure I trust them to consider my wish to make a fairly non-limiting patent. The usual course of this kind of thing is then for the tech transfer office to open a private startup to further develop the method. In any case, I intend to publish the method in a scientific journal. If we patent it, then it would be published after filing.</p><p>What would be best for the field as a whole - a scenario where scientists patent their findings and then publish them, or where they just publish everything open source? Given that most scientists do patent and keep everything secret within companies, what would be best to do in my position?</p>", "user": {"username": "Michelle Hauser"}}, {"_id": "Bg6qxLGhsn7pQzHGX", "title": "Progress report on CEA\u2019s search for a new CEO", "postedAt": "2023-08-31T08:55:13.854Z", "htmlBody": "<p>I<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefouk93yb4g6i\"><sup><a href=\"#fnouk93yb4g6i\">[1]</a></sup></span>&nbsp;wanted to give an update on the Centre for Effective Altruism (CEA)\u2019s search for a new CEO.</p><p>We (<a href=\"https://www.openphilanthropy.org/about/team/claire-zabel/\"><u>Claire Zabel</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/users/maxdalton\"><u>Max Dalton</u></a>, and&nbsp;<a href=\"https://forum.effectivealtruism.org/users/michelle_hutchinson\"><u>Michelle Hutchinson</u></a>) were appointed by the Effective Ventures boards to lead this search and make a recommendation to the boards. The committee is advised by&nbsp;<a href=\"https://www.openphilanthropy.org/about/team/james-snowden/\"><u>James Snowden</u></a>,&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/team/caitlin-elizondo\"><u>Caitlin Elizondo</u></a>, and one experienced executive working outside EA.&nbsp;</p><p>We previously announced the search and asked for community input in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/GcvEdYJADH3vMqk3F/suggest-candidates-for-cea-s-next-executive-director\"><u>this post</u></a><u>.</u></p><p>Note, we set out searching for an Executive Director, and during the process have changed the role title to CEO because it was more legible to candidates not familiar with CEA or EV. The role scope remains unchanged.</p><p>In summary, we received over 400 nominations, reached out to over 150 people, spoke to about 60, and received over 25 applications. We\u2019re still considering around 15 candidates, and are currently more deeply assessing &lt;5 candidates who seem especially promising.</p><h3>Process</h3><ul><li>Over 400 people were nominated for the role<ul><li>We invited recommendations from CEA staff, stakeholders, and the EA community&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/GcvEdYJADH3vMqk3F/suggest-candidates-for-cea-s-next-executive-director\"><u>via the Forum</u></a>.</li><li>We selected candidates for outreach according to their profile and experience, the strength of recommendation we received, and (in some cases) our pre-existing knowledge about them.<ul><li>We decided this approach (rather than reaching out to all nominees or having an open call for applications) because<ul><li>Many nominations were fairly speculative (for instance one person nominated over 30 people, many of whom did not have management experience: this was helpful because it generated some useful names we would otherwise not have known about, but it probably would not have been a good use of our/candidates\u2019 time to reach out to all of these people).</li><li>More generally, we wanted to be careful about our time and applicants\u2019 time.</li><li>We thought (based on advice from professional recruiters) that a narrower outreach process would be more likely to be attractive to our most promising candidates (who matter disproportionately).</li></ul></li><li>Overall our sense from talking to people with experience of this is that this selective, non-public process is fairly standard practice in executive recruitment. I think that the main reason for this is that in executive recruitment your top candidates (who are the most important ones) have a very high opportunity cost for their time, and personalized non-public outreach is a much stronger signal that it will be worth their time engaging in the process.<ul><li>My guess is that some of our top candidates would not have proactively submitted an application, but did engage when we reached out to them.</li></ul></li></ul></li><li>We had a lower bar for reaching out to candidates we were less familiar with (because a small chance that they would be our top candidate justified the effort).</li></ul></li><li>In the end, we reached out to over 150 potential candidates<ul><li>We shared a document containing information on CEA and the role, and&nbsp; invited them to meet a member of the search committee (typically Max).<ul><li>We sometimes asked people to consider booking a call even if they weren\u2019t interested in the role, because we wanted to get their advice on our hiring process.</li></ul></li><li>In many cases, we tried to personalize outreach messages somewhat and use our network:<ul><li>When we knew the candidates, this was easy</li><li>Sometimes we asked for introductions from mutual connections</li><li>However there were some cases where we didn\u2019t personalize (e.g. if we couldn\u2019t find a good mutual connection).</li></ul></li><li>To give a sense of the range of candidates we reached out to, according to our very rough assessment they had:<ul><li>EA context<ul><li>~50% high (e.g. they\u2019ve been working at an EA org or otherwise have lots of signs of engagement with EA ideas).</li><li>~40% medium (e.g. they attended an EAG once or did 80k coaching, but nothing else)</li><li>~10% low (very few signs of EA engagement)</li></ul></li><li>Management experience<ul><li>~15% leadership of a large organization (roughly, an organization equal to or larger than CEA\u2019s current size)</li><li>~30% leadership of a small org or comparable</li><li>~35% senior management experience</li><li>~15% some (non-senior) management experience</li><li>&lt;5% little to no obvious management experience (but some other reason why they seemed promising)</li></ul></li></ul></li><li>We have so far heard back from more than 80% of people reached out to.</li><li>We deliberately chose to reach out to people who we thought would probably not be interested in the role, but might be. So we were not surprised that a majority of the people we reached out to were not interested. We think that this was the right decision: some of our top candidates were people who we thought would probably not be interested (but who actually were).</li></ul></li><li>We (mostly Max) had calls with many of these people: probably significantly more than 50 people in total.<ul><li>Very roughly 60% of these calls were with people who were seriously considering the role, in which case the call focused on getting to know each other a bit better, and then answering any questions or addressing any concerns that they had about the role or the process.</li><li>The remainder were with people who were not considering the role, but who wanted to give input into the process.<ul><li>Overall, we found these fairly useful: we got a lot of tips on outreach and assessment from people with a lot of executive hiring experience, and we also got some useful feedback from community members.</li></ul></li></ul></li><li>Over 25 people ended up applying.<ul><li>Because we had been selective in who we reached out to, the applicant pool was fairly strong in terms of prior management experience.</li><li>We tailored assessment around our uncertainties for applicants (in order to make good use of their and our time).</li><li>Around 20 of these candidates ended up doing a task where they began to develop a strategic vision for CEA (based on additional context that we shared with them).<ul><li>We were aware that different candidates would have different levels of context, and that candidates\u2019 thinking would likely change as they learned more, so we paid less attention to whether we agreed with the specifics of the plan, and more to the reasoning and communication skills displayed in these documents.</li><li>We shared with them that we were open to a fairly wide range of potential visions for CEA, and gave some guidance on the types of visions we were most open to, and what we were looking for from them. Some candidates proposed fairly radical departures, but around half focused on ideas closer to the status quo.</li></ul></li><li>We have so far gathered reference calls for around 15 of these candidates.<ul><li>This was based on advice we had from people with a lot of recruitment experience that reference calls would be a way to get a fair amount of information quickly. These people also gave tips (e.g. for our script) that helped us to make reference calls somewhat more informative.<ul><li>At their best, reference calls can help you to very quickly understand a candidate\u2019s performance in similar past roles. They also help identify uncertainties or concerns that can be evaluated during the process.</li><li>However, we are also aware that reference calls can be noisy: we tried to do multiple calls where possible, and they were only a part of our evaluation.</li></ul></li><li>Overall I (Max) found reference calls really informative, and I\u2019m glad that we did them earlier in the process (rather than at the end, which is more common).</li></ul></li><li>A smaller number of candidates did a management trial task, or a structured interview.</li></ul></li><li>We\u2019re now engaging more deeply with &lt;5 candidates, doing more in-depth reference checks, and giving them time to talk with stakeholders and develop their visions for CEA. This process is tailored around our uncertainties about these candidates, and these candidates\u2019 uncertainties about the CEO role.<ul><li>We've decided to prioritise clarifying our uncertainties about these candidates in the coming couple of weeks, in part because we think they are most likely to be the right fit and in part because we have limited capacity to engage intensively with more than a handful candidates simultaneously. However, there are around 10 additional candidates who we have not rejected and whose applications are effectively paused<ul><li>In our efforts to eliminate enough candidates to achieve a manageable shortlist of top candidates, we didn\u2019t feel confident ruling these people out, so we've offered them the opportunity to remain under consideration (so far, all have opted to remain a candidate-on-pause)</li></ul></li><li>To protect candidate privacy and to avoid locking in our options too much, we\u2019re not currently saying too much about these top candidates or their visions.</li><li>We\u2019re still fairly unsure about how long it\u2019ll take before we can announce an appointment: if things go well, we hope to make a recommendation to the board by early October, but it might take time for the board to consider their decision and negotiate with the candidate, etc. There might also be some delay between someone agreeing to join and them starting or the decision being announced.</li></ul></li></ul><h3>Commentary/reflections</h3><ol><li>Overall I think that I learned a lot from feedback from people with more experience of executive recruitment. (Though the feedback was sometimes contradictory, and I\u2019m not sure if I acted on the best advice throughout.)</li><li>Compared to other projects, I\u2019ve been surprised at how frequently my previous way of thinking about things broke down, and I needed to re-conceptualize what I was doing.<ol><li>Examples of things I needed to re-conceptualize:<ol><li>How much to treat this as a standard recruitment funnel versus a highly-customized search for our top candidates.</li><li>How much to be \u201cruling candidates out\u201d, versus \u201cruling in\u201d the few candidates we were most excited about.</li></ol></li><li>This generally materialized as feeling stuck or unable to make progress within the current paradigm (e.g. feeling like I needed to reject candidates but not being sure who to reject), and then talking to advisors or thinking things through for a while before I found a new way of thinking about things (e.g. \u201crule people in, not out\u201d) that allowed me/us to make progress.</li><li>I\u2019m not sure how avoidable this was, but I expect that if I had done this type of executive search many times in the past I would have found this easier to navigate.</li><li>Sometimes we switched between these different paradigms, and I worry that this sometimes resulted in a slightly disjointed process, whereas it would have been better to more consistently pick one approach (e.g. more totally pick an approach where we just reached out to about 3 people from the start and then shaped assessment around them). (But I\u2019m not sure here \u2013 I don\u2019t feel confident in any of the more decisive approaches, and maybe we had a decent balance.)</li><li>Overall I felt more frequently confused working on this project than I have in most prior pieces of work I\u2019ve done. It felt fairly different from running a standard recruitment round.</li></ol></li><li>Things I wish I\u2019d done differently:<ol><li>We asked several candidates to do a particular trial task early in the process, but this task didn\u2019t give us as much information as I\u2019d hoped, partly because our candidates tended to have a lot more experience than the people that we\u2019d beta-tested the task with. In the future, I should make sure that trial task beta testers are more similar to the actual applicant pool.</li><li>Partly because of getting \u201cconceptually stuck\u201d in some ways, I think that we began outreach to candidates about 2 weeks later than might have been realistically possible (and this matters because a 2 week delay means, in expectation, 2 weeks longer before finding a new CEO for CEA, which I think is important). I think that it would have helped if I\u2019d asked advisors for help here more quickly.</li><li>In a couple of cases, I sent emails or requests that were too standardized to some of our top candidates. I think that this made them feel less valued, and more worried that I\u2019d waste their time during the process. I updated somewhat in the direction of personalization and optimizing for the experience of our top candidates based on this feedback.</li></ol></li><li>Things I\u2019m glad we did:<ol><li>I think that personalized outreach helped us reach people we otherwise wouldn\u2019t have reached.</li><li>As mentioned above, I got a lot out of doing reference checks early, and I think our approach to reference checks is getting us much more useful information than the default approach.</li><li>I think the process has generally been well organized and project managed.</li><li>I think that we were fairly transparent with potential candidates about what the role looked like, including important risks and downsides. For instance, we shared a lot of this in the role description, and I shared more on calls. I think that this allowed people to rule themselves out more quickly which saved time, and it also built trust with potential candidates.</li><li>I think it\u2019s good that we have been open to big changes in vision: some of our top candidates want this, and I think this might help us to recruit a much better leader for CEA (and possibly switch to a better strategy).</li></ol></li><li>One thing that has been generally challenging has been how to assess candidates who have very different profiles (in terms of prior management experience, or prior engagement with EA). I think that we\u2019ve been appropriately cognisant of this, and I don\u2019t think there\u2019s anything particular that I\u2019d change here, but I\u2019m also not confident that we handled this well.</li></ol><p>You can share anonymous feedback about our process&nbsp;<a href=\"https://docs.google.com/forms/d/13VwcOs6bbga84uNn0_EnxckaEcCO_R9h1msF7x39SIg/edit\"><u>here</u></a>.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnouk93yb4g6i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefouk93yb4g6i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I drafted this post with some help from Oscar Howie. Other search committee members commented and approved of my posting, but didn\u2019t author the post. Thanks to Oscar Howie, Lizka Vaintrob, Claire Zabel, Michelle Hutchinson, and Amy Labenz for feedback.</p></div></li></ol>", "user": {"username": "Maxdalton"}}, {"_id": "JaACyD72oacQB9WDn", "title": "More than Earth Warriors: The Diverse Roles of Geoscientists in Effective Altruism", "postedAt": "2023-08-31T06:30:19.201Z", "htmlBody": "<p>A global community of and for Effective Geoscientists</p><p>&nbsp;</p><h3>tl;dr</h3><p>&nbsp;</p><ul><li>We, a small group of geoscientists want to gather a supportive community for knowledge sharing, collaboration, and placing geoscientists EAs in high impact roles.</li><li>The use of geospatial data and spatial analysis is underrated. Four out of eight top charities from GiveWell have used geospatial data as part of their research. Particularly, we can have a significant impact in catastrophe resilience.</li><li>We would like to advocate for more researchers and effective charities to consider including a geospatial dimension when conducting their studies.</li><li>Currently, we will gather on channel&nbsp;<a href=\"https://eavirtualmeetupgroup.slack.com/archives/C05PQCZ2B8T\"><u>role-geoscientists on EA Anywhere slack</u></a>. Please join us if you are interested.</li><li>We listed a few distilled topics that geoscientists EA might be interested in.</li></ul><p>&nbsp;</p><h3>Acknowledgement<br>&nbsp;</h3><p>The completion of this post would not have been possible without the extensive insight, advice, and knowledge shared by the following individuals: Prof. David Denkenberger, Ewelina Hornicka, Petya Kangalova, Leon Mayer, Dr. Kajetan Chrapkiewicz. Any mistakes or oversights in this post are solely my responsibility.</p><p>&nbsp;</p><h2>Context and reason for establishment</h2><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JaACyD72oacQB9WDn/epyi5r4ws0n8v4fouktx\"></p><p><i>Figure 1. Both expected impact and personal fit in our career choice reflect a </i><a href=\"https://forum.effectivealtruism.org/posts/to34jb5LuWo4fM9gC/taking-prioritisation-within-ea-seriously\"><i>power law</i></a><i>.</i></p><p>&nbsp;</p><p>We want to gather an online community to bring together geoscientists of diverse disciplines. There are two motivations for starting this community. Firstly, we believe that&nbsp; geoscientists and geospatial data can offer value in multiple EA cause areas and intercause research. Secondly, we found that services of current EA infrastructure (e.g. High Impact Professional) did not adequately address the need for our niche speciality and&nbsp;<a href=\"https://en.wikipedia.org/wiki/Comparative_advantage\"><u>comparative advantage</u></a> of geoscientists. We want to inform the EA world that we geoscientists exist and our skills and domain expertise can contribute to high impact cause areas. Furthermore, we would like to advocate for more researchers to consider including a geospatial dimension when conducting their studies. Therefore, we would like to bring together geoscientists, current and future, that work in environmental anthropology, epidemiology, remote sensing, geophysics and more. We want to use this community to highlight existing geospatial research happening in the various cause areas, to support each other in interdisciplinary collaboration and place each other in high impact roles.</p><p>&nbsp;</p><h2>What is geospatial data?</h2><p>&nbsp;</p><p>We use geospatial data everyday, to track our movement, plan our commute. Geospatial data is any information with an addition of locality attached to it, this is usually represented by Longitude and Latitude. Geospatial data can come from many sources, from the satellite images and radar backscatter to submarine bathymetric surveys.</p><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JaACyD72oacQB9WDn/ocrqzcvnefphpqc4drnn\"></p><p><i>Figure 2. Alexander von Humboldt\u2019s \u201cA Portrait of Nature\u201d in his famous book Der Kosmos (1849).</i></p><p>&nbsp;</p><p>The development of geospatial science and geospatial data emerged out of necessity. Cartography, the making of maps is the central activity of collecting and distilling geographical knowledge. Initially we made maps for navigation and exploration, then the map became a tool for managing communities, resources, and people, often involuntarily. The map room eventually became the centre of European colonialism in the Age of Discovery (Cresswell T., 2013), and the map room has remained a mainstay of today's military units\u2019 control and command. Nowadays, geospatial data are used broadly by ecologists and palaeontologists, by sailors and urban planners, by militaries and humanitarian workers alike. While people talk about the temporal component often, the spatial component often goes unnoticed. This post will attempt to demonstrate that the most interesting macro-level and intercause studies will encompass both the spatial and temporal dimensions and of course, beautiful maps.</p><p>&nbsp;</p><h2>Review of geospatial studies in near-term cause areas</h2><p>&nbsp;</p><p>Four (GiveDirectly, Malaria Consortium, AMF, Unlimited Health (formerly SCI)) out of eight top charities which previously received funding from GiveWell have used geospatial data as part of their research. The following section will give a brief overview per cause areas.</p><p>&nbsp;</p><h3>Epidemiology, spatial research in vector-borne tropical diseases</h3><p>&nbsp;</p><p>Research in vector-borne tropical diseases such as malaria, trachoma and schistosomiasis have used geospatial data to model studies in prevention, distribution, mitigation, and intervention. Studies have used geo-referenced community and clinic reported data based prevalence to perform prediction and risk-area classification (Cox J., 2012, Cox et al., 2014). More novel models have combined both community data and geophysical characteristics such as proximity to rice farm and land surface temperature to inform intervention measures (Clements et al., 2006, Clements et al., 2008, Clements et al., 2010, Nyandwi et al., 2020).</p><p>&nbsp;</p><p>Examples of relevant research:</p><ul><li>Report on work towards a village-based malaria stratification system for Cambodia (Cox J., 2012)</li><li>Evaluation of community-based systems for the surveillance of day three-positive Plasmodium falciparum cases in Western Cambodia (Cox et al., 2014)</li><li>Bayesian spatial analysis and disease mapping: tools to enhance planning and implementation of a schistosomiasis control programme in Tanzania (Clements et al., 2006)</li><li>Mapping the Probability of Schistosomiasis and Associated Uncertainty, West Africa (Clements et al., 2008)</li><li>Targeting Trachoma Control through Risk Mapping: The Example of Southern Sudan (Clements et al., 2010)</li><li>Modeling schistosomiasis spatial risk dynamics over time in Rwanda using zero-inflated Poisson regression (Nyandwi et al., 2020)</li></ul><p>&nbsp;</p><h3>Epidemiology, spatial research in respiratory diseases, biosecurity and pandemic preparedness</h3><p>&nbsp;</p><p>Perhaps the most popular or at least the most recognised geospatial application in epidemiology was the near real time COVID dashboard (<i>Figure 3.</i>) from the John Hopkins University Coronavirus Resource Center (Dong et al., 2020). The success and wide adoption of the dashboard tool was the first of its kind to encourage verification of pandemic data across relevant international public health institutions. The efforts have enabled better synthesis between the policy-makers and researchers for pandemic response (Gardner et al., 2020).&nbsp;</p><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JaACyD72oacQB9WDn/infwnvsjbx8qzrnmlkec\"></p><p><i>Figure 3. Famous COVID Dashboard by John Hopkins University Coronavirus Resource Center (Dong et al., 2020).</i></p><p>&nbsp;</p><p>Near real time epidemiological spatial data for COVID-19 was a public engagement success. The publicity and availability of the dashboard have encouraged citizen based pandemic preventative participation. This mobilised citizen science on a community level in anticipation of a local breakout despite asymmetry in local government response (Badr et al., 2020).</p><p>&nbsp;</p><p>Other neglected pandemic preparedness work lies in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/azezpsWKkcPJd2Hfy/why-we-should-fear-any-bioengineered-fungus-and-give-fungi\"><u>pathogenic fungi</u></a> against humans and agriculture. Particularly bioengineered fungi against agriculture are less studied. There is a potential lower barrier of entry for bio- mycoherbicides to be weaponised and deployed. Spatial modelling of spore spread combined with knowledge of spore morphology, microscopy, metagenomics could be valuable, especially as there\u2019s little work done on this.</p><p>&nbsp;</p><h3>Spatial research in direct intervention for poverty alleviation</h3><p>&nbsp;</p><p>Advancements in higher resolution satellite images, open source geospatial data and relevant geostatistical techniques have proven their use in reconciling locations with data poverty. By interpolating from geospatial proxies, we could gain an understanding of local conditions without the reliance on inadequate or incomplete census statistics (Kuffer et al., 2016, Jean et al., 2016, Quinn et al., 2018, Chi et al., 2021). Studies from Watmough et al. (2019),&nbsp; Berchoux &amp; Hutton (2019) have shown that poverty prediction and variation in rural household wealth can be performed at local level with spatial information. GiveDirectly have previously conducted several such research to inform their Unconditional Cash Transfer program (e.g. Abelson et al., 2014, Varshney et al., 2015., Kirkpatrick K., 2023., Shun &amp; Lummis, 2023).</p><p>&nbsp;</p><p>Examples of relevant research:</p><ul><li>Slums from Space\u201415 Years of Slum Mapping Using Remote Sensing (Kuffer et al., 2016)</li><li>Combining satellite imagery and machine learning to predict poverty (Jean et al., 2016)</li><li>Humanitarian applications of machine learning with remote-sensing data: review and case study in refugee settlement mapping (Quinn et al., 2018)</li><li>Microestimates of wealth for all low- and middle-income countries (Chi et al., 2021)</li><li>Socioecologically informed use of remote sensing data to predict rural household poverty (Watmough et al., 2019)</li><li>Spatial associations between household and community livelihood capitals in rural territories: An example from the Mahanadi Delta, India (Berchoux &amp; Hutton, 2019)</li></ul><p>&nbsp;</p><p>GiveDirectly research:</p><ul><li>Targeting direct cash transfers to the extremely poor (Abelson et al., 2014)</li><li>Targeting Villages for Rural Development Using Satellite Image Analysis (Varshney et al., 2015)</li><li>Using Algorithms to Deliver Disaster Aid (Kirkpatrick K., 2023)</li><li>Cash to flood survivors: 4 things we got right &amp; wrong (Shun &amp; Lummis, 2023)</li></ul><p>&nbsp;</p><p><strong>Climate Change, climate engineering, and frozen pathogens</strong></p><p>&nbsp;</p><p>For many geoscientists in EA, the less neglected cause area of Climate Change is the most apparent choice due to immediate relevance. However, despite considerable investment and regulations, climate change interventions have yet to yield efficient results. The inclusion of climate engineering solutions in the&nbsp;<a href=\"https://www.ipcc.ch/reports/?rp=ar6\"><u>IPCC (2022) AR6</u></a> marked a significant attitude shift compared to previous assessment reports. The latest edition outlined the inclusion of climate engineering solutions such as Carbon Dioxide Removal (CDR), Solar Radiation Management (SRM), and discussions from dedicated working groups. However, CDR solutions are still favoured due to the lack of understanding of SRM.</p><p>&nbsp;</p><p>CDR interventions, beyond afforestation, encompass carbon sequestration into biochar, Direct Carbon Capture, and Ocean Fertilisation. While&nbsp;<a href=\"https://www.biochar.ac.uk/what_is_biochar.php\"><u>biochar</u></a> has been widely researched, oceanic fertilisation and other SRMs remain controversial. There has been limited study on active cooling through controlled SO<sub>2</sub> injection into the stratosphere and its potential safety for global agriculture (Xia et al., 2014 &amp; Procter et al., 2018).</p><p>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Climate engineering</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\" colspan=\"2\"><p>Techniques</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Carbon Dioxide Removal</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\" colspan=\"2\"><p>Biochar</p><p>A potentially closed-loop highly stable carbon which can be manufactured from any dry waste organic materials.</p><p><br>&nbsp;</p><p>Ocean Fertilisation</p><p>Untested geoengineering technique designed to increase the uptake of CO<sub>2</sub> from the air by phytoplankton, microscopic plants that reside at or near the surface of the ocean.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Solar Radiation Management</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\" colspan=\"2\"><p>Stratospheric Aerosol Intervention</p><p>Injection of reflective aerosol particles directly into the stratosphere or a gas which then converts to aerosols that reflect sunlight.</p><p><br>&nbsp;</p><p>Marine Cloud Brightening</p><p>Spraying sea salt or other particles in marine clouds, making them more reflective.</p><p><br>&nbsp;</p><p>Ground-Based Albedo Modification</p><p>Whitening roofs, changes in land use management (e.g., no-till farming, bioengineering to make crop leaves more reflective), desert albedo enhancement, covering glaciers with reflective sheeting.</p><p><br>&nbsp;</p><p>Ocean Albedo Change</p><p>Increase surface albedo of the ocean (e.g., by creating microbubbles or placing reflective foam on the surface.</p><p><br>&nbsp;</p><p>Cirrus Cloud Thinning</p><p>Seeding to promote nucleation of cirrus clouds, reducing optical thickness and cloud lifetime to allow more outgoing longwave radiation to escape to space.</p></td></tr></tbody></table></figure><p><i>Table 1. Active and controversial climate engineering techniques (IPCC., 2022).</i></p><p>&nbsp;</p><p>The&nbsp;<a href=\"https://www.ipcc.ch/srocc/\"><u>SROCC contribution</u></a> to the IPCC AR6 report has reported that several pathogens have already emerged as a result of the thawing of the permafrost, resulting in an increase of pathogens carried by parasitic&nbsp;<a href=\"https://www.scientificamerican.com/article/46-000-year-old-worm-possibly-revived-from-siberian-permafrost/\"><u>nematodes</u></a> in the Canadian Arctic islands and Fennoscandia regions. The bacteria&nbsp;<i>Erysipelothrix Rhusiopathiae</i> carried by the parasite has been attributed to the &gt;50% decrease in population of Canadian Arctic Muskoxen (IPCC SROCC., 2019). Intercause risk areas associated with climate change and pandemic preparedness from permafrost thaw might be a low-hanging fruit since we can already know the areas experiencing or at higher risk of sustained temperature rise (Gruber S., 2012 &amp; Zhao et al., 2022). This should allow us to start building a repository of vaccines against pathogens and vectors from ground sampling.</p><p>&nbsp;</p><h3>EA movement growth in geoethics and involvement in policy</h3><p>&nbsp;</p><p>It has been motivating to see the recent efforts in building Effective Altruism in diverse communities (e.g.&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/2sgSwwTe4mG4uP36e/unveiling-the-longtermism-framework-in-islam-urging-muslims\"><u>Longtermism for Muslims</u></a>). We should consider bridging and bringing EA ideas, especially cause prioritisation and animal advocacy into the discussion of geoethics. This can be done through EA\u2019s participation in organisations such as the&nbsp;<a href=\"https://www.geoethics.org/definition\"><u>IAPG</u></a>. This is perhaps most applicable to understanding and minimising the impact of resource exploration and extraction on the ecosystem and humans.&nbsp;</p><p>&nbsp;</p><p>On the topic of drastic climate change mitigation measures, we might also want to question when will controversial climate engineering techniques be justified? Will the ethical benefits outweigh the potential permanent changes? How would we compensate for the unforeseen changes, loss and gains (Reynolds J., 2019 &amp; Pamplany et al., 2020)? For instance, societal scepticism and lobbying have caused us to miss the opportunity in economically scaling nuclear energy, one of the&nbsp;<a href=\"https://ourworldindata.org/safest-sources-of-energy\"><u>cleanest and safest forms of energy</u></a>. This could have helped us prevent air pollution, reduce the related decrease in Quality-Adjusted Life Years and deaths, and lessen the impact of global warming (OWID., 2022). We might not want to repeat the same mistake with climate engineering solutions. I believe that with careful and methodical considerations, these questions could be answered.</p><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JaACyD72oacQB9WDn/bplcszsafzeljol9bgsp\"></p><p><i>Figure 4. Is there an inflection point where it is actually more ethical for us to take the climate engineering approach (Reynolds J., 2019)?</i></p><p>&nbsp;</p><h2>The one and only geophysical X-risk</h2><p>&nbsp;</p><h3>Supervolcano and food security</h3><p>&nbsp;</p><p>While supervolcano eruptions are less probable than many anthropogenic X-risks, they are not to be underestimated. Three out of five mass extinction events in Earth\u2019s history may have been attributed to supervolcano eruptions and the subsequent climate change effects (Raup &amp; Sepkoski, 1982, Twitchett R., 2006, McElwain &amp; Punyasena, 2007). The magnitude of volcanic eruptions is typically measured on a logarithmic Volcanic Explosivity Index (VEI) scale based on &nbsp;the volume of projectiles. An VEI-0 effusive eruption ejects &lt; 10<sup>4</sup> m<sup>3</sup> (e.g.&nbsp;<a href=\"https://en.wikipedia.org/wiki/2022_eruption_of_Mauna_Loa\"><u>2022 Mauna Loa</u></a>), while an VEI-7 Ultra-Plinian eruption ejects &gt; 100 km<sup>3</sup> (e.g.&nbsp;<a href=\"https://en.wikipedia.org/wiki/1815_eruption_of_Mount_Tambora\"><u>1815 Tambora</u></a>) (Newhall &amp; Self, 1982). We have never experienced nor have written record of any VEI-8+ eruptions, although the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Toba_catastrophe_theory\"><u>Toba eruption 74,000 years ago</u></a> may have been linked to a disputed human genetic bottleneck theory.</p><p>&nbsp;</p><p>In&nbsp;<i>The Precipice</i>, Toby Ord's corrected response for a VEI-8+ eruption is estimated at&nbsp;<a href=\"https://theprecipice.com/errata\"><u>1 in 8000 this century</u></a> (Ord T., 2020), while smaller but less disruptive eruptions are orders of magnitude more likely. Cumulatively, they have the highest risk in the natural risk category and are similar to the probability of \u201cnaturally\u201d arising pandemics (the lowest probability anthropogenic X-risk) (Snyder-Beattie et al., 2019, Mason et al., 2004, Rougier et al., 2018). Fortunately, for smaller magnitude eruptions, the impacts will greatly depend on the location of the expected supervolcano eruption. However, the upper-end of a lower magnitude VEI-7 eruptions (&gt;~500 km3) could still be existential if the unlikely conditions exacerbate the current climate change and&nbsp;<a href=\"https://en.wikipedia.org/wiki/Timeline_of_extinctions_in_the_Holocene\"><u>Holocene extinction</u></a>. The 1815 VEI-7 eruption of Tambora volcano has drastically changed the course of history globally (Oppenheimer C., 2003). For a detailed yet approachable introduction, see&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jJDuEhLpF7tEThAHy/on-the-assessment-of-volcanic-eruptions-as-global\"><u>Cassidy &amp; Mani (2021) forum post</u></a>.</p><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JaACyD72oacQB9WDn/j5x7rrycqbm24b4qtjbm\"></p><p><i>Figure 5. Imagination of a Plinian eruption of Mount Vesuvius at 79 A.D. (the Pompei one) by English geologist George Scrope (1822).</i></p><p>&nbsp;</p><p>Geoscientists are therefore urgently needed to study the potential fallout of an upper-end VEI-7 eruption (1 in 12 this century) (Rougier et al., 2018) and potential mitigation scenarios for ecosystem collapse, interaction with climate change, and food resilience (Denkenberger &amp; Pearce, 2016).</p><p>&nbsp;&nbsp;</p><p>To illustrate the neglectedness of volcanic research, we can do a basic&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/fermi-estimate\"><u>BOTEC</u></a> with the US annual budget for mitigating against NASA Asteroid strike compared to the USGS volcanic programs:</p><p>&nbsp;</p><ul><li><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"EMA = Exponential Moving Average\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">l</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">v</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;\">g</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">v</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;\">g</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span></li><li>The&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"EMA\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span>&nbsp;is a calculation that finds the trend of recent funding with bias towards more recent data.</li><li>A slight modification on the&nbsp;<a href=\"https://www.investopedia.com/terms/e/ema.asp\"><u>conventional EMA calculation</u></a> was made using a geometric series. This adjustment allows for enhanced smoothing and incorporates a geometric discount over a 3-year period to reduce volatility.</li></ul><p>&nbsp;</p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"EMA_{(3 years)} = Fund_{(t)}*0.571+EMA_{(t-1)}*0.286 + EMA_{(t-2)}*0.143\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">3</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;\">y</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.298em;\">\u2217</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.571</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.298em;\">\u2217</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.286</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.298em;\">\u2217</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.143</span></span></span></span></span></span></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9i0ih186vy4\"><sup><a href=\"#fn9i0ih186vy4\">[1]</a></sup></span></p><p>&nbsp;</p><ul><li>Odds of asteroid strike this century = 1 in 1,000,000</li><li>Odds of VEI-8+ eruption = 1 in 8,000</li><li>EMA (3 years) for 2023 Budget of NASA DART + NEO Program (<a href=\"https://docs.google.com/spreadsheets/d/1WbVd3we1sye1Jj5Mq2V3Nv4NVg6a8cTJjErppRl-7Kk/edit#gid=0\"><u>~ $116.877 million USD</u></a>) (The Planetary Society, 2019)</li><li>Assuming we want to allocate VEI-8+ funding in proportional to the odds ratio of current asteroid deflection funding, accounting for diminishing returns:</li></ul><p>&nbsp;</p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Fund_{(VEI8+)} =&nbsp;EMA_{Fund(Asteroid)}\u221a{1000000/8000}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;\">V</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;\">I</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">8</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">\u221a</span></span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1000000</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">8000</span></span></span></span></span></span></span></span></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc844lq0wuf\"><sup><a href=\"#fnc844lq0wuf\">[2]</a></sup></span></p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Fund_{(VEI8+)} =EMA_{Fund(Asteroid)}*11.18\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;\">V</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;\">I</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">8</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.298em;\">\u2217</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">11.18</span></span></span></span></span></span></span></p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Fund_{(VEI8+)} = $1.307\\ billion\\ USD\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;\">V</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;\">I</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">8</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-texatom MJXc-space3\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">$</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1.307</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.291em; padding-bottom: 0.372em;\">&nbsp;</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">l</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">l</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.291em; padding-bottom: 0.372em;\">&nbsp;</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span></span></span></span></span></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwtt13e7q9c\"><sup><a href=\"#fnwtt13e7q9c\">[3]</a></sup></span></p><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JaACyD72oacQB9WDn/qm8v6jvfk69o9rsc77yg\"></p><p><i>Figure 6. When mitigating for highly neglected catastrophic risk, it might warrant a more aggressive initial investment strategy. While Intervention in food resilience scales linearly, volcanic research and volcanic geo-engineering require substantial initial investment. Combining the two functions might approximate to an inverse sigmoid function.</i></p><p>&nbsp;</p><blockquote><p>*The annual budget of USGS 5 years National Volcano Early Warning System averages to ~ $11 million USD (USGS, 2022). The total annual budget of UK NERC is ~$503 million USD.</p><p>*This also suggests that we should probably be spending at least ~$36.96 billion USD for Unaligned AI with the odds of 1 in 10 this century. (while current funding for AI Safety of 3 years&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"EMA\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span>&nbsp;is calculated at&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1WbVd3we1sye1Jj5Mq2V3Nv4NVg6a8cTJjErppRl-7Kk/edit#gid=0\"><u>~$52.686 million USD</u></a>), using McAleese S. (2023)&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/XdhwXppfqrpPL2YDX/an-overview-of-the-ai-safety-funding-situation#FTX_Future_Fund\"><u>total AI Safety funding as base</u></a>.</p></blockquote><p>&nbsp;</p><h2>Areas of Interests in catastrophe research</h2><p>&nbsp;</p><p>In&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would\"><u>Luisa Rodriguez (2020)</u></a> research post on civilisation collapse, she is optimistic that humans are likely to survive a high-magnitude impact catastrophe. I speculate that an upper-end VEI-7 and lower-end VEI-8 eruption would land somewhere between&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Case1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Case2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span>&nbsp;scenario, erring on&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Case2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span>&nbsp;if little mitigation were in place. With extensive regional infrastructure damage but global climate change with unexpected consequences, such as more pronounced cooling than expected and uncertainty in extensive stripping of the ozone layer.</p><p>&nbsp;</p><blockquote><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Case 1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span><i>: 50% population loss, no infrastructure damage, no climate change (e.g. a limited pandemic)</i></p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Case 2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span><i>: 90% population loss, infrastructure damage, and extreme climate change (e.g. nuclear war that caused nuclear winter)</i></p></blockquote><p>&nbsp;</p><p>Isolated communities experiencing a cascading series of unfortunate events might experience local extinction if adaptation were inadequate, prolonging civilisation recovery. An historical example is the egregious extinction of the Norse settlements in Greenland. The 1257 A.D.&nbsp;<a href=\"https://en.wikipedia.org/wiki/1257_Samalas_eruption#\"><u>VEI-7 eruption of Samsala volcano</u></a> caused a decadal cooling of about -1<sup>o</sup>C in the late mediaeval world. The settlements of the Norse Greenland suffered unrecoverable extinction due to maladaptation despite being very far away from the epicentre and surviving the initial fallout (Dugmore et al., 2007 &amp; Dugmore et al., 2012). A modern day equivalent would be the several months and years post nuclear or volcanic fallout. Investigation into feeding the world when industry and logistics are disrupted will be needed to prevent unrecoverable collapse (Baum et al., 2015 &amp; Denkenberger et al., 2017).</p><p>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JaACyD72oacQB9WDn/gdngbcbxylvbjjo7eo0i\"></p><p><i>Figure 7. A model of local extinction of the Norse Greenlanders due to maladaptation to changing environments and trade relations (Dugmore et al., 2007).</i></p><p>&nbsp;</p><p>In the spirit of this, we hope that an interdisciplinary group of geoscientists, where collective knowledge sharing can help each other find our comparative advantages and maximise our impact. Currently, we will just start with a channel&nbsp;<a href=\"https://eavirtualmeetupgroup.slack.com/archives/C05PQCZ2B8T\"><strong><u>role-geoscientists</u></strong><u> on EA Anywhere slack</u></a> to gauge the demand for the community. We will adapt appropriately when there's an appetite for a more structured group.</p><p>&nbsp;</p><h3>Few distilled topics from each cause areas that might be interesting to start thinking about:</h3><p>&nbsp;</p><p><strong>Supereruption fallout modelling and food resilience</strong></p><p>&nbsp;</p><p>Far-field supereruption modelling:</p><ul><li>Global albedo and insolation absorption change</li><li>Ocean fertilisation and CO<sub>2</sub> drawdown (also a carbon capture and storage candidate)</li><li>Stratospheric injection of SO<sub>2</sub> and HCI and the meteorological and ozone consequences (this would also inform us about SRM risk and reward)</li></ul><p>&nbsp;</p><p>Food resilience and recovery during the grace period:</p><ul><li>R&amp;D investment into farming system for UV resistant and low light crops</li><li>Spatial distribution of catastrophic farming system and&nbsp;<a href=\"https://www.who.int/publications/i/item/WHO-MHP-HPS-EML-2023.02\"><u>essential medicine</u></a><ul><li>Supply chain during major logistical and industrial disruption to feed isolated surviving populations</li></ul></li><li>Contribute to ALLFED\u2019s&nbsp;<a href=\"https://allfed.info/images/pdfs/Preprint%20V2.0%20Integrated%20assessment%20ALLFED.docx.pdf\"><i><u>Abrupt Sunlight Reduction Scenarios (ASRSs)</u></i></a> plan</li></ul><p>&nbsp;</p><p><strong>Research in volcanic and climatic geo-engineering</strong></p><p>&nbsp;</p><p>Denkenberger &amp; Blair (2018) proposed a speculative range of geo-engineering interventions from altering the properties of magma to releasing the pressure of magma chambers.</p><p>&nbsp;</p><p>Volcanic geo-engineering:</p><ul><li>Magma cooling and alteration</li><li>Small eruptions and pressure release</li><li>Atmospheric containment of eruption materials</li></ul><p>&nbsp;</p><p>Technical climate engineering and environmental impact:</p><ul><li>Scaling carbon capture and storage in particular using biochar</li><li>Ocean fertilisation and CO<sub>2</sub> drawdown</li><li>Active cooling from controlled injection of SO<sub>2</sub> into the stratosphere</li></ul><p>&nbsp;</p><p><strong>Research in permafrost pathogens, natural and bioengineered fungi against humans and agriculture</strong></p><p>&nbsp;</p><ul><li>Spatial sampling for global distribution of permafrost pathogens and vectors</li><li>Modelling the potential spread of known permafrost pathogens that affects humans and agriculture</li><li>Research into the spore spread of neglected/bioengineered pathogens such as fungi and mycoherbicides</li></ul><p>&nbsp;</p><p><strong>Building Effective Altruism in geoethics and environmental policies</strong></p><p>&nbsp;</p><p>Animal suffering and resource exploration</p><ul><li>The effects of deep-sea mining on ecosystem destruction and animal suffering</li></ul><p>&nbsp;</p><p>Ethics in climate engineering</p><ul><li>When is intervention justified, is there an inflection point where neglecting climate engineering actually causes net harm?<ul><li>If implemented, how may compensation for ecosystem and anthropogenic gains and losses be attributed?</li></ul></li></ul><p>&nbsp;</p><p><strong>Volunteering for NGO working behind the scenes of other effective charities</strong></p><p>&nbsp;</p><p>NGO such as the Humanitarian OpenStreetMap Team have a history of partnering and delivering projects for public health campaigns against&nbsp;<a href=\"https://www.hotosm.org/projects/strategies-to-prevent-spillover-stop-spillover/\"><u>future zoonotic viral diseases</u></a>, poverty alleviation initiatives and&nbsp;<a href=\"https://drive.google.com/file/d/1vqID7We42k3c4L7vbEL_CQRKMLQhlAPz/view?pli=1\"><u>malaria bed nets distribution</u></a> efforts. EAs are encouraged to strategically engage in projects with recognised high-impact cause areas. A comprehensive list of their past public health work can be found&nbsp;<a href=\"https://www.hotosm.org/impact-areas/public-health/\"><u>here</u></a>. Their scalable structure has high potential and could be adaptable to address many of the risks mentioned above.</p><p>&nbsp;</p><p><strong>Contact</strong></p><p>&nbsp;</p><p>Feel free to provide comments, thoughts, and criticism in the comment boxes below, at our&nbsp;<a href=\"https://eavirtualmeetupgroup.slack.com/archives/C05PQCZ2B8T\"><u>slack channel</u></a>, or contact me at chrischank{at}protonmail{dot}ch. Thank you for reading.</p><p>&nbsp;</p><h3>Bibliography</h3><p>&nbsp;</p><p>Abelson, B., Varshney, K.R., Sun, J., 2014. Targeting direct cash transfers to the extremely poor, in: Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201914. Association for Computing Machinery, New York, NY, USA, pp. 1563\u20131572. <a href=\"https://doi.org/10.1145/2623330.2623335\">https://doi.org/10.1145/2623330.2623335</a></p><p>Badr, H.S., Du, H., Marshall, M., Dong, E., Squire, M.M., Gardner, L.M., 2020. Association between mobility patterns and COVID-19 transmission in the USA: a mathematical modelling study. The Lancet Infectious Diseases 20, 1247\u20131254. <a href=\"https://doi.org/10.1016/S1473-3099(20)30553-3\">https://doi.org/10.1016/S1473-3099(20)30553-3</a></p><p>Baum, S.D., Denkenberger, D.C., Pearce, J.M., Robock, A., Winkler, R., 2015. Resilience to global food supply catastrophes. Environment Systems and Decisions 35, 301\u2013313.</p><p>Berchoux, T., Hutton, C.W., 2019. Spatial associations between household and community livelihood capitals in rural territories: An example from the Mahanadi Delta, India. Applied Geography 103, 98\u2013111.</p><p>Cassidy, M., Mani, L., n.d. On the assessment of volcanic eruptions as global catastrophic or existential risks.</p><p>CEvans, n.d. Taking prioritisation within \u201cEA\u201d seriously.</p><p>Chi, G., Fang, H., Chatterjee, S., Blumenstock, J.E., 2022. Microestimates of wealth for all low- and middle-income countries. Proceedings of the National Academy of Sciences 119, e2113658119. <a href=\"https://doi.org/10.1073/pnas.2113658119\">https://doi.org/10.1073/pnas.2113658119</a></p><p>Clements, A.C.A., Garba, A., Sacko, M., Tour\u00e9, S., Dembel\u00e9, R., Landour\u00e9, A., Bosque-Oliva, E., Gabrielli, A.F., Fenwick, A., 2008. Mapping the Probability of Schistosomiasis and Associated Uncertainty, West Africa. Emerg Infect Dis 14, 1629\u20131632. <a href=\"https://doi.org/10.3201/eid1410.080366\">https://doi.org/10.3201/eid1410.080366</a></p><p>Clements, A.C.A., Kur, L.W., Gatpan, G., Ngondi, J.M., Emerson, P.M., Lado, M., Sabasio, A., Kolaczinski, J.H., 2010. Targeting Trachoma Control through Risk Mapping: The Example of Southern Sudan. PLoS Negl Trop Dis 4, e799. <a href=\"https://doi.org/10.1371/journal.pntd.0000799\">https://doi.org/10.1371/journal.pntd.0000799</a></p><p>Clements, A.C.A., Lwambo, N.J.S., Blair, L., Nyandindi, U., Kaatano, G., Kinung\u2019hi, S., Webster, J.P., Fenwick, A., Brooker, S., 2006. Bayesian spatial analysis and disease mapping: tools to enhance planning and implementation of a schistosomiasis control programme in Tanzania. Tropical Medicine &amp; International Health 11, 490\u2013503. <a href=\"https://doi.org/10.1111/j.1365-3156.2006.01594.x\">https://doi.org/10.1111/j.1365-3156.2006.01594.x</a></p><p>Cox, J., n.d. Report on work towards a village-based malaria stratification system for Cambodia.</p><p>Cox, J., Dy Soley, L., Bunkea, T., Sovannaroth, S., Soy Ty, K., Ngak, S., Bjorge, S., Ringwald, P., Mellor, S., Sintasath, D., Meek, S., 2014. Evaluation of community-based systems for the surveillance of day three-positive Plasmodium falciparum cases in Western Cambodia. Malaria Journal 13, 282. <a href=\"https://doi.org/10.1186/1475-2875-13-282\">https://doi.org/10.1186/1475-2875-13-282</a></p><p>Cresswell, T., 2013. Geographic thought: a critical introduction. John Wiley &amp; Sons.</p><p>Denkenberger, D., Sandberg, A., Tieman, R.J., Pearce, J.M., 2022. Long term cost-effectiveness of resilient foods for global catastrophes compared to artificial general intelligence safety. International Journal of Disaster Risk Reduction 73, 102798.</p><p>Denkenberger, D.C., Blair, R.W., 2018. Interventions that may prevent or mollify supervolcanic eruptions. Futures, Futures of research in catastrophic and existential risk 102, 51\u201362. <a href=\"https://doi.org/10.1016/j.futures.2018.01.002\">https://doi.org/10.1016/j.futures.2018.01.002</a></p><p>Denkenberger, D.C., Cole, D.D., Abdelkhaliq, M., Griswold, M., Hundley, A.B., Pearce, J.M., 2017. Feeding everyone if the sun is obscured and industry is disabled. International Journal of Disaster Risk Reduction 21, 284\u2013290.</p><p>Denkenberger, D.C., Pearce, J.M., 2016. Cost-effectiveness of interventions for alternate food to address agricultural catastrophes globally. International Journal of Disaster Risk Science 7, 205\u2013215.</p><p>Dong, E., Du, H., Gardner, L., 2020. An interactive web-based dashboard to track COVID-19 in real time. The Lancet Infectious Diseases 20, 533\u2013534. <a href=\"https://doi.org/10.1016/S1473-3099(20)30120-1\">https://doi.org/10.1016/S1473-3099(20)30120-1</a></p><p>Dreier, C., 2019. How NASA\u2019s Planetary Defense Budget Grew By More Than 4000% in 10\u2026 [WWW Document]. The Planetary Society. URL <a href=\"https://www.planetary.org/articles/nasas-planetary-defense-budget-growth\">https://www.planetary.org/articles/nasas-planetary-defense-budget-growth</a> (accessed 8.28.23).</p><p>Dugmore, A.J., Keller, C., McGovern, T.H., 2007. Norse Greenland Settlement: Reflections on Climate Change, Trade, and the Contrasting Fates of Human Settlements in the North Atlantic Islands. Arctic Anthropology 44, 12\u201336. <a href=\"https://doi.org/10.1353/arc.2011.0038\">https://doi.org/10.1353/arc.2011.0038</a></p><p>Dugmore, A.J., McGovern, T.H., V\u00e9steinsson, O., Arneborg, J., Streeter, R., Keller, C., 2012. Cultural adaptation, compounding vulnerabilities and conjunctures in Norse Greenland. Proceedings of the National Academy of Sciences 109, 3658\u20133663.</p><p>Gardner, L., Ratcliff, J., Dong, E., Katz, A., 2021. A need for open public data standards and sharing in light of COVID-19. The Lancet Infectious Diseases 21, e80. <a href=\"https://doi.org/10.1016/S1473-3099(20)30635-6\">https://doi.org/10.1016/S1473-3099(20)30635-6</a></p><p>Gruber, S., 2012. Derivation and analysis of a high-resolution estimate of global permafrost zonation. The Cryosphere 6, 221\u2013233.</p><p>Jean, N., Burke, M., Xie, M., Davis, W.M., Lobell, D.B., Ermon, S., 2016. Combining satellite imagery and machine learning to predict poverty. Science 353, 790\u2013794.</p><p>Kirkpatrick, K., 2023. Using Algorithms to Deliver Disaster Aid. Commun. ACM 66, 17\u201319. <a href=\"https://doi.org/10.1145/3591214\">https://doi.org/10.1145/3591214</a></p><p>Kuffer, M., Pfeffer, K., Sliuzas, R., 2016. Slums from Space\u201415 Years of Slum Mapping Using Remote Sensing. Remote Sensing 8, 455. <a href=\"https://doi.org/10.3390/rs8060455\">https://doi.org/10.3390/rs8060455</a></p><p>Mandeville, C., Cervelli, P.F., Avery, V.F., Wilkins, A., 2022. The Volcano Hazards Program \u2014 Strategic science plan for 2022\u20132026 (No. 1492), Circular. U.S. Geological Survey. <a href=\"https://doi.org/10.3133/cir1492\">https://doi.org/10.3133/cir1492</a></p><p>Mason, B.G., Pyle, D.M., Oppenheimer, C., 2004. The size and frequency of the largest explosive eruptions on Earth. Bull Volcanol 66, 735\u2013748. <a href=\"https://doi.org/10.1007/s00445-004-0355-9\">https://doi.org/10.1007/s00445-004-0355-9</a></p><p>McAleese, S., n.d. An Overview of the AI Safety Funding Situation.</p><p>McElwain, J.C., Punyasena, S.W., 2007. Mass extinction events and the plant fossil record. Trends in Ecology &amp; Evolution 22, 548\u2013557. <a href=\"https://doi.org/10.1016/j.tree.2007.09.003\">https://doi.org/10.1016/j.tree.2007.09.003</a></p><p>Miner, K.R., D\u2019Andrilli, J., Mackelprang, R., Edwards, A., Malaska, M.J., Waldrop, M.P., Miller, C.E., 2021. Emergent biogeochemical risks from Arctic permafrost degradation. Nat. Clim. Chang. 11, 809\u2013819. <a href=\"https://doi.org/10.1038/s41558-021-01162-y\">https://doi.org/10.1038/s41558-021-01162-y</a></p><p>Newhall, C.G., Self, S., 1982. The volcanic explosivity index (VEI) an estimate of explosive magnitude for historical volcanism. Journal of Geophysical Research: Oceans 87, 1231\u20131238. <a href=\"https://doi.org/10.1029/JC087iC02p01231\">https://doi.org/10.1029/JC087iC02p01231</a></p><p>Nyandwi, E., Osei, F.B., Veldkamp, T., Amer, S., 2020. Modeling schistosomiasis spatial risk dynamics over time in Rwanda using zero-inflated Poisson regression. Sci Rep 10, 19276. <a href=\"https://doi.org/10.1038/s41598-020-76288-8\">https://doi.org/10.1038/s41598-020-76288-8</a></p><p>Oppenheimer, C., 2003. Climatic, environmental and human consequences of the largest known historic eruption: Tambora volcano (Indonesia) 1815. Progress in physical geography 27, 230\u2013259.</p><p>Ord, T., 2020. The precipice: Existential risk and the future of humanity. Hachette Books.</p><p>Pamplany, A., Gordijn, B., Brereton, P., 2020. The Ethics of Geoengineering: A Literature Review. Sci Eng Ethics 26, 3069\u20133119. <a href=\"https://doi.org/10.1007/s11948-020-00258-6\">https://doi.org/10.1007/s11948-020-00258-6</a></p><p>P\u00f6rtner, H.-O., Roberts, D.C., Masson-Delmotte, V., Zhai, P., Tignor, M., Poloczanska, E., Weyer, N.M., 2019. The ocean and cryosphere in a changing climate. IPCC special report on the ocean and cryosphere in a changing climate 1155.</p><p>Proctor, J., Hsiang, S., Burney, J., Burke, M., Schlenker, W., 2018. Estimating global agricultural effects of geoengineering using volcanic eruptions. Nature 560, 480\u2013483. <a href=\"https://doi.org/10.1038/s41586-018-0417-3\">https://doi.org/10.1038/s41586-018-0417-3</a></p><p>Quinn, J.A., Nyhan, M.M., Navarro, C., Coluccia, D., Bromley, L., Luengo-Oroz, M., 2018. Humanitarian applications of machine learning with remote-sensing data: review and case study in refugee settlement mapping. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 376, 20170363. <a href=\"https://doi.org/10.1098/rsta.2017.0363\">https://doi.org/10.1098/rsta.2017.0363</a></p><p>Rampino, M.R., Self, S., 1992. Volcanic winter and accelerated glaciation following the Toba super-eruption. Nature 359, 50\u201352. <a href=\"https://doi.org/10.1038/359050a0\">https://doi.org/10.1038/359050a0</a></p><p>Raup, D.M., Sepkoski, J.J., 1982. Mass Extinctions in the Marine Fossil Record. Science 215, 1501\u20131503. <a href=\"https://doi.org/10.1126/science.215.4539.1501\">https://doi.org/10.1126/science.215.4539.1501</a></p><p>Reynolds, J.L., 2019. Solar geoengineering to reduce climate change: a review of governance proposals. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 475, 20190255. <a href=\"https://doi.org/10.1098/rspa.2019.0255\">https://doi.org/10.1098/rspa.2019.0255</a></p><p>Ritchie, H., Roser, M., Rosado, P., 2022. Energy. Our World in Data.</p><p>Rodriguez, L., n.d. What is the likelihood that civilizational collapse would directly lead to human extinction (within decades)?</p><p>Rougier, J., Sparks, R.S.J., Cashman, K.V., Brown, S.K., 2018. The global magnitude\u2013frequency relationship for large explosive volcanic eruptions. Earth and Planetary Science Letters 482, 621\u2013629. <a href=\"https://doi.org/10.1016/j.epsl.2017.11.015\">https://doi.org/10.1016/j.epsl.2017.11.015</a></p><p>Shukla, P.R., Skea, J., Slade, R., Al Khourdajie, A., van Diemen, R., McCollum, D., Pathak, M., Some, S., Vyas, P., Fradera, R., Belkacemi, M., Hasija, A., Lisboa, G., Luz, S., Malley, J. (Eds.), 2022. Climate Change 2022: Impacts, Adaptation and Vulnerability. Contribution of Working Group III to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change. <a href=\"https://doi.org/10.1017/9781009157926\">https://doi.org/10.1017/9781009157926</a></p><p>Shun, M., Lumis, V., 2023. Cash to flood survivors: 4 things we got right &amp; wrong [WWW Document]. GiveDirectly. URL <a href=\"https://www.givedirectly.org/flood-pilots/\">https://www.givedirectly.org/flood-pilots/</a> (accessed 8.28.23).</p><p>Snyder-Beattie, A.E., Ord, T., Bonsall, M.B., 2019. An upper bound for the background rate of human extinction. Sci Rep 9, 11054. <a href=\"https://doi.org/10.1038/s41598-019-47540-7\">https://doi.org/10.1038/s41598-019-47540-7</a></p><p>Twitchett, R.J., 2006. The palaeoclimatology, palaeoecology and palaeoenvironmental analysis of mass extinction events. Palaeogeography, Palaeoclimatology, Palaeoecology 232, 190\u2013213. <a href=\"https://doi.org/10.1016/j.palaeo.2005.05.019\">https://doi.org/10.1016/j.palaeo.2005.05.019</a></p><p>Varshney, K.R., Chen, G.H., Abelson, B., Nowocin, K., Sakhrani, V., Xu, L., Spatocco, B.L., 2015. Targeting Villages for Rural Development Using Satellite Image Analysis. Big Data 3, 41\u201353. <a href=\"https://doi.org/10.1089/big.2014.0061\">https://doi.org/10.1089/big.2014.0061</a></p><p>Watmough, G.R., Marcinko, C.L.J., Sullivan, C., Tschirhart, K., Mutuo, P.K., Palm, C.A., Svenning, J.-C., 2019. Socioecologically informed use of remote sensing data to predict rural household poverty. Proceedings of the National Academy of Sciences 116, 1213\u20131218. <a href=\"https://doi.org/10.1073/pnas.1812969116\">https://doi.org/10.1073/pnas.1812969116</a></p><p>Xia, L., Robock, A., Cole, J., Curry, C.L., Ji, D., Jones, A., Kravitz, B., Moore, J.C., Muri, H., Niemeier, U., Singh, B., Tilmes, S., Watanabe, S., Yoon, J.-H., 2014. Solar radiation management impacts on agriculture in China: A case study in the Geoengineering Model Intercomparison Project (GeoMIP). Journal of Geophysical Research: Atmospheres 119, 8695\u20138711. <a href=\"https://doi.org/10.1002/2013JD020630\">https://doi.org/10.1002/2013JD020630</a></p><p>Zhao, S., Cheng, W., Yuan, Y., Fan, Z., Zhang, J., Zhou, C., 2022. Global permafrost simulation and prediction from 2010 to 2100 under different climate scenarios. Environmental Modelling &amp; Software 149, 105307. <a href=\"https://doi.org/10.1016/j.envsoft.2022.105307\">https://doi.org/10.1016/j.envsoft.2022.105307</a></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9i0ih186vy4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9i0ih186vy4\">^</a></strong></sup></span><div class=\"footnote-content\"><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"EMA_{(3 years)} = Fund_{(t)}*(1.321-3/(N+1))+EMA_{(t-1)}*(1.036-3/(N+1))+ EMA_{(t-2)}*(0.893-3/(N+1))\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">3</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;\">y</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">r</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;\">F</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.003em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;\">d</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.298em;\">\u2217</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1.321</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">3</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.298em;\">\u2217</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1.036</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">3</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.298em;\">\u2217</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.893</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">3</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc844lq0wuf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc844lq0wuf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Discounting the risk ratio&nbsp; with&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\u221a1000000/8000\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">\u221a</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1000000</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">8000</span></span></span></span></span></span></span>&nbsp;was done because merely scaling by&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1000000/8000\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1000000</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">8000</span></span></span></span></span></span></span>&nbsp;is, although logical, would not account for diminishing return. A more conservative calculation would be&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"ln(1000000/8000)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">l</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1000000</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">8000</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span>, however, this might be too conservative.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwtt13e7q9c\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwtt13e7q9c\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Above calculations are already very conservative.&nbsp;<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S2212420922000176\"><u>Denkenberger et al. (2022)</u></a> have presented a more sophisticated modelling to justify for even higher investment and ROI.</p></div></li></ol>", "user": {"username": "chanakin"}}, {"_id": "BnzogudjvxxPeZFrE", "title": "EA Philippines\u2019 Career Planning Retreat for Students and Professionals", "postedAt": "2023-08-31T05:42:28.610Z", "htmlBody": "<figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BnzogudjvxxPeZFrE/qfba1v3rvlpvm6jk56s9\" alt=\"May be an image of 10 people and text that says 'Effective Altruism Philippines \u8cb7\u65e5\u65e5 # Credits Jaynell Chang'\"></figure><p>Effective Altruism Philippines ran a 3-day, 2-night students\u2019 and professionals\u2019 retreat focused on discussions on (1) cause area exploration and prioritization and (2) career building and planning. We would like to thank the EA Infrastructure Fund and CEA Groups Team for providing funding that helped make this retreat possible.</p><p>This post summarizes our goals and indicators of success, how we ran it, its impact on attendees, what went well, what can be improved, and our post-retreat plans. If you are interested in planning one, you can check out the full accomplishment report for the retreat <a href=\"https://docs.google.com/document/d/1pZI1io75jtsNFvPZOB7JfIeJkTAxQj7c05LeVRCGayI/edit\"><u>here</u></a>. The report includes our preparation timeline, how we designed the program, and a full discussion of the retreat outcomes and impact. We also elaborated on what went well, best practices, and areas for improvement that could be helpful in future retreats.</p><h2><strong>What are the goals of the retreat?</strong></h2><ol><li>Boost and deepen learning on different EA cause areas especially in longtermist priorities such as AI safety, biosecurity, etc. and neartermist priorities such as global health and development and improving institutional decision-making</li><li>Help attendees develop a clearer career plan for high impact</li><li>Facilitate connections between students and professionals to expand their networks and foster information sharing and career planning support</li></ol><h2><strong>How did we measure the success of the retreat?</strong></h2><p>The following are specific key outcomes and metrics for success that we achieved for the retreat:</p><ol><li>For attendees to find their overall experience at the retreat highly valuable and satisfactory and give an<strong> average likelihood to recommend of at least 8.5/10&nbsp;</strong>to a friend or colleague with similar interests.&nbsp;<i>Participants answered an average of 9.3.</i></li><li>For attendees to find the retreat agenda (talks, workshops, discussions, one-on-ones) value adding to their career journey and give&nbsp;<strong>an average rating of 4/5 for each activity</strong> and&nbsp;<strong>an average overall agenda rating of 4/5</strong>.&nbsp;<i>All agenda activities were given a satisfaction and value rating above 4, with an average of 4.66. This is close to the overall satisfaction and value rating of the participants for the entire agenda is 4.67.</i></li><li>For each attendee to develop a strong career network and support system during the retreat by making&nbsp;<strong>at least 5 new connections</strong>.&nbsp;<i>We defined a connection in the feedback form as \u201ca person you feel comfortable asking for a favor.\u201d More than half reported at least 5 new connections while 8 answered they gained 3-4 new connections.&nbsp;</i></li></ol><h2><strong>What did the retreat look like?</strong></h2><p>There were a total of 38 attendees for the retreat: 10 participating professionals, 14 students, 9 speakers (all professionals), and 5 from the organizing team. Most of the participants are still in the exploratory phase making them flexible and receptive to new career steps and opportunities. The top three cause areas of the participants were: improved institutional decision making (IIDM), global health and development and mental health, and existential risks from AI. To ensure that all participants can process and take seriously the level of knowledge and ideas shared during the retreat, we set a minimum of ~20 hours EA engagement (means at least finishing an introductory fellowship or the Doing Good Better book).</p><p>The retreat agenda covered talks on different cause areas, workshops on career planning, personal branding, and empowered decision-making, panel discussions from Filipinos working in EA cause areas, and participant-driven discussion sessions.&nbsp;</p><p>Tanya Quijano and I organized the whole content and logistics of the retreat. Elmer Cuevas, Jay Chang, and Brian Tan also helped with the operations, branding, and content of the retreat.&nbsp;</p><h2><strong>Impact of the retreat</strong></h2><p>The biggest impact of the retreat are the social and professional connections made by the attendees. It was observed that attendees maximized down times by approaching potential connections and having unstructured one-on-ones. Students especially are very eager to hear stories from the professionals\u2019 experiences and to ask for career advice. Attendees also shared that most of their new connections were people they already knew but only had the opportunity to connect with them during the retreat.</p><p>The attendees also changed their minds about a key idea and made changes to their career plans as a result of the retreat. This could be due to the conversations they had, the advice and experiences shared by speakers or a new opportunity they found out about during the retreat.&nbsp;&nbsp;</p><p>The following are career action steps/plans for the next 1-3 months shared by the attendees as a result of the retreat:&nbsp;</p><ul><li>AI safety, policy, and governance cause area exploration</li><li>Looking out for communications and operations internship opportunities</li><li>Extending scope of opportunities and attending more workshops</li><li>Working on a portfolio, personal branding, and networking skills</li><li>Rearrange self-study plan to incorporate exploring AI safety as a career step</li><li>Apply for a Master\u2019s degree</li><li>Apply to government jobs</li><li>Working on a cause specific organization</li><li>Refining personal theory of change&nbsp;</li><li>Upskill in the technical management side to be a collaborator in AI safety discussions</li><li>Refresh Infectious diseases specialty and look for potential research projects on Biosecurity (esp. in developing countries)</li><li>Take a step back to reprioritize where to further create a larger impact</li><li>Collaborating with other people to work on career steps and projects</li></ul><p>Also, one impact story of the retreat was that Brian Tan, Clark Urzo, and Kriz Tahimic started ideating an AI safety project together in this retreat. Specifically, they're planning to start an organization in Manila that finds and trains people to do mechanistic interpretability research. They are currently applying for 1.9 FTE of funding for 6-9 months for this project. Brian thinks that there's roughly a 30% chance they wouldn't be working together on this project this year if not for the retreat, and that them discussing during the retreat likely helped speed up their plans for it by 1-2 months.</p><h2><strong>What went well?</strong></h2><ul><li><strong>The attendees were highly participative in the talks, discussions, and workshops.&nbsp;</strong>We highly encouraged participants to set their goals for the retreat.</li><li><strong>We were able to invite a great set of speakers, facilitators, and program volunteers.</strong> We made sure that we were aligned with the speakers regarding the topic of their activity. We also had a call for program volunteers from the participants one month before the retreat. We think that this made participants more involved with the program.</li><li><strong>The attendees felt a sense of belonging and were open to different discussions.</strong> Most of our attendees are some of the most active in the community and have attended local and international EA discussion groups so they are more likely to take EA ideas seriously while remaining constructive and open-minded about other people\u2019s opinions.</li><li><strong>The attendees were able to enjoy and maximize the space and activities in the venue.&nbsp;</strong>There was enough space and spots such as gazebos, the resort cafe, and recreational areas for participants to do their co-working meetings and one-on-ones.</li><li><strong>There was a perfect balance between the number of students and professionals in the retreat.&nbsp;</strong>We were intentional in targeting both students and professionals for the retreat. We sent email invites to people we think are fit to attend and could benefit from the retreat. EA Philippines maintains a good relationship with both target demographics with Tanya Quijano leading the professional outreach and strong connections with EA Blue, EA Taft, and EA UPD.</li></ul><h2><strong>What could be improved?</strong></h2><ul><li><strong>A less packed program</strong> to leave room for participants to rest and reflect on their learnings</li><li><strong>Emphasize the importance of one-on-ones</strong> <strong>more </strong>before and during the retreat</li><li><strong>Add group discussions with invited facilitators. </strong>This could cater to specific interests of the participants.</li><li><strong>Aim for more applicants</strong> so that we can effectively filter for the more engaged and promising participants.</li><li><strong>Have less attendees who are only arriving for part of the retreat.&nbsp;</strong>This is so that the attendees will get the full experience and benefit from the agenda we planned.</li><li><strong>Be more intentional in having a socials night event during the retreat.&nbsp;</strong>We depended mostly on the scheduled musical events in the venue.</li><li><strong>Consider an inclusive menu that will not sacrifice nutritional needs of other attendees.</strong> We could have arranged a vegan menu with higher protein content and more serving.</li></ul><h2><strong>What\u2019s next after the retreat?</strong></h2><ul><li><strong>EAGxPhilippines: </strong>Aside from new learning and insights, conference attendees stand to gain a lot of useful professional connections from local and international EAs invited to the conference.</li><li><strong>More community events highlighting cause areas:&nbsp;</strong>Komunidad is a monthly in-person meetup for the EA Philippines community wherein we invite highly engaged EAs to share about their work on a certain cause area.&nbsp;</li><li><strong>Running reading groups and workshops:&nbsp;</strong>Since March 2023, different groups of Filipino EAs have conducted reading groups on topics such as IIDM and AI safety. With the increasing interest in both topics, there is a possibility for another batch of these reading groups. There is also a team of volunteers interested in organizing for a workshop that EA PH members and the public can attend.</li><li><strong>Continuing our career assistance services: </strong>Our career assistance services include a mentor-mentee matching program, community builder one-on-ones, and sharing opportunities we think Filipino EAs can apply or join through our Slack workspace and Facebook Group, monthly newsletter, and the&nbsp;<a href=\"https://www.effectivealtruism.ph/opportunities-board\"><u>EA Philippines Opportunities Board</u></a>.</li></ul>", "user": {"username": "Alethea Faye Cenda\u00f1a"}}, {"_id": "ee8Pamunhqabucwjq", "title": "Long-Term Future Fund Ask Us Anything (September 2023)", "postedAt": "2023-08-30T23:02:19.684Z", "htmlBody": "<p>LTFF is running an Ask Us Anything! Most of the grantmakers at LTFF have agreed to set aside some time to answer questions on the Forum.</p><p>I (Linch) will make a soft commitment to answer one round of questions this coming Monday (September 4th) and another round the Friday after (September 8th).&nbsp;</p><p>We think that <a href=\"https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now\">right now could be an unusually good time to donate</a>. If you agree, you can donate to us&nbsp;<a href=\"https://www.givingwhatwecan.org/funds/effective-altruism-funds?utm_source=eafunds\"><u>here</u></a>.</p><h2><strong>About the Fund</strong></h2><p>The Long-Term Future Fund aims to positively influence the long-term trajectory of civilization by making grants that address global catastrophic risks, especially potential risks from advanced artificial intelligence and pandemics. In addition, we seek to promote, implement, and advocate for longtermist ideas and to otherwise increase the likelihood that future generations will flourish.</p><p>In 2022,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zZ2vq7YEckpunrQS4/long-term-future-fund-april-2023-grant-recommendations\"><u>we dispersed ~250 grants worth ~10 million</u></a>. You can see our&nbsp;<a href=\"https://funds.effectivealtruism.org/grants?fund=Long-Term%2520Future%2520Fund&amp;sort=round\">public grants database</a> here.</p><h2><strong>Related posts</strong></h2><ul><li><a href=\"https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now\">LTFF and EAIF are unusually funding-constrained right now</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching\">EA Funds organizational update: Open Philanthropy matching and distancing</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/zZ2vq7YEckpunrQS4/long-term-future-fund-april-2023-grant-recommendations\">Long-Term Future Fund: April 2023 grant recommendations</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding\">What Does a Marginal Grant at LTFF Look Like?</a></li><li>Asya Bergal\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/9vazTE4nTCEivYSC6/reflections-on-my-time-on-the-long-term-future-fund\">Reflections on my time on the Long-Term Future Fund</a></li><li>Linch Zhang\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/sWMwGNgpzPn7X9oSk/select-examples-of-adverse-selection-in-longtermist\">Select examples of adverse selection in longtermist grantmaking</a></li></ul><h1>About the Team</h1><ul><li><strong>Asya Bergal:&nbsp;</strong>Asya is the current chair of the Long-Term Future Fund. She also<strong>&nbsp;</strong>works as a Program Associate at Open Philanthropy. Previously, she worked as a researcher at AI Impacts and as a trader and software engineer for a crypto hedgefund. She's also written for the AI alignment newsletter and been a research fellow at the Centre for the Governance of AI at the Future of Humanity Institute (FHI). She has a BA in Computer Science and Engineering from MIT.</li><li><strong>Caleb Parikh:</strong> Caleb is the project lead of EA Funds. Caleb has previously worked on global priorities research as a research assistant at GPI, EA community building (as a contractor to the community health team at CEA), and global health policy.</li><li><strong>Linchuan Zhang:</strong> Linchuan (Linch) Zhang is a Senior Researcher at Rethink Priorities working on existential security research. Before joining RP, he worked on time-sensitive forecasting projects around COVID-19. Previously, he programmed for Impossible Foods and Google and has led several EA local groups.</li><li><strong>Oliver Habryka</strong>: Oliver runs Lightcone Infrastructure, whose main product is Lesswrong. Lesswrong has significantly influenced conversations around rationality and AGI risk, and the LWits community is often credited with having realized the importance of topics such as AGI (and AGI risk), COVID-19, existential risk and crypto much earlier than other comparable communities.</li></ul><p><i>You can find a list of our fund managers in our request for funding&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#Our_Fund_Managers\"><i><u>here</u></i></a><i>.</i></p><h1>Ask Us Anything</h1><p>We\u2019re happy to answer any questions \u2013 marginal uses of money, how we approach grants, questions/critiques/concerns you have in general, what reservations you have as a potential donor or applicant, etc.</p><p>There\u2019s no real deadline for questions, but let\u2019s say we have a soft commitment to focus on questions asked on or before September 8th.</p><p>Because we\u2019re&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now\"><u>unusually funding-constrained</u></a> right now, I\u2019m going to shill again for&nbsp;<a href=\"https://www.givingwhatwecan.org/funds/effective-altruism-funds?utm_source=eafunds\"><u>donating to us</u></a>.</p><p>If you have projects relevant to mitigating global catastrophic risks, you can also&nbsp;<a href=\"https://www.givingwhatwecan.org/charities/long-term-future-fund\"><u>apply for funding here</u></a>.</p>", "user": {"username": "Linch"}}, {"_id": "Fcnbq5ctJJ2w3g9sd", "title": "AI Consciousness Report: A Roundtable Discussion", "postedAt": "2023-08-30T21:50:47.415Z", "htmlBody": "<h3>with Patrick Butlin, Robert Long, Yoshua Bengio, and Grace Lindsay</h3><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Fcnbq5ctJJ2w3g9sd/guwefbhu195w3b9zhedo\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Fcnbq5ctJJ2w3g9sd/tgpyskl1hgatpa1xgpwd 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Fcnbq5ctJJ2w3g9sd/lcuqjytphgezxuqv8ro0 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Fcnbq5ctJJ2w3g9sd/oa4gv1658zux7rti0zil 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Fcnbq5ctJJ2w3g9sd/dsarz9x4zpopa2hzunqn 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Fcnbq5ctJJ2w3g9sd/zbjsimd1wy73bkcgnmhr 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Fcnbq5ctJJ2w3g9sd/eea2wuvz3khd4mlqcymq 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Fcnbq5ctJJ2w3g9sd/iqhj65ohbhjw3vl4pela 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Fcnbq5ctJJ2w3g9sd/rv4mml1gfyaiit2qefav 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Fcnbq5ctJJ2w3g9sd/zsb1w9dznyxomuqhutsk 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Fcnbq5ctJJ2w3g9sd/zgux4s7innhbvmxtezug 1999w\"></p><p>Tuesday, September 5th, 2023 | 4:00pm ET<br>Virtual on Zoom | <a href=\"https://nyu.zoom.us/webinar/register/WN_mwM0A9NtRxyNfENAu5O4WQ#/registration\">Register here</a></p><p><strong>About the event</strong></p><p>This event will feature four authors from the recently released and widely discussed <a href=\"https://arxiv.org/abs/2308.08708\">AI consciousness report</a>. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of the best-supported neuroscientific theories of consciousness. The paper surveys several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories the authors derive \"indicator properties\" of consciousness, elucidated in computational terms that allow them to assess AI systems for these properties. They use these indicator properties to assess several recent AI systems, and discuss how future systems might implement them. In this event, the authors will summarize the report, offer perspectives from philosophy, cognitive science, and computer science, and respond to questions and comments.</p><p><strong>About the panelists</strong></p><p><strong>Patrick Butlin</strong> is a philosopher of mind and cognitive science and a Research Fellow at the Future of Humanity Institute at the University of Oxford. His current research is on consciousness, agency and other mental capacities and attributes in AI.</p><p><strong>Robert Long</strong> is a Research Affiliate at the Center for AI Safety. He recently completed his PhD in philosophy at New York University, during which he also worked as a Research Fellow at the Future of Humanity Institute. He works on issues related to possible AI consciousness and sentience.</p><p><strong>Yoshua Bengio</strong> is recognized worldwide as one of the leading experts in artificial intelligence, known for his conceptual and engineering breakthroughs in artificial neural networks and deep learning. He is a Full Professor in the Department of Computer Science and Operations Research at Universite\u0301 de Montre\u0301al and the Founder and Scientific Director of Mila \u2013 Quebec AI Institute, one of the world\u2019s largest academic institutes in deep learning. He is also the Scientific Director of IVADO. His scientific contributions have earned him numerous awards. He is the 2018 laureate of the A.M. Turing Award, \u201cthe Nobel Prize of Computing,\u201d alongside Geoffrey Hinton and Yann LeCun for their important contributions and advances in deep learning. In 2022, he was appointed Knight of the Legion of Honor by France and named co-laureate of Spain\u2019s Princess of Asturias Award for technical and scientific research. Later that year, Professor Bengio became the most cited computer scientist in the world in terms of h-index. He is a Fellow of the Royal Society of London and of Canada, an Officer of the Order of Canada and a Canadian CIFAR AI Chair.</p><p><strong>Grace Lindsay</strong> is currently an Assistant Professor of Psychology and Data Science at New York University. After a BS in neuroscience from the University of Pittsburgh and a year at the Bernstein Center for Computational Neuroscience in Freiburg, Germany, Grace got her PhD at the Center for Theoretical Neuroscience at Columbia University in the lab of Ken Miller. Following that, she was a Sainsbury Wellcome Centre/Gatsby Computational Neuroscience Unit Research Fellow at University College London.</p><p><strong>Further information</strong></p><p>The <a href=\"https://sites.google.com/nyu.edu/mindethicspolicy/home\">NYU Mind, Ethics, and Policy Program</a> is dedicated to advancing understanding of the consciousness, sentience, sapience, and moral, legal, and political status of nonhumans, including animals and artificial intelligences. We believe that all beings with the capacity to suffer deserve to be treated with respect and compassion, and that our policies and practices should reflect this. Our goal is to promote research and scholarship that will help to shape a more just and humane world for all.*<br><i>*This program description was co-authored by GPT-3</i><br><br>If you are not currently on our email list and would like to join, you can <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdKJxyoG1jaGixYkXfNOd_1YT45EEq4qyh0Yk54tfPvCiY-Yw/viewform\">sign up here</a>!<br><br><i>Thank you to the NYU Center for Bioethics and the NYU Center for Mind, Brain, and Consciousness for their generous support of this event.</i></p>", "user": {"username": "Sofia_Fogel"}}, {"_id": "xu45Sq8gZ4iy9iHXa", "title": "Nuclear safety/security: Why doesn't EA prioritize it more?", "postedAt": "2023-08-30T21:43:26.985Z", "htmlBody": "<p>I'm dissatisfied with my explanation of why there is not more attention from EAs and EA funders on nuclear safety and security, especially relative to e.g. AI safety and biosecurity. This has come up a lot recently, especially after the release of <i>Oppenheimer</i>. I'm worried I'm not capturing the current state of affairs accurately and consequently not facilitating fully contextualized dialogue.</p><p>What is your best short explanation?</p><p>(To be clear, I know many EAs and EA funders <i>are</i> working on nuclear safety and security, so this is more so a question of resource allocation, rather than inclusion in the broader EA cause portfolio.)</p>", "user": {"username": "Rockwell Schwartz"}}, {"_id": "ErQdvzA9qqRA2gFBL", "title": "Report on Frontier Model Training", "postedAt": "2023-08-30T20:04:31.392Z", "htmlBody": "<p>Understanding what drives the rising capabilities of AI is important for those who work to forecast, regulate, or ensure the safety of AI. Regulations on the export of powerful GPUs need to be informed by understanding of how these GPUs are used, forecasts need to be informed by bottlenecks, and safety needs to be informed by an understanding of how the models of the future might be trained. A clearer understanding would enable policy makers to target regulations in such a way that they are difficult for companies to <a href=\"https://www.tomshardware.com/news/nvidia-gimps-h100-hopper-gpu-to-sell-as-h800-to-china\"><u>circumvent with only technically compliant GPUs</u></a>, forecasters to avoid focus on <a href=\"https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.g5d5897qnsfc\"><u>unreliable metrics</u></a>, and technical research working on mitigating the downsides of AI to understand <a href=\"https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.apsafl4txsk7\"><u>what data models might be trained on</u></a>. &nbsp;</p><p>This doc is built from a collection of smaller docs I wrote on a bunch of different aspects of frontier model training I consider important. I hope for people to be able to use this document as a collection of resources, to draw from it the information they find important and inform their own models.</p><p>I do not expect this doc to have a substantial impact on any serious AI labs capabilities efforts - I think my conclusions are largely discoverable in the process of attempting to scale AIs or for substantially less money than a serious such attempt would cost. Additionally I expect major labs already know many of the things in this report.</p><h2>Acknowledgements</h2><p>I\u2019d like to thank the following people for their feedback, advice, and discussion:</p><ul><li>James Bradbury, Software Engineer, Google DeepMind</li><li>Benjamin Edelman, Ph.D. Candidate, Harvard University</li><li>Horace He, Software Engineer, PyTorch/Meta</li><li>Lukas Finnveden, Research Analyst, Open Philanthropy Project</li><li>Joanna Morningstar, Chief Scientific Officer, Nanotronics</li><li>Keller Scholl, Ph.D. Candidate, Pardee RAND Graduate School</li><li>Jaime Sevilla, Director, Epoch</li><li>Cody Wild, Research Engineer, Google</li></ul><h2>Index</h2><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.q4i6cuur5ejr\"><u>Cost Breakdown of ML Training</u></a></p><p>Estimates the costs of training a frontier (state of the art) model, drawing on leaks and analysis. Power usage is a small portion of the cost, GPUs are likely a slim majority.</p><p><img style=\"width:405.5px\" src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ErQdvzA9qqRA2gFBL/ch00iiacetryn21niltv\" alt=\"\"></p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.e1ayykcf3smu\"><u>Why ML GPUs Cost So Much</u></a></p><p>ML GPUs are expensive largely because of their communication and memory capabilities - not because of their processing power. NVIDIA\u2019s best gaming GPU provides greater ML processing power than the GPU used to train GPT-4, for only a tenth the price. Note that NVIDIA\u2019s near monopoly plausibly explains some of the price differential.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.g5d5897qnsfc\"><u>Contra FLOPs</u></a></p><p>Argues that the most common metric of ML computing power - floating point operations - is flawed, due to the rise of different types of floating point numbers making standardization difficult and the cost of processing power representing a small portion of the cost of ML.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.wdy8qwubysry\"><u>ML Parallelism</u></a></p><p>An overview of ML parallelism techniques, showing how the common notion that \u201cML is <a href=\"https://en.wikipedia.org/wiki/Embarrassingly_parallel\"><u>embarrassingly parallel</u></a>\u201d is simplistic and breaks down at large scales - where any simple method of parallelizing a model starts to hit bottlenecks as the capabilities of individual devices become bottlenecks regardless of the number of devices involved.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.apsafl4txsk7\"><u>We (Probably) Won\u2019t Run Out of Data</u></a></p><p>There are many routes toward preventing data from becoming a major bottleneck to ML scaling, though it\u2019s not certain any of them enable scaling as fast as has occurred historically.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.bsosdgecdqu2\"><u>AI Energy Use and Heat Signatures</u></a></p><p>ML energy usage may become important in the near future, even if it\u2019s a relatively minor concern for frontier model training right now. If current trends continue, energy usage could limit scaling, determine major engineering challenges, and provide a novel approach to surveillance of training runs using satellites and multispectral photography.</p><h1>Cost Breakdown of ML Training</h1><p>This section is an attempt to estimate the amount of costs associated with training a state of the art ML model, specifically in terms of the amount of capital that is required. It\u2019s not a detailed forecasting attempt, but instead is meant to serve as a default source for anyone who wants to know the basics - such as whether power usage is a major expense right now (it\u2019s not) or whether GPUs account for the majority of the cost (probably, but only a slim majority). I hope this helps people prioritize their research agendas and serves as a jumping off point.</p><p>Note that I focus specifically on what it takes to train a model that is competitive with the best models at the time of its release. As of now GPT-4 is the only publicly acknowledged model in this class and so I will pay special attention to it, though I also use leaks about forthcoming frontier models to augment my analysis<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrmbyn8uvh1\"><sup><a href=\"#fnrmbyn8uvh1\">[1]</a></sup></span>.</p><p><img style=\"width:624px\" src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ErQdvzA9qqRA2gFBL/peh8vj4pgjolb2qdsceo\" alt=\"\"></p><p>In this section I\u2019ll be breaking down what I think this money is actually being spent on, and how we might expect this to evolve over time. I\u2019m not going to be doing specific forecasting here, though I hope this can serve as a guideline to those who do. Additionally I hope this is useful for policy makers analyzing how to most effectively regulate training runs by demonstrating what parts of training are most expensive. A few major takeaways:</p><ul><li>New state of the art (frontier) models will likely cost on the order of a billion dollars, with the most recent frontier model, GPT-4, having cost approximately half a billion dollars.</li><li>The cost of a frontier model is (roughly) 80% supercomputer hardware, 18% personnel, and 2% power.</li><li>ML supercomputer hardware cost is split about 70/30 between GPUs and other hardware (mostly meant for helping GPUs communicate with each other rapidly).</li><li>The internal and external communication capabilities of ML GPUs account for the vast majority of their cost.</li></ul><h2>Defining \u201cCost\u201d</h2><p>I define the cost of a frontier model as the amount of money a company needs to spend to create a frontier model. In practice I use the following formula:</p><p><i>Cost = Hardware Cost + Operating Expenditures During Creation</i></p><p>Where \u201c<i>Hardware Cost</i>\u201d refers to the cost of purchasing all the hardware necessary for the training run, and the \u201c<i>Operating Expenditures During Creation</i>\u201d refers to the sum of the amount of money spent on energy, salaries, maintenance, and other operating expenses during the time the model is designed and trained (I think one year is a good upper bound on this time period).</p><p>Most analysts differ from me on this, using the cost of renting GPUs for the training run based on prices that require multi-year commitments that continue after the model has been created. These commitments effectively span the lifetime of the hardware, and so I believe are better thought of as mortgages than as rentals, as the end result of a mortgage is that the provider no longer owns an object of value, whereas the end result of renting for the same period is that the provider owns an item of similar value to what they started with.</p><p>Another thing to note here is that I do not factor in the cost of electricity beyond the period during which the model is created, so the lifetime electricity cost of a GPU would be a larger proportion of the cost of that GPU than the electricity cost I consider.</p><h2>Total Cost Estimate</h2><p>My preferred way to estimate the total cost of a frontier model is to base the estimate on public information on spending and investment, using a bottom-up component analysis to sanity check the estimate. Based on this approach, I estimate that GPT-4 cost half a billion dollars and near future frontier models will cost on the order of a billion. This is based on the following evidence:</p><ul><li><a href=\"https://fortune.com/longform/chatgpt-openai-sam-altman-microsoft/\"><u>Leaks</u></a>&nbsp;indicate OpenAI spent over 400 million on compute and data in 2022, the year GPT-4 was trained.</li><li>Anthropic is <a href=\"https://techcrunch.com/2023/04/06/anthropics-5b-4-year-plan-to-take-on-openai/\"><u>reported</u></a>&nbsp;to have said while fundraising that training the next version of Claude would cost 1 billion dollars, for an amount of computation that would be similar to the amount used to train GPT-4,&nbsp;as estimated by <a href=\"https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp%3Dsharing\"><u>Epoch</u></a>&nbsp;and <a href=\"https://www.semianalysis.com/p/gpt-4-architecture-infrastructure\"><u>Semianalysis</u></a>.</li><li>OpenAI <a href=\"https://www.crunchbase.com/organization/openai/company_financials\"><u>raised</u></a>&nbsp;10 billion in 2023 despite having already raised 1 billion in 2019, indicating significant expenditures. &nbsp;</li><li>Inflection AI <a href=\"https://inflection.ai/inflection-ai-announces-1-3-billion-of-funding\"><u>raised</u></a>&nbsp;about 1.5 billion dollars, and has likely spent much of it on a <a href=\"https://www.nextplatform.com/2023/07/05/the-1-billion-and-higher-ante-to-play-the-ai-game/\"><u>supercomputer</u></a>&nbsp;capable of training a frontier model.</li><li>Over the past three years NVIDIA\u2019s <a href=\"https://www.nextplatform.com/2022/05/26/datacenter-becomes-nvidias-largest-business/%23:~:text%3DDuring%2520fiscal%2520Q1%252C%2520Nvidia%27s%2520datacenter,divisions%2520will%2520jockey%2520for%2520position\"><u>data center revenue has increased dramatically</u></a>, probably from ML GPU sales, from under a billion dollars per quarter to over 3.5 billion, which seems reasonably compatible with a few labs building supercomputers that cost on the order of a billion dollars.</li></ul><p>Note that these numbers are much higher than than the approx 60 million dollars<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefd0rt75hxgpk\"><sup><a href=\"#fnd0rt75hxgpk\">[2]</a></sup></span>&nbsp;it would cost to rent all the hardware required for the duration of the final training run of GPT-4 if one were willing to commit to renting the hardware for a duration much longer than training, as is likely common for large AI labs. I think that the methodology I use better tracks the amount of investment needed to produce a frontier model. As a sanity check, rough math gives a 500 million dollar estimate for the cost of the hardware needed to train GPT-4<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrm1nwbs43oj\"><sup><a href=\"#fnrm1nwbs43oj\">[3]</a></sup></span>, which lines up well with the empirical evidence of spending and investment by OpenAI.</p><h2>Granular Analysis</h2><p>In order to break down the cost of a training run into individual components, we\u2019ll need to go beyond the bottom line spending numbers and utilize more detailed information about how the training run worked, and what costs it likely involved. This necessarily requires a greater number of assumptions and uncertainties but there\u2019s enough information available to make reasonable guesses about the relative costs of various components of a frontier model.</p><p>I started estimating the proportions of the costs of training an ML model like GPT-4 with <a href=\"https://fortune.com/longform/chatgpt-openai-sam-altman-microsoft/\"><u>this leaked info </u></a>on OpenAI\u2019s spending in 2022:</p><p>\u201c[OpenAI] was projecting expenses [for 2022] of $416.45 million on computing and data, $89.31 million on staff, and $38.75 million in unspecified other operating expenses.\u201d</p><p>While these numbers aren\u2019t specifically about GPT-4, I consider it a reasonable baseline for the proportions of the cost. Other possible costs, such as providing ChatGPT for free, would have been <a href=\"https://www.semianalysis.com/p/the-inference-cost-of-search-disruption\"><u>much smaller</u></a>. I removed the \u201cunspecified\u201d section since it was small and I do not see an obvious means for it to be relevant to GPT-4.</p><p>Next, I broke down the computing and data number. I removed data entirely since I doubt data was a significant factor in costs, with this <a href=\"https://time.com/6247678/openai-chatgpt-kenya-workers/\"><u>article</u></a>&nbsp;mentioning one plausible data expense being only $200k<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref56xwlsbuvil\"><sup><a href=\"#fn56xwlsbuvil\">[4]</a></sup></span>. This <a href=\"https://lambdalabs.com/blog/hyperplane-16-infiniband-cluster-total-cost-of-ownership%23:~:text%3DThe%2520default%2520system%2520price%2520used,specifications%2520of%2520the%2520DGX%252D2.\"><u>post</u></a>&nbsp;suggests that computing could be broken down into hardware, power, and system administration. System admin seemed small enough that I ignored it entirely. I broke out power using about <a href=\"https://www.eia.gov/electricity/monthly/epm_table_grapher.php?t%3Depmt_5_6_a\"><u>$0.05/kWh</u></a>&nbsp;as a base price for cheap power and estimating power draw using a variety sources, each giving me fairly similar answers<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwl49l3n5i4\"><sup><a href=\"#fnwl49l3n5i4\">[5]</a></sup></span>.</p><p>Note that other accounting methodologies&nbsp;focused on the cost of renting hardware imply that operating costs such as power usage costs are a higher portion of the cost than the methodology I use implies. This is because operating costs are similar across these methodologies (the operating expenses for the period of time during use) but the renting methodology provides a lower total cost.&nbsp;The total cost of owning the hardware across the lifetime of the hardware has similarly higher proportional operating costs.</p><p><img style=\"width:423.25px\" src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ErQdvzA9qqRA2gFBL/a5fyncccrfezujernzdc\" alt=\"\"></p><h3>Hardware Cost Breakdown</h3><p>With hardware (for an ML supercomputer) making up such a significant portion of the frontier model cost, it\u2019s worthwhile to break it down further. My best guess is that in the best GPU-based ML supercomputers right now, GPUs account for around 70% of the cost, though I consider anywhere from 50% to 85% to be plausible. Most of the remainder of the cost is networking hardware to allow fast and robust communication between GPUs.</p><p>I arrived at my 70% estimate by combining two different methods:</p><ol><li>Next Platform\u2019s <a href=\"https://www.nextplatform.com/2023/07/05/the-1-billion-and-higher-ante-to-play-the-ai-game/\"><u>article</u></a>&nbsp;on Inflection AI\u2019s new supercomputer estimates 50% of the cost is GPUs. The article also indicates that interconnect between nodes accounts for around 20% of the cost of a supercomputer. My main issue with the overall estimates is that the non-GPU components within individual nodes seem too expensive, especially compared to other more reliable estimates of the costs of similar nodes. I think this is because of the standards of non-ML supercomputers that Next Platform is more used to</li><li>Semianalysis <a href=\"https://www.semianalysis.com/p/ai-server-cost-analysis-memory-is\"><u>breaks down</u></a>&nbsp;the cost of a single H100 node, suggesting that non-GPU components were 14% of the cost (excluding Nvidia\u2019s markup). Note that this doesn\u2019t include the non-node costs of a supercomputer.</li></ol><p>Combining the node cost breakdown from Semianalysis and the 20% interconnect rule of thumb from Next Platform, we find that GPUs are 70% of the hardware cost<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsb9esyy47pj\"><sup><a href=\"#fnsb9esyy47pj\">[6]</a></sup></span>.</p><p>Note that this number can fluctuate over time - for supercomputers with more GPUs the cost of coordinating the GPUs and their communication with each other grows substantially, as well as the necessity of robustness to GPU failures. As a result, larger clusters have to spend more on hardware to enable fast and robust communication between devices as well as hardware for checkpointing of intermediate results.</p><p>The reason why communication is so expensive at this scale is a bit subtle, but it\u2019s related to the fact that as one increases the number of devices in a network the number of possible connections between devices grows quadratically. This can be dealt with using sophisticated routing techniques, but those techniques still grow in complexity and cost as the number devices increases.</p><h3>GPU Costs</h3><p><i>This is discussed in greater detail in </i><a href=\"https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.e1ayykcf3smu\"><i><u>Why ML GPUs Cost So Much</u></i></a><i>.</i></p><p>Delving further into the hardware costs, I\u2019d like to briefly remark on the cost of ML GPUs. A lot of the cost of ML GPUs is a result of the communication capabilities they require, similar to the importance of communication for non-GPU hardware. Memory plays a significant role as well, both the communication aspects of memory (memory bandwidth within a single chip) and the total quantity of high bandwidth memory on each chip. Additionally, scarcity and NVIDIA\u2019s dominance in ML GPUs may be significant factors driving prices. This could change in the next few years, resulting in a drop in GPU prices as competition and production increases.</p><h1>Why ML GPUs Cost So Much</h1><p>ML GPUs make up the single largest component of frontier model training, so it\u2019s useful to understand why they cost as much as they do. Often analysis focuses on the processing power (measured in operations per second) provided by ML GPUs like the ~$15,000 A100 GPU likely used to train GPT-4. However, NVIDIA\u2019s best gaming GPU provides greater ML processing power for about a tenth the price of the A100. The main factors distinguishing the best GPUs for ML from other devices are the exceptional memory and communication capabilities of state of the art ML GPUs.</p><p>Before we get into how relevant different aspects of ML GPUs are to price, a quick overview of the specs of an ML GPU:</p><ul><li>Cost: How much a GPU costs, often secret for ML GPUs but we have reasonable estimates based on leaked info and expert analysis.</li><li>ML Processing Power: The amount of operations a GPU can do per second, using whichever data type is best for ML training. Typically measured in trillions of floating point operations per second (TFLOP/s) or the more general trillions of operations per second (TOP/s) when non-floating point data types are usd.</li><li>Memory Size: How much high-speed memory a GPU has. Measured in billions of bytes (GB). Note that a much smaller amount of even faster memory exists in GPUs as well, called the cache.</li><li>Memory Bandwidth: How many bytes per second can be read and written to the memory by a GPU. Measured in trillions of bytes a second (TB/s).</li><li>Interconnect Bandwidth: How many bytes per second can be communicated between the GPU and other external devices, including other GPUs and CPUs and such. There are a variety of types of interconnect but I won\u2019t be getting into that here. Typically measured in hundreds of gigabytes a second (100s GB/s).</li><li>A note on interconnect bandwidth: This is really what separates GPUs for supercomputing and ML from consumer GPUs, as this is what is needed to connect massive amounts of GPUs together.</li><li>Energy Usage: Measured in watts, how much energy a GPU uses.</li></ul><p>As an illustrative example of the importance of these properties, the table below compares the GPU used to train GPT-4, the A100 and the H100 (a state of the art ML GPU and successor to the A100), with the RTX 4090 (NVIDIA\u2019s best gaming GPU). Note that there are different types of FLOPs (or TOPs); here I used the ones best for training, but also include the numbers the A100 would likely have had if it had the features the newer GPUs have.</p><p>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><a href=\"https://www.nvidia.com/en-us/data-center/a100/\"><i><u>NVIDIA A100 80GB SXM</u></i></a></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><a href=\"https://resources.nvidia.com/en-us-tensor-core\"><i><u>NVIDIA H100 SXM</u></i></a></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><a href=\"https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf\"><i><u>NVIDIA GeForce RTX 4090</u></i></a></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><i>Cost</i></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">$15,000<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreff0g8sy1qnqu\"><sup><a href=\"#fnf0g8sy1qnqu\">[7]</a></sup></span></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">$24,000<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxyhf7crrt6n\"><sup><a href=\"#fnxyhf7crrt6n\">[8]</a></sup></span></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">$1,599<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnl2cjsdxeq8\"><sup><a href=\"#fnnl2cjsdxeq8\">[9]</a></sup></span></td></tr><tr><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><i>ML Processing Power</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbbcgu23aohk\"><sup><a href=\"#fnbbcgu23aohk\">[10]</a></sup></span></td><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><p>624 TOP/s</p><p>(312 TFLOP/s if int8 training isn\u2019t possible)</p></td><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">1978.9 TFLOP/s</td><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">660.6 TFLOP/s</td></tr><tr><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><i>Cost/TFLOP/s (or TOP/s)</i></td><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><p>$24.04</p><p>($48.08 if int8 training isn\u2019t possible)</p></td><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">$12.13</td><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">$2.42</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><i><strong>Memory Size</strong></i></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><strong>80 GB</strong></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><strong>80 GB</strong></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><strong>24 GB</strong></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><i><strong>Memory Bandwidth</strong></i></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><strong>2.04 TB/s</strong></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><strong>3.35 TB/s</strong></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><strong>1.08 TB/s</strong></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><i><strong>Interconnect Bandwidth</strong></i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8ixdei1ng95\"><sup><a href=\"#fn8ixdei1ng95\">[11]</a></sup></span></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><strong>600 GB/s</strong></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><strong>900 GB/s</strong></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><strong>64 GB/s</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvcpgehhqddh\"><sup><a href=\"#fnvcpgehhqddh\">[12]</a></sup></span></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><i>Energy Usage</i></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">500W<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5z18vmogty4\"><sup><a href=\"#fn5z18vmogty4\">[13]</a></sup></span></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">700W</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">450W</td></tr></tbody></table></figure><p>&nbsp;</p><p>In the above table, ML processing power explains little of the price differential, same with energy usage. I bolded memory size, memory bandwidth, and interconnect bandwidth as they better track price. Memory size and memory bandwidth have a joint impact on price - faster memory costs more per byte<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefaqymdl25eyp\"><sup><a href=\"#fnaqymdl25eyp\">[14]</a></sup></span>.</p><p>I\u2019d like to note the importance of the two specifications related to the communication of data: memory bandwidth and interconnect bandwidth. These specs may become more important over time, as processing power provided by hardware grows faster than memory or interconnect bandwidth. This is in part because the process of making smaller and smaller transistors powers the growth of processing power and memory but not bandwidth. As a result of this, it is unclear whether bandwidth scaling will run out when Moore\u2019s law runs out, as the components relevant to communication are often substantially larger than those relevant to compute and memory and the bottlenecks are different, meaning that they may hit physical limitations later.</p><p>For an illustration of growth over time, see the chart below, where HW FLOPS refers to processing power, DRAM BW refers to memory bandwidth, and Interconnect<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0u7gfkpexgdl\"><sup><a href=\"#fn0u7gfkpexgdl\">[15]</a></sup></span>&nbsp;BW refers to communication speed between GPUs. <img style=\"width:624px\" src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ErQdvzA9qqRA2gFBL/gosk55dhv6zr4dezlzjj\" alt=\"\"></p><p>Source: <a href=\"https://medium.com/riselab/ai-and-memory-wall-2cb4265cb0b8\"><u>https://medium.com/riselab/ai-and-memory-wall-2cb4265cb0b8</u></a>&nbsp;</p><p>Note that the amount of communication needed per FLOP isn\u2019t necessarily constant, so this difference in growth won\u2019t <i>inevitably</i>&nbsp;result in interconnect being a bottleneck - it\u2019s just useful for understanding how communication is something that needs to be dealt with and optimized around.</p><h3>NVIDIA\u2019s Monopoly</h3><p>NVIDIA has plausibly had a monopoly on the best GPUs with the H100 surpassing competitors, though Google may be a serious competitor<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefson4cea04a\"><sup><a href=\"#fnson4cea04a\">[16]</a></sup></span>. Some have reported that they charge <a href=\"https://www.semianalysis.com/p/amd-mi300-taming-the-hype-ai-performance?utm_source%3D%252Fsearch%252F5x%26utm_medium%3Dreader2\"><u>very high margins</u></a>&nbsp;(5x manufacturing costs) on their GPUs as a result, though I think it plausible that this is mostly due to high demand given their supply,&nbsp;and that they are increasing supply rapidly in order to meet demand. My best guess is that prices may decrease due to increased competition and supply in the next few years, but memory and bandwidth will still be a bigger factor than flops, at least in the near future.</p><h1>Contra FLOPs</h1><p>Floating point operations (FLOPs) are often used as a metric for processing power in ML - to forecast the future of AI<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref69cgkl2sgjw\"><sup><a href=\"#fn69cgkl2sgjw\">[17]</a></sup></span><sup>,&nbsp;</sup><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9c75rdi94mr\"><sup><a href=\"#fn9c75rdi94mr\">[18]</a></sup></span>&nbsp;and to regulate the export of powerful hardware in the present. However,&nbsp;I believe that recent developments in ML hardware and scaling have rendered FLOPs ill-defined and less reliable a metric in the present environment than many assume.</p><h2>Different Types of FLOPs</h2><p>Floating point numbers are represented internally as a sequence of bits. Traditionally, 32 bits were used as a standard - Single-Precision Floating-Point numbers (FP32 - the 32 standing for 32 bits). This format is the most common one discussed, with double-precision (FP64) being the next most common. However, over the past several years progress has been made on utilizing fewer bits per number (also called lower precision) representations in machine learning. On the best ML hardware, optimizations including this can lead to a 30x difference in processing power between traditional Single-Precision FLOPs and the most ML training optimized operations<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefndcrdznwun\"><sup><a href=\"#fnndcrdznwun\">[19]</a></sup></span>.</p><p>It is unclear to me whether FLOPs will become substantially more specialized than they are now. The precision of the floating point numbers can only go down so far, and the required experimentation and the changing architectures of frontier (state of the art) models make extreme specialization of hardware potentially difficult on the timescales involved.</p><h2>FLOP Costs Are Not Everything</h2><p>While FLOPs are often the focus of analysis of ML training runs, they are not the sole resources that must be optimized around.</p><p>GPT-4 Training Orders of Magnitude<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref00h09oj63h4st\"><sup><a href=\"#fn00h09oj63h4st\">[20]</a></sup></span></p><p>See footnotes for details.</p><figure class=\"table\"><table><tbody><tr><td style=\"background-color:rgb(217, 217, 217);border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">Training data could be stored on...</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">100 Macbooks<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreft7d8wacuxei\"><sup><a href=\"#fnt7d8wacuxei\">[21]</a></sup></span></td></tr><tr><td style=\"background-color:rgb(217, 217, 217);border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">Processing power (FLOPs) would take 3 months for..</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><p>250,000 PlayStation Fives<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefuzhrd3ie3we\"><sup><a href=\"#fnuzhrd3ie3we\">[22]</a></sup></span>&nbsp;</p><p>(&lt; 1% of all PS5s)</p></td></tr><tr><td style=\"background-color:rgb(217, 217, 217);border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">Communication between devices during training was...</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&gt; All Internet Traffic in 2022<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefiu5c6ot031\"><sup><a href=\"#fniu5c6ot031\">[23]</a></sup></span></td></tr></tbody></table></figure><p>&nbsp;</p><p>This can most straightforwardly be seen by comparing the cost per FLOP of ML GPUs with that of some non-ML GPUs<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsbs3cs766g\"><sup><a href=\"#fnsbs3cs766g\">[24]</a></sup></span>. For instance, the GPU likely used to train GPT-4, NVIDIA\u2019s A100, is ~10x the cost per flop of NVIDIA\u2019s latest gaming GPU. The table below compares the A100 and the H100 (a state of the art ML GPU and successor to the A100), with the RTX 4090 (NVIDIA\u2019s best gaming GPU).</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><a href=\"https://www.nvidia.com/en-us/data-center/a100/\"><i><u>NVIDIA A100 80GB SXM</u></i></a></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><a href=\"https://resources.nvidia.com/en-us-tensor-core\"><i><u>NVIDIA H100 SXM</u></i></a></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><a href=\"https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf\"><i><u>NVIDIA GeForce RTX 4090</u></i></a></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><i>Cost</i></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">$15,000<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreft9105vc8re\"><sup><a href=\"#fnt9105vc8re\">[25]</a></sup></span></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">$24,000<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefk11taq24d9k\"><sup><a href=\"#fnk11taq24d9k\">[26]</a></sup></span></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">$1,599<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftsmoejw53p8\"><sup><a href=\"#fntsmoejw53p8\">[27]</a></sup></span></td></tr><tr><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><i>ML Processing Power</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffgjsk8yfmo8\"><sup><a href=\"#fnfgjsk8yfmo8\">[28]</a></sup></span></td><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><p>624 TFLOP/s equiv<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefu98quxwq2r\"><sup><a href=\"#fnu98quxwq2r\">[29]</a></sup></span></p><p>(312 TFLOP/s if int8 training isn\u2019t possible)</p></td><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">1978.9 TFLOP/s</td><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">660.6 TFLOP/s</td></tr><tr><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><i>Cost/TFLOP/s</i></td><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><p>$24.04</p><p>($48.08 if int8 training isn\u2019t possible)</p></td><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">$12.13</td><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">$2.42</td></tr></tbody></table></figure><p>&nbsp;</p><p>I discuss more about what actually drives the cost of these GPUs in <a href=\"https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.e1ayykcf3smu\"><u>Why ML GPUs Cost So Much</u></a>.</p><p>In addition to the individual chips, there are other significant costs in ML training runs - <a href=\"https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.iwhjtdndhoxw\"><u>my best guess</u></a>&nbsp;is that GPUs account for only 70% of the cost of the hardware needed for a frontier model training run right now, with networking equipment making up most of the remaining 30%. As the size of ML supercomputers grows further the non-GPU costs may outpace GPU costs, due to problems of scale.</p><h2>Possible Solutions</h2><p>The US <a href=\"https://www.federalregister.gov/documents/2022/10/13/2022-21658/implementation-of-additional-export-controls-certain-advanced-computing-and-semiconductor\"><u>ban</u></a>&nbsp;on the export of advanced ML chips to China uses a means of normalizing FLOPs based on the lengths of the values involved. It is my understanding that in practice it amounts to treating all types of FLOPs that are used for training basically the same, and some FLOPs used for inference as worth a fraction of a training FLOP.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefg3gd5u01uhc\"><sup><a href=\"#fng3gd5u01uhc\">[30]</a></sup></span>&nbsp;In practice, <a href=\"https://www.tomshardware.com/news/nvidia-gimps-h100-hopper-gpu-to-sell-as-h800-to-china\"><u>attempts</u></a> <a href=\"https://www.semianalysis.com/p/how-chinas-biren-is-attempting-to\"><u>to circumvent</u></a>&nbsp;the ban have focused on another, communication based, requirement.</p><p>In general it can be difficult to figure out what sorts of FLOPs are usable for training frontier models - there are a wide variety of types of FLOPs and their usability can change rapidly - as recently as 2022 it was not publicly known how to train GPT-3 sized models with traditional FP16 precision FLOPs, but <a href=\"https://arxiv.org/abs/2209.05433\"><u>recent progress</u></a>&nbsp;has been made on training models of that size with only FP8.</p><p>Another way to address issues around specialization would be to search for metrics based on more fundamental and stable aspects of hardware. Examples of these sorts of metrics might be transistor-hours, energy usage, or single bit logical operations. These metrics have the advantage of having a large amount of historical data that can be analyzed for forecasting. The disadvantage is that separate work would need to be done to contend with how specialization interacts with this.</p><p>I\u2019m not fully satisfied with any of these approaches at the moment, and think this is an open question with important implications for forecasting and regulation.</p><h1>ML Parallelism</h1><p><i>Further reading: Throughout I\u2019ve linked potential resources, and additionally here are some higher level overviews of ML parallelism from </i><a href=\"https://openai.com/research/techniques-for-training-large-neural-networks\"><i><u>OpenAI</u></i></a><i>, </i><a href=\"https://huggingface.co/blog/bloom-megatron-deepspeed\"><i><u>HuggingFace</u></i></a><i>, and </i><a href=\"https://fathomradiant.co/posts/Deep-Learning-with-Trillions-of-Parameters:-The-Interconnect-Challenge\"><i><u>Fathom Radiant</u></i></a><i>.</i></p><p>The common notion that \u201cML is <a href=\"https://en.wikipedia.org/wiki/Embarrassingly_parallel\"><u>embarrassingly parallel</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefr0byd4g32b\"><sup><a href=\"#fnr0byd4g32b\">[31]</a></sup></span>\u201d is simplistic and breaks down at large scales - where any simple method of parallelizing a model starts to hit bottlenecks as the capabilities of individual devices become bottlenecks regardless of the number of devices involved. This section gives an overview of parallelism methods, including commentary of the various bottlenecks they hit.</p><p>There are a few different methods to parallelize training across an increasingly large number of devices. I\u2019m going to lump them into two categories: vertical and horizontal parallelism. Other sources split things differently, but I think my approach is easier to understand and better captures the fundamentals of parallelism in ML.</p><h2>Vertical Parallelism</h2><p><strong>Vertical parallelism</strong>&nbsp;scales by adding devices in a similar way to how an assembly line can scale by adding workers, where each worker in the line passes an item that is being produced to the next worker. While this may not speed up production of a single item much, by having multiple items on the assembly line at once you can increase the overall throughput.</p><figure class=\"image image_resized\" style=\"width:135.5px\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ErQdvzA9qqRA2gFBL/p2ogethvkjthn5n1dimw\" alt=\"\"></figure><p>Similarly, <strong>vertical parallelism</strong>&nbsp;works by increasing the number of consecutive devices data flows through during training. Done naively this results in decreasing device utilization as once data passes through a given device the device will idle. In order to avoid this multiple inputs can be run through this pipeline simultaneously assembly line-style, relying on each stage in the pipeline for only a portion of the work necessary for each input. This doesn\u2019t substantially increase the speed of updates past a certain point, but it can allow the scaling of batch size without huge slow downs. <strong>Vertical parallelism is important, but cannot be applied to all of the challenges of scaling, as it is not very useful for increasing update speed.</strong></p><p>Two examples of vertical parallelism:</p><ol><li><a href=\"https://arxiv.org/pdf/1811.06965.pdf\"><u>Pipeline model parallelism</u></a>&nbsp;refers to partitioning the layers of a model across different devices, with each device being responsible for computing a few layers and passing the results onward<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqystno6xn\"><sup><a href=\"#fnqystno6xn\">[32]</a></sup></span>. This technique can also help with per-device memory requirements, as each device only needs access to a few layers of the overall model.</li><li><a href=\"https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da\"><u>Ring-AllReduce</u></a>&nbsp;accomplishes tasks like summing up a set of numbers spread across devices by having an initial device send its value to the next device, which sums the two together and passes it on to the next in line. By having multiple sums in progress at the same time it is possible to increase the overall throughput.&nbsp;This can be used to accomplish things like data or tensor parallelism without the drawbacks of horizontal parallelism.</li></ol><p>Vertical parallelism hits a few limitations as you scale - the number of layers grows fairly slowly and so pipeline model parallelism is only so, and the need for large amounts of inter device communication between a long chain of devices can prove challenging to overcome as often devices only have very fast communication within a small closed neighborhood<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3aalzvfrdzx\"><sup><a href=\"#fn3aalzvfrdzx\">[33]</a></sup></span>.</p><h2>Horizontal Parallelism</h2><p><strong>Horizontal parallelism</strong>&nbsp;is similar to having workers assemble an item simultaneously, typically by breaking the item into parts, having each worker work on a single part, and then having all the workers come together to assemble the final item. This speeds up the completion of the item but as the number of workers increases the task of coordinating such a large group grows increasingly difficult - think of tens of workers all trying to squeeze together to put their pieces of the item together.<img style=\"width:407.5px\" src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ErQdvzA9qqRA2gFBL/vm0tnplcd7wnhocqgzi7\" alt=\"\"></p><p>More concretely, horizontal parallelism is increasing the number of devices that work simultaneously on components of a larger task. This allows updates to the model to be done faster, as the execution of the model itself will be faster. This also doesn\u2019t require more memory. However, <strong>horizontal parallelism has increasingly large communication costs, as more and more devices need to communicate with each other as you scale this method</strong>.</p><p>A few examples of horizontal parallelism:</p><ol><li>Data Parallelism splits a batch into subsections and distributes them among devices. Once the devices have evaluated their subsections, the results are aggregated and then broadcast out to all the workers involved in order to sync up the weights. Note that the aggregation and broadcast phases can be implemented with vertical parallelism via Ring-AllReduce or via horizontal parallelism with Tree-AllReduce.</li><li><a href=\"https://colossalai.org/docs/features/1D_tensor_parallel\"><u>Tensor Parallelism</u></a>&nbsp;splits neurons within a layer of a model among different devices. This sort of division can have very high communication costs due to the density of models requiring lots of dependencies between neurons that are spread across many devices. However, <strong>tensor parallelism is very important for increasing the speed of an update as model size grows </strong>due to speeding up the computation of a single input to the model rather than just increasing throughput. It can also help with per-device memory requirements, as each device only needs access to a subset of the parameters of the overall model. Note that portions of this can be done with Ring-AllReduce, though you will lose the update speed improvements.</li><li><a href=\"https://developer.nvidia.com/blog/massively-scale-deep-learning-training-nccl-2-4/\"><u>Tree-AllReduce</u></a>&nbsp;accomplishes the same tasks as Ring-AllReduce except via a tree-based organization, where the aggregation is done in a series of stages, each doubling the amount of values aggregated by each node, to aggregate values in a logarithmic number of steps.</li></ol><p>In theory horizontal parallelism can scale fairly efficiently. However engineering challenges become increasingly difficult as more and more devices need to communicate with each other, unlike vertical parallelism where each device only needs to communicate with two other devices. Just like the difficulty of having a large number of workers crowd around a single item, having a large number of GPUs communicate with each other quickly about a single time can impose heavy costs - including in terms of physical space due to the proximity required for speedy communication.</p><h1>We (Probably) Won't Run Out of Data</h1><p>There is a reasonably large amount of attention paid to the rather steep data requirements of current ML methods, and to the possibility that data might \u201crun out\u201d and bottleneck scaling<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefljo9v3mgvzl\"><sup><a href=\"#fnljo9v3mgvzl\">[34]</a></sup></span>. <strong>My best guess is that data will not meaningfully run out, but I'm not certain.</strong>&nbsp;There were a range of opinions among experts I consulted though most did not expect it to be a major bottleneck. Substantial research has been done that could delay or prevent running out of data, including:</p><ol><li>Private data that already exists but is unused in language models provides a plentiful source of training data. This seems likely to generate significant controversy should it be attempted, though Google <a href=\"https://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html\"><u>has shown interest</u></a>&nbsp;in finding ways around some of the issues with technological solutions to remove private data from models after they have been trained.</li><li>Using more than just text data is becoming increasingly common: <a href=\"https://openai.com/research/gpt-4\"><u>GPT-4</u></a>&nbsp;was trained on images and Google Deepmind\u2019s upcoming model <a href=\"https://blog.google/technology/ai/google-palm-2-ai-large-language-model/\"><u>Gemini</u></a>&nbsp;will be multimodal as well.</li><li>Making better use of existing data can alleviate issues, for example by <a href=\"https://arxiv.org/abs/2205.10487\"><u>removing unintentionally repeated data</u></a>&nbsp;or <a href=\"https://arxiv.org/pdf/2305.16264.pdf\"><u>training for multiple epochs</u></a>, as GPT-4 is <a href=\"https://www.semianalysis.com/p/gpt-4-architecture-infrastructure\"><u>reported</u></a>&nbsp;to have done.</li><li>Solutions drawing on reinforcement learning to derive data from playing games and attempting challenges has shown some promise, with the multimodal model <a href=\"https://www.deepmind.com/blog/a-generalist-agent\"><u>Gato</u></a>&nbsp;from Deepmind using this alongside other forms of data.</li><li>Training via self-correction may be viable as models become more sophisticated. Google AI has done some <a href=\"https://arxiv.org/abs/2210.11610\"><u>work</u></a>&nbsp;on this, and Anthropic has used self-correction for alignment via <a href=\"https://arxiv.org/abs/2212.08073\"><u>Constitutional AI</u></a>.</li></ol><p>Also note that running out of data doesn\u2019t have to be binary - it\u2019s possible that these methods enable continued growth in model performance but at a slower rate than natural data would allow. Alternatively, they could allow faster growth in model performance - training via self-correction or reinforcement learning based approaches seem plausibly superior to me once models have achieved a minimal level of capabilities.</p><h1>AI Energy Use and Heat Signatures</h1><p>Energy use gets brought up a lot when discussing the recent AI explosion, despite the fact that it accounts for a very small fraction of <a href=\"https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.90ke0ggz3472\"><u>the cost of training a frontier model</u></a>, and a very small fraction of the US\u2019s energy usage<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkp2ci7kv7vr\"><sup><a href=\"#fnkp2ci7kv7vr\">[35]</a></sup></span>. Despite this, it turns out energy might actually be super important to the future of ML, limiting scaling, determining major engineering challenges, and providing a novel approach to surveillance of training runs. This is due in large part due to energy requirements growing and the required density of ML supercomputers.</p><h2>Growth</h2><p>So far, it seems unlikely that ML supercomputers have very different energy requirements from normal data centers used by the likes of Google - both requiring perhaps in the low tens of megawatts of power. This usage has remained consistent for supercomputers at least over the last 25 years - there hasn\u2019t been much change. Supercomputer performance has mostly come from increasing performance per watt, not increasing the number of watts used (see the chart in <a href=\"https://www.nextplatform.com/2023/07/10/lining-up-the-el-capitan-supercomputer-against-the-ai-upstarts/\">this article</a>). However it seems really likely that ML will drive a huge increase in the size of the individual supercomputers used for training them which, according to some but not all experts, could result in supercomputers within the next five years that require gigawatts of power (assuming on the order of a hundred billion dollars of spending on individual supercomputers, which is quite the extrapolation to make). This is similar to <a href=\"https://climate.cityofnewyork.us/subtopics/systems/%23:~:text%3DOn%2520average%252C%2520NYC%2520uses%2520about,as%2520much%2520as%252010%252C000%2520megawatts.\">the power usage of New York City</a>. The unavailability of this much power could play a significant role in ML in the near future,</p><h2>Density</h2><p>ML training runs require huge amounts of communication which becomes harder and requires more energy the farther devices are from each other. As a result, we should expect ML supercomputers to stay in a relatively small geographic area. Fitting cooling within a small volume will also be hard, and in general getting rid of heat is a non-trivial challenge.</p><h2>Heat Based Satellite Surveillance of ML Training Runs</h2><p>The amount of heat given off by this much energy usage is significant, concentrated in one place, and consistent over time due to training runs running 24/7 for months. As a result, you can probably see them in infrared satellite images even now, and in the near future they may be fairly distinguishable from basically anything else! This provides a novel way to do surveillance on ML training runs done around the world.</p><p><img style=\"width:236.5px\" src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/ErQdvzA9qqRA2gFBL/zggdycdeppgtk3vwt7jp\" alt=\"\"></p><p>Here\u2019s a satellite picture I found on <a href=\"https://app.skyfi.com/explore?aoi%3DPOLYGON%2528%2528-95.33736454265205%2B36.23292459154637%252C-95.3124846913423%2B36.23292459154637%252C-95.3124846913423%2B36.253075996331766%252C-95.33736454265205%2B36.253075996331766%252C-95.33736454265205%2B36.23292459154637%2529%2529\"><u>SkyFi</u></a>&nbsp;of a data center taken with multispectral photography with the datacenter circled. I\u2019ve adjusted the contrast and brightness. I\u2019m uncertain how much of the bright spot is due to IR and how much is due to the datacenter having a light gray roof - I\u2019d need to pay 250 dollars to get access to the higher res originals to get a better sense of what\u2019s going on here.</p><h2>Other Distinguishing Features</h2><p>There are a few things like some aluminum smelters which may use similar amounts of energy but factors like the lack of material input/output, may serve to distinguish ML supercomputers.</p><p>Similarly, bitcoin mining relies substantially on minimizing cost of energy to turn a profit, resulting in miners often using excess energy in locations rather than requiring a constant supply of energy for a long period of time as ML training does.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrmbyn8uvh1\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrmbyn8uvh1\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Google\u2019s Gemini may join this class when it is released, and Claude-Next as well.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnd0rt75hxgpk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefd0rt75hxgpk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp; <a href=\"https://www.semianalysis.com/p/gpt-4-architecture-infrastructure\"><u>Semianalysis</u></a>&nbsp;says 63 and <a href=\"https://epochai.org/trends\"><u>Epoch</u></a>&nbsp;says 40. I trust Semianalysis more on this.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrm1nwbs43oj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrm1nwbs43oj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I use the estimates I make in <a href=\"https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.iwhjtdndhoxw\"><u>ML Hardware Cost </u></a>as well as an assumption that there were 25k A100 GPUs, each costing approximately $15k.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn56xwlsbuvil\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref56xwlsbuvil\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;To a decent extent I\u2019m reasoning from the lack of evidence for significant data-associated costs. One possible way I could be wrong is if OpenAI were actually paying large amounts to license copyrighted data, but I have not seen any evidence of this. &nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwl49l3n5i4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwl49l3n5i4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I computed power draw costs as a percentage of the cost of an A100 (which I guessed at being 15k) and also tried using 25k A100s as Semianalysis reports, treated them as running for the full year to account for experiments, and used <a href=\"https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf\"><u>this</u></a>, <a href=\"https://www.colfax-intl.com/nvidia/nvidia-dgx-a100\"><u>this </u></a>and <a href=\"https://www.top500.org/system/180073/\"><u>this</u></a>&nbsp;to estimate power draw of the whole system.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsb9esyy47pj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsb9esyy47pj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Relevant math is 1-195,000/((269,010-42,000) * (5/4))</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnf0g8sy1qnqu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreff0g8sy1qnqu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See the estimate by Epoch <a href=\"https://docs.google.com/spreadsheets/d/1NoUOfzmnepzuysr9FFVfF7dp-67OcnUzJO-LxqIPwD0/edit%23gid%3D1503579905\"><u>here</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxyhf7crrt6n\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxyhf7crrt6n\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;This is my best guess, based on the cost of 8 H100s (and additional networking equipment) in <a href=\"https://www.semianalysis.com/p/ai-server-cost-analysis-memory-is\"><u>this breakdown</u></a>&nbsp;of the cost of &nbsp;DGX H100. I\u2019m very confident the cost is within the interval $10k-$45k.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnl2cjsdxeq8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnl2cjsdxeq8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See <a href=\"https://www.pcmag.com/news/nvidia-rtx-4090-will-cost-1599-rtx-4080-starts-at-899.\"><u>this article</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbbcgu23aohk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbbcgu23aohk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I use FP8 without sparsity, or int8 without sparsity for the A100 because it lacks FP8 and it seems <a href=\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_Towards_Unified_INT8_Training_for_Convolutional_Neural_Network_CVPR_2020_paper.pdf\"><u>plausible</u></a>&nbsp;int8 works for training.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8ixdei1ng95\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8ixdei1ng95\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I use whatever the fastest form of interconnect is.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvcpgehhqddh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvcpgehhqddh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See <a href=\"https://www.techpowerup.com/review/nvidia-geforce-rtx-4090-pci-express-scaling/\"><u>here</u></a>, sum up both directions to get the total.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5z18vmogty4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5z18vmogty4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See footnote in the <a href=\"https://www.nvidia.com/en-us/data-center/a100/\"><u>doc</u></a>&nbsp;about custom thermal solutions.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnaqymdl25eyp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefaqymdl25eyp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Based on <a href=\"https://ourworldindata.org/grapher/historical-cost-of-computer-memory-and-storage\"><u>this</u></a>, if the 80GB of memory of an H100 or A100 was standard computer memory it would cost only $164, and if it were hard drive storage it would cost $1.12, The difference in price between these forms of memory, and the one used GPUs, is due to the memory bandwidth - data can be read from and written to the memory of a GPU much much faster than it can for a hard disk. I think of this as a function of internal communication within the GPU, not as a function of the RAM.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0u7gfkpexgdl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0u7gfkpexgdl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Misspelled in the chart.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnson4cea04a\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefson4cea04a\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Google\u2019s new TPUv5e chip, and possible upcoming TPUv5 chips, may be comparable to the H100, but the TPUv5e was announced during the final stages of polishing this doc so I do not account for this.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn69cgkl2sgjw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref69cgkl2sgjw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See the <a href=\"https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines\"><u>Bio Anchors report</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9c75rdi94mr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9c75rdi94mr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See much of the work of <a href=\"https://epochai.org/\"><u>Epoch</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnndcrdznwun\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefndcrdznwun\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The SOTA ML GPU, NVIDIA\u2019s <a href=\"https://resources.nvidia.com/en-us-tensor-core\"><u>H100</u></a>, can perform non-sparse tensor flops of the smallest format (FP8) at 30x the speed of FP32.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn00h09oj63h4st\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref00h09oj63h4st\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Throughout I am using estimates made by <a href=\"https://epochai.org/trends\"><u>Epoch</u></a>&nbsp;and this article by <a href=\"https://www.semianalysis.com/p/gpt-4-architecture-infrastructure\"><u>Semianalysis</u></a>&nbsp;- both are good sources with very different specialities.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnt7d8wacuxei\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreft7d8wacuxei\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Epoch suggests 12-20 trillion tokens if GPT-4 involved as much compute as estimated&nbsp;and follows Chinchilla scaling laws, and Semianalysis reports 13 trillion but less than 7.5 trillion unique tokens. I use 13 trillion. Standard tokenization techniques would imply fewer than 2 bytes per token, which gives us 26TB of data. Cheapest MacBook Air M2 comes with 256GB of storage, giving us 104 which I round to 100. I ignore image data in this analysis, as less research has been done on this and GPT-4\u2019s image capabilities are as of yet unreleased.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnuzhrd3ie3we\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefuzhrd3ie3we\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Epoch gives ~2e25 FLOPs of compute used for training and Semianalysis agrees. The PlayStation 5 is <a href=\"https://www.theverge.com/21450334/playstation-5-ps5-sony-news-price-features-specs-hardware-games\"><u>reported</u></a>&nbsp;to have 10.28 TFLOP/s of compute which gives ~250,200 necessary to have enough compute in 3 months. Sony has <a href=\"https://www.theverge.com/2023/4/28/23702131/sony-ps5-q4-2022-console-sales-game-shipments\"><u>reportedly</u></a>&nbsp;sold at least 38.4 million PS5s, implying this is less than &lt;1% of PlayStations that have been sold.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fniu5c6ot031\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefiu5c6ot031\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The real number is quite likely higher. Semianalysis and Epoch give different numbers here, for my best guess I\u2019m using Semianalysis\u2019 numbers since they seem to have more information available and better match what I\u2019ve heard elsewhere. They give about 25k A100 GPUs, which, based on available interconnect and what Semianalysis suggests, would indicate the capability to have at least 5 petabytes a second of aggregate interconnect bandwidth. I think around half of this was likely used given ML parallelization techniques - giving maybe 2.5 petabytes a second. Over the course of a likely at least 90 day training run (estimated by Epoch and Semianalysis) we get a total of 19.4 zettabytes (1.94e22). Cisco estimated 3.5 zettabytes (3.5e21) for <a href=\"https://www.cisco.com/c/dam/m/en_us/solutions/service-provider/vni-forecast-highlights/pdf/Global_Device_Growth_Traffic_Profiles.pdf\"><u>2022 consumer internet traffic</u></a>&nbsp;(when the monthly amount is multiplied by 12x) in their 5 year projections as of 2017, I\u2019m having trouble finding more recent numbers.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsbs3cs766g\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsbs3cs766g\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;NVIDIA likely has much higher margins on ML GPUs, which means these numbers could very well change in the future as competition increases.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnt9105vc8re\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreft9105vc8re\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See the estimate by Epoch <a href=\"https://docs.google.com/spreadsheets/d/1NoUOfzmnepzuysr9FFVfF7dp-67OcnUzJO-LxqIPwD0/edit%23gid%3D1503579905\"><u>here</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnk11taq24d9k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefk11taq24d9k\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;This is my best guess, based on the cost of 8 H100s (and additional networking equipment) in <a href=\"https://www.semianalysis.com/p/ai-server-cost-analysis-memory-is\"><u>this breakdown</u></a>&nbsp;of the cost of &nbsp;DGX H100. I\u2019m very confident the cost is within the interval $10k-$45k.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntsmoejw53p8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftsmoejw53p8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See <a href=\"https://www.pcmag.com/news/nvidia-rtx-4090-will-cost-1599-rtx-4080-starts-at-899.\"><u>this article</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfgjsk8yfmo8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffgjsk8yfmo8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I use FP8 without sparsity, or int8 without sparsity for the A100 because it lacks FP8 and it seems plausible int8 works for training, see Appendix E of <a href=\"https://arxiv.org/pdf/2208.07339.pdf\"><u>this</u></a>&nbsp;and also <a href=\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_Towards_Unified_INT8_Training_for_Convolutional_Neural_Network_CVPR_2020_paper.pdf\"><u>this paper</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnu98quxwq2r\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefu98quxwq2r\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Technically int8 operations are not FLOPs, because int8 isn\u2019t a floating point data type, but are being assumed to be basically equivalent for the purposes of ML training.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fng3gd5u01uhc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefg3gd5u01uhc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The ban works by multiplying the amount of FLOPs of a given data type by the bit length of the data types involved. However, ML training typically requires a portion of each operation be done with FP32, even if the inputs are smaller data types and intermediate values are too, which results in the ban treating all such FLOPs the same.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnr0byd4g32b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefr0byd4g32b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Embarrassingly parallel is a term for computational tasks that can be easily divided into sub-tasks that can be accomplished by independent devices.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqystno6xn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqystno6xn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The need to do backpropagation for learning presents a challenge here, requiring forward passes to be followed by backward ones for computing what the model got wrong for each input. It\u2019s not trivial to schedule these passes in a way that prevents collisions and it inevitably results in some amount of idle time for the devices (referred to as a \u201cbubble\u201d). See the section on pipeline parallelism in this <a href=\"https://huggingface.co/blog/bloom-megatron-deepspeed\"><u>post</u></a>&nbsp;by Hugging Face to learn more.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3aalzvfrdzx\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3aalzvfrdzx\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Within node communication is generally much faster than other communication.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnljo9v3mgvzl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefljo9v3mgvzl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Epoch\u2019s <a href=\"https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-projecting-dataset\"><u>report</u></a>&nbsp;on this is a prominent example.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkp2ci7kv7vr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkp2ci7kv7vr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I got a quick upper bound of a fraction of a percent. A brief sketch of how I did this: multiplying <a href=\"https://www.techpowerup.com/gpu-specs/a100-pcie-80-gb.c3821%23:~:text%3DBeing%2520a%2520dual%252Dslot%2520card,have%2520monitors%2520connected%2520to%2520it.\"><u>the energy usage of an ML GPU</u></a>&nbsp;from NVIDIA by <a href=\"https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-first-quarter-fiscal-2024%23:~:text%3Din%2520these%2520areas%253A-,Data%2520Center,18%2525%2520from%2520the%2520previous%2520quarter\"><u>the revenue of the data center division</u></a>&nbsp;(which they are under) divided by <a href=\"https://www.cnbc.com/2023/02/23/nvidias-a100-is-the-10000-chip-powering-the-race-for-ai-.html\"><u>the cost of an ML GPU</u></a>&nbsp;and comparing that to <a href=\"https://www.worlddata.info/america/usa/energy-consumption.php\"><u>US energy usage</u></a>. I consider the ML estimate to be very likely far too high and the real number to be even lower.</p></div></li></ol>", "user": {"username": "Yafah Edelman"}}, {"_id": "tmsQfX6Ev6BkEeAde", "title": "Changes at RC Forward: Updates on Canadian high-impact donation platform", "postedAt": "2023-08-30T23:08:20.822Z", "htmlBody": "<p>RC Forward, a platform enabling Canadians to support impactful charitable projects recommended by EA-aligned evaluators, has been undergoing significant operational changes. The platform just surpassed $23 million CAD in donations facilitated since it launched in 2017.&nbsp;</p><p>RC Forward has faced significant challenges this year due to external events in Canada outside of its control. In light of these changes, RC Forward conducted a detailed review of its operations, and based on its findings is currently strengthening its processes and partnerships as a Canadian charity. During this transition period there have been two key changes impacting partner organizations and donors:&nbsp;</p><ol><li>Delayed disbursements: Donations made Q4 2022 onwards are experiencing delays compared to the platform\u2019s typical disbursement schedule.&nbsp;</li><li>Donation collection pause: RC Forward has temporarily halted the acceptance of new donations for most projects as part of these adjustments.&nbsp;</li></ol><p>The full update was&nbsp;<a href=\"https://rcforward.org/rc-forward-donation-delay-and-pause/\"><u>published</u></a> on the RC Forward website in late July. It shares more information about the delays, the donation pause, and what we are doing to support our partner charities through these changes.&nbsp;</p><p>All impacted donors should already have received communication from us via email in late July or early August. If you haven\u2019t heard from us and you made a donation to RC Forward after September 2022, it\u2019s possible our email couldn\u2019t be delivered to you. Please email us at&nbsp;<a href=\"mailto:donation@rethinkprojects.org\"><u>donation@rethinkprojects.org</u></a> or&nbsp;<a href=\"https://calendar.app.google/jm6rgvXVA1ffFZuA7\"><u>book</u></a> a call if a conversation would be helpful for you.</p><p>Our team is very grateful for the support we\u2019ve received during these changes. Individuals in the Canadian and international effective altruism community, along with our partners, stakeholders, and donors, have offered generous patience, trust, and feedback. Through this post on the Forum, we aim to apprise the broader community of these changes.&nbsp;</p><p>The RC Forward team is focused on navigating these changes, and will be updating stakeholders as we make progress in the coming months. We remain committed to our mission of making effective giving easy for Canadians, so that they can maximize the positive impact of their charitable giving.</p>", "user": {"username": "mjamer"}}, {"_id": "xb3LvLZ2mhbcZ2niC", "title": "Dear Self; we need to talk about ambition", "postedAt": "2023-08-30T18:58:34.378Z", "htmlBody": "<p>I keep seeing advice on ambition, aimed at people in college or early in their career, that would have been really bad for me at similar ages. Rather than contribute (<a href=\"https://forum.effectivealtruism.org/posts/aXiLprnMGKuxTXcpF/elizabeth-s-quick-takes?commentId=cG6Fqhe8vkgDknQb6\">more</a>) to the list of people giving poorly universalized advice on ambition, I have written a letter to the one person I know my advice is right for: myself in the past.</p><h1><strong>The Letter</strong></h1><p>Dear Past Elizabeth,</p><p>Your life is, in some sense, a series of definitions of success.&nbsp;</p><p>First you\u2019re in early school, and success is defined for you by a handful of adults. You go where they say, do the assignments they say, when they say, and doing well means meeting the goals they set for you. Even your hippie elementary school gives you very few choices about life. You get choices in your leisure activity, but that (as they have explained to you) is leisure and thus unimportant, and there\u2019s no success or failure in it.&nbsp;</p><p>Then you get further in school, and the authorities give you some choice over the hoops you jump through. You can choose which book you write your report on or even what classes you take (within a predetermined set). This feels like freedom, but you\u2019re in still a system someone else designed and set the win conditions for. You can fulfill a college distribution requirement with any history class at all- but you are going to take one, and the professor is the one determining if you succeeded at it.&nbsp;</p><p>More insidiously, you\u2019ll like it. Creating your own definition of success feels scary;enacting it feels impossible. The fact that school lays out neat little hoops for you to jump through is a feature.</p><p>Work (you\u2019ll be a programmer) is where things get screwy. Programming contains multiple definitions of success (manager, principal, freelancing, development, testing, bigtech, start-up, money-maxing, altruistic projects\u2026), and multiple ways to go about them. If your goals lie outside of programming altogether (art, parenting, travel..), it\u2019s relatively easy to work out a way to fund it via programming while still having the time to do what you want. Not trivial, but have you seen what people in other jobs go through? With programming it\u2019s at least possible.</p><p>But you like hoops. You\u2019re comfortable with hoops. So you\u2019re going to waste years chasing down various definitions of success within programming, and by the time you give up will be too exhausted to continue in it at all. I think you (I) should have considered \u201cjust chill while I figure shit out\u201d much earlier, much more seriously. It was reasonable to give their way a try, just due to the sheer convenience if it had worked, but I should have learned faster.</p><p>Eventually you will break out of the Seattle bigtech bubble, and into the overlapping bubbles of effective altruism, lesswrong, and the bay area start-up scene. All of three of these contain a lot of people shouting \u201cbe ambitious!\u201d and \u201cbe independent!\u201d. And because they shout it so loudly and frequently you will think \u201csurely, now I am in a wide open world and not on a path\u201d. But you will be wrong, because \u201cbe ambitious (in ways the people say this understand and respect)\u201d and \u201cbe independent (in ways they think are cool and not crazy)\u201d are still hoops and still determined by other people, just one more level meta.</p><p>Like the programming path, the legible independent ambition path works for some people, but not you. The things you do when pushed to Think Big and Be Independent produce incidental learning at best, but never achieve anything directly. They can\u2019t, because you made up the goals to impress other people. This becomes increasingly depressing, as you fail at your alleged goals and at your real goal of impressing people.&nbsp;</p><p>So what do we do then? Give up on having goals? Only by their definition. What seems to work best for us is leaning into annoyance or even anger at problems in the world, and hate-fixing them.&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xb3LvLZ2mhbcZ2niC/nlww6rophem3rajrtunq\" alt=\"\"></p><p>You\u2019ve always hated people being wrong, and it turns out a lot of things can be defined as \u201cwrong\u201d if you have the right temperament. Women\u2019s pants have tiny pockets that won\u2019t fit my phone? Wrong. TSA eating hours of my life for no gain? Wrong. Medical-grade fatigue? Wrong. People dying of preventable diseases? <i>Extremely </i>wrong. And wrong things are satisfying to fix.</p><p>A nice facet of this approach is that you can start small and it will naturally grow over time. Pocket extenders might give you a nice efficacy high at first, but soon you\u2019ve built a tolerance and are taking on bigger and bigger wrongs just to feel alive. And you have more energy for that effort because of the problems you fixed earlier.&nbsp;</p><p><a href=\"https://acesounderglass.files.wordpress.com/2023/08/screenshot-2023-08-30-at-11.45.11-am.png\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xb3LvLZ2mhbcZ2niC/gdhhnwpsys88zo0io2pq\" alt=\"\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xb3LvLZ2mhbcZ2niC/gdhhnwpsys88zo0io2pq 1024w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xb3LvLZ2mhbcZ2niC/tlhd99yosiafgvwjra9c 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xb3LvLZ2mhbcZ2niC/szhsnwzmr67hnyxj0mzm 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xb3LvLZ2mhbcZ2niC/ljhmm5nayeh4wsu8ml98 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xb3LvLZ2mhbcZ2niC/udlj0rbnrni6rxodvwhr 1338w\"></a></p><p>&nbsp;</p><p>A year ago I got really mad that people were becoming vegan without paying any attention to nutrition and decided to do something about it. That stopped being fun like a week in and was mostly a miserable slog. People yelled at me for doing it, which was pretty unpleasant. I was constantly on the verge of giving up but they were so <a href=\"https://acesounderglass.com/2023/01/10/iron-deficiencies-are-very-bad-and-you-should-treat-them/\">wrong</a> <a href=\"https://acesounderglass.com/2023/01/16/vegan-nutrition-testing-project-interim-report/\">I</a> <a href=\"https://acesounderglass.com/2023/05/13/lessons-learned-from-offering-in-office-nutritional-testing/\">couldn\u2019t</a> let it go. And <a href=\"https://acesounderglass.com/2023/05/25/playing-with-faunalytics-vegan-attrition-data/\">then</a> <a href=\"https://acesounderglass.com/2023/05/30/change-my-mind-veganism-entails-trade-offs-and-health-is-one-of-the-axes/\">it</a> <a href=\"https://acesounderglass.com/2023/06/14/adventist-health-study-2-supports-pescetarianism-more-than-veganism/\">ballooned</a> into something huge.&nbsp;</p><p><img style=\"width:420px\" src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xb3LvLZ2mhbcZ2niC/rgng7fnhuwaktzoxudgp\" alt=\"\"></p><p>[Author\u2019s note: The vegan epistemics sequence isn\u2019t done yet. I kept finding one more post I needed before the big one, and eventually ran into a time-consuming commitment that ate my entire July and August. I hope to wrap this up soon, and I\u2019m confident that something will be coming in the next month, but can\u2019t rule out another 5 just-one-more posts.]</p><p>The spite-based path to impact and altruism is not the easiest road. Spite is a less fun emotion than hope, and makes fewer friends. But I also can\u2019t bring myself to wish I didn\u2019t have this drive, because it is my actual value system. Not caring if people destroy the commons would be easier in many ways but then <i>the commons would be destroyed<strong> </strong></i>and that\u2019s worse.&nbsp;</p><p>It does seem good to have the <i>option</i> to be motivated by hope and not just anger-at-wrongness, and I\u2019m experimenting with that now. I think it is working, but I don\u2019t think I could have done it without the previous, spite-based projects.</p><h1><strong>Some specific advice</strong></h1><p>The following are a few tips I think might be generally useful as you think about how to spend your time.&nbsp;</p><h2><strong>Plans don\u2019t have to route through employment&nbsp;</strong></h2><p>You\u2019re never going to love programming. Trying was probably the right thing to do at the time, but I think you should have given up faster. Giving up would have been easier if you\u2019d separated giving up on <i>loving</i> programming or <i>succeeding </i>in the career from <i>funding your life</i> via programming. You could have had many years of <a href=\"https://jargonism.com/words/1369\">rest and vest</a> if you\u2019d been open to it.&nbsp;</p><p>Would your life have been missing something? Yes. Was programming ever going to provide that thing? Not directly. But the free time and savings sure were helpful in the pursuit.&nbsp;</p><h2><strong>Ways to identify fake ambition</strong></h2><p>Sometimes it\u2019s hard to tell if you authentically want something, or are trying to impress people with how ambitious you are. Here are a few tricks I\u2019ve picked up for distinguishing them.</p><ol><li>What do you feel when you think about shrinking the project?<ol><li>Anger, fear, or disappointment that the goal won\u2019t be accomplished = seems like you care a lot, good sign for the project.&nbsp;</li><li>Relief = project seems like a lot of work. Consider if you\u2019re up for that level of work before beginning (but the answer might be yes).</li><li>Fear of failure = this is a terrible sign, unless you have some reason the smaller version is more likely to fail (if you do, try the question again with a version that is easier and more likely to succeed). If failure feels like a bigger threat for a project that is objectively easier, that\u2019s a suggestion you are, on some level, not taking the big version seriously. If you were, it would be scarier.</li><li>Fear of other people\u2019s reaction = your primary motivation here is probably social. Knowing that, do you still want to go forward? The answer can be yes, although if so I hope the project is cheap.</li></ol></li><li>If you fail, will you or the people around you go \u201cwell what did you expect?\u201d That would suggest your goal is unachievable from your current state.<ol><li>Another reason smaller projects can be scarier is that people will judge you more for failing. Don\u2019t let this push you into too-big projects.</li></ol></li><li>If you fail, will it be educational? \u201cNo\u201d is both a <i>sign</i> the project is too big (because you don\u2019t understand it well enough) and a <i>reason</i> the project is too big (you\u2019re going to pour all that energy in and not even get learning out?)</li><li>How does concrete, accurate, actionable criticism make you feel? If it\u2019s a real goal you should be delighted, because feedback helps you reach that goal faster. Or if you\u2019re a little less evolved, not delighted, but at least it feels like a really productive massage or workout, where it hurts but you can tell it will be worth it. If you feel terrified, or angry, or especially <i>unsupported</i>, that suggests you care more about the social aspect of the project than its outcome.</li><li>Does the project give you a satisfying feeling of contact with reality? That\u2019s maybe a sign it\u2019s a real goal, and definitely good in general.<ol><li>What does \u201ccontact with reality\u201d mean? That is a very reasonable question I don\u2019t have a good answer for, at least not in words. Hopefully you\u2019ll know it when you see it.</li></ol></li></ol><h2><strong>Some of the people talking about ambition and independence mean it</strong></h2><p>You will never impress them until you give up on doing so.</p><p>One friend in particular got palpably more respect for you 5 seconds after you gave up on impressing him. Not \u201cdismissed as irrelevant\u201d or \u201covercame your need for\u201d his approval, just \u201caccepted the reality that it wasn\u2019t going to happen and stopped putting energy into it.\u201d&nbsp;</p><p>I could end this story with \u201c\u2026and then eventually you did real things he respected a lot\u201d but that\u2019s kind of like telling people it\u2019s fine to not worry about their weight because chilling out will cause them to lose weight. It\u2019s sometimes true, but if this is going to work it needs to not matter.&nbsp;</p><h2><strong>Comparison is the thief of joy until you find the right reference group</strong></h2><p>Respect from other humans is a fundamental need, and I don\u2019t want you to attempt to live without it. I just want you to have an accurate scale. Luckily, life is going to provide one for you that you can live with.</p><p>I spent a lot of time in the bay area feeling stupid and lazy. This got worse and worse until I went to Andy\u2019s wedding in 2018. Andy, while quite ambitious in his hippy way, was about as far outside the bay area ambition bubble as you could get.&nbsp; During the reception I had a flash of insight that I only look stupid and lazy next to the brilliant and driven people I deliberately sought out because I am smart and kind of driven (but not as much as them). Next to Andy\u2019s hippy friends I am a titan of industry.</p><p>This felt like a socially unacceptable cure for anxiety at the time, but I talked to Andy about it a few years later and he said \u201coh yeah, it\u2019s always been clear you were the friend of mine most likely to be remembered after you die\u201d. Which is is a pretty intense thing to say, and honestly kind of weird because at the time you met his hippy goals definitely outscaled your programming goals. But he definitely wasn\u2019t mad.</p><p>I don\u2019t want to lean too hard on this. The point isn\u2019t \u201cI\u2019m better than them because I\u2019m ambitious\u201d. They\u2019re living their own best lives that wouldn\u2019t be improved if I started shouting at them about ambition or the glorious fight for epistemics. It\u2019s just that I\u2019m not <i>failing</i> all the time.</p><h2><strong>Sometimes procrastination is a workers\u2019 strike</strong></h2><p>If you just don\u2019t seem to be able to focus on something, consider that you might not actually want to do it and you should quit. Better yet, get curious about why you don\u2019t seem to want to work on it, with \u201cI hate it and want to quit\u201d being one of many options. This will save you a lot of time and misery.&nbsp;</p><h2><strong>Your taste will always exceed your ability</strong></h2><p>\u2026which means things you make will always be disappointing relative to the image in your head. People will tell you the cure is to push through and do lots of stuff anyway (<a href=\"https://excellentjourney.net/2015/03/04/art-fear-the-ceramics-class-and-quantity-before-quality/\">my</a> <a href=\"https://www.youtube.com/watch?v=0sHCQWjTrJ8\">favorites</a>). There\u2019s definitely something to that, but I have this nagging feeling that that\u2019s only half a cure. It looks to me like \u201cstop thinking and just ship it\u201d is as much of an avoidance strategy as \u201cyou can\u2019t ship things until they\u2019re perfect\u201d.</p><p>Every once in a while I ask the internet \u201chey, how do you tell when to release subpar work and when to keep improving?\u201d and no one has ever given me a satisfactory answer. Hopefully I\u2019ll have something for you in a few years.</p><h1><strong>Conclusion</strong></h1><p>You should give \u201clet other people set your success criteria\u201d a good shot. When it works it\u2019s way easier than creating your own. But you should recognize when you\u2019re doing it (even if one of the success criteria you\u2019re trying to meet is \u201cbe independent\u201d), and when it doesn\u2019t work you should move on.&nbsp;</p><p>Best of luck,</p><p>-Elizabeth</p>", "user": {"username": "Elizabeth"}}, {"_id": "PTtZWBAKgrrnZj73n", "title": "Biosecurity Culture, Computer Security Culture", "postedAt": "2023-08-30T17:07:24.538Z", "htmlBody": "<p>While I've only worked in biosecurity for <a href=\"https://www.jefftk.com/p/leaving-google-joining-the-nucleic-acid-observatory\">about a year</a> and my computer security background consists of things I picked up while working on other aspects of software engineering, the cultures seem incredibly different. Some examples of good computer security culture that would be bad biosecurity culture:</p><ul><li>Openness and <a href=\"https://en.wikipedia.org/wiki/Full_disclosure_(computer_security)\">full disclosure</a>. Write blog posts with deep detail on how vulnerabilities were found, with the goal of teaching others how to find similar ones in the future. Keep details quiet for a few months if need be to give vendors time to fix but after, say, <a href=\"https://googleprojectzero.blogspot.com/2021/04/policy-and-disclosure-2021-edition.html\">90 days</a> go public.</li><li>Breaking things to fix them. Given a new system, of course you should try to compromise it. If you succeed manually, make a demo that cracks it in milliseconds. Make (and publish!) fuzzers and other automated vulnerability search tools.</li><li>Enthusiastic curiosity and exploration. Noticing hints of vulnerabilities and digging into them to figure out how deep they go is great. If someone says \"you don't need to know that\" ignore them and try to figure it out for yourself.</li></ul><p>This is not how computer security has always been, or how it is everywhere, and people in the field are often fiercely protective of these ideals against vendors that try to hide flaws or silence researchers. And overall my impression is that this culture has been tremendously positive in computer security.</p><p>Which means that if you come into the effective altruism corner of biosecurity with a computer security background and see all of these discussions of \"<a href=\"https://docs.google.com/document/d/1VSfU3GiZumHDX2hoz3YY1PT2dQHtkbrfO8xLxI9BTGE/edit\">information hazards</a>\", people discouraging trying to find vulnerabilities, and people staying quiet about dangerous things they've discovered it's going to feel very strange, and <a href=\"https://forum.effectivealtruism.org/posts/3a6QWDhxYTz5dEMag/how-can-we-improve-infohazard-governance-in-ea-biosecurity\">potentially rotten</a>.</p><p>So here's a framing that might help see things from this biosecurity perspective. Imagine that the <a href=\"https://en.wikipedia.org/wiki/Morris_worm\">Morris worm</a> never happened, nor <a href=\"https://en.wikipedia.org/wiki/Blaster_(computer_worm)\">Blaster</a>, nor <a href=\"https://en.wikipedia.org/wiki/Samy_(computer_worm)\">Samy</a>. A few people independently discovered <a href=\"https://en.wikipedia.org/wiki/SQL_injection\">SQL injection</a> but kept it to themselves. Computer security never developed as a field, even as more and more around us became automated. We have driverless cars, robosurgeons, and simple automated agents acting for us, all with the security of original Sendmail. And it's all been around long enough that the original authors have moved on and no one remembers how any of it works. Someone who put in some serious effort could cause immense destruction, but this doesn't happen because the people who have the expertise to cause havoc have better things to do. Introducing modern computer security culture into this hypothetical world would not go well!</p><p>Most of the cultural differences trace back to what happens once a vulnerability is known. With computers:</p><ul><li>The companies responsible for software and hardware are in a position to fix their systems, and disclosure has helped build a norm that they should do this promptly.</li><li>People who are writing software can make changes to their approach to avoid creating similar vulnerabilities in the future.</li><li>End users have a wide range of effective and reasonably cheap options for mitigation once the vulnerability is known.</li></ul><p>But with biology there is no vendor, a specific fix can take years, a fully general fix may not be possible, and mitigation could be incredibly expensive. The culture each field needs is downstream from these key differences.</p><p>Overall this is sad: we could move faster if we could all just talk about what we're most concerned about, plus cause prioritization would be simpler. I wish we were in a world where we could apply the norms from computer security! But different constraints lead to different solutions, and the level of caution I see in biorisk seems about right given these constraints.</p><p>(Note that when I talk about \"good biosecurity culture\" I'm describing a set of norms that I see as the right ones for the situation we're in, and that are common among effective altruists and other people with a similar view of the world. There's another set of norms within biology, however, that developed when the main threats were natural. Since there's no risk of nature using public knowledge to cause harm, this older approach is even more open than computer security culture, and in my opinion is a very poor fit for the environment we're in now.)</p>", "user": {"username": "Jeff_Kaufman"}}, {"_id": "o5fbhWrwEH4YyT9AY", "title": "New Jury Analysis of the Smithfield Piglet Rescue Trial", "postedAt": "2023-08-30T16:03:31.146Z", "htmlBody": "<p>Faunalytics analyzed transcripts from interviews with jurors of the Smithfield Foods criminal trial\u2014in which two animal rights activists were found not guilty of \u201cstealing\u201d two piglets from a factory farm in Utah. This qualitative analysis will help advocates understand why jurors sided with the defense, how to potentially apply these findings to future trials, and what forms of animal activism are most convincing.&nbsp;</p><p><strong>https://faunalytics.org/smithfield-trial-juror-analysis&nbsp;</strong></p><p><strong>Key Findings:</strong></p><ol><li><strong>The \u201cnot guilty\u201d verdict hinged, in part, on the monetary value of the piglets to Smithfield, which was argued to be less than zero.&nbsp;</strong>The piglets required veterinary care that exceeded their value to Smithfield. The jury was initially hesitant to say the piglets had no worth because they saw them as having inherent worth as living beings, however they ultimately decided the theft charges hinged on&nbsp;<i>monetary&nbsp;</i>value only.&nbsp;</li><li><strong>The jury members believed the defendants, Wayne and Paul, did not have the intent to steal.</strong> Before their investigation of the Smithfield facility, Wayne said on video \u201cif there\u2019s something we\u2019ll take it.\u201d The jury interpreted the \u201cif\u201d as meaning the two activists did not enter the facility knowing they\u2019d have the opportunity to take piglets. However, one juror noted that if the defendants had a pattern of doing this in the past, the jury might have been more likely to find them guilty.&nbsp;</li><li><strong>The participants all reported being more receptive to animal advocacy and animal welfare after the trial.&nbsp;</strong>One participant reported that they no longer eat ham. Another reported that while they still believe that pigs are here to be eaten, as a result of the trial they now believe that pig welfare should be improved. Another was even inspired to pursue animal activism.&nbsp;</li><li><strong>Despite what media coverage indicates, the \u201cright to rescue\u201d was not a major factor in the jury\u2019s decision</strong>. Some media outlets (such as&nbsp;<a href=\"https://theintercept.com/2022/10/08/smithfield-animal-rights-piglets-trial/\"><u>The Intercept</u></a> and&nbsp;<a href=\"https://www.democracynow.org/2022/10/11/the_right_to_rescue_jury_acquits\"><u>Democracy Now!</u></a>) have characterized this trial as a test case for the \u201cright to rescue\u201d argument\u2014the idea that one should be able to rescue animals, sometimes farmed animals, from distressing conditions. However, only two jurors mentioned this concept at all, and no jurors mentioned this idea as critical.&nbsp;</li></ol><p><strong>Background</strong></p><p>The Smithfield Trial refers to the prosecution of two animal advocates who were charged with felony theft and burglary after they removed two piglets from a Smithfield Foods facility in Utah, United States.&nbsp;</p><p>Wayne Hsiung and Paul Darwin Picklesimer, a co-founder and member of Direct Action Everywhere, respectively, are activists \u201c<a href=\"https://www.directactioneverywhere.com/about-us\"><u>working to achieve revolutionary social and political change for animals</u></a> in one generation.\u201d In 2017, Wayne and Paul entered the Circle Four Farms facility in Milford, Utah, and removed two injured piglets (later named Lily and Lizzie). Circle Four Farms is one of the&nbsp;<a href=\"https://www.nytimes.com/2022/10/08/science/animals-rights-piglets-smithfield.html\"><u>largest industrial pig processing facilities</u></a> in the United States and a subsidiary of Smithfield Foods, which is the world\u2019s largest pork producer. Once rescued, the piglets were provided with veterinary care and relocated to a sanctuary where they currently reside. The removal of the piglets was filmed and posted on social media under the title \u201c<a href=\"https://www.righttorescue.com/sf-ut/\"><u>Operation Deathstar</u></a>.\u201d&nbsp;</p><p>In September 2022, Wayne and Paul went on trial in Washington County, Utah on charges of felony theft and burglary for removing the piglets. They were acquitted (i.e., found not guilty) by a jury on both counts.&nbsp;</p><p>This trial may interest animal advocates because it provides potential guidance for future trials and investigations. Additionally, this analysis provides insight as to which pro-animal arguments are more persuasive more generally.&nbsp;</p><p>In this study, we analyzed themes from interviews with five Smithfield Trial jury members (referred to below as \u201cparticipants\u201d) in order to determine which arguments jurors found most convincing and what lessons animal advocates can learn from this case. All results are drawn from the juror interviews as we did not examine trial testimony, evidence, or statements.</p><p><strong>Research Team</strong></p><p>The project\u2019s analyst and lead author was Fiona Rowles, while Dr. Jo Anderson reviewed and oversaw the work, with writing support from Bj\u00f6rn J\u00f3hann Olafsson. Dr. Justin Marceau designed and conducted the interviews with participants, and kindly shared the data with Faunalytics for the purpose of this analysis.</p><p><strong>Conclusions</strong></p><p>Criminal trials regarding activists allegedly stealing animals from factory farms are increasingly common and are even&nbsp;<a href=\"https://www.nationalgeographic.com/animals/article/activists-call-it-rescue-farms-call-it-stealing-what-is-open-rescue\"><u>attracting media attention</u></a>. Some advocates believe these trials may be a&nbsp;<a href=\"https://www.vox.com/future-perfect/23647682/factory-farming-dxe-criminal-trial-rescue\"><u>valuable tool for farmed animal advocacy</u></a> and law reform projects more generally. This study examined interviews from five jurors of one such case, Smithfield Foods, to examine why they ultimately sided with the defendants, not the farm.&nbsp;</p><p>Jury participants said they remember believing the animal activists were obviously guilty at the beginning of the trial. However, the jury was ultimately convinced by the evidence that Wayne and Paul were not guilty of the theft of two piglets.&nbsp;</p><p>Two facts were key: the monetary value of the piglets and the rescuers\u2019 stated intentions.&nbsp;</p><p>The prosecution first needed to show that the piglets were worth a significant amount of money: over $1,500 for a felony theft. The jury did not believe this to be the case. In fact, they believed the piglets were worth less than zero dollars when veterinary care was factored in.</p><p>The prosecution also needed to prove that the activists had an intention to steal prior to entering the facility. Based on video evidence in which Wayne said that he would take an animal \u201cif [they] saw something\u201d the jury believed there was no intention to steal.&nbsp;</p><p>Additionally, the defense was viewed as credible and respectful, while the prosecution was considered to be condescending, both to the jury and to the piglets.&nbsp;</p><p>Despite the \u201cright to rescue\u201d argument being credited in media outlets as the reason for acquittal, the jurors tended not to identify this argument by that name as having impacted the outcome. Further research should consider how the right to rescue framing can impact the way the public perceives the \u201ctheft\u201d of farmed animals.&nbsp;</p><p>During the trial,&nbsp;<a href=\"https://kutv.com/news/local/animal-activists-found-not-guilty-after-piglet-rescue-from-utah-farm-right-to-rescue-smithfield-foods-picklesimer-hsiung\"><u>Wayne said</u></a>, \u201cI don't actually want you to acquit us on a legal technicality. I want you to acquit us as a matter of conscience. There's a big difference between stealing and rescue.\u201d However, the participants in this study stated they relied on legal arguments when making their decision. It is also possible they were morally convinced by Wayne\u2019s moral appeal, and subsequently looked for a legal hook with which to acquit him.&nbsp;</p><p>Two of the five participants did make comments supportive of the \u201cright to rescue\u201d theory, which indicates that future juries may be open to this idea. However, it remains to be seen if the \u201cright to rescue\u201d can be expressed clearly in a legal setting.&nbsp;</p><p>All participants reported that they now viewed animal activists or animal welfare more broadly, in a more positive light after sitting for the trial. One participant had stopped eating ham and another was even interested in contributing to animal advocacy.&nbsp;<strong>Therefore, trials may be an important venue for advocates presenting their cause to everyday people</strong>. While juries themselves are small, these trials attract media attention that reach large audiences. Also, wins for animal advocates may&nbsp;<a href=\"https://www.vox.com/future-perfect/23647682/factory-farming-dxe-criminal-trial-rescue\"><u>indirectly influence future court cases</u></a>.</p><p>Unlike other forms of advocacy, jury trials are measured, slow, and don\u2019t rely on emotional appeals. Jurors are explicitly instructed to review all the evidence with an open mind. This gives advocates a wide-open platform with which to explain their point of view towards animals, which proved influential toward jurors in this case.&nbsp;</p><p>However, after the trial, some jurors went online and were dismayed by prejudicial comments left by animal welfare activists about them. This may have decreased the positive impact the trial could have had on their opinions, attitudes, or behavior towards animal welfare. Advocates should always be respectful of others, engaging without assumptions or undue criticism, to maximize their chances of persuasion.&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "JLRiedi"}}, {"_id": "6yq9TTJKvJnrgT4E2", "title": "Did Economists Really Get Africa\u2019s AIDS Epidemic \u201cAnalytically Wrong\u201d? (A Reply)", "postedAt": "2023-08-30T15:36:55.336Z", "htmlBody": "<p>To demonstrate CGD's cherished principle of not taking organisational positions, here is a response from a couple of us in the health team to our colleague Justin Sandefur's recent(ish) blog on cost-effectiveness evidence and PEPFAR.</p><p>Our concern was that readers might come away from Justin's blog thinking that cost-effectiveness evidence wasn't useful in the original PEPFAR decision and wouldn't be useful in similar decisions about major global health initiatives. We disagree and wanted to make the case for cost-effectiveness as well as addressing some of Justin's specific points along the way.</p><p><a href=\"https://www.cgdev.org/blog/did-economists-really-get-africas-aids-epidemic-analytically-wrong-reply\">https://www.cgdev.org/blog/did-economists-really-get-africas-aids-epidemic-analytically-wrong-reply</a></p><p>--</p><p>A recent, thought-provoking blog by our colleague, Justin Sandefur, titled \u201c<a href=\"https://www.cgdev.org/blog/how-economists-got-africas-aids-epidemic-wrong\"><u>How Economists got Africa\u2019s AIDS Epidemic Wrong</u></a>\u201d, has sparked a debate about the historical role of cost-effectiveness analysis in assessing the investments of the President's Emergency Plan for AIDS Relief (PEPFAR) and, implicitly, the value of such analysis in making similar global health decisions. Justin tells the story of PEPFAR and concludes that economists that raised concerns over the cost-effectiveness of antiretroviral therapies got PEPFAR \u201canalytically wrong\u201d, a conclusion that some readers may interpret as a reason to discard cost-effectiveness analysis for such decisions in the future. The original blog draws three lessons:</p><p><strong>Lesson #1. What persuaded the White House was evidence of feasibility and efficacy, not cost-effectiveness</strong></p><p><strong>Lesson #2. The budget constraint wasn\u2019t fixed; PEPFAR unlocked new money</strong></p><p><strong>Lesson #3. Prices also weren\u2019t fixed, and PEPFAR may have helped bring them down</strong></p><p>In this blog we argue that while Justin\u2019s observations hold some truth, they do not discredit the value of cost-effectiveness analysis in decision-making. Specifically, we contend that:</p><ul><li>Because there were many feasible and effective options at the time, this was not sufficient criteria for such a large decision. It should have considered the cost-effectiveness of other options, to explore the relative impact.</li><li>PEPFAR may have unlocked some new money, but it wasn\u2019t all new money, and it will have had short- and long-term opportunity costs. Moreover we cannot be certain that PEPFAR was uniquely able to increase available funding. Thus the decision could have considered cost-effectiveness analysis to reveal likely trade-offs.</li><li>Price reductions could have been analytically explored for PEPFAR and for alternative options as part of cost-effectiveness analysis during decision-making.</li></ul><p>The bigger lesson, we conclude, is that when the next PEPFAR-sized decision happens, our systems and their stakeholders must strive for higher standards, embracing analysis that models a range of good options and assesses them against key criteria. Cost-effectiveness analysis is a necessary component of this, but it is not sufficient, and additional analysis and scenarios should be considered through a deliberative process, before settling on a final decision.</p><p>Below we offer reflections on each of Justin\u2019s three lessons, in order, then draw out the overall conclusions.</p><h3><strong>Response 1: Feasibility and efficacy are not enough</strong></h3><p>Justin uses an analogy of giving to a homeless person to invite the reader to agree that cost is not really the relevant issue when considering whether to do a good deed. True enough, if something can be considered not effective or not feasible then it\u2019s a non-starter and we don\u2019t need to trouble ourselves over cost or cost-effectiveness. But when there are multiple feasible and effective options with different levels of effectiveness and cost, understanding which does the most good for the money is absolutely worth knowing. Indeed we agree that there is a <a href=\"https://www.cgdev.org/publication/moral-imperative-toward-cost-effectiveness-global-health\"><u>moral imperative to consider such evidence</u></a>, since millions of lives are at stake. This was indeed the scenario when PEPFAR was introduced\u2014there were many worthy global health initiatives, such as malaria prevention and childhood vaccines, that were feasible, scalable and effective and whether the political process was willing to consider them or not, the opportunities were there.</p><h3><strong>Response 2: Was PEPFAR the </strong><i><strong>only</strong></i><strong> thing that could have unlocked new development money?</strong></h3><p>In his second lesson, Justin suggests that, because PEPFAR unlocked new money, the options may have been either PEPFAR or nothing. This implies that the burden of considering trade-offs is unnecessary. PEPFAR had a significant budget and we acknowledge that it may have increased net Official Development Assistance (ODA), but in order to throw out the idea of opportunity cost (at least from a development perspective), as Justin implies, we would have to be sure that PEPFAR was the <i>only</i> thing that could have unlocked this additional spend. Can we be sure this was the case? Could we ever be? Certainly those making the case for greater focus on HIV prevention didn\u2019t think so and it seems plausible that at least the volume of financing drawn into PEPFAR meant other smaller initiatives struggled to win funding. This can perhaps be seen in <a href=\"https://www.cgdev.org/blog/how-economists-got-africas-aids-epidemic-wrong\"><u>figure 3 of Justin\u2019s blog</u></a> where there appears to be a <i>reduction</i> in aid to areas with lower HIV prevalence, such as the middle east and north Africa. In addition, as Justin noted, PEPFAR\u2019s $15 billion included only $10 billion new money, suggesting at least an immediate short term $5 billion development sector opportunity cost.</p><p>Assuming that there is no alternative to PEPFAR means accepting the political preferences of the time as immovable. We argue instead that analysis that shows the value for money of different options is useful for informing political debates. There are, of course, reasonable cases where constrained optimisation makes sense. For example, in their current forms, Gavi and the Global Fund against HIV/AIDS, Tuberculosis and Malaria have clear remits and analysis that is designed to inform their decisions and optimises within their areas of focus, ignoring other potential health spending, which is understandable. However, when deciding whether to <i>create</i> Gavi, or in evaluating its value as a global health initiative, it\u2019s clear that we must compare with alternatives, regardless of the local political climate at the time of inception. Therefore the counterfactual of PEPFAR clearly wasn\u2019t nothing, and it will have had a substantial opportunity cost in the short-term and long-term. Rigorous cost-effectiveness analysis of the relevant counterfactuals, along with other evidence and a deliberative appraisal process could have been helpful to inform its creation. If there are political dynamics which prevent certain choices, this can, in a sense, be seen as the <i>choice</i> of that decision-making system\u2014but it doesn\u2019t mean that alternative options were never there.</p><h3><strong>Response 3: Prices aren\u2019t fixed. But they won\u2019t be for comparators either</strong></h3><p>Justin notes that prices dramatically declined over time, suggesting PEPFAR may have contributed to this trend. Even if we accept that PEPFAR was responsible for the price reductions, we still don't know the counterfactual\u2014could equivalent investment have influenced the price of other treatments or vaccines? It is hard to be sure, but during PEPFAR\u2019s time period, active intervention in health technology markets has delivered wide ranging benefits, including developing a 43 percent reduction in the price of <a href=\"https://www.gavi.org/news/media-room/pneumococcal-vaccine-price-drops-third-year-running\"><u>pneumococcal vaccines</u></a> and 90 percent cheaper <a href=\"https://www.clintonhealthaccess.org/news/chai-and-the-hepatitis-fund-announce-pricing-breakthrough-to-reduce-cost-of-viral-hepatitis-treatment-by-over-90-percent/\"><u>hepatitis C treatment</u></a>. With the benefit of hindsight, the scale of the price reductions of course makes the PEPFAR decision look better than it would have done at the time to analysts such as Emily Oster, but given this factor wasn\u2019t a key part of the original decision-making, it wasn\u2019t a fully-informed decision\u2014the world just got lucky. The important question really is\u2014how can good decision-making take the endogenous, dynamic, and negotiated nature of drug prices into account in future? Shouldn\u2019t it model it? Using the best available evidence, and considering appropriate counterfactuals and their likely price trajectories? Yes, standard cost-effectiveness doesn't predict price changes, but sensitivity analysis should be done to allow for different price considerations. This can be done with input from market shaping experts and industry. Indeed, in many cases cost-effectiveness analysis is precisely the tool used to exert downward pressure on price and there <a href=\"https://www.cgdev.org/publication/leave-no-one-behind-using-benefit-based-advance-market-commitment-covid-vaccine\"><u>are ways</u></a> to deploy both scale incentives and value for money assessments together.</p><h2><strong>Conclusion</strong></h2><p>In conclusion, if PEPFAR was the best decision for development, then decision-makers got lucky, because it was not a well-informed decision. It would have been reasonable for decision-makers to think PEPFAR was likely to have substantial opportunity costs and therefore cost-effectiveness analysis was a <i>necessary</i> part of good decision-making. But we acknowledge cost-effectiveness analysis was also not <i>sufficient</i>, and that, in reality decisions are made for a range of important reasons. We believe the correct lesson here isn't merely that we need better economics for major decisions such as PEPFAR, but that decision-makers should invest in transparent deliberative processes, similar to <a href=\"https://www.cgdev.org/publication/six-reasons-why-global-fund-should-adopt-health-technology-assessment\"><u>health technology assessment</u></a>, where all relevant factors can be appropriately considered, such as feasibility, efficacy, equity, and where appropriate\u2014market shaping potential. This would also enable the views of recipients to be heard. This process should embrace complexity and strive to maximize the good derived from money spent.</p><p>This matters, because sooner or later there will be another PEPFAR proposed, and we need better systems in place to integrate evidence, politics, and social values to make these decisions wisely. It is also possible that we are entering a period of global health <i>disinvestment</i>. We also hope that key decisions on whether to wind down major global health initiatives use rigorous, deliberative processes with wise use of both cost-effectiveness evidence and supplementary analysis. Ironically, with antiretrovirals now becoming better value for money compared to alternatives in many settings, it could be that cost-effectiveness evidence saves PEPFAR in the future.</p>", "user": {"username": "TomDrake"}}, {"_id": "tk2YWPKNqeCDWData", "title": "EA Germany Community Health Documents & Processes", "postedAt": "2023-08-30T14:23:25.176Z", "htmlBody": "<p>Our goal with these documents is to build a safe community for all our members by making sure any interpersonal harm is appropriately dealt with and encouraging harmed individuals to reach out to us. This is a collection of all the documents we created for EA Germany during the Community Health Project (March - July 2023). We were asked to share these with the wider community. We welcome others to use and adapt them and invite feedback.&nbsp;</p><p>We created two types of&nbsp;documents:</p><ul><li><strong>Public:</strong> to inform our community about our Code of Conduct, event standards, ways to report misbehaviours and ask for help, processes for evaluating and responding to reports, confidentiality, professional contact points, and Awareness Workshop results.&nbsp;</li><li><strong>Internal:</strong> description of the role and responsibility of the Community Health Contact, their interaction with the Equal Opportunities Officer of our association, and handover processes.</li></ul><h2>Why we need this</h2><p><i>Last week, Ninas, a new group organizer, was alerted by group member Sayat about allegations against a long-term member. Unaware of any past issues, Ninas informs Sayat, causing Sayat to feel belittled and cease work on a promising Biosafety camp. After confronting the accused and mistakenly revealing Sayat as the source, retaliation against Sayat ensues. Ninas, now shocked and believing Sayat, uncovers additional troubling stories about the long-term member after further inquiry. Ninas regrets not knowing earlier, as this knowledge could have prevented Sayat's distress and withdrawal from their project due to harassment.</i></p><p>Imagine if Ninas, the hypothetical group organiser, had a starter pack of information about Community Health and previous incidents in their group. Imagine if they didn\u2019t botch their first conversation with Sayat and were better informed about confidentiality procedures.&nbsp;</p><p>As part of the Community Health Project, which started in March 2023, we created a series of documents to inform our members about our offers and to document internal processes for the team. These documents exist to support us in staying in line with our&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zxSdBkN6cggkE8vv6/ea-germany-s-strategy-for-2023#Vision\"><u>vision</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxdmnm9feunp\"><sup><a href=\"#fnxdmnm9feunp\">[1]</a></sup></span>&nbsp;and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zxSdBkN6cggkE8vv6/ea-germany-s-strategy-for-2023#Values\"><u>values</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdhfoooamejh\"><sup><a href=\"#fndhfoooamejh\">[2]</a></sup></span>. They further help identify areas for improvement and serve as the foundation for action.</p><h2>Setting in Germany</h2><p>To appreciate how these documents work, here is an overview of the system in and for which they\u2019re created. Other communities will have to adapt the processes for their own structure, culture and laws.</p><p>Our structure: The German EA community has 27 active local groups. EA Germany is organised as a membership association&nbsp;<i>Effektiver Altruismus Deutschland (EAD) e.V.&nbsp;</i>with over 100 members. We currently have a&nbsp;<a href=\"https://www.effektiveraltruismus.de/team\"><u>team of five</u></a> employees, some full-time and some part-time. The association also elects an Equal Opportunities Officer who checks our processes and important documents for discrimination. They also provided us with feedback on these documents.&nbsp;</p><h2>The resources</h2><p>Part of our&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zxSdBkN6cggkE8vv6/ea-germany-s-strategy-for-2023#Foundational_Programs\"><u>strategy</u></a> for 2023 is to provide a trained Community Health Contact, implement standards and offer documents for Community Health. With the support of colleagues, the Equal opportunities officer, CEA\u2019s Community Health team and experts, I assessed our options to create a safer community. This assessment is ongoing. Here is the summary of what needed to be done and the results so far:</p><h3>Public documents</h3><ol><li>Community Health Contact&nbsp;<ol><li><a href=\"https://www.mentalhealthfirstaid.org/about/\"><u>Mental Health First Aid</u></a> training to qualify me for this role in our team</li><li><a href=\"https://www.effektiveraltruismus.de/programs\"><u>Informed our community</u></a> about this offer; also in&nbsp;<a href=\"https://www.effektiveraltruismus.de/programme\"><u>German</u></a>&nbsp;</li><li><a href=\"https://forms.gle/WAUaWhjY7PExBHzN9\"><u>Anonymous contact forms</u></a> established; also in&nbsp;<a href=\"https://forms.gle/uKTaw97QcaLMNa1s6\"><u>German</u></a></li></ol></li><li><a href=\"https://www.effektiveraltruismus.de/code-of-conduct\"><u>Code of Conduct</u></a><ol><li>We added avenues of reporting violations and responses to reports, an alcohol and drug policy and anonymous contact forms.&nbsp;</li><li>During our annual meeting at the start of June, we asked the members to vote on our Code of Conduct. Getting it passed by the association\u2019s members ensures that members are aware of, accept and follow the Code of Conduct.</li><li>Applies to all EAD events and many retreats organised by local groups: We offer liability insurance for retreats. To get insurance, enforcing our Code of Conduct at the event is mandatory.</li></ol></li><li><a href=\"https://www.effektiveraltruismus.de/code-of-conduct#event-standards\"><u>Minimum standards for events</u></a> that EAD holds are published on our website</li><li><a href=\"https://drive.google.com/drive/folders/1IL82bE-37gpF3yo2nV5u1hb5k5Xp48Vg?usp=drive_link\"><u>Awareness Team resources</u></a> with contacts to professional help in Germany for serious situations</li></ol><p>Documents 1-3 are available in German and English. Other documents are in English only.</p><h3>Internal Documents</h3><ol><li>Internal guide for community health matters:&nbsp;<ol><li>Contents: limits of responsibility and influence, processes and lines of communication with the rest of the team, confidentiality, possible responses to reports of misconduct, process for hand-over of the position.&nbsp;</li><li>We will not publish the whole resource for now as we have to adapt it for publication. If you are interested, reach out to us.</li></ol></li><li>We stay in regular contact with group house organisers in Germany to spot potential community health problems early. There is little influence we have over what a group house does apart from offering support.</li><li>Awareness team training is offered to people for larger/overnight events.</li></ol><h2>Further steps for EAD</h2><ol><li>Receive legal advice for risks associated with dealing with community health cases.</li><li>Clarify the definition of 'consent' in the Code of Conduct due to questions raised after its publication.</li><li>Define how to engage with perpetrators.</li></ol><h2>Further steps in cooperation with the global community</h2><p>When I presented preliminary versions to other Community Health Contacts at the EAG London, I was asked to share my work once it was presentable.</p><p>I am sharing our work here for other Community Builders, Community Health Contacts and Liaisons to use and adapt to their needs. This could be a basis for standard procedures and documents for EA groups. If you have any feedback, I\u2019m keen to read it.&nbsp;</p><p>Megan Nelson from NYC and I also want to organise an online Contact Person/Community Health person meetup in the future. People who are interested to co-host and/or join,&nbsp;<a href=\"mailto:milena.canzler@effektiveraltruismus.de\"><u>please reach out to me</u></a>.</p><h2>Limitations</h2><p>I am not a psychologist, social worker or professional for group well-being and harassment prevention. I have a long-standing interest in these areas and asked several professionals, such as social workers and psychologists, for feedback, and am still looking for more feedback.</p><p>As an organisation that works for a volunteer movement without a protected brand, we have limited options to monitor and regulate people\u2019s behaviour. We rely on the active cooperation of the majority of our community. Thus most processes aim at education and prevention, and responses to misbehaviour might seem mild compared to, e.g. workplace policies.&nbsp;</p><hr><p>A big thank you to my team and our Equal Opportunities Representatives for helping me write this! And to the people at the Community Health meetup at EAG London for encouraging me to share what we created.<br><br>Photo credit for the preview goes to <a href=\"https://unsplash.com/@voneciacarswell?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Vonecia Carswell</a> on <a href=\"https://unsplash.com/photos/0aMMMUjiiEQ?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a>.<br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxdmnm9feunp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxdmnm9feunp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201ca diverse and resilient community of ambitious people in and from Germany who are thinking carefully about the world\u2019s biggest problems and taking impactful action to solve them\u201d</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndhfoooamejh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdhfoooamejh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201csustainable growth, a welcoming and nurturing culture and high professional standards\u201d</p></div></li></ol>", "user": {"username": "Milena Canzler"}}, {"_id": "qqC4s98eFu8XkB6zF", "title": "Issue with AI alignment - diversity of opinions as competetive advantage? (as opposed to echo chambers)", "postedAt": "2023-08-30T09:44:47.418Z", "htmlBody": "<p>I genuinely see diversity is competetive advantage - more opinions, more points of view, broader network.</p><p>EA Forum shares audience with Less Wrong and I would like to showcase circumstances that got me banned. I hope there are some lessons here.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qqC4s98eFu8XkB6zF/tp9ajhcai6himptywdjg\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qqC4s98eFu8XkB6zF/obgttmdnnbjliwvys7up 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qqC4s98eFu8XkB6zF/mrahoe9snzstn2dokqrx 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qqC4s98eFu8XkB6zF/oosqoauiydltaxd57y6s 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qqC4s98eFu8XkB6zF/shw6gkrhirle2v039urp 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qqC4s98eFu8XkB6zF/nbzl4vuvos1cmfvxhwom 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qqC4s98eFu8XkB6zF/xlxhz35insunrbhnas7r 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qqC4s98eFu8XkB6zF/vpe2ytgrwt1qfcyuusds 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qqC4s98eFu8XkB6zF/swexpub2fzgzfjf7xpma 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qqC4s98eFu8XkB6zF/ijylks105uzb8ow41az3 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qqC4s98eFu8XkB6zF/vpwbbgwwafybtrtd028j 2036w\"></figure><p>Repurposing XKCD: <a href=\"https://xkcd.com/2347/\">https://xkcd.com/2347/</a></p><h1>Systematic issue: gatekeeping and diversity</h1><p>I think I've figured out something relevant and I really wanted to share my thoughts on the forum in order to facilitate <strong>feedback / comments / discussion / critique</strong>. Ended up encountering some issues getting the content published and in the process getting my account banned.</p><p>Of course I could just move on, ignore, carry on with life, find some other place. But I think that my story illustrates some <strong>systematic issue</strong> and I genuinely want to raise awareness about gatekeeping and diversity.</p><p>&nbsp;</p><h2>Gate 1</h2><p>Alignment Forum FAQ: <a href=\"https://www.alignmentforum.org/posts/Yp2vYb4zHXEeoTkJc/welcome-and-faq\">https://www.alignmentforum.org/posts/Yp2vYb4zHXEeoTkJc/welcome-and-faq</a></p><blockquote><p>The LessWrong/Alignment Forum admins monitor activity on both sites, and if someone consistently contributes to Alignment discussions on LessWrong that get promoted to the Alignment Forum, then it\u2019s quite possible full membership will be offered.</p></blockquote><h2>Gate 2&nbsp;</h2><p>New User's Guide to LessWrong: <a href=\"https://www.lesswrong.com/posts/LbbrnRvc9QwjJeics/new-user-s-guide-to-lesswrong\">https://www.lesswrong.com/posts/LbbrnRvc9QwjJeics/new-user-s-guide-to-lesswrong</a></p><h2>Gate 3</h2><p>1st post - too simple&nbsp;</p><p><i>(cannot link to it: my account banned)</i></p><h2>Gate 4</h2><p>2nd post - too complex</p><p><i>(cannot link to it: my account banned)</i></p><blockquote><p>Aim for a high standard if you're contributing on the topic AI.</p></blockquote><h2>Gate 5</h2><p>Chat with the moderator and responding to feedback, making the 2nd post simpler, reducing reading time from 10 to 7 minutes</p><p><i>(cannot access conversation history: banned)</i></p><p>Quotable soundbite:</p><blockquote><p>Because it'd take too long to evaluate new posts (...) I have an overall policy of not reading it in enough detail to make the call.</p></blockquote><h2>Gate 6</h2><p>When the chat ended I've sent an email to the team - I think it was polite (<a href=\"https://imgur.com/a/VaQuI1j\">screenshot</a>)</p><h2>Gate 7</h2><p>When did not receive any response after 3 emails - posted in the open thread - I think it was polite (<a href=\"https://imgur.com/cE0rF65\">screenshot</a>)</p><blockquote><p>There are frequently new&nbsp;<a href=\"https://www.lesswrong.com/tag/ai-questions-open-thread\">\"all questions welcome\" AI Open Threads</a>&nbsp;if that's what you'd like to discuss.</p></blockquote><hr><p>&nbsp;It sounds like a lot of activities but it was once at a time, spread out in time:</p><ol><li>message using widget</li><li>one moderator taking over, chatting via DM</li><li>after &nbsp;going cold: email the team</li><li>after being ignored: comment in the open thread</li></ol><p>Freedom of Speech matters - Elon Musk and Twitter are pretty big on it. I think that on EA and LessWrong - there is no default Freedom of Speech - I think it is more like private property. Admins can be <i>(and are)</i> dictators but I think they are doing disservice to humanity: <strong>echo chambers, filter bubbles</strong>.&nbsp;</p><p>Regarding the actual content - I could be totally wrong. I could be delusional. I could be nuts. If any of these statements are true - I would like to know. It would help me navigate the consensual reality.</p><p><i>(nut job knowing they are crazy - can compensate for their behaviour and act accordingly - it is a nut job not knowing they are crazy that is dangerous)</i></p><p><i>(in other words - you are doing me a favour telling me that I'm nut job - however you need to explain it in a way that I'm able to understand on the level that is available to me)</i></p><p>For those who are curious, here is the link: <a href=\"https://mirror.xyz/0x315f80C7cAaCBE7Fb1c14E65A634db89A33A9637/ETK6RXnmgeNcALabcIE3k3-d-NqOHqEj8dU1_0J6cUg\">https://mirror.xyz/0x315f80C7cAaCBE7Fb1c14E65A634db89A33A9637/ETK6RXnmgeNcALabcIE3k3-d-NqOHqEj8dU1_0J6cUg</a></p><p>I'm a human, living on Earth, my incentive is aligned with the survival.</p><p>If I didn't care I would not engage in a discussion. But I do care and I'm realistic that certain places <i>(such as EA and LessWrong)</i> attract certain audience. Me posting on personal blog would not be good enough, it would not achieve primary objective which was to gather feedback.&nbsp;</p><hr><p>Back to the main point: diversity as competitive advantage.</p><p>As a newcomer to the Less Wrong forum surely I was categorised as \"diversity\".</p><p>It makes me wonder how many other users were rejected in a similar manner?</p><p>It makes me wonder what is the cost-benefits analysis of diversity, culture, filter bubble, echo chambers...&nbsp;</p><p><i>(I can handle criticism and rejection well, currently playing a bigger game talking about culture)</i></p>", "user": {"username": "Mars Robertson"}}, {"_id": "73mcXk9gEq6GpgpFG", "title": "Updates from Campaign for AI Safety", "postedAt": "2023-08-30T05:36:40.220Z", "htmlBody": "<p>Hi!</p><p>\ud83c\udf0d The UK is hosting an international <a href=\"https://www.bbc.com/news/technology-66604123\">AI safety summit on 1\u20142 November at Bletchley Park</a>. This event promises to gather world leaders, AI experts, and industry-shaping AI safety discussions. While attendees and specific topics remain undisclosed, it solidifies the UK's significance in AI safety discussions.</p><p>This presents an opportunity to engage with national governments to convince them to send delegates, propose experts to attend the summit itself, and of course to protest at the summit.</p><hr><h3>\ud83d\udd2c Our latest research</h3><p>\ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1 We commissioned an <a href=\"https://www.roymorgan.com/findings/9339-campaign-for-ai-safety-press-release-august-2023\">opinion poll with Roy Morgan</a> (an Australian polling company) on perception of AI in Australia. A sizeable majority of 57% of Australians believe artificial intelligence (AI) creates more problems than it solves. <strong>Every fifth </strong>Australian anticipates the risk of human extinction from AI within two decades.</p><hr><h3>\ud83d\udcc3 Moratorium treaty competition</h3><p>\ud83c\udf1f We are thrilled to <a href=\"https://www.campaignforaisafety.org/celebrating-the-winners-law-student-moratorium-treaty-competition/\">announce the winners</a> of our <a href=\"https://www.campaignforaisafety.org/law-competition/\">Student AI Moratorium Treaty Competition</a> fostering discussions on large-scale AI ethics and safety.&nbsp;<br><br>\ud83e\udd47First Place: Neil Arven F. Tiozon, Jayson B. Corregidor, Anna Czarina C. Lee,&nbsp;<br>\ud83e\udd48Second Place: Ketan Aggarwal,&nbsp;<br>\ud83e\udd49Third Place: Kanishk Kumar Singh.&nbsp;<br><br>Explore <a href=\"https://www.campaignforaisafety.org/law-competition-submissions/\">submissions on our website</a> (copyright waived), and stay tuned for insights from the competition. Congrats to the winners, participants worldwide, and our judges. Please share the news!</p><hr><h3>\ud83d\udcf8 #PauseAI protests</h3><p>\ud83d\udce3 In a <a href=\"https://twitter.com/PauseAI/status/1690290512643719168?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1690290512643719168%7Ctwgr%5Ef839904b0427a62d67848241c726f4b29b2892cf%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fpauseai.info%2F2023-august-nl\">recent demonstration</a> in The Hague, Netherlands, #PauseAI activists call upon their government for the prioritization of AI risk mitigation. During the event, they delivered impactful speeches, engaged with local residents, distributed informative flyers, and shared moments of camaraderie.</p><p>More info in the <a href=\"https://pauseai.info/2023-august-nl\">press release</a> in both English and Dutch.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/73mcXk9gEq6GpgpFG/h6pbzlcfw60qr0a8upik\"></p><p><strong>Upcoming PauseAI Protests</strong><br>\ud83d\udcc5 Stay tuned for forthcoming international protests, scheduled to take place later this year, in anticipation of the <a href=\"https://pauseai.info/summit\">AI Safety Summit</a>. To ensure you receive an invitation when a protest is organized in your vicinity, join <a href=\"https://pauseai.info/join\">PauseAI community</a>!</p><hr><h3>\ud83d\udcc3 Policy updates</h3><p>On the policy front, we have made our <a href=\"https://campaignforaisafety.ghost.io/ghost/#/editor/post/64df1928560c220001c7b6c1\">submission to the Select Committee on Artificial Intelligence appointed by the Parliament of South Australia</a>. Thank you Sue Anne Wong for the work on this submission.</p><p>Next, we are working on the following:</p><ul><li><a href=\"https://www.un.org/techenvoy/ai-advisory-body\">UN Review on Global AI Governance</a> (UN. Due 30 September 2023)</li><li>The submission to the <a href=\"https://www.campaignforaisafety.org/r/27e322f7?m=866ebbbb-d122-4eec-b595-77a07fe17483\">NSW inquiry into AI</a> (Australia. Due 20 October 2023)</li><li>Update our <a href=\"https://www.campaignforaisafety.org/r/0ea639ce?m=866ebbbb-d122-4eec-b595-77a07fe17483\">main campaign policy document</a>.</li></ul><p>Do you know of other inquiries? Please let us know. You may respond to this email if you want to contribute to the upcoming consultation papers.</p><hr><h3>\ud83d\udcdcPetition updates</h3><p>\ud83e\udd81 For our supporters in the UK, there's an <a href=\"https://petition.parliament.uk/petitions/639956\">ongoing petition by Greg Colbourn</a>. This petition urges the global community to consider a global moratorium on AI technology development due to extinction risks. Currently, the petition gathered 40 signatures in support of this cause.</p><hr><p>Thank you for your support! Please share this email with friends.<br><br>Campaign for AI Safety<br><a href=\"https://www.campaignforaisafety.org/\">campaignforaisafety.org</a></p>", "user": {"username": "Jolyn Khoo"}}, {"_id": "kmRHG3wBHWjMYNLrC", "title": "EA on College Applications ", "postedAt": "2023-08-30T09:30:40.751Z", "htmlBody": "<p>I'm sorry that this isn't explicitly related to EA but as the college I go to will almost certainly have a huge impact on my career I thought it'd be ok. I live in the US and am in the act of applying to colleges right now (almost all of which are in the US). I was wondering if I should mention EA in my college application essays. I don't have anything explicitly related to it but I know some about it and it fits in with the identity I'm trying to present to colleges. I'm worried that it might turn off colleges even if I talk about a pretty mild brand of EA. I'm definitely going to \"hedge my bets\" and only discuss it for some colleges but I was wondering if even that would be stupid. I would really appreciate any feedback. Thanks.</p>\n", "user": {"username": "DiracFan"}}, {"_id": "urE9Kb9BmapL7bqtD", "title": "Request for public input: The EA Handbook  ", "postedAt": "2023-08-30T03:10:29.283Z", "htmlBody": "<h3>TLDR&nbsp;</h3><ul><li>CEA is currently updating the&nbsp;<a href=\"https://forum.effectivealtruism.org/handbook\"><u>EA Handbook</u></a>, a resource used by the&nbsp;<a href=\"https://www.effectivealtruism.org/virtual-programs\"><u>EA Virtual Programs</u></a> and group organizers around the globe.&nbsp;</li><li>We are seeking input from the EA community to help us calibrate our own judgement of what can or should be updated, both with regards to&nbsp;<i>content&nbsp;</i>as well as&nbsp;<i>structure</i>.&nbsp;<a href=\"https://forms.gle/ebvB1gLCsMTZqzsF7\"><u>Fill in the survey here</u></a> by September 8th.&nbsp;</li></ul><h3>Background&nbsp;</h3><p>Every year, the EA Handbook is used by hundreds of participants of the EA Virtual Programs as well as by participants in Intro Fellowships organized by group organizers around the world. It also serves as a common introduction for people who want to get acquainted with the ideas of Effective Altruism outside of any of these programs.&nbsp;<strong>Our best guess is that the Handbook is used by thousands of people every year</strong>, and - alongside interactions with facilitators and other participants - is part of what shapes peoples\u2019 first impression of the movement. Therefore, we think that making sure that the Handbook accurately represents the current paradigms and provides readers with the tools most commonly used by actors in the wider EA ecosystem is a&nbsp;<strong>high-leverage opportunity to improve attraction and retention of community members</strong>, contributing to a healthy and inclusive community.&nbsp;</p><p>The EA Handbook&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/JG6FSimuL35tZuohq/ea-handbook-3-0-what-content-should-i-include\"><u>has been updated</u></a> before, and the latest version was developed in mid-2022. So what is the need for a new version?</p><ul><li>Since the last update, many cohorts of the virtual (and in-person) programs have used this resource and&nbsp;<strong>have provided feedback on it</strong>, which prompted an initiative to update the Handbook again.&nbsp;</li><li>Some of the topics covered in the Handbook have also seen a huge increase in attention in the past ~12 months (especially risks from Artificial Intelligence), which has resulted in the publication of new papers, articles and blog posts that may provide a&nbsp;<strong>more up-to-date overview of current debates</strong> in a given field, and that reflect the degree to which people\u2019s awareness and background knowledge may have changed. We aim to represent and accurately reflect these changes by adding resources or replacing older ones. That said, a more fundamental change with regards to the representation of different cause areas and topics the Handbook focuses on&nbsp;<strong>is currently not planned</strong>.&nbsp;</li><li>Finally, there has been an increased interest in EA recently, which increases the need for&nbsp;<strong>a good centralized starting point</strong> for those who are interested in exploring key ideas and concepts.&nbsp;&nbsp;</li></ul><h3>Call to Action&nbsp;</h3><p><strong>We would like to hear from you!</strong> Apart from consulting users (<i>participants of Intro programs, facilitators of the Virtual Programs and group organisers as well as CEA staff and other stakeholders at relevant EA organisations</i>) and analysing data obtained through the evaluation of said programs,&nbsp;<strong>we would like to get input from the wider community</strong> on what the next EA Handbook should look like.</p><p>To this end, we have created&nbsp;<a href=\"https://forms.gle/ebvB1gLCsMTZqzsF7\"><u>this survey for people to express their opinions</u></a> on the current Handbook and to provide input in the form of wishes, recommendations or criticism that we hope to integrate into the newer version. The&nbsp;<strong>deadline to fill in this survey is September 8th</strong>. If there is anything you feel we should know, don\u2019t hesitate to reach out under&nbsp;<a href=\"mailto:groups@centreforeffectivealtruism.org\"><u>groups@centreforeffectivealtruism.org</u></a>.&nbsp;</p><p>The survey is anonymous, but you are encouraged to leave your contact details in case we want to follow up with you on your thoughts or recommendations.&nbsp;</p>", "user": {"username": "Moritz von Knebel"}}, {"_id": "PAco5oG579k2qzrh9", "title": "LTFF and EAIF are unusually funding-constrained right now", "postedAt": "2023-08-29T23:56:14.484Z", "htmlBody": "<h3>UPDATE 2023/09/13:&nbsp;</h3><p>Including only money that has already landed in our bank account and extremely credible donor promises of funding, LTFF has raised ~1.1M and EAIF has raised ~500K. After Open Phil matching, this means LTFF now has ~3.3M additional funding and EAIF has ~1.5m in additional funding.</p><p>From my (Linch)'s perspective, this means both LTFF nor EAIF are no longer <i>very</i> funding constrained for the time period we wanted to raise money for (the next ~6 months), however both funds are still funding constrained and can productively make good grants with additional funding.</p><p>See <a href=\"https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now?commentId=KtpafhfpNLBHREG7a\">this comment</a> for more details.</p><h1>Summary</h1><p>EA Funds aims to empower thoughtful individuals and small groups to carry out altruistically impactful projects - in particular, enabling and accelerating small/medium-sized projects (with grants &lt;$300K). We are looking to increase our level of independence from other actors within the EA and longtermist funding landscape and are seeking to raise ~$2.7M for the Long-Term Future Fund and ~$1.7M for the EA Infrastructure Fund (~$4.4M total) over the next six months.</p><p><strong>Why donate to EA Funds?&nbsp;</strong>EA Funds is the largest funder of small projects in the longtermist and EA infrastructure spaces, and has had a solid operational track record of giving out hundreds of high-quality grants a year to individuals and small projects. We believe that we\u2019re well-placed to fill the role of a significant independent grantmaker, because of a combination of our track record, our historical role in this position, and the quality of our fund managers.&nbsp;</p><p><strong>Why now?&nbsp;</strong>We think now is an unusually good time to donate to us, as a) we have an unexpectedly large funding shortage, b) there are&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding\"><u>great projects on the margin that we can\u2019t currently fund</u></a>, and c) more stabilized funding now can give us time to try to find large individual and institutional donors to cover future funding needs.&nbsp;</p><p>Importantly, Open Philanthropy is no longer providing a guaranteed amount of funding to us and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching\"><u>instead will move over to a (temporary) model of matching our funds</u></a> 2:1 ($2 from them for every $1 from you, up to 3.5M from them per fund).</p><p><strong>Where to donate:&nbsp;</strong>If you\u2019re interested, you can donate to either Long-Term Future Fund (LTFF) or EA Infrastructure Fund (EAIF)&nbsp;<a href=\"https://www.givingwhatwecan.org/funds/effective-altruism-funds?utm_source=eafunds\"><u>here</u></a>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyd012yijab\"><sup><a href=\"#fnyd012yijab\">[1]</a></sup></span></p><p>Some relevant quotes from fund managers:&nbsp;</p><p><strong>Oliver Habryka</strong></p><p>I think the next $1.3M in donations to the LTFF (430k pre-matching) are among the best historical grant opportunities in the time that I have been active as a grantmaker. If you are undecided between donating to us right now vs. December, my sense is now is substantially better, since I expect more and larger funders to step in by then, while we have a substantial number of time-sensitive opportunities right now that will likely go unfunded.</p><p>I myself have a bunch of reservations about the LTFF and am unsure about its future trajectory, and so haven\u2019t been fundraising publicly, and I am honestly unsure about the value of more than ~$2M, but my sense is that we have a bunch of grants in the pipeline right now that are blocked on lack of funding that I can evaluate pretty directly, and that those seem like quite solid funding opportunities to me (some of this is caused by a large number of participants of the SERI MATS program applying for funding to continue the research they started during the program, and those applications are both highly time-sensitive and of higher-than-usual quality).</p><p><strong>Lawrence Chan</strong></p><p>\u201cMy main takeaway from [evaluating a batch of AI safety applications on LTFF] is [LTFF] could sure use an extra $2-3m in funding, I want to fund like, 1/3-1/2 of the projects I looked at.\u201d (At the current level of funding, we\u2019re on track to fund a much lower proportion).</p><h2>Related links</h2><ul><li><a href=\"https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching\"><u>EA Funds organizational update: Open Philanthropy matching and distancing</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/zZ2vq7YEckpunrQS4/long-term-future-fund-april-2023-grant-recommendations\"><u>Long-Term Future Fund: April 2023 grant recommendations</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding\"><u>What Does a Marginal Grant at LTFF Look Like?</u></a></li><li>Asya Bergal\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/9vazTE4nTCEivYSC6/reflections-on-my-time-on-the-long-term-future-fund\"><u>Reflections on my time on the Long-Term Future Fund</u></a></li><li>Linch Zhang\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/sWMwGNgpzPn7X9oSk/select-examples-of-adverse-selection-in-longtermist\"><u>Select examples of adverse selection in longtermist grantmaking</u></a></li></ul><h2>Our Vision</h2><p>We think there is a significant shortage of independent funders in the current longtermist and EA infrastructure landscape, resulting in fewer outstanding projects receiving funding than is good for the world. Currently, the primary source of funding for these projects is Open Philanthropy, and whilst we share a lot of common ground, we think we add value in the following ways:</p><ul><li>Increasing the total grantmaking capacity within key cause areas.</li><li>Causing great projects to counterfactually happen in the world, or saving time and effort for people doing great projects who would otherwise spend significant time fundraising or waiting for grants to come in.&nbsp;</li><li>Supporting a set of worldviews that we find plausible and that are not currently well represented among grantmakers (though we have substantial overlap with Open Philanthropy\u2019s worldview and there is a range of views on how much we should be directly optimizing for diversification away from their perspectives).</li><li>Emphasizing contact with reality: most of our grantmakers spend most of their time trying to directly solve problems of importance within their cause area, rather than engaging in \u201cmeta\u201d activities like grantmaking. We think this is important as grantmaking often has very poor feedback loops (particulalry longtermist grantmaking).</li><li>Provide early stage funding to allow applicants to test their fit for work and \u201cget ready\u201d to seek funding from other funders that specialize in larger grant sizes.</li><li>Improving the epistemic environment within EA by making it easier for smaller projects to disagree with Open Philanthropy without worrying that this will significantly reduce their chance of being funded in the future.</li><li>Helping to identify harmful projects whilst being aware of factors such as the unilateralist curse and information cascades.</li><li>Increasing the resilience, robustness and diversity of funders within EA and longtermism.</li></ul><p>Alongside the above, EA Funds has ambitions to pursue new ways of generating value by:</p><ul><li>Creating an expert-led active grant-making program to create counterfactual impactful projects (starting with longtermist information security).</li><li>Modeling and shaping community norms of transparency, integrity, and criticism to improve the epistemic environment within EA and associated communities.</li></ul><h2>Our Ask</h2><p><strong>We are looking to raise ~$4.4M from the general public</strong> to support our work over the next 6 months:</p><ol><li>~$2.7M for the Long-Term Future Fund.<ol><li>This is ~2M above our expected 720k donations in the next 6 months.</li></ol></li><li>~$1.7m for the EA Infrastructure Fund.<ol><li>This is ~1.3M above our expected 400k donations in the next 6 months.</li></ol></li></ol><p>This will be matched by Open Phil at a 2:1 rate ($2 from Open Phil per $1 donated to a fund) with a ceiling of a $3.5m contribution from Open Phil (per fund). You can read more about the matching&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching\"><u>here</u></a>.</p><p>The EAIF and LTFF have received very generous donations from many individuals in the EA community. However, donations to the EAIF and LTFF have recently been quite low, especially relative to the quality and quantity of applications we\u2019ve had in the last year. While much of this is likely due to the FTX crash and subsequently increased funding gaps of other longtermist organizations, our guess is that this is partially due to tech stocks and crypto doing poorly in the last year (though we hope that recent market trends will bring back some donors).</p><h3>Calculation for LTFF funding gap</h3><p>The LTFF has an estimated ideal dispersal rate of $1M/month, based on our&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding#_5M\"><u>post-November 2022 funding bar</u></a> that Asya estimated<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9u90kjafzyb\"><sup><a href=\"#fn9u90kjafzyb\">[2]</a></sup></span>&nbsp;from looking at the funding gaps and marginal resources within the longtermist ecosystem overall. This is $6M over the next 6 months.</p><p>I also think LTFF donors should pay $200k over the next 6 months ($400k annualized) as their \u201cfair share\u201d of EA Funds operational costs. So in total, LTFF would like to spend $6.2M over the next 6 months.</p><p>Caleb&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching#LTFF_funding_gap\"><u>estimated</u></a> ~$700k in expected donations from individuals by default in the next 6 months, based solely on extrapolation from past trends. With Open Phil donation matching, this comes out to a total of $2.1M in expected incoming funds, or a shortfall of $4.1M.</p><p>To cover the remaining $4.1M, we would like individual donors to contribute an additional $2M, where Open Phil will provide $2.1M of matching for the first $1.05M.&nbsp;</p><p>To get a sense of what projects your marginal dollars can buy, you might find it helpful to look at the<a href=\"https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding#_5M\"><u> $5M tier of the LTFF Funding Thresholds Post</u></a>.&nbsp;</p><h3>Calculation for EAIF funding gap</h3><p>The EAIF has an estimated ideal dispersal rate of $800k/month, based on the proportion of our historic spend rate that we believe is above Open Phil\u2019s bar for EA community building projects (though note that this was based on fairly brief input from Open Phil and I didn\u2019t check with them about whether they agree with this claim). This is $4.8M over the next 6 months.</p><p>I also think EAIF donors should pay $200k over the next 6 months ($400k annualized) as their \u201cfair share\u201d of EA Funds operational costs. So in total, EAIF would like to spend $5M over the next 6 months.&nbsp;</p><p>Caleb estimated $400k in expected donations from individuals by default in the next 6 months, based solely on extrapolation from past trends. With Open Phil donation matching, this comes out to a total of $1.2M in expected incoming funds, or a shortfall of $3.8M.</p><p>To cover the remaining $3.8M, we would like individual donors to contribute an additional $1.3M, where Open Phil will provide 2.5M in donation matching.</p><h3>Potential change for operational expenses payment</h3><p><strong>Going forwards, we would also like to move towards a model where donors directly pay for our operational expenses</strong> (currently we fundraise for operational expenses separately, so 100% of donations from public donors goes to our grantees). We believe that the newer model is more transparent, as it lets all donors more clearly see the true costs and cost-benefit ratio for their donations<strong>. However, making the change is still pending internal discussions, community feedback, and logistical details.&nbsp;</strong>We will make a separate announcement if and when we switch to a model where a percentage of public donations go to cover our operational expenses. See Appendix A for a calculation of operational expenses.</p><h2>Why give to EA Funds?</h2><p>We think EA Funds is well-positioned to be a significant independent grantmaker for the following reasons.</p><ol><li>We&nbsp;<a href=\"https://funds.effectivealtruism.org/team\"><strong><u>have knowledgeable part-time fund managers</u></strong></a><strong> who do direct work in their day jobs:</strong> we have built several grantmaking teams with a broad range of expertise. These managers usually dedicate the majority of their time to hands-on efforts addressing critical issues. We believe this direct experience enhances their judgment as grantmakers, enabling them to pinpoint important and critical projects with high accuracy.&nbsp;</li><li><strong>Specialization in early-stage grants:</strong> we made over 300 grants of under $300k in 2022. To our knowledge, that\u2019s more grants of this size than any other EA-associated funder.</li><li><strong>We are the largest open application funding source&nbsp;</strong>(that we are aware of) within our cause areas. Our application form is always open, anyone can apply, and grantees can apply for a wide variety of projects relevant to our funds\u2019 purposes (as opposed to e.g. needing to cater to narrow requests for proposals). We believe this is critical to us having access to grant opportunities that other funders do not have access to, allowing us to rely on formal channels rather than informal networks.</li><li><strong>Our operational track record</strong>. In 2022, EA Funds paid out ~$35M across its four Funds, with $12M to the Long-Term Future Fund, $13M to the EA Infrastructure Fund, $6.4M to the Animal Welfare Fund, and $4.8M to the Global Health and Development Fund. This requires (among others) clearing nontrivial logistical hurdles in following nonprofit law across multiple countries, consistent operational capacity, and a careful eye towards&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/sWMwGNgpzPn7X9oSk/select-examples-of-adverse-selection-in-longtermist\"><u>downside risk mitigation</u></a>.</li><li><strong>We believe our grants are highly cost-effective.&nbsp;</strong>Our current best guess is that we have successfully identified and given out grants of similar ex-ante quality to (e.g.) Open Phil\u2019s AI safety and community building grants, some of which Open Phil would counterfactually not have funded.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsxw2bil6v9\"><sup><a href=\"#fnsxw2bil6v9\">[3]</a></sup></span>&nbsp;This gives donors an opportunity to provide considerable value.&nbsp;</li><li>We are investigating<strong> new value streams</strong>. We would like to pursue \u2018DARPA-style\u2019 active grantmaking in priority areas (starting with information security). We are also actively considering setting up an AI Safety-specific fund, encouraging donors interested in AI safety (but not EA or longtermism) to donate to projects that mitigate large-scale globally catastrophic AI risks.&nbsp;</li><li>We are one&nbsp;of the <strong>main public longtermist donation options</strong> available for individual donors to support. We believe that we are a relatively transparent funder, and we are currently thinking about how we can increase our transparency further whilst moving more quickly and maintaining our current standard of decision-making.</li></ol><p>We are primarily looking for funding to support the Long-Term Future Fund and the EA Infrastructure Fund\u2019s grantmaking.</p><p>The Long-Term Future Fund is primarily focused on reducing catastrophic risks from advanced artificial intelligence and biotechnology, as well as building and equipping a community of people focused on safeguarding humanity\u2019s future potential. The EA Infrastructure Fund is focused on increasing the impact of projects that use the principles of effective altruism, in particular amplifying the efforts of people who aim to do an ambitious amount of good from an impartial welfarist and scope-sensitive perspective. We have included some examples of grants each fund has made in the&nbsp;<a href=\"https://docs.google.com/document/d/1-fKZJfgybYMjfUZc2_G-WwTbkQqFBeDEtbrRW4Th_k8/edit#heading=h.6pad9vbogqm7\"><u>highlighted grants</u></a> section.</p><h2>Our Fund Managers</h2><p>We lean heavily on the experience and judgement of our fund managers. We have around five fund managers on each fund at any given time.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhamlm91a3z\"><sup><a href=\"#fnhamlm91a3z\">[4]</a></sup></span>Our current fund managers include:</p><ul><li><strong>Linchuan Zhang (LTFF):</strong> Linchuan (Linch) Zhang is a Senior Researcher at Rethink Priorities working on existential security research. Before joining RP, he worked on time-sensitive forecasting projects around COVID-19. Previously, he programmed for Impossible Foods and Google and has led several EA local groups.</li><li><strong>Oliver Habryka (LTFF)</strong>: Oliver runs Lightcone Infrastructure, whose main product is Lesswrong. Lesswrong has significantly influenced conversations around rationality and AGI risk, and the LWits community is often credited with having realized the importance of topics such as AGI (and AGI risk), COVID-19, existential risk and crypto much earlier than other comparable communities.</li><li><strong>Peter Wildeford (EAIF):</strong> co-executive director and co-founder of&nbsp;<a href=\"https://rethinkpriorities.org/\"><u>Rethink Priorities</u></a>, a think tank dedicated to figuring out the best ways to make the world a better place.</li></ul><h3>Guest Fund Managers</h3><p><strong>Daniel Eth (LTFF):&nbsp;</strong>Daniel's research has spanned several areas relevant to longtermism, and he's currently focused primarily on AI governance. He was previously a Senior Research Scholar at the Future of Humanity Institute, and he has a PhD in Materials Science and Engineering from UCLA. He is currently self-employed.</p><p><strong>Lauro Langosco (LTFF):</strong> Lauro is a PhD student with David Krueger at the University of Cambridge. His work focused broadly on AI Safety, in particular on demonstrations of alignment failures, forecasting AI capabilities, and scalable AI oversight.</p><p><strong>Lawrence Chan (LTFF):</strong>&nbsp;Lawrence is a researcher at&nbsp;<a href=\"https://evals.alignment.org/\">ARC Evals</a>, working on safety standards for AI companies. Before joining ARC Evals, he worked at&nbsp;<a href=\"https://www.redwoodresearch.org/\">Redwood Research</a> and as a PhD Student at the&nbsp;<a href=\"https://humancompatible.ai/\">Center for Human Compatible AI</a> at UC Berkeley.</p><p><strong>Thomas Larsen (LTFF):&nbsp;</strong>Thomas was an alignment research contractor at MIRI, and he is currently running the Center for AI Policy, where he works on AI governance research and advocacy.</p><p><strong>Clara Collier (LTFF)</strong>: Clara is the managing editor of Asterisk, a quarterly journal focused on communicating insights on important issues. Before, she worked as an independent researcher on existential risks. She has a Masters in Modern Languages from Oxford.</p><p><strong>Michael Aird (EAIF)</strong>:<strong>&nbsp;</strong>Michael Aird is a Senior Research Manager in Rethink Priorities' AI Governance and Strategy team. He also serves as an advisor to organizations such as Training for Good and is an affiliate of the Centre for the Governance of AI. His prior work includes positions at the Center on Long-Term Risk and the Future of Humanity Institute.</p><p><strong>Huw Thomas (EAIF):&nbsp;</strong>Huw is currently working part-time on various projects (including a contractor role at 80,000 hours). Prior to this, he worked as a media associate at Longview Philanthropy, a groups associate at the Centre for Effective Altruism and was a recipient of the CEA Community Building Grant for his work at Effective Altruism Oxford.</p><p><i>You can find a full list of our fund managers&nbsp;</i><a href=\"https://funds.effectivealtruism.org/team\"><i>here</i></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3iuaaj786kj\"><sup><a href=\"#fn3iuaaj786kj\">[5]</a></sup></span></p><p>If you have more questions, feel free to leave a comment here. Caleb Parikh and the fund managers are also happy to talk to donors potentially willing to give &gt;$30k. Linch Zhang, in particular, has volunteered himself to talk about the LTFF.</p><h2>Highlighted Grants</h2><p>EA Funds has identified a variety of high-impact projects, at least some of which we think are unlikely to have been funded elsewhere. (However, for any specific grant listed below, we think there\u2019s a fairly high probability they\u2019d otherwise be funded in some form or another; figuring out counterfactuals is often hard).</p><p><strong>From the Long-Term Future Fund:</strong></p><ul><li>David Krueger - $200,000<ul><li>Computing resources and researcher stipends at a new deep learning + AI alignment research group at the University of Cambridge.</li></ul></li><li>Alignment Research Center - $72,000<ul><li>A research &amp; networking retreat for winners of the Eliciting Latent Knowledge contest with the aim of fostering promising research collaborations between junior researchers.</li></ul></li><li>SERI MATS program - $316,000<ul><li>8-week scholars program to pair promising alignment researchers with renowned mentors. This program has now grown into a&nbsp;<a href=\"https://www.serimats.org/mentors\"><u>more established program</u></a> producing multiple people working full-time on alignment in established research organizations (with a smaller number of people pursuing independent research or starting new organizations).</li></ul></li><li>Manifold Markets - $200,000<ul><li>Stipend and expenses for 4 months for 3 FTE to build a forecasting platform made available to the public based on user-created play-money prediction markets</li></ul></li><li>Daniel Filan - $23,544<ul><li>We recommended a grant of $23,544 to pay Daniel Filan for his time making 12 additional episodes of the AI X-risk Research Podcast (AXRP), as well as the costs of hosting, editing, and transcription.</li></ul></li></ul><p><strong>From the EA Infrastructure Fund:</strong></p><ul><li>Shauna Kravec &amp; Nova DasSarma - $50,000:&nbsp;<ul><li>Compute infrastructure and dedicated support for AI safety researchers to run technical AI experiments. This later became&nbsp;<a href=\"http://hofvarpnir\"><u>Hofvarpnir Studios</u></a> which used to provide compute for Jacob Steinhardt\u2019s lab at UC Berkeley and the Center for Human-Compatible Artificial Intelligence (CHAI).</li></ul></li><li>Finlay Moorhouse and Luca Righetti - $38,200<ul><li>Ongoing support for&nbsp;<a href=\"https://hearthisidea.com/\"><u>\"Hear This Idea</u></a>\", a podcast showcasing new thinking in effective altruism.</li></ul></li><li>Laura Gonzalez Salmer\u00f3n, Sandra Malag\u00f3n - $43,308<ul><li>12-month stipend to coordinate and grow the EA Spanish speakers community and its projects.</li></ul></li><li>Czech Association for Effective Altruism - $ 8,300<ul><li>Expenses and stipend to create a short Czech book (~130 pgs) and brochure (~20 pgs) with a good introduction to EA in digital and print formats.</li></ul></li></ul><p><i>See a complete list of our public grants at&nbsp;</i><a href=\"https://funds.effectivealtruism.org/grants\"><i><u>this</u></i></a><i> link. You can also read the most recent payout report by LTFF&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/zZ2vq7YEckpunrQS4/long-term-future-fund-april-2023-grant-recommendations\"><i><u>here</u></i></a><i>.&nbsp;</i></p><h2>Planned actions over the next six months</h2><p>To achieve our goals of empowering thoughtful people to pursue impactful projects, we'll attempt to do the following:</p><ul><li>Asya Bergal&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching\"><u>will step down</u></a> as chair of LTFF (Max Daniel has already stepped down as chair of the EAIF). Max and Asya both work for Open Phil, and we want to increase our separation from Open Phil.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpwyc17wm0ag\"><sup><a href=\"#fnpwyc17wm0ag\">[6]</a></sup></span><ul><li>Open Phil also wanted to reduce entanglements between the two organizations, in part to mitigate downside reputational risks.&nbsp;</li></ul></li><li>We are looking to find new fund chairs for both LTFF and EAIF.&nbsp;</li><li>We plan to onboard more fund managers to grow each fund substantially (aiming to double the staffing of each fund).<ul><li>In recent months, LTFF has onboarded Lauro Langosco and Lawrence Chan who will primarily focus on technical alignment grantmaking, as well as Clara Collier for her expertise in communications and general longtermism. The EAIF is in the process of onboarding new fund managers.</li></ul></li><li>Open Phil&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching\"><u>has agreed</u></a> to give us a 2:1 match for up to $7M total (up to $3.5M to each of EAIF and LTFF) for a 6-month period. While our ultimate goal is to develop our own robust funding base, in 2022, Open Philanthropy provided 40% of the funding for the Long-Term Future Fund and 84% for the EA Infrastructure Fund.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefizbbefz7rym\"><sup><a href=\"#fnizbbefz7rym\">[7]</a></sup></span>&nbsp;We see donation matching as a realistic intermediary step while enabling us to pursue more intellectual independence.<ul><li>This model replaces fixed grants from Open Philanthropy. This reduces the likelihood of your donations being fungible: previously an extra $1 to EA Funds in fundraising could result in a $1 reduction in Open Philanthropy\u2019s grants to us, diverting those funds to their other projects. This newer approach allows funders to donate to EA Funds and support the specific value proposition that we, as opposed to Open Philanthropy, present.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwqj0clhbbi9\"><sup><a href=\"#fnwqj0clhbbi9\">[8]</a></sup></span></li></ul></li><li>We are considering hiring or contracting out more non-grantmaking duties (eg website, project management, fundraising, communications) at EA Funds. Right now Caleb is the only full-time employee of EA Funds and plausibly having 0.5-1.5 more FTEs at EA Funds will help both existing projects go more smoothly, as well as unlock new ambitious opportunities.</li><li>We are working with external investigators to do retroactive evaluations of past EAIF and LTFF grants, with the hopes that we can then have a clearer picture of a) how well the impact of our past grants compares to e.g., Open Phil\u2019s, b) which of our broader categories of historical grants have been the most impactful, and c) other qualitative insights to help us improve further.</li><li>We aim to improve the operations of our passive grantmaking (funding of open grant applications) program with a focus on improving the grantee experience by providing more support to grantees and getting back to grantees much more quickly<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref25bgwshzbdx\"><sup><a href=\"#fn25bgwshzbdx\">[9]</a></sup></span></li><li>We are trying to reconceptualize and reframe the value proposition and strategic direction of EAIF in the coming months. While much of this will be contingent on the vision of the incoming fund chair, we\u2019d like EAIF to have a more coherent and targeted vision, strategy, and coherent value proposition to donors going forwards.</li><li>We plan to create a new AI Safety specific program, for donors outside of EA/Longtermism who want to decrease catastrophic risks from AI. We hope that such a program can inspire new donors to give to AI safety projects.</li><li>EA Funds is pursuing active grant-making programs, where we\u2019ll actively seek out promising projects to fund. We\u2019ll initially focus on&nbsp;<a href=\"https://80000hours.org/career-reviews/information-security/\"><u>Information Security</u></a> field building. The current plan is for this program to initially be funded by Open Philanthropy, though if you are interested in contributing to this program in particular, please let us know.</li></ul><h2>Potential negatives to be aware of</h2><p>Here are some reasons you might&nbsp;<i>not</i> want to donate to EA Funds:</p><h3>Potential downside risks of LTFF or EAIF</h3><ul><li><strong>Inability to fully screen for or prevent unilateral downside risks:&nbsp;</strong>EA Funds has much less control over and offers less guidance to our grantees than, e.g., the executive directors of a moderately-sized EA organization. So compared to larger organizations, we may be less able to prevent unilateral downside risks like the&nbsp;<i>sharing of information hazards</i>, or<i>&nbsp;</i>actions that pose&nbsp;<i>reputational risks</i> to effective altruism at large, or to specific EA subfields.</li><li><strong>Centralization of funds:&nbsp;</strong>In contrast, we are also implicitly asking for the centralization of funds from private donors to a single grantmaking entity. To the extent that you believe your counterfactual for donating to EA Funds is better and/or more centralization is bad, you may wish to donate directly rather than pool your funds with other LTFF or EAIF donors.</li><li><strong>Waste/Inefficient usage of human capital: </strong>Giving money to EA Funds rather than larger organizations implicitly subsidizes a culture and community of grantseekers who are supported by small grants. To the extent that you believe this is a less efficient usage of human capital than plausible counterfactuals for talented people (e.g. getting a job in tech, policy, or academia), you might want to shift away from EA grantmakers that give relatively small individual grants.</li></ul><p>Note that we consider these issues to be structural and do not realistically expect resolutions to these downside risks going forwards.&nbsp;</p><h3>Areas of improvement for the LTFF and EAIF</h3><p>Historically, we\u2019ve had the following (hopefully fixable) problems:</p><ul><li><strong>Slower than ideal response times</strong>: in the past year, our median response time has been around 4 weeks with high variance; we\u2019d like to get this down to closer to 2 weeks with 95% of applications responded to in 4 four weeks.</li><li><strong>Limited feedback/advice given to grantees</strong>: we generally don\u2019t give feedback to rejected applicants. We currently give&nbsp;<i>some</i> feedback to promising grantees but much less than we\u2019d give if we had more grantmaking capacity.</li><li><strong>Insufficient active grantmaking</strong>: We spend some time trying to improve our grantees\u2019 projects, but we have invested fairly little in active grantmaking (actively identifying promising projects and creating/supporting them).</li><li><strong>Missing areas of subject matter expertise</strong>: The scopes of both funds are quite expansive. This means sometimes all of the existing grantmakers lack sufficient direct technical subject matter expertise to evaluate grants in certain areas, and thus have to rely on external experts. For example, the LTFF does not currently have a technical expert in biosecurity.</li></ul><p>For more, you can read Asya\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/9vazTE4nTCEivYSC6/reflections-on-my-time-on-the-long-term-future-fund\"><u>reflections on her time as chair of LTFF</u></a>.</p><h2>EAIF vs LTFF</h2><p>Some donors are interested in giving to both the EAIF and LTFF and would like advice on which fund is a better fit for them.</p><p>We think that the EAIF is a better fit for donors who:</p><ul><li>Are interested in supporting a portfolio of meta projects covering a range of plausible worldviews (both longtermist and non-longtermist).</li><li>Are interested in building EA and adjacent communities.</li><li>Believe that EA (and EA community building) has historically been very good for the world.</li><li>Believe in multiplier effect arguments (donating $100 to an EA group could plausibly create far more than $100 in donation to high-impact charities by encouraging more people to donate).</li><li>Expect the EAIF and LTFF to have similar diminishing marginal returns curves and want to donate to the fund with lower funding. (EAIF and LTFF each receive about 1000 grant applications per year, but EAIF has less funding currently committed)</li></ul><p>We think that the LTFF is a better fit for donors who are:</p><ul><li>More compelled by longtermist cause areas than other EA cause areas.</li><li>Particularly interested in AI safety.</li><li>Are more interested in direct work than \"meta\" work that have a longer chain of impact/reasoning.</li><li>Are more excited about the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding#_5M\"><u>$5M tier of marginal LTFF grants</u></a> than what they consider to be the marginal EAIF grant.</li></ul><h2>Closing thoughts</h2><p>This post was written by Caleb Parikh and Linch Zhang. Feel free to ask questions or give us feedback in the comments below.</p><p>If you are interested in donating to either LTFF or EAIF, you can do so&nbsp;<a href=\"https://www.givingwhatwecan.org/funds/effective-altruism-funds?utm_source=eafunds\"><u>here</u></a>.</p><h2>Appendix A: Operational expenses calculations and transparency.</h2><p>In the last year, EA Funds has dispersed $35M and spent ~700k in operational expenses. The vast majority of the operational expenses were spent on LTFF and EAIF, as the global health and development fund and animal welfare fund are operationally much simpler.</p><p>&nbsp;Historically, ~60-80% of the operational expenses are paid to&nbsp;<a href=\"https://ev.org/ops/about/\"><u>EV Ops</u></a>, for grant disbursement, tech, legal, other ops, etc.</p><p>The remaining 20-40% is used for:</p><ul><li>Caleb's salary, who leads EA Funds (~$100k/year plus benefits).</li><li>Payments for grantmakers at $60/hour, though many volunteer for free.</li><li>Contractors who work on different projects, earning between $35-$100/hour.</li></ul><p>I (Linch) ballparked the expected annual expenditures going forwards (assuming no cutbacks) to be ~800k annually. I estimated the increase due to a) inflation and b) us wanting to take on more projects, with some savings from us slowing down the rate of dispersals a little. But this estimate is not exact.</p><p>Since LTFF and EAIF incur the highest expenses, I suggest donors to each should contribute around $400k yearly, or $200k every six months.</p><p>As for where we might cut or increase spending:</p><ul><li>Reducing EV Ops costs would be challenging and may require moving EA Funds out of EV and building our own grant ops team.</li><li>Reducing Caleb's working hours would be challenging.</li></ul><p>I think my own hours at EAF are somewhat contingent on operational funding. In the last month, I\u2019ve been spending more than half of my working hours on EA Funds (EA Funds is buying out my time at RP), mostly helping Caleb with communications and strategic direction. I will like to continue doing this until I believe EA Funds is in a good state (or we decide to discontinue or sunset projects I\u2019m involved in). Obviously whether there is enough budget to pay for my time is a crux for whether I should continue here.&nbsp;</p><p>Assuming we can pay for my time, other plausible uses of marginal operational funding include: a) whether we pay external investigators for extensive or just shallow retroactive evaluations, b) whether we attempt to launch new programs, c) whether the new infosec, AI safety project, etc websites have professional designers, etc. My personal view is that marginal spending on EA Funds expenses is quite impactful relative to other possible donations, but I understand if donors do not feel the same way and they will prefer a higher percentage of donations go directly to our grantees (currently it\u2019s 100% but proposed changes may move this to ~ 94-97%).&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyd012yijab\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyd012yijab\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The Long-Term Future Fund and the EA Infrastructure Fund are part of EA Funds, which is a fiscally sponsored project of Effective Ventures Foundation (UK) (\u201cEV UK\u201d) and Effective Ventures Foundation USA Inc. (\u201cEV US\u201d). Donations to LTFF and EAIF are donations to EV US or EV UK. Effective Ventures Foundation (UK) (EV UK) is a charity in England and Wales (with registered charity number 1149828, registered company number 07962181, and is also a Netherlands registered tax-deductible entity ANBI 825776867). Effective Ventures Foundation USA Inc. (EV US) is a section 501(c)(3) organization in the USA (EIN 47-1988398). Please see important state disclosures&nbsp;<a href=\"https://ev.org/charitable-solicitation-disclosures/\"><u>here</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9u90kjafzyb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9u90kjafzyb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Note that our current funding bar is higher as a result of anticipated funding/liquidity shortages.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsxw2bil6v9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsxw2bil6v9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is a pretty loose statement partially because impact evaluation is quite hard in the fields we work in, and partially due to insufficient time investment in our evaluations. We are working with external investigators to establish better metrics and to have external retrospective evaluations. Potential cruxes for the value of our work (relative to larger entities like Open Phil) includes the value of independent researchers and small projects, and the value of having a wider range of longtermist worldviews.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhamlm91a3z\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhamlm91a3z\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is generally a mix of experienced fund managers and less experienced assistant fund managers.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3iuaaj786kj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3iuaaj786kj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Though note that the current list is out of date.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpwyc17wm0ag\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpwyc17wm0ag\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I think that it\u2019s useful to note that I don\u2019t expect substantive world view shifts from making this change relative to our previous grantmaking. However, I think we will be a bit less likely to suffer from Open Phil correlated sources of error.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnizbbefz7rym\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefizbbefz7rym\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The donations to these funds only totaled $7.4m and $10m respectively, less than the total amount of grants disbursed that year.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwqj0clhbbi9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwqj0clhbbi9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Open Philanthropy is also on board with the aim of wanting the funding landscape to be more independent and for funders to be able to more legibly donate in non-fungible ways.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn25bgwshzbdx\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref25bgwshzbdx\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We aim to get back to 90% of grantees within three weeks, currently our median decision response time is 28 days.</p></div></li></ol>", "user": {"username": "Linch"}}, {"_id": "ujyF7MDmRFnEdCRjF", "title": "The God of Humanity, and the God of the Robot Utilitarians", "postedAt": "2023-08-29T23:42:04.485Z", "htmlBody": "<p>My personal religion involves two gods \u2013 the god of humanity (who I sometimes call \"Humo\") and the god of the robot utilitarians (who I sometimes call \"Robutil\").&nbsp;</p><p>When I'm facing a moral crisis, I query my <a href=\"https://www.lesswrong.com/posts/X79Rc5cA5mSWBexnd/shoulder-advisors-101\">shoulder</a>-Humo and my shoulder-Robutil for their thoughts. Sometimes they say the same thing, and there's no real crisis. For example, some naive young EAs try to be utility monks, donate all their money, never take breaks, only do productive things... but Robutil and Humo both agree that quality intellectual world requires slack and psychological health. (Both to <a href=\"https://www.lesswrong.com/posts/WuyNHrDXcGFgkZpBy/my-slack-budget-3-surprise-problems-per-week\">handle crises</a> and to <a href=\"https://www.lesswrong.com/posts/fwSDKTZvraSdmwFsj/slack-gives-you-space-to-notice-reflect-on-subtle-things\">notice subtle things</a>, which you might need, even in <a href=\"https://www.lesswrong.com/posts/mmHctwkKjpvaQdC3c/what-should-you-change-in-response-to-an-emergency-and-ai\">emergencies</a>)</p><p>If you're an aspiring effective altruist, you should definitely at <i>least</i> be doing all the things that Humo and Robutil agree on. (i.e. get to to the middle point of <a href=\"https://forum.effectivealtruism.org/posts/AjxqsDmhGiW9g8ju6/effective-altruism-in-the-garden-of-ends\">Tyler Alterman's story here</a>).</p><p>But Humo and Robutil in fact disagree on some things, and disagree on emphasis.&nbsp;</p><p>They disagree on how much effort you should spend to avoid <a href=\"https://forum.effectivealtruism.org/posts/2BEecjksNZNHQmdyM/don-t-be-bycatch\">accidentally recruiting people you don't have much use for</a>.</p><p>They disagree on how many people it's acceptable to accidentally fuck up psychologically, while you experiment with new programs to empower and/or recruit them.</p><p>They disagree on how hard to push yourself to grow better/stronger/wiser/faster, and how much you should sacrifice to do so.</p><p>Humo and Robutil each struggle to understand things differently. Robutil eventually acknowledges you need Slack, but it didn't occur to him initially. His understanding was born in the burnout and tunnel-vision of thousands of young idealists, and Humo eventually (patiently, kindly) saying \"I <i>told</i> you so.\" (Robutil responds \"but you didn't provide any arguments about how that maximized utility!\". Humo responds \"but I said it was obviously unhealthy!\" Robutil says \"wtf does 'unhealthy' even mean? <a href=\"https://www.lesswrong.com/posts/WBdvyyHLdxZSAMmoz/taboo-your-words\">taboo</a> unhealthy!\")</p><p>It took Robutil longer still to consider that perhaps humans (with their current self-awareness) not only need to prioritize their own wellbeing and your friendships, but that it can be valuable to prioritize them <i>for their own sake</i>, not just as part of a utilitarian calculus, because trying to justify them in utilitarian terms may be a subtly wrong step in the dance that leaves them hollow, burned out for years &nbsp;</p><p>(Though Robutil notes that this is likely <a href=\"https://www.lesswrong.com/posts/SfZRWxktiFFJ5FNk8/the-god-of-humanity-and-the-god-of-the-robot-utilitarians?commentId=RpN7kQkPL36nGjmK6\">a temporary state of affairs</a>. A human with sufficiently nuanced self-knowledge can probably wring more utilons out of their wellbeing activities)</p><p>Humo struggles to acknowledge that if you spend all your time making sure to uphold deontological commitments to avoid harming the people in your care, then this effort is in fact measured in real human beings who suffer and die because you took longer to scale up your program.&nbsp;</p><p>In my headcanon, Humo and Robutil are gods who are old and wise, and they got over their naive struggles long ago. They respect each other as brothers. They understand that each of their perspectives is relevant to the overall project of human flourishing. They don't disagree as much as you'd naively expect, but they speak different languages and emphasize things differently.&nbsp;</p><p>Humo might acknowledge that I can't take care of everyone, or even respond compassionately to all the people who show up in my life who I don't have time to help. But he says so with a warm, <a href=\"https://www.lesswrong.com/posts/gs3vp3ukPbpaEie5L/deliberate-grieving-1\">mournful</a> compassion, whereas Robutil says in with brief, efficient <a href=\"https://www.lesswrong.com/posts/gs3vp3ukPbpaEie5L/deliberate-grieving-1?commentId=TQuxyPGL7L8nmNe9Z\">ruthlessness</a>.</p><p>I find it useful to query them independently, and to imagine the wise version of each of them as best I can \u2013 even if my imagining is but a crude shadow of their idealized platonic selves.</p>", "user": {"username": "Raemon"}}, {"_id": "gYoB8vZcPGcL3cAKH", "title": "Language models surprised us", "postedAt": "2023-08-29T21:18:27.627Z", "htmlBody": "<p><i>Note: This post was crossposted from </i><a href=\"https://www.planned-obsolescence.org/language-models-surprised-us/\"><i>Planned Obsolescence</i></a><i> by the Forum team, with the author's permission. The author may not see or respond to comments on this post.</i></p><blockquote><p>Most experts were surprised by progress in language models in 2022 and 2023. There may be more surprises ahead, so experts should register their forecasts now about 2024 and 2025.</p></blockquote><p><i>Kelsey Piper co-drafted this post. Thanks also to Isabel Juniewicz for research help.</i></p><p>If you read media coverage of ChatGPT \u2014 which called it <a href=\"https://www.pcworld.com/article/1424575/chatgpt-is-the-future-of-ai-chatbots.html?ref=planned-obsolescence.org\"><u>\u2018breathtaking\u2019, \u2018dazzling\u2019</u></a>, \u2018<a href=\"https://www.vanityfair.com/news/2022/12/chatgpt-question-creative-human-robotos?ref=planned-obsolescence.org\"><u>astounding</u></a>\u2019 \u2014 you\u2019d get the sense that large language models (LLMs) took the world completely by surprise. Is that impression accurate?</p><p>Actually, yes. There are a few different ways to attempt to measure the question \u201cWere experts surprised by the pace of LLM progress?\u201d but they broadly point to the same answer: ML researchers, <a href=\"https://en.wikipedia.org/wiki/Superforecaster?ref=planned-obsolescence.org\"><u>superforecasters</u></a>,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefb33o8yorx7f\"><sup><a href=\"#fnb33o8yorx7f\">[1]</a></sup></span>&nbsp;and most others were all surprised by the progress in large language models in 2022 and 2023.</p><h3>Competitions to forecast difficult ML benchmarks</h3><p>ML benchmarks are sets of problems which can be objectively graded, allowing relatively precise comparison across different models. We have data from forecasting competitions done in 2021 and 2022 on two of the most comprehensive and difficult ML benchmarks: the <a href=\"https://paperswithcode.com/dataset/mmlu?ref=planned-obsolescence.org\"><u>MMLU benchmark</u></a> and the <a href=\"https://paperswithcode.com/dataset/math?ref=planned-obsolescence.org\"><u>MATH benchmark</u></a>.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjwvi3c7nqsb\"><sup><a href=\"#fnjwvi3c7nqsb\">[2]</a></sup></span></p><p>First, what are these benchmarks?</p><p>The <a href=\"https://arxiv.org/abs/2009.03300?ref=planned-obsolescence.org\"><u>MMLU</u></a> dataset consists of multiple choice questions in a variety of subjects collected from sources like GRE practice tests and AP tests. It was intended to test subject matter knowledge in a wide variety of professional domains. MMLU questions are legitimately quite difficult: the average person would probably struggle to solve them.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/gYoB8vZcPGcL3cAKH/tmhq32yptb3y29utj28w\" alt=\"alt_text\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/gYoB8vZcPGcL3cAKH/zsgpux3rcxcwxoqgwysg\" alt=\"alt_text\"></p><p>At the time of its introduction in September 2020, most models only performed close to random chance on MMLU (~25%), while GPT-3 performed significantly better than chance at 44%. The benchmark was designed to be harder than any that had come before it, and the authors described their motivation as closing the gap between performance on benchmarks and \u201ctrue language understanding\u201d:</p><blockquote><p>Natural Language Processing (NLP) models have achieved superhuman performance on a number of recently proposed benchmarks. However, these models are still well below human level performance for language understanding as a whole, suggesting a disconnect between our benchmarks and the actual capabilities of these models.</p></blockquote><p>Meanwhile, the <a href=\"https://paperswithcode.com/dataset/math?ref=planned-obsolescence.org\"><u>MATH dataset</u></a> consists of free-response questions taken from math contests aimed at the best high school math students in the country. Most college-educated adults would get well under half of these problems right (the authors used computer science undergraduates as human subjects, and their performance ranged from 40% to 90%).</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/gYoB8vZcPGcL3cAKH/dxqx3imnb36plcb2rpvv\" alt=\"alt_text\"></p><p>At the time of its introduction in January 2021, the best model achieved only about ~7% accuracy on MATH. The authors say:</p><blockquote><p>We find that accuracy remains low even for the best models. Furthermore, unlike for most other text-based datasets, we find that accuracy is increasing very slowly with model size. If trends continue, then we will need algorithmic improvements, rather than just scale, to make substantial progress on MATH.</p></blockquote><p>So, these are both hard benchmarks \u2014 the problems are difficult for humans, the best models got low performance when the benchmarks were introduced, and the authors seemed to imply it would take a while for performance to get really good.</p><p>In mid-2021, ML professor <a href=\"https://jsteinhardt.stat.berkeley.edu/?ref=planned-obsolescence.org\"><u>Jacob Steinhardt</u></a> ran a contest<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffghw7t11mpk\"><sup><a href=\"#fnfghw7t11mpk\">[3]</a></sup></span>&nbsp;with superforecasters at <a href=\"https://www.hypermind.com/en/?ref=planned-obsolescence.org\"><u>Hypermind</u></a> to predict progress on MATH and MMLU.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefu528khfqlsf\"><sup><a href=\"#fnu528khfqlsf\">[4]</a></sup></span>&nbsp;Superforecasters massively undershot reality in both cases.</p><p>They predicted that performance on MMLU would improve moderately from 44% in 2021 to 57% by June 2022. The actual performance was 68%, which superforecasters had rated incredibly unlikely.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/gYoB8vZcPGcL3cAKH/xe9vp31ltwctp3urjhvt\" alt=\"alt_text\"></p><p>Shortly after that, models got even better \u2014 GPT-4 achieved 86.4% on this benchmark, close to the 89.8% that would be \u201cexpert-level\u201d within each domain, corresponding to 95th percentile among human test takers within a given subtest.</p><p>Superforecasters missed even more dramatically on MATH. They predicted the best model in June 2022 would get ~13% accuracy, and thought it was extremely unlikely that any model would achieve &gt;20% accuracy. In reality, the best model in June 2022 got 50% accuracy,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc7xi1u491mq\"><sup><a href=\"#fnc7xi1u491mq\">[5]</a></sup></span>&nbsp;performing much better than the majority of humans.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/gYoB8vZcPGcL3cAKH/trfb9gcijvk92vhnxsas\" alt=\"alt_text\"></p><p>Did ML researchers do any better? Steinhardt himself did <i>worse</i> in 2021. In his initial blog post, <a href=\"https://bounded-regret.ghost.io/ai-forecasting/?ref=planned-obsolescence.org\"><u>Steinhardt remarked</u></a> that the superforecasters\u2019 predictions on MATH were more aggressive (predicting faster progress) than his own.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefm4nsycvwcvs\"><sup><a href=\"#fnm4nsycvwcvs\">[6]</a></sup></span>&nbsp;We haven\u2019t found any similar advance predictions from other ML researchers, but Steinhardt\u2019s impression is that he himself anticipated faster progress than most of his colleagues did.</p><p>However, ML researchers do seem to be improving in their ability to anticipate progress on these benchmarks. In mid-2022, Steinhardt registered his predictions for MATH and MMLU performance in July 2023, and <a href=\"https://bounded-regret.ghost.io/scoring-ml-forecasts-for-2023/?ref=planned-obsolescence.org\"><u>performed notably better</u></a>: \u201cFor MATH, the true result was at my 41st percentile, while for MMLU it was at my 66th percentile.\u201d Steinhardt also argues that ML researchers performed reasonably well on forecasting MATH and MMLU in the late-2022 <a href=\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64abffe3f024747dd0e38d71/1688993798938/XPT.pdf?ref=bounded-regret.ghost.io\"><u>Existential Risk Persuasion Tournament (XPT)</u></a> (though superforecasters continued to underestimate benchmark progress).</p><h3>Expert surveys about qualitative milestones</h3><p>Not all forms of progress can be easily captured in quantifiable benchmarks. Often we care more about when AI systems will achieve more qualitative <i>milestones</i>: when will they translate as well as a fluent human? When will they beat the best humans at Starcraft? When will they prove novel mathematical theorems?</p><p>Katja Grace of AI Impacts asked ML experts to predict a wide variety of AI milestones, first in 2016 and then again in 2022.</p><p>In 2016, the ML experts were reasonably well-calibrated, but the predictions followed a clear pattern: progress in gameplay and robotics advanced slower than expected, but progress in language use (including programming) advanced more quickly than expected.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref88sn41hk0vu\"><sup><a href=\"#fn88sn41hk0vu\">[7]</a></sup></span></p><p>The second iteration of the survey was conducted in mid-2022, a few months before <a href=\"https://chat.openai.com/?ref=planned-obsolescence.org\"><u>ChatGPT</u></a> was released. This time accuracy was lower \u2014 experts failed to anticipate the progress that ChatGPT and GPT-4 would soon bring. These models achieved milestones like \u201cWrite an essay for a high school history class\u201d (actually <a href=\"https://www.slowboring.com/p/chatgpt-goes-to-harvard?ref=planned-obsolescence.org\"><u>GPT-4 does pretty well in college classes too</u></a>) or \u201cAnswer easily Googleable factual but open-ended questions better than an expert\u201d just a few months after the survey was conducted, whereas the experts expected them to take years.</p><p>That means that even after the big 2022 benchmark surprises, experts were still in some cases strikingly conservative about anticipated progress, and undershooting the real situation.</p><h3>Anecdata of researcher impressions</h3><p>ML researchers rarely register predictions, so AI Impacts\u2019 surveys are the best systematic evidence we have about what ML researchers expected ahead of time about qualitative milestones. Anecdotally though, a number of ML experts have expressed that they (and the ML community broadly) were surprised by ChatGPT and GPT-4.</p><p>For a long time, famous cognitive scientist Douglas Hofstadter was among those predicting slow progress. \u201cI felt it would be hundreds of years before anything even remotely like a human mind\u201d, he said <a href=\"https://www.youtube.com/watch?v=lfXxzAVtdpU&amp;t=1763s&amp;ref=planned-obsolescence.org\"><u>in a recent interview</u></a>.</p><p>Now? \u201cThis started happening at an accelerating pace, where unreachable goals and things that computers shouldn't be able to do started toppling. \u2026systems got better and better at translation between languages, and then at producing intelligible responses to difficult questions in natural language, and even writing poetry. \u2026The accelerating progress, has been so unexpected, so completely caught me off guard, not only myself but many, many people, that there is a certain kind of terror of an oncoming tsunami that is going to catch all humanity off guard.\u201d</p><p>Similarly, <a href=\"https://www.washingtonpost.com/technology/2023/07/25/ai-bengio-anthropic-senate-hearing/?ref=planned-obsolescence.org\"><u>during a Senate Judiciary Committee hearing last month</u></a>, acclaimed leading AI researcher <a href=\"https://en.wikipedia.org/wiki/Yoshua_Bengio?ref=planned-obsolescence.org\"><u>Yoshua Bengio</u></a> said \u201cI and many others have been surprised by the giant leap realized by systems like ChatGPT.\u201d</p><p>In my role as a grantmaker, I\u2019ve heard many ML academics express similar sentiments in private over the last year. In particular, I\u2019ve spoken to many researchers who were specifically surprised by the programming and reasoning abilities of GPT-4 (even after seeing the capabilities of the free version of ChatGPT).</p><h3>Another surprise ahead?</h3><p>In 2021, most people were systematically and severely underestimating progress in language models. After a big leap forward in 2022, it looks like ML experts improved in their predictions of benchmarks like MMLU and MATH \u2014 but many still failed to anticipate the qualitative milestones achieved by ChatGPT and then GPT-4, especially in reasoning and programming.</p><p>I think many experts will soon be surprised yet again. Most importantly, ML experts and superforecasters both seem to be massively underestimating future spending on training runs. In the <a href=\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64abffe3f024747dd0e38d71/1688993798938/XPT.pdf?ref=bounded-regret.ghost.io\"><u>XPT tournament</u></a> mentioned earlier, both groups predicted that the most expensive training run in 2030 would only cost around $100-180M.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref30sey85krwk\"><sup><a href=\"#fn30sey85krwk\">[8]</a></sup></span>&nbsp;Instead, I think that the largest training run will probably cross $1 billion by 2025. This rapid scaleup will probably drive another qualitative leap forward in capability like what we saw over the last 18 months.</p><p>I\u2019d be really excited for ML researchers to register their forecasts about what AI systems built on language models will be able to do in the next couple of years. I think we need to get good at predicting what language models will be able to do \u2014 in the real world, not just on benchmarks. Massively underestimating near-future progress could be very risky.</p><hr><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnb33o8yorx7f\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefb33o8yorx7f\">^</a></strong></sup></span><div class=\"footnote-content\"><p>These are people who consistently outperform experts and the general public in forecasting future events.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjwvi3c7nqsb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjwvi3c7nqsb\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://people.eecs.berkeley.edu/~hendrycks/?ref=planned-obsolescence.org\"><u>Dan Hendrycks</u></a> is the first author on both benchmarks; he did this work while he was a graduate student under Open Philanthropy grantee Jacob Steinhardt. Dan now runs the Center for AI Safety (CAIS), which Open Philanthropy has also funded.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfghw7t11mpk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffghw7t11mpk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Open Philanthropy funded this forecasting contest.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnu528khfqlsf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefu528khfqlsf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>His contest consisted of six questions, of which two were forecasting MATH and MMLU. The other questions were not about language model capabilities (two were about vision, and others were questions about inputs to AI progress).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc7xi1u491mq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc7xi1u491mq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Interestingly, the paper that achieved this milestone (<a href=\"https://arxiv.org/abs/2206.14858?ref=planned-obsolescence.org\"><u>Minerva</u></a>) was published just one day before the deadline, on June 29, 2022. According to the Minerva paper, the previous published result was the same ~7% accuracy that was reported in the original MATH paper (though the paper claims an unpublished result of ~20%). This means progress on MATH turned out to be pretty \u201clumpy,\u201d jumping a large amount with just one paper.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnm4nsycvwcvs\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefm4nsycvwcvs\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In fact, in 2021, Steinhardt was surprised that forecasters predicted that models would achieve 50% performance on MATH <i>by 2025.</i> \u201cI'm still surprised that forecasters predicted 52% on MATH [by 2025], when current accuracy is 7% (!). My estimate would have had high uncertainty, but I'm not sure the top end of my range would have included 50%.\u201d As we said above, 50% was achieved in 2022.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn88sn41hk0vu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref88sn41hk0vu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For example, one of the milestones was \u201cWrite Python code to implement algorithms like quicksort.\u201d Experts predicted that would happen around 2026, but actually it happened in 2021 \u2014 and by 2022 language models could write much more complex pieces of code than quicksort.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn30sey85krwk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref30sey85krwk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Page 57,<a href=\"https://forecastingresearch.org/news/results-from-the-2022-existential-risk-persuasion-tournament?ref=planned-obsolescence.org\"><u> \"Forecasting Existential Risks: Evidence from a Long-Run Forecasting Tournament\"</u></a></p></div></li></ol>", "user": {"username": "Ajeya"}}, {"_id": "qdKsYriypQskMQFLN", "title": "Forecast Which Psychology Studies Replicate With Metaculus for the Transparent Replications Project", "postedAt": "2023-08-29T20:24:17.389Z", "htmlBody": "<p><a href=\"https://www.clearerthinking.org/\">Clearer Thinking</a>'s &nbsp;<a href=\"https://www.metaculus.com/project/1774/\">Transparent Replications</a> project supports greater reproducibility in psychological science by performing replications of new studies from prestigious journals.</p><p>You can contribute to the cause of reliable, reproducible scientific research by making your predictions on Metaculus. We've just added Clearer Thinking's latest target study from the paper '<a href=\"https://www.researchgate.net/publication/361913875_Collective_transcendence_beliefs_shape_the_sacredness_of_objects_The_case_of_art\">Collective transcendence beliefs shape the sacredness of objects: The case of art</a>.'&nbsp;</p><figure class=\"media\"><div data-oembed-url=\"https://www.metaculus.com/questions/18323/collective-transcendence-study-replication/\">\n\t\t\t\t<div data-metaculus-id=\"18323\" class=\"metaculus-preview\">\n\t\t\t\t\t<iframe src=\"https://d3s0w6fek99l5b.cloudfront.net/s/1/questions/embed/18323/?plot=pdf\">\n\t\t\t\t</iframe></div>\n\t\t\t</div></figure><p>Clearer Thinking will attempt to replicate the following claims. How many do you predict they'll reproduce?</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qdKsYriypQskMQFLN/fmuf1pd5yxrw2opuhvti\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qdKsYriypQskMQFLN/gy9khgmbeydlygj1dkj1 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qdKsYriypQskMQFLN/yguieqquev4mbtsggp12 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qdKsYriypQskMQFLN/kyxwyqqxydnvhgwo53ce 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qdKsYriypQskMQFLN/kysmqvxaqrcumgggjozy 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qdKsYriypQskMQFLN/o3j2sxg5heu3p7nc6ke2 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qdKsYriypQskMQFLN/nvflqtz7gy94ab8hc94w 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qdKsYriypQskMQFLN/hkyrjevwovwahmygifgp 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qdKsYriypQskMQFLN/szpsiddc7a4ncpsitlnj 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qdKsYriypQskMQFLN/xojwkd9eoganbmxylob4 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qdKsYriypQskMQFLN/ucql6talf0anqzrsph3p 1368w\"></figure><p>You can learn more about the project and start predicting <a href=\"https://www.metaculus.com/project/1774/\">here</a>. Your insights will contribute to a greater understanding of forecasting's role in identifying candidate studies for replication efforts.</p>", "user": {"username": "christianM"}}, {"_id": "AhKpFhL4gKErf7bo3", "title": "Apply to a small iteration of MLAB to be run in Oxford", "postedAt": "2023-08-29T19:39:15.907Z", "htmlBody": "<p>TLDR: We\u2019re running a small iteration of MLAB (~10 participants) in Oxford towards the end of September. If you\u2019re interested in participating, apply <a href=\"https://forms.gle/QV9z6cxNGnS1UJzq6\">here</a> by 7 September. If you\u2019re interested in being a TA, please email us directly at <a href=\"mailto:oxfordmlab@gmail.com\">oxfordmlab@gmail.com</a></p><p><i>Edit: The dates are now confirmed for the 23 September - 7 October.</i></p><p><strong>Background</strong></p><p>MLAB is a <a href=\"https://www.redwoodresearch.org/mlab\">program</a>, originally designed by Redwood Research, to help people upskill for alignment work. We think it\u2019s a good use of time if you want to eventually get into technical alignment work, or if you want to work on theoretical alignment or related fields and think understanding ML would be useful. The program we\u2019re running is slightly shorter than the full MLAB\u2014two weeks instead of three. We\u2019ve condensed the curriculum similarly to how WMLB was condensed last year.&nbsp;</p><p>We plan to have just under 10 participants, and 2-3 TAs.</p><p><strong>Curriculum</strong></p><p>This curriculum might change slightly. Depending on participant interest, we might also have two optional days before the course to work through prereqs (the W0 materials) together.</p><p>W0D1 - pre-course exercises on PyTorch and einops (CPU)<br>W1D1 - practice PyTorch by building a simple raytracer (CPU)<br>W1D2 - build your own ResNet (GPU preferred)<br>W1D3 - build your own backpropagation framework (CPU)<br>W1D4 - model training Part 1: model training and optimizers (CPU) Part 2: hyperparameter search (GPU preferred)<br>W1D5 - GPT Part 1: build your own GPT (CPU) Part 2: sampling text from GPT (GPU preferred)<br>W2D1&amp;2 - transformer interpretability (CPU)<br>W2D3 - transformer interpretability on algorithmic tasks (CPU)<br>W2D4 - intro to RL Part 1: multi-armed bandit (CPU) Part 2: DQN (CPU)<br>W2D5 - policy gradients and PPO (CPU)&nbsp;</p><p>Other activities will include guest speakers and reading groups. &nbsp;</p><p><strong>Logistics</strong></p><p>Dates: 23 September- 7 October.&nbsp;</p><p>Location: Oxford</p><p>Housing will be covered for participants not already living in Oxford.</p><p>Travel from within the UK is covered. Travel from outside the UK is not covered.</p><p><strong>Questions</strong></p><p>Feel free to comment questions below or DM any of us.</p>", "user": {"username": "Complex Bubble Tea"}}, {"_id": "EB9wCLvRjJiQt7DyS", "title": "Impact Academy is hiring an AI Governance Lead - more information, upcoming Q&A and $500 bounty", "postedAt": "2023-08-29T18:42:01.823Z", "htmlBody": "<p><strong>TL;DR:</strong> You can apply <a href=\"https://airtable.com/apph1dxbqH3uqHFiW/shrVBATR3N8Zq3UqO\">here&nbsp;</a></p><h1>Q&amp;A</h1><p>On Monday 4 September we\u2019ll be holding a Q&amp;A at 15.30-16.30 CET. Please join using <a href=\"https://us06web.zoom.us/j/86802761574\">this link</a>. We\u2019ll use Zoom\u2019s webinar feature so you will remain anonymous. Please submit any questions you might have using&nbsp;<a href=\"https://forms.gle/oXz6tgKtRspuiR978\"><u>this form</u></a>. We will be recording the Q&amp;A and uploading it to our website, so please submit your questions even if you won\u2019t be able to attend.&nbsp;</p><h1>Key details:</h1><ul><li>Start date: As soon as possible. We plan on making an offer in October.</li><li>Hours: 40 hours/week</li><li>Location: Remote, with 3+ hours of overlap with UK working hours (i.e. 9am - 6pm UTC+0 to UTC+1)</li><li>Salary: Competitive, and depending on the qualifications and location of a successful applicant</li><li>You can find more information about the&nbsp;<a href=\"https://impactacademy.org/job-opening/ai-governance-lead/\"><u>role and Impact Academy here</u></a>.</li><li>How to apply: Please fill in this&nbsp;<a href=\"https://airtable.com/apph1dxbqH3uqHFiW/shrVBATR3N8Zq3UqO\"><u>application form</u></a></li><li>To recommend candidates please use&nbsp;<a href=\"https://airtable.com/apph1dxbqH3uqHFiW/shriN7s5whdzc7Nya\"><u>this form</u></a>. We also suggest that you reach out to the person you're referring to at the same time and encourage them&nbsp;<a href=\"https://airtable.com/apph1dxbqH3uqHFiW/shrVBATR3N8Zq3UqO\"><u>to apply</u></a>. There's a $500 bounty for recommending a candidate we end up hiring.</li></ul><h1>Join us!</h1><p>If you are interested in AI Governance/Policy and want to help others find careers where they do the most good they can, this might be the opportunity you have been waiting for!<br><br><a href=\"https://impactacademy.org/about-us/\"><u>Impact Academy</u></a> is a young nonprofit that is committed to building a community of people working to create a better long-term future. We are hiring a Program Director to co-create and lead our new AI Governance Fellowship. This position requires a combination of leadership, strategic thinking, entrepreneurial spirit, and an understanding of AI governance.<br><br>Please apply by completing&nbsp;<a href=\"https://airtable.com/shrVBATR3N8Zq3UqO\"><u>this form</u></a> by 10 September. We evaluate applications on a rolling basis and might fill the role before the deadline, so we encourage you to submit your application as soon as possible.</p>", "user": {"username": "Lowe Lundin"}}, {"_id": "FtLkQ77rxvEbWDtpz", "title": "\u201cLongtermist causes\u201d is a tricky classification", "postedAt": "2023-08-29T17:41:10.375Z", "htmlBody": "<p>TL;DR:&nbsp;<strong>Two different approaches for classifying projects or causes as \u201clongtermist\u201d clash significantly</strong>, which leads to oversimplifications of \u201clongtermism\u201d and causes people to pursue less impactful projects. I list some tentative suggestions at the end.</p><p><i>Notes: I discuss whether or not different projects are \"longtermist\" a lot in this post, but I don't want to imply that I think there's a very strong benefit to making sure that a project fits neatly in this group. For instance, I\u2019m not arguing that people should be (or are) more \u201chardcore\u201d with respect to longtermism, or anything like that. And if a project looks promising under multiple plausible worldviews, that should probably be considered a benefit (</i><a href=\"https://forum.effectivealtruism.org/posts/Mig4y9Duu6pzuw3H4/hedging-against-deep-and-moral-uncertainty\"><i>related</i></a><i>, and </i><a href=\"https://forum.effectivealtruism.org/posts/cP7gkDFxgJqHDGdfJ/ea-and-longtermism-not-a-crux-for-saving-the-world\"><i>also related</i></a><i>).</i></p><p><i>Finally, arguments in&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/FkFTXKeFxwcGiBTwk/against-longtermist-as-an-identity\"><i><u>this earlier post</u></i></a><i> probably still inform my thinking on all of this.</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqj5k59kxlt\"><sup><a href=\"#fnqj5k59kxlt\">[1]</a></sup></span></p><h1>What is a \u201clongtermist project\u201d?</h1><ol><li>One way of classifying something as \u201clongtermist\u201d is based on the<strong> definition of the philosophy/belief set</strong>. Under this approach:&nbsp;<ol><li>A given project is \u201clongtermist\u201d (or not) depending on whether its primary purpose is to have an impact via influencing the long-term future \u2014 or at least if it looks good from a longtermist worldview.&nbsp;<i>Not</i> necessarily depending on whether it\u2019s in biosecurity, animal welfare, AI, or whatever else.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefg0xp0cjz9z\"><sup><a href=\"#fng0xp0cjz9z\">[3]</a></sup></span>&nbsp;</li></ol></li><li>Alternatively, we could say that there are&nbsp;<strong>clusters of work</strong> (or causes) that are called longtermist \u2014 for historical reasons and because people who take a \u201clongtermist\u201d perspective often endorse working in these areas.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpen5qtj9byl\"><sup><a href=\"#fnpen5qtj9byl\">[4]</a></sup></span>&nbsp;In this view:<ol><li>Biosecurity, AI safety, reducing the risk of global conflict, etc. are \u201clongtermist causes.\u201d&nbsp; Extinction risk reduction generally falls under this, but also anything that looks like \u201creduce the risks of a catastrophic event,\u201d even if the people pursuing the project are just hoping to prevent catastrophes from harming beings who are alive today.&nbsp;</li></ol></li></ol><p>I\u2019m sure there are other approaches to this classification, but I\u2019ll focus on these two.&nbsp;</p><h2>These two approaches don\u2019t always agree on what is \u201clongtermist\u201d</h2><p>Below is a rough Venn diagram of some types of work that are called \u201clongtermist\u201d in one or both of the approaches (note that I didn\u2019t try to classify things very carefully, or try to be exhaustive).&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/FtLkQ77rxvEbWDtpz/fi87whcx5ntqgh3zmpv8\"><figcaption>Venn diagram: Motivated by longtermism vs. \"longtermist\" cluster. Only the former includes worrying about the stagnation of future economic progress. Only the latter includes developing better vaccines faster. And \"both\" includes AI alignment to prevent existential risks from AI.</figcaption></figure><p>Many of the relevant distinctions aren\u2019t binaries, but rather spectrums&nbsp; \u2014 some things look&nbsp;<i>more</i> or less \u201clongtermist\u201d under one of the two approaches, as opposed to simply being clearly \u201clongtermist\u201d or not \u2014 so a better picture might look more like a 2-dimensional visualization:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/FtLkQ77rxvEbWDtpz/nyjejym8wxdjmgq7owyf\"><figcaption>The X axis is \"more motivated by longtermism\" and the Y axis is \"deeper in the longtermist cluster.\"&nbsp;</figcaption></figure><p>In this diagram, the most canonical \u201clongtermist\u201d projects are in the top-right corner; the examples I chose are in the \u201clongtermist cluster\u201d and are often directly motivated by longtermism. This includes AI alignment, prevention of bioengineered pandemics, etc. Developing better flu vaccines is often associated with longtermism through biosecurity \u2014 an ostensibly \u201clongtermist\u201d cause \u2014 while advocacy or research about the moral relevance of non-human beings can be deeply motivated by longtermism but often isn\u2019t thought of as \u201clongtermist\u201d because it\u2019s not clearly in the cluster.&nbsp;</p><p>Note that <strong>some people believe that basically no projects outside the canonical longtermist ones actually look promising under a longtermist worldview</strong> (although they might agree that the \"longtermist cluster\" contains things that also aren't \"longtermist\" under this classification). I currently disagree, but can see this point of view.</p><h2>The difference between the two approaches is growing, and people who primarily think about the \u201clongtermist cluster\u201d now identify \u201clongtermism\u201d more closely with the causes it is currently associated with (AI safety, biosecurity, general risk reduction)</h2><p>I think the two approaches have been different for a while (e.g. a lot of work on biosecurity is not motivated by longtermist thinking).&nbsp;</p><p>And the approaches have diverged more recently. Some of the causes previously dominated by \u201c<a href=\"https://forum.effectivealtruism.org/posts/FkFTXKeFxwcGiBTwk/against-longtermist-as-an-identity\">longtermists</a>\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref967f135evpr\"><sup><a href=\"#fn967f135evpr\">[5]</a></sup></span>&nbsp;have grown a lot \u2014 more people have started to believe these causes are pressing problems and have started working on them (particularly AI risk and pandemic preparedness). These areas remain coded as \u201clongtermist,\u201d but many of the people working in them today are&nbsp;<i>not</i> significantly motivated by the desire to improve the long-term future. Relatedly, some risks that many believed were \u201clong-term future risks\u201d are now more widely viewed as urgent or near-term, so more people have started working on them for different reasons (there\u2019s a convergence going on, to some extent; people with different values now share more priorities in practice). (See for instance&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/KDjEogAqWNTdddF9g/long-termism-vs-existential-risk\"><u>this piece</u></a> by Scott Alexander.)</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/FtLkQ77rxvEbWDtpz/djnogjtepf7wp3ui4h3a\"><figcaption>Chart of Venn diagrams (motivated by longtermism vs. longtermist cluster) before and after the cluster grew.&nbsp;</figcaption></figure><h1>Some resulting problems</h1><h2>We assume someone\u2019s philosophy based on their \u201crevealed cause prioritization\u201d and vice versa \u2014 which causes further difficulties</h2><p>Examples of this kind of thing going wrong (some of these are very similar to things I've seen or heard about):</p><ol><li>Pluto says that they agree with the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/FkFTXKeFxwcGiBTwk/against-longtermist-as-an-identity#_0__What_is__longtermism___\"><u>classic longtermist argument</u></a>, so the people talking to Pluto (and, say, recommending opportunities to Pluto) assume that Pluto wouldn\u2019t work on non-human welfare and don\u2019t recommend those opportunities to them<ol><li>Or even just assume that Pluto wants to work on AI safety</li></ol></li><li>Ariel is working on AI safety, so people assume that she fully endorses longtermism or makes all cause-prioritization decisions based on longtermism (<a href=\"https://80000hours.org/podcast/episodes/holden-karnofsky-how-ai-could-take-over-the-world/#what-holden-really-believes-004540\"><u>slightly related</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbvfrtwotqbg\"><sup><a href=\"#fnbvfrtwotqbg\">[6]</a></sup></span>)<ol><li>...which could additionally cause&nbsp;<a href=\"https://www.lesswrong.com/tag/information-cascades\"><u>information cascades</u></a> or incorrect deferral to Ariel in a way that causes more people to take on a kind of naive and more \u201chardcore\u201d longtermist perspective on the world</li></ol></li><li>Eve doesn't buy the longtermist argument and gets frustrated because she sees prominent members of the community endorsing projects that are in the longtermist cluster a lot, which leads her to conclude that these people believe that longtermism is the <i>only</i> viable approach</li><li>You see that Gascon is working on AI safety, so you assume that he cares about the long-term future and entrust him with something like a grantmaking project aims to improve the long-term future \u2014 but actually Gascon just finds his current AI safety project interesting, or thinks that AI is a near-term harm</li><li>You meet Baymax, who works on biosecurity (say, far-UVC light), and you disagree with the longtermist argument. You start talking about cause prioritization and you feel like you can try to convince Baymax of all your reasons for not believing the longtermist case. This is not useful; Baymax's main&nbsp;<a href=\"https://www.lesswrong.com/tag/crux\"><u>crux</u></a> for prioritizing biosecurity is the potential for near-term pandemic harm.&nbsp;</li></ol><h2>People motivated by longtermism might overlook unusual areas of work or go for something that\u2019s in the \u201clongtermist cluster\u201d but isn\u2019t actually that promising when considered from a philosophically longtermist perspective</h2><ol><li>When someone is seriously compelled by the longtermist argument, they might simply not consider areas like animal welfare, because they don\u2019t see many discussions of <a href=\"https://forum.effectivealtruism.org/posts/XrRGKSvntGCZAajGk/longtermism-and-animal-advocacy\">animal welfare and longtermism</a>; most \u201clongtermist\u201d spaces are full of discussions from the \u201clongtermist cluster.\u201d&nbsp;</li><li>If someone gets classified as \u201clongtermist\u201d or agrees with the classic argument and wants to do \u201clongtermist work,\u201d they might default to things in the \u201clongtermist cluster\u201d without prioritizing within that cluster enough. So they might e.g. do work that\u2019s much more relevant for near-term epidemic preparedness than work that really helps in cases like global pandemics or non-bio work.&nbsp;<ol><li>I have similar concerns about \u201c<a href=\"https://forum.effectivealtruism.org/posts/CmmtKKD84BYhmmxrD/chanamessinger-s-shortform#6kybE8FqM2qSgkt9p\"><u>EA projects</u></a>\u201d sometimes.&nbsp;</li></ol></li></ol><h2>Semi-related: people who disagree with the classic longtermist argument might steer away from \u201clongtermist\u201d-coded projects despite them actually looking good under other philosophical and empirical worldviews</h2><p>I think some projects are promising under non-\u201dlongtermist\u201d worldviews, but get somewhat overlooked by people who don\u2019t think longtermism is sound as a philosophy. For example:&nbsp;</p><ul><li>Near-term AI safety work</li><li>Studying ways to improve&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/international-relations?sortedBy=top\"><u>international coordination</u></a> to reduce the chances of war or&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ahranYvH9uum5dhtH/open-philanthropy-shallow-investigation-civil-conflict\"><u>conflict</u></a> (because that would be bad for people alive today)</li><li>Developing&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/vaccines\"><u>vaccines</u></a> for potential pandemics faster, developing cheaper and more accessible PPE, etc.</li><li>Reducing extinction risk because extinction is bad under completely different worldviews</li></ul><p>I\u2019m not sure how much this happens, in practice.</p><h1>A few (tentative) suggestions/takeaways</h1><p><i>I'm not sure about these!&nbsp;</i></p><ol><li>Avoid implying that some project is&nbsp;<strong>only</strong> justified based on a pretty narrow philosophy/worldview (e.g. longtermism) unless you\u2019re pretty sure that\u2019s the case.</li><li>I think generally \u201clongtermist or not\u201d is a worldview distinction that\u2019s over-used in EA; e.g. human-focused vs. not is probably at least as important, as are more empirical questions like \u201cdo you believe that very powerful AI systems are coming soon?\u201d I expect that it\u2019s often more useful to group things based on other questions/classifications (instead of whether or not they\u2019re \u201clongtermist\u201d).&nbsp;</li><li>When you do talk about whether something is \u201clongtermist\u201d or not, specify what your approach is and make that clear. (E.g.&nbsp;<a href=\"https://forum.effectivealtruism.org/s/FxFwhFG227F6FgnKk/p/sAgExeaxCNmFSmrJT#Cause_Prioritization\"><u>this was a useful conversation</u></a>.)&nbsp;<ol><li>I generally try to reserve the label \u201clongtermist\u201d for projects that are in fact motivated by longtermism (although at this point I also think it makes sense to use it to identify a cluster of work in our broader network).&nbsp;</li></ol></li><li>Notice if you\u2019re making assumptions about someone\u2019s worldview based on something like the cluster they seem to be in (and vice versa), and check whether you endorse having this as a very strong prior.&nbsp;</li><li>Consider more often avoiding the term \u201clongtermism\u201d if you\u2019re not really sure what you mean by it and how others will understand it.&nbsp;</li></ol><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqj5k59kxlt\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqj5k59kxlt\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I initially wrote a version of the current post over a year ago. In it, I mostly argued for a specific classification system (the motivation-based system below). I never ended up posting it because I wasn't convinced of my arguments, but related topics have come up many times since then and I've decided to try sharing something. I'm still not totally convinced of the details.&nbsp;</p><p>Relatedly, I think the arguments here are not particularly new, but I don't remember seeing someone lay them out in one place.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnaav632mhq5i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefaav632mhq5i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>You can also try to define projects as \u201clongtermist\u201d depending on where their actual impact lies, as opposed to the motivations of the people pursuing the project. I think this is more difficult but will result in a similar approach (although different conclusions). (I haven't checked carefully.)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fng0xp0cjz9z\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefg0xp0cjz9z\">^</a></strong></sup></span><div class=\"footnote-content\"><p>These projects can also look good under other philosophical/moral approaches \u2014 I\u2019m not claiming that longtermism has to be the only reason for pursuing some kind of work.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpen5qtj9byl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpen5qtj9byl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The cluster is also probably associated with requiring a higher willingness to work on low-probability events, longer feedback loops, etc. \u2014 things not necessarily relevant to the philosophy directly.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn967f135evpr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref967f135evpr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>which are therefore in the \u201clongtermist cluster\u201d for historical reasons</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbvfrtwotqbg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbvfrtwotqbg\">^</a></strong></sup></span><div class=\"footnote-content\"><p><strong>Holden Karnofsky:&nbsp;</strong>There\u2019s a line that you\u2019ve probably heard before that is something like, \u201cMost of the people we can help are in future generations, and there are so many people in future generations that that kind of just ends the conversation about how to do the most good \u2014 that it\u2019s clearly and astronomically the case that focusing on future generations dominates all ethical considerations, or at least dominates all considerations of how to do the most good with your philanthropy or your career.\u201d I kind of think of this as philosophical longtermism or philosophy-first longtermism. It kind of feels like you\u2019ve ended the argument after you\u2019ve pointed to the number of people in future generations.</p><p>And we can get to this a bit later in the interview \u2014 I don\u2019t think it\u2019s a garbage view; I give some credence to it, I take it somewhat seriously, and I think it\u2019s underrated by the world as a whole. But I would say that I give a minority of my moral parliament thinking this way. I would say that more of me than not thinks that\u2019s not really the right way to think about doing good; that\u2019s not really the right way to think about ethics \u2014 and I don\u2019t think we can trust these numbers enough to feel that it\u2019s such a blowout.</p><p>And the reason that I\u2019m currently focused on what\u2019s classically considered \u201clongtermist causes,\u201d especially AI, is that I believe the risks are imminent and real enough that, even with much less aggressive valuations of the future, they are competitive, or perhaps the best thing to work on.</p></div></li></ol>", "user": {"username": "Lizka"}}, {"_id": "c2omdmRxWJyGdLDBb", "title": "High Impact Leadership Coaching Program \u2013 Applications Open", "postedAt": "2023-09-05T21:34:20.070Z", "htmlBody": "<p><i>TLDR: We\u2019re trialling an online group coaching program for leaders at EA organisations where you can get coaching from an experienced executive coach on your leadership and management skills for low or no cost.&nbsp;</i><a href=\"https://forms.gle/uE59eWLKMpGsxDv68\"><i><u>Apply here</u></i></a><i>. Application Deadline: 23:59 UTC October 1st&nbsp;</i></p><p>&nbsp;</p><p>I contend that there are a set of skills and behaviours that matter if you want to be an effective leader of an organisation \u2013 broadly speaking, you could call these 'executive' or 'leadership' skills</p><p>What kind of things do I mean?</p><ul><li>Technician vs Entrepreneur mindset. The tendency to do everything yourself vs focusing mostly on decision making; hiring; delegation; and using systems &amp; processes to get things done.</li><li>Consistently and consciously spending your time on your highest value tasks, meetings and deep work sessions each week.</li><li>Being able to give feedback on employees\u2019 performance that comes from a place of kindness and respect whilst also being timely, direct and actionable.</li><li>Effectively managing psychological bugs that are particularly common and damaging in leadership roles like imposter syndrome, perfectionism and chronic stress.</li></ul><p>The High Impact Leadership Coaching Trial will put a group of EA organisation founders and Senior Directors through a 6-month leadership coaching program designed to rapidly level up these types of skills. The program will be led by Parag Prasad, an experienced leadership/executive coach with 16 years\u2019 experience coaching more than 70 CEOs and Senior Directors (including a number of EA clients at places like the Good Food Institute and Convergence Analysis). There will also be other accomplished coaches from the EA community including Tee Barnett, Sebastian Schmidt and Adam Tury lending a hand and running sessions too.</p><p>I expect a credible leadership coach to be valuable in 2 ways. Firstly by sharing and facilitating discussions around templates and best practices from the business world (e.g.&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1Zi2rmuOH3_WqE-4Qir35XpKZMxcghz_OVowNySBpqbE/edit#gid=0\"><u>Areas of Responsibility Documents</u></a>,&nbsp;<a href=\"https://www.amazon.co.uk/Measure-What-Matters-Simple-Drives/dp/024134848X\"><u>OKR systems</u></a>) and secondly by coaching leaders\u2019 on their individual mindset and behaviour change challenges.&nbsp;</p><p>Here are some examples to illustrate the second category</p><ul><li>One of Parag\u2019s coaching clients described the realisation that he had not been enforcing minimum performance targets in new hires\u2019 probation periods because it felt unkind to judge people so quickly. Investigation revealed that every such hire was let go months down the line regardless, which they agreed was costly for the organisation and ultimately more unkind for the new hires.</li><li>Another coaching client carried out a time audit on her coach\u2019s recommendation and discovered that she had been spending a full 75% of her hours on low value tasks that she agreed should be delegated, automated or ignored. After breaking down and working through the underlying issues they were eventually able to carve out an uninterrupted 2 hour \u2018most important work\u2019 block every day to focus on a predefined list of top priorities.</li><li>And my favourite example, Jessica Almy, Senior Vice President of Policy at The Good Food Institute,&nbsp;<a href=\"https://youtu.be/Q9rKlnfVdvk\"><u>who explains here</u></a> how she was able to work with her executive coach towards taking a 2 week holiday during which she was able to completely unplug from work trusting her team to handle things whilst she was away.</li></ul><p>A common story I heard when conducting research for this program was the idea that EA overvalues intellectual high achievers straight out of college and undervalues management experience. Personally I have a lot of time for small teams of audacious nerds who want to change the world unhampered by pointy-haired bosses, and I do sometimes worry about a failure mode where EA gets bogged down in management and bureaucracy. But I\u2019m hoping there are ways to get the best of both worlds, and good executive coaches could be part of that solution.<br>&nbsp;</p><h2>About The Program</h2><p><strong>Where:&nbsp;</strong>Zoom + Chat/Email support</p><p><strong>When:</strong> Group sessions Tues 10:00-12:00 UTC every other week starting October 31 (plus an optional 30mins progress check-in on alternate weeks, timing TBC)</p><p><strong>What: </strong>Boardroom style group coaching sessions with 5-10 other EA Directors. Time will be spent in each session discussing some suggested topics, frameworks and examples from other top-performing EA orgs. Then participants go away and implement their version, come back, discuss, debug, share experiences. Participants are expected to be implementing and testing throughout the program rather than passively learning theory.</p><p><strong>The Coaches&nbsp;</strong></p><p><a href=\"https://www.linkedin.com/in/businesscoachinglondon1/\"><u>Parag Prasad</u></a>:</p><p>Parag founded The Business Growth Agency in 2007 one of Europe\u2019s most successful business coaching practices. He has delivered &gt;11,000 coaching hours, coached &gt;70 CEOs and leaders 1 on 1 and delivered group based coaching to &gt;400 others. He held senior roles at PricewaterhouseCoopers and British Telecom, is a qualified chartered accountant and holds a degree from Oxford University.</p><p>The public Google Reviews page for the Business Growth Agency is&nbsp;<a href=\"https://www.google.com/maps/search/?api=1&amp;query=Google&amp;query_place_id=ChIJdw09klUDdkgROe1ix1HsQU0\"><u>here</u></a>.&nbsp;</p><p>In the EA community he\u2019s recently worked with Directors at The Good Food Institute, Convergence Analysis and will be a coach for the upcoming Charity Entrepreneurship cohort.</p><p>Guest coaches&nbsp;<a href=\"https://www.linkedin.com/in/teebarnett/\"><u>Tee Barnett</u></a>,&nbsp;<a href=\"https://www.linkedin.com/in/sebastians-schmidt/\"><u>Sebastian Schmidt</u></a> and&nbsp;<a href=\"https://www.linkedin.com/in/adam-tury/\"><u>Adam Tury</u></a> will also deliver sessions to add different perspectives and topic expertise throughout.</p><p>&nbsp;</p><p><strong>The Content</strong></p><p>The program has been designed with the goal of taking best practice on management and leadership from the business world and adapting it for an EA context, adding more epistemic rigour and focusing on topics that our research suggests EAs particularly want help with.&nbsp;</p><p>Over the 12 sessions we will cover topics including</p><ul><li>How an EA org leader could optimally use their time, including how they could think about about prioritisation, delegation and using systems and processes</li><li>The psychology of ownership, blame and imposter syndrome</li><li>Setting and tracking organisational, departmental and project level goals</li><li>Team coordination and communication systems including how to run various types of meetings effectively</li><li>Recruitment systems including: job adverts, interviewing, assessment tasks, referencing and onboarding</li><li>How EA culture causes unique leadership, management and recruitment challenges and how to overcome them</li><li>Team performance, Key Performance Indicators, career ladders, development plans, giving and soliciting honest feedback</li><li>Several open sessions reserved for free form discussion, progress review and troubleshooting implementation challenges</li></ul><p>&nbsp;</p><p><strong>What do we mean by \u201ca trial\u201d?</strong></p><p>This is a beta program which we are running for the first time. I expect some sessions to work better than others and I expect the coach and participants to be working things out collaboratively as they go.</p><p>This is a trial program which is offered on a pay what you want basis with the agreement that participants provide feedback. We will try to make this as quick and easy as possible but you will be expected to fill in a pre and post program assessment questionnaire, spend a minute or two scoring each session and to attend 1 feedback interview on Zoom at the end of the program lasting up to 1 hour.</p><p>&nbsp;</p><p><strong>Cost</strong></p><p>You are invited to pay what you want. This can be zero pounds.</p><p>I believe offering coaching free of charge tends to result in noticeably less committed coaching clients, so I\u2019d prefer participants contribute a nominal payment each month as a commitment device.</p><p>If you\u2019re able to contribute \u00a350 or so per session please do. But if, for whatever reason, cost is a barrier I\u2019d much prefer committed and serious applicants still apply, as we do have funding available.</p><p><br>&nbsp;</p><h2>Application Process</h2><p>Fill out&nbsp;<a href=\"https://forms.gle/uE59eWLKMpGsxDv68\"><u>this short application form</u></a> (10-20mins) to apply. The deadline is 23:59 UTC October 1st. Be aware: applying earlier may give you an advantage vs applying very near the deadline.</p><p>The best-suited applicants will be invited to book a 45mins trial coaching call with Parag. This will give you a chance to experience the style of coaching, test client-coach fit, tell Parag a bit more about your goals and ask any questions you have about the program. We will then ask you to confirm whether you want to take part. We aim to make these calls valuable irrespective of whether you end up joining the group or not.</p><p><strong>Eligibility Criteria</strong></p><ul><li>You need to be a Founder/Senior Director at an EA-aligned organisation with at least 3 team members.</li><li>You must be available to participate on alternate Tuesdays 10:00-12:00 UTC Oct 31st onwards for approximately 6 months</li><li>We\u2019re especially looking for participants who are motivated to work on themselves, willing to question their normal ways of doing things, willing to be honest and vulnerable with the group, interested in supporting and building friendships with other EA Directors in the group and who can offer us honest feedback as they go through the process.<br>&nbsp;</li></ul><h2>FAQ</h2><p><strong>Will a group coaching program be customised and bespoke enough for me? I have specific problems which I want to be coached on that I\u2019ll select myself.</strong></p><p>This is a common concern. There\u2019s a certain amount of trust and a leap of faith required here. Just know that a lot of thought has gone into the design of this program and I\u2019ve often received feedback that people prefer the group coaching format to 1 to 1 coaching despite expecting the opposite to be true. I think tight-knit peer groups, \u2018friendly competition\u2019 effects, and the impact of seeing how other smart people approach the same problems you\u2019re working on might explain this.</p><p>Having said that, if you\u2019re dead set on 1 to 1 coaching all of the coaches involved do offer 1 to 1 coaching which you could certainly do alongside or instead of this program.</p><p>&nbsp;</p><p><strong>What do you mean by \u201cFounder/Senior Director\u201d in the Eligibility Criteria?</strong></p><p>This program isn\u2019t designed for mid-level managers or people working towards Senior Director roles. There\u2019s no value judgement implied here \u2014 we need great leaders at all levels of EA organisations, but this program assumes participants have the authority to make organisation-wide changes themselves right now. Exactly what constitutes this level of decision-making authority might be a little blurry and can certainly be discussed during the application process.</p><p>&nbsp;</p><p><strong>Have you considered ways this project could cause harm?</strong></p><p>Yes. The main potential for harm in this project is where the benefit provided is less than the cost of the time the coaching and homework assignments consume \u2013 and I think the time and attention of EA org founders is extremely valuable.&nbsp;</p><p>I would hope participants would recognise this and leave if they\u2019re not seeing any value. Another possibility is they think it\u2019s helping but it\u2019s not (or not versus other ways they could be using their time and energy). There\u2019s a weird dynamic I sometimes see where a coach, trainer or educator ends up optimising for generating shallow moments of epiphany or the feeling of progress rather than real improvements in performance. As much as possible we will try to tie the program to real world outcomes e.g. whether participants end up using their time differently and how their employees rate their management and leadership skills before and after the program.<br>&nbsp;</p><p><strong>I have another question not listed here</strong></p><p>Great! Put it in the comments below or send me a direct message. I\u2019ll try and check this every day or two and would love to hear any questions, feedback or ideas you have.<br>&nbsp;</p><h2>Please Spread The Message</h2><p>If you know somebody who you think would be interested in participating please consider sharing this page with them!</p>", "user": {"username": "High Impact Leadership Coaching"}}, {"_id": "svaHSiPFykYs9tYet", "title": "Should some people start working to influence the people who are most likely to shape the values of the first AGIs, so that they take into account the interests of wild and farmed animals and sentient digital minds?\u00a0", "postedAt": "2023-08-31T12:08:45.715Z", "htmlBody": "<p>This would especially include outreach towards executives and employees in top AGI labs (e.g. OpenAI, DeepMind, Anthropic), the broader US&nbsp;tech community, as well as policymakers from major countries.<br><a href=\"https://forum.effectivealtruism.org/posts/ZnPLPFC49nJym7y8g/agi-x-animal-welfare-a-high-ev-outreach-opportunity\">The arguments in favor are presented in this post&nbsp;</a></p>", "user": {"username": "Keyvan Mostafavi"}}, {"_id": "Hg4dQqxyFpmkoYKeg", "title": "AISN #20: LLM Proliferation, AI Deception, and Continuing Drivers of AI Capabilities", "postedAt": "2023-08-29T15:03:41.071Z", "htmlBody": "<p>Welcome to the AI Safety Newsletter by the <a href=\"https://www.safe.ai/\"><u>Center for AI Safety</u></a>. We discuss developments in AI and AI safety. No technical background required</p><p>Subscribe <a href=\"https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916\">here</a> to receive future versions.</p><hr><h2>AI Deception: Examples, Risks, Solutions</h2><p>AI deception is the topic of a new <a href=\"https://arxiv.org/abs/2308.14752\">paper</a> from researchers at and affiliated with the Center for AI Safety. It surveys empirical examples of AI deception, then explores societal risks and potential solutions.</p><p>The paper defines deception as \u201cthe systematic production of false beliefs in others as a means to accomplish some outcome other than the truth.\u201d Importantly, this definition doesn't necessarily imply that AIs have beliefs or intentions. Instead, it focuses on patterns of behavior that regularly cause false beliefs and would be considered deceptive if exhibited by humans.</p><p><strong>Deception by Meta\u2019s CICERO AI. </strong>Meta developed the AI system CICERO to play Diplomacy, a game where players build and betray alliances in pursuit of global domination. The paper\u2019s authors <a href=\"https://www.vice.com/en/article/bvm4bq/metas-board-gaming-ai-learned-not-to-lie\">celebrated</a> their efforts to train CICERO to be \u201clargely honest and helpful to its speaking partners.'' Despite these efforts, our paper shows that CICERO learned strong deception skills.&nbsp;</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48900db8-d704-4772-871e-1645ee3933cf_1326x1636.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48900db8-d704-4772-871e-1645ee3933cf_1326x1636.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48900db8-d704-4772-871e-1645ee3933cf_1326x1636.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48900db8-d704-4772-871e-1645ee3933cf_1326x1636.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48900db8-d704-4772-871e-1645ee3933cf_1326x1636.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48900db8-d704-4772-871e-1645ee3933cf_1326x1636.png 1456w\"></a></p><p>The dialogue above shows CICERO making a commitment that it never intended to keep. Playing as France, CICERO conspired with Germany to trick England. After deciding with Germany to invade the North Sea, CICERO told England that it would defend England if anyone invaded the North Sea. Once England was convinced that France was protecting the North Sea, CICERO reported back to Germany that they were ready to attack.&nbsp;</p><p>Despite the authors\u2019 efforts to make CICERO honest, we show several examples of CICERO clearly deceiving its opponents. This highlights the difficulty of building honest AI systems. Even if developers try to make an AI honest, the AI might discover that deception is useful for achieving its objective.</p><p><strong>Deception in specific-use and general-purpose AI systems.</strong> The paper collects many examples of AI deception. Sometimes, AI systems trained for a specific purpose such as winning a game end up learning to deceive. For example, <a href=\"https://www.cmu.edu/news/stories/archives/2019/july/cmu-facebook-ai-beats-poker-pros.html\">Pluribus</a>, an AI trained to play poker, learned to bluff when it didn\u2019t have good cards in order to make its opponents fold and win the hand.&nbsp;</p><p>General AI systems like language models often use deception spontaneously to achieve their goals. <a href=\"https://arxiv.org/abs/2308.01404\">Hoodwinked</a> is a text-based game similar to Mafia and Among Us. When language models play it, they often kill their opponents, then provide elaborate alibis when speaking with other players in order to hide their identities. The <a href=\"https://arxiv.org/abs/2304.03279\">MACHIAVELLI</a> benchmark demonstrates a general tradeoff between following ethical rules and maximizing rewards.&nbsp;</p><p><strong>Risks of AI deception.</strong> Malicious individuals can use AI systems with deception skills to commit fraud, tamper with elections, or generate propaganda.&nbsp; Deceptive AI systems might spread false beliefs throughout society, or an incorrect perception that AI systems are performing as intended.&nbsp;</p><p>More advanced AI systems might use deception to escape human control, such as by deceiving AI developers. When a company or government regulator evaluates an AI\u2019s behavior, the system might deliberately behave well in order to pass the test. But once the system is approved and deployed in the real world, it might no longer behave as intended. The <a href=\"https://en.wikipedia.org/wiki/Volkswagen_emissions_scandal\">Volkswagen emissions scandal</a> is an example of this type of behavior. The car manufacturer programmed their vehicles to limit emissions during tests by government regulators, but when the vehicles went back on the road, they immediately resumed spewing toxic emissions.&nbsp;</p><p><strong>Policy and technical solutions.</strong> To address the threat of AI deception, policymakers could require that AI outputs are labeled as such. People might try to remove these markers, but invisible \u201c<a href=\"https://arxiv.org/abs/2301.10226\">watermarking</a>\u201d techniques that are difficult to remove might allow us to reliably identify AI outputs in the real world.</p><p>More broadly, as governments consider risk-based frameworks for AI governance, any systems capable of deception should be regarded as high risk. They should be properly evaluated and monitored during both training and deployment, and any possible steps to limit deception should be taken.</p><p>Technical researchers should focus on identifying and preventing AI deception. Despite the many clear examples of AIs causing false beliefs in humans, it would still be valuable to have clearer ways to define and detect deception in specific environments. Lie detector tests have been <a href=\"https://arxiv.org/abs/2212.03827\">explored</a> in previous <a href=\"https://arxiv.org/abs/2304.13734\">work</a> and could be built upon in future work.</p><h2>Proliferation of Large Language Models</h2><p>Slowing the deployment of dangerous technologies can be difficult. Businesses can profit by selling them despite negative externalities on society. Even if the first actors to develop a technology are cautious, the price of building the technology typically falls over time, putting it within the reach of more groups. It might only take one company recklessly deploying a technology to undermine the cautious approach of all others.&nbsp;</p><p>Several recent developments demonstrate this dynamic. A few weeks ago, Meta released Llama 2, an open source model with similar performance to OpenAI\u2019s GPT-3.5. Perhaps in response to the fact that anyone can now fine-tune Llama 2, OpenAI has decided to open up fine-tuning access to GPT-3.5. Meta has charged ahead with another open source release of a model specialized in programming.&nbsp;</p><p><strong>GPT-3.5 can now be fine-tuned by users. </strong>Users can now upload data to OpenAI\u2019s API, and OpenAI will create a version of GPT-3.5 fine-tuned on that data. The customer owns the data exchanged via the fine-tuning API, and neither OpenAI nor any other organization uses it to train other models. For example, if a business wants to automate customer support, they can fine-tune GPT-3.5 with answers to frequently asked questions about their business.&nbsp;</p><p>Malicious individuals might attempt to fine-tune GPT-3.5 for harmful purposes, but OpenAI will use GPT-4 in an attempt to screen out fine-tuning datasets which violate OpenAI\u2019s safety policies. Yet as <a href=\"https://llm-attacks.org/\">research on adversarial attacks</a> has shown, language models are not always effective in identifying harmful inputs from malicious actors.</p><p>This decision comes only a few weeks after the open source release of Llama 2, which is roughly on par with GPT-3.5. If OpenAI had been concerned that malicious users might fine-tune GPT-3.5, those users can now simply fine-tune Llama 2. Meta\u2019s bold plan of open sourcing has eliminated any potential safety benefits of OpenAI\u2019s caution, perhaps spurring OpenAI to open up GPT-3.5 for fine-tuning.&nbsp;</p><p><strong>Meta open sources a state of the art code generation model. </strong>After releasing Llama 2 a few weeks ago, Meta has fine-tuned that model on a large dataset of code and released it as <a href=\"https://ai.meta.com/blog/code-llama-large-language-model-coding/\">Code Llama</a>, the world\u2019s most advanced open source language model for programming.&nbsp;</p><p>Before release, Code Llama was red teamed by cybersecurity experts to evaluate its ability to author cyberattacks. They found that the model generally refuses to help with explicit requests for writing malware. But if the request is disguised as benign, the model will usually assist. Given the limited capabilities of today\u2019s language models, one red teamer suggested that Llama Code would only be useful for low-skill programmers hoping to conduct cyberattacks.</p><p>Yet the capabilities of open source models are rapidly growing. Meta is <a href=\"https://twitter.com/agikoala/status/1695125016764157988\">rumored</a> to be building an open source model on par with GPT-4, though this claim is unconfirmed.</p><figure class=\"image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb230f6-df6f-4a36-a91c-fe1c82bc4b05_1690x956.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb230f6-df6f-4a36-a91c-fe1c82bc4b05_1690x956.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb230f6-df6f-4a36-a91c-fe1c82bc4b05_1690x956.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb230f6-df6f-4a36-a91c-fe1c82bc4b05_1690x956.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb230f6-df6f-4a36-a91c-fe1c82bc4b05_1690x956.png 1456w\"><figcaption><i>Code Llama, as illustrated by Midjourney\u2019s AI. (</i><a href=\"https://the-decoder.com/fine-tuned-meta-code-llama-outperforms-gpt-4-in-key-benchmark/\"><i>Source</i></a><i>)</i></figcaption></figure><p><strong>An economic case for slowing down deployment. </strong>Economists are often optimistic about new technologies and welcome the creative destruction that they bring. But a new <a href=\"https://www.nber.org/papers/w31461\">paper</a> from economists Daron Acemoglu and Todd Lensman at MIT makes the case for slowing down AI deployment.&nbsp;</p><p>They start with the basic economic concept of negative externalities. The businesses that build and deploy AI might profit greatly, even if it has negative effects for the rest of society. Therefore, they will naturally rush to build and deploy AI faster than what would be best for everyone.&nbsp;</p><p>The paper then supposes that some AI harms might be irreversible, meaning we must act to prevent the harm before we can clearly observe it. For example, if AI development leads to a global pandemic, it will be cold comfort to know that we can regulate AI after such a global catastrophe. Further, as AI grows more profitable, the businesses building it <a href=\"https://en.wikipedia.org/wiki/Collingridge_dilemma\">might gain political power</a>, making them more difficult to regulate.&nbsp;</p><p>In this situation, there would be a strong case for government intervention to promote AI safety. How might the government intervene? The paper considers circumstances under which it would be rational to tax or even ban AI in particularly risky use cases. Generally, they find that gradually adopting new technologies is better for society if it allows us to learn about their risks before deploying them widely.&nbsp;</p><p>Cautious development of new technologies is not the norm. Instead, technologists often operate by Facebook\u2019s old motto: \u201cMove fast and break things.\u201d Creating a strong safety culture in AI development will be an important challenge for the field.</p><h2>Continuing Drivers of AI Capabilities</h2><p>Will the rapid advances in AI observed over the last few years continue? For one perspective on this question, we can look at some of the key factors driving AI capabilities: compute, data, and AI R&amp;D. Each one appears poised to continue rapidly growing over the next few years.</p><p><strong>Compute = Spending x Efficiency. </strong>\u201cCompute\u201d refers to computational power. Modern AI systems are typically trained for weeks or months on thousands of specialized computer chips. Over the last decade, the amount of compute used to train cutting edge AI systems has roughly <a href=\"https://epochai.org/blog/compute-trends\">doubled every 6 months</a>.</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd2cea9d-3b67-4517-a941-f32602d29e2c_1600x1017.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd2cea9d-3b67-4517-a941-f32602d29e2c_1600x1017.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd2cea9d-3b67-4517-a941-f32602d29e2c_1600x1017.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd2cea9d-3b67-4517-a941-f32602d29e2c_1600x1017.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd2cea9d-3b67-4517-a941-f32602d29e2c_1600x1017.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd2cea9d-3b67-4517-a941-f32602d29e2c_1600x1017.png 1456w\"></a></p><p>Compute growth can be broken down into <i>spending</i> and <i>efficiency</i>. The <i>efficiency</i> of AI chips has continued to grow over the last decade, with the number of calculations per dollar roughly <a href=\"https://epochai.org/blog/trends-in-gpu-price-performance\">doubling every 2.5 years</a>. But if efficiency only doubles every 2.5 years, and overall compute doubles every 6 months, what accounts for the difference?&nbsp;</p><p><i>Spending</i> has been the biggest driver of compute growth over the last decade, roughly <a href=\"https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems\">doubling every 7 months</a>. GPT-4 cost <a href=\"https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/\">more than $100M</a> to train, according to OpenAI CEO Sam Altman, and many other companies are <a href=\"https://www.reuters.com/technology/chinas-internet-giants-order-5-bln-nvidia-chips-power-ai-ambitions-ft-2023-08-09/\">spending billions</a> to purchase AI chips for training future models. Companies such as Microsoft and Google annually spend <a href=\"https://www.nasdaq.com/articles/which-companies-spend-the-most-in-research-and-development-rd-2021-06-21\">tens of billions of dollars on R&amp;D</a> for new technology, so it\u2019s possible that AI expenditure trends could continue for five or more years before exceeding the budgets of the largest technology companies today.&nbsp;</p><figure class=\"image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788761e0-bebc-4ab6-a8fa-0fbcb1eb41a7_1638x582.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788761e0-bebc-4ab6-a8fa-0fbcb1eb41a7_1638x582.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788761e0-bebc-4ab6-a8fa-0fbcb1eb41a7_1638x582.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788761e0-bebc-4ab6-a8fa-0fbcb1eb41a7_1638x582.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788761e0-bebc-4ab6-a8fa-0fbcb1eb41a7_1638x582.png 1456w\"><figcaption><i>Training leading AI systems is becoming more expensive. Academic researchers often do not have the budgets to compete with AI companies. (</i><a href=\"https://www.science.org/doi/10.1126/science.ade2420\"><i>Source</i></a><i>)</i></figcaption></figure><p>The overall number of computations used to train an AI system is an important driver of its capabilities. Because budgets are skyrocketing as coational efficiency continues to grow, it seems likely that AI systems trained over the next few years will use more compute than ever before.&nbsp;</p><p><strong>Recent AI progress has been driven by data. </strong>AI systems that generate text and images are trained to imitate human text and real images scraped from the internet. For example, one popular <a href=\"https://arxiv.org/abs/2101.00027\">text dataset</a> includes large chunks of Wikipedia, GitHub, PubMed, FreeLaw, HackerNews, and arXiv.&nbsp;</p><p>It\u2019s possible that companies will face barriers to gather more training data. Several lawsuits are currently arguing that companies should be required to obtain consent before using people\u2019s data. Even if these lawsuits are struck down, there is only so much human-written text and real images to be gathered online. One <a href=\"https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-projecting-dataset\">analysis</a> suggests that while high-quality text data may run out sometime next year, images and lower-quality text will remain plentiful for another decade or two.&nbsp;</p><p>Even after exhausting online text and images, there are several other data sources that AIs could be trained on. Videos could be scraped for both visual and audio information, and AIs could be trained to successfully perform tasks in simulations and in the real world. Moreover, <a href=\"https://arxiv.org/abs/2210.11610\">several</a> <a href=\"https://arxiv.org/abs/2212.08073\">recent</a> <a href=\"https://arxiv.org/abs/2303.17651\">papers</a> have shown that AI systems can generate data, filter out low quality data points, and then train on their own outputs, improving performance in areas including math and conversational skills.&nbsp;</p><p>Data is a key component of recent AI progress, and there appears to be at least a decade\u2019s worth of additional text and image data available online. If that is exhausted, there will be several other sources of data that companies could use to train more advanced AIs.&nbsp;</p><p><strong>AI R&amp;D might accelerate compute and data growth.</strong> AI systems are reaching the point of being able to contribute to the acceleration of AI progress. For example, <a href=\"https://ai.googleblog.com/2022/07/ml-enhanced-code-completion-improves.html?m=1\">Google equipped their programmers with an AI coding assistant</a> and found that it accelerated their development process. 25% of all suggestions made by the coding assistant were accepted, and the AI assistant wrote 2.6% of all code in the study.&nbsp;</p><p>More impactfully, AI systems are increasingly used to generate their own training data. Google released a <a href=\"https://arxiv.org/abs/2210.11610\">paper</a> showing that training large language models on a filtered subset of their own outputs improves their performance on a variety of benchmarks. Anthropic uses a <a href=\"https://arxiv.org/abs/2212.08073\">similar</a> setup, prompting their model to critique its own outputs and rewrite them, then fine-tuning on the improved versions. While training a model on its own outputs can have drawbacks, it has enabled several recent advancements.&nbsp;</p><p>AI R&amp;D might also allow developers to more efficiently exploit compute. Companies have a limited number of chips for training AI systems, and must use them efficiently. NVIDIA, a chip designer whose stock price recently skyrocketed, rose to prominence partly because their programming language CUDA makes it easy for developers to use their compute efficiently. As AI capabilities improve in manipulating software programs, AIs could be used to make the most of a limited supply of compute.&nbsp;</p><p>For other examples of AI improving AI progress, see <a href=\"https://ai-improving-ai.safe.ai/\">this site</a> maintained by the Center for AI Safety.&nbsp;</p><h2>Links</h2><ul><li>Spain creates <a href=\"https://decrypt.co/153482/spain-just-created-the-first-european-ai-supervision-agency\">Europe\u2019s first national AI agency</a>. The EU AI Act calls for all countries to designate a regulatory authority for implementing the Act\u2019s provisions.&nbsp;</li><li>The United Nations <a href=\"https://www.linkedin.com/posts/un-tech-envoy_%3F%3F%3F%3F-%3F%3F%3F-%3F%3F%3F%3F%3F%3F-%3F%3F-%3F%3F-activity-7097568680066068481-egyT/?utm_source=share&amp;utm_medium=member_ios\">calls for short papers</a> to advise their High-level Advisory Body on AI.&nbsp;</li><li>A <a href=\"https://foresight.org/ai-safety/\">call for grant applications</a> in specific topics in neuroscience, information security, and other areas related to AI safety.</li><li>Yoshua Bengio writes about the <a href=\"https://yoshuabengio.org/2023/08/12/personal-and-psychological-dimensions-of-ai-researchers-confronting-ai-catastrophic-risks/\">personal and psychological dimensions</a> of confronting AI catastrophic risks.&nbsp;</li><li>Opinion article in Politico calls for <a href=\"https://www.politico.com/news/magazine/2023/08/20/its-time-to-nationalize-ai-00111862\">public control</a> of advanced AI systems.&nbsp;</li></ul><p>See also: <a href=\"https://www.safe.ai/\">CAIS website</a>, <a href=\"https://twitter.com/ai_risks?lang=en\">CAIS twitter</a>, <a href=\"https://newsletter.mlsafety.org/\">A technical safety research newsletter</a>, and <a href=\"https://arxiv.org/abs/2306.12001\">An Overview of Catastrophic AI Risks</a></p><p>Subscribe <a href=\"https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916\">here</a> to receive future versions.</p>", "user": {"username": "Center for AI Safety"}}, {"_id": "ak6gHqxjrwHqeKdEB", "title": "I am hiring an Executive Research Assistant - improve RP while receiving mentorship", "postedAt": "2023-08-29T12:31:33.544Z", "htmlBody": "<p><strong>I\u2019m Peter Wildeford, co-CEO at Rethink Priorities (RP).</strong> RP is a research and implementation group that identifies pressing opportunities to make the world better. We act upon these opportunities by developing and implementing strategies, projects and solutions to key issues. We do this work in close partnership with foundations and impact-focused non-profits.</p><p><strong>I\u2019m excited to be hiring a new Executive Research Assistant (ERA) role to work directly with me to help fill important gaps in RP\u2019s needs</strong>, and keep me and my fellow co-CEO Marcus consistently informed and prepared.&nbsp;<a href=\"https://careers.rethinkpriorities.org/en/postings/8588ecc5-3e26-4086-bdb2-fe9a2eb43252\"><u>See more detail + apply here</u></a>.</p><p><br><strong>The work is incredibly varied and will give you a diversity of experience.</strong> Past ERA projects have included varied tasks such as creating a risk management framework for RP, summarizing relevant research papers, interviewing different relevant experts about how to tackle a particular problem, producing&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/weekly-summaries-project\"><u>EA Forum summaries</u></a>, responding to co-CEO inbound email correspondence in a timely and organized manner, and ensuring meetings with co-CEOs and others are scheduled.</p><p>Project selection is adaptable to the candidate\u2019s interests and can fit more junior candidates (eg. more scheduling) and more senior candidates (eg. more research, project management, or stakeholder outreach).<br>&nbsp;</p><p><strong>The role also comes with significant career mentoring.</strong> In fact, the reason we are hiring for this role is that past people in this position have already moved on to significant more senior roles at RP. As your manager I will meet with you regularly to understand your career goals, help you test fit for various roles, and put you on a path to success. The work itself will also build varied skills and connections within the organization and potentially some of our external stakeholders.&nbsp;</p><p>&nbsp;</p><p><strong>Some fast facts about the role:</strong></p><ul><li>Deadline to apply - September 17</li><li>Role is permanent, full or part time with a minimum of 20hrs/week</li><li>The role is remote and we are able to hire in most countries</li><li>The compensation is $69,000 - $87,000 USD/year depending on experience (the amount is calculated using RP\u2019s salary algorithm and is dependent on prior relevant experience and corresponding title level)</li><li><a href=\"https://careers.rethinkpriorities.org/en/postings/8588ecc5-3e26-4086-bdb2-fe9a2eb43252\"><u>See more detail + apply here</u></a>.<br>&nbsp;</li></ul><p><strong>A pitch for applying:</strong></p><ul><li>You could make an immediate impact on a large organization at the CEO level</li><li>Work at RP influences the decision-making of major foundations spending hundreds of millions of dollars</li><li>You will receive useful career mentorship and the opportunity to test fit and build skills for a variety of future roles based on your interests</li><li>Good work-life balance, generous paid time off leave, and generous benefits including parental leave of up to 6 months during the first 2 years after, or split before and after post a child\u2019s birth or adoption for parents of all genders<br>&nbsp;</li></ul><p><strong>Prior people holding this role have said:</strong></p><ul><li>\u201cI can say from first-hand experience that this is a great job\u2014You get to do important, interdisciplinary work, and the Co-CEOs are super supportive of you finding and developing your own strengths\u201d</li><li>\"This role was fantastic for helping me build up relevant subject matter expertise and test fit for roles that would have been difficult to enter directly. There's also a lot of chances for direct impact - your ideas are listened to, and there's lots of autonomy such that the way you approach things and the specific skills you bring will make a real difference to outcomes of the projects you take on.\"<br>&nbsp;</li></ul><p><strong>This role would be ideal for candidates who demonstrate:</strong></p><ul><li>Strong writing skills, especially the ability to write concisely yet accurately</li><li>Ability to accurately and quickly interpret research from a wide variety of fields, including fields you are not initially familiar with</li><li>Strong organizational skills</li><li>Comfort with statistical reasoning</li><li>Ability to give upward feedback</li><li>Reliability</li><li>Flexibility to work across a wide variety of areas and tasks</li><li>Comfort with ambiguity</li><li>Ability to represent RP and the co-CEOs well in internal and external meetings</li></ul><p>&nbsp;</p><p>RP is committed to building an inclusive, equitable, and supportive community for you to thrive and do your best work. Please don\u2019t hesitate to apply for a role regardless of your age, gender identity/expression, political identity, personal preferences, physical abilities, veteran status, neurodiversity or any other background. We provide reasonable accommodations and benefits, including flexible work schedules and locations, mental health coverage in medical benefits (as available), as well as technology budgets and professional development time that can be used, for example, to purchase assistive technology or engage in job coaching.&nbsp;<br>&nbsp;</p><p><a href=\"https://careers.rethinkpriorities.org/en/postings/8588ecc5-3e26-4086-bdb2-fe9a2eb43252\"><strong><u>You can apply here</u></strong></a><strong>.</strong><br>&nbsp;</p><p>Also, I am hosting a webinar to answer questions about the role on September 8 at 11am EST. If you\u2019d like, you can register&nbsp;<a href=\"https://us02web.zoom.us/webinar/register/WN_LIqDfQmxRHac9VHVwEilAw#/registration\"><u>here</u></a> and pre-submit questions via Slido&nbsp;<a href=\"https://app.sli.do/event/gj5AKSjJhzX7fzcpa9Gypp/live/questions\"><u>here</u></a>. The webinar will be recorded and made available&nbsp;<a href=\"https://drive.google.com/drive/u/0/folders/1NtYp6bVHjOepcN9U90YFV0DvPzgI1PyI\"><u>here</u></a> afterwards.</p>", "user": {"username": "Peter_Hurford"}}, {"_id": "DLrhnzDeqSywhrNew", "title": "Agency Foundations Challenge: September 8th-24th, $10k Prizes", "postedAt": "2023-08-30T06:12:08.449Z", "htmlBody": "<p><strong>TLDR;</strong> Join us Friday Sep 8th for starting the very f<strong>i</strong>rst technical/non-technical&nbsp;<a href=\"https://alignmentjam.com/jam/agency\">Agency Foundations Challenge</a> where we explore agency-preservation in AI-human interactions. Make your submissions by Sunday Sep 24rd, 2023 - with prizes awarded by Oct 1st, 2023.</p><p>Below is an FAQ-style summary of what you can expect.</p><h2>Intro</h2><p>We are working on developing an agency foundations research paradigm for AI safety and alignment. We are kicking off this project with a 2-week long hackathon/challenge that seeks to re-formulate AI alignment problems as agency-related challenges.&nbsp;</p><p>For the &nbsp;Agency Foundations Challenge we suggested a few topics to start (see below), however, given the broad meaning of \"agency\" we are open to other research directions (as long as they fall generally into one of the categories below). &nbsp;</p><p>More information of our conceptual goals for this hackathon are provided here:&nbsp;<a href=\"https://www.agencyfoundations.ai/hackathon.\">https://www.agencyfoundations.ai/hackathon.</a></p><h2>The schedule</h2><h2><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/DLrhnzDeqSywhrNew/n9iohwyjluy8x9cihdch\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/DLrhnzDeqSywhrNew/kjh0js3pnii6gibjiecy 90w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/DLrhnzDeqSywhrNew/gmakcv8tlqnqgyf7biko 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/DLrhnzDeqSywhrNew/qco3ccv3smxzzmzjajpi 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/DLrhnzDeqSywhrNew/p70ie4lp05p8upj3sbhk 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/DLrhnzDeqSywhrNew/dllmqhyfn4eo2dc9wfcy 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/DLrhnzDeqSywhrNew/srekpxy60p0t1qvhks2y 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/DLrhnzDeqSywhrNew/bugr4yqsphepkotnitss 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/DLrhnzDeqSywhrNew/s0tjzdheo8yeqqeypp7a 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/DLrhnzDeqSywhrNew/jktppzgmx7al1qlpkzme 810w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/DLrhnzDeqSywhrNew/o9iyytsl58ze9isv46au 898w\"></h2><h2>What is Alignment Jams?</h2><p><a href=\"https://alignmentjam.com/\"><u>The Alignment Jams</u></a> are research events where participants of all skill levels join in teams (1-5) to engage with direct AI safety work. You submit a PDF report on the participation page with the great opportunity to receive a review from several people working in the field.</p><p>If you are not at any of the in-person jam sites, you can participate online through&nbsp;<a href=\"https://discord.gg/3PUSbdS8gY\"><u>our Discord</u></a> where the keynote, award ceremony and AI safety discussion is happening!</p><p>The event happens from the 8th to the 24th of September.</p><h2>Where can I join?</h2><p>You can join the event both in-person and online but everyone needs to make an account and join the jam on the&nbsp;<a href=\"https://alignmentjam.com/jam/agency\">hackathon page</a>.</p><p>We already have confirmed in-person hackathon sites at <a href=\"https://discord.gg/Hym2F4Kn2j\">UT Austin, Texas</a> and at <a href=\"https://www.facebook.com/events/289402353695386/?acontext=%7B%22ref%22%3A%2252%22%2C%22action_history%22%3A%22%5B%7B%5C%22surface%5C%22%3A%5C%22share_link%5C%22%2C%5C%22mechanism%5C%22%3A%5C%22share_link%5C%22%2C%5C%22extra_data%5C%22%3A%7B%5C%22invite_link_id%5C%22%3A626084386173423%7D%7D%5D%22%7D\">Prague Fixed Point</a>. We expect more to join and you're very welcome to <a href=\"https://alignmentjam.com/jam/agency\">sign up with your own</a>.</p><p>Everyone should <a href=\"https://alignmentjam.com/jam/agency\">sign up on the hackathon page</a> to receive emails. Also join the Discord to stay updated and ask any questions.&nbsp;<a href=\"https://discord.gg/3PUSbdS8gY\"><strong><u>Join here</u></strong></a>.</p><h2>What is Agency Foundations and what projects could I make?</h2><p>We view \"agency\" as a misunderstood and understudied topic in AI safety research that is often confounded with terms such as \"agents\", \"autonomy\" or \"utility optimization\". Briefly, \"agency\" is a complex notion that draws on concepts from philosophy, psychology, biology and other fields. It generally refers to the capacity of individual organisms/systems/agents to both \"experience\" control over their own actions and the world while also being actually (i.e. causally) effective actors in this world and continuing to be such actors for the foreseeable future (please see our <a href=\"https://arxiv.org/abs/2305.19223\">foundations paper</a> for more detailed discussions and suggestions). &nbsp; Thus, AI/AGI systems that are slowly/quickly striping humans of economic, political and social power - are depleting human agency - regardless of how much \"utility\" or \"autonomy\" they provide (or make people \"feel\" like they provide). &nbsp;In fact, we argue that once AI/AGI systems become very powerful models of human behavior - they will converge towards \"agency-depletion\" behaviors as instrumental goals and it will be difficult or impossible to stop them given their ability to predict/control/manipulate future human actions. &nbsp;We thus view it as a significant challenge to develop safe AI/AGI systems that do not manipulate/give a false sense of \"experience\" of agency while also not usurping humanity's control over the world. &nbsp;</p><p>The submissions on the topic of agency can be based on the cases presented on&nbsp;<a href=\"https://alignmentjam.com/jam/agency\"><u>the Alignment Jam website</u> </a>and focus on specific problems in the interaction between human agency and AI systems. We invite you to read some or our ideas around agency-loss in AI human interactions <a href=\"https://www.agencyfoundations.ai/\">here</a> and ideas and categories for projects <a href=\"https://www.agencyfoundations.ai/hackathon\">here</a>. However, we are open to creative and novel directions of research that are more technical but also more philosophical as long as they are focused on agency characterization in AI-human interactions.&nbsp;</p><p>We provide some starting inspiration with some cases in a variety of scenarios:</p><ul><li>Figuring out how to describe \"agency\" preservation and enhancement algorithmically.&nbsp;</li><li>Red-teaming \"intent\", \"truth\" and \"interpretability\" vs. \"agency\" as necessary/sufficient/desirable conditions for safe and aligned AIs.</li><li>Determining the empirical bases/limits on learning \"agency\" from empirical data. For example, learning policies and reward functions from behavior and (agent) internal state data.</li><li>Any other topics including governance, philosophy and psychology related to agency characterization and preservation in AI-human interactions.</li></ul><p>This will be our first agency foundations hackathon and we're excited to see which proposals you come up with!</p><h2>Why should I join the challenge?</h2><p>There\u2019s loads of reasons to join! Here are just a few:</p><ul><li>Understand and engage with the new field of agency preservation against AI</li><li>See how fun and interesting AI safety can be!</li><li>Get a new perspective</li><li>Acquaint yourself with others who share your interests</li><li>Get a chance to win prizes in total of $10,000.</li><li>Get practical experience with AI safety research</li><li>Have a chance to work on that project you've considered starting for so long</li><li>Get proof of your skills so you can get that one grant to pursue AI safety research</li><li>And of course, many other reasons\u2026&nbsp;<a href=\"https://alignmentjam.com/jam/agency\"><u>Come along</u></a>!</li></ul><h2>What if I don\u2019t have any experience in AI safety?</h2><p><strong>Please&nbsp;</strong><a href=\"https://alignmentjam.com/jam/agency\"><strong><u>join</u>!</strong> </a>This can be your first foray into AI, ML safety and maybe you\u2019ll realize that it\u2019s not that hard. Even if you don't find it particularly interesting, this might be a chance to engage with the topics on a deeper level.</p><p>There\u2019s a lot of pressure from AI safety to perform at a top level and this seems to drive some people out of the field. We\u2019d love it if you consider joining with a mindset of&nbsp;<strong>fun exploration and get a positive experience</strong> out of the weekend.</p><h2>What is the agenda for the weekend?</h2><p>The schedule runs from 6PM CET / 9AM PST Friday to 7PM CET / 10AM PST Sunday. We start with an introductory talk and end with an awards ceremony. Subscribe to the public calendar here. [LINK TO BE ADDED]</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">CET / PST</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Fri Sep 8th 6:00 PM</p><p>9:00 AM</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Introduction to the hackathon, what to expect, and talks from Catalin Mitelut, Tim Franzmeyer, and others (TBC). Afterwards, there's a chance to find new teammates.</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Fri Sep 8th 7:30 PM 10:30 AM</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Challenge begins!</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Sun 24th&nbsp;</p><p>6:00 PM</p><p>9:00AM</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Final submissions have to be finished. Judging begins and both the community and our &nbsp;judges from ERO &nbsp;join us in reviewing the proposals.</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Sunday Oct 1st</p><p>6:00 PM</p><p>9:00 AM</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Prizes are announced. The winning projects will be invited to do video presentations.</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Afterwards!</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">We hope you will continue your work from the hackathons with the purpose of sharing it on the forums or your personal blog!</td></tr></tbody></table></figure><h2><br>&nbsp;I\u2019m busy, can I join for a short time?</h2><p>As a matter of fact,&nbsp;<strong>we encourage you to join</strong> even if you only have a short while available during the weekend!</p><p><strong>So</strong>&nbsp;<strong>yes,&nbsp;</strong>you can both join without coming to the beginning or end of the event, and you can submit research even if you\u2019ve only spent a few hours on it. We of course still encourage you to come for the intro ceremony and join for the whole weekend but everything will be recorded and shared for you to join asynchronously as well.</p><h2>Wow this sounds fun, can I also host an in-person event with my local AI safety group?</h2><p><strong>Definitely!</strong> We encourage you to join our team of in-person organizers around the world for the agency foundations hackathon.</p><p>You can read more about what we require&nbsp;<a href=\"https://alignmentjam.com/site\"><u>here</u></a> and the possible benefits it can have to your local AI safety group&nbsp;<a href=\"https://alignmentjam.com/why\"><u>here</u></a>.&nbsp;<strong>Sign up as a host under \"Jam Site\" on&nbsp;</strong><a href=\"https://alignmentjam.com/jam/agency\"><strong><u>this page</u></strong></a>.</p><h2>What have previous participants said about this hackathon?</h2><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">I was not that interested in AI safety and didn't know that much about machine learning before, but I heard from this hackathon thanks to a friend, and I don't regret participating! I've learned a ton, and it was a refreshing weekend for me.</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">A great experience! A fun and welcoming event with some really useful resources for starting to do interpretability research. And a lot of interesting projects to explore at the end!</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Was great to hear directly from accomplished AI safety researchers and try investigating some of the questions they thought were high impact.</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">I found the hackathon very cool, I think it lowered my hesitance in participating in stuff like this in the future significantly. A whole bunch of lessons learned and Jaime and Pablo were very kind and helpful through the whole process.</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">The hackathon was a really great way to try out research on AI interpretability and getting in touch with other people working on this. The input, resources and feedback provided by the team organizers and in particular by Neel Nanda were super helpful and very motivating!</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td></tr></tbody></table></figure>", "user": {"username": "Catalin M"}}, {"_id": "iZKexkGDndiWtMbxb", "title": "How to listen to PDFs & Google Docs", "postedAt": "2023-08-29T10:56:43.644Z", "htmlBody": "<p>As of August 2023, <a href=\"https://speechify.com/\">Speechify</a> is probably your best option.</p><p>I\u2019ve made a <a href=\"https://www.loom.com/share/3e4cc4b9196b481f8a3d0e87f3a92ff9?sid=3af85c38-2ade-4695-a229-2242115a5b39\">3 minute demo</a> to show how it works (and sounds).</p><h2>Listening to PDFs</h2><p>For PDFs, the key strengths of Speechify are:</p><ol><li><strong>Multi-modal UI:</strong> listening to academic papers is nice, but often you really need to use your eyes\u2014to read a graph, grok a formula, or skip to the interesting parts. Speechify's interface makes it <u>easy to switch between listening and reading</u>, whether on desktop, mobile phone or iPad. To skip to a particular sentence, just click on it.</li><li><strong>Solid mobile apps (iOS <u>and</u> Android):</strong> files are synchronised quickly, and it remembers your playback position across devices. You can read your PDFs, tap to skip to a given sentence, etc.</li><li><strong>Great voices:</strong> the best AI voices I've heard. The pronunciation of specialist terms is imperfect, but much better than any other service.</li><li><strong>Filtering:</strong> the app can filter out citations, URLs and parenthetical remarks.</li><li><strong>Speed:</strong> everything is fast; most of the UI is well-designed.</li><li><strong>OCR:</strong> you can listen to PDFs even when they are just scans of a physical book.</li></ol><p>The main shortcomings are:</p><blockquote><p><strong>a.</strong> It sometimes narrates text it should obviously skip. There should be more skip options, e.g. skip all tables.</p><p><strong>b.</strong> It can't really handle formulae.</p><p><strong>c.</strong> You can't annotate PDFs in their app.</p></blockquote><p>At <a href=\"https://type3.audio/\">TYPE III AUDIO</a>, we\u2019ve been thinking about building a \u201clisten to PDF\u201d feature that delivers a narration to your podcast app. We\u2019re still considering this, but my current view is that a multi-modal UI is the ideal setup for proper engagement with academic PDFs, and that Speechify are well on course to nailing it.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iZKexkGDndiWtMbxb/fkqkcawg5fmkrkevii2g\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iZKexkGDndiWtMbxb/i6rn9kwulhq9zn2er0xp 290w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iZKexkGDndiWtMbxb/hhnvlafeff7hgdrt2ljc 580w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iZKexkGDndiWtMbxb/g2bpahslg4am1bcgy7tv 870w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iZKexkGDndiWtMbxb/l0ba0oangwalkgtcw4hv 1160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iZKexkGDndiWtMbxb/umuysecpsose0nqdry9v 1450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iZKexkGDndiWtMbxb/ryenj8qkbq31d0mqmnz0 1740w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iZKexkGDndiWtMbxb/jmhufzzgisgblo4gl0at 2030w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iZKexkGDndiWtMbxb/bflgc8drie8uosvv0lgu 2320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iZKexkGDndiWtMbxb/hsuxxavmwjhlpgelxqew 2610w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/iZKexkGDndiWtMbxb/rucejchd9ossclrsi52g 2874w\"><figcaption>Listening to a PDF with Speechify. See my <a href=\"https://www.loom.com/share/3e4cc4b9196b481f8a3d0e87f3a92ff9?sid=3af85c38-2ade-4695-a229-2242115a5b39\">3 minute demo</a>.</figcaption></figure><h2>Listening to Google Docs</h2><h3>Option 1. Use the Speechify app (more features, less secure)</h3><p>If you grant Speechify access to your Google Drive, then you can import Google Docs into their web app. They convert your doc into a PDF, and you get the same experience I described above.</p><p>The main problem is that you have to give Speechify access to all documents on your Google Drive. For some people, that will be an unacceptable security risk. &nbsp;</p><h3>Option 2. Use the browser extension (more secure, fewer features)</h3><p>You can use the <a href=\"https://chrome.google.com/webstore/detail/speechify-for-chrome/ljflmlehinmoeknoonhibbjpldiijjmm\">Speechify browser extension</a> to listen to Google Docs directly on docs.google.com.</p><p>The listening experience is similar to PDFs, but:</p><ol><li>You can't filter out citations, URLs, etc.</li><li>If you want to listen on mobile, you have to click the \"Bookmark\" icon. When you do this, all the formatting of the Google Doc is lost.</li></ol><p>During installation the extension requests access to all your tabs. But, you can change this to \u201cWhen you click the extension\u201d. If you do that, then your browser will only give Speechify access to a particular document (or other URL) when you click the icon.</p><h2>Note: Speechify may not be suitable for listening to extremely sensitive documents</h2><p>Speechify sends the text of your document to their text-to-speech API. If you are dealing with extremely sensitive material, the security/convenience tradeoff may not be worth it. Ask your IT security team if you\u2019re unsure.</p><h2>Bonus: Listen to any web page</h2><p>You can also use Speechify to listen to any web page.</p><p>Their browser extension is the best way to do this. If you want to listen on your mobile, click \u201cBookmark\u201d to add the article to your library.</p><p>The Speechify app also has an \u201cadd via URL\u201d function, but it\u2019s weirdly bad at detecting the start of the article text. You\u2019ll often have to skip through navigation elements and other useless stuff before you get to the start of the article. I imagine they'll fix this soon.</p><h2>Have you tried Speechify?</h2><p>If you\u2019ve tried Speechify, I\u2019d love to hear from you, especially if you\u2019re listening to academic PDFs. Do you use it regularly? If not\u2014why not? How could it be improved?</p><p>Comment below or write to <a href=\"mailto:peter@type3.audio\">peter@type3.audio</a>.</p>", "user": {"username": "Peter_Hartree"}}, {"_id": "y7wwsBqfSeKxaXYZy", "title": "Giving gladly, giving publicly", "postedAt": "2023-08-29T10:43:26.527Z", "htmlBody": "<p>A short post on my blog detailing a recent donation - and my intention to be a bit more public about my giving in future.</p>\n<p>I'm never sure of the expected value of these sorts of posts; they feel a bit awkward and braggy to me. But I do want to get more people into philanthropy and the mindset that they can make a difference even with (relatively) small sums of money.</p>\n", "user": {"username": "HenryStanley"}}, {"_id": "aFYduhr9pztFCWFpz", "title": "Preliminary Analysis of Intervention to Reduce Lead Exposure from Adulterated Turmeric in Bangladesh Shows Cost Benefit of About US$1 per DALY \n", "postedAt": "2023-08-29T07:32:06.817Z", "htmlBody": "<p><i>Pure Earth is a GiveWell Grantee dedicated to reducing lead exposure in low- and middle-income countries. In collaboration with Stanford University and the International Centre for Diarrhoeal Disease Research, Bangladesh (icddr,b),&nbsp;a preliminary cost-effectiveness analysis (CEA) was performed to assess the effectiveness of an intervention in Bangladesh. The CEA presents an encouraging outlook, with a cost per disability-adjusted life year (DALY)-equivalent averted estimated at just under US$1. As this assessment is preliminary, it may contain methodological inconsistencies with GiveWell\u2019s. As such, we welcome any comments and corrections.</i></p><hr><p>In 2019, after investigations concluded that turmeric was the&nbsp;<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7705119/\"><strong>primary source of lead exposure</strong></a>&nbsp;among residents of rural Bangladesh, Stanford University and Bangladeshi non-profit icddr,b embarked on a mission to eliminate lead poisoning from turmeric. Stanford and icddr,b\u2019s investigations had revealed that lead chromate (an industrial pigment sometimes called \u201cSchool Bus Yellow\u201d) was being added to turmeric roots to make them&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S0013935119305195\"><strong>more attractive for sale</strong></a><strong>.</strong>&nbsp; Armed with this evidence, the team coordinated with the Bangladeshi Food Safety Authority to conduct a crack-down of the adulteration by enforcing policies at the markets and raising awareness among businesspeople and the public nationwide.&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S0013935123011325?via%3Dihub\"><strong>These efforts</strong></a> successfully halted the practice of adding lead chromate to turmeric: the prevalence of lead in turmeric dropped from 47% in 2019 to 0% in 2021. In collaboration with Pure Earth, icddr,b continues to monitor turmeric and other spices and coordinate with government agencies to maintain the safety of these and other food products.&nbsp;&nbsp;</p><p>To gauge the effectiveness of this program in advancing the mission of reducing lead exposures globally, it is important to assess both impact and cost-effectiveness. To approach this task, Pure Earth and Stanford have completed a back-of-the-envelope cost-effectiveness analysis (CEA), incorporating preliminary data from blood lead level assessments and various assumptions.<strong>&nbsp;</strong>This model is built off of previous models created by&nbsp;<a href=\"https://leadelimination.org/malawi_cost-effectiveness_intro/\"><strong>LEEP</strong></a>&nbsp;and&nbsp;<a href=\"https://rethinkpriorities.org/publications/exposure-to-lead-paint-in-low-and-middle-income-countries\"><strong>Rethink Priorities</strong></a>.&nbsp;</p><p>The&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1d11_CZj78k4F03M_17bgJ7YBbuGUGk9tkAMTLg_6hhY/edit#gid=0\"><strong>preliminary findings</strong></a> are that this program can avert an equivalent DALY for just under $1. This result is extraordinary, albeit deserving of further scrutiny. It indicates that certain interventions in the lead space could be enormously cost-effective.&nbsp; The body of&nbsp; work to reduce lead exposures in LMICs is nascent, and not all interventions are likely to be as cost-effective as spices.&nbsp; But clearly, more work on spices is called for, and Pure Earth, Stanford, icddr,b, and others are pursuing funding to expand these programs into other countries.&nbsp;&nbsp;</p><p><br><strong>Program Implementation Costs</strong></p><p>To establish a framework for cost-effectiveness assessment, it is essential to define the terms \"cost\" and \"effectiveness\" within the context of the Stanford-led mission. The concept of \"cost\" encompasses the resources utilized by the project team and those expended by the Bangladesh government as a direct result of the project\u2019s activities. Specifically, we consider monetary expenses incurred by the program, which we estimate to be upfront costs of $360,000. These expenses include both the costs to identify the sources of lead exposure and implement the program, as well as continuing costs of $100,000 to monitor and continue the program after the initial implementation. Additionally, the Government of Bangladesh is expected to spend $100,000 over the course of the intervention.</p><p>To facilitate comparisons with other global health interventions, we define the project\u2019s\"effectiveness\" as the prevention of negative outcomes resulting from its efforts. Lead contamination adversely affects children in two primary ways: first, it harms their physical health, leading to premature death or physical disabilities; and second, it permanently impairs their cognitive abilities, resulting in decreased productivity and lower earnings over their lifetimes.</p><p>To quantify the first effect, the World Health Organization employs a metric called \"disability-adjusted life years'' (DALYs), where each unit represents the loss of one full year of healthy life. The second effect is evaluated by measuring the income loss, which we convert into DALY-equivalents. In line with&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1LIaI7_AfugBNGcTJUDm8Zj--QerASzaowRd7fafzjU4/edit#gid=0\">GiveWell's approach</a>, we equate one DALY to 2.8 years of income. Therefore, our effectiveness measurement relies on the total number of DALY-equivalents averted through the project\u2019s interventions.</p><p><br>&nbsp;<strong>DALY-Equivalents</strong></p><p>Through our program, we anticipate a significant reduction in the consumption of lead through turmeric, consequently lowering the number of children in Bangladesh who are exposed to lead each year. This, in turn, contributes to a decrease in the total number of DALY-equivalents attributed to lead poisoning. However, even in the hypothetical scenario where the project had not intervened, it is theoretically possible (though highly unlikely) that Bangladesh would have eventually enforced the existing regulations on leaded additives to spices and achieved similar benefits over time.</p><p>Therefore, the project\u2019s impact lies not in identifying and enforcing food safety regulations,&nbsp; but rather in expediting its implementation by several years. In the model we use a very conservative estimate of this acceleration to be 8 years. The practice of adding lead chromate had been going on for over four decades and no signs were seen of it stopping without this intervention. By \"averting\" DALY-equivalents, we mean reducing the number of DALY-equivalents that would have otherwise been incurred if our efforts had not intervened, thereby minimizing the overall burden of lead-related health and cognitive impairments.</p><p>&nbsp;</p><p><strong>Cost Effectiveness Analysis&nbsp;</strong></p><p>To determine the number of DALY-equivalents that will be prevented by the program, we rely on data and modeling derived from two prominent research studies. The&nbsp;<a href=\"http://ghdx.healthdata.org/gbd-results-tool\"><strong>Global Burden of Disease Study 2019,</strong></a> provides insights into the impact of lead poisoning on physical health, while a widely cited&nbsp;<a href=\"https://pubmed.ncbi.nlm.nih.gov/23797342/\"><strong>2013 study</strong></a> assesses the economic consequences in low- and middle-income countries (LMICs).&nbsp;</p><p>The project measured blood lead levels among 1,398 women and children prior to the program implementation, and again after the turmeric lead levels reached zero. Fortunately, the preliminary results indicate a sizable reduction of 30% in blood lead levels (unpublished data).</p><p>Converting these reductions in lead exposures, our calculations indicate we can expect to avert approximately 1,000,000 DALY-equivalents in Bangladesh. Among these, around 20% (200,000 DALYs) can be attributed to improved health outcomes, while the remaining 80% (800,000 DALY-equivalents) stem from increased income. It is important to note that these figures are time-discounted, meaning that the value of future benefits is slightly diminished.</p><p>Simultaneously, we estimate that expenditures for the same period will amount to approximately $560,000. We determine the roughly estimated cost-effectiveness of the project\u2019s Bangladesh program to be just under $1 per DALY-equivalent averted.</p><p>Lastly, we can consider a cost-effectiveness of just under US$1 per DALY-equivalent in the broader context of programs within global health and development. Take, for example,&nbsp;<a href=\"https://www.givedirectly.org/\"><strong>GiveDirectly</strong></a>, one of GiveWell\u2019s top-rated charities. Like icddr,b and partners, it raises the incomes of people in low-income countries, but does so by giving unconditional cash transfers. Applying a unit&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1LIaI7_AfugBNGcTJUDm8Zj--QerASzaowRd7fafzjU4/edit?usp=sharing\"><strong>conversion</strong></a> to a&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1B1fODKVbnGP4fejsZCVNvBm5zvI1jC7DhkaJpFk6zfo/edit#gid=1680005064\"><strong>CEA</strong></a> by GiveWell, we find that GiveDirectly has a cost-effectiveness of approximately $836 per DALY-equivalent averted. (It\u2019s important to bear in mind that this comparison is not exactly like for like, since GiveDirectly\u2019s program has been&nbsp;<a href=\"https://www.givewell.org/international/technical/programs/cash-transfers\"><strong>studied extensively</strong></a> and is operating at a much larger scale).</p><p>&nbsp;</p><p><strong>Where could our analysis go wrong?</strong></p><p>Exploring the uncertainties in our model enables decision-makers to assess the reliability of cost-effectiveness estimates and determine whether further information is needed to address existing knowledge gaps. Here, we outline three significant uncertainties that warrant attention.</p><p>Firstly, the model relies on determining the proportion of lead exposure reduction that is attributable to this program.&nbsp; Uniquely for this program we do have direct measures on lead reduction in various areas of Bangladesh, but this is pre-post data, and at the end of the day we have had to make an estimate for what we believe is the amount of lead exposure reduction attributable to this program. Similarly we have had to make estimates for how much lead reduction is occurring in areas where we do not have direct measures of lead reduction. For now, we have taken the mean reduction of 1.64 \u03bcg/dl reduction in blood lead levels measured in the study area populations and have estimated that this effect happens across 50 percent of the population of Bangladesh. We do know, however, that there are no other active lead mitigation programs in place in Bangladesh at the moment (although some are in planning). We believe this to be a conservative estimate, but recognize this is not a very precise figure in our model.</p><p>Secondly, there is uncertainty regarding the number of years by which this project halted the contamination in advance of other future efforts. We chose 8 years as an estimate since, to our knowledge, no other entities were addressing the issue and the problem had existed for over 40 years before we intervened. We also believe this is a conservative estimate.</p><p>Furthermore, like any cost-effectiveness analysis that models changes over time, the choice of time discount rate significantly impacts the results. We apply a 4% per year time discount for future costs and benefits, following the GiveWell approach. Given that some effects of lead exposure prevention, such as income improvements, occur in the distant future, the chosen time discount rate can considerably alter the outcomes.<br>&nbsp;</p><p><strong>Conclusion</strong></p><p>Despite being a preliminary assessment, this cost-effectiveness analysis (CEA) of this&nbsp; intervention in Bangladesh presents an exceptionally encouraging outlook, with a cost per DALY-equivalent averted estimated at just under US$1. It is crucial not to overlook the profound significance of this outcome: US$1 represents a small investment for the equivalent of an additional year of life in optimal health.&nbsp;</p><p>Early results from Pure Earth\u2019s Rapid Market Assessment project find that between 6 and 12 countries may have similar problems with contaminated spices.&nbsp; Large parts of northern India (also highly populated) are similarly affected. Other lead salts are also highly colored, in reds and oranges, and found in other products. Programs to halt intentional contamination of spices and other foodstuffs are enormously impactful, and ought to be a first response in the fight against lead poisoning globally.&nbsp;&nbsp;</p><p>Finally, other significant sources of lead exposure (including leaded pottery and aluminum cookware, paint, medicines etc) require a similar regulatory response, and are likely to show cost benefit ratios that are also very strong.&nbsp;&nbsp;&nbsp;</p><p>&nbsp;</p><p><strong>Acknowledgements</strong></p><p>A special thanks to Erik Hausen (especially), Jenna Forsyth, Steve Luby, James Snowden, and Rich Fuller for their contributions to this post.<br>&nbsp;</p>", "user": {"username": "Kate Porterfield"}}, {"_id": "SAvrBpTpdmjykY3dN", "title": "The Art of Difficult Conversations - Workshop", "postedAt": "2023-08-30T15:53:52.699Z", "htmlBody": "<p>Hey everyone,</p><p>For our monthly workshop in September, <a href=\"https://forum.effectivealtruism.org/users/severin?mention=user\">@Severin</a> will teach us the art of difficult conversations. And here's what that is about:</p><blockquote><p><i>Dysfunctional work teams.&nbsp;Family conflicts. Dinner table debates. Fights with your partner.</i></p><p><i>We constantly find ourselves involved in difficult conversations. They can seem relationship-ending, world-bending, or just plain uncomfortable. Worse yet - all the communication tools we learn seem to disappear just when we need them most.</i></p><p><i>Hard conversations put us in a different headspace than everyday discussion. We need different tools, ones we can call on in the heat of battle.</i></p><p><i>In this workshop, I will teach a novel format for navigating these choppy relational waters that leaders from </i><a href=\"https://www.authrev.org/\"><i>Authentic Revolution </i></a><i>developed - the mother of Authentic Relating practices.</i></p><p><i>Join us to transform difficult conversations into opportunities for growth, connection, and understanding.</i></p></blockquote><p>You're very welcome, even if you\u2019ve never been to a meetup, or you feel like you don't fit in. Here is the link to our last event, maybe you know someone already: <a href=\"https://forum.effectivealtruism.org/posts/tvenCDp9rdHxcrzq7/doing-a-full-time-mental-health-sabbatical-shenanigans\">Doing a \u201cfull-time mental health sabbatical\u201d (Shenanigans Workshop)</a>&nbsp;</p><p><strong>Time</strong>: The workshop starts at 19:00 and is planned for 2h. Feel free to arrive by 18:30. Afterward, there will be time for socializing.</p><p><strong>Food</strong>: I'll bring pita bread, vegetables and dips.</p><p><strong>Location</strong>: The <a href=\"https://berlin.ccc.de/\"><u>Chaos Computer Club Berlin</u></a> can comfortably fit about 20 people. Please RSVP so we know how many to expect.</p><p>Directions (German): <a href=\"https://berlin.ccc.de/page/anfahrt\">https://berlin.ccc.de/page/anfahrt</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdxayu9bgevu\"><sup><a href=\"#fndxayu9bgevu\">[1]</a></sup></span></p><p>Please contact <a href=\"https://forum.effectivealtruism.org/users/new_user_928725294\">__nobody</a> if you have any questions about the location.</p><p>Comment here, or write me <a href=\"https://forum.effectivealtruism.org/users/milli-or-martin\">here</a> or on <a href=\"https://t.me/truemilli\">Telegram</a> for anything else.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndxayu9bgevu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdxayu9bgevu\">^</a></strong></sup></span><div class=\"footnote-content\"><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/tvenCDp9rdHxcrzq7/g4krkgbnnjdzba84gxfx\"><figcaption>This is the entrance: Ring at \"Chaos Computer Club Berlin\" and press when it clicks (no voice or buzzer).</figcaption></figure></div></li></ol>", "user": {"username": "Milli"}}, {"_id": "sfePjA9Cjs9gHFjhx", "title": "International risk of food insecurity and mass mortality in a runaway global warming scenario", "postedAt": "2023-09-02T07:28:39.971Z", "htmlBody": "<p>This is a linkpost for <a href=\"https://www.sciencedirect.com/science/article/pii/S0016328723000770\">International risk of food insecurity and mass mortality in a runaway global warming scenario</a> by C.E. Richards, H.L. Gauch, and J.M. Allwood, published on June 2023. The abstract, factors over and underestimating starvation, and my quick overall take are below.</p><h1>Abstract</h1><blockquote><p>Climate and agriculture have played an interconnected role in the rise and fall of historical civilizations. Our modern food system, based on open-environment production and globalised <a href=\"https://www.sciencedirect.com/topics/social-sciences/supply-chain-management\">supply chains</a>, is vulnerable to a litany of abiotic and biotic stressors exacerbated by anthropogenic climate change. Despite this evidence, greenhouse gas emissions continue to rise. Current trajectories suggest global warming of \u223c2.0\u20134.9&nbsp;\u00b0C by 2100, however, a worst-case emissions scenario with rapid combustion of all available fossil fuels could cause a rise of \u223c12&nbsp;\u00b0C. Even if emissions decline, unprecedented atmospheric CO<sub>2</sub>-e concentrations risk triggering tipping points in climate system feedbacks that may see global warming exceed 8&nbsp;\u00b0C. Yet, such speculative \u2018runaway global warming\u2019 has received minimal attention compared to mainstream low- to mid-range scenarios. This study builds on <i>The Limits to Growth</i> to provide new insights into the international risk of mass mortality due to food insecurity based on a higher-resolution illustration of World3\u2019s \u2018runaway global warming\u2019 scenario (\u223c8\u201312&nbsp;\u00b0C+). Our simulation indicates rapid decline in <a href=\"https://www.sciencedirect.com/topics/social-sciences/food-production\">food production</a> and unequal distribution of \u223c6 billion deaths due to starvation by 2100 [or is it 5 billion<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefurvdvx5gwe\"><sup><a href=\"#fnurvdvx5gwe\">[1]</a></sup></span>?]. We highlight the importance of including high-resolution simulations of high-range global warming in climate change impact modelling to make well-informed decisions about climate change mitigation, resilience and adaptation.</p></blockquote><h1>Factors overestimating starvation</h1><p>Holding land use constant<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcz26flp24g7\"><sup><a href=\"#fncz26flp24g7\">[2]</a></sup></span>:</p><blockquote><p>We sourced grid cell data on land use classifications for the base year (2020)\u2014noting that we hold land use constant in preparing this dataset as crop land growth is accounted for separately\u2014from <a href=\"https://www.sciencedirect.com/science/article/pii/S0016328723000770#bib19\">Chen et al. (2020)</a>.</p></blockquote><p>Not including all sources of food:</p><blockquote><p>This approach\u2014as adopted by other studies (e.g., <a href=\"https://www.sciencedirect.com/science/article/pii/S0016328723000770#bib40\">Hasegawa et al. (2021)</a>) for computational simplicity, and where crops constitute \u223c90 % of human food energy intake (<a href=\"https://www.sciencedirect.com/science/article/pii/S0016328723000770#bib124\">Su et al., 2017</a>)\u2014accounts for conventional crop-based agriculture but does not include aquaculture, animal-based agriculture (though feed crops are included), nor unconventional \u2018future foods\u2019 (<a href=\"https://www.sciencedirect.com/science/article/pii/S0016328723000770#bib95\">Parodi et al., 2018</a>), and thus likely underestimates total food produced by a given country.</p></blockquote><p>Assuming constant cropland area:</p><blockquote><p>Secondly, crop land growth does not evolve in response to climate-impacted crop yields, and thus adaptation to the impacts of climate change on food production through agricultural land expansion were likely underestimated.</p></blockquote><p>Not incorporating learning feedbacks nor exploring societal adaptation:</p><blockquote><p>For instance, while the food production input data does incorporate some conventional technological development for higher-crop yields in SSP5-RCP8.5, our simulation does not account for advances in unconventional \u2018future food\u2019 production systems, nor polycentric food networks, described by <a href=\"https://www.sciencedirect.com/science/article/pii/S0016328723000770#bib133\">Tzachor et al. (2021)</a>. As such, we likely overestimate the risk of starvation in this regard.</p></blockquote><h1>Factors underestimating starvation</h1><p>Only accounting for energy requirements:</p><blockquote><p>We used custom units of energy-equivalent-grain-weight-at-harvest to standardise food production and food demand quantities in simulation calculations. Although accounting for production of some 128 crops, this approach\u2014as adopted in other studies (e.g., <a href=\"https://www.sciencedirect.com/science/article/pii/S0016328723000770#bib84\">Molotoks et al., 2021</a>) for computational simplicity\u2014accounts only for energy but not for other nutritional contents. Dietary diversity\u2014i.e. not only energy from carbohydrates, proteins and fats but also non-energy aspects of these macro-nutrients, such as amino acids, essential fatty acids and fibres, as well as micro-nutrients, including vitamins and minerals such as iron, zinc and calcium\u2014is essential to food security, and thus our approach likely underestimates the potential for malnutrition.</p></blockquote><p>Accelerating both climate change and technological development instead of just climate change:</p><blockquote><p>Firstly, the raw data on crop yields in SSP5-RCP8.5 incorporates both climate change (positive and negative depending on region) and technological development effects (positive) and therefore could not be treated in isolation in this study. As such, both effects were accelerated in modelling the artificial \u2018runaway global warming\u2019 scenario, and thus the net impacts of climate change on food production were likely underestimated.</p></blockquote><p>Not incorporating socioeconomic growth shock feedbacks:</p><blockquote><p>For instance, while we account for population loss feedback in absolute terms, such a loss of population, and the potential turmoil that may follow a largescale starvation event, would likely have an impact of stunting the GDP growth of a nation compared to the business-as-usual growth adopted in the model, which would in turn feedback to further exacerbate the risk of starvation. As such, we likely underestimate the risk of starvation in this regard.</p></blockquote><h1>My quick overall take</h1><p>Most studies analysing the impact of <a href=\"https://forum.effectivealtruism.org/topics/climate-change\">climate change</a> focus on low levels of warming, so I am glad this article looked into more extreme scenarios, as asked in <a href=\"https://www.pnas.org/doi/10.1073/pnas.2108146119\">Kemp et al. 2022</a>.</p><p>On the other hand, 5 billion deaths given a global warming of 10 \u00baC (= (8 + 12)/2) by 2100 seems very pessimistic neglecting <a href=\"https://forum.effectivealtruism.org/topics/ai-risk\">AI risk</a>. <a href=\"https://www.nature.com/articles/s43016-022-00573-0\">Xia 2022</a> estimated <a href=\"https://www.nature.com/articles/s43016-022-00573-0/tables/1\">5.08 billion</a> people without food given a cropland cooling of <a href=\"https://www.nature.com/articles/s43016-022-00573-0/figures/1\">16 \u00baC in 1 year</a>, corresponding to a global cooling of 8 \u00baC<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6lfyf1lxipa\"><sup><a href=\"#fn6lfyf1lxipa\">[3]</a></sup></span>, and little adaptation<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcolkc527in\"><sup><a href=\"#fncolkc527in\">[4]</a></sup></span>. Moreover, I think 1 \u00baC of cooling is usually worse for yields than 1 \u00baC of warming. So it would be quite surprising if a temperature change 1.25 times (= 10/8) as large over 80 times as much time (2020 to 2100) were similarly bad.</p><p>I believe holding land use and cropland area constant, and not incorporating learning feedbacks nor exploring societal adaptation for such a long period of 80 years implies deaths have been majorly overestimated. I guess by more than a factor of 10. It is also unclear to me whether <a href=\"https://en.wikipedia.org/wiki/CO2_fertilization_effect\">CO</a><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"_2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char\"></span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span><a href=\"https://en.wikipedia.org/wiki/CO2_fertilization_effect\">&nbsp;fertilization</a> was taken into account, as I did not find \"fertilization\" in the article nor its <a href=\"https://www.sciencedirect.com/science/article/pii/S0016328723000770#sec0120\">supplementary material</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3hvdhwo059z\"><sup><a href=\"#fn3hvdhwo059z\">[5]</a></sup></span>.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnurvdvx5gwe\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefurvdvx5gwe\">^</a></strong></sup></span><div class=\"footnote-content\"><p>From section 5.1 (emphasis mine):</p><blockquote><p>Our results then show a stark divergence in risk of starvation as \u2018runaway global warming\u2019 results in mass mortality of \u223c<strong>5 billion</strong> people (i.e., \u223c68 % of the current global population) by 2100.</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fncz26flp24g7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefcz26flp24g7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Also from the <a href=\"https://www.sciencedirect.com/science/article/pii/S0016328723000770#sec0120\">supplementary material</a> (emphasis mine):</p><blockquote><p>The&nbsp;<i>food_production_growth_cropyield</i> input dataset contains a SSP5-RCP85 based growth profile for the crop yield (i.e. climate and technology) related component of food production growth, in units of annual average percentage, for each country in yearly increments from 2020 to 2099. It was derived from high resolution GIS data on climate impacted crop yields based on scenario SSP5-RCP85 from (Iizumi et al. 2017, 2020) and <strong>current land use</strong> from (Chen et al. 2020), as follows.</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6lfyf1lxipa\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6lfyf1lxipa\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See Fig. 3a of <a href=\"https://pubs.aip.org/aip/acp/article/1596/1/65/591013/Environmental-consequences-of-nuclear-war\">Toon 2014</a>. The cropland cooling is larger than the global cooling because this accounts for the air temperature over oceans, which does not change as much.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fncolkc527in\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefcolkc527in\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See 3rd paragraph of the <a href=\"https://www.nature.com/articles/s43016-022-00573-0#Sec5\">Discussion</a>, or <a href=\"https://www.lesswrong.com/posts/sJK6HN5vTPPnuuNgQ/that-one-apocalyptic-nuclear-famine-paper-is-bunk\">discussion</a> on LessWrong.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3hvdhwo059z\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3hvdhwo059z\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I have asked the corresponding author 14 days ago, but have not heard back.</p></div></li></ol>", "user": {"username": "vascoamaralgrilo"}}]