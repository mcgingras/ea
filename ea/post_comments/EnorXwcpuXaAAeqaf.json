[{"_id": "CvvsK9EYsJaeywDv3", "postedAt": "2024-03-26T17:55:29.371Z", "postId": "EnorXwcpuXaAAeqaf", "htmlBody": "<p>Related recent talk:</p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=4r9Y01H2sEU\"><div><iframe src=\"https://www.youtube.com/embed/4r9Y01H2sEU\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p>and <a href=\"https://petergodfreysmith.com/wp-content/uploads/2023/12/NYU-Oct-2023-Animals-AI-Functionalism-paper-Post-C3.pdf\">notes</a>.</p>", "parentCommentId": null, "user": {"username": "MichaelStJules"}}, {"_id": "dn8hQGPHSpNSddHFp", "postedAt": "2024-03-26T21:08:27.108Z", "postId": "EnorXwcpuXaAAeqaf", "htmlBody": "<p>(EDIT: Split this up into two comments, the other <a href=\"https://forum.effectivealtruism.org/posts/EnorXwcpuXaAAeqaf/how-to-resist-the-fading-qualia-argument-andreas-mogensen?commentId=5ha6HYzWYHxBef43w\">here</a>.)</p><p>I think that there's probably a minimum level of substrate independence we should accept, e.g. that it doesn't matter exactly what matter a \"brain\" is made out of, as long as the causal structure is similar enough on a fine enough level. The mere fact that neurons are largely made out of carbon doesn't seem essential. Furthermore, human and (apparently) conscious animal brains are noisy and vary substantially from one another, so exact duplication of the causal structure doesn't seem necessary, as long as the errors don't accumulate so much that the result isn't similar to a plausible state for a plausible conscious biological brain.<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"wzmmf8tjv1m\" role=\"doc-noteref\" id=\"fnrefwzmmf8tjv1m\"><sup><a href=\"#fnwzmmf8tjv1m\">[1]</a></sup></span>&nbsp;So, I'm inclined to say that we could replace biological neurons with artificial neurons and retain consciousness, at least in principle, but it could depend on the artificial neurons.</p><p>It's worth pointing out that the <a href=\"https://en.wikipedia.org/wiki/China_brain\">China brain</a><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"2\" data-footnote-id=\"7rzttlw0rnl\" role=\"doc-noteref\" id=\"fnref7rzttlw0rnl\"><sup><a href=\"#fn7rzttlw0rnl\">[2]</a></sup></span>&nbsp;and a digital mind (or digital simulation of a mind, on computers like today's) aren't really causally isomorphic to biological brains even if you ignore a lot of the details of biological brains. Obviously, you <i>also</i> have to ignore a lot of the details of the China brain and digital minds. But I could imagine that the extra details in the China brain and digital minds make a difference.</p><ol><li>In a simulated neuron, both in the China brain and digital minds, there are details to ignore. In the China brain, that's all the stuff happening <i>inside</i> each person simulating a neuron. For a digital mind, there's probably lots of extra hardware stuff going on.</li><li>In a digital mind on a computer like today's computers or even distributed across hundreds of computers or processing units (CPU cores, GPUs), it seems you must ignore the fact that the digital state transitions are orchestrated centrally \"from the outside\", today through some kind of loop (e.g. a for-loop or while-loop, or some number of these with some asynchronous distribution). Individual biological neurons act relatively autonomously/asynchronously just in response to local neural activity (including electrical and chemical signals), without this kind of external central orchestration. Actually, if you were to ignore the centralized orchestration in a digital mind, depending on how you cash that out, the digital mind might never change states, so maybe the digital mind isn't actually isomorphic to a biological brain at the right level(s) of causal structure for each at all.</li></ol><p>These extra details make me less sure that we should attribute consciousness to the China brain and digital minds, but they don\u2019t seem decisive.</p><ol class=\"footnote-section footnotes\" data-footnote-section=\"\" role=\"doc-endnotes\"><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"1\" data-footnote-id=\"wzmmf8tjv1m\" role=\"doc-endnote\" id=\"fnwzmmf8tjv1m\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"wzmmf8tjv1m\"><sup><strong><a href=\"#fnrefwzmmf8tjv1m\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>From footnote 4 from <a href=\"https://petergodfreysmith.com/wp-content/uploads/2023/12/NYU-Oct-2023-Animals-AI-Functionalism-paper-Post-C3.pdf\">Godfrey-Smith, 2023</a> (based on <a href=\"https://www.youtube.com/watch?v=4r9Y01H2sEU\">the talk he gave</a>):</p><blockquote><p>At the NYU talk, Chalmers raised a passage from The Conscious Mind (p. 331) where he claims, in relation to replacement scenarios, that \"when it comes to duplicating our cognitive capacities, a close approximation is as good as the real thing.\" His argument is that in biological systems, random \"noise\" processes play a role (greater than the role of any analogous processes in a computer). When the biological system performs some operation, the outcome is never entirely reliable and will instead fall within a band of possibilities. An artificial duplicate of the biological system only has to give a result somewhere in that band. The duplicate's output might depart from what the biological system actually does, on some occasion, but the biological system could just as well have produced the same output as the duplicate, if noise had played a different role. When a duplicate gives a result within the band, it is doing \"as well as the system itself can reliably do.\"</p><p>In response, it is true that this role for noise is an important micro-functional feature of living systems. In addition, neurons change what they do as a result of their normal operation, they don't respond to the \"same\" stimulus twice in the same way (see \"Mind, Matter, and Metabolism\" for references). The \"rules\" or the \"program\" being followed are always changing as a result of the activity of the system itself and its embedding in other biological processes. Over time, the effects of these factors will accumulate and compound \u2013 a comparison of what a living system and a duplicate might do in a single operation doesn't capture their importance. I see all this not as a \"lowering of the bar\" that enables us to keep talking in a rough way about functional identity, but another functional difference between living and artificial systems.</p></blockquote></div></li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"2\" data-footnote-id=\"7rzttlw0rnl\" role=\"doc-endnote\" id=\"fn7rzttlw0rnl\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"7rzttlw0rnl\"><sup><strong><a href=\"#fnref7rzttlw0rnl\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>From the <a href=\"https://en.wikipedia.org/wiki/China_brain\">Wikipedia page</a>:</p><blockquote><p>the <strong>China brain</strong> <a href=\"https://en.wikipedia.org/wiki/Thought_experiment\">thought experiment</a> (also known as the <strong>Chinese Nation</strong> or <strong>Chinese Gym</strong>) considers what would happen if each member of the <a href=\"https://en.wikipedia.org/wiki/China\">Chinese nation</a> were asked to simulate the action of one <a href=\"https://en.wikipedia.org/wiki/Neuron\">neuron</a> in the brain, using telephones or <a href=\"https://en.wikipedia.org/wiki/Walkie-talkie\">walkie-talkies</a> to simulate the <a href=\"https://en.wikipedia.org/wiki/Axons\">axons</a> and <a href=\"https://en.wikipedia.org/wiki/Dendrites\">dendrites</a> that connect neurons. Would this arrangement have a <a href=\"https://en.wikipedia.org/wiki/Mind\">mind</a> or <a href=\"https://en.wikipedia.org/wiki/Consciousness\">consciousness</a> in the same way that brains do?</p></blockquote><p>(China's population, at 1.4 billion, isn't large enough for each person to only simulate one neuron and so simulate a whole human brain with &gt;80 billion neurons, but we could imagine a larger population, or a smaller animal brain being simulated, e.g. <a href=\"https://en.wikipedia.org/wiki/List_of_animals_by_number_of_neurons\">various mammals or birds</a>.)</p></div></li></ol>", "parentCommentId": null, "user": {"username": "MichaelStJules"}}, {"_id": "5ha6HYzWYHxBef43w", "postedAt": "2024-03-26T23:07:54.114Z", "postId": "EnorXwcpuXaAAeqaf", "htmlBody": "<p>Some other arguments that push in favour of functionalism, the consciousness of simulated brains, including <a href=\"https://en.wikipedia.org/wiki/China_brain\">the China brain</a> and digital minds, and brains with other artificial neurons:</p><ol><li>We may ourselves be simulated in a similar way without knowing it, if our entire reality is also simulated. We wouldn't necessarily have access to what the simulation is run on.</li><li>In a simulated brain and the conscious biological brain it simulates, introspection would give the brains same beliefs about phenomenal properties and qualia, because it's only sensitive to the causal/functional structure at a given level of detail, and those details are by design/assumption preserved under simulation. If the biological brain is phenomenally conscious, but the simulated brain is not, then it's a surprising coincidence that the resulting beliefs about phenomenal consciousness are accurate in the biological brain but not in the simulated brain. Introspection doesn't seem to give the biological brain any more reason to believe in its own phenomenal consciousness than it should for the simulated brain in its own, because introspection is only sensitive to causal/functional details common to both.<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"1h5vkwtwyqkh\" role=\"doc-noteref\" id=\"fnref1h5vkwtwyqkh\"><sup><a href=\"#fn1h5vkwtwyqkh\">[1]</a></sup></span></li><li>It's hard for me to imagine a compelling explanation of our consciousness that doesn't extend to simulated brains, including the China brain and digital minds. Theories out there now don't seem on track to address the hard problem, and this and other reasons (like above) incline me to <i>dissolve</i> it and accept illusionism about <i>phenomenal</i> properties/consciousness. Illusionism is generally functionalist, and I don't see how an illusionist theory would deny the consciousness of the China brain and digital simulations of brains.</li></ol><ol class=\"footnote-section footnotes\" data-footnote-section=\"\" role=\"doc-endnotes\"><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"1\" data-footnote-id=\"1h5vkwtwyqkh\" role=\"doc-endnote\" id=\"fn1h5vkwtwyqkh\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"1h5vkwtwyqkh\"><sup><strong><a href=\"#fnref1h5vkwtwyqkh\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>This is essentially the coincidence argument for illusionism in <a href=\"https://philpapers.org/archive/CHATMO-32.pdf\">Chalmers, 2018</a>.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "MichaelStJules"}}, {"_id": "6iRqZNvhMnx258hvu", "postedAt": "2024-03-27T05:15:22.744Z", "postId": "EnorXwcpuXaAAeqaf", "htmlBody": "<p>Is there are online version of the case <i>for</i> the fading qualia argument? This feels a bit abstract without it...</p>", "parentCommentId": null, "user": {"username": "Arepo"}}, {"_id": "onHJeYGX7dKGLyY8F", "postedAt": "2024-03-27T05:31:53.495Z", "postId": "EnorXwcpuXaAAeqaf", "htmlBody": "<blockquote><ol><li>We may ourselves be simulated in a similar way without knowing it, if our entire reality is also simulated. We wouldn't necessarily have access to what the simulation is run on.</li></ol></blockquote><p>It seems weird to meaningfully update in favour of some concrete view on the basis that something <i>might</i> be true but that</p><ol><li>we have no evidence for it, and&nbsp;</li><li>if it is true then everything we know about the universe is equally undermined</li></ol>", "parentCommentId": "5ha6HYzWYHxBef43w", "user": {"username": "Arepo"}}, {"_id": "8ABghiHyq5CPnsasW", "postedAt": "2024-03-27T09:35:34.413Z", "postId": "EnorXwcpuXaAAeqaf", "htmlBody": "<p>The best argument for functionalism* in my opinion is that there aren't really any good alternatives. If mental state kinds aren't <i>functional</i> kinds, they'd presumably have to be neuroscientific kinds. &nbsp;But if &nbsp;that's right, then we can already know now aliens without neurons aren't conscious. Which seems wild to me: how can we possibly know if aliens are conscious till we meet them, and observe their behavior and how it depends on what goes on inside them? And surely once we do meet them, no one is going to say \"oh consciousness is this sort of neurobiological property, we looked in their head with our scanner and they have no neurons, problem solved, we know they aren't conscious. People seem to want there to be some intermediate view that says \"oh of course there might be conscious aliens with different biology, we just mean to rule out <i>weird</i> functional duplicates of humans like a robot controlled by radio signals running the same program as human**\", but it's really unclear how to do that in a principled way. (And I suspect the root of the desire to do so is a sort of primitive sense that <i>living</i> matter can have feelings but dead matter can't, which I think people would consciously disavow if they understood it was driving their views)<br><br>*There's an incredibly technical complication here about the fact that \"functionalism\" is usually defined in opposition to mind-body dualism, but in the current context it makes more sense to classify certain forms of dualism as functionalist, since they agree with functionalism about what guarantees something is conscious in the actual world. But I'm going to ignore it because I don't think I can explain it to non-philosophers quickly and easily.&nbsp;<br><br>**https://en.wikipedia.org/wiki/China_brain</p>", "parentCommentId": "6iRqZNvhMnx258hvu", "user": {"username": "Dr. David Mathers"}}, {"_id": "Cpe9qBcFfkQphhvpi", "postedAt": "2024-03-27T13:05:51.995Z", "postId": "EnorXwcpuXaAAeqaf", "htmlBody": "<p>I agree there is something a bit weird about it, but I'm not sure I endorse that reaction. This doesn\u2019t seem so different from p-zombies, and probably some moral thought experiments.</p>\n<p>I don't think it's true that everything we know about the universe would be equally undermined. Most things wouldn't be undermined at all or at worst would need to be slightly reinterpreted. Our understanding of physics in our universe could still be about as reliable (depending on the simulation), and so would anything that follows from it. There's just more stuff outside our universe.</p>\n<p>I guess you can imagine short simulations where all our understanding of physics is actually just implanted memories and fabricated records. But in doing so, you're throwing away too much of the causal structure that apparently explains our beliefs and makes them reliable. Longer simulations can preserve that causal structure.</p>\n", "parentCommentId": "onHJeYGX7dKGLyY8F", "user": {"username": "MichaelStJules"}}, {"_id": "vgcCwEmA26AJ8s6ae", "postedAt": "2024-03-28T02:32:03.686Z", "postId": "EnorXwcpuXaAAeqaf", "htmlBody": "<blockquote><p>This doesn\u2019t seem so different from p-zombies, and probably some moral thought experiments.</p></blockquote><p>I'm not sure what you mean here. That the simulation argument doesn't seem different from those? Or that the argument that 'we have no evidence of their existence and therefore shouldn't update on speculation about them' is comparable to what I'm saying about the simulation hypothesis?&nbsp;</p><p>If the latter, fwiw, I feel the same way about p-zombies and (other) thought experiments. They are a terrible methodology for reasoning about anything, very occasionally the <i>only</i> option we can think of, but one from which philosophers don't feel nearly enough urgency to find alternatives to which to move.</p><blockquote><p>Our understanding of physics in our universe could still be about as reliable (depending on the simulation), and so would anything that follows from it. There's just more stuff outside our universe.</p></blockquote><p>I don't see how this would allow us to update on anything based on speculation about the 'more stuff'. Yeah, we might choose to presume our pocket simulation will continue to behave as it has, but we don't get to then say 'there's some class of matter other than our own simulated matter which generates consciousness therefore consciousnessness is substrate independence.</p><p>As you say in your other comment, there's probably some minimal level of substrate independence that non-solipsists have to accept, but that turns it into an empirical question (as it should be) - so an imagined metaverse gives us no reason to change our view on <i>how</i> substrate independent consciousness is.</p><blockquote><p>in doing so, you're throwing away too much of the causal structure that apparently explains our beliefs and makes them reliable</p></blockquote><p>This seems like an <a href=\"https://www.smbc-comics.com/comic/2012-07-15\">argument from sadness</a>. What we would lose by imagining some outcomes shouldn't affect our overall epistemics.</p>", "parentCommentId": "Cpe9qBcFfkQphhvpi", "user": {"username": "Arepo"}}]