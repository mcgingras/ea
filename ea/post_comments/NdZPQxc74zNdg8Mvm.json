[{"_id": "uZcQeEafLBiC9zTme", "postedAt": "2023-01-13T20:12:38.034Z", "postId": "NdZPQxc74zNdg8Mvm", "htmlBody": "<p>Cowen thinks there are limits to EA\u2019s idea that we should be completely impartial in our decisions (that we should weigh all human lives as being equal in value when we make decisions, to the point where we only care about how many lives we can impact and not where in the world those lives are). He cites a thought experiment where aliens come to Earth and want to enslave humankind for their benefit. We don\u2019t calculate whether more net happiness is generated if the aliens get what they want: the vast majority of people would always choose to fight alongside their fellow humans (thus being partial).</p><p>Cowen then claims that some degree of partiality is an inescapable part of human psychology, so we ought not to strive to be completely impartial. Not only does this run into Hume\u2019s is-ought problem, as he\u2019s using (what he believes to be) an empirical fact to derive an ought, but this doesn\u2019t get to the core reason of&nbsp;<i>why&nbsp; </i>we ought to be partial in some situations. This matters because having a core principle would more clearly define what limits to our impartiality should be.&nbsp;</p><p>For example, I think the notion of personal and collective responsibility is extremely important here for setting clear limits: I am partial to, say, my family over strangers because I have relationships with them that make me accountable to them over strangers. Governments need to be partial to the citizens of their country over the citizens of other countries because they are funded through taxes and voted in by citizens.&nbsp;</p><p>Humans should fight on the side of humans in the war against aliens for two reasons: the first is that every human being is in a relationship with herself, making her responsible for not letting herself be enslaved. Secondly, one can include the idea of moral responsibility under the umbrella of personal and collective responsibility: even if only some humans are enslaved and there isn\u2019t a personal benefit for most people to fight on the side of those humans, slavery is immoral, so we ought to fight for the rights and dignity of those people if there is something we can do about it. If a specific subset of humans engaged a whole race of aliens in battle (both sides were voluntarily engaged in the battle), and the winner didn\u2019t enslave the loser, it would actually be wise to pick the side that would lead to the most net happiness, as mere tribalism is not a valid reason to be partial.&nbsp;</p>", "parentCommentId": null, "user": {"username": "andrew_goldbaum"}}, {"_id": "k6fxKk8s9yt7Dm7ki", "postedAt": "2023-01-14T06:48:04.748Z", "postId": "NdZPQxc74zNdg8Mvm", "htmlBody": "<p>I don't get his point about social conservatism. Does he mean that a mass appeal socially conservative EA will be more EA somehow than a mass appeal socially liberal EA? Or that EAs should recruit social conservatives to influence more segments of the population?&nbsp;</p>", "parentCommentId": null, "user": {"username": "SantaRedux"}}, {"_id": "7G7Ds6B4vMFEX4ARd", "postedAt": "2023-01-14T18:46:19.987Z", "postId": "NdZPQxc74zNdg8Mvm", "htmlBody": "<p>My understanding is that he basically thinks norms associated with social conservatives, in particular Mormons -- he lists \"savings, mutual assistance, family values and no drug and alcohol abuse\" in <a href=\"https://www.nytimes.com/2016/04/17/upshot/why-theres-hope-for-the-middle-class-with-help-from-china.html\">this NYT piece</a> &nbsp;-- just make people better off. He's especially big on the teetotaling thing; he thinks alcohol abuse is a major social problem we don't do enough to address. I don't exactly know if he thinks it's more important for EA's to adopt conservative norms to improve their own welfare/productivity, or if EA's need to see the value of conservative norms for other people generally and start promoting them.</p><p>I don't think he's thinking of it as giving EA more mass appeal.</p>", "parentCommentId": "k6fxKk8s9yt7Dm7ki", "user": null}, {"_id": "rbG4H3bxH7qNKSjFA", "postedAt": "2023-01-16T16:44:13.551Z", "postId": "NdZPQxc74zNdg8Mvm", "htmlBody": "<p>In addition to the other comment, I think he\u2019s also indirectly pointing to the demographic trends (I.e. fertility rates) of social conservatives. Social conservatives have more kids, so they inherit the future. If EA is anti natalist and socially liberal, we will lose out in the long run.</p>\n", "parentCommentId": "k6fxKk8s9yt7Dm7ki", "user": {"username": "Wil Perkins"}}, {"_id": "xjbfKe7QxgdwcvW3B", "postedAt": "2023-01-16T16:46:26.723Z", "postId": "NdZPQxc74zNdg8Mvm", "htmlBody": "<p>He fails to bring up the tension with short AI timelines which I think is important here. Lots of AI safety folks I\u2019ve talked to argue that long term concerns about movement building, fertility trends, etc aren\u2019t important because of AGI happening soon.</p>\n<p>I think this tension underlies a lot of discussions in the community.</p>\n", "parentCommentId": null, "user": {"username": "Wil Perkins"}}, {"_id": "xbpRHPuK7zbztBAQm", "postedAt": "2023-01-16T16:59:30.934Z", "postId": "NdZPQxc74zNdg8Mvm", "htmlBody": "<p>I listened to the interview yesterday. My take on what he said on this was rather that EA's core principles don't have to necessarily restrict it to what is its de facto socially liberal, coastal, Democratic party demographic, and that socially conservative people could perfectly buy into them, if they aren't packaged as 'this lefty thing'.</p>", "parentCommentId": "k6fxKk8s9yt7Dm7ki", "user": {"username": "Manuel Del R\u00edo Rodr\u00edguez"}}, {"_id": "SmsxPahGPfgdC4PFu", "postedAt": "2023-01-16T17:00:50.947Z", "postId": "NdZPQxc74zNdg8Mvm", "htmlBody": "<p>(Not sure if the above is a view stated above is your own, or one that you attribut to Tyler - in any case just adding this as a counterargument :) for anyone reading this thread and finding this topic interesting)</p><p>This is only true if you think that political values are set at an early age and remain stable throughout life - or if you commit to unrealistic <i>ceteris paribus</i> assumptions about the future political landscape. Furthermore, there is also evidence that these beliefs can change, for example <a href=\"https://www.tandfonline.com/doi/abs/10.1080/03054985.2016.1151408\">exposure to education is linked to liberalisation of social attitudes</a>. So, even if social conservatives have more kids, an 'anti-natlist and socially liberal' EA could still inherit the future as long as it manages to persuade people to support it.</p><p>In general, this sounds like a <a href=\"https://www.theatlantic.com/politics/archive/2012/11/the-emerging-democratic-majority-turns-10/265005/\">'Demographics are Destiny' </a>idea, which might be intuitively plausible but to me comes off as quite <a href=\"https://www.lesswrong.com/tag/cognitive-style#Foxes_and_Hedgehogs\">'hedgehog-y'</a>. You could always find a reason why an emerging majority hasn't arrived in a specific election and will emerge at the next one, and then the next one, and so on. I think one can maybe make broad assessments of political future given demographic trends but, as always, prediction is hard - especially about the future.</p>", "parentCommentId": "rbG4H3bxH7qNKSjFA", "user": {"username": "JWS"}}, {"_id": "qX6gAyLvQteGpz2sq", "postedAt": "2023-01-16T17:12:11.939Z", "postId": "NdZPQxc74zNdg8Mvm", "htmlBody": "<p>I find myself agreeing with quite a lot of what he says in this video as, on a personal level, the greatest difficulty I find when trying to wind my mind around the values and principles &nbsp;of EA (as opposed to effective altruism, in small caps) is the axiom of impartiality, and its extension to a degree to animals. Like, in some aspects, it is trivially obvious that all humans (and by extension, this applies to any creatures with sufficient reason and moral conscience) should be the possessors of an equal set of rights, but if you try to push it into moral-ethical obligations towards them, I can't quite understand why we are supposed not to make distinctions, like valuing more those that are closest to us (our community) and those we consider wise, good, etc... That would not preclude the possession, at the same time, of a more generalist and abstract empathy for all sentient beings, and a feeling of a degree of moral obligation to help them (even if to a lesser degree than those closest to one).</p>", "parentCommentId": null, "user": {"username": "Manuel Del R\u00edo Rodr\u00edguez"}}, {"_id": "Q9Lg4Ne6aahcQct2A", "postedAt": "2023-01-16T17:23:25.127Z", "postId": "NdZPQxc74zNdg8Mvm", "htmlBody": "<p>Cowen would benefit by understanding that EA has to have a duality, encompassing both philosophical aspects and practical aspects. &nbsp;The goal of EA is to have a practical effect on the world, but it's effect will be limited if it's not persuasive. &nbsp;The persuasive aspect requires it to describe it's philosophical underpinnings. &nbsp;It cannot rely solely upon a pre-existing partial point of view, because it's a departure from pre-existing partial points of view. &nbsp;You might call it a refinement of reciprocal altruism which would be a widely shared pre-existing partial point of view that is not individualistic, that escapes some of our other biases that limit the scope to which that point of view is applied.</p><p>That said, since EA does not try to force or coerce, it is left with persuasiveness. Persuasiveness must be rooted in something, and philosophy is where all such roots begin for individuals &nbsp;if they are not natural or imposed by force. &nbsp;To disambiguate the preceding, it's not formal philosophy I refer to, but the broader scope philosophy which every individual engages with and is what formal philosophy attempts to describe and discuss.</p><p>The problem with the way Cowen engages with trolley problems and repugnant conclusions is that these are formal questions, out of formal philosophy, which are then forced into conflict with the practical world. &nbsp;The difference between someone who flips the trolley lever or accepts infinite double or nothing 51% bets, and someone who doesn't, is whether they answer this as a formal question, or do not.</p><p>As a formal question, everything practical is removed. &nbsp;That is not possible in a practical environment. Hubris, is necessary to accepting 51% double of nothing bets. &nbsp;Hubris, is what would have prevented FTX's path, not a different answer to a formal philosophical question.</p><p>You can actually feel Cowen is drawn toward this answer, but continue to reject it. &nbsp;He doesn't want EA to change, because he recognizes the value it provides. &nbsp;At the same time, he's saying it's wrong and should be more socially conservative.</p><p>I'd disagree with the specifically socially conservative aspect, but agree with the conservative aspect. In this case I mean conservative in it's most original form.. cautious about change. &nbsp;That said, I'm not convinced that the EA movement overall is not conservative, and I would not agree that \"social conservatives\" are all that conservative. &nbsp;The typical \"social conservative\" is willing to make grand unsubstantiated statements, advocate for some truly repressive actions to enforce maintaining a prior social order (or reverting to one long since abandoned).</p><p>Being socially conservative does not make you conservative. &nbsp;This narrow form of conservatism, can put you in conflict with other aspects of conservatism, and I'd argue it has put today's social conservatives at great odds with it. &nbsp;In addition, there are large groups of people we allow to use the social conservative label who are regressive. &nbsp;Regressivity is not conservative, as it's no longer attempting to maintain the status quo. &nbsp;An attempt to regress will without a doubt have unintended consequences. &nbsp;It's the existence of unintended consequences and the hubris to accept that you cannot see all of them that is the only value proposition to conservatism, so once this is abandoned there's no value left and we really shouldn't accept the use of conservative to describe such groups.</p><p>So, in short, EA should continue to engage on it's formal side, but should also continue to embrace practical principles during the translation of formal ideas to practical reality. &nbsp;If an alien does come to us and offer endless 51% double or nothing bets, we should be skeptical of the honesty, ability, and all the margins that go with that around accepting that bet. &nbsp;In like ways, when operating a financial company, someone should accept the possibility of downside, the inability to forecast the future, the reality of margin calls, the sometimes inscrutable wisdom of regulation, and the downsides of disregarding rules, even when it seems (to us) like we know better.</p><p>Forcing EA to divorce it's practical and philosophical sides and then attacking the philosophical side using practical arguments is either dishonest, or a failure to understand EA. &nbsp;Accepting \"partial point of view\" as an absolute, provides no room for EA to argue for any change. &nbsp;Conservatism may have a place, but it would be internally inconsistent if it ever was all encompassing, because the one thing that has always been constant, is change. &nbsp;Conservatism that rejects change, rather than simply applying some caution to it, posits an impossibility, change cannot be halted, only managed.</p>", "parentCommentId": null, "user": {"username": "Ryan Baker"}}, {"_id": "8ZwPAdc48w9x7nP9D", "postedAt": "2023-01-16T17:46:39.726Z", "postId": "NdZPQxc74zNdg8Mvm", "htmlBody": "<p>I don't endorse that view myself, but yeah pointing out that I think Tyler believes it.</p>", "parentCommentId": "SmsxPahGPfgdC4PFu", "user": {"username": "Wil Perkins"}}, {"_id": "mEGaLdornknHn7Ykx", "postedAt": "2023-01-16T23:30:08.497Z", "postId": "NdZPQxc74zNdg8Mvm", "htmlBody": "<p>I think this is a reasonable response, but Cowen did anticipate the \"slavery is immoral\" response, and is right that this wouldn't be a utilitarian response. &nbsp;You can fix that since there is an easily drawn line from utilitarianism to this response, but I think Cowen would respond that in this scenario we both wouldn't and shouldn't bother to do such fine reasoning and just accept our partialities. &nbsp;He does make a similar statement during the Q&amp;A.</p><p>I'd contend that this an example of mixing practical considerations with philosophical considerations. Of course we wouldn't stop during an invasion of little green men who are killing and enslaving humans and wonder.. \"would it be better for them to win?\" &nbsp;If you did stop to wonder, there might be many good reasons to say no, but if you're asking a question of whether you'd stop and ask a question, it's not a philosophical question anymore, or at least not a thought experiment. &nbsp;Timing is practical not theoretical.&nbsp;</p><p>If it was really all about partialities, and not practical, it wouldn't matter what side we were on. If we showed up on another planet, and could enslave/exterminate a bunch of little green men, should we stop to think about it before we did? &nbsp;Of course we should. &nbsp;And while maybe you can concoct a scenario in which it's kill or be killed, there would be little question about the necessity to be certain that it wasn't an option to simply turn around and go the other way.&nbsp;</p>", "parentCommentId": "uZcQeEafLBiC9zTme", "user": {"username": "Ryan Baker"}}]