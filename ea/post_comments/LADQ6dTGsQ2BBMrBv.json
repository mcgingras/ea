[{"_id": "Rs7PhWJDxdKpkP2bK", "postedAt": "2022-09-04T21:36:14.660Z", "postId": "LADQ6dTGsQ2BBMrBv", "htmlBody": "<p>I enjoyed skimming your post, and appreciate many of your points.</p>\n<p>One concept particularly struck me, \"It\u2019s worth noting that if longer human durations are unlikely, this also means larger human population sizes per century are also vanishingly unlikely. \"</p>\n<p>I often hear the classic argument that \"there is a possibility human populations are really really big in the future, and the future is so long that their wellbeing matters really quite a lot.\" I've never played around with the idea that the is a lot of doubt over large populations for long times. Which must be accounted for and would lessen their importance for me taking action now.</p>\n<p>Arguing \"this is all the more reason to work harder to make their lives come to pass\" strikes me as slightly dishonest, since the opposite is equally true: maybe we do our best make that giant future possible, and for some unforeseen reason people still don't (want to?) proliferate.</p>\n<p>Thank you for provoking some reflection on my assumptions, and thanks for such a comprehensive post with accessible bolded topic sentences.</p>\n", "parentCommentId": null, "user": {"username": "EcologyInterventions"}}, {"_id": "FKxH89fEssPuuKxsc", "postedAt": "2022-09-07T11:19:05.697Z", "postId": "LADQ6dTGsQ2BBMrBv", "htmlBody": "<p>Thank you for writing this post. I agree with many of your arguments and criticisms like yours deserve to get more attention. Nevertheless, I still call myself a longtermist; mainly for the following reasons:</p><ul><li>There exist longtermist interventions that are good with respect to a broad range of ethical theories and views about the far future, e.g. searching the waste water for unknown pathogens.</li><li>Sometimes it is possible to gather further evidence for counter-intuitive claims. For example, you could experiment with existing large language models and search for signs of misaligned behaviour.</li><li>There may exist unknown longtermist interventions that satisfy all of our criteria. Therefore, &nbsp;a certain amount of speculative thinking is OK as long as you keep in mind that most speculative theories will &nbsp;die. &nbsp;&nbsp;</li></ul><p>All in all, you should keep the balance between too conservative and too speculative thinking.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Frank_R"}}, {"_id": "nfcHffPB9jQrkYvqC", "postedAt": "2022-09-08T03:14:52.860Z", "postId": "LADQ6dTGsQ2BBMrBv", "htmlBody": "<p>Thanks for the thoughts. I basically agree with you. I'd consider myself a \"longtermist,\" too, for similar reasons. I mainly want to reject the comparatively extreme implications of \"strong longtermism\" as defended by <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf\">Greaves and MacAskill</a> that extremely speculative and epistemically fragile longtermist interventions are more cost effective than even the most robust and impactful near termist ones.&nbsp;</p><p>I think there's likely a lot of &nbsp;steps we could and should be taking that could quite reasonably be expected to reduce real and pressing risks. &nbsp; &nbsp;</p><p>I would add to your last bullet, though, that speculative theories will only die if there's some way to falsify &nbsp;them or at least seriously call them into question. Strong longtermism is particularly worrying because it is an unfalsifiable theory. For one thing, too much weight is placed on one fundamentally untestable contention: the size and goodness of the far future. Also, it's basically impossible to actually test whether speculative interventions intended to very slightly reduce existential risk actually are successful (how could we possibly tell if risk was actually reduced by 0.00001%? or increased by 0.00000001%?). As a result, it could survive forever, no matter how poor a job it's doing.&nbsp;</p><p>Longtermist interventions (even speculative ones) supported by \"cluster thinking\" styles that put more weight on more testable assumptions (e.g. about the neglectedness &nbsp;or tractability of some particular issue, about the effect an intervention could have on certain \"signposts\" like international coordination, rate of near misses, etc.) or are intended to lead to more significant reductions in existential risk (which could be somewhat easier to measure than very small ones) are likely easier to reject if they prove ineffective.&nbsp;</p>", "parentCommentId": "FKxH89fEssPuuKxsc", "user": {"username": "A. Wolff"}}, {"_id": "oDnAWRwToaMKFPks6", "postedAt": "2022-09-13T09:14:33.188Z", "postId": "LADQ6dTGsQ2BBMrBv", "htmlBody": "<blockquote><p><i><strong>This is in essence the claim of the epistemic critique of strong longtermism. </strong></i>Notice that this way of framing the epistemic concern does not involve rejecting the Bayesian mindset or the usefulness of expected value theory. Instead, <i><strong>it involves recognizing that to maximize expected value, we might in some cases want to not rely on expected value calculations.</strong></i></p></blockquote><p>&nbsp;</p><p>Hmm. I get what you mean. To make the best decision that I can, I might not use expected value calculations to compare alternative actions when for example there's no feedback on my action or the probabilities are very low and so hard to guess because of my own cognitive limitations.</p><p>An outside view applies heuristics &nbsp;(for example, the heuristic \"don't do EV when a subjective probability is below 0.0001\") to my decision of whether use EV calculations, but it doesn't calculate EV. I would consider it a belief.&nbsp;</p><p><strong>Belief:</strong> \"Subjective probabilities below 0.0001% attached to values in an EV calculation are subject to scope insensitivity and their associated EV calculations contain errors.\"</p><p><strong>Rule:</strong> \"If an EV calculation and comparison effort relies on a probability below 0.0001%, then cease the whole effort.\"</p><p><strong>Bayesian</strong>: \"As an EV calculation subjective probability drops below 0.0001%, the probability that the EV calculation probability is actually unknown increases past 80%.\"</p><p>I can see this issue of the EV calculation subjective probability being so small as similar to a visual distinction between tiny and very tiny. You might be able to see something, but it's too small to tell if it's 1/2 the size of something bigger, or 1/10. All you know is that you can barely see it and anything 10X bigger than it.</p><p>The real question for me is whether the Bayesian formulation is meaningful. &nbsp;Is there another formulation that a Bayesian could make that is better suited, involving priors, different assertions, probabilities, etc?</p><p>I tried imagining how this might go if it were me answering the question.</p><p>Me:</p><blockquote><p>Well, when I think someone's wrong, I pull out a subjective probability that lets me communicate that. I like 80% because it makes people think of the 80/20 rule and then they think I'm really smart and believe what I'm telling them. I could list a higher probability, but they would actually quibble with me about it, and I don't want that. &nbsp;Also, at that percentage, I'm not fully disagreeing with them. They like that.</p><p>I say stuff like, \"I estimate an 80% probability that you're wrong.\"when I think they're wrong.&nbsp;</p><p>And here's the problem with the \"put some money behind that probability\" thing. I really think they're wrong, but I also know that this is a situation in which verifying the truth to both side's satisfaction is tricky, and because the verification is over money, there's all kinds of distortion that's likely to occur if money enters into it. It might actually be impossible to verify the truth. Me and the other side both know that.&nbsp;</p><p>That's the perfect time to use a probability and really make it sound carefully considered, like I really believe it's 80%, and not 98%.&nbsp;</p><p>It's like knowing when to say \"dibs\" or \"jinx\". You have got to understand context.</p></blockquote><p>I'm joking. I don't deceive like that. However, how do you qualify a Bayesian estimate as legitimate or not, in general?</p><p>You have gone partway here, rejecting EV calculations in some circumstances. You have also said that you still believe in probability estimates and expected value theory, and are instead just careful about when to use them.&nbsp;</p><p>So do you or can you use expected value calculations or subjective probabilities to decide when to use either?</p>", "parentCommentId": null, "user": {"username": "Noah Scales"}}]