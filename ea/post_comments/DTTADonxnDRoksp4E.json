[{"_id": "Go7X5CBgBymF3hAw3", "postedAt": "2022-10-17T20:28:42.161Z", "postId": "DTTADonxnDRoksp4E", "htmlBody": "<p>The application form is showing up as private for me. Very cool idea though, the success of Eleuther and Stability suggests that this is a viable model. Excited to see it unfold and hopefully contribute!</p>", "parentCommentId": null, "user": {"username": "Aidan O'Gara"}}, {"_id": "pdiMqEHFbQk8pemMJ", "postedAt": "2022-10-17T21:06:27.409Z", "postId": "DTTADonxnDRoksp4E", "htmlBody": "<p>Thank you! It should be fixed now.</p>", "parentCommentId": "Go7X5CBgBymF3hAw3", "user": {"username": "esben-kran"}}, {"_id": "RKqLqSWXr7qR7QC3H", "postedAt": "2022-10-18T13:37:08.541Z", "postId": "DTTADonxnDRoksp4E", "htmlBody": "<p>Small note: the title made me think the platform is made by the organization Open AI</p>", "parentCommentId": null, "user": {"username": "Leksu"}}, {"_id": "SW8gcsgp47byc5gt8", "postedAt": "2022-10-18T13:39:16.204Z", "postId": "DTTADonxnDRoksp4E", "htmlBody": "<p>Yeah, I thought this too.&nbsp;</p>", "parentCommentId": "RKqLqSWXr7qR7QC3H", "user": {"username": "AryanYadav"}}, {"_id": "sotpb8yBM8iKuNnFF", "postedAt": "2022-10-18T15:00:15.596Z", "postId": "DTTADonxnDRoksp4E", "htmlBody": "<p>Same. I suggest \"AI Safety Ideas: a collaborative AI safety research platform\"</p>", "parentCommentId": "RKqLqSWXr7qR7QC3H", "user": {"username": "FlorentBerthet"}}, {"_id": "eWaaKHfcEKx45avmL", "postedAt": "2022-10-18T18:54:12.303Z", "postId": "DTTADonxnDRoksp4E", "htmlBody": "<p>This looks like an interesting platform for sharing ideas about AI safety.</p><p>You mention that AI safety is a 'pre-paradigmatic field' -- however, to a newcomer like me, the safety ideas and projects on the AI Safety Ideas site so far look pretty 'paradigmatic', in the sense of closely following the standard EA AGI X-risk paradigm that's centered around the ideas of Yudkowsky, Bostrom, MIRI, utility maximization, instrumental convergence, deep learning, fast takeoff, etc.</p><p>I worry that this reflects &amp; encourages a premature convergence onto a governing paradigm that may deter newcomers from contributing new ideas that fall outside the current 'Overton window' of AI alignment. For example: (1) a lot of AI alignment work seems based on a tacit assumption that AI won't become an global catastrophic risk until it reaches AGI level, but I can see reasonable arguments that even quite narrow AI could be severely risky long before AGI is reached. Also, (2) a lot of AI alignment seems focused much more on how to align specific AI systems with specific human users, rather than on how to align human groups that are using AI, with other potentially conflicting groups that are using AI.</p><p>So, I guess the question arises: to what extent do you want AI Safety Ideas to elicit new ideas within the current paradigm, versus new ideas that stray outside the current paradigm?</p>", "parentCommentId": null, "user": {"username": "geoffreymiller"}}, {"_id": "4gwzCyr4mEoS5dF6w", "postedAt": "2022-10-18T19:50:49.129Z", "postId": "DTTADonxnDRoksp4E", "htmlBody": "<p>Very true! We have graciously adopted your formulation. Thank you.</p>", "parentCommentId": "sotpb8yBM8iKuNnFF", "user": {"username": "Apart Research"}}, {"_id": "G4gCqvkfwdyqd8ukn", "postedAt": "2022-10-18T19:59:23.290Z", "postId": "DTTADonxnDRoksp4E", "htmlBody": "<p>You raise a very good point that I agree with. Right now, the platform is definitely biased towards the existing paradigm. This will probably be the case during the first few months, but we hope that it will help make the exploration of new directions and paradigms easier at the same time.&nbsp;</p><p>This also raises the point of the ideas currently playing into the canon of AI safety instead of looking at the vast literature outside of AI safety that concerns itself with the same topics but with another framing.</p><p>So to answer your questions; we want AISI to make it easier to elicit new ideas in all paradigms and directions with our personal bias moving that more towards new perspectives as we implement better functionality.</p>", "parentCommentId": "eWaaKHfcEKx45avmL", "user": {"username": "esben-kran"}}, {"_id": "7qwnHF7ipDttq76GG", "postedAt": "2022-10-19T16:48:52.526Z", "postId": "DTTADonxnDRoksp4E", "htmlBody": "<p>Esben -- thanks very much for your reply. That all makes sense -- to develop a gradual broadening-out from the current paradigm to welcoming new perspectives from other existing research traditions.</p>", "parentCommentId": "G4gCqvkfwdyqd8ukn", "user": {"username": "geoffreymiller"}}, {"_id": "FXZvBbnG4RHYjW7uK", "postedAt": "2022-10-22T11:13:16.939Z", "postId": "DTTADonxnDRoksp4E", "htmlBody": "<p>One way of doing automated AI safety research is for AI safety researchers to create AI safety ideas on aisafetyideas.com and then use the titles as prompts for a language model. Here is GPT-3 generating a response to one of the ideas:</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4fcec77ef2289798500de3f7eab01d481927430668b091a1.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4fcec77ef2289798500de3f7eab01d481927430668b091a1.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4fcec77ef2289798500de3f7eab01d481927430668b091a1.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4fcec77ef2289798500de3f7eab01d481927430668b091a1.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4fcec77ef2289798500de3f7eab01d481927430668b091a1.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4fcec77ef2289798500de3f7eab01d481927430668b091a1.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4fcec77ef2289798500de3f7eab01d481927430668b091a1.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4fcec77ef2289798500de3f7eab01d481927430668b091a1.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4fcec77ef2289798500de3f7eab01d481927430668b091a1.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4fcec77ef2289798500de3f7eab01d481927430668b091a1.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4fcec77ef2289798500de3f7eab01d481927430668b091a1.png/w_999 999w\"></figure>", "parentCommentId": null, "user": {"username": "Stephen McAleese"}}, {"_id": "DviNQGyZ8Fd7sbjrq", "postedAt": "2022-10-25T12:01:06.258Z", "postId": "DTTADonxnDRoksp4E", "htmlBody": "<p>Uuh, interesting! Maybe I'll do that as a weekend project for fun. An automatic comment based on the whole idea as a prompt.</p>", "parentCommentId": "FXZvBbnG4RHYjW7uK", "user": {"username": "esben-kran"}}, {"_id": "9XXoedrTmuwCd2GjL", "postedAt": "2022-10-25T23:12:26.437Z", "postId": "DTTADonxnDRoksp4E", "htmlBody": "", "parentCommentId": null, "user": {"username": "esben-kran"}}, {"_id": "CCGgisTfKKZnNbbJb", "postedAt": "2022-10-31T10:39:52.594Z", "postId": "DTTADonxnDRoksp4E", "htmlBody": "", "parentCommentId": "9XXoedrTmuwCd2GjL", "user": {"username": "esben-kran"}}, {"_id": "vjgeJPQtNSi2kpHro", "postedAt": "2022-10-18T19:57:39.575Z", "postId": "DTTADonxnDRoksp4E", "htmlBody": null, "parentCommentId": "eWaaKHfcEKx45avmL", "user": null}]