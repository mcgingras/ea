[{"_id": "qzChnS9mN9DTTcp4G", "postedAt": "2017-08-30T12:01:38.127Z", "postId": "YFoddZtXdeJE5KEM7", "htmlBody": "<p>This post is a bait-and-switch: It starts off with a discussion of the Good Judgement Project and what lessons it teaches us about forecasting superintelligence. However, starting with the section &quot;What lessons should we learn?&quot;, you switch from a general discussion of these techniques towards making a narrow point about which areas of expertise forecasters should rely on, an opinion which I suspect the author arrived at through means not strongly motivated from the Good Judgement Project.</p>\n<p>While I also suspect the Good Judgement Project could have valuable lessons on superintelligence forecasting, I think that taking verbal descriptions of the how superforecasters make good predictions and citing them for arguments about loosely related specific policies is a poor way to do that. As a comparison, I don't think that giving a forecaster this list of suggestions and asking them to make predictions with those suggestions in mind would lead to performance similar to that of a superforecaster. In my opinion, the best way to draw lessons from the Good Judgement Project is to directly rely on existing forecasting teams, or new forecasting teams trained and tested in the same manner, to give us their predictions on potential superintelligence, and to give the appropriate weight to their expertise.</p>\n<p>Moreover, among the list of suggestions in the section &quot;What they found to work&quot;, you almost entirely focus on the second one, &quot;Looking at a problem from multiple different view points and synthesising them?&quot; to make your argument. You can also be said to be relying on the last suggestion to the extent they say essentially the same thing, that we should rely on multiple points of view. The only exception is that you rely on the fifth suggestion, &quot;Striving to distinguish as many degrees of doubt as possible - be as precise in your estimates as you can&quot;, when you argue their strategy documents should have more explicit probability estimates. In response to that, keep in mind that these forecasters are specifically tested on giving well-calibrated probabilistic predictions. Therefore I expect that this overestimates the importance of precise probability estimates in other contexts. My hunch is that giving numerically precise subjective probability estimates is useful in discussions among people already trained to have a good subjective impression of what these probabilities mean, but among people without such training the effect of using precise probabilities is neutral or harmful. However, I have no evidence for this hunch.</p>\n<p>I disapprove of this bait-and-switch. I think it deceptively builds a case for diversity in intelligence forecasting, and adds confusion to both the topics it discusses.</p>\n", "parentCommentId": null, "user": {"username": "itaibn"}}, {"_id": "wJaTchyWXBMTACD5k", "postedAt": "2017-08-30T19:27:00.511Z", "postId": "YFoddZtXdeJE5KEM7", "htmlBody": "<p>Sorry if you felt I was being deceptive. The list of areas of expertise I mentioned in the 80K hours section was relatively broad and not meant to be exhaustive. I could add physics and economics off the top of my head. I'm sure there were many more. I was considering each AGI team as having to do small amounts of forecasting about the likely success and usefulness of their projects. I think building it in the superforecasting mindset at all levels of endeavours could be valuable, without having to rely on explicit superforecasters for every decision.  </p>\n<blockquote>\n<p>In my opinion, the best way to draw lessons from the Good Judgement Project is to directly rely on existing forecasting teams, or new forecasting teams trained and tested in the same manner, to give us their predictions on potential superintelligence, and to give the appropriate weight to their expertise.</p>\n</blockquote>\n<p> It would be great to have a full team of forecasters working on intelligence in general (so they would have something to correlate their answers on Superintelligence). I was being moderate in my demands in how much Open Philanthropy Project should change how they make forecasts about what is good to do. I just wanted it to be directionally correct.  </p>\n<blockquote>\n<p> As a comparison, I don't think that giving a forecaster this list of suggestions and asking them to make predictions with those suggestions in mind would lead to performance similar to that of a superforecaster</p>\n</blockquote>\n<p>There was a simple thing people could do to improve their predictions. </p>\n<p>From the book:</p>\n<blockquote>\n<p>One result was particularly surprised me was the effect of a tutorial covering some basic concepts that we'll explore in this book and are summarized in the Ten Commandments appendix. It took only sixty minutes to read and yet it improved accuracy by roughly 10% through the entire tournament year.</p>\n</blockquote>\n<p>The ten commandment appendix is where I got the list of things to do. I figure if I managed to get Open Philosophy Project to try and follow them, things would improve.  But I agree them getting good forecasters somehow would be a lot better.</p>\n<p>Does that clear up where I was coming from?</p>\n", "parentCommentId": "qzChnS9mN9DTTcp4G", "user": {"username": "WillPearson"}}, {"_id": "a8krPWxhfBiwZRtfC", "postedAt": "2017-08-30T22:37:43.254Z", "postId": "YFoddZtXdeJE5KEM7", "htmlBody": "<p>Without taking the time to reply to the post as a whole, a few things to be aware of\u2026</p>\n<p><a href=\"http://www.openphilanthropy.org/blog/efforts-improve-accuracy-our-judgments-and-forecasts\">Efforts to Improve the Accuracy of Our Judgments and Forecasts</a></p>\n<p>Tetlock forecasting grants <a href=\"http://www.openphilanthropy.org/giving/grants/university-pennsylvania-philip-tetlock-forecasting\">1</a> and <a href=\"http://www.openphilanthropy.org/giving/grants/university-pennsylvania-philip-tetlock-making-conversations-smarter-faster\">2</a></p>\n<p><a href=\"http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-timelines\">What Do We Know about AI Timelines?</a></p>\n<p>Some AI forecasting grants: <a href=\"http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-impacts-general-support\">1</a>, <a href=\"http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/electronic-frontier-foundation-ai-social\">2</a>, <a href=\"http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/george-mason-university-research-future-artificial-intelligence-scenarios\">3</a>.</p>\n", "parentCommentId": null, "user": {"username": "lukeprog"}}, {"_id": "WHs63DzyEgq28xZu3", "postedAt": "2017-08-31T08:12:40.675Z", "postId": "YFoddZtXdeJE5KEM7", "htmlBody": "<p>Thanks for the links. It would have been nice to have got them when I emailed OPP a few days ago with a draft of this article.</p>\n<p>I look forward to seeing the fruits of &quot;Making Conversations Smarter, Faster&quot;</p>\n<p>I'm going to dig into the AI timeline stuff, but from what I have seen from similar things, there is an inferential step missing. The question is &quot;Will HLMI (of any technology) might happen with probability X by Y&quot;  and the action is then &quot;we should invest in most of the money in a community for machine learning people and people working on AI safety for machine learning&quot;.   I think worth asking the question, &quot;Do you expect HLMI to come from X technology&quot;. If you want to invest lots in that class of technology.</p>\n<p>Rodney Brooks has an interesting blog about the <a href=\"http://rodneybrooks.com/forai-machine-learning-explained/\">future of robotics and AI</a>. Worth keeping an eye on as a dissenter, and might be an example of someone who has said we will have intelligent agents by 2050, but doesn't think it will be current ML.</p>\n", "parentCommentId": "a8krPWxhfBiwZRtfC", "user": {"username": "WillPearson"}}]