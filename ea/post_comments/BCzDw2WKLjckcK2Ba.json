[{"_id": "Kyx4DQGRonwjkDCJq", "postedAt": "2022-10-12T01:42:42.147Z", "postId": "BCzDw2WKLjckcK2Ba", "htmlBody": "<p>See also <a href=\"https://forum.effectivealtruism.org/posts/FhjDSijdWrhFMgZrb/the-epistemic-challenge-to-longtermism-tarsney-2020\">Michael Aird's comments</a> on this Tarsney (2020) paper. His main points are:</p><ul><li>'Tarsney's model updates me towards thinking reducing <a href=\"https://forum.effectivealtruism.org/posts/AJbZ2hHR4bmeZKznG/venn-diagrams-of-existential-global-and-suffering\">non-extinction existential risks</a> should be a little less &nbsp;of a priority than I previously thought.' (<a href=\"https://forum.effectivealtruism.org/posts/FhjDSijdWrhFMgZrb/the-epistemic-challenge-to-longtermism-tarsney-2020?commentId=BPxvimxtGnaSiXTvH\">link</a> to full comment)</li><li>'Tarsney seems to me to understate the likelihood that accounting for non-human animals would substantially affect the case for longtermism.' (<a href=\"https://forum.effectivealtruism.org/posts/FhjDSijdWrhFMgZrb/the-epistemic-challenge-to-longtermism-tarsney-2020?commentId=NByN3Jhgg4X2z6ACe\">link</a>)</li><li>'The paper ignores 2 factors that could strengthen the case for longtermism - namely, possible increases in how <i>efficiently </i>resources are used and in what <i>extremes </i>of experiences can be reached.' (<a href=\"https://forum.effectivealtruism.org/posts/FhjDSijdWrhFMgZrb/the-epistemic-challenge-to-longtermism-tarsney-2020?commentId=GTxLwzXN97Qak4EKm\">link</a>)</li><li>'Tarsney writes \"resources committed at earlier time should have greater impact, all else being equal\". I think that this is misleading and an oversimplification. See <a href=\"https://forum.effectivealtruism.org/posts/LD3mNJ367tSMna6WR/crucial-questions-about-optimal-timing-of-work-and-donations\">Crucial questions about optimal timing of work and donations</a> and other posts tagged <a href=\"https://forum.effectivealtruism.org/topics/timing-of-philanthropy\">Timing of Philanthropy</a>.' (<a href=\"https://forum.effectivealtruism.org/posts/FhjDSijdWrhFMgZrb/the-epistemic-challenge-to-longtermism-tarsney-2020?commentId=kD6kXBMfJtZQeLu8h\">link</a>)</li><li>'I think it'd be interesting to run a sensitivity analysis on Tarsney's model(s), and to think about the value of information we'd get from further investigation of:&nbsp;<ul><li>how likely the future is to resemble Tarsney's cubic growth model vs his steady model</li><li>whether there are other models that are substantially likely, whether the model structures should be changed</li><li>what the most reasonable distribution for each parameter is.' (<a href=\"https://forum.effectivealtruism.org/posts/FhjDSijdWrhFMgZrb/the-epistemic-challenge-to-longtermism-tarsney-2020?commentId=CKwnzYNN2LXZDCAa2\">link</a>)</li></ul></li></ul>", "parentCommentId": null, "user": {"username": "Will Aldred"}}, {"_id": "siWy3JdS3c3d5QmsA", "postedAt": "2022-10-12T09:07:23.215Z", "postId": "BCzDw2WKLjckcK2Ba", "htmlBody": "<p>If I understand the proposed model correctly (I haven't read thoroughly, so apologies if not): <strong>The model basically assumes that \"longtermist interventions\" cannot cause accidental harm.</strong> That is, it assumes that if a \"longtermist intervention\" is carried out, the worst-case scenario is that the intervention will end up being neutral (e.g. due to an \"exogenous nullifying event\") and thus resources were wasted.</p>\n<p>But this means assuming away the following major part of <a href=\"https://forum.effectivealtruism.org/topics/cluelessness\">complex cluelessness</a>: due to an abundance of <a href=\"https://www.effectivealtruism.org/articles/crucial-considerations-and-wise-philanthropy-nick-bostrom\">crucial considerations</a>, it is usually extremely hard to judge whether an intervention that is related to anthropogenic x-risks or meta-EA is net-positive or net-negative. For example, such an intervention may cause accidental harm due to:</p>\n<ol>\n<li>Drawing attention to <a href=\"https://www.nickbostrom.com/information-hazards.pdf\">dangerous information</a> (e.g. certain exciting approaches for AGI development / virology experimentation).\n<ul>\n<li>If a researcher believes they came up with an impressive <a href=\"https://www.alignmentforum.org/posts/HSETWwdJnb45jsvT8/autonomy-the-missing-agi-ingredient\">insight</a>, they will probably be biased towards publishing it, even if it may draw attention to potentially dangerous information. Their career capital, future compensation and status may be on the line.</li>\n<li>Alexander Berger (co-CEO of OpenPhil) said in an <a href=\"https://80000hours.org/podcast/episodes/alexander-berger-improving-global-health-wellbeing-clear-direct-ways/#transcript\">interview</a> :\n<blockquote>\n<p>I think if you have the opposite perspective and think we live in a really vulnerable world \u2014 maybe an offense-biased world where it\u2019s much easier to do great harm than to protect against it \u2014 I think that increasing attention to anthropogenic risks could be really dangerous in that world. Because I think not very many people, as we discussed, go around thinking about the vast future.</p>\n<p>If one in every 1,000 people who go around thinking about the vast future decide, \u201cWow, I would really hate for there to be a vast future; I would like to end it,\u201d and if it\u2019s just 1,000 times easier to end it than to stop it from being ended, that could be a really, really dangerous recipe where again, everybody\u2019s well intentioned, we\u2019re raising attention to these risks that we should reduce, but the increasing salience of it could have been net negative.</p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li>\"Patching\" a problem and preventing a non-catastrophic, highly-visible outcome that would have caused an astronomically beneficial \"immune response\".\n<ul>\n<li>Nick Bostrom said in a <a href=\"https://www.effectivealtruism.org/articles/crucial-considerations-and-wise-philanthropy-nick-bostrom/\">talk</a> (\"lightly edited for readability\"):\n<blockquote>\n<p>Small and medium scale catastrophe prevention? Also looks good. So global catastrophic risks falling short of existential risk. Again, very difficult to know the sign of that. Here we are bracketing leverage at all, even just knowing whether we would want more or less, if we could get it for free, it\u2019s non-obvious. On the one hand, small-scale catastrophes might create an immune response that makes us better, puts in place better safeguards, and stuff like that, that could protect us from the big stuff. If we\u2019re thinking about medium-scale catastrophes that could cause civilizational collapse, large by ordinary standards but only medium-scale in comparison to existential catastrophes, which are large in this context, again, it is not totally obvious what the sign of that is: there\u2019s a lot more work to be done to try to figure that out. If recovery looks very likely, you might then have guesses as to whether the recovered civilization would be more likely to avoid existential catastrophe having gone through this experience or not.</p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li>Causing decision makers to have a false sense of security.\n<ul>\n<li>For example, perhaps it's not feasible to solve AI alignment in a competitive way without strong coordination, etcetera. But researchers are biased towards saying good things about their field, their colleagues and their (potential) employers.</li>\n</ul>\n</li>\n<li>Causing progress in AI capabilities to accelerate in a certain way.</li>\n<li>Causing the competition dynamics among AI labs / states to intensify.</li>\n<li>Decreasing the EV of the EA community by exacerbating bad incentives and conflicts of interest, and by reducing coordination.\n<ul>\n<li>For example, by creating impact markets.</li>\n</ul>\n</li>\n<li>Causing accidental harm via outreach campaigns or regulation advocacy (e.g. by causing people to get a bad first impression of something important).</li>\n<li>Causing a catastrophic leak from a virology lab, or an analogous catastrophe involving an AI lab.</li>\n</ol>\n", "parentCommentId": null, "user": {"username": "ofer"}}, {"_id": "ifMp2EFg34YxtDJHB", "postedAt": "2022-10-12T11:12:43.071Z", "postId": "BCzDw2WKLjckcK2Ba", "htmlBody": "<p>On the summary: I'd have found this summary more useful if it had made the ideas in the paper simpler, so it was easier to get an intuitive grasp on what was going on. This summary has made the paper shorter, but (as far as I can recall) mostly by compressing the complexity, rather than lessening it!</p>\n<p>On the paper itself: I still find Tarsney's argument hard to make sense of (in addition to the above, I've read the full paper itself a couple of times).<br>\nAFAIT, the set up is that the longtermist wants to show that there are things we can do now that will continually make the future better than it would have been ('persistent-difference strategies').\nHowever, Tarnsey takes the challenge to be that there are things that might happen that would stop these positive states happening ('exogenously nullifying events').\nAnd what does all the work is that if the human population expands really fast ('cubic growth model'), that is, because it's fled to the stars, but the negative events should happen at a constant rate, then longtermism looks good.</p>\n<p>I think what bothers me about the above is this: why think that we could ever identify and do something that would, in expectation, make a persistent positive difference, i.e. a difference for ever and ever and ever? Isn't Tarsney assuming the existence of the thing he seeks to prove, ie 'begging the question'? I think the sceptic is entitled to respond with a puzzled frown - or an incredulous stare -  about whether we can really expect to knowingly change the whole trajectory of the future - that, after all, presumably is the epistemic challenge. That challenge seems unmet.</p>\n<p>I've perhaps misunderstood something. Happy to be corrected!</p>\n", "parentCommentId": null, "user": {"username": "MichaelPlant"}}, {"_id": "vL5ZKxrgkMvREvpbx", "postedAt": "2022-10-17T15:14:21.478Z", "postId": "BCzDw2WKLjckcK2Ba", "htmlBody": "<p>Thanks! This is valuable feedback.</p><p>By 'persistent difference', Tarsney doesn't mean a difference that persists forever. He just means a difference that persists for a long time in expectation: long enough to make the expected value of the longtermist intervention greater than the expected value of the neartermist benchmark intervention.</p><p>Perhaps you want to know why we should think that we can make this kind of persistent difference. I can talk a little about that in another comment if so.</p>", "parentCommentId": "ifMp2EFg34YxtDJHB", "user": {"username": "elliottthornley"}}, {"_id": "YNbo2gH4R7saDo8E2", "postedAt": "2022-10-18T08:38:12.652Z", "postId": "BCzDw2WKLjckcK2Ba", "htmlBody": "<p>All good points, but Tarsney's argument doesn't depend on the assumption that longtermist interventions cannot accidentally increase x-risk. It just depends on the assumption that there's some way that we could spend $1 million &nbsp;that would increase the epistemic probability that humanity survives the next thousand years by at least 2x10^-14.</p>", "parentCommentId": "siWy3JdS3c3d5QmsA", "user": {"username": "elliottthornley"}}]