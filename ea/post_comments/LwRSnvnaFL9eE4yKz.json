[{"_id": "xpTmbEx3YLt2ecpCM", "postedAt": "2022-10-06T19:22:09.633Z", "postId": "LwRSnvnaFL9eE4yKz", "htmlBody": "<p>I think this reasoning applies to the initial funding for new, neglected areas. It often appears like grantmakers are evaluating a project in absolute terms as to whether they think it is likely to succeed.</p>\n<p>But exploration and discovery costs are often a pittance compared to potential impact of new ideas, and when the potential for exploitation of promising interventions is incorporated, we should definitely be more risk-seeking with the resources we deploy as a community.</p>\n<p>The greatest EV fund distributor may very well be one with many duds and perhaps we should be wary of incentivizing funds that have a bunch of <em>good</em> outcomes. You hear on 80k and on many other sources that we should be risk-neutral re altruistic projects, but this neutrality depends on institutions that will enable new ideas.</p>\n", "parentCommentId": null, "user": {"username": "Brad West"}}, {"_id": "5CxwmBkkXGNkzj94C", "postedAt": "2022-10-07T00:35:19.734Z", "postId": "LwRSnvnaFL9eE4yKz", "htmlBody": "<p>While I agree with a lot in this post, I do want to push back on this reasoning:&nbsp;</p><blockquote><p>About 500 more people who met the admissions bar would get to experience the conference. However much impact the default conference would produce \u2014 sparked collaborations, connections for years to come, inspirations for new projects, positive career changes \u2014 there would be about double that</p></blockquote><p>I think an estimate of \"double that\" is pretty wrong. I think the first 500 people who would be admitted would of course be selected for getting value out of the conference, and I expect the value that different people gain to be heavy-tailed. It is hard to predict who exactly will get value out of a conference, but it wouldn't surprise me if you get to a state where you capture 90% of the value by admitting the right 500 people.&nbsp;</p><p>On the other hand, I think a conference might produce value in the square of the number of the participants, since people can self sort, and meeting more people is more valuable than meeting less people.&nbsp;</p><p>I think in one line of reasoning you get something like \"a conference twice the size would be maybe 10-20% more valuable\" and in the other line of reasoning you get \"a conference twice the size could be 4x as valuable\", but I don't have any line of reasoning I endorse that outputs the 2x number.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Habryka"}}, {"_id": "8FYyJrq5yLCvm2fps", "postedAt": "2022-10-07T04:35:18.001Z", "postId": "LwRSnvnaFL9eE4yKz", "htmlBody": "<p>I would say the square in the number of participants is too extreme, since the average attendee probably wouldn\u2019t meet many more people than otherwise, except for those who wouldn't have gotten to attend at all.</p>\n<p>(EDIT, nvm this bit; I don\u2019t know how to strike it out via mobile: Plus, because people are going to meet based on interests, if you were thinking about the number of possible meetings, I think it would be better to think about it like multiple cliques doubling in size than a single large clique doubling in size, or something more complicated.)</p>\n<p>The first 500 (except those hiring?) probably wouldn't get much more out of it, since it at most only slightly adds to who they might have met in terms of counterfactual value, and they might even get less, since they need to compete with the new 500 over meetings with the first 500. Then the next 500 at least get to meet each other, but also the first 500, and especially the (I assume) roughly fixed number of organizations that are hiring.</p>\n<p>The first 500 is also plausibly made up of many people who are already largely in contact with one another because they work at EA-related orgs, either the same org, or orgs working in the same area who review each other's work, strategize together or collaborate.</p>\n<p>Around 2x seems plausible to me, but my best guess is less than 2x.</p>\n", "parentCommentId": "5CxwmBkkXGNkzj94C", "user": {"username": "MichaelStJules"}}, {"_id": "yy5Tyno5rc6iEKbD7", "postedAt": "2022-10-07T04:56:27.889Z", "postId": "LwRSnvnaFL9eE4yKz", "htmlBody": "<p>Sorry, I think you could arrive at 2x for bayesian reasons (like weighing multiple models), but I just wanted to push back on the model that an event with twice as many attendees should be straightforwardly modeled as twice as valuable.</p>", "parentCommentId": "8FYyJrq5yLCvm2fps", "user": {"username": "Habryka"}}, {"_id": "soR9H8pzLpaLNCb24", "postedAt": "2022-10-07T06:26:43.591Z", "postId": "LwRSnvnaFL9eE4yKz", "htmlBody": "<p>I agree that it's not straightforward that a linear model is approximately correct. I do think a linear model could still be approximately correct for straightforward linear reasons, like the value being roughly proportional to the number of one-on-ones, though, and not just because you weighed multiple models together and it happened to come out to about 2x.</p>\n", "parentCommentId": "yy5Tyno5rc6iEKbD7", "user": {"username": "MichaelStJules"}}, {"_id": "2JWAyGiaCBAfeyXh4", "postedAt": "2022-10-07T09:44:33.592Z", "postId": "LwRSnvnaFL9eE4yKz", "htmlBody": "<p>I couldn't agree more with this post. I've been referring to it in my circles as the \"risks of inaction\" and \"leaving impact on the table\", if any of those terms resonate more with people.&nbsp;</p><p>Will MacAskill also mentioned in a post once the \"bureaucrat's curse\", which I love. It's the inverse of the unilateralist's curse, where if just one person doesn't like the idea, it gets killed.&nbsp;</p><p>I see this everywhere, especially in longtermism. The fear of accidentally making things worse (which is a warranted fear!), overshadows the fear of accidentally moving too slowly.</p><p>If you're on a bus hurtling towards a cliff, instinctively acting in a panic can make things worse, but also moving too slowly or not at all also leads to high downsides</p>", "parentCommentId": null, "user": {"username": "katherinesavoie"}}, {"_id": "SmAg9hyPM72n4QHFz", "postedAt": "2022-10-07T09:47:35.137Z", "postId": "LwRSnvnaFL9eE4yKz", "htmlBody": "<p><strong>Potential way to feel better about fear of EA criticism:</strong></p><p>Fear of criticism is indeed a huge source of this problem. One thing I've found recently that's improved my ability to weather EA criticism online is to think of the comment section not as a comment section, but as the debate section.&nbsp;</p><p>Then it's not that the post is being criticised, but that it's being debated, which is something I enjoy and appreciate.&nbsp;</p><p>Maybe this framing will help others, but open for debate on it :)&nbsp;</p>", "parentCommentId": null, "user": {"username": "katherinesavoie"}}, {"_id": "FAuJPmPpnu8yqnK6x", "postedAt": "2022-10-07T10:33:10.307Z", "postId": "LwRSnvnaFL9eE4yKz", "htmlBody": "<p>Thank you!&nbsp;</p><p>I also really like the phrase bureaucrat's curse. Here's the relevant passage (in <a href=\"https://forum.effectivealtruism.org/posts/cfdnJ3sDbCSkShiSZ/ea-and-the-current-funding-situation\">this post</a>):&nbsp;</p><blockquote><p>As well as the unilateralist\u2019s curse (where the most optimistic decision-maker determines what happens), there\u2019s a risk of falling into what we could call the <i>bureaucrat\u2019s curse</i>,<a href=\"https://forum.effectivealtruism.org/posts/cfdnJ3sDbCSkShiSZ/ea-and-the-current-funding-situation#fnpzpq1mxls7k\"><sup>[10]</sup></a>&nbsp;where everyone has a veto over the actions of others; in such a situation, if everyone follows their own best-guesses, then the most pessimistic decision-maker determines what happens. I\u2019ve certainly seen something closer to the bureaucrat\u2019s curse in play: if you\u2019re getting feedback and your plans, and one person voices strong objections, it feels irresponsible to go ahead anyway, even in cases where you should. At its worst, I\u2019ve seen the idea of unilateralism taken as a reason against competition within the EA ecosystem, as if all EA organisations should be monopolies.&nbsp;</p></blockquote><p>(<a href=\"https://forum.effectivealtruism.org/posts/cfdnJ3sDbCSkShiSZ/ea-and-the-current-funding-situation?commentId=s6WdCQy3k4MebT5qv\">In a comment, Linch points out</a> that this is a special case of the unilateralist's curse.) I also really like the suggestions below the cited passage \u2014 on what we need to do or keep doing to manage risks properly:&nbsp;</p><blockquote><ul><li>Stay in constant communication about our plans with others, inside and outside of the EA community, who have similar aims to do the most good they can</li><li>Remember that, in the standard solution to the unilateralist\u2019s dilemma, it\u2019s the median view that\u2019s the right (rather than the most optimistic or most pessimistic view)</li><li>Are highly willing to course-correct in response to feedback</li></ul></blockquote><p>(In writing, I think there's something somewhat related to the bureaucrat's curse, which is writing-by-committee, or what <a href=\"https://forum.effectivealtruism.org/posts/pPAvWAaD9AnLnLgJX/stephen-clare-s-shortform?commentId=c4p5Cj9KA4vLZmTXn#c4p5Cj9KA4vLZmTXn\">Stephen Clare called \"death by feedback\"</a>.)</p>", "parentCommentId": "2JWAyGiaCBAfeyXh4", "user": {"username": "Lizka"}}, {"_id": "EFk6Lrebci9bPgwtq", "postedAt": "2022-10-07T10:59:36.883Z", "postId": "LwRSnvnaFL9eE4yKz", "htmlBody": "<p>Thanks for the pushback. I agree that a linear model will be importantly wrong, although if you approximate the impact from the conference using the number of connections people report and assume that stays roughly the same, it doesn't seem wild as a first pass. (Please let me know if you disagree!)&nbsp;</p><p>[Half-formed thoughts below.]</p><p>On the other hand, I think 10-20% more valuable seems very off to me, especially in this case, given we were not \"lowering the bar\" for the second group of attendees. &nbsp;Setting this case aside, I can imagine a world in which someone is very confident in their ability to admit the people who will benefit the most from a conference (and the people who would be most useful for them to meet with), and in this world, you might be able to get 90% of the value with 50% of the size \u2014 but I don't really think we're in this world (especially in terms of identifying people who will benefit most from the event).&nbsp;</p><p>I'm not really sure how well people self-sort at conferences, which was a big uncertainty for me when I was thinking about these things more. I do think people will often identify (often with <a href=\"https://forum.effectivealtruism.org/posts/sxKJckCiZQyux4kCx/ea-global-tips-networking-with-others-in-mind\">help</a>) some of the people with whom it would be most useful to meet. If people are good at self-sorting (e.g. searching through swapcard and finding the most promising 10-15 meetings), and if those most-useful meetings over the whole conference aren't somehow concentrated on meetings with a small number of nodes, then admitting double the people will likely lead to more than double the impact.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefew1ed30nxqe\"><sup><a href=\"#fnew1ed30nxqe\">[1]</a></sup></span>&nbsp;If people are not good at self-sorting, though, it seems more likely that we'd get closer to straightforward doubling, I think. (I'm fairly confident that people are better than random, though.)</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnew1ed30nxqe\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefew1ed30nxqe\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;It does seem possible that there are some \"nodes\" in the network \u2014 at a very bad first pass, you could imagine that everyone's most valuable meetings are with the speakers. The speakers each meet with lots of people (say, they have lots of time and don't get tired) and would be at the conference in any world (doubling or not). Then the addition of 500 extra people doesn't significantly improve the set of possible meetings for the 500 first attendees, although 500 extra people get to meet with the speakers (which is nearly all that matters in this model).&nbsp;</p><p>I'm really unsure about the extent to which the \"nodes\" thing is true (and if it's true I don't really think that \"speakers\" are the right group), but there's something here that seems like it could be right given what we hear. There's also the added nuance that some nodes are probably in the second group of 500, and also that the size and capacity for meetings of the \"nodes\" group would matter.</p></div></li></ol>", "parentCommentId": "5CxwmBkkXGNkzj94C", "user": {"username": "Lizka"}}, {"_id": "quMcztvzwCHS6knDF", "postedAt": "2022-10-07T18:07:36.073Z", "postId": "LwRSnvnaFL9eE4yKz", "htmlBody": "<p>Hmm, I do really think there is a very wrong intuition here. I think by-default in most situations the return to doubling a specific resource should be modeled as logarithmic (i.e. the first doubling is as valuable as the second doubling). I think in this model, it is very rare that doubling a thing along any specific dimension produces twice the value. I think the value of marginally more people in EA should likely also be modeled as having logarithmic returns (or I might argue worse than logarithmic returns, but I think logarithmic is the right prior).</p><p>I think you will get estimates wrong by many orders of magnitude if you do reasoning of the type \"if I just double this resource that will double the whole value of the event\", unless you have a strong argument for network effects.</p>", "parentCommentId": "EFk6Lrebci9bPgwtq", "user": {"username": "Habryka"}}, {"_id": "GJqrgQ76GsKX4PqhJ", "postedAt": "2022-10-07T18:59:27.021Z", "postId": "LwRSnvnaFL9eE4yKz", "htmlBody": "<p>Furthermore, the quality distribution of jammie dodgers is arguably fat-tailed.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefj2ey7qvegc\"><sup><a href=\"#fnj2ey7qvegc\">[1]</a></sup></span>&nbsp;If by many examples you've trained your intuition about what \"good cookies\" look like, you're most likely still sampling near the median part of the distribution. The <i>very best </i>might be very different. What you naively perceive as \"lumpy\"--a trait you rarely see in \"good cookies\" so you instead grab another one--might in fact be part of the unusual character that takes it into the <i>very best </i>category.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc6dmkego21e\"><sup><a href=\"#fnc6dmkego21e\">[2]</a></sup></span>&nbsp;After all, you should expect the extreme outliers to be different in some unusual way compared to the merely good outliers you've trained your intuitions on. I always eat the ones that don't fit in.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnj2ey7qvegc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefj2ey7qvegc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Although more realistically the distribution has several peaks due to recipe variation and baker idiosyncrasies.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc6dmkego21e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc6dmkego21e\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://slatestarcodex.com/2019/02/26/rule-genius-in-not-out/\">Sensitivity over specificity</a> for non-poisonous cookie-distributions! Not only because, as you say, flaws are easier to notice than hitherto-unknowable outlier winning-traits, but also because flaws are <i>less consequential</i> in lower-bounded distributions.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Emrik"}}, {"_id": "v4hRjNvnKtAEt5ixa", "postedAt": "2022-10-07T19:25:45.855Z", "postId": "LwRSnvnaFL9eE4yKz", "htmlBody": "<p>I wonder how much of your intuition comes from thinking that marginal (ex ante) impact of marginal EAG attendees is much lower than the existing average, vs <a href=\"https://forum.effectivealtruism.org/posts/4rGpNNoHxxNyEHde3/most-problems-fall-within-a-100x-tractability-range-under?commentId=r2oFA5RStyQtF63Gq#comments\">normal logarithmic prior considerations</a> vs how much of it comes from diseases of scale (e.g. higher population making things harder to coordinate, pressure towards conformity).<br><br>The first consideration is especially interesting to isolate, since:</p><blockquote><p>I think the value of marginally more people in EA should likely also be modeled as having logarithmic returns (or I might argue worse than logarithmic returns, but I think logarithmic is the right prior</p></blockquote><p>If you think doubling the quality-adjusted people in EA overall has logarithmic returns, you still get ~linear effects from doubling &nbsp;the output of one event or outreach project, since differential functions are locally linear.&nbsp;</p>", "parentCommentId": "quMcztvzwCHS6knDF", "user": {"username": "Linch"}}, {"_id": "WkhHM39d5AQGYTAJE", "postedAt": "2022-10-08T00:16:53.726Z", "postId": "LwRSnvnaFL9eE4yKz", "htmlBody": "<p>With this in mind, can I edit the wiki to try and make it a much better resource? I think it's basically dead and you don't lose much by risking this, but there is a lot of upside if I'm right.</p><p><i>I know I pester about this, but I think that this is the topic of the post. You have the chance to either permit or deny a change of norms with low downside and high upside. I suggest you take the risk.</i></p>", "parentCommentId": null, "user": {"username": "nathan"}}, {"_id": "qFEg586i6qntFRNNw", "postedAt": "2022-10-15T11:31:09.329Z", "postId": "LwRSnvnaFL9eE4yKz", "htmlBody": "<p>Thanks for writing this!</p><p>Related to minimising downside, I like 80,000 Hours' article <a href=\"https://80000hours.org/articles/accidental-harm/\">Ways people trying to do good accidentally make things worse, and how to avoid&nbsp;them</a>.</p>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "ZfdKtBHvyHpFhktGc", "postedAt": "2023-08-10T07:48:20.535Z", "postId": "LwRSnvnaFL9eE4yKz", "htmlBody": "<p>This is a great and very insightful post (and some good comments too). Definitely time well spent reading all of this.&nbsp;<br><br>One tragic example of this that we see every day is homelessness. I presume I'm not the only person living in a city where there are people sleeping outside even during the winter, while the city council is spending millions and many years renovating existing buildings to make them meet exacting standards so that they can eventually house these people.&nbsp;<br><br>And it feels like we need someone to go in there and say \"listen, living in a substandard apartment, even one that doesn't meet the requirements for fire-safety or accessibility is still far, far safer and better than living on the street when it's below freezing and there are criminals and addicts likely to attack you.\" But I imagine that if I'm a city bureaucrat, the reward-system facing me is that if I house 10 people and they are all happy, I get a good score, but if I house 100 and one of them suffers an accident due to a flaw in the building, I will likely get fired - an evaluation that fails to factor in what would happen to the other 90 people if they spent the winter sleeping on the street.&nbsp;<br><br>At the same time, they can also go too far in the other direction (where impact loss doesn't apply), for example providing hostels which offer no safety or privacy, so that many people actually choose to stay on the street.&nbsp;<br><br>I don't want to over-simplify, I'm not an expert on this area. Nor do I want to criticise the many people who I'm sure are doing everything they can to help, including many public servants. I just think it's one of many areas where Lizka's reasoning and approach would be very valuable, and it feels like it's not happening.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Denis "}}]