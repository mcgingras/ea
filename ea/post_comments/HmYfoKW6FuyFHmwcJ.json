[{"_id": "ZsjhaHuPtxL8Nh3oT", "postedAt": "2022-12-24T01:49:43.980Z", "postId": "HmYfoKW6FuyFHmwcJ", "htmlBody": "<p>There is some non-prose discussion of arguments around AI safety. Might be worth checking out:\n<a href=\"https://www.lesswrong.com/posts/brFGvPqo8sKpb9mZf/the-basics-of-agi-policy-flowchart\">https://www.lesswrong.com/posts/brFGvPqo8sKpb9mZf/the-basics-of-agi-policy-flowchart</a>\nSome of the stuff linked here: <a href=\"https://www.lesswrong.com/posts/4az2cFrJp3ya4y6Wx/resources-for-ai-alignment-cartography\">https://www.lesswrong.com/posts/4az2cFrJp3ya4y6Wx/resources-for-ai-alignment-cartography</a>\nIncluding: <a href=\"https://www.lesswrong.com/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment\">https://www.lesswrong.com/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment</a></p>\n", "parentCommentId": null, "user": {"username": "Aaron_Scher"}}]