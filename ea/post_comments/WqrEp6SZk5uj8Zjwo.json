[{"_id": "uj6jdKYYfXQyPnevo", "postedAt": "2022-10-02T23:35:34.546Z", "postId": "WqrEp6SZk5uj8Zjwo", "htmlBody": "<p>This post reads a little like someone pushing at an open door to me. So you write that FTX should ask themselves whether humanity should create AGI. The feeling I get from that is that you think FTX assume that AGI will be good. But the reason they've announced the contest is that they think the development of AGI carries a serious risk of global catastrophe.</p>\n<blockquote>\n<p>Two of the propositions focus on when AGI will arrive. This makes it seem like AGI is a natural event, like an asteroid strike or earthquake. But AGI is something we will create, if we create it.</p>\n</blockquote>\n<p>There are immense (economical, other) incentives to build AGI, so while humanity can simply choose not to build AGI, FTX (or any other single actor) is not in a position to choose not to build AGI. I expect FTX is open to considering interventions aimed at making that happen (not least as there's been <a href=\"https://astralcodexten.substack.com/p/why-not-slow-ai-progress\">some discussion</a> on whether to try to slow down AI progress recently). But whether those work at all is not obvious.</p>\n<blockquote>\n<p>How would know we had successful AGI if/when we created it? It would nothing like human intelligence, which is shaped not only by information processing, but by embodiment and the emotions central to human existence. ... So AGI cannot be like human intelligence.</p>\n</blockquote>\n<p>As far as I'm aware, writers on AGI risk have been clear from the beginning that there's no reason to expect an AGI to take the same form as a human mind (unless it's the result of whole-brain emulation). E.g. Bostrom roughly defines superintelligence as \"any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\" (Superintelligence ch. 2). There's no reason to think a highly capable but alien-to-us intelligence poses less of a threat than one that's similar to us.</p>\n<blockquote>\n<p>AGI might be helpful for thinking about complex human problems, but it is doubtful that it would be better than task specific AI. Task specific AI has already proven successful at useful, difficult jobs (such as cancer screening for tissue samples and hypothesizing protein folding structures). Part of what has enabled such successful applications is the task specificity. That allows for clear success/fail training and ongoing evaluation measures.</p>\n</blockquote>\n<p>There are advantages to generality too, like reducing the need for task-specific data. There's at least one example of a general intelligence being extremely successful, and that is our own, as evidenced by the last few billions' years of evolutionary history. An example of fairly successful general-ish AI is GPT-3, which was just trained on next-word prediction but ended up being capable of everything from translation and spell-checking to creative writing and chess-playing.</p>\n", "parentCommentId": null, "user": {"username": "Erich_Grunewald"}}, {"_id": "KWPfnhnLsaptQnfPo", "postedAt": "2022-10-03T12:28:41.306Z", "postId": "WqrEp6SZk5uj8Zjwo", "htmlBody": "<p>Let me then be more specific. &nbsp;Take Bostrom's definition. &nbsp;What are all the cognitive tasks in all the domains of human interest? &nbsp;I think &nbsp;this is super vague and ill-defined. &nbsp;We can train up AI on specific tasks we want to accomplish (and have ways of training AI to do so, because success or failure can be made clear). &nbsp; But there is no \"all cognitive tasks in the domains of human interest\" training because we have no such list, and for some crucial tasks (e.g. ethics) we cannot even define success clearly. &nbsp;</p><p>GPT-3 is impressive for writing, other AI is impressive for image production, but such systems also produce amusingly wrong outputs at times. &nbsp;What is more impressive is useful AI for tasks like the protein folding success. &nbsp;</p><p>Our success has been evolutionarily framed (survival, spread of species), and tested against a punishing world. &nbsp;But AGI will not be embodied. &nbsp;So what counts as success or failure for non-task specific AI? &nbsp;Back to Bostrom's definition, we have no such set of cognitive tasks defined or delineated.&nbsp;</p><p>So what are the incentives for such a creation? &nbsp;You say they are immense. &nbsp;I want to press on the idea that they are substantial.</p>", "parentCommentId": "uj6jdKYYfXQyPnevo", "user": {"username": "Heather Douglas"}}, {"_id": "BKw6FBmd9T4TE2MND", "postedAt": "2022-10-03T15:27:07.925Z", "postId": "WqrEp6SZk5uj8Zjwo", "htmlBody": "<p>Thanks for responding! I think I now understand better what you're getting at, though I'm still a bit unsure about how much work each of these beliefs are doing:</p>\n<ol>\n<li>We shouldn't build AGI.</li>\n<li>We can't build AGI (because there's no coherent reward function we can give it, since many of the tasks it'd have to do have fuzzy success criteria).</li>\n<li>We won't build AGI (because the incentives mean narrow AI will be far more useful).</li>\n</ol>\n<p>Could you clarify whether you agree with these and how important you think each point is? Or is it something else entirely that's key?</p>\n", "parentCommentId": "KWPfnhnLsaptQnfPo", "user": {"username": "Erich_Grunewald"}}, {"_id": "g6qKuEDdzHNiJGCcc", "postedAt": "2022-10-03T16:34:27.297Z", "postId": "WqrEp6SZk5uj8Zjwo", "htmlBody": "<p>I think we could &nbsp;try to build AGI, but I am skeptical it could be anything useful or helpful (a broad alignment problem) because of vague or inapt success criteria, and because of the lack of embodiment of AGI (so it won\u2019t get beat up on by the world generally or have emotional/affective learning). &nbsp;Because of these problems, I think we shouldn't try (1). &nbsp;&nbsp;</p><p>Further, I am trying this line of argument out to see if it will encourage (3) (not building AGI), because these concerns cast doubt on the value of AGI to us (and thus the incentives to build it).</p><p>This takes on additional potency if we embrace the shift to thinking about \u201cshould\" and not just \u201ccan\" in scientific and technological development generally. &nbsp;So that brings us to the questions I think we should be asking, which is how to encourage a properly responsible approach to AI, rather than shifting credences on the Future Funds' propositions about.</p><p>Does that make sense?</p>", "parentCommentId": "BKw6FBmd9T4TE2MND", "user": {"username": "Heather Douglas"}}, {"_id": "xqY2khvhgRADAfwnr", "postedAt": "2022-10-03T20:36:27.084Z", "postId": "WqrEp6SZk5uj8Zjwo", "htmlBody": "<blockquote><p>and because of the lack of embodiment of AGI (so it won\u2019t get beat up on by the world generally or have emotional/affective learning).</p></blockquote><p>There are two ways to plausibly embody AGI.</p><ol><li>as supervisors of dumb robot bodies, the AGI remotely controls a robot body, processing a portion or all of the robot's sensor data.</li><li>as host of an AGI, the AGI's hardware is resident in the robot body.</li></ol>", "parentCommentId": "g6qKuEDdzHNiJGCcc", "user": {"username": "Noah Scales"}}, {"_id": "csmE6TgPeGstvkoPA", "postedAt": "2022-10-04T16:50:00.140Z", "postId": "WqrEp6SZk5uj8Zjwo", "htmlBody": "<p>I can plausibly see such sensors for physical pain but not for emotional pain. &nbsp;Emotional pain is the far more potent teacher of what is valuable and what is not, what is important and what is not. &nbsp; Intelligence needs direction of this sort for learning. &nbsp;</p><p>So, can you build embodied AGI with emotional responses built in-- that last like emotions and so are suitable teachers like emotions? &nbsp;Building in empathy (both for happiness and suffering) and the pain of &nbsp;disapproval to AGI would be crucial.</p>", "parentCommentId": "xqY2khvhgRADAfwnr", "user": {"username": "Heather Douglas"}}, {"_id": "xBqZwCciuALQMggpX", "postedAt": "2022-10-04T20:44:55.965Z", "postId": "WqrEp6SZk5uj8Zjwo", "htmlBody": "<blockquote>\n<p>I think we could try to build AGI, but I am skeptical it could be anything useful or helpful (a broad alignment problem) because of vague or inapt success criteria, and because of the lack of embodiment of AGI (so it won\u2019t get beat up on by the world generally or have emotional/affective learning). Because of these problems, I think we shouldn't try (1).</p>\n</blockquote>\n<p>Hmm, I guess I don't think lack of emotional/affective states is a problem for making useful AGIs. Obviously those are parts of how humans learn, but seems like a machine can learn with any reward function -- it just needs some way of mapping a world state to value.</p>\n<p>Re success criteria, you could for example train an AI to improve a company's profit in a simulated environment. That task requires a broad set of capacities, including high-level ones like planning/strategising. If you do this for many things humans care about, you'll get a more general system, as with <a href=\"https://medium.com/aiguys/gato-googles-generalized-ai-379a2ff88663\">DeepMind's GATO</a>. But of course I'm speculating.</p>\n<blockquote>\n<p>Further, I am trying this line of argument out to see if it will encourage (3) (not building AGI), because these concerns cast doubt on the value of AGI to us (and thus the incentives to build it).</p>\n</blockquote>\n<p>I suppose if you don't think there's any value for us in AGI, and if you don't think there are sufficient incentives for us to build it, there's no need to encourage not building it? Or is your concern more that we're wasting energy and resources trying to build it, or even thinking about it?</p>\n<blockquote>\n<p>This takes on additional potency if we embrace the shift to thinking about \u201cshould\" and not just \u201ccan\" in scientific and technological development generally. So that brings us to the questions I think we should be asking, which is how to encourage a properly responsible approach to AI, rather than shifting credences on the Future Funds' propositions about.</p>\n</blockquote>\n<p>The first proposition -- \"Conditional on AGI being developed by 2070, humanity will go extinct or drastically curtail its future potential due to loss of control of AGI\" -- seems directly linked to whether or not we should build AGI. If AGI carries a serious risk of catastrophe, we obviously shouldn't build it. So to me it looks like the Future Fund is already thinking about the \"should\" question?</p>\n", "parentCommentId": "g6qKuEDdzHNiJGCcc", "user": {"username": "Erich_Grunewald"}}]