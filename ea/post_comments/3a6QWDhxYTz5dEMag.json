[{"_id": "BiTK8aNPcGpNwwfoi", "postedAt": "2023-08-05T12:42:35.767Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>Thanks for writing this, I found it helpful for understanding the biosecurity space better!</p>\n<p>I wanted to ask if you had advice for handling the issue around difficulties for biosecurity in cause prioritisation as a community builder.</p>\n<p>I think it is easy to build an intuitive case for biohazards not being very important or an existential risk, and this is often done by my group members (even good fits for biosecurity like biologists and engineers), who then dismiss the area in favour of other things. They (and me) do not have access to the threat models which people in biosecurity are actually worried about, making it extremely difficult to evaluate. An example of this kind of thinking is David Thorstad's post on overestimating risks from biohazards which I thought was somewhat disappointing epistemically:\n<a href=\"https://ineffectivealtruismblog.com/2023/07/08/exaggerating-the-risks-part-9-biorisk-grounds-for-doubt/\">https://ineffectivealtruismblog.com/2023/07/08/exaggerating-the-risks-part-9-biorisk-grounds-for-doubt/</a>.</p>\n<p>I suppose the options for managing this situation are:</p>\n<ol>\n<li>\n<p>Encourage deference to the field that biosecurity is worth working on relative to other EA areas.</p>\n</li>\n<li>\n<p>Create some kind of resource which isn't an infohazard in itself, but would be able to make a good case of biosecurity's importance by perhaps gesturing at some credible threat models.</p>\n</li>\n<li>\n<p>Permit the status quo which seems to probably lead to an underprioritisation of biosecurity.</p>\n</li>\n</ol>\n<p>2 seems best if it is at all feasible, but am unsure what to do between 1 and 3.</p>\n", "parentCommentId": null, "user": {"username": "CEvans"}}, {"_id": "ffmgMbbwekRe2X5qR", "postedAt": "2023-08-05T13:02:46.793Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>+1 for the idea of a gatekept biosecurity forum.</p>\n<p>My model of dealing with info-hazards in policy advocacy is to remember that most people are not scope sensitive, and that we should go with the least infohazardous justification for a given policy idea.</p>\n<p>Most policy ideas in GCBR response can be justified based on the risk of natural pandemics, and many policy ideas in GCBR prevention can be justified based on risks of accidental release. Discussing the risks of deliberate bioterrorism using engineered pathogens is only needed to justify a very small subset of GCBR prevention policy ideas.</p>\n", "parentCommentId": null, "user": {"username": "freedomandutility"}}, {"_id": "YKc2JwXTWtNgmxKaN", "postedAt": "2023-08-05T14:28:43.187Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>Nice comment, to respond to your options</p><ol><li>Deference doesn't seem ideal, seems against the norms of the EA community</li><li>Like you say seems very feesable. <i><strong>I would be surprised if there wasn't something like this already?</strong></i> And even you could make the point that the threat models used aren't even the highest risk - others that you don't talk about could be even worse.</li><li>Obviously not ideal</li></ol>", "parentCommentId": "BiTK8aNPcGpNwwfoi", "user": {"username": "NickLaing"}}, {"_id": "LBch9QipvKtEP4ZcS", "postedAt": "2023-08-06T02:27:50.449Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>These are really important points, thanks for starting a discussion on this topic. It seems like the infohazard manual by Chris Bakerlee and Tessa Alexanian is an excellent way forward. What do you think it is lacking? I am not an expert here; I'm not trying to imply their framework is complete, I genuinely want to learn more about what's needed and it's hard to convey tone via text.</p><p>You also mention Chris and Tessa's manual \"doesn't echo a consensus among senior community members\". This surprises me, because I consider Chris to be a key senior community member in the biosecurity space. He is quite literally Open Philanthropy Project's senior program associate in biosecurity and pandemic preparedness. Tessa also seems to be a leader in the biosecurity space; she has run all the biosecurity information sessions I've attended, has been the featured guest on the most popular biosecurity podcasts I recommend people listen to, and she has written extensively on possible new projects in the biosecurity space. At the bottom of the manual, Chris and Tessa thank a bunch of other people for contributing. This list of people encompasses a big percentage of who I consider to be senior members of the EA biosecurity community.<br><br>You clearly didn't write this in a vacuum--indeed you seem to have written this post with feedback from Tessa--so I am again asking with genuine curiosity, what do other senior biosecurity community members think we should do about infohazards? And are these EA aligned people with different frameworks? Or are you referencing people outside the EA community who do work in government bio weapons programs or academic synthetic biology researchers?&nbsp;<br><br>Thanks very much for your time!</p>", "parentCommentId": null, "user": {"username": "AbbyBabby"}}, {"_id": "yNtf9niB5Sn6W2okD", "postedAt": "2023-08-06T05:55:25.589Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>This is more a response to \"it is easy to build an intuitive case for biohazards not being very important or an existential risk\", rather than your proposals...</p>\n<p>My feeling is that it is fairly difficult to make the case that biological hazards present an existential as opposed to catastrophic risk and that this matters for some EA types selecting their career paths, but it doesn't matter as much in the grand scale of advocacy? The set of philosophical assumptions under which \"not an existential risk\" can be rounded to \"not very important\" seems common in the EA community, but extremely uncommon outside of it.</p>\n<p>My best guess is that any existential biorisk scenarios probably route through civilisational collapse, and that those large-scale risks are most likely a result of deliberate misuse, rather than accidents. This seems importantly different from AI risk (though I do think you might run into trouble with reckless or careless actors in bio as well).</p>\n<p>I think a focus on global catastrophic biological risks already puts one's focus in a pretty different (and fairly neglected) place from many people working on reducing pandemic risks, and that the benefit of trying to get into the details of whether a specific threat is existential or catastrophic doesn\u2019t really outweigh the costs of potentially generating infohazards.</p>\n<p>My guess is that (2) will be fairly hard to achieve, because the sorts of threat models that are sufficiently detailed to be credible to people trying to do hardcore existential-risk-motivated cause prioritization are dubiously cost-benefitted from an infohazard perspective.</p>\n", "parentCommentId": "BiTK8aNPcGpNwwfoi", "user": {"username": "tessa"}}, {"_id": "KK5tLpwkof92tqb3K", "postedAt": "2023-08-06T09:01:23.539Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>Thanks for your comment!</p><p>On what is lacking: It was written for reading groups, which is already a softly gatekept space. It doesn't provide guidance on other communication channels: what people could write blogs or tweets about, what is safe to talk to LLMs about, what about google docs, etc. Indeed, I was concerned about infinitely abstract galaxy-brain infohazard potential from this very post.</p><p>On dissent:</p><ol><li>I wanted to double down on the message in the document itself that is preliminary and not the be-all-end-all.</li><li>I have reached out to one person I have in mind within EA biosecurity who pushed back on the infohazard guidance document to give them the option to share their disagreement, potentially anonymously.</li></ol>", "parentCommentId": "LBch9QipvKtEP4ZcS", "user": {"username": "nadia_mir-montazeri"}}, {"_id": "WnRbJiBKicogspMun", "postedAt": "2023-08-06T09:15:50.854Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>Do you think the PPE/PAPR example is part of that very small subset? It just happens to be the area I started working on by deference, and I might've gotten unlucky.</p><p>Or is the crux here response vs prevention?</p>", "parentCommentId": "ffmgMbbwekRe2X5qR", "user": {"username": "nadia_mir-montazeri"}}, {"_id": "D4y2QTdnu2hzcBNvt", "postedAt": "2023-08-06T09:24:00.083Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>I think PAPR / developing very high quality PPE can probably be justified on the basis of accidental release risks and discussing deliberate threats wouldn\u2019t add much to the argument, but stockpiles for basic PPE would be easily justified on just natural threats</p>\n", "parentCommentId": "WnRbJiBKicogspMun", "user": {"username": "freedomandutility"}}, {"_id": "QBHZuamevjwhkP5Zk", "postedAt": "2023-08-06T09:26:06.828Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>I think in addition to policymakers not being scope sensitive, they\u2019re also rarely thinking in terms of expected value, such that concern around accidents can drive similar action to concern around deliberate threats, since the probability of accidents is greater</p>\n", "parentCommentId": "D4y2QTdnu2hzcBNvt", "user": {"username": "freedomandutility"}}, {"_id": "8BnP6ch95sTJsEDNF", "postedAt": "2023-08-06T09:28:50.541Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>Actually big caveat here is that policymakers in defence / national security departments might be more responsive to the deliberate threat risk, since that falls more clearly within their scope</p>\n", "parentCommentId": "QBHZuamevjwhkP5Zk", "user": {"username": "freedomandutility"}}, {"_id": "td3HoRoRGyKPiJkzK", "postedAt": "2023-08-06T19:29:54.346Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>Upvoted.</p><p>There needs to be more infosec people. <a href=\"https://80000hours.org/career-reviews/#our-priority-paths\">80k is on the ball on this</a>. If you train a large number of people and they're well networked, you still get a lot of duds who don't know critical basics, like how conversations near smartphones are compromised by default, but you also pump out top-performers, like the people who know that smartphone-free dark zones stick out like a sore thumb in 3d space. It's the top-performers who can do things like weigh the costs and benefits of crowdsourcing strategies like a closed-off forum dedicated to biosecurity, since that cost benefit analysis requires people who know critical details, like how everyone on such a forum would be using insecure operating systems, or how major militaries and intelligence agencies around the world can completely sign-change their evaluations/esteem of GOF research, at unpredictable times, and in contravention of previous agreements and norms (e.g. yearslong periods of what appears to be consensus opposition to GOF research). I think that evaluation can be done, I'm currently leaning towards \"no\", but I don't have nearly enough on-the-ground exposure to the disruptions and opportunity costs caused by the current paradigm, so I can only weigh in.</p><p>Bringing more people in also introduces liabilities. The best advice I can think of is skilling existing people up, e.g. by having them read good books about counterintelligence and infosec (I currently don't have good models for how to distinguish good books from bad books, you need to find people you trust who already know which is which). Actually, I think I might be able to confidently recommend <a href=\"https://www.lesswrong.com/tag/security-mindset\">the security mindset tag on Lesswrong</a> and the <a href=\"https://www.lesswrong.com/posts/dbDHEQyKqnMDDqq2G/cfar-handbook-introduction\">CFAR handbook</a>, both of those should consistently allow more good work and broader perspectives to be handled by fewer people.</p><p>An infohazard manual seems like a great way to distill best practice and streamline the upskilling process for people, but there should be multiple different manuals depending on the roles. There should not be one single manual, Chris and Tessa's manual is far from optimal upskilling (compared to distillations of the security mindset tag and the CFAR handbook alone), you can even have someone make updated versions and roles on the go (e.g. several times per year per reader). But one way or another, each should be distributed and read in printed form AND NOT digital form (even if that means a great waste of paper and ink and space).</p>", "parentCommentId": null, "user": {"username": "trevorw96"}}, {"_id": "Gs7njodvv5Lysherk", "postedAt": "2023-08-07T00:32:54.195Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>Thanks for this comment, and thanks to Nadia for writing the post, I'm really happy to see it up on the forum!</p>\n<p>Chris and I wrote the guidance for reading groups and early entrants to the field; this was partly because we felt that new folks are most likely to feel stuck/intimidated/forced-into-deference/etc. and because it's where we most often found ourselves repeating the same advice over and over.</p>\n<p>I think there are people whose opinions I respect who would disagree with the guidance in a few ways:</p>\n<ul>\n<li>We recommend a few kinds of interpersonal interventions, and some people think this is a poor way to manage information hazards, and the community should aim to have much more explicit / regimented policies</li>\n<li>We recommend quite a bit of caution about information hazards, which more conservative people might consider an attention hazard in and of itself (drawing attention to the fact that information that would enable harm could be generated)</li>\n<li>We recommend quite a bit of caution about information hazards, which less conservative people might consider too encouraging of deference or secrecy (e.g. people who have run into more trouble doing successful advocacy or recruiting/fostering talent, people who have different models of infohazard dyanmics, people who are worried that a lack of transparency worsens the community's prioritization)</li>\n<li>We don't cover a lot of common scenarios, as Nadia noted <a href=\"https://forum.effectivealtruism.org/posts/3a6QWDhxYTz5dEMag/how-can-we-improve-infohazard-governance-in-ea-biosecurity?commentId=KK5tLpwkof92tqb3K\">in her comment</a></li>\n</ul>\n<p>(Side note: it's always both flattering and confusing to be considered a \"senior member\" of this community. I suppose it's true, because EA is very young, but I have many collaborators and colleagues who have decade(s) of experience working full-time on biorisk reduction, which I most certainly do not.)</p>\n", "parentCommentId": "LBch9QipvKtEP4ZcS", "user": {"username": "tessa"}}, {"_id": "qsHGEEXejczrPgeqK", "postedAt": "2023-08-07T12:47:47.076Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>Thank you, super helpful contect!</p>", "parentCommentId": "KK5tLpwkof92tqb3K", "user": {"username": "AbbyBabby"}}, {"_id": "DEp8u79FGQgQabwQw", "postedAt": "2023-08-07T12:49:20.773Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>Thanks, really helpful context!&nbsp;<br><br>Looking around and realizing you're the grown up now can be startling. When did I sign up for this responsibility????</p>", "parentCommentId": "Gs7njodvv5Lysherk", "user": {"username": "AbbyBabby"}}, {"_id": "FrsJnzQpkpKNvsH2S", "postedAt": "2023-08-07T14:34:14.014Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<blockquote><p>(Side note: it's always both flattering and confusing to be considered a \"senior member\" of this community. I suppose it's true, because EA is very young, but I have many collaborators and colleagues who have decade(s) of experience working full-time on biorisk reduction, which I most certainly do not.)</p></blockquote><p>I think part of this is that you are quite active on the forum, give talks at conferences, etc., making you much more visible to newcomers in the field. Others in biosecurity have decades of experience but are less visible to newcomers. Thus, it is understandable to infer that you are a \"senior member.\"</p>", "parentCommentId": "Gs7njodvv5Lysherk", "user": {"username": "MaxG"}}, {"_id": "HwyvSH6wBwExfWvjq", "postedAt": "2023-08-08T09:47:59.008Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>A related point that I have observed in myself:&nbsp;</p><p>I think dual-use technologies have a higher potential for infohazards. I have a preference for not needing to be \"secretive,\" i.e., not needing to be mindful about what information I can share publicly. Probably there is also some deference going on where I shied away from working on more infohazard-y seeming technologies since I wasn't sure how to deal with selectively sharing information. Accordingly, I have preferred to work on biorisk mitigation strategies that have little dual-use potential and, thus, low infohazard risk. (In my case far-UVC, another example would be PPE).</p><p>The problem with this is that it might be much more impactful for me to work on a biorisk mitigation technology that has more dual-use potential, but I haven't pursued this work because of infohazard vibes and uncertainty about how to deal with that.&nbsp;</p><p>Another difficulty, especially for junior people, is that working on projects with significant infohazard risk could prevent you from showing your work and proving your competence. Since you might not be able to share your work publicly, this could reduce your chances of career advancement since you (seemingly) have a smaller track record.&nbsp;</p>", "parentCommentId": null, "user": {"username": "MaxG"}}, {"_id": "mEvxQ8QPG23qPeoY7", "postedAt": "2023-08-09T11:45:56.306Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>Another data point from a <a href=\"https://ineffectivealtruismblog.com/2023/07/08/exaggerating-the-risks-part-9-biorisk-grounds-for-doubt/\">post</a> on Reflective Altruism about biorisk:</p><blockquote><p>This post begins a sub-series, \u201c<a href=\"https://ineffectivealtruismblog.com/category/exaggerating-the-risks/biorisk/\"><u>Biorisk</u></a>\u201d <strong>arguing that levels of existential biorisk may be lower than many effective altruists suppose.</strong></p><p>I have to admit that I had a hard time writing this series. The reason for that is that I\u2019ve had a hard time getting people to tell me exactly what the risk is supposed to be. I\u2019ve approached high-ranking effective altruists, including those working in biosecurity, and been told that they cannot give me details, because those details pose an information hazard. (Apparently the hazard is so great that they refused my suggestion of bringing their concerns to a leading government health agency, or even to the CIA, for independent evaluation). One of them told me point-blank that they understood why I could not believe them if they were not willing to argue in detail for their views, and that they could make do with that result.</p></blockquote>", "parentCommentId": null, "user": {"username": "MaxG"}}, {"_id": "zZpr83bSYjR4QexRz", "postedAt": "2023-08-09T17:25:34.283Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>You know what would be a great way to teach this. Just make up an info hazard type event.\nTabletop war game it. Say the thing is called a mome rath and it's anti-memetic or something(Go read worth the candle). Have the experts explain how they would treat the problem, and use that as a guide for how to interact with info hazards.</p>\n<p>Maybe I am focused on the government piece of this but there's probably damaging information that would hurt national security if it got out. That's why we have classification systems in place. If we can't even have the experts talk about it, then we need to really think about why that is and give them a softball to explain it. (Then think about why they're thinking this, etc.)</p>\n<p>Look, there is a risk. We just need to be able to explain it so that people don't go looking at the door of forbidden knowledge thinking, \"I really need to know everything in there.\" When really what is behind the door is just a bunch of formulas that the current batch of 3d printers know not to make.</p>\n<p>Counterpoint: make it boring and now one will be interested. Instead if Rokos Basilisk, think of calling it IRS-CP Form 23A.</p>\n", "parentCommentId": null, "user": {"username": "Weaver"}}, {"_id": "3WD3iJkvD6bzCQBqP", "postedAt": "2023-08-09T21:13:03.125Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>Hi Nadia, thanks for writing this post! It's a thorny topic, and I think people are doing the field a real service when they take the time to write about problems as they see them \u2013\u2013 I particularly appreciate that you wrote candidly about challenges involving influential funders.<br><br>Infohazards truly are a wicked problem, with lots of very compelling arguments pushing in different directions (hence the lack of consensus you alluded to), and it's frustratingly difficult to devise sound solutions. But I think infohazards are just one of many factors contributing to the overall opacity in the field causing some of these epistemic problems, and I'm a bit more hopeful about other ways of reducing that opacity. For example, if the field had more open discussions about things that are <i>not</i> very infohazardous (e.g., comparing strategies for pursuing well-defined goals, such as maintaining the norm against biological weapons), I suspect it'd mitigate the consequences of not being able to discuss certain topics (e.g. detailed threat models) openly. Of course, that just raises the question of what is and isn't an infohazard (which itself may be infohazardous...), but I do think there are some areas where we could pretty safety move in the direction of more transparency.</p><p>I can't speak for other organisations, but I think my organisation (Effective Giving, where I lead the biosecurity grantmaking program) could do a <i>lot </i>to be more transparent just by overcoming obstacles to transparency that are unrelated to infohazards. These include the (time) costs of disseminating information; concerns about how transparency might affect certain key relationships, e.g. with prospective donors whom we might advise in the future; and public relations considerations more generally; and they're definitely very real obstacles, but they generally seem more tractable than the infohazard issue.</p><p>I think we (again, just speaking for Effective Giving's biosecurity program) have a long way to go, and I'd personally be quite disappointed if we didn't manage to move in the direction of sharing more of our work during my tenure. This post was a good reminder of that, so thanks again for writing it!</p>", "parentCommentId": null, "user": {"username": "jtm"}}, {"_id": "yk3dLsusfLDYyoosQ", "postedAt": "2023-08-10T12:20:21.533Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>Thanks for posting this, Nadia!</p><blockquote><p>Another example is the difficulty of comparing biorisk and AI risk without engaging in potentially infohazardous concrete threat models. While both are considered core cause areas of longtermism, it is challenging to determine how to prioritise these risks without evaluating the likelihood of a catastrophic event.</p></blockquote><p>I would go further, and say that it is challenging to determine whether biorisk should be one of the core areas of longtermism. FWIW, the superforecasters and domain experts of The Existential Risk Persuasion Tournament (<a href=\"https://forecastingresearch.org/xpt\">XPT</a>) predicted the extinction risk until 2100 from engineered pathogens to be 13.5 % (= 0.01/0.074) and 1.82 times (= 0.01/0.0055) that of nuclear war. This is seemingly in contrast with nuclear not being a core area of longtermis (unlike AI and bio).</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/KGwywgpZMddpirhb6/ytwx1hqz1gwafhiodlmi\"></figure><p>I personally think both superforecasters and domain experts are greatly overestimating nuclear extinction risk (I guess it is more like 10^-6 in the next 100 years). However, I find it plausible that extinction risk from engineered pathogens is also much lower than the 3 % bio existential risk from 2021 to 2120 guessed by Toby Ord in <a href=\"https://theprecipice.com/\">The Precipice</a>. David Thorstad will be exploring this in a <a href=\"https://ineffectivealtruismblog.com/category/exaggerating-the-risks/biorisk/\">series</a> (the 1st post is already out).</p>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "t9hadAK6asjJAKvhC", "postedAt": "2023-08-14T02:31:02.046Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<blockquote>\n<p>predicted the extinction risk until 2100 from engineered pathogens to be 13.5 % (= 0.01/0.074) and 1.82 times (= 0.01/0.0055) that of nuclear war. This is seemingly in contrast with nuclear not being a core area of longtermism (unlike AI and bio).</p>\n</blockquote>\n<p>My impression was that nuclear risk has usually ended up as a somewhat lower priority for EAs because it's less neglected?</p>\n", "parentCommentId": "yk3dLsusfLDYyoosQ", "user": {"username": "Jeff_Kaufman"}}, {"_id": "fRAtedxgeBgNiBTzF", "postedAt": "2023-08-14T13:42:14.593Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>It feels very plausible to me that \"many people know about biorisk thread models\" is the most important lever to impact biorisk. I've heard that many state bioweapons programs were started because states found out that other states thought bioweapons were powerful. If mere rumors caused them to invest millions in bioweapons, then preventing those rumors would have been an immensely powerful intervention, and preventing further such rumors is critically important.</p>", "parentCommentId": null, "user": {"username": "taoroalin@gmail.com"}}, {"_id": "2JyaD57ku6k4ueWoB", "postedAt": "2023-08-14T16:52:09.511Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>Thanks for asking, Jeff!</p><p>According to 80,000 Hours' profiles on <a href=\"https://80000hours.org/problem-profiles/nuclear-security/\">nuclear war</a> and <a href=\"https://80000hours.org/problem-profiles/preventing-catastrophic-pandemics/\">catastrophic pandemics</a>, it looks like scale, neglectedness and solvability play similar roles:</p><ul><li>The scale of nuclear war might be 10 % that of catastrophic pandemics:<ul><li>\"We think the direct existential risk from nuclear war (i.e. not including secondary effects) is less than 0.01%. The indirect existential risk seems around 10 times higher\". So existential nuclear risk is less than 0.1 %, which might be interpreted as 0.01 %?</li><li>\"Overall, we think the risk [from \"existential biological catastrophe\"] is around 0.1%, and very likely to be greater than 0.01%, but we haven't thought about this in detail\".</li></ul></li><li>Catastrophic pandemics might be 3 times as neglected as nuclear war:<ul><li>\"This issue is not as neglected as most other issues we prioritise. Current spending is between $1 billion and $10 billion per year (quality-adjusted).<a href=\"https://80000hours.org/problem-profiles/nuclear-security/#fn-1\"><sup>1</sup></a>\" So maybe 3 billion (geometric mean)?</li><li>\"As a result, our <a href=\"https://80000hours.org/problem-profiles/global-catastrophic-biological-risks/full-report/#gcbrs-may-be-both-neglected-and-tractable\">quality-adjusted estimate</a> suggests that current spending is around $1 billion per year. (For comparison with other significant risks, we estimate that <a href=\"https://80000hours.org/problem-profiles/climate-change/#reasons-not-to-work-on-climate-change\">hundreds of billions per year are spent on climate change</a>, while <a href=\"https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/\">tens of millions are spent on reducing risks from AI</a>.)\"</li></ul></li><li>It sounds like they think reducing the risk from catastrophic pandemics is more tractable:<ul><li>\"Making progress on nuclear security seems somewhat tractable. While many routes to progress face significant political controversy, there may also be some more neglected ways to reduce this risk.<a href=\"https://80000hours.org/problem-profiles/nuclear-security/#fn-2\"><sup>2</sup></a>\"</li><li>\"There are promising existing approaches to improving biosecurity, including both developing technology that could <i>reduce</i> these risks (e.g. better bio-surveillance), and working on strategy and policy to develop plans to prevent and mitigate biological catastrophes.\"</li></ul></li></ul><p>So you may be right that the level of risk is not a major driver for nuclear war not being a core area. However, I guess other organisations believe the bio existential risk to be higher than 80,000 Hours, whereas few will have higher estimates for nuclear existential risk.</p>", "parentCommentId": "t9hadAK6asjJAKvhC", "user": {"username": "vascoamaralgrilo"}}, {"_id": "WDPRd8wKwE5c68PCz", "postedAt": "2023-08-25T14:51:27.379Z", "postId": "3a6QWDhxYTz5dEMag", "htmlBody": "<p>Great post! One thing that came to mind is caution truly is the \"norm\" that is really pointed at when starting doing biosecurity-relevant work in EA, which has had its tradeoffs with me, some of which you've pointed out</p>", "parentCommentId": null, "user": {"username": "kirstenangeles"}}]