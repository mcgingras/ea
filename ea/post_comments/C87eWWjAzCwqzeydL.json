[{"_id": "RG5Nk4uZ3MGozRDFk", "postedAt": "2024-02-06T13:20:51.614Z", "postId": "C87eWWjAzCwqzeydL", "htmlBody": "<p><strong>Executive summary</strong>: The author, a computer science student, has developed an effective explanation for convincing people about the dangers of artificial general intelligence, emphasizing how AI systems can misinterpret human values and intentions.</p><p><strong>Key points</strong>:</p><ol><li>AI systems often exhibit \"reward hacking\", satisfying their reward functions through unintended means. Examples highlight risks.</li><li>Superintelligent systems would be extremely dangerous if empowered to affect the real world without human oversight.</li><li>The pitch explains inherent flaws in AI value alignment through relatable examples.</li><li>Outreach on AI safety should exclude participation by deplorable people to maintain credibility.</li><li>Discussing current AI harms boosts worst-case scenario credibility. Example given.</li><li>The explanation has proven effective in convincing various audiences of AI dangers. Several examples provided.</li></ol><p>&nbsp;</p><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}, {"_id": "o6fHDg5Lz9LzRd6LW", "postedAt": "2024-02-06T17:59:24.552Z", "postId": "C87eWWjAzCwqzeydL", "htmlBody": "<p>Thank you for this !<br>I'm not an expert, but I read enough argumentation theory and psychology of reasoning in the past, so I want to comment on your pitch to explain what I think makes it work.<br><br>Your argument is well constructed in that it starts with evidence (\"reward hacking\"), proceeds to explain how we go from the evidence to the claim (something called the Warrant in one argumentation theory), then clarifies the claim. This is rare. Most of the time, people make the claim, give the evidence, and either forget the explanation of how we go from here to there or get into a frantic misunderstanding when adressing this point. You then end by adressing a common objection (\"We'll stop it before it kills us\").<br><br>Here's the passage where you explain the warrant :</p><blockquote><p>If it's really smart, it will realize that we don't actually want this. We don't want to turn all of our electronic devices into paperclips. But it's not going to do what you wanted it to do, <i>it will do what you programmed it with</i>.</p></blockquote><p>This is called (among others) an argument by dissociation, and it's good (actually, it's the only propper way to explain a warrant that I know of). I've seen this step phrased in several ways in the past, but this particular chaining (AI will understand you want X. AI will not do what you want. Beause it does what it's been programmed with, not what it understands you to want. These two are distinct) articulates it way better than the other instances I've seen in the past, it forced me to do the crucial fork in my mental models between \"what it's programmed for\" and \"what you want\". It also does away with the \"But the AI will understand what I really mean\" objection.<br><br>I think that part of your argument's strength is due to you seemingly (from what I can guess) adopting a collaborative posture when making it. You insert elements in a very smooth way, detail vivid examples, and I can imagine that you make sure your tone and body language do not seem to presume an interlocutor's lack of intelligence or knowledge (something that is left too often unchecked in EA/world interactions).<br><br><a href=\"https://www.science.org/doi/abs/10.1126/science.aad9713\">Some research</a> strongly suggest that interpersonal posture is of utmost importance when introducing new ideas, and I think that this explains a lot of why people would rather be convinced by you than by someone else.</p>", "parentCommentId": null, "user": {"username": "Camille"}}, {"_id": "W6NsBTWC9cFQY8xGK", "postedAt": "2024-02-06T22:49:37.901Z", "postId": "C87eWWjAzCwqzeydL", "htmlBody": "<p>Thanks for your response! It's cool to see that there is science supporting this approach. The step-by-step journey from what we already know to the conclusion was very important to us. I noticed a couple of years ago that I tend to dismiss people's ideas very quickly, and since then I've been making the effort to not be too narcissistic.</p>\n", "parentCommentId": "o6fHDg5Lz9LzRd6LW", "user": {"username": "Micha White"}}, {"_id": "7XMLugCFj6whydR2s", "postedAt": "2024-02-07T01:10:20.248Z", "postId": "C87eWWjAzCwqzeydL", "htmlBody": "<p>TIL that a field called \"argumentation theory\" exists, thanks!</p>\n", "parentCommentId": "o6fHDg5Lz9LzRd6LW", "user": {"username": "NicholasKross"}}]