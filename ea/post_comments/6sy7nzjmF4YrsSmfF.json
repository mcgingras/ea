[{"_id": "kCeuYcsoCC3woaszp", "postedAt": "2024-02-06T13:22:14.536Z", "postId": "6sy7nzjmF4YrsSmfF", "htmlBody": "<p><strong>Executive summary</strong>: The key to AI alignment is formulating it as a translation problem between AI and human models and interests, rather than an oversight or generalisation problem. This perspective clarifies interpretability approaches and suggests incorporating more human-like inductive biases into AI systems.</p><p><strong>Key points</strong>:</p><ol><li>AI alignment requires finding shared models and plans that respect the interests of both humans and AIs.</li><li>Alignment is better framed as a translation problem rather than oversight or generalisation.</li><li>\"Reverse engineering\" interpretability is more productive than mechanistic interpretability.</li><li>Incorporating human inductive biases like the \"consciousness prior\" makes alignment more natural.</li><li>Economic incentives could encourage adopting more human-like AI approaches.</li><li>Cross-organisational causal model sharing could incentivise compact, causal models.</li></ol><p>&nbsp;</p><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}]