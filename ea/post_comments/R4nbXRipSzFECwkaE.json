[{"_id": "JWBrmtfhxrJ8JHxjs", "postedAt": "2022-10-16T07:17:50.875Z", "postId": "R4nbXRipSzFECwkaE", "htmlBody": "<p>Agreed. I'm sure many people on this Forum will be a better fit to answer this question than myself, but in general, your best bet is probably to figure out whether the program(s) and advisor(s) you're applying to work with do work in technical alignment. And mention your interests in alignment if they do, and don't if they don't.</p><p>For example, at Berkeley, CHAI and Jacob Steinhardt's group do work in technical alignment. At Cambridge, David Krueger's lab. I believe there's a handful of others.</p><blockquote><p>or is it to be expected that it has become such a basic thing, that it is just useless (or even negatively) impacting my application?</p></blockquote><p>(Low confidence) I would not guess \"basic\" would be the main issue with mentioning alignment. Bigger problems may include:</p><ul><li>many ML academics are probably skeptical that useful work can be done in alignment, and/or find x-risky arguments kooky.</li><li>I expect there's a negative correlation between technical alignment interest and ML ability, conditional upon applying to ML grad school.</li></ul>", "parentCommentId": "H8WQeNu22wqkhjGjM", "user": {"username": "Linch"}}, {"_id": "RHJ25M6ZAXqv7K4zY", "postedAt": "2022-10-16T07:53:33.644Z", "postId": "R4nbXRipSzFECwkaE", "htmlBody": "", "parentCommentId": "H8WQeNu22wqkhjGjM", "user": {"username": "Franziska Fischer"}}, {"_id": "Fb9Jh4rBdFoMwaTcB", "postedAt": "2022-10-16T16:01:07.928Z", "postId": "R4nbXRipSzFECwkaE", "htmlBody": "<p>This might be helpful context: <a href=\"https://www.lesswrong.com/posts/SqjQFhn5KTarfW8v7/lessons-learned-from-talking-to-greater-than-100-academics\">https://www.lesswrong.com/posts/SqjQFhn5KTarfW8v7/lessons-learned-from-talking-to-greater-than-100-academics</a></p>\n<p>In general my sense is that most potential advisors would be put off discussion of AGI or x-risk. Talking about safety in more mainstream terms might get a better reception, for example Unsolved Problems in ML Safety and X-Risk Analysis for AI Research by Hendrycks et al both present AI safety concerns in a way that might have broader appeal. Another approach would be presenting specific technical challenges that you want to work on, such as ELK or interpretability or OOD robustness, which can interest people on technical grounds even if they don\u2019t share your motivations.</p>\n<p>I don\u2019t mean to totally discourage x-risk discussion, I\u2019m actually writing a thesis proposal right now and trying to figure out how to productively mention my real motivations. I think it\u2019s tough but hopefully you can find a way to make it work.</p>\n", "parentCommentId": null, "user": {"username": "Aidan O'Gara"}}, {"_id": "YZXmJAtNciceYaPxD", "postedAt": "2022-10-16T17:48:29.649Z", "postId": "R4nbXRipSzFECwkaE", "htmlBody": "<p>Anecdote: My grad school personal statement mentioned \"Concrete Problems in AI Safety\" and <i>Superintelligence</i>, though at a fairly vague level about the risks of distributional shift or the like. I got into some pretty respectable programs. I wouldn't take this as strong evidence, of course.</p>", "parentCommentId": null, "user": {"username": "antimonyanthony"}}]