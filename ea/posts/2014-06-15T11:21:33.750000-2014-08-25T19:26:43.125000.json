[{"_id": "fKbiJbu6hwbSCgQ9o", "title": "Scott Alexander \u2014 Meditations on Moloch", "postedAt": "2014-07-30T14:09:10.461Z", "htmlBody": "<p>Scott Alexander's master work. I tried looking for an excerpt to use, but it's hard to quote. Block out some time this evening and read it through if you haven't. It has my high recommendation.</p>", "user": {"username": "jpaddison"}}, {"_id": "CajfSAbMFC6Fnnt3w", "title": "The Moral Value of the Far Future", "postedAt": "2014-07-03T12:43:57.739Z", "htmlBody": "<p><i>Note: The Open Philanthropy Project was </i><a href=\"https://www.openphilanthropy.org/blog/open-philanthropy-project-formerly-givewell-labs\"><i>formerly known as GiveWell Labs</i></a><i>. Before the launch of the Open Philanthropy Project Blog, this post appeared on the </i><a href=\"http://blog.givewell.org/\"><i>GiveWell Blog</i></a><i>. Uses of \u201cwe\u201d and \u201cour\u201d in the below post may refer to the Open Philanthropy Project or to GiveWell as an organization. Additional comments may be available at the </i><a href=\"http://blog.givewell.org/2014/07/03/the-moral-value-of-the-far-future/\"><i>original post</i></a><i>.</i></p><p>A <a href=\"http://lesswrong.com/lw/hx4/four_focus_areas_of_effective_altruism/\">popular idea</a> in the <a href=\"http://blog.givewell.org/2013/08/13/effective-altruism/\">effective altruism community</a> is the idea that <i>most of the people we can help</i> (with our giving, our work, etc.) <i>are people who haven\u2019t been born yet.</i> By working to lower <a href=\"https://www.openphilanthropy.org/blog/potential-global-catastrophic-risk-focus-areas\">global catastrophic risks</a>, speed <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">economic development and technological innovation</a>, and generally improve people\u2019s resources, capabilities, and values, we may have an impact that (even if small today) reverberates for generations to come, helping more people in the future than we can hope to help in the present.</p><p>This belief is sometimes coupled with a belief that the most important goal of an altruist should be to reduce \u201cexistential risk\u201d: the risk of an extreme catastrophe that causes complete human extinction (as, for example, a sufficiently bad <a href=\"https://www.openphilanthropy.org/research/cause-reports/global-catastrophic-risks/biosecurity\">pandemic</a> - or extreme unexpected developments related to <a href=\"https://www.openphilanthropy.org/research/cause-reports/policy/anthropogenic-climate-change\">climate change</a> - could theoretically do), and thus curtails large numbers of future generations.</p><p>We are often asked about our views on these topics, and this post attempts to lay them out. There is not complete internal consensus on these matters, so I speak for myself, though most staff members would accept most of what I write here. In brief:</p><ul><li>I broadly accept the idea that the bulk of our impact may come from effects on future generations, and this view causes me to be more interested in <a href=\"https://www.openphilanthropy.org/blog/scientific-research-funding\">scientific research funding</a>, <a href=\"https://www.openphilanthropy.org/blog/potential-global-catastrophic-risk-focus-areas\">global catastrophic risk mitigation</a>, and other causes outside of aid to the developing-world poor. (If not for this view, I would likely favor the latter and would likely be far more interested in <a href=\"https://www.openphilanthropy.org/research/cause-reports/policy/treatment-animals-industrial-agriculture\">animal welfare</a> as well.) However, I place only limited weight on the specific argument given by Nick Bostrom in <a href=\"http://www.nickbostrom.com/astronomical/waste.html\">Astronomical Waste</a> - that the potential future population is so massive as to <i>clearly</i> (in a probabilistic framework) dwarf all present-day considerations. <a href=\"https://www.openphilanthropy.org/blog/moral-value-far-future#Sec1\">More</a></li><li>I reject the idea that placing high value on the far future - no matter how high the value - makes it clear that one should focus on reducing the risks of catastrophes such as extreme climate change, pandemics, misuse of advanced artificial intelligence, etc. Even one who fully accepts the conclusions of \u201cAstronomical Waste\u201d has good reason to consider focusing on shorter-term, more tangible, higher-certainty opportunities to do good - including donating to GiveWell\u2019s current <a href=\"http://www.givewell.org/charities/top-charities\">top charities</a> and reaping the associated <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a>. <a href=\"https://www.openphilanthropy.org/blog/moral-value-far-future#Sec2\">More</a></li><li>I consider \u201cglobal catastrophic risk reduction\u201d to be a promising area for a philanthropist. As <a href=\"https://www.openphilanthropy.org/blog/potential-global-catastrophic-risk-focus-areas\">discussed previously</a>, we are investigating this area actively. <a href=\"https://www.openphilanthropy.org/blog/moral-value-far-future#Sec3\">More</a></li></ul><p>Those interested in related materials may wish to look at two transcripts of recorded conversations I had on these topics: <a href=\"http://www.jefftk.com/p/flow-through-effects-conversation\">a conversation on flow-through effects with Carl Shulman, Robert Wiblin, Paul Christiano, and Nick Beckstead</a> and <a href=\"http://intelligence.org/2014/01/27/existential-risk-strategy-conversation-with-holden-karnofsky/\">a conversation on existential risk with Eliezer Yudkowsky and Luke Muehlhauser</a>.</p><h1>The importance of the far future</h1><p>As discussed <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">previously,</a> I believe that the general state of the world has improved dramatically over the past several hundred years. It seems reasonable to state that the people who made contributions (large or small) to this improvement have made a major difference to the lives of people living today, and that when all future generations are taken into account, their impact on generations following them could easily dwarf their impact in their own time.</p><p>I believe it is reasonable to expect this basic dynamic to continue, and I believe that there remains huge room for further improvement (possibly dwarfing the improvements we\u2019ve seen to date). I place some probability on <a href=\"https://www.openphilanthropy.org/blog/possible-global-catastrophic-risks\">global upside possibilities</a> including breakthrough technology, space colonization, and widespread improvements in interconnectedness, empathy and altruism. Even if these don\u2019t pan out, there remains a great deal of room for further reduction in poverty and in other causes of suffering.</p><p>In <a href=\"http://www.nickbostrom.com/astronomical/waste.html\">Astronomical Waste</a>, Nick Bostrom makes a more extreme and more specific claim: that the number of human lives possible under space colonization is so great that the mere <i>possibility</i> of a hugely populated future, when considered in an \u201cexpected value\u201d framework, dwarfs all other moral considerations. I see no obvious analytical flaw in this claim, and give it some weight. However, because the argument relies heavily on specific predictions about a distant future, seemingly (as far as I can tell) backed by little other than speculation, I do not consider it \u201crobust,\u201d and so I do not consider it rational to let it play an overwhelming role in my belief system and actions. (More on my epistemology and method for handling non-robust arguments containing massive quantities <a href=\"http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">here</a>.) In addition, if I did fully accept the reasoning of \u201cAstronomical Waste\u201d and evaluate all actions by their far future consequences, it isn\u2019t clear what implications this would have. As discussed below, given our uncertainty about the specifics of the far future and our reasons to believe that doing good in the present day can have substantial impacts on the future as well, it seems possible that \u201cseeing a large amount of value in future generations\u201d and \u201cseeing an overwhelming amount of value in future generations\u201d lead to similar consequences for our actions.</p><h1>Catastrophic risk reduction vs. doing tangible good</h1><p>Many people have cited \u201cAstronomical Waste\u201d to me as evidence that the greatest opportunities for doing good are in the form of reducing the risks of catastrophes such as extreme climate change, pandemics, problematic developments related to artificial intelligence, etc. Indeed, \u201cAstronomical Waste\u201d seems to argue something like this:</p><blockquote><p><i>For standard utilitarians, priority number one, two, three and four should consequently be to reduce existential risk. The utilitarian imperative \u201cMaximize expected aggregate utility!\u201d can be simplified to the maxim \u201cMinimize existential risk!\u201d.</i></p></blockquote><p>I have always found this inference flawed, and in my <a href=\"http://intelligence.org/2014/01/27/existential-risk-strategy-conversation-with-holden-karnofsky/\">recent discussion with Eliezer Yudkowsky and Luke Muehlhauser</a>, it was argued to me that the \u201cAstronomical Waste\u201d essay never meant to make this inference in the first place. The author\u2019s <a href=\"http://www.jetpress.org/volume9/risks.html\">definition of existential risk</a> includes <i>anything that stops humanity far short of realizing its full potential</i> - including, presumably, stagnation in economic and technological progress leading to a long-lived but limited civilization. Under that definition, \u201cMinimize existential risk!\u201d would seem to potentially include any contribution to general human empowerment.</p><p>I have often been challenged to explain how one could possibly reconcile (a) caring a great deal about the far future with (b) donating to one of <a href=\"http://www.givewell.org/charities/top-charities\">GiveWell\u2019s top charities</a>. My general response is that in the face of sufficient <a href=\"http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">uncertainty</a> about one\u2019s options, and lack of conviction that there are good (in the sense of high expected value) opportunities to make an <i>enormous</i> difference, it is rational to try to make a smaller but <a href=\"http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">robustly positive</a> difference, whether or not one can trace a specific causal pathway from doing this small amount of good to making a large impact on the far future. A few brief arguments in support of this position:</p><ul><li>I believe that the track record of \u201ctaking robustly strong opportunities to do \u2018something good\u2019\u2009\u201d is far better than the track record of \u201ctaking actions whose value is contingent on high-uncertainty arguments about where the highest utility lies, and/or arguments about what is likely to happen in the far future.\u201d This is true even when one evaluates track record only in terms of seeming impact on the far future. The developments that seem most positive in retrospect - from large ones like the development of the steam engine to small ones like the many economic contributions that facilitated strong overall growth - seem to have been driven by the former approach, and I\u2019m not aware of many examples in which the latter approach has yielded great benefits.</li><li>I see some sense in which the world\u2019s <i>overall civilizational ecosystem</i> seems to have done a better job optimizing for the far future than any of the world\u2019s <i>individual minds</i>. It\u2019s often the case that people acting on relatively short-term, tangible considerations (especially when they did so with creativity, integrity, transparency, consensuality, and pursuit of gain via value creation rather than value transfer) have done good in ways they themselves wouldn\u2019t have been able to foresee. If this is correct, it seems to imply that one should be focused on \u201cplaying one\u2019s role as well as possible\u201d - on finding opportunities to <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/\">\u201cbeat the broad market\u201d</a> (to do more good than people with similar goals would be able to) rather than pouring one\u2019s resources into the areas that non-robust estimates have indicated as most important to the far future.</li><li>The <i>process</i> of trying to accomplish tangible good can lead to a great deal of learning and unexpected positive developments, more so (in my view) than the process of putting resources into a low-feedback endeavor based on one\u2019s current best-guess theory. In my <a href=\"http://intelligence.org/2014/01/27/existential-risk-strategy-conversation-with-holden-karnofsky/\">conversation with Luke and Eliezer</a>, the two of them hypothesized that the greatest positive benefit of supporting GiveWell\u2019s top charities may have been to raise the profile, influence, and learning abilities of GiveWell. If this were true, I don\u2019t believe it would be an inexplicable stroke of luck for donors to top charities; rather, it would be the sort of development (facilitating feedback loops that lead to learning, organizational development, growing influence, etc.) that is often associated with \u201cdoing something well\u201d as opposed to \u201cdoing the most worthwhile thing poorly.\u201d</li><li>I see multiple reasons to believe that contributing to general human empowerment mitigates global catastrophic risks. I laid some of these out in a <a href=\"https://www.openphilanthropy.org/blog/empowerment-and-catastrophic-risk\">blog post</a> and discussed them further in my <a href=\"http://intelligence.org/2014/01/27/existential-risk-strategy-conversation-with-holden-karnofsky/\">conversation with Luke and Eliezer</a>.</li></ul><p>For one who accepts these considerations, it seems to me that:</p><ul><li>It is not clear whether placing <i>enormous</i> value on the far future ought to change one\u2019s actions from what they would be if one simply placed <i>large</i> value on the far future. In both cases, attempts to reduce global catastrophic risks and otherwise plan for far-off events must be weighed against attempts to do tangible good, and the question of which has more potential to shape the far future will often be a difficult one to answer.</li><li>If one sees few robustly good opportunities to \u201cmake a huge difference to the far future,\u201d the best approach to making a positive far-future difference may be \u201cmake a small but robustly positive difference to the present.\u201d</li><li>One ought to be interested in \u201cunusual, outstanding opportunities to do good\u201d even if they don\u2019t have a clear connection to improving the far future.</li></ul><p>With that said:</p><ul><li>This line of reasoning is <i>not</i> the only or overwhelming consideration in our current <a href=\"http://www.givewell.org/charities/top-charities\">top charity recommendations</a>. As discussed in the previous section, we place some weight on the importance of the far future but believe it would be irrational to let our beliefs about it take on excessive weight in our decision-making. The possibility that arguments about the importance of the far future are simply mistaken, and that the best way to do good is to focus on the present, carries weight.</li><li>I also do not claim that the above reasoning should push all those interested in the far future into nearer-term, higher-certainty actions. People who are well-positioned to take on low-probability, high-upside projects aiming to make a huge difference - especially when their projects are <a href=\"http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">robustly</a> worthwhile and especially when their projects represent promising <a href=\"http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/#Sec5\">novel ideas</a> - should do so. People who have formed the deep understanding necessary to evaluate such projects well should not take us to be claiming that <i>their</i> convictions are irrational given what they know (though we do believe <i>some</i> people form irrationally confident convictions based on speculative arguments). As GiveWell has matured, we\u2019ve become (in my view) much better-positioned to take on such low-probability, high-upside projects; hence our launch of <a href=\"https://www.openphilanthropy.org/blog\">GiveWell Labs</a> and our <a href=\"https://www.openphilanthropy.org/blog/potential-global-catastrophic-risk-focus-areas\">current investigations on global catastrophic risks</a>. The better-informed we become, the more willing we will be to go out on a limb.</li></ul><h1>Global catastrophic risk reduction as a promising area for philanthropy</h1><p>I see global catastrophic risk reduction as a promising area for philanthropy, for many of the reasons laid out in <a href=\"https://www.openphilanthropy.org/blog/potential-global-catastrophic-risk-focus-areas\">a previous post</a>:</p><ul><li>It is a good conceptual fit for philanthropy, which is seemingly better suited than other approaches to working toward diffused benefits over long time horizons.</li><li>Many global catastrophic risks appear to get little attention from philanthropy.</li><li>I place some (though not overwhelming) weight on the argument that the implications of a catastrophe for the far future could be sufficiently catastrophic and long-lasting that even a small mitigation could have huge value.</li></ul><p>I believe that declaring global catastrophic risk reduction to be the <i>clearly most important</i> cause to work on, on the basis of what we know today, would not be warranted. A broad variety of other causes could be superior under reasonable assumptions. <a href=\"https://www.openphilanthropy.org/focus/scientific-research\">Scientific research funding</a> may be far more important to the far future (especially if global catastrophic risks turn out to be relatively minor, or science turns out to be a key lever in mitigating them). Helping low-income people (including via our <a href=\"http://www.givewell.org/charities/top-charities\">top charities</a>) could be the better area to work in if our views regarding the far future are fundamentally flawed, or if opportunities to substantially mitigate global catastrophic risks turn out to be highly limited. <a href=\"https://www.openphilanthropy.org/focus/us-policy\">Working toward better public policy</a> could also have major implications for both the present and the future, and having knowledge of this area could be an important tool no matter what causes we end up working on. More generally, by exploring multiple promising areas, we create better opportunities for \u201cunknown unknown\u201d positive developments, and the discovery of outstanding giving opportunities that are difficult to imagine given our current knowledge. (We also will become more broadly informed, something we believe will be very helpful in pitching funders on the best giving opportunities we can find - whatever those turn out to be.)</p>", "user": {"username": "HoldenKarnofsky"}}, {"_id": "aiNbxdzKC5fbi5mJF", "title": "Why we should err in both directions", "postedAt": "2014-08-21T02:23:06.000Z", "htmlBody": "<html><body><p><em>Crossposted from the <a href=\"http://www.fhi.ox.ac.uk/why-we-should-err-in-both-directions/\">Global Priorities Project</a></em></p>\n<p>This is an introduction to the principle that when we are making decisions under uncertainty, we should choose so that we may err in either direction. We justify the principle, explore the relation with Umeshisms, and look at applications in priority-setting.</p>\n<h2>Some trade-offs</h2>\n<p>How much should you spend on your bike lock? A cheaper lock saves you money at the cost of security.</p>\n<p>&#xA0;</p>\n<p>How long should you spend weighing up which charity to donate to before choosing one? Longer means less time for doing other useful things, but you&#x2019;re more likely to make a good choice.</p>\n<p>How early should you aim to arrive at the station for your train? Earlier means less chance of missing it, but more time hanging around at the station.</p>\n<p>Should you be willing to undertake risky projects, or stick only to safe ones? The safer your threshold, the more confident you can be that you won&#x2019;t waste resources, but some of the best opportunities may have a degree of risk, and you might be able to achieve a lot more with a weaker constraint.</p>\n<h2><strong>The principle</strong></h2>\n<p>We face trade-offs and make judgements all the time, and inevitably we sometimes make bad calls. In some cases we should have known better; sometimes we are just unlucky. As well as trying to make fewer mistakes, we should try to minimise the damage from the mistakes that we do make.</p>\n<p>&#xA0;</p>\n<p>Here&#x2019;s a rule which can be useful in helping you do this:</p>\n<p>When making decisions that lie along a spectrum, you should choose so that you think you have some chance of being off from the best choice in each direction.</p>\n<p>We could call this principle <em>erring in both directions</em>. It might seem counterintuitive -- isn&#x2019;t it worse to not even know what direction you&#x2019;re wrong in? -- but it&#x2019;s based on some fairly straightforward economics. I give a non-technical sketch of a proof at the end, but the essence is: if you&#x2019;re not going to be perfect, you want to be close to perfect, and this is best achieved by putting your actual choice near the middle of your error bar.</p>\n<p>So the principle suggests that you should aim to arrive at the station with a bit of time wasted, but not so much that you won&#x2019;t miss the train even if something goes wrong.</p>\n<h3>Refinements</h3>\n<p>Just saying that you should have some chance of erring in either direction isn&#x2019;t enough to tell you what you should actually choose. It can be a useful warning sign in the cases where you&#x2019;re going substantially wrong, though, and as these are the most important cases to fix it has some use in this form.</p>\n<p>&#xA0;</p>\n<p>A more careful analysis would tell you that at the best point on the spectrum, a small change in your decision produces about as much expected benefit as expected cost. In ideal circumstances we can use this to work out exactly where on the spectrum we should be (in some cases more than one point may fit this, so you need to compare them directly). In practice it is often hard to estimate the marginal benefits and costs well enough for this to be useful approach. So although it is theoretically optimal, you will only sometimes want to try to apply this version.</p>\n<p>Say in our train example that you found missing the train as bad as 100 minutes waiting at the station. Then you want to leave time so that an extra minute of safety margin gives you a 1% reduction in the absolute chance of missing the train.</p>\n<p>For instance, say your options in the train case look like this: \n<table>\n<tbody>\n<tr>\n<td>Safety margin (min)</td>\n<td>1</td>\n<td>2</td>\n<td>3</td>\n<td>4</td>\n<td>5</td>\n<td>6</td>\n<td>7</td>\n<td>8</td>\n<td>9</td>\n<td>10</td>\n<td>11</td>\n<td>12</td>\n<td>13</td>\n<td>14</td>\n<td>15</td>\n</tr>\n<tr>\n<td>Chance of missing train (%)</td>\n<td>50</td>\n<td>30</td>\n<td>15</td>\n<td>8</td>\n<td>5</td>\n<td>3</td>\n<td>2</td>\n<td>1.5</td>\n<td>1.1</td>\n<td>0.8</td>\n<td>0.6</td>\n<td>0.4</td>\n<td>0.3</td>\n<td>0.2</td>\n<td>0.1</td>\n</tr>\n</tbody>\n</table>\nThen the optimal safety margin to leave is somewhere between 6 and 7 minutes: this is where the marginal minute leads to a 1% reduction in the chance of missing the train.</p>\n<h2>Predictions and track records</h2>\n<p>So far, we&apos;ve phrased the idea in terms of the predicted outcomes of actions.</p>\n<p>&#xA0;</p>\n<p>Another more well-known perspective on the idea looks at events that have already happened. For example:</p>\n<ul>\n<li>&#x201C;If you&apos;ve never missed a flight, you&apos;re spending too much time in airports.&#x201D;</li>\n<li><a href=\"http://arcanesentiment.blogspot.co.uk/2010/01/umeshism.html\">&#x201C;If your code never has bugs, you&#x2019;re being too careful.&#x201D;</a></li>\n</ul>\n<p>These formulations, <a href=\"http://www.scottaaronson.com/blog/?p=40\">dubbed &apos;Umeshisms&apos;</a>, only work for decisions that you make multiple times, so that you can gather a track record.</p>\n<p>&#xA0;</p>\n<p>An advantage of applying the principle to track records is that it&#x2019;s more obvious when you&#x2019;re going wrong. Introspection can be hard.</p>\n<p>You can even apply the principle to track records of decisions which don&#x2019;t look like they are choosing from a spectrum. For example it is given as advice in the game of bridge: if you don&#x2019;t sometimes double the stakes on hands which eventually go against you, you&#x2019;re not doubling enough. Although doubling or not is a binary choice, erring in both directions still works because &#x2018;how often to do double&#x2019; is a trait that roughly falls on a spectrum.</p>\n<h2>Failures</h2>\n<p>There are some circumstances where the principle may not apply.</p>\n<p>&#xA0;</p>\n<p>First, if you think the correct point is at one extreme of the available spectrum. For instance nobody says &#x2018;if you&#x2019;re not worried about going to jail, you&#x2019;re not committing enough armed robberies&#x2019;, because we think the best number of armed robberies to commit is probably zero.</p>\n<p>Second, if the available points in the spectrum are discrete and few in number. Take the example of the bike locks. Perhaps there are only three options available: the Cheap-o lock (&#xA3;5), the Regular lock (&#xA3;20), and the Super lock (&#xA3;50). You might reasonably decide on the Regular lock, thinking that maybe the Super lock is better, but that the Cheap-o one certainly isn&#x2019;t. When you buy the Regular lock, you&#x2019;re pretty sure you&#x2019;re not buying a lock that&#x2019;s too tough. But since only two of the locks are good candidates, there is no decision you <em>could </em>make which tries to err in both directions.</p>\n<p>Third, in the case of evaluating track records, it may be that your record isn&#x2019;t long enough to expect to have seen errors in both directions, even if they should both come up eventually. If you haven&#x2019;t flown that many times, you could well be spending the right amount of time -- or even too little -- in airports, even if you&#x2019;ve never missed a flight.</p>\n<p>Finally, a warning about a case where the principle is not supposed to apply. It shouldn&#x2019;t be applied directly to try to equalise the probability of being wrong in either direction, without taking any account of magnitude of loss. So for example if someone says you should err on the side of caution by getting an early train to your job interview, it might look as though that were in conflict with the idea of erring in both directions. But normally what&#x2019;s meant is that you should have a higher probability of failing in one direction (wasting time by taking an earlier train than needed), <em>because</em> the consequences of failing in the other direction (missing the interview) are much higher.</p>\n<h2>Conclusions and applications to prioritisation</h2>\n<p>Seeking to err in both directions can provide a useful tool in helping to form better judgements in uncertain situations. Many people may already have internalised key points, but it can be useful to have a label to facilitate discussion. Additionally, having a clear principle can help you to apply it in cases where you might not have noticed it was relevant.</p>\n<p>&#xA0;</p>\n<p>How might this principle apply to priority-setting? It suggests that:</p>\n<ul>\n<li>You should spend enough time and resources on the prioritisation itself that you think some of time may have been wasted (for example you should spend a while at the end without changing your mind much), but not so much that you are totally confident you have the right answer.</li>\n<li>If you are unsure what discount rate to use, you should choose one so that you think that it could be either too high or too low.</li>\n<li>If you don&#x2019;t know how strongly to weigh fragile cost-effectiveness estimates against more robust evidence, you should choose a level so that you might be over- or under-weighing them.</li>\n<li>When you are providing a best-guess estimate, you should choose a figure which could plausibly be wrong either way.</li>\n</ul>\n<p>And one on track records:</p>\n<ul>\n<li>Suppose you&#x2019;ve made lots of grants. Then if you&#x2019;ve never backed a project which has failed, you&#x2019;re probably too risk-averse in your grantmaking.</li>\n</ul>\n<h2>Appendix: a sketch proof of the principle</h2>\n<p>Assume the true graph of value (on the vertical axis) against the decision you make (on the horizontal axis, representing the spectrum) is smooth, looking something like this:</p>\n<p>&#xA0;</p>\n<p><span><span><img src=\"https://docs.google.com/drawings/d/s4bPrmx_PQtz3TEkCD0Wncg/image?w=624&amp;h=229&amp;rev=45&amp;ac=1\" alt=\"\"></span></span></p>\n<p>The highest value is achieved at d, so this is where you&#x2019;d like to be. But assume you don&#x2019;t know quite where d is. Say your best guess is that d=g. But you think it&#x2019;s quite possible that d&gt;g, and quite unlikely that d&lt;g. Should you choose g?</p>\n<p>Suppose we compare g to g&#x2019;, which is just a little bit bigger than g. If d&gt;g, then switching from g to g&#x2019; would be moving up the slope on the left of the diagram, which is an improvement. If d=g then it would be better to stick with g, but it doesn&#x2019;t make so much difference because the curve is fairly flat at the top. And if g were bigger than d, we&#x2019;d be moving down the slope on the right of the diagram, which is worse for g&#x2019; -- but this scenario was deemed unlikely.</p>\n<p>Aggregating the three possibilities, we found that two of them were better for sticking with g, but in one of these (d=g) it didn&#x2019;t matter very much, and the other (d&lt;g) just wasn&#x2019;t very likely. In contrast, the third case (d&gt;g) was reasonably likely, and noticeably better for g&#x2019; than g. So overall we should prefer g&#x2019; to g.</p>\n<p>In fact we&#x2019;d want to continue moving until the marginal upside from going slightly higher was equal to the marginal downside; this would have to involve a non-trivial chance that we are going too high. So our choice should have a chance of failure in either direction. This completes the (sketch) proof.</p>\n<p><strong>Note</strong>: There was an assumption of smoothness in this argument. I suspect it may be possible to get slightly stronger conclusions or work from slightly weaker assumptions, but I&#x2019;m not certain what the most general form of this argument is. It is often easier to build a careful argument in specific cases.</p>\n<p><em>Acknowledgements: thanks to Ryan Carey, Max Dalton, and Toby Ord for useful comments and suggestions.</em></p></body></html>", "user": {"username": "Owen_Cotton-Barratt"}}, {"_id": "KA8icGDEZynRqqHmg", "title": "Conversation with Holden Karnofsky, Nick Beckstead, and Eliezer Yudkowsky on the \"long-run\" perspective on effective altruism", "postedAt": "2014-08-18T04:30:03.000Z", "htmlBody": "<html><body><p>Earlier this year, I had an email conversation with Holden Karnofsky, Eliezer Yudkowsky, and Luke Muehlhauser about future-oriented effective altruism, as a follow-up to an <a href=\"http://intelligence.org/2014/01/27/existential-risk-strategy-conversation-with-holden-karnofsky/\">earlier conversation</a> Holden had with Luke and Eliezer.</p><p>The conversation is now available <a href=\"http://www.effective-altruism.com/wp-content/uploads/2014/08/HK-NB-EY-LM-conversation-in-Feb-2014-1.1.pdf\">here</a>. My highlights from the conversation:</p><p></p><hr><p></p><p>NICK: I think the case for &#x201C;do the most good&#x201D; coinciding with &#x201C;do what is best in terms of very long-term considerations&#x201D; rests on weaker normative premises than your conversation suggests it does. For example, I don&#x2019;t believe you need the assumption that creating a life is as good as saving a life, or a constant fraction as good as that. I have discussed a more general kind of argument&#x2014;as well as some of the most natural and common alternative moral frameworks I could think of&#x2014;in my dissertation (especially ch. 3 and ch. 5). It may seem like a small point, but I think you can introduce a considerable amount of complicated holistic evaluation into the framework without undermining the argument for focusing primarily on long-term considerations.</p><p>For another point, you can have trajectory changes or more severe &#x201C;flawed realizations&#x201D; that don&#x2019;t involve extinction. E.g., you could imagine a version of climate change where bad management of the problem results in the future being 1% worse forever or you could have a somewhat suboptimal AI that makes the future 1% worse than it could have been (just treat these as toy examples that illustrate a point rather than empirical claims). If you&#x2019;ve got a big enough future civilization, these changes could plausibly outweigh short-term considerations (apart from their long-term consequences) even if you don&#x2019;t think that creating a life is within some constant fraction of saving a life.</p><p>HOLDEN: On your first point - I think you&apos;re right about the *far future* but I have more trouble seeing the connection to *x-risk* (even broadly defined). Placing a great deal of value on a 1% improvement seems to point more in the direction of working toward broad empowerment/improvement and weigh toward e.g. AMF. I think I need to accept the creating/saving multiplier to believe that &quot;all the value comes from whether or not we colonize the stars.&quot;</p><p>NICK: The claim was explicitly meant to be about &quot;very long-term considerations.&quot; I just mean to be speaking to your hesitations about the moral framework (rather than your hesitations about what the moral framework implies).</p><p>I agree that an increased emphasis on trajectory changes/flawed realizations (in comparison with creating extra people) supports putting more emphasis on factors like broad human empowerment relative to avoiding doomsday scenarios and other major global disruptions.</p><p>ELIEZER: How does AMF get us to a 1% better *long-term* future?&#xA0; Are you\nenvisioning something along the lines of &quot;Starting with a 1% more prosperous Earth results in 1% more colonization and hence 1% more utility by the time the stars finally burn out&quot;?</p><p>HOLDEN: I guess so. A 1% better earth does a 1% better job in the SWH transition? I haven&apos;t thought about this much and don&apos;t feel strongly about what I said.</p><p>ELIEZER: SWH?</p><p>HOLDEN: Something Weird Happens - Eliezer&apos;s term for what I think he originally intended Singularity to mean (or how I interpret Singularity).</p><p>(will write more later)</p><p></p><hr><p></p><p>NICK: I feel that the space between your take on astronomical waste and Bostrom&#x2019;s take is smaller than you recognize in this discussion and in discussions we&#x2019;ve had previously. In the grand scheme of things, it seems the position you articulated (under the assumptions that future generations matter in the appropriate way) puts you closer to Bostrom than it does to (say) 99.9% of the population. I think most outsiders would see this dispute as analogous to a dispute between two highly specific factions of Marxism or something. As Eliezer said, I think your disagreement is more about how to apply maxipok than whether maxipok is right (in the abstract).[&#x2026;]</p><p>I think there&#x2019;s an interesting analogy with the animal rights people. Suppose you hadn&#x2019;t considered the long-run consequences of helping people so much and you become convinced that animal suffering on factory farms is of comparable importance to billions of humans being tortured and killed each year, and that getting one person to be a vegetarian is like preventing many humans from being tortured and killed. Given that you accept this conclusion, I think it wouldn&#x2019;t be unreasonable for you to update strongly in favor of factory farming being one of the most high priority areas for doing good in the world, even if you didn&#x2019;t know a great deal about RFMF and so on. Anyway, it does seem pretty analogous in some important ways. This looks to me like a case where some animal rights people did something analogous to the process you critiqued and thereby identified factory farming,</p><p>HOLDEN: Re: Bostrom&apos;s essay - I see things differently. I see &quot;the far future is extremely important&quot; as a reasonably mainstream position. There are a lot of mainstream people who place substantial value on funding and promoting science, for that exact reason. Certainly there are a lot of people who don&apos;t feel this way, and I have arguments with them, but I don&apos;t feel Bostrom&apos;s essay tells us nearly as much when read as agreeing with me. I&apos;d say it gives us a framework that may or may not turn out to be useful.</p><p>So far I haven&apos;t found it to be particularly useful. I think valuing extinction prevention as equivalent to saving something like 5*N lives (N=current global population) leads to most of the same conclusions. Most of my experience with Bostrom&apos;s essay has been people pointing to it as a convincing defense of a much more substantive position.</p><p>I think non-climate-change x-risks are neglected because of how diffuse their constituencies are (the classic issue), not so much because of apathy toward the far future, particularly not from failure to value the far future at [huge number] instead of 5*N.</p><p>NICK: [&#x2026;] Though I&apos;m not particularly excited about refuges, they might be a good test case. I think that if you had this 5N view, refuges would be obviously dumb but if you had the view that I defended in my dissertation then refuges would be interesting from a conceptual perspective.</p><p></p><hr><p></p><p>HOLDEN: One of the things I&apos;m hoping to clarify with my upcoming posts is that my comfort with a framework is not independent of what the framework implies. Many of the ways in which you try to break down arguments do not map well onto my actual process for generating conclusions.</p><p>NICK: I&apos;m aware that this isn&apos;t how you operate. But doesn&apos;t this seem like an &quot;in the trenches&quot; case where we&apos;re trying to learn and clarify our reasoning, and therefore your post would suggest that now is a good time to do engage in sequence thinking?</p><p>HOLDEN: Really good question that made me think and is going to make me edit my post. I concede that sequence thinking has important superiorities for communication; I also think that it COULD be used to build a model of cluster thinking (this is basically what I tried to do in my post - define cluster thinking as a vaguely specified &quot;formula&quot;). One of the main goals of my post is to help sequence thinkers do a better job modeling and explicitly discussing what cluster thinking is doing.</p><p>What&apos;s frustrating to me is getting accused of being evasive, inconsistent, or indifferent about questions like this far future thing; I&apos;d rather be accused of using a process that is hard to understand by its nature (and shouldn&apos;t be assumed to be either rational or irrational; it could be either or a mix).</p><p>Anyway, what I&apos;d say in this case is:\n</p><ul>\n\t<li>I think we&apos;ve hit diminishing returns on examining this particular model of the far future. I&apos;ve named all the problems I see with it; I have no more. I concede that this model doesn&apos;t have other holes that I&apos;ve identified, for the moment. I&apos;ve been wrong before re: thinking we&apos;ve hit diminishing returns before we have, so I&apos;m open to more questions.</li>\n</ul>\n<ul>\n\t<li>In terms of how I integrate the model into my decisions, I cap its signal and give it moderate weight. &quot;Action X would be robustly better if I accepted this model of the far future&quot; is an argument in favor of action X but not a decisive one. This is the bit that I&apos;ve previously had trouble defending as a principled action, and hopefully I&apos;ve made some progress on that front. I don&apos;t intend this statement to cut off discussion on the sequence thinking bit, because more argument along those lines could strengthen the robustness of the argument for me and increase its weight.</li>\n</ul>\nHOLDEN: Say that you buy Apple stock because &quot;there&apos;s a 10% chance that they develop a wearable computer over the next 2 years and this sells over 10x as well as the iPad has.&apos; I short Apple stock because &quot;I think their new CEO sucks.&quot; IMO, it is the case that you made a wild guess about the probability of the wearable computer thing, and it is not the case that I did.<p></p><p>NICK: I think I&apos;ve understood your perspective for a while, I&apos;m mainly talking about how to explain it to people.</p><p>I think this example clarifies the situation. If your P(Apple develops a wearable computer over the next 2 years and this sells over 10x as well as the iPad has) = 10%, then you&apos;d want to buy apple stock. In this sense, if you short Apple stock, you&apos;re committed to P(Apple develops a wearable computer over the next 2 years and this sells over 10x as well as the iPad has) &lt; 10%. In this sense, you often can&apos;t get out of being committed to ranges of subjective probabilities.</p><p>The way you think about it, the cognitive procedure is more like: ask a bunch of questions, give answers to the questions, give weights to your question/answer pairs, make a decision as a result. You&apos;re &quot;relying on an assumption&quot; only if that assumption is your answer to one of the questions and you put a lot of weight on that question/answer pair. Since you just relied on the pair (How good is the CEO?, The CEO sucks), you didn&apos;t rely on a wild guess about P(Apple develops a wearable computer over the next 2 years and this sells over 10x as well as the iPad has). And, in this sense, you can often avoid being committed to subjective probabilities.</p><p>When I first heard you say, &quot;You&apos;re relying on a wild guess,&quot; my initial reaction was something like, &quot;Holden is making the mistake of thinking that his actions don&apos;t commit him to ranges of subjective probabilities (in the first sense). It looks like he hasn&apos;t thought through the Bayesian perspective on this.&quot; I do think this is a real mistake that people make, though they may (often?) be operating more on the kind of basis you have described . I started thinking you had a more interesting perspective when, when I was pressing you on this point, you said something like, &quot;I&apos;m committed to whatever subjective probability I&apos;m committed to on the basis of the decision that&apos;s an outcome of this cognitive procedure.&quot;</p></body></html>", "user": {"username": "Nick_Beckstead"}}, {"_id": "SZbA2AkZR8PKz2tWS", "title": "Strategic considerations about different speeds of AI takeoff", "postedAt": "2014-08-13T00:18:47.000Z", "htmlBody": "<html><body><p><em><strong><em>Crossposted from the&#xA0;<a href=\"http://www.fhi.ox.ac.uk/strategic-considerations-about-different-speeds-of-ai-takeoff/\">Global Priorities Project</a></em></strong></em></p><p><em><strong>Co-written by Owen Cotton-Barratt and Toby Ord</strong></em></p><p>There are several different kinds of artificial general intelligence (AGI) which might be developed, and there are different scenarios which could play out after one of them reaches a roughly human level of ability across a wide range of tasks. We shall discuss some of the implications we can see for these different scenarios, and what that might tell us about how we should act today.</p><p>A key difference between different types of post-AGI scenario is the &#x2018;speed of takeoff&#x2019;. This could be thought of as the time between first reaching a near human-level artificial intelligence and reaching one that far exceeds our capacities in almost all areas (or reaching a world where almost all economically productive work is done by artificial intelligences). In fast takeoff scenarios, this might happen over a scale of months, weeks, or days. In slow takeoff scenarios, it might take years or decades. There has been considerable discussion about which speed of takeoff is more likely, but less discussion about which is more desirable and what that implies.\n</p><h2>Are slow takeoffs more desirable?</h2>\nThere are a few reasons to think that we&#x2019;re more likely to get a good outcome in a slow takeoff scenario.<p></p><p>First, safety work today has an issue of <a href=\"http://www.fhi.ox.ac.uk/the-timing-of-labour-aimed-at-reducing-existential-risk/\">neartsightedness</a>. Since we don&#x2019;t know quite what form artificial intelligence will eventually take, specific work today may end up being of no help on the problem we eventually face. If we had a slow takeoff scenario, there would be a period of time in which AGI safety researchers had a much better idea of the nature of the threat, and were able to optimise their work accordingly. This could make their work several times more valuable.</p><p>Second, and perhaps more crucially, in a slow takeoff the concerns about AGI safety are likely to spread much more widely through society. It is easy to imagine this producing widespread societal support of a level at or exceeding that for work on climate change, because the issue would be seen to be imminent. This could translate to much more work on securing a good outcome -- perhaps hundreds of times the total which had previously been done. Although there are some benefits to have work done serially rather than in parallel, these are likely to be overwhelmed by the sheer quantity of extra high-quality work which would attack the problem. Furthermore, the slower the takeoff, the more this additional work can also be done serially.</p><p>A third key factor is that a slow takeoff seems more likely to lead to a highly multipolar scenario. If AGI has been developed commercially, the creators are likely to licence out copies for various applications. Moreover it could give enough time for competitors to bring alternatives up to speed.</p><p>We don&#x2019;t think it&#x2019;s clear whether multipolar outcomes are overall a good thing, but we note that they have some advantages. In the short term they are likely to preserve something closer to the existing balance of power, which gives more time for work to ensure a safe future. They are additionally less sensitive to the prospect of a treacherous turn or of any single-point failure mode in an AGI.\n</p><h2>Strategic implications</h2>\nIf we think that there will be much more time for safety work in slow takeoff scenarios, there seem to be two main implications:<p></p><p>First, when there is any chance to influence matters, we should generally push towards slow takeoff scenarios. They are likely to have much more safety work done, and this is a large factor which could easily outweigh our other information about the relative desirability of the scenarios.</p><p>Second, we should generally focus safety research today on fast takeoff scenarios. Since there will be much less safety work in total in these scenarios, extra work is likely to have a much larger marginal effect. This can be seen as hedging against a fast takeoff even if we think it is undesirable.</p><p>Overall it seems to us that the AGI safety community has internalised the second point, and sensibly focused on work addressing fast takeoff scenarios. It is less clear that we have appropriately weighed the first point. Either of these points could be strengthened or outweighed by a better understanding of the relevant scenarios.</p><p>For example, it seems that neuromorphic AGI would be much harder to understand and control than an AGI with a much clearer internal architecture. So conditional on a fast takeoff, it would be bad if the AGI were neuromorphic. People concerned with AGI safety have argued against a neuromorphic approach on these grounds. However, precisely because it is opaque, neuromorphic AGI may be less able to perform fast recursive self-improvement, and this would decrease the chance of a fast takeoff. Given how much better a slow takeoff appears, we should perhaps <i>prefer</i> neuromorphic approaches.</p><p>In general, the AGI safety community focuses much of its attention on recursive self-improvement approaches to designing a highly intelligent system. We think that this makes sense in as much as it draws attention to the dangers of fast takeoff scenarios and hedges against being in one, but we would want to take care not to <i>promote</i> the approach for those considering designing an AGI. Drawing attention to the power of recursive self improvement could end up being self-defeating if it encourages people to design such systems, producing a faster takeoff.\nIn conclusion it seems that when doing direct technical safety work, may be reasonable to condition on a fast takeoff, as that is the scenario where our early work matters most. When choosing strategic direction, however, it is a mistake to condition on a fast takeoff, precisely because our decisions may affect the probability of a fast takeoff.</p><p><em>Thanks to Daniel Dewey for conversations and comments.</em></p></body></html>", "user": {"username": "Owen_Cotton-Barratt"}}, {"_id": "SbmJfGk5wH3g2XbXc", "title": "A relatively atheoretical perspective on astronomical waste", "postedAt": "2014-08-06T00:55:16.000Z", "htmlBody": "<html><body><p><em>Crossposted from the&#xA0;<a href=\"http://www.fhi.ox.ac.uk/a-relatively-atheoretical-perspective-on-astronomical-waste/\">Global Priorities Project</a></em>\n</p><h1>Introduction</h1>\nIt is commonly objected that the &#x201C;<a href=\"/ea/5g/a_longrun_perspective_on_strategic_cause/\">long-run</a>&#x201D; perspective on effective altruism rests on esoteric assumptions from moral philosophy that are highly debatable. Yes, the long-term future may overwhelm aggregate welfare considerations, but does it follow that the long-term future is overwhelmingly important? Do I really want my plan for helping the world to rest on the assumption that the benefit from allowing extra people to exist scales linearly with population when large numbers of extra people are allowed to exist?<p></p><p>In <a href=\"http://www.nickbeckstead.com/research\">my dissertation</a> on this topic, I tried to defend the conclusion that the distant future is overwhelmingly important without committing to a highly specific view about population ethics (such as total utilitarianism). I did this by appealing to more general principles, but I did end up delving pretty deeply into some standard philosophical issues related to population ethics. And I don&#x2019;t see how to avoid that if you want to independently evaluate whether it&#x2019;s overwhelmingly important for humanity to survive in the long-term future (rather than, say, just <a href=\"http://lesswrong.com/lw/iao/common_sense_as_a_prior/\">deferring to common sense</a>).</p><p>In this post, I outline a relatively atheoretical argument that affecting long-run outcomes for civilization is overwhelmingly important, and attempt to side-step some of the deeper philosophical disagreements. It won&#x2019;t be an argument that preventing extinction would be overwhelmingly important, but it will be an argument that other changes to humanity&#x2019;s long-term trajectory overwhelm short-term considerations. And I&#x2019;m just going to stick to the moral philosophy here. I will not discuss important issues related to how to handle Knightian uncertainty, &#x201C;robust&#x201D; probability estimates, or the long-term consequences of accomplishing good in the short run. I think those issues are more important, but I&#x2019;m just taking on one piece of the puzzle that has to do with moral philosophy, where I thought I could quickly explain something that may help people think through the issues.</p><p>In outline form, my argument is as follows:\n</p><ol>\n\t<li>In very ordinary resource conservation cases that are easy to think about, it is clearly important to ensure that the lives of future generations go well, and it&#x2019;s natural to think that the importance scales linearly with the number of future people whose lives will be affected by the conservation work.</li>\n\t<li>By analogy, it is important to ensure that, if humanity does survive into the distant future, its trajectory is as good as possible, and the importance of shaping the long-term future scales roughly linearly with the expected number of people in the future.</li>\n\t<li>Premise (2), when combined with the standard set of (admittedly debatable) empirical and decision-theoretic assumptions of the astronomical waste argument, yields the standard conclusion of that argument: shaping the long-term future is overwhelmingly important.</li>\n</ol>\nAs when I have discussed this issue in other contexts (such as Nick Bostrom&#x2019;s papers &#x201C;<a href=\"http://www.nickbostrom.com/astronomical/waste.html\">Astronomical Waste</a>&#x201D; and &#x201C;<a href=\"http://www.existential-risk.org/concept.pdf\">Existential Risk Prevention as Global Priority</a>,&#x201D; and my dissertation) this conversation is going to generally assume that we&#x2019;re talking about good accomplished from an impartial perspective, and will not attend to deontological, virtue-theoretic, or justice-related considerations.\n<h1>A review of the astronomical waste argument and an adjustment to it</h1>\nThe standard version of the astronomical waste argument runs as follows:\n<ol>\n\t<li>The expected size of humanity&apos;s future influence is astronomically great.</li>\n\t<li>If the expected size of humanity&apos;s future influence is astronomically great, then the expected value of the future is astronomically great.</li>\n\t<li>If&#xA0;the expected value of the future is astronomically great, then what matters most is that we maximize humanity&#x2019;s long-term potential.</li>\n\t<li>Some of our actions are expected to reduce existential risk in not-ridiculously-small ways.</li>\n\t<li>If what matters most is that we maximize humanity&#x2019;s future potential and some of our actions are expected to reduce existential risk in not-ridiculously-small ways, what it is best to do is primarily determined by how our actions are expected to reduce existential risk.</li>\n\t<li>Therefore, what it is best to do is primarily determined by how our actions are expected to reduce existential risk.</li>\n</ol>\nI&#x2019;ve <a href=\"http://lesswrong.com/lw/hjb/a_proposed_adjustment_to_the_astronomical_waste/\">argued</a> for adjusting the last three steps of this argument in the following way:\n<p>4&#x2019;. &#xA0; Some of our actions are expected to change our development trajectory in not-ridiculously-small ways.</p>\n<p>5&#x2019;. &#xA0; If what matters most is that we maximize humanity&#x2019;s future potential and some of our actions are expected to change our development trajectory in not-ridiculously-small ways, what it is best to do is primarily determined by how our actions are expected to change our development trajectory.</p>\n<p>6&#x2019;. &#xA0; Therefore, what it is best to do is primarily determined by how our actions are expected to change our development trajectory.</p>\nThe basic thought here is that what the astronomical waste argument really shows is that future welfare considerations swamp short-term considerations, so that long-term consequences for the distant future are overwhelmingly important in comparison with purely short-term considerations (apart from <a href=\"http://www.jefftk.com/p/flow-through-effects-conversation\">long-term consequences</a> that short-term consequences may produce).\n<h1>Astronomical waste may involve changes in quality of life, rather than size of population</h1>\nOften, the astronomical waste argument is combined with the idea that the best way to minimize astronomical waste is to minimize the probability of pre-mature human extinction. How important it is to prevent pre-mature human extinction is a subject of philosophical <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/1468-0114.00150/abstract\">debate</a>, and the debate largely rests on whether it is important to allow large numbers of people to exist in the future. So when someone complains that the astronomical waste argument rests on esoteric assumptions about moral philosophy, they are implicitly objecting to premise (2) or (3). They are saying that even if human influence on the future is astronomically great, maybe changing how well humanity exercises its long-term potential isn&#x2019;t very important because maybe it isn&#x2019;t important to ensure that there are a large number of people living in the future.<p></p><p>However, the concept of existential risk is wide enough to include any drastic curtailment to humanity&#x2019;s long-term potential, and the concept of a &#x201C;trajectory change&#x201D; is wide enough to include any small but important change in humanity&#x2019;s long-term development. And the value of these existential risks or trajectory changes need not depend on changes in the population. For example,\n</p><ul>\n\t<li>In &#x201C;<a href=\"http://www.nickbostrom.com/fut/evolution.html\">The Future of Human Evolution</a>,&#x201D; Nick Bostrom discusses a scenario in which evolutionary dynamics result in substantial decreases in quality of for all future generations, and the main problem is not a population deficit.</li>\n\t<li>Paul Christiano <a href=\"http://rationalaltruist.com/2014/05/14/machine-intelligence-and-capital-accumulation/\">outlined</a> long-term resource inequality as a possible consequence of developing advanced machine intelligence.</li>\n\t<li>I discussed various specific trajectory changes in a <a href=\"http://lesswrong.com/lw/hjb/a_proposed_adjustment_to_the_astronomical_waste/92m0\">comment</a> on an essay mentioned above.</li>\n</ul>\n<h1>There is limited philosophical debate about the importance of changes in the quality of life of future generations</h1>\nThe main group of people who deny that it is important that future people exist have &#x201C;person-affecting views.&#x201D; These people claim that if I must choose between outcome A and outcome B, and person X exists in outcome A but not outcome B, it&#x2019;s not possible to affect person X by choosing outcome A rather than B. Because of this, they claim that causing people to exist can&#x2019;t benefit them and isn&#x2019;t important. I think this view suffers from fatal objections which I have discussed in chapter 4 of my dissertation, and you can check that out if you want to learn more. But, for the sake of argument, let&#x2019;s agree that creating &#x201C;extra&#x201D; people can&#x2019;t help the people created and isn&#x2019;t important.<p></p><p>A puzzle for people with person-affecting views goes as <a href=\"http://plato.stanford.edu/entries/nonidentity-problem/\">follows</a>:\n</p><blockquote>Suppose that agents as a community have chosen to deplete rather than conserve certain resources. The consequences of that choice for the persons who exist now or will come into existence over the next two centuries will be &#x201C;slightly higher&#x201D; than under a conservation alternative (Parfit 1987, 362; see also Parfit 2011 (vol. 2), 218). Thereafter, however, for many centuries the quality of life would be much lower. &#x201C;The great lowering of the quality of life must provide some moral reason not to choose Depletion&#x201D; (Parfit 1987, 363). Surely agents ought to have chosen conservation in some form or another instead. But note that, at the same time, depletion seems to harm&#xA0;<em>no one</em>. While distant future persons, by hypothesis, will suffer as a result of depletion, it is also true that for each such person a conservation choice (very probably) would have changed the timing and manner of the relevant conception. That change, in turn, would have changed the identities of the people conceived and the identities of the people who eventually exist. Any suffering, then, that they endure under the depletion choice would seem to be unavoidable if those persons are ever to exist at all. Assuming (here and throughout) that that existence is worth having, we seem forced to conclude that depletion does not harm, or make things worse for, and is not otherwise &#x201C;bad for,&#x201D; anyone at all (Parfit 1987, 363). At least: depletion does not harm, or make things worse for, and is not &quot;bad for,&quot; anyone who does or will exist&#xA0;<em>under the depletion choice</em>.</blockquote>\nThe seemingly natural thing to say if you have a person-affecting view is that because conservation doesn&#x2019;t benefit anyone, it isn&#x2019;t important. But this is a very strange thing to say, and people having this conversation generally recognize that saying it involves biting a bullet. The general tenor of the conversation is that conservation is obviously important in this example, and people with person-affecting views need to provide an explanation consonant with that intuition.<p></p><p>Whatever the ultimate philosophical justification, I think we should say that choosing conservation in the above example is important, and this has something to do with the fact that choosing conservation has consequences that are relevant to the quality of life of many future people.\n</p><h1>Intuitively, giving N times as many future people higher quality of life is N times as important</h1>\nSuppose that conservation would have consequences relevant to 100 times as many people in case A than it would in case B. How much more important would conservation be in case A? Intuitively, it would be 100 times more important. This generally fits with Holden Karnofsky&#x2019;s <a href=\"http://www.givewell.org/modeling-extreme-model-uncertainty\">intuition</a> that a 1/N probability of saving N lives is about as important as saving one life, for any N:\n<blockquote>I wish to be the sort of person who would happily pay $1 for a robust (reliable, true, correct) 10/N probability of saving N lives, for astronomically huge N - while simultaneously refusing to pay $1 to a random person on the street claiming s/he will save N lives with it.</blockquote>\nMore generally, we could say:\n<p>Principle of Scale: Other things being equal, it is N times better (in itself) to ensure that N people in some position have higher quality of life than other people who would be in their position than it is to do this for one person.</p>\nI had to state the principle circuitously to avoid saying that things like conservation programs could &#x201C;help&#x201D; future generations, because according to people with person-affecting views, if our &quot;helping&quot; changes the identities of future people, then we aren&apos;t &quot;helping&quot; anyone and that&apos;s&#xA0;relevant. If I had said it in ordinary language, the principle would have said, &#x201C;If you can help N people, that&#x2019;s N times better than helping one person.&#x201D; The principle could use some tinkering to deal with concerns about equality and so on, but it will serve well enough for our purposes.<p></p><p>The Principle of Scale may seem obvious, but even it would be debatable. You wouldn&#x2019;t find philosophical agreement about it. For example, some philosophers who claim that additional lives have diminishing marginal value would claim that in situations where many people already exist, it matters much less if a person is helped. I attack these perspectives in chapter 5 of my dissertation, and you can check that out if you want to learn more. But, in any case, the Principle of Scale does seem pretty compelling&#x2014;especially if you&#x2019;re the kind of person that doesn&#x2019;t have time for esoteric debates about population ethics&#x2014;so let&#x2019;s run with it.</p><p>Now for the most questionable steps: Let&#x2019;s assume with the astronomical waste argument that the expected number of future people is overwhelming, and that it is possible to improve the quality of life for an overwhelming number of future people through forward-thinking interventions. If we combine this with the principle from the last paragraph and wave our hands a bit, we get the conclusion that shifting quality of life for an overwhelming number of future people is overwhelmingly more important than any short term consideration. And that is very close to what the long-run perspective says about helping future generations, though importantly different because this version of the argument might not put weight on preventing extinction. (I say &#x201C;might not&#x201D; rather than &#x201C;would not&#x201D; because if you disagree with the people with person-affecting views but accept the Principle of Scale outlined above, you might just accept the usual conclusion of the astronomical waste argument.)\n</p><h1>Does the Principle of Scale break down when large numbers are at stake?</h1>\nI have no argument that it doesn&#x2019;t, but I note that (i) this wasn&#x2019;t Holden Karnofsky&#x2019;s intuition about saving N lives, (ii) it isn&#x2019;t mine, and (iii) I don&#x2019;t really see a compelling justification for it. The main reason I can think of for wanting it to break down is not liking the conclusion that affecting long-run outcomes for humanity is overwhelmingly important in comparison with short-term considerations.&#xA0; If you really want to avoid the conclusion that shaping the long-term future is overwhelmingly important, I believe it would be better to accommodate this idea by appealing to other perspectives and a framework for integrating the insights of different perspectives&#x2014;such as <a href=\"http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">the one that Holden has talked about</a>&#x2014;rather than altering this perspective. For such people, my hope would be that reading this post would cause you to put more weight on the perspectives that place great importance on the future.\n<h1>Summary</h1>\nTo wrap up, I&#x2019;ve argued that:\n<ol>\n\t<li>Reducing astronomical waste need not involve preventing human extinction&#x2014;it can involve other changes in humanity&#x2019;s long-term trajectory.</li>\n\t<li>While not widely discussed, the Principle of Scale is fairly attractive from an atheoretical standpoint.</li>\n\t<li>The Principle of Scale&#x2014;when combined with other standard assumptions in the literature on astronomical waste&#x2014;suggests that some trajectory changes would be overwhelmingly important in comparison with short-term considerations. It could be accepted by people who have person-affecting views or people who don&#x2019;t want to get too bogged down in esoteric debates about moral philosophy.</li>\n</ol>\nThe perspective I&#x2019;ve outlined here is still philosophically controversial, but it is at least somewhat independent of the standard approach to astronomical waste. Ultimately, any take on astronomical waste&#x2014;including ignoring it&#x2014;will be committed to philosophical assumptions of some kind, but perhaps the perspective outlined would be accepted more widely, especially by people with temperaments consonant with effective altruism, than perspectives relying on more specific theories or a larger number of principles.<p></p></body></html>", "user": {"username": "Nick_Beckstead"}}, {"_id": "3a3viueN5DA2jwxCu", "title": "Agricultural research and development", "postedAt": "2014-07-30T21:38:04.000Z", "htmlBody": "<html><body><p><em>Crossposted from the <a href=\"http://www.givingwhatwecan.org/blog/2014-07-25/agricultural-research-and-development\">Giving What We Can blog</a></em></p><p><strong>Foreword</strong>: <em>The Copenhagen Consensus and other authors have highlighted the potential of agricultural R&amp;D as a high-leverage opportunity. This was enough to get us interested in understanding the area better, so we asked David Goll, a Giving What We Can member and a professional economist, to investigate how it compares to our existing recommendations.</em> --&#xA0;<strong>Owen Cotton-Barratt, Director of Research for Giving What We Can</strong></p><p>&#xA0;</p><p>Around one in every eight people suffers from chronic hunger, according to the Food and Agricultural Organisation&#x2019;s most recent estimates (FAO, 2013). Two billion suffer from micronutrient deficiencies. One quarter of children are stunted. Increasing agricultural yields and therefore availability of food will be essential in tackling these problems, which are likely to get worse as population and income growth place ever greater pressure on supply. To some extent, yield growth can be achieved through improved use of existing technologies. Access to and use of irrigation, fertilizer and agricultural machinery remains limited in some developing countries. However, targeted research and development will also be required to generate new technologies (seeds, animal vaccines and so on) that allow burgeoning food demand to be met.</p><p>Agricultural research and development encompasses an extremely broad range of activities and potential innovations. A 2008 paper issued by Consultative Group on International Agricultural Research (von Braun et al., 2008), an international organization that funds and coordinates agricultural research, identifies 14 &#x2018;best bets&#x2019;. These include developing hybrid and inbred seeds with improved yield potential, better resistance to wheat rust, increased drought tolerance and added nutritional value, but also encompasses the development new animal vaccines, better fertilizer use and improved processing and management techniques for fisheries.</p><p>Notable successes in seed development seem to have generated immense social benefit. The high-yielding varieties that spread through the &#x2018;Green Revolution&#x2019; are often credited with driving a doubling of rice and wheat yields in Asia from the late 60s to the 90s, saving hundreds of millions of people people from famine (see, for instance, Economist, 2014). Given the prevalence of hunger and the high proportion of the extremely poor that work as farmers, agricultural research and development seems to offer a potential opportunity for effective altruism.</p><p>Existing benefit-cost estimates are promising, though not spectacular. The Copenhagen Consensus project ranked R&amp;D to increase yield enhancements as the sixth most valuable social investment available, behind deworming and micronutrient interventions but ahead of popular programmes such as conditional cash transfers for education (Copenhagen Consensus, 2012).</p><p>The calculations that fed into this decision were based on two main categories of benefit. First, higher yield seeds allow production of larger quantities of agricultural output at a lower cost, bolstering the income of farmers. Around 70 per cent of the African labour-force work in agriculture, many in smallholdings that generate little income above subsistence (IFPRI, 2012). Boosting gains from agriculture could clearly provide large benefits for many of the worst off. Second, decreased costs of production lead to lower prices for food, allowing consumers to purchase more or freeing up their income to be spent elsewhere.</p><p>Projecting out to 2050, these two types of benefit alone are expected to outweigh the costs of increased R&amp;D by 16 to 1 (Hoddinott et al., 2012). By comparison, the benefit-cost ratios estimated within the same project for salt iodization (a form of micronutrient supplement) range between 15 to 1 and 520 to 1, with the latest estimates finding a benefit-cost ratio of 81 to 1 (Hoddinott et al., 2012), and most of the estimates reported to the Copenhagen Consensus panel for the benefit-cost ratio of conditional cash transfers for education fall between 10 to 1 and 2 to 1 (Orazem, 2012). Using a very crude method, we can also convert the benefit-cost ratios into approximate QALY terms. Using a QALY value of three times annual income and taking the income of the beneficiaries to be $4.50 a day (around average income per capita in Sub-Saharan Africa), agricultural R&amp;D is estimated to generate a benefit equivalent to one QALY for every $304.</p><p>Other types of benefit were not tabulated in the Copenhagen Consensus study, but should also be high. Strains that are resistant to drought, for instance, could greatly reduce year-to-year variation in crop yields. More resilient seeds could mitigate the negative effects of climate change on agriculture. Lower food prices may lead to better child nutrition, with life-long improved health and productivity. Finally, higher yields may decrease the potential for conflict due to the pressure on limited land, food and water resources resulting from climate change and population growth. Each of these benefits alone may justify the costs of research and development but, with our limited knowledge, they are not easily quantified.</p><p>The high benefit-cost ratio found by the Copenhagen Consensus team is broadly consistent with other literature. Meta-analysis of 292 academic studies on this topic has found that the median rate of return of agricultural R&amp;D is around 44% (Alston et al., 2000). A rate of return, in this sense, indicates the discount rate at which the costs of an investment are equal to the benefits &#x2013; rather like the interest rate on a bank account. More recent studies, focusing on research in Sub-Saharan Africa, have found aggregate returns of 55% (Alene, 2008).</p><p>Unfortunately, the rate of return on investment is not directly comparable to a benefit-cost ratio; the methodology applied often deviates from the welfare based approach applied by the Copenhagen Consensus team and the two numbers cannot be accurately converted into similar terms. Nonetheless, a crude conversion method can be applied to reach a ballpark estimate of the benefit-cost ratio implied by these studies. Assuming a marginal increase in spending on research is borne upfront and that research generates a constant stream of equal benefits each year from then on, the benefit-cost ratio for an investment with a 44% rate of return at a 5% discount rate is 9 to 1.</p><p>There are, however, at least two reasons to treat these high benefit-cost estimates with skepticism.</p><p>First, estimating the effect of research and development is difficult. One problem is attribution. Growth in yields can be observed as can spending on research and development, but it is much more difficult to observe which spending on research led to which increase in yields. If yields grew last year in Ethiopia, was this the result of research that occurred two years ago or ten years ago? Were the improved yields driven by spending on research within Ethiopia, or was it a spillover from research conducted elsewhere in the region or, even, research conducted on another continent? Estimating the effect of R&amp;D spend requires researchers to adopt a specific temporal and spatial model dictating which expenditures can effect which yields in which countries. Teasing out causality can therefore be tricky, and some studies have suggested that inappropriate attribution may have led to systematic bias in the available estimates (e.g. Alston et al., 2009).</p><p>Another problem is cherry picking. Estimates garnered from meta-analysis are likely to be upwardly biased because studies are much more likely to be conducted on R&amp;D programmes that are perceived to be successful. Failed programmes, on the other hand, are likely to be ignored and, as a result, the research may paint an overly optimistic picture of the potential impact of R&amp;D.</p><p>Second, for new technologies to have an impact on the poor, they need to be widely adopted. This step should not be taken for granted. Adoption rates for improved varieties of crops remain low throughout Africa; farmer-saved seeds, which are unlikely to be improved, account for around 80 per cent of planted seeds in Africa compared to a global average of 35 per cent (AGRA, 2013). To some extent, this is because previous research has been poorly targeted at regional needs. The high-yield varieties developed during the Green Revolution require irrigation or very high levels of rainfall. New seed development was focused on wheat and rice, rather than alternative crops such as sorghum, cassava and millet. High yielding varieties required extensive fertilizer use. All of these features rendered them unsuitable for the African context, and explain why it was not easy to replicate the Asian success story elsewhere (Elliot, 2010).</p><p>However, there are more structural features of many developing countries that will limit adoption. Lack of available markets for surplus production can mean that smallholders can see limited benefit from larger harvests, especially when new seeds are costly and require additional labour and expensive fertilizer. Weak property rights undermine incentives to invest, given that farmers may be unable to hold on to their surplus crop or sell it at a fair price. Unavailability of credit means that, even when it makes good economic sense for farmers to invest in improved seeds, they may not be able to raise the initial capital required. The benefit-cost estimates discussed above, based on a synthesis of evidence from a diverse set of contexts, may underestimate the difficulties with adoption in more challenging countries.</p><p>Even in Asia during the Green Revolution, high-yield varieties were adopted first and foremost by large agricultural interests rather than smallholders (Wiggins et al., 2013). If this was the case for newly developed seeds, the impact on the poorest would be more limited than suggested in the Copenhagen Consensus study. They could still benefit from lower food prices and increased employment in the agricultural sector, but in extreme scenarios smallholders may even lose out due to low cost competition from larger farms that adopt new seeds.</p><p>In combination, the difficulties with estimating the effects of R&amp;D and the potential barriers to adoption suggest that the estimated benefit-cost ratios reported earlier are likely to be upwardly biased. The benefit-cost ratios estimated are also lower than those associated with Giving What We Can&#x2019;s currently recommended charities. For instance, the $304 per QALY estimate based on the Copenhagen Consensus benefit-cost ratio, which appears to be at the higher end of the literature, compares unfavourably to GiveWell&#x2019;s baseline estimate of $45 to $115 per DALY for insecticide treated bednets (GiveWell, 2013). The benefit-cost ratios also appear to be lower than those associated with micronutrient supplements, as discussed earlier. While there are significant benefits that remain unquantified within agricultural R&amp;D, the same is also true for interventions based on bednet distribution, deworming and micronutrient supplements. As a result, while this area could yield individual high impact opportunities, the literature as it stands does not seem to support the claim that agricultural R&amp;D is likely to be more effective than the best other interventions.\n</p><h2>References</h2>\n<ul>\n\t<li>Food and Agricultural Organisation,&#x2019;The State of Food and Agriculture 2013&#x2019; (2013)</li>\n\t<li>von Braun, J., Fan, S., Meinzen-Dick, R., Rosegrant, M. and Nin Pratt, A., &#x2018;What to Expect from Scaling Up CGIAR Investments and &#x2018;Best Bet&#x2019; Programs&#x2019; (2008)</li>\n\t<li>Copenhagen Consensus, &#x2018;Expert Panel Findings&#x2019; (2012)</li>\n\t<li>Hoddinott, J., Rosegrant, M. and Torero, M. &#x2018;Investments to reduce hunger and undernutrition&#x2019; (2012)</li>\n\t<li>Orazem, P. &#x2018;The Case for Improving School Quality and Student Health as a Development Strategy&#x2019; (2012)</li>\n\t<li>Alliance for Green Revolution in Africa, &#x2018;Africa Agriculture Status Report 2013: Focus on Staple Crops&#x2019;, (2013)</li>\n\t<li>International Food Policy Research Institute, &#x2018;2012 Global Food Policy Report&#x2019;, (2012)</li>\n\t<li>Elliot, K., &#x2018;Pulling Agricultural Innovation and the Market Together&#x2019;, (2010)</li>\n\t<li>Wiggins, S., Farrington, J., Henley, G., Grist, N. and Locke, A. &#x2018;Agricultural development policy: a contemporary agenda&#x2019; (2013)</li>\n\t<li>Givewell, &#x2018;Mass distribution of long-lasting insecticide-treated nets (LLINs)&#x2019;, 2013, <a href=\"http://www.givewell.org/international/technical/programs/insecticide-treated-nets\">http://www.givewell.org/international/technical/programs/insecticide-treated-nets</a>, retrieved July 10th 2014</li>\n\t<li>The Economist, &#x2018;A bigger rice bowl&#x2019;, May 10th 2014</li>\n</ul><p></p></body></html>", "user": {"username": "David_Goll"}}, {"_id": "yHwFesN6ScWJXXMMA", "title": "How to treat problems of unknown difficulty", "postedAt": "2014-07-30T02:57:26.000Z", "htmlBody": "<html><body><p><em>Crossposted from the&#xA0;<a href=\"http://www.fhi.ox.ac.uk/how-to-treat-problems-of-unknown-difficulty/\">Global Priorities Project</a></em></p><p>This is the first in a series of posts which take aim at the question: how should we prioritise work on problems where we have very little idea of our chances of success. In this post we&#x2019;ll see some simple models-from-ignorance which allow us to produce some estimates of the chances of success from extra work. In later posts we&#x2019;ll examine the counterfactuals to estimate the value of the work. For those who prefer a different medium, I gave <a href=\"http://www.gooddoneright.com/#!owen-cotton-barratt/c6xs\">a talk on this topic</a> at the Good Done Right conference in Oxford this July.\n</p><h2>Introduction</h2>\nHow hard is it to build an economically efficient fusion reactor? How hard is it to prove or disprove the Goldbach conjecture? How hard is it to produce a machine superintelligence? How hard is it to write down a concrete description of our values?<p></p><p>These are all hard problems, but we don&#x2019;t even have a good idea of just how hard they are, even to an order of magnitude. This is in contrast to a problem like giving a laptop to every child, where we know that it&#x2019;s hard but we could produce a fairly good estimate of how much resources it would take.</p><p>Since we need to make choices about how to prioritise between work on different problems, this is clearly an important issue. We can prioritise using benefit-cost analysis, choosing the projects with the highest ratio of future benefits to present costs. When we don&#x2019;t know how hard a problem is, though, our ignorance makes the size of the costs unclear, and so the analysis is harder to perform. Since we make decisions anyway, we are implicitly making some judgements about when work on these projects is worthwhile, but we may be making mistakes.</p><p>In this article, we&#x2019;ll explore practical epistemology for dealing with these problems of unknown difficulty.\n</p><h3>Definition</h3>\nWe will use a simplifying model for problems: that they have a critical threshold <i>D</i> such that the problem will be completely solved when <i>D</i> resources are expended, and not at all before that. We refer to this as the <i>difficulty</i> of the problem. After the fact the graph of success with resources will look something like this:<p></p><p><img title=\"\" src=\"https://docs.google.com/drawings/image?id=sjmeN788LI628vjzrvHe9aA&amp;rev=1&amp;h=193&amp;w=620&amp;ac=1\" alt=\"\">\n</p><p>Of course the assumption is that we don&#x2019;t know <i>D</i>. So our uncertainty about where the threshold is will smooth out the curve in expectation. Our expectation beforehand for success with resources will end up looking something like this:</p>\n<p><img title=\"\" src=\"https://docs.google.com/drawings/image?id=s4UN1-jqnB8OVgxnGYeZYOw&amp;rev=1&amp;h=193&amp;w=620&amp;ac=1\" alt=\"\"></p>\n<p>Assuming a fixed difficulty is a simplification, since of course resources are not all homogenous, and we may get lucky or unlucky. I believe that this is a reasonable simplification, and that taking these considerations into account would not change our expectations by much, but I plan to explore this more carefully in a future post.</p><p></p><p></p><h2>What kind of problems are we looking at?</h2>\nWe&#x2019;re interested in one-off problems where we have a lot of uncertainty about the difficulty. That is, the kind of problem we only need to solve once (answering a question a first time can be Herculean; answering it a second time is trivial), and which may not easily be placed in a reference class with other tasks of similar difficulty. Knowledge problems, as in research, are a central example: they boil down to finding the answer to a question. The category might also include trying to effect some systemic change (for example by political lobbying).<p></p><p>This is in contrast to engineering problems which can be reduced down, roughly, to performing a known task many times. Then we get a fairly good picture of how the problem scales. Note that this includes some knowledge work: the &#x201C;known task&#x201D; may actually be different each time. For example, proofreading two pages of text is quite the same, but we have a fairly good reference class so we can estimate moderately well the difficulty of proofreading a page of text, and quite well the difficulty of proofreading a 100,000-word book (where the length helps to smooth out the variance in estimates of individual pages).</p><p>Some knowledge questions can naturally be broken up into smaller sub-questions. However these typically won&#x2019;t be a tight enough class that we can use this to estimate the difficulty of the overall problem from the difficult of the first few sub-questions. It may well be that one of the sub-questions carries essentially all of the difficulty, so making progress on the others is only a very small help.\n</p><h2>Model from extreme ignorance</h2>\nOne approach to estimating the difficulty of a problem is to assume that we understand essentially nothing about it. If we are completely ignorant, we have no information about the scale of the difficulty, so we want a scale-free prior. This determines that the prior obeys a power law. Then, we update on the amount of resources we have already expended on the problem without success. Our posterior probability distribution for how many resources are required to solve the problem will then be a <a href=\"http://en.wikipedia.org/wiki/Pareto_distribution\">Pareto distribution</a>. (Fallenstein and Mennen <a href=\"http://intelligence.org/files/PredictingAGI.pdf\">proposed this model</a> for the difficulty of the problem of making a general-purpose artificial intelligence.)<p></p><p>There is still a question about the shape parameter of the Pareto distribution, which governs how thick the tail is. It is hard to see how to infer this from a priori reasons, but we might hope to estimate it by generalising from a very broad class of problems people have successfully solved in the past.</p><p>This idealised case is a good starting point, but in actual cases, our estimate may be wider or narrower than this. Narrower if either we have some idea of a reasonable (if very approximate) reference class for the problem, or we have some idea of the rate of progress made towards the solution. For example, assuming a Pareto distribution implies that there&#x2019;s always a nontrivial chance of solving the problem at any minute, and we may be confident that we are not that close to solving it. Broader because a Pareto distribution implies that the problem is certainly solvable, and some problems will turn out to be impossible.</p><p>This might lead people to criticise the idea of using a Pareto distribution. If they have enough extra information that they don&#x2019;t think their beliefs represent a Pareto distribution, can we still say anything sensible?\n</p><h2>Reasoning about broader classes of model</h2>\nIn the previous section, we looked at a very specific and explicit model. Now we take a step back. We assume that people will have complicated enough priors and enough minor sources of evidence that it will in practice be impossible to write down a true distribution for their beliefs. Instead we will reason about some properties that this true distribution should have.<p></p><p>The cases we are interested in are cases where we do not have a good idea of the order of magnitude of the difficulty of a task. This is an imprecise condition, but we might think of it as meaning something like:</p><p>There is no difficulty X such that we believe the probability of D lying between X and 10X is more than 30%.</p><p>Here the &#x201C;30%&#x201D; figure can be adjusted up for a less stringent requirement of uncertainty, or down for a more stringent one.</p><p>Now consider what our subjective probability distribution might look like, where difficulty lies on a logarithmic scale. Our high level of uncertainty will smooth things out, so it is likely to be a reasonably smooth curve. Unless we have specific distinct ideas for how the task is likely to be completed, this curve will probably be unimodal. Finally, since we are unsure even of the order of magnitude, the curve cannot be too tight on the log scale.</p><p>Note that this should be our <i>prior </i>subjective probability distribution: we are gauging how hard we would have thought it was before embarking on the project. We&#x2019;ll discuss below how to update this in the light of information gained by working on it.</p><p>The distribution might look something like this:\n</p><p><img title=\"\" src=\"https://docs.google.com/drawings/image?id=sbp3WocPE859Y40AHunc8ew&amp;rev=1&amp;h=235&amp;w=620&amp;ac=1\" alt=\"\"></p>\n<p>In some cases it is probably worth trying to construct an explicit approximation of this curve. However, this could be quite labour-intensive, and we usually have uncertainty even about our uncertainty, so we will not be entirely confident with what we end up with.</p>\nInstead, we could ask what properties tend to hold for this kind of probability distribution. For example, one well-known phenomenon which is roughly true of these distributions but not all probability distributions is <a href=\"http://en.wikipedia.org/wiki/Benford&apos;s_law\">Benford&#x2019;s law</a>.\n<h3>Approximating as locally log-uniform</h3>\nIt would sometimes be useful to be able to make a simple analytically tractable approximation to the curve. This could be faster to produce, and easily used in a wider range of further analyses than an explicit attempt to model the curve exactly.<p></p><p>As a candidate for this role, we propose working with the assumption that the distribution is locally flat. This corresponds to being log-uniform. The smoothness assumptions we made should mean that our curve is nowhere too far from flat. Moreover, it is a very easy assumption to work with, since it means that the expected returns scale logarithmically with the resources put in: in expectation, a doubling of the resources is equally good regardless of the starting point.</p><p>It is, unfortunately, never exactly true. Although our curves may be approximately flat, they cannot be everywhere flat -- this can&#x2019;t even give a probability distribution! But it may work reasonably as a model of local behaviour. If we want to turn it into a probability distribution, we can do this by estimating the plausible ranges of D and assuming it is uniform across this scale. In our example we would be approximating the blue curve by something like this red box:\n</p><p><img title=\"\" src=\"https://docs.google.com/drawings/image?id=suNHX4cQOodnl0rQgtSpMrw&amp;rev=1&amp;h=235&amp;w=620&amp;ac=1\" alt=\"\"></p>\nObviously in the example the red box is not a fantastic approximation. But nor is it a terrible one. Over the central range, it is never out from the true value by much more than a factor of 2. While crude, this could still represent a substantial improvement on the current state of some of our estimates. A big advantage is that it is easily analytically tractable, so it will be quick to work with. In the rest of this post we&#x2019;ll explore the consequences of this assumption.\n<h3>Places this might fail</h3>\nIn some circumstances, we might expect high uncertainty over difficulty without everywhere having local log-returns. A key example is if we have bounds on the difficulty at one or both ends.<p></p><p>For example, if we are interested in X, which comprises a task of radically unknown difficulty plus a repetitive and predictable part of difficulty 1000, then our distribution of beliefs of the difficulty about X will only include values above 1000, and may be quite clustered there (so not even approximately logarithmic returns). The behaviour in the positive tail might still be roughly logarithmic.</p><p>In the other direction, we may know that there is a slow and repetitive way to achieve X, with difficulty 100,000. We are unsure whether there could be a quicker way. In this case our distribution will be uncertain over difficulties up to around 100,000, then have a spike. This will give the reverse behaviour, with roughly logarithmic expected returns in the negative tail, and a different behaviour around the spike at the upper end of the distribution.</p><p>In some sense each of these is diverging from the idea that we are very ignorant about the difficulty of the problem, but it may be useful to see how the conclusions vary with the assumptions.\n</p><h2>Implications for expected returns</h2>\nWhat does this model tell us about the expected returns from putting resources into trying to solve the problem?<p></p><p>Under the assumption that the prior is locally log-uniform, the full value is realised over the width of the box in the diagram. This is <i>w = </i>log(<i>y</i>) - log(<i>x</i>)<i>, </i>where <i>x </i>is the value at the start of the box (where the problem could first be plausibly solved), <i>y</i> is the value at the end of the box, and our logarithms are natural. Since it&#x2019;s a probability distribution, the height of the box is 1/<i>w</i>.</p><p>For any <i>z</i> between <i>x</i> and <i>y</i>, the modelled chance of success from investing <i>z</i> resources is equal to the fraction of the box which has been covered by that point. That is:</p><p>(1)Chance of success before reaching <i>z</i> resources = log(<i>z</i>/<i>x</i>)/log(<i>y</i>/<i>x</i>)<i>.</i></p><p>So while we are in the relevant range, the chance of success is equal for any doubling of the total resources. We could say that we expect <i>logarithmic returns </i>on investing resources.\n</p><h3>Marginal returns</h3>\nSometimes of greater relevance to our decisions is the marginal chance of success from adding an extra unit of resources at <i>z</i>. This is given by the derivative of Equation (1):<p></p><p>(2)Chance of success from a marginal unit of resource at <i>z</i> = 1/<i>zw</i>.</p><p>So far, we&#x2019;ve just been looking at estimating the prior probabilities -- before we start work on the problem. Of course when we start work we generally get more information. In particular, if we would have been able to recognise success, and we have invested <i>z</i> resources without observing success, then we learn that the difficulty is at least <i>z</i>. We must update our probability distribution to account for this. In some cases we will have relatively little information beyond the fact that we haven&#x2019;t succeeded yet. In that case the update will just be to curtail the distribution to the left of <i>z</i> and renormalise, looking roughly like this:\n</p><p><img title=\"\" src=\"https://docs.google.com/drawings/image?id=spv3l7LP74kqoBzS2UjJttA&amp;rev=1&amp;h=235&amp;w=620&amp;ac=1\" alt=\"\"></p>\nAgain the blue curve represents our true subjective probability distribution, and the red box represents a simple model approximating this. Now the simple model gives slightly higher estimated chance of success from an extra marginal unit of resources:<p></p><p>(3)Chance of success from an extra unit of resources after <i>z</i> = 1/(<i>z</i>*(ln(<i>y</i>)-ln(<i>z</i>))).</p><p>Of course in practice we often will update more. Even if we don&#x2019;t have a good idea of how hard fusion is, we can reasonably assign close to zero probability that an extra $100 today will solve the problem today, because we can see enough to know that the solution won&#x2019;t be found imminently. This looks like it might present problems for this approach. However, the truly decision-relevant question is about the counterfactual impact of extra resource investment. The region where we can see little chance of success has a much smaller effect on that calculation, which we discuss below.\n</p><h3>Comparison with returns from a Pareto distribution</h3>\nWe mentioned that one natural model of such a process is as a Pareto distribution. If we have a Pareto distribution with shape parameter <i>&#x3B1;</i>, and we have so far invested <i>z</i> resources without success, then we get:<p></p><p>(4) Chance of success from an extra unit of resources = <i>&#x3B1;</i>/<i>z</i>.</p><p>This is broadly in line with equation (3). In both cases the key term is a factor of 1/<i>z</i>. In each case there is also an additional factor, representing roughly how hard the problem is. In the case of the log-linear box, this depends on estimating an upper bound for the difficulty of the problem; in the case of the Pareto distribution it is handled by the shape parameter. It may be easier to introspect and extract a sensible estimate for the width of the box than for the shape parameter, since it is couched more in terms that we naturally understand.\n</p><h2>Further work</h2>\nIn this post, we&#x2019;ve just explored a simple model for the basic question of how likely success is at various stages. Of course it should not be used blindly, as you may often have more information than is incorporated into the model, but it represents a starting point if you don&apos;t know where to begin, and it gives us something explicit which we can discuss, critique, and refine.<p></p><p>In future posts, I plan to:\n</p><ul>\n\t<li>Explore what happens in a field of related problems (such as a research field), and explain why we might expect to see logarithmic returns <i>ex post</i> as well as <i>ex ante</i>.\n<ul>\n\t<li>Look at some examples of this behaviour in the real world.</li>\n</ul>\n</li>\n\t<li>Examine the counterfactual impact of investing resources working on these problems, since this is the standard we should be using to prioritise.</li>\n\t<li>Apply the framework to some questions of interest, with worked proof-of-concept calculations.</li>\n\t<li>Consider what happens if we relax some of the assumptions or take different models.</li>\n</ul><p></p></body></html>", "user": {"username": "Owen_Cotton-Barratt"}}, {"_id": "LdwmSMSjuy2Qug4Me", "title": "Ben Kuhn on the effective altruist movement", "postedAt": "2014-07-23T07:53:39.000Z", "htmlBody": "<html><body><p>Ben Kuhn is a data scientist and engineer at a small financial technology firm. He previously studied mathematics and computer science at Harvard, where he was also co-president of <a href=\"http://harvardea.org/\">Harvard College Effective Altruism</a>. He writes on effective altruism and other topics at <a href=\"http://www.benkuhn.net/\">his website</a>.</p><p></p><hr><p></p><p><b>Pablo</b>: How did you become involved in the EA movement?</p><p><b>Ben</b>: When I was a sophomore in high school (that&apos;s age 15 for non-Americans), Peter Singer gave his <a href=\"http://www.thelifeyoucansave.org/\"><i>The Life You Can Save</i></a> talk at <a href=\"http://commschool.org/\">my high school</a>. He went through his whole &quot;child drowning in the pond&quot; spiel and explained that we were morally obligated to give money to charities that helped those who were worse off than us. In particular, I think at that point he was recommending donating to Oxfam in a sort of Kantian way where you gave an amount of money such that if everyone gave the same percentage it would eliminate world poverty. My friends and I realized that there was no utilitarian reason to stop at that amount of money--you should just donate everything that you didn&apos;t need to survive.</p><p>So, being not only sophomores but also sophomoric, we decided that since Prof. Singer didn&apos;t live in a cardboard box and wear only burlap sacks, he must be a hypocrite and therefore not worth paying attention to.</p><p>Sometime in the intervening two years I ran across Yvain&apos;s essay <a href=\"http://lesswrong.com/lw/3gj/efficient_charity_do_unto_others/\"><i>Efficient Charity: Do Unto Others</i></a> and through it <a href=\"http://www.givewell.org/\">GiveWell</a>. I think that was the point where I started to realize Singer might have been onto something. By my senior year (ages 17-18) I at least professed to believe pretty strongly in some version of effective altruism, although I think I hadn&apos;t heard of the term yet. I wrote <a href=\"https://s3.amazonaws.com/bknet/on_charity.pdf\">an essay</a> on the subject in a publication that my writing class put together. It was anonymous (under the brilliant <i>nom de plume</i> of &quot;Jenny Ross&quot;) but somehow my classmates all figured out it was me.</p><p>The next big update happened during the spring of my first year of Harvard, when I started going to the Cambridge Less Wrong meetups and met <a href=\"http://www.jefftk.com/index\">Jeff</a> and <a href=\"http://www.givinggladly.com/\">Julia</a>. Through some chain of events they set me up with the folks who were then running Harvard High-Impact Philanthropy (which later became <a href=\"http://harvardea.org/\">Harvard Effective Altruism</a>). After that spring, almost everyone else involved in HHIP left and I ended up becoming president. At that point I guess I counted as &quot;involved in the EA movement&quot;, although things were still touch-and-go for a while until John Sturm came onto the scene and made HHIP get its act together and actually do things.</p><p><b>Pablo</b>: In spite of being generally sympathetic to EA ideas, you have recently written a thorough <a href=\"http://www.benkuhn.net/ea-critique\">critique of effective altruism</a>. &#xA0;I&apos;d like to ask you a few questions about some of the objections you raise in that critical essay. &#xA0;First, you have drawn a distinction between pretending to try and actually trying. &#xA0;Can you tell us what you mean by this, and why do you claim that a lot of effective altruism can be summarized as &#x201C;pretending to actually try&#x201D;?</p><p><b>Ben</b>: I&apos;m not sure I can explain better than what I wrote in that post, but I&apos;ll try to expand on it. For reference, here&apos;s the excerpt that you referred to:</p><p>By way of clarification, consider a distinction between two senses of the word &#x201C;trying&#x201D;.... Let&#x2019;s call them &#x201C;actually trying&#x201D; and &#x201C;pretending to try&#x201D;. Pretending to try to improve the world is something like responding to social pressure to improve the world by querying your brain for a thing which improves the world, taking the first search result and rolling with it. For example, for a while I thought that I would try to improve the world by developing computerized methods of checking informally-written proofs, thus allowing more scalable teaching of higher math, democratizing education, etc. Coincidentally, computer programming and higher math happened to be the two things that I was best at. This is pretending to try. Actually trying is looking at the things that improve the world, figuring out which one maximizes utility, and then doing that thing. For instance, I now run an effective altruist student organization at Harvard because I realized that even though I&#x2019;m a comparatively bad leader and don&#x2019;t enjoy it very much, it&#x2019;s still very high-impact if I work hard enough at it. This isn&#x2019;t to say that I&#x2019;m actually trying yet, but I&#x2019;ve gotten closer.</p><p>Most people say they want to improve the world. Some of them say this because they actually want to improve the world, and some of them say this because they want to be perceived as the kind of person who wants to improve the world. Of course, in reality, everyone is motivated by other people&apos;s perceptions to some extent--the only question is by how much, and how closely other people are watching. But to simplify things let&apos;s divide the world up into those two categories, &quot;altruists&quot; and &quot;signalers.&quot;</p><p>If you&apos;re a signaler, what are you going to do? If you don&apos;t try to improve the world at all, people will notice that you&apos;re a hypocrite. On the other hand, improving the world takes lots of resources that you&apos;d prefer to spend on other goals if possible. But fortunately, looking like you&apos;re improving the world is easier than actually improving the world. Since people usually don&apos;t do a lot of due diligence, the kind of improvements that signallers make tend to be ones with very good appearances and surface characteristics--like <a href=\"http://www.pbs.org/frontlineworld/stories/southernafrica904/video_index.html\">PlayPumps</a>, water-pumping merry-go-rounds which initially appeared to be a clever and elegant way to solve the problem of water shortage in developing countries. PlayPumps got tons of money and celebrity endorsements, and their creators got lots of social rewards, even though the pumps turned out to be hideously expensive, massively inefficient, prone to breaking down, and basically a disaster in every way.</p><p>So in this oversimplified world, the EA observation that &quot;charities vary in effectiveness by orders of magnitude&quot; is explained by &quot;charities&quot; actually being two different things: one group optimizing for looking cool, and one group optimizing for actually doing good. A large part of effective altruism is realizing that signaling-charities (&quot;pretending to try&quot;) often don&apos;t do very much good compared to altruist-charities.</p><p>(In reality, of course, everyone is driven by some amount of signalling and some amount of altruism, so these groups overlap substantially. And there are other motivations for running a charity, like being able to convince <i>yourself</i> that you&apos;re doing good. So it gets messier, but I think the vastly oversimplified model above is a good illustration of where my point is coming from.)</p><p>Okay, so let&apos;s move to the second paragraph of the post you referenced:</p><p>Using this distinction between pretending and actually trying, I would summarize a lot of effective altruism as &#x201C;pretending to actually try&#x201D;. As a social group, effective altruists have successfully noticed the pretending/actually-trying distinction. But they seem to have stopped there, assuming that knowing the difference between fake trying and actually trying translates into ability to actually try. Empirically, it most certainly doesn&#x2019;t. A lot of effective altruists still end up satisficing&#x2014;finding actions that are on their face acceptable under core EA standards and then picking those which seem appealing because of other essentially random factors. This is more likely to converge on good actions than what society does by default, because the principles are better than society&#x2019;s default principles. Nevertheless, it fails to make much progress over what is directly obvious from the core EA principles. As a result, although &#x201C;doing effective altruism&#x201D; feels like truth-seeking, it often ends up being just a more credible way to pretend to try.</p><p>The observation I&apos;m making here is roughly that EA seems not to have switched entirely to doing good for altruistic rather than signaling reasons. It&apos;s more like we&apos;ve switched to <i>signaling</i> that we&apos;re doing good for altruistic rather than signaling reasons. In other words, the motivation didn&apos;t switch from &quot;looking good to outsiders&quot; to &quot;actually being good&quot;--it switched from &quot;looking good to outsiders&quot; to &quot;looking good to the EA movement.&quot;</p><p>Now, the EA movement is way better than random outsiders at distinguishing between things with good surface characteristics and things that are actually helpful, so the latter criterion is much stricter than the former, and probably leads to much more good being done per dollar. (For instance, I doubt the EA community would ever endorse something like PlayPumps.) But, at least at the time of writing that post, I saw a lot of behavior that seemed to be based on finding something pleasant and with good surface appearances rather than finding the thing that optimized utility--for instance, donating to causes without a particularly good case that they were better than saving or picking career options that seemed decent-but-not-great from an EA perspective. That&apos;s the source of the phrase &quot;pretending to actually try&quot;--the signaling isn&apos;t going away, it&apos;s just moving up a level in the hierarchy, to signaling that you don&apos;t care about signaling.</p><p>Looking back on that piece, I think &#x201C;pretending to actually try&#x201D; is still a problem, but my intuition is now that it&apos;s probably not huge in the scheme of things. I&apos;m not quite sure why that is, but here are some arguments against it being very bad that have occurred to me:\n</p><ul>\n\t<li>It&apos;s probably somewhat less prevalent than I initially thought, because the EAs making weird-seeming decisions may be doing them for reasons that aren&apos;t transparent to me and that get left out by the typical EA analysis. The typical EA analysis tends to be a 50000-foot average-case argument that can easily be invalidated by particular personal factors.</li>\n\t<li>As Katja Grace <a href=\"http://meteuphoric.wordpress.com/2013/12/22/pretend-to-really-really-try/\">points out</a>, encouraging pretending to really try might be optimal from a movement-building perspective, inasmuch as it&apos;s somewhat inescapable and still leads to pretty good results.</li>\n\t<li>I probably overestimated the extent to which motivated/socially-pressured life choices are bad, for a couple reasons. I discounted the benefit of having people do a diversity of things, even if the way they came to be doing those things wasn&apos;t purely rational. I also discounted the cost of doing something EA tells you to do instead of something you also want to do.</li>\n\t<li>For instance, suppose for the sake of argument that there&apos;s a pretty strong EA case that politics isn&apos;t very good (I know this isn&apos;t actually true). It&apos;s probably good for marginal EAs to be dissuaded from going into politics by this, but I think it would still be bad for every single EA to be dissuaded from going into politics, for two reasons. First, the arguments against politics might turn out to be wrong, and having a few people in politics hedges against that case. Second, it&apos;s much easier to excel at something you&apos;re motivated at, and the category of &quot;people who are excellent at what they do&quot; is probably as important to the EA movement as &quot;people doing job X&quot; for most X.</li>\n</ul>\nI also just haven&apos;t noticed as much pretending-stuff going on in the last few months, so maybe we&apos;re just getting better at avoiding it (or maybe I&apos;m getting worse at noticing it). Anyway, I still definitely think there&apos;s pretending-to-actually-try going on, but I don&apos;t think it&apos;s a huge problem.<p></p><p><b>Pablo</b>: In another section of that critique, you express surprise at the fact that so many effective altruists donate to global health causes now. &#xA0;Why would you expect EAs to use their money in other ways--whether it&apos;s donating now to other causes, or donating later--, and what explains, in your opinion, this focus on causes for which we have relatively good data?</p><p><b>Ben</b>; I&apos;m no longer sure enough of where people&apos;s donations are going to say with certainty that too much is going to global health. My update here is from of a combination of being overconfident when I wrote the piece, and what looks like an increase in waiting to donate shortly after I wrote it. The latter was probably due in large part to <a href=\"http://blog.givewell.org/2013/11/26/change-in-against-malaria-foundation-recommendation-status-room-for-more-funding-related/\">AMF&apos;s delisting</a> and perhaps the precedent set by <a href=\"http://blog.givewell.org/2013/12/12/staff-members-personal-donations/\">GiveWell employees</a>, many of whom waited last year (though <a href=\"http://blog.givewell.org/2013/12/31/some-considerations-against-saving-for-next-year/\">others argued against it</a>). (Incidentally, I&apos;m excited about the projects going on to make this more transparent, e.g. the questions on the survey about giving!)</p><p>The giving now vs. later debate has been <a href=\"/ea/4e/giving_now_vs_later_a_summary/\">ably summarized by Julia Wise</a> on the EA blog. My sense from reading various arguments for both sides is that I more often see bad arguments for giving now. There are definitely good arguments for giving at least some money now, but on balance I suspect I&#x2019;d like to see more saving. Again, though, I don&#x2019;t have a great idea of what people&#x2019;s donation behavior actually is; my samples could easily be biased.</p><p>I think my strongest impression right now is that I suspect we should be exploring more different ways to use our donations. For instance, some people who are earning to give have experimented with funding people to do independent research, which was a pretty cool idea. Off the top of my head, some other things we could try include scholarships, essay contest prizes, career assistance for other EAs, etc. In general it seems like there are tons of ways to use money to improve the world, many of which haven&#x2019;t been explored by GiveWell or other evaluators and many of which don&#x2019;t even fall in the category of things they care about (because they&#x2019;re too small or too early-stage or something), but we should still be able to do something about them.</p><p><b>Pablo</b>: In the concluding section of your essay, you propose that <i>self-awareness</i> be added to the list of principles that define effective altruism. Any thoughts on how to make the EA movement more self-aware?</p><p><b>Ben</b>: One thing that I like to do is think about what our blind spots are. I think it&apos;s pretty easy to look at all the stuff that is obviously a bad idea from an EA point of view, and think that our main problem is getting people &quot;on board&quot; (or even &quot;getting people to admit they&apos;re wrong&quot;) so that they stop doing obviously bad ideas. And that&apos;s certainly helpful, but we also have a ways to go just in terms of figuring things out.</p><p>For instance, here&apos;s my current list of blind spots--areas where I wish there were a lot more thinking and idea-spreading going on then there currently is:\n</p><ul>\n\t<li><b>Being a good community.</b> The EA community is already having occasional growing pains, and this is only going to get worse as we gain steam e.g. with Will MacAskill&apos;s upcoming book. And beyond that, I think that ways of making groups more effective (as opposed to individuals) have a lot of promise for making the movement better at what we do. Many, many intellectual groups fail to accomplish their goals for basically silly reasons, while seemingly much worse groups do much better on this dimension. It seems like there&#x2019;s no intrinsic reason we should be worse than, say, Mormons at building an effective community, but we&#x2019;re clearly not there yet. I think there&#x2019;s absolutely huge value in getting better at this, yet almost no one putting in a serious concerted effort.</li>\n\t<li><b>Knowing history.</b> Probably as a result of EA&apos;s roots in math/philosophy, my impression is that our average level of historical informedness is pretty low, and that this makes us miss some important pattern-matches and cues. For instance, I think a better knowledge of history could help us think about capacity-building interventions, policy advocacy, and community building.</li>\n</ul>\n<ul>\n\t<li><b>Fostering more intellectual diversity.</b> Again because of the math/philosophy/utilitarianism thing, we have a massive problem with intellectual monoculture. Of my friends, the ones I enjoy talking about altruism the most with now are largely actually the ones who associate least with the broader EA community, because they have more interesting and novel perspectives.<b>\n</b></li>\n</ul>\n<ul>\n\t<li><strong>Finding individual effective opportunities</strong>. I suspect that there&#x2019;s a lot of room for good EA opportunities that GiveWell hasn&#x2019;t picked up on because they&#x2019;re specific to a few people at a particular time. Some interesting stuff has been done in this vein in the past, like funding small EA-related experiments, funding people to do independent secondary research, or giving loans to other EAs investing in themselves (at least I believe this has been done). But I&#x2019;m not sure if most people are adequately on the lookout for this kind of opportunity.</li>\n</ul>\n(Since it&apos;s not fair to say &quot;we need more X&quot; without specifying how we get it, I should probably also include at least one anti-blind spots that I think we should be spending fewer resources on, on the margin: Object-level donations to e.g. global health causes. I feel like we may be hitting diminishing returns here. Probably donating some is important for signalling reasons, but I think it doesn&apos;t have a very high naive expected value right now.)<p></p><p><b>Pablo</b>: Finally, what are your plans for the mid-term future? &#xA0;What EA-relevant activities will you engage in over the next few years, and what sort of impact do you expect to have?</p><p><b>Ben</b>: A while ago I did some reflecting and realized that most of the things I did that I was most happy about were pretty much unplanned--they happened not because I carefully thought things through and decided that they were the best way to achieve some goal, but because they intuitively seemed like a cool thing to do. (Things in this category include starting a blog, getting involved in the EA/rationality communities, running Harvard Effective Altruism, getting my current job, etc.) As a result, I don&apos;t really have &quot;plans for the mid-term future&quot; per se. Instead, I typically make decisions based on intuitions/heuristics about what will lead to the best opportunities later on, without precisely knowing (or even knowing at all, often) what form those opportunities will take.</p><p>So I can&apos;t tell you what I&apos;ll be doing for the next few years--only that it will probably follow some of my general intuitions and heuristics:\n</p><ul>\n\t<li><b>Do lots of things</b>. The more things I do, the more I increase my &quot;luck surface area&quot; to find awesome opportunities.</li>\n\t<li><b>Do a few things really well</b>. The point of this heuristic is hopefully obvious.</li>\n\t<li><b>Do things that other people aren&apos;t doing</b>--or more accurately, things that not enough people are doing relative to how useful or important they are. My effort is most likely to make a difference in an area that is relatively under-resourced.</li>\n</ul>\nI&#x2019;d like to take a moment here to plug the <a href=\"http://www.givewell.org/altruistic-career-choice\">conference call on altruistic career choice</a> that Holden Karnofsky of GiveWell had, which makes some great specific points along these lines.<p></p><p>Anyway, that&apos;s my long-winded answer to the first part of this question. As far as EA-relevant activities and impacts, all the same caveats apply as above, but I can at least go over some things I&apos;m currently interested in:\n</p><ul>\n\t<li>Now that I&#x2019;m employed full-time, I need to start thinking much harder about where exactly I want to give: both what causes seem best, and which interventions within those causes. I actually currently don&apos;t have much of a view on what I would do with more unrestricted funds.</li>\n\t<li>Related to the point above about self-awareness, I&apos;m interested in learning some more EA-relevant history--how previous social movements have worked out, how well various capacity-building interventions have worked, more about policy and the various systems that philanthropy comes into contact with, etc.</li>\n\t<li>I&apos;m interested to see to what extent the success of Harvard Effective Altruism can be sustained at Harvard and replicated at other universities.</li>\n</ul>\nI also have some more speculative/gestational interests--I&apos;m keeping my eye on these, but don&apos;t even have concrete next steps in mind:\n<ul>\n\t<li>I think there may be under-investment in healthy EA community dynamics, preventing common failure modes like unfriendliness, ossification to new ideas, groupthink etc.--though I can&apos;t say for sure because I don&apos;t have a great big-picture perspective of the EA community.</li>\n\t<li>I&apos;m also interested in generally adding more intellectual/epistemic diversity to EA--we have something of a monoculture problem right now. Anecdotally, there are a number of people who I think would have a really awesome perspective on many problems that we face, but who get turned off of the community for one reason or another.</li>\n</ul>\n<em>Crossposted from <a href=\"http://www.stafforini.com/blog/ben-kuhn-on-the-effective-altruist-movement/\">Pablo&apos;s blog</a></em><p></p></body></html>", "user": {"username": "admin3"}}, {"_id": "342Nb9pp7snWrupTT", "title": "Audio recordings from Good Done Right available online", "postedAt": "2014-07-18T22:42:35.000Z", "htmlBody": "<html><body><p>This July saw the first academic conference on effective altruism. The three-day event took place at All Souls College, one of the constituent colleges of the University of Oxford. The conference featured a diverse range of speakers addressing issues related to effective altruism in a shared setting. It was a fantastic opportunity to share insights and ideas from some of the best minds working on these issues.</p><p>I&#x2019;m very pleased to announce that audio recordings from most of the talks are now available <a href=\"http://www.gooddoneright.com/#!programme/c1dj9\">on the conference website</a>, alongside speakers&#x2019; slides (where applicable). I&#x2019;m very grateful to all of the participants for their fantastic presentations, and to All Souls College and the Centre for Effective Altruism for supporting the conference.</p><p><em>Crossposted from the <a href=\"http://www.givingwhatwecan.org/blog/2014-07-18/good-done-right-conference-effective-altruism#sthash.wcxejMT4.dpuf\">Giving What We Can blog</a></em></p></body></html>", "user": {"username": "Andreas_Mogensen"}}, {"_id": "FzjJpZWmnMpbaJpCc", "title": "'Special Projects' at the Centre for Effective Altruism", "postedAt": "2014-07-07T15:47:40.000Z", "htmlBody": "<html><body><p><em>This is a short overview&#xA0;of a talk&#xA0;that I gave alongside&#xA0;<a href=\"/author/william-macaskill/\">William MacAskill</a> and <a href=\"/author/owen-cotton-barratt/\">Owen Cotton-Barratt</a> at the <a href=\"http://centreforeffectivealtruism.org/\">Centre for Effective Altruism</a> <a title=\"2014 Weekend Away\" href=\"/ea/63/2014_weekend_away/\">Weekend Away</a> last weekend. &#xA0;This post does not contain&#xA0;new information for people familiar with the Centre for Effective Altruism&apos;s work. &#xA0;</em></p><p>New projects at the Centre for Effective Altruism are incubated within the Special Projects team. &#xA0;We carry out a number of activities before choosing which ones to scale up. &#xA0;The projects that we are currently working on are listed below.</p><p><a href=\"http://centreforeffectivealtruism.org/wp-content/uploads/2014/06/Screen-Shot-2014-06-20-at-2.17.06-pm.png\"><img src=\"http://centreforeffectivealtruism.org/wp-content/uploads/2014/06/Screen-Shot-2014-06-20-at-2.17.06-pm.png\" alt=\"Screen Shot 2014-06-20 at 2.17.06 pm\"></a>The&#xA0;<a href=\"http://www.fhi.ox.ac.uk/research/global-priorities-project/\">Global Priorities Project</a>&#xA0;is a joint research initiative between the&#xA0;<a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a>&#xA0;at the University of Oxford and the <a href=\"http://www.centreforeffectivealtruism.org\">Centre for Effective Altruism</a>. &#xA0;It attempts to prioritise between the pressing problems currently facing the world in order to establish in which areas we might have the most impact. &#xA0;You can read more on about the project&#xA0;<a href=\"http://www.fhi.ox.ac.uk/research/global-priorities-project/\">here</a>.</p><p>Through the Global Priorities Project we are also engaged in policy advising for the UK Government. &#xA0;Our first report to be published under this initiative is on unprecedented technological risk. &#xA0;Our team regularly&#xA0;<a href=\"http://www.fhi.ox.ac.uk/dr-toby-ord-at-10-downing-street/\">visits</a>&#xA0;Government departments and No. 10 Downing Street to discuss policy proposals that we are developing as part of this work.</p><p>We are also scaling up our&#xA0;<a href=\"/ea/68/effective_altruism_outreach_plans/\">effective altruism outreach</a>. &#xA0;As part of this work we are developing&#xA0;<a href=\"http://www.effectivealtruism.org/\">EffectiveAltruism.org</a>&#xA0;into a landing page for people new to effective altruism. &#xA0;We are also developing outreach activities to coincide with the release of multiple books on effective altruism in 2015, including one by our co-founder <a href=\"http://www.williammacaskill.com\">William MacAskill</a> which will be published by Penguin in USA, and Guardian Faber (the publishing house of the national newspaper) in the UK.</p><p>We have also launched Effective Altruism Ventures, a commercial company that will hold the rights to William MacAskill&#x2019;s&#xA0;<a href=\"/ea/60/announcing_a_forthcoming_book_on_effective/\">upcoming book</a>, which will also engage in outreach activities related to effective altruism. &#xA0;This company is not part of the Centre for Effective Altruism.</p><p>If you have any questions about any of these projects, please do not hesitate to contact me at firstname.lastname@centreforeffectivealtruism.org or in the comments below.</p></body></html>", "user": {"username": "Niel_Bowerman"}}, {"_id": "owZ54ujtaFkcjp45T", "title": "The timing of labour aimed at reducing existential risk", "postedAt": "2014-07-04T04:08:24.000Z", "htmlBody": "<html><body><p><em>Crossposted from the <a href=\"http://www.fhi.ox.ac.uk/the-timing-of-labour-aimed-at-reducing-existential-risk\">Global Priorities Project</a></em></p><p>Work towards reducing existential risk is likely to happen over a timescale of decades. For many parts of this work, the benefits of that labour is greatly affected by when it happens. This has a large effect when it comes to strategic thinking about what to do now in order to best help the overall existential risk reduction effort. I look at the effects of nearsightedness, course setting, self-improvement, growth, and serial depth, showing that there are competing considerations which make some parts of labour particularly valuable earlier, while others are more valuable later on. We can thus improve our overall efforts by encouraging more meta-level work on course setting, self-improvement, and growth over the next decade, with more of a focus on the object-level research on specific risks to come in decades beyond that.\n</p><h2>Nearsightedness</h2>\nSuppose someone considers AI to be the largest source of existential risk, and so spends a decade working on approaches to make self-improving AI safer. It might later become clear that AI was not the most critical area to worry about, or that this part of AI was not the most critical part, or that this work was going to get done anyway by mainstream AI research, or that working on policy to regulate research on AI was more important than working on AI. In any of these cases she wasted some of the value of her work by doing it now. She couldn&apos;t be faulted for lack of omniscience, but she could be faulted for making herself unnecessarily at the mercy of bad luck. She could have achieved more by doing her work later, when she had a better idea of what was the most important thing to do.<p></p><p>We are nearsighted with respect to time. The further away in time something is, the harder it is to perceive its shape: its form, its likelihood, the best ways to get purchase on it. This means that work done now on avoiding threats in the far future can be considerably less valuable than the same amount of work done later on. The extra information we have when the threat is up close lets us more accurately tailor our efforts to overcome it.</p><p>Other things being equal, this suggests that a given unit of labour directed at reducing existential risk is worth more the later in time it comes.\n</p><h2>Course setting, self-improvement &amp; growth</h2>\nAs it happens, other things are not equal. There are at least three major effects which can make earlier labour matter more.<p></p><p>The first of these is if it helps to change course. If we are moving steadily in the wrong direction, we would do well to change our course, and this has a larger benefit the earlier we do so. For example, perhaps effective altruists are building up large resources in terms of specialist labour directed at combatting a particular existential risk, when they should be focusing on more general purpose labour. Switching to the superior course sooner matters more, so efforts to determine the better course and to switch onto it matter more the earlier they happen.</p><p>The second is if labour can be used for self-improvement. For example, if you are going to work to get a university degree, it makes sense to do this earlier in your career rather than later as there is more time to be using the additional skills. Education and training, both formal and informal, are major examples of self-improvement. Better time management is another, and so is gaining political or other influence. However this category only includes things that create a lasting improvement to your capacities and that require only a small upkeep. We can also think of self-improvement for an organisation. If there is benefit to be had from improved organisational efficiency, it is generally better to get this sooner. A particularly important form is lowering the risk of the organisation or movement collapsing, or cutting off its potential to grow.</p><p>The third is if the labour can be used to increase the amount of labour we have later. There are many ways this could happen, several of which give exponential growth. A simple example is investment. An early hour of labour could be used to gain funds which are then invested. If they are invested in a bank or the stock market, one could expect a few percent real return, letting you buy twice as much labour two or three decades later. If they are invested in raising funds through other means (such as a fundraising campaign) then you might be able to achieve a faster rate of growth, though probably only over a limited number of years until you are using a significant fraction of the easy opportunities.</p><p>A very important example of growth is movement building: encouraging other people to dedicate part of their own labour or resources to the common cause, part of which will involve more movement building. This will typically have an exponential improvement with the potential for double digit percentage growth until the most easily reached or naturally interested people have become part of the movement at which point it will start to plateau. An extra hour of labour spent on movement building early on, could very well produce a hundred extra hours of labour to be spent later. Note that there might be strong reasons not to build a movement as quickly as possible: rapid growth could involve increasing the signal to noise ratio in the movement, or changing its core values, or making it more likely to collapse, and this would have to be balanced against the benefits of growth sooner.</p><p>If the growth is exponential for a while but will spend a lot of time stuck at a plateau, it might be better in the long term to think of it like self improvement. An organisation might have been able to raise $10,000 of funds per year after costs before the improvement and then gains the power to raise $1,000,000 of funds per year afterwards -- only before it hits the plateau does it have the exponential structure characteristic of growth.</p><p>Finally, there is a matter of serial depth. Some things require a long succession of stages each of which must be complete before the next begins. If you are building a skyscraper, you will need to build the structure for one story before you can build the structure for the next. You will therefore want to allow enough time for each of these stages to be completed and might need to have some people start building soon. Similarly, if a lot of novel and deep research needs to be done to avoid a risk, this might involve such a long pipeline that it could be worth starting it sooner to avoid the diminishing marginal returns that might come from labour applied in parallel. This effect is fairly common in computation and labour dynamics (see The Mythical Man Month), but it is the factor that I am least certain of here. We obviously shouldn&apos;t hoard research labour (or other resources) until the last possible year, and so there is a reason based on serial depth to do some of that research earlier. But it isn&apos;t clear how many years ahead of time it needs to start getting allocated (examples from the business literature seem to have a time scale of a couple of years at most) or how this compares to the downsides of accidentally working on the wrong problem.\n</p><h2>Consequences</h2>\nWe have seen that nearsightedness can provide a reason to delay labour, while course setting, self-improvement, growth, and serial depth provide reasons to use labour sooner. In different cases, the relative weights of these reasons will change. The creation of general purpose resources such as political influence, advocates for the cause, money, or earning potential, is especially resistant to the nearsightedness problem as they have more flexibility to be applied to whatever the most important final steps happen to be. Creating general purpose resources, or doing course setting, self-improvement, or growth are thus comparatively better to do in the earlier times. Direct work on the cause is comparatively better to do later on (with a caveat about allowing enough time to allow for the required serial depth).<p></p><p>In the case of existential risk, I think that many of the percentage points of total existential risk lie decades or more in the future. There is quite plausibly more existential risk in the 22nd century than in the 21st. For AI risk, the recent FHI survey of 174 experts, the median estimate for when there would be a 50% chance of reaching roughly human level AI was 2040. For the subgroup of those who are part of the &apos;Top 100&apos; researchers in AI, it was 2050. This gives something like 25 to 35 years before we think most of this risk will occur. That is a long time and will produce a large nearsightedness problem for conducting specific research now and a large potential benefit for course setting, self-improvement, and growth. Given a portfolio of labour to reduce risk over that time, it is particularly important to think about moving types of labour towards the times where they have a comparative advantage. If we are trying to convince others to help use their careers to reduce this risk, the best career advice might change over the coming decades from help with movement building or course setting, to accumulating more flexible resources, to doing specialist technical work.</p><p>The temporal location of a unit of labour can change its value by a great deal. It is quite plausible that due to nearsightedness, doing specific research now could have less than a tenth the expected value of doing it later, since it could so easily be on the wrong risk, or the wrong way of addressing the risk, or would have been done anyway, or could have been done more easily using tools people later build etc. It is also quite plausible that using labour to produce growth now, or to point us in a better direction, could produce ten times as much value. It is thus pivotal to think carefully about when we want to have different kinds of labour.</p><p>I think that this overall picture is right and important. However, I should add some caveats. We might need to do some specialist research early on in order to gain information about whether the risk is credible or which parts to focus on, to better help us with course setting. Or we might need to do research early in order to give research on risk reduction enough academic credibility to attract a wealth of mainstream academic attention, thereby achieving vast growth in terms of the labour that will be spent on the research in the future. Some early object level research will also help with early fundraising and movement building -- if things remain too abstract for a long time, it would be extremely difficult to maintain a movement. But in these examples, the overall picture is the same. If we want to do early object-level research, it is because of its instrumental effects on course setting, self-improvement, and growth.</p><p>The writing of this document and the thought that preceded it are an example of course setting: trying to significantly improve the value of the long-term effort in existential risk reduction by changing the direction we head in. I think there are considerable gains here and as with other course setting work, it is typically good to do it sooner. I&apos;ve tried to outline the major systematic effects that make the value of our labour vary greatly with time, and to present them qualitatively. But perhaps there is a major effect I&apos;ve missed, or perhaps some big gains by using quantitative models. I think that more research on this would be very valuable.</p></body></html>", "user": {"username": "Toby_Ord2"}}, {"_id": "DcFFjqtK7tXzptCdJ", "title": "On 'causes'", "postedAt": "2014-06-24T17:19:54.000Z", "htmlBody": "<html><body><p><em>Crossposted from the <a href=\"http://www.fhi.ox.ac.uk/on-causes/\">Global Priorities Project</a></em></p><p>This post has two distinct parts. The first explores the meanings that have been attached to the term &#x2018;cause&#x2019;, and suggests my preferred usage. The second makes use of these distinctions to clarify the claims I made in a recent post on the long-term effects of animal welfare improvements.\n</p><h2>On the meaning of &#x2018;cause&#x2019;</h2>\nThere are at least two distinct concepts which could reasonably be labelled a &#x2018;cause&#x2019;:\n<ol>\n\t<li>An intervention area, <i>i.e.</i> a cluster of interventions which are related and share some characteristics. It is often the case that improving our understanding of some intervention in this area will improve our understanding of the whole area. We can view different-sized clusters as broader or narrower causes in this sense. GiveWell has <a href=\"http://blog.givewell.org/2013/05/30/refining-the-goals-of-givewell-labs/\">promoted this meaning</a>. Examples might include: interventions to improve health in developing countries; interventions giving out leaflets to change behaviour.</li>\n\t<li>A goal, something we might devote resources towards optimising. Some causes in this sense might be useful instrumental sub-goals for other causes. For example, &#x201C;minimise existential risk&#x201D; may be a useful instrumental goal for the cause &#x201C;make the long-term future flourish&#x201D;. When 80,000 Hours <a href=\"http://80000hours.org/blog/281-why-pick-a-cause\">discussed reasons to select a cause</a>, they didn&#x2019;t explicitly use this meaning, but many of their arguments relate to it. A cause of this type may be very close to one of the first type, but defined by its goal rather than its methods: for example, maximising the number of quality-adjusted life-years lived in developing countries. Similarly, one could think of a cause a problem one can work towards solving.</li>\n</ol>\nThese two characteristics often appear together, so we don&#x2019;t always need to distinguish. But they can come apart: we can have a goal without a good idea of what intervention area will best support that goal. On the other hand, one intervention area could be worthwhile for multiple different goals, and it may not be apparent what goal an intervention is supposed to be targeting. Below I explain how these concepts can diverge substantially.<p></p><p>Which is the better usage? Or should we be using the word for both meanings? (Indeed there may be other possible meanings, such as defining a cause by its beneficiaries, but I think these are the two most natural.) I am not sure about this and would be interested in comments from others towards finding the most natural community norm. Key questions are whether we need to distinguish the concepts, and if we do then which is the more frequently the useful one to think of, and what other names fit them well.</p><p>My personal inclination is that when the meanings coincide of course we can use the one word, and that when they come apart it is better to use the second. This is because I think conversations about choosing a cause are generally concerned with the second, and because I think that &#x201C;intervention area&#x201D; is a good alternate term for the first meaning, while we lack such good alternatives for the second.\n</p><h2>Conclusions about animals</h2>\nIn <a href=\"/ea/6c/human_and_animal_interventions_the_longterm_view/\">a recent post</a> I discussed why the long-term effects of animal welfare improvements in themselves are probably small. A question we danced around in the comments is whether this meant that animal welfare was not the best cause. Some felt it did not, because of various plausible routes to impact from animal welfare interventions. I was unsure because the argument did appear to show this, but the rebuttals were also compelling.<p></p><p>My confusion at least was stemming, at least in part, from the term &#x2018;cause&#x2019; being overloaded.</p><p>Now that I see that more clearly I can explain exactly what I am and am not claiming.</p><p>In that post, I contrasted human welfare improvements, which have many significant indirect and long-run effects, with animal welfare improvements, which appear not to. That is not to say that interventions which improve animal welfare do not have these large long-run effects, but that the long-run effects of such interventions are enacted via shifts in the views of humans rather than directly via the welfare improvement.</p><p>I believe that the appropriate conclusion is that &#x201C;improve animal welfare&#x201D; is extremely unlikely to be the best simple proxy for the goal &#x201C;make the long-term future flourish&#x201D;. In particular, it is likely dominated by the proxy &#x201C;increase empathy&#x201D;. So we can say with confidence that improving animal welfare is not the best cause in the second sense (whereas it may still be a good intervention area). In contrast, we do not have similarly strong reasons to think &#x201C;improve human welfare&#x201D; is definitely not the best approach.</p><p>Two things I am not claiming:\n</p><ul>\n\t<li>That improving human welfare is a better instrumental sub-goal for improving the long-term future than improving animal welfare.</li>\n\t<li>That interventions which improve animal welfare are not among the best available, if they also have other effects.</li>\n</ul>\nIf you are not persuaded that it&#x2019;s worth optimising for the long-term rather than the short-term, the argument won&#x2019;t be convincing. If you are, though, I think you should not adopt animal welfare as a cause in the second sense. I am not arguing against &#x2018;increasing empathy&#x2019; as possibly the top goal we can target (although I plan to look more deeply into making comparisons between this and other goals), and it may be that &#x2018;increase vegetarianism&#x2019; is a useful way to increase empathy. But we should keep an open mind, and if we adopt &#x2018;increasing empathy&#x2019; as a goal we should look for the best ways to do this, whether or not they relate to animal welfare.<p></p></body></html>", "user": {"username": "Owen_Cotton-Barratt"}}, {"_id": "5dgFWods87kkE9TpZ", "title": "Will we eventually be able to colonize other stars? Notes from a preliminary review", "postedAt": "2014-06-22T18:19:50.000Z", "htmlBody": "<html><body><p><em>Crossposted from the <a href=\"http://www.fhi.ox.ac.uk/will-we-eventually-be-able-to-colonize-other-stars-notes-from-a-preliminary-review/\">Global Priorities Project</a></em></p><p></p><h2>Summary</h2>\nI investigated this question because of its potential relevance to existential risk and the long-term future more generally. There are a limited number of books and scientific papers on the topic and the core questions are generally not regarded as resolved, but the people who seem most informed about the issue generally believe that space colonization will eventually be possible. I found no books or scientific papers arguing for in-principle infeasibility, and believe I would have found important ones if they existed. The blog posts and journalistic pieces arguing for the infeasibility of space colonization are largely unconvincing due to lack of depth and failure to engage with relevant counterarguments.<p></p><p>The potential obstacles to space colonization include: very large energy requirements, health and reproductive challenges from microgravity and cosmic radiation, short human lifespans in comparison with great distances for interstellar travel, maintaining a minimal level of genetic diversity, finding a hospitable target, substantial scale requirements for building another civilization, economic challenges due to large costs and delayed returns, and potential political resistance. Each of these obstacles has various proposed solutions and/or arguments that the problem is not insurmountable. Many of these obstacles would be easier to overcome given potential advances in AI, robotics, manufacturing, and propulsion technology.</p><p>Deeper investigation of this topic could address the feasibility of the relevant advances in AI, robotics, manufacturing, and propulsion technology. My intuition is that such investigation would lend further support to the conclusion that interstellar colonization will eventually be possible.</p><p>Note: This investigation relied significantly on interviews and Wikipedia articles because I&#x2019;m unfamiliar with the area, there are not many very authoritative sources, and I was trying to review this question quickly.\n</p><h2>Why did I look into this question?</h2>\n<ul>\n\t<li>If people are likely to eventually colonize space, then it increases the potential scale and duration of civilization. This could affect arguments about the importance of trying to affect the long-run future of civilization. For example, Nick Bostrom<sup><sup>[1]</sup></sup> and I<sup><sup>[2]</sup></sup> have appealed to the possibility of interstellar space colonization in our arguments for the importance of reducing existential risk or otherwise affecting distant future generations.</li>\n\t<li>In correspondence, some people have tried to resist arguments for the extreme importance of the distant future by rejecting the claim that there is a reasonable chance of colonizing space in the future. I don&#x2019;t believe these arguments essentially depend on the feasibility of space colonization. However, I believed that the evidence for the feasibility of space colonization was strong enough to carry that argument, and I wanted to test that assumption.</li>\n</ul>\n<h2>Clarifying the question</h2>\nWill we eventually be able to colonize other stars? I focused on a version of this question assuming unlimited time horizons and &#x201C;business as usual&#x201D; (no major catastrophes or unexpected reversals of global trends), with the aim of establishing settlements which could function independently of Earth-based civilization. My version of &#x201C;business as usual&#x201D; could be disputed, especially by people who believe ecological constraints may spell an end to innovation and economic development in the coming century or two, leaving insufficient time for the technological developments necessary to make a project of this kind feasible. Apart from noting the potential problem, that is a discussion best left for another investigation.\n<h2>Range of current opinion on this topic</h2>\nA recent, lengthy report by the National Research Council discussing rationales and approaches stated that it currently isn&#x2019;t known whether we&#x2019;ll eventually be able to create self-sufficient off-Earth settlements,<sup><sup>[3]</sup></sup> but only two of 285 pages were devoted to this issue. People who have done the most in-depth work on the feasibility of space colonization generally believe it is possible. For instance, I found several books and academic papers published on this topic, and they are all written by people who think it is possible,<sup><sup>[4]</sup></sup> but I found no books or academic papers arguing for that interstellar colonization is impossible. I discuss whether this is due to a selection effect below. I found some journalistic pieces and magazine articles arguing for the impossibility of space colonization because of worries about microgravity<sup><sup>[5]</sup></sup> or getting enough energy,<sup><sup>[6]</sup></sup> but I generally found them very unconvincing due to lack of depth and failure to discuss obvious counterarguments. For example, the articles arguing from challenges of microgravity to the impossibility of space colonization failed to consider the most obvious solution: rotating the spaceship to induce artificial gravity. For the most part, the articles arguing from energy/propulsion challenges didn&#x2019;t discuss any specific problems with existing proposals to get enough energy using alternatives to the current chemical rockets we have today, such as nuclear fission, fusion, beam propulsion, solar sails, or antimatter (more on this below). An exception was a brief essay written for an Edge.org competition by Ed Regis, which did quickly discuss some of these potential propulsion methods<sup><sup>[7]</sup></sup> (in addition to several of the other challenges I discuss below). However, Regis makes the strange choice of highlighting the most speculative and unlikely propulsion methods, and does not address fission or fusion. It&#x2019;s possible that the scientists quoted have more developed arguments that the journalists have failed to communicate, which would suggest that better critiques along these lines are possible, but it isn&#x2019;t clear to me how these critiques would be developed based on what has been said.<p></p><p>Of the four people I interviewed on this topic, only one (Charles Stross) was pessimistic about our prospects for colonizing the stars. I sought out Charles Stross specifically because he had been referenced by other sources as a notable critic of the feasibility of space colonization<sup><sup>[8]</sup></sup> and because his essay on the topic had the best pessimistic arguments I had seen. From my perspective, his most compelling concerns were about motivation to try in light of large expenses and scale-related challenges for getting a civilization off the ground (more on these below). However, even Stross believes that, if we develop advanced AI/robotics, we are likely to colonize the stars. I asked the four people I interviewed for more references from people who believe we can&#x2019;t colonize the stars, and their comments<sup><sup>[9]</sup></sup> suggest I am not missing any major critiques of the feasibility of interstellar colonization. Comments from these interviews also suggested to me that people publishing work on this topic generally believe interstellar colonization is feasible.<sup><sup>[10]</sup></sup></p><p>Perhaps only people who think space colonization is likely to be feasible get excited enough to write books and careful scientific articles on the subject. But, for a few reasons listed below, it seems this could only partly explain the distribution of informed opinion on the feasibility of space colonization.&#xA0; First, according to Geoffrey Landis, informed experts in nearby fields, such as researchers at NASA, generally haven&#x2019;t thought very much about whether interstellar colonization is possible,<sup><sup>[11]</sup></sup>and don&#x2019;t have opinions one way or the other. If there were good arguments for the infeasibility of space colonization that just weren&#x2019;t being considered by the people I spoke with, I would expect that this would be the group that was aware of them. Second, I have cited a reasonable number of journalistic articles and blog posts arguing for the infeasibility of space colonization, and these articles are surprisingly bad if you think some informed people have convincing arguments for skepticism about the feasibility of space colonization. Third, some claims about interstellar colonization--e.g. by Hawking--have received significant interest from journalists and the public, and if skeptics had other convincing arguments that we probably wouldn&#x2019;t ever be able to colonize the stars, I would expect someone to make these arguments more generally known.\n</p><h2>What are the potential obstacles to interstellar colonization?</h2>\nBig picture, interstellar colonization requires success at each of the following steps (though not necessarily on the first attempt):\n<ol>\n\t<li>Attempt to colonize space</li>\n\t<li>Get everything you need to build a civilization into a spaceship</li>\n\t<li>Get the spaceship going fast in the right direction</li>\n\t<li>Have enough of what you need to build a civilization survive/remain intact during the voyage</li>\n\t<li>Slow the spaceship down when you&#x2019;re getting close enough to your target location</li>\n\t<li>Build a civilization at your target location</li>\n</ol>\nThis review will discuss these steps out of order because it will help with clarifying what we need to bring on a trip of this kind and what kind of motivation would be needed in order to make the attempt.\n<h3>Speeding up and slowing down</h3>\nProxima Centuari&#x2014;the closest star outside our solar system&#x2014;is 4.2 light years away.<sup><sup>[12]</sup></sup> Currently, Voyager 1 is moving away from the sun at 17 km/s, faster than any other human-made object.<sup><sup>[13]</sup></sup> At its current speed, it would take over 70,000 years to reach travel that distance (though it isn&#x2019;t going in that direction).<sup><sup>[14]</sup></sup> The voyage would take decades even at a significant fraction of the speed of light. The longer the trip, the harder it is to ensure that all critical parts of the system (including passengers or descendants of passengers) survive the journey. These issues are discussed in the next sections. This section will focus on propulsion methods and energy requirements.<p></p><p>Charles Stross estimated that it would take at least 10<sup>18</sup> Joules of energy to accelerate a 2000 kg spacecraft to 10% of the speed of light, and an equal amount to slow it down&#x2014;making generous assumptions such as 100% efficient energy conversion and no reaction mass. This was equivalent to five days of human civilization&#x2019;s total electrical energy production in 2007,<sup><sup>[15]</sup></sup> or about one day of civilization&#x2019;s whole energy production.<sup><sup>[16]</sup></sup> However, there has been a long history of exponential growth in energy production and economic productivity.<sup><sup>[17]</sup></sup> If these trends continue long enough, interstellar colonization will become much more affordable.<sup><sup>[18]</sup></sup></p><p>It might be objected that there are some physical reasons to be skeptical that this kind of exponential growth in energy output will continue for long enough to carry this argument. For example, Tom Murphy has argued&#x2014;based on thermodynamic principles&#x2014;that hundreds of years of increases in energy production at this exponential rate would have absurd consequences, such as increasing the Earth&#x2019;s surface temperature to boiling temperature.<sup><sup>[19]</sup></sup><sup>&#xA0; </sup>However, Murphy&apos;s calculations suggest that it would be possible to have 100 times as much energy output on Earth as we currently do without substantially affecting Earth&apos;s temperature,<sup><sup>[20]</sup></sup> which would make the energy cost of an interstellar mission not unreasonable on assumptions like Stross&#x2019;s. More importantly though, an interstellar mission could be launched from interplanetary space, substantially weakening the original line of argument.</p><p>Some proposed propulsion technologies&#x2014;such as nuclear fission, fusion, anti-matter, beam propulsion, and solar sails&#x2014;might address these challenges. I encountered disagreement about whether these proposed technologies were known to be feasible or not. Robert Zubrin claimed in our interview that there were specific technologies which were known to be physically possible, and could allow us to travel at a few percent of the speed of light.<sup><sup>[21]</sup></sup> Ed Regis said we didn&#x2019;t know whether some of these technologies were feasible,<sup><sup>[22]</sup></sup> but he didn&#x2019;t discuss fission, fusion, beam propulsion, or solar sails, which seem much less speculative than the technologies whose feasibility he questioned. I have not looked at these proposals closely, but the Orion project, which was led by Freeman Dyson, received substantial funding and attention from serious people.<sup><sup>[23]</sup></sup> My intuition is that nuclear propulsion is the least speculative, and would be sufficient for bringing a spacecraft to a significant fraction of the speed of light.\n</p><h3>Surviving the journey: challenges for human passengers</h3>\n<h4>Microgravity</h4>\nThe human body is adapted to the level of gravitational force normally experienced on the surface of the Earth, and extended exposure to a zero-g environment has various adverse health effects, including loss of bone and muscle mass, retinal damage, redistribution of bodily fluids toward the upper half of the body, balance disorders, and loss of taste and smell.<sup><sup>[24]</sup></sup> According to Wikipedia, we have very limited knowledge about the potential effects on the very young and the elderly.<sup><sup>[25]</sup></sup> In addition, attempts to breed mice and fish eggs in space have come out badly, and the prevailing view is that microgravity is the source of the problem.<sup><sup>[26]</sup></sup> This may be a problem because&#x2014;given the distance to other stars&#x2014;human reproduction may be necessary in transit.<p></p><p>A simple solution to this problem is to induce artificial gravity by rotating the vessel/habitat,<sup><sup>[27]</sup></sup> though some other solutions have been proposed as well.<sup><sup>[28]</sup></sup>\n</p><h4>Cosmic radiation</h4>\nExtended exposure to cosmic radiation damages DNA, and might cause cancer or other negative health effects. Earth&#x2019;s atmosphere and/or magnetic field prevent these problems near Earth&#x2019;s surface. Cosmic radiation could also damage electronic equipment.<sup><sup>[29]</sup></sup><p></p><p>Some proposed solutions to this problem include mass shielding and magnetic shielding. Mass shielding would definitely work, but increases the energy requirements for travel.<sup><sup>[30]</sup></sup> It is less certain that magnetic shielding would work and magnetic shielding would require less mass,<sup><sup>[31]</sup></sup> but it may create other health risks.<sup><sup>[32]</sup></sup></p><p>Zubrin suggested that cosmic radiation was not a significant concern for interstellar travel. In support of this, he argued that we&#x2019;ve had people working close to nuclear reactors on nuclear submarines for over 50 years without major problems.<sup><sup>[33]</sup></sup> There is some additional background on this obstacle on Wikipedia, &#x201C;Health threat from cosmic rays.&#x201D;\n</p><h4>Human lifespan</h4>\nInterstellar voyages would take decades even at a significant fraction of the speed of light, and centuries or even millennia at more modest speeds. Therefore, it may be impossible to complete the trip within a human lifetime.<p></p><p>Proposed solutions to this problem include &#x201C;generation ships&#x201D; designed to rear children and train them to continue the voyage mid-flight, extending the human lifespan, suspended animation/re-animation upon arrival.<sup><sup>[34]</sup></sup>\n</p><h3>Surviving the journey: general physical challenges</h3>\n<h4>Interstellar medium</h4>\nFrom notes from a conversation with Charles Stross:<p></p><p>When travelling at a few percent of the speed of light, collisions with interstellar matter could cause significant damage to a vessel. Like radiation, this is something that might be overcome with appropriate shielding, though there are trade-offs between mass from shielding and the amount of energy necessary for propulsion.</p><p>The other people I spoke with about this issue also believed that this challenge could be overcome with appropriate shielding.<sup><sup>[35]</sup></sup> However, in his brief critique of the feasibility of interstellar missions, Ed Regis claimed that &#x201C;a high-speed collision with something as small as a grain of salt would be like encountering an H-bomb,&#x201D; suggesting he did not find it clear that this challenge could be overcome.<sup><sup>[36]</sup></sup>Some simple calculations, however, suggest that Regis&apos;s claim could only be realistic under very extreme assumptions. Assuming that 100% of the kinetic energy of the salt grain were converted to an explosion, a one-milligram mass could only produce a one-megaton explosion if the spacecraft were travelling at extremely close to light speed. At 10% of light speed the impact would be equivalent to about 100 kg of TNT, which is about 10 million times smaller. At 1% of light speed, the impact would be equivalent to about 1 kg of TNT.<sup><sup>[37]</sup></sup> These smaller explosions would not be negligible, but would probably be within the range of explosions that some bunkers and armored vehicles are capable of withstanding today.</p><p>In the paper I saw which investigated this issue most deeply, the authors recommended that, to be conservative, interstellar missions conducted at &#x2265; 0.1c prepare for the possibility of hitting dust particles of 0.01 mg. These collisions would carry only 1% of the kinetic energy discussed in the paragraph above.<sup><sup>[38]</sup></sup>\n</p><h4>Mechanical integrity over a long voyage</h4>\nFrom notes from a conversation with Geoffrey Landis:<p></p><p>Interstellar colonization may require making machines that will work for hundreds, thousands, or tens of thousands of years. Few machines can do this now, but perhaps appropriate repair systems would solve the problem. This is something that has not been done, and it&#x2019;s not clear that it could be done.</p><p>This concern was echoed by Anders Sandberg, though, in his estimation, this is a solvable engineering problem.<sup><sup>[39]</sup></sup>As with collisions with interstellar matter, this problem could potentially be addressed through trial and error if multiple colonization or exploration attempts are made.</p><p>Voyager 1, mentioned above, was launched on September 5, 1977, has had no critical mechanical failures for over 36 years, and is expected to continue to function until 2025.<sup><sup>[40]</sup></sup> Therefore, designing a spacecraft which would operate without a critical mechanical error for decades would not be wholly unprecedented.\n</p><h3>Building a civilization on the other end</h3>\n<h4>Genetic diversity</h4>\nA small population establishing a space colony would risk genetic damage due to inbreeding. This challenge might be addressed by bringing tens or hundreds of people (at increased energy costs) or bringing a few people plus enough embryos/gametes.<sup><sup>[41]</sup></sup>\n<h4>Hospitable location</h4>\nAccording to Wikipedia, &#x201C;There are 59 known stellar systems within 20 light years from the Sun, containing 81 visible stars,&#x201D; with some of the most appealing targets for interstellar travel in that range.<sup><sup>[42]</sup></sup><p></p><p>Stross argued that for most places on Earth throughout most of Earth&#x2019;s history, the planet wouldn&#x2019;t be very hospitable to humans, and this raises questions about how hard it would be to find a hospitable planet.<sup><sup>[43]</sup></sup></p><p>Potential solutions to this challenge include terraforming, searching through many planets, living off planets, and doing colonization with advanced AI, robotics, and manufacturing. Conceivably, advances in AI, robotics, and manufacturing would allow a civilization to thrive in space, eliminating the need for finding habitable planets.\n</p><h4>Infrastructure and scale requirements</h4>\nFrom a conversation with Charles Stross:<p></p><p>If the goal of space colonization is to create another civilization that is viable independently of the continued success of Earth-based civilization, there will be enormous challenges putting in place the large infrastructure necessary for an industrial civilization. Our current industrial civilization&#x2014;including systems for manufacturing, science, education, entertainment, support systems for people who aren&#x2019;t working, etc.&#x2014;probably requires at least a billion people. It is extremely hard to see how to create a space colony capable of replacing our current industrial civilization without at least hundreds of thousands of people.</p><p>This objection has received relatively little attention from people optimistic about space colonization. An exception to this is the science fiction novel <em>Learning the World</em> by Ken MacLeod, which focuses on a &#x201C;generation ship&#x201D; which has been travelling for thousands of years.</p><p>Possible solutions to this problem include bringing a large number of people, bringing a few people with a plan to build up a large infrastructure, and creating self-replicating robots that would be capable of building a civilization.<sup><sup>[44]</sup></sup></p><p>I did not see much detailed discussion of how this could be done,<sup><sup>[45]</sup></sup> but my intuition is that with advances in AI/robotics that would create machines capable of doing essentially all the tasks humans do, it should be possible for machines to build a civilization, even in a highly hostile environment. Stross expressed a similar view, though he had more uncertainty about whether such advances would ever be made.<sup><sup>[46]</sup></sup> This is an area where someone with different background assumptions would be especially likely to disagree with me, and I&#x2019;d have to do substantially more work to convince them of my position.\n</p><h3>Unknown obstacles</h3>\nTwo people I spoke with, Sandberg<sup><sup>[47]</sup></sup> and Zubrin,<sup><sup>[48]</sup></sup> thought that the impossibility of space colonization would require the discovery of new physics. Landis thought it was possible that there were unknown obstacles that could make space colonization impossible, and pointed to Fermi&#x2019;s Paradox as a reason that one might expect this.<sup><sup>[49]</sup></sup> Some possible explanations of Fermi&#x2019;s Paradox would point to other reasons that space colonization is impossible, but others would not. For example, crucial technologies could fail for some reason we can&#x2019;t appreciate, we might be living in a computer simulation without real stars outside our solar system, or some aliens could be preventing interstellar colonization. On the other hand, some step we&#x2019;ve already passed (e.g. the creation of the first life, multicellular organisms, intelligent life, industrialization) may happen only extremely rarely.<sup><sup>[50]</sup></sup>\n<h3>Will to do it</h3>\nPeople I spoke with disagreed about whether people would ever attempt interstellar colonization. Stross argued that interstellar colonization could only have a very uncertain and very long-term economic payoff, which would make it very hard to finance by appeal to economic motives.<sup><sup>[51]</sup></sup> However, Carl Shulman has forthcoming blog post in which he argues that there will probably eventually be strong economic incentives for interstellar colonization.<p></p><p>Other potential motives for colonizing space include curiosity and adventure, increasing civilization&#x2019;s ability to survive global catastrophes,<sup><sup>[52]</sup></sup> or increasing the size of civilization. More generally, in a world with great diversity of motivations, it may not be too hard to find someone who wants to try to colonize space.<sup><sup>[53]</sup></sup> Historically speaking, Gerard O&#x2019;Neill&#x2019;s L5 Society&#x2014;which had the aim of colonizing (interplanetary) space&#x2014;had over 10,000 members when it merged with the National Space Institute.<sup><sup>[54]</sup></sup></p><p>On the opposing side, something that could gather the energy necessary for interstellar travel might be weaponized, and this could&#x2014;imaginably&#x2014;create political opposition to interstellar travel.<sup><sup>[55]</sup></sup> Some further relevant notes from my conversation with Stross:</p><p>&#x201C;However, other ideological perspectives&#x2014;such as perspectives emphasizing the sacredness of nature&#x2014;might oppose space colonization. Opposition from these perspectives could prevent an optimistic minority from colonizing space even if it were feasible.&#x201D;\n</p><h2>Implications of advanced AI, robotics, and manufacturing for the feasibility of interstellar colonization</h2>\nAdvances in AI, robotics, or manufacturing could reduce many of the challenges of space colonization noted above.<sup><sup>[56]</sup></sup> Spelling this out a bit more, microgravity, cosmic radiation, human lifespan, genetic diversity, and habitability are either unproblematic or much less problematic with robots instead of humans. They could also reduce the amount of mass required for the mission, and therefore reduce the energy requirements. However, the use of advanced AI/robotics was rarely discussed in the materials I read, and was not emphasized by the people I interviewed, with the exception of Anders Sandberg.\n<h2>Conclusions and questions for further investigation</h2>\nMy impression is that the most informed people thinking about these issues believe that space colonization will eventually be possible, and that they believe this for reasons that make sense to me. In my view, the most uncertain step is the part where we build a civilization upon arriving at another star system. However, my intuition&#x2014;as stated above&#x2014;is that advances in AI and robotics will make it possible for machines to substitute for humans in building a civilization, even in environments that would be very inhospitable to humans.<p></p><p>Further investigation into this topic might focus on the following questions:\n</p><ol>\n\t<li>Does this list contain all significant known obstacles to interstellar colonization?</li>\n\t<li>How likely is it that there are unknown obstacles that might make interstellar colonization impossible?</li>\n\t<li>Given sufficiently advanced AI, robotics, and molecular manufacturing, is interstellar colonization definitely feasible?</li>\n\t<li>Given business as usual, is it likely that these advances in AI, robotics, and molecular manufacturing will eventually be made?</li>\n\t<li>Do we know, as Zubrin claimed, that some physically possible propulsion technologies could get us to several percent of the speed of light? If so, which are they and how do we know this?</li>\n</ol>\n<h2>My process</h2>\nThis research draws on interviews with Anders Sandberg, Geoffrey Landis, Robert Zubrin, and Charles Stross. Charles Stross was the most credible skeptic of the feasibility of space colonization that I could find. He was cited as a critic of <em>The High Frontier</em> on the book&#x2019;s Wikipedia page and multiple people I spoke with referred me to Stross as a skeptic that was worth speaking to.<p></p><p>A list of many, but not all, of the sources I considered, how I found them, and how closely I looked into them is available <a href=\"https://docs.google.com/spreadsheet/ccc?key=0AsP94pg6WYCIdGk2alNnTWEzOUNsazYtOU51SWdfZUE&amp;usp=drive_web#gid=0\">here</a>. I began by looking for articles arguing for or against the feasibility of space colonization, searching Google, Google Scholar, and Amazon.com with terms like &#x201C;space colonization,&#x201D; &#x201C;space settlement,&#x201D; &#x201C;possible,&#x201D; &#x201C;impossible,&#x201D; &#x201C;feasible,&#x201D; and &#x201C;infeasible,&#x201D; and then followed up on articles referenced in the most relevant sources I found. None of the people I interviewed were aware of notable articles arguing for the impossibility of space colonization that I had missed, though I asked all of them specifically about this. I spent 36 hours on this project.</p><p>I am grateful to Robin Hanson, Pablo Stafforini, Carl Shulman, Anders Sandberg, and Toby Ord for feedback on a draft of this review.\n</p><h2>Sources</h2>\nArmstrong, Stuart and Anders Sandberg. 2013. &#x201C;Eternity in six hours: Intergalactic spreading of intelligent life and sharpening the Fermi paradox.&#x201D; <em>Acta Astronautica </em>89 (2013): 1-13. URL: <a href=\"http://www.sciencedirect.com/science/article/pii/S0094576513001148\">http://www.sciencedirect.com/science/article/pii/S0094576513001148</a>.<p></p><p>Beckstead, Nick. 2013. &#x201C;On the Overwhelming Importance of Shaping the Far Future,&#x201D; 2013. PhD Thesis. Department of Philosophy, Rutgers University. URL: <a href=\"http://www.nickbeckstead.com/research\">http://www.nickbeckstead.com/research</a>.</p><p>Beckstead, Nick. 2014. Notes from a conversation with Geoffrey Landis. URL: <a href=\"http://www.nickbeckstead.com/conversations/landisapr2014\">http://www.nickbeckstead.com/conversations/landisapr2014</a>.</p><p>Beckstead, Nick. 2014. Notes from a conversation with Anders Sandberg. URL: <a href=\"https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxuYmVja3N0ZWFkfGd4OjY4YTcyZjM3ZDFlYTc4NDc\">https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxuYmVja3N0ZWFkfGd4OjY4YTcyZjM3ZDFlYTc4NDc</a>.</p><p>Beckstead, Nick. 2014. Notes from a conversation with Charles Stross. URL: <a href=\"http://www.nickbeckstead.com/conversations/stross\">http://www.nickbeckstead.com/conversations/stross</a>.</p><p>Beckstead, Nick. 2014. Notes from a conversation with Robert Zubrin. URL: <a href=\"http://www.nickbeckstead.com/conversations/zubrinmar2014\">http://www.nickbeckstead.com/conversations/zubrinmar2014</a>.</p><p>Bostrom, Nick. 2003. &#x201C;Astronomical Waste,&#x201D; <em>Utilitas</em>&#xA0;15(3): 308-314. URL: <a href=\"http://www.nickbostrom.com/astronomical/waste.html\">http://www.nickbostrom.com/astronomical/waste.html</a>.</p><p>Dyson, Freeman. 1965. &#x201C;Death of a Project,&#x201D; <em>Science</em> 149:141-144.&#x201D; URL: <a href=\"http://www.patrickmccray.com/wp/wp-content/uploads/2013/11/1965-Dyson-Death-of-a-Project.pdf\">http://www.patrickmccray.com/wp/wp-content/uploads/2013/11/1965-Dyson-Death-of-a-Project.pdf</a></p><p>Finkel, Alan. 2011. &#x201C;Forget space travel: it&#x2019;s just a dream,&#x201D; <em>Cosmos Magazine.</em> URL: <a href=\"http://cosmosmagazine.com/planets-galaxies/the-future-space-travel/\">http://cosmosmagazine.com/planets-galaxies/the-future-space-travel/</a>.</p><p>Freitas Jr, Robert A., and William Zachary. 1981. &quot;A self-replicating, growing lunar factory.&quot; <em>Princeton/AIAA/SSI Conference on Space Manufacturing</em>. Vol. 35. URL: <a href=\"http://www.rfreitas.com/Astro/GrowingLunarFactory1981.htm\">http://www.rfreitas.com/Astro/GrowingLunarFactory1981.htm</a>.</p><p>Gilster, Paul. 2004.&#xA0;<em>Centauri Dreams: Imagining and Planning Interstellar Exploration</em>. Springer.</p><p>Landis, Geoffrey. 1991. &#x201C;Magnetic Radiation Shielding: An Idea Whose Time Has Returned?,&#x201D; <em>Space Manufacturing 8: Energy and Materials from Space</em> 383-386. URL: <a href=\"http://www.islandone.org/Settlements/MagShield.html\">http://www.islandone.org/Settlements/MagShield.html</a>.</p><p>Landis, Geoffrey. 2004. &quot;Interstellar flight by particle beam.&quot;&#xA0;<em>Acta Astronautica </em>55:931-934. URL: <a href=\"http://www.sciencedirect.com/science/article/pii/S009457650400133X\">http://www.sciencedirect.com/science/article/pii/S009457650400133X</a>.</p><p>Mallove, Eugene F., and Gregory L. Matloff. 1989. <em>The Starflight Handbook: A Pioneer&apos;s Guide to Interstellar Travel</em>.</p><p>McLellan, Heather. 2011. &#x201C;Microgravity Makes Interstellar Travel Impossible, Say Experts,&#x201D; Escapist Magazine. URL: <a href=\"http://www.escapistmagazine.com/news/view/113507-Microgravity-Makes-Interstellar-Travel-Impossible-Say-Experts\">http://www.escapistmagazine.com/news/view/113507-Microgravity-Makes-Interstellar-Travel-Impossible-Say-Experts</a>.</p><p>Murphy, Tom. 2011. &#x201C;Galactic Scale Energy,&#x201D; <em>Do the Math</em>. URL: <a href=\"http://physics.ucsd.edu/do-the-math/2011/07/galactic-scale-energy/\">http://physics.ucsd.edu/do-the-math/2011/07/galactic-scale-energy/</a>.</p><p>National Research Council. 2014. <em>Pathways to Exploration: Rationales and Approaches for a U.S. Program of Human Space Exploration</em>. Washington, DC: The National Academies Press. URL: <a href=\"http://nap.edu/catalog.php?record_id=18801\">http://nap.edu/catalog.php?record_id=18801</a>.</p><p>O&apos;Neill, Gerard. 1977. <em>The High Frontier: Human Colonies in Space.</em> William Morrow and Company.</p><p>O&#x2019;Neill, Ian. 2008. &#x201C;Bad News: Interstellar Travel May Remain in Science Fiction,&#x201D; <em>Universe Today</em>. URL: <a href=\"http://www.universetoday.com/17044/bad-news-insterstellar-travel-may-remain-in-science-fiction/\">http://www.universetoday.com/17044/bad-news-insterstellar-travel-may-remain-in-science-fiction/</a>.</p><p>Parker, Eugene. 2006. &#x201C;Shielding Space Travelers.&#x201D; <em>Scientific American </em>294(3): 40-47.URL: http://engineering.dartmouth.edu/~d76205x/research/shielding/docs/Parker_06.pdf.</p><p>Piersma, Theunis. 2010. &#x201C;Why space is the impossible frontier,&#x201D; <em>New Scientist</em>. URL: <a href=\"http://www.newscientist.com/article/mg20827860.100-why-space-is-the-impossible-frontier.html#.U4ig4Pk7t8F\">http://www.newscientist.com/article/mg20827860.100-why-space-is-the-impossible-frontier.html#.U4ig4Pk7t8F</a>.</p><p>Regis, Ed. 2013. &#x201C;Being Told That Our Destiny Is Among The Stars,&#x201D; in <em>What Should We Be Worried About?</em> URL: <a href=\"http://edge.org/responses/q2013\">http://edge.org/responses/q2013</a>.</p><p>Sato, Rebecca. 2007. &#x201C;The &quot;Hawking Solution&quot;: Will Saving Humanity Require Leaving Earth Behind?,&#x201D; <em>The Daily Galaxy</em>. URL: <a href=\"http://www.dailygalaxy.com/my_weblog/2007/05/the_hawking_sol.html\">http://www.dailygalaxy.com/my_weblog/2007/05/the_hawking_sol.html</a>.</p><p>Stross, Charles. 2007. &#x201C;The High Frontier, Redux,&#x201D; <em>Charles&#x2019;s Diary</em>. URL: <a href=\"http://www.antipope.org/charlie/blog-static/2007/06/the-high-frontier-redux.html\">http://www.antipope.org/Charles/blog-static/2007/06/the-high-frontier-redux.html</a>.</p><p>Wikipedia, &#x201C;Artificial gravity.&#x201D; URL: <a href=\"http://en.wikipedia.org/wiki/Artificial_gravity\">http://en.wikipedia.org/wiki/Artificial_gravity</a>.</p><p>Wikipedia, &#x201C;Effect of space flight on the human body.&#x201D; URL: <a href=\"http://en.wikipedia.org/wiki/Effect_of_spaceflight_on_the_human_body\">http://en.wikipedia.org/wiki/Effect_of_spaceflight_on_the_human_body</a>.</p><p>Wikipedia, &#x201C;Fermi&#x2019;s Paradox.&#x201D; URL: <a href=\"http://en.wikipedia.org/wiki/Fermi_paradox\">http://en.wikipedia.org/wiki/Fermi_paradox</a>.</p><p>Wikipedia, &#x201C;Health threat from cosmic rays.&#x201D; URL: <a href=\"http://en.wikipedia.org/wiki/Health_threat_from_cosmic_rays\">http://en.wikipedia.org/wiki/Health_threat_from_cosmic_rays</a>.</p><p>Wikipedia, &#x201C;The High Frontier.&#x201D; URL: <a href=\"http://en.wikipedia.org/wiki/The_High_Frontier:_Human_Colonies_in_Space\">http://en.wikipedia.org/wiki/The_High_Frontier:_Human_Colonies_in_Space</a>.</p><p>Wikipedia, &#x201C;Interstellar travel.&#x201D; URL: <a href=\"http://en.wikipedia.org/wiki/Interstellar_travel\">http://en.wikipedia.org/wiki/Interstellar_travel</a>.</p><p>Wikipedia, &#x201C;L5 Society.&#x201D; URL: <a href=\"http://en.wikipedia.org/wiki/L5_Society\">http://en.wikipedia.org/wiki/L5_Society</a>.</p><p>Wikipedia, &#x201C;Proxima Centauri.&#x201D; URL: <a href=\"http://en.wikipedia.org/wiki/Proxima_Centauri\">http://en.wikipedia.org/wiki/Proxima_Centauri</a>.</p><p>Wikipedia, &#x201C;Self-replicating Spacecraft.&#x201D; URL: <a href=\"http://en.wikipedia.org/wiki/Self-replicating_spacecraft\">http://en.wikipedia.org/wiki/Self-replicating_spacecraft</a>.</p><p>Wikipedia, &#x201C;Space colonization.&#x201D; URL: <a href=\"http://en.wikipedia.org/wiki/Space_colonization\">http://en.wikipedia.org/wiki/Space_colonization</a>.</p><p>Wikipedia, &#x201C;Voyager 1.&#x201D; URL: <a href=\"http://en.wikipedia.org/wiki/Voyager_1\">http://en.wikipedia.org/wiki/Voyager_1</a>.</p><p><sup><sup>[1]</sup></sup> &#x201C;As a rough approximation, let us say the Virgo Supercluster contains 10^13 stars. One estimate of the computing power extractable from a star and with an associated planet-sized computational structure, using advanced molecular nanotechnology, is 10^42 operations per second. A typical estimate of the human brain&#x2019;s processing power is roughly 10^17 operations per second or less. Not much more seems to be needed to simulate the relevant parts of the environment in sufficient detail to enable the simulated minds to have experiences indistinguishable from typical current human experiences. Given these estimates, it follows that the potential for approximately 10^38 human lives is lost every century that colonization of our local supercluster is delayed; or equivalently, about 10^29 potential human lives per second.</p><p>While this estimate is conservative in that it assumes only computational mechanisms whose implementation has been at least outlined in the literature, it is useful to have an even more conservative estimate that does not assume a non-biological instantiation of the potential persons. Suppose that about 10^10 biological humans could be sustained around an average star. Then the Virgo Supercluster could contain 10^23 biological humans. This corresponds to a loss of potential equal to about 10^14 potential human lives per second of delayed colonization.&#x201D; Bostrom 2003, &#x201C;Astronomical Waste.&#x201D;</p><p><sup><sup>[2]</sup></sup> &#x201C;The lion&#x2019;s share of the expected duration of our existence comes from the possibility that our descendants colonize planets outside our solar system. There are many stars that we may be able to reach with future technology (about 1013 in our supercluster). Some of them will probably have planets that are hospitable to life, perhaps many of these planets could be made hospitable with appropriate technological developments. Some of these are near stars that will burn for much longer than our sun, some for as much as 100 trillion years (Adams, 2008, p. 39). If multiple locations were colonized, the risk of total destruction would dramatically decrease, since it would take independent global disasters, or a cosmological catastrophe, to destroy civilization. Because of this, it is possible that our descendants would survive until the very end, and that there could be extraordinarily large numbers of them.&#x201D; Beckstead 2013, &#x201C;On the Overwhelming Importance of Shaping the Far Future,&#x201D; p. 57.</p><p><sup><sup>[3]</sup></sup> When listing potential&#xA0; rationales for space exploration, they wrote:</p><p>&#x201C;<em>Human Survival</em>. It is not possible to say whether off-Earth settlements could eventually be developed that would outlive human presence on Earth and lengthen the survival of our species. This is a question that can only be settled by pushing the human frontier in space.&#x201D;</p><p>National Research Council, &#x201C;Pathways to Exploration: Rationales and Approaches for a U.S. Program of Human Space Exploration.&#x201D; p. S-2. Further discussion on pp. 2-26-27.</p><p><sup><sup>[4]</sup></sup> Examples include O&#x2019;Neill 1977, Mallove and Matloff 1989, Landis 2004, Gilster 2004, and Armstrong and Sandberg 2013.</p><p><sup><sup>[5]</sup></sup> &#x201C;&quot;Giving birth in zero gravity is going to be hell because gravity helps you [on Earth],&quot; said Athena Andreadis, a biologist from the University of Massachusetts Medical School. &quot;You rely on the weight of the baby.&quot;</p><p>All of this means that we&apos;re not going anywhere, perhaps not even Mars, until we master either artificial gravity or some seriously speedy travel methods. Although this news won&apos;t come as a surprise to anybody who&apos;s put serious thought into interstellar travel, it is humbling to be reminded of these things from time to time. Humans are perfectly adjusted for life on Earth; as Andreadis noted, we&apos;ll have to adapt to both the journey and the destination if we&apos;re ever to leave.&#x201D; McLellan 2011. &#x201C;Microgravity Makes Interstellar Travel Impossible, Say Experts.&#x201D;</p><p>&#x201C;Hawking, Obama and other proponents of long-term space travel are making a grave error. Humans cannot leave Earth for the several years that it takes to travel to Mars and back, for the simple reason that our biology is intimately connected to Earth.</p><p>To function properly, we need gravity. Without it, the environment is less demanding on the human body in several ways, and this shows upon the return to Earth.&#x201D; Piersma 2010. &#x201C;Why space is the impossible frontier.&#x201D;</p><p><sup><sup>[6]</sup></sup> &#x201C;Already there are huge challenges facing the notion of travelling to Proxima Centauri, but in a recent gathering of experts in the field of space propulsion, there are even more insurmountable obstacles to mankind&#x2019;s spread beyond the Solar System. In response to the idea we might make the Proxima trek in a single lifetime, Paulo Lozano, an assistant professor of aeronautics and astronautics at MIT and conference deligate said, &#x201C;In those cases, you are talking about a scale of engineering that you can&#x2019;t even imagine.&#x201D;</p><p>OK, so the speed simply isn&#x2019;t there for a quick flight over 4.3 light years. But there is an even bigger problem than that. How would these interstellar spaceships be fuelled? According to Brice N. Cassenti, an associate professor with the Department of Engineering and Science at Rensselaer Polytechnic Institute, at least 100 times the total energy output of the entire world would be required for the voyage. &#x201C;We just can&#x2019;t extract the resources from the Earth,&#x201D; Cassenti said during his conference presentation. &#x201C;They just don&#x2019;t exist. We would need to mine the outer planets.&#x201D;&#x201D; O&#x2019;Neill 2008, &#x201C;Bad News: Interstellar Travel May Remain in Science Fiction.&#x201D;</p><p>&#x201C;&quot;Human expansion across the Solar System is an optimist&#x2019;s fantasy. Why? Because of the clash of two titans: physics versus chemistry.</p><p>In the red corner, the laws of physics argue that an enormous amount of energy is required to send a human payload out of Earth&#x2019;s gravitational field to its deep space destination and back again.</p><p>In the blue corner, the laws of chemistry argue that there is a hard limit to how much energy you can extract from the rocket fuel, and that no amount of ingenuity will change that.&quot;&#x201D; Finkel 2011, &#x201C;Forget space travel: it&#x2019;s just a dream.&#x201D;</p><p><sup><sup>[7]</sup></sup> &#x201C;But traveling at significantly faster speeds requires prohibitive amounts of energy. If the starship were propelled by conventional chemical fuels at even ten percent of the speed of light, it would need for the voyage a quantity of propellant equivalent in mass to the planet Jupiter. To overcome this limitation, champions of interstellar travel have proposed &quot;exotic&quot; propulsion systems such as antimatter, pi meson, and space warp propulsion devices. Each of these schemes faces substantial difficulties of its own: for example, since matter and antimatter annihilate each other, an antimatter propulsion system must solve the problem of confining the antimatter and directing the antimatter nozzle in the required direction. Both pi meson and space warp propulsion systems are so very exotic that neither is known to be scientifically feasible.&#x201D; Regis 2013, &#x201C;Being Told That Our Destiny Is Among The Stars.&#x201D;</p><p><sup><sup>[8]</sup></sup> &#x201C;It&#x2019;s hard to think of notable pessimists. Charles Stross wrote a good essay arguing for pessimism about space colonies, and he might be a good person to talk to. (Nick raised Stross as the most credible pessimist he was aware of.)&#x201D; Notes from a conversation with Geoffrey Landis.</p><p>&#x201C;Science fiction writer Charles Stross wrote a critical essay with a similar title on the feasibility of interstellar space travel and making practical use of various moons and planets in our own Solar System: <em>The High Frontier: Redux</em>.&#x201D; Wikipedia, &#x201C;The High Frontier.&#x201D;</p><p>In our conversation on this topic, Anders Sandberg recommended that I speak with Charles Stross.</p><p><sup><sup>[9]</sup></sup> &#x201C;Stross has kept his eye on this literature over the years. He was disappointed to know that I didn&#x2019;t find any critiques of the feasibility of space colonization that were superior to his, though he wasn&#x2019;t aware of any more developed critiques.&#x201D; Notes from a conversation with Charles Stross</p><p><sup><sup>[10]</sup></sup> &#x201C;The people in the above groups would probably have opinions that are generally along the same lines as Dr. Landis. Some would be more optimistic, and some would be less optimistic. They would generally agree that space colonization is possible in principle, and most of the disagreement would be about how hard it is.&#x201D; Notes from a conversation with Geoffrey Landis</p><p><sup><sup>[11]</sup></sup> &#x201C;Most people at NASA generally haven&#x2019;t thought deeply about this question.&#x201D; Notes from a conversation with Geoffrey Landis</p><p><sup><sup>[12]</sup></sup> &#x201C;Proxima Centauri (Latin proxima, meaning &quot;next to&quot; or &quot;nearest to&quot;) is a red dwarf about 4.24 light-years from the Sun, inside the G-cloud, in the constellation of Centaurus. It was discovered in 1915 by Scottish astronomer Robert Innes, the Director of the Union Observatory in South Africa, and is the nearest known star to the Sun&#x2026;&#x201D; Wikipedia, &#x201C;Proxima Centauri.&#x201D;</p><p><sup><sup>[13]</sup></sup> &#x201C;Travelling at about 17 kilometers per second (11 mi/s)[35] it has the fastest heliocentric recession speed of any human-made object.&#x201D; Wikipedia, &#x201C;Voyager 1.&#x201D;</p><p><sup><sup>[14]</sup></sup> &#x201C;Voyager 1 was traveling at 17,043 m/s (38,120 mph) relative to the Sun (about 3.595 AU per year). It would need about 17,565 years at this speed to travel a complete light year.[3] To compare, Proxima Centauri, the closest star to the Sun, is about 4.2 light-years (or 2.65&#xD7;10<sup>5</sup> AU) distant. Were the spacecraft traveling in the direction of that star, 73,775 years would pass before reaching it.&#x201D; Wikipedia, &#x201C;Voyager 1.&#x201D;</p><p><sup><sup>[15]</sup></sup> &#x201C;It&apos;s going to be pretty boring in there, but I think we can conceive of our minimal manned interstellar mission as being about the size and mass of a Mercury capsule. And I&apos;m going to nail a target to the barn door and call it 2000kg in total&#x2026;.Now, let&apos;s say we want to deliver our canned monkey to Proxima Centauri within its own lifetime. We&apos;re sending them on a one-way trip, so a 42 year flight time isn&apos;t unreasonable. (Their job is to supervise the machinery as it unpacks itself and begins to brew up a bunch of new colonists using an artificial uterus. Okay?) This means they need to achieve a mean cruise speed of 10% of the speed of light. They then need to decelerate at the other end. At 10% of c relativistic effects are minor &#x2014; there&apos;s going to be time dilation, but it&apos;ll be on the order of hours or days over the duration of the 42-year voyage. So we need to accelerate our astronaut to 30,000,000 metres per second, and decelerate them at the other end. Cheating and using Newton&apos;s laws of motion, the kinetic energy acquired by acceleration is 9 x 10<sup>17</sup> Joules, so we can call it 2 x 10<sup>18 </sup>Joules in round numbers for the entire trip&#x2026;.our entire planetary economy runs on roughly 4 terawatts of electricity (4 x 10<sup>12</sup> watts). So it would take our total planetary electricity production for a period of half a million seconds &#x2014; roughly 5 days &#x2014; to supply the necessary va-va-voom.&#x201D; Stross 2007, &#x201C;The High Frontier, Redux.&#x201D;</p><p><sup><sup>[16]</sup></sup> &#x201C;In 2008, total worldwide energy consumption was 474 exajoules (132,000 TWh). This is equivalent to an average power use of 15 terawatts (2.0&#xD7;10<sup>10</sup> hp).&#x201D;</p><p><sup><sup>[17]</sup></sup> &#x201C;Since the beginning of the Industrial Revolution, we have seen an impressive and sustained growth in the scale of energy consumption by human civilization. Plotting data from the Energy Information Agency on U.S. energy use since 1650 (1635-1945, 1949-2009, including wood, biomass, fossil fuels, hydro, nuclear, etc.) shows a remarkably steady growth trajectory, characterized by an annual growth rate of 2.9%.&#x201D; Murphy 2011, &#x201C;Galactic-Scale Energy.&#x201D;</p><p><sup><sup>[18]</sup></sup> &#x201C;In 1968, Freeman Dyson looked at the economics of interstellar colonization, and it was prohibitively expensive. But he pointed out that there has been a long history of exponential growth in economic productivity and energy production. If these trends continue long enough, interstellar colonization will become much more affordable. On the other hand, exponential trends will not last forever.&#x201D; Notes from a conversation with Geoffrey Landis.</p><p><sup><sup>[19]</sup></sup> This is not clearly summarized in the text, but it is clear in the following graph:</p><p>And the following caption:</p><p>&#x201C;Earth surface temperature given steady 2.3% energy growth, assuming some source other than sunlight is employed to provide our energy needs and that its use transpires on the surface of the planet. Even a dream source like fusion makes for unbearable conditions in a few hundred years if growth continues. Note that the vertical scale is logarithmic.&#x201D;</p><p>and clarificatory remarks:</p><p>&#x201C;absorbs abundant energy from the sun&#x2014;far in excess of our current societal enterprise. The Earth gets rid of its energy by radiating into space, mostly at infrared wavelengths. No other paths are available for heat disposal. The absorption and emission are in near-perfect balance, in fact. If they were not, Earth would slowly heat up or cool down. Indeed, we have diminished the ability of infrared radiation to escape, leading to global warming. Even so, we are still in balance to within less than the 1% level. Because radiated power scales as the fourth power of temperature (when expressed in absolute terms, like Kelvin), we can compute the equilibrium temperature of Earth&#x2019;s surface given additional loading from societal enterprise.&#x201D;</p><p>Murphy 2011, &#x201C;Galactic-Scale Energy.&#x201D;</p><p><sup><sup>[20]</sup></sup> Eyeballing the graph in the above footnote, there is not a substantial temperature increase after 200 years, and at that point we have 100 times as much energy on his model. &#x201C;This post provides a striking example of the impossibility of continued growth at current rates&#x2014;even within familiar timescales. For a matter of convenience, we lower the energy growth rate from 2.9% to 2.3% per year so that we see a factor of ten increase every 100 years.&#x201D; Murphy 2011, &#x201C;Galactic-Scale Energy.&#x201D;</p><p><sup><sup>[21]</sup></sup> &#x201C;He said that there are identifiable propulsion technologies which would work&#x2014;given the laws of physics as they are currently known&#x2014;and these could get us up to a few percent of the speed of light, allowing us to get to the nearest stars in decades rather than millennia.&#x201D; Notes from a conversation with Robert Zubrin.</p><p><sup><sup>[22]</sup></sup> &#x201C;champions of interstellar travel have proposed &quot;exotic&quot; propulsion systems such as antimatter, pi meson, and space warp propulsion devices. Each of these schemes faces substantial difficulties of its own: for example, since matter and antimatter annihilate each other, an antimatter propulsion system must solve the problem of confining the antimatter and directing the antimatter nozzle in the required direction. Both pi meson and space warp propulsion systems are so very exotic that neither is known to be scientifically feasible.&#x201D; Regis 2013. &#x201C;Being Told That Our Destiny Is Among The Stars.&#x201D;</p><p><sup><sup>[23]</sup></sup> &#x201C;Orion is a project to design a vehicle which would be propelled through space by repeated nuclear explosions occurring at a distance behind it. The vehicle may be either manned or unmanned; it carries a large supply of bombs, and nlachinery for throwing them out at the right place and time for efficient propulsion; it carries shock absorbers to protect the machinery and the crew from destructive jolts, and sufficient shielding to protect against heat and radiation. The vehicle has, of course, never been built. The project in its 7 years of existence was confined to physics experiments, engineering tests of components, design studies, and theory. The total cost of the project was $10 million, spread over 7 years, and the end result was a rather firm technical basis for believing that vehicles of this type could be developed, tested, and flown. The technical findings of the project have not been seriously challenged by anybody. Its major troubles have been, from the beginning, political. The level of scientific and engineering talent devoted to it was, for a classified project, unusually high.&#x201D; Dyson 1965, &#x201C;Death of a Project,&#x201D; p. 141.</p><p><sup><sup>[24]</sup></sup> &#x201C;Microgravity causes bone decalcification and liquid redistribution in humans.&#x201D; Notes from a conversation with Anders Sandberg.</p><p>&#x201C;Short-term exposure to microgravity causes space adaptation syndrome, a self-limiting nausea caused by derangement of the vestibular system. Long-term exposure causes multiple health problems, one of the most significant being loss of bone and muscle mass. Over time these deconditioning effects can impair astronauts&#x2019; performance, increase their risk of injury, reduce their aerobic capacity, and slow down their cardiovascular system. As the human body consists mostly of fluids, gravity tends to force them into the lower half of the body, and our bodies have many systems to balance this situation. When released from the pull of gravity, these systems continue to work, causing a general redistribution of fluids into the upper half of the body. This is the cause of the round-faced &apos;puffiness&apos; seen in astronauts. Redistributing fluids around the body itself causes balance disorders, distorted vision, and a loss of taste and smell.&#x201D; Wikipedia, &#x201C;Effect of Space Flight on the Human Body.&#x201D;</p><p>&#x201C;Because weightlessness increases the amount of fluid in the upper part of the body, astronauts experience increased intracranial pressure. This appears to increase pressure on the backs of the eyeballs, affecting their shape and slightly crushing the optic nerve. This effect was noticed in 2012 in a study using MRI scans of astronauts who had returned to Earth following at least one month in space. Such eyesight problems may be a major concern for future deep space flight missions, including a manned mission to the planet Mars.&#x201D; Wikipedia, &#x201C;Effect of Space Flight on the Human Body.&#x201D;</p><p>&quot;If off-world colonization someday begins, many types of people will be exposed to these dangers, and the effects on the elderly and on the very young are completely unknown.&quot; Wikipedia, &#x201C;Effect of Space Flight on the Human Body.&#x201D;</p><p><sup><sup>[25]</sup></sup> &quot;If off-world colonization someday begins, many types of people will be exposed to these dangers, and the effects on the elderly and on the very young are completely unknown.&quot; Wikipedia, &#x201C;Effect of Space Flight on the Human Body.&#x201D;</p><p><sup><sup>[26]</sup></sup> &#x201C;One potential obstacle to human survival in space is the effect of microgravity on health and reproduction. Microgravity causes bone decalcification and liquid redistribution in humans. In addition, attempts to breed mice and fish eggs in space have come out badly; the prevailing view is that microgravity is the source of the problem.&#x201D; Notes from a conversation with Anders Sandberg.</p><p><sup><sup>[27]</sup></sup> &#x201C;One potential obstacle to human survival in space is the effect of microgravity on health and reproduction&#x2026;.However, this problem could be solved by rotating the station or vessel and thereby inducing artificial gravity.&#x201D; Notes from a conversation with Anders Sandberg.</p><p><sup><sup>[28]</sup></sup> See Wikipedia, &#x201C;Artificial Gravity.&#x201D;</p><p><sup><sup>[29]</sup></sup> &#x201C;Space radiation would damage both humans and electronic equipment left in space for a long time. This problem gets more severe for very long-distance travel at relativistic speeds.&#x201D; Notes from a conversation with Anders Sandberg.</p><p><sup><sup>[30]</sup></sup> &#x201C;It would be possible to prevent the negative consequences of radiation with sufficiently thick shielding on the space vessel, though this increases the mass of the vessel and the amount of resources required to travel. Error-correcting codes and other measures like shielded electronics could prevent fatal damage to computers.&#x201D; Notes from a conversation with Anders Sandberg.</p><p><sup><sup>[31]</sup></sup> &#x201C;This looks like a problem that could be addressed through shielding. Dr. Landis believes this problem can be solved by creating a strong enough magnetic field.&#x201D; Notes from a conversation with Geoffrey Landis.</p><p>&#x201C;One solution to the problem of shielding crew from particulate radiation in space is to use active electromagnetic shielding. Practical types of shield include the magnetic shield, in which a strong magnetic field diverts charged particles from the crew region, and the magnetic/electrostatic plasma shield, in which an electrostatic field shields the crew from positively charged particles, while a magnetic field confines electrons from the space plasma to provide charge neutrality. Advances in technology include high-strength composite materials, high temperature superconductors, numerical computational solutions to particle transport in electromagnetic fields, and a technology base for construction and operation of large superconducting magnets. These advances make electromagnetic shielding a practical alternative for near-term future missions.&#x201D; Landis 1991, &#x201C;Magnetic Radiation Shielding: An Idea Whose Time Has Returned?&#x201D;abstract.</p><p><sup><sup>[32]</sup></sup> &#x201C;A spherical shell of water or plastic could protect space travelers, but it would take a total mass of at least 400 tons&#x2014;beyond the capacity of heavy-lift rockets. A superconducting magnet would repel cosmic particles and weigh an estimated nine tons, but that is still too much, and the magnetic field itself would pose health risks. No other proposed scheme is even vaguely realistic.&#x201D; Parker, Eugene. &#x201C;Shielding Space Travelers.&#x201D; Pg 42.</p><p><sup><sup>[33]</sup></sup> &#x201C;Dr. Zubrin didn&#x2019;t think space dust was a serious concern, and argued that damage from radiation could clearly be prevented with adequate shielding. He pointed to the fact that we&#x2019;ve had people working close to nuclear reactors on nuclear submarines for over 50 years without major problems.&#x201D; Notes from a conversation with Robert Zubrin.</p><p><sup><sup>[34]</sup></sup> &#x201C;&#x2026;if interstellar travel must proceed more slowly, some possible solutions would include (i) a generation ship, (ii) freezing people and re-animating them upon arrival, and (iii) travelling with machines or uploads, possibly creating biological humans upon arrival.&#x201D; Notes from a conversation with Geoffrey Landis.</p><p>See &#x201C;Slow Missions&#x201D; under Wikipedia, &#x201C;Interstellar Travel.&#x201D;</p><p><sup><sup>[35]</sup></sup> &#x201C;There are good theoretical reasons to think there isn&#x2019;t much millimeter-sized gravel in interstellar space. But if there are sand-sized particles, that would be a challenge for interstellar travel at relativistic or near/relativistic speeds. Shielding may be a solution to this.&#x201D; Notes from a conversation with Geoffrey Landis.</p><p>&#x201C;There is interstellar dust. If objects are travelling at a high speed, hitting a very small piece of dust could do substantial damage. Travelling at a high speed is desirable for interstellar or intergalactic travel. The larger the vessel, the worse the problem. Intergalactic dust is less common, and would be much less of a problem.</p><p>This problem could be averted by creating adequate shielding. At a conservative end, we know that space dust does not prevent comet nuclei from doing interstellar travel at a few tens of km/s. A vessel could simply dig into a comet and use it as shielding (for a very slow trip). Anders believes that other shields could be constructed out of metal and graphene foils which would make interstellar and intergalactic space colonization possible with at higher speeds and with less mass.&#x201D; Notes from a conversation with Anders Sandberg.</p><p>&#x201C;I mentioned a couple of the potential obstacles to interstellar colonization that came up in my interview with Anders Sandberg: space dust (which might damage vessels moving close to the speed of light) and radiation. Dr. Zubrin didn&#x2019;t think space dust was a serious concern, and argued that damage from radiation could clearly be prevented with adequate shielding.&#x201D; Notes from a conversation with Robert Zubrin.</p><p><sup><sup>[36]</sup></sup> &#x201C;Even if by some miracle suitable propulsion systems became available, a starship traveling at relativistic speeds would have to be equipped with sophisticated collision detection and avoidance systems, given that a high-speed collision with something as small as a grain of salt would be like encountering an H-bomb.&#x201D; Regis 2013, &#x201C;Being Told That Our Destiny Is Among The Stars.&#x201D;</p><p><sup><sup>[37]</sup></sup> Calculations were based on WolframAlpha&#x2019;s formula for relativistic kinetic energy. I am grateful to Anders Sandberg and Carl Shulman for help selecting appropriate formulas and challenging Regis&#x2019;s calculation.</p><p><sup><sup>[38]</sup></sup> &#x201C;Observations over the last decade have revealed an unexpected high-mass tail to the local interstellar grain size distribution. Individual particles with masses as high as 10^-12 kg (4.5 &#x3BC;m radius) are almost certainly present. Moreover, radar detections of interstellar dust particles entering the Earth&#x2019;s atmosphere ([27] and references therein) imply a population of interstellar grains with masses &gt; 3*10-10 kg (corresponding to radii &gt; 30 &#x3BC;m). While there are difficulties reconciling the presence of such large grains with other astronomical observations [36], a conservative planning assumption would be that particles as large as 100 &#x3BC;m radius (10^-8 kg), and perhaps larger, might be encountered in the course of a several light-year journey through the LISM. The kinetic energy of such large particles striking an interstellar space vehicle with a relative velocity of 0.1c are considerable (4.5*10^6 J), and some kind of active dust detection and mitigation system may need to be considered.</p><p><sup><sup>[39]</sup></sup> &#x201C;Someone might question whether it&#x2019;s possible to create electronic devices that would work for very long periods of time in the presence of radiation, as would be necessary for space colonization. Anders believes this is a solvable engineering problem.&#x201D; Notes from a conversation with Anders Sandberg.</p><p><sup><sup>[40]</sup></sup> &#x201C;Voyager 1 is a 722-kilogram (1,592 lb) space probe launched by NASA on September 5, 1977 to study the outer Solar System. Operating for 36 years, 8 months and 17 days as of 22 May 2014, the spacecraft communicates with the Deep Space Network to receive routine commands and return data. At a distance of about 127.74 AU (1.911&#xD7;10<sup>10</sup> km) from the Earth as of May 8, 2014, it is the farthest human-made object from Earth&#x2026;.</p><p>On September 12, 2013, NASA announced that Voyager 1 had crossed the heliopause and entered interstellar space on August 25, 2012, making it the first human-made object to do so&#x2026;.The probe is expected to continue its mission until 2025, when it will be no longer supplied with enough power from its generators to operate any of its instruments.&#x201D; Wikipedia, &#x201C;Voyager 1.&#x201D;</p><p><sup><sup>[41]</sup></sup> &quot;In 2002, the anthropologist John H. Moore estimated that a population of 150&#x2013;180 would allow normal reproduction for 60 to 80 generations &#x2014; equivalent to 2000 years.</p><p>A much smaller initial population of as little as two women should be viable as long as human embryos are available from Earth. Use of a sperm bank from Earth also allows a smaller starting base with negligible inbreeding.</p><p>Researchers in conservation biology have tended to adopt the &quot;50/500&quot; rule of thumb initially advanced by Franklin and Soule. This rule says a short-term effective population size (Ne) of 50 is needed to prevent an unacceptable rate of inbreeding, whereas a long&#x2010;term Ne of 500 is required to maintain overall genetic variability. The Ne = 50 prescription corresponds to an inbreeding rate of 1% per generation, approximately half the maximum rate tolerated by domestic animal breeders. The Ne = 500 value attempts to balance the rate of gain in genetic variation due to mutation with the rate of loss due to genetic drift.&quot; Wikipedia, &#x201C;Space colonization.&#x201D;</p><p><sup><sup>[42]</sup></sup> Wikipedia, &#x201C;Interstellar Travel.&#x201D; See the article for more detail.</p><p><sup><sup>[43]</sup></sup> &#x201C;Finding a hospitable location: If you consider all the past and future locations on Earth over time, only a very small fraction of them (probably less than 0.1%) would be habitable for humans. Why? 80% of the Earth&#x2019;s surface is covered by water, much of the Earth is too cold, there has only been enough oxygen for humans during the last 600 million years, a billion years from now the Earth will no longer be habitable for humans, and so on.&#x201D; Notes from a conversation with Charles Stross.</p><p><sup><sup>[44]</sup></sup> For a discussion of the last possibility, see Wikipedia, &#x201C;Self-replicating Spacecraft.&#x201D;</p><p><sup><sup>[45]</sup></sup> While commenting on this paper, Anders Sandberg pointed me to Freitas 1981, &#x201C;A self-replicating, self-growing lunar factory,&#x201D; which attempted to outline something along these lines. I have not yet looked closely at this paper, and feel it would take a substantial effort on my part to tell whether the idea would hold up. But someone who wanted to explore these issues further might look at this paper and other papers citing it to get some idea of what kind of thinking has already been done on this topic.</p><p><sup><sup>[46]</sup></sup> &#x201C;Given the existence of mind uploading or advanced AI, Stross sees no insurmountable obstacle to interstellar colonization. In this scenario, it seems that a near-light-speed colonization wave could occur roughly along the lines envisioned by Robert Bradbury.</p><p>In Stross&#x2019;s view, it is not settled whether mind uploading or advanced AI are feasible in principle. If the mind is inherently analog or quantum, digital uploads may be impossible.&#x201D; Notes from a conversation with Charles Stross.</p><p><sup><sup>[47]</sup></sup> &#x201C;This would be very surprising to Anders. In his view, it would probably require learning new physics for us to learn that space colonization is in principle infeasible for some reason not listed here.&#x201D; Notes from a conversation with Anders Sandberg.</p><p><sup><sup>[48]</sup></sup> &#x201C;&#x201C;I asked Dr. Zubrin whether he could imagine anything we could learn&#x2014;consistent with everything we currently know about physics&#x2014;that would mean space colonization would be impossible. He said he could not think of anything remotely plausible fitting the description. It&#x2019;s just a question of how fast we can get there.&#x201D; Notes from a conversation with Robert Zubrin.</p><p><sup><sup>[49]</sup></sup> &#x201C;It&#x2019;s possible that there are unknown obstacles. Most of the obstacles we&#x2019;ve discussed seem like they could be overcome. If they can and these are the only obstacles, that makes the Fermi Paradox more puzzling. Someone could argue that this suggests that interstellar colonization is impossible for some unknown reason.&#x201D; Notes from a conversation with Geoffrey Landis.</p><p><sup><sup>[50]</sup></sup> See Wikipedia, &#x201C;Fermi&#x2019;s Paradox&#x201D; for further discussion.</p><p><sup><sup>[51]</sup></sup> &#x201C;Creating a civilization of this size outside of the Earth would require a very large economic investment, and one that could (at best) have only a very long-term payoff. We currently do not have institutions which function well for highly unprecedented ventures which would require decades&#x2014;or perhaps even centuries&#x2014;to pay off. This issue is further explored in Stross&#x2019;s novel <em>Neptune&#x2019;s Brood</em>.&#x201D; Notes from a conversation with Charles Stross.</p><p><sup><sup>[52]</sup></sup> &#x201C;Professor Stephen Hawking, celebrated expert on the cosmological theories of gravity and black holes, believes that traveling into space is the only way humans will be able to survive in the long-term. He has said, &quot;Life on Earth is at the ever-increasing risk of being wiped out by a disaster such as sudden global warming, nuclear war, a genetically engineered virus or other dangers ... I think the human race has no future if it doesn&apos;t go into space.&quot;&#x201D; Sato 2007, &#x201C;The &quot;Hawking Solution&quot;: Will Saving Humanity Require Leaving Earth Behind?&#x201D;</p><p><sup><sup>[53]</sup></sup> &#x201C;Anders can see various reasons people might want to do this eventually: diversity of preferences, desire to do research, desire to continue civilization.&#x201D; Notes from a conversation with Anders Sandberg.</p><p>&#x201C;There is a lot of diversity in the world, and Landis&#x2019;s intuition is that enough people would want to do it.&#x201D; Notes from a conversation with Geoffrey Landis.</p><p><sup><sup>[54]</sup></sup> &#x201C;In 1986 the Society, which had grown to about 10,000 members, merged with the 25,000 member National Space Institute, founded by German rocket engineer and Project Apollo program manager Wernher von Braun of NASA&apos;s Marshall Space Flight Center to form the present-day National Space Society.&#x201D; Wikipedia, L5 Society.</p><p><sup><sup>[55]</sup></sup> &#x201C;A possible issue is that because it might require so much energy to do interstellar colonization, anything that could produce that much energy could probably be weaponized. Conceivably, there could be opposition from a larger group of people uninterested in space colonization to creating something that could be used to create a dangerous weapon.&#x201D; Notes from a conversation with Geoffrey Landis.</p><p><sup><sup>[56]</sup></sup> &#x201C;Given the existence of mind uploading or advanced AI, Stross sees no insurmountable obstacle to interstellar colonization. In this scenario, it seems that a near-light-speed colonization wave could occur roughly along the lines envisioned by Robert Bradbury.</p><p>In Stross&#x2019;s view, it is not settled whether mind uploading or advanced AI are feasible in principle. If the mind is inherently analog or quantum, digital uploads may be impossible.&#x201D; Notes from a conversation with Charles Stross.</p><p>&#x201C;Space colonization is especially likely to be possible if advanced AGI and molecular manufacturing are possible&#x2014;as Anders believes they are&#x2014;though he also thinks it is possible if they are impossible.&#x201D; Notes from a conversation with Anders Sandberg.</p></body></html>", "user": {"username": "Nick_Beckstead"}}]