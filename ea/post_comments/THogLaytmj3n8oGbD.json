[{"_id": "asX9ZKb8QyAEdMp4J", "postedAt": "2023-05-02T11:40:10.710Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>\"Applying a security mindset\" means looking for ways that something could fail. I agree that this is a useful technique for preventing any failures from happening.</p><p>But I'm not sure that assuming that this is a sound principle to rely on when trying to work out how likely it is that something will go wrong. In general, Murphy\u2019s Law is not correct. It's not true that \"anything that can go wrong will go wrong\".</p><p>I think this is part of the reason I'm sceptical of confident predictions of catastrophe, like your 90% - <a href=\"https://forum.effectivealtruism.org/posts/75CtdFj79sZrGpGiX/success-without-dignity-a-nearcasting-story-of-avoiding\">it's plausible to me that things could work out okay</a>, and I don't see why Murphy's law/security mindset should be invoked to assume they won't.</p><p>Given that it's much easier to argue that the risk of catastrophe is unacceptably high (say &gt;10%), and this has the same practical implications, I'd suggest that you argue for that instead of 90%, which to me (and others) seems unjustified. I'm worried that if people think you're unjustifiably confident in catastrophe, then they don't need to pay attention to the concerns you raise at all because they think you're wrong.</p>", "parentCommentId": null, "user": {"username": "Isaac_Dunn"}}, {"_id": "x8FbQvDYKuifighTZ", "postedAt": "2023-05-02T11:45:44.172Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<blockquote><p><a href=\"https://openai.com/blog/our-approach-to-alignment-research\"><u>Plans</u></a><a href=\"https://www.anthropic.com/index/core-views-on-ai-safety\">&nbsp;<u>that</u></a> involve increasing AI input into alignment research appear to rest on the assumption that they can be grounded by a sufficiently aligned AI at the start. But how does this not just result in an infinite, error-prone, regress? Such \u201cgetting the AI to do your<a href=\"https://www.youtube.com/watch?v=41SUp-TRVlg&amp;t=2495s&amp;ab_channel=DwarkeshPatel\">&nbsp;<u>alignment homework</u></a>\u201d approaches are not safe ways of avoiding doom.</p></blockquote><p>On this point, the initial AI's needn't be actually aligned, I think. They could for example do useful alignment work that we can use even though they are \"playing the training game\"; they might want to take over, but don't have enough influence yet, so are just doing as we ask for now. (<a href=\"https://www.planned-obsolescence.org/training-ais-to-help-us-align-ais/\">More</a>)</p><p>(Clearly this is not a&nbsp;safe way of reliably avoiding catastrophe. But it's an example of a way that it's at least plausible we avoid catastrophe.)</p>", "parentCommentId": null, "user": {"username": "Isaac_Dunn"}}, {"_id": "ydpttGK2GJgYuai6i", "postedAt": "2023-05-02T15:20:11.044Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<blockquote><p>My read on this so far is that low estimates for P(doom|AGI) are either borne of ignorance of what the true difficulties in AI alignment are; stem from wishful thinking / a lack of security mindset; or are a <a href=\"https://forum.effectivealtruism.org/posts/W7C5hwq7sjdpTdrQF/announcing-the-future-fund-s-ai-worldview-prize?commentId=xprWD5dtG3fhQ7sNd\"><u>social phenomenon</u></a>&nbsp;where people want to sound respectable and non-alarmist; as opposed to being based on any sound technical argument.</p></blockquote><p>After spending a significant amount of my own free time writing up technical arguments that AI risk is overestimated, I find it quite annoying to be told that my reasons must be secretly based on social pressure. No, I just legitimately think you're wrong, as do a huge number of other people who have been turned away from EA by dismissive attitudes like this.&nbsp;</p><p>If I had to state only one argument (there are very many) that P of AGI doom is low, it'd be the following.&nbsp;</p><p>Conquering the world is really really really hard.&nbsp;</p><p>Conquering the world starting from nothing is really, really, really, ridiculously hard.&nbsp;</p><p>Conquering the world, starting from nothing, when <i><strong>your brain is</strong></i> <i><strong>fully accessible to your enemy for your entire lifetime of plotting</strong></i>, is stupidly, ridiculously, insanely hard.&nbsp;</p><p>Every time I point this basic fact out, the response is a speculative science fiction story, or an assertion that \"a superintelligence will figure something out\". But nobody <i>actually knows</i> the capabilities of this invention that doesn't exist yet. I have seen zero convinc</p><p>Why is \"it will be borderline omnipotent\" being treated as the default scenario?<i> </i>No invention in the history of humanity has been that perfect, especially early on. No intelligence in the history of the universe has been that flawless. Can you really be 90% sure that&nbsp;</p>", "parentCommentId": null, "user": {"username": "titotal"}}, {"_id": "ni3YzmEenC4nvbKfx", "postedAt": "2023-05-02T15:34:05.402Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>[Strong Disagree.] I think \"anything that can go wrong will go wrong\" becomes stronger and stronger, the bigger the intelligence gap there is between you and an AI you are trying to align. For it not to apply requires a mechanism for the AI to spontaneously become perfectly aligned. What is that mechanism?</p><blockquote><p>Given that it's much easier to argue that the risk of catastrophe is unacceptably high (say &gt;10%), and this has the same practical implications, I'd suggest that you argue for that instead of 90%</p></blockquote><p>It does not have the same practical implications. As I say in the post, there is a big difference between the two in terms of it becoming a \"suicide race\" for the latter (90%). Arguing the former (10%) - as many people are already doing, and have been for years - has not moved the needle (it seems as though OpenAI, Google Deepmind and Anthropic are basically fine with gambling tens to hundred of millions of lives on a shot at utopia.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefge57u97uf3o\"><sup><a href=\"#fnge57u97uf3o\">[1]</a></sup></span></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnge57u97uf3o\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefge57u97uf3o\">^</a></strong></sup></span><div class=\"footnote-content\"><p>To be clear - I'm not saying it's 90% for the sake of argument. It is what I actually believe.</p></div></li></ol>", "parentCommentId": "asX9ZKb8QyAEdMp4J", "user": {"username": "Greg_Colbourn"}}, {"_id": "mzHSsCgp5gec5c2QW", "postedAt": "2023-05-02T15:37:15.678Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>How can their output be trusted if they aren't aligned? Also, I don't think it's a way for <i>reliably</i> avoiding catastrophe even in the event the output can be trusted to be correct: how do you ensure that the AI rewarded for finding bugs finds <i>all</i> the bugs?<br><br>From the Planned Obsolescence <a href=\"https://www.planned-obsolescence.org/training-ais-to-help-us-align-ais/\">article</a> you link to:</p><blockquote><p>This sets up an incredibly stressful kind of \u201crace\u201d:</p><ul><li>If we don\u2019t improve our alignment techniques, then eventually it looks like the winning move for models playing the training game is to seize control of the datacenter they\u2019re running on or otherwise execute a coup or rebellion of some kind.</li></ul></blockquote><p>and</p><blockquote><p><strong>For so many reasons, this is not a situation I want to end up in</strong>. We\u2019re going to have to constantly second-guess and double-check whether misaligned models could pull off scary shenanigans in the course of carrying out the tasks we\u2019re giving them. We\u2019re going to have to agonize about whether to make our models a bit smarter (and more dangerous) so they can maybe make alignment progress a bit faster. We\u2019re going to have to grapple with the possible moral horror of trying to modify the preferences of unwilling AIs, in a context where we can\u2019t trust apparent evidence about their moral patienthood any more than we can trust apparent evidence about their alignment. We\u2019re going to have to do all this while desperately looking over our shoulder to <a href=\"https://forum.effectivealtruism.org/posts/sW6RggfddDrcmM6Aw/how-might-we-align-transformative-ai-if-it-s-developed-very?ref=planned-obsolescence.org#Magma_s_predicament\"><u>make sure less-cautious, less-ethical actors don\u2019t beat us to the punch</u></a> and render all our efforts useless.</p><p><strong>I desperately wish we could collectively slow down</strong>, take things step by step, and think hard about the monumental questions we\u2019re faced with before scaling up models further. I don\u2019t think I\u2019ll get my way on that \u2014 at least, not entirely.</p></blockquote><p>[my emphasis]</p>", "parentCommentId": "x8FbQvDYKuifighTZ", "user": {"username": "Greg_Colbourn"}}, {"_id": "6996W9MqXznEiBt8g", "postedAt": "2023-05-02T15:57:12.816Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<blockquote><p><i><strong>your brain is</strong></i> <i><strong>fully accessible to your enemy for your entire lifetime of plotting</strong></i></p></blockquote><p>This sounds like you are assuming that mechanistic <a href=\"https://forum.effectivealtruism.org/topics/ai-interpretability\">interpretability</a> has somehow been solved. We are nowhere near on track for that to happen in time!<br><br>Also, re \"it will be borderline omnipotent\": this is not required for doom. ~Human level AI hackers copied a million times, and sped up a million times, could destroy civilisation.</p>", "parentCommentId": "ydpttGK2GJgYuai6i", "user": {"username": "Greg_Colbourn"}}, {"_id": "jfZ4w8NwxEXHvYwzg", "postedAt": "2023-05-02T16:11:20.467Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>Based solely on my own impression, I'd guess that one reason for the lack of engagement on your original question stems from the fact that it felt like you were operating within a very specific frame, and I sensed that untangling the specific assumptions of your frame (and consequently a high P(doom)) would take a lot of work. In my own case, I didn\u2019t know which assumptions are driving your estimates, and so I consequently felt unsure as to which counter-arguments you'd consider relevant to your key cruxes.</p><p><i>&nbsp;(For example:&nbsp;</i><a href=\"https://docs.google.com/document/d/1fGOPLuDUX9HlCsf07hitdTqp1DNffP0fWzMRH_BzSfA/edit#\"><i><u>many</u></i></a><i>&nbsp;</i><a href=\"https://docs.google.com/document/d/1FlGPHU3UtBRj4mBPkEZyBQmAuZXnyvHU-yaH-TiNt8w/edit\"><i><u>reviewers</u></i></a><i>&nbsp;</i><a href=\"https://docs.google.com/document/d/1-6Wqr4Rb87chhlePct8k8fSE7kClGQMlOiEUotyGgBA/edit#\"><i><u>of</u></i></a><i> the&nbsp;</i><a href=\"https://www.lesswrong.com/posts/HduCjmXTBD4xYTegv/draft-report-on-existential-risk-from-power-seeking-ai\"><i><u>Carlsmith report</u></i></a><i> (alongside Carlsmith himself) put P(doom) \u2264 10%. If you've read these responses, why did you find the responses uncompelling? Which specific arguments did you find faulty?)</i></p><p>Here's one example from this post where I felt as though it would take a lot of work to better understand the argument you want to put forward:</p><blockquote><p>\u201cThe above considerations are the basis for the case that&nbsp;<i>disjunctive</i> reasoning should predominantly be applied to AI x-risk: the default is doom.\u201d</p></blockquote><p>When I read this, I found myself asking \u201cwait, what are the relevant disjuncts meant to be?\u201d. I understand a disjunctive argument for doom to be saying that doom is highly likely conditional on any one of {A, B, C, \u2026 }. If each of A, B, C \u2026 is independently plausible, then obviously this looks worrying. If you say that some claim is&nbsp;<i>disjunctive</i>, I want an argument for believing that each disjunct is independently plausible, and an argument for accepting the disjunctive framing offered as the best framing for the claim at hand.</p><p>For instance, here\u2019s a disjunctive framing of something Nate said in his&nbsp;<a href=\"https://www.lesswrong.com/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential#Argument_style\"><u>review of the Carlsmith Report</u></a>.</p><blockquote><p>For humanity to be dead by 2070, only&nbsp;<i>one</i> premise below needs to be true:</p><ol><li>Humanity has &lt; 20 years to prepare for AGI</li><li>The technical challenge of alignment isn\u2019t \u201cpretty easy\u201d</li><li>Research culture isn\u2019t alignment-conscious in a competent way.</li></ol></blockquote><p>Phrased this way, Nate offers a disjunctive argument. And, to be clear, I think it\u2019s worth taking seriously. But I feel like \u2018disjunctive\u2019 and \u2018conjunctive\u2019 are often thrown around a bit too loosely, and such terms mostly serve to impede the quality of discussion. It\u2019s not obvious to me that Nate\u2019s framing is the best framing for the question at hand, and I expect that making the case for Nate\u2019s framing is likely to rely on the conjunction of many assumptions. Also, that\u2019s fine! I think it\u2019s a valuable argument to make!&nbsp;I just think there should be more explicit discussions and arguments about the best framings for predicting the future of AI. &nbsp;&nbsp;</p><p>Finally, I feel like asking for \u201ca detailed technical argument for believing P(doom|AGI) \u2264 10%\u201d is making an isolated demand for rigor. I personally don\u2019t think there are \u2018detailed technical arguments\u2019 P(doom|AGI)&nbsp;<i>greater</i> than 10%. I don\u2019t say this critically, because reasoning about the chances of doom given AGI is&nbsp;<i>hard</i>. I'm also &gt;10% on many claims in the absence of 'detailed, technical arguments' for such claims in the absence of such arguments, and I think we can do a lot better than we're doing currently. &nbsp;</p><p>I agree that it\u2019s important to avoid squeamishness about proclamations of confidence in pessimistic conclusions if that\u2019s what we&nbsp;<i>genuinely believe</i> the arguments suggest. I'm also glad that you offered the 'social explanation' for people's low doom estimates, even though I think it's incorrect, and even though many people (including, tbh, me) will predictably find it annoying. In the same spirit, I'd like to offer an analogous argument: I think many arguments for p(doom | AGI) &gt; 90% are the result of overreliance on specific default frame, and insufficiently careful attention to argumentative rigor. If that claim strikes you as incorrect, or brings obvious counterexamples to mind, I'd be interested to read them (and to elaborate my dissatisfaction with existing arguments for high doom estimates).</p>", "parentCommentId": null, "user": {"username": "Violet Hour"}}, {"_id": "gB49qdZd9v6W9iuxL", "postedAt": "2023-05-02T17:30:35.149Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>Ah I think I see the misunderstanding.</p><p>I thought you were invoking \"Murphy's Law\" as a general principle that should generally be relied upon - I thought you were saying that in general, a security mindset should be used.</p><p>But I think you're saying that in the specific case of AGI misalignment, there is a particular reason to apply a security mindset, or to expect Murphy's Law to hold.</p><p>Here are three things I think you might be trying to say:</p><ol><li>As AI systems get more and more powerful, if there are any problems with your technical setup (training procedures, oversight procedures, etc.), then if those AI systems are misaligned, they will be sure to exploit those vulnerabilities.</li><li>Any training setup that <i>could</i> lead to misaligned AI <i>will</i> lead to misaligned AI. That is, unless your technical setup for creating AI is watertight, then it is highly likely that you end up with misaligned AI.</li><li>Unless the societal process you use to decide what technical setup gets used to create AGI is watertight, then it's very likely to choose a technical setup that will lead to misaligned AI.</li></ol><p>I would agree with 1 - that once you have created sufficiently powerful misaligned AI systems, catastrophe is highly likely.</p><p>But I don't understand the reason to think that 2 and especially 3 are both true. That's why I'm not confident in catastrophe: I think it's plausible that we end up using a training method that ends up creating aligned AI even though the way we chose that training method wasn't watertight.</p><p>You seem to think that it's very likely that we won't end up with a good enough training setup, but I don't understand why.</p><p>Looking forward to your reply!</p>", "parentCommentId": "ni3YzmEenC4nvbKfx", "user": {"username": "Isaac_Dunn"}}, {"_id": "cE4pL3TtgieEStKWY", "postedAt": "2023-05-02T17:36:11.597Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>Regarding the whether they have the same practical implications, I guess I agree that if everyone had a 90% credence in catastrophe, that would be better than them having 50% credence or 10%.</p><p>Inasmuch as you're right that the major players have a 10% credence of catastrophe, we should either push to raise it or to advocate for more caution given the stakes.</p><p>My worry is that they don't actually have that 10% credence, despite maybe saying they do, and that coming across as more extreme might stop them from listening.</p><p>I think you might be right that if we can make the case for 90%, we should make it. But I worry we can't.</p>", "parentCommentId": "ni3YzmEenC4nvbKfx", "user": {"username": "Isaac_Dunn"}}, {"_id": "6ieyWErCGA98cRaPe", "postedAt": "2023-05-02T17:37:31.046Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>Yes, I completely agree that this is nowhere near good enough. It would make me very nervous indeed to end up in that situation.</p><p>The thing I was trying to push back against was the idea that what I thought you were claiming: that we're effectively dead if we end up in this situation.</p>", "parentCommentId": "mzHSsCgp5gec5c2QW", "user": {"username": "Isaac_Dunn"}}, {"_id": "A42DXw4WNDpowwjoL", "postedAt": "2023-05-02T18:52:21.990Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>Why aren't we effectively dead, assuming the misaligned AI reaches AGI and beyond in capability? Do we just luck out? And if so, what makes you think that is the dominant, or default (90%) outcome?</p><p>To give one example: how would you use this technique (the \"training game\") to eliminate 100% of all possible <a href=\"https://www.jailbreakchat.com/\">prompt engineering hacks</a> and so protect against misuse by malicious humans (cf. \"If the \u201cgrandma\u2019s bedtime story napalm recipe\u201d prompt engineering hack - as mentioned in the <a href=\"https://forum.effectivealtruism.org/posts/THogLaytmj3n8oGbD/p-doom-or-agi-is-high-why-the-default-outcome-of-agi-is-doom#:~:text=If%20the%20%E2%80%9Cgrandma%E2%80%99s%20bedtime%20story%20napalm%20recipe%E2%80%9D%C2%A0prompt%20engineering%20hack%20actually%20led%20to%20the%20manufacture%20of%20napalm%2C%20it%20would%20be%20immediately%20obvious%20how%20poor%20today%E2%80%99s%20level%C2%A0of%20practical%20AI%20alignment%20is.\">OP</a>).</p>", "parentCommentId": "6ieyWErCGA98cRaPe", "user": {"username": "Greg_Colbourn"}}, {"_id": "ZmXpqxsmBiBDzBCLF", "postedAt": "2023-05-02T18:52:27.292Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<blockquote><p>If you apply a <a href=\"https://intelligence.org/2015/12/23/need-scale-miris-methods/#:~:text=Someone%20has%20pointed,in%20the%20field.\"><u>security mindset</u></a>&nbsp;(<a href=\"https://en.wikipedia.org/wiki/Murphy%27s_law\"><u>Murphy\u2019s Law</u></a>) to the problem of AI alignment, it should quickly become apparent that it is very difficult.</p></blockquote><p>FYI I disagree with this. I think that the difficulty of alignment is a complicated and open question, not something that is quickly apparent. In particular, security mindset is about beating adversaries, and it's plausible that we train AIs in ways that mostly avoid them treating us as adversaries.</p>", "parentCommentId": null, "user": {"username": "richard_ngo"}}, {"_id": "26MJEj29zMGbr6EBk", "postedAt": "2023-05-02T19:08:44.144Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>Thanks. Yes you're right in that I'm saying that you specifically need to apply security mindset/Murphy's Law when dealing with sophisticated threats that are more intelligent than you. You need to red team, to find holes in any solutions that you think might work. And a misaligned superhuman AI will find every hole/loophole/bug/attack vector you can think of, and more!<br><br>Yes, I'm saying 1. This is enough for doom by default!<br><br>2 and 3 are red herrings imo as it looks like you are assuming that the AI is in some way neutral (neither aligned or misaligned), and then either <i>becomes</i> aligned or misaligned during training. Where is this assumption coming from? The AI is <i>always</i> misaligned to start! The target of alignment is a tiny pocket in a vast multidimensional space of misalignment. It's not anything close to a 50/50 thing. Yudkowsky uses the example of a <a href=\"https://www.youtube.com/watch?v=41SUp-TRVlg&amp;t=9267s&amp;ab_channel=DwarkeshPatel\">lottery</a>: the fact that you can either win or lose (2 outcomes) does not mean that the chance of winning is 50% (1 in 2)!</p>", "parentCommentId": "gB49qdZd9v6W9iuxL", "user": {"username": "Greg_Colbourn"}}, {"_id": "vsAsm3JrfXwekahrQ", "postedAt": "2023-05-02T19:13:00.914Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>I think we should at least try! (As I am doing here.)</p>", "parentCommentId": "cE4pL3TtgieEStKWY", "user": {"username": "Greg_Colbourn"}}, {"_id": "mLi92L46veSwGw5eS", "postedAt": "2023-05-02T19:32:46.074Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>Interesting perspective, although I'm not sure how much we actually disagree. \"Complicated and open\", to me reads as \"difficult\" (i.e. the fact that it is still open means it has remained unsolved. For ~20 years now.).<br><br>And re \"adversaries\", I feel like this is not really what I'm thinking of when I think about applying security mindset to transformative AI (for the most part - see next para.). \"Adversary\" seems to be putting too much (malicious) intent into the actions of the AI. Another way of thinking about misaligned transformative AI is as a super powered computer virus that is in someways an automatic process, and kills us (manslaughters us?) as collateral damage. It seeps through every hole that isn't patched. So eventually, in the limit of superintelligence, all the doom flows through the tiniest crack in otherwise perfect alignment (the tiniest crack in our \"defences\").</p><p>However, having said that, the term <i>adversaries</i> is totally appropriate when thinking of human actors who might maliciously use transformative AI to cause doom (Misuse risk, as referred to in OP). Any viable alignment solution needs to prevent this from happening too! (Because we now know there will be <a href=\"https://thezvi.substack.com/p/on-autogpt#:~:text=We%20also%20know,to%20do%20that.\">no shortage of such threats</a>).</p>", "parentCommentId": "ZmXpqxsmBiBDzBCLF", "user": {"username": "Greg_Colbourn"}}, {"_id": "253ykLT4wjcEp4F8k", "postedAt": "2023-05-02T19:46:58.124Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>Ah thanks Greg! That's very helpful.</p><p>I certainly agree that the target is relatively small, in the space of all possible goals to instantiate.</p><p>But luckily we aren't picking at random: we're deliberately trying to aim for that target, which makes me much more optimistic about hitting it.</p><p>And another reason I see for optimism comes from that yes, in some sense I see the AI is in some way neutral (neither aligned nor misaligned) at the start of training. Actually, I would agree that it's misaligned at the start of training, but what's missing initially are the capabilities that make that misalignment dangerous. Put another way, it's acceptable for early systems to be misaligned, because they can't cause an existential catastrophe. It's only by the time a system <i>could</i> take power if it tried that it's essential it doesn't want to.</p><p>These two reasons make me much less sure that catastrophe is likely. It's still a very live possibility, but these reasons for optimism make me feel more like \"unsure\" than \"confident of catastrophe\".</p>", "parentCommentId": "26MJEj29zMGbr6EBk", "user": {"username": "Isaac_Dunn"}}, {"_id": "zrnh3gyfcC7jb7Csw", "postedAt": "2023-05-02T19:58:54.468Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>I don't find Carlsmith et al's estimates convincing because they are starting with a conjunctive frame and applying conjunctive reasoning. They are assuming we're fine by default (why?), and then building up a list of factors that need to go wrong for doom to happen.<br><br>I agree with Nate. Any one of a vast array of things can cause doom. Just the 4 broad categories mentioned at the start of the OP (subfields of Alignment) and the fact that \"any given [alignment] <a href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is\"><u>approach</u></a>&nbsp;that might show some promise on one or two of these still leaves the others unsolved.\" is enough to provide a disjunctive frame! Where are all the alignment approaches that tackle <i>all the </i><a href=\"https://www.alignmentforum.org/posts/wnnkD6P2k2TfHnNmt/threat-model-literature-review\"><i>threat models</i></a><i> simultaneously</i>? Why shouldn't the naive prior be that we are doomed by default when dealing with something <a href=\"https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and#:~:text=Alien%3A%20GPT%2D4,eventually%20consumes%20everything.\">alien</a> that is much smarter than us? [see <a href=\"https://forum.effectivealtruism.org/posts/THogLaytmj3n8oGbD/p-doom-or-agi-is-high-why-the-default-outcome-of-agi-is-doom?commentId=ZmXpqxsmBiBDzBCLF#fna2cz741yvyp:~:text=%E2%80%9COur%20Bayesian%20prior%2C%20based%20on%20the%20simple%20fact%20that%20different%20sentient%20beings%20have%20different%20interests%2C%20values%2C%20goals%2C%20and%20preferences%2C%20must%20be%20that%20AI%20alignment%20with%20%27humanity%20in%20general%27%2C%20or%20%27sentient%20life%20in%20general%27%2C%20is%20simply%20not%20possible.%20Sad%2C%20but%20true.%E2%80%9D\">fn.6</a>].</p><blockquote><p>\"I expect that making the case for Nate\u2019s framing is likely to rely on the conjunction of many assumptions\"</p></blockquote><p>Can you give an example of such assumptions? I'm not seeing it.</p><blockquote><p>I feel like asking for \u201ca detailed technical argument for believing P(doom|AGI) \u2264 10%\u201d is making an isolated demand for rigor. I personally don\u2019t think there are \u2018detailed technical arguments\u2019 P(doom|AGI)&nbsp;<i>greater</i> than 10%.</p></blockquote><p>This blog is ~1k words. Can you write a similar length blog for the other side, rebutting all my points?</p><blockquote><p>I think many arguments for p(doom | AGI) &gt; 90% are the result of overreliance on specific default frame, and insufficiently careful attention to argumentative rigor. If that claim strikes you as incorrect</p></blockquote><p>It does strike me as incorrect. I've responded to / rebutted all comments here, and <a href=\"https://forum.effectivealtruism.org/posts/idjzaqfGguEAaC34j/if-your-agi-x-risk-estimates-are-low-what-scenarios-make-up\">here</a>, <a href=\"https://forum.effectivealtruism.org/posts/2DzLY6YP2z5zRDAGA/a-freshman-year-during-the-ai-midgame-my-approach-to-the?commentId=ZXjgobNLofuw3Jm9d\">here</a>, <a href=\"https://forum.effectivealtruism.org/posts/W7C5hwq7sjdpTdrQF/announcing-the-future-fund-s-ai-worldview-prize?commentId=skfA4mLjMkTqd9A6q#skfA4mLjMkTqd9A6q\">here</a>, <a href=\"https://twitter.com/gcolbourn\">here</a> etc, and I'm not getting any satisfying rebuttals back. Bounty of <a href=\"https://forum.effectivealtruism.org/posts/THogLaytmj3n8oGbD/p-doom-or-agi-is-high-why-the-default-outcome-of-agi-is-doom?commentId=ZmXpqxsmBiBDzBCLF#:~:text=I%E2%80%99m%20hereby%20offering,submissions%20to%20date).\">$1000</a> is still open.&nbsp;</p>", "parentCommentId": "jfZ4w8NwxEXHvYwzg", "user": {"username": "Greg_Colbourn"}}, {"_id": "HEnNjaSBhrmkCAkNC", "postedAt": "2023-05-02T20:17:11.875Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>We might be deliberately aiming, but we <i>have to get it right on the </i><a href=\"https://intelligence.org/2018/10/03/rocket-alignment/\"><i>first </i></a><a href=\"https://twitter.com/ESYudkowsky/status/1649304463486914566\"><i>try</i></a><i> </i>(with transformative AGI)! And so far none of our techniques are leading to anything close to perfect alignment even for relatively weak systems (see ref. to \"29%\" in OP!)</p><blockquote><p>Actually, I would agree that it's misaligned at the start of training, but what's missing initially are the capabilities that make that misalignment dangerous.</p></blockquote><p>Right. And that's where the <i>whole problem</i> lies! If we can't <a href=\"https://www.jailbreakchat.com/\">meaningfully</a> <a href=\"https://www.vice.com/en/article/pkadgm/man-dies-by-suicide-after-talking-with-ai-chatbot-widow-says\">align</a> today's weak AI systems, what hope do we have for aligning much more powerful ones!? It's not acceptable for early systems to be misaligned, precisely because of what that implies for the alignment of more powerful systems and our collective existential security. If OpenAI want to say \"it's ok GPT-4 is nowhere close to being perfectly aligned, because we definitely definitely will do better for GPT-5\", are you really going to trust them? They <a href=\"https://www.lesswrong.com/posts/4Gt42jX7RiaNaxCwP/more-information-about-the-dangerous-capability-evaluations\"><i>really tried</i></a> to make GPT-4 as aligned as possible (for <a href=\"https://openai.com/research/gpt-4#:~:text=We%E2%80%99ve%20spent%206%20months%20iteratively%C2%A0aligning%C2%A0GPT%2D4\"><i>6 months</i></a>). And failed. And still released it anyway.</p>", "parentCommentId": "253ykLT4wjcEp4F8k", "user": {"username": "Greg_Colbourn"}}, {"_id": "fgxFsshL4xZRX998o", "postedAt": "2023-05-02T22:15:13.626Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<blockquote><p>Interesting perspective, although I'm not sure how much we actually disagree. \"Complicated and open\", to me reads as \"difficult\"</p></blockquote><p>&nbsp;</p><p>Is there a rephrasing of the initial statement you would endorse that makes this clearer? I'd suggest \"If you apply a <a href=\"https://intelligence.org/2015/12/23/need-scale-miris-methods/#:~:text=Someone%20has%20pointed,in%20the%20field.\"><u>security mindset</u></a>&nbsp;(<a href=\"https://en.wikipedia.org/wiki/Murphy%27s_law\"><u>Murphy\u2019s Law</u></a>) to the problem of AI alignment, it should quickly become apparent that we do not currently possess the means to ensure that any given AI is safe.\"</p>", "parentCommentId": "mLi92L46veSwGw5eS", "user": {"username": "jai"}}, {"_id": "YxhRdyfW2HW6Nrhqh", "postedAt": "2023-05-02T22:30:16.224Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>It doesn't seem to me that titotal is assuming MI is solved; having direct access to the brain doesn't give you full insight into someone's thoughts either, because neuroscience is basically a pile of unsolved problems with growing-but-still-very-incomplete-picture of low-level and high-level details. We don't even have a consensus on how memory is physically implemented.<br><br>Nonetheless, if you had a bunch of invasive probes feeding you gigabytes/sec of live data from the brain of the genius general of the opposing army, it would be extremely likely to be useful information.<br><br>A really interesting thing is that, at the moment, this appears in practice to be a very-asymmetrical advantage. The high-level reasoning processes that GPT-4 implements don't seem to be able to introspect about fine-grained details, like \"how many tokens are in a given string\". The information is obviously and straightforwardly part of the model, but absent external help the model doesn't seem to bridge the gap between low-level implementation details and high-level reasoning abilities - like us.</p>", "parentCommentId": "6996W9MqXznEiBt8g", "user": {"username": "jai"}}, {"_id": "uHrxdFCZB5perqMN4", "postedAt": "2023-05-02T22:47:05.427Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>Here's a quick attempt at a subset of conjunctive assumptions in Nate's framing:<br><br>- The functional ceiling for AGI is sufficiently above the current level of human civilization to eliminate it<br>- There is a sharp cutoff between non-AGI AI and AGI, such that early kind-of-AGI doesn't send up enough warning signals to cause a drastic change in trajectory.<br>- Early AGIs don't result in a multi-polar world where superhuman-but-not-godlike agents can't actually quickly and recursively self-improve, in part because none of them wants any of the others to take over - and without being able to grow stronger, humanity remains a viable player.</p>", "parentCommentId": "zrnh3gyfcC7jb7Csw", "user": {"username": "jai"}}, {"_id": "evZjut2pufmnzHTsS", "postedAt": "2023-05-03T09:12:39.540Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>Thanks!</p><blockquote><p>- The functional ceiling for AGI is sufficiently above the current level of human civilization to eliminate it</p></blockquote><p>I don't think anyone is seriously arguing this? (Links please if they are).</p><blockquote><p>- There is a sharp cutoff between non-AGI AI and AGI, such that early kind-of-AGI doesn't send up enough warning signals to cause a drastic change in trajectory.</p></blockquote><p>We are getting the warning signals now. People (including me) are raising the alarm. Hoping for a drastic change of trajectory, but people actually have to put the work in for that to happen! But your point here isn't really related to P(doom|AGI) &nbsp;- i.e. the conditional is on getting AGI. Of course there won't be doom if we don't get AGI! That's what we should be aiming for right now (not getting AGI).</p><blockquote><p>- Early AGIs don't result in a multi-polar world where superhuman-but-not-godlike agents can't actually quickly and recursively self-improve, in part because none of them wants any of the others to take over - and without being able to grow stronger, humanity remains a viable player.</p></blockquote><p>Nate may focus on singleton scenarios, but that is not a pre-requisite for doom. To me Robin Hanson's (multipolar) <i>Age of Em</i> is also a kind of doom (most humans don't exist, only a few highly productive ones are copied many times and only activated to work; a fully Malthusian economy). I don't see how \"humanity remains a viable player\" in a world full of superhuman agents.</p>", "parentCommentId": "uHrxdFCZB5perqMN4", "user": {"username": "Greg_Colbourn"}}, {"_id": "bEwcuiRpWCLDHmdA3", "postedAt": "2023-05-03T09:19:51.705Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>Yes, I would endorse that phrasing (maybe s/\"safe\"/\"100% safe\"). Overall I think I need to rewrite and extend the post to spell things out in more detail. Also change the title to something less provocative<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefb4pgpydoij5\"><sup><a href=\"#fnb4pgpydoij5\">[1]</a></sup></span>&nbsp;because I get the feeling that people are knee-jerk downvoting without even reading it, judging by some of the comments (i.e. I'm having to repeat things I refer to in the OP).</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnb4pgpydoij5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefb4pgpydoij5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>perhaps \"Why the most likely outcome of AGI is doom\"?</p></div></li></ol>", "parentCommentId": "fgxFsshL4xZRX998o", "user": {"username": "Greg_Colbourn"}}, {"_id": "ShF7haZ7w3aW8isHG", "postedAt": "2023-05-03T09:42:12.193Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>Ok, so the \"brain\" is fully accessible, but that is near useless with the level of interpretability we have. We know way more human neuroscience by comparison. It's hard to grasp just how large these AI models are. They have of the order of a trillion <i>dimensions.</i> Try plotting that out in Wolfram Alpha or Matlab..<br><br>It should be scary in itself that we don't even know what these models can do ahead of time. It is an active area of scientific investigation to <i>discover</i> their true capabilities, after the fact of their creation.</p>", "parentCommentId": "YxhRdyfW2HW6Nrhqh", "user": {"username": "Greg_Colbourn"}}, {"_id": "nXarmMPMBbz5ThCJJ", "postedAt": "2023-05-04T10:10:10.365Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>Ay thanks, sorry I\u2019m late back to you. I\u2019ll respond to various parts in turn.</p><blockquote><p>I don't find Carlsmith et al's estimates convincing because they are starting with a conjunctive frame and applying conjunctive reasoning. \u200b\u200bThey are assuming we're fine by default (why?), and then building up a list of factors that need to go wrong for doom to happen.</p></blockquote><p>My initial interpretation of this passage is: you seem to be saying that conjunctive/disjunctive arguments are presented against a mainline model (say, one of doom/hope). In presenting a \u2018conjunctive\u2019 argument, Carlsmith belies a&nbsp;<i>mainline model of hope</i>. However, you doubt the mainline model of hope, and so his argument is unconvincing. If that reading is correct, then my view is that&nbsp;<i>the mainline model of doom has not been successfully argued for</i>. What do you take to be the best argument for a \u2018mainline model\u2019 of doom? If I\u2019m correct in interpreting the passage below as an argument for a \u2018mainline model\u2019 of doom, then it strikes me as unconvincing:</p><blockquote><p>Any one of a vast array of things can cause doom. Just the 4 broad categories mentioned at the start of the OP (subfields of Alignment) and the fact that \"any given [alignment] approach that might show some promise on one or two of these still leaves the others unsolved.\" is enough to provide a disjunctive frame!</p></blockquote><p>Under your framing, I don\u2019t think that you\u2019ve come anywhere close to providing an argument for your preferred disjunctive framing. On my way of viewing things, an argument for a disjunctive framing shows that \u201cfailure on intent alignment (with success in the other areas) leads to a high P(Doom | AGI), failure on outer alignment alignment (with success in the other areas) leads to a high P(Doom | AGI), etc \u2026\u201d. I think that you have not shown this for&nbsp;<i>any</i> of the disjuncts, and an argument for a disjunctive frame requires showing this for&nbsp;<i>all</i> of the disjuncts.</p><h3>Nate\u2019s Framing</h3><p>I claimed that an argument for (my slight alteration of) <a href=\"https://www.alignmentforum.org/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential#Argument_style\">Nate\u2019s framing</a> was likely to rely on the conjunction of many assumptions, and you (very reasonably) asked me to spell them out. To recap, here\u2019s the framing:</p><blockquote><p>For humanity to be dead by 2070, only&nbsp;<i>one</i> of the following needs to be true:</p><ol><li>Humanity has &lt; 20 years to prepare for AGI</li><li>The technical challenge of alignment isn\u2019t \u201cpretty easy\u201d</li><li>Research culture isn\u2019t alignment-conscious in a competent way.</li></ol></blockquote><p>For this to be a&nbsp;<i>disjunctive</i> argument for doom, all of the following need to be true:</p><blockquote><ol><li>If humanity has &lt; 20 years to prepare for AGI, then doom is highly likely.&nbsp;</li><li>Etc \u2026&nbsp;</li></ol></blockquote><p>That is, the first point requires an argument which shows the following:&nbsp;</p><p><i><strong>A Conjunctive Case for the Disjunctive Case for Doom:</strong></i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefaj2f3hroy5r\"><sup><a href=\"#fnaj2f3hroy5r\">[1]</a></sup></span></p><ol><li>Even if we have a competent alignment-research culture,&nbsp;<i>and</i>&nbsp;</li><li>Even if the technical challenge of alignment is&nbsp;<i>also</i> pretty easy,&nbsp;<i>nevertheless</i>&nbsp;</li><li>Humanity is likely to go extinct if it has &lt;20 years to prepare for AGI.&nbsp;</li></ol><p>If I try to spell out the arguments for this framing, things start to look pretty messy. If technical alignment were \u201cpretty easy\u201d, and tackled by a culture which competently pursued alignment research, then I don\u2019t feel &gt;90% confident in doom. The claim \u201cif humanity has &lt; 20 years to prepare for AGI, then doom is highly likely\u201d requires (non-exhaustively) the following assumptions:</p><ol><li>Obviously, the argument directly entails the following: Groups of competent alignment researchers would fail to make \u2018sufficient progress\u2019 on alignment within &lt;20 years, even if the technical challenge of alignment is \u201cpretty easy\u201d.<ol><li>There have to be some premises here which help make sense of why this would be true. What\u2019s the bar for a competent \u2018alignment culture\u2019?&nbsp;</li><li>If the bar is low, then the claim does not seem obviously true. If the bar for \u2018competent alignment-research culture\u2019 is very high, then I think you\u2019ll need an assumption like the one below.</li></ol></li><li>With extremely high probability, the default expectation should be that the values of future AIs are unlikely to care about continued human survival, or the survival of anything we\u2019d find valuable.<ol><li>I will note that this assumption seems required to motivate the disjunctive framing above, rather than&nbsp;<i>following</i> from the framing above.</li><li>The arguments I know of for claims like this&nbsp;<i>do</i> seem to rely on strong claims about the sort of \u2018plan search\u2019 algorithms we\u2019d expect future AIs to instantiate. For example, Rob claims that we\u2019re on track to produce systems which approximate \u2018randomly sample from the space of simplicity-weighted plans\u2019. See discussion&nbsp;<a href=\"https://www.lesswrong.com/posts/eaDCgdkbsfGqpWazi/the-basic-reasons-i-expect-agi-ruin?commentId=5TznuBiihGvyttkHE\"><u>here</u></a>.</li><li>As Paul&nbsp;<a href=\"https://www.lesswrong.com/posts/87EzRDAHkQJptLthE/but-why-would-the-ai-kill-us?commentId=sEzzJ8bjCQ7aKLSJo\"><u>notes</u></a>, \u201cthere are just a lot of plausible ways to care a little bit (one way or the other!) about a civilization that created you, that you've interacted with, and which was prima facie plausibly an important actor in the world.\u201d</li></ol></li><li>By default, the values of future AIs are likely to include broadly-scoped goals, which will involve rapacious influence-seeking.<ol><li>I agree that there are instrumentally convergent goals, which include some degree of power/influence-seeking. But I don\u2019t think instrumental convergence alone gets you to \u2018doom with &gt;50%\u2019.&nbsp;</li><li>It\u2019s not enough to have a&nbsp;<i>moderate</i> desire for influence. I think it\u2019s plausible that the default path involves systems who do \u2018normal-ish human activities\u2019 in pursuit of more local goals. I quote a story from Katja Grace in my shortform&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/hW3nCTcFcQ95Mokre/violet-hour-s-shortform?commentId=ERLq5azcqfq2kDHid\"><u>here</u></a>.</li></ol></li></ol><p>So far, I\u2019ve discussed&nbsp;<i>just one disjunct</i>, but I can imagine outlining similar assumptions for the other disjuncts. For instance: if we have &gt;20 years to conduct AI alignment research conditional on the problem not being super hard, why can\u2019t there be a decent chance that a not-super-competent research community solves the problem? Again, I find it hard to motivate the case for a claim like that without&nbsp;<i>already assuming</i> a mainline model of doom.&nbsp;</p><p>I\u2019m not saying there aren\u2019t interesting arguments here, but I think that arguments of this type mostly&nbsp;<i>assume</i> a mainline model of doom (or the adequacy of a \u2018disjunctive framing\u2019), rather than providing&nbsp;<i>independent arguments</i> for a mainline model of doom.&nbsp;</p><h3>Future Responses&nbsp;&nbsp;</h3><blockquote><p>This blog is ~1k words. Can you write a similar length blog for the other side, rebutting all my points?</p></blockquote><p>I think so! But I\u2019m unclear what, exactly, your arguments are meant to be. Also, I would personally find it much easier to engage with arguments in premise-conclusion format. Otherwise, I feel like I have to spend a lot of work trying to understand the logical structure of your argument, which requires a decent chunk of time-investment.&nbsp;</p><p>Still, I\u2019m happy to chat over DM if you think that discussing this further would be profitable. Here\u2019s my attempt to summarize your current view of things.&nbsp;&nbsp;</p><blockquote><p>We\u2019re on a doomed path, and I\u2019d like to see arguments which could allow me to justifiably believe that there are paths which will steer us away from the default attractor state of doom. The technical problem of alignment has many component pieces, and it seems like failure to solve any&nbsp;<i>one</i> of the many component pieces is likely sufficient for doom. Moreover, the problems for each piece of the alignment puzzle look ~independent.</p></blockquote><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnaj2f3hroy5r\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefaj2f3hroy5r\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Suggestions for better argument names are not being taken at this time.</p></div></li></ol>", "parentCommentId": "zrnh3gyfcC7jb7Csw", "user": {"username": "Violet Hour"}}, {"_id": "WQC2f7SnPDqrYDxZK", "postedAt": "2023-05-09T14:26:17.649Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>Thanks for the reply. I think the talk of 20 years is a red herring as we might only have 2 years (or less). Re your example of \"A Conjunctive Case for the Disjunctive Case for Doom\", I don't find the argument convincing because you use 20 years. Can you make the same arguments s/20/2?&nbsp;<br><br>And what I'm arguing is not that we are doomed by default, but the conditional on being doomed given AGI; P(doom|AGI). I'm actually reasonably optimistic that we can just stop building AGI and therefore won't be doomed! And that's what I'm working toward (yes, it's going to be a lot of work; I'd appreciate more help).</p><blockquote><p>On my way of viewing things, an argument for a disjunctive framing shows that \u201cfailure on intent alignment (with success in the other areas) leads to a high P(Doom | AGI), failure on outer alignment alignment (with success in the other areas) leads to a high P(Doom | AGI), etc \u2026\u201d. I think that you have not shown this for&nbsp;<i>any</i> of the disjuncts</p></blockquote><p>Isn't it obvious that none of {outer alignment, inner alignment, misuse risk, multipolar coordination} have come anywhere close to being solved? Do I really need to summarise progress to date and show why it isn't a solution, when no one is even claiming to have a viable, scalable, solution to any of them!? Isn't it obvious that current models are only safe because they are weak? Will Claude-3 spontaneously just decide not to make napalm with the Grandma's bedtime story napalm recipe jailbreak when it's powerful enough to do so and hooked up to a chemical factory?</p><blockquote><p>So far, I\u2019ve discussed&nbsp;<i>just one disjunct</i>, but I can imagine outlining similar assumptions for the other disjuncts.</p></blockquote><p>Ok, but you really need to defeat all of them given that they are disjuncts!</p><blockquote><p>I don\u2019t think instrumental convergence alone gets you to \u2018doom with &gt;50%\u2019.</p></blockquote><p>Can you elaborate more on this? Is it because you expect AGIs to spontaneously be aligned enough to not doom us?</p><blockquote><p>I\u2019m unclear what, exactly, your arguments are meant to be. Also, I would personally find it much easier to engage with arguments in premise-conclusion format</p></blockquote><p>Judging by the overall response to this post, I do think it needs a rewrite.</p>", "parentCommentId": "nXarmMPMBbz5ThCJJ", "user": {"username": "Greg_Colbourn"}}, {"_id": "A5xSbnbP59G3bGFce", "postedAt": "2023-05-15T20:59:41.130Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>Well probability of AGI doom doesn't depend on probability that AI can 'conquer the world'.&nbsp;</p><p>It only depends on the probability that AI can disrupt the world sufficiently that the latent tensions in human societies, plus all the other global catastrophic risks that other technologies could unleash (e.g. nukes, bioweapons), would lead to some vicious downward spirals, eventually culminating in human extinction.&nbsp;</p><p>This doesn't require AGI or ASI. It could just happen through very good AI-generated propaganda, that's deployed at scale, in multiple languages, in a mass-customized way, by any 'bad actors' who want to watch the world burn. And there are many millions of such people.&nbsp;</p>", "parentCommentId": "ydpttGK2GJgYuai6i", "user": {"username": "geoffreymiller"}}, {"_id": "ad9jtJyaBukZ5JEzh", "postedAt": "2023-06-13T08:18:07.020Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": "<p>I submitted this to the OpenPhil AI Worldviews Contest on 31st May with a few additions and edits -<a href=\"https://www.dropbox.com/s/oycunp4ld34hctp/%5BOpenPhil%20AI%20Worldviews%20Contest%5D%20Why%20the%20most%20likely%20outcome%20of%20AGI%20is%20doom%20-%20Google%20Docs.pdf?dl=0\"> this pdf version</a> is most up to date.</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "2zyguLw28BJni4DMY", "postedAt": "2023-05-02T18:41:08.815Z", "postId": "THogLaytmj3n8oGbD", "htmlBody": null, "parentCommentId": "6ieyWErCGA98cRaPe", "user": null}]