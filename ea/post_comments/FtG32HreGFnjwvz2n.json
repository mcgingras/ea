[{"_id": "rd3JdWvm4akKxcCBs", "postedAt": "2023-12-11T13:51:30.178Z", "postId": "FtG32HreGFnjwvz2n", "htmlBody": "<p><strong>Executive summary</strong>: Implementing logical decision theory helps AI companies align their capabilities progress to reduce existential risk and maximize mutual utility.</p><p><strong>Key points:</strong></p><ol><li>AI companies racing for capabilities creates unnecessary existential risk from unaligned superintelligences.</li><li>Logical decision theory incentives both companies to cooperate by slowing progress.</li><li>Cooperation allows more worlds where an aligned superintelligence satisfies their utility functions.</li><li>Defecting by racing progress causes more dead worlds and less mutual utility.</li><li>For negative utilitarians, cooperation also reduces remote suffering.</li><li>Some confidence in capabilities is warranted, but extreme overconfidence risks catastrophe.</li></ol><p>&nbsp;</p><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}]