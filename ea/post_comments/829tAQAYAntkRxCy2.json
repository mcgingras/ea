[{"_id": "JnZ4fk5bqn6TbzKho", "postedAt": "2022-10-18T15:55:24.218Z", "postId": "829tAQAYAntkRxCy2", "htmlBody": "<p>I would have enjoyed all your axioms in a list at the top</p>", "parentCommentId": null, "user": {"username": "nathan"}}, {"_id": "nuegMWr44SwG8kdn5", "postedAt": "2022-10-19T18:44:28.636Z", "postId": "829tAQAYAntkRxCy2", "htmlBody": "<p>Nitpick: I suspect EAs lean more toward Objective Bayesianism than Subjective Bayesianism. I'm unclear whether it's valuable to distinguish between them.</p>\n", "parentCommentId": null, "user": {"username": "PeterMcCluskey"}}, {"_id": "JJ4fGX8NgGTBSBeer", "postedAt": "2022-10-19T23:19:55.703Z", "postId": "829tAQAYAntkRxCy2", "htmlBody": "<p>Thanks for the suggestion! Done now. :)</p>", "parentCommentId": "JnZ4fk5bqn6TbzKho", "user": {"username": "Violet Hour"}}, {"_id": "agoBKXHwmgSyCSNgo", "postedAt": "2022-10-20T00:25:35.577Z", "postId": "829tAQAYAntkRxCy2", "htmlBody": "<p>I read Violet's post, am reviewing some of the background material, and just browsed some online stuff about Bayesianism. I would learn something by your elaboration on the difference you think applies to EA's.</p>", "parentCommentId": "nuegMWr44SwG8kdn5", "user": {"username": "Noah Scales"}}, {"_id": "hpfJhBGtWpDRnhYMo", "postedAt": "2022-10-20T06:42:18.135Z", "postId": "829tAQAYAntkRxCy2", "htmlBody": "<p>Hi, Violet Hour</p><p>I read your post a couple times, and the appendices. You're interested in exploring informal logic applied in EA thought. I will offer one of my biggest learnings about informal logic applied here in EA thought.&nbsp;</p><p>When evaluating longtermism's apparent commitment to a large future population in a few forum posts, I attempted to gather information about whether EA's think that:</p><ul><li>the act of procreation is a moral act.&nbsp;</li><li>continuation of the species is a moral on-going event.</li><li>hypothetical future humans (or other hypothetical beings) deserve moral status. &nbsp;</li></ul><p>I got the response, \"I believe in making happy people\" along with making people happy.&nbsp;</p><p>Exploring this with some helpful folks on the forum, I learned about an argument built on the money-pump template for thought experiments.</p><ol><li>There's a money pump argument for the combination of:<ol><li>moral preference for happy people.</li><li>moral indifference for making happy people.&nbsp;</li></ol></li><li>Keeping preferences 1.1 and 1.2 means I can be money-pumped.&nbsp;</li><li>Any preferences that can be money-pumped is irrational.</li><li>My moral preferences 1.1 and 1.2 are irrational in combination.</li><li>I should make my moral preferences rationally consistent.</li></ol><p>Basically anyone who thinks its good to make people happy should also think its good to make happy people.&nbsp;</p><p>I had never come across such an argument before! Money-pumping, this is new. As I have explored it, I have seen it increasingly as an argument from analogy. It works as follows:</p><ul><li>EA has this list of money-pump thought experiments.&nbsp;</li><li>Suppose I hold point of view X.</li><li>There's an analogous money-pump thought experiment.</li><li>By analogy, my point of view X is irrational.&nbsp;</li><li>I should change my point of view.</li></ul><p>Well, so there are three points of potential disagreement here:</p><ol><li>Is the money-pump thought experiment analogous?</li><li>if it is analogous, does it prove my point of view is irrational?</li><li>if my point of view (an ethical one) is irrational, should I change it?</li></ol><p><br>I doubt that EA thought experiments are analogous to my ethical beliefs and preferences, which I think are in the minority in some respects. However, in exploring the answers to each of those questions, an insider could determine the cluster of beliefs &nbsp;that EA's keep around rationality, mathematics, and ethics.&nbsp;</p><p>You claim to be a long-time EA, and I believe you. I don't know who you are, of course, but if you're a long-time member of this community of interesting people, then I guess you've got the connections to get some answers to some casual questions.&nbsp;</p><p>I actually can't do the same. I don't have the time to build engagement or trust in this community enough for such an important project nor to read through past forum posts to track relevant lines of thought in enough detail to use those as a proxy.&nbsp;</p><p>My interest in doing so would be different, but that's another story.</p><blockquote><p>You wrote:</p><p>Now, we know that&nbsp;if your uncertainty can\u2019t be represented with a probability function, then you can be money pumped. There are&nbsp;<a href=\"https://johanegustafsson.net/books/money-pump-arguments.pdf\"><u>proofs</u></a> of this. Guaranteed losses are bad, thus, so the argument goes, you should behave so that your uncertainty can be represented with a probability function.</p></blockquote><p>I followed the link and am browsing the book. Are you sure that link presents any arguments or proofs of why a probability function is necessary in the case of uncertainty?&nbsp;</p><p>I see content on: cyclic preferences; expected value; decision trees; money pump set ups with optional souring and sweetening; induction, backward and forward; indifference vs preferential gaps; various principles. It all goes to show that having cyclic preferences has different implications than acyclic preferences and cyclic preferences can be considered irrational (or at least mismatched to circumstances). The result is very interesting and useful for modeling some forms of irrationality well, but that's not what I am looking for as I read it.</p><p>I can't find any proof of a probability distribution requirement under uncertainty. Is there a specific page you can point me to in the Gustaffson reference or should I use the alternate reference on arguments for probabilism?</p><p>Once I tackle this topic, and detail my conclusions about it in a criticism document that I am still writing (about unweighted beliefs, I can't get anybody to actually read it, lol), I will pursue some other topics you raised.&nbsp;</p><p>Thanks so much!&nbsp;</p>", "parentCommentId": null, "user": {"username": "Noah Scales"}}, {"_id": "mfmZnF5nP9z5heBwF", "postedAt": "2022-10-20T12:12:12.199Z", "postId": "829tAQAYAntkRxCy2", "htmlBody": "<p>Brilliant! I found this a really good introduction to some of the epistemic norms I most value in the EA community.</p>\n<p>It's super well written too.</p>\n", "parentCommentId": null, "user": {"username": "Jsevillamol"}}, {"_id": "z6DwaFnRu6JKuKgzg", "postedAt": "2022-10-20T17:26:12.797Z", "postId": "829tAQAYAntkRxCy2", "htmlBody": "<p>Weakly downvoted this because</p>\n<blockquote>\n<p>(From hereon, I\u2019ll use \u2018EA\u2019 to talk primarily about longtermist EA. Hopefully this won\u2019t annoy too many people).</p>\n</blockquote>\n<p>This seems dangerous to me. There are many EAs that are not longtermist and it is not even clear whether the majority is longtermist  (see e.g., <a href=\"https://forum.effectivealtruism.org/posts/83tEL2sHDTiWR6nwo/ea-survey-2020-cause-prioritization\">here</a>).</p>\n<p>You could just as easily use an acronym like LT-EA or just say Longtermism. By trying to redefine it in this way, you needlessly alienate people.</p>\n", "parentCommentId": null, "user": {"username": "david_reinstein"}}, {"_id": "d6bHs33z68EgBGT6t", "postedAt": "2022-10-31T11:31:50.000Z", "postId": "829tAQAYAntkRxCy2", "htmlBody": "<p>Great post.</p>\n<p>One disagreement:</p>\n<blockquote>\n<p>Principle 3: Our explicit, subjective credences are approximately accurate enough, most of the time, even in crazy domains, for it to be worth treating those credences as a salient input into action.</p>\n</blockquote>\n<p>I think for me at least, and I'd guess for other people, the thing that makes the explicit subjective credences worth using is that since we have to make prioritisation decisions//decisions about how to act <em>anyway</em>, and we're going to make them using some kind of fuzzy approximated expected value reasoning , making our probabilities explicit should improve our reasoning.</p>\n<p>e.g. Why do some others not work on reducing risks of catastrophe from AI? It seems like it's at least partly because they think it's very very unlikely that such a catastrophe could happen. EAs are more likely to think that asking themselves \"how unlikely do I really think it is?\" and then be able to reason with the result, is helpful.</p>\n<p>The Jadagul post is good push back on that, but I do think it helps put \"rational pressure\" on one's beliefs in a way that is often productive. I'd guess that without naming the probabilities explicitly, the people in that story would still have similar (and similarly not-consistent) beliefs.</p>\n", "parentCommentId": null, "user": {"username": "Ardenlk"}}, {"_id": "JKCyqQ4ijvRdWA4Hw", "postedAt": "2022-11-02T22:59:22.973Z", "postId": "829tAQAYAntkRxCy2", "htmlBody": "<p>I'd suggest \"LEA,\" which is almost as easy to type as EA.</p>", "parentCommentId": "z6DwaFnRu6JKuKgzg", "user": {"username": "Jenny K E"}}, {"_id": "hvPKXvENAtsbk6Ciu", "postedAt": "2023-03-24T23:48:24.346Z", "postId": "829tAQAYAntkRxCy2", "htmlBody": "<p>Thanks for the excellent post!</p><p>I think you are right that this might be a norm/heuristic in the community, but in the spirit of a \"justificatory story of our epistemic practices,\" I want to look a little more at&nbsp;</p><blockquote><p>4. When arguments lead us to conclusions that are both speculative and fanatical, treat this as a sign that something has gone wrong.&nbsp;&nbsp;</p></blockquote><p>First, I'm not sure that \"speculative\" is an independent reason that conclusions are discounted, in the sense of a filter that is applied ex-post. In your 15AI thought experiment, for example, I think that expected value calculations would get you most of the way toward explaining an increase in fanaticism; the probability that we can solve the problem might increase on net, despite the considerations you note about replication. The remaining intuition might be explained by availability/salience bias, to which EA is not immune.</p><p>Now, \"speculative\" scenarios might be discounted during the reasoning process if we are anchored to commonsense priors, but this would fall under typical bayesian reasoning. The priors we use and the weight we grant various pieces of evidence are still epistemic norms worthy of examination! But a different kind than suggested by the fourth principle.</p><p>Suppose \"speculative\" arguments <i>are</i> discounted ex-post in EA. I think this practice can still be redeemed on purely bayesian grounds as a correction to the following problems:</p><ol><li><strong>Undiscovered Evidence</strong>: An argument seems speculative not just insofar as it is divorced from empirical observations, but also insofar as we have not thought about it very much. It seems that AI risk has become less speculative as people spend more time thinking about it, holding constant actual progress in AI capabilities. We have some sense of the space of possible arguments that might be made and evidence that might be uncovered, given further research on a topic. And these undiscovered arguments/evidence might not enter neatly into our initial reasoning process. We want some way to say \"I haven't thought of it yet, but I bet there's a good reason this is wrong,\" as we might respond to some clever conspiracy theorist who presents a superficially bulletproof case for a crazy theory we haven't encountered before. And discounting speculative conclusions is one way to achieve this.&nbsp;<ol><li>This point is especially relevant for speculative conclusions because they often rely on chains of uncertain premises, making our credence in their conclusions all the more sensitive to new information that could update multiple steps of the argument.</li></ol></li><li><strong>Model Uncertainty</strong>: Even in a domain where we have excavated all the major arguments available to us, we may still suffer from \"reasoning in the dark,\" ie, in the absence of solid empirics. When reasoning about extremely unlikely events, the probability our model is wrong can swamp our credence in its conclusion. Discounting speculative conclusions allows us to say \"we should be fanatical insofar as my reasoning is correct, but I am not confident in my reasoning.\"<ol><li>We can lump uncertainty in our axiology, epistemology, and decision theory under this section. That is, a speculative conclusion might look good only under total utilitarian axiology, bayesian epistemology, and causal decision theory, but a more conventional conclusion might be more robust to alternatives in these categories. (Note that this is a prior question to the evidential-hedging double bind set up in Appendix B.)</li><li>Chains of uncertain premises also make model uncertainty doubly important for speculative conclusions. As Anders Sandberg <a href=\"https://80000hours.org/podcast/episodes/anders-sandberg-fermi-paradox/\">points out</a>, \"if you have a long argument, the probability of there being some slight error somewhere is almost 1.\"</li></ol></li></ol><p>Even after accounting for these considerations, we might find that the EV of pursuing the speculative path warrants fanaticism. In this event, discounting the speculative conclusion might be a pragmatic move to deprioritize actions on this front in anticipation of new evidence that will come to light, including evidence that will bear on model uncertainty. (We might treat this as a motivation for imprecise credences, prioritizing views with sharper credences over speculative views with fuzzier ones.)</p>", "parentCommentId": null, "user": {"username": "Rocket"}}, {"_id": "9rmbzhpkAfraWPcAv", "postedAt": "2023-03-29T11:51:25.363Z", "postId": "829tAQAYAntkRxCy2", "htmlBody": "<p>Thanks for the comment!&nbsp;</p><p><i>(Fair warning, my response will be quite long)</i></p><p>I understand you to be offering two potential stories to justify \u2018speculativeness-discounting\u2019.&nbsp;</p><ol><li>First, EAs don\u2019t (by and large) apply a speculativeness-discount&nbsp;<i>ex post</i>. Instead, there\u2019s a more straightforward \u2018Bayesian+EUM\u2019 rationalization of the practice. For instance, the epistemic practice of EAs may be better explained with reference to more common-sense priors, potentially mediated by orthodox biases.</li><li>Or perhaps EAs&nbsp;<i>do</i> apply a speculativeness-discount&nbsp;<i>ex post</i>. This too can be justified on Bayesian grounds.&nbsp;<ol><li>We often face doubts about our ability to reason through all the relevant considerations, particularly in speculative domains. For this reason, we update on higher-order uncertainty, and implement heuristics which themselves are justified on Bayesian grounds.&nbsp;</li></ol></li></ol><p>In my response, I\u2019ll assume that your attempted rationale for Principle 4 involves justifying the norm with respect to the following two views:</p><ul><li>Expected Utility Maximization (EUM) is the optimal&nbsp;<i>decision-procedure</i>.</li><li>The relevant probabilities to be used as inputs into our EUM calculation are our subjective credences.</li></ul><h3>The \u2018Common Sense Priors\u2019 Story</h3><p>I think your argument in (1) is very unlikely to provide a rationalization of EA practice on \u2018Bayesian + EUM\u2019 grounds.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcbqhggdnja7\"><sup><a href=\"#fncbqhggdnja7\">[1]</a></sup></span>&nbsp;</p><p>Take&nbsp;<i>Pascal\u2019s Mugging</i>. The stakes can be made high enough that the value involved can easily swamp your common-sense priors. Of course, people have stories for why they shouldn\u2019t give the money to the mugger. But these stories are usually generated because handing over their wallet is judged to be ridiculous, rather than the judgment arising from an independent EU calculation. I think other fanatical cases will be similar. The stakes involved under (e.g.) various religious theories and our ability to acausally affect an infinite amount of value are simply going to be large enough to swamp our initial common-sense priors.&nbsp;</p><p>Thus, I think the only feasible \u2018Bayes+EUM\u2019 justification you could offer would have to rely on your \u2018higher-order evidence\u2019 story about the fallibility of our first-order reasoning, which we\u2019ll turn to below.&nbsp;</p><h3>The \u2018Higher-Order Evidence\u2019 Story</h3><p>I agree that we can say: \u201cwe should be fanatical insofar as my reasoning is correct, but I am not confident in my reasoning.\u201d&nbsp;</p><p>The question, then, is how to update after reflecting on your higher-order evidence. I can see two options: either you have&nbsp;<i>some</i> faith in your first-order reasoning, or no faith.&nbsp;</p><p>Let\u2019s start with the case where you have some faith in your first-order reasoning. Higher-order evidence about your own reasoning might decrease the&nbsp;<i>confidence</i> in your initial conclusion. But, as you note, \u201cwe might find that the EV of pursuing the speculative path warrants fanaticism\u201d. So, what to do in that case?</p><ul><li>I think it\u2019s true that many people will cite considerations of the form \u201clet\u2019s pragmatically deprioritize the high EV actions that are both speculative and fanatical, in anticipation of new evidence\u201d. I don\u2019t think that provides a sufficient justificatory story of the epistemic norms to which most of us hold ourselves.</li><li>Suppose we knew that our evidential situation was as good as it\u2019s ever going to be. Whatever evidence we currently have about (e.g.) paradoxes in infinite ethics, or the truth of various religions constitutes ~all the evidence we\u2019re ever going to have.&nbsp;</li><li>I&nbsp;<i>still</i> don\u2019t expect people to follow through on the highest EV option, when that option is both speculative and fanatical.&nbsp;<ul><li>Under MEC, EAs should plausibly be <a href=\"https://forum.effectivealtruism.org/posts/Gk7NhzFy2hHFdFTYr/a-dilemma-for-maximize-expected-choiceworthiness-mec\">funneling all their money into soteriological research</a>. Or perhaps you don\u2019t like MEC, and think we should work out the most plausible worldview under which we can affect&nbsp;<a href=\"https://youtu.be/SrU9YDoXE88?t=1261\"><u>strongly-Ramsey</u></a>-many sentient observers.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhly9884t5lb\"><sup><a href=\"#fnhly9884t5lb\">[2]</a></sup></span>&nbsp;</li><li>Or maybe you have a bounded utility function. In that case, imagine that the world already contains a sufficiently large number of suffering entities. How blase are you, really, about the creation of arbitrarily many suffering-filled hellscapes?</li></ul></li></ul><p>There\u2019s more to say here, but the long and short of it is: if you fail to reach a point where you&nbsp;<i>entirely</i> discount certain forms of speculative reasoning, I don\u2019t think you\u2019ll be able to recover anything like Principle 4. My honest view is that many EAs have a vague hope that such theories will recover something approaching normality, but very few people actually try to trace out the implications of such theories on their own terms, and follow through on these implications. I\u2019m sympathetic to&nbsp;<a href=\"https://www.lesswrong.com/posts/gJxHRxnuFudzBFPuu/better-impossibility-result-for-unbounded-utilities?commentId=7bib383hJryDWGitJ\"><u>this quote</u></a> from Paul Christiano:</p><blockquote><p>I tried to answer questions like \"How valuable is it to accelerate technological progress?\" or \"How bad is it if unaligned AI takes over the world?\" and immediately found that EU maximization with anything like \"utility linear in population size\" seemed to be unworkable in practice. I could find no sort of common-sensical regularization that let me get coherent answers out of these theories, and I'm not sure what it would look like in practice to try to use them to guide our actions.&nbsp;&nbsp;&nbsp;</p></blockquote><h3>Higher-Order Evidence and Epistemic Learned Helplessness</h3><p>Maybe you\u2019d like to say: \u201cin certain domains, we should assign our first-order calculations about which actions maximize EU zero weight. The heuristic \u2018<a href=\"https://slatestarcodex.com/2019/06/03/repost-epistemic-learned-helplessness/\"><u>sometimes assign first-order reasoning zero weight</u></a>\u2019 can be justified on Bayesian grounds.\u201d&nbsp;</p><ul><li>I agree that we should sometimes assign our first-order calculations about which actions maximize EU zero weight. I\u2019m doubtful that Bayesianism or EUM play much of a role in explaining why this norm is justified.</li><li>When we\u2019re confronted with the output of an EUM calculation that feels off, we should&nbsp;<i>listen to the parts of us&nbsp;</i>which tell us to check again, and ask&nbsp;<i>why</i> we feel tempted to check again.&nbsp;<ul><li>If we\u2019re saying \u201cno, sorry, sometimes I\u2019m going to put zero weight on a subjective EU calculation\u201d, then we\u2019re&nbsp;<i>already committed</i> to a view under which subjective EU calculations only provide action-guidance in the presence of certain background conditions.</li><li>If we\u2019re willing to grant&nbsp;<i>that</i>, then I think the interesting justificatory story is a story which informs us of what the background conditions for trusting EU calculations actually&nbsp;<i>are</i> \u2014 rather than attempts to tell&nbsp;<i>post hoc</i> stories about how our practices can ultimately be squared with more foundational theories like Bayesianism + EUM.<ul><li>If you\u2019re interested, I\u2019ll have a post in April touching on these themes. :)</li></ul></li></ul></li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fncbqhggdnja7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefcbqhggdnja7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I also think the sociological claim you made is probably false. However, as you\u2019re primarily asking about the justificatory side of things, I\u2019ll bracket that here \u2014 though I\u2019m happy to make this case in more detail if you\u2019d like.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhly9884t5lb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhly9884t5lb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Presumably acausally.</p></div></li></ol>", "parentCommentId": "hvPKXvENAtsbk6Ciu", "user": {"username": "Violet Hour"}}, {"_id": "HfS4uMyCvtxAKGSyk", "postedAt": "2023-04-14T20:31:18.511Z", "postId": "829tAQAYAntkRxCy2", "htmlBody": "<p>Thanks for the thorough response! I agree with a lot of what you wrote, especially the third section on Epistemic Learned Helplessness: \"Bayesianism + EUM, but only when I feel like it\" is not a justification in any meaningful sense.</p><h3>On Priors</h3><p>I agree that we can construct thought experiments (Pascal's Mugging, acausal trade) with arbitrarily high stakes to swamp commonsense priors (even without religious scenarios or infinite value, which are so contested I think it would be difficult to extract a sociological lesson from them).</p><h3>On Higher Order Evidence</h3><p>I still think a lot of speculative conclusions we encounter in the wild suffer from undiscovered evidence and model uncertainty, and even barring this we might want to defer taking action until we've had a chance to learn more.&nbsp;</p><p>Your response jumps over these cases to those where we have \"~all the evidence we\u2019re ever going to have,\" but I'm skeptical these cases exist. Even with religion, we might expect some future miracles or divine revelations to provide new evidence; we have some impossibility theorems in ethics, but new ideas might come to light that resolve paradoxes or avoid them completely. In fact, soteriological research and finding the worldview that best acausally benefits observers are proposals to find new evidence.</p><p>But ok, yes, I think we can probably come up with cases where we do have ~all the evidence and still refrain from acting on speculative + fanatical conclusions.&nbsp;</p><h3>Problem 1: Nicheness</h3><p>From here on, I'm abandoning the justification thing. I agree that we've found some instances where the Fourth Principle holds without Bayesian + EUM justification. Instead, I'm getting more into the semantics of what is a \"norm.\"</p><p>The problem is that the support for this behavior among EAs comes from niche pieces of philosophy like Pascal's Mugging, noncausal decision theory, and infinite ethics, ideas that are niche not just relative to the general population, but also <i>within EA</i>. So I feel like the Fourth &nbsp;Principle amounts to \"the minority of EAs who are aware of these edge cases behave this way when confronted with them,\" &nbsp;which doesn't really seem like a <i>norm</i> about EA.</p><h3>Problem 2: Everyone's Doing It</h3><p>(This is also not a justification, it's an observation about the Fourth Principle)</p><p>The first three principles capture ways that EA differs from other communities. The Fourth Principle, on the other hand, seems like the kind of thing that all people do? For example, a lot of people write off earning to give when they first learn about it because it looks speculative and fanatical. Now, maybe EAs differ from other people on which crazy train stop they deem \"speculative,\" and I think that would qualify as a norm, but relative to each person's threshold for \"speculative,\" I think this is more of a human-norm than an EA-norm.</p><p>Would love your thoughts on this, and I'm looking forward to your April post :)</p>", "parentCommentId": "9rmbzhpkAfraWPcAv", "user": {"username": "Rocket"}}]