[{"_id": "jGi8nsu5jmNTwYm48", "postedAt": "2023-05-31T17:59:23.983Z", "postId": "MDkYSuCzFbEgGgtAd", "htmlBody": "<p>I found this post very interesting and useful for my own thinking about the subject.</p>\n<p>Note that while the conclusions here are ones intended for OP specifically, there's actually another striking conclusion that goes against the ideas of many from this community: we need more evidence! We need to build stronger AI (perhaps in more strictly regulated contexts) in order to have enough data to reason about the dangers from it. The \"arms race\" of DeepMind and OpenAI is not existentially dangerous to the world, but is rather contributing to its chance of survival.</p>\n<p>This is still at odds, of course, with the fact that rapid advancements in AI create well-known <em>non-existential</em> dangers, so at times we trade off mitigating those with finding out more about existentially-dangerous AI. This is not an easy decision, and should be paid attention to, especially if you're not a longtermist.</p>\n", "parentCommentId": null, "user": {"username": "Guy Raveh"}}, {"_id": "ryDt7vzZ76QDtkGie", "postedAt": "2023-06-01T21:41:37.219Z", "postId": "MDkYSuCzFbEgGgtAd", "htmlBody": "<p>Is your claim just that people should generally \"increase [their] error bars and widen [their] probability distribution\"? (I was frustrated by the difficulty of figuring out what this post is actually claiming; it seems like it would benefit from a \"I make the following X major claims...\" TLDR.)</p><p>I probably disagree with your points about empiricism vs. rationalism (on priors that I dislike the way most people approach the two concepts), but I think I agree that most people should substantially widen their \"error bars\" and be receptive to new information. And it's for precisely that reason which I feel decently confident in saying \"most people whose risk estimates are very low (&lt;0.5%) are significantly overconfident.\" You logically cannot have extremely low probability estimates while also believing \"there's a &gt;10% chance that in the future I will justifiably think there is a &gt;5% chance of doom, but right now the evidence tells me the risk is &lt;0.5%.\"</p>", "parentCommentId": null, "user": {"username": "Harrison D"}}, {"_id": "4kpc4BD4Ke2BZeany", "postedAt": "2023-06-02T21:15:57.934Z", "postId": "MDkYSuCzFbEgGgtAd", "htmlBody": "<p>Thanks for the feedback! definitely a helpful question. That error bars answer was aimed at OpenPhil based on what I've read from them on AI risk + the prompt in their essay question. I'm sure many others are capable of answering the \"what is the probability\" forecasting question better/more directly than me, but my two cents was to step back and question underlying assumptions about forecasting that seem common in these conversations.</p><p>Hume wrote that \"all probable reasoning is nothing but a species of sensation.\" This doesn\u2019t mean we should avoid probable reasoning (we can't) but I think we should recognize it is based only on our experiences/observations of the world. and question how rational its foundations are. I don't think at this stage anyone actually has the empirical basis to give a meaningful % for \"AI will kill everyone.\" Call it .5 or 1 or 7 or whatever but my essay is about trying to take a step back and question epistemological foundations. Anthropic seems much better at this so far (if they mean it that they'd stop given further empirical evidence of risks).<br><br>I did list two premises from Hume that I think are true (or truer than the average person concerned about AI x-risk holds them to be), so those were my TLDR I guess also.</p>", "parentCommentId": "ryDt7vzZ76QDtkGie", "user": {"username": "Matt Beard"}}, {"_id": "iJ3cDLX7KBDEv3k9h", "postedAt": "2023-06-04T19:06:04.121Z", "postId": "MDkYSuCzFbEgGgtAd", "htmlBody": "<p>I see. (For others' reference, those two points are pasted below)</p><blockquote><ol><li>All knowledge is derived from impressions of the external world. Our ability to reason is limited, particularly about ideas of cause and effect with limited empirical experience.</li><li>History shows that societies develop in an emergent process, evolving like an organism into an unknown and unknowable future. History was shaped less by far-seeing individuals informed by reason than by contexts which were far too complex to realize at the time.</li></ol></blockquote><p>Overall, I don't really know what to make of these. They are fairly vague statements, making them very liable to motte-and-bailey interpretations; they border on deepities, in my reading.&nbsp;</p><p>\"All knowledge is derived from impressions of the external world\" might be true in a trivially obvious sense that you often need at least some iota of external information to develop accurate beliefs or effective actions (although even this might be somewhat untrue with regard to biological instincts). However, it makes no clear claim about how much and what kind of \"impressions from the external world\" are necessary for \"knowledge.\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz09fy4heq6o\"><sup><a href=\"#fnz09fy4heq6o\">[1]</a></sup></span>&nbsp;Insofar as the claim is that forecasts about AI x-risks are not \"derived from impressions of the external world,\" I think this is completely untrue. In such an interpretation, I question whether the principle even lives up to its own claims: what empirical evidence was this claim derived from?</p><p>The second claim suffers from similar problems in my view: I obviously wouldn't claim that there have always been seers who could just divine the long-run future. However, insofar as it is saying that the future is so \"unknowable\" that people cannot reason about what actions in front of them are good, I also reject this: it seems obviously untrue with regards to, e.g., fighting Nazi Germany in WW2. Moreover, I would say that even if this has been true, that does not mean it will always be true, especially given the potential for value lock-in from superintelligent AI.&nbsp;</p><p>&nbsp;</p><p>Overall, I agree that it's important to be humble about our forecasts and that we should be actively searching for more information and methods to improve our accuracy, questioning our biases, etc. But I also don't trust vague statements that could be interpreted as saying it's largely hopeless to make <i>decision-informing predictions</i> about what to do in the short term to increase the chance of making the long-run future go well.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnz09fy4heq6o\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefz09fy4heq6o\">^</a></strong></sup></span><div class=\"footnote-content\"><p>A term I generally dislike for its ambiguity and philosophical denotations (which IMO are often dubious at best).</p></div></li></ol>", "parentCommentId": "4kpc4BD4Ke2BZeany", "user": {"username": "Harrison D"}}, {"_id": "DajJfddRHmWb8sLcB", "postedAt": "2023-06-07T14:54:21.031Z", "postId": "MDkYSuCzFbEgGgtAd", "htmlBody": "<p>Thanks for the feedback. I agree that trying to present an alternative worldview ends up quite broad with some good counter examples. And I certainly didn't want to give this impression:</p><blockquote><p>it's largely hopeless to make <i>decision-informing predictions</i> about what to do in the short term to increase the chance of making the long-run future go well.</p></blockquote><p>Instead I'd say that it is difficult to make these predictions based on a priori reasoning, which this community often tries for AI, and that we should shift resources towards rigorous empirical evidence to better inform our predictions. I tried to give specific examples- Anthropic style alignment research is empiricist, Yudkowsky style theorizing is a priori rationalist. This sort of epistemological critique of longtermism is <a href=\"https://forum.effectivealtruism.org/posts/2455tgtiBsm5KXBfv/an-epistemic-critique-of-longtermism\">somewhat</a> <a href=\"https://forum.effectivealtruism.org/posts/BCzDw2WKLjckcK2Ba/paper-summary-the-epistemic-challenge-to-longtermism\">common</a>.</p>", "parentCommentId": "iJ3cDLX7KBDEv3k9h", "user": {"username": "Matt Beard"}}, {"_id": "ma8Ef4xFwTb6JRkh7", "postedAt": "2023-06-07T19:38:54.529Z", "postId": "MDkYSuCzFbEgGgtAd", "htmlBody": "<p>Ultimately, I've found that the line between empirical and theoretical analysis is often very blurry, and if someone does develop a decent brightline to distinguish the two, it turns out that there are often still plenty of valuable theoretical methods, and some of the empirical methods can be very misleading.&nbsp;</p><p>For example, high-fidelity simulations are arguably theoretical under most definitions, but they can be far more accurate than empirical tests.</p><p>Overall, I tend to be quite supportive of using whatever empirical evidence we can, especially experimental methods when they are possible, but there are many situations where we cannot do this. (I've written more on this here: <a href=\"https://georgetownsecuritystudiesreview.org/2022/11/30/complexity-demands-adaptation-two-proposals-for-facilitating-better-debate-in-international-relations-and-conflict-research/\">https://georgetownsecuritystudiesreview.org/2022/11/30/complexity-demands-adaptation-two-proposals-for-facilitating-better-debate-in-international-relations-and-conflict-research/</a> )</p>", "parentCommentId": "DajJfddRHmWb8sLcB", "user": {"username": "Harrison D"}}]