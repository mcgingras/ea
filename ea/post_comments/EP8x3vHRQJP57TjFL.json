[{"_id": "K9NWTwrPEvrb4iGJe", "postedAt": "2018-05-27T15:39:55.656Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>There's a game theory concept for this called <a href=\"https://en.wikipedia.org/wiki/Shapley_value\">Shapley value</a> which allocates credit in a way that does sum to 1, but takes into account a bunch of people each independently being required for all of the impact to happen. </p>\n<blockquote>\n<p>Consider some project worked on by multiple organisations A, B, C and D. The benefit of the project is x. Each of the organisations is a necessary condition of the benefit x. The counterfactual impact of A is x; the counterfactual impact of B is x; etc. Despite this, the counterfactual impact of A, B, C, and D acting together is not 4*x, rather it is x.</p>\n</blockquote>\n<p>In this example, Shapley value would give everyone x/4 of the credit, adding together to x as we'd naturally expect.</p>\n<blockquote>\n<p>The alternative approach (which I argue is wrong) is to say that each of the n A voters is counterfactually responsible for 1/n of the $10bn benefit. Suppose there are 10m A voters. Then each A voter\u2019s counterfactual social impact is 1/10m<em>$10bn = $1000. But on this approach the common EA view that it is rational for individuals to vote as long as the probability of being decisive is not too small, is wrong. Suppose the ex ante chance of being decisive is 1/1m. Then the expected value of Emma voting is a mere 1/1m</em>$1000 = $0.001. On the correct approach, the expected value of Emma voting is 1/10m*$10bn = $1000. If voting takes 5 minutes, this is obviously a worthwhile investment for the benevolent voter, as per common EA wisdom.</p>\n</blockquote>\n<p>This doesn't really seem like an argument to me... it seems like you start from the premise that voting must be rational, and that something like Shapley value would make it irrational, and thus that can't be the case. But this seems to me to be assuming the conclusion?</p>\n<p>I guess this is a case where the expected value of the action and the Shapley value are not the same, because in one case you're analyzing an entire system and in the other case you're analyzing the individual action. But just as it may seem weird that Shapley value assigns such a small value to each vote, the expected value approach is basically saying that the first few votes have a value of literally 0, which also seems nonsensical to me.</p>\n", "parentCommentId": null, "user": {"username": "Peter_Hurford"}}, {"_id": "AiiQjTxqqvGTzBF8F", "postedAt": "2018-05-27T16:04:14.251Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>It needs to be explained why there is a paradox. I have not yet seen an explanation of why there might be thought to be one. EAs are concerned with having counterfactual impact. If you were a necessary condition of some benefit B occurring, then you have had counterfactual impact. </p>\n<p>Re voting I'm appealing to how almost everyone in the academic literature assesses the expected value of voting, which is not by dividing the total value by each voter. I'm also appealing to a common EA idea which is discussed by Parfit and mentioned in Will's book, which is that voting is sometimes rational for altruistic voters. On your approach, it would pretty much always be irrational to vote even if the social benefits were extremely large: every social benefit would always be divided by the number of decisive voters, and so would be divided by many millions in any large election</p>\n<p>I don't understand why the expected value approach says that the first few votes have a value of 0. Also, the ordering in which votes are cast is completely irrelevant to judging a voter's counterfactual imapct because all votes are indistinguishable wrt causing the outcome: it doesn't matter if I voted first and Emma voted last, we would still be decisive voters.</p>\n", "parentCommentId": "K9NWTwrPEvrb4iGJe", "user": null}, {"_id": "SCkGZLnnCZkjh7ALR", "postedAt": "2018-05-27T16:21:52.062Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>Here's another way of putting it. Imagine that you are deciding whether to support a project P with some action A. Lots of other agents have been involved in P and have contributed more than you in the past. You can make some small contribution (action A) which tips the balance making P a success, producing $1bn benefits, otherwise P fails. Should I say &quot;by acting I personally will ensure that $1bn are produced&quot;? Or should I say that &quot;by acting I personally will ensure that $1bn divided by other's contributions are produced?&quot; The difference between my acting and not acting is $1bn, so the former.</p>\n<p>Simply, the term 'counterfactual impact' refers to the difference between the world in which I act and the world in which I don't.</p>\n", "parentCommentId": "K9NWTwrPEvrb4iGJe", "user": null}, {"_id": "9KJM54ydQiucy22Gy", "postedAt": "2018-05-27T19:08:23.634Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>It's not a paradox. The problem is just that, if everyone thought this way, we would get suboptimal outcomes -- so maybe we should figure out how to avoid that.</p>\n<p>Suppose there are three possible outcomes:\nP has cost $2000 and gives 15 utility to the world\nQ has cost $1000 and gives 10 utility to the world\nR has cost $1000 and gives 10 utility to the world</p>\n<p>Suppose Alice and Bob each have $1000 to donate. Consider two scenarios:</p>\n<p>Scenario 1: Both Alice and Bob give $1000 to P. The world gets 15 more utility. Both Alice and Bob are counterfactually responsible for giving 15 utility to the world.</p>\n<p>Scenario 2: Alice gives $1000 to Q and Bob gives $1000 to R. The world gets 20 more utility. Both Alice and Bob are counterfactually responsible for giving 10 utility to the world.</p>\n<p>From the world's perspective, scenario 2 is better. However, from Alice and Bob's individual perspective (if they are maximizing their own counterfactual impact), scenario 1 is better. This seems wrong, we'd want to somehow coordinate so that we achieve scenario 2 instead of scenario 1.</p>\n", "parentCommentId": "AiiQjTxqqvGTzBF8F", "user": {"username": "rohinmshah"}}, {"_id": "iytCJ89rAZe8ERRr3", "postedAt": "2018-05-27T19:11:35.253Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<blockquote>\n<p>For example, suppose Victoria hears about EA through GWWC and wouldn\u2019t have heard about it otherwise. She makes the pledge and gives $1m to ACE charities, which she wouldn\u2019t have found otherwise (and otherwise would have donated to a non-effective animal charity let\u2019s suppose). Who counterfactually produced the $1m donation benefit: Victoria, GWWC or ACE? Each of them is a necessary condition for the benefit: if Victoria hadn\u2019t acted, then the $1m wouldn\u2019t have been donated; if GWWC hadn\u2019t existed, then the $1m wouldn\u2019t have been donated; and if ACE hadn\u2019t existed then the $1m wouldn\u2019t have been donated effectively. Therefore, Victoria\u2019s counterfactual impact is $1m to effective charities, GWWC\u2019s counterfactual impact is $1m to effective charities, and ACE\u2019s impact is $1m to effective charities.</p>\n</blockquote>\n<blockquote>\n<p>Apparent paradox: doesn\u2019t this entail that the aggregate counterfactual impact of Victoria, GWWC and ACE is $3m to effective charities? No. When we are assessing the counterfactual impact of Victoria, GWWC and ACE acting together, we now ask a different question to the one we asked above viz. \u201cif Victoria, GWWC and ACE had not acted, what benefit would there have been?\u201d. This is a different question and so gets a different answer: $1m.</p>\n</blockquote>\n<p>A good way of seeing this is to think about a single actor taking three actions. Suppose that you come across a child drowning in a pond. You pull the child out, call emergency services, and perform CPR until an ambulance arrives. While it may be the case that each of your actions saved the child's life (in the sense that the child would have died if any one of the actions had not been taken), it is certainly not the case that your three actions collectively saved three lives. And if that's true of three actions taken by a single person, it should also be true of three actions taken by three separate people.</p>\n", "parentCommentId": null, "user": {"username": "RandomEA"}}, {"_id": "Sm5s2tsYZNHN7AGdj", "postedAt": "2018-05-27T19:43:05.339Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>Great point!</p>\n", "parentCommentId": "iytCJ89rAZe8ERRr3", "user": null}, {"_id": "M7sL9RNdpceLSp8WA", "postedAt": "2018-05-28T05:30:13.427Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>^ This is what I wanted to say, but even better than how I was going to say it.</p>\n", "parentCommentId": "9KJM54ydQiucy22Gy", "user": {"username": "Peter_Hurford"}}, {"_id": "Fog4pjpg6esPNbF5f", "postedAt": "2018-05-28T05:31:01.928Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>Are you neglecting to count the negative impact from causing other people to do the suboptimal thing? If I use my funds to set up an exploding matching grant that will divert the funds of other donors from better things too a less effective charity, that is a negative part of my impact.</p>\n", "parentCommentId": "9KJM54ydQiucy22Gy", "user": {"username": "CarlShulman"}}, {"_id": "qECnkqu6mt7Mf2HZg", "postedAt": "2018-05-28T11:04:46.166Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>Firstly, people who believe in the correct account of counterfactual impact would have incentives to coordinate in the case you outline. Alice would maximise her counterfactual impact (defined as I define it) by coordinating with Bob on project R. The counterfactual impact of her coordinating with Bob would be +5 utility compared to scenario 1. There is no puzzle here. </p>\n<p>Secondly, dividing counterfactual impact by contribution does not solve all these coordination problems. If everyone thought as per the Shapely value, then no rational altruists would ever vote, even when the true theory dictates that the expected value of doing so was very high. </p>\n<p>Also consider the $1bn benefits case outlined above. Suppose that the situation is as described above but my action costs $2 and I take one billionth of the credit for the success of the project. In that case, the Shapely-adjusted benefits of my action would be $1 and the costs $2, so my action would not be worthwhile. I would therefore leave $1bn of value on the table. </p>\n", "parentCommentId": "9KJM54ydQiucy22Gy", "user": null}, {"_id": "Y7pS5CXLBPEwQEY3G", "postedAt": "2018-05-28T14:09:06.284Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>&quot;The alternative approach (which I argue is wrong) is to say that each of the n A voters is counterfactually responsible for 1/n of the $10bn benefit. Suppose there are 10m A voters. Then each A voter\u2019s counterfactual social impact is 1/10m<em>$10bn = $1000. But on this approach the common EA view that it is rational for individuals to vote as long as the probability of being decisive is not too small, is wrong. Suppose the ex ante chance of being decisive is 1/1m. Then the expected value of Emma voting is a mere 1/1m</em>$1000 = $0.001. On the correct approach, the expected value of Emma voting is 1/10m*$10bn = $1000. If voting takes 5 minutes, this is obviously a worthwhile investment for the benevolent voter, as per common EA wisdom.&quot;</p>\n<p>I am not sure, whether anyone is arguing for discounting twice. The alternative approach using the shapley value would divide the potential impact amongst the contributors, but not additionally account for the probability. Therefore, in this example both approaches seem to assign the same counterfactual impact.</p>\n<p>More generally, it seems like most disagreements in this thread could be resolved by a more charitable interpretation of the other side (from both sides, as the validity of your argument against rohinmshah's counterexample seems to show)</p>\n<p>Right now, a comment from someone more proficient with the shapley value arguing against </p>\n<p>&quot;Also consider the $1bn benefits case outlined above. Suppose that the situation is as described above but my action costs $2 and I take one billionth of the credit for the success of the project. In that case, the Shapely-adjusted benefits of my action would be $1 and the costs $2, so my action would not be worthwhile. I would therefore leave $1bn of value on the table.&quot;</p>\n<p>might be helpful for a better understanding. </p>\n", "parentCommentId": null, "user": {"username": "Flodorner"}}, {"_id": "5uf5kiJfD4Mp44FNS", "postedAt": "2018-05-28T16:24:30.219Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>So, on the Shapely value approach, we would ignore the probability of different states of affairs when deciding what to do? This seems wrong. Also, the only time the discount would be the same is when the probability of counterfactual impact happens to equal the discount applied according to the shapely value. This would only happen coincidentally: e.g. the chance of being decisive could be 1/1,000 and the shapely discount 1/1m. </p>\n", "parentCommentId": "Y7pS5CXLBPEwQEY3G", "user": null}, {"_id": "zmY6nG6XktpvJC7sn", "postedAt": "2018-05-29T13:07:00.215Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>Where are you actually disagreeing with Joey and the conclusions he is drawing?</p>\n<p>Joey is arguing that the  --EA Movement-- might accidentally overcount its impact by adding each individual actor's counterfactual impact together. You point out a scenario in which various individual actor's actions are necessary for the counterfactual impact to happen so it is legitimate for each actor to claim the full counterfactual impact. This seems tangential to Joey's point, which is fundamentally about the practical implications of this problem. \nThe question of who is responsible for the counterfactual impact and who should get credit are being asked because as the EA Movement we have to decide how to allocate our resources to the different actors. We also need to be cautious not to overcount impact as a movement in our outside communications and to not get the wrong impression ourselves.</p>\n", "parentCommentId": null, "user": {"username": "Denise_Melchin"}}, {"_id": "qnwJgZiR8bP4YkF4K", "postedAt": "2018-05-29T14:30:45.621Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>If that is what he is arguing I agree, but I don't think he is arguing that. He writes</p>\n<p>&quot;This person would become quadruple counted in EA, with each organization using their donations as impact to justify their running.&quot;</p>\n<p>Each organisation would in fact be right to count the impact in the way described. </p>\n", "parentCommentId": "zmY6nG6XktpvJC7sn", "user": null}, {"_id": "ED5KKYiXGaPQxpykP", "postedAt": "2018-05-29T17:13:07.271Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>Yes, that's right. I agree that a perfect calculation of your counterfactual impact would do the right thing in this scenario, and probably all scenarios. This is an empirical claim that  the actual impact calculations that meta-orgs do are of the form that I wrote in my previous comment.</p>\n<p>For example, consider the impact calculations that GWWC and other meta orgs have. If those impact calculations (with their current methodologies) showed a ratio of 1.1:1, that seems nominally worthwhile (you still have the multiplicative impact), but I would expect that it would be better to give directly to charities to avoid effects like the ones Joey talked about in his post.</p>\n<p>A true full counterfactual impact calculation would consider the world in which GWWC just sends the money straight to charities and convinces other meta orgs to do the same, at which point they see that more money gets donated to charities in total, and so they all close operations and send money straight to charities. I'm arguing that this doesn't happen in practice. (I think Joey and Peter are arguing the same thing.)</p>\n", "parentCommentId": "Fog4pjpg6esPNbF5f", "user": {"username": "rohinmshah"}}, {"_id": "uvLy6SR6TgoAWfmuW", "postedAt": "2018-05-29T18:28:50.628Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>For the first point, see my response to Carl above. I think you're right in theory, but in practice it's still a problem.</p>\n<p>For the second point, I agree with Flodorner that you would either use the Shapley value, or you would use the probability of changing the outcome, not both. I don't know much about Shapley values, but I suspect I would agree with you that they are suboptimal in many cases. I don't think there is a good theoretical solution besides &quot;consider every possible outcome and choose the best one&quot; which we obviously can't do as humans. Shapley values are one tractable way of attacking the problem without having to think about all possible worlds, but I'm not surprised that there are cases where they fail. I'm advocating for &quot;think about this scenario&quot;, not &quot;use Shapley values&quot;.</p>\n<p>I think the $1bn benefits case is a good example of a pathological case where Shapley values fail horribly (assuming they do what you say they do, again, I don't know much about them).</p>\n<p>My overall position is something like &quot;In the real world when we can't consider all possibilities, one common failure mode in impact calculations is the failure to consider the scenario in which <em>all</em> the participants who contributed to this outcome instead do other altruistic things with their money&quot;.</p>\n", "parentCommentId": "qECnkqu6mt7Mf2HZg", "user": {"username": "rohinmshah"}}, {"_id": "DswB9GT4zYbWBad4x", "postedAt": "2018-05-29T18:29:02.878Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>To try to narrow down the disagreement: Would you donate to GWWC instead of AMF if their impact calculation (using their current methodology) showed that $1.10 went to AMF for every $1 given to GWWC? I wouldn't.</p>\n", "parentCommentId": "qnwJgZiR8bP4YkF4K", "user": {"username": "rohinmshah"}}, {"_id": "B34FFStM2dMzE3Rvw", "postedAt": "2018-05-29T20:51:16.530Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>At this point, i think that to analyze the $1bn case correctly, you'd have to substract everyone's opportunity cost in the calculation of the shapley value (if you want to use it here). This way, the example should yield what we expect. </p>\n<p>I might do a more general writeup about shapley values, their advantages, disadvantages and when it makes sense to use them, if i find the time to read a bit more about the topic first. </p>\n", "parentCommentId": "uvLy6SR6TgoAWfmuW", "user": {"username": "Flodorner"}}, {"_id": "mseCt9Qc6jwwp8DGt", "postedAt": "2018-05-30T09:39:06.058Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>In Joey's example, I can donate $500 to GWWC instead of AMF. If I donate to AMF, AMF gets $500 compared to the world in which i don't donate. If I donate to GWWC, then AMF gets $1000 compared to the world in which I don't donate. Clearly, I should donate to GWWC if I care about counterfactual impact. If GWWC donates the $500 directly to AMF, then value has been lost. </p>\n<p>The coordination problem is a separate question to how individual organisations should count their own counterfactual impact.</p>\n", "parentCommentId": "DswB9GT4zYbWBad4x", "user": null}, {"_id": "3vyaiW3ocxRqPrtCu", "postedAt": "2018-06-01T21:32:13.279Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>Forget about the organization's own counterfactual impact for a moment.</p>\n<p>Do you agree that, from the world's perspective, it would be better in Joey's scenario if GWWC, Charity Science, and TLYCS were to all donate their money directly to AMF? </p>\n", "parentCommentId": "mseCt9Qc6jwwp8DGt", "user": {"username": "rohinmshah"}}, {"_id": "j3XrTFyMjtRjnQp89", "postedAt": "2018-06-04T13:54:28.301Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<p>yes</p>\n", "parentCommentId": "3vyaiW3ocxRqPrtCu", "user": null}, {"_id": "4FYcGGmFKZrguzt5M", "postedAt": "2020-05-03T09:59:26.198Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<html><head></head><body><p>I don't see Joey's article cited anywhere. Can someone help pointing to that article?</p>\n</body></html>", "parentCommentId": "zmY6nG6XktpvJC7sn", "user": {"username": "agent18"}}, {"_id": "ohm4EyFfrWKqt5d52", "postedAt": "2020-05-03T10:08:19.705Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<html><head></head><body><p>Found it.</p>\n<p><a href=\"https://ea.greaterwrong.com/posts/fnBnEiwged7y5vQFf/triple-counting-impact-in-ea?hide-nav-bars=true\">https://ea.greaterwrong.com/posts/fnBnEiwged7y5vQFf/triple-counting-impact-in-ea?hide-nav-bars=true</a></p>\n</body></html>", "parentCommentId": "4FYcGGmFKZrguzt5M", "user": {"username": "agent18"}}, {"_id": "ztMtSTSahnN2seWPr", "postedAt": "2020-05-03T10:36:40.667Z", "postId": "EP8x3vHRQJP57TjFL", "htmlBody": "<html><head></head><body><p>I don't understand the difference between the following in \"Notes on leveraging and funging\". <strong>Is that a typo?</strong> They look the same to me.</p>\n<blockquote>\n<p>Comparison 1:\nCounterfactual world (A): A doesn\u2019t act, <strong>B and C act as they would have done if A had not acted.</strong></p>\n</blockquote>\n<p>\"B and C act as they would have done if A had not acted.\"</p>\n<blockquote>\n<p>Comparison 2:\nCounterfactual world (A): A doesn\u2019t act, <strong>B and C act as they did in the actual world</strong>.</p>\n</blockquote>\n<p>\"B and C act as they did in the actual world\"</p>\n</body></html>", "parentCommentId": null, "user": {"username": "agent18"}}]