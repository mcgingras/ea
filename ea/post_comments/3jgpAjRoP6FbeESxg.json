[{"_id": "tjqdootkhFXi4mR8W", "postedAt": "2018-09-27T15:20:47.788Z", "postId": "3jgpAjRoP6FbeESxg", "htmlBody": "<blockquote>\n<p>However, this trick will increase the total suffering in the multiverse, from the purely utilitarian perspective, by 1000 times, as the number of suffering observer-moments will increase. But here we could add one more moral assumption: \u201cVery short pain should be discounted\u201d, based on the intuition that 0.1 seconds of intense pain is bearable (assuming it does not cause brain damage)\u2014simply because it will pass very quickly.  </p>\n</blockquote>\n<p>I'd say pain experienced during 0.1 seconds is about 10 times less bad than pain experienced during 1 second. I don't see why we should discount it any further than that. Our particular human psychology might be better at dealing with injury if we expect it to end soon, but we can't change what the observer-moment S(t) expects to happen without changing the state of it's mind. If we change the state of it's mind, it's not a copy of S(t) anymore, and the argument fails.</p>\n<p>In general, I can't see how this plan would work. As you say, you can't decrease the absolute number of suffering oberver-moments, so it won't do any good from the perspective of total utilitarianism. The closest thing I can imagine is to &quot;dilute&quot; pain by creating similar but somewhat happier copies, if you believe in some sort of average utilitarianism that cares about identity. That seems like a strange moral theory, though.</p>\n", "parentCommentId": null, "user": {"username": "Lukas_Finnveden"}}, {"_id": "2GPi8Gs9NHKE9HvHw", "postedAt": "2018-09-27T18:50:19.083Z", "postId": "3jgpAjRoP6FbeESxg", "htmlBody": "<p>This is an algorithmic trick without ethical value. The person who experienced suffering still experienced suffering. You can outweigh it by creating lots of good scenarios, but making those scenarios similar to the original one is irrelevant.</p>\n", "parentCommentId": null, "user": {"username": "kbog"}}, {"_id": "z2Xn2MtC6zQ7SkYZe", "postedAt": "2018-09-27T22:11:16.947Z", "postId": "3jgpAjRoP6FbeESxg", "htmlBody": "<p>The point, presumably, is that people would feel better because of the expectation that things would improve.</p>\n<p>Of course, the criticism is that rather than simulating someone who starts in pain and then improves gradually, you could simply simulate someone with high welfare all along. But if you could achieve identity-continuity without welfare-level-continuity this cost wouldn't apply.</p>\n", "parentCommentId": "2GPi8Gs9NHKE9HvHw", "user": {"username": "Larks"}}, {"_id": "8G4hErdGCxF33G3CH", "postedAt": "2018-09-27T22:24:57.926Z", "postId": "3jgpAjRoP6FbeESxg", "htmlBody": "<p>You should consider whether something has gone terribly wrong if your method for preventing s-risks is to simulate individuals suffering intensely in huge quantities.</p>\n", "parentCommentId": null, "user": {"username": "itaibn"}}, {"_id": "8peXBsGzH4xPBcHQe", "postedAt": "2018-09-27T23:30:33.097Z", "postId": "3jgpAjRoP6FbeESxg", "htmlBody": "<p>Reading your comment I come to the following patch of my argument: benevolent AI starts not from S(t), but immediately from many copies of those S(t+1) which have much less intense sufferings, but still have enough similarity with S(t) to be regarded as its next moment of experience. Not S(t) will be diluted, but the next moments of the S(t). This solves the need to create many S(t)-moments which seems morally wrong and computationally intensive.</p>\n<p>My plan is that FAI can't decrease the number of suffering moments, but the plan is to create an immediate way out of each such moment. While total utilitarian will not feel the difference, it is just a theory which was not designed to account for the length of suffering, but for any particular observer, this will be a salvation.</p>\n", "parentCommentId": "tjqdootkhFXi4mR8W", "user": {"username": "turchin"}}, {"_id": "yz7Wf4YBJAH2HPRsG", "postedAt": "2018-09-27T23:39:20.642Z", "postId": "3jgpAjRoP6FbeESxg", "htmlBody": "<p>I could see three possible problems: </p>\n<p>The method will create new suffering moments, and even may be those suffering moments, which will not exist otherwise. But there is a patch for it: see my comment above to Lukas.</p>\n<p>The second possible problem is that the universe will be tiled with past simulations which try to resurrect any ant ever lived on Earth \u2013 and thus there will be an opportunity cost, as many other good things could be done. This could be patched by what could be called &quot;cheating death in Damascus&quot; approach where some timelines choose not to play this game by using a random generator, or by capping amount of resources which they may spend on the past sufferings prevention. </p>\n<p>The third problem could be ontological, like a wrong theory of the human personal identity. But if a (pseudo)-Benevolent AI has a wrong understanding of the human identity, we will have many other problems, e.g. during uploading.</p>\n", "parentCommentId": "8G4hErdGCxF33G3CH", "user": {"username": "turchin"}}, {"_id": "99ZmMobi3LRkvyWeX", "postedAt": "2018-09-27T23:48:11.038Z", "postId": "3jgpAjRoP6FbeESxg", "htmlBody": "<p>It is an algorithmic trick only if personal identity is strongly connected to exact this physical brain. But in the text, it is assumed that identity is not brain-connected, without any discussion. However, it doesn't mean that I completely endorse this &quot;copy-friendly&quot; theory of identity.</p>\n", "parentCommentId": "2GPi8Gs9NHKE9HvHw", "user": {"username": "turchin"}}, {"_id": "GAxY3BB4poCkYw6Mo", "postedAt": "2018-09-27T23:50:16.838Z", "postId": "3jgpAjRoP6FbeESxg", "htmlBody": "<p>See my patch to the argument in the comment to Lukas: we can simulate those moments which are not in intense pain, but still are very close to the initial suffering-observer moment, so they could be regarded its continuation.</p>\n", "parentCommentId": "z2Xn2MtC6zQ7SkYZe", "user": {"username": "turchin"}}, {"_id": "M9NbzEJL5xhJBy4KR", "postedAt": "2018-09-28T09:04:46.164Z", "postId": "3jgpAjRoP6FbeESxg", "htmlBody": "<blockquote>\n<p>The point, presumably, is that people would feel better because of the expectation that things would improve.</p>\n</blockquote>\n<p>1/1000 people supposedly feels better, but then 999/1000 people will feel slightly worse, because they are given a scenario where they think that things may get worse, when we have the power to give them a guaranteed good scenario instead. It's just shifting expectations around, trying to create a free lunch.</p>\n<p>It also requires that people in bad situations actually believe that someone is going to build an AI that does this. As far as ways of making people feel more optimistic about life go, this is perhaps the most convoluted one that I have seen. Really there are easier ways of doing that: for instance, make them believe that someone is going to build an AI which actually solves their problem.</p>\n", "parentCommentId": "z2Xn2MtC6zQ7SkYZe", "user": {"username": "kbog"}}, {"_id": "4ynb8z7H695tjsvQe", "postedAt": "2018-09-28T09:07:56.640Z", "postId": "3jgpAjRoP6FbeESxg", "htmlBody": "<p>Identity is irrelevant if you evaluate total or average welfare through a standard utilitarian model.</p>\n", "parentCommentId": "99ZmMobi3LRkvyWeX", "user": {"username": "kbog"}}, {"_id": "Fny7cZ6By6hAfYRi9", "postedAt": "2018-09-28T09:23:08.864Z", "postId": "3jgpAjRoP6FbeESxg", "htmlBody": "<p>I remain unconvinced, probably because I mostly care about observer-moments, and don't really care what happens to individuals independently of this. You could plausibly construct some ethical theory that cares about identity in particular way such that this works, but I can't quite see how it would look, yet. You might want to make those ethical intuitions as concrete as you can, and put them under 'Assumptions'. </p>\n", "parentCommentId": "8peXBsGzH4xPBcHQe", "user": {"username": "Lukas_Finnveden"}}, {"_id": "4jsaufRD5nkn29TJW", "postedAt": "2018-09-29T13:42:28.807Z", "postId": "3jgpAjRoP6FbeESxg", "htmlBody": "<p>If I were suffering intensely, it wouldn't be comforting to me that there are other people who were just like me at one point but are now very happy \u2013 that feels like a completely different person to me. I'd rather there be someone completely happy than someone who had to undergo unnecessary suffering just to be more similar to me. Insofar as I care about personal identity, I care about whether it is a continuation of my brain, not whether it has similar experiences as me.</p>\n<p>Also, &quot;saving&quot; people using this method and having &quot;benevolent AIs [...] distribute parts of the task between each other using randomness&quot; seems indistinguishable from randomly torturing people, and that's very unappealing for me.</p>\n", "parentCommentId": null, "user": {"username": "michaelchen"}}, {"_id": "S3nu5HAz5y2Mj6uBB", "postedAt": "2018-09-29T16:26:10.827Z", "postId": "3jgpAjRoP6FbeESxg", "htmlBody": "<p>This is because you use not-copy-friendly theory of personal identity, which is reasonable but has other consequences.</p>\n<p>I patched the second problem in comments above - only the next moment after suffering will be simulated and diluted, and this will be obviously the happiest moment for someone in agony - to feel that the pain disappeared and to know that he is saved from hell. </p>\n<p>It will be like an angel, who comes to a cancer patient and tells him: your disease was just completely cured. If one ever got a negative result for cancer test, he may know this feeling of relief. </p>\n<p>Also, the fact that benevolent AI is capable to save observers from Evil AI (and also model Evil AIs in simulations and punish them if they dare to torture anyone) will significantly reduce (I hope) the number of Evil AI. </p>\n<p>Thus, the combination of the pleasure of being saved from Evil AI plus lowering the world-share of Evil AIs, as they can't win and know it - will increase the total positive utility in the universe.</p>\n", "parentCommentId": "4jsaufRD5nkn29TJW", "user": {"username": "turchin"}}, {"_id": "h3NkbWPf6ruqYHa8w", "postedAt": "2018-09-29T16:29:22.286Z", "postId": "3jgpAjRoP6FbeESxg", "htmlBody": "<p>I just find the way how the whole trick will increase total welfare in the multiverse, copied from the comment below: </p>\n<p>No copies of suffering observer-moments will be created -  only the next moment after suffering will be simulated and diluted, and this will be obviously the happiest moment for someone in agony - to feel that the pain disappeared and to know that he is saved from hell. </p>\n<p>It will be like an angel, who comes to a cancer patient and tells him: your disease was just completely cured. If one ever got a negative result for cancer test, he may know this feeling of relief. </p>\n<p>Also, the fact that benevolent AI is capable to save observers from Evil AI (and also model Evil AIs in simulations and punish them if they dare to torture anyone) will significantly reduce (I hope) the number of Evil AIs. </p>\n<p>Thus, the combination of the pleasure of being saved from Evil AI plus lowering the world-share of Evil AIs, as they can't win and know it - will increase the total positive utility in the universe.</p>\n", "parentCommentId": "4ynb8z7H695tjsvQe", "user": {"username": "turchin"}}, {"_id": "rANGzQXFexjHfijce", "postedAt": "2018-09-29T16:32:46.796Z", "postId": "3jgpAjRoP6FbeESxg", "htmlBody": "<p>It will also increase the number of happy observer-moments globally, because of the happiness of being saved from agony plus lowering the number of Evil AIs, as they will know they will lose and will be punished. </p>\n", "parentCommentId": "Fny7cZ6By6hAfYRi9", "user": {"username": "turchin"}}, {"_id": "isiX2erkiC6R7nu3J", "postedAt": "2018-09-29T16:35:04.683Z", "postId": "3jgpAjRoP6FbeESxg", "htmlBody": "<p>See patches in comments below: there are ways to do the trick not increasing the total number of suffering observer-moments.</p>\n", "parentCommentId": "8G4hErdGCxF33G3CH", "user": {"username": "turchin"}}, {"_id": "pvYYcXL3mskg78iH2", "postedAt": "2018-09-30T10:39:45.999Z", "postId": "3jgpAjRoP6FbeESxg", "htmlBody": "<p>There is no reason to prefer that over simply creating people with happy lives. You can always simulate someone who believes that they have just been saved from suffering if that constitutes the best life. The relation to some historical person who suffered is irrelevant and inefficient. </p>\n<p>Deterring unfriendly AI is another matter. There are so many possible goal functions that can be used to describe possible unfriendly AIs that a general strategy for deterring them doesn't make sense. At least not without a lot of theoretical groundwork that is presently lacking.</p>\n", "parentCommentId": "h3NkbWPf6ruqYHa8w", "user": {"username": "kbog"}}, {"_id": "de3yTW6xTrv6u3Hhi", "postedAt": "2018-09-30T15:20:34.918Z", "postId": "3jgpAjRoP6FbeESxg", "htmlBody": "<p>If the connection between observer-moments is not regarded as important (as I understand, this is your point of view: there is no identity that chains observer-moments in timelines) - than the creation of happy observer-moments is also a way to dilute past sufferings.</p>\n<h2>In other words, the creation of infinitely many happy observer-moments in the future makes the probability that a person finds himself into a suffering observer-moment infinitely small.</h2>\n<p>I addressed the problem of the simulation of evil AIs in the &quot;Message to any future AI&quot; text, which we already discussed. The main types of Evil AIs may be predicted by simulating the worlds where they likely to appear. We could get the advantage over the Evil AIs by declaring now, before their creation, that we will simulate and punish them in those timelines where will be able to create Benevolent AI. But we don't need to discuss all the technical details how we will do it, as a benevolent AI will find better ways. (The idea comes from Rolf Nelson)</p>\n", "parentCommentId": "pvYYcXL3mskg78iH2", "user": {"username": "turchin"}}]