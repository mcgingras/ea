[{"_id": "ZiX8hFSYpnqbdatwj", "postedAt": "2022-08-25T15:39:32.432Z", "postId": "AHoZX4JwFS2ygAoZR", "htmlBody": "<p>One design ideation method is instead of trying to think of good ideas, try to think of the worst possible idea.</p>\n<p>With that in mind, encourage the writers of \"It's Always Sunny in Philadelphia\" to do an episode \"The Gang Solves AGI Alignment\".</p>\n", "parentCommentId": null, "user": {"username": "MatthewDahlhausen"}}, {"_id": "7JdEHaaJjuyjiznyp", "postedAt": "2022-08-25T18:53:46.053Z", "postId": "AHoZX4JwFS2ygAoZR", "htmlBody": "<p>My primary reaction to this was \"ah man, I hope this person doesn't inadvertently annoy important people about AI safety being important, hurting the reputation of AI safety/longtermism/EA etc\"</p>", "parentCommentId": null, "user": {"username": "JackRyan"}}, {"_id": "6BjnL7w8729ntfF2y", "postedAt": "2022-08-25T21:50:15.799Z", "postId": "AHoZX4JwFS2ygAoZR", "htmlBody": "<p>:( As far as I know, no one from EA has annoyed &nbsp;(randomly emailed) Terry Tao about it, despite many people saying he would be a great person to have on board.</p><p>Obviously I'm not in favour of random EAs annoying important people (and hurting the reputation of EA/AI Alignment), but I do think given the <a href=\"https://forum.effectivealtruism.org/posts/9BPs6ZmtqCbNfYaKg/agi-x-risk-timelines-10-chance-by-year-x-estimates-should-be\">urgency</a> of the <a href=\"https://forum.effectivealtruism.org/posts/Y3sWcbcF7np35nzgu/without-specific-countermeasures-the-easiest-path-to-1\">situation</a> we are in, at some point, some high up people in EA/AI Alignment have to make some serious attempt at putting together such a dream team (<a href=\"https://forum.effectivealtruism.org/posts/KigFfo4TN7jZTcqNH/the-future-fund-s-project-ideas-competition?commentId=nzu63bZuLnDndyMnC\">more</a>).</p>", "parentCommentId": "7JdEHaaJjuyjiznyp", "user": {"username": "Greg_Colbourn"}}, {"_id": "RTcfMtEBCvTkFeFzh", "postedAt": "2022-08-26T21:59:59.013Z", "postId": "AHoZX4JwFS2ygAoZR", "htmlBody": "<p>You are probably aware, but someone <a href=\"https://www.lesswrong.com/posts/SG6fcAFhhJjys2WtH/has-anyone-actually-tried-to-convince-terry-tao-or-other-top\">recently</a> drafted an email and intended to send it, but was convinced not to send the email.</p>", "parentCommentId": "6BjnL7w8729ntfF2y", "user": {"username": "harfe"}}, {"_id": "3NMigYmeAX6pkdyLB", "postedAt": "2022-09-07T09:40:17.651Z", "postId": "AHoZX4JwFS2ygAoZR", "htmlBody": "<p>A bit sad that no one has actually answered the object level question and nearly all the discussion is meta. I can understand why. But I also think that we are at <a href=\"https://forum.effectivealtruism.org/posts/9BPs6ZmtqCbNfYaKg/agi-x-risk-timelines-10-chance-by-year-x-estimates-should-be\">crunch time</a> with this, and the stakes are as high as they can be. So this is actually a very serious question that serious people should be considering. Maybe (some) people high up in EA are considering it. I hope so!</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "jSzPojEuNughB8x9G", "postedAt": "2022-09-07T09:43:43.470Z", "postId": "AHoZX4JwFS2ygAoZR", "htmlBody": "<p>Yes, I think the fact that they didn't go through with it is some evidence that such a list need not be counterproductive to our goal (and the EV is probably positive). Ultimately the Dream Team needs to be approached, but I'm optimistic that this can be done in a careful and coordinated manor by the relevant senior people in EA/Alignment.</p>", "parentCommentId": "RTcfMtEBCvTkFeFzh", "user": {"username": "Greg_Colbourn"}}, {"_id": "pc3si4odwYXjEkyNt", "postedAt": "2022-09-12T04:45:30.681Z", "postId": "AHoZX4JwFS2ygAoZR", "htmlBody": "<p>I think the question is basically \"who are the most talented researchers in fields at least vaguely related to AI?\" The EA community is probably not the best group for answering this question. But it's an important question for sure!</p>", "parentCommentId": "3NMigYmeAX6pkdyLB", "user": {"username": "jakubkraus07@gmail.com"}}, {"_id": "Bso3TnFPpbGuxwpHw", "postedAt": "2022-11-02T20:24:26.793Z", "postId": "AHoZX4JwFS2ygAoZR", "htmlBody": "<p>Some ideas for identifying dream team members:</p><ul><li>&nbsp;Anyone a panel of top people in AGI Safety would have on their dream team (who otherwise would be unlikely to work on the problem).</li><li>Fields Medalists, Nobel Prize winners in Physics, other equivalent prize recipients in Computer Science, or Philosophy(?) or Economics(?).</li><li>Those topping&nbsp;relevant <a href=\"https://research.com/news-events/world-ranking-of-top-mathematics-scientists-2022\">world ranking lists</a>.&nbsp;</li><li>People who've scored at the very top end in&nbsp;<a href=\"https://en.wikipedia.org/wiki/Standardized_test\">standardised tests</a>.</li><li>Those with a track record of multiple field-changingly-important research accomplishments.</li><li>The most highly paid engineers or researchers.</li><li>Winners of top computer programming, maths or physics competitions/olympiads.</li><li>The world's best (board and video) games players</li><li>Being able to grok what is being alluded to&nbsp;<a href=\"https://www.lesswrong.com/posts/hwxj4gieR7FWNwYfa/ngo-and-yudkowsky-on-ai-capability-gains-1?commentId=uoexqbjgaHzRRoqcE#comments\">here</a>?</li></ul>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}]