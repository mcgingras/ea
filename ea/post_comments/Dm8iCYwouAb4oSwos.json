[{"_id": "b2cMMwqRrs4XRGMTu", "postedAt": "2022-11-21T08:33:50.868Z", "postId": "Dm8iCYwouAb4oSwos", "htmlBody": "<p>Great post, well explained!</p>\n<p>I like the idea of having a more relatable message than \"do the most good\", but I am not sure how much more relatable \"do alot of good\" is. To me it seems that there might not be that much of a difference between the two, at least in how they are used in day to day discussion (that is, applying a filter of \"practicality\" to the maximization problem).</p>\n<p>For example, I thought it was common EA Knowledge that there are \"top recommended cause areas\" (on 80K), where some are higher on the list but with a big * of uncertainty. Theres also enough people to work on all of them, so there's no need for a final judgement of a top 3, let alone 1 most important cause. In a way this could be a \"macro\" EA perspective - asking not what is the most good an individual could do, but what is the most good a group/society can do, with appropriate allocation between cause areas of high ITN.</p>\n<p>I think EA can come across as a bit elitist to others, especially to people voulenteering in non-EA charities or trying to do good in \"traditional\" ways (doctors, Med-tech, activism, etc). Perhaps the \"do alot of good\" can help with that - but I still think it would come to similar conclusions in some cases.\nI have a friend who is volenteering in \"Make a Wish\" for the past 10 years, and I felt a little uneasy telling him about EA without offending him - though I was able to, and while he was intrigued I don't think he was convinced.</p>\n<p>I had a thought a while ago, that perhaps the world would be much better if there were alot of people committed to doing \"at least a little good\" , rather than a (relatively) small group of highly ambitious people doing \"the most good\". However, perhaps there is room for that as a separate movement from EA. Plus, someone for sure needs to work on the \"big things\" too, which seems like a good niche for EA.</p>\n", "parentCommentId": null, "user": {"username": "Ariel G."}}, {"_id": "Kys8wfwZskCnDJa7d", "postedAt": "2022-11-21T15:22:06.738Z", "postId": "Dm8iCYwouAb4oSwos", "htmlBody": "<p>Great post!</p><p>I might be wrong but I don't think many EAs actually believe that say, donating to GiveWell is the single most good they can do for the world. &nbsp;In the actual situation, given epistemic uncertainty, it happens to be a clear example of what you mentioned - \"actions that they can be reasonably sure do a lot of good\" So there is an implicit belief, revealed in actions that merely doing a lot of good is not only an acceptable, but a recommended behaviour.</p><p>However I'm not sure it logically follows from this that seeking to do \"the most\" good should be abandoned as a goal. This is particularly the case if effective altruism is not defined as an imperative of any kind but as an overall approach that says \"given that I've already decided on my own to be more altruistic, how can my time/money make the biggest difference\"?&nbsp;<br><br>Despite being an unattainable ideal if you take it literally, &nbsp;the \"most\" framing is still fruitful - it gives altruistic, open minded, but resource constrained people (which describes a lot more people than we might've thought) a scope sensitive framework to prioritize resource allocations.&nbsp;<br><br>To see why, let's take an example. It could be argued that giving to the community theatre does not just a little, but a lot of good. If you are a billionaire giving millions to community theatres all over the world there is a reasonable chance that you are doing a lot of good. (And such altruism should be praised, compared to spending those same millions say lobbying for big tobacco).<br><br>What effective altruism then brings to the table is to say \"look, if you have a sentimental attachment to giving to the community theatre, that's fine. But if you're indifferent towards particular means and your goal is simply to be a good person and help the world,<i> the same money could carry you much further &nbsp;towards your goal if you did X.</i>\"</p><p>Of course you can then say sure, X sounds good, but what about Y? What about Z? And so on, ad infinitum. At some point though, you have to make a decision. That decision will be far from perfect, since you lack perfect information. However, by using a scope sensitive optimization framework, you will have been able to achieve a lot more good than you would have otherwise.<br><br>So while optimization has its flaws, I would characterize it on the whole as one of those \"wrong, but useful\" models.</p>", "parentCommentId": null, "user": null}, {"_id": "cbrzQCrh5XsgAjrJ4", "postedAt": "2022-11-21T16:13:00.077Z", "postId": "Dm8iCYwouAb4oSwos", "htmlBody": "<p>Thank you for reading the post and for the helpful comment! I totally agree that \"do a lot of good\" isn't particularly unique or sexy a message. As I mentioned in the post, I believe that part of EA's early appeal was that maximization was so radical and elusive. I do think there is a fairly big difference between the two messages though, especially when it comes to elite donors. Academics like Anand Giridharadas (in <i>Winners Take All</i>) and Rob Reich (in <i>Just Giving</i>) argue that elite philanthropy is often a well-disguised charade to boost the donor's own power and status. I'm sympathetic to the argument that sometimes, these elites are unaware of the ways in which their giving (e.g. to private schools, religious institutions, the arts, etc.) increase existing inequalities. However, it's also easy to argue that some of these wealthy individuals are not doing \"a lot of good\". I don't think \"do a lot of good\" is some magic bullet that will fix billionaire philanthropy, but I also think that it might make it easier to convince elites they need to change their philanthropy (and other actions...) than the rhetoric of maximization.</p><p>Great point about 80k Hours. It's certainly interesting to take a macro-level view of what a large group or whole society could do with respect to cause areas. It reminds me a little bit of trying to reason like a Rawlsian about ideal theory. Our decisions as individuals become incredibly contingent on what other members of the group decide, especially other members with wealth and power. In this ideal world, the \"Neglect\" area becomes arguably the most important, which seems to take away from the power of EA (because, as you say, our niche is taking on the big things not just the things that nobody is doing). I think we're in a time when there are tons of Important and Tractable causes which aren't being altogether neglected but still need more resources. All this is just roundabout way of saying I'm skeptical of both the usefulness and feasibility of trying to take the macro perspective and maximize a group's impact.</p><p>I'm particularly sympathetic to your point about elitism, and part of my motivation for this post was to try to temper that problem within EA. In my conversations with friends about EA, it's never the idea of maximization that changes their worldview. Instead, they're usually more interested in the argument that there are big ways to make impact which elite philanthropy largely ignores. If you're talking with somebody who dedicates her free time volunteering for an org like Make A Wish, maximization is a non-starter, but sewing the seed that there might be additional avenues for impact will at least allow for some discourse.</p><p>On your last point, I think that the \"everybody just does a little good\" world is already the world we live in! I agree that there is serious need to for groups of people to tackle the big things, but in an ideal world, this is what governments do. Just like many nonprofits say, EA's main goal should be to not need to exist (because institutions are tackling high-ITN goals efficiently).</p>", "parentCommentId": "b2cMMwqRrs4XRGMTu", "user": {"username": "Jasper Meyer"}}, {"_id": "zDtxpBLuPBawzMJ63", "postedAt": "2022-11-21T16:31:09.924Z", "postId": "Dm8iCYwouAb4oSwos", "htmlBody": "<p>Neat stuff here - thank you for the thoughtful comment!&nbsp;</p><p>I agree that few people believe that their choice of intervention is actually the <i>most </i>useful and that we often lavish praise onto people who do just a lot of good. For example, many people consider characters like Warren Buffett and Bill Gates very praiseworthy because, even though they have private jets, they still do a lot of good.</p><p>I also agree that maximization ought not be reveled as an imperative. An imperative to maximize, like thick &nbsp;consequentialism as a moral theory generally, is too demanding. Following this, I struggle to see why we need it at all. Folks who truly want to do a lot of good will still perform optimization calculations even if they aren't explicitly trying to maximize. This makes maximization neither a normative nor descriptive part of anything we do.</p><p>In your example about \"<i>the same money could carry you much further &nbsp;towards your goal if you did X</i>\", there is no maximization rhetoric present. If you were using maximization as a \"wrong but useful\" model, you would likely say something like, \"<i>I deduced that the same money could carry you farthest if you did X, so don't give to community theater and don't do Y or Z either unless you show me why they're more effective than X</i>\".</p><p>As an analogy, you don't have to try to be the best philosopher of all time in order to produce great thinking.&nbsp;</p>", "parentCommentId": "Kys8wfwZskCnDJa7d", "user": {"username": "Jasper Meyer"}}]