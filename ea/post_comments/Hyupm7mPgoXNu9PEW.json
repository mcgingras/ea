[{"_id": "TLFhMA4fYhejTWfNn", "postedAt": "2023-07-26T08:17:17.223Z", "postId": "Hyupm7mPgoXNu9PEW", "htmlBody": "<p>Hey, there is a common plan I hear that maybe you'd like to respond to directly.</p><p>It goes something like this: \"I'll go work at a top AI lab as an engineer, build technical skills, and I care about safety so I can push a bit towards safe decisions, or push a lot if it's important, overall it seems good to have people there who care about safety like me. I don't have a good understanding of how to do alignment but there are some people I trust\"</p><p>If you're willing to reply to this, I'll probably refer people directly to your answer sometimes</p>", "parentCommentId": null, "user": {"username": "hibukki"}}, {"_id": "toXxqQu6BjAZjonyo", "postedAt": "2023-07-26T11:00:09.556Z", "postId": "Hyupm7mPgoXNu9PEW", "htmlBody": "<p>Hi Yonatan,</p><p>I think that for many people (but not everyone) and for many roles they might work in (but not all roles), this is a reasonable plan.</p><p>Most importantly, I think it's true that working at a top AI lab as an engineer is one of the best ways to build technical skills (see the section above on \"it's often excellent career capital\").&nbsp;</p><p>I'm more sceptical about the ability to push towards safe decisions (see the section above on \"you may be able to help labs reduce risks\").</p><p>The right answer here depends a lot on the specific role. I think it's important to remember than not all AI capabilities work is necessarily harmful (see the section above on \"you might advance AI capabilities, which could be (really) harmful\"), and that top AI labs could be some of the most positive-impact organisations in the world (see the section above on \"labs could be a huge force for good - or harm\"). On the other hand, there are roles that seem harmful to me (see \"how can you mitigate the downsides of this option\").</p><p>I'm not sure of the relevance of \"having a good understanding of how to do alignment\" to your question. I'd guess that lots of knowing \"how to do alignment\" is being very good at ML engineering or ML research in general, and that working at a top AI lab is one of the best ways to learn those skills.</p>", "parentCommentId": "TLFhMA4fYhejTWfNn", "user": {"username": "Benjamin Hilton"}}, {"_id": "txGRgS6xphnb4jFmd", "postedAt": "2023-07-26T19:44:05.574Z", "postId": "Hyupm7mPgoXNu9PEW", "htmlBody": "<p>Hi! Thanks for your answer. TL;DR: I understand and don't have further questions on this point</p><p>&nbsp;</p><p>What I mean by \"having a good understanding of how to do alignment\" is \"being opinionated about (and learning to notice) which directions make sense, as opposed to only applying one's engineering skills towards someone else's plan\".</p><p>I think this is important if someone wants to affect the situation from inside, because the alternative is something like \"trust authority\".</p><p>But it sounds like you don't count on \"the ability to push towards safe decisions\" anyway</p>", "parentCommentId": "toXxqQu6BjAZjonyo", "user": {"username": "hibukki"}}, {"_id": "xDGKfdcKCyDd9QjPb", "postedAt": "2023-07-26T19:45:44.455Z", "postId": "Hyupm7mPgoXNu9PEW", "htmlBody": "<p>What do you think about the effect of many people (EAs) joining top AI labs - on the race dynamics between those labs?</p><p>Hard for me to make up my mind here</p><p>&nbsp;</p><p>Adding [edit] :</p><p>This seems especially important as you're advising many people to consider entering the field, where one of the reasons to do it is \"Moving faster could <a href=\"https://www.cold-takes.com/racing-through-a-minefield-the-ai-deployment-problem/\">reduce the risk that AI projects that are <i>less</i> cautious than the existing ones can enter the field</a>.\" (but you're sending people to many different orgs).</p><p>In other words: It seems maybe negative to encourage many people to enter a race, on many different competing \"teams\", if you want the entire field to move slowly, no?</p><p>When I talk to people, I sometimes explicitly say that this is a way of thinking that I hope most people WON'T use.</p>", "parentCommentId": null, "user": {"username": "hibukki"}}, {"_id": "3JLkg8MtjGdt89Exd", "postedAt": "2023-07-27T11:12:22.718Z", "postId": "Hyupm7mPgoXNu9PEW", "htmlBody": "<p>I think at this point, we are not far off this being&nbsp;<br><br>\"<strong>Should you work at a leading oil company? (including in non-renewables roles)</strong>\".&nbsp;<br><br>Or even&nbsp;<br><br>\"<a href=\"https://forum.effectivealtruism.org/posts/7yK5fB7y3bb8dEMED/aisn-16-white-house-secures-voluntary-commitments-from#Lessons_from_Oppenheimer\"><strong>Hans Bethe</strong></a><strong> has just calculated that the chance of the first A-bomb test igniting the atmosphere is </strong><a href=\"https://twitter.com/AISafetyMemes/status/1682698894361124865\"><strong>10%</strong></a><strong>; should you work at the Manhattan Project? (including in non-</strong><a href=\"https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\"><strong>shutting-it-down</strong></a><strong> roles)</strong>\".<br><br>EA has already contributed massively to the <a href=\"https://forum.effectivealtruism.org/posts/f2qojPr8NaMPo2KJC/beware-safety-washing\">safety-washing</a> of the big AI companies (not to mention<a href=\"https://www.openphilanthropy.org/grants/openai-general-support/\"> kicking off</a> and <a href=\"https://www.anthropic.com/\">accelerating</a> the race toward AGI in the first place!) I think EAs should be focusing more on applying external pressure now. There are ways to have higher leverage on existential safety by joining (not yet captured) <a href=\"https://www.gov.uk/government/news/initial-100-million-for-expert-taskforce-to-help-uk-build-and-adopt-next-generation-of-safe-ai\">AI governance</a>, <a href=\"https://www.aipolicy.us/\">lobbying</a> and <a href=\"https://pauseai.info/\">public</a> <a href=\"https://www.campaignforaisafety.org/\">campaigning</a> <a href=\"https://join.slack.com/t/agi-moratorium-hq/shared_invite/zt-1yvu9d7ye-Wj8pAh3ZEhce4W7JwviLMg\">efforts</a>.</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "AGBawaAok4bxaGowJ", "postedAt": "2023-07-27T13:52:20.089Z", "postId": "Hyupm7mPgoXNu9PEW", "htmlBody": "<p>TL;DR: \"which lab\" seems important, no?</p><hr><p>You wrote:</p><blockquote><p><strong>Don\u2019t work in certain positions</strong> unless you feel <i>awesome</i> about the lab being a <a href=\"https://80000hours.org/career-reviews/working-at-an-ai-lab/#force-for-good-or-bad\">force for good</a>.</p></blockquote><p>First of all I agree, thumbs up from me! \ud83d\ude4c</p><p>&nbsp;</p><p>But you also wrote:</p><blockquote><h2><strong>Recommended organisations</strong></h2><p>We\u2019re really not sure. It seems like <a href=\"https://openai.com/\">OpenAI</a>, <a href=\"https://www.deepmind.com/\">Google DeepMind</a>, and <a href=\"https://www.anthropic.com/\">Anthropic</a> are currently taking existential risk more seriously than other labs.</p></blockquote><p>&nbsp;</p><p>I assume you don't recommend people go work for whatever lab \"currently [seems like they're] taking existential risk more seriously than other labs\" ?</p><p>&nbsp;</p><p>Do you have further recommendations on how to pick a lab?</p><p>(Do you agree this is a really important part of an AI-Safety-Career plan, or does it seem sort-of-secondary to you?)</p><p>&nbsp;</p><p>I'm asking in the context of an engineer considering working on capabilities (and if they're building skill - they might ask themselves \"what am I going to use this skill for\", which I think is a good question). Also, I noticed you wrote \"broadly advancing AI capabilities should be regarded overall as probably harmful\", which I agree with, and seems to make this question even more important.</p>", "parentCommentId": null, "user": {"username": "hibukki"}}, {"_id": "gcPJsC5nMDqpSAwRJ", "postedAt": "2023-07-27T13:52:33.783Z", "postId": "Hyupm7mPgoXNu9PEW", "htmlBody": "<p>For transparency: I'd personally encourage 80k to be more opinionated here, I think you're well positioned and have relevant abilities and respect and critical-mass-of-engineers-and-orgs. Or at least as a fallback (if you're not confident in being opinionated) - I think you're well positioned to make a high quality discussion about it, but that's a long story and maybe off topic.</p>", "parentCommentId": "AGBawaAok4bxaGowJ", "user": {"username": "hibukki"}}, {"_id": "AzaNbeLAawEEYdTmz", "postedAt": "2023-07-28T12:15:22.523Z", "postId": "Hyupm7mPgoXNu9PEW", "htmlBody": "<p>Thanks, this is an interesting heuristic, but I think I don't find it as valuable as you do.&nbsp;</p><p>First, while I <i>do</i> think it'd probably be harmful in expectation to work at leading oil companies / at the Manhattan project, I'm not confident in that view \u2014 I just haven't thought about this very much.</p><p>Second, I think that AI labs are in a pretty different reference class from oil companies and the development of nuclear weapons.</p><p>Why? Roughly:</p><ol><li>Whether, in a broad sense, capabilities advances are good or bad is pretty unclear. (Note some capabilities advances in particular areas are very clearly harmful.) In comparison, I <i>do</i> think that, in a broad sense, the development of nuclear weapons, and the release of greenhouse gases are harmful.&nbsp;</li><li>Unlike with oil companies and the Manhattan Project, I think that there's a good chance that a leading, careful AI project could be a <i>huge</i> force for good, substantially reducing existential risk \u2014 and so it seems weird not to consider working at what could be one of the world's most (positively) impactful organisations. Of course, you should also consider the chance that the organisation could be one of the world's most <i>negatively</i> impactful organisations.</li></ol><p>Because these issues are difficult and we don\u2019t think we have all the answers, I also published a <a href=\"https://80000hours.org/articles/ai-capabilities/\">range of opinions about a related question in our anonymous advice series</a>. Some of the respondents took a very sceptical view of <i>any</i> work that advances capabilities, but others disagreed.</p>", "parentCommentId": "3JLkg8MtjGdt89Exd", "user": {"username": "Benjamin Hilton"}}, {"_id": "FRpadecmtDZwJzRBP", "postedAt": "2023-07-28T12:17:52.533Z", "postId": "Hyupm7mPgoXNu9PEW", "htmlBody": "<p>I don't currently have a confident view on this beyond \"We\u2019re really not sure. It seems like OpenAI, Google DeepMind, and Anthropic are currently taking existential risk more seriously than other labs.\"</p><p>But I agree that if we could reach a confident position here (or even just a confident list of considerations), that would be useful for people \u2014 so thanks, this is a helpful suggestion!</p>", "parentCommentId": "gcPJsC5nMDqpSAwRJ", "user": {"username": "Benjamin Hilton"}}, {"_id": "mMTYANPqfvyqhpgbW", "postedAt": "2023-08-01T14:49:17.339Z", "postId": "Hyupm7mPgoXNu9PEW", "htmlBody": "<blockquote><p>I think that there's a good chance that a leading, careful AI project could be a <i>huge</i> force for good, substantially reducing existential risk</p></blockquote><p>I think the burden of proof should be on the big AI companies to show that this is actually a possibility. Because right now, the technology, as based on the current paradigm, looks like it's <a href=\"https://iai.tv/articles/the-hard-problem-of-ai-safety-auid-1773\">fundamentally</a> <a href=\"https://llm-attacks.org/\">uncontrollable</a>.</p>", "parentCommentId": "AzaNbeLAawEEYdTmz", "user": {"username": "Greg_Colbourn"}}, {"_id": "7ZMz6SZ3xxrwbCSbh", "postedAt": "2023-08-03T07:22:47.786Z", "postId": "Hyupm7mPgoXNu9PEW", "htmlBody": "<p>TL;DR: I don't like talking about \"burden of proof\"</p><p>&nbsp;</p><p>I prefer talking about \"priors\".</p><p>Seems like you ( <a href=\"https://forum.effectivealtruism.org/users/greg_colbourn?mention=user\">@Greg_Colbourn</a> ) have priors that AI labs will cause damage, and I'd assume <a href=\"https://forum.effectivealtruism.org/users/benjamin-hilton-1?mention=user\">@Benjamin Hilton</a> would agree with that?</p><p>I also guess you both have priors that ~random (average) capabilities research will be net negative?</p><p>If so, I suggest we should ask if the AI lab (or the specific capabilities research) has overcome that prior somehow.</p><p>wdyt?</p>", "parentCommentId": "mMTYANPqfvyqhpgbW", "user": {"username": "hibukki"}}, {"_id": "cRJybzr6HAiiGLEsC", "postedAt": "2023-08-04T13:43:50.688Z", "postId": "Hyupm7mPgoXNu9PEW", "htmlBody": "<p>I don't think any of the big AI labs have overcome that prior, but I also have the prior that their safety plans don't even make sense theoretically - hence the \"burden of proof\" is on them to show that it is possible to align the kind of AI they are building. Another <a href=\"https://manifund.org/projects/alignment-is-hard\">thing</a> pointing in the opposite direction.</p>", "parentCommentId": "7ZMz6SZ3xxrwbCSbh", "user": {"username": "Greg_Colbourn"}}, {"_id": "Pj3bcjWJSzYKfNXtE", "postedAt": "2023-08-05T09:58:21.780Z", "postId": "Hyupm7mPgoXNu9PEW", "htmlBody": "<p>Whoever downvoted this, I'd really prefer if you tell me why</p><p>You can do it anonymously:</p><p><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSca6NOTbFMU9BBQBYHecUfjPsxhGbzzlFO5BNNR1AIXZjpvcw/viewform\">https://docs.google.com/forms/d/e/1FAIpQLSca6NOTbFMU9BBQBYHecUfjPsxhGbzzlFO5BNNR1AIXZjpvcw/viewform</a></p>", "parentCommentId": "7ZMz6SZ3xxrwbCSbh", "user": {"username": "hibukki"}}]