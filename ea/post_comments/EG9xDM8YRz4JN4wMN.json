[{"_id": "5NkjqKQ7wHjFFG2ih", "postedAt": "2022-09-09T05:44:27.413Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<blockquote><p>A superforecaster aggregate (I\u2019m biased re: quality of Samotsvety vs. superforecasters, but I\u2019m pretty confident based on personal experience)</p></blockquote><p>Is this on specific topic areas (e.g., \"TAI forecasting\" or \"EA topics\") or more generally?&nbsp;</p>", "parentCommentId": null, "user": {"username": "Lukas_Gloor"}}, {"_id": "XAzH7NCg8sXdfateo", "postedAt": "2022-09-09T05:53:01.605Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>More generally, though probably especially on AI. I'm not exactly sure how to handle some of the evidence for why I'm pretty confident on this, but I can gesture at a few data points that seem fine to share:</p><ol><li>Nu\u00f1o, Misha and I each did much better on INFER than at least a few superforecasters (and we were 3 of the top 4 on the leaderboard; no superforecasters did better than us).</li><li>I've seen a few superforecaster aggregate predictions that seemed pretty obviously bad in advance, which I pre-registered and in both cases I was right.<ol><li><a href=\"https://twitter.com/eli_lifland/status/1490007448144560134\">I said on Twitter </a>I thought supers were overconfident on Russia invasion of Ukraine; supers were at 84% no invasion, l was at 55% (and Metaculus was similar, to be fair). Unfortunately, Russia went on to invade Ukraine a few weeks later.</li><li><a href=\"https://www.metaculus.com/questions/7542/over-200k-us-covid-daily-cases-by-2022/#comment-66707\">I noticed</a> that superforecasters may be overconfident on the rise of the Delta COVID variant, and it seems they likely were: they predicted a 14% chance 7-day median would rise above 140k cases and a 2% (!) chance it would rise above 210k; it ended up peaking at about 150k so this is weak evidence, but it still seems like 2% was crazy low.</li></ol></li></ol><p>I'm hesitant to make it seem like I'm bashing on superforecasters, I think they're amazing overall relative to the general public. But I think Samotsvety is even more amazing :D I also think some superforecasters are much better forecasters than others.</p>", "parentCommentId": "5NkjqKQ7wHjFFG2ih", "user": {"username": "elifland"}}, {"_id": "HBfDt5ZfYWWbqNcp3", "postedAt": "2022-09-09T11:46:52.469Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>I am told that APS, in this context, stands for \"advanced, planning, strategically aware\" and is from Carlsmith's report <a href=\"https://arxiv.org/abs/2206.13353\">https://arxiv.org/abs/2206.13353</a></p>", "parentCommentId": null, "user": {"username": "KevinO"}}, {"_id": "GGcF34hxsdhkS3PRj", "postedAt": "2022-09-09T11:58:56.474Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>I'm curious whether there's any answer AI experts could have given that would be a reasonably big update for you.</p>\n<p>For example is there any level of consensus against ~AGI by 2070 (or some other date) that would be strong enough to move your forecast by 10 percentage points?</p>\n", "parentCommentId": null, "user": {"username": "HowieL"}}, {"_id": "LfEgdnhi62BEe3ayo", "postedAt": "2022-09-09T12:18:43.694Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>This doesn't sound like an outlandish claim to me. Still, I'm not yet convinced.<br><br>I was really into Covid forecasting at the time, so I was tempted to go back through my comment history and noticed that this seemed like an extremely easy call at the time. (I made <a href=\"https://www.lesswrong.com/posts/3JPiTXTq7QYJJd3LN/will-the-us-have-more-than-100-000-new-daily-covid-19-cases?commentId=4NfHaqcFTMhvxZBJ9\">this comment</a> 15 days before yours where I was predicting &gt;100,000 cases with 98% confidence, saying I'd probably go to 99% after more checking of my assumptions. &nbsp;Admittedly, &gt;100,000 cases in a single day is significantly less than &gt;140,000 cases for the 7-day average. Still, a confidence level of 98%+ suggests that I'd definitely have put a lot more than 14% on the latter.) This makes me suspect that maybe that particular question &nbsp;was quite unrepresentative for the average track record of superforecasters? Relatedly, if we only focus on instances where it's obvious that some group's consensus is wrong, it's probably somewhat easy to find such instances (even for elite groups) because of the favorable selection effect at work. A through analysis would look at the track record on a pre-registered selection of questions.<br><br>Edit: The particular Covid question is strong evidence for \"sometimes superforecasters don't seem to be trying as much as they could.\" So maybe your point is something like \"On questions where we try as hard as possible, I trust us more than the average superforecaster prediction.\" I think that stance might be reasonable.&nbsp;</p>", "parentCommentId": "XAzH7NCg8sXdfateo", "user": {"username": "Lukas_Gloor"}}, {"_id": "YAntnfLft6MXvgRMD", "postedAt": "2022-09-09T13:09:36.788Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>Good question. I think AI researchers views inform/can inform me. A few examples from the recent <a href=\"https://nlpsurvey.net/\">NLP Community Metasurvey</a>. I would quote bits from  <a href=\"https://www.lesswrong.com/posts/3zfFPjMv9fioDAeHi/survey-of-nlp-researchers-nlp-is-contributing-to-agi\">this</a> summary.</p>\n<blockquote>\n<p>Few scaling maximalists: 17% agreed that Given resources (i.e., compute and data) that could come to exist this century, scaled-up implementations of established existing techniques will be sufficient to practically solve any important real-world problem or application in NLP.</p>\n</blockquote>\n<p>This was surprsing and updated me somewhat against shorter timelines (and higher risk) as, for example, it clashes with the \"+12 OOMS Enough\" premise of the Kokotajlo's <a href=\"https://docs.google.com/document/d/1oqTuVMa5Lxf6z8ybF7eirtuWEFekJJ6nwudhE7ZjNSY/edit\">argument</a> for short timelines of the Carlsmith report (see also <a href=\"https://www.lesswrong.com/posts/rzqACeBGycZtqCfaX/fun-with-12-ooms-of-compute\">this</a> and <a href=\"https://docs.google.com/document/d/1GwT7AS_PWpglWWrVrpiMqeKiJ_E2VgAUIG5tTdVhVeM/edit#\">his review</a> of Carlsmith report).</p>\n<blockquote>\n<p>NLP is on a path to AGI: 58% agreed that Understanding the potential development of artificial general intelligence (AGI) and the benefits/risks associated with it should be a significant priority for NLP researchers.\nRelated: 57% agreed that Recent developments in large-scale ML modeling (such as in language modeling and reinforcement learning) are significant steps toward the development of AGI.</p>\n</blockquote>\n<p>If these numbers were significantly lower or higher, it would also probably update my views.</p>\n<blockquote>\n<p>AGI could be catastrophic: 36% agreed that It is plausible that decisions made by AI or machine learning systems could cause a catastrophe this century that is at least as bad as an all-out nuclear war.\n46% of women and 53% of URM respondents agreed.\nThe comments suggested that people took a pretty wide range of interpretations to this, including things like OOD robustness failures leading to weapons launches.</p>\n</blockquote>\n<p>This number is puzzling and hard to interpret. It seems appropriate in light of AI Impacts' <a href=\"https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/\">What do ML researchers think about AI in 2022?</a> where \"48% of respondents gave at least 10% chance of an extremely bad outcome\".</p>\n<p>I don't fully understand what this implies about the ML community's views on AI alignment. But I can see myself updating positively if these concerns would lead to more safety culture, alignment research, etc.</p>\n", "parentCommentId": "GGcF34hxsdhkS3PRj", "user": {"username": "Misha_Yagudin"}}, {"_id": "tcff2HYZMjhbudLvZ", "postedAt": "2022-09-09T13:20:35.877Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>Really appreciate this deep dive!</p><blockquote><p>This doesn't sound like an outlandish claim to me. Still, I'm not yet convinced.</p></blockquote><p>Yeah, I think the evidence I felt comfortable sharing right now is enough to get to some confidence but perhaps not high confidence, so this is fair. The INFER point is probably stronger than the two bad predictions which is why I put it first.</p><blockquote><p>I was really into Covid forecasting at the time, so I was tempted to go back through my comment history and noticed that this seemed like an extremely easy call at the time... Relatedly, if we only focus on instances where it's obvious that some group's consensus is wrong, it's probably somewhat easy to find such instances (even for elite groups) because of the favorable selection effect at work. A through analysis would look at the track record on a pre-registered selection of questions.</p></blockquote><p>I agree a more thorough analysis would look at the track record on a pre-registered selection of questions would be great. It's pretty hard to know because the vast majority of superforecaster predictions are private and not on their public dashboard. Speaking for myself, I'd be pretty excited about a Samotsvety vs. supers vs. [any other teams who were interested] tournament happening.</p><p>That being said, I'm confused about how you seem to be taking \"I was really into Covid forecasting at the time, so I was tempted to go back through my comment history and noticed that this seemed like an extremely easy call at the time\" as an update toward superforecasters being better? If anything this feels like an update against superforecasters? The point I was trying to make was that it was a foreseeably wrong prediction and you further confirmed it?</p><p>I'd also say that on the cherry-picking point, I wasn't exactly checking the superforecaster public dashboard super often over the last few years (like maybe I've checked ~25-50 days total) and there are only like 5 predictions up at a time.</p><blockquote><p>Edit: The particular Covid question is strong evidence for \"sometimes superforecasters don't seem to be trying as much as they could.\" So maybe your point is something like \"On questions where we try as hard as possible, I trust us more than the average superforecaster prediction.\" I think that stance might be reasonable.&nbsp;</p></blockquote><p>I think it's fair to interpret the Covid question to some extent as superforecasters not trying, but I'm confused about how you seem to be attributing little of it to prediction error? It could be a combination of both.</p>", "parentCommentId": "LfEgdnhi62BEe3ayo", "user": {"username": "elifland"}}, {"_id": "s9PoLdazxrQroEorT", "postedAt": "2022-09-09T13:22:58.614Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>Yup, I linked the text \"APS-AI\" in the post to the <a href=\"https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit#heading=h.14onymzb0y9\">relevant section of the report</a>. Sorry if it wasn't that noticeable!</p>", "parentCommentId": "HBfDt5ZfYWWbqNcp3", "user": {"username": "elifland"}}, {"_id": "Phz3GoHsJ2rDjuS5W", "postedAt": "2022-09-09T13:26:15.741Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>I believe your linked text for existential catastrophe (in the second table) is incorrect- I get a page not found error.</p>\n<p>Substantively, I realize this is probably not something you originally asked (nor am I asking for it since presumably this\u2019d take a bunch of time), but I\u2019d be super curious to see what kind of uncertainty estimates folks put in this, and how combining using those uncertainties might look. If you have some intuition on what those intervals look like, that\u2019d be interesting.</p>\n<p>The reason I\u2019m curious about this is probably fairly transparent, but given the pretty extensive broader community uncertainty on the topic, aggregating using those might yield a different point estimate, but more importantly might help people understand the problem better by seeing the large degree uncertainty involved. For example, it\u2019d be interesting/useful to see how much probability people put outside a 10-90% range.</p>\n", "parentCommentId": null, "user": {"username": "Andy Timm"}}, {"_id": "kGhzSutZin4rt93kt", "postedAt": "2022-09-09T13:33:32.701Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>Fair question. I say little weight but if it was far enough from my view I would update a little. My view also may not be representative of other forecasters, as is evident from Misha's comment.</p><blockquote><p>For example is there any level of consensus against ~AGI by 2070 (or some other date) that would be strong enough to move your forecast by 10 percentage points?</p></blockquote><p>From the original <a href=\"https://www.google.com/url?q=https://arxiv.org/abs/1705.08807&amp;sa=D&amp;source=editors&amp;ust=1662684319718836&amp;usg=AOvVaw1Q-hjAYd-UOUc6_RsA4Pjl\">Grace et al. survey</a> (and I think the more recent ones as well? but haven't read as closely) the ML researchers clearly had very incoherent views depending on the question being asked and elicitation techniques, which I think provides some evidence they haven't thought about it that deeply and we shouldn't take it too seriously (some incoherence is expected, but I think they gave wildly different answers for HLMI (human-level machine intelligence) and full automation of labor).</p><p>So I think I'd split up the thresholds by somewhat coherent vs. still very incoherent.</p><p>My current forecast for ~AGI by 2100 barring pre-AGI catastrophe is 80%. To move it to 70% based just a survey of ML experts, I think I'd have to see something like one of:</p><ol><li>ML experts still appear to be very incoherent, but are giving a ~10% chance of ~AGI by 2100 on average across framings.</li><li>ML experts appear to be somewhat coherent, and are giving a ~25% chance of ~AGI by 2100.</li></ol><p>(but I haven't thought about this a lot, these numbers could change substantially on reflection or discussion/debate)</p>", "parentCommentId": "GGcF34hxsdhkS3PRj", "user": {"username": "elifland"}}, {"_id": "2r8nTREQpP6Hka8mC", "postedAt": "2022-09-09T13:36:58.860Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<blockquote><p>I believe your linked text for existential catastrophe (in the second table) is incorrect- I get a page not found error.</p></blockquote><p>Thanks, fixed.</p><blockquote><p>Substantively, I realize this is probably not something you originally asked (nor am I asking for it since presumably this\u2019d take a bunch of time), but I\u2019d be super curious to see what kind of uncertainty estimates folks put in this, and how combining using those uncertainties might look. If you have some intuition on what those intervals look like, that\u2019d be interesting.</p><p>The reason I\u2019m curious about this is probably fairly transparent, but given the pretty extensive broader community uncertainty on the topic, aggregating using those might yield a different point estimate, but more importantly might help people understand the problem better by seeing the large degree uncertainty involved. For example, it\u2019d be interesting/useful to see how much probability people put outside a 10-90% range.</p></blockquote><p>Good question! Perhaps we can include better uncertainty information in a future post at some point. For now, regarding my personal uncertainty, I'll quote what <a href=\"https://forum.effectivealtruism.org/posts/9Y6Y6qoAigRC7A8eX/my-take-on-what-we-owe-the-future#Aggregating_inputs\">I wrote about a week ago</a>:</p><blockquote><p>[Regarding chance of misaligned takeover by 2100] I\u2019m going to stick to 35% for now, but it\u2019s a very tough question and I could see ending up at anywhere between 10-90% on further reflection and discussion.</p><p>[Regarding chance of TAI by 2100] Mashing all these intuitions together gives me a best guess of 80%, though I think I could end up at anywhere between 60% and 90% on further reflection and discussion.</p></blockquote>", "parentCommentId": "Phz3GoHsJ2rDjuS5W", "user": {"username": "elifland"}}, {"_id": "p9EJ3gkN3BBtpXxTM", "postedAt": "2022-09-09T13:41:58.789Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<blockquote><p>I think it's fair to interpret the Covid question to some extent as superforecasters not trying, but I'm confused about how you seem to be attributing little of it to prediction error? It could be a combination of both.</p></blockquote><p>Good point. I over-updated on my feeling of \"this particular question felt so easy at the time\" so that I couldn't imagine why anyone who puts serious time into it would get it badly wrong.<br><br>However, on reflection, I think it's most plausible that different types of information were salient to different people, which could have caused superforecasters to make prediction errors even if they were trying seriously. (Specifically, the question felt easy to me because I happened to have a lot of detailed info on the UK situation, which presented one of the best available examples to use for forming a reference class.)&nbsp;<br><br>You're right that I essentially gave even more evidence for the claim you were making.&nbsp;</p>", "parentCommentId": "tcff2HYZMjhbudLvZ", "user": {"username": "Lukas_Gloor"}}, {"_id": "RBLQyfyjYp8BCbdoD", "postedAt": "2022-09-09T16:05:23.331Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>This 25% forecast is an order of magnitude different from the Metaculus estimate of 2-2.5%<br><br>I don\u2019t get why two different groups of forecasters aggregated results end up with an over an order of magnitude difference.<br><br>Any idea why? Have I misunderstood something? Is one group know to be better? Or is one group more likely to be bias? Or is forecasting risks just really super unreliable and not a thing to put much weight on?<br>&nbsp;</p><p><a href=\"https://www.metaculus.com/questions/2568/ragnar%25C3%25B6k-seriesresults-so-far/\">https://www.metaculus.com/questions/2568/ragnar%25C3%25B6k-seriesresults-so-far/</a><br>&nbsp;</p>", "parentCommentId": null, "user": {"username": "weeatquince"}}, {"_id": "cs4xBdGuuXhtZFqrB", "postedAt": "2022-09-09T16:18:06.948Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>My take is that we should give little weight to Metaculus. From footnote 2 of the post:</p><blockquote><p>Why do I give little weight to Metaculus\u2019s views on AI? Primarily because of the <a href=\"https://forum.effectivealtruism.org/posts/S2vfrZsFHn7Wy4ocm/bottlenecks-to-more-impactful-crowd-forecasting-2%23Failure_modes1&amp;sa=D&amp;source=editors&amp;ust=1662684319718320&amp;usg=AOvVaw1fAtfmtM1lGgAFVVVgT0j2\"><u>incentives</u></a>&nbsp;to make&nbsp;very shallow forecasts on a ton of questions (e.g. probably &lt;20% of Metaculus AI forecasters have done the equivalent work of reading the Carlsmith report), and secondarily that forecasts aren\u2019t aggregated from a select group of high performers but instead from anyone who wants to make an account and predict on that question.</p></blockquote><p>(Edited to add: I see the post you linked also includes the \"Metaculus prediction\" which theoretically performs significantly better than the community prediction by weighting stronger predictors more heavily. But if you look at its actual <a href=\"https://www.metaculus.com/questions/track-record/\">track record</a>, it doesn't do much better than the community. &nbsp;For binary questions at resolve time, it has a log score of 0.438 vs. 0.426 for community. At all times, it gets 0.280 vs. 0.261. &nbsp;For continuous questions at resolve time, it has a log score of 2.19 vs. 2.12. At all times, it gets 1.57 vs. 1.55.)</p><p>&nbsp;That said:</p><blockquote><p>Or is forecasting risks just really super unreliable and not a thing to put much weight on?</p></blockquote><p>I wouldn't want people to overestimate the precision of the estimates in this post! Take them as a few data points among many. I also think it's very healthy for the community if many people are forming inside views about AI risk, though I understand it's difficult and had a hard time with it myself for a while.</p>", "parentCommentId": "RBLQyfyjYp8BCbdoD", "user": {"username": "elifland"}}, {"_id": "nkB3DWHiSQeyQpQxe", "postedAt": "2022-09-09T17:24:16.679Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>In terms of forecasting accuracy on Metaculus, Eli's individual performance is comparable<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjnszdzgo29j\"><sup><a href=\"#fnjnszdzgo29j\">[1]</a></sup></span>&nbsp;to the community aggregate on his own, despite him having optimised for volume (he's 10th on the heavily volume weighted leaderboard). I expect that were he to have pushed less hard for volume, he'd have significantly outperformed the community aggregate even as an individual.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefszdggetmxhl\"><sup><a href=\"#fnszdggetmxhl\">[2]</a></sup></span><br><br>Assuming the other Samotsvety forecasters are comparably good, I'd expect the aggregated forecasts from the group to very comfortably outperform the community aggregate, even if they weren't paying unusual attention to the questions (which they are).</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjnszdzgo29j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjnszdzgo29j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Comparing 'score at resolution time', Eli looks slightly worse than the community. Comparing 'score across all times', Eli looks better than the community. Score across all times is a better measure of skill when comparing individuals, but does disadvantage the community prediction, because at earlier times questions have fewer predictors.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnszdggetmxhl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefszdggetmxhl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>As some independent evidence of this, I comfortably outperform the community aggregate, having tried less hard than Eli to optimise for volume. Eli has beaten me in more than one competition, and think he's a better forecaster.</p></div></li></ol>", "parentCommentId": "RBLQyfyjYp8BCbdoD", "user": {"username": "alexrjl"}}, {"_id": "eHieC3pMyFRksZvNj", "postedAt": "2022-09-09T19:11:48.398Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>Ah the answer was in the footnotes all along. Silly me. Thank you for the reply!</p>\n", "parentCommentId": "cs4xBdGuuXhtZFqrB", "user": {"username": "weeatquince"}}, {"_id": "pMA9KZCtKYmsuy7pv", "postedAt": "2022-09-09T21:01:36.479Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>This is great; thanks for sharing!</p><p>The ranges on these questions seem pretty wide. Do you have thoughts on that? For hard questions (e.g., about emerging technology several decades in the future, like these questions), do superforecasters/Samotsvety often have such a wide range?</p>", "parentCommentId": null, "user": {"username": "zsp"}}, {"_id": "a6vtvdLz7f26dDW9h", "postedAt": "2022-09-09T22:50:05.855Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>My impression is wide ranges are pretty common on questions as difficult/complex as these. I think large differences can often come from very-hard-to-resolve deep disagreements in intuitions, as we've seen with the MIRI conversations.</p><p>If the range was too small on this type of question I might be worried about herding/anchoring. In a few cases there are 1-2 outlier forecasts on each end and the rest are relatively close together.</p>", "parentCommentId": "pMA9KZCtKYmsuy7pv", "user": {"username": "elifland"}}, {"_id": "BihJsPec2aYvemxJH", "postedAt": "2022-09-10T07:58:47.398Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>To add to Eli's comment, I think on such complex topics, it's just common for even <em>personal</em> estimates to fluctuate quite a bit. For example, here is an excerpt from footnote 181 of <a href=\"https://arxiv.org/pdf/2206.13353.pdf#page=51\">Carlsmith report</a>:</p>\n<blockquote>\n<p>[...] And my central estimate varies between ~1-10% depending on my mood, what considerations are salient to me at the time, and so forth. This instability is yet another reason not to put too much weight on these numbers.</p>\n</blockquote>\n", "parentCommentId": "pMA9KZCtKYmsuy7pv", "user": {"username": "Misha_Yagudin"}}, {"_id": "z75fjTcHxuuPZW7BQ", "postedAt": "2022-09-10T11:01:23.082Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>I appreciate the link. I didn't make good use of it, unfortunately - instead of reading it carefully I searched the page for the acronym hoping to find an expansion, and didn't end up reading the list of properties.</p>", "parentCommentId": "s9PoLdazxrQroEorT", "user": {"username": "KevinO"}}, {"_id": "F47ksrFtosxeu2Cxh", "postedAt": "2022-09-10T11:01:44.167Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>Note that we'd probably also look at the object level reasons for why they think that. E.g., new scaling laws findings could definitely shift our/my forecast by 10%.</p>", "parentCommentId": "GGcF34hxsdhkS3PRj", "user": {"username": "NunoSempere"}}, {"_id": "EMq3JppM7ew2nR6gi", "postedAt": "2022-09-10T11:03:23.758Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>In addition to the points above, there have been a few jokes on questions like that about the scoring rule not being proper (if the world ends, you don't get the negative points for being wrong!). Not sure how much of a factor that is, though, and I could imagine it being minimal.</p>", "parentCommentId": "RBLQyfyjYp8BCbdoD", "user": {"username": "NunoSempere"}}, {"_id": "QKrRrofqCx5tFqFCu", "postedAt": "2022-09-14T12:13:07.544Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>As a superforecaster, I'm going to strongly agree with \"sometimes superforecasters don't seem to be trying as much as they could,\" and they aren't incentivized to do deep dives into every question.&nbsp;<br><br>I'd say they are individually somewhere between metaculus and a more ideal group, which Samovetseky seems to be close to, but I'm not an insider, and have limited knowledge of how you manage epistemic issues like independent elicitation before discussion. One thing Samovestsky does not have, unfortunately, is the type of more sophisticated algorithm to aggregates that are used by Metaculus and GJ, nor the same level of diversity as either - though overall I see those as less important than more effort by properly calibrated forecasters.&nbsp;</p>", "parentCommentId": "LfEgdnhi62BEe3ayo", "user": {"username": "Davidmanheim"}}, {"_id": "yhKaBv9ANrhz3qspP", "postedAt": "2022-09-14T20:37:01.017Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>Do the ranges 3-91.5% and 45-99.5% include or exclude the highest and lowest forecasts?</p>", "parentCommentId": null, "user": {"username": "WilliamKiely"}}, {"_id": "JgCqmzhDDpSumQkpf", "postedAt": "2022-09-14T21:03:18.028Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>Includes the highest and lowest</p>\n", "parentCommentId": "yhKaBv9ANrhz3qspP", "user": {"username": "elifland"}}, {"_id": "na2Hr26NR3WQepZYR", "postedAt": "2022-09-15T16:33:51.565Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>I think it would be very valuable if more reports of this kind were citable in contexts where people are sensitive to signs of credibility and prestige. In other words, I think there are contexts where <i>if</i> this existed as a report on SSRN or even ArXiV, or on the website of an established institution, I think it could be citable and would be valuable as such. Currently I don't think it could be cited (or taken seriously if cited). So if there are low-cost ways of publishing this or similar reports in a more polished way, I think that would be great.</p><p>Caveats that (i) maybe you have done this and I missed it; (ii) this comment isn't really specific to this post but it's been on my mind and this is the most recent post where it is applicable; and (iii) on balance it does nonetheles seem likely that the work required to turn this into a 'polished' report means doing so is not (close to) worthwhile.</p><p>That said: this is an excellent post and I'm very grateful for these forecasts.</p>", "parentCommentId": null, "user": {"username": "finm"}}, {"_id": "h98cwC72jcC7z9z3k", "postedAt": "2022-09-20T18:46:52.035Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>Thanks for the suggestion and glad you found the forecasts helpful :)</p><p>I personally have a distaste for academic credentialist culture so am probably not the best person to turn this into a more prestigious looking report. I agree it might be valuable despite my distaste, so if anyone reading this is interested in doing so feel free to DM me and I can probably help with funding and review if you have a good writing track record.</p>", "parentCommentId": "na2Hr26NR3WQepZYR", "user": {"username": "elifland"}}, {"_id": "MLwuEugFmW53D29DS", "postedAt": "2022-09-22T19:52:56.119Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>Thanks for writing this!</p><p>You may want to consider creating a topic for \"Samotsvety\", where posts such as this could be tagged.</p>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "HnJxwtyf9NAM6GDjb", "postedAt": "2022-09-22T21:16:08.237Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>Good idea, I made a very quick version. Anyone should feel free to update it.</p>", "parentCommentId": "MLwuEugFmW53D29DS", "user": {"username": "elifland"}}, {"_id": "Gw2XYgsdcL8MgF4xd", "postedAt": "2022-09-26T05:25:23.837Z", "postId": "EG9xDM8YRz4JN4wMN", "htmlBody": "<p>It could be cool to see the individual forecasts presented in a histogram.</p>\n", "parentCommentId": "a6vtvdLz7f26dDW9h", "user": {"username": "Peter_Hurford"}}]