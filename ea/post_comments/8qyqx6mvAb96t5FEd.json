[{"_id": "T4XH6jwnofKFQvYo3", "postedAt": "2017-07-16T16:29:16.523Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>EAs seems pretty open to the idea of being big-tent with respect to key normative differences (animals, future people, etc). But total indifference to cause area seems too lax. What if I just want to improve my local neighborhood or family? Or my country club? At some point, it becomes silly. </p>\n<p>It might be worth considering parallels with the Catholic Church and the Jesuits. The broader church is &quot;high level&quot;, but the requirements for membership are far from trivial.</p>\n", "parentCommentId": null, "user": {"username": "Jess_Riedel"}}, {"_id": "tPciCwrsNqKdsDvay", "postedAt": "2017-07-16T16:43:23.695Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>&quot;Total indifference to cause area&quot; isn't quite how I'd describe my proposal - after all, we <em>would</em> still be talking about high-level EA, a lot of people would still be focused on high-level EA and doing that, etc. The general recommendation would still be to go into high-impact causes if you had no strong preference.</p>\n", "parentCommentId": "T4XH6jwnofKFQvYo3", "user": {"username": "Kaj_Sotala"}}, {"_id": "5GqEaFKnwSxaT9hf5", "postedAt": "2017-07-16T17:38:41.592Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>Really appreciate you taking the time to write this up! My initial reaction is that the central point about mindset-shifting seems really right.</p>\n<blockquote>\n<p>My proposal is to explicitly talk about two kinds of EA (these may need catchier names)</p>\n</blockquote>\n<p>It seems (to me) \u201clow-level\u201d and \u201chigh-level\u201d <em>could</em> read as value-laden in a way that might make people practicing \u201clow-level\u201d EA (especially in cause areas not already embraced by lots of other EAs) feel like they\u2019re not viewed as \u201creal\u201d EAs and so work at cross-purposes with the tent-broadening goal of the proposal. Quick brainstorm of terms that make some kind of descriptive distinction instead:</p>\n<ol>\n<li>cause-blind EA vs. cause-specific or cause-limited EA</li>\n<li>broad EA vs. narrow EA</li>\n<li>inter-cause vs. intra-cause</li>\n</ol>\n<p>(Thoughts/views only my own, not my employer\u2019s.)</p>\n", "parentCommentId": null, "user": {"username": "Taylor"}}, {"_id": "a4M4f2PC69rycWLYZ", "postedAt": "2017-07-16T20:39:34.386Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>Hmm, maybe</p>\n<ul>\n<li>Global EA vs local EA </li>\n<li>Total EA vs focused  EA</li>\n</ul>\n", "parentCommentId": "5GqEaFKnwSxaT9hf5", "user": {"username": "WillPearson"}}, {"_id": "u7T7qJghgELKuv9uf", "postedAt": "2017-07-17T00:51:38.088Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>Ian David Moss has a <a href=\"http://effective-altruism.com/ea/120/all_causes_are_ea_causes/\">post</a> on this forum arguing for things along the lines of 'EA for the rich country fine arts' and other such restricted scope versions of EA.</p>\n<p>My biggest objection to this is that to stay in line with people's habitual activities the rationales for the restricted scope have to be very gerrymandered (perhaps too much to be credible if stated explicitly), and optimizing within that restricted objective function may be pick out things that are overall bad, e.g. the recent media discussion comparing interventions purely in terms of their carbon emissions without taking anything else into account, suggesting that the existence of a member of a society with GDP per capita of $56,000 is bad if it includes carbon emissions with a social cost of $2,000 per person.</p>\n", "parentCommentId": null, "user": {"username": "CarlShulman"}}, {"_id": "WJRL8pmxfg75oxKDa", "postedAt": "2017-07-17T03:18:35.284Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>A few thoughts:</p>\n<ul>\n<li>If you believe that existential risk is literally the most important issue in the world and that we will be facing possible extinction events imminently, then it follows that we can't wait to develop a mass movement and that we <em>need</em> to find a way to make the small, exceptional group strategy work (although, we may also spread low-level EA, but not as our focus)</li>\n<li>I suspect that most EAs would agree that spreading low-level EA is worthwhile. The first question is whether this should be the focus/a major focus (as noted above). The second question is whether this should occur within EA or be a spin-off/a set of spin-offs. For example, I would really like to see an Effective Environmentalism movement.</li>\n<li>Some people take issue with the name Effective Altruism because it implies that everything else is Ineffective Altruism. Your suggestion might mitigate this to a certain extent, but we really need better names!</li>\n</ul>\n", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "N8KffJJ3RzMYo6KhJ", "postedAt": "2017-07-17T04:11:39.090Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>Views my own, not my employers.</p>\n<p>Thanks for writing this up! I agree that it could be a big win if general EA ideas besides cause prioritization (or the idea of scope-limited cause prioritization) spread to the point of being as widely accepted as environmentalism. Some alternatives to this proposal though:</p>\n<ol>\n<li>It might be better to spread rationality and numeracy concepts like expected value, opportunity costs, comparative advantage, cognitive biases, etc completely unconnected to altruism than to try to explicitly spread narrow or cause-specific EA. People on average care much more about being productive, making money, having good relationships, finding meaning, etc than about their preferred altruistic causes. And it really would be a big win if they succeeded -- less ambiguously so than with narrow EA I think (see Carl's comment below). The biggest objection to this is probably crowdedness/lack of obvious low-hanging fruit.</li>\n<li>Another alternative might be to focus on spreading the prerequisites/correlates of cause-neutral, intense EA: e.g. math education, high levels of caring/empathy, cosmopolitanism, motivation to think systematically about ethics, etc. I'm unsure how difficult this would be.</li>\n</ol>\n<p>Both of these alternatives seem to have what is (to me) an advantage: they don't involve the brand and terminology of EA. I think it would be easier to push on the frontiers of cause-neutral/broad EA if the label were a good signal of a large set of pretty unusual beliefs and attitudes, so that people can have high trust collaboration relatively quickly.</p>\n<p>FWIW, I think I would be much more excited to evangelize broad low-level EA memes if there were some strong alternative channel to distinguish cause-neutral, super intense/obsessive EAs. Science has a very explicit distinction between science fans and scientists, and a very explicit funnel from one to the other (several years of formal education). EA doesn't have that yet, and may never. My instinct is that we should work on building a really really great &quot;product&quot;, then build high and publicly-recognized walls around &quot;practitioners&quot; and &quot;consumers&quot; (a practical division of labor rather than a moral high ground thing), and <em>then</em> market the product hard to consumers.</p>\n", "parentCommentId": null, "user": {"username": "Ajeya"}}, {"_id": "b3TZTaBuNbCKZ7ocY", "postedAt": "2017-07-17T04:23:26.146Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>Does anyone know which version of your analogy early science actually looked like?  I don't know very much about the history of science, but it seems worth noting that science is strongly associated with academia, which is famous for being exclusionary &amp; elitist.  (&quot;The scientific community&quot; is almost synonymous with &quot;the academic science community&quot;.)</p>\n<p>Did science ever call itself a &quot;movement&quot; the way EA calls itself a movement?  My impression is that the the skeptic movement (the thing that spreads scientific ideas and attitudes through society at large) came well after science proved its worth.  If broad scientific attitudes were a prerequisite for science, that predicts that the popular atheism movement should have come several centuries sooner than it did.</p>\n<p>If one's goal is to promote scientific progress, it seems like you're better off focusing on a few top people who make important discoveries.  There's plausibly something similar going on with EA.</p>\n<p>I'm somewhat confused that you list the formation of many groups as a benefit of broad mindset spread, but then say that we should try to achieve the formation of one very large group (that of &quot;low-level EA&quot;).  If our goal is many groups, maybe it would be better to just create many groups?  If our goal is to spread particular memes, why not the naive approach of trying to achieve positions of influence in order to spread those particular memes?</p>\n<p>The current situation WRT growth of the EA movement seems like it could be the worst of both worlds.  The EA movement does marketing, but we also have discussions internally about how exclusive to be.  So people hear about EA because of the marketing, but they also hear that some people in the EA movement think that maybe the EA movement should be too exclusive to let them in.  We'd plausibly be better off if we adopted a compromise position of doing less marketing and also having fewer discussions about how exclusive to be.</p>\n<p>Growth is a <a href=\"http://effective-altruism.com/ea/18i/hardtoreverse_decisions_destroy_option_value/\">hard to reverse decision</a>.  Companies like Google are very selective about who they hire because firing people is bad for morale.  The analogy here is that instead of &quot;firing&quot; people from EA, we're better off if we don't do outreach to those people in the first place.</p>\n<p>[Highly speculative]: One nice thing about companies and universities is that they have a clear, well-understood inclusion/exclusion mechanism.  In the absence of such a mechanism, you can get concentric circles of inclusion/exclusion and associated internal politics.  People don't resent Harvard for rejecting them, at least not for more than a month or two.  But getting a <a href=\"https://www.facebook.com/groups/473795076132698/permalink/628251507353720/\">subtle cold shoulder</a> from people in the EA community will produce a lasting negative impression.  Covert exclusiveness feels worse than overt exclusiveness, and having an official party line that &quot;the EA movement must be welcoming to everyone&quot; will just cause people to be exclusive in a more covert way.</p>\n", "parentCommentId": null, "user": {"username": "John_Maxwell_IV"}}, {"_id": "e9EryrX38PHwEhpWc", "postedAt": "2017-07-17T04:47:34.790Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>Hey Kaj,</p>\n<p>I agree with a lot of these points. I just want to throw some counter-points out there for consideration. I'm not necessarily endorsing them, and don't intend them as a direct response, but thought they might be interesting. It's all very rough and quickly written.</p>\n<p>1) Having a high/low distinction is part of what has led people to claim EAs are misleading. One version of it involves getting people interested through global poverty (or whatever causes they're already interested in), and then later trying to upsell them into high-level EA, which presumably has a major focus on GCRs, meta and so on.</p>\n<p>It becomes particularly difficult because the leaders, who do the broad outreach, want to focus on high-level EA. It's more transparent and open to pitch high-level EA directly.</p>\n<p>There are probably ways you could implement a division without incurring these problems, but it would need some careful thought.</p>\n<p>2) It sometimes seems like the most innovative and valuable idea within EA is cause-selection. It's what makes us different from simply &quot;competent&quot; do-gooding, and often seems to be where the biggest gains in impact lie. Low level EA seems to basically be EA minus cause selection, so by promoting it, you might lose most of the value. You might need a very big increase in scale of influence to offset this.</p>\n<p>3) Often the best way to promote general ideas is to live them. With your example of promoting science, people often seem to think the Royal Society was important in building the scientific culture in the UK. It was an elite group of scientists who just got about the business of doing science. Early members included Newton and Boyle. The society brought likeminded people together, and helped them to be more successful, ultimately spreading the scientific mindset.</p>\n<p>Another example is Y Combinator, which has helped to spread norms about how to run startups, encourage younger people to do them, reduce the power of VCs, and have other significant effects on the ecosystem. The partners often say they became famous and influential due to reddit -&gt; dropbox -&gt; airbnb, so much of their general impact was due to having a couple of concrete successes. </p>\n<p>Maybe if EA wants to have more general impact on societal norms, the first thing we should focus on doing is just having a huge impact - finding the &quot;airbnb of EA&quot; or the &quot;Newton of EA&quot;. </p>\n", "parentCommentId": null, "user": {"username": "Benjamin_Todd"}}, {"_id": "HssdJwMHjF5kpJevc", "postedAt": "2017-07-17T09:02:55.127Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>Thanks!</p>\n<blockquote>\n<p>1) Having a high/low distinction is part of what has led people to claim EAs are misleading. One version of it involves getting people interested through global poverty (or whatever causes they're already interested in), and then later trying to upsell them into high-level EA, which presumably has a major focus on GCRs, meta and so on.</p>\n</blockquote>\n<p>Yeah, agreed. Though part of what I was trying to say is that, as you mentioned, we have the high/low distinction <em>already</em> - &quot;implementing&quot; that distinction would just be giving an explicit name to something that already exists. And something that has a name is easier to refer to and talk about, so having some set of terms for the two types could make it easier to be more transparent about the existence of the distinction when doing outreach. (This would be the case regardless of whether we want to expand EA to lower-impact causes or not.)</p>\n<blockquote>\n<p>2) It sometimes seems like the most innovative and valuable idea within EA is cause-selection. It's what makes us different from simply &quot;competent&quot; do-gooding, and often seems to be where the biggest gains in impact lie. Low level EA seems to basically be EA minus cause selection, so by promoting it, you might lose most of the value. You might need a very big increase in scale of influence to offset this.</p>\n</blockquote>\n<p>I guess the question here is, how much would efforts to bring in low-level EAs hurt the efforts to bring in high-level EAs. My intuition would be that the net effect would be to bring in more high-level EAs overall (a smaller percentage of incoming people would become high-level EAs, but that would be offset by there being more incoming people overall), but I don't have any firm support for that intuition and one would have to test it somehow.</p>\n<blockquote>\n<p>3) Often the best way to promote general ideas is to live them. ... Maybe if EA wants to have more general impact on societal norms, the first thing we should focus on doing is just having a huge impact - finding the &quot;airbnb of EA&quot; or the &quot;Newton of EA&quot;.</p>\n</blockquote>\n<p>I agree that the best way to promote general ideas can be to live them. But I think we need to be more specific about what a &quot;huge impact&quot; would mean in this context. E.g. <a href=\"https://80000hours.org/2011/11/high-impact-science/\">High Impact Science</a> suggests that Norman Borlaug is one of the people who have had the biggest positive impact on the world - but most people have probably never heard of him. So for spreading social norms, it's not enough to live the ideas and make a big impact, one has to do it in a sufficiently visible way.</p>\n", "parentCommentId": "e9EryrX38PHwEhpWc", "user": {"username": "Kaj_Sotala"}}, {"_id": "Px6hYJ2GXJib8NQfZ", "postedAt": "2017-07-17T09:43:15.387Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>Thanks for the comment!</p>\n<blockquote>\n<ol>\n<li>It might be better to spread rationality and numeracy concepts like expected value, opportunity costs, comparative advantage, cognitive biases, etc completely unconnected to altruism than to try to explicitly spread narrow or cause-specific EA. People on average care much more about being productive, making money, having good relationships, finding meaning, etc than about their preferred altruistic causes. And it really would be a big win if they succeeded -- less ambiguously so than with narrow EA I think (see Carl's comment below). The biggest objection to this is probably crowdedness/lack of obvious low-hanging fruit.</li>\n</ol>\n</blockquote>\n<p>I agree with the &quot;lack of obvious low-hanging fruit&quot;. It doesn't actually seem obvious to me how useful these concepts <em>are</em> to people in general, as opposed to more specific concrete advice (such as specific exercises for improving their social skills etc.). In particular, Less Wrong has been devoted to roughly this kind of thing, and even among LW regulars who may have spent hundreds of hours participating on the site, it's <a href=\"http://lesswrong.com/lw/9p/extreme_rationality_its_not_that_great/\">always been controversial</a> whether the concepts they've learned from the site have translated into <em>any</em> major life gains. My current inclination would be that &quot;general thinking skills&quot; just <em>aren't</em> very useful for dealing with your practical life, and that concrete domain-specific ideas are much more useful.</p>\n<p>You said that people in general care much more about concrete things in their own lives than their preferred altruistic causes, and I agree with this. But on the other hand, the kinds of people who are already committed to working on some altruistic cause are probably a different case: if you're already devoted to some specific goal, then you might have more of an interest in applying those things. If you first targeted people working in existing organizations and won them over to using these ideas, then they might start teaching the ideas to all of their future hires, and over time the concepts could start to spread to the general population more.</p>\n<blockquote>\n<ol>\n<li>Another alternative might be to focus on spreading the prerequisites/correlates of cause-neutral, intense EA: e.g. math education, high levels of caring/empathy, cosmopolitanism, motivation to think systematically about ethics, etc. I'm unsure how difficult this would be.</li>\n</ol>\n</blockquote>\n<p>Maybe. One problem here is that some of these correlate only very loosely with EA: a <em>lot</em> of people have completed math education who aren't EAs. And I think that another problem is that in order to really internalize an idea, you need to actively use it. My thinking here is similar to <a href=\"https://www.ribbonfarm.com/2014/02/20/the-cactus-and-the-weasel/\">Venkatesh Rao's, who wrote</a>:</p>\n<blockquote>\n<p>Strong views represent a kind of high sunk cost. When you have invested a lot of effort forming habits, and beliefs justifying those habits, shifting a view involves more than just accepting a new set of beliefs. You have to:</p>\n<ol>\n<li>Learn new habits based on the new view</li>\n<li>Learn new patterns of thinking within the new view</li>\n</ol>\n<p>The order is very important. I have never met anybody who has changed their reasoning first and their habits second. You change your habits first. This is a behavioral conditioning problem largely unrelated to the logical structure and content of the behavior. Once you\u2019ve done that, you learn the new conscious analysis and synthesis patterns.</p>\n<p>This is why I would never attempt to debate a literal creationist. If forced to attempt to convert one, I\u2019d try to get them to learn innocuous habits whose effectiveness depends on evolutionary principles (the simplest thing I can think of is A/B testing; once you learn that they work, and then understand how and why they work, you\u2019re on a slippery slope towards understanding things like genetic algorithms, and from there to an appreciation of the power of evolutionary processes).</p>\n</blockquote>\n<p>I wouldn't know how to spread something like cosmopolitanism, to a large extent because I don't know how to teach the kind of thinking habits that would cause you to internalize cosmopolitanism. And even after that, there would still be the step of getting from all of those prerequisites to applying EA principles in concepts. In contrast, teaching EA concepts by getting people to apply them to a charitable field they already care about, gets them into applying EA-ish thinking habits <em>directly</em>. </p>\n<blockquote>\n<p>Both of these alternatives seem to have what is (to me) an advantage: they don't involve the brand and terminology of EA. I think it would be easier to push on the frontiers of cause-neutral/broad EA if the label were a good signal of a large set of pretty unusual beliefs and attitudes, so that people can have high trust collaboration relatively quickly.</p>\n</blockquote>\n<p>That's an interesting view, which I hadn't considered. I might view it more as a disadvantage, in that in the model that I was thinking of, people who got into low-level EA would almost automatically also be exposed to high-level EA, causing the idea of high-level EA to spread further. If you were only teaching related concepts, that jump from them to high-level EA wouldn't happen automatically, but would require some additional steps. (That said, <em>if</em> you could teach enough of those prerequisites, maybe the jump would be relatively automatic. But this seems challenging for the reasons I've mentioned above.)</p>\n", "parentCommentId": "N8KffJJ3RzMYo6KhJ", "user": {"username": "Kaj_Sotala"}}, {"_id": "CTy5fFM8WnDtS9cGP", "postedAt": "2017-07-17T09:48:53.093Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<blockquote>\n<p>Ian David Moss has a post on this forum arguing for things along the lines of 'EA for the rich country fine arts' and other such restricted scope versions of EA.</p>\n</blockquote>\n<p>Thanks for the link! I did a quick search to find if someone had already said something similar, but missed that.</p>\n<blockquote>\n<p>My biggest objection to this is that to stay in line with people's habitual activities the rationales for the restricted scope have to be very gerrymandered (perhaps too much to be credible if stated explicitly), and optimizing within that restricted objective function may be pick out things that are overall bad,</p>\n</blockquote>\n<p>I'm not sure whether the first one is really an issue - just saying that &quot;these are general tools that you can use to improve whatever it is that you care about, and if you're not sure what you care about, you can also apply the same concepts to find that&quot; seems reasonable enough to me, and not particularly gerrymandering.</p>\n<p>I do agree that optimizing too specifically within some narrow domain can be a problem that produces results that are globally undesirable, though. </p>\n", "parentCommentId": "u7T7qJghgELKuv9uf", "user": {"username": "Kaj_Sotala"}}, {"_id": "dvLtxnPEzmLvBsBb5", "postedAt": "2017-07-17T09:49:46.564Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>&quot;General vs. specific&quot; could also be one</p>\n", "parentCommentId": "5GqEaFKnwSxaT9hf5", "user": {"username": "Kaj_Sotala"}}, {"_id": "yk4i96nq6Ei7SXgtJ", "postedAt": "2017-07-17T10:00:47.167Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<blockquote>\n<p>I'm somewhat confused that you list the formation of many groups as a benefit of broad mindset spread, but then say that we should try to achieve the formation of one very large group (that of &quot;low-level EA&quot;). If our goal is many groups, maybe it would be better to just create many groups?</p>\n</blockquote>\n<p>I must have expressed myself badly somehow - I specifically meant that &quot;low-level EA&quot; would be composed of multiple groups. What gave you the opposite impression?</p>\n<p>For example, the current situation is that organizations like the Centre for Effective Altruism and Open Philanthropy Project are high-level organizations: they are devoted to finding the best ways of doing good in general. At the same time, organizations like Centre for the Study of Existential Risk, Animal Charity Evaluators, and Center for Applied Rationality are low-level organizations, as they are each devoted to some specific cause area (x-risk, animal welfare, and rationality, respectively). We already have several high- and low-level EA groups, and spreading the ideas would ideally cause even more of both to be formed.</p>\n<blockquote>\n<p>If our goal is to spread particular memes, why not the naive approach of trying to achieve positions of influence in order to spread those particular memes?</p>\n</blockquote>\n<p>This seems completely compatible with what I said? On my own behalf, I'm definitely interested in trying to achieve a position of higher influence to better spread these ideas.</p>\n", "parentCommentId": "b3TZTaBuNbCKZ7ocY", "user": {"username": "Kaj_Sotala"}}, {"_id": "9baaSAafHy9Tp3JMT", "postedAt": "2017-07-17T10:02:17.586Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>I agree that if one thinks that x-risk is an immediate concern, then one should focus specifically on that now. This is explicitly a long-term strategy, so assumes that there will <em>be</em> a long term.</p>\n", "parentCommentId": "WJRL8pmxfg75oxKDa", "user": {"username": "Kaj_Sotala"}}, {"_id": "vXpeGdMn6A9kdrxEo", "postedAt": "2017-07-17T11:36:24.343Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>I think first we would need to ascertain whether low level (maybe foundational) EA were taking place, otherwise we could risk creating a divide within the movement around consistency.  So we would need to see the evidence for where process has been applied. Perhaps there could be a scheme that could grade how much EA process has been applied, and direct us to where we could locate that information.  Maybe it could also be undertaken by an external group that is neutral to EA.</p>\n<p>I think we ought to be fairly uncertain around how much process is presently applied by EA backed organisations (particularly in EAA, i don't know so much about other areas), and be cautious about getting too far ahead when groups may have further to go in order to meet what may reasonably be considered a foundational level.</p>\n", "parentCommentId": null, "user": {"username": "KevinWatkinson"}}, {"_id": "WDZDqQT5TE3sNfscQ", "postedAt": "2017-07-18T02:25:59.032Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>One thing to keep in mind is that people often (or usually, even) choose the middle ground by themselves. Matt Ball often mentions how this happens in animal rights with people deciding to reduce meat after learning about the merits vegetarianism and mentions that Nobel laureate Herb Simon is known for this realization of people opting for sub-optimal decisions. </p>\n<p>Thus, I think that in promoting pure EA, most people will practice weak EA (ie. not cause neutral) on their own accord, so perhaps the best way to proliferate weak EA is by promoting strong EA.</p>\n", "parentCommentId": null, "user": {"username": "Austen_Forrester"}}, {"_id": "n3QNtGT84v9iA8dAu", "postedAt": "2017-07-18T08:23:48.405Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>This can be an issue, but i think Matt Ball has chosen not to present a strong position because he believes that is offputting, instead he undermines the strong position and presents a sub optimal one.  However, he says this is in fact optimal as it reduces more harm.</p>\n<p>If applied to EA we would undermine a position we believe might put people off, because it is too complicated / esoteric, and present a first step that will do more good.</p>\n", "parentCommentId": "WDZDqQT5TE3sNfscQ", "user": {"username": "KevinWatkinson"}}, {"_id": "shRYGgbRYjHC8kKpx", "postedAt": "2017-07-18T19:04:23.907Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>My point was that EAs probably <em>should</em> exclusively promote full-blown EA, because that has a good chance of leading to more uptake of both full-blown <em>and</em> weak EA. Ball's issue with the effect of people choosing to go part-way after hearing the veg message is that it often leads to more animals being killed due to people replacing beef and pork with chicken. That's a major impetus for his direct \u201ccut out chicken before pork and beef\u201d message. It doesn't undermine veganism because chicken-reducers are more likely to continue on towards that lifestyle, probably more so even than someone who went vegetarian right away Vegetarians have a very high drop out rate, but many believe that those who transitioned gradually last longer.</p>\n<p>I think that promoting effectively giving 10% of one's time and/or income (for the gainfully employed) is a good balance between promoting a high impact lifestyle and being rejected due to high demandingness. I don't think it would be productive to lower the bar on that (ie. By saying cause neutrality is optional).</p>\n", "parentCommentId": "n3QNtGT84v9iA8dAu", "user": {"username": "Austen_Forrester"}}, {"_id": "QdETtod6psh3KBQJ7", "postedAt": "2017-07-20T06:28:08.102Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>On the face of it, the idea does sound quite good.  However, we need to place it into a broader movement context and look at how it has been evaluated to consider how effective it is likely to be, and what other impacts the approach has that aren\u2019t immediately clear.</p>\n<p>A central issue with EA is that it says for instance, that we need to consider scope, neglectedness and tractability, but meeting this criteria doesn\u2019t then lead to effectiveness, or optimal outcomes, it just flags that it is an approach worth more consideration. </p>\n<p>Consequently, we can note the \u2018pragmatic\u2019 trend in EA support for animal related groups, but this trend isn\u2019t well understood, and neither is it contextualised.  Where we are trying to be inclusive and encourage more people into EA then this is the type of thing we need to consider, so we need to consider things like ideology and organisational / movement culture when determining how groups inter-relate and what impact this has.  I think for many people who are looking at different aspects of EA, they don\u2019t have the time to do this, and expect EAAs to do this work, but there isn\u2019t any evidence this form of evaluation has been taking place up to now.  My own observation of the movement is that this is a neglected area, and will likely be quite important in terms of inclusion.  </p>\n<p>In terms of EA, the trade off would be making EA look more appealing by diminishing it in terms of elitism, specifically where a certain \u2018lower\u2019 section of EAs were to say they aren\u2019t like the \u2018higher\u2019 ones.  The corollary in the animal movement is to claim veganism is extreme, all or nothing, fundamentalist, angry, crazy, puritan, dogmatic, absolutist, hardline and so on.  These are stereotypes that Matt Ball, Tobias Leenaert and Brian Kateman have played on in order to centre their pragmatic (or not vegan) approach.  I think people who have paid attention to what they say are likely to recognise this (see in particular Matt Ball\u2019s recent Vox video), it is just that rights activists are more sensitive to it because it infringes on our work.</p>\n<p>I think it is possible to claim the work of the mainstream groups hasn\u2019t been contextualised, or even criticised from within EA, it has largely been encouraged and supported by EAs and other mainstream animal activists because it either sounds good on the face of it, or it hasn\u2019t caused any issues for the work they are doing, or it is simply expedient to go along with that flow.  We can also look at the divisions created and perpetuated and ask whether we really want to replicate the behaviour of some EAs within the animal movement and transfer that into EA.  I think the answer would be no, however, we would then still need to consider whether we ought to be validating that work in the organisations that EAs support, and I would say no to that as well. </p>\n<p>Links.</p>\n<p>Disrupting the animal movement:\n<a href=\"https://qz.com/829956/how-the-vegan-movement-broke-out-of-its-echo-chamber-and-finally-started-disrupting-things/\">https://qz.com/829956/how-the-vegan-movement-broke-out-of-its-echo-chamber-and-finally-started-disrupting-things/</a></p>\n<p>Focus on Fish: A Call to Effective Altruists:\n<a href=\"http://commons.pacificu.edu/cgi/viewcontent.cgi?article=1567&amp;context=eip\">http://commons.pacificu.edu/cgi/viewcontent.cgi?article=1567&amp;context=eip</a></p>\n<p>Utilitarian equivocation and moral consistency:\n<a href=\"https://network23.org/orcasandanimals/2017/06/21/effective-altruism-for-animals-utilitarian-equivocation-and-moral-consistency/\">https://network23.org/orcasandanimals/2017/06/21/effective-altruism-for-animals-utilitarian-equivocation-and-moral-consistency/</a></p>\n", "parentCommentId": "shRYGgbRYjHC8kKpx", "user": {"username": "KevinWatkinson"}}, {"_id": "ZHLieQzXNfwA2JNym", "postedAt": "2017-09-06T08:10:40.566Z", "postId": "8qyqx6mvAb96t5FEd", "htmlBody": "<p>I want to suggest a more general version of Ajeya's views which is:</p>\n<p>If someone did want to put time and effort into creating the resources to promote something akin to &quot;broad effective altruism&quot; they could focus their effort in two ways:</p>\n<ol>\n<li><p>on research and advocacy that does not add to (and possibly detracts attention from) the &quot;narrow effective altruism&quot; movement.</p>\n</li>\n<li><p>on research and advocacy that benefits the effective altruism movement.</p>\n</li>\n</ol>\n<p>EXAMPLES</p>\n<ol>\n<li><p>Eg. Researching what is the best arts charity in the UK.\nNot useful as it is very unlikely that anyone who does take a cause neutral approach to charity would want to give to a UK arts charity.\nThere is a risk of misleading, for example if you google effective altruism and a bunch of materials on UK arts comes up first.</p>\n</li>\n<li><p>Eg. Researching general principles of how to evaluate charities. Researching climate change solutions. Researching systemic change charities.\nThese would all expand the scope of EA research and writings, might produce plausible candidates for the best charity/cause, and at the same time act to attract more people into the movement.\nConsider climate change. It is a problem that at some point this century humanity has to solve (unlike UK arts) and it is also a cause many non-EAs care about strongly</p>\n</li>\n</ol>\n<p>CONCLUSION</p>\n<p>So if there was at least some effort put into any &quot;broad effective altruism&quot; expansion I would strongly recommend starting with finding ways to expand the movement that are simultaneously useful areas for us to be considering in more detail.</p>\n<p>(That said, FWIW I am very wary of attempts to expanding to have a &quot;broad effective altruism&quot; for some of the reasons mentioned by others)</p>\n", "parentCommentId": "N8KffJJ3RzMYo6KhJ", "user": {"username": "weeatquince"}}]