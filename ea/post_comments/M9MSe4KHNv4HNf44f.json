[{"_id": "35RTccd3tWaiYLdzD", "postedAt": "2024-01-06T08:26:04.334Z", "postId": "M9MSe4KHNv4HNf44f", "htmlBody": "<p>I appreciate that a ton of work went into this, and the results are interesting. That said, I am skeptical of the value of surveys with low response rates (in this case, 15%), especially when those surveys are likely subject to non-response bias, as I suspect this one is, given: (1) many AI researchers just don\u2019t seem too concerned about the risks posed by AI, so may not have opened the survey and (2) those researchers would likely have answered the questions on the survey differently. (I do appreciate that the authors took steps to mitigate the risk of non-response bias at the survey level, and did not find evidence of this at the question level.)</p>\n<p>I don\u2019t find the \u201cexpert surveys tend to have low response rates\u201d defense particularly compelling, given: (1) the loaded nature of the content of the survey (meaning bias is especially likely), (2) the fact that such a broad group of people were surveyed that it\u2019s hard to imagine they\u2019re all actually \u201cexperts\u201d (let alone have relevant expertise), (3) the fact that expert surveys often do have higher response rates (26% is a lot higher than 15%), especially when you account for the fact that it\u2019s extremely unlikely other large surveys are compensating participants anywhere close to this well, and (4) the possibility that many expert surveys just aren\u2019t very useful.</p>\n<p>Given the non-response bias issue, I am not inclined to update very much on what AI researchers in general think about AI risk on the basis of this survey. I recognize that the survey may have value independent of its knowledge value\u2014for instance, I can see how other researchers citing these kinds of results (as I have!) may serve a useful rhetorical function, given readers of work that cites this work are unlikely to review the references closely. That said, I don\u2019t think we should make a habit of citing work that has methodological issues simply because such results may be compelling to people who won\u2019t dig into them.</p>\n<p>Given my aforementioned concerns, I wonder whether the cost of this survey can be justified (am I calculating correctly that $138,000 was spent just compensating participants for taking this survey, and that doesn\u2019t include other costs, like those associated with using the outside firm to compensate participants, researchers\u2019 time, etc?). In light of my concerns about cost and non-response bias, I am wondering whether a better approach would instead be to randomly sample a subset of potential respondents (say, 4,000 people), and offer to compensate them at a much higher rate (e.g., $100), given this strategy could both reduce costs and improve response rates.</p>\n", "parentCommentId": null, "user": {"username": "hobbes"}}, {"_id": "yQrq8vRogjuhgyESS", "postedAt": "2024-01-06T17:57:08.832Z", "postId": "M9MSe4KHNv4HNf44f", "htmlBody": "<blockquote><p>I am wondering whether a better approach would instead be to randomly sample a subset of potential respondents (say, 4,000 people), and offer to compensate them at a much higher rate (e.g., $100), given this strategy could both reduce costs and improve response rates.</p></blockquote><p>Note that 4,000 * $100 is $400,000 which is higher than the cost you cited above.<br><br>FWIW, both of these costs seem pretty small to me.</p>", "parentCommentId": "35RTccd3tWaiYLdzD", "user": {"username": "Ryan Greenblatt"}}, {"_id": "ja2SNBJQh9WzbWLjJ", "postedAt": "2024-01-06T18:04:27.496Z", "postId": "M9MSe4KHNv4HNf44f", "htmlBody": "<p>No, because the response rate wouldn't be 100%; even if it doubled to 30% (which I doubt it would), the cost would still be lower ($120k).</p>", "parentCommentId": "yQrq8vRogjuhgyESS", "user": {"username": "hobbes"}}, {"_id": "eSofAEenAPACiJyjS", "postedAt": "2024-01-06T18:48:10.320Z", "postId": "M9MSe4KHNv4HNf44f", "htmlBody": "<blockquote>\n<p>many AI researchers just don\u2019t seem too concerned about the risks posed by AI, so may not have opened the survey</p>\n</blockquote>\n<p>Note that we didn't tell them the topic that specifically.</p>\n<blockquote>\n<p>I am wondering whether a better approach would instead be to randomly sample a subset of potential respondents (say, 4,000 people), and offer to compensate them at a much higher rate (e.g., $100)..</p>\n</blockquote>\n<p>Tried sending them $100 last year and if anything it lowered the response rate.</p>\n<p>If you are inclined to dismiss this based on your premise \"many AI researchers just don\u2019t seem too concerned about the risks posed by AI\", I'm curious where you get that view from, and why you think it is a less biased source.</p>\n", "parentCommentId": "35RTccd3tWaiYLdzD", "user": {"username": "Katja_Grace"}}, {"_id": "csLcRqHPA5XeYGj6c", "postedAt": "2024-01-06T18:48:25.222Z", "postId": "M9MSe4KHNv4HNf44f", "htmlBody": "<p>Quantitatively how large do you think the non-response bias might be? Do you have some experience or evidence in this area that would help estimate the effect size? I don't have much to go on, so I'd definitely welcome pointers.</p><p>Let's consider the 40% of people who put a 10% probability on extinction or similarly bad outcomes (which seems like what you are focusing on). Perhaps you are worried about something like: researchers concerned about risk might be 3x more likely to answer the survey than those who aren't concerned about risk, and so in fact only 20% of people assign a 10% probability, not the 40% suggested by the survey.</p><p>Changing from 40% to 20% would be a significant revision of the results, but honestly that's probably comparable to other sources of error and I'm not sure you should be trying to make that precise an inference.</p><p>But more importantly a 3x selection effect seems implausibly large to me. The survey was presented as being about \"progress in AI\" and there's not an obvious mechanism for huge selection effects on these questions. I haven't seen literature that would help estimate the effect size, but based on a general sense of correlation sizes in other domains I'd be pretty surprised by getting a 3x or even 2x selection effect based on this kind of indirect association. (A 2x effect on response rate based on views about risks seems to imply a very serious <a href=\"http://www.stat.columbia.edu/~gelman/research/unpublished/piranhas.pdf\">piranha problem</a>) &nbsp;</p><p>The largest demographic selection effects were that some groups (e.g. academia vs industry, junior vs senior authors) were about 1.5x more likely to fill out the survey. Those small selection effects seem more like what I'd expect and are around where I'd set the prior (so: 40% being concerned might really be 30% or 50%).</p><blockquote><p>many AI researchers just don\u2019t seem too concerned about the risks posed by AI, so may not have opened the survey ... the loaded nature of the content of the survey (meaning bias is especially likely),</p></blockquote><p>I think the survey was described as about \"progress in AI\" (and mostly concerned progress in AI), and this seems like all people saw when deciding to take it. Once people started taking the survey it looks like there was negligible non-response at the question level. You can see the first page of the survey <a href=\"https://wiki.aiimpacts.org/_media/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2023_espai_paid.pdf\">here</a>, which I assume is representative of what people saw when deciding to take the survey.</p><p>I'm not sure if this was just a misunderstanding of the way the survey was framed. Or perhaps you think people have seen reporting on the survey in previous years and are aware that the question on risks attracted a lot of public attention, and therefore are much more likely to fill out the survey if they think risk is large? (But I think the mechanism and sign here are kind of unclear.)</p><blockquote><p>specially when you account for the fact that it\u2019s extremely unlikely other large surveys are compensating participants anywhere close to this well</p></blockquote><p>If compensation is a significant part of why participants take the survey, then I think it lowers the scope for selection bias based on views (though increases the chances that e.g. academics or junior employees are more likely to respond).</p><blockquote><p>I can see how other researchers citing these kinds of results (as I have!) may serve a useful rhetorical function, given readers of work that cites this work are unlikely to review the references closely</p></blockquote><p>I think it's dishonest to cite work that you think doesn't provide evidence. That's even more true if you think readers won't review the citations for themselves. In my view the 15% response rate doesn't undermine the bottom line conclusions very seriously, but if your views about non-response mean the survey isn't evidence then I think you definitely shouldn't cite it.</p><blockquote><p>the fact that such a broad group of people were surveyed that it\u2019s hard to imagine they\u2019re all actually \u201cexperts\u201d (let alone have relevant expertise),</p></blockquote><p>I think the goal was to survey researchers in machine learning, and so it was sent to researchers who publish in the top venues in machine learning. I don't think \"expert\" was meant to imply that these respondents had e.g. some kind of particular expertise about risk. In fact the preprint emphasizes that very few of the respondents have thought at length about the long-term impacts of AI.</p><blockquote><p>Given my aforementioned concerns, I wonder whether the cost of this survey can be justified</p></blockquote><p>I think it can easily be justified. This survey covers a set of extremely important questions, where policy decisions have trillions of dollars of value at stake and the views of the community of experts are frequently cited in policy discussions.</p><p>You didn't make your concerns about selection bias quantitative, but I'm skeptical about quantitatively how much they decrease the value of information. And even if we think non-response is fatal for some purposes, it doesn't interfere as much with comparisons across questions (e.g. what tasks do people expect to be accomplished sooner or later, what risks do they take more or less seriously) or for observing how the views of the community change with time.</p><p>I think there are many ways in which the survey could be improved, and it would be worth spending additional labor to make those improvements. I agree that sending a survey to a smaller group of recipients with larger compensation could be a good way to measure the effects of non-response bias (and might be more respectful of the research community's time).</p><blockquote><p>I am not inclined to update very much on what AI researchers in general think about AI risk on the basis of this survey</p></blockquote><p>I think the main takeaway w.r.t. risk is that typical researchers in ML (like most of the public) have not thought about impacts of AI very seriously but their intuitive reaction is that a range of negative outcomes are plausible. They are particularly concerned about some impacts (like misinformation), particularly unconcerned about others (like loss of meaning), and are more ambivalent about others (like loss of control).</p><p>I think this kind of \"haven't thought about it\" is a much larger complication for interpreting the results of the survey, although I think it's fine as long as you bear it in mind. (I think ML researchers who have thought about the issue in detail tend if anything to be somewhat more concerned than the survey respondents.)</p><blockquote><p>many AI researchers just don\u2019t seem too concerned about the risks posed by AI</p></blockquote><p>My impressions of academic opinion have been broadly consistent with these survey results. I agree there is large variation and that many AI researchers are extremely skeptical about risk.</p>", "parentCommentId": "35RTccd3tWaiYLdzD", "user": {"username": "Paul_Christiano"}}, {"_id": "GPmDHwfA93ZQutpx5", "postedAt": "2024-01-06T19:20:21.026Z", "postId": "M9MSe4KHNv4HNf44f", "htmlBody": "<p>Just to give your final point some context: the average in-depth research project by Rethink Priorities&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/db5TbLPSZL2XPeaed/ama-peter-wildeford-co-ceo-at-rethink-priorities?commentId=FB22zzAhXD8dEz9Sc\">reportedly <u>costs $70K-$100K</u></a>. So, if this AI Impacts survey cost $138K in participant compensation, plus some additional amount for things like researcher time, then it looks like this survey was two or three times more expensive than the average research project in its approximate reference class.</p><p>I haven\u2019t thought hard about whether the costs of EA-funded research make sense in general, but I thought I\u2019d leave this comment so that readers don\u2019t go away thinking that this survey cost like an order of magnitude more than what\u2019s standard.</p>", "parentCommentId": "35RTccd3tWaiYLdzD", "user": {"username": "Will Aldred"}}, {"_id": "QiwvA7LZjMcxPqDPL", "postedAt": "2024-01-06T20:03:57.488Z", "postId": "M9MSe4KHNv4HNf44f", "htmlBody": "<blockquote><p>Note that we didn't tell them the topic that specifically.</p></blockquote><p>I understand that, and think this was the right call. But there seems to be <a href=\"https://nces.ed.gov/FCSM/pdf/IHSNG_StatsCan2_JB.pdf\">consensus</a> <a href=\"https://academic.oup.com/poq/article/79/1/130/2330003\">that</a> in general, a response rate below ~70% introduces concerns of non-response bias, and when you're at 15%\u2014with (imo) good reason to think there would be non-response bias\u2014you really cannot rule this out. (Even basic stuff like: responders probably earn less money than non-responders, and are thus probably younger, work in academia rather than industry, etc.; responders are more likely to be familiar with the prior AI Impacts survey, and all that that entails; and so on.) In short, there is a reason many medical journals have a policy of not publishing surveys with response rates below 60%; e.g., <a href=\"https://jamanetwork.com/journals/jama/pages/instructions-for-authors#SecOtherObservationalStudies\">JAMA</a> asks for &gt;60%, less prestigious <a href=\"https://jamanetwork.com/journals/jamanetworkopen/pages/instructions-for-authors\">JAMA journals</a> also ask for &gt;60%, and <a href=\"https://www.bmj.com/about-bmj/resources-authors/bmj-right-journal-my-research-article\">BMJ</a> asks for &gt;65%. (I cite medical journals because their policies are the ones I'm most familiar with, not because I think there's something special about medical journals.)</p><blockquote><p>Tried sending them $100 last year and if anything it lowered the response rate.</p></blockquote><p>I find it a bit hard to believe that this lowered response rates (was this statistically significant?), although I would buy that it didn't increase response rates much, since I think I remember reading that response rates fall off pretty quickly as compensation for survey respondents increases. I also appreciate that you're studying a high-earning group of experts, making it difficult to incentivize participation. That said, my reaction to this is: determine what the higher-order goals of this kind of project are, and adopt a methodology that aligns with that. I have a hard time believing that at this price point, conducting a survey with a 15% response rate is the optimal methodology.&nbsp;</p><blockquote><p>If you are inclined to dismiss this based on your premise \"many AI researchers just don\u2019t seem too concerned about the risks posed by AI\", I'm curious where you get that view from, and why you think it is a less biased source.</p></blockquote><p>My impression stems from conversations I've had with two CS professor friends about how concerned the CS community is about the risks posed by AI. For instance, last week, I was discussing the last AI Impacts survey with a CS professor (who has conducted surveys, as have I); I was defending the survey, and they were criticizing it for reasons similar to those outlined above. They said something to the effect of: the AI Impacts survey results do not align with my impression of people's level of concern based on discussions I've had with friends and colleagues in the field. And I took that seriously, because this friend is EA-adjacent; extremely competent, careful, and trustworthy; and themselves sympathetic to concerns about AI risk. (I recognize I'm not giving you enough information for this to be at all worth updating on for <i>you</i>, but I'm just trying to give some context for my own skepticism, since you asked.)&nbsp;</p><p>Lastly, as someone immersed in the EA community myself, I think my bias is\u2014if anything\u2014in the direction of wanting to believe these results, but I just don't think I should update much based on a survey with such a low response rate.</p><p>I think this is going to be my last word on the issue, since I suspect we'd need to delve more deeply into the literature on non-response bias/response rates to progress this discussion, and I don't really have time to do that, but if you/others want to, I would definitely be eager to learn more.</p>", "parentCommentId": "eSofAEenAPACiJyjS", "user": {"username": "hobbes"}}, {"_id": "rMPDqKbWB246aBeC2", "postedAt": "2024-01-06T20:19:48.217Z", "postId": "M9MSe4KHNv4HNf44f", "htmlBody": "<p>I really appreciate your and <a href=\"https://forum.effectivealtruism.org/users/katja_grace?mention=user\">@Katja_Grace</a>'s thoughtful responses, and wish more of this discussion had made it into the manuscript. (This is a minor thing, but I also didn't love that the response rate/related concerns were introduced on page 20 [right?], since it's standard practice\u2014at least in my area\u2014to include a response rate up front, if not in the abstract.) I wish I had more time to respond to the many reasonable points you've raised, and will try to come back to this in the next few days if I do have time, but I've written up a few thoughts <a href=\"https://forum.effectivealtruism.org/posts/M9MSe4KHNv4HNf44f/survey-of-2-778-ai-authors-six-parts-in-pictures?commentId=QiwvA7LZjMcxPqDPL\">here</a>.</p>", "parentCommentId": "csLcRqHPA5XeYGj6c", "user": {"username": "hobbes"}}, {"_id": "pq32RKS4PyfGQETdJ", "postedAt": "2024-01-09T11:58:46.781Z", "postId": "M9MSe4KHNv4HNf44f", "htmlBody": "<p>Link-commenting <a href=\"https://twitter.com/JoshuaBlake_/status/1742983901260530095\">my Twitter thread of immediate reaction</a> and summary of paper. Some light editing for readability. Would be interested on feedback if this slightly odd for a forum comment content is helpful or interesting to people.</p><p>Overall take: this is a well done survey, but all surveys of this sort have big caveats. I think this survey is as good as it is reasonable to expect a survey of AI researchers to be. But, there is still likely bias due to who chooses to respond, and it's unclear how much we should be deferring to this group. It would be good to see an attempt to correct for response bias (eg weighting). Appendix D implies it would likely only have small effects though, except widening the distributions because women were more uncertain and less likely to respond.</p><h1>Timelines</h1><p>Wording of questions matters <i>a lot</i> when asking about time for AI to be able to do all tasks/jobs. 60 year difference in median due to a small change, which researchers can't explain. Will allow cherry picking to support different arguments. In particular, Timeline predictions are extremely non-robust. HLMI = High-Level Machine Intelligence. FAOL = Full Automation of Labor. 60 year apart in time to occurence.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/vvy6ewmbozummdeetjfz\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/xqqzk9kqwt1b9telms3k 90w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/xd6sqvcq8j625jddo5ip 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/i2wofshlyyviefpgzun8 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/jn9e0zmmne1aryfcwki4 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/xsb5o3241up90lp0ob6d 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/nnepyy6kmgoufhyuixlb 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/wft2pelrh7tcnpi6ixhs 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/n19vtuerwutzo665azby 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/sfbs8siqrmmynkrkq08t 810w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/pzzhzps8owxal44roajz 900w\"></figure><p>Other bits of question wording matter, not anywhere near as much (see below). Annoyingly, there's no CIs for the median times so it's hard to assess how much is noise. I guess not much due to the sample size.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/ctbty0btgbclvchtjgty\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/ayzfnuy5nmzpo7vwydy2 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/ogaeosvglekeppsgnckj 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/lfrkvfcddsrbbswkjzxo 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/mnc4ygdjxesa4wwitxn3 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/bns9np0duuu1dgtn3vgc 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/soisqtgzglqt7odldcmf 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/zhgvtduvxbyllktgskq1 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/wgzimxfyy76std3b4nrq 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/q1ayyvnb3rrekbywo0yk 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/zbkek9ggxpkupdhqsyab 1080w\"></figure><p>This might be just uncertainty though. Any single year is a bad summary when uncertainty is this high. The ranges below are where the distribution aggregated across researchers place 50% of the mass.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/sjpcm6osn6ncdcpr4esv\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/p1eneb1hlvn4aiqmoqj2 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/feuek26mt0a9rohxlfw3 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/sllyvuytzvnzodqg9szx 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/zdtccyvv5deokxxqj54e 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/emxi4yhzkcrynmvcy9rm 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/efrlanzwl3ggtje8jitb 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/cwgp00zhtx3cgqehz5ng 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/dujsn3e9eg7llutowivm 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/keatgsug9vcp5cazl9lp 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/zllvqgtxvqyeg5bq8sjj 1053w\"></figure><p>I find it very amusing that AI researchers think the hardest task to get AI to do (of all the ones they asked about) is... Being an AI researcher. Glad they're confident in their own job security.</p><p>Note that time to being \"feasible\" is defined quite loosely. It would still cost millions of dollars (if not more) to implement and only be available to top labs. Annoyingly, it means that the predictions can only be falsified as too long, not too short.</p><p>The aggregation is making a strong assumption about the shape of respondents' distribution. I'm suspicious of any extrapolation or interpolation based on it. A sensitivity analysis would be nice here. Also, why not a three-parmeter distribution so it can be fit exactly?</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/s4m7abco6cg7w9viy5o5\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/r7suswdak57emdnnn192 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/rnllqixsyym7dnmekugi 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/jokqgena6gqhdvyewkcx 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/jdtuvfxcvuspoqaxvjxk 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/cqrglf9v8dayhwaqlk4o 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/l8j2lkx020qtb0vm9vii 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/ocoz5nlbeqjth0cds5p6 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/rbxrrkvhccvwbonzq3gc 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/urtaqz4oat9fhl4mjdrl 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/laycgzzag0nmrcztfodr 1970w\"></figure><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/ma1uzptu5rcrhuyngsk5\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/rsemgzldmlactmczywk9 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/hqsne1hc2hzpyzbbbnh6 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/sgctjfblyzmiosrsmnx2 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/snjqjczzwpj46fkdualy 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/axhsoib8nipbpemrsbcs 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/ufesvyrywe14lrnjrq3n 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/njvb44c1zzllvbtdadl3 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/sgllgdkw3k7bz24ns4s0 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/g47sf5nrlhedqizltgsx 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/kemboyuxi92iivj0uccy 1080w\"></figure><p>Time to ~AGI in 2016 and 2022 surveys very similar, but big change in median times for 2023. Remember previous caveat about no CIs and wide distributions though.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/vbhgefcsuxnfkhcm44xj\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/ivswlacxd3efn5j2wdhx 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/dixuaqabqt6a30ff6mqu 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/wttcxwvt9p1igzfzrdcu 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/ik4subuqwuuaqzfrghe2 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/jndz8zyxyrggvpvcv2ys 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/n1giy9ps8oi3mhmllcsy 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/xdvfoe4zgr2p6r64tauy 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/jxvypokxgx7dhwlspdum 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/yq9nle6i2ao4yhnhicxu 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/sqtofzkgcqas2tjunhdi 973w\"></figure><p>Some recommendations on quoting timelines from this survey.</p><ul><li>Don't use just the HLMI or FAOL questions.</li><li>Use intervals not medians.</li><li>Be clear it's an expert survey, and might be biased. Being an AI researcher selects for thinking AI is promising!</li></ul><h1>Outcomes of AGI</h1><p>I don't have much to say on the probabilities of different oucomes. I note they're aggregating with means/medians. These reduce the weight on very low end or very high probabilities a lot (relative to geometric mean of odds, which I think is better). So these are probably closer to 50% than they should be.</p><p>Headline result below! Probability of very bad outcomes, conditional on high-level machine intelligence existing. Median respondent unchanged at 5-10%. I'd guess heavily affected by rounding and putting 5% for \"small chance, don't know\". An upper bound on the truth for AI researchers' median IMO.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/wkjskavazpbxbumfrf0z\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/lboouucsoc8t3kq8v7h1 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/zfeqtdr6jiu23nxkr00i 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/aeujuj8kiwqpiqevem5r 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/uwjtn5kt9srdremx61o6 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/licvmaxm5u4a74ehna3k 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/gvbanbrcnvqkie540mql 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/vxj07ayc3qdwvwouxxhb 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/gpqaa5r9jt6fe1krzx5k 1920w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/bysiwihnphfx9efbqfh6 2160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/pq32RKS4PyfGQETdJ/onph8l1ktqszjbh75gch 2400w\"></figure><p>There's lots of demographic breakdowns, mostly uninteresting IMO. They didn't ask or otherwise assess how much work respondents had done on AI safety. Would have been interesting to see the split and also to assess response bias.</p>", "parentCommentId": null, "user": {"username": "jooke"}}, {"_id": "iAcYLnot6mcjwkjhE", "postedAt": "2024-01-20T00:14:41.935Z", "postId": "M9MSe4KHNv4HNf44f", "htmlBody": "<p>Thanks for citing the survey here, and thank you Joshua for your analysis.</p><p>Your post doesn\u00b4t seem strange to me at this place; at the very least I can\u00b4t find any harm in posting it here. (If someone is more interested in other discussions, they may read the first two lines and then skip it.) The only question would be if this is worth YOUR time, and I am confident you are able to judge this (and you apparently did and found it worth your time).</p><p>Since you already delved that deep into the material and since I don\u00b4t see myself doing the same, here a question to you (or whoever else feeling inclined to answer):</p><p>Were there a significant part of experts who thought that HLMI and/or FAOL are downright impossible (at least with anything resembling our current approaches)? I do hear/read doubts like these sometimes. If so, how were these experts included in the mean, since you can\u00b4t just include infinity with non-zero probability without the whole number going up to infinity? (If they even used a mean. \"Aggregate Forecast\" is not very clear; if they used the median ore something similar the second question can be ignored.)</p>", "parentCommentId": "pq32RKS4PyfGQETdJ", "user": {"username": "MarcKr\u00fcger"}}]