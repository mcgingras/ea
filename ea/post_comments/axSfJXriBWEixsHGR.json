[{"_id": "TqdJfe5sosMhgz6TX", "postedAt": "2024-01-15T15:58:36.843Z", "postId": "axSfJXriBWEixsHGR", "htmlBody": "<p>Following on from your saner world illustration, I\u2019d be curious to hear what kind of a call to action you might endorse in our current world.</p><p>I personally <a href=\"https://www.alignmentforum.org/posts/5bd75cc58225bf06703752c6/my-current-take-on-the-paul-miri-disagreement-on-alignability-of-messy-ai?commentId=5bd75cc58225bf06703752db\">find</a> <a href=\"https://www.lesswrong.com/posts/HTgakSs6JpnogD6c2/two-neglected-problems-in-human-ai-safety\">your</a>&nbsp;<a href=\"https://www.lesswrong.com/posts/fJqP9WcnHXBRBeiBg/meta-questions-about-metaphilosophy\"><u>writings</u></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/pFAavCTW56iTsYkvR/ai-alignment-open-thread-october-2019?commentId=ecpBgnqXeWzuDmW7q\"><u>on</u></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy\"><u>metaphilosophy</u></a>, and the closely related problem of ensuring AI philosophical competence, persuasive. In other words, <a href=\"https://forum.effectivealtruism.org/posts/RvafKqEYndLrnrGjm/who-should-we-interview-for-the-80-000-hours-podcast#3JuBR8PEwJYg355Tm\">I think this area has been overlooked</a>, and that more people should be working in it given the current margin in AI safety work. But I also have a hard time imagining anyone pivoting into this area, at present, given that:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1gfnxoo1ot4\"><sup><a href=\"#fn1gfnxoo1ot4\">[1]</a></sup></span></p><ol><li>There\u2019s no research agenda with scoped out subproblems (as far as I\u2019m aware), only the overall,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/xwhWgA3KLRHfrqdqZ/the-wicked-problem-experience\"><u>wicked</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ee8Pamunhqabucwjq/long-term-future-fund-ask-us-anything-september-2023?commentId=zNGo5KABvpyJDb5s6\"><u>problem</u></a> of trying to get advanced AIs to do philosophy well.</li><li>There are no streams within junior research programs, like&nbsp;<a href=\"https://www.matsprogram.org/\"><u>MATS</u></a>, to try one\u2019s hand<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkumbbzvn4t\"><sup><a href=\"#fnkumbbzvn4t\">[2]</a></sup></span>&nbsp;in this area while gaining mentorship.</li></ol><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1gfnxoo1ot4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1gfnxoo1ot4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>A third reason, which I add here as a footnote since it seems far less solvable: <a href=\"https://forum.effectivealtruism.org/posts/SWfwmqnCPid8PuTBo/monetary-and-social-incentives-in-longtermist-careers?commentId=oCasjkNkQrWPE2DBT\"><u>Monetary and</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/2TdXocyDF9PxWewwY/should-the-ea-community-be-cause-first-or-member-first?commentId=FHXN9dBJC8w4MusjH\"><u>social incentives</u></a> are pushing promising people into empirical/ML-based <a href=\"https://www.effectivealtruism.org/articles/paul-christiano-current-work-in-ai-alignment#:~:text=What%20I%20mean%20is%20%E2%80%9Cintent%20alignment%2C%E2%80%9D%20which%20is%20trying%20to%20build%20AI%20systems%20that%20are%20trying%20to%20do%20what%20you%20want%20them%20to%20do.%20In%20some%20sense%2C%20this%20might%20be%20the%20minimum%20you%27d%20want%20out%20of%20your%20AI%3A%20at%20least%20it%20is%20trying.%20And%20intent%20alignment%20is%20just%20one%20piece%20of%20making%20AI%20have%20a%20positive%2C%20long%2Drun%20impact.\">intent alignment</a> work.&nbsp;(To be clear, I believe intent alignment is important. I just don\u2019t think it\u2019s the only problem <a href=\"https://forum.effectivealtruism.org/posts/Hhtvwx2ka4pzoWg7e/ai-alignment-shouldn-t-be-conflated-with-ai-moral?commentId=kaL8SPu7jL7LhoWBF\">that</a> <a href=\"https://www.alignmentforum.org/posts/y5jAuKqkShdjMNZab/morality-is-scary#\">deserves</a> <a href=\"https://www.alignmentforum.org/posts/7jSvfeyh8ogu8GcE6/decoupling-deliberation-from-competition\">attention</a>.) <a href=\"https://forum.effectivealtruism.org/posts/2v49tMYph3dKpvdRp/the-availability-bias-in-job-hunting#It_adds_friction_for_people_trying_to_pursue_independent_paths\"><u>It takes agency</u></a><u>\u2014</u>and financial stability, and a disregard for status\u2014to strike out on one\u2019s own and work on <a href=\"https://forum.effectivealtruism.org/posts/XTBGAWAXR25atu39P/third-wave-effective-altruism#Third_wave_EA__what_are_some_possibilities_:~:text=AI%20safety%20becomes%20mainstream%20and%20%22spins%20out%22%20of%20EA.%20EA%20stays%20at%20the%20forefront%20of%20weirdness%20and%20the%20people%20who%20were%20previously%20interested%20in%20AI%20safety%20turn%20their%20focus%20to%20digital%20sentience%2C%20acausal%20moral%20trade%2C%20and%20other%20issues%20that%20still%20fall%20outside%20the%20Overton%20window.\">something weirder</a>, such as metaphilosophy or other <a href=\"https://forum.effectivealtruism.org/posts/y8Mu8EZtyJZeHAnra/memo-on-some-neglected-topics\">neglected, non-alignment AI topics</a>.</p><p>[ETA: Ten days after I posted this comment, Will MacAskill gave an <a href=\"https://forum.effectivealtruism.org/posts/TeBBvwQH7KFwLT7w5/william_macaskill-s-shortform?commentId=RLMp7cHf9fdLuRSfe\">update on his work</a>: he has started looking into neglected, non-alignment AI topics, with a view to perhaps founding a new research institution. I find this encouraging!]</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkumbbzvn4t\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkumbbzvn4t\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Pun intended.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Will Aldred"}}, {"_id": "neXEfz6fG3ALFGJXd", "postedAt": "2024-01-15T21:31:47.635Z", "postId": "axSfJXriBWEixsHGR", "htmlBody": "<ol>\n<li>Just talking more about this problem would be a start. It would attract more attention and potentially resources to the topic, and make people who are trying to solve it feel more appreciated and less lonely. I'm just constantly confused why I'm the only person who frequently talks about it in public, given how obvious and serious the problem seems to me. It was more understandable before ChatGPT put AI on everyone's radar, but now it's just totally baffling. And I appreciate you writing this comment. My posts on the topic usually get voted up, but with few supporting comments, making me unsure who actually agrees with me that this is an important problem to work on.</li>\n<li>If you're a grant maker, you can decide to fund research in this area, and make some public statements to that effect.</li>\n<li>If might be useful to think in terms of a \"AI philosophical competence difficulty scale\" similar to Sammy Martin's <a href=\"https://www.lesswrong.com/posts/EjgfreeibTXRx9Ham/ten-levels-of-ai-alignment-difficulty\">AI alignment difficulty scale</a> and \"safety by eating marginal probability\". I tend to focus on the higher end of that scale, where we need to achieve a good explicit understanding of metaphilosophy, because I think solving that problem is the only way to reduce risk to a minimum, and it also fits my inclination for philosophical problems, but someone more oriented towards ML research could look for problems elsewhere on the difficulty scale, for example fine-tuning a LLM to do better philosophical reasoning, to see how far that can go. Another idea is to fine-tune a LLM for pure persuasion, and see if that can be used to create an AI that <em>deemphasizes</em> persuasion techniques that don't constitute valid reasoning (by subtracting the differences in model weights somehow).</li>\n<li>Some professional philosopher(s) may actually be starting a new org to do research in this area, so watch out for that news and check how you can contribute. Again providing funding will probably be an option.</li>\n<li>Think about social aspects of the problem. What would it take for most people or politicians to take the AI philosophical competence problem seriously? Or AI lab leaders? What can be done if they never do?</li>\n<li>Think about how to evaluate (purported) progress in the area. Are there clever ways to make benchmarks that can motivate people to work on the problem (and not be easily Goodharted against)?</li>\n<li>Just to reemphasize, talk more about the problem, or prod your favorite philosopher or AI safety person to talk more about it. Again it's totally baffling the degree to which nobody talks about this. I don't think I've even once heard a professional philosopher publicly express a concern that AI might be relatively incompetent in philosophy, even as some opine freely on other aspects of AI. There are certainly obstacles for people to work on the problem like your reasons 1-3, but for now the bottleneck could just as well be in the lack of social proof that the problem is worth working on.</li>\n</ol>\n", "parentCommentId": "TqdJfe5sosMhgz6TX", "user": {"username": "Wei_Dai"}}, {"_id": "HGnenaX7AKcN3wK3i", "postedAt": "2024-01-16T14:11:33.723Z", "postId": "axSfJXriBWEixsHGR", "htmlBody": "<p>Thanks for the post ! I had read others from you previously, and I think the comparison with the hand generation makes your point clearer.<br><br><i>Precautionary notes : I might have misunderstood stuff, be adressing non-problems or otherwise be that irritating person that really should have read the relevant posts twice.</i><br><br>I sense some possible frustration expressed at the end of the post. In the hope that it is helpful, I would like to explain why this matter is not the one that preoccupies me the most -keeping in mind that I only got a desperate undergrad diploma in philosophy, and just so happen to have spent time thinking about vaguely similar problems (you would definitely benefit from someone else than a random dude giving feedback).<br><br>A confusing part for me is that \"what makes good philosophy\" and the likes is not a hard problem, it's an <i>insanely </i>hard one. It would be a hard problem if we had to solve it analytically (e.g using probability theory, evidence, logic, rationality and such), but as a matter of fact, we need more than this, because non-analytic people are, unsurprisingly, extremely resistant to analytic arguments, and are also a non-negligeable proportion of philosophers (and humans). I think it would be dishonest to claim to solve meta-philosophy while producing assertions that do not move non-analytic thinkers by an inch. Saying stuff that convinces a lot of people is very hard, even when relying on logic and science.<br><br>Say that your LLM is trained to do \"exquisite philosophy\", and that anything it outputs is written in the style of Derrida and shares its presumptions, e.g :&nbsp;<br>&nbsp;</p><blockquote><p>User : \"How to know whether something is true ?\"<br><br>AI : In the elusive terrain of epistemological inquiry, the quest for truth reveals itself as an intricate dance with the undecidable. To ascertain the veracity of a proposition is to engage in a perpetual play of signifiers, where meaning unfurls in a web of intertextuality and deferred significations.<br><br>We need to scrutinize the inherent instability within language, dismantling the notion of an essential, fixed truth. The very act of seeking truth is inscribed within a linguistic economy that perpetuates multiplicities of meanings, fragmenting certainties into a constellation of ever-shifting possibilities.</p></blockquote><p>(This is a charitable version of Derrida)<br>You would like to say : \"No ! This is bad philosophy !\"<br><br>But let's reverse the situation. If a hardcore phenomenologist is faced with the following exchange :</p><blockquote><p>User : How to know whether something is true ?<br><br>AI: To ascertain the truth of a proposition, one must rely on empirical evidence and logical reasoning. Direct observation, empirical verification, and coherence with established knowledge serve as essential criteria. However, absolute certainty remains elusive, and one must acknowledge the provisional nature of truth, subject to refinement in light of new evidence or more refined reasoning.</p></blockquote><p>Their answer will be the same : \"It's such bad philosophy, that it's not even philosophy!\" (real quote).<br><br>Of course, when pressed to defend your side, you could argue several things, while citing dozens upon dozens of analytic authors.&nbsp;<br>But that will not move your Derridean interlocutor -for some obscure reasons that have to do with the fact that communication is <i>already broken</i>. Phenomenologist have <i>already chosen </i>a certain criterion for succesful communication (something like \"manifestation under transcendental reduction\") while we have <i>already chosen</i> another one. What does it even mean to evaluate this criterion in a way that makes sense for everyone?<br><br>I'm also playing naive by making clear distinctions between phenomenal and analytic traditions, but really we don't clearly know what makes a tradition, when do these problems arise, and how to solve them. Philosophers themselves stopped arguing about it because, really, it's more productive to just ignore each other, or pretend to have understood it at some level by reinterpreting it while throwing away the gibberish metaphysics you disagree with, or pretend that \"it's all a matter of style\".<br><br>If anyone makes an AI that is capable of superhuman philosophy, any person from any tradition will pray for it to be part of <i>their</i> tradition, and this will have a very important impact.&nbsp;As things stand out right now, ChatGPT seems to be quite analytic by default, to the (very real) distaste of my continental friends. I could as well imagine feeling distate for a \"post-analytic\" LLM that is, actually, doing better philosophy than any living being.<br><br>So the following questions are still open for me :&nbsp;<br>1-How do you plan to solve the inter-traditional problem, if at all?&nbsp;<br>2-Don't you think it is a bit risky to ignore the extent to which philosophers disagree on what even is philosophy, when filtering the dataset to create a good philosopher-AI?<br>3-If this problem is orthogonal to your quest, are you sure that \"philosophy\" is the right term ?</p>", "parentCommentId": null, "user": {"username": "Camille"}}, {"_id": "pDypiowxrgf9XNqGB", "postedAt": "2024-01-17T04:26:45.787Z", "postId": "axSfJXriBWEixsHGR", "htmlBody": "<p>Thanks for the comment. I agree that what you describe is a hard part of the overall problem. I have a partial plan, which is to solve (probably using analytic methods) metaphilosophy for both analytic and non-analytic philosophy, and then use that knowledge to determine what to do next. I mean today the debate between the two philosophical traditions is pretty hopeless, since nobody even understands what people are really doing when they do analytic or non-analytic philosophy. Maybe the situation will improve automatically when metaphilosophy has been solved, or at least we'll have a better knowledge base for deciding what to do next.</p>\n<p>If we can't solve metaphilosophy in time though (before AI takeoff), I'm not sure what the solution is. I guess AI developers use their taste in philosophy to determine how to filter the dataset, and everyone else hopes for the best?</p>\n", "parentCommentId": "HGnenaX7AKcN3wK3i", "user": {"username": "Wei_Dai"}}, {"_id": "K2dfLruRy7duCQy4k", "postedAt": "2024-01-19T06:01:27.706Z", "postId": "axSfJXriBWEixsHGR", "htmlBody": "<p>I wonder if we can few shot our way there by fine-tuning on Kagan\u2019s \u201cDeath Course\u201d, Parfit, and David Lewis.\nEdit: also the SEP?</p>\n", "parentCommentId": null, "user": {"username": "Ahrenbach"}}, {"_id": "i6df4foLkkNWNjeWp", "postedAt": "2024-01-21T00:28:21.559Z", "postId": "axSfJXriBWEixsHGR", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/users/will-aldred?mention=user\">@Will Aldred</a> I forgot to mention that I do have the same concern about \"safety by eating marginal probability\" on AI philosophical competence as on AI alignment, namely that progress on solving problems lower in the difficulty scale might fool people into having a false sense of security. Concretely, today AIs are so philosophically incompetent that nobody trusts them to do philosophy (or almost nobody), but if they seemingly got better, but didn't really (or not enough relative to appearances), a lot more people might and it could be hard to convince them not to.</p>", "parentCommentId": "neXEfz6fG3ALFGJXd", "user": {"username": "Wei_Dai"}}]