[{"_id": "giMvvpSfurn8sdJsb", "postedAt": "2023-08-10T04:36:09.841Z", "postId": "3kMQTjtdWqkxGuWxB", "htmlBody": "<p>This is a useful post! Are there any of the discussions or justifications behind various people's responses (especially in points of disagreement) that might be shareable? Did people change their mind or come to a consensus after the discussions, and if so, in which directions?</p>", "parentCommentId": null, "user": {"username": "pseudonym"}}, {"_id": "sDzouq4QLEnqzegSg", "postedAt": "2023-08-10T09:37:01.801Z", "postId": "3kMQTjtdWqkxGuWxB", "htmlBody": "<p>As a biased mostly near termist (full disclosure), I've got some comments and questions ;)<br><br>First a concern about the framing<br><i><strong>\"Prompted by the FTX collapse, the rapid progress in AI, and increased mainstream acceptance of AI risk concerns, there has recently been a fair amount of discussion among EAs whether it would make sense to rebalance the movement\u2019s portfolio of outreach/recruitment/movement-building activities away from efforts that use EA/EA-related framings and towards projects that instead focus on the constituent causes.\"</strong></i></p><p>This framing for the discussion seems a bit unclear. First I don't see the direct logical connection between \"<i><strong>Prompted by the FTX collapse, the rapid progress in AI, and increased mainstream acceptance of AI risk concerns\" &nbsp;</strong></i>and <i><strong>\"rebalance the movement\u2019s portfolio of outreach/recruitment/movement-building activities away from efforts that use EA/EA-related framings and towards projects that instead focus on the constituent causes.\" &nbsp;</strong></i>There must be a some implied assumptions filling the gap between these two statements that I'm missing, its certainly not A + B= C. I'm guessing it something like FTX collapse causing <i><strong>potential significant reputational loss to the EA brand &nbsp;etc. </strong></i>I think being explicit is important when framing a discussion.</p><p>Second, when we are talking about <i><strong>\"focus on the constituent causes\", </strong></i>and \"<i><strong>cause specific\" </strong></i>does that practically mean growth in AI Safety focused groups while general EA groups remain, or further specialisation within EA with Global health / Animal advocacy / Climate change / Biorisk etc? &nbsp;(I might be very wrong here). Does <i><strong>\"constituent cause\"</strong></i> and <i><strong>\"cause specific\"</strong></i> mostly translate to <i>\"<strong>AI safety\"</strong></i><strong> </strong>in the context of this article or not?<br>&nbsp;</p><p>One other comment that concerned me was that among this group there was a <i><strong>\"consensus that a <u>non-trivial</u> fraction of outreach efforts that are framed in EA terms are still worth supporting.\" &nbsp;</strong></i>This is quite shocking to me, the idea that EA framed outreach should perhaps be downgraded from the status quo (most of the outreach) to \"non-trivial\" (which I interpret as very little of the outreach). That's a big change which personally I don't like and I wonder what the wider community thinks.<br><br>it already seems like a lot (the majority?) of community building is bent towards AI safety, so its interesting that the push from EA thought leaders seems to be to move further in this direction.</p><ul><li>As this post itself states, 80,000 hours in practise seems pretty close to an AI/long termist career advice platform , here are their latest 3 releases.<br><br><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sDzouq4QLEnqzegSg/mjmj1cnlaeseftq81xl7\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sDzouq4QLEnqzegSg/cgtdzw3fwsijl42ogwym 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sDzouq4QLEnqzegSg/vvr72reharhii00vioew 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sDzouq4QLEnqzegSg/nigbf4p97gbhfesgzn3j 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sDzouq4QLEnqzegSg/hzaxpmgtoye795ob6geh 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sDzouq4QLEnqzegSg/r0z9pf0rt6pt7tuzfuek 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sDzouq4QLEnqzegSg/ksxziyieb2aeyi9e7inh 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sDzouq4QLEnqzegSg/ktroafaxrj62pqm7ohs7 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sDzouq4QLEnqzegSg/txae9nddd9nkrlskg8sa 1920w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sDzouq4QLEnqzegSg/dwf8b66l9pegqmyqc1sa 2160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sDzouq4QLEnqzegSg/wvthb1vxqyds25lbkpzl 2324w\"></li><li>There <a href=\"https://forum.effectivealtruism.org/posts/euzDpFvbLqPdwCnXF/university-ea-groups-need-fixing\">have already been concerns raised </a>that EA university groups can often intentionally or unintentionally push AI safety as the clearly most important cause, to the point where it may be compromising epistemics.<br>&nbsp;</li></ul><p>Finally this didn't sit well with me<i><strong> \"There was significant disagreement whether OP should start a separate program (distinct from&nbsp;</strong></i><a href=\"https://www.openphilanthropy.org/focus/effective-altruism-community-growth-longtermism/\"><i><strong><u>Claire\u2019s</u></strong></i></a><i><strong> and&nbsp;</strong></i><a href=\"https://www.openphilanthropy.org/focus/ea-community-growth-global-health-and-wellbeing/\"><i><strong><u>James\u2019</u></strong></i></a><i><strong> teams) focused on \u201cEA-as-a-principle\u201d/\u201dEA qua EA\u201d-grantmaking. Five respondents agreed with a corresponding prompt (answers between 6-9), two respondents disagreed (answers between 2-4), one neither agreed nor disagreed.\"</strong></i></p><p>These discussions are important but I don't love the idea of the most important discussion with the important people steering the EA ship being led by Openphil (a funding organisation), rather than perhaps by CEA or even perhaps a wider forum. Should the most important discussions about the potential future of a movement should be led by a funding body?</p><p><strong>Things I agreed with/liked</strong><br>- I instinctively like the idea of an AI conference, as maybe that will mean the other conferences have a much higher proportion of EAs who are into other things.<br>- More support to AI safety specific groups in general. Even as a near termist, that makes a lot of sense as there is a big buzz about it right now and they can attract non-EA people to the field and provide focus for those groups.<br>- I'm happy that 5 out of 8 disagreed with renaming the forum (although was surprised that even 3 voted for it). For branding/understanding/broadchurch and other reasons I struggle to see positive EV in that one.<br>- I'm happy that 5 out of the 8 agreed that 80,000 hours should be more explicit about its longtermism focus. It feels a bit disingenous at the moment - although I know that isn't the intention.</p><p>Looking forward to more discussion along these lines!</p>", "parentCommentId": null, "user": {"username": "NickLaing"}}, {"_id": "KGSTASXatsTjehWDm", "postedAt": "2023-08-10T12:03:32.225Z", "postId": "3kMQTjtdWqkxGuWxB", "htmlBody": "<blockquote><p>There was near-consensus that Open Phil should generously fund promising AI safety community/movement-building projects they come across</p></blockquote><p>Would you be able to say a bit about to what extent members of this working group have engaged with the arguments around AI safety movement-building potentially doing more harm than good? For instance, points 6 through 11 of Oli Habryka's second message in the \u201cShutting Down the Lightcone Offices\u201d post (<a href=\"https://forum.effectivealtruism.org/posts/rsnrpvKofps5Py7di/shutting-down-the-lightcone-offices#Oliver_s_2nd_message\">link</a>). If they have strong counterpoints to such arguments, then I imagine it would be valuable for these to be written up.</p><p>(Probably the strongest response I've seen to such arguments is the post \u201c<a href=\"https://forum.effectivealtruism.org/posts/sGwPgwvaL2FkBHsRh/how-mats-addresses-mass-movement-building-concerns\">How MATS addresses \u2018mass movement building\u2019 concerns</a>\u201d. But this response is MATS-specific and doesn't cover concerns around other forms of movement building, for example, ML upskilling bootcamps or AI safety courses operating through broad outreach.)</p>", "parentCommentId": null, "user": {"username": "Will Aldred"}}, {"_id": "FHmkuC7tvemggAfu9", "postedAt": "2023-08-10T14:09:50.154Z", "postId": "3kMQTjtdWqkxGuWxB", "htmlBody": "<p>I think all the questions here are important and nontrivial. However, <strong>I'm concerned about taking large actions too quickly</strong> when changing plans based on high-level strategic considerations.&nbsp;</p><ol><li>Views are likely to change quite a bit and grow more nuanced over time. We should expect to take most quick actions when we are overly confident about a particular view.</li><li>Community infrastructure is composed of many organisations and projects that have long-term plans (many months - years). Changing directions abruptly, especially in regards to funding but also in messaging and branding, can hinder the success of such projects.</li><li>Perhaps most importantly, people in the community and workers at EA orgs need stability. Currently, I worry that many people in the community feel poor job security and that what they are working on now will indeed be considered important/relevant later.&nbsp;</li></ol><p>So I think we need to have much more strategic clarity, but to make sure to manage transitions carefully and over long time horizons.&nbsp;</p>", "parentCommentId": null, "user": {"username": "edoarad"}}, {"_id": "sp3vDqjuxTTEi9xHq", "postedAt": "2023-08-11T02:47:23.190Z", "postId": "3kMQTjtdWqkxGuWxB", "htmlBody": "<p>I found this post very informative. Thank you for sharing.<br><br>Some miscellaneous questions:</p><blockquote><p>There was significant disagreement whether OP should start a separate program (distinct from&nbsp;<a href=\"https://www.openphilanthropy.org/focus/effective-altruism-community-growth-longtermism/\"><u>Claire\u2019s</u></a> and&nbsp;<a href=\"https://www.openphilanthropy.org/focus/ea-community-growth-global-health-and-wellbeing/\"><u>James\u2019</u></a> teams) focused on \u201cEA-as-a-principle\u201d/\u201dEA qua EA\u201d-grantmaking.</p></blockquote><p>1. Is there information on why Open Phil originally made the decision to bifurcate community growth funding between LT and GHWB? (I've coincidentally been trying to better understand this and was considering asking on the Forum!) My impression is that this has had extreme shaping effects on EA community-building efforts, possibly more so than any other structural decision in EA.</p><blockquote><p>There was consensus that it would be good if CEA replaced one of its (currently) three annual conferences with a conference that\u2019s explicitly framed as being about x-risk or AI-risk focused conference.</p></blockquote><blockquote><p>Open Phil\u2019s Longtermist EA Community Growth team expects to rebalance its field-building investments by proportionally spending more on longtermist cause-specific field building and less on EA field building than in the past</p></blockquote><p>2. There are two perspectives that seem in opposition here:<br><br>The first is that existing organizations that have previously focused on \"big tent EA\" should create new x-risk programming in the areas they excel (e.g. conference organizing) and it is okay that this new x-risk programming will be carried out by an EA-branded organization.<br><br>The second is that existing organizations that have previously focused on \"big tent EA\" should, to some degree, be replaced by new projects that are longtermist in origin and not EA-branded.<br><br>I share the concern of \"scaling back on forms of outreach with a strong track-record and thereby 'throwing out the baby with the bathwater.'\" But even beyond that, I'm concerned that big tent organizations with years of established infrastructure and knowledge may essentially be dismantled and replaced with brand new organizations, instead of recruiting and resourcing the established organizations to execute new, strategic projects. Just like CEA's events team is likely better at arranging an x-risk conference than a new organization started specifically for that purpose, a longstanding regional EA group will have many advantages in regional field-building compared to a brand-new, cause-specific regional group. We are risking losing infrastructure that took years to develop, instead fo collectively figuring out how we might reorient it.</p><blockquote><p>In March 2023, Open Philanthropy\u2019s Alexander Berger invited Claire Zabel (Open Phil), James Snowden (Open Phil), Max Dalton (CEA), Nicole Ross (CEA), Niel Bowerman (80k), Will MacAskill (GPI), and myself (Open Phil, staffing the group) to join a working group on this and related questions.</p></blockquote><p>3. Finally, I would love to see a version of this that incorporates leaders of cause area and big tent &nbsp;\"outreach/recruitment/movement-building\" organizations who engage \"on the ground\" with members of the community. I respect the perspectives of everyone involved. I also imagine they have a very different vantage point than our team at EA NYC and other regional organizations. We directly influence hundreds of people's experiences of both big-tent EA and cause-specific work through on-the-ground guidance and programming, often as one of their first touchpoints to both. My understanding of the value of cause-specific work is radically different from what it would have been without this in-person, immersive engagement with hundreds of people at varying stages of the engagement funnel, and at varying stages of their individual progress over years of involvement. And though I don't think this experience is necessary to make sound strategic decisions on the topics discussed in the post, I'm worried that the disconnect between the broader \"on the ground\" EA community and those making these judgments may lead to weaker calibration.</p>", "parentCommentId": null, "user": {"username": "Rockwell Schwartz"}}, {"_id": "iqw56RyxseB84D9Kv", "postedAt": "2023-08-11T11:38:29.720Z", "postId": "3kMQTjtdWqkxGuWxB", "htmlBody": "<p>I've <a href=\"https://forum.effectivealtruism.org/posts/Zm6iaaJhoZsoZ2uMD/effective-altruism-as-coordination-and-field-incubation\">written about this</a> idea before FTX and think that FTX is a minor influence compared to the increased interest in AI risk.</p><p>My original reasoning was that AI safety is a separate field but doesn't really have much movement building work being put into it outside of EA/longtermism/x-risk framed activities.&nbsp;</p><p>Another reason why AI takes up a lot of EA space, is that there aren't many other places to go to discuss these topics, which is bad for the growth of AI safety if it's hidden behind donating 10% and going vegan and bad for EA if it gets overcrowded by something that should have it's own institutions/events/etc.</p>", "parentCommentId": "sDzouq4QLEnqzegSg", "user": {"username": "DavidNash"}}, {"_id": "DBtYEQRP6x2Cvhkiz", "postedAt": "2023-08-11T15:17:19.281Z", "postId": "3kMQTjtdWqkxGuWxB", "htmlBody": "<p><i>\"Which is bad for the growth of AI safety if it's hidden behind donating 10% and going vegan\"</i></p><p>This may be true and the converse is also possible concurrently, with the growth of giving 10% and going vegan potentially being hidden at times behind AI safety ;)</p><p>From an optimistic angle \"Big tent EA\" and AI safety can be synergistic - much AI safety funding comes from within the EA community. A huge reason those hundreds of millions are available, is because the AI safety cause grew out of and is often melded with founding EA principles, which includes giving what we can to high EV causes. This has motivated people to provide the very money EA safety work relies on.</p><p>Community dynamics are complicated and I don't think the answers are straightforward.</p>", "parentCommentId": "iqw56RyxseB84D9Kv", "user": {"username": "NickLaing"}}, {"_id": "r2maaTpA8EH4LNute", "postedAt": "2023-08-13T12:31:05.770Z", "postId": "3kMQTjtdWqkxGuWxB", "htmlBody": "<blockquote><p>to rebalance the movement\u2019s portfolio of outreach/recruitment/movement-building activities away from efforts that use EA/EA-related framings and towards projects that instead focus on the constituent causes. In March 2023, Open Philanthropy\u2019s Alexander Berger invited Claire Zabel (Open Phil), James Snowden (Open Phil), Max Dalton (CEA), Nicole Ross (CEA), Niel Bowerman (80k), Will MacAskill (GPI), and myself (Open Phil, staffing the group) to join a working group on this and related questions.</p></blockquote><p>In the proposals discussed, was the idea that non-AI-related causes would decrease the share of support they received from current levels? Or would eg the EAG replacement process be offset by making one of the others non-AI focused (or increase the amount of support those causes received in some other way)?</p><blockquote><p>There was significant disagreement whether 80k (which was chosen as a concrete example to shed light on a more general question that many meta-orgs run into) should be more explicit about its focus on longtermism/existential risk.&nbsp;</p></blockquote><p>I have to say, this really worries me. It seems like it should be self-evidently good after FTX and all the subsequent focus on honesty and virtue that EA organisations should be as transparent as possible about their motivations. Do we know what the rationale of the people who disagreed was?</p>", "parentCommentId": null, "user": {"username": "Arepo"}}, {"_id": "EuKG5gbY98Ei7Qvpk", "postedAt": "2023-08-14T08:04:04.227Z", "postId": "3kMQTjtdWqkxGuWxB", "htmlBody": "<blockquote><p>I have to say, this really worries me.</p></blockquote><p>I can't speak for other people who filled out the survey but: I agree that orgs should be transparent about their motivations.&nbsp;</p><p>The questions asks (basically) \"should 80k be <i>more </i>transparent [than it currently is]\", and I think I gave a \"probably not\" type answer, because I think that 80k is already fairly transparent about this (e.g. it's pretty clear when you look at their problem profiles or whatever).</p>", "parentCommentId": "r2maaTpA8EH4LNute", "user": {"username": "Maxdalton"}}, {"_id": "bocNgFLfSN4guftty", "postedAt": "2023-08-14T10:28:10.305Z", "postId": "3kMQTjtdWqkxGuWxB", "htmlBody": "<p>Hey, I wasn\u2019t a part of these discussions, but from my perspective (web director at 80k), I think we are transparent about the fact that our work comes from a longtermist perspective that suggests that existential risks are the most pressing issues. The reason we try to present, which is also the true reason, is that we think these are the areas where many of our readers, and thereofre we, can make the biggest positive impact.</p><p>Here are some of the places we talk about this:</p><p>1. Our&nbsp;<a href=\"https://80000hours.org/problem-profiles/\"><u>problem profiles page&nbsp;</u></a>(one of our most popular pages) explicitly says we rank existential risks as most pressing (ranking AI first) and explains why - both at the very top of the page \"We aim to list issues where&nbsp;<a href=\"https://80000hours.org/articles/problem-framework/\"><u>each additional person can have the most positive impact</u></a>. So we focus on problems that others neglect, which are solvable, and which are unusually big in scale, often because they could affect&nbsp;<a href=\"https://80000hours.org/articles/future-generations/\"><u>many future generations</u></a> \u2014 such as&nbsp;<a href=\"https://80000hours.org/articles/existential-risks/\"><u>existential risks</u></a>. This makes our list different from those you might find elsewhere.\" and more in the&nbsp;<a href=\"https://80000hours.org/problem-profiles/#problems-faq\"><u>FAQ</u></a>, as well as in the problem profiles themselves.</p><p>2. We say at the top of our \"priority paths\" list that these are aimed at people who \"want to help tackle&nbsp;<a href=\"https://80000hours.org/problem-profiles/\"><u>the global problems we think are most pressing</u></a>\", linking back to the problems ranking.</p><p>3. We also have in-depth discussions of our views on&nbsp;<a href=\"https://80000hours.org/articles/future-generations/\"><u>longtermism</u></a> and the importance of&nbsp;<a href=\"https://80000hours.org/articles/existential-risks/\"><u>existential risk</u></a> in our advanced series.&nbsp;</p><p>So we are aiming to be honest about our motivations and problem prioritization, and I think we succeed. For what it's worth I don't often come across cases of people who have misconceptions about what issues we think are most pressing (though if you know of any such people please let me know!).&nbsp;</p><p>That said, I basically agree we could make these views more obvious! E.g. we don't talk much on the front page of the site or in our 'start here' essay or much at the beginning of the career guide. I'm open to thinking we should.&nbsp;</p><p>One way of interpreting the call to make our longtermist perspective more \u201cexplicit\": I think some people think we should <i>pitch</i> our career advice exclusively at longtermists, or people who already want to work on x-risk. We could definitely move further in this direction, but I think we have some good reasons not to, including:</p><ol><li>We think we offer a lot of value by introducing the ideas of longtermism and x-risk mitigation to people who aren\u2019t familiar with these ideas already, and <strong>making the case</strong> that they are important \u2013 so narrowly targeting an audience that already shares these priorities (a very small number of people!) would mean leaving this source of impact on the table.</li><li>We have a lot of materials that can be useful to people who want to do good in their careers but won't necessarily adopt a longtermist perspective. And insofar as having EA be \u201cbig tent\u201d is a good thing (which <a href=\"https://forum.effectivealtruism.org/posts/CEtKAP5Gr7QrTXHRW/on-focusing-resources-more-on-particular-fields-vs-ea-per-se\">I tend to think it is though am not that confident</a>), I'm happy 80k introduces a lot of people who will take different perspectives to EA.</li><li>We are <a href=\"https://forum.effectivealtruism.org/topics/cause-neutrality#:~:text=Cause%20neutrality%20(sometimes%20called%20cause,as%20saliency%20or%20personal%20attachment.\">cause neutral</a>[1] \u2013 we prioritise x-risk reduction because we think it's most pressing, but it\u2019s possible we could learn more that would make us change our priorities. Since we\u2019re open to that, it seems reasonable not to fully tie our brand to longtermism or existential risk. It might even be misleading to open with x-risk, since it'd fail to communicate that we are prioriritsing that <i>because</i> of our views about the pressingess of existential risk reduction. And since the value proposition of our site for readers is in part to help them have more impact, I think they <i>want</i> to know which issues we think are most pressing.</li></ol><p>[1] Contrast with unopinionated about causes. Cause neutrality in this usage means being open to prioritising whatever causes you think will allow you to help others the most, which you might have an opinion on.</p>", "parentCommentId": "r2maaTpA8EH4LNute", "user": {"username": "Ardenlk"}}, {"_id": "ZJqnTnpQ3E2ru3uqa", "postedAt": "2023-08-14T19:43:12.460Z", "postId": "3kMQTjtdWqkxGuWxB", "htmlBody": "<p>I appreciate the open communication shared in your post. However, I'd like to express a few reservations regarding the makeup of the working group. I've observed that a significant portion comprises either current or former trustees and senior executives from Effective Ventures. Considering that this organization has faced challenges in management and is presently under the scrutiny of the Charity Commission, this does raise concerns. Moreover, the absence of a representative from the animal welfare sector is noteworthy. While I recognize that the funding is derived from OP's own resources, the outcomes have broad implications for the EA community. For many, it could influence pivotal career decisions. Thus, the responsibility associated with such initiatives cannot be overstated.</p>", "parentCommentId": null, "user": {"username": "anolderea"}}, {"_id": "StGKu5ahyAc7TCmHK", "postedAt": "2023-08-14T20:15:35.176Z", "postId": "3kMQTjtdWqkxGuWxB", "htmlBody": "<p>\"E.g. we don't talk much on the front page of the site or in our 'start here' essay or much at the beginning of the career guide. I'm open to thinking we should. \"</p>\n<p>I agree with this, and feel like the best transparent approach might be to put your headline findings on the front page and more clearly, because like you say you do have to dig a surprising amount to find your headline findings.</p>\n<p>Something like (forgive the average wording)</p>\n<p>\"We think that working on longtermists causes is the best way to do good, so check these out here...\"</p>\n<p>Then maybe even as a caveat somewhere (blatant near termist plug) \"some people believe  near termist causes are the most important, and others due to their skills or life stage may be in a better position to work on  near term causes. If you're interested in learning more about high impact near termist causes check these out here ..\"</p>\n<p>Obviously as a web manager  you could do far better with the wording but you get my drift!</p>\n", "parentCommentId": "bocNgFLfSN4guftty", "user": {"username": "NickLaing"}}, {"_id": "vPWwhJCdhM6Hn88yt", "postedAt": "2023-08-15T07:44:30.322Z", "postId": "3kMQTjtdWqkxGuWxB", "htmlBody": "<p>Thanks : ) we might workshop a few ways of getting something about this earlier in the user experience.</p>\n", "parentCommentId": "StGKu5ahyAc7TCmHK", "user": {"username": "Ardenlk"}}, {"_id": "PoEJ8z2KcSFqdmrYi", "postedAt": "2023-08-17T09:31:01.996Z", "postId": "3kMQTjtdWqkxGuWxB", "htmlBody": "<p>Thanks for the post!</p><blockquote><p>There was consensus that it would be good if CEA replaced one of its (currently) three annual conferences with a conference that\u2019s explicitly framed as being about x-risk or AI-risk focused conference.<br><br>In response to a corresponding prompt (\u201c \u2026 at least one of the EAGs should get replaced by an x-risk or AI-risk focused conference \u2026\u201d)</p></blockquote><p>I'm curious if you felt the thrust was that the group thought it's good if <i>CEA in particular</i> replace the activity of running its 3rd EAG with running an AI safety conference, or that there should be<i> an </i>AI safety conference?</p><p>In general when we talk about 'cause area specific field building', the purpose that makes most sense to me is to build a community <i>around those cause areas, </i>which people who don't buy the whole EA philosophy can join if they spot a legible cause they think is worthwhile working on.</p><p>I'm a little hesitant to default to repurpose existing EA institutions, communities and events to house the proposed cause area specific field building. It seems to me that the main benefit of cause area specific field building is to potentially build something new, fresh and separate from the other cultural norms and beliefs that the EA community brings with it.</p><p>Perhaps the crux for me is \"is this a conference for <i>EAs</i> interested in AI safety, or is it a conference for <i>anyone</i> interested in AI safety?\" If the latter, this points away from an EA-affiliated conference (though I appreciate there are pragmatic questions around \"who else would do it\"). A fresh feel and new audience might still be achievable in the case that CEA runs the conference ops, but I imagine it would be important to bear in mind during CEA's branding, outreach and choices made during the execution of such a conference.</p>", "parentCommentId": null, "user": {"username": "j_bernardi"}}, {"_id": "Yhv5qKs8PBFJWBEoB", "postedAt": "2023-08-18T15:27:00.697Z", "postId": "3kMQTjtdWqkxGuWxB", "htmlBody": "<p>Agree. Inviting at least one person from a major neartermist organisation in EA such as Charity Entrepreneurship would have been helpful, to represent all \"non-longtermists\" in EA.</p>\n<p>(disclaimer: I'm friends with some CE staff but not affiliated with the org in any way, and I lean towards longtermism myself)</p>\n<p>Also appreciate the transparency, thanks Bastian!</p>\n", "parentCommentId": "ZJqnTnpQ3E2ru3uqa", "user": {"username": "Manuel_Allgaier"}}, {"_id": "JQ43uNaSQCStJh7oS", "postedAt": "2023-08-24T01:19:29.125Z", "postId": "3kMQTjtdWqkxGuWxB", "htmlBody": "<p>Some added context on the 80k podcasts:<br><br>At the beginning of the <a href=\"https://80000hours.org/podcast/episodes/jan-leike-superalignment/\">Jan Leike episode</a>, Rob says:</p><blockquote><p><br>Two quick notes before that:</p><p>We\u2019ve had a lot of AI episodes in a row lately, so those of you who aren\u2019t that interested in AI or perhaps just aren\u2019t in a position to work on it, might be wondering if this is an all AI show now.</p><p>But don\u2019t unsubscribe because we\u2019re working on plenty of non-AI episodes that I think you\u2019ll love \u2014 over the next year we plan to do roughly half our episodes on AI and AI-relevant topics, and half on things that have nothing to do with AI.</p><p>What happened here is that in March it hit Keiran and Luisa and me that so much very important stuff had happened in the AI space that had simply never been talked about on the show, and we\u2019ve been working down that coverage backlog, which felt pretty urgent to do.</p><p>But soon we\u2019ll get back to a better balance between AI and non-AI interviews. I\u2019m looking forward to mixing it up a bit myself.</p></blockquote><p><br>&nbsp;</p>", "parentCommentId": "sDzouq4QLEnqzegSg", "user": {"username": "ChanaMessinger"}}, {"_id": "CF9QrPGdydRBzD8ei", "postedAt": "2023-09-19T01:03:05.101Z", "postId": "3kMQTjtdWqkxGuWxB", "htmlBody": "<blockquote>\n<p>That said, I basically agree we could make these views more obvious! E.g. we don't talk much on the front page of the site or in our 'start here' essay or much at the beginning of the career guide. I'm open to thinking we should.</p>\n</blockquote>\n<p>Update: we added some copy on this to our <a href=\"https://80000hours.org/about/\">'about us' page</a>, the <a href=\"https://80000hours.org/\">front page</a> where we talk about 'list of the world' most pressing problems', our <a href=\"80000hours.org/start-here/\">'start here' page</a>, and the <a href=\"https://80000hours.org/career-guide/introduction/\">introduction to our career guide</a>.</p>\n", "parentCommentId": "bocNgFLfSN4guftty", "user": {"username": "Ardenlk"}}, {"_id": "eHxcae9cBSzaAdJ7x", "postedAt": "2023-09-19T01:04:03.266Z", "postId": "3kMQTjtdWqkxGuWxB", "htmlBody": "<p>Copying from my comment above:</p><p>Update: we've now added some copy on this to our <a href=\"https://80000hours.org/about/\">'about us' page</a>, the <a href=\"https://80000hours.org/\">front page</a> where we talk about 'list of the world' most pressing problems', our <a href=\"https://forum.effectivealtruism.org/posts/3kMQTjtdWqkxGuWxB/80000hours.org/start-here/\">'start here' page</a>, and the <a href=\"https://80000hours.org/career-guide/introduction/\">introduction to our career guide</a>.</p>", "parentCommentId": "StGKu5ahyAc7TCmHK", "user": {"username": "Ardenlk"}}]