[{"_id": "yuCcvzPJrQoRBAusg", "postedAt": "2022-09-09T01:07:06.009Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<p>I think AI alignment isn't really about designing AI to maximize for the preference satisfaction of a certain set of humans. I think an aligned AI would look more like an AI which:</p><ul><li>is not trying to cause an existential catastrophe or take control of humanity</li><li>has had undesirable behavior trained out or adversarially filtered</li><li>learns from human feedback about what behavior is more or less preferable<ul><li>In this case, we would hope the AI would be aligned to the people who are allowed to provide feedback</li></ul></li><li>has goals which are corrigible</li><li>is honest, non-deceptive, and non-power-seeking</li></ul>", "parentCommentId": null, "user": {"username": "michaelchen"}}, {"_id": "4rFftACQiByyA53aJ", "postedAt": "2022-09-09T02:01:19.771Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<p>Hi mic,</p><p>I understand that's how 'alignment' is normally defined in AI safety research.&nbsp;</p><p>But it seems like such a narrow notion of alignment that it glosses over almost all of the really hard problems in real AI safety -- which concern the very real conflicts between the humans who will be using AI.</p><p>For example, if the AI is aligned 'to the people who are allowed to provide feedback' (eg the feedback to a CIRL system), that raises the question of who is actually going to be allowed to provide feedback. For most real-world applications, deciding that issue is tantamount to deciding which humans will be in control of that real-world domain -- and it may leave the AI looking very 'unaligned' to all the other humans involved.</p>", "parentCommentId": "yuCcvzPJrQoRBAusg", "user": {"username": "geoffreymiller"}}, {"_id": "HkyPRGBGAockCGmKz", "postedAt": "2022-09-09T02:38:26.375Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<p>The closest thing that comes to mind is Critch's work on multi-user alignment, e.g. <a href=\"https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic\">What Multipolar Failure Looks Like, and Robust Agent-Agnostic Processes (RAAPs)</a>.</p>", "parentCommentId": null, "user": {"username": "plex"}}, {"_id": "eJtN3GFkpMwa2NkH2", "postedAt": "2022-09-09T02:43:28.789Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<p>Here are a couple of other links that come to mind:</p><p><a href=\"https://arxiv.org/abs/2008.02275\">https://arxiv.org/abs/2008.02275</a></p><p><a href=\"https://www.brookings.edu/research/aligned-with-whom-direct-and-social-goals-for-ai-systems/\">https://www.brookings.edu/research/aligned-with-whom-direct-and-social-goals-for-ai-systems/</a></p>", "parentCommentId": "HkyPRGBGAockCGmKz", "user": {"username": "blonergan"}}, {"_id": "qt69uFpbvyod85QKe", "postedAt": "2022-09-09T02:43:38.593Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<p>I agree, that seems concerning. Ultimately, since the AI developers are designing the AIs, I would guess that they would try to align the AI to be helpful to the users/consumers or to the concerns of the company/government, if they succeed at aligning the AI at all. As for your suggestions \"Alignment with whoever bought the AI? Whoever users it most often? Whoever might be most positively or negatively affected by its behavior? Whoever the AI's company's legal team says would impose the highest litigation risk?\" \u2013 these all seem plausible to me.</p><p>On the separate question of handling conflicting interests: there's some work on this (e.g., \"<a href=\"https://arxiv.org/abs/2006.09519\">Aligning with Heterogeneous Preferences for Kidney Exchange</a>\" and \"<a href=\"https://arxiv.org/pdf/2201.00012.pdf\">Aligning AI with Human Norms through Multi-Objective Reinforced Active Learning</a>\"), though perhaps not as much as we would like.</p>", "parentCommentId": "4rFftACQiByyA53aJ", "user": {"username": "michaelchen"}}, {"_id": "ZE3YdeMmrmy7J5KqA", "postedAt": "2022-09-09T03:20:38.975Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<p>(Edit: Accidentally posted a duplicate link.)</p><p><a href=\"https://www.brookings.edu/research/aligned-with-whom-direct-and-social-goals-for-ai-systems/\">Aligned with whom?</a> by Anton Korinek and Avital Balwit (2022) has a possible answer. They write that an aligned AI system should have</p><ul><li>direct alignment with its operator, and</li><li>social alignment with society at large.</li></ul><p>Some examples of failures in direct and social alignment are provided in <a href=\"https://www.brookings.edu/research/why-we-need-a-new-agency-to-regulate-advanced-artificial-intelligence-lessons-on-ai-control-from-the-facebook-files/\">Why we need a new agency to regulate advanced artificial intelligence: Lessons on AI control from the Facebook Files</a> (Korinek, 2021).</p><p>We could expand the moral circle further by aligning AI with the interests of both human and non-human animals. Direct, social and sentient alignment?</p><p>As you mentioned, these alignments present conflicting interests that need mediation and resolution.</p>", "parentCommentId": null, "user": {"username": "Michael Huang"}}, {"_id": "5HcMjjukov39KRL2m", "postedAt": "2022-09-09T03:53:38.432Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<blockquote>\n<p>But it seems like such a narrow notion of alignment that it glosses over almost all of the really hard problems in real AI safety -- which concern the very real conflicts between the humans who will be using AI.</p>\n</blockquote>\n<p>I very much agree these these political questions matter, and that alignment to multiple humans is conceptually pretty shaky; thanks for bringing up these issues. Still, I think some important context is that many AI safety researchers think that it's a hard, unsolved problem to just keep future powerful AI systems from causing many deaths (or doing other unambiguously terrible things). They're often worried that CIRL and every other approach that's been proposed will completely fail. From that perspective, it no longer looks like almost all of the really hard problems are about conflicts between humans.</p>\n<p>(On CIRL, here's a <a href=\"https://twitter.com/RichardMCNgo/status/1539891460085927936\">thread</a> and a longer <a href=\"https://arbital.com/p/updated_deference/\">writeup</a> on why some think that \"it almost entirely fails to address the core problems\" of AI safety.)</p>\n", "parentCommentId": "4rFftACQiByyA53aJ", "user": {"username": "Mauricio"}}, {"_id": "ejKXLwYweDTmpwNbJ", "postedAt": "2022-09-09T09:09:15.859Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<p>You claimed that your starting question was naive, so allow me to respond with similar naivete:&nbsp;</p><p>If AI become smart enough to perform behaviors that we consider potentially threatening or beyond our control, aren't they really artificial life?</p><p>As such, and imbued with consciousness equal to or greater than our own, don't we consider them to have rights or legal protections or freedoms?</p><p>If they did, they would also be subject to legal restrictions on their behavior, in similar fashion to human beings. However, with additional freedoms, those legal restrictions would be inadequate to punish their behavior. As a consequence, we face an ethical challenge in how we integrate such life into our society. We don't want to be unfair, but we prefer them as servants than as equals or superiors.</p><p>The continued development of AI could reflect <a href=\"https://arxiv.org/pdf/2201.11214.pdf\">techno-utopianism</a>, or <a href=\"https://fleishmanhillard.co.uk/2017/04/techmunch-technology-driving-change-people-drive-technology/#:~:text=At%20its%20most%20basic%20level,constructs%20alone%20determine%20individual%20behaviour.\">technological determinism</a> (marketing and memes), leading everyone to a condition in which the actual motives of people paying for it all are really poorly thought out and short-term, but the larger vision looks more attractive than it is.</p>", "parentCommentId": null, "user": {"username": "Noah Scales"}}, {"_id": "LmskthAJKvxW8HYPi", "postedAt": "2022-09-09T14:46:02.270Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<p>It does seem like alignment researchers often focus on the case of aligning AI to a single human. Here are some views that might explain this. I think these views are at least somewhat common among alignment researchers.</p>\n<ul>\n<li>\n<p>Aligning with a single human contains most of the difficulty of the problem of aligning with groups of humans. Once we figure out how to align AI with a single human, figuring out how to align it with groups of humans will be relatively easy. We should focus on the hard part first, which is aligning AI with a single human. (<em>edit</em>: I am not saying that aligning with a single human is harder than aligning with groups of humans. See also my comment below.)</p>\n</li>\n<li>\n<p>If AI is aligned with a single random human, this is still much better than unaligned AI. Therefore this kind of research is very valuable.</p>\n</li>\n<li>\n<p>If the AI acts according to the <a href=\"https://www.lesswrong.com/tag/coherent-extrapolated-volition\">CEV</a> of a single random human, then the results will be probably good for humanity as a whole.</p>\n</li>\n</ul>\n", "parentCommentId": null, "user": {"username": "harfe"}}, {"_id": "nYZxjAnWiTXDzgPfp", "postedAt": "2022-09-09T16:49:12.415Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<p>harfe - I'm not convinced that aligning with a single human is much harder than aligning with a group of humans that have diverse and partly conflicting interests. &nbsp;Single human preferences can already be learned pretty well by product recommendation engines, but group preferences are much more complicated.</p><p>We already know from game theory that there is no way, even in principle, for a single agent (such as an AI) to represent and enact the collective interests of a group that doesn't actually have internally aligned collective interests. The only exceptions to this are edge cases like pure coordination games (e.g. which side of the road to drive on, left or right).&nbsp;</p><p>My concern is that if we think we've solved the single-human alignment problem, we'll be tempted to scale AI systems up to try to reflect the general preferences of human groups (or humanity in general) -- but that this will simply not be possible, given that groups, and even the human species itself, does not actually have collectively aligned interests (even the principle of 'don't let AI drive us extinct' won't seem aligned with the agendas of the Voluntary Human Extinction movement, the anti-natalists, the Earth First eco-activists, the religious extremists expecting imminent messiahs or Raptures, or the depressed nihilists.)</p><p>And, if group alignment isn't possible, we'll end up in a situation where whichever subgroup has the most direct control over AI design, training, and feedback will end up being basically in control of everybody else.</p>", "parentCommentId": "LmskthAJKvxW8HYPi", "user": {"username": "geoffreymiller"}}, {"_id": "pYx5CGEn2CDf3ha23", "postedAt": "2022-09-09T20:45:20.893Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<blockquote>\n<p>I'm not convinced that aligning with a single human is much harder than aligning with a group of humans that have diverse and partly conflicting interests.</p>\n</blockquote>\n<p>I did not claim that aligning with a single human is harder than aligning with a group of humans (nor have I claimed that others believe that). I have probably expressed myself poorly, if that was the impression after reading my comment. In fact, I believe the opposite!</p>\n<p>Let my make another attempt at explaining.</p>\n<ul>\n<li><strong>A</strong>: Figuring out how to align an AGI with a single human.</li>\n<li><strong>B</strong>: Figuring out how to align an AGI with a group of human.</li>\n<li><strong>C</strong>: Doing <strong>B</strong> after you have completed <strong>A</strong>.</li>\n</ul>\n<p>Then, for the difficulties of these, I currently believe</p>\n<ul>\n<li>all three of <strong>A</strong>, <strong>B</strong>, <strong>C</strong> are hard</li>\n<li><strong>B</strong> is harder than <strong>A</strong></li>\n<li><strong>B</strong> is harder than <strong>C</strong></li>\n<li><strong>A</strong> is much harder than <strong>C</strong> (this was what I was trying to state in the comment above)</li>\n<li>A reasonable strategy for doing <strong>B</strong> would be to do <strong>A</strong>, and then do <strong>C</strong> (I am not super confident here, and things might be much more complex)</li>\n<li>If you do both <strong>A</strong> and <strong>C</strong>, it is better to first focus on <strong>A</strong> (and put more resources into it), because <strong>A</strong> is harder than <strong>C</strong>.</li>\n</ul>\n<p>I would be curious what other people think. My current guess would be that at least some alignment researchers believe these (or a part of these) points too. I do not recall hearing opposing viewpoints.</p>\n<p>I do not believe that, for example, the author of the <a href=\"https://www.lesswrong.com/posts/WcWzLSn8ZjJhCZxP4/predca-vanessa-kosoy-s-alignment-protocol\">PreDCA alignment proposal</a> wishes that the values of a random human are imposed (via AGI) on the rest of humanity, even though PreDCA (currently) is a protocol that aligns AGI with a single human (called \"user\").</p>\n", "parentCommentId": "nYZxjAnWiTXDzgPfp", "user": {"username": "harfe"}}, {"_id": "wdp5viXy4ohmAqX4u", "postedAt": "2022-09-09T21:07:52.920Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<p>Hi harfe, thanks for this helpful clarification.&nbsp;</p><p>I'd agree that A, B, and C seem hard; that B is harder than A, and that B is harder than C.</p><p>Where we disagree is that I suspect that C is harder than A, for basic game-theoretic reasons I mentioned in the original post.&nbsp;</p><p>I'm also not confident that C is a whole lot easier than B -- I'm not sure that alignment with individual humans will actually give us all that much help in doing alignment with complicated groups of humans.</p><p>But, I need to think further about this, and do some more readings!</p>", "parentCommentId": "pYx5CGEn2CDf3ha23", "user": {"username": "geoffreymiller"}}, {"_id": "jcJFbbpKiL8MGwCpv", "postedAt": "2022-11-19T01:25:34.671Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<p>This is a great post.&nbsp;</p><p>Law is the best solution I can think to address the issues you raise.&nbsp;</p><ul><li>Here <a href=\"https://forum.effectivealtruism.org/posts/9YLbtehKLT4ByLvos/agi-misalignment-x-risk-may-be-lower-due-to-an-overlooked\">https://forum.effectivealtruism.org/posts/9YLbtehKLT4ByLvos/agi-misalignment-x-risk-may-be-lower-due-to-an-overlooked</a> I argue that law-informed AI is likely the best path forward for societal alignment</li><li>Here <a href=\"https://forum.effectivealtruism.org/posts/4ykDJA57wstYWq9HK/intent-alignment-should-not-be-the-goal-for-agi-x-risk\">https://forum.effectivealtruism.org/posts/4ykDJA57wstYWq9HK/intent-alignment-should-not-be-the-goal-for-agi-x-risk</a> I explore the difference between intent alignment and societal alignment.</li></ul>", "parentCommentId": null, "user": {"username": "johnjnay"}}, {"_id": "uqahHGZmd5MY6rbyj", "postedAt": "2022-11-19T02:14:39.339Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<p>Thanks; appreciate the feedback, and for sharing these links.</p><p>I agree that AI alignment with actual humans &amp; groups needs to take law much more seriously as a legacy system for trying to manage 'misalignments' amongst actual humans and groups. New legal concepts may need to be invented -- but AI alignment shouldn't find itself in the hubristic position of trying to reinvent law from scratch.</p>", "parentCommentId": "jcJFbbpKiL8MGwCpv", "user": {"username": "geoffreymiller"}}, {"_id": "LWCikf2pPN7LEuiNK", "postedAt": "2022-11-19T19:22:53.061Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<p>I think AI alignment can draw from existing law to a large degree. New legal concepts may be needed but I think there is a lot legal reasoning, legal concepts, legal methods, etc. that are directly applicable now (discussed in more detail here https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4218031).</p><p>Also, I think we should keep the involvement of AI in law-making (broadly defined) as limited as we can. And we should train AI to understand when there is sufficient legal uncertainty that a human is needed to resolve the correct action to be taken.</p>", "parentCommentId": "uqahHGZmd5MY6rbyj", "user": {"username": "johnjnay"}}, {"_id": "8PndFHtS9ZaEPBDt2", "postedAt": "2023-03-17T19:31:25.544Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<p>I agree with Miller's response to mic (6-mos ago). Is it even possible for us to stop avoiding the \"hard problem\" of human nature?</p><p>Also, any given agent's or interest group's priorities and agendas always will be dynamic, compounding the problem of maintaining multiple mutually satisfactory alignments. Natural selection has designed us to exhibit complex contingent responsiveness to both subtle and dramatic environmental contingencies.</p><p>In addition, the humans providing the feedback, even if THEY can find sustainable alignment amongst themselves (remember they are all reproductive competitors, and deeply programmed by natural selection to act accordingly, intentionally or not), will change over time, possibly a very short time, <i>by being exposed to such power</i>. They will be corrupted by that power, and in due time corrupted absolutely.</p><p>Finally, an important Darwinian sub-theory has to do with the problem of nonconscious self-deception. I have to wonder whether even our <i>discussing</i> the <i>possibility</i> of properly managing substantive AI systems is just a way of obscuring from ourselves the utter impossibility, given human nature, of managing them properly, morally, wisely? Are all these conversations a way (a kind of competition) to convince ourselves and others that we or our (perceived) allies deserve the power to program and maintain these systems?</p>", "parentCommentId": null, "user": {"username": "Linyphia"}}, {"_id": "BPjdjN4Cgg8MDWdxN", "postedAt": "2023-03-17T22:00:21.350Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<p>Linyphia -- totally agree (unsurprisingly!).&nbsp;</p><p>You raise good additional points about the dynamism and unpredictability of human values and preferences. Some of that unpredictability may reflect <i>adaptive unpredictability</i> (what biologists call 'protean behavior') that makes it harder for evolutionary enemies and rivals to predict what one's going to do next. I discuss this issue extensively in this 1997 <a href=\"https://www.primalpoly.com/s/1997-protean-primates.pdf\">chapter </a>and this 1996 simulation <a href=\"https://www.primalpoly.com/s/1996-human-simulation.pdf\">study</a>. Insofar as human values are somewhat adaptively unpredictable by design, for good functional reasons, it will be very hard for reinforcement learning systems to get a good 'fix' on our preferences.</p><p>The other issues of adaptive self-deception (e.g. virtue signaling, as discussed in my 2019 <a href=\"https://www.primalpoly.com/virtue-signaling-2019\">book</a> on the topic) about our values, and about AI power corrupting humans, also deserve much more attention in AI alignment work, IMHO.</p>", "parentCommentId": "8PndFHtS9ZaEPBDt2", "user": {"username": "geoffreymiller"}}, {"_id": "g9bSGrgjFgBfYhcXt", "postedAt": "2023-04-26T15:59:10.713Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<p>I think the original <a href=\"https://intelligence.org/files/CEV.pdf\">CEV paper</a> from 2003 addresses (or at least discusses) a lot of these concerns. Basically, the thing that a group attempting to build an aligned AI <i>should</i> try to align it with is the collective CEV of humanity, not any individual humans.</p><p>On anti-natalism, religious extremism, voluntary extinction, etc. - &nbsp;if those values end up being stable under reflection, faster and more coherent thinking, and don't end up dominated by other values of the people who hold them, then the Future may indeed include things which satisfy or maximize those values.</p><p>(Though those values, and the people that hold them don't necessarily get <i>more</i> say than people who believe the opposite. If some interests and values are truly irreconcilable, &nbsp;a compromise might look like dividing up chunks of the lightcone.)</p><p><br>Of course, the first group who attempts to build a super-intelligence might try to align it with something else - their own personal CEV (which may or may not have a component for the collective CEV of humanity), or some kind of equal or unequal split between the individual CEVs of every human, or every sentient, etc. or something else entirely.</p><p>This would be inadvisable for various reasons discussed in the paper, and I agree it is a real danger / problem. (Mostly though, I think anyone who tries to build any kind of CEV sovereign right now just fails, and we end up with tiny molecular squiggles.)</p>", "parentCommentId": "nYZxjAnWiTXDzgPfp", "user": {"username": "Max H"}}, {"_id": "6v654SBPzQDcTLh3G", "postedAt": "2023-04-26T21:01:17.442Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<p>Max - thanks for the reply. I'm familiar with the CEV concept. But I don't see how it helps solve any of the hard problems in alignment that involve any conflicts of interest between human individuals, groups, corporations, or nation-states. It just sweeps all of those conflicts of interest under the rug.</p><p>In reality, corporations and nation-states won't be building AIs to embody the collective CEV of humanity. They will build AIs to embody the profit-making or geopolitical interests of the builders.&nbsp;</p><p>We can preach at them that their AIs <i>should</i> embody humanity's collective CEV. But they would get no comparative advantage from doing so. It wouldn't help promote their group profit or power. It would be a purely altruistic act. So, given the current state of for-profit corporate governance, and for-power nation-state governance, that seems very unlikely.</p>", "parentCommentId": "g9bSGrgjFgBfYhcXt", "user": {"username": "geoffreymiller"}}, {"_id": "CJjDW3kqvTonksTwd", "postedAt": "2023-04-26T22:56:26.478Z", "postId": "DXuwsXsqGq5GtmsB3", "htmlBody": "<blockquote><p>So, given the current state of for-profit corporate governance, and for-power nation-state governance, that seems very unlikely.</p></blockquote><p>Yep. I think in my ideal world, there would be exactly one <a href=\"https://www.lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects\">operationally adequate</a> organization permitted to build AGI. Membership in that organization would require a credible pledge to altruism and a test of oath-keeping ability.</p><p>Monopoly power of this organization to build AGI would be enforced by a global majority of nation states, with monitoring and deterrence against defection.</p><p>I think a stable equilibrium of that kind is possible in principle, though obviously we're pretty far away from it being anywhere near the Overton Window. (For good reason - it's a scary idea, and probably ends up looking pretty dystopian when implemented by existing Earth governments. Alas! Sometimes draconian measures really are necessary; reality is not always nice.)</p><p>In the absence of such a radically different global political order we might have to take our chances on the hope that the decision-makers at OpenAI, Deepmind, Anthropic, etc. will all be reasonably nice and altruistic, and not power / profit-seeking. Not great!<br><br>There might be worlds in between the most radical one sketched above and our current trajectory, but I worry that any \"half measures\" end up being ineffective and costly and worse than nothing, mirroring many countries' approach to COVID lockdowns.</p>", "parentCommentId": "6v654SBPzQDcTLh3G", "user": {"username": "Max H"}}]