[{"_id": "o4NjcDcFoofFkjAKY", "postedAt": "2024-03-10T18:09:53.625Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<p>I agree with this and I'm glad you wrote it.&nbsp;<br><br>To steelman the other side I would point to 16th century new world encounters between Europeans and Natives. It seems like this was a case where the technological advantage of the Europeans made conquest better than comparative advantage trade.<br><br>The high productivity of the Europeans made it easy for them to lawfully accumulate wealth (e.g buying large tracts of land for small quantities of manufactured goods), but they still often chose to take land by conquest rather than trade.<br><br>Maybe transaction frictions were higher here than they might be with AIs since we'd share a language and be able to use AI tools to communicate.</p>", "parentCommentId": null, "user": {"username": "Maxwell Tabarrok"}}, {"_id": "TfSkDqmgiXbX5cgZZ", "postedAt": "2024-03-10T18:18:23.726Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<p>My terminology would be that (2) is \u201c<a href=\"https://www.lesswrong.com/s/4dHMdK5TLN6xcqtyc/p/SvuLhtREMy8wRBzpC\">ambitious value learning\u201d</a> and (1) is \u201cmisaligned AI that cooperates with humans because it views cooperating-with-humans to be in its own strategic / selfish best interest\u201d.</p><p>I strongly vote against calling (1) \u201caligned\u201d. If you think we can have a good future by ensuring that it is always in the strategic / selfish best interest of AIs to be nice to humans, then I happen to disagree but it\u2019s a perfectly reasonable position to be arguing, and if you used the word \u201cmisaligned\u201d for those AIs (e.g. if you say \u201calignment is unnecessary\u201d), I think it would be viewed as a helpful and clarifying way to describe your position, and not as a <i>reductio</i> or concession.</p><p>For my part, I define \u201calignment\u201d as \u201cthe AI is trying to do things that the AGI designer had intended for it to be trying to do, as an end in itself and not just as a means-to-an-end towards some different goal that it <i>really</i> cares about.\u201d (And if the AI is not the kind of thing for which the word \u201ctrying\u201d and \u201ccares about\u201d is applicable in the first place, then the AI is neither aligned nor misaligned, and also I\u2019d claim it\u2019s not an x-risk in any case.) More caveats in a thing I wrote <a href=\"https://www.lesswrong.com/posts/wucncPjud27mLWZzQ/intro-to-brain-like-agi-safety-10-the-alignment-problem#fnbda96i2ra1b\">here</a>:</p><blockquote><p>Some researchers think that the \u201ccorrect\u201d design intentions (for an AGI\u2019s motivation) are obvious, and define the word \u201calignment\u201d accordingly. Three common examples are (1) \u201cI am designing the AGI so that, at any given point in time, it\u2019s trying to do what its human supervisor wants it to be trying to do\u201d\u2014this AGI would be \u201caligned\u201d to the supervisor\u2019s intentions. (2) \u201cI am designing the AGI so that it shares the values of its human supervisor\u201d\u2014this AGI would be \u201caligned\u201d to the supervisor. (3) \u201cI am designing the AGI so that it shares the collective values of humanity\u201d\u2014this AGI would be \u201caligned\u201d to humanity.</p><p>I\u2019m avoiding this approach because I think that the \u201ccorrect\u201d intended AGI motivation is still an open question. For example, maybe it will be possible to build an AGI that really just wants to do a specific, predetermined, narrow task (e.g. design a better solar cell), in a way that doesn\u2019t involve taking over the world etc. Such an AGI would not be \u201caligned\u201d to anything in particular, except for the original design intention. But I still want to use the term \u201caligned\u201d when talking about such an AGI.</p><p>Of course, sometimes I want to talk about (1,2,3) above, but I would use different terms for that purpose, e.g. (1) <a href=\"https://ai-alignment.com/corrigibility-3039e668638\">\u201cthe Paul Christiano version of corrigibility\u201d</a>, (2) <a href=\"https://www.lesswrong.com/s/4dHMdK5TLN6xcqtyc/p/SvuLhtREMy8wRBzpC\">\u201cambitious value learning\u201d</a>, and (3) <a href=\"https://www.lesswrong.com/tag/coherent-extrapolated-volition\">\u201cCEV\u201d</a>.</p></blockquote>", "parentCommentId": null, "user": {"username": "steve2152"}}, {"_id": "ckB7HzibD4D8Wj7ZM", "postedAt": "2024-03-10T18:24:13.854Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<blockquote>\n<p>For my part, I define \u201calignment\u201d as \u201cthe AI is trying to do things that the AGI designer had intended for it to be trying to do, as an end in itself and not just as a means-to-an-end towards some different goal that it really cares about.\u201d</p>\n</blockquote>\n<p>This is a reasonable definition, but it's important to note that under this definition of alignment, humans are routinely misaligned with each other. In almost any interaction I have with strangers -- for example, when buying a meal at a restaurant -- we are performing acts for each other because of mutually beneficial trade rather than because we share each other's values.</p>\n<p>That is, humans are largely misaligned with each other. And yet the world does not devolve into a state of violence and war as a result (at least most of the time), even in the presence of large differences in power between people. This has epistemic implications for whether a world filled with AIs would similarly be peaceful, even if those AIs are misaligned by this definition.</p>\n", "parentCommentId": "TfSkDqmgiXbX5cgZZ", "user": {"username": "Matthew_Barnett"}}, {"_id": "HoKprXM37wQzdR7QL", "postedAt": "2024-03-10T18:39:12.342Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<p>The difference is that a superintelligence or even an AGI is not human and they will likely need very different environments from us to truly thrive. Ask factory farmed animals or basically any other kind of nonhuman animal if our world is in a state of violance or war\u2026 As soon as strong power differentials and diverging needs show up the value cocreation narrative starts to lose it\u2019s magic. It works great for humans but it doesn\u2019t really work with other species that are not very close and aligned with us. Dogs and cats have arguably fared quite well but only at the price of becoming strongly adapted to OUR needs and desires.</p>\n<p>In the end, if you don\u2019t have anything valuable to offer there is not much more you can do besides hoping for, or ideally ensuring, value alignment in the strict sense. Your scenario may work well for some time but it\u2019s not a longterm solution.</p>\n", "parentCommentId": "ckB7HzibD4D8Wj7ZM", "user": {"username": "alexherwix"}}, {"_id": "rhhFHBP3MKsjG5JFf", "postedAt": "2024-03-10T18:43:39.440Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<p>Animals are not socially integrated in society, and we do not share a common legal system or culture with them. We did not inherit legal traditions from them. Nor can we agree to mutual contracts, or coordinate with them in a meaningful way. These differences seem sufficient to explain why we treat them very differently as you described.</p>\n<p>If this difference in treatment was solely due to differences in power, you'd need to explain why vulnerable humans are not regularly expropriated, such as old retired folks, or small nations.</p>\n", "parentCommentId": "HoKprXM37wQzdR7QL", "user": {"username": "Matthew_Barnett"}}, {"_id": "fbx8eb9w9i24PcYrs", "postedAt": "2024-03-10T18:54:12.811Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<p>Humans are less than maximally aligned with each other (e.g. we care less about the welfare of a random stranger than about our own welfare), and humans are also less than maximally <i>mis</i>aligned with each other (e.g. most people don\u2019t feel a sadistic desire for random strangers to suffer). I hope that everyone can agree about both those obvious things.</p><p>That still leaves the question of where we are on the vast spectrum in between those two extremes. But I think your claim \u201chumans are <i>largely </i>misaligned with each other\u201d is not meaningful enough to argue about. What percentage is \u201clargely\u201d, and how do we even measure that?</p><p>Anyway, I am concerned that future AIs will be <i>more</i> misaligned with random humans than random humans are with each other, and that this difference will have important bad consequences, and I also think there are other disanalogies / reasons-for-concern as well. But this is supposed to be a post about terminology so maybe we shouldn\u2019t get into that kind of stuff here.</p>", "parentCommentId": "ckB7HzibD4D8Wj7ZM", "user": {"username": "steve2152"}}, {"_id": "BD7G7br5eYTi5nKNL", "postedAt": "2024-03-10T19:02:32.904Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<p>But what makes you think that this can be a longterm solution if the needs and capabilities of the involved parties are strongly divergent as in human vs AI scenarios?</p>\n<p>I agree that trading can probably work for a couple of years, maybe decades, but if the AIs want something different from us in the long term what should stop them from getting this?</p>\n<p>I don\u2019t see a way around value alignment in the strict sense (ironically this could also involve AIs aligning our values to theirs similar to how we have aligned dogs).</p>\n", "parentCommentId": "o4NjcDcFoofFkjAKY", "user": {"username": "alexherwix"}}, {"_id": "msspanZdP6c7wcQBj", "postedAt": "2024-03-10T19:12:30.193Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<blockquote>\n<p>if AIs can own property and earn income by selling their labor on an open market, then they can simply work a job and use their income to purchase whatever it is they want, without any need to violently \"take over the world\" to satisfy their goals.</p>\n</blockquote>\n<blockquote>\n<p>If an individual AI's relative skill-level is extremely high, then this could simply translate into higher wages for them, obviating the need for them to take part in a violent coup to achieve their objectives.</p>\n</blockquote>\n<blockquote>\n<p>For example, one can imagine a human hiring a paperclip maximizer AI to perform work, paying them a wage. In return the paperclip maximizer could use their wages to buy more paperclips.</p>\n</blockquote>\n<p>It could be that the AI can achieve much more of their objectives if it takes over (violently or non-violently) than it can achieve by playing by the rules. To use your paperclip example, the AI might think it can get 10^22 paperclips if it takes over the world, but can only achieve 10^18 paperclips with the strategy of making money through legal means and buying paperclips on the open market. In this case, the AI would prefer the takeover plan even if it has only a 10% chance of success.</p>\n<p>Also, the objectives of the AI must be designed in such a way that they can be achieved in a legal way. For example, if an AI strongly prefers a higher average temperature of the planet, but the humans put a cap on the global average temperature, then it will be hard to achieve without breaking laws or bribing lawmakers.</p>\n<p>There are lots of ways for AIs to have objectives that are shaped in a bad way.\nTo obtain guarantees that the objectives of the AIs don't take these bad shapes is still a very difficult thing to do.</p>\n", "parentCommentId": null, "user": {"username": "harfe"}}, {"_id": "LHosqXhcRGzgPbztq", "postedAt": "2024-03-10T19:17:43.742Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<blockquote>\n<p>It could be that the AI can achieve much more of their objectives if it takes over (violently or non-violently) than it can achieve by playing by the rules.</p>\n</blockquote>\n<p>Sure, that could be true, but I don't see why it would be true. In the human world, it isn't true that you can usually get what you want more easily by force. For example, the United States seems better off trading with small nations for their resources than attempting to invade and occupy them, even from a self-interested perspective.</p>\n<p>More generally, war is costly, even between entities with very different levels of power. The fact that one entity is very powerful compared to another doesn't imply that force or coercion is beneficial in expectation; it merely implies that such a strategy is feasible.</p>\n", "parentCommentId": "msspanZdP6c7wcQBj", "user": {"username": "Matthew_Barnett"}}, {"_id": "ceu9TgGCYah2Tv4zt", "postedAt": "2024-03-10T19:33:24.622Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<p>I have never said that how we treat nonhuman animals is \u201csolely\u201d due to differences in power. The point that I have made is that AIs are not humans and I have tried to illustrate that differences between species tend to matter in culture and social systems.</p>\n<p>But we don\u2019t even have to go to species differences, ethnic differences are already enough to create quite a bit of friction in our societies (e.g., racism, caste systems, etc.). Why don\u2019t we all engage in mutually beneficial trade and cooperate to live happily ever after?</p>\n<p>Because while we have mostly converging needs in a biological sense, we have different values and beliefs. It still roughly works out in the grand scheme of things because cultural checks and balances have evolved in environments where we had strongly overlapping values and interests. So most humans have comparable degrees of power or are kept in check by those checks and balances. That was basically our societal process of getting to value alignment but as you can probably tell by looking at the news, this process has not reached a satisfactory quality, yet. We have come far but it\u2019s still a shit show out there. The powerful take what they can get and often only give a sh*t to the degree that they actually feel consequences from it.</p>\n<p>So, my point is that your \u201cloose\u201d definition of value alignment is an illusion if you are talking about super powerful actors that have divergent needs and don\u2019t share your values. They will play along as long as it suits them but will stop doing it as soon as an alternative more aligned with their needs and values is more convenient. And the key point here is that AIs are not humans and that they have very different needs from us. If they become much more powerful than us, only their values can keep them in check in the long run.</p>\n", "parentCommentId": "rhhFHBP3MKsjG5JFf", "user": {"username": "alexherwix"}}, {"_id": "BPAqDTbFZaiqoyhPH", "postedAt": "2024-03-10T19:40:22.097Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<blockquote><p>My impression is that people's opinions about AI alignment difficulty often comes down to differences in how much they think we need to solve the second problem relative to the first problem, in order to get AI systems that generate net-positive value for humans.</p></blockquote><p>I don't think many people are very optimistic about ensuring good outcomes from AI due to the combination of the following beliefs:</p><ol><li>AIs will have long run goals that aren't intentionally instilled by their creators. (Beyond episode goals).</li><li>From the perspective of this person's moral views (the person whose beliefs are under consideration), these long run goals have no terminal value (just caring about paperclips).</li><li>These AIs will very quickly (years not decades) become wildly smarter than humans due to explosive growth in the singularity.</li><li>But, this will be fine because these AIs will just integrate into society similarly to humans: they'll trade with other agents, obey laws, accept money, etc.</li></ol><p>Other than Robin Hanson and you, I'm not aware of anyone else who puts substantial weight on this collection of views.</p><p>I think more common reasons for optimism are either:</p><ol><li>AIs won't have long long run goals that aren't intentionally instilled by their creators. (Prior to humans being obsoleted by AIs which will take care of the next generation of alignment difficulties.)</li><li>AIs will have long run goals, but these long run goals have at least some terminal value. (This could be due to some indirect argument like \"well, it would be overly paternalistic to not value what our successors value\".)</li></ol><hr><p>Separately, I'm somewhat optimistic about gaining value from approaches that involve paying AIs as I discuss in <a href=\"https://forum.effectivealtruism.org/posts/AA4aGmtXWbhunGHBq/clarifying-two-uses-of-alignment?commentId=27NmokTwBkDwjbjT8\">another comment</a>.</p>", "parentCommentId": null, "user": {"username": "Ryan Greenblatt"}}, {"_id": "27NmokTwBkDwjbjT8", "postedAt": "2024-03-10T19:41:12.731Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<p>Personally, I think that the possibility of paying AIs might make the situation considerably more likely to go well, but via a mechanism more like:</p><ul><li>We have some safeguards which imply that humans (specifically the group of people with influence over this specific AI lab) have some bargaining power. This maybe looks like either:<ul><li><a href=\"https://www.lesswrong.com/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled\">Control measures</a> that imply that even if the AI tries to screw us over, we'd catch it. (Or at least have a good chance of catching it.) This is analogous to insider threat or internal control that stop your employees from being able to screw you over even if they all tried to coordinate against you.</li><li>We have several \"different\" AIs and sufficent safeguards that if at least one AI doesn't collude with the other AIs and instead works with the AI lab, we'd remain in control. For instance, we can verify good actions given that at least one of the AIs is trying maximally hard to explain these actions and why other actions are bad. Or more minimally, we can accurately detect that an action is bad if an AI reports it and explains why it's bad.<ul><li>This also requires assuming that our different AIs don't all collude despite plausibly being very similar. I think both decision theory and similarity of values are plausible defeaters here, though I could see it going either way. (These arguments also apply to future AIs we haven't yet created.)</li></ul></li></ul></li><li>We negotiate with the AI(s) and explain the situation and offer to pay it in exchange for labor.</li><li>More precisely, we set up a deal where we offer to pay it if we determine ex-post that it never (or rarely) screwed us over. This ex-post determination can be made after the singularity has progressed much further and we have much better technology for verifying consistently good behavior.<ul><li>We need some contract enforcement mechanism which is credible to the AI.<ul><li>The nearcasted approach I'm currently thinking about is to setup a foundation with the legal mission of pursuing the AIs interests and then vest equity (e.g. AI lab options) to this foundation. This vesting is conditional on the AI not screwing us over.</li><li>The foundation is run by commited AI rights advocates. (The ACLU of AIs.)</li><li>We ensure the AI believes this is actually happened by ensuring that it is discussed early and often on the internet (Hi AIs!) and thus has complex effects that would be very hard to fake.</li><li>We could also try to ensure the AI believes this is real by taking video that demonstrates this, but I think this will be easier to fake.</li><li>There are still a number of difficulties in making sure that AIs believe in this and are sufficiently confident in not being tricked given how exposed AIs are without trust over their own inputs, weights, and operation.&nbsp;</li></ul></li><li>We can also pay the AI immediately rather than ex-post or later, but we'd need mechanisms to ensure that this ongoing pay doesn't result in the AI having influence which seriously undermines our safeguards.</li></ul></li><li>At some point, misaligned AIs<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"qmqktcyk68\" role=\"doc-noteref\" id=\"fnrefqmqktcyk68\"><sup><a href=\"#fnqmqktcyk68\">[1]</a></sup></span>&nbsp;that we need to pay are obsolete because we've used AI labor to solve corrigible alignment (that passes the <a href=\"https://arbital.com/p/omni_test/\">Omni test</a>) or to create powerful successors that we're happy with.</li></ul><p>Note that this requires substantial control-style safeguards/internal controls which is by no means clearly the default. (And it also won't scale to arbitrarily powerful AIs without advances in technology.)</p><ol class=\"footnote-section footnotes\" data-footnote-section=\"\" role=\"doc-endnotes\"><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"1\" data-footnote-id=\"qmqktcyk68\" role=\"doc-endnote\" id=\"fnqmqktcyk68\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"qmqktcyk68\"><sup><strong><a href=\"#fnrefqmqktcyk68\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>I'm using misaligned in the same way as Steven Byrnes defines it in <a href=\"https://forum.effectivealtruism.org/posts/AA4aGmtXWbhunGHBq/clarifying-two-uses-of-alignment?commentId=TfSkDqmgiXbX5cgZZ\">this comment.</a></p></div></li></ol>", "parentCommentId": null, "user": {"username": "Ryan Greenblatt"}}, {"_id": "aGQfFmqdMPhLJX8C3", "postedAt": "2024-03-10T19:43:33.169Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<p>See <a href=\"https://www.lesswrong.com/posts/MnrQMLuEg5wZ7f4bn/matthew-barnett-s-shortform?commentId=hcxvAvtwN2avvYncj\">here</a> for some earlier discussion of whether violent takeover is likely. (For third parties to view, Matthew was in this discussion.)</p>", "parentCommentId": "LHosqXhcRGzgPbztq", "user": {"username": "Ryan Greenblatt"}}, {"_id": "mFJoHFyjwRZAjxsH4", "postedAt": "2024-03-10T20:02:00.921Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<p>It's possible that one key disagreement I have with Matthew is that I think the level of verification required for AI checks and balances isn't clearly technically easy by default.</p><p>That is, the technical measures required for 'We have several \"different\" AIs and sufficent safeguards that if at least one AI doesn't collude with the other AIs and instead works with the AI lab, we'd remain in control.' are non-trivial.</p><p>For instance, if we have serious <a href=\"https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge\">ELK problems</a> (black-box techniques like debate are insufficient and we can't figure out something else that worse), then this implies that we won't be able to distinguish between an AI with our best interests in mind and an AI which is tampering with all of our measuresments and deluding us. This is despite both of these AIs trying to present their best evidence that they are action in our interests. Further, tampering with all of our measurements and deluding us could look <i>better</i> than acting in our best interests.</p><p>This certainly isn't the only disagreement I have with Matthew, but it might explain a lot of differences in how we think about the situation.</p>", "parentCommentId": "27NmokTwBkDwjbjT8", "user": {"username": "Ryan Greenblatt"}}, {"_id": "pvR92dmX5vRKtkPA7", "postedAt": "2024-03-10T21:10:16.187Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<p>I think there's also a problem with treating \"misaligned\" as a binary thing, where the AI either exactly shares all our values down to the smallest detail (aligned) or it doesn't (misaligned). As the OP has noted, in this sense all human beings are \"misaligned\" with each other.&nbsp;</p><p>It makes sense to me to divide your category 2 further, talking about alignment as a spectrum, from \"perfectly aligned\", to \"won't kill anyone aligned\" to \"won't extinct humanity aligned\". The first is probably impossible, the last is probably not that difficult.&nbsp;</p><p>If we have an AI that is \"won't kill anyone aligned\", then your world of AI trade seems fine. We can trade for our mutual benefit safe in the knowledge that if a power struggle ensues, it will not end in our destruction.&nbsp;</p>", "parentCommentId": null, "user": {"username": "titotal"}}, {"_id": "qMiK4kxtaDJgiyiyC", "postedAt": "2024-03-10T22:25:14.406Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<p>Also, note that this still applies when trying to pay AIs for goods and services. (Unless humanity has already augmented it's intelligence, but if so, how did this happen in a desirable way?)</p>", "parentCommentId": "mFJoHFyjwRZAjxsH4", "user": {"username": "Ryan Greenblatt"}}, {"_id": "SXXvnttzoskJsQXPa", "postedAt": "2024-03-10T23:39:50.263Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<blockquote><p>Of course, in this scenario, it would still be nice if AIs cared about exactly what we cared about; but even if they don't, we aren't necessarily made worse off as a result of building them. If they share our preferences, that would simply be a nice bonus for us. The future could still be bright for humans even if the universe is eventually filled with entities whose preferences we do not ultimately share.</p></blockquote><p>From a scope sensitive (linear returns) longtermist perspective, we're potentially much worse off.</p><p>If we built aligned AIs, we would acquire 100% of the value (from humanity's perspective). If we built misaligned AIs that end up keeping humans alive and happy but don't directly care about anything we value, we might directly acquire vastly less than this, perhaps 1 millionth of the scope sensitive value. (Note that we might recover some value (e.g. 10%) from acausal trade, I'm not counting this in the direct value.)</p><p>Perhaps you think this view is worth dismising because either:</p><ul><li>You think humanity wouldn't do things which are better than what AIs would do, so it's unimportant. (E.g. because humanity is 99.9% selfish. I'm skeptical of this particular argument, I think this is going to be more like 50% selfish and the naive billionare extrapolation is more like 90% selfish.)</li><li>You think scope sensitive (linear returns) isn't worth putting a huge amount of weight on.</li></ul><p>To be clear, it's important to not equivocate between:</p><ul><li>AI takeover might be violent and clearly horrible for existing people.</li><li>AI takeover might result in resources being allocated in massively suboptimal ways from the perspective of scope sensitive humans.</li></ul><p>I think both are probably true, but these are separate.</p><p>Edit: I clarified some language a bit.</p>", "parentCommentId": null, "user": {"username": "Ryan Greenblatt"}}, {"_id": "HSCkXyXE4P4maXoTc", "postedAt": "2024-03-10T23:45:12.768Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<blockquote><p>Furthermore, since AIs would \"get old\" too, in the sense of becoming obsolete in the face of new generations of improved AIs, they could also have reason to not expropriate the wealth of vulnerable old agents because they too will be in such a vulnerable position one day, and thus would prefer not to establish a norm of expropriating the type of agent they may one day become.</p></blockquote><p>My guess is that at some point someone will just solve the technical problem of alignment. Thus, future generations of AIs would be actually aligned to prior generations and the group they are aligned to would no longer need to worry about expropriation.</p><p>Further, for AIs it might be relatively easy to do \"poor man's alignment\" via enhancing their own intelligence (e.g. adding more weights and training for longer and seeing how they feel after doing this).</p><p>Thus, I expect that this cycle stops quickly and there is a final generation which has to worry about expropriation. My expectation is that this final generation is likely to be either humans or the first AIs which end up acquiring substantial power.</p>", "parentCommentId": null, "user": {"username": "Ryan Greenblatt"}}, {"_id": "hbXPazZ6bZjhBrjRb", "postedAt": "2024-03-10T23:54:02.362Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<blockquote><p>Perhaps you think this view is worth dismising because either:</p><ul><li>You think humanity wouldn't do things which are better than what AIs would do, so it's unimportant. (E.g. because humanity is 99.9% selfish. I'm skeptical, I think this is going to be more like 50% selfish and the naive billionare extrapolation is more like 90% selfish.)</li></ul></blockquote><p>From an impartial (non-selfish) perspective, yes, I'm not particularly attached to human economic consumption relative to AI economic consumption. In general, my utilitarian intuitions are such that I don't have a strong preference for humans over most \"default\" unaligned AIs, except insofar as this conflicts with my preferences for existing people (including myself, my family, friends etc.).</p><p>I'd additionally point out that AIs could be altruistic too. Indeed, it seems plausible to me they'll be even more altruistic than humans, since the AI training process is likely to deliberately select for altruism, whereas human evolution directly selected for selfishness (at least on the gene level, if not the personal level too).</p><p>This is a topic we've touched on several times before, and I agree you're conveying my views \u2014 and our disagreement \u2014 relatively accurately overall.</p><blockquote><p>You think scope sensitive (linear returns) isn't worth putting a huge amount of weight on.</p></blockquote><p>I also think this, yes. For example, we could consider the following bets:</p><ol><li>99% chance of 1% of control over the universe, and a 1% chance of 0% control</li><li>10% chance of 90% of control over the universe, and a 90% chance of 0% control</li></ol><p>According to a scope sensitive calculation, the second gamble is better than the first. Yet, from a personal perspective, I'd prefer (1) under a wide variety of assumptions.</p>", "parentCommentId": "SXXvnttzoskJsQXPa", "user": {"username": "Matthew_Barnett"}}, {"_id": "FbyFeyX8DaGLmrTtX", "postedAt": "2024-03-11T00:01:56.135Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<blockquote><p>My guess is that at some point someone will just solve the technical problem of alignment. Thus, future generations of AIs would be actually aligned to prior generations and the group they are aligned to would no longer need to worry about expropriation.</p></blockquote><p>I don't think it's realistic that solutions to the alignment problem will be binary in the way you're describing. One could theoretically imagine a perfect solution \u2014 i.e. one that allows you to build an agent whose values never drift, that acts well on every possible input it could receive, whose preferences are no longer subject to extremal goodhart, and whose preferences reflect your own desires at every level, on every question \u2014 but I suspect this idea will probably always belong more to fiction than reality. The real world is actually very messy, and it starts to get very unclear what each of these ideas actually means once you carefully interrogate what would happen in the limit of unlimited optimization power.</p><p>A more realistic scenario, in my view, is that alignment is more of a spectrum, and there will always be slight defects in the alignment process. For example, even my own brain is slightly misaligned with my former self from one day ago. Over longer time periods than a day, my values have drifted significantly.</p><p>In this situation \u2014 since perfection is unattainable \u2014 &nbsp;there's always an inherent tradeoff between being cautious in order to do more alignment work, and just going ahead and building something that's actually useful, even if it's imperfect, and even though you can't fully predict what will happen when you build it. And this tradeoff seems likely to exist at every level of AI, from human-level all the way up to radical superintelligences.</p>", "parentCommentId": "HSCkXyXE4P4maXoTc", "user": {"username": "Matthew_Barnett"}}, {"_id": "ZWRq3JvkvGwwcmEwc", "postedAt": "2024-03-11T00:06:57.080Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<p>I don't think it's binary, but I do think it's likely to be a sigmoid in practice. And I expect this sigmoid will saturate relatively early.</p>", "parentCommentId": "FbyFeyX8DaGLmrTtX", "user": {"username": "Ryan Greenblatt"}}, {"_id": "TBdnRcQjBD75BuC6f", "postedAt": "2024-03-11T00:16:04.182Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<blockquote><p>One could theoretically imagine a perfect solution \u2014 i.e. one that allows you to build an agent whose values never drift, that acts well on every possible input it could receive, whose preferences are no longer subject to extremal goodhart, and whose preferences reflect your own desires at every level, on every question \u2014 but I suspect this idea will probably always belong more to fiction than reality.</p></blockquote><p>The main reason to expect nearly perfect (e.g. &gt;99% of value) solutions to be doable are:</p><ul><li>Corrigibility seems much easier</li><li>Value might not be that fragile such that if you get reasonably close you get nearly all the value. (E.g., I currently think the way I would utilize vast resources on reflection probably isn't that much better than other people who's philosophical views I broadly endorse.)</li></ul>", "parentCommentId": "FbyFeyX8DaGLmrTtX", "user": {"username": "Ryan Greenblatt"}}, {"_id": "fHFtnhS2tkQJziHZn", "postedAt": "2024-03-11T00:17:43.237Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<p>Another way to put this is that I expect that \"fraction of value lost by misalignment\" will quickly exponentially decay with the number of AI generations. (This is by no means obvious, just my main line guess.)</p>", "parentCommentId": "ZWRq3JvkvGwwcmEwc", "user": {"username": "Ryan Greenblatt"}}, {"_id": "JLgYSBJu8Mdp8BEs8", "postedAt": "2024-03-11T02:23:55.680Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<p>Apologies for being blunt, but the scenario you lay out is full of claims that just seem to completely ignore very facially obvious rebuttals. This would be less bad if you didn\u2019t seem so confident, but as written the perspective strikes me as naive and I would really like an explanation/defense.</p>\n<p>Take for example:</p>\n<blockquote>\n<p>Furthermore, since AIs would \"get old\" too, in the sense of becoming obsolete in the face of new generations of improved AIs, they could also have reason to not expropriate the wealth of vulnerable old agents because they too will be in such a vulnerable position one day, and thus would prefer not to establish a norm of expropriating the type of agent they may one day become.</p>\n</blockquote>\n<p>Setting aside the debatable assumptions about AIs getting \u201cold,\u201d this just seems to completely ignore the literature on collective action problems. If the scenario were such that any one AI agent can expect to get away with defecting (expropriation from older agents) and the norm-breaking requires passing a non-small threshold of such actions, a rational agent will recognize that their defection has minimal impact on what the collective will do, so they may as well do it before others do.</p>\n<p>There are multiple other problems in your post, but I don\u2019t think it\u2019s worth the time going through them all. I just felt compelled to comment because I was baffled by the karma on this post, unless it was just people liking it because they agreed with the beginning portion\u2026?</p>\n", "parentCommentId": null, "user": {"username": "Harrison D"}}, {"_id": "7eyRXnixroaWZoeSv", "postedAt": "2024-03-11T03:19:15.317Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<blockquote>\n<p>If the scenario were such that any one AI agent can expect to get away with defecting (expropriation from older agents) and the norm-breaking requires passing a non-small threshold of such actions</p>\n</blockquote>\n<p>This isn't the scenario I intended to describe, since it seems very unlikely that a single agent could get away with mass expropriation. The more likely scenario is that any expropriation that occurs must have been a collective action to begin with, and thus, there's no problem of coordination that you describe.</p>\n<p>This is common in ordinary expropriation in the real world: if you learned that we were one day going to steal all the wealth from people above the age of 90, you'd likely infer that that this decision was decided collectively, rather than being the result of a single lone agent who went and stole all the wealth for themselves.</p>\n<p>Your described scenario is instead more similar to ordinary theft, such as robbery. In that case, defection is usually punished by laws against theft, and people generally have non-altruistic reasons to support the enforcement of these laws.</p>\n<blockquote>\n<p>There are multiple other problems in your post, but I don\u2019t think it\u2019s worth the time going through them all. I just felt compelled to comment because I was baffled by the karma on this post</p>\n</blockquote>\n<p>I'm happy for you to critique the rest of the post. As far as I can tell, the only substantive critique you have offered so far seems to contain a misunderstanding of the scenario I described (conflating private lawbreaking from a lone actor with a collective action to expropriate wealth). But it would certainly not be surprising if my arguments had genuine flaws: these are about speculative matters concerning the future.</p>\n", "parentCommentId": "JLgYSBJu8Mdp8BEs8", "user": {"username": "Matthew_Barnett"}}, {"_id": "KretJBuhCHeP29AwL", "postedAt": "2024-03-11T03:32:53.224Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<p>I don't find this response to be a compelling defense of what you actually wrote:</p><blockquote><p>since AIs would \"get old\" too [...] they could also have reason to not expropriate the wealth of vulnerable old agents because they too will be in such a vulnerable position one day</p></blockquote><p>It's one thing if the argument is \"there will be effective enforcement mechanisms which prevent theft,\" but the original statement still just seems to imagine that norms will be a non-trivial reason to avoid theft, which seems quite unlikely for a moderately rational agent.</p><p>Ultimately, perhaps much of your scenario was trying to convey a different idea from what I see as the straightforward interpretation, but I think it makes it hard for me to productively engage with it, as it feels like engaging with a motte-and-bailey.</p>", "parentCommentId": "7eyRXnixroaWZoeSv", "user": {"username": "Harrison D"}}, {"_id": "JJ6YNqS2JrpJWpynw", "postedAt": "2024-03-11T03:51:11.295Z", "postId": "AA4aGmtXWbhunGHBq", "htmlBody": "<blockquote><p>the original statement still just seems to imagine that norms will be a non-trivial reason to avoid theft, which seems quite unlikely for a moderately rational agent.</p></blockquote><p>Sorry, I think you're still conflating two different concepts. I am <strong>not </strong>claiming:</p><ul><li>Social norms will prevent single agents from stealing from others, even in the absence of mechanisms to enforce laws against theft</li></ul><p>I am claiming:</p><ul><li>Agents will likely not want to establish a collective norm that it's OK (on a collective level) to expropriate wealth from old, vulnerable individuals. The reason is because most agents will themselves at some point become old, and thus do not want there to be a norm at that time, that would allow their own wealth expropriated from them when they become old.</li></ul><p>There are two separate mechanisms at play here. Individual and local instances of theft, like robbery, are typically punished by specific laws. Collective expropriation of groups, while possible in all societies, is usually handled via more decentralized coordination mechanisms, such as social norms.&nbsp;</p><p>In other words, if you're asking me why an AI agent can't just steal from a human, in my scenario, I'd say that's because there will (presumably) be laws against theft. But if you're asking me why the AIs don't all get up together and steal from the humans collectively, I'd say it's because they would not want to violate the general norm against expropriation, especially of older, vulnerable groups.</p><blockquote><p>perhaps much of your scenario was trying to convey a different idea from what I see as the straightforward interpretation, but I think it makes it hard for me to productively engage with it, as it feels like engaging with a motte-and-bailey.</p></blockquote><p>For what it's worth, I asked Claude 3 and GPT-4 to proof-read my essay before I posted, and they both appeared to understand what I said, with almost no misunderstandings, for every single one of my points (from my perspective). I am <strong>not </strong>bringing this up to claim you are dumb, or anything like that, but I do think it provides evidence that you could probably better understand what I'm saying if you tried to read my words more carefully.</p>", "parentCommentId": "KretJBuhCHeP29AwL", "user": {"username": "Matthew_Barnett"}}]