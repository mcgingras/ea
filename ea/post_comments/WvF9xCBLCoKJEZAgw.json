[{"_id": "ASFv4cKJdEYz3uh72", "postedAt": "2023-12-12T20:30:29.814Z", "postId": "WvF9xCBLCoKJEZAgw", "htmlBody": "<blockquote><blockquote><p>What <i>are</i> human values?</p></blockquote><p>We don't need to figure out this problem, we can just implement <a href=\"https://www.lesswrong.com/tag/coherent-extrapolated-volition\">CEV</a> without ever having a good model of what \"human values\" are.</p><blockquote><p>Aligned to <i>whom</i>?</p></blockquote><p><a href=\"https://www.lesswrong.com/posts/A4nfKtD9MPFBaa5ME/we-re-all-in-this-together\">The vast majority of the utility you have to gain is from {getting a utopia rather than everyone-dying-forever}, rather than {making sure you get the right utopia}.</a></p><blockquote><p>What does it mean for something to be an optimizer?</p></blockquote><p>Expected utility maximization seems to fully cover this. More general models aren't particularly useful to saving the world.</p></blockquote><p>For what it's worth, I have significant disagreements with basically all of your short replies to these basic questions, and I've been heavily engaged in AI alignment discussions for several years. So, I strongly disagree with your claim that these questions are \"either already solved or there's a good reason why thinking about them is not useful to the solution\", at least in the way you seem to think they have been solved.</p>", "parentCommentId": null, "user": {"username": "Matthew_Barnett"}}, {"_id": "BmdE5xBaQiyFBcKqZ", "postedAt": "2023-12-12T22:46:41.923Z", "postId": "WvF9xCBLCoKJEZAgw", "htmlBody": "<p>I feel like they're at least solved-enough that they're not particularly what should be getting focused on. I predict that in worlds where we survive, spending time on those question doesn't end up having cashed out to much value.</p>", "parentCommentId": "ASFv4cKJdEYz3uh72", "user": {"username": "carado"}}, {"_id": "Bf5SqB4agC5yyozhL", "postedAt": "2023-12-13T14:08:29.385Z", "postId": "WvF9xCBLCoKJEZAgw", "htmlBody": "<p><strong>Executive summary:</strong> The post discusses three selection effects biasing AI risk discourse: overvaluing outside views, filtering arguments for safety, and pursuing useless research based on confusion.</p><p><strong>Key points:</strong></p><ol><li>Overreliance on outside views like consensus opinions double counts evidence and feels safer than developing independent expertise.</li><li>Strong arguments for high extinction risk often look unsafe to share, so discourse misses hazardous insights.</li><li>Confusions about core issues lead researchers down useless paths instead of focusing on decisive factors.</li><li>Checking whether a question is coherent or helps save the world can avoid wasted effort.</li><li>Tabooing terms like AGI may help avoid distraction on irrelevant definitional debates.</li><li>Recognizing these selection effects can improve individual and collective epistemics.</li></ol><p>&nbsp;</p><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}]