[{"_id": "gyGFknXXacgW5pdka", "postedAt": "2023-02-19T01:29:11.065Z", "postId": "Bi8av6iknHFXkSxnS", "htmlBody": "<p>My current belief in the sentience of most nonhuman animals comes partly from the fact that they were subjected to many of the same evolutionary forces that gave consciousness to humans. &nbsp;Other animals also share many brain structures with us. &nbsp;ChatGPT never went through that process and doesn't have the same structures, so I wouldn't really expect it to be conscious. &nbsp;I guess your post looks at the outputs of conscious beings, which are very similar to what ChatGPT produces, whereas I'm partly looking at the inputs that we know have created consciousness.<br><br>Just my two cents. &nbsp;And I do think this is a worthwhile question to ask! &nbsp;But I would probably update more in the direction of \"digital sentience is a (future) possibility\" than \"more nonhuman animals probably aren't conscious\".</p>", "parentCommentId": null, "user": {"username": "Pete Rowlett"}}, {"_id": "rvjhJdtkfETTxhs7f", "postedAt": "2023-02-19T01:48:03.483Z", "postId": "Bi8av6iknHFXkSxnS", "htmlBody": "<p>Other animals do share many brain structures with us, but by the same token, most animals lack brain structures that are the most fundamental to what make us human. As far as I am aware (and I will quickly get out of my depth here), only mammals have a neocortex, and small mammals don't have much of one.&nbsp;</p><p>Hopefully this is clear from my post, but ChatGPT hasn't made me rethink my beliefs about primates or even dogs. It definitely has made me more uncertain about invertebrates, reptiles, and &nbsp;fish. (I have no idea what to think about birds.)</p>", "parentCommentId": "gyGFknXXacgW5pdka", "user": {"username": "splinter"}}, {"_id": "CGLReM3qJhzXrCTkt", "postedAt": "2023-02-19T06:38:18.345Z", "postId": "Bi8av6iknHFXkSxnS", "htmlBody": "<p>Birds have structures with functions and evolutionary origin similar to the (neo)cortex:</p>\n<p><a href=\"https://www.science.org/content/article/newfound-brain-structure-explains-why-some-birds-are-so-smart-and-maybe-even-self-aware\">https://www.science.org/content/article/newfound-brain-structure-explains-why-some-birds-are-so-smart-and-maybe-even-self-aware</a></p>\n<p><a href=\"https://www.scientificamerican.com/article/bird-brains-are-far-more-humanlike-than-once-thought/\">https://www.scientificamerican.com/article/bird-brains-are-far-more-humanlike-than-once-thought/</a></p>\n", "parentCommentId": "rvjhJdtkfETTxhs7f", "user": {"username": "MichaelStJules"}}, {"_id": "KYS3eFp6SyoQQFcGD", "postedAt": "2023-02-19T06:51:17.681Z", "postId": "Bi8av6iknHFXkSxnS", "htmlBody": "<p>It seems reasonable to guess that modern language models aren't conscious in any morally relevant sense. But it seems odd to use that as the basis for a <i>reductio</i> of arguments about consciousness, given that we know nothing about the consciousness of language models.</p><p>Put differently: if a line of reasoning would suggest that language models are conscious, then I feel like the main update should be about consciousness of language models rather than about the validity of the line of reasoning. If you think that e.g. fish are conscious based on analysis of their behavior rather than evolutionary analogies with humans, then I think you should apply the same reasoning to ML systems.</p><p>I don't think that biological brains are plausibly necessary for consciousness. It seems extremely likely to me that a big neural network can <i>in principle</i> be conscious without adding any of these bells or whistles, and it seems clear that SGD could find conscious models.&nbsp;</p><p>I don't think the fact that language models say untrue things show they have no representation of the world (in fact for a pre-trained model that would be a clearly absurd &nbsp;inference---they are trained to predict what someone else would say and then sample from that distribution, which will of course lead to confidently saying false things when the predicted-speaker can know things the model does not!)</p><p>That all said, I think it's worth noting and emphasizing that existing language models' <i>statements about their own consciousness</i> are not evidence &nbsp;that they are conscious, and that more generally the relationship between a language model's inner life and its utterances is completely unlike the relationship between a human's inner life and their utterances (because they are trained to produce these utterances by mimicking humans, and they would make similar utterances regardless of whether they are conscious). A careful analysis of how models generalize out of distribution, or about surprisingly high accuracy on some kinds of prediction tasks could provide evidence of consciousness, but we don't have that kind of evidence right now.</p>", "parentCommentId": null, "user": {"username": "Paul_Christiano"}}, {"_id": "jYgPnJ4Dbz5QPRQgw", "postedAt": "2023-02-19T12:25:22.674Z", "postId": "Bi8av6iknHFXkSxnS", "htmlBody": "<p>Many nonhuman animals also show long-term abnormal behaviours, and will try to access analgesia (even paying a cost to do so), if they are in pain. I don\u2019t think we have evidence that\u2019s quite analogous to that with large language models, and if we did, it would cause me to update in favour of current models having sentience. It\u2019s also worth noting that the same lines of evidence that cause me to believe nonhuman animals are sentient also lead me to believe that humans are sentient, even if some of the evidence (like physiological and neuro-anatomical similarities, and evolutionary distance) may be somewhat stronger in humans.</p>\n", "parentCommentId": "gyGFknXXacgW5pdka", "user": {"username": "JBentham"}}, {"_id": "dBnBufCetJaZqof4X", "postedAt": "2023-02-19T16:47:30.417Z", "postId": "Bi8av6iknHFXkSxnS", "htmlBody": "<p>Even in humans, language production is generally subconscious. At least, my experience of talking is that I generally first become conscious of what I say as I'm saying it. I have some sense of what I might want to say before I say it, but the machinery that selects specific words is not conscious. Sometimes, I think of a couple of different things I could say and consciously select between them. But often I don't: I just hear myself speak. Language generation may often lead to conscious perceptions of inner speech, but it doesn't seem to rely on it.</p>\n<p>All of this suggests that the possibility of non-conscious chatbots should not be surprising. It may be that chatbots provide pretty good evidence that cognitive complexity can come apart from consciousness. But introspection alone should provide sufficient evidence for that.</p>\n", "parentCommentId": null, "user": {"username": "Derek Shiller"}}, {"_id": "cpAgFuQyJf2pvcA9L", "postedAt": "2023-02-19T19:56:04.795Z", "postId": "Bi8av6iknHFXkSxnS", "htmlBody": "<p>Thanks for this response. It seems like we are coming at this topic from very different starting assumptions. If I'm understanding you correctly, you're saying that we have no idea whether LLMs are conscious, so it doesn't make sense to draw any inferences from them to other minds.</p><p>That's fair enough, but I'm starting from the premise that LLMs in their current form are almost certainly not conscious. Of course, I can't prove this. It's my belief based on my understanding of their architecture. I'm very much not saying they lack consciousness because they aren't instantiated in a biological brain. Rather, I don't think that GPUs performing parallel searches through a probabilistic word space by themselves are likely to support consciousness.</p><p>Stepping back a bit: I can't know if any animal other than myself is conscious, even fellow humans. I can only reason through induction that consciousness is a feature of my brain, so other animals that have brains similar in construction to mine may also have consciousness. And I can use the observed output of those brains -- behavior -- as an external proxy for internal function. This makes me highly confident that, for example, &nbsp;primates are conscious, with my uncertainty growing with greater evolutionary distance.</p><p>Now along come LLMs to throw a wrench in that inductive chain. LLMs are -- in my view -- zombies that can do things previously only humans were capable of. &nbsp;And the truth is, a mosquito's brain doesn't really have all that much in common with a human's. So now I'm even more uncertain -- is complex behavior really a sign for interiority? Does having a brain made of neurons really put lower animals on a continuum with humans? I'm not sure anymore.&nbsp;</p>", "parentCommentId": "KYS3eFp6SyoQQFcGD", "user": {"username": "splinter"}}, {"_id": "Ldw2SHJsfMeqQ5SRH", "postedAt": "2023-02-19T20:03:28.996Z", "postId": "Bi8av6iknHFXkSxnS", "htmlBody": "<p>Comparing the \"consciousness\" of LLMs and AI models with the consciousness of living organisms feels to me almost like comparing apples to oranges.</p><p>Yet &nbsp;I'm of the opinion that the process in which living brains manifest consciousness may not be all that different from the process that LLMs use, just translated into its biochemical near-equivalent.</p><p>However I'm 100% confident that LLMs and other AI (now or in the future) can never be conscious of the world in the same way that living organisms are conscious (or not conscious).</p><p>This is because there is something in living things that is missing and can never be found in LLMs and AI. That is the soul / spirit which is what brings about that consciousness and life that living organisms have. (Don't quote me on this though because, of course, I can not prove it LOL.)</p><p>However, at some point LLMs (or AIs) will be able to perfectly simulate human-like consciousness that it would become nearly impossible to tell that they are really not conscious or sentient (if GPT-3 is like this, imagine what GPT-50 would be like!!!)</p><p>But they will never have the same kind of consciousness as even the lowest of living organisms.</p><p>Unless a way to give them a spirit or a soul is discovered.</p>", "parentCommentId": null, "user": {"username": "Joyce Alvino"}}, {"_id": "fnvfY533bLhk4uERn", "postedAt": "2023-02-20T02:58:38.528Z", "postId": "Bi8av6iknHFXkSxnS", "htmlBody": "<p>splinter -- if we restrict attention to sentience (capacity to feel pleasure/pain, or to flourish/suffer) rather than consciousness, then it would be very difficult for any AI findings or capabilities to challenge my conviction that most non-human, mobile animals are sentient.</p><p>The reasons are evolutionary and functional. Almost every animal nervous system evolves to be capable of adjusting its behavior based on feedback from the environment, in the form of positive and negative reinforcers, which basically boil down to pleasure and pain signals. My hunch is that any animal capable of operant conditioning is sentient in a legitimate sense -- and that would include basically all vertebrates with a central nervous system (inc. mammals, birds, reptiles), and also most invertebrates that evolved to move around to find food and avoid predators.</p><p>So, consciousness is a red herring. If we're interested in the question of whether non-human animals can suffer, we need to ask whether they can be operantly conditioned by any negative reinforcers. The answer, almost always, is 'yes'.&nbsp;</p>", "parentCommentId": null, "user": {"username": "geoffreymiller"}}, {"_id": "TJk5P3GmHqfnm4uKn", "postedAt": "2023-02-20T07:26:30.717Z", "postId": "Bi8av6iknHFXkSxnS", "htmlBody": "<blockquote><p>Rather, I don't think that GPUs performing parallel searches through a probabilistic word space by themselves are likely to support consciousness.</p></blockquote><p>This seems like the crux. It feels like a big neural network run on a GPU, trained to predict the next word, could definitely be conscious. So to me this is just a question about the particular weights of large language models, not something that can be established a priori based on architecture.</p>", "parentCommentId": "cpAgFuQyJf2pvcA9L", "user": {"username": "Paul_Christiano"}}, {"_id": "Jh3fAhF8nYmd78hza", "postedAt": "2023-02-20T09:14:09.746Z", "postId": "Bi8av6iknHFXkSxnS", "htmlBody": "<p>Not to rehash everyone's well-rehearsed position on the hard problem, but surely in the above sentience is the red herring? If non-human animals are not conscious, i.e. \"there are no lights on inside\" not just \"the lights are on but dimmer\", then there is actually no suffering?&nbsp;</p><p><strong>Edit: </strong>A good intuition pump on this crux is David Chalmer's 'Vulcan' thought experiment (see the 80k podcast <a href=\"https://80000hours.org/podcast/episodes/david-chalmers-nature-ethics-consciousness/#the-zombie-and-vulcan-trolley-problems-23843\">transcript</a>) - my intuition tells me we care about the Vulcans, but maybe the dominant philosophy of mind position in EA is to not care about them (I might be confounding overlap between illusionism and negative utiliarianism though)? That seems like a pretty big crux to me.</p>", "parentCommentId": "fnvfY533bLhk4uERn", "user": {"username": "JWS"}}, {"_id": "uQXMKpacDJjMqvqHq", "postedAt": "2023-02-20T16:28:30.637Z", "postId": "Bi8av6iknHFXkSxnS", "htmlBody": "<p>I don't see, at the evolutionary-functional level, why human-type 'consciousness' (whatever that means) would be required for sentience (adaptive responsiveness to positive/negative reinforcers, i.e. pleasure/pain). Sentience seems much more foundational, operationalizable, testable, functional, and clear.</p><p>But then, 99% of philosophical writing about consciousness strikes me as wildly misguided, speculative, vague, and irrelevant.&nbsp;</p><p>Psychology has been studying 'consciousness' ever since the 1850s, and has made a lot of progress. Philosophy, not so much, IMHO.</p>", "parentCommentId": "Jh3fAhF8nYmd78hza", "user": {"username": "geoffreymiller"}}, {"_id": "aiSRx4bfn2R2rndtt", "postedAt": "2023-02-20T19:20:23.203Z", "postId": "Bi8av6iknHFXkSxnS", "htmlBody": "<p>I am using conscious and sentient as synonyms. Apologies if this is confusing.&nbsp;</p><p>I don't doubt at all that all animals are sentient in the sense that you mean. But I am &nbsp;referring to the question of whether they have subjective experience -- not just pleasure and pain signals but also a subjective experience of pleasure and pain.</p><p>This doesn't feel like a red herring to me. Suffering only takes on a moral valence if it describes a conscious experience.</p>", "parentCommentId": "fnvfY533bLhk4uERn", "user": {"username": "splinter"}}, {"_id": "3wZtsfvJGEQdRDyHH", "postedAt": "2023-02-20T19:52:49.473Z", "postId": "Bi8av6iknHFXkSxnS", "htmlBody": "<p>Follow-up: I've never found Chalmers' zombie or vulcan thought experiments at all compelling. They sound plausible at first glance as interesting edge cases, but I think they're not at all plausible or illuminating if one asks how such a hypothetical being could have evolved, and whether their cognitive/affective architecture really makes sense. The notion of a mind that doesn't have any valences regarding external objects, beings, or situations would boil down to a mind that can't make any decisions, can't learn anything (through operant conditioning), and can't pursue any goals -- i.e. not a 'mind' at all.</p><p>I critiqued the Chalmers zombie thought experiment in <a href=\"https://www.primalpoly.com/s/1999-consciousness.pdf\">this essay</a> from c. 1999. Also see this <a href=\"https://www.primalpoly.com/s/2003-consciousness-as-corporate-pep-rally.pdf\">shorter essay</a> about the possible functions of human consciousness, which I think center around 'public relations' functions in our hypersocial tribal context, more than anything else.</p>", "parentCommentId": "Jh3fAhF8nYmd78hza", "user": {"username": "geoffreymiller"}}, {"_id": "fyLyEsSLyvFeSkxET", "postedAt": "2023-02-20T19:55:48.799Z", "postId": "Bi8av6iknHFXkSxnS", "htmlBody": "<p>splinter -- I strongly disagree on that. I think consciousness is built up out of valenced reactions to things (e.g. pleasure/pain signals); it's not some qualitatively special overlay on top of those signals.&nbsp;</p><p>And I don't agree that suffering is only morally relevant if it's 'consciously experienced'. &nbsp;</p>", "parentCommentId": "aiSRx4bfn2R2rndtt", "user": {"username": "geoffreymiller"}}]