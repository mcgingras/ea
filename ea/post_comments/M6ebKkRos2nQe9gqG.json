[{"_id": "HHnhFwq7Ssj6yPnWh", "postedAt": "2022-12-12T18:21:48.280Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>I think that paragraph is quite misguided. \"Becoming much more risk averse\" is a great way to stop doing anything at all because it's passed through eight layers of  garbage. On top of this, it's not like \"literally becoming the US federal government\" and \"not having any accounting or governance at all\" are your only two options; this creates a sad false dichotomy. SBF was actively and flagrantly ignoring governance, regulation, and accounting. This is not remotely common for EA orgs.</p>\n<p>Like, for the last couple of decades we've been witnesssing over and over again how established, risk-averse institutions fail because they're unable to compete with new, scrappy, risk-tolerant ones (that is, startups).</p>\n<p>\"Good governance\" and bureaucracy are, while correlated, emphatically not the same thing. EA turning into a movement that fundamentally values these over just doing good in the world as effectively as possible will be a colossal failure, because bureaucracy is a slippery slope and the Thing That Happens when you emulate the practices that have been used for centuries is that you end up not being able to do anything. I'd be very sad if this was our final legacy.</p>\n", "parentCommentId": null, "user": {"username": "devanshpandey"}}, {"_id": "37uNcZAGQArw6rSxD", "postedAt": "2022-12-12T19:06:59.769Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>Right, I wouldn't want to <i>over</i>-correct, but personally \"more respect for good governance (even at cost of some increase in bureaucracy)\" is the major lesson I've drawn from recent events. &nbsp;(I expect I'm <i>still&nbsp;</i> more anti-bureaucratic than most people, but maybe finding a more balanced view than I previously had.)</p><p>I'm unsure whether \"risk aversion\" is the right way to put this, but even if it is I think we probably just want a <i>bit</i> more of it rather than <i>much</i> more.</p>", "parentCommentId": "HHnhFwq7Ssj6yPnWh", "user": {"username": "RYC"}}, {"_id": "dszGAo5yeHXDWRf8Z", "postedAt": "2022-12-12T19:54:40.605Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>(EDIT: Added a <a href=\"https://forum.effectivealtruism.org/posts/M6ebKkRos2nQe9gqG/reflections-on-vox-s-how-effective-altruism-let-sbf-happen?commentId=tf23tDqdnjThPqrs7\">follow-up comment</a> after reading the original article)</p>\n<p>A lot of this discourse feels like it's missing the point to me. <strong>FTX was not an EA org.</strong> (Alameda <em>was</em> founded mostly with EAs, but then most of them left, in part because of bad governance and lack of ethics!). FTX was not beholden to EAs, and EA and EA orgs didn't have any say in how FTX was governed. EA may have been well placed to blow the whistle on this, and <em>maybe</em> to say things to Sam about this, but it seems very off to say that the governance of EA orgs led to the bad governance of FTX.</p>\n<p>(Also, Alameda was founded in ~2018, where the EA scene was very, very different and much less mature (and probably much worse governed). I expect many bad governance decisions are baked in when an org is founded)</p>\n<p>I think the correct reference class for the governance of FTX is more like startups, esp crypto startups. I don't see that much of a causal link between, eg, how well OpenPhil or MIRI is governed, and how FTX was governed.</p>\n<p>To be clear, I am not arguing that EA orgs are <em>not</em> badly governed, I just think these should be two separate conversations. And, even, that the fact that FTX was badly governed and this led to a disaster, is at best weak evidence that EA orgs governed in a similar way will also lead to a disaster, given how different the work is (it's harder to fuck up when you aren't managing &gt;$10bn of customer funds!) (Though, to be clear, I think that if I discovered an EA org being governed in the same way as FTX it would be a concerning red flag. Just that it should have been a red flag regardless of the FTX blow up!)</p>\n", "parentCommentId": null, "user": {"username": "Neel Nanda"}}, {"_id": "YCeqEri7dyKdx8NbK", "postedAt": "2022-12-12T20:13:31.087Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>FTX and Alameda definitely needed more bureaucracy -- as in, doing stuff in a way that doesn't resemble a scene from Idiocracy. <a href=\"https://docs.house.gov/meetings/BA/BA00/20221213/115246/HHRG-117-BA00-Wstate-RayJ-20221213.pdf\">https://docs.house.gov/meetings/BA/BA00/20221213/115246/HHRG-117-BA00-Wstate-RayJ-20221213.pdf</a> &nbsp;\"Although our investigation is ongoing and detailed findings will have to await its conclusion, the FTX Group\u2019s collapse appears to stem from the absolute concentration of control in the hands of a very small group of grossly inexperienced and unsophisticated individuals who failed to implement virtually any of the systems or controls that are necessary for a company that is entrusted with other people\u2019s money or assets.\"</p>", "parentCommentId": "37uNcZAGQArw6rSxD", "user": {"username": "Stuart Buck"}}, {"_id": "M9GJGGJvDrj58i2ne", "postedAt": "2022-12-12T21:03:47.060Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>Strongly approve of this comment.&nbsp;<br><br>Established procedures should be questioned. You should definitely use good business practices such as proper accounting, separation of entities with conflicts of interest but you don't want to copy the copious amounts of \"established procedures\" that amount to getting nothing done through piles of pointless paperwork and administrative bloat and endless committees who talk about nothing. There's lots of teams in EA who get a lot done, specifically because they aren't bogged down in bureaucracy and have a clear focus and mission.</p>", "parentCommentId": "HHnhFwq7Ssj6yPnWh", "user": {"username": "MarcusAbramovitch"}}, {"_id": "EpZ4XEoiQLytBw5hv", "postedAt": "2022-12-12T22:34:47.832Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>He is saying something about SBF's charitable ventures, of which at least FTXFF is reasonably seen as an EA organization based on the staff list:</p><p><i>There\u2019s a fundamental difference between Bankman-Fried\u2019s charitable efforts and august ones like the Rockefeller and Ford foundations: these philanthropies are, fundamentally, professional. They\u2019re well-staffed, normally run institutions. They have HR departments and comms teams and accountants and all the other stuff you have when you\u2019re a grown-up running a grown-up organization.</i></p><p><i>. . . .</i></p><p><i>The good news for EAs is that Open Philanthropy, the remaining major EA-aligned funding group, is a much more normal organization. Its form of professionalization is something for the rest of the movement to emulate. [Elsewhere, the author has kind words for GiveWell as an organization.]</i></p><p>I am less than convinced, though, that this particular criticism is fairly directed at EA as a whole as opposed to SBF in particular. And it may be cultural/generational: how many 30-year old billionaires are going to be interested in passively giving away their wealth and letting Rockefeller/Ford-style bureaucrats decide where it goes? I'm a decade older, and that sort of passivity/disengagement feels awfully unappealing to me.</p><p>My take on the author's criticism is to repeat the Cynic's Golden Rule: \"He who has the gold makes the rules.\" If SBF really wanted to do it his way, what was EA-as-a-community supposed to do, refuse to take his money? That's a major downside to hits-based donor development. If the EA movement continues to rely so heavily on a few megadonors, the Cynic's Golden Rule will remain in full force whenever the megadonors wish. It is a testament to Moskovitz and Tuna's humility that Open Phil operates as it does, not an expectation EA can demand of would-be megadonors with its non-existent leverage.</p>", "parentCommentId": "dszGAo5yeHXDWRf8Z", "user": {"username": "Jason"}}, {"_id": "F6fWak76xHzs2fx39", "postedAt": "2022-12-12T22:46:23.109Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>I think an increase in bureaucracy / risk-aversion is inevitable -- and probably necessary -- with increasing size/power/influence after a certain point. The 51% coin flip is great when the wager is $100, not so great when it is all life on Earth. I would submit that part of the answer is to prevent any one organization from getting too massive so that it doesn't get mired down in bureaucracy and ossified. The one thing I will give FTXFF some credit for is the interest in regranting programs.</p>", "parentCommentId": "HHnhFwq7Ssj6yPnWh", "user": {"username": "Jason"}}, {"_id": "JJFXscKBzPq9qKSy4", "postedAt": "2022-12-13T01:17:18.138Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>If one of the main motivations for effective altruism is to challenge traditional, ineffective ways of doing things, such as bureaucracy, mismanagement, passivity, and established procedures, and to differentiate itself from the world of charity and its connection to the establishment, especially in societies like the UK, then traditional institutional experience by older persons won't be enough. I would argue that a better goal would be identifying what is not working and must be worked on &nbsp;in relation to learning about and fighting fraud, including developing better tools to do so.</p><p>To truly go against the traditional ineffective way of doing things and create differentiation, effective altruism needs to prioritize developing new tools and approaches for addressing issues like charity fraud. This could include using coordination technologies, artificial intelligence, and knowledge networks to identify and review potential frauds, as well as working on building a community of expertsand an academic network who can help develop and implement these solutions. By focusing on innovation and tackling these challenges head-on, effective altruism can continue to set itself apart and make a real impact.</p>", "parentCommentId": null, "user": {"username": "cookiecut"}}, {"_id": "qvENDSvBskZgpwMdy", "postedAt": "2022-12-13T01:33:40.643Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>The key passages (my emphasis):</p><blockquote><p><strong>Longtermism seems weird not because of its critics but because of its proponents: it\u2019s expressed mainly by philosophers</strong>, and there are strong incentives in academic philosophy to carry out thought experiments to increasingly bizarre (and thus more interesting) conclusions.</p><p>This means that longtermism as a concept has been defined not by run-of-the-mill stuff like donating to nuclear nonproliferation groups, but by the philosophical writings of figures like Nick Bostrom, MacAskill, Greaves, and Nick Beckstead, figures who have risen to prominence in part because of their willingness to expound on extreme ideas.</p><p><strong>These are all smart people, but they are philosophers, which means their entire job is to test out theories and frameworks for understanding the world, and try to sort through what those theories and frameworks imply.</strong> There are professional incentives to defend surprising or counterintuitive positions, to poke at widely held pieties and components of \u201ccommon sense morality,\u201d and to develop thought experiments that are memorable and powerful (and because of that, pretty weird).</p></blockquote><blockquote><p>This isn\u2019t a knock on philosophy; it\u2019s what I studied in college and a field from which I have learned a tremendous amount. <strong>It\u2019s good for society to have a space for people to test out strange and surprising concepts. But whatever the boundary-pushing concepts being explored, it\u2019s important not to mistake that exploration for practical decision-making.</strong></p><p><strong>[\u2026]</strong></p></blockquote><blockquote><p>The dominance of academic philosophers in EA, and those philosophers\u2019 increasing attempts to apply these kinds of thought experiments to real life \u2014 aided and abetted by the sudden burst of billions into EA, due in large part to figures like Bankman-Fried \u2014 has <strong>eroded the boundary between this kind of philosophizing and real-world decision-making</strong>. Poets, as Percy Shelley <a href=\"https://interestingliterature.com/2021/12/shelley-poets-unacknowledged-legislators-world-meaning/#:~:text='Poets%20are%20the%20unacknowledged%20legislators%20of%20the%20world'%20is%20one,from%20a%20work%20of%20prose.\"><strong>wrote</strong></a>, may be the unacknowledged legislators of the world, but EA made the mistake of trying to turn philosophers into the actual legislators of the future. A good start would be more clearly stating that funding priorities, for now, are less \u201clongtermist\u201d in this galaxy-brained Bostrom sense and more about fighting specific existential risks \u2014 which is exactly what EA funders are <a href=\"https://www.openphilanthropy.org/grants/?q&amp;focus-area%5B0%5D=longtermism&amp;sort=high-to-low#categories\"><strong>doing in most cases</strong></a>. <strong>The philosophers can trod the cosmos, but the funders and advocates should be tethered closer to Earth.</strong></p></blockquote><blockquote><p><strong>[\u2026]</strong><br>&nbsp;</p><p><strong>The problem is utilitarianism free from any guardrails \u2026</strong></p><p>Sam Bankman-Fried is a hardcore, pure, uncut <a href=\"https://www.vox.com/future-perfect/23458282/effective-altruism-sam-bankman-fried-ftx-crypto-ethics\"><strong>Benthamite utilitarian</strong></a>. His mother, Barbara Fried, is an influential philosopher known for her arguments that consequentialist moral theories like utilitarianism that focus on the actual results of individual actions are <a href=\"https://smile.amazon.com/Facing-Up-Scarcity-Nonconsequentialist-Thought/dp/0198847874?ascsubtag=[]vx[p]23264055[t]w[r]forum.effectivealtruism.org[d]D\"><strong>better suited for the difficult real-world trade-offs one faces</strong></a> in a complex society. Her son apparently took that insight very, very seriously.</p></blockquote><blockquote><p>Effective altruists aren\u2019t all utilitarians, but the core idea of EA \u2014 that you should attempt to act in such a way to promote the greatest human and animal happiness and flourishing achievable \u2014 is shot through with consequentialist reasoning. The whole project of trying to do the most good you can implies maximizing, and maximizing of \u201cthe good,\u201d and that is the literal definition of consequentialism.</p><p>It\u2019s not hard to see the problem here: If you\u2019re intent on maximizing the good, you better know what the good is \u2014 and that isn\u2019t easy. \u201c<strong>\u200b\u200bEA is about maximizing a property of the world that we\u2019re conceptually confused about, can\u2019t reliably define or measure, and have massive disagreements about even within EA,\u201d </strong><a href=\"https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous\"><strong>Holden Karnofsky, the co-CEO of Open Philanthropy</strong></a><strong> and a leading figure in the development of effective altruism, wrote in September. \u201cBy default, that seems like a recipe for trouble.\u201d</strong></p></blockquote><blockquote><p>Indeed it was. It looks increasingly likely that Sam Bankman-Fried appears to have engaged in extreme misconduct precisely because he believed in utilitarianism and effective altruism, and that his mostly EA-affiliated colleagues at FTX and Alameda Research went along with the plan for the same reasons.</p><p>&nbsp;</p><p>[\u2026]<br>&nbsp;</p><p>I think taking a high-earning job with the explicit aim of donating the money still makes a lot of sense for most big-money options.</p><p>But what SBF did was not just quantitatively but <i>qualitatively </i>different from classic \u201cearn to give.\u201d You can make seven figures a year as a trader in a hedge fund, but unless you manage the whole fund, you probably won\u2019t become a billionaire. Bankman-Fried very much wanted to be a billionaire \u2014 so he could have more resources to devote to EA giving, if we take him at his word \u2014 and to do that, he set up whole new corporations that never would\u2019ve existed without him. Those corporations then engaged in incredibly risky business practices that never would\u2019ve occurred if he and his team hadn\u2019t entered the field. He was not one-for-one replacing another finance bro who would have used the earnings on sushi and strippers rather than altruistic causes. He was building a whole new financial world, with consequences that would be much grander in scale.</p></blockquote><blockquote><p>And<strong> in building this world, he acted like a vulgar utilitarian. Philosophers like to talk about \u201cbiting the bullet\u201d: accepting an unsavory implication of a theory you\u2019ve adopted, and arguing that this implication really isn\u2019t that bad.</strong></p><p>&nbsp;</p><p>[\u2026]<br>&nbsp;</p><p><strong>Bankman-Fried\u2019s error was an extreme hubris that led him to bite bullets he never should have bitten.</strong> He famously told economist Tyler Cowen in a <a href=\"https://conversationswithtyler.com/episodes/sam-bankman-fried/\"><strong>podcast interview</strong></a> that if faced with a game where \u201c51 percent [of the time], you double the Earth out somewhere else; 49 percent, it all disappears,\u201d he\u2019d keep playing the game continually.</p><p>&nbsp;</p><p>This is known as the <a href=\"https://plato.stanford.edu/entries/paradox-stpetersburg/\"><strong>St. Petersburg paradox</strong></a>, and it\u2019s a confounding problem in probability theory, because it\u2019s true that playing the game creates more happy human lives in expectation (that is, adjusting for probabilities) than not playing. But if you keep playing, you\u2019ll almost certainly wipe out humankind. <strong>It\u2019s an example of where normal rules of rationality seem to break down.</strong></p></blockquote><blockquote><p>But Bankman-Fried was not interested in playing by the normal rules of rationality. Cowen notes that if Bankman-Fried kept this up, he\u2019d almost certainly wipe out the Earth eventually. Bankman-Fried replied, \u201cWell, not necessarily. Maybe you St. Petersburg paradox into an enormously valuable existence. That\u2019s the other option.\u201d</p></blockquote><blockquote><p>These are fun dorm room arguments. <strong>They should not guide the decision-making of an actual financial company, yet there is some evidence they did. An </strong><a href=\"https://twitter.com/AutismCapital/status/1592406121368543233\"><strong>as-yet-unconfirmed account</strong></a><strong> of an Alameda all-hands meeting describes CEO Caroline Ellison explaining to staff that she and Bankman-Fried faced a choice in early summer 2022: either to let Alameda default after some catastrophic losses, or to raid consumer funds at FTX to bolster Alameda. As the researcher </strong><a href=\"https://twitter.com/davidad/status/1592509445682388996\"><strong>David Dalrymple has noted</strong></a><strong>, this was basically her and Bankman-Fried making a \u201cdouble or nothing\u201d coin flip: By taking this step, they reasoned they could either save Alameda </strong><i><strong>and</strong></i><strong> FTX or lose both</strong> (as wound up happening), rather than keep just FTX, as in a scenario where the consumer funds were not raided.</p></blockquote>", "parentCommentId": null, "user": {"username": "Peter_Hartree"}}, {"_id": "rDdJizPk9mqYiZEnP", "postedAt": "2022-12-13T01:51:59.949Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>I disagree that things are coupled in this way. You can be innovative and new in some important respects (like cause selection, prioritisation, taking philosophy seriously, etc.) while being boring and traditional in others (good governance, accounting, fraud detection, trust etc.).&nbsp;</p>", "parentCommentId": "JJFXscKBzPq9qKSy4", "user": {"username": "BenStewart"}}, {"_id": "8xqFLDWNWdEQirzCb", "postedAt": "2022-12-13T01:55:26.012Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>Richard: you wrote\u2026</p><blockquote><p>I agree we should be wary of philosopher-kings. But that's mostly just because we should be wary of \"kings\" (or immature dictators) in general.</p></blockquote><p>Two options:</p><blockquote><p>(1) An immature 30 year old king.</p><p>(2) An immature 30 year old king who has chosen to spend the last 10 years in a bubble of consequentialist-flavoured philosophy. [1]</p></blockquote><p>Knowing nothing else, I\u2019d pick (1). You?</p><p>Option (2) seems much higher variance. Most likely, this guy is a midwit, or perhaps a high IQ idiot. In either case, electing him king probably means serious trouble ahead.</p><p>Given more information I can imagine picking (2). I\u2019d be looking for evidence of practical wisdom, whether that\u2019s <a href=\"https://twitter.com/peterhartree/status/1594066354747117568\">taking context seriously</a>, Karnofsky-style \u201c<a href=\"https://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">cluster thinking</a>\u201d, <a href=\"https://twitter.com/peterhartree/status/1602307470487003136\">Burkean worldliness</a>, or <a href=\"https://twitter.com/peterhartree/status/1594066327823470592\">the pragmatism of Fat Tony</a>.</p><p>See also: <a href=\"https://forum.effectivealtruism.org/posts/YFYgtZzfwjW7cEnkf/byrne-hobart-and-dwarkesh-patel-on-hardcore-believers\">Byrne Hobart and Dwarkesh Patel on hardcore believers &amp; monasteries</a>.</p><p>[1] I\u2019m drawing a caricature here, to make the point clearer. I\u2019m not sure how accurate (1) would be as a description of SBF, but it seems roughly right.</p>", "parentCommentId": "qvENDSvBskZgpwMdy", "user": {"username": "Peter_Hartree"}}, {"_id": "GpLDttcPNpAsmrfHe", "postedAt": "2022-12-13T06:18:41.320Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p><i>Sharing my reflections on the piece here (not directly addressing this particular post but my own reflections I shared with a friend.)</i><br><br><strong>While I agree with lots of points the author makes and think he raises valuable critiques of EA, I don\u2019t find his arguments related to SBF to be especially compelling.&nbsp;</strong> My run-through of the perceived problems within EA that the author describes and my reactions:</p><ol><li><strong>The dominance of philosophy.</strong> I personally find parts of long-termism kooky and I'm not strongly compelled by many of its claims, but the Vox author doesn\u2019t explain how this relates to SBF (or his misdeeds)... it feels more like shoehorning a critique of EA in to a piece on SBF?&nbsp;</li><li><strong>Porous boundaries between billionaires and their giving. </strong>So yes it sounds like SBF was very directly involved in the philanthropy his funds went toward but I don\u2019t think that caused (much? any?) incremental reputational harm to EA vs. a world where he created the \u201cSBF family foundation\u201d and had other people running the organization.&nbsp;</li></ol><ul><li>If I wanted to rescue this argument, <strong>maybe I could say SBF\u2019s behavior here is representative of a common trait of his (at FTX and in his charity) \u2013 SBF doesn\u2019t even have the dignity to surround himself with yes-men; he insists on doing it all himself! And maybe that\u2019s a red-flag RE cult of personality/genius and/or fraud that EA should have caught on to.&nbsp;</strong></li><li>I will say, though, that the FTX Future Fund had a board/team that was fairly star-studded and ran a big re-granting program (i.e., let others make grants with their money). Which is to say I\u2019m not sure how directly involved SBF actually was in the giving. [As an aside, I think it\u2019s fine for billionaires to direct their own giving and am a lot more suspect of non-profit bloat and organizational incentives than the Vox author is.]&nbsp;</li></ul><ol><li>3. <strong>Utilitarianism free of guardrails.</strong> I agree a lack of guardrails is a problem, but:&nbsp;</li></ol><ul><li>a) On utilitarianism\u2019s own account it seems to me you should recognize that if you commit massive fraud you\u2019ll probably get caught and it will all be worthless (+ cause serious reputational harm to utilitarianism), so then committing the fraud is doing utilitarianism wrong. [I don\u2019t <i>think </i>I\u2019m no-true-Scotsman-ing here?]&nbsp;</li><li>b) More importantly\u2026 the author doesn't explain how unabashed utilitarianism led to SBF's actions - it's sort of vaguely hand-waving and trying to make a point by association vs. actual causal reasoning / proof, in the same vein as the dominance of philosophy point above? I guess the steelman is: SBF wanted to do the most good at any cost, and genuinely thought the best way to do so was to commit fraud (?) A bit tough for me to swallow.&nbsp;</li></ul><ol><li>4.<strong> Utilitarianism full of hubris.</strong> A rare reference to evidence (well, an unconfirmed account, but at least it\u2019s something!) Comparing the St. Petersburg paradox to SBF figuring let\u2019s double-or-nothing our way out of letting Alameda default is an interesting point to make, but SBF's take on this was so wild as to surprise other EA-ers. So it strikes me as a point in favor of \u201cSBF has absurd viewpoints and his actions reflect that\u201d vs. \u201cEA enabled SBF.\u201d Meanwhile the author moves directly from this anecdote to \u201cThis is not, I should say, the first time a consequentialist <u>movement</u> has made this kind of error\u201d (emphasis added). &nbsp;SBF != the movement and I think the consensus EA view is the opposite of SBF\u2019s, so this feels misleading at best.</li></ol><p>One EA critique in the piece that resonated with me - and I'm not sure I'd seen put so succinctly elsewhere is:&nbsp;</p><blockquote><p>\u201cThe philosophy-based contrarian culture means participants are incentivized to produce \u2018fucking insane and bad\u2019 ideas, which in turn become what many commentators latch to when trying to grasp what\u2019s distinctive about EA.\"&nbsp;</p></blockquote><p>While not about SBF, it's a point I don't see us talking about often enough with regard to EA perceptions / reputation and I appreciated the author making it.&nbsp;<br><br><strong>TL;DR: I thought it was an interesting and thought-provoking piece with some good critiques of EA, but the author (or - perhaps more likely - editor who wrote the title / sub-headers) bit off more than they could chew in actually connecting &nbsp;EA to SBF's actions.</strong></p>", "parentCommentId": null, "user": {"username": "JeremyR"}}, {"_id": "tf23tDqdnjThPqrs7", "postedAt": "2022-12-13T12:11:12.231Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p><em><a href=\"https://forum.effectivealtruism.org/posts/M6ebKkRos2nQe9gqG/reflections-on-vox-s-how-effective-altruism-let-sbf-happen?commentId=dszGAo5yeHXDWRf8Z\">Follow-up to my previous comment</a>: I was responding to the post written here, not the underlying article in the above comment. I've now read the article, and broadly agree with it (esp on the significant over-focus on philosophy and controversial ideas within EA, and how this is harmful), but think that the claims about governance are badly argued and don't hold up, even if the conclusions may be correct</em></p>\n<p>The specific part of the article on the importance of good, professional governance in foundations, is something I feel more confused about, though Matthews' was not arguing that EA being better governed would have prevented <em>FTX</em> collapsing, more that it would have reduced general harm to EA + limited SBF's power.</p>\n<p>Like, sure, I think SBF's various foundations being further from him would have been a good idea a priori (and even better in hindsight!). But I don't actually see how the FTX disaster is <em>evidence</em> for this, beyond being evidence of SBF's poor judgement and bad intentions. Like, the issue at hand is not that SBF funded a bunch of pandemic prevention etc! I don't think it's even that he was involved in the foundation's decisions (\"foundation staff would sometimes cc SBF on a pitching email\"). I think it's that funding was unstable and got pulled out unexpectedly/there's risks of clawbacks, it harmed the reputation of those that received it, that the funding was gained immorally (though evaluating this is confusing to me, since <em>some</em> of their money was gained legitimately, some was from stealing from customers), that it's harmed the reputation of the movement + causes he donated to, and that the giving plausibly allowed him to launder his reputation in a way that gained him more credibility and less suspicion to perpetuate the fraud. <strong>None of these are that related to how involved he was in the foundations!</strong></p>\n<p>And, finally, I'd argue that the Future Fund was actually much <em>more</em> de-centralised and democratic  than eg OpenPhil, given their <a href=\"https://ftxfuturefund.org/our-regrants/\">enormous regranter program</a>. It wasn't quite just giving people money and letting them do whatever they wanted with it, but I'd say that it made giving <em>much</em> more diverse and accounting for many more different perspectives than any other foundation I'm aware of. (I think this comes with many downsides, to be clear, though I'm net a fan of the regranter program. Just that criticising it as SBF being too involved seems silly)</p>\n", "parentCommentId": null, "user": {"username": "Neel Nanda"}}, {"_id": "4MEt64qnA8fXC937t", "postedAt": "2022-12-13T12:12:52.133Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>Agreed, I made the above comment responding to the forum post, and hadn't read the article - I've added a <a href=\"https://forum.effectivealtruism.org/posts/M6ebKkRos2nQe9gqG/reflections-on-vox-s-how-effective-altruism-let-sbf-happen?commentId=tf23tDqdnjThPqrs7\">follow-up</a> comment responding to the article's claims about philanthropic governance.</p>\n<p>TLDR: As far as I can tell, the governance of FTXFF was fine and SBF didn't have undue power. Even if he <em>did</em> have undue power, this seems <em>totally unrelated</em> to the multi-billion dollar blow-up, harm to countless people, harm to the EA ecosystem and to EA's reputation.</p>\n", "parentCommentId": "EpZ4XEoiQLytBw5hv", "user": {"username": "Neel Nanda"}}, {"_id": "8fj7wn8gQfLfK2iiN", "postedAt": "2022-12-13T13:14:39.017Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>The stuff about academic incentives makes it sound like there's some \"commonsensical\" alternative to longtermism out there that philosophers are burying in order to be more \"interesting\", and that <a href=\"https://forum.effectivealtruism.org/posts/AgQx44vtw5bi4LmBh/puzzles-for-everyone\">just isn't true</a>. &nbsp;There's literally <i>no possible way</i> to systematize ethics without ending up somewhere puzzling.</p><p>I've written elsewhere about the importance of distinguishing <a href=\"https://rychappell.substack.com/p/ethical-theory-and-practice\">ethical theory and practice</a>. This is a completely standard part of the consequentialist philosophical tradition. &nbsp;So again, I sort of agree with some of what Matthews says here, except for the philosophy-blaming part of it.&nbsp;</p><p>I also don't see any evidence for the claim of EA philosophers having \"eroded the boundary between this kind of philosophizing and real-world decision-making\"<strong>. &nbsp;</strong>That would presumably require a critique of EA funding priorities (esp. by the Future Fund, as directed by Will and Nick), but he instead seems to allow that actual funding decisions have been well-grounded (at least \"in most cases\"), and merely recommends \"more clearly stating\" that this is so. That seems to give the game away that his critique here is purely about optics and communications, and not the \"real-world decision-making\" at all.</p><p>Finally, on SBF's lack of guard-rails: yes, he made crazy bad decisions. There is <strong>no philosophical view on which he made wise decisions</strong>. &nbsp;He didn't maximize happiness. (Bentham would be rolling in his grave right now, if he had a grave.) &nbsp;So the worries about <i>maximizing the wrong thing</i> are completely irrelevant here. &nbsp;The problem was a total lack of practical wisdom or <a href=\"https://forum.effectivealtruism.org/posts/nvus8kuGxyacyfXeg/naive-vs-prudent-utilitarianism\">prudence</a>.</p><p>As <a href=\"https://www.utilitarianism.net/books/utilitarianism-john-stuart-mill/2\">J.S. Mill put it</a>:</p><blockquote><p>People talk as if... at the moment when some man feels tempted to meddle with the property or life of another, he had to begin considering for the first time whether murder and theft are injurious to human happiness. Even then I do not think that he would find the question very puzzling...&nbsp;</p><p>There is no difficulty in proving any ethical standard whatever to work ill, if we suppose universal idiocy to be conjoined with it; but on any hypothesis short of that, mankind must by this time have acquired positive beliefs as to the effects of some actions on their happiness; and the beliefs which have thus come down are the rules of morality for the multitude, and for the philosopher until he has succeeded in finding better.</p></blockquote>", "parentCommentId": "qvENDSvBskZgpwMdy", "user": {"username": "RYC"}}, {"_id": "KokDTMZuDwQbJgBSF", "postedAt": "2022-12-13T14:31:52.421Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>The \"move fast and break things\" model of startups works great for something like software businesses where the failures are harmless and easily forgotten.&nbsp;</p><p>But we're not in the software business. We're in the <i>charity</i> business. And in the charity business, reputation matters in a real, monetary sense. Thanks to FTX, EA has now been associated in pretty much every major newspaper with reckless, harmful, and irresponsible behavior. If you make an EA startup that goes wrong somehow, it's going to be written up in the guardian or the wall street journal, reminding everyone of FTX again.&nbsp;</p><p>And then potential donors are going to read those articles, and know that other people around them are reading said articles as well. &nbsp;If when people see the words \"Effective altruism\", the words that come to mind are \"<i>fraud and mismanagement</i>\", then most donors are going to go somewhere else, where their donations are met with applause rather than raised eyebrows. &nbsp;This damages <i>everyone</i> associated with EA, no matter how responsible they are for the latest mistake.&nbsp;</p><p>A small amount of bureaucracy and checks and balances is a very small price to pay, if we want to avoid being <i>permanently hobbled</i> by a poor reputation.&nbsp;</p>", "parentCommentId": "HHnhFwq7Ssj6yPnWh", "user": {"username": "titotal"}}, {"_id": "eh2zkwDWjDPhuanu5", "postedAt": "2022-12-13T18:45:00.331Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<blockquote>\n<p>There's literally no possible way to systematize ethics without ending up somewhere puzzling.</p>\n</blockquote>\n<p>Central plank of this perspective: systematizing ethics may not be the best idea, but some kinds of folks have a hard time recognising this. Systematising has its merits but if you find ideological mess hard to tolerate, you shouldn't be a king.</p>\n<p>Related reading:</p>\n<ul>\n<li>Karnofsky on <a href=\"https://sun.pjh.is/holden-karnofsky-on-bounded-commensurability-as-a-way-to-get-ahead-of-the-curve-on-moral-values\">worldview diversification</a>.</li>\n<li>Karnofsky on <a href=\"https://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">sequence vs cluster thinking</a>.</li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/8wWYmHsnqPvQEnapu/getting-on-a-different-train-can-effective-altruism-avoid\">Possibly the most underrated criticism of EA on the EA Forum</a>.</li>\n</ul>\n", "parentCommentId": "8fj7wn8gQfLfK2iiN", "user": {"username": "Peter_Hartree"}}, {"_id": "vJzsbp9g6aKznkeaS", "postedAt": "2022-12-13T19:27:28.096Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>I myself am a moral anti-realist, so I don't care much about these debates, though it's perpetually interesting to see debates on morality.</p>\n", "parentCommentId": "eh2zkwDWjDPhuanu5", "user": {"username": "Sharmake"}}, {"_id": "3GoRnxfx4njoewoCc", "postedAt": "2022-12-13T20:55:48.305Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>There's also <a href=\"https://notes.pjh.is/Two-thirds+utilitarianism\">Nick Beckstead disavowing his earlier \"hardcore utilitarianism\" in favour of something like Tyler Cowen's  two thirds utilitarianism</a>.</p>\n", "parentCommentId": "eh2zkwDWjDPhuanu5", "user": {"username": "Peter_Hartree"}}, {"_id": "RDAXdS2YHhjLRQnYj", "postedAt": "2022-12-14T09:21:54.238Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<blockquote>\n<p>I also don't see any evidence for the claim of EA philosophers having \"eroded the boundary between this kind of philosophizing and real-world decision-making\".</p>\n</blockquote>\n<p>Have you visited the 80,000 Hours website recently?</p>\n<p>I think that effective altruism centrally involves taking the ideas of philosophers and using them to inform real-world decision-making. I am very glad we\u2019re attempting this, but we must recognise that this is an extraordinarily risky business. Even the wisest humans are unqualified for this role. <a href=\"https://sun.pjh.is/tyler-cowen-on-how-to-be-a-good-agnostic\">Many of our attempts are 51:49 bets at best</a>\u2014sometimes worth trying, rarely without grave downside risk, never without an accompanying imperative to listen carefully for feedback from the world. And yes\u2014<a href=\"https://sun.pjh.is/tyler-cowen-on-reasons-to-be-dogmatic\">diverse, hedged experiments in overconfidence also make sense</a>. And no, SBF was not hedged anything like enough to take his 51:49 bets\u2014to the point of blameworthy, perhaps criminal negligence.</p>\n<p>A notable exception to the \u201cwe\u2019re mostly clueless\u201d situation is: <strong>catastrophes are bad</strong>. This view passes the \u201ccommon sense\u201d test, and the \u201cnearly all the reasonable takes on moral philosophy\u201d test too (negative utilitarianism is the notable exception). But our global resource allocation mechanisms are not taking \u201ccatastrophes are bad\u201d seriously enough. So, EA\u2014along with other groups and individuals\u2014has a role to play in pushing sensible measures to reduce catastrophic risks up the agenda (as well as the sensible disaster mitigation prep).</p>\n<p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4648cb3c38386e8aa3d164204ae7530417b1706ed7a7707e.png\" alt=\"\"></p>\n<p>(Derek Parfit\u2019s \u201cextinction is much worse than 99.9% wipeout\u201d claim is far more questionable\u2014I put some of my chips on this, but not the majority.)</p>\n<p>As you suggest, the transform function from \u201cabstract philosophical idea\u201d to \u201cwhat do\u201d is complicated and messy, and involves a lot of deference to existing norms and customs. Sadly, I think that many people with a \u201cphysics and philosophy\u201d sensibility underrate just how complicated and messy the transform function really has to be. So they sometimes make bad decisions on principle instead of good decisions grounded in messy common sense.</p>\n<p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6bd0677bed4686fd214e1ef68715e71945d6f96ea44d2a87.jpeg\" alt=\"\"></p>\n<p>I\u2019m glad you shared the J.S. Mill quote.</p>\n<blockquote>\n<p>\u2026the beliefs which have thus come down are the rules of morality for the multitude, and for the philosopher until he has succeeded in finding better</p>\n</blockquote>\n<p>EAs should not be encouraged to grant themselves practical exception from \u201cthe rules of morality for the multitude\u201d if they think of themselves as philosophers. Genius, wise philosophers are extremely rare (cold take: Parfit wasn\u2019t one of them).</p>\n<p>To be clear: I am strongly in favour of attempts to act on important insights from philosophy. I just think that this is hard to do well. One reason is that there is a notable minority of \u201cphysics and philosophy\u201d folks who should not be made kings, because their \u201cneed for systematisation\u201d is so dominant as to be a disastrous impediment for that role.</p>\n<p>In my other comment, I shared links to Karnofsky, Beckstead and Cowen expressing views in the spirit of the above. From memory, <a href=\"https://80000hours.org/podcast/episodes/carl-shulman-common-sense-case-existential-risks/\">Carl Shuman is in a similar place</a>, and so are <a href=\"https://80000hours.org/podcast/episodes/alexander-berger-improving-global-health-wellbeing-clear-direct-ways/\">Alexander Berger</a> and <a href=\"https://80000hours.org/podcast/episodes/ajeya-cotra-worldview-diversification/\">Ajeya Cotra</a>.</p>\n<p>My impression is that more than half of the most influential people in effective altruism are roughly where they should be on these topics, but some of the top \u201cinfluencers\u201d, and many of the \u201dsecond tier\u201d, are not.</p>\n<p>(Views my own. Sword meme credit: the artist currently known as <a href=\"@mealreplacer\">John Stewart Chill</a>.)</p>\n", "parentCommentId": "8fj7wn8gQfLfK2iiN", "user": {"username": "Peter_Hartree"}}, {"_id": "oQiBtHcqwYeQd2qiA", "postedAt": "2022-12-14T13:49:58.133Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>Distinguish:<br>(i) philosophically-informed ethical practice, vs<br>(ii) \"erod[ing] the boundary between [fantastical thought experiments] and real-world decision-making\"</p><p>I think that (i) is straightforwardly good, central to EA, and a key component of what makes EA distinctively good. &nbsp;You seem to be asserting that (ii) is a common problem within EA, and I'm wondering what the evidence for this is. &nbsp;I don't see anyone advocating for implementing the repugnant conclusion in real life, for example.</p><blockquote><p>I think that effective altruism centrally involves taking the ideas of philosophers and using them to inform real-world decision-making. I am very glad we\u2019re attempting this, but we must recognise that this is an extraordinarily risky business.</p></blockquote><p>I think this is conflating distinct ideas. &nbsp;The \"risky business\" is simply <i>real-world decision-making</i>. &nbsp;There is no sense to the idea that philosophically-informed decision-making is inherently <i>more risky</i> than philosophically <i>ignorant </i>decision-making. [Quite the opposite: it wasn't until philosophers raised the stakes to salience that x-risk started to be taken even close to sufficiently seriously.]</p><p>&nbsp;Philosophers think about tricky edge cases which others tend to ignore, but unless you've some evidence that thinking about the edge cases <i>makes us worse at responding to central cases</i> -- and again, I'm still waiting for evidence of this -- then it seems to me that you're inventing associations where none exist in reality.</p><blockquote><p>EAs should not be encouraged to grant themselves practical exception from \u201cthe rules of morality for the multitude\u201d if they think of themselves as philosophers.</p></blockquote><p>Of course. The end of the Mill quote is just flagging that traditional social norms are not beyond revision. We may have good grounds for critiquing the anti-gay sexual morality of our ancestors, for example, and so reject such outmoded norms (for everyone, not just ourselves) when we have truly \"succeeded in finding better\".</p><blockquote><p>there is a notable minority of \u201cphysics and philosophy\u201d folks who should not be made kings, because their \u201cneed for systematisation\u201d is so dominant as to be a disastrous impediment for that role.</p></blockquote><p>Do you take yourself to be disagreeing with me here? &nbsp;(Me: \"People shouldn't be kings\". You: \"systematizing philosophers shouldn't be kings!\" &nbsp;You realize that my claim entails yours, right?) &nbsp;I'm finding a lot of this exchange somewhat frustrating, because we seem to be talking past each other, and in a way where you seem to be implicitly attributing to me views or positions that I've already explicitly disavowed.</p><p>My sense is that we probably agree about which concrete things are bad, you perhaps have the false belief that I disagree with you on that, but actually the only disagreement is about whether philosophy tells us to do the things we both agree are bad (I say it doesn't). &nbsp;But if that doesn't match your sense of the dialectic, maybe you can clarify what it is that you take us to disagree about?</p><p>[12/15: Edited to tone down an intemperate sentence.]</p>", "parentCommentId": "RDAXdS2YHhjLRQnYj", "user": {"username": "RYC"}}, {"_id": "pzjQyJiS8CmoLjXq4", "postedAt": "2022-12-14T15:48:20.220Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<blockquote>\n<p>There is no sense to the idea that philosophically-informed decision-making is inherently more risky than philosophically ignorant decision-making. [Quite the opposite: it wasn't until philosophers raised the stakes to salience that x-risk started to be taken even close to sufficiently seriously.]</p>\n</blockquote>\n<p>I strongly disagree with this. The key reason is: most of the time, norms that have been exposed to evolutionary selection pressures beat explicit \u201crational reflection\u201d by individual humans. One of the major mistakes of Enlightenment philosophers was to think it is usually the other way around. These mistakes were plausibly a necessary condition for some of the horrific violence that\u2019s taken place since they started trending.</p>\n<p>I often run into philosophy graduates who tell me that relying on intuitive moral judgements about particular cases is \u201carrogant\u201d. I reply by asking \u201cwhere do these intuitions come from?\u201d The metaphysical realists say \u201cthey are truths of reason, underwritten by the non-natural essence of rationality itself\u201d. The naturalists say: \u201cthese intuitions were transmitted to you via culture and genetics, itself subject to aeons of evolutionary pressure\u201d. I side with the naturalists, despite all the best arguments for non-naturalism (to my mind, they\u2019re mostly bad!).</p>\n<p>One way to think about the 21st century predicament is that we usually learn via trial and error and selection pressures, but this dynamic in a world with modern technology seems unlikely to go well.</p>\n", "parentCommentId": "oQiBtHcqwYeQd2qiA", "user": {"username": "Peter_Hartree"}}, {"_id": "aJa8knuufLjWkknox", "postedAt": "2022-12-14T15:54:03.701Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<blockquote>\n<p>it wasn't until philosophers raised the stakes to salience that x-risk started to be taken even close to sufficiently seriously.</p>\n</blockquote>\n<p>I agree that philosophers, especially Derek Parfit, Nick Bostrom and Tyler Cowen*, have helped get this up the agenda. So too have many economists, astronomers, futurists, etc. Philosophers don\u2019t have a monopoly on identifying what matters in practice\u2014in fact they\u2019re usually pretty bad at this.</p>\n<p>Same thing goes if we look at social movements instead of individuals: the anti-nuclear bomb and environmental folks may have done more for getting catastrophic risk up the agenda than effective altruism has so far\u2014especially in terms of generating a widespread culture concern and sense of unease, which certainly warmed up the audience for Bostrom, Parfit, and so on.</p>\n<p>Effective altruism movement is only just getting started (hopefully), and it has achieved remarkable successes already. So I do think we\u2019re on track to play a critical role, and we have Bostrom and Parfit and Ord and Sidgwick and Cowen to thank for that\u2014along with many, many others.</p>\n<p>*Those who don\u2019t see Tyler Cowen as fundamentally a philosopher\u2014perhaps one of the greats, certainly better than Parfit (with whom he collaborated early on)\u2014are not following carefully.</p>\n", "parentCommentId": "oQiBtHcqwYeQd2qiA", "user": {"username": "Peter_Hartree"}}, {"_id": "osmxN4Wm2CDaLwkqE", "postedAt": "2022-12-14T15:57:50.326Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>I\u2019m sorry to hear you\u2019re finding this frustrating. Personally I\u2019m enjoying our exchange because it\u2019s giving me a reason to clarify and write down a bunch of things I\u2019ve been thinking about for a long time, and I\u2019m interested to hear what you and others make of them.</p>\n<p>On Twitter I suggested we arrange a time to call. Would you be up for this? If yes, send me a DM.</p>\n", "parentCommentId": "oQiBtHcqwYeQd2qiA", "user": {"username": "Peter_Hartree"}}, {"_id": "scrYoscvfCpBvdaX8", "postedAt": "2022-12-14T16:01:08.498Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>I\u2019m not going to respond to the \u201cshow me the evidence\u201d requests for now because I\u2019m short on time and it\u2019s hard to do this well. Also: I think you and most readers can probably identify a bunch of evidence in favour of these takes if you take a while to look.</p>\n", "parentCommentId": "oQiBtHcqwYeQd2qiA", "user": {"username": "Peter_Hartree"}}, {"_id": "hCc6qkC7pFtLMmfzC", "postedAt": "2022-12-20T12:41:06.526Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>We should distinguish risk aversity, transparency, and bureaucracy. They're obviously related but different concepts. I would argue that transparency is far more important than risk aversity, the more so the less risk averse you are - and unfortunately nontransparency often seems to be <i>correlated</i> with risk-taking. This is sometimes justified on infohazard logic (cf MIRI in general) or some harder-to-pin-down lack of urgency to communicate controversial decisions (cf <a href=\"https://forum.effectivealtruism.org/posts/xof7iFB3uh8Kc53bG/why-did-cea-buy-wytham-abbey?commentId=u3yJfbm2pes8TFpYX\">Wytham Abbey</a>). Increasing transparency necessarily increases bureaucracy, but there are many other ways bureaucracy can increase, so we shouldn't expect it to balloon uncontrollably just because of one upward pressure.</p><p>I feel like most core EA organisations would come nowhere near meeting the transparency requirements Givewell place on charities they recommend (though Givewell themselves do impressively well on this score, so it's clearly not impossible for metacharities).</p>", "parentCommentId": "HHnhFwq7Ssj6yPnWh", "user": {"username": "Arepo"}}, {"_id": "AJF9jXzzrHw6NBBmT", "postedAt": "2022-12-20T12:44:01.511Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>As I understand it, CEA pseudo-incubated what became Alameda and/or FTX, working closely with SBF to help him get set up. Obviously that doesn't make them responsible for what happened ~5 years later, but nor does it seem reasonable to treat them as unrelated.</p>", "parentCommentId": "dszGAo5yeHXDWRf8Z", "user": {"username": "Arepo"}}, {"_id": "P5XqYxgxsa5r9HX2j", "postedAt": "2022-12-20T12:49:50.993Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<blockquote><p>\u201cThe philosophy-based contrarian culture means participants are incentivized to produce \u2018fucking insane and bad\u2019 ideas, which in turn become what many commentators latch to when trying to grasp what\u2019s distinctive about EA.\"&nbsp;</p></blockquote><p>(Was that originally in the article? If so it's been edited now)</p><p>Regardless, I've been concerned for years about the perverse incentives for (EA) academics &nbsp;both to produce weird ideas and to end the discussion of those ideas with 'more research necessary'. While I also disagree with much of the article, I'm glad to finally see that sentiment in print. It needs to be discussed <i>much</i> more IMO.</p>", "parentCommentId": "GpLDttcPNpAsmrfHe", "user": {"username": "Arepo"}}, {"_id": "sDZhioSqwXEvB78QT", "postedAt": "2022-12-20T13:51:49.926Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<blockquote><p>The stuff about academic incentives makes it sound like there's some \"commonsensical\" alternative to longtermism out there that philosophers are burying in order to be more \"interesting\", and that <a href=\"https://forum.effectivealtruism.org/posts/AgQx44vtw5bi4LmBh/puzzles-for-everyone\">just isn't true</a>. &nbsp;There's literally <i>no possible way</i> to systematize ethics without ending up somewhere puzzling.</p></blockquote><p>This seems importantly strawmanny. Matthews' point (which I strongly agree with, fwiw) is an outside view one - something like 'there are strong financial and reputational incentives for (EA) academics to reach \"interesting\" conclusions requiring more research' and thus, by what I take as its extension, that whatever the 'true importance' of such concerns is, we should expect it to be systemically overstated by those academics.</p><p>It is hardly a counterpoint to this for anyone (especially an academic!) to say 'ah, but those interesting conclusions are of true importance!' - any more than it would be to hear (say) super wealthy people arguing for lower taxation on the grounds that it encourages productivity. The arguments/inside view aren't necessarily wrong, but they just doesn't really interact with the outside view, and finding a good epistemic balance is very hard.</p><p>To date, as far as I'm aware, the EA movement has been <i>entirely</i> focused on the inside view arguments, totally ignoring the incentives Matthews observes. As interested as I personally am in utilitarian philosophy, it's very unclear to me whether any of the puzzles you mention have any practical relevance to doing good in the current world, or whether more research would make it any clearer. And in addition to the worries about population ethics, there's a whole bunch of EA-adjacent research programmes that we could completely ignore (and have taken no practical action on to date), which nonetheless get significant funding that might counterfactually have gone to mosquito nets, GCR-prevention, etc:</p><ul><li>Doomsday argument reasoning</li><li>Simulation argument reasoning</li><li>Wild animal suffering</li><li>Infinitarian ethics</li><li>Moral uncertainty</li><li>Cluelessness</li><li>Research into obscure decision theories*</li></ul><p>* (less sure about this one. Maybe MIRI have done something with it behind closed doors, but if so I don't believe they've communicated it)</p><p>On top of those examples, Will has openly advocated the importance of '<a href=\"https://www.youtube.com/watch?v=8s1RUDFM_Mk&amp;t=294s\">keeping &nbsp;EA weird</a>'.</p><p>So I think this is an issue that deserves a lot more scrutiny (presumably, ironically, most of which would come from academic EAs).</p>", "parentCommentId": "8fj7wn8gQfLfK2iiN", "user": {"username": "Arepo"}}, {"_id": "i8EAxp49gmzYX469y", "postedAt": "2022-12-20T14:38:15.334Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>Distinguish two critiques in this general vicinity:</p><p>(1) Longtermism seems weird <i>because </i>its main proponents are philosophers who have professional incentives to make \"interesting\"/extreme claims regardless of their truth or plausibility.</p><p>(2) Academics are likely to \"systematically overstate\" the importance of their own research, so we shouldn't take their claims about \"true importance\" at face value.</p><p><strong>These are two very different critiques!&nbsp;</strong> Matthews clearly said (1), and that's what I was responding to. &nbsp;His explanatory claim is demonstrably false. &nbsp;Your critique (2) seems right to me, though a trivial generalization of the broader claim:</p><p>(2*) Everyone is likely to systematically overstate the importance of their own work, so we shouldn't take their claims about the true importance of their work at face value.</p><p>I agree that we need to critically evaluate claims that someone's work is important. &nbsp;There's nothing special about academic work in this respect, though.</p>", "parentCommentId": "sDZhioSqwXEvB78QT", "user": {"username": "RYC"}}, {"_id": "wFy6z4Z8PnTstAr3f", "postedAt": "2022-12-20T14:57:06.737Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<blockquote><p>I agree that we need to critically evaluate claims that someone's work is important. &nbsp;There's nothing special about academic work in this respect, though.</p></blockquote><p>Strong disagree with this part. Academics, in the sense of 'people who are paid to do specialised research' are substantially more incentivised to overstate their value than a) people who aren't paid, or b) people who are paid to do more superficial/multi-focus research (eg consultants), and who could therefore pivot easily if it turned out some project they were on was low value.</p>", "parentCommentId": "i8EAxp49gmzYX469y", "user": {"username": "Arepo"}}, {"_id": "vsppFEz6n3wzWetwd", "postedAt": "2022-12-20T15:22:57.178Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>It sounds like you're talking about researchers outside of academia. &nbsp;Academics aren't paid directly for their research, and the objective \"importance\" of our research counts for literally nothing in tenure and promotion decisions, compared to more mundane metrics like how many papers we've published and in what venues, and whether it is deemed suitably impressive (by disciplinary standards, which again have zero connection to objective importance) by senior evaluators within the discipline.</p><p>A tenured academic, like a supreme court justice, has a job for life which leaves them far less vulnerable to incentives than almost anyone else.</p>", "parentCommentId": "wFy6z4Z8PnTstAr3f", "user": {"username": "RYC"}}, {"_id": "WKsfDumHqANB3uH3q", "postedAt": "2022-12-20T22:30:49.272Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>That's not my understanding? I'm curious where you heard that from.</p>\n<p>Either way, I stand by this - I believe CEA <em>in particular</em> was a big mess back then:</p>\n<blockquote>\n<p>(Also, Alameda was founded in ~2018, where the EA scene was very, very different and much less mature (and probably much worse governed). I expect many bad governance decisions are baked in when an org is founded)</p>\n</blockquote>\n", "parentCommentId": "AJF9jXzzrHw6NBBmT", "user": {"username": "Neel Nanda"}}, {"_id": "pWf3vwwv8fQ4C4eHp", "postedAt": "2022-12-20T23:53:32.150Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>Why was this downvoted?</p>", "parentCommentId": "vsppFEz6n3wzWetwd", "user": {"username": "RYC"}}, {"_id": "iNT4z5huqtrsfKp6o", "postedAt": "2022-12-21T19:02:35.312Z", "postId": "M6ebKkRos2nQe9gqG", "htmlBody": "<p>Just seeing this, but yes it was a quote from the original piece! FWIW I appreciate your use of \u201cweird\u201d vs. the original author\u2019s more colorful language (though no idea if that\u2019s what your pre-edit comment was in reference to)</p>\n", "parentCommentId": "P5XqYxgxsa5r9HX2j", "user": {"username": "JeremyR"}}]