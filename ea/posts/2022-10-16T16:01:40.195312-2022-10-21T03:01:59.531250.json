[{"_id": "9YLbtehKLT4ByLvos", "title": "AGI misalignment x-risk may be lower due to an overlooked goal specification technology", "postedAt": "2022-10-21T02:03:27.344Z", "htmlBody": "", "user": {"username": "johnjnay"}}, {"_id": "DmGYDeSKonHKdztFa", "title": "Copies sold of What We Owe the Future", "postedAt": "2022-10-21T00:18:44.504Z", "htmlBody": "<p>Does anyone know roughly how many copies of What We Owe the Future by William MacAskill have been sold so far? If not, any ideas how I could estimate this?</p>", "user": {"username": "Anthony Fleming"}}, {"_id": "g6tj62H5L8zugqcxD", "title": "aisafety.community - A living document of AI safety communities", "postedAt": "2022-10-20T22:08:45.768Z", "htmlBody": "", "user": {"username": "zeshen"}}, {"_id": "Rxbp5FtLzsr9qcwgC", "title": "Rough Sketch for Product to Enhance Citizen Participation in Politics", "postedAt": "2022-10-20T20:09:11.211Z", "htmlBody": "", "user": {"username": "rodeo_flagellum"}}, {"_id": "q4TonbjzuBiDLcBPE", "title": "Open position at 80,000 Hours: Recruiter", "postedAt": "2022-10-20T16:56:14.108Z", "htmlBody": "<h1><strong>Summary</strong></h1><p>We\u2019re <a href=\"https://80000hours.org/2022/10/open-position-recruiter/\">hiring a recruiter</a> to help us grow the 80,000 Hours team.</p><p>Not being able to hire fast enough is one of our biggest bottlenecks as an organisation. The person in this role will directly address this by helping to identify potential candidates, run hiring rounds, and scale our recruitment processes as we grow. They\u2019ll be key to increasing 80,000 Hours\u2019 impact over the coming years.</p><p>You might be a great fit if you:</p><ul><li><strong>Have a strong understanding of effective altruism / longtermism.</strong></li><li><strong>Have excellent organisational / project management skills.</strong></li><li><strong>Are a clear communicator, both in writing and in person.</strong></li><li><strong>Would enjoy interacting with and evaluating people.</strong></li></ul><p>This is a full-time role. Ideally, you\u2019d be based at our London office, but we\u2019re open to remote candidates (as long as you\u2019re able to work within UK working hours).</p><p>We\u2019re willing to consider candidates who are available to start anytime between <strong>December 2022 and August 2023</strong> \u2014 so please consider applying even if you\u2019re not available immediately.</p><p>The starting salary for someone with one year of highly relevant experience is <strong>\u00a359,200 per year.</strong></p><h1><strong>The role</strong></h1><p>You\u2019ll be managed by <a href=\"https://www.linkedin.com/in/sashika-coxhead-68ba09100/\">Sashika Coxhead</a>, our Head of Recruiting, and will have the opportunity to work closely with hiring managers from other teams.</p><p>Initial responsibilities will include:</p><ul><li><strong>Project management of active recruiting rounds.</strong> For example, overseeing the candidate pipeline and logistics of hiring rounds, making decisions on initial applications, and managing candidate communications.</li><li><strong>Sourcing potential candidates.</strong> This might include generating leads for specific roles, publicising new positions, reaching out to potential candidates, and answering any questions they have about working at 80,000 Hours.</li><li><strong>Taking on special projects to improve our recruiting systems.</strong> For example, you might help to build an excellent applicant tracking system, test ways to improve our ability to generate leads, or introduce strategies to make our hiring rounds more efficient.</li></ul><p>Depending on your skills and interests, you might also:</p><ul><li><strong>Take ownership of a particular area of our recruiting process</strong>, e.g. proactive outreach to potential candidates, our applicant tracking system, or metrics for the recruiting team\u2019s success.</li><li><strong>Conduct screening interviews</strong> where needed, to assess applicants\u2019 fit for particular roles at 80,000 Hours.</li></ul><p>After some time in the role, we\u2019d hope for you to sit on <a href=\"https://rework.withgoogle.com/guides/hiring-hire-by-committe/steps/introduction/\">internal hiring committees</a>. This involves forming an inside view on candidates\u2019 performance; discussing uncertainties with the hiring manager and committee; and, with the other committee members, giving final approval on who to make offers to.</p><h1><strong>Who we\u2019re looking for</strong></h1><p>You might be a great fit if you:</p><ul><li>Have <strong>a strong interest in and understanding of effective altruism and longtermism</strong>. Ideally, you have evidence of previous involvement with EA (e.g. being a member or organiser of a local or university group, attending a previous EAG or EAGx conference, volunteering or working with an EA organisation) and have a deep understanding of key EA concepts.</li><li>Are <strong>highly conscientious and organised</strong>. You take pride in your attention to detail, can keep track of multiple moving parts within a project, and don\u2019t let things fall through the cracks. Ideally, you have previous experience successfully managing projects (e.g. running events or doing local/university group organising).</li><li>Are <strong>a clear communicator</strong>, both in writing and in person. You\u2019re able to discuss uncertainties with hiring managers efficiently and can communicate clearly but sensitively with candidates.</li><li>Are <strong>interested in thinking about people</strong>. You have strong interpersonal skills, love trying to develop accurate models of others, and would enjoy thinking about their fit for roles at 80,000 Hours.</li></ul><p>You <strong>don\u2019t</strong> need any previous experience with recruiting to apply. In fact, we\u2019d encourage you to apply even if you\u2019re not sure you meet all of the above criteria \u2013 we\u2019d much prefer to hear from you than not!</p><p>We\u2019re aware that factors like gender, race, and socioeconomic background can affect people\u2019s willingness to apply for roles for which they meet many but not all of the suggested attributes. We\u2019d especially like to encourage people from underrepresented backgrounds to express interest in this role, including if you don\u2019t meet all the suggested criteria.</p><h1><strong>Role details</strong></h1><p>This is a full-time role. Ideally, you\u2019d be based at our London office, but we\u2019re open to remote candidates (as long as you\u2019re able to work within UK working hours). We are able to sponsor visa applications if required.</p><p>We\u2019re willing to consider candidates who are available to start anytime between <strong>December 2022 and August 2023</strong> \u2014 so please consider applying even if you\u2019re not available immediately.</p><p>The salary will depend on your previous experience, but to give a rough sense, the starting salary for someone with one year of highly relevant experience would be <strong>\u00a359,200 per year.</strong></p><p>Our benefits include:</p><ul><li>The option to use 10% of your time for self development.</li><li>25 days of paid holiday, plus bank holidays.</li><li>Standard UK pension with 3% contribution from employer.</li><li>\u00a35,000 mental health support allowance.</li><li>Private medical insurance.</li><li>Generous paid parental leave.</li><li>Long-term disability insurance.</li><li>Gym, shower facilities, and free food provided at our London office.</li></ul><h1><strong>Application process</strong></h1><p>To apply, please complete <a href=\"https://80000hours.typeform.com/recruiter\">this application form</a> by <strong>11pm GMT on Wednesday, November 2, 2022</strong>.</p><p><i>We\u2019re reviewing applications on a rolling basis, so we encourage you to apply as soon as you are able to.</i></p><p>The application process will vary depending on the candidate, but will likely involve:</p><ul><li>1\u20133 (paid) written work samples</li><li>A ~45-minute interview</li><li>A ~3-day in-person trial</li></ul>", "user": {"username": "80000_Hours"}}, {"_id": "bm4qeNJcc82BKJnWk", "title": "The heritability of human values: A behavior genetic critique of Shard Theory", "postedAt": "2022-10-20T15:53:55.461Z", "htmlBody": "<p><strong>Overview (TL;DR):</strong></p><p>Shard Theory is a new approach to understanding the formation of human values, which aims to help solve the problem of how to align advanced AI systems with human values (the \u2018AI alignment problem\u2019). <a href=\"https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values\">Shard Theory</a> has provoked a lot of interest and discussion on LessWrong, AI Alignment Forum, and EA Forum in recent months. However, Shard Theory incorporates a relatively Blank Slate view about the origins of human values that is empirically inconsistent with many studies in behavior genetics indicating that many human values show heritable genetic variation across individuals. I\u2019ll focus in this essay on the empirical claims of Shard Theory, the behavior genetic evidence that challenges those claims, and the implications for developing more accurate models of human values for AI alignment.</p><p><strong>Introduction: Shard Theory as an falsifiable theory of human values</strong></p><p>The goal of the \u2018AI alignment\u2019 field is to help future Artificial Intelligence systems become better aligned with human values. Thus, to achieve AI alignment, we might need a good theory of human values. A new approach called \u201c<a href=\"https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values\">Shard Theory</a>\u201d aims to develop such a theory of how humans develop values.&nbsp;</p><p>My goal in this essay is to assess whether Shard Theory offers an empirically accurate model of human value formation, given what we know from behavior genetics about the heritability of human values. The stakes here are high. If Shard Theory becomes influential in guiding further alignment research, but if its model of human values is not accurate, then Shard Theory may not help improve AI safety.&nbsp;</p><p>These kinds of empirical problems are not limited to Shard Theory. Many proposals that I\u2019ve seen for AI \u2018alignment with human values\u2019 seem to ignore most of the research on human values in the behavioral and social sciences. I\u2019ve tried to challenge this empirical neglect of value research in four previous essays for EA Forum, on the <a href=\"https://forum.effectivealtruism.org/posts/KZiaBCWWW3FtZXGBi/the-heterogeneity-of-human-value-types-implications-for-ai\">heterogeneity of value types</a> in humans individuals, the <a href=\"https://forum.effectivealtruism.org/posts/DXuwsXsqGq5GtmsB3/ai-alignment-with-humans-but-with-which-humans\">diversity of values across individuals</a>, the importance of <a href=\"https://forum.effectivealtruism.org/posts/zNS53uu2tLGEJKnk9/ea-s-brain-over-body-bias-and-the-embodied-value-problem-in\">body/corporeal values</a>, and the importance of <a href=\"https://forum.effectivealtruism.org/posts/YwnfPtxHktfowyrMD/the-religion-problem-in-ai-alignment\">religious values</a>.&nbsp;</p><p>Note that this essay is a rough draft of some preliminary thoughts, and I welcome any feedback, comments, criticisms, and elaborations. In future essays I plan to critique Shard Theory from the perspectives of several other fields, such as evolutionary biology, animal behavior research, behaviorist learning theory, and evolutionary psychology.</p><p><strong>Background on Shard Theory</strong></p><p>Shard Theory has been developed mostly by Quintin Pope (a computer science Ph.D. student at Oregon State University) and Alex Turner (a post-doctoral researcher at the Center for Human-Compatible AI at UC Berkeley). Over the last few months, they posted a series of essays about Shard Theory on LessWrong.com, including this main essay here , \u2018<a href=\"https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values\">The shard theory of human values\u2019</a> (dated Sept 3, 2022), plus auxiliary essays such as: \u2018<a href=\"https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome\">Human values &amp; biases are not accessible to the genome\u2019</a> (July 7, 2022), \u2018<a href=\"https://www.lesswrong.com/posts/CjFZeDD6iCnNubDoS/humans-provide-an-untapped-wealth-of-evidence-about\">Humans provide an untapped wealth of evidence about alignment</a>\u2019 (July 13, 2022), \u2018<a href=\"https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target\">Reward is not the optimizer\u2019</a> (July 24, 2022), and \u2018<a href=\"https://www.lesswrong.com/posts/FyChg3kYG54tEN3u6/evolution-is-a-bad-analogy-for-agi-inner-alignment\">Evolution is a bad analogy for AGI: Inner alignment</a>\u2019 (Aug 13, 2022). [This is not a complete list of their Shard Theory writings; it\u2019s just the set that seems most relevant to the critiques I\u2019ll make in this essay.] Also, David Udell published this useful summary: \u2018<a href=\"https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview\">Shard Theory: An overview\u2019</a> (Aug 10, 2022).&nbsp;</p><p>There\u2019s a lot to like about Shard Theory. It takes seriously the potentially catastrophic risks from AI. It understands that \u2018AI alignment with human values\u2019 requires some fairly well-developed notions about where human values come from, what they\u2019re for, and how they work. It is intellectually ambitious, and tries to integrate reinforcement learning, self-supervised predictive learning, decision theory, developmental psychology, and cognitive biases. It seeks to build some common ground between human intelligence and artificial intelligence, at the level of how complex cognitive systems develop accurate world models and useful values. It tries to be explicit about its empirical commitments and theoretical assumptions. It is open about being a work-in-progress rather than a complete, comprehensive, or empirically validated theory. It has already provoked much discussion and debate.</p><p>Even if my critiques of Shard Theory are correct, and some of its key evolutionary, genetic, and psychological assumptions are wrong, that isn\u2019t necessarily fatal to the whole Shard Theory project. I imagine some form of Shard Theory 2.0 could be developed that updates its assumptions in the light of these critiques, and that still makes some progress in developing a more accurate model of human values that is useful for AI alignment.</p><p><strong>Shard Theory as a Blank Slate theory</strong></p><p>However, Shard Theory includes a model of human values that is not consistent with what behavioral scientists have learned about the origins and nature of values over the last 170 years of research in psychology, biology, animal behavior, neurogenetics, behavior genetics, and other fields.</p><p>The key problem is that Shard Theory re-invents a relatively \u2018Blank Slate\u2019 theory of human values. Note that no Blank Slate theory posits that the mind is 100% blank. Every Blank Slate theory that\u2019s even marginally credible accepts that there are at least a few \u2018innate instincts\u2019 and some \u2018hardwired reward circuitry\u2019. Blank Slate theories generally accept that human brains have at least a few \u2018innate reinforcers\u2019 that can act as a scaffold for the socio-cultural learning of everything else. For example, even the most radical Blank Slate theorists would generally agree that sugar consumption is reinforcing because we evolved taste receptors for sweetness.&nbsp;</p><p>The existence of a few innate reinforcement circuits was accepted even by the most radical Behaviorists of the 1920s through 1960s, and by the most \u2018social constructivist\u2019 researchers in the social sciences and humanities from the 1960s onwards. Blank Slate theorists just try to minimize the role of evolution and genetics in shaping human psychology, and strongly favor Nurture over Nature in explaining both psychological commonalities across sentient beings, and psychological differences across species, sexes, ages, and individuals. Historically, Blank Slate theories were motivated not so much by empirical evidence, as by progressive political ideologies about the equality and perfectibility of humans. (See the 2002 book <a href=\"https://en.wikipedia.org/wiki/The_Blank_Slate\">The Blank Slate</a> by Steven Pinker, and the 2000 book <a href=\"https://www.amazon.com/Defenders-Truth-Sociobiology-Ullica-Segerstrale/dp/0192862154\">Defenders of the Truth</a> by Ullica Segerstrale.)</p><p>Shard Theory seems to follow in that tradition \u2013 although I suspect that it\u2019s not so much due to political ideology, as to a quest for theoretical simplicity, and for not having to pay too much attention to the behavioral sciences in chasing AI alignment.</p><p>At the beginning of their <a href=\"https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values\">main statement</a> of Shard Theory, in their TL;DR, Pope and Turner include this bold statement: \u201cHuman values are not e.g. an incredibly complicated, genetically hard-coded set of drives, but rather sets of contextually activated heuristics which were shaped by and bootstrapped from crude, genetically hard-coded reward circuitry.\u201d&nbsp;</p><p>Then they make three explicit neuroscientific assumptions. I\u2019ll focus on Assumption 1 of Shard Theory: \u201cMost of the circuits in the brain are learned from scratch, in the sense of being mostly randomly initialized and not mostly genetically hard-coded.\u201d</p><p>This assumption is motivated by an argument explored <a href=\"https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome\">here</a> that \u2018human values and biases are inaccessible to the genome\u2019. For example, Quintin Trout argues \u201cit seems intractable for the genome to scan a human brain and back out the \u201cdeath\u201d abstraction, which probably will not form at a predictable neural address. Therefore, we infer that the genome can\u2019t directly make us afraid of death by e.g. specifying circuitry which detects when we think about death and then makes us afraid. In turn, this implies that there are a lot of values and biases which the genome cannot hardcode.\u201d&nbsp;</p><p>This Shard Theory argument seems to reflect a fundamental misunderstanding of how evolution shapes genomes to produce phenotypic traits and complex adaptations. The genome never needs&nbsp;to \u2018scan\u2019 an adaptation and figure out how to reverse-engineer it back into genes. The genetic variants simply build a slightly new phenotypic variant of an adaptation, and if it works better than existing variants, then the genes that built it will tend to propagate through the population. The flow of design information is always from genes to phenotypes, even if the flow of selection pressures is back from phenotypes to genes. This one-way flow of information from DNA to RNA to proteins to adaptations has been called the \u2018<a href=\"https://en.wikipedia.org/wiki/Central_dogma_of_molecular_biology\">Central Dogma of molecular biology\u2019</a>, and it still holds largely true (the recent hype about epigenetics notwithstanding).&nbsp;</p><p>Shard Theory implies that biology has no mechanism to \u2018scan\u2019 the design of fully-mature, complex adaptations back into the genome, and therefore there\u2019s no way for the genome to code for fully-mature, complex adaptations. If we take that argument at face value, then there\u2019s no mechanism for the genome to \u2018scan\u2019 the design of a human spine, heart, hormone, antibody, cochlea, or retina, and there would be no way for evolution or genes to influence the design of the human body, physiology, or sensory organs. Evolution would grind to a halt \u2013 not just at the level of human values, but at the level of all complex adaptations in all species that have ever evolved.&nbsp;</p><p>As we will see, this idea that \u2018human values and biases are inaccessible to the genome\u2019 is empirically incorrect.</p><p><strong>A behavior genetic critique of Shard Theory</strong></p><p>In future essays, I plan to address the ways that Shard Theory, as presently conceived, is inconsistent with findings from several other research areas: (1) evolutionary biology models of how complex adaptations evolve, (2) animal behavior models of how nervous systems evolved to act in alignment with fitness interests, (3) behaviorist learning models of how reinforcement learning and reward systems operate in animals and humans, and (4) evolutionary psychology models of human motivations, emotions, preferences, morals, mental disorders, and personality traits.</p><p>For now, I want to focus on some conflicts between Shard Theory and behavior genetics research. As mentioned above, Shard Theory adopts a relatively \u2018Blank Slate\u2019 view of human values, positing that we inherit only a few simple, crude values related to midbrain reward circuitry, which are presumably universal across humans, and all other values are scaffolded and constructed on top of those.</p><p>However, behavior genetics research over the last several decades has shown show that most human values that differ across people, and that can be measured reliably \u2013 including some quite abstract values associated with political, religious, and moral ideology \u2013 are moderately heritable. Moreover, many of these values show relatively little influence from \u2018shared family environment\u2019, which includes all of the opportunities and experiences shared by children growing up in the same household and culture. This means that genetic variants influence the formation of human values, and genetic differences between people explain a significant proportion of the differences in their adult values, and family environment explains a lot less about differences in human values than we might have thought. This research is based on convergent findings using diverse methods such as <a href=\"https://en.wikipedia.org/wiki/Twin_study\">twin studies</a>, <a href=\"https://en.wikipedia.org/wiki/Adoption_study\">adoption studies</a>, <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3228846/\">extended twin family designs</a>, <a href=\"https://en.wikipedia.org/wiki/Complex_segregation_analysis\">complex segregation analysis</a>, and <a href=\"https://en.wikipedia.org/wiki/Genome-wide_association_study\">genome-wide association studies</a> (GWAS). All of these behavior genetic observations are inconsistent with Shard Theory, particularly its Assumption 1.&nbsp;</p><p>Behavior genetics was launched in 1869 when <a href=\"https://en.wikipedia.org/wiki/Francis_Galton\">Sir Francis Galton</a> published his book <i>Hereditary Genius</i>, which proposed some empirical methods for studying the inheritance of high levels of human intelligence. A few years earlier, Galton\u2019s cousin Charles Darwin had developed the theory of evolution by natural selection, which focused on the interplay of heritable genetic variance and evolutionary selection pressures. Galton was interested in how scientists might analyze heritable genetic variance in human mental traits such as intelligence, personality, and altruism. He understood that Nature and Nurture interact in very complicated ways to produce species-typical human universals. However, he also understood that it was an open question how much variation in Nature versus variation in Nurture contributed to individual differences in each trait.</p><p>Note that behavior genetics was always about explaining the factors that influence statistical variation in quantitative traits, not about explaining the causal, mechanistic development of traits. This point is often misunderstood by modern critics of behavior genetics who claim \u2018every trait is an inextricable combination of Nature and Nurture, so there\u2019s no point in trying to partition their influence.\u2019 The mapping from genotype (the whole set of genes in an organism) to phenotype (the whole set of body, brain, and behavioral traits in an organism) is, indeed, extremely complicated and remains poorly understood. However, behavior genetics doesn\u2019t need to understand the whole mapping; it can trace how genetic variants influence phenotypic trait variants using empirical methods such as twin, adoption, and GWAS studies.&nbsp;</p><p>In modern behavior genetics, the influence of genetic variants on traits is indexed by a metric called <a href=\"https://en.wikipedia.org/wiki/Heritability\">heritability</a>, which can range from 0 (meaning genetic variants have no influence on individual differences in a phenotypic trait) to 1 (meaning genetic variants explain 100% of individual differences in a phenotypic trait). So-called \u2018narrow-sense heritability\u2019 includes only additive genetic effects due to the average effects of alleles; additive genetic effects are most important for predicting responses to evolutionary selection pressures \u2013 whether in the wild or in artificial selective breeding of domesticated species. \u2018Broad-sense heritability\u2019 includes additive effects plus dominant and epistatic genetic effects. For most behavioral traits, additive effects are by far the most important, so broad-sense heritability is usually only a little higher than narrow-sense heritability.&nbsp;</p><p>The most important result from behavior genetics is that all human behavioral traits that differ across people, and that can be measured reliably, are heritable to some degree \u2013 and often to a surprisingly high degree. This is sometimes called the <a href=\"http://faculty.umb.edu/peter_taylor/epi/turkheimer00.pdf\">First Law of Behavior Genetics</a> \u2013 not because it\u2019s some kind of natural law that all behavioral traits must be heritable, but because the last 150 years of research has found no replicable exceptions to this empirical generalization. Some behavioral traits such as general intelligence show very high heritability \u2013 over <a href=\"https://www.cambridge.org/core/journals/twin-research-and-human-genetics/article/wilson-effect-the-increase-in-heritability-of-iq-with-age/FF406CC4CF286D78AF72C9E7EF9B5E3F\">0.70</a> \u2013 in adults, which is about as heritable as human <a href=\"https://www.nature.com/articles/d41586-019-01157-y\">height</a>. (For a good recent introduction to the \u2018Top 10 replicated findings from behavior genetics, see <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4739500/\">this paper</a>.)</p><p><strong>Does anybody really believe that values are heritable?</strong></p><p>To people who accept a Blank Slate view of human nature, it might seem obvious that human values, preferences, motivations, and moral judgments are instilled by family, culture, media, and institutions \u2013 and the idea that genes could influence values might sound absurd. Conversely, to people familiar with behavior genetics, who know that all psychological traits are somewhat heritable, it might seem obvious that human values, like other psychological traits, will be somewhat heritable. It\u2019s unclear what proportion of people lean towards the Blank Slate view of human values, versus the \u2018hereditarian\u2019 view that values can be heritable.</p><p>As a reality check, I ran this Twitter poll on Oct 17, 2022, with the results shown in this screenshot:</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/747647dce9b0d5f82deb2efa0f56308444a14a2a2bd67553.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/747647dce9b0d5f82deb2efa0f56308444a14a2a2bd67553.png/w_97 97w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/747647dce9b0d5f82deb2efa0f56308444a14a2a2bd67553.png/w_177 177w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/747647dce9b0d5f82deb2efa0f56308444a14a2a2bd67553.png/w_257 257w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/747647dce9b0d5f82deb2efa0f56308444a14a2a2bd67553.png/w_337 337w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/747647dce9b0d5f82deb2efa0f56308444a14a2a2bd67553.png/w_417 417w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/747647dce9b0d5f82deb2efa0f56308444a14a2a2bd67553.png/w_497 497w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/747647dce9b0d5f82deb2efa0f56308444a14a2a2bd67553.png/w_577 577w\"></p><p>I was surprised that so many people took a slightly or strongly hereditarian view of values. Maybe the idea isn\u2019t as crazy as it might seem at first glance. However, this poll is just illustrative that there is real variation in people\u2019s views about this. It should not be taken too seriously as data, because it is just one informal question on social media, answered by a highly non-random sample. Only about 1.4% of my followers (1,749 out of 124,600) responded to this poll (which is a fairly normal response rate). My typical follower is an American male who\u2019s politically centrist, conservative, or libertarian, and probably has a somewhat more hereditarian view of human nature than average. The poll\u2019s main relevance here is in showing that a lot of people (not just me) believe that values can be heritable.</p><p><strong>Human traits in general are heritable</strong></p><p>A 2015 <a href=\"https://www.nature.com/articles/ng.3285.\">meta-analysis</a> of human twin studies analyzed 17,804 traits from 2,748 papers including over 14 million twin pairs. These included mostly behavioral traits (e.g. psychiatric conditions, cognitive abilities, activities, social interactions, social values), and physiological traits (e.g. metabolic, neurological, cardiovascular, and endocrine traits). Across all traits, average heritability was .49, and shared family environment (e.g. parenting, upbringing, local culture) typically had negligible effects on the traits. For 69% of traits, heritability seemed due solely to additive genetic variation, with no influence of dominance or epistatic genetic variation.&nbsp;</p><p>Heritability of human traits is generally caused by many genes that each have very small, roughly additive effects, rather than by a few genes that have big effects (see <a href=\"https://journals.sagepub.com/doi/abs/10.1177/1745691615617439\">this review</a>). Thus, to predict individual values for a given trait, molecular behavior genetics studies generally aggregate the effects of thousands of DNA variants into a <a href=\"https://en.wikipedia.org/wiki/Polygenic_score\">polygenic score</a>. Thus, each trait is influenced by many genes. But also, each gene influences many traits (this is called <a href=\"https://en.wikipedia.org/wiki/Pleiotropy\">pleiotropy</a>). So, there is a complex <a href=\"https://en.wikipedia.org/wiki/Genetic_architecture\">genetic architecture</a> that maps from many genetic variants onto many phenotypic traits, and this can be explored using multivariate behavior genetics methods that track <a href=\"https://en.wikipedia.org/wiki/Genetic_correlation\">genetic correlations</a> between traits. (Elucidating the genetic architecture of human values would be enormously useful for AI alignment, in my opinion.)</p><p><strong>Human values are heritable</strong></p><p>The key point here, in relation to Shard Theory, is that \u2018all human behavioral traits\u2019 being heritable includes \u2018all human values that differ across people\u2019. Over the last few decades, behavior geneticists have expanded their focus from studying classic traits, such as general intelligence and mental disorders, to explicitly studying the heritability of human values, and values-adjacent traits. So far, behavior geneticists have found mild to moderate heritability for a wide range of values-related traits, including the following:</p><ul><li>Food preferences are <a href=\"https://www.mdpi.com/2072-6643/11/8/1735\">heritable</a>, and they are not just influenced by genes that predict basic taste or smell functions. Genes influence <a href=\"https://www.mdpi.com/2072-6643/11/8/1735\">heritabilities of tastes</a> for specific food categories such as vegetables, fruit, starchy foods, meat/fish, dairy, and snacks. <a href=\"https://www.sciencedirect.com/science/article/pii/S0950329321003037\">Different genes</a> underlie meat preferences in men versus women. Food fussiness and food neophobia are both <a href=\"https://acamh.onlinelibrary.wiley.com/doi/full/10.1111/jcpp.12647\">heritable in kids</a>, and reflect a common genetic etiology. Obesity, reflecting a high reward-sensitivity for food, is about <a href=\"https://onlinelibrary.wiley.com/doi/full/10.1002/oby.23116\">45% heritable</a>.</li><li>Mate preferences are somewhat heritable, including <a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/j.1558-5646.2011.01546.x\">rated importance</a> of key traits in potential partners, and <a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0049294\">preference for partner height</a>. These heritable mate preferences can lead to positive genetic correlations between the preferences and with the actual traits preferred, as in <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S1090513814000798\">this study</a> of height, intelligence, creativity, exciting personality, and religiosity.</li><li>Sexual values, reinforcers, and reward systems are heritable, including <a href=\"https://www.nature.com/articles/s41598-017-15736-4\">sexual orientation</a>, <a href=\"https://www.tandfonline.com/doi/abs/10.1080/03637751.2020.1760327\">affectionate communication</a>, <a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1743-6109.2011.02300.x\">frequency of female orgasm</a>, <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S1090513814001317\">extrapair mating</a> (infidelity), <a href=\"https://www.sciencedirect.com/science/article/pii/S1090513821000611\">sexual jealousy</a>, and <a href=\"https://www.tandfonline.com/doi/abs/10.1080/10683160802621925\">sexual coerciveness</a>.</li><li>Parenting behaviors and values are heritable, according to a <a href=\"https://psycnet.apa.org/doiLanding?doi=10.1037%2Fa0034205\">meta-analysis</a> of 56 studies. Also, the shared family environment created by parents when raising their kids has many heritable components (according to studies on the \u2018heritability of the environment\u2019, and \u2018the <a href=\"https://www.taylorfrancis.com/chapters/edit/10.4324/9780203838013-8/nature-nurture-robert-plomin\">Nature of Nurture\u2019</a>.)</li><li>Economic values and consumer preferences are heritable, including <a href=\"https://academic.oup.com/jcr/article-abstract/37/6/951/1869443\">consumer decision heuristics</a>, <a>vocational interests</a>, <a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0060542\">preferences for self-employment</a>, <a href=\"https://www.sciencedirect.com/science/article/pii/S0883902619301247\">entrepreneurship</a>, <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0006322314008282\">delay discounting</a>, <a href=\"https://www.pnas.org/doi/abs/10.1073/pnas.1120666109\">economic policy preferences</a>, <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0304405X14000889\">investment biases</a>, <a href=\"https://www.nature.com/articles/s41562-021-01053-4\">socio-economic status</a>, and <a href=\"https://link.springer.com/article/10.1007/s10888-019-09413-x\">lifetime earnings</a>.</li><li>Moral values are <a href=\"https://journals.sagepub.com/doi/abs/10.1177/1948550611412793\">heritable</a>, including <a href=\"https://link.springer.com/article/10.1007/s12110-020-09380-7\">moral intuitions</a>, <a href=\"https://www.nature.com/articles/mp2017122\">cognitive empathy</a>, <a href=\"https://www.nature.com/articles/s41598-022-09253-2\">justice sensitivity</a>, <a href=\"https://www.sciencedirect.com/science/article/pii/S2352250X15001323\">prosociality</a>, <a href=\"https://www.sciencedirect.com/science/article/pii/S0149763418307905\">self-control</a>, <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S016726811300125X\">attitudes towards dishonesty</a>, and <a href=\"https://www.sciencedirect.com/science/article/pii/S095032932200180X\">vegetarianism</a>.</li><li>Immoral behaviors and values are also heritable, including <a href=\"https://link.springer.com/article/10.1007/s10519-011-9483-0\">violent crime</a>, <a href=\"https://academic.oup.com/ije/article/44/2/713/753089\">sexual coercion</a>, <a href=\"https://www.cambridge.org/core/journals/psychological-medicine/article/abs/swedish-national-twin-study-of-criminal-behavior-and-its-violent-whitecollar-and-property-subtypes/0D9A88185ED0FD5525A5EBD5D2EBA117\">white-collar crime</a>, and <a href=\"https://link.springer.com/article/10.1007/s10578-020-01119-w\">juvenile delinquency</a>.</li><li>Political values are about <a href=\"https://www.pnas.org/doi/abs/10.1073/pnas.1818711116\">40% heritable</a>; see <a href=\"https://journals.sagepub.com/doi/abs/10.1177/14789299211053780\">2021 review here</a>; these heritable political values include <a href=\"https://www.cambridge.org/core/journals/journal-of-experimental-political-science/article/abs/genes-ideology-and-sophistication/91C7C343BBA8801732F62E7D55B16676\">conservatism</a>, <a href=\"https://www.journals.uchicago.edu/doi/abs/10.1017/S0022381610001015\">liberalism</a>, <a href=\"https://www.pnas.org/doi/abs/10.1073/pnas.1818711116\">social dominance orientation</a>, <a href=\"https://journals.sagepub.com/doi/abs/10.1177/1065912917698045\">political engagement</a>, <a href=\"https://www.elgaronline.com/view/edcoll/9781782545101/9781782545101.00020.xml\">political trust</a>, <a href=\"https://www.cambridge.org/core/journals/politics-and-the-life-sciences/article/genes-personality-and-political-behavior/CE6A2F64A262E29F396893965E286FAF\">political interest</a>, <a href=\"https://www.cambridge.org/core/journals/twin-research-and-human-genetics/article/genetic-basis-of-political-sophistication/9E69BA562FEF42FA4F7117ED1E3FF0EE\">political sophistication</a>, <a href=\"https://journals.sagepub.com/doi/abs/10.1177/0095327X18765449\">military service</a>, <a href=\"https://www.cambridge.org/core/journals/twin-research-and-human-genetics/article/heritability-of-foreign-policy-preferences/61AD34FFC1B0FF174FDFC6AA819050D4\">foreign policy preferences</a>, <a href=\"https://royalsocietypublishing.org/doi/full/10.1098/rstb.2015.0015\">civic engagement</a>, and <a href=\"https://www.jstor.org/stable/23260396\">voter turnout</a>.</li><li>Religious values are heritable, including overall <a href=\"https://link.springer.com/article/10.1007/s10519-010-9388-3\">religiosity</a>, <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0092656613000500\">existential certainty</a>, <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0191886913001384\">obedience to traditional authority</a>, and <a href=\"https://www.cambridge.org/core/journals/twin-research-and-human-genetics/article/is-apostasy-heritable-a-behavior-genetics-study/2F93769FEBAACB2FC4AFC502B123BA83\">apostasy</a>.</li></ul><p>In addition, the <a href=\"https://en.wikipedia.org/wiki/Big_Five_personality_traits\">Big Five personality traits</a> are moderately heritable (about 40%) according to this <a href=\"https://psycnet.apa.org/record/2015-20360-001?doi=1\">2015 meta-analysis</a> of 134 studies.&nbsp; Each personality trait is centered around some latent values that represent how rewarding and reinforcing various kinds of experiences are. For example, people higher in Extraversion value social interaction and energetic activity more, people higher in Openness value new experiences and creative exploration more, people higher in Agreeableness value friendliness and compassion more, people higher in Conscientiousness value efficiency and organization more, and people higher in Neuroticism value safety and risk-aversion more. Each of these personality traits is heritable, so these values are also heritable. In fact, personality traits might be central to the genetic architecture of human values.</p><p>Moreover, common mental disorders, which are <a href=\"https://www.nature.com/articles/s41380-017-0010-4\">all heritable</a>, can be viewed as embodying different values. <a href=\"https://en.wikipedia.org/wiki/Depression_(mood)\">Depression</a> reflects low reward sensitivity and disengagement from normally reinforcing behaviors. <a href=\"https://en.wikipedia.org/wiki/Anxiety_disorder\">Anxiety disorders</a> reflect heightened risk-aversion, loss aversion, and hyper-sensitivity to threatening stimuli; these concerns can be quite specific (e.g. social anxiety disorder vs. specific phobias vs. panic disorder). The <a href=\"https://en.wikipedia.org/wiki/Schizophrenia#Negative_symptoms\">negative symptoms</a> of schizophrenia reflect reduced reward-sensitivity to social interaction (asociality), speech (alogia), pleasure (anhedonia), and motivation (avolution). The \u2018<a href=\"https://en.wikipedia.org/wiki/Dark_triad\">Dark Triad\u2019</a> personality traits (Machiavellianism, Narcissism, Psychopathy) reflect a higher value placed on personal status-seeking and short-term mating, and a lower value placed on other people\u2019s suffering. A <a href=\"https://www.researchgate.net/publication/44589642_Psychiatric_'diseases'_versus_behavioral_disorders_and_degree_of_genetic_influence\">2010 review paper</a> showed that heritabilities of psychiatric \u2018diseases\u2019 (such as schizophrenia or depression) that were assumed to develop \u2018involuntarily\u2019 are about the same as heritabilities of \u2018behavioral disorders\u2019 (such as drug addiction or anorexia) that were assumed to reflect individual choices and values.</p><p>Specific drug dependencies and addictions are all heritable, reflecting the differential rewards that psychoactive chemicals have in different brains. Genetic influences have been especially well-studied in <a href=\"https://link.springer.com/article/10.1007/s11920-019-1008-1\">alcoholism</a>, <a href=\"https://www.cambridge.org/core/journals/psychological-medicine/article/abs/overlap-of-heritable-influences-between-cannabis-use-disorder-frequency-of-use-and-opportunity-to-use-cannabis-trivariate-twin-modelling-and-implications-for-genetic-design/A758DD589C6C621BF3C680E0609CD026\">cannabis use</a>, <a href=\"https://www.sciencedirect.com/science/article/pii/S2352250X1830112X\">opiate addiction</a>, <a href=\"https://www.nature.com/articles/s41386-018-0008-x\">cocaine addiction</a>, and <a href=\"https://www.tandfonline.com/doi/full/10.31887/DCNS.2017.19.3/pgorwood\">nicotine addiction</a>. Other kinds of \u2018behavioral disorders\u2019 also show <a href=\"https://link.springer.com/chapter/10.1007/978-3-030-36391-8_63\">heritability</a>, including <a href=\"https://www.frontiersin.org/articles/10.3389/fpsyg.2017.02121/full\">gambling</a>, <a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/adb.12218\">compulsive Internet use</a>, and <a href=\"https://academic.oup.com/ajcn/article-abstract/104/4/1144/4557113\">sugar addiction</a> \u2013 and each reflects a genetic modulation of the relevant reward/reinforcement systems that govern responses to these experiences.</p><p><strong>Heritability for behavioral traits tends to increase, not decrease, during lifespan development</strong></p><p>Shard Theory implies that genes shape human brains mostly before birth, setting up the basic limbic reinforcement system, and then Nurture takes over, such that heritability should decrease from birth to adulthood. This is exactly the opposite of what we typically see in <a href=\"https://psycnet.apa.org/record/2014-08122-001?doi=1\">longutudinal behavior genetic studies</a> that compare heritabilities across different ages. Often, heritabilities for behavioral traits increase rather than decrease as people mature from birth to adulthood. For example, the heritability of general intelligence <a href=\"https://link.springer.com/article/10.1023/A:1019772628912\">increases gradually</a> from early childhood through young adulthood, and genes, rather than shared family environment, explain most of the continuity in intelligence across ages. A <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3954471/\">2013 meta-analysis</a> confirmed increasing heritability of intelligence between ages 6 months and 18 years. A <a href=\"https://www.nature.com/articles/mp2014105\">2014 review</a> observed that heritability of intelligence is about 20% in infancy, but about 80% in adulthood. This increased heritability with age has been called \u2018<a href=\"https://www.cambridge.org/core/journals/twin-research-and-human-genetics/article/wilson-effect-the-increase-in-heritability-of-iq-with-age/FF406CC4CF286D78AF72C9E7EF9B5E3F\">the Wilson Effect\u2019</a> (after its discoverer Ronald Wilson), and it is typically accompanied by a decrease in the effect of shared family environment.&nbsp;</p><p>Increasing heritability with age is not restricted to intelligence. <a href=\"https://pubmed.ncbi.nlm.nih.gov/16953685/\">This study</a> found increasing heritability of prosocial behavior in children from ages 2 through 7, and decreasing effects of shared family environment. Personality traits show relatively stable genetic influences across age, with small increases in genetic stability offsetting small decreases in heritability, according to this <a href=\"https://pubmed.ncbi.nlm.nih.gov/24956122/\">meta-analysis</a> of 24 studies including 21,057 sibling pairs. A frequent finding in longitudinal behavior genetics is that the stability of traits across life is better explained by the <a href=\"https://www.sciencedirect.com/science/article/pii/B9780128046746000296\">stability of genes</a> across life, than by the persistence of early experiences, shared family environment effects, or contextually reinforced values.&nbsp;</p><p>More generally, note that heritability does not just influence \u2018innate traits\u2019 that are present at birth. Heritability also influences traits that emerge with key developmental milestones such as social-cognitive maturation in middle childhood, sexual maturation in adolescence, political and religious maturation in young adulthood, and parenting behaviors after reproduction. Consider some of the findings in the previous section, which are revealed only after individuals reach certain life stages. The heritability of mate preferences, sexual orientation, orgasm rate, and sexual jealousy are not typically manifest until puberty, so are not \u2018innate\u2019 in the sense of \u2018present at birth\u2019. The heritability of voter behavior is not manifest until people are old enough to vote. The heritability of investment biases is not manifest until people acquire their own money to invest. The heritability of parenting behaviors is not manifest until people have kids of their own. It seems difficult to reconcile the heritability of so many late-developing values with the Shard Theory assumption that genes influence only a few crude, simple, reinforcement systems that are present at birth.</p><p><strong>Human Connectome Project studies show that genetic influences on brain structure are not restricted to \u2018subcortical hardwiring\u2019</strong></p><p>Shard Theory seems to view genetic influences on human values as being restricted mostly to the subcortical limbic system. Recall that Assumption 1 of Shard Theory was that \u201cThe cortex is basically (locally) randomly initialized.\u201d&nbsp; &nbsp;Recent studies in neurogenetics show that this is not accurate. Genetically informative studies in the Human Connectome Project <a href=\"https://direct.mit.edu/netn/article/02/02/175/2208/Heritability-of-the-human-connectome-A\">show</a> pervasive heritability in neural structure and function across all brain areas, not just limbic areas. A recent <a href=\"https://www.sciencedirect.com/science/article/pii/S1053811921008430\">review</a> shows that genetic influences are quite strong for global white-matter microstructure and anatomical connectivity between brain regions; these effects pervade the entire neocortex, not just the limbic system. Note that these results based on brain imaging include not just the classic twin design, but also genome-wide association studies, and studies of gene expression using transcriptional data. Another <a href=\"https://elifesciences.org/articles/20178\">study</a> showed that genes, rather than shared family environment, played a more important role in shaping connectivity patterns among 39 cortical regions. Genetic influences on the brain\u2019s connectome are often <a href=\"https://www.biorxiv.org/content/10.1101/2020.12.09.417725v2.abstract\">modulated by age and sex</a> \u2013 in contrast to Shard Theory\u2019s implicit model that all humans, of all ages, and both sexes, shared the same subcortical hardwiring. Another <a href=\"https://www.sciencedirect.com/science/article/pii/S1053811922003950\">study</a> showed high heritability for how the brain\u2019s connectome transitions across states through time \u2013 in contrast to Shard Theory\u2019s claim that genes mostly determine the static \u2018hardwiring\u2019 of the brain.</p><p>It should not be surprising that genetic variants influence all areas of the human brain, and the values that they embody. Analysis of the <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S1053811919300114\">Allen Human Brain Atlas</a>, a map of gene expression patterns throughout the human brain, shows that over <a href=\"https://www.nature.com/articles/s41598-017-00952-9\">80% of genes</a> are expressed in at least one of 190 brain structures studied. Neurogenetics research is making <a href=\"https://www.science.org/doi/abs/10.1126/science.aat8464\">rapid progress</a> on characterizing the gene regulatory network that governs human brain development \u2013 including neocortex. This is also helping genome-wide association studies to discover and analyze the millions of <a href=\"https://en.wikipedia.org/wiki/Quantitative_trait_locus\">quantitative trait loci</a> (minor genetic variants) that influence individual differences in brain development. Integration of the Human Connectome Project and the Allen Human Brain Atlas reveals <a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/gbb.12537\">pervasive heritability</a> for myelination patterns in human neocortex \u2013 which directly contradicts Shard Theory\u2019s Assumption 1 that \u201cMost of the circuits in the brain are learned from scratch, in the sense of being mostly randomly initialized and not mostly genetically hard-coded.\u201d&nbsp;</p><p><strong>Behavioral traits and values are also heritable in non-human animals&nbsp;</strong></p><p>A recent 2019 <a href=\"https://academic.oup.com/jhered/article/110/4/403/5497135\">meta-analysis</a> examined 476 heritability estimates in 101 publications across many species, and across a wide range of 11 behavioral traits\u2013 including activity, aggression, boldness, communication, exploration, foraging, mating, migration, parenting, social interaction, and other behaviors. Overall average heritability of behavior was 0.24. (This may sound low, but remember that empirical heritability estimates are limited by the measurement accuracy for traits, and many behavioral traits in animals can measured with only modest reliability and validity.) Crucially, heritability was positive for every type of behavioral trait, was similar for domestic and wild species, was similar for field and lab measures of behavior, and was just as high for vertebrates as for invertebrates. Also, average heritability of behavioral traits was just as high as average heritability of physiological traits (e.g. blood pressure, hormone levels) and life history traits (e.g. age at sexual maturation, life span), and were only a bit lower than the heritability for morphological traits (e.g. height, limb length).&nbsp;</p><p>Note that most of these behavioral traits in animals involve \u2018values\u2019, broadly construed as reinforcement or reward systems that shape the development of adaptive behavior. For example, \u2018activity\u2019 reflects how rewarding it is to move around a lot; \u2018aggression\u2019 reflects how rewarding it is to attack others, \u2018boldness\u2019 reflects how rewarding it is to track and investigate dangerous predators, \u2018exploratory behavior\u2019 reflects how rewarding it is to investigate novel environments, \u2018foraging\u2019 reflects how rewarding it is to find, handle, and consume food, \u2018mating\u2019 reflects how rewarding it is to do mate search, courtship, and copulation, \u2018parental effort\u2019 reflects how rewarding it is to take care of offspring, and \u2018social behavior\u2019 reflects how reward it is to groom others or to hang around in groups.</p><p>In other words, every type of value that can vary across individual animals, and that can be reliably measured by animal behavior researchers, seems to show positive heritability, and heritability of values is just as high in animals with complex central nervous systems (vertebrates) as in animals with simpler nervous systems (invertebrates).</p><p><strong>So what if human values are heritable?</strong></p><p>You might be thinking, OK, all this behavior genetics stuff is fine, and it challenges a na\u00efve Blank Slate model of human nature, but what difference does it really make for Shard Theory, or for AI alignment in general?&nbsp;</p><p>Well, Shard Theory certainly think it matters. Assumption 1 in Shard Theory is presented as foundational to the whole project (although I\u2019m not sure it really is). Shard Theory repeatedly talks about human values being built up from just a few, crude, simple, innate, species-typical reinforcement systems centered in the midbrain (in contrast to the rich set of many, evolved, adaptive, domain-specific psychological adaptations posited by evolutionary psychology). Shard Theory seems to allow no role for genes influencing value formation after birth, even at crucial life stages such as middle childhood, sexual maturation, and parenting. More generally, Shard Theory seems to underplay the genetic and phenotypic diversity of human values across individuals, and seems to imply that humans have only a few basic reinforcement systems in common, and that all divergence of values across individuals reflects differences in family, socialization, cultural, and media exposure.&nbsp;</p><p>Thus, I think that Shard Theory has some good insights and some promise as a research paradigm, but I think it needs some updating in terms of its model of human evolution, genetics, development, neuroscience, psychology, and values.&nbsp;</p><p><strong>Why does the heritability of human values matter for AI alignment?</strong></p><p>Apart from Shard Theory, why does it matter for AI alignment if human values are heritable? Well, I think it might matter in several ways.&nbsp;</p><p>First, polygenic scores for value prediction. In the near future, human scientists and AI systems will be able to predict the values of an individual, to some degree, just from their genotype. As GWAS research discovers thousands of new genetic loci that influence particular human values, it will become possible to develop polygenic scores that predict someone\u2019s values given their complete sequenced genome \u2013 even without knowing anything else about them. Polygenic scores to predict intelligence are already <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0160289621000143\">improving</a> at a rapid rate. Polygenic value prediction would require large sample sizes of sequenced genomes linked to individuals\u2019 preferences and values (whether self-reported or inferred behaviorally from digital records), but it is entirely possible given current behavior genetics methods. As the <a href=\"https://sequencing.com/education-center/whole-genome-sequencing/whole-genome-sequencing-cost\">cost</a> of whole-genome sequencing falls below $1,000, and the medical benefits of sequencing rise, we can expect hundreds of millions of people to get genotyped in the next decade or two. AI systems could request free access to individual genomic data as part of standard terms and conditions, or could offer discounts to users willing to share their genomic data in order to improve the accuracy of their recommendation engines and interaction styles. We should expect that advanced AI systems will typically have access to the complete genomes of the people they interact with most often \u2013 and will be able to use polygenic scores to translate those genomes into predicted value profiles.</p><p>Second, familial aggregation of values. Heritability means that values of one individual can be predicted somewhat by the values of their close genetic relatives. For example, learning about the values of one identical twin might be highly predictive of the values of the other identical twin \u2013 even if they were separated at birth and raised in different families and cultures. This means that an AI system trying to understand the values of one individual could start from the known values of their parents, siblings, and other genetic relatives, as a sort of maximum-likelihood familial Bayesian prior. An AI system could also take into account developmental behavior genetic findings and life-stage effects \u2013 for example, an individual\u2019s values at age 40 after they have kids might be more similar in some ways to those of their own parents at age 40, than to themselves as they were at age 20.&nbsp;</p><p>Third, the genetic architecture of values. For a given individual, their values in one domain can sometimes be predicted by values in other domains. Values are not orthogonal to each other; they are shaped by genetic correlations across values. As behavior genetics researchers develop a more complete genetic architecture of values, AI systems could potentially use this to infer a person\u2019s unknown values from their known values. For example, their consumer preferences might predict their political values, or their sexual values might predict their religious values.</p><p>Fourth, the authenticity of values. Given information about an individual\u2019s genome, the values of their close family members, and the genetic architecture of values, an AI system could infer a fairly complete expected profile of values for that individual, at each expected life-stage. What if the AI discovers that there\u2019s a big mismatch between an individual\u2019s \u2018genetic prior\u2019 (their values are predicted from genomic and family information), and their current stated or revealed values? That might be evidence that the individual has heroically overcome their genetic programming through education, enlightenment, and self-actualization. Or if might be evidence that the individual has been manipulated by a lifetime of indoctrination, mis-education, and propaganda that has alienated them from their instinctive preferences and morals. The heritability of values raises profound questions about the authenticity of human values in our credentialist, careerist, consumerist, media-obsessed civilization. When AI systems are trying to align with our values, but our heritable values don\u2019t align with our current stated cultural values (e.g. this month\u2019s fashionable virtue signals), which should the AI weigh most heavily?</p><p><strong>Conclusion</strong></p><p>If we\u2019re serious about AI alignment with human values, we need to get more serious about integrating empirical evidence about the origins, nature, and variety of human values. One recent attempt to ground AI alignment in human values \u2013 Shard Theory \u2013 has some merits and some interesting potential. However, this potential is undermined by Shard Theory\u2019s empirical commitments to a fairly Blank Slate view of human value formation. That view is inconsistent with a large volume of research in behavior genetics on the heritability of many human values. By taking genetic influences on human values more seriously, we might be able to improve Shard Theory and other approaches to AI safety, and we might identify new issues in AI alignment such as polygenic scores for value prediction, familial aggregation of values, and the genetic architecture of values. Finally, a hereditarian perspective raises the thorny issue of which of our values are most authentic and most worthy of being aligned with AI systems \u2013 the ones our genes are nudging us towards, the ones our parents taught us, the ones that society indoctrinates us into, or the ones that we \u2018freely choose\u2019 (whatever that means).&nbsp;</p><p>&nbsp;</p><p><strong>Appendix 1: Epistemic status of my arguments</strong></p><p>I\u2019m moderately confident that some key assumptions of Shard Theory as currently presented are not empirically consistent with the findings of behavior genetics, but I have very low confidence about whether or not Shard Theory can be updated to become consistent, and I have no idea yet what that update would look like.</p><p>As a newbie AI alignment researcher, I\u2019ve probably made some errors in my understanding of the more AI-oriented elements of Shard Theory. I worked a fair amount on neural networks, genetic algorithms, autonomous agents, and machine learning from the late 1980s through the mid-1990s, but I\u2019m still getting up to date with more recent work on deep learning, reinforcement learning, and technical alignment research.&nbsp;</p><p>As an evolutionary psychology professor, I\u2019m moderately familiar with behavior genetics methods and findings, and I\u2019ve published several papers using behavior genetics methods. I\u2019ve been thinking about behavior genetics issues since the late 1990s, especially in relation to human intelligence. I taught a course on behavior genetics in 2004 (syllabus <a href=\"https://www.primalpoly.com/s/bg-syllabus-2004.doc\">here</a>). I did a sabbatical in 2006 at the Genetic Epidemiology Center at QIMR in Brisbane, Australia, run by <a href=\"https://scholar.google.com/citations?user=Ba2kwtkAAAAJ&amp;hl=en&amp;oi=ao\">Nick Martin</a>. We published two behavior genetics studies, one in <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S1743609515336304\">2011</a> on the heritability of female orgasm rates, and one in <a href=\"https://www.cambridge.org/core/journals/twin-research-and-human-genetics/article/heritability-and-genetic-correlates-of-mobile-phone-use-a-twin-study-of-consumer-behavior/56022F02DE9EDBE7A79607E719B652DC\">2012</a> on the heritability of talking and texting on smartphones. I did a 2007 <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0160289606001073\">meta-analysis</a> of brain imaging data to estimate the coefficient of additive genetic variance in brain size. I also published a couple of papers in 2008 on genetic admixture studies, such as <a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1002/ajpa.20945\">this</a>. However, I\u2019m not a full-time behavior genetics researcher, and I\u2019m not actively involved in the large international genetics consortia that dominate current behavior genetics studies.</p><p>Overall, I\u2019m highly confident in the key lessons of behavior genetics (e.g. all psychological traits are heritable, including many values; shared family environment has surprisingly small effects on many traits). I\u2019m moderately confident in the results from meta-analyses and large-scale international consortia studies. I\u2019m less confident in specific heritability estimates from individual papers that haven\u2019t yet been replicated.&nbsp;</p>", "user": {"username": "geoffreymiller"}}, {"_id": "sP2yKZYER2p7ooyRe", "title": "A Conflict Between Longtermism and Veganism, Pick One.", "postedAt": "2022-10-20T14:30:03.172Z", "htmlBody": "<p>[This is a cross-post from my blog, which you can find <a href=\"https://alltrades.substack.com/\">here</a>]</p><p>The EA space is certainly a unique intersection of people from many walks of life, each with their own priorities and goals. However, an interesting contradiction arose in a recent conversation I had over dinner with friends. As I state in the conclusion, this may be either a criticism of longtermism or of vegetarian/veganism, depending on your perspective.</p><p>If you are someone who subscribes to Longtermism, (the idea that future people hold equal moral weight as compared to present people, and that we should adjust our actions to be accordingly biased to creating future growth.) Then, it seems to me that it would actually be non-optimal of you not to eat the most convenient/delicious/nutritious meal that you can find, whenever possible, and without much regard for animal welfare.<br><br><strong>ANIMAL WELFARE VS HUMAN PREFERENCES</strong></p><p>The argument goes like this: Whatever people may do to make future people better off, they will probably do more of it/do it better if they are more satisfied/happier. There are some studies on this (<a href=\"https://www.ox.ac.uk/news/2019-10-24-happy-workers-are-13-more-productive\"><u>link</u></a>, <a href=\"https://www.forbes.com/sites/forbescoachescouncil/2017/12/13/promoting-employee-happiness-benefits-everyone/?sh=8146c33581a1\"><u>link</u></a>, <a href=\"https://wrap.warwick.ac.uk/63228/7/WRAP_Oswald_681096.pdf\"><u>link</u></a>), that suggest it might be a difference somewhere between 10-20%. Anecdotally, just take a look at the sometimes ludicrous lengths that tech companies go to please their employees. This is not altruism, it\u2019s just good business.&nbsp;</p><p>So okay, great. We agree that happy people are more productive. Now let\u2019s consider this within the domain of diet choice.&nbsp;</p><p>Is veganism/vegetarianism a choice that makes people happy? Maybe for some people, but usually not in a vacuum. If you truly do enjoy eating vegetarian/vegan more than a meat based diet on the basis of taste and convenience alone, more power to you. However, it seems that around the world, there is a strong revealed preference for people to eat more meat as it becomes more available. We can tell this by looking at the rate of meat consumption vs. GDP per capita.&nbsp;&nbsp;</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0b8892f4-c059-4077-8e0a-b9edc94358fc_1600x1130.png\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1669045514/mirroredImages/sP2yKZYER2p7ooyRe/kwtb2zqhnlwkvtnzhf63.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0b8892f4-c059-4077-8e0a-b9edc94358fc_1600x1130.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0b8892f4-c059-4077-8e0a-b9edc94358fc_1600x1130.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0b8892f4-c059-4077-8e0a-b9edc94358fc_1600x1130.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0b8892f4-c059-4077-8e0a-b9edc94358fc_1600x1130.png 1456w\"></a></p><p>&nbsp;</p><p>Many vegetarians/vegans do so for religious or moral reasons, but messages that our inherent preference to eat meat is morally bad can be harmful, because they don\u2019t actually change how a meatless diet tastes, they simply add costs like guilt and disgust in an attempt to tip the scale in people\u2019s choices, and do not make material changes in either diet.</p><p>Okay, so we\u2019ve now established that there is a large cost to not eating meat for those whose inherent preference it is to do so, which are a large number of people. But the key idea here, and where the Longtermist perspective becomes important, is that this cost is compounding.&nbsp;</p><p>Consider the following.</p><p>A researcher orders and eats a chicken sandwich for lunch everyday. He loves the chicken sandwich, it\u2019s one of the best parts of his day, and he extracts many utils from this sandwich that allow him to exercise more willpower, and work 5% longer or harder each day. His research is completed and implemented 5% more quickly, and improves people\u2019s lives around the world by .0001 utils on average. That\u2019s an 800,000 util increase in well being across the world population. Some of these utils will go to other researchers, who will continue the cycle, laundering utils over and over again into untold riches and wealth for future generations.</p><p>Now consider that the 100 or so chickens that it takes to make the researchers 365 sandwiches were spared. Even if we weigh chicken utils as equal to human utils, and even if the total # of utils they experience is the same sum of 800,000, what do they do with them? A happy chicken doesn\u2019t benefit humanity any more than a sad one, and a sad chicken doesn\u2019t necessarily do psychic damage to humanity (although some may elect to do psychic damage to themselves on the chickens behalf). Benefits to animals are a one-time event. Therefore, the researcher\u2019s compounding benefit will always win in the long run.&nbsp;</p><p>From this example, we can tell that every util that falls into human hands is worth many, many, more utils experienced by birds in the bush. Animals may very well deserve to be included in the \u201cmoral circle\u201d, but not to the point of excluding future humans! Framing the problem in this way reveals a discrepancy in people\u2019s sensibilities. If we ought to be so concerned about future welfare, why obsess over present suffering, especially suffering that can be written off as easily as that of animals?</p><p>Okay, so the large cost to going vegetarian/vegan is compounding while the benefits to animals are not. In my mind, that nullifies pretty much the entire animal welfare argument, but there are still other costs to eating meat that we haven't covered, and which do compound, since they are costs to humans.&nbsp;</p><p><strong>COSTS TO HUMANS</strong></p><p>Maybe it\u2019s not about animal suffering, but rather that vegan/vegetarianism is better for the world. Even the impact of going vegetarian/vegan on the environment is not huge when compared to the opportunity cost of being more productive and experiencing fewer inconveniences. In Will MacAsckill\u2019s book, <i>What We Owe The Future</i>, he lists the stat that going vegetarian averts 0.8 tonnes of CO2 per year. At<a href=\"https://terrapass.com/?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=TP_Brand_US_DT&amp;gclid=Cj0KCQjwnbmaBhD-ARIsAGTPcfWgAZbs-G4UsWpjHvfCE2SMtasmjmtDt_nOrLnKHzxIBCJMW0NQSvAaArQVEALw_wcB\"><u> Terrapass</u></a>, a carbon offset of 1000 pounds, (about .5 tonnes) is available for as low as $7.49 a month. That\u2019s about $13.22 a month to have the same impact on the environment as going vegetarian. I think most people would agree that the cost of a Netflix subscription is worth it to continue to eat meat. If Terrapass focused on efforts that accelerated green energy technology, I\u2019m sure they could get the price even lower. (discussed in a previous post<a href=\"https://alltrades.substack.com/p/malthus-mankind-and-mother-earth\"><u> here</u></a>). Therefore, if we assume the low end of the productivity effect range I gave above (10%) and you think your work creates more than $140 a month in positive externalities, you can and should go ahead and fulfill your preference.</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8fcacde2-eb14-422b-919c-434db08ea62b_754x469.png\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1669045514/mirroredImages/sP2yKZYER2p7ooyRe/hxfwkygsimjislyvkkjw.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8fcacde2-eb14-422b-919c-434db08ea62b_754x469.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8fcacde2-eb14-422b-919c-434db08ea62b_754x469.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8fcacde2-eb14-422b-919c-434db08ea62b_754x469.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8fcacde2-eb14-422b-919c-434db08ea62b_754x469.png 1456w\"></a></p><p>&nbsp;</p><p>Graph illustrating the power of funding innovation from the<a href=\"https://founderspledge.com/stories/climate-and-lifestyle-report\"><u> Founders Pledge Climate and Lifestyle Report</u></a></p><p>This leads me to health and nutrition claims. I think that nutrition is still a rather woo-woo area of science today, and that most diet studies do not show significant health results when compared to a control diet. Therefore, I rate the health claims on either side of the meat eating divide as a wash. So let\u2019s assume you are equally healthy eating meats vs. plants. However, this does not necessitate that you are equally as productive. Eating meat may be equally healthy as eating veggies, but it is fulfilling your preferences that makes you happy. If the researcher from the example above didn\u2019t like the taste of chicken, it\u2019s likely that he would not have been able to leverage that into working harder for the day.&nbsp;</p><p>So, to sum up, the longtermist viewpoint on whether or not you should eat meat essentially has nothing to do with animal welfare, as it is a one-time cost. When viewed on long time horizons, it is a balance between the negative externalities to the climate, and the positive externalities to work productivity which may benefit the future. Even if you elevate current animal experience to the same level as current human experience, humans are much better at carrying utility forward to future generations, while animals are just inefficient by comparison. I don\u2019t believe that trading future humans for present animals is justifiable, and thus, if it is your preference to eat meat, I find it likely that you should continue to do so.&nbsp;</p><p>As always, if you think differently, please feel free to dunk on me in the comments below, maybe this is more of a criticism of longtermism than vegetarian/veganism, depending on your perspective!<br>&nbsp;</p><p><strong>FOOTNOTE:</strong></p><p>Many vegetarians/vegans might argue that continuing to consume meat perpetuates and normalizes the <a href=\"https://open.substack.com/pub/hazell/p/what-the-fuck?r=ce9uk&amp;utm_campaign=post&amp;utm_medium=web\"><u>way in which animals suffer in factory farming</u></a>, reframing the one time benefit to animal welfare as a slippery slope, but I think that this is not a reality that is likely to persist much longer. I think that the advent of <a href=\"https://sitn.hms.harvard.edu/flash/2022/meat-pie-in-the-sky-when-will-our-appetite-for-lab-grown-meat-be-satisfied/\"><u>lab grown meat</u></a> and breeding dumber animals will allow us to continue our consumption at a lower unit cost of animal welfare. There is some pretty compelling data that we are quite close to seeing lab grown alternatives available in supermarkets soon with some reports estimating that <a href=\"https://www.forbes.com/sites/briankateman/2020/02/17/will-cultured-meat-soon-be-a-common-sight-in-supermarkets-across-the-globe/?sh=2b46933a7c66\"><u>35% of all meat being cultured by 2040</u></a>, and that demand for plant based meats may have <a href=\"https://www.winsightgrocerybusiness.com/fresh-food/consumer-appetite-plant-based-meat-waning\"><u>already peaked</u></a>. However, when people do adopt cultured meats, I think that it will largely be because they are more delicious, more consistent, and cheaper, and not because of an ethical appeal. I want to eat an A-5 Wagyu steak everyday, and I think it\u2019ll be sooner rather than later that the easiest way to produce such a luxury will be via cultured meat.&nbsp;</p><p>Q: Can't we just change our preferences?&nbsp;</p><p>I don\u2019t really think so, and it seems even less likely that we could change our preferences without imposing costs like guilt and disgust to tip the scale. If it is your taste preference to eat meat, then it seems unlikely that you can consciously decide to prefer veggies or meat alternatives, and this is probably why veg alternatives so often try to emulate the taste of meat. I\u2019m certainly not that in control of my preferences, but if you can will your will, more power to you.</p><p>Another example:</p><p>If your grandfather had worked one extra year in his life, how much richer might you be today? Conversely, if he had spent one of his years finding the nearest vegan restaurant, deliberating over the ethics of the impossible burger vs. the un-burger, spending more money on these hard to find ethical alternatives, watching factory farm documentaries and feeling guilty about his animal nature, and reaping fewer utils from worse-tasting food, how much poorer do you think you might be?</p><p>&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "Connor Tabarrok"}}, {"_id": "zzcppHEFcDtYgkAz9", "title": "Top companies for donation matching", "postedAt": "2022-10-20T12:17:00.043Z", "htmlBody": "<h2>TL;DR</h2><ul><li>Donation matching is an excellent way to boost your impact during the giving season.</li><li>Some companies have very generous donation matching programs, as highlighted in the list below. (Big shoutout to&nbsp;<a href=\"https://forum.effectivealtruism.org/users/drwahl\"><u>Dan Wahl</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/users/owyongsk\"><u>Soon Khen Ow Yong</u></a> for helping to put the list together!)</li><li>Take advantage of those donation-boosting opportunities yourself and encourage your colleges and network to do the same.</li><li>An effective way to encourage your colleagues is through a fundraising campaign; High Impact Professionals (HIP) can provide support and resources to help there, starting with this&nbsp;<a href=\"https://www.highimpactprofessionals.org/corporate-fundraising-campaigns\"><u>short overview video</u></a>.</li><li>An additional way is to reach out to people in your network (e.g., on Linkedin) who work at donation matching companies and encourage them to donate.&nbsp;Dan Wahl created a&nbsp;<a href=\"https://danwahl.net/pages/linkedin-matching\"><u>tool</u></a> that leverages LinkedIn's search functionality to do just this.</li></ul><h2>Intro</h2><p>End-of-year giving season is quickly approaching. An excellent way to multiply your impact during is to take advantage of the donation matching offered by your company, especially if you work at an organization with a particularly generous matching program. Below we compiled a list of companies with the most generous donation matching.</p><p>Regardless of where you work, but especially if you work at one of these organizations, encouraging your colleagues to make effective donations is a powerful way to multiply the impact of donation matching. Please&nbsp;<a href=\"https://calendly.com/federico-hip/hip-fundraising-campaign\"><u>book a time</u></a> or&nbsp;<a href=\"mailto:fundraising@highimpactprofessionals.org?subject=Fundraising%20Campaign%20Support\"><u>email us</u></a> to explore how we can support organizing a fundraising campaign at your organization.</p><h2>Companies with the largest donation matching</h2><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667996243/mirroredImages/zzcppHEFcDtYgkAz9/qq5n6ctlhayc6z3fwvgw.png\"></p><p>The graph above shows the 30 companies with the largest donation matching. The amounts are in thousand USD and are per-employee and per-year. The color coding indicates the match ratio\u2014e.g., blue is a 1:1 ratio, red is 2:1 ratio, and so forth. For example, Intermec Foundation (IF) has a maximum donation match of 150,000 USD and has a 5:1 match ratio; so, if an IF employee donates 30,000 USD, IF matches up to 150,000 USD. Note, the source data is not always 100% exact and can sometimes come with caveats (e.g., it\u2019s accessible only to some employees, the company/matching program will only support certain charities, etc.). However, this information is a good starting point to identify opportunities to scale your impact.</p><h2>Well-known companies with generous donation matching</h2><p>Many of the companies with the most generous donation matching shown in the graph above are not very well known. So, below is a list of larger, more well-known companies with generous donation matching programs. To compile this list, we multiplied each company\u2019s donation matching amount times its total number of employees. The results are presented in billion USD and are again color coded for the corresponding multiplier.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwaifd54hcv8\"><sup><a href=\"#fnwaifd54hcv8\">[1]</a></sup></span></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667996244/mirroredImages/zzcppHEFcDtYgkAz9/dywgni5tlxjaloghdlxp.png\"></p><h2>What you can do about it</h2><p>Now that you have this information, what can you do about it? A few things actually.</p><p>If you\u2019re working at one of those companies, take advantage of the donation matching yourself by making donations.</p><p>Also, and very importantly, encourage your colleges to do the same. Again, a good way to do that is to organize a fundraising campaign at your company. We\u2019ve found running a campaign is&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/bsqtndpfKbcbQRFEs/fundraising-campaigns-at-your-organization-a-reliable-path\"><u>a highly effective way</u></a> to multiply your impact, particularly when coupled with donation matching and other&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/sE7Y43JRQARAKzZ6k/key-factors-for-success-in-organizing-a-fundraising-campaign\"><u>key factors</u></a>. We even put together a&nbsp;<a href=\"https://www.highimpactprofessionals.org/corporate-fundraising-campaigns\"><u>short overview video</u></a> about it.</p><p>Finally, if you\u2019re not working at one of those companies but know someone who is, you can reach out to them. Ask them to donate through their company\u2019s donation matching program and encourage them to get their colleagues to do the same, ideally through a fundraising campaign.</p><p>Dan Wahl created a&nbsp;<a href=\"https://danwahl.net/pages/linkedin-matching\"><u>LinkedIn tool</u></a> that can show you which of your connections work in companies with generous donation matching programs. With that information, you can then send those connections a message like: \u201cHi, I noticed you work for a company with a generous donation matching program. Please consider donating to effective charities like [insert links to effective charity/-ies of your choice].\u201d</p><p><a href=\"mailto:hi@danwahl.net\"><u>Reach out to Dan Wahl</u></a> to learn more about the LinkedIn tool. Contact HIP for everything related to how to have an impact as an EA in the private sector\u2014in particular, HIP is currently supporting people in organizing workplace fundraising campaigns, so please&nbsp;<a href=\"https://calendly.com/federico-hip/hip-fundraising-campaign\"><u>book a time</u></a> or&nbsp;<a href=\"mailto:fundraising@highimpactprofessionals.org?subject=Fundraising%20Campaign%20Support\"><u>email us</u></a> if you want to know more.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwaifd54hcv8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwaifd54hcv8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We are not suggesting that those companies would allocate the full amount to donation matching, as it is sometimes a relevant fraction of their net income. But, we believe it is a useful indicator that combines the size of the company with its donation matching amount.</p></div></li></ol>", "user": {"username": "High Impact Professionals"}}, {"_id": "9HAQqNw5yBYFuoZgc", "title": "A couple of expert-recommended jobs in biosecurity at the moment (Oct 2022)", "postedAt": "2022-10-20T12:13:38.941Z", "htmlBody": "<p>There are a lot of jobs out there. I have a hunch that it would be useful to get a sense of what field leaders thought were the most pressing roles to fill to motivate people to apply or share with people they know.</p><p>I asked&nbsp;<a href=\"https://www.openphilanthropy.org/about/team/chris-bakerlee/\"><u>Chris Bakerlee</u></a> (Senior Program Associate for Biosecurity and Pandemic Preparedness at&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/open-philanthropy\"><u>Open Philanthropy</u></a>) for three biosecurity roles he is excited to see filled right now. I was excited to get his view as I feel like he has a good overview of the opportunities that are available within biosecurity and which seem highly impactful to fill. He highlighted the following two roles.</p><p>For more information on why people prioritise Global Catastrophic Biological Risk, I recommend reading&nbsp;<a href=\"https://80000hours.org/problem-profiles/preventing-catastrophic-pandemics/full-report/\"><u>80000 hours profile</u></a>.&nbsp;</p><p><i>This post is (in part) a test. Please give me&nbsp;</i><a href=\"https://forms.gle/KWXCbEqQbrZMSXja8\"><i><u>feedback on this post here</u></i></a><i>, in a DM, or in the comments. I\u2019d be particularly interested to know if you found this post useful and/or if this was the first time you\u2019d heard about any of these roles.</i></p><h1>Open Philanthropy, Biosecurity and Pandemic Preparedness, Executive Assistant</h1><p><i>Flexible location (within the US and US time zones). The team is currently split between Boston and the Bay Area.</i></p><h2>Who is it for?</h2><p>Someone with project management, organization, and prioritization skills.</p><h2>What\u2019s the role in a nutshell?</h2><p>The executive assistant role would unlock valuable time that will be spent on research, grantmaking, and other essential activities. The role would focus on providing administrative and operational support for the team. Responsibilities include:</p><ul><li>providing executive assistant-style support to Senior Program Officer, Andrew Snyder-Beattie</li><li>managing and organizing team documents</li><li>doing ad hoc research or organizational tasks (e.g. proofreading documents),</li></ul><p>Read more about the impact case for working in executive assistant roles&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/bzXBZyMrnMiWu2DeF/to-pa-or-not-to-pa\"><u>here</u></a>.</p><p><a href=\"https://efctv.org/3SZiEHL\"><u>Read full job description</u></a>.</p><h2>Why this organisation?</h2><p>Open Philanthropy is one of the biggest funders in effective altruism. They have given over $131 million for projects aimed at reducing biological risks. You can&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/open-philanthropy\"><u>read more about Open Philanthropy here</u></a>.</p><h2><a href=\"https://efctv.org/3SZiEHL\"><u>Learn more</u></a></h2><h1>Senior Program Officer / Senior Director, Global Biological Policy and Programs (NTI | bio)</h1><p><i>Washington D.C.</i></p><h2>Who is this for?</h2><p>Someone with 7-10 years of policy, research, and/or project management experience, including in biology/biotechnology, international security, global health, or international affairs.</p><h2>What\u2019s the role in a nutshell?</h2><p>This role presents an excellent chance to work alongside Jaime Yassif and the&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/nuclear-threat-initiative\"><u>Nuclear Threat Initiative</u></a>\u2019s \u2018Bio\u2019 team to advance creative solutions to challenging problems in biotech governance and bioweapons nonproliferation. Listen to Jaime\u2019s episode on the 80,000 hours podcast&nbsp;<a href=\"https://80000hours.org/podcast/episodes/jaime-yassif-safeguarding-bioscience/\"><u>here</u></a> which describes some of the problems they are tackling.</p><p>Responsibilities include:</p><ul><li>overseeing research on the development of practical, innovative risk-reduction approaches;</li><li>managing projects, budgets, and staff;</li><li>convening international events and managing relationships with global stakeholders;</li><li>drafting internal leadership memos and briefing documents;</li></ul><p><a href=\"https://efctv.org/3CX1Hbq\"><u>Read full job description</u></a></p><h2>Why this organisation?</h2><p>The Nuclear Threat Initiative (NTI) works with governments, scientists, and citizens to prevent catastrophic attacks with weapons of mass destruction and disruption\u2014nuclear, biological, radiological, chemical and cyber. You can&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/nuclear-threat-initiative\"><u>read more about NTI here</u></a>.</p><h2><a href=\"https://efctv.org/3CX1Hbq\"><u>Learn more</u></a></h2>", "user": {"username": "imben"}}, {"_id": "a7TeXNw9oYCHD7fmh", "title": "Notes on \"Can you control the past\"", "postedAt": "2022-10-20T03:41:55.248Z", "htmlBody": "", "user": {"username": "So8res"}}, {"_id": "mL2t47FzGqcsPXv6i", "title": "Shallow Report on Asteroids", "postedAt": "2022-10-20T01:34:35.991Z", "htmlBody": "<p><strong>Note</strong>: This report was produced with only one week of desktop research, for the purpose of identifying promising causes to evaluate at depth. We only have low confidence in our findings here, and the conclusions should generally be taken by readers as merely suggestive rather determinative.</p><h1><br><strong><u>Summary</u></strong></h1><p><a href=\"https://docs.google.com/spreadsheets/d/1CgqHIMztZtErcwM-kUUZarDLtAVRYRODRJP_yQUo9vQ/edit#gid=0\"><u>Factoring in</u></a> the expected benefits of preventing asteroid impact events (i.e. fewer deaths and injuries) as well as the tractability of lobbying for asteroid defence, I find that the marginal expected value of such asteroid defence lobbying to be&nbsp;<strong>1,352 DALYs per USD 100,000</strong>, which is around 2x as cost-effective as giving to a GiveWell top charity.</p><p>Discussion:</p><ul><li>The results surprised me \u2013 I came in expecting extremely low marginal expected value to working on this cause, given low neglectedness (c.f. DART), low risk of occurrence (c.f. fairly low per annum death rates compared to the likes of malaria), and middling tractability (i.e. effective but expensive interventions available).</li><li>I do not model the expected costs of dual use (i.e. deflection technology potentially being used as a weapon to kill or injure vast numbers of people), as such a possibility, slight or otherwise, does not truly increase existential risk. (a) Firstly, there is no reason for the great powers to ever deploy or develop planet-killing kinetic bombardment capabilities (i.e. ones capable of causing catastrophic global winter or an immediate extinction-level event) \u2013 these cannot be used without destroying yourself, unlike nuclear weapons where there is the theoretical possibility of a successful first strike. (b) Secondly, while the great powers may see military use for smaller scale orbital bombardment weapons (i.e. ones capable of causing sub-global or Tunguska-like asteroid events), these are only as destructive as nuclear weapons and similarly cannot be used without risking nuclear retaliation. This means that such weapons are functionally identical to any other missile in a nuclear arsenal, whose existence (given nuclear weapons already exist) does not \u2013 at the margin \u2013 increase the risk of death and injury for humanity, whether from direct kinetic exchange or from climatic shifts due to full-scale WMD exchange.</li><li>For interventions, funding additional search telescopes (whether ground-based or space-based) isn't considered because existing assets, augmented by the soon-online Vera C. Rubin Observatory and NEO Surveyor, will probably identify all the &gt;140m near-earth objects \u2013 which pose 99% of the risk \u2013 by 2028. Any new telescopes would literally not be built or launched before we succeeded with current assets.</li><li>Encouraging results are driven by high tractability, which is itself a function of (a) a large fraction of the problem being solved by the intervention, and (b) heavily discounting the counterfactual value of US government funding relative to EA funding.</li><li>Very sensitive to costing estimates</li><li>If further research is done, expert advice on tractability \u2013 both on the political aspect&nbsp; (lobbying success) and on the technical aspects (ATLAS and deflection efforts) \u2013 would be very valuable.</li></ul><h1><u>Expected Benefit: Averting Asteroid Impact Deaths</u></h1><p>The primary expected benefit of preventing asteroid impact is averting deaths. This benefit is modelled in the following way.</p><p><strong><u>Moral Weights</u></strong>: I take the value of averting one death to be&nbsp;<strong>29.3 DALYs</strong>. This is calculated as a function of (a) a human's full healthy life expectancy of 63.69, (b) a minor age-based philosophical discount, and (c) assuming we save someone of the median age in the relevant population. For more details, refer to&nbsp;<a href=\"https://docs.google.com/document/d/1j67pOUpC5vhC1-H516PAj2-4Fvm5rxJhZNeBVav3kik/edit\"><u>CEARCH's evaluative framework</u></a>.</p><p><strong><u>Scale</u></strong>: To calculate the deaths from asteroid impact, I consider four scenarios: (1) deaths from a Tunguska-like asteroid event (i.e. the impactor being 50-300 m in diameter); (2) deaths from a large sub-global asteroid event (i.e. the impactor being 0.3-1.5 km in diameter); (3) deaths from a global asteroid event (i.e. the impactor being &gt;1.5 km in diameter); and (4) deaths from rare K/T scale asteroid event (i.e. the impactor being &gt;10 km in diameter). Moreover, to calculate deaths within each scenario, I use three different estimates (i.e.&nbsp;<a href=\"https://www.nature.com/articles/367033a0\"><u>Morrison &amp; Chapman</u></a>,&nbsp;<a href=\"https://cneos.jpl.nasa.gov/doc/neoreport030825.pdf\"><u>Stokes et al</u></a>, as well as the&nbsp;<a href=\"https://nap.nationalacademies.org/catalog/12842/defending-planet-earth-near-earth-object-surveys-and-hazard-mitigation\"><u>National Research Council</u></a>). I adjust for&nbsp;<a href=\"https://data.worldbank.org/indicator/SP.POP.TOTL\"><u>population growth</u></a> since the estimate was made, and take a weighted average of the three estimates, penalizing the Chapman &amp; Morrison estimate for&nbsp;<a href=\"https://www.nature.com/articles/367033a0\"><u>not taking into account tsunami effects</u></a>.</p><p>Overall, this yields the following deaths per event:</p><ul><li>Tunguska-like asteroid event:&nbsp;<strong>145,000 deaths</strong>.</li><li>Large sub-global asteroid event:&nbsp;<strong>2.06 million deaths</strong>.</li><li>Global asteroid event:&nbsp;<strong>3.4 billion deaths</strong>.</li><li>Rare K/T scale asteroid event:&nbsp;<strong>6.3 billion deaths</strong>.</li></ul><p><strong><u>Persistence</u></strong>: If we managed to eliminate asteroid risk, the per annum benefits would persist over time and would hence need to be summed up across various years, but there are a couple of important discounts that we need to implement.</p><p>Firstly, we must discount for the probability of the solution not persisting (i.e. an effectual planetary defence being dismantled). Assuming our intervention is lobbying Congress for more money so that NASA's Planetary Defense Coordination Office can better destroy any asteroids that will potentially hit and harm earth, the risk we face is that the PDCO is defunded further down the line. I take this to be the risk that NASA suffers budget cuts \u2013 as calculated from&nbsp;<a href=\"https://aerospace.csis.org/data/history-nasa-budget-csis/\"><u>historical trends</u></a> (26 years in which cuts occurred out of 63 years surveyed)\u2013 multiplied by the risk that the PDCO in particular then suffers significant budget cuts which makes it unable to carry out its core mission (n.b. I assign a further 1% chance to this, on the basis that the PDCO will likely be prioritized due to its obvious importance and it being only around&nbsp;<a href=\"https://www.planetary.org/articles/nasas-planetary-defense-budget-growth\"><u>0.7%</u></a> of NASA's budget in any case. Overall, this gives a reversal rate of&nbsp;<strong>0.41%</strong>.</p><p>Secondly, we must discount for the proportion of the solution being counterfactually solved (i.e. planetary defence being conducted without additional intervention). Here, I look at three specific considerations.</p><p>&nbsp;</p><ul><li>(a) Existing success of NASA, as supported by by various US government agencies, non-profits, corporations and foreign countries: Of the around 25000&nbsp;<a href=\"https://www.nasa.gov/sites/default/files/atoms/files/2017_neo_sdt_final_e-version.pdf\"><u>&gt;140m near-earth objects (NEOs</u></a>) that pose 99% of the risk to humanity, the NASA-led effort has identified around&nbsp;<a href=\"https://cneos.jpl.nasa.gov/stats/site_140.html\"><u>9951</u></a> as of early October 2022 (i.e. around 40%), and if any turns out to be a threat, we would have the time to prepare countermeasures and subsequently destroy or deflect the asteroid/comet with a 97.76% chance of success (refer to the section on tractability for the calculation of this figure). Overall, this means that NASA is on track to prevent&nbsp;<strong>38.71%&nbsp;</strong>of any harm.</li></ul><p>&nbsp;</p><ul><li>(b) Expected future success of the NASA-led effort in mitigating asteroid risk: For the remaining&nbsp;<a href=\"https://www.nasa.gov/sites/default/files/atoms/files/2017_neo_sdt_final_e-version.pdf\"><u>&gt;140m NEOs</u></a> that pose 99% of the risk to humanity, the current NASA-led effort is discovering them at a rate of around&nbsp;<a href=\"https://www.nasa.gov/planetarydefense/faq\"><u>500&nbsp;</u></a>a year (i.e. 2% of these NEOs).<br><br>Meanwhile, the&nbsp;<a href=\"https://www.lsst.org/about\"><u>Vera C. Rubin Observatory</u></a> (i.e. a ground-based shared LSST) is scheduled to come online in 2024. Per the&nbsp;<a href=\"https://www.nasa.gov/pdf/171331main_NEO_report_march07.pdf\"><u>NASA 2007 NEOSDAA report</u></a> examining whether a ground-based shared LSST be built, and considering the last 5 years' track record of&nbsp;<a href=\"https://cneos.jpl.nasa.gov/stats/site_140.html\"><u>350</u></a> potentially hazardous objects (PHOs) found per annum, business as usual from a 2007 baseline would have seen 7,824\u202c PHOs (31.3%) found by 2020, while a ground-based shared LSST (originally supposed to starting operation in 2014) would have reached 75% completion (i.e. 18,750 PHOs found) by 2020 \u2013 which means the LSST is capable of finding an additional 1,561 PHOs (i.e. 6.2%) per annum.<br><br>Further, the&nbsp;<a href=\"https://en.wikipedia.org/wiki/NEO_Surveyor\"><u>NEO Surveyor</u></a> (i.e. a space-based 0.5m IR telescope @ L1) will be launched in 2026. The NASA 2007 NEOSDAA report similarly considered this L1 satellite as a option for searching for asteroids and comets, and it was expected that with it starting operations in 2013, we would reach 85% completion (i.e. \u202d21,250\u202c PHOs found) by 2020 \u2013 accordingly, the L1 satellite can find an additional 1,678 PHOs (6.7%) per annum.<br><br>Both the LSST and the L1 satellite were delayed, and since they will only start operations in the future, their impact are partially discounted.<br><br>Overall, with the above detection rates factored in and taking into account a subsequent asteroid destruction/deflation rate of 97.76%, this yields a per annum discount rate of around&nbsp;<strong>12.92%</strong> from this cause area becoming increasingly less neglected going forward.</li></ul><p>&nbsp;</p><ul><li>(c) In terms of structural factors \u2013 while trends might theoretically make concerted planetary defence efforts more likely (e.g. economic growth making us more able to fund planetary defence efforts, or cultural change moving us towards post-materialistic values which in turn cause us to be worried about the long term future rather than our own short term interests), this will be a vanishingly weak effect, and I do not bother modelling it (i.e. a&nbsp;<strong>0%</strong> discount).</li></ul><p><br>Thirdly, I apply a standard existential risk discount. Here, I factor in the probability of total nuclear annihilation, since the benefits of saving people from asteroid impact in one year is nullified if they had already died in a previous year. For the exact risk of total nuclear annihilation, I take it to be one magnitude lower than the risk of nuclear war itself, since nuclear war may not kill everyone. For the probability of nuclear war, I use the various estimates on the probability of nuclear war per annum collated by&nbsp;<a href=\"https://forum.effectivealtruism.org/s/KJNrGbt3JWcYeifLk/p/PAYa6on5gJKwAywrF\"><u>Luisa</u></a>, but with accidental nuclear war factored in, and then calculate a weighted average that significantly favours the superforecasters. The reason for this is that (a) the estimate of the probability of intentional nuclear war based on historical frequency is likely biased upwards due to historical use being in a MAD-free context; (b) the probability of accidental nuclear war based on historical close calls is highly uncertain due to the difficulty of translating close calls to actual probabilities of eventual launch; and (c) experts are notoriously bad at long-range forecasts, relative to superforecasters. Meanwhile, I do not take into account other existential risks like supervolcano eruption and asteroid impact, since the chances of those occurring at all is very marginal per&nbsp;<a href=\"https://www.amazon.com/Feeding-Everyone-Matter-What-Catastrophe/dp/0128044470\"><u>Denkenberger &amp; Pearce</u></a>, let alone the chances of such events killing everyone and not just most people. Overall, therefore, I treat the general existential risk discount to be just the risk of nuclear war but adjusted a magnitude down (i.e.&nbsp;<strong>0.07%</strong>)</p><p>Fourthly and finally, I apply a broad uncertainty discount of&nbsp;<strong>0.1%</strong> to take into account the fact that there is a non-zero chance that in the future, the benefits or costs do not persist for factors we do not and cannot identify in the present (e.g. actors directing resources to solve the problem when none are currently doing so).</p><p><strong><u>Value of Outcome</u></strong>: Overall, the raw perpetual value of averting asteroid impact deaths is:</p><ul><li>Tunguska-like asteroid event:&nbsp;<strong>1.94 * 10<sup>7</sup> DALYs</strong>.</li><li>Large sub-global asteroid event:&nbsp;<strong>2.76 * 10<sup>8</sup> DALYs</strong>.</li><li>Global asteroid event:&nbsp;<strong>4.55 * 10<sup>11</sup> DALYs</strong>.</li><li>Rare K/T scale asteroid event:&nbsp;<strong>8.42 * 10<sup>11</sup> DALYs</strong>.</li></ul><p><strong><u>Probability of Occurrence</u></strong>: For the probability of a Tunguska-like asteroid event, I look at three estimates \u2013&nbsp;<a href=\"https://www.nature.com/articles/367033a0\"><u>Chapman &amp; Morrison</u></a>,&nbsp;<a href=\"https://cneos.jpl.nasa.gov/doc/neoreport030825.pdf\"><u>Stokes et al</u></a>, and the&nbsp;<a href=\"https://nap.nationalacademies.org/catalog/12842/defending-planet-earth-near-earth-object-surveys-and-hazard-mitigation\"><u>National Research Council</u></a>. The NRC figures obtained are relative approximations, but Stokes et al is particularly detailed in listing the impact probabilities of potentially hazardous objects (i.e. near-earth asteroids and extinct short-period comets with a minimum orbit intersection distance of &lt;0.05 astronomical units from the Earth's orbit) \u2013 for diameter 0.0313 km, the frequency per year is 3.17E-03; for 0.0394 km, 1.84E-03; for 0.0496 km, 1.07E-03; for 0.0625 km, 6.19E-04; for 0.0787 km, 3.60E-04; for 0.0992 km, 2.09E-04; for 0.125 km, 1.21E-04; for 0.157 km, 7.03E-05; for 0.198 km, 4.08E-05; for 0.25 km, 2.37E-05; for 0.315 km, 1.38E-05; for 0.397 km, 7.99E-06; for 0.5 km, 4.64E-06; for 0.63 km, 2.69E-06; for 0.794 km, 1.56E-06; for 1 km, 9.07E-07; for 1.26 km, 5.26E-07; for 1.59 km, 3.06E-07; for 2 km, 1.77E-07; for 2.52 km, 1.03E-07; for 3.17 km, 5.98E-08; for 4 km, 3.47E-08; for 5.04 km, 2.01E-08; for 6.35 km, 1.17E-08; and for 8 km, 6.79E-09. For the 50-300 m bucket, therefore, the aggregate probability is around 2.53E-03. Aggregating this with the other two estimates, I arrive at a weighted average, wherein (a) I penalize the Stokes et al estimate (given the possibility of my making an error when aggregating the discrete probabilities, and (b) I penalize the NRC estimate given that the figures sourced from there are relative approximations. Overall, this yields&nbsp;<strong>0.5%</strong> probability per annum of a Tunguska-like event occurring.</p><p>For the probability of a large sub-global asteroid event, I again look at the Chapman &amp; Morrison estimate, the Stokes et al estimate&nbsp; (as summed from the discrete probabilities, yielding an aggregate probability of around 3.24E-05 for the 0.3-1.5 km bucket) and NRC estimate (again an approximation). As before, I penalize Stokes et al and the NRC in the weighted average, for reasons highlighted above. Overall, we have a&nbsp;<strong>0.004%</strong> probability per annum of a large sub-global asteroid event occurring.</p><p>For the probability of a global asteroid event, I once more rely on Chapman &amp; Morrison, Stokes et al (aggregate probability is around 7.19E-07), and NRC (once again an approximation), and perform the weighing in a similar fashion as above for similar reasons, to get a&nbsp;<strong>0.0002%</strong> probability per annum of a global asteroid event occurring.</p><p>Finally, for the probability of a rare K/T scale asteroid event, I shift to relying on&nbsp;<a href=\"https://pubs.geoscienceworld.org/books/book/388/chapter-abstract/3797391/Asteroid-and-comet-flux-in-the-neighborhood-of\"><u>Shoemaker, Wolfe &amp; Shoemaker</u></a> on top of Chapman &amp; Morrison as well as the NRC. They all agree on the precise probability of a &gt;10km asteroid hitting earth (<strong>0.000001%</strong> per annum), and the simple average is just that.</p><p><strong><u>Expected Value</u></strong>: Overall, the expected value of averting asteroid impact deaths is&nbsp;<strong>9.36 * 10<sup>5</sup> DALYs</strong>.</p><p>&nbsp;</p><h1><u>Expected Benefit: Averting Asteroid Impact Injuries</u></h1><p>Beyond killing people, the impact of an asteroid or comet hitting earth can cause injuries not amounting to death, the effects of which I also model.</p><p><strong><u>Moral Weights</u></strong>: I take the value of averting a typical injury in nuclear war to be&nbsp;<strong>5.88 DALYs</strong>. This is calculated as a function of (a) the average disability weight for all injuries, (b) a minor age-based philosophical discount and (c) assuming we save someone of the median age in the relevant population. For more details, refer to&nbsp;<a href=\"https://docs.google.com/document/d/1j67pOUpC5vhC1-H516PAj2-4Fvm5rxJhZNeBVav3kik/edit\"><u>CEARCH's evaluative framework</u></a>.</p><p><strong><u>Scale</u></strong>: As&nbsp;<a href=\"https://cneos.jpl.nasa.gov/doc/neoreport030825.pdf\"><u>Stokes et al</u></a> note, the blast damage from an asteroid's impact on land is similar in nature and scale to the damage from nuclear explosions, so I use the injury-to-fatalities ratio from a nuclear war context here. I consider three separate reference points \u2013 the Wellerstein et al estimate of NATO-Russia nuclear war, the Kristensen, Norris &amp; McKinzie estimate of US-China nuclear war, and a US intelligence estimate of India-Pakistan nuclear war, yielding an average injury-to-fatalities ratio, which I then apply to the already-calculated fatalities per event category:</p><ul><li>Tunguska-like asteroid event:&nbsp;<strong>108,000 injuries</strong>.</li><li>Large sub-global asteroid event:&nbsp;<strong>1.9 million injuries</strong>.</li><li>Global asteroid event:&nbsp;<strong>71.9 million injuries</strong>.</li><li>Rare K/T scale asteroid event:&nbsp;<strong>248 million injuries</strong>.</li></ul><p><strong><u>Persistence</u></strong>: The same discounts discussed in the section on asteroid deaths are applied here.</p><p><strong><u>Value of Outcome</u></strong>: Overall, the raw perpetual value of averting asteroid impact injuries are as follows:</p><ul><li>Tunguska-like asteroid event:&nbsp;<strong>2.9 * 10<sup>6</sup> DALYs</strong>.</li><li>Large sub-global asteroid event:&nbsp;<strong>5.1 * 10<sup>7</sup> DALYs</strong>.</li><li>Global asteroid event:&nbsp;<strong>1.93 * 10<sup>9</sup> DALYs</strong>.</li><li>Rare K/T scale asteroid event:&nbsp;<strong>6.65 * 10<sup>9</sup> DALYs</strong>.</li></ul><p><strong><u>Probability of Occurrence</u></strong>: The same probabilities discussed previously are applied here.</p><p><strong><u>Expected Value</u></strong>: All in all, the expected value of averting asteroid impact injuries is&nbsp;<strong>1.7 * 10<sup>4</sup> DALYs</strong>.</p><h1><u>Tractability</u></h1><p>To mitigate the risk posed by asteroid impact, I consider the potential solution of lobbying for asteroid defence, the theory of change for which is as follows:</p><ul><li>Step 1: Lobby the US Government (i.e. White House and Congress) to fund a standby planetary defence rocket.</li><li>Step 2: ATLAS performs last minute detection of larger asteroids not yet found, or of smaller asteroids that aren't being systematically catalogued</li><li>Step 3: Use of standby planetary defence rocket to successfully destroy or deflect incoming asteroid.</li></ul><p>&nbsp;</p><p>Step 1: The main outstanding risk, given that the NASA-led effort is on track to catalogue all the &gt;140m PHOs, is that either a &gt;140m asteroid is on track to hit earth before the cataloging is&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1CgqHIMztZtErcwM-kUUZarDLtAVRYRODRJP_yQUo9vQ/edit#gid=0\"><u>complete</u></a>, or that smaller asteroids slip through more generally, giving us not enough time to prepare mitigation efforts (n.b.&nbsp;<a href=\"https://www.businessinsider.com/nasa-asteroid-simulation-reveals-need-years-of-warning-2021-5\"><u>five years of preparation</u></a> would be needed for a mission to destroy or deflect an asteroid). Consequently, it would be useful to have a planetary defence rocket on standby \u2013 but whether that is achievable depends on our ability to successfully lobby the US Government accordingly.</p><p>To assess the probability of lobbying success, I take both an outside and inside view.</p><p>For the outside view, I consider three reference classes. First, there is the&nbsp;<a href=\"https://journals.sagepub.com/doi/abs/10.1177/1065912911424285\"><u>general success rate of Washington lobbyists</u></a> (56%). Secondly, I consider past instances of&nbsp;<a href=\"https://www.planetary.org/articles/nasas-planetary-defense-budget-growth\"><u>budget increase for planetary defence</u></a> at NASA. The logic here is that bureaucracies always push the legislature and chief executive for more money, so any actual subsequent increase is indicative of the latter being supportive. Summed across the years, we can hence get a sense of the probability in any particular year that the White House and Congress will be supportive and likely to agree to additional funding for any given purpose. And indeed, 92% of the time, planetary defence gets a year-on-year budget increase. Thirdly, I consider past instances of&nbsp;<a href=\"https://aerospace.csis.org/data/history-nasa-budget-csis/\"><u>a budget increase at NASA more generally</u></a> (same logic as above). Overall, NASA has enjoyed a budget increase in 33 out of 63 years surveyed (52%). In taking a weighted average of the three estimates, the planetary defence/NASA reference classes are penalized for relying on an uncertain chain of logic, and the NASA reference class is then penalized two more times again on top of that, firstly for relying partly on&nbsp;<a href=\"https://www.planetary.org/articles/nasas-planetary-defense-budget-growth\"><u>decades-old data</u></a>; and secondly for just being less relevant than the planetary defence reference class</p><p>For the inside view, my reasoning is as follows.&nbsp;<a href=\"https://www.bloomberg.com/news/articles/2022-09-01/killer-asteroid-search-hits-bump-as-biden-team-urges-nasa-delay\"><u>Congress is generally supportive of planetary defence, but the current Biden administration seems sceptical</u></a>, and together with the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Space_Launch_System\"><u>significant&nbsp;</u></a><a href=\"https://aerospace.csis.org/data/history-nasa-budget-csis/\"><u>cost&nbsp;</u></a>of the standby planetary defence rocket \u2013 assuming similar figures to the Space Launch System \u2013 I estimate that chances of success are fairly low, perhaps 10%.</p><p>Putting the outside and inside views together, I give more weight to the outside view than the inside view, given that the inside view is subject to the usual worries about inferential uncertainty. This yields a 54% chance of success at this stage of the theory of change.</p><p>&nbsp;</p><p>Step 2: Of course, even after a hypothetical planetary defence rocket is ready to go, it can only be launched to destroy or deflect an incoming asteroid if the latter object is detected in the first place. The Asteroid Terrestrial-impact Last Alert System (ATLAS) serves as our last line of defence (assuming other detection systems have failed to detect the incoming threat at longer ranges), but even ATLAS isn't foolproof \u2013 the smaller the object, the harder it is to detect. Assuming that 50-140m asteroids are the main remnant threat, I take an average of the&nbsp;<a href=\"https://arxiv.org/pdf/1011.1028.pdf\"><u>probabilities&nbsp;</u></a>of detection for 50m and 140m asteroids, yielding a 59% detection probability.</p><p>&nbsp;</p><p>Step 3: Having detected the incoming threat, the planetary defence rocket would be launched for intercept. As for whether destruction or deflection is even&nbsp;<a href=\"https://www.nasa.gov/pdf/171331main_NEO_report_march07.pdf\"><u>conceptually possible</u></a> \u2013 (a) in the impulsive category, the use of a nuclear device is the most effective means to deflect a potentially hazardous object (PHO); (b) meanwhile, non-nuclear kinetic impact alternatives (i.e. the DART method) are the most effective non-nuclear option, transferring 10-100 times less momentum than nuclear options for a fixed launch mass; and (c) for slow push techniques, they are useful in relatively rare cases (&lt;1% of expected threat scenarios). The upshot of all this, in any case, is that asteroid deflection is certainly technically plausible given adequate funding.</p><p>As for the specific probability that an attempted intercept achieves success \u2013 I take an outside view to evaluate this. An inside view is not considered, insofar as I lack the technical expertise to make any such determination. For the outside view, the three reference classes I consider are as follows. Firstly, DART, which&nbsp;<a href=\"https://www.nytimes.com/2022/10/11/science/nasa-dart-asteroid-spacecraft.html\"><u>successfully altered</u></a> Dimorphos's orbit around Didymos by 32 minutes, showing that if an Earth-threatening asteroid was discovered sufficiently early it could be deflected by kinetic methods (100% success rate from a sample size of 1). Secondly, I consider the rate of success for NASA's human spaceflight missions \u2013 166 such missions were conducted, of which 3 ended in deadly failure (98% success rate). Thirdly, I consider the rate of success for SpaceX rocket launches (97% so far). I then take a weighted average, doubly penalizing DART because: (a) it has a far&nbsp;<a href=\"https://dart.jhuapl.edu/Mission/Impactor-Spacecraft.php\"><u>lower&nbsp;</u></a>weight (relative to the&nbsp;<a href=\"https://cneos.jpl.nasa.gov/doc/neo_report2007.html\"><u>payloads&nbsp;</u></a>we'll likely need to deflect threatening asteroids), unlike&nbsp;<a href=\"https://www.nasa.gov/centers/johnson/pdf/167751main_FS_SpaceShuttle508c.pdf\"><u>NASA space shuttles</u></a> or&nbsp;<a href=\"https://www.spacex.com/vehicles/falcon-9/\"><u>SpaceX rockets</u></a>, the upshot of which is that more force must be generated for lift, which brings with it greater risks; and (b) DART offers but a sample size of 1. Overall, this suggests a 98% chance of success for a launch of a planetary defence rocket on a destruction/deflection mission.</p><p>Putting the estimates across all three steps in the theory of change together (i.e. the probability of successfully lobbying the US Government to fund a standby planetary defence rocket, the probability of ATLAS managing last minute detection of larger asteroids not yet found, or of smaller asteroids that aren't being systematically catalogued, and the probability of mitigation success), the intervention is expected to eliminate 32% of the remaining risk from asteroids.</p><p>Moving on, let us consider the costs to all this. On the one hand, we have the cost of hiring lobbyists to persuade the US government; I compute this as the average of&nbsp;<a href=\"https://www.opensecrets.org/federal-lobbying/top-spenders\"><u>firms' top-end spending on K-street lobbying in 2021</u></a> (around 21.7 million USD). On the other hand, we have the&nbsp;<a href=\"https://cdn.arstechnica.net/wp-content/uploads/2019/11/shelby-mega-approps-10-23-19.pdf\"><u>immense costs</u></a> of building and operating the hypothetical planetary defence rocket. However, this cost has to be discounted, and fairly significantly: (a) for the probability that it is even incurred (not so, if we fail in the lobbying); and (b) the lower counterfactual cost of US government spending relative to EA funding going to top GiveWell charities or similar. The latter is in turn a function of&nbsp;<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0047272708000248\"><u>diminishing marginal utility of income</u></a>,&nbsp;<a href=\"https://data.worldbank.org/indicator/NY.GDP.PCAP.CD?locations=US-XM\"><u>higher&nbsp;</u></a>US GDP per capita relative to the poor country average, and the top GiveWell health charity's&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1tytvmV_32H8XGGRJlUzRDTKTHrdevPIYmb_uc6aLeas/\"><u>cost-effectiveness</u></a> relative to just giving cash to poor people,&nbsp;<a href=\"https://docs.google.com/document/d/1j67pOUpC5vhC1-H516PAj2-4Fvm5rxJhZNeBVav3kik/edit\"><u>correcting</u></a> for GiveWell potentially undervaluing life vs income. Overall, I estimate that the counterfactual cost of the standby rocket itself is only around 593,000 dollars. In total, this puts the total cost of the intervention at 22.3 million dollars.</p><p>Consequently, the proportion of the problem solved per additional USD 100,000 spent is around&nbsp;<strong>0.00142</strong>.</p><h1><u>Marginal Expected Value of Lobbying for Asteroid Defence</u></h1><p>Combining everything, the marginal expected value of lobbying for asteroid defence is&nbsp;<strong>1,352 DALYs per USD 100,000 spent</strong>, making this 2x as cost-effective as a GiveWell top charity.</p>", "user": {"username": "Joel Tan"}}, {"_id": "mnGL8J62LpgPdNcEn", "title": "Intro to Cyberbiosecurity", "postedAt": "2022-10-19T23:47:07.131Z", "htmlBody": "<p><a href=\"https://www.futurelearn.com/profiles/1046881\">Kathryn Millet</a> at <a href=\"http://biosecu.re/biosecure/index.html\">Biosecure Ltd</a> will be giving an introduction to the emerging field of cyberbiosecurity. This field is about maintaining cybersecurity for the purpose of biosecurity. <strong>The content will be simple enough for beginners at a high school level to understand.&nbsp;</strong></p><p><strong>Some topics covered include</strong>:&nbsp;</p><ul><li>Why emerging cyberattack techniques and the democratisation of biotechnology is increasing cyberbiosecurity risk.</li><li>Why we need to focus on promoting cybersecurity and biosecurity for bio<i>weapons</i> in today's age. As compared to naturally-occuring biological diseases.</li><li>Next steps for those interested in diving deeper into the field.</li></ul><p>This talk is part of a series of talks on technological risks. Other talks in the series include:</p><ul><li>An introduction to safety engineering by me. Link to details <a href=\"https://forum.effectivealtruism.org/events/7v3GMtvKukkgv6QYM/intro-to-safety-engineering\">here</a>.</li><li>An introduction on AI Safety by <a href=\"https://www.linkedin.com/in/joshua-clymer\">Joshua Clymer</a> at the <a href=\"http://safe.ai/\">Centre for AI Safety</a>. Link to details <a href=\"https://forum.effectivealtruism.org/events/T7H9w54m2vrKtchX8/intro-to-ai-safety\">here</a>.</li></ul><p><strong>The event will be hosted via Zoom. If you're interested in joining, message </strong><a href=\"https://forum.effectivealtruism.org/users/madhav-malhotra\"><strong>@Madhav Malhotra</strong></a> for a link.</p>", "user": {"username": "madhav-malhotra"}}, {"_id": "T7H9w54m2vrKtchX8", "title": "Intro to AI Safety", "postedAt": "2022-10-19T23:45:07.490Z", "htmlBody": "<p><a href=\"https://www.linkedin.com/in/joshua-clymer\">Joshua Clymer</a> at the <a href=\"http://safe.ai/\">Centre for AI Safety</a> will be giving an introduction to technical research in AI safety. <strong>The content will be simple enough for beginners at a high school level to understand.&nbsp;</strong></p><p><strong>Some topics covered include</strong>:&nbsp;</p><ul><li>The main categories of AI safety research: making AI algorithms handle unexpected inputs, monitoring them for unwanted outcomes, setting proper goals for them, etc.</li><li>Examples of where AI safety research is needed today and where it might be needed in the future.&nbsp;</li><li>How AI safety research relates to other safety engineering and risk analysis disciplines.</li><li>Next steps for those interested in diving deeper into the field.</li></ul><p>This talk is part of a series of talks on technological risks. Other talks in the series include:</p><ul><li>An introduction to safety engineering by me. Link to details <a href=\"https://forum.effectivealtruism.org/events/7v3GMtvKukkgv6QYM/intro-to-safety-engineering\">here</a>.</li><li>An introduction to cyberbiosecurity risks by <a href=\"https://www.futurelearn.com/profiles/1046881\">Kathryn Millet</a> at <a href=\"http://biosecu.re/biosecure/index.html\">Biosecure Ltd</a>. Link to details <a href=\"https://forum.effectivealtruism.org/events/mnGL8J62LpgPdNcEn/intro-to-cyberbiosecurity\">here</a>.</li></ul><p><strong>The event will be hosted via Zoom. If you're interested in joining, message </strong><a href=\"https://forum.effectivealtruism.org/users/madhav-malhotra\"><strong>@Madhav Malhotra</strong></a> for a link.</p>", "user": {"username": "madhav-malhotra"}}, {"_id": "7v3GMtvKukkgv6QYM", "title": "Intro to Safety Engineering", "postedAt": "2022-10-19T23:44:15.305Z", "htmlBody": "<p>I'll be presenting what I learned about safety engineering at the <a href=\"https://forum.effectivealtruism.org/posts/GQqbbEJzBd4GsraPT/announcing-the-introduction-to-ml-safety-course\">Center for AI Safety's ML Safety Scholars fellowship</a>. <strong>The content will be simple enough for beginners at a high school level to understand.&nbsp;</strong></p><p><strong>Some topics covered include</strong>:&nbsp;</p><ul><li>Examples of failures in lots of engineering applications (from bridges to cars to spacecraft to chemical factories and more).</li><li>Creating quantitative risk equations.</li><li>Traditional safety engineering analyses like Failure Modes and Effects Analysis (FMEA) or Fault Tree Analysis.</li><li>Novel safety engineering analyses like the Systems-Theoretic Accident Model and Process (STAMP).&nbsp;</li></ul><p>This talk is part of a series of talks on technological risks. It'll be followed by:</p><ul><li>An introduction on AI Safety by <a href=\"https://www.linkedin.com/in/joshua-clymer\">Joshua Clymer</a> at the <a href=\"http://safe.ai/\">Centre for AI Safety</a>. Link to details <a href=\"https://forum.effectivealtruism.org/events/T7H9w54m2vrKtchX8/intro-to-ai-safety\">here</a>.</li><li>An introduction to cyberbiosecurity risks by <a href=\"https://www.futurelearn.com/profiles/1046881\">Kathryn Millet</a> at <a href=\"http://biosecu.re/biosecure/index.html\">Biosecure Ltd</a>. Link to details <a href=\"https://forum.effectivealtruism.org/events/mnGL8J62LpgPdNcEn/intro-to-cyberbiosecurity\">here</a>.</li></ul><p><strong>The event will be hosted via Zoom. If you're interested in joining, message </strong><a href=\"https://forum.effectivealtruism.org/users/madhav-malhotra\"><strong>@Madhav Malhotra</strong></a> for the link.</p>", "user": {"username": "madhav-malhotra"}}, {"_id": "GqLGFsZxApn87qQa8", "title": "What is Consciousness?", "postedAt": "2022-10-20T08:11:06.744Z", "htmlBody": "", "user": {"username": "belkarx"}}, {"_id": "YZNswjspTd2MYAaDf", "title": "Growing the US tofu market - a roadmap", "postedAt": "2022-10-20T16:46:04.189Z", "htmlBody": "<h3><i>This post is a roadmap to growing the US tofu market. There is a ton of delicious impact to be had, and I hope more of you in the alt protein space will consider joining me.&nbsp;</i></h3><p><i>(Thank you to Kris Chari and Nick Corvino for helpful feedback.)</i></p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1d1bc3a50a51ed00e28a270f6455b5ce412dfb0ec75d3b66.jpg\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1d1bc3a50a51ed00e28a270f6455b5ce412dfb0ec75d3b66.jpg/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1d1bc3a50a51ed00e28a270f6455b5ce412dfb0ec75d3b66.jpg/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1d1bc3a50a51ed00e28a270f6455b5ce412dfb0ec75d3b66.jpg/w_1800 1800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1d1bc3a50a51ed00e28a270f6455b5ce412dfb0ec75d3b66.jpg/w_2400 2400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1d1bc3a50a51ed00e28a270f6455b5ce412dfb0ec75d3b66.jpg/w_3000 3000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1d1bc3a50a51ed00e28a270f6455b5ce412dfb0ec75d3b66.jpg/w_3600 3600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1d1bc3a50a51ed00e28a270f6455b5ce412dfb0ec75d3b66.jpg/w_4200 4200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1d1bc3a50a51ed00e28a270f6455b5ce412dfb0ec75d3b66.jpg/w_4800 4800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1d1bc3a50a51ed00e28a270f6455b5ce412dfb0ec75d3b66.jpg/w_5400 5400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1d1bc3a50a51ed00e28a270f6455b5ce412dfb0ec75d3b66.jpg/w_6000 6000w\"></figure><p><i>A yuba-based Napoleon pastry from my forthcoming book, Broken Cuisine</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefroe7ezeu1re\"><sup><a href=\"#fnroe7ezeu1re\">[1]</a></sup></span><i>. Photo credit: Ryan Tanaka</i></p><h2><strong>Why should I care about tofu?</strong></h2><p>When I first visited China, I assumed tofu was just a pasty, white block.&nbsp;</p><p>I never would\u2019ve imagined that tofus could <strong>melt</strong>\u2026 or that some varieties tasted like <strong>aged cheese</strong>\u2026 or that others had a <strong>bread-like crumb</strong>!</p><p>Chinese tofus are some of the most culinarily exciting plant-based proteins in existence. Yet, hardly anyone outside the Chinese community knows they exist!&nbsp;</p><p>&nbsp;</p><figure class=\"image image_resized\" style=\"width:504.266px\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/308eed90913d9c1c2c2a9e8a7a5eead69ec00ea8d7355a5c.PNG\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/308eed90913d9c1c2c2a9e8a7a5eead69ec00ea8d7355a5c.PNG/w_151 151w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/308eed90913d9c1c2c2a9e8a7a5eead69ec00ea8d7355a5c.PNG/w_231 231w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/308eed90913d9c1c2c2a9e8a7a5eead69ec00ea8d7355a5c.PNG/w_311 311w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/308eed90913d9c1c2c2a9e8a7a5eead69ec00ea8d7355a5c.PNG/w_391 391w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/308eed90913d9c1c2c2a9e8a7a5eead69ec00ea8d7355a5c.PNG/w_471 471w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/308eed90913d9c1c2c2a9e8a7a5eead69ec00ea8d7355a5c.PNG/w_551 551w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/308eed90913d9c1c2c2a9e8a7a5eead69ec00ea8d7355a5c.PNG/w_631 631w\"></figure><p><i>Yunnan juicy tofu, grilled over charcoal with seasoned chili powder.</i></p><p>The average American eats just 1 block of conventional tofu per year<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefr5fl89q68lg\"><sup><a href=\"#fnr5fl89q68lg\">[2]</a></sup></span>. Yet, even this tiny consumption, added up, is enough to displace millions of animal lives from factory farms each year<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6fmao8g35pc\"><sup><a href=\"#fn6fmao8g35pc\">[3]</a></sup></span>.</p><p>While I personally love firm, soft, and silken tofu, these ingredients are poor fits for western cooking styles and taste preferences. In contrast, several Chinese tofus are much, much, much better fits<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5w6qm21403w\"><sup><a href=\"#fn5w6qm21403w\">[4]</a></sup></span>. If we can find the \"avocado toast\" of these tofus, expand ingredient supply, and successfully promote, the market for these varieties could thus be many times larger, someday saving 10s or 100s of millions of animal lives each year.&nbsp;</p><p>Beyond directly reducing demand for meat, promoting Chinese tofus could have several other benefits:</p><ol><li>Chinese tofus could make plant-based cooking more appealing to chefs in ways that meat analogs (plant-based meats, cultivated meat, etc.) cannot<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyy8unpsplqo\"><sup><a href=\"#fnyy8unpsplqo\">[5]</a></sup></span>. If true, this could turn some of our biggest opponents to plant-based progress into allies and evangelists, shifting the <i>culture&nbsp;</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbz6xl632iib\"><sup><a href=\"#fnbz6xl632iib\">[6]</a></sup></span>around dining. Shifting culture could have more impact than merely substituting meat for mock meat or tofu<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1h430u0i79li\"><sup><a href=\"#fn1h430u0i79li\">[7]</a></sup></span>.</li><li>Better tasting vegan food seems necessary for moral circle expansion<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefecjkhfh5qya\"><sup><a href=\"#fnecjkhfh5qya\">[8]</a></sup></span>. Chinese tofus are one of the best opportunities to expand vegan cooking, as their unique culinary properties can do things that meat and vegetables cannot<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmfcyv77zpjm\"><sup><a href=\"#fnmfcyv77zpjm\">[9]</a></sup></span>. (This seems especially pressing if you have short AI timelines and worry about value lock-in, but there don't seem to be many interventions currently being pursued.)</li><li>Making tofu a high prestige food in the States could possibly raise its status in China. After living in China for several years and studying the Chinese farmed animal welfare space throughout 2020, I am pretty pessimistic about near-term prospects for animals. Promoting tofu in the west, to raise its status in China, could, however, be a straightforward and significant way to contribute.</li><li>Because traditional plant protein advocacy is so neglected within the alt protein space, there could be a lot of information value in trying to promote Chinese tofus<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqioy2bbsgqg\"><sup><a href=\"#fnqioy2bbsgqg\">[10]</a></sup></span>. &nbsp;</li></ol><p>Beyond being massive in scale and very neglected, I am hopeful that this opportunity is tractable, as you'll see below...</p><h2><strong>Wait! What is Chinese tofu?&nbsp;</strong></h2><p>China is the birthplace of tofu and has over 20 distinct varieties<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrxv5jrxbuc\"><sup><a href=\"#fnrxv5jrxbuc\">[11]</a></sup></span>. While some of these varieties are common across Asia, the majority are specific to Chinese cooking. There are a few, in particular, that seem to be great fits for western cooking styles:</p><ol><li><a href=\"https://baike.baidu.com/item/%E7%B4%A0%E9%B8%A1/265385\">Shanghai tofu</a> (\u4e0a\u6d77\u7d20\u9e21 <i>shanghaisuji) </i>- ultra high in protein, rich/eggy/custardy flavor, bready structure that can be cooked into cakes, soup dumplings, \"crostini\", protein crumbles, etc.&nbsp;</li><li><a href=\"https://baike.baidu.com/item/%E5%8D%83%E9%A1%B5%E8%B1%86%E8%85%90/1617695?fr=aladdin\">Spongy tofu</a> (\u5343\u9875\u8c46\u8150<i> qianyedoufu</i>) - a fishcake-like tofu, pre-seasoned (NOT bland), delicious blended, braised, or grilled, can be dense and chewy or light and silky.</li><li><a href=\"https://baike.baidu.com/item/%E5%8D%83%E9%A1%B5%E8%B1%86%E8%85%90/1617695?fr=aladdin\">Fermented tofu</a> (\u8150\u4e73<i> furu</i>) - a funky, cheesy, umami seasoning that's blended into sauces or rubbed onto bread or rice.&nbsp;</li><li><a href=\"https://baike.baidu.com/item/%E8%B1%86%E8%85%90%E5%B9%B2/26675\">Pressed tofu </a>(\u8c46\u8150\u5e72 <i>doufugan</i>) - a type of extra-firm tofu, wintery-spiced flavor, great on grilled cheese, grain bowls, with apple/sugar/cinnamon spices, etc.</li></ol><p>There are a number of rarer tofus that I hope can one day be available in the west, but that are already hard to find in China:</p><ol><li><a href=\"https://baike.baidu.com/item/%E5%8C%85%E6%B5%86%E8%B1%86%E8%85%90\">Juicy tofu</a> (\u5305\u6d46\u8c46\u8150<i>baojiangdoufu</i>) - a meltable tofu popular in Yunnan province.&nbsp;</li><li><a href=\"https://baike.baidu.com/item/%E8%8D%9E%E7%81%B0%E8%B1%86%E8%85%90\">Charcoal ash tofu</a> (\u835e\u7070\u8c46\u8150 <i>qiaohuidoufu</i>) - a tofu from Guizhou province that's coated in ash and slow cooked for a few days. It looks somewhat like pressed tofu, but has a very tender texture and nice smoky flavor.</li><li><a href=\"https://www.beihuo.net/shangpin/452813192903.html\">Bubbly tofu</a> (\u6ce1\u8c46\u8150<i> paodoufu</i>) - a thin tofu sheet from Guizhou province that's dried then cooked in hot rocks to puff up. It needs to be reconstituted (soaked) in liquid before using and has a wonderful texture.&nbsp;</li></ol><p>Another great-fit tofu is <a href=\"https://baike.baidu.com/item/%E6%B2%B9%E7%9A%AE/1617481\">yuba</a> &nbsp;(\u6cb9\u8c46\u76ae <i>youdoupi</i>) - the film that forms atop heated soymilk. It's high in protein, keto, rich in flavor, easy to season, and has many interesting uses - from laminated pastries to easy protein add-ons for salads or pasta. I don't include it in the above list because it's eaten across Asia so is not strictly Chinese.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2101c188ecc51d6ce0de3b8c5b5e59320ec9b8b6d5316af4.jpg\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2101c188ecc51d6ce0de3b8c5b5e59320ec9b8b6d5316af4.jpg/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2101c188ecc51d6ce0de3b8c5b5e59320ec9b8b6d5316af4.jpg/w_1600 1600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2101c188ecc51d6ce0de3b8c5b5e59320ec9b8b6d5316af4.jpg/w_2400 2400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2101c188ecc51d6ce0de3b8c5b5e59320ec9b8b6d5316af4.jpg/w_3200 3200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2101c188ecc51d6ce0de3b8c5b5e59320ec9b8b6d5316af4.jpg/w_4000 4000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2101c188ecc51d6ce0de3b8c5b5e59320ec9b8b6d5316af4.jpg/w_4800 4800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2101c188ecc51d6ce0de3b8c5b5e59320ec9b8b6d5316af4.jpg/w_5600 5600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2101c188ecc51d6ce0de3b8c5b5e59320ec9b8b6d5316af4.jpg/w_6400 6400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2101c188ecc51d6ce0de3b8c5b5e59320ec9b8b6d5316af4.jpg/w_7200 7200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2101c188ecc51d6ce0de3b8c5b5e59320ec9b8b6d5316af4.jpg/w_7952 7952w\"></figure><p><i>A smorgasbord of Chinese fermented and pressed tofus. Photo credit: Ryan Tanaka</i></p><h2><strong>How do we popularize these foods?</strong></h2><p>Our first challenge is to <strong>create a market</strong> for Chinese tofus for non-Chinese audiences. To do so, we need to solve three bottlenecks:&nbsp;</p><ol><li>There\u2019s very limited supply.</li><li>Because of limited supply, most Americans aren\u2019t aware that these tofus exist.</li><li>Because there is limited supply and awareness, there\u2019s little exploration into how to use these ingredients in other cuisines beyond Chinese cooking - meaning it's unclear what needs they're solving.</li></ol><p>Our second challenge is to <strong>scale this market</strong> as big as possible. To do so, it would be helpful to understand a few questions:</p><ol><li>Historically, how have new foods become super popular?&nbsp;</li><li>How can food cultures learn from each other without culturally appropriating, or being perceived as culturally appropriating?</li><li>How can we get the public to re-embrace soy?</li></ol><p>I want to emphasize that few of these things will happen without more firepower. There is so much to be done in this space - so much delicious and meaningful impact to be had! If you're even slightly tofu-curious, <a href=\"https://airtable.com/shrkGupPVHjnEy5zR\"><strong>let me know here</strong></a>. See the summary at the bottom of this post for a long list of ways to support.</p><h1><strong>Creating a Market</strong></h1><h2><strong>Supply.</strong></h2><h3><strong>Current supply</strong>.&nbsp;</h3><p>There\u2019s very little Chinese tofu in the States.</p><p>In fact, there\u2019s not a single company here specifically focused on promoting Chinese tofus. This is probably because the market is tiny and importing frozen goods at low volumes is expensive. As just one example, I estimate current annual profits from importers of Shanghai tofu, or&nbsp;<i>suji</i>, to be on the order of $10,000/year<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefs6pa9wnje2k\"><sup><a href=\"#fns6pa9wnje2k\">[12]</a></sup></span>. That\u2019s not even enough profit to support one full time employee. Other Chinese tofus fare similarly<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhurxdxv0jfa\"><sup><a href=\"#fnhurxdxv0jfa\">[13]</a></sup></span>.&nbsp;</p><p>For low volumes, cold shipping is prohibitively expensive. There are two main options: air freight and ocean freight. Ocean freight is relatively affordable, but requires more volume. I estimate that the smallest ocean container could fit roughly the entire US annual supply of Shanghai tofu<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzjm3evfj19b\"><sup><a href=\"#fnzjm3evfj19b\">[14]</a></sup></span>. Because several trading companies already compete for that market, an extra container of product would likely sit in storage. On the flip side, air freight has 10x lower minimums in terms of container size<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc2lbyo29iz\"><sup><a href=\"#fnc2lbyo29iz\">[15]</a></sup></span>, but the unit cost per tofu product is way too high, likely over $5/log or block-worth<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref99et8md6dah\"><sup><a href=\"#fn99et8md6dah\">[16]</a></sup></span>. Of course, you can try to book a portion of an ocean container, but that\u2019s both a hassle and more costly.</p><p>The main workaround is to bundle small quantities of various products into one shipping container, then ship via ocean freight. By using a whole container, trading and import companies can secure much better rates. This is where most of the US supply comes from.&nbsp;</p><p>A limited supply of a couple other tofus - pressed and yuba are - are domestically produced. I'm familiar with at least four brands producing pressed tofu, and at least one making yuba. These markets are slightly larger - pressed tofu is used across Chinese cuisines, so likely has a 10-100x bigger market than Shanghai tofu; some varieties of yuba are used in other Asian cuisines, making that market bigger too. (That said, a $1M market is still very small, especially when multiple producers and importers are competing for it.)</p><p>These ingredients are available, in limited supply, through Asian food distributors and Chinese supermarkets. Occasionally, a Japanese, Korean, Vietnamese, or pan-Asian market might carry a couple Chinese tofus, but this is uncommon, and the selection is very limited. There is also at least one online grocery store that carries these products. (<a href=\"http://www.sayweee.com/\"><u>www.sayweee.com</u></a>)</p><h3><strong>Building supply.</strong></h3><p>There seem to be three ways to grow the US supply of Chinese tofus.&nbsp;</p><p>First, we could <strong>encourage existing trading companies to import more tofu</strong> and help them partner with more mainstream distributors, maybe by working from within the company or acting as a sales broker. I\u2019m not excited by this approach, for a few reasons:</p><ul><li>Distributors won\u2019t pick up new ingredients unless there\u2019s clear demand. Yet, it\u2019s hard to build demand for an unfamiliar product without spending a lot on marketing, and these trading companies don\u2019t have sufficient budgets.</li><li>These trading companies aren\u2019t investing much employee focus into their tofu lines.</li><li>They generally haven\u2019t responded to my outreach, though I could be more proactive with follow-ups.</li></ul><p>Second, we could <strong>help local producers scale up</strong>. This might involve supporting a company that already produces the product, or encouraging a conventional tofu company to add new product lines. In terms of current producers, only pressed tofu and one variety of yuba are manufactured in the States. While most of these products (in my mind) are lower quality than the imported stuff, there are a couple good ones. Convincing a brand like Huaxing to invest more into marketing and mainstream sales channel could be valuable; granted, as with supporting trading companies, the biggest challenges here are validating demand and securing funds for marketing. I'm not sure how possible that is.</p><p>To convince manufacturers to produce more of the other Chinese tofu varieties, we would again need to help them validate demand. Yet, this runs into the same problems around marketing spend and employee attention, while also requiring capital investment on new machinery. I\u2019ve chatted multiple times with the head of Hodo Foods, the third leading US tofu producer, about adding other tofu products, and they\u2019ve been disinterested each time.</p><p>Third, we could help <strong>start a new distribution company</strong> specifically focused on sourcing and promoting tofu. This would mean finding reliable factories in China, handling import logistics, and selling to US distributors. This is the approach I\u2019m most excited about and is what I\u2019m currently working on. A key advantage is that it could raise venture funding, which could pay for marketing spend and a full team. It could also give us pricing power to keep tofu costs low.&nbsp;</p><p>Starting an import company would require a few things:</p><ol><li>Finding product-market fit. If we can show that a specific tofu fulfills a specific market need, and that market is large, then we can demonstrate to investors that there is a large business opportunity. To get there, I'm trying to persuade LA restaurants&nbsp;to pilot tofu menu items within specific niches (i.e. tofu sheets as Italian \"pasta,\" spongy tofu as the center of a braise, pressed tofu slices in grilled cheese, etc.)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefljed36mar2f\"><sup><a href=\"#fnljed36mar2f\">[17]</a></sup></span>&nbsp;If restaurants see benefits, then I'll pitch to similar restaurants, collect an interest list, and show that to investors. If the pilots don't work well, then I'll try to figure out why and adjust<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefaqk1emsgm\"><sup><a href=\"#fnaqk1emsgm\">[18]</a></sup></span>. While I just started restaurant outreach and am unsure how many warm leads will convert, I'm hopeful.</li><li>Raising funds. Building a pitch deck and business plan. Finding partners \u2013 co-founders, Chinese tofu factories, promoters, distributors, storage facilities. Incorporating as a c-corp. Building branding and basic social media/web presence\u2026</li><li>Importing a container of one type of tofu. Pitching our products to distributors. Selling. Marketing. Invest in R&amp;D to find more use cases.</li><li>Scaling out to other varieties of tofu.</li></ol><h3><strong>Ways you can support:</strong></h3><ul><li><strong>Co-founding </strong>a tofu import company<strong>. </strong>I\u2019m looking for a co-founder with some of the following characteristics: Chinese or Chinese American, action oriented, knowledgeable about startups, social media, or the food industry.</li><li><strong>Funding.&nbsp;</strong><ul><li>I could use grant funding to cover my cost of living before raising venture money. This would allow me to work on this full time, maybe accelerating progress by 3-4 months. (I'm applying to the EA Animal Welfare Fund but am unsure if my grant will go through.)</li><li>I'm also looking for angel investors to invest in our seed round. Not immediately, but once we validate the market, hash out our business model, and have a better sense of budget needs.</li></ul></li><li><strong>Advising. </strong>On the food side (importing, distribution, restaurant sales) or business side (fundraising, building startup infrastructure).</li><li><strong>Offering connections.</strong> To potential co-founders, advisors, funders, or other food people (chefs, restaurateurs, influencers, food writers, culinary school directors, folks who work with restaurants, ...)</li><li><strong>Volunteering.</strong> Helping with informational graphics, photography, import company branding, ... &nbsp;</li></ul><h2><strong>Few use cases and low awareness.&nbsp;</strong><br>(Discussed together, as their solutions overlap.)</h2><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/466787c0a79a741f7e5b722dfaebf87ce4453eed8f58dfb0.jpg\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/466787c0a79a741f7e5b722dfaebf87ce4453eed8f58dfb0.jpg/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/466787c0a79a741f7e5b722dfaebf87ce4453eed8f58dfb0.jpg/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/466787c0a79a741f7e5b722dfaebf87ce4453eed8f58dfb0.jpg/w_1800 1800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/466787c0a79a741f7e5b722dfaebf87ce4453eed8f58dfb0.jpg/w_2400 2400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/466787c0a79a741f7e5b722dfaebf87ce4453eed8f58dfb0.jpg/w_3000 3000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/466787c0a79a741f7e5b722dfaebf87ce4453eed8f58dfb0.jpg/w_3600 3600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/466787c0a79a741f7e5b722dfaebf87ce4453eed8f58dfb0.jpg/w_4200 4200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/466787c0a79a741f7e5b722dfaebf87ce4453eed8f58dfb0.jpg/w_4800 4800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/466787c0a79a741f7e5b722dfaebf87ce4453eed8f58dfb0.jpg/w_5400 5400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/466787c0a79a741f7e5b722dfaebf87ce4453eed8f58dfb0.jpg/w_6000 6000w\"><i>An adapted bagna cauda dip made from shiitakes and fermented tofu, rather than anchovies. Photo credit: Ryan Tanaka</i></p><h3><strong>In western cooking</strong>.&nbsp;</h3><p>To create a market for Chinese tofus in the States, we need to develop scalable&nbsp;ways to use it. By scalable, I mean uses that could be adopted quickly by a wide range of food service establishments. Scalable uses tend to be delicious, low-cost, and easy to assemble - things that could become the next avocado toast, freezer waffles, or acai bowls. For these ingredients to take off outside of the Chinese American community, the use cases will likely be within western cuisines.</p><p>There are several ways to create more use cases.&nbsp;</p><ol><li><strong>Hiring an in-house chef </strong>. A couple years ago, it might cost $5-20k to hire a well-respected R&amp;D chef to develop 20-30 recipes, targeted towards specific use cases. A tofu importer has the clearest case for investing in such R&amp;D; they could use recipes to market to new clients. That said, recipes could also be commissioned by others interested in promoting alt proteins, or by organizations that encourage restaurants to veganize their menus, like Veganuary. They could also be commissioned for a consumer-focused cookbook - but this seems like less of a priority<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref59yx44zv7c6\"><sup><a href=\"#fn59yx44zv7c6\">[19]</a></sup></span>.</li><li><strong>CPGs. </strong>Instead of developing uses to \u2018cook\u2019, someone might develop recipes for a CPG (packaged food) brand. Doing CPG R&amp;D wouldn\u2019t be as helpful for promoting tofu \"as an ingredient\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkds8z9ox2sk\"><sup><a href=\"#fnkds8z9ox2sk\">[20]</a></sup></span>, but it could generally help promote plant-based foods. Moreover, if targeted towards food niches dominated by chicken/fish/eggs, it could scalably reduce animal suffering.</li><li><strong>Out-of-house R&amp;D. </strong>Instead of hiring a chef, we could inspire food bloggers, restaurant chefs, or CPG brands to develop their own recipes. If we mailed ten $100 tofu boxes to ten different food bloggers, I'd guess that at least one or two would try out, like them, and create a video or two on their channel. While it would be harder to direct their R&amp;D towards the highest impact food niches, it\u2019s likely that there would be overlap. More significantly, these videos could expose hundreds of thousands of subscribers to Chinese tofus, creating instant hype. With a little coordination with the bloggers, we could direct this hype in a few ways:<ul><li>To persuade grocery stores to carry these ingredients, we could add a petition to their videos \u2192 collect names and zip codes \u2192 send petition to nearby restaurants or grocery stores \u2192 connect those restaurants/grocery stores to distributors.</li><li>Even better, we could add a mailing list signup. Collecting email addresses would allow us to send future asks, share new ingredient availability, or promote anything tofu.</li><li>We could show CPG (packaged food) companies the interest in those videos and try to persuade them to start tofu CPGs.</li><li>We could create paid ads to market those videos more widely and increase their reach even further\u2026</li></ul></li></ol><p>One risk of this approach is not being able to curate the viewer experience. If a food blogger introduces these tofus to hundreds of thousands of viewers in a suboptimal way, that could lock in disinterest<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpa2fiweir9k\"><sup><a href=\"#fnpa2fiweir9k\">[21]</a></sup></span>. That said, this risk can be minimized by working closely with bloggers - offering recipes and general support - or by selectively screening potential partners.&nbsp;</p><p><i><strong>How much impact would this have?</strong></i></p><p>Developing novel uses cases expands the total addressable market for these ingredients. Finding a use that's 10-100x more scalable than the current uses (i.e., in Shanghai or Beijing cooking), could literally expand that total addressable market by 10-100x. (This wouldn't expand the actual market 10-100x; scaling requires a lot more magic. But it would make it easier to raise investment for a venture, find partners, etc.)</p><p>&nbsp;</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1fb911302cda895c67bf3a4d1588806c02f0515d195b443b.jpg\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1fb911302cda895c67bf3a4d1588806c02f0515d195b443b.jpg/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1fb911302cda895c67bf3a4d1588806c02f0515d195b443b.jpg/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1fb911302cda895c67bf3a4d1588806c02f0515d195b443b.jpg/w_1800 1800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1fb911302cda895c67bf3a4d1588806c02f0515d195b443b.jpg/w_2400 2400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1fb911302cda895c67bf3a4d1588806c02f0515d195b443b.jpg/w_3000 3000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1fb911302cda895c67bf3a4d1588806c02f0515d195b443b.jpg/w_3600 3600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1fb911302cda895c67bf3a4d1588806c02f0515d195b443b.jpg/w_4200 4200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1fb911302cda895c67bf3a4d1588806c02f0515d195b443b.jpg/w_4800 4800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1fb911302cda895c67bf3a4d1588806c02f0515d195b443b.jpg/w_5400 5400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1fb911302cda895c67bf3a4d1588806c02f0515d195b443b.jpg/w_6000 6000w\"><i>Five-spiced pressed tofu in a coconut dulce de leche apple tart from my forthcoming book, Broken Cuisine</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefroe7ezeu1re\"><sup><a href=\"#fnroe7ezeu1re\">[1]</a></sup></span>. <i>Photo credit: Ryan Tanaka</i></p><h3><strong>In Chinese cooking</strong>.&nbsp;</h3><p>Besides creating new uses for Chinese tofus in western cooking, there would be a lot of value in highlighting traditional Chinese uses. This could build demand for these tofu ingredients, while raising awareness for vegan Chinese food, making it easier for meat lovers to find delicious alternatives. Some paths to impact in this area include:</p><ul><li><strong>Partnering with Chinese food bloggers</strong>. Linking petitions or mailing list signups, as described above, could encourage distributors to carry Chinese tofus, improving supply. (Granted, it might be less useful for creating new demand, if Chinese food blogger audiences are already familiar with the ingredients.)&nbsp;</li><li><strong>Writing a book. </strong>Great books seem to have been instrumental in establishing their own culinary fields and making them widely accessible<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhm4pveyaj28\"><sup><a href=\"#fnhm4pveyaj28\">[22]</a></sup></span>. I wonder if writing a bestselling, foundational cookbook on [veg] Chinese tofu cooking could establish it as a high status culinary art, something up there with cheese, soul food, or sushi. If that were possible, it could create a lot of demand for these ingredients and encourage others to throw a lot more resources at starting restaurants, researching these foods, or educating others about them. Assuming there were good fit people who wanted to pursue such a project, investing $100k with a 5% chance at a breakaway bestseller seems like it could do more good than investing similar money in the alt protein space<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkkqbrumtoy\"><sup><a href=\"#fnkkqbrumtoy\">[23]</a></sup></span>.&nbsp;&nbsp;The cost to produce a book would be much higher than one-off partnerships<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefakp1b7gua35\"><sup><a href=\"#fnakp1b7gua35\">[24]</a></sup></span>, but the upsides seem high.&nbsp;</li></ul><h3><strong>General demand.</strong></h3><p>Another option to grow demand would be to open a tofu-themed restaurant tailored towards a specific niche (taco fillings, tofu in Chinese cooking, fast food, etc.) Several upsides to this approach:</p><ul><li>Restaurants that develop cult followings have incredible marketing power - able to influence other restaurants, consumers, media, etc.</li><li>A ghost restaurant model, in particular, could quickly validate different menu concepts. Some of these could be scaled to brick and mortar locations or sold/marketed to others. This would not be too expensive to set up<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref00y4vvvvyvwf\"><sup><a href=\"#fn00y4vvvvyvwf\">[25]</a></sup></span>.</li><li>A restaurant that went through significant volumes of tofu could create a lot of demand for importers, improving their early stage cash flow.</li><li>Restaurants are the best way to curate customer experience and give consumers a positive first impression of tofu. That said, this is tough and slow to scale.</li></ul><h3><strong>Ways you can support:</strong></h3><ul><li><strong>Co-founding </strong>the import project.</li><li><strong>Founding</strong> a CPG brand. (I can share a long list of product ideas and connect you with tofu suppliers.)</li><li><strong>Funding </strong>me<ul><li>$10-20k to commission an R&amp;D chef to develop scalable tofu use cases.</li><li>$1000 to send tofu boxes to 10 food influencers.</li></ul></li><li><strong>Writing a bestselling book</strong> on Chinese tofu cooking.</li><li><strong>Partnering on social media promotion</strong>.</li><li><strong>Opening a ghost restaurant</strong>.&nbsp;</li><li><strong>Connecting </strong>me with chefs, restaurateurs, influencers, food writers, culinary schools, \u2026</li></ul><h1><strong>Scaling</strong></h1><h2><strong>Understanding how ingredients become super popular.&nbsp;</strong></h2><p>A number of ingredients have recently taken off in the States. Researching how these foods became popular could help us be more effective in promoting Chinese tofus, or other plant-based ingredients.&nbsp;</p><p>Some foods that could make interesting case studies include:</p><ul><li>Kimchi</li><li>Gochujang</li><li>Sriracha</li><li>Salmon</li><li>Sushi</li><li>Bubble tea</li><li>Oatmilk</li></ul><p>Some relevant research questions might include:</p><ul><li>How fast did these foods become popular? What did that growth look like over time? Was growth generally constant, or did it follow discrete stages?</li><li>Who were the main interest groups that promoted these foods?</li><li>What environmental conditions supported this growth?</li><li>What challenges did these foods overcome, and how did they overcome them?</li><li>What challenges have they still not solved?</li><li>Did these ingredients take off via similar pathways? If not, which pathways seem most relevant to Chinese tofus?</li></ul><p>I\u2019m sure there are many other relevant questions we could investigate.</p><p><strong>How much value would this research provide?&nbsp;</strong></p><p>I think it\u2019s unlikely that a new market could be created without certain elements: an early adopter community, scalable use cases, stable ingredient supply, etc. That said, scaling such a market is much more uncertain. Great strategies could have 10 or 100x more upside than average strategies (i.e., a 10-100x greater eventual market size.) Some strategies can also operate much, much faster than average ones.&nbsp;</p><p>Worst case scenario, research findings don\u2019t generalize. Mid-case scenario, research simply teaches us what we would have learned from iteration, mentorship, consulting, etc. \u2013 accelerating our progress by several months or years. Best case scenario, research improves our odds of success and helps us grow the tofu market 1x, 2x, or 10x more than we originally could have.&nbsp;</p><p>The impact would thus equal the new tofu trajectory minus the old tofu trajectory. Say research simply accelerated progress by 6 months, with a 10% odds of eventually doubling the tofu market: that would mean nearly 250,000 factory farmed lives averted<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjimv6vruex\"><sup><a href=\"#fnjimv6vruex\">[26]</a></sup></span>. If you think (as I do), that the tofu market could grow 10x, not just double, then that number increases proportionally. Or assume, instead, that we had 10% odds of doubling the tofu market and better research increased those odds to 15%. In expected value terms, that could mean nearly 250,000 factory farmed lives saved <i>each year</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjdn2gc8j8v\"><sup><a href=\"#fnjdn2gc8j8v\">[27]</a></sup></span>. What if we had a 10% odds to double the tofu market, but research gave us a 10% odds to triple it? That would mean nearly 500,000 lives saved <i>each year</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9blc9sz7kih\"><sup><a href=\"#fn9blc9sz7kih\">[28]</a></sup></span>. Since research might affect all of these factors, the impact could add up. (These calculations are obviously very simplistic. Percentage growth in the overall tofu market is a bad benchmark - much better to look at individual use cases and the addressable obtainable markets within<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefd7mo9xrjxw6\"><sup><a href=\"#fnd7mo9xrjxw6\">[29]</a></sup></span>. Yet, it's hard to do so without more clear use cases, thus the need for culinary research.)</p><p>This is all to say, accelerating change could have a lot of benefit, but it would be even more impactful to increase our odds of growing the market, or growing it to much larger size. A month of researcher time (or less) could have a large impact.</p><h3><strong>Ways you can support:</strong></h3><ul><li><strong>Conduct </strong>this research.&nbsp;</li><li><strong>Mentor</strong> this research project.</li><li><strong>Connect </strong>me with someone who might want to pursue this research.</li></ul><h2><strong>Avoiding cultural appropriation.</strong></h2><p>A failure mode I am particularly concerned about is being perceived as culturally appropriating tofu, which could alienate important allies and make it harder to grow the tofu market in the future.&nbsp;</p><p>To be honest, I didn\u2019t really understand this worry until recently. I lived in China for several years, have Chinese god parents, and worked in tofu production. Tofu artisanry is a low-class trade in China, and my shifu (teacher) was ecstatic to meet a foreigner who wanted to promote his craft abroad. Most other Chinese people I meet are supportive too \u2013 what better way to counter rising anti-Chinese sentiment than delicious food, that\u2019s also healthy and great for the climate?</p><p>Yet, back in the States, I\u2019ve encountered some genuine worries about cultural appropriation, which seem to boil down to one thing: historically, white people have commodified elements of cultures they weren\u2019t born into, profited from it, and left the traditional cultures stuck worse off and less understood \u2013 often needing to fight through new racial stereotypes. While it\u2019s unlikely that people in China would suffer from a white guy trying to grow the US tofu market, perhaps Asian Americans might, if done carelessly.&nbsp;</p><p>To that end, I think it\u2019s important that this work doesn\u2019t inadvertently harm the local Asian American community, such as by misrepresenting Asian American tofu culture, diminishing or crowding out existing tofu voices, implying cultural ownership, or anything like that.&nbsp;</p><p>Beyond avoiding the direct problems with cultural appropriation, it\u2019s also important to avoid its perception. Alienating Chinese American, Asian American, or young liberal allies would make it much harder to grow the tofu market. As described above, merely delaying progress by 6 months could lead to 250,000 lives of horrible factory farmed suffering. If opposition to this work spread beyond Asian American communities, and tofu became politicized, that could permanently curtail its future demand; the toll would be millions of animal lives. If we can lower these odds by even 5%, the impact could be millions of animal lives saved.&nbsp;</p><p>Here are some concrete steps that could reduce this risk:</p><ul><li><strong>Finding other Chinese Americans to collaborate with.</strong> If you are Chinese American and excited about popularizing Chinese tofus, I would love to chat. If you know of folks who are in that boat, please pass them my contact info!&nbsp;</li><li><strong>Performing a risk analysis</strong> to figure out the worst ways we could go wrong, including looking at case studies.</li></ul><h3><strong>Ways you can support:</strong></h3><ul><li><strong>Co-found</strong> or <strong>partner </strong>on one of these projects (for Chinese or Chinese Americans.)&nbsp;</li><li><strong>Advise </strong>around cultural appropriation-relevant risk, branding, or PR.</li><li><strong>Help run</strong> a risk assessment.</li></ul><p>All in all, I don\u2019t think cultural appropriation is as big an issue in food as other spaces of society. No one boycotts Thai food for using American chilies, or Mexican food for using European wheat flour tortillas, even though those ingredients originally came from a different culture. Let\u2019s find a way to do this right.</p><h2><strong>Destigmatizing soy.</strong></h2><p>While I don\u2019t think these beliefs are as widespread as most people assume, some Americans are wary of soy, believing that:</p><ul><li>Soy can disrupt hormones, cause infertility, give men boobs, etc. (This is&nbsp;<i>maybe&nbsp;</i>possible if you eat obscene amounts of soy, but not if you eat even a block or two a day<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref75x9ghnf0q6\"><sup><a href=\"#fn75x9ghnf0q6\">[30]</a></sup></span>.)</li><li>Soy is a common allergen. (False \u2013 just 0.1-0.6% of Americans are allergic. Compare that with 2-4% for dairy<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdrpbr9frbqb\"><sup><a href=\"#fndrpbr9frbqb\">[31]</a></sup></span>.)&nbsp;</li><li>Soy is bad for the climate. (False \u2013 most soy is grown for animal feed, which is horrible for the planet. If soy were eaten instead of meat, it might reduce land use by 9x<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxm2uag1k7h\"><sup><a href=\"#fnxm2uag1k7h\">[32]</a></sup></span>, which can create space for reforestation, if you\u2019re into that<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhnqo3m0cip6\"><sup><a href=\"#fnhnqo3m0cip6\">[33]</a></sup></span>. One reason why soy is so widespread is that its hyper efficient to grow.)</li><li>Soy is largely GMO. (True \u2013 but this isn\u2019t a bad thing. It may even be good for the climate as GMO soy requires less land. Also, most of GMO soy is fed to livestock \u2013 if you\u2019re afraid of GMOs, you should be scared of meat.)</li></ul><p>If soyfoods were less stigmatized, I think the tofu market could be much bigger. One scalable source of growth might come from the CPG space. A ton of new food and beverage brands are launched each year, and they absorb millions in investment and invest millions into marketing and consumer education. Yet, more and more brands are moving away from soy. Making it easier for new CPG brands to use tofu could a) funnel more money (from CPG brands and their investors) into tofu marketing efforts, leading to increased overall tofu demand; b) create more plant-based food options for consumers, with a portion replacing meat demand, depending on the type of CPG.&nbsp;</p><p>Another source of demand could be from the bodybuilding community, if reducing misconceptions around soy and testosterone led more bodybuilders to explore soy protein (rather than, say, chicken breast).</p><p>Destigmatizing soy for alt protein brands might allow them to use cheaper protein, lowering their costs<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflzvpi7wrx4k\"><sup><a href=\"#fnlzvpi7wrx4k\">[34]</a></sup></span>?</p><p>I\u2019m unsure how to begin quantifying that potential value. There are probably many sub-communities that could use more tofu instead of animals. The tofu market could probably grow bigger in general.</p><p>I\u2019m also unsure how tractable it would be to reduce stigma. Some half-baked strategies might include:</p><ul><li>Running a campaign to establish anti-soy attitudes as culturally insensitive or racist. The argument being, soyfoods have been consumed for thousands of years in Asia and&nbsp; there is no evidence that they are unsafe for humans at normal consumption levels. Thus, being anti-soy is ignoring Asia. A campaign could include a series of video ads, featuring Asian American celebrities or doctors and various big brand partnerships. (I don\u2019t think people who are anti-soy are racist \u2013 but convincing a swath of Americans that being anti-soy is culturally insensitive could be one way to reduce stigma.)</li><li>A media campaign pointing out double standards between how consumers view edamame (fresh soybeans, seen as a health food), miso (a soybean-based seasoning), and tofu. I genuinely believe there is a double standard between how Americans view Japanese-branded foods (refined, high end, healthy) and Chinese-branded foods (cheap, mass produced, fast food). Such a campaign should be done in a way that doesn't put down Japanese cooking, but instead celebrates both.</li><li>A partnership campaign with leading and respected health agencies clarifying misconceptions around soy.</li><li>A media campaign making fun of the ridiculous soy boob myth and leading into more educational content. Perhaps featuring people perceived as healthy and fit, like professional athletes.</li></ul><p>An alternative to directly reducing soy stigma would be to remove the elements of soy foods that consumers most worry about - isoflavones, the phytoestrogen in soybeans. Removing this component entirely might alleviate fears of hormonal disruption and make it possible to eat even larger quantities of soy protein. The first step might be to conduct a literature review on factors that effect isoflavone concentration in soybeans, such as varietal<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhzvawk9szl\"><sup><a href=\"#fnhzvawk9szl\">[35]</a></sup></span>, cooking time<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6ek62zvtx37\"><sup><a href=\"#fn6ek62zvtx37\">[36]</a></sup></span>, processing<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefo9rksuqmw6i\"><sup><a href=\"#fno9rksuqmw6i\">[37]</a></sup></span>, etc. Perhaps there's a cheap, efficient processing method that can remove this content from tofu. Or perhaps there's a type of tofu with naturally low isoflavone content.&nbsp;</p><p>(Granted, this seems like a risky strategy. It feels likely that marketing foods as non-GMO reinforces consumer concerns, leading to greater anti-GMO sentiment. Something similar could happen here, increasing opposition to mainstream soy foods.)</p><p><br><strong>Ways you can support:</strong></p><ul><li><strong>Organize a destigmatize soy SWAT team.&nbsp;</strong></li><li><strong>Conduct a literature review </strong>on how to lower the isoflavone content of soyfoods or tofu.</li><li><strong>For an MD,</strong> collate soy relevant medical advice into an actionable recommendation on how much tofu to consume.</li></ul><h1><strong>Let's get started!</strong></h1><p>If you think I'm missing important considerations, I would love to hear them in the comments.</p><p>If you would like to learn more about tofu or support any way, <a href=\"https://airtable.com/shrkGupPVHjnEy5zR\"><strong>let me know here</strong></a><strong>.&nbsp;</strong></p><p>Thanks for your time - I look forward to working with more of you on popularizing Chinese tofus in the States!</p><h1><strong>SUMMARY: Ways you can support</strong></h1><ul><li><strong>Founding</strong><ul><li>Co-founding a tofu import company.</li><li>Founding a tofu CPG (packaged food) brand. (I can share a long list of product ideas and connect you with tofu suppliers.)</li><li>Opening a ghost restaurant.&nbsp;</li><li>Organizing a destigmatize soy SWAT team.</li><li>Writing a bestselling book on Chinese tofu cooking.</li></ul></li><li><strong>Funding </strong>me<ul><li>$10-20k to commission an R&amp;D chef to develop scalable tofu use cases.</li><li>$1000 to send tofu boxes to 10 food influencers.</li><li>$13k to cover my cost of living for 4 months to pursue tofu work full time. (I'm applying to the EA Animal Welfare Fund but am unsure whether my grant will go through.)</li><li>Angel investment in a tofu import company, once we validate a market, hash out a business model, and have a better sense of budget needs.</li></ul></li><li><strong>Promoting&nbsp;</strong><ul><li>This post to your networks.</li><li>Tofu projects/products via social and other media.</li></ul></li><li><strong>Connecting </strong>me with&nbsp;<ul><li>Potential co-founders</li><li>Chefs, restaurateurs, influencers, food writers, culinary schools, etc.</li><li>Researchers</li></ul></li><li><strong>Researching</strong><ul><li>Case studies - how have other foods become crazy popular?</li><li>Risk assessment - how can we minimize the risk of cultural appropriation (and its perception)?</li><li>Literature review - can we lower the isoflavone content of soybeans or tofu?</li><li>Soy health - collating relevant medical advice into an actionable recommendation on how much tofu to consume (for an MD or something with medical authority.)</li></ul></li><li><strong>Advising</strong><ul><li>Around business/startups</li><li>Around food industry&nbsp;</li><li>Around<strong> </strong>cultural appropriation-relevant risk, branding, or PR.</li></ul></li></ul><p><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnroe7ezeu1re\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefroe7ezeu1re\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I'm working with a small team of chefs on a book to develop new uses for 5 tofus within western cooking. See <a href=\"https://www.brokencuisine.com/\">https://www.brokencuisine.com</a> for more info.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnr5fl89q68lg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefr5fl89q68lg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This report suggests that US tofu market sales are around ~$500M. If the average supermarket tofu block is $1.50, this implies that less than 1 block per American is sold each year. <a href=\"https://www.grandviewresearch.com/industry-analysis/tofu-market.\">https://www.grandviewresearch.com/industry-analysis/tofu-market.</a></p><p>This market size is different from that in my previous post, which estimated that the average Californian ate 1-3 blocks tofu/month. Perhaps this latest number is an underestimate, but I doubt it's off by more than an order of magnitude.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6fmao8g35pc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6fmao8g35pc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>2020 US per capita meat consumption=115 lbs chicken. <a href=\"https://farmdocdaily.illinois.edu/2021/05/an-overview-of-meat-consumption-in-the-united-states.html\">https://farmdocdaily.illinois.edu/2021/05/an-overview-of-meat-consumption-in-the-united-states.html</a></p><p>The average broiler chicken liveweight is 6.5 lbs <a href=\"https://www.nationalchickencouncil.org/about-the-industry/statistics/u-s-broiler-performance/\">https://www.nationalchickencouncil.org/about-the-industry/statistics/u-s-broiler-performance/</a>&nbsp;</p><p>65% chicken live weight is edible. <a href=\"https://yourmeatguide.com/whole-chicken-portions-meat-yield/\">https://yourmeatguide.com/whole-chicken-portions-meat-yield/</a>&nbsp;</p><p>(115lbs ) / (6.5lbs/bird * 65%) = 27.2 birds eaten per person / per year.</p><p>If 80% of chicken is eaten at lunch and dinner, and tofu displaced 50% of the meat eaten at a given meal, this would mean .015 chickens displaced per meal. Multiplied by the 333 million blocks of tofu eaten per year, that would mean nearly 5 million chickens/year.</p><p>(27.2birds*80%)/(365*2)*(333,000,000*50%)=4,900,000 chickens.</p><p>A 50% meat substitution rate would also displace other meat consumption beyond chicken. The impact of reduced beef and pork consumption is likely dwarfed by chicken, as they're much larger animals. Chicken substitution may itself be dwarfed by fish and seafood substitutions. While meat demand may not immediately lead to lower supply (like if less chicken consumption led to lower demand, which reduced the price, which led to increased consumption...), it should go down over the longer run.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5w6qm21403w\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5w6qm21403w\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I'm writing a book on this subject - see <a href=\"https://www.brokencuisine.com/\">https://www.brokencuisine.com</a>. The reasons are somewhat convoluted, but if you shoot me a message and your email I can send you a full explanation from our book.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyy8unpsplqo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyy8unpsplqo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>While some plant-based meat products have taken off, these ingredients are still, overwhelmingly, perceived by chefs and food snobs to be less desirable than meat. This is because most still don\u2019t taste as good, those that taste as good in one use (like burgers) don\u2019t taste as good in other uses, and the products are highly processed. While processing is not inherently bad in my mind, chefs\u2019 favorite ingredients are almost always farm-to-table, natural, artisan, or in-season ingredients, meaning that plant-based meats have an inherent disadvantage.&nbsp;</p><p>In contrast, Chinese tofus aren\u2019t meat analogs; they have unique properties, allowing them to do unique things that meat cannot. They are also natural, healthy, and can have artisan, cultural stories.&nbsp;</p><p>These ingredients, I believe, can turn chefs from our biggest antagonists \u2013 the biggest stalwarts to plant-based progress \u2013 to our evangelists \u2013 literally using these ingredients to create new culinary experiences, serving them to their customers, and beginning to shift America\u2019s gastronomic culture away from meat.&nbsp;</p><p>Granted, this is just a hypothesis - it could be wrong. We should have a better sense once more restaurants begin trialing these ingredients.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbz6xl632iib\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbz6xl632iib\">^</a></strong></sup></span><div class=\"footnote-content\"><p>What chefs enjoy cooking and diners love eating.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1h430u0i79li\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1h430u0i79li\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Before tofu: chefs dislike plant protein options -&gt; restaurant menus are mostly meat.</p><p>After tofu: chefs fall in love with a plant protein -&gt; start designing menus around these ingredients (not as substitutes for meat, just as their own thing) -&gt; tofu crowds out meat options, resulting in less meat being ordered -&gt; consumers get used to eating more plants and normalize different habits.</p><p>The impact comes from crowding out and changing norms, rather than directly replacing meat dishes one-by-one.&nbsp;</p><p>In contrast, to completely replace the meat on a mid-sized restaurant menu, a chef might need 10 great, high fidelity meat/dairy/egg analogs. We simply don't have those yet, and probably won't for several decades (<a href=\"https://forum.effectivealtruism.org/posts/4uYebcr5G2jqxuXG3/when-can-i-eat-meat-again\">https://forum.effectivealtruism.org/posts/4uYebcr5G2jqxuXG3/when-can-i-eat-meat-again</a>). And most chefs don't want highly processed plant-based meats, even if they taste as good.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnecjkhfh5qya\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefecjkhfh5qya\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Most Americans care about animal suffering (https://news.gallup.com/poll/183275/say-animals-rights-people.aspx), yet the vast majority eat meat. The chief reason is taste.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmfcyv77zpjm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmfcyv77zpjm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Some people have asked why I focus on tofu, rather than tempeh, lentils, or anther plant-based protein. The reason is that Chinese tofus are far more culinarily diverse than these others. Of the 20+ types, each has unique strengths that can do things that the others cannot. The difference, for example, between a cheesy and a dried tofu is huge - they're entirely different ingredients.&nbsp;</p><p>(And they're also delicious, affordable, sustainable, and healthy.)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqioy2bbsgqg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqioy2bbsgqg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Besides a few food bloggers, there aren't many people promoting Chinese tofus in the States. An even smaller portion are focused on non-Chinese or non-Asian audiences.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrxv5jrxbuc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrxv5jrxbuc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In its simplest form, tofu is <i>bean curd</i>. Think of this like soymilk cheese. Drop something sour or minerally into fresh, hot soymilk and its proteins will <i>coagulate</i>, or bind together, into soft, pillowy <i>curds</i>. These curds can be left as is \u2013 to coalesce into <i>silken tofu</i> \u2013 or spooned into a mold and pressed into <i>soft tofu</i>, <i>firm tofu</i>, <i>pressed tofu, </i>or <i>tofu sheets</i>. In China, firm and pressed tofu are often smoked, salted, dehydrated, or fermented \u2013 which transforms them into several other distinct varieties.</p><p>Not all tofu is&nbsp;bean curd, however. Some varieties are made from soy protein, not whole beans (see <i>spongy tofu</i>). Others are biproducts from making bean curd (see <i>yuba</i>).</p><p>While most tofus are made from soybeans, there are also non-soy varieties. These are generally made from rice, almonds, peanuts, chickpeas, oat chestnuts, hemp, sesame, egg, pig\u2019s blood, or milk. Don\u2019t let their name fool you, though \u2013 these ingredients taste, feel, and cook up completely differently from the originals.</p><p>See <a href=\"https://georgestiffman.medium.com/a-complete-list-of-chinas-27-tofus-7274f764d24\">https://georgestiffman.medium.com/a-complete-list-of-chinas-27-tofus-7274f764d24</a> for a long list of different varieties.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fns6pa9wnje2k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefs6pa9wnje2k\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This tofu is not really used outside of Shanghai/Zhejiang/Jiangsu cooking; there are maybe 10 Shanghai restaurants in greater LA, and given that roughly 10% of Chinese Americans live in greater LA (<a href=\"https://en.wikipedia.org/wiki/Chinese_Americans\"><u>https://en.wikipedia.org/wiki/Chinese_Americans</u></a>;&nbsp;<a href=\"https://en.wikipedia.org/wiki/List_of_U.S._cities_with_significant_Chinese-American_populations\"><u>https://en.wikipedia.org/wiki/List_of_U.S._cities_with_significant_Chinese-American_populations</u></a>), I\u2019d guess that there are less than 100 Shanghai restaurants in the U.S.</p><p>Less than 20% of Shanghai restaurants in LA use this tofu. Those that do might use 10 logs/week (it\u2019s not popular.) That would imply that restaurants use roughly 10,000 logs per year. If retail demand were similar, that would imply a total yearly demand of just 20,000 logs. Even if retail demand were a lot higher, it still wouldn\u2019t be much.)</p><p>Import-to-wholesale markups are roughly 20%, implying gross revenues of roughly $.50/log, or $10,000.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhurxdxv0jfa\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhurxdxv0jfa\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Spongy tofu, fermented tofu, stinky tofu, and tofu sheets all seem similarly uncommon.&nbsp;</p><p>Pressed tofu is more popular, as it's used across regional Chinese cuisines - maybe there's a 10-50x bigger market? This is the main type of Chinese tofu that is produced locally.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzjm3evfj19b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzjm3evfj19b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>One 20' shipping container should fit 10 pallets of Shanghai tofu, or roughly 20,000 logs.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc2lbyo29iz\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc2lbyo29iz\">^</a></strong></sup></span><div class=\"footnote-content\"><p>LD-3 are the smallest air freight containers with refrigeration (reefer) options that I could find. They can fit roughly 1 pallet, or 1/10 of a 20' container. <a href=\"https://targettransport.com/wp-content/uploads/2018/09/Air-Freight-Container-Specifications.pdf\">https://targettransport.com/wp-content/uploads/2018/09/Air-Freight-Container-Specifications.pdf</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn99et8md6dah\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref99et8md6dah\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The <a href=\"https://www.freightos.com/\">Freightos.com</a> shipping calculator quoted me ~$3000 for a 180kg pallet of 11 cases x 30 packs of suji \u2192 $10/pack, or $5/log or block-worth. This article has similarly high rates: <a href=\"https://huntersourcing.com/shipping-from-china-to-us/\">https://huntersourcing.com/shipping-from-china-to-us/</a> .&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnljed36mar2f\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefljed36mar2f\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Talking to restaurants has been surprisingly straightforward. I've literally just walked into restaurants during the slow time (or when they're closed in the afternoon preparing for the dinner rush), asked if their manager was around, and described that I'm thinking of starting a tofu import business and trying to gauge interest. Then we'll have a brief conversation, I'll offer to bring samples, and things happen.&nbsp;</p><p>I only just started restaurant outreach so am unsure whether many of these warm leads will become partners.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnaqk1emsgm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefaqk1emsgm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Whether it's something about the dish - flavor, appearance, name, preparation -, restaurant clientele, overall menu composition, etc.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn59yx44zv7c6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref59yx44zv7c6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>It\u2019s probably easiest to promote tofu via food service/restaurants first, then retail/grocery stores. This is because consumer education is harder for home cooks than restaurants; you can better control the user experience in food service so increase the likelihood of enjoyment; and food service demand seems easier to initially scale. Moreover, a book would need more than 30 recipes (ideally 100+), meaning it would require more upfront investment.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkds8z9ox2sk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkds8z9ox2sk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Promoting tofu \"as an ingredient\" that chefs can build around could give it legs - resulting in dozens of interesting use cases.&nbsp;</p><p>Promoting it as a product (like a frozen meal) will only ever have one use.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpa2fiweir9k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpa2fiweir9k\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Letting food bloggers loose scares me. What if they decide we need to <i>press </i>the other tofus?? Or even worse, <i>marinate </i>them!? These techniques have stuck around for a long, long time - even though they don't work for their stated goals.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhm4pveyaj28\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhm4pveyaj28\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Thinking about&nbsp;<a href=\"https://www.amazon.com/Art-Fermentation-Depth-Exploration-Essential/dp/160358286X\"><u>The Art of Fermentation</u></a>,&nbsp;<a href=\"https://smile.amazon.com/Food-Lab-Cooking-Through-Science/dp/0393081087/ref=sr_1_1?crid=14TNVXGDC4TDK&amp;keywords=the+food+lab&amp;qid=1664900482&amp;qu=eyJxc2MiOiIxLjQ0IiwicXNhIjoiMC44NiIsInFzcCI6IjAuODEifQ%3D%3D&amp;sprefix=the+food+lab%2Caps%2C217&amp;sr=8-1\"><u>The Food Lab</u></a> for science-informed cooking,&nbsp;<a href=\"https://www.amazon.com/Modernist-Cuisine-Science-Stainless-Slipcase/dp/1734386142/ref=sr_1_1?crid=2TRRSG2WPC1DA&amp;keywords=modernist+cuisine&amp;qid=1664900972&amp;qu=eyJxc2MiOiIzLjI1IiwicXNhIjoiMy4xNSIsInFzcCI6IjIuODkifQ%3D%3D&amp;sprefix=modernist+cuisine%2Caps%2C485&amp;sr=8-1\"><u>Modernist Cuisine</u></a>, etc.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkkqbrumtoy\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkkqbrumtoy\">^</a></strong></sup></span><div class=\"footnote-content\"><p>To my knowledge, much less than 5% of alt protein seed stage companies have later grown and won over mainstream meat-eating audiences.&nbsp;</p><p>Moreover, even the top companies, like Impossible and Beyond, have hardly shifted animal suffering. An 888lb average carcass weight for cows (https://www.beefmagazine.com/beef/cattle-slaughter-and-carcass-weights-2021) means that one cow could make over 3,500 quarter-pound burgers. Beyond Meat is projecting ~$400M in revenue in 2022. (https://www.reuters.com/business/retail-consumer/beyond-meat-slashes-revenue-forecast-cut-200-jobs-2022-10-14/) Assuming that this came entirely from burgers, each burger was directly replacing an animal meat burger, and one burger sold for $2 (for easy math - I think the number is closer to 3 or 4), that would mean they sold 200M burgers, or the equivalent of not even 60,000 cows.</p><p>The impact would be higher for a chicken, shrimp, or fish product, but most companies haven't approached 1/10 of Beyond's valuation, and most don't appeal to meat eaters.</p><p>Granted, these numbers are ever shifting. Maybe a new breakout company will emerge. Or maybe Beyond or Impossible will grow another 10x.</p><p>Are 5% odds of producing a bestseller too optimistic? It\u2019s hard to say - plant-based eating is trending up, cultural foods are taking off, this topic has incredible narratives around US-China dialogue/peacebuilding, much of producing a bestseller is just PR, many folks in the veg and EA communities have written bestsellers and could support such a project...</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnakp1b7gua35\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefakp1b7gua35\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Cookbook often take&nbsp; 1-2 years of part-time effort from a solid team, familiar with the cuisine and book publishing. Research-intensive ones might take much longer, but fortunately there are millions of tofu recipes on the Chinese internet that can be referenced and cited.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn00y4vvvvyvwf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref00y4vvvvyvwf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Two years ago, a small LA-based virtual restaurant space might cost $2-3k/month with just a few months required down-payment. Hiring a quality chef might require a bit more. Getting enough tofu could be a challenge if someone can't start up an import business and the existing importers can't guarantee supply.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjimv6vruex\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjimv6vruex\">^</a></strong></sup></span><div class=\"footnote-content\"><p>4.9 million chickens saved each year because of conventional tofu (see note 2) * 1.0 (growth in tofu market) * 10% likelihood *1/2year (acceleration) = 246,000 chickens saved.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjdn2gc8j8v\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjdn2gc8j8v\">^</a></strong></sup></span><div class=\"footnote-content\"><p>4.9 million chickens saved each year because of conventional tofu (see note 2) * 1.0 (growth in tofu market) * (15-10% likelihood) = 246,000 chickens saved each year.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9blc9sz7kih\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9blc9sz7kih\">^</a></strong></sup></span><div class=\"footnote-content\"><p>4.9 million chicken saved each year because of conventional tofu (see note 2) * (2-1.0) (growth in tofu market) * (10% likelihood) = 492,000 years of chicken suffering saved each year.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnd7mo9xrjxw6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefd7mo9xrjxw6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>More rigorous estimates might model growth situations based on use cases, then sum up the weighted probabilities of each use case panning out.&nbsp;</p><p>Take one example use case: Shanghai tofu used as soup dumplings.</p><p>Say Shanghai tofu soup dumpling balls were a great add-in for chicken noodle soup that allowed restaurants to reduce the amount of chicken per serving by 50%, saving money while maintaining the deliciousness of the soup. Say there were 100,000 US restaurants that served 5 servings of chicken noodle soup per day (this is a totally made up number), 5% of them were in California (which would allow for easy tofu distribution), and 5% were eventually attainable as customers. This would imply a \"serviceable obtainable market\" for this one use case of Shanghai tofu of ~460,000 bowls of soup per year. If one original bowl of soup used 1/10 of a chicken, and one adjusted bowl used 1/20, then the amount of savable chickens per year would be ~23,000.</p><p>Say there was a 10% chance of achieving that entire market, a 20% chance of achieving 50%, a 30% chance of achieving 20%, and on... Then we could add up those numbers and create a more precise impact estimate.</p><p>(This is a pretty unimpactful use case, as not much chicken is used. Culinary research could definitely uncover better ones.)</p><p>For each use case, there would be a new potential addressable market. Potential impact estimates could roughly add up.&nbsp;</p><p>Research could shift the likelihood percentages, which use cases we end up focusing on (i.e., market size), or the speed at which we come up with them.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn75x9ghnf0q6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref75x9ghnf0q6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Human studies have looked at soy intake on diverse hormonal indicators (testosterone, fertility, etc.) and found that consuming 60-240 milligrams of isoflavones per day has zero downsides. (<a href=\"https://examine.com/articles/is-soy-good-or-bad/\">https://examine.com/articles/is-soy-good-or-bad/</a>) &nbsp;</p><p>For reference, 240 milligrams of isoflavones is roughly two 14 ounce blocks of firm tofu. That\u2019s over 60 grams of soy protein \u2013 in one day! (<a href=\"https://www.ars.usda.gov/ARSUserFiles/80400525/Data/isoflav/Isoflav_R2-1.pdf\">https://www.ars.usda.gov/ARSUserFiles/80400525/Data/isoflav/Isoflav_R2-1.pdf</a> - 100g of Azumaya Firm Tofu has 31 mg of isoflavones, which is high compared to other comparable tofus. Thus, one 14 oz block would have roughly 123 mg isoflavones. Cooking seems to reduce this amount.)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndrpbr9frbqb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdrpbr9frbqb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See section on \"PREVALENCE DATA\": <a href=\"https://journals.lww.com/nutritiontodayonline/Fulltext/2020/01000/Recent_Surveys_on_Food_Allergy_Prevalence.6.aspx\">https://journals.lww.com/nutritiontodayonline/Fulltext/2020/01000/Recent_Surveys_on_Food_Allergy_Prevalence.6.aspx</a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxm2uag1k7h\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxm2uag1k7h\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Chickens, the most efficient livestock animal, only convert 11% of the feed they eat into calories. That means to get 110 calories of chicken meat, farmers need to first grow 1000 calories worth of corn and soy feed. Pigs and cows are even less efficient. <a href=\"https://awellfedworld.org/feed-ratios/\">https://awellfedworld.org/feed-ratios/</a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhnqo3m0cip6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhnqo3m0cip6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The US government Congressional Research Center has modeled reforestation opportunities and finds that they could significantly reverse climate change. <a href=\"https://sgp.fas.org/crs/misc/R40562.pdf\">https://sgp.fas.org/crs/misc/R40562.pdf</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlzvpi7wrx4k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflzvpi7wrx4k\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I have always assumed pea protein to be more expensive than soy but didn't find good comparisons in a 3 minute search. If anyone has knowledge about this, feel free to chime in.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhzvawk9szl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhzvawk9szl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://pubmed.ncbi.nlm.nih.gov/15137811/</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6ek62zvtx37\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6ek62zvtx37\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://pubmed.ncbi.nlm.nih.gov/27211649/</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fno9rksuqmw6i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefo9rksuqmw6i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://pubmed.ncbi.nlm.nih.gov/9848521</p></div></li></ol>", "user": {"username": "George Stiffman"}}, {"_id": "nj9FLkifyb3s6Eijx", "title": "Announcing Squigglepy, a Python package for Squiggle", "postedAt": "2022-10-19T18:34:20.431Z", "htmlBody": "<p><a href=\"https://www.squiggle-language.com/\">Squiggle</a> is a \"simple programming language for intuitive probabilistic estimation\". It serves as its own standalone programming language with its own syntax, but it is implemented in JavaScript.</p>\n<p>I like the features of Squiggle and intend to use it frequently, but I also frequently want to use similar functionalities in Python, especially alongside other Python statistical programming packages like Numpy, Pandas, and Matplotlib.</p>\n<p>The squigglepy package here implements many Squiggle-like functionalities in Python.</p>\n<p>The package also has useful utility functions for Bayesian networks (using rejection sampling), pooling forecasts (via <a href=\"https://forum.effectivealtruism.org/posts/sMjcjnnpoAQCcedL2/when-pooling-forecasts-use-the-geometric-mean-of-odds\">weighted geometric mean of odds</a> and others), laplace (including the <a href=\"https://www.lesswrong.com/posts/wE7SK8w8AixqknArs/a-time-invariant-version-of-laplace-s-rule\">time-invariant version</a>), and <a href=\"https://en.wikipedia.org/wiki/Kelly_criterion\">kelly betting</a>.</p>\n<p><strong><a href=\"https://github.com/rethinkpriorities/squigglepy\">The package and documentation are available on GitHub.</a></strong></p>\n<p>The package can be <a href=\"https://pypi.org/project/squigglepy/\">downloaded from Pypi</a> using <code>pip install squigglepy</code>.</p>\n<p>This package is unofficial and supported by myself and Rethink Priorities. It is not affiliated with or associated with the Quantified Uncertainty Research Institute, which maintains the Squiggle language (in JavaScript).</p>\n<p>This package is also new and not yet in a stable production version, so you may encounter bugs and other errors. Please report those so they can be fixed. It's also possible that future versions of the package may introduce breaking changes.</p>\n<p>This package is available under an MIT license.</p>\n", "user": {"username": "Peter_Hurford"}}, {"_id": "z3J439wF8Xk8qSZku", "title": "Announcing VIVID: A new EA organization aspiring to scale effective self-improvement & reflection", "postedAt": "2022-10-19T18:19:03.886Z", "htmlBody": "<h1>Summary</h1><p>We\u2019re excited to announce VIVID - a new EA startup aspiring to increase the effectiveness of any personal growth pursuit, and scale effective self-improvement &amp; reflection.&nbsp;</p><p>With an investment of $1.75M from one of the biggest EA funds, &nbsp;we're building a two-sided platform: On one side, a mobile app that helps individuals overcome their internal obstacles. On the other side, a flexible tool for personal-development professionals to implement interventions and create long-lasting impact in the everyday lives of their clients and audience.&nbsp;</p><p>Over time, VIVID learns what are the most effective interventions for different topics and people, and aims to make effective self-improvement accessible everywhere.</p><p>The app is soon coming out of beta phase, so we\u2019re looking for feedback, positive and negative! <a href=\"https://vivid-app.onelink.me/hYKI/EAForum1\"><strong><u>Our app is available here</u></strong></a> (both for Android and iOS).&nbsp;</p><h1>Overview</h1><p>There is an inherent problem with self-change advice. On the one hand, it has to be generic because it must be applicable to a wide range of subjects and individuals. On the other hand, individuals can\u2019t really apply the generic advice as it is - they need to complete the last mile of customization in order to apply it.<br>This is the point of failure of many good theories and best practices that are found in self-help books, academic articles, and even advice from therapists - they are often lost in adaptation.</p><p>There are two possible solutions to this problem. The first is to make customization easier. The second is to assume that we don\u2019t need to create all of the possible customized self-change plans that could ever exist, but rather just find the most effective ones for common problems. For instance, find the five most effective plans for the&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/impostor-syndrome\"><u>specific context of impostor syndrome within the specific context of EA</u></a>.&nbsp;</p><p>How can we find them? And do so for thousands of other common use cases? This is why we started VIVID.</p><p>The VIVID app helps individuals find the most effective ways to remove internal obstacles, by allowing&nbsp;<strong>extensive customization and self-testing</strong>, and by&nbsp;<strong>empowering wellbeing professionals to assist&nbsp;</strong>in this process.<br>With VIVID, individuals&nbsp;<strong>continuously self-improve</strong> by working on multiple plans -&nbsp; each plan&nbsp;<strong>addressing a mindset</strong> that underlies an internal obstacle.</p><p>One of VIVID's cofounders is Dr. Tal Ben-Shahar - a leading expert in positive psychology, who taught two of Harvard's most popular courses of all time. Ben-Shahar is the author of several worldwide best-selling books, and his&nbsp;<strong>upcoming book</strong>&nbsp;<strong>discusses the methodology of VIVID.</strong></p><h2>Theory of change&nbsp;</h2><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/z3J439wF8Xk8qSZku/yqw53wbrcfogajvzz3rc\"></figure><p>Our theory of change is divided into two main parts:</p><ol><li><strong><u>Short-term</u></strong><i>:&nbsp;</i>Assist individuals in overcoming internal obstacles. We specifically intend to direct VIVID to high-impact communities like EA or Schmidt Future's fellows. VIVID is meant to grow better at this point over time - the <a href=\"https://www.vivid-app.me/impact#:~:text=Modularity\">main idea behind its design</a> is that it learns over time which self-change methods are most effective for each category (e.g. self-esteem, guilt, friendship, etc.), and for which user profiles.</li><li><strong><u>Long-term</u>:</strong> The&nbsp;<i>a priori</i> chances of projects such as this becoming widespread are low (but we\u2019re optimistic!). However, the upsides are stellar. We could:<ol><li><strong>Figure out an intentional way to radically self-improve:&nbsp;</strong>This project provides an intentional way for highly motivated individuals to radically self-improve. In other words, this means that once we have a robust methodology to increase the potential of talent, even if it is likely that such a methodology requires a high amount of effort, individuals with high enough motivation could utilize it to become the best versions of themselves.</li><li><strong>Accelerate progress in the fields of wellbeing and personal improvement </strong>and align them with large-scale evidence. In other words, we may discover scalable methods to improve wellbeing and individual potential in dozens of common use cases, relevant both to high-income and low-income countries. There are very few interventions that aim to improve wellbeing at scale, which is generally important - but <a href=\"https://docs.google.com/document/d/1oiGJTGgJS7NLxsQqb8wBajrLXz6d-6eiVvBCyz27jB4/edit#\">depending on your assumptions regarding the current baseline of wellbeing</a>, it might be even more important.</li><li><strong>Broadly promote the value of self-reflection</strong>. We believe that this point is of high importance, particularly in terms of mitigating risks associated with value lock-in (this idea is explained in&nbsp;<a href=\"https://whatweowethefuture.com/\"><u>WWOTF, ch. 3</u></a>, and we will share a more thorough write-up about this soon).</li><li><strong>Figure out easier ways to remove internal obstacles:&nbsp;</strong>We believe the importance of internal obstacles increases as individuals have fewer physical and social restrictions.<br>Even in futures that are based on virtual realities, internal obstacles will still be significant to individual decisions and, therefore, collective decisions.<br>Moreover, the wider the range of individuals' choices, the more weight that internal obstacles will have on the decision results. That is because the existence of restrictions and obligations in modern society requires individuals to face their internal obstacles. Yet, in a world with basic universal income and an option to spend time in a virtual simulation disconnected from society, the motivation to deal with internal obstacles will be incredibly low. This will have significant implications on individual motivation to pursue values and participate in society\u2019s progression.<br>Another trend that will likely exacerbate this problem is more effective (and more immersive) non-aligned marketing and advertisement practices, drawing individuals further from self-directed goals.</li></ol></li></ol><h2>How it works</h2><p><a href=\"https://www.vivid-app.me/impact#:~:text=Our%20approach%20to%20this%20problem\">This page goes into detail about VIVID's methodology</a>. You can leave any feedback or question through the link at the bottom of this page.</p><p>In short, our strategy is inspired by other platforms (such as Notion, Airtable, and many others) which provide users with a high level of flexibility and the ability to share solutions with one another. We\u2019ve applied this strategy to the self-improvement field by incorporating scientific principles of personal change into its design (such as&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/8aBiqvT9uDo56Cckx/esh-a-guide-to-forming-habits-you-ll-keep#Summary_of_our_findings\"><u>prompts, self-monitoring, social and financial commitment/incentives, or implementation intention</u></a>). VIVID is built to learn over time which techniques and plans are most effective for each category (e.g. self-esteem, guilt, friendship, etc.), and for which user profiles.</p><h1>How you can help</h1><ul><li><strong>Help us tailor&nbsp;</strong><a href=\"http://www.vivid-app.me/\"><strong><u>VIVID</u></strong></a><strong> for you!&nbsp;</strong>Your feedback is extremely valuable due to our focus on the EA movement. We're not looking for feedback on \"what is objectively best for the app,\" but rather on what would help&nbsp;<i>you&nbsp;</i>with finding the most effective ways for you to self-improve.&nbsp;<br>(if you\u2019d like to help us even more with feedback, or view/vote on our upcoming features,<strong>&nbsp;</strong><a href=\"http://bit.ly/VIVID-Upcoming-Feat\"><strong><u>visit this page</u></strong></a>)</li><li><strong>If you\u2019re offering workshops on self-improvement / wellbeing for EAs, or coaching -&nbsp;</strong><a href=\"https://www.vivid-app.me/professionals\"><strong><u>let\u2019s have a chat</u></strong></a><strong>!&nbsp;</strong>VIVID can help assist with implementing insights from your workshop/session, and publicizing your services for more EAs. VIVID is free for your use and will most likely not disrupt your current workflow, but will only complement it.</li><li><strong>We\u2019re hiring!&nbsp;</strong>If you know someone who might be a good fit as one of our very first employees (<a href=\"https://forum.effectivealtruism.org/posts/z3J439wF8Xk8qSZku/announcing-vivid-a-new-ea-organization-aspiring-to-scale-1#Join_our_team___________\">details below</a>) or might be interested in this project as an intern,&nbsp;<strong>please share this post with them!</strong></li><li><a href=\"https://forms.gle/Jm7MNXRoG4JUbBcJA\"><strong><u>Sign up for updates</u></strong></a><strong> </strong>about this project.</li><li>We\u2019re still setting things up. This means that some of our writeups, or even our strategy, have room for improvement. We\u2019d appreciate feedback on our writeups through our website (each page has its own link), and any other feedback by&nbsp;<a href=\"mailto:feedback@VIVID-app.me\"><u>email</u></a> /&nbsp;<a href=\"http://bit.ly/VIVID-anony-feedback\"><u>anonymously</u></a>.</li></ul><h1>Plans</h1><p>Over the next two years, we plan to:</p><ul><li>Iteratively improve VIVID\u2019s individual-level impact while working mostly with EAs.</li><li>Investigate VIVID's potential for growth, as the long-term part of our theory of change is dependent on reaching broad audiences.&nbsp;<ul><li>This will most likely include a B2B model aimed at organizations' wellness budgets. As a result, we're likely to use a freemium model, in which the majority of the app is free to use, but some features require payment. To boost motivation and commitment, the app will provide a success-based discount to users who persist with their practice.</li><li>We intend to significantly subsidize subscriptions for people who are heavily involved with the EA movement or who work in jobs that we believe think are especially impactful.</li><li>More information about our financial model and its rationale can be found&nbsp;<a href=\"https://docs.google.com/document/d/1SbPvGfNul7F-WSc65w8FWC2tuBiePlQcwGW5YlqIZ8g/edit#\"><u>here</u></a>. It is worth stating explicitly that VIVID will never sell user data, and that in general, <a href=\"https://forum.effectivealtruism.org/posts/z3J439wF8Xk8qSZku/announcing-vivid-a-new-ea-organization-aspiring-to-scale-1?commentId=MkNNQEaTDjiYkgMTd\">we take the privacy of our users seriously</a>.</li></ul></li><li>Conditional on funding, establish an independent nonprofit research team, using VIVID\u2019s platform and anonymized data to conduct and publish free-to-access research on effective personal change.&nbsp;<br>As a part of this direction, VIVID and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/i2Q3DTsQq9THhFEgR/introducing-effective-self-help\"><u>Effective Self-help</u></a> intend to work together to disseminate high-quality advice on these topics.</li></ul><p><br>We\u2019re considering many technological developments down our roadmap; from using AI to make better recommendations, to vital-signs-based prompts, to integrations for self-testing with automatically collected metrics (like stress indicators or productivity time). As VIVID\u2019s community grows, we\u2019ll have more information about which of these paths would help users the most.</p><h1>Join our team \ud83e\uddb8\u200d\u2640\ufe0f\ud83e\uddb8\u200d\u2642\ufe0f</h1><p>We\u2019re hiring for two positions:</p><ul><li><a href=\"http://www.vivid-app.me/hiring-1st-product\"><strong><u>Product Manager &amp; Data Specialist</u></strong></a></li><li><a href=\"http://www.vivid-app.me/hiring-1st-ops\"><strong><u>Operations &amp; Communications Manager</u></strong></a></li></ul><p>We\u2019ve decided to build our core team in Tel Aviv, Israel - working from the new EA Israel office (shared with other EA organizations!). Therefore, these positions require eligibility for an Israeli work visa (which by default has very strict rules, but can be quite easy&nbsp;<a href=\"https://lawoffice.org.il/en/moving-to-israel#:~:text=you%20must%20meet%20at%20least%20one%20of%20the%20following%20criteria\"><u>in some cases</u></a>. Feel free to ask us anything on this topic through the application form).</p><p>If you\u2019re considering a relocation, you can rely on our social support - both with the awesome people that will work on VIVID, and with the amazing EA Israel group (which is&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/3iSLtT5tsAyCWsyKB/cea-groups-q4-21-and-q1-22-update#City___National_Groups_Support\"><u>considered</u></a> among the strongest EA groups, with hundreds involved in its community and&nbsp;<a href=\"https://docs.google.com/document/d/1UoAdSID0crufgE6S_vLWxu8huCj0L1snCnbYSZygPCk/edit#heading=h.own1v7xrhvaw\"><u>6 large-scale ($300K+) EA projects&nbsp;</u></a>running from Israel).</p><p>We\u2019re also looking for a community-building intern, and are open to various internship positions that require ~3 weekly hours -&nbsp;<a href=\"http://www.vivid-app.me/internships\"><strong><u>apply here.</u></strong></a></p><p>&nbsp;</p><p>&nbsp;</p><p><i>Many thanks to Omer Nevo, Sella Nevo, Jack Lewars, Sofia Vanhanen, Edo Arad, Inga Grossmann, Dion Tan and Dvir Caspi, who gave feedback on this post and its appendixes.</i></p>", "user": {"username": "GidonKadosh"}}, {"_id": "Eq8nwNPNhfXvt2TWj", "title": "My experience experimenting with a bunch of antidepressants I'd never heard of", "postedAt": "2022-10-19T17:35:23.392Z", "htmlBody": "<p><strong>This isn't professional medical advice, it's just my experience and amateur knowledge.&nbsp;</strong></p><h2>Summary</h2><ul><li>I got moderately (and occasionally, severely) depressed about four months into the pandemic.&nbsp;</li><li>I tried a bunch of things to treat it, including therapy, antidepressants, meditation, and a long list of other things.</li><li>I was prescribed the antidepressant sertraline (aka Zoloft) by my NHS GP, and it really helped! But it had a number of side effects that made me very unexcited to stay on it.</li><li>I experimented with a bunch of different types of antidepressants to see if I could find one that worked well&nbsp;<i>and</i> didn\u2019t give me bad side effects.&nbsp;</li><li>I knew this would probably be a long and grueling experiment... and it was. It took over a year, and was&nbsp;<i>really</i> hard, both emotionally and physically.&nbsp;</li><li>But I ended up finding a great one for me, and it now feels well worth the costs.&nbsp;</li><li>I think this kind of experimentation might be a good approach for&nbsp;<i>some&nbsp;</i>people who also experience depression and anxiety.</li><li>Caveats:<ul><li>This was expensive. It\u2019s probably much harder to do this without savings or great insurance, or both.&nbsp;</li><li>I have a particularly supportive work environment and partner. It\u2019s probably much harder to do this without those things.</li><li>Things besides antidepressants were also really important to feeling better (and also allowed me to stick with the experiment). Therapy and cognitive behavioral therapy (CBT) were extremely effective for me in treating&nbsp;<a href=\"https://80000hours.org/2022/04/imposter-syndrome/\"><u>my imposter syndrome</u></a> and perfectionism, and in making my depression manageable. Searching for an antidepressant that worked for me probably wouldn\u2019t have gone nearly as well if I hadn\u2019t done it alongside these other practices.</li><li>Antidepressants have not solved all of my problems \u2014 and it\u2019s important to set reasonable expectations for how much they can help.&nbsp;</li></ul></li></ul><h2>Context</h2><p>I got moderately (and occasionally, severely) depressed about four months into the pandemic.&nbsp;</p><p>This was triggered by:&nbsp;</p><ul><li>The pandemic \u2014 including things like: feeling very isolated, not having much personal space, shitty British weather, having to work from home, not being able to see my family for years.&nbsp;</li><li>Genetics \u2014 my mom has suffered from moderate-severe depression since she was in her early thirties.</li><li>Imposter syndrome at work \u2014 I\u2019d gotten a dream job, but it felt like, sooner or later, my employer would realize I was a total phony. I was afraid to do really any task that might reveal I was actually a big idiot, which made me anxious for a lot of my day-to-day activities.&nbsp;</li></ul><p>My main symptoms were:&nbsp;</p><ul><li>Having continuous low mood or sadness</li><li>Feeling hopeless</li><li>Having very low self-esteem, feeling like I was letting everyone down all the time</li><li>Feeling tearful</li><li>Feeling guilt-ridden</li><li>Feeling irritable and intolerant of others</li><li>Having little motivation or interest in things \u2014 finding myself and everyone else really boring</li><li>Finding it difficult to make decisions</li><li>Not getting much enjoyment out of life, including out of things that were previously some of my favorite activities (hiking, seeing close friends)&nbsp;</li><li>Feeling anxious and worried</li></ul><p>Non-pharmacological things I did to treat my depression:</p><ul><li>Weekly therapy with an excellent therapist, who taught me a bunch about CBT (helped a lot)</li><li>Weekly therapy with a couples therapist (sometimes my depression made me especially angst-y about my relationship) (helped a bit)</li><li>Taking time off work (sometimes helped, sometimes made things a bunch worse)</li><li>My depression was a factor in switching jobs (kind of helped)</li><li>Changing my role at work significantly to focus on tasks I found especially exciting and satisfying (helped a lot)</li><li>Spending time in sunnier countries (helped a fair bit)&nbsp;</li><li>Regular exercise (helped a fair bit when I actually had the motivation to do it)</li><li>Meditation (unclear if it helped)</li><li>Kept a gratitude and positive-self-talk diary (maybe helped a bit)</li><li>Bright lights at my desk and indoor tanning for seasonal affective disorder (unclear if it helped)</li></ul><p>These things helped a bit, but not enough to make a dent in my worst symptoms. I had a strong feeling that there was just some basic chemistry going wrong in my brain that was \u201cshifting all of my experience down\u201d \u2014 making things I used to enjoy unenjoyable, and things that used to feel&nbsp;<i>kind of hard</i> feel&nbsp;<i>intolerably bad</i>.&nbsp;</p><p>My GP and therapist agreed and encouraged me to try an antidepressant.</p><p>I was prescribed the antidepressant sertraline (aka Zoloft) by my NHS GP, and it really helped! But it had side effects:</p><ul><li>Insomnia</li><li>Anorexia (especially at the beginning)</li><li>Stomach problems (really bad ones)</li><li>And what the field of psychiatry calls \u201csexual dysfunction\u201d</li></ul><p>I won\u2019t say much about the details of these, but I will say: I didn\u2019t take these side effects very seriously for a long time, despite the fact that they were making my day-to-day life a lot shittier in some ways. I thought:&nbsp;<i>What right do I have to complain about e.g. sleeping badly, when the alternative is depression? If I went off these, I\u2019d be sad again, which would make it hard to work \u2014 I can\u2019t jeopardize my work just because of these</i>.&nbsp;</p><p>It was hard for me to come to terms with the fact these were real and serious issues, and that it wasn\u2019t \u201cirresponsible\u201d or \u201cselfish\u201d of me to try to treat my depression in a way that didn\u2019t reduce my life satisfaction in other areas of my life.&nbsp;</p><p>When I eventually did come to terms with that, I decided to try going off sertraline. And then got very depressed again.</p><h2>The idea: experimenting with a bunch of antidepressants</h2><p>I explained all of this to one of my colleagues, Howie Lempel, who lots of people will know is just&nbsp;<a href=\"https://80000hours.org/podcast/episodes/depression-anxiety-imposter-syndrome/\"><u>extremely thoughtful and wise about mental health</u></a>.</p><p>Howie suggested I see a psychiatrist (rather than an NHS GP), to learn about other classes of antidepressants, and try a bunch out to find out which actually worked best.</p><p>I was very, very opposed to this idea. Going on and off antidepressants was really hard for me. When I started taking them, I had such bad side effects I had to take time off. And when I went off them, I had&nbsp;<a href=\"https://www.webmd.com/brain/ss/slideshow-brain-fog\"><u>brain fog</u></a> and \u201c<a href=\"https://psychonautwiki.org/wiki/Brain_zaps\"><u>brain zaps</u></a>,\u201d both of which are really uncomfortable and frustrating. Plus, it can take up to four weeks to know if an antidepressant is working for you, so trying several takes many months.</p><p>Howie countered:</p><ol><li>The upside potential for a person with a predisposition for depression could be huge: spending three to twelve months of my life going on and off antidepressants, while horrible, would be so incredibly worth it if I ended up finding an antidepressant that made me happier and more resilient without ruining my sleep and destroying my digestion.</li><li>The sooner I found the right antidepressant for me, the more years of Feeling Happy I would get to have over the course of my life \u2014 hopefully another 60+ years. So a huge deal, and one that I think lots of people don\u2019t fully appreciate when deciding whether to try to address the problems in their lives now or put it off.&nbsp;</li><li>And reminded me I\u2019m currently lucky enough to have very supportive employers, who would support me with both my mental health expenses and in taking time off if necessary.&nbsp;</li></ol><p>And I was convinced.&nbsp;</p><p>So I got a psychiatrist and started learning about antidepressants.</p><h2>What I learned about antidepressants</h2><ul><li>There are so many! I\u2019d only really heard of the selective serotonin reuptake inhibitors (SSRIs) (and Wellbutrin aka bupropion, because of&nbsp;<a href=\"https://docs.google.com/document/d/1niiV8I4cgk_xZ1Blou15ImPmqXU4eb_li9eRVp5NgYo/edit\"><u>Rob Wiblin\u2019s blog post on the topic</u></a>) \u2014 but there are a bunch of other types: serotonin-noradrenaline reuptake inhibitors (SNRIs), noradrenaline and specific serotonergic antidepressants (NASSAs), tricyclics (TCAs), serotonin antagonists and reuptake inhibitors (SARIs), and monoamine oxidase inhibitors (MAOIs).</li><li>And different ones seem to work for different people, and they have pretty varied side effect profiles.</li><li>It\u2019s really hard to predict which will work for you! Very little is known about why some people respond better to some antidepressants while other people respond better to others. And while you can do&nbsp;<a href=\"https://cnsdose.com/\"><u>genetic testing</u></a> to try to get a bit of additional evidence about which antidepressants might work best, it\u2019s not super accurate yet \u2014 plus, it\u2019s expensive and doesn\u2019t tell you anything about which drugs are likely to give you side effects.&nbsp;</li><li>But research on antidepressants can help.</li></ul><p>I\u2019m not going to do a full literature review here, but I wanted to highlight the&nbsp;<a href=\"https://www.thelancet.com/action/showPdf?pii=S0140-6736%2817%2932802-7\"><u>meta-analysis</u></a> my psychiatrist and I used to decide which antidepressants to try and in what order. Here are the headline findings from the \u201chead-to-head studies\u201d \u2014 which compare treatments to each other, rather than just comparing treatments to a placebo:</p><p><i>Effectiveness</i>:&nbsp;</p><blockquote><p>In head-to-head studies,&nbsp;<strong>agomelatine, amitriptyline, escitalopram, mirtazapine, paroxetine, venlafaxine, and vortioxetine were more effective than other antidepressants</strong> (range of ORs [odds ratios] 1\u00b719\u20131\u00b796), whereas fluoxetine, fluvoxamine, reboxetine, and trazodone were the least efficacious drugs (0\u00b751\u20130\u00b784).</p></blockquote><p><i>Acceptability</i> (how \u201cacceptable\u201d is the drug \u2014 how bad are the side effects, measured by dropout rates):</p><blockquote><p>For acceptability,&nbsp;<strong>agomelatine, citalopram, escitalopram, fluoxetine, sertraline, and vortioxetine were more tolerable than other antidepressants</strong> (range of ORs 0\u00b743\u20130\u00b777), whereas amitriptyline, clomipramine, duloxetine, fluvoxamine, reboxetine, trazodone, and venlafaxine had the highest dropout rates (1\u00b730\u20132\u00b732). 46 (9%) of 522 trials were rated as high risk of bias, 380 (73%) trials as moderate, and 96 (18%) as low; and the certainty of evidence was moderate to very low.</p></blockquote><p>The evidence for drugs with smaller sample sizes is weaker than the evidence for drugs with larger sample sizes. The size of the circles in the diagram below represent the number of randomly assigned participants in all of the studies included in the meta-analysis. The width of the lines is proportional to the number of trials comparing every pair of treatments. So those with the biggest circles and the thickest lines have the strongest evidence base.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995039/mirroredImages/Eq8nwNPNhfXvt2TWj/xxw9n7to0czj0aamxsxp.png\"></p><p><br>&nbsp;</p><p>The graph below brings these two factors (effectiveness and acceptability) together<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqoiqjthj84f\"><sup><a href=\"#fnqoiqjthj84f\">[1]</a></sup></span>&nbsp;\u2014 top right is good, bottom left is bad:</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995039/mirroredImages/Eq8nwNPNhfXvt2TWj/vjafp1wgnp63w5sbxgia.png\"><figcaption>1=agomelatine 2=amitriptyline 3=bupropion 4=citalopram 5=clomipramine 6=desvenlafaxine 7=duloxetine 8=escitalopram 9=fluoxetine 10=fluvoxamine 11=levomilnacipran 12=milnacipran 13=mirtazapine 14=nefazodone 15=paroxetine 16=reboxetine 17=sertraline 18=trazodone 19=venlafaxine 20=vilazodone 21=vortioxetine 22=placebo</figcaption></figure><p>&nbsp;</p><h2>My experiment</h2><ol><li>Start with the SSRIs \u2014 they seem to work for lots of people, and very easy to get from a general doctor.</li><li>Then, with the help of a psychiatrist, try as many antidepressants as it takes to find one that both helps me,&nbsp;<i>and</i> doesn\u2019t give me loads of side effects.&nbsp;</li><li>Start with the antidepressants with the fewest side effects.</li><li>Start with the lowest possible dose to see if I can get an effect on my depressive symptoms while reducing the chance of side effects.<br>&nbsp;</li></ol><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995039/mirroredImages/Eq8nwNPNhfXvt2TWj/gnlsb0koizh5edhs27l0.png\"><figcaption><i>You can open this image in a new window to read the detail.</i></figcaption></figure><p><br>With this in mind (and using the handy chart above!), my psychiatrist and I developed an ordered list of which antidepressants to try:</p><ol><li>Sertraline (Zoloft)&nbsp;</li><li>Bupropion (Wellbutrin)&nbsp;</li><li>Fluoxetine (Prozac)&nbsp;</li><li>Citalopram (Celexa)&nbsp;</li><li>Agomelatine (Valdoxan)&nbsp;</li><li>Vortioxetine (Brintellix)</li><li>Reboxetine (Edronax)</li><li>Venlafaxine (Effexor)</li><li>After that\u2026 come back to the drawing board.</li></ol><p>And we used these&nbsp;<a href=\"https://www.slideshare.net/juanluisdelgadoestve/guidelines-forswitchingbetweenspecificantidepressantsa3posterpdf1\"><u>guidelines for switching between specific antidepressants</u></a> to figure out how to transition from one to another.&nbsp;</p><h2>The results</h2><p>I tried a total of six drugs (some at several doses) over the course of a year and a half.&nbsp;</p><p>There were lots of ups and downs associated with going on and off these drugs:<br><br>&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995039/mirroredImages/Eq8nwNPNhfXvt2TWj/jzj6cv1lyjbmf9baxcba.png\"><figcaption>\ud83d\udc45=dry mouth \ud83d\udc94=sexual dysfunction \ud83d\udcad=intense/strange dreams \ud83d\ude31=extreme anxiety \ud83d\udc97=heart palpitations&nbsp;<br>\ud83e\udd22=stomach issues \ud83e\udd71=insomnia \ud83d\ude34=sleeping hard \ud83e\udd2e=nausea and/or vomiting \ud83e\udd2a=hypomania&nbsp;<br>\ud83e\udd12=flu-like symptoms \ud83d\ude2d=extreme depression and anxiety \ud83d\ude36\u200d\ud83c\udf2b\ufe0f=brain fog \ud83e\udde0\u26a1=brain zaps</figcaption></figure><p>&nbsp;</p><p>But overall, I was able to find an antidepressant (vortioxetine) with much more manageable side effects, and overall,&nbsp;<strong>I feel much happier than I was in 2020 and 2021.</strong></p><p>I don\u2019t feel \u201cdone\u201d \u2014 I\u2019m not happy all the time, and there are still emotional bumps in the road of my recovery.&nbsp;</p><p>There\u2019s definitely still some room for improvement, but I\u2019m happier than I\u2019ve been in a long time, and I no longer score in the clinical ranges on depression. I tracked all this progress as part of the experiment, which you can see in the graph below \u2014 lower scores are better, as they indicate fewer symptoms of anxiety and depression.&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995039/mirroredImages/Eq8nwNPNhfXvt2TWj/xrk8vvbixdqf7o4f0bxu.png\"><figcaption>I measured my depression severity using the Patient Health Questionnaire (PHQ-9).&nbsp;<br>The scores indicate the following severity: 0-4 none, 5-9 mild, 10-14 moderate, 15-19 moderately severe, 20-27 severe.<br><br>I measured my anxiety severity using the Generalized Anxiety Disorder Assessment (GAD-7).&nbsp;<br>The scores indicate the following severity: 0-4 none, 5-9 mild, 10-14 moderate, 15-21 severe.<br><br><i>You can open this image in a new window to read the detail.&nbsp;</i></figcaption></figure><h2><br>The hardest parts of this, and what got me through</h2><h3>The hard parts</h3><ul><li>This sucked. It was extremely difficult to stick to this plan. Staying depressed for a long time, when it felt like I could just go back on some side-effect-y SSRI and feel somewhat normal, was terrible.</li><li>This affected my job (e.g. sometimes the side effects of going on a new antidepressant forced me to take time off work) and my relationship (e.g. I was sometimes much more emotionally turbulent than my partner was used to). &nbsp;</li><li>Going on and off some of these was BRUTAL: a week or two of nausea, stomach problems, insomnia, tearfulness, and depression getting WORSE rather than better. These were some of the biggest costs, and while I was experiencing them, I often felt like my approach wasn\u2019t worth the costs.&nbsp;&nbsp;</li><li>Antidepressants have not solved all of my problems. Again, I\u2019m not happy all the time\u2026 I still feel social anxiety, work stress, and sadness a fair bit, and I still catastrophize about things \u2014 but I've learned a lot of healthy ways to deal with these tendencies (more on this below).&nbsp;</li><li>This was&nbsp;<strong>expensive</strong>. I had to meet with a private psychiatrist a bunch of times (\u00a3\u00a3\u00a3), and I paid for the drugs out of pocket because it was a lot faster than going through my GP \u2014 I think I wouldn\u2019t have been able to do this if I\u2019d been in the US. I recognize this could very well make experiments like this prohibitively expensive for lots of people. If I were to try to do it more cheaply, I\u2019d ask to be prescribed generics (which can take longer / involve more faff), and go through a GP for the more traditional medications (SSRIs, bupropion if in the US).&nbsp;</li></ul><h3>What got me through</h3><ul><li>Having a therapist who knew my plan, supported it, and helped me get perspective when I lost sight of why I was subjecting myself to the plan.</li><li>An extremely supportive partner, plus supportive friends who knew about my experiment, and with whom I was very open about my symptoms and side effects.</li><li>I was lucky that my workplace was extremely supportive of me spending time, money, and energy to improve my mental health. I told my manager about my plan, and got tons of support and understanding from her throughout my experiment (e.g. she was encouraging and understanding even when going on and off different drugs made it especially hard to work normal hours).&nbsp;</li><li>Things besides antidepressants were really important to feeling better (and sticking with the experiment). Therapy and CBT were extremely effective for me in treating my imposter syndrome and perfectionism, and in making my depression manageable.&nbsp;</li><li>Writing a note to myself I could read when I felt especially down:<br><i>Remember: Until [e.g. October 25th], you might feel especially sad. But it\u2019s not going to be this way forever. You\u2019ll get through it and then you\u2019ll feel as happy as you did in late September. Remember that? That was fucking great. That\u2019s very achievable again! You\u2019ll have it back, just gotta be patient and kind to yourself for a bit longer.&nbsp;</i><br>&nbsp;</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqoiqjthj84f\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqoiqjthj84f\">^</a></strong></sup></span><div class=\"footnote-content\"><blockquote><p>\"Data are reported as ORs in comparison with reboxetine, which is the reference drug. Error bars are 95% CrIs.&nbsp;</p></blockquote><blockquote><p>Desvenlafaxine, levomilnacipran, and vilazodone were not included in the head-to-head analysis because these three antidepressants had only placebo-controlled trials.\"</p></blockquote></div></li></ol>", "user": {"username": "Luisa_Rodriguez"}}, {"_id": "oGf4bfB3WpCowaiz8", "title": " Evaluating neglectedness. Where is the 'world portfolio'?", "postedAt": "2022-10-19T14:39:06.078Z", "htmlBody": "<p>Hi all, I am in the midst of my <a href=\"https://80000hours.org/career-planning/process/\">career planning process as per the format of 80,000 Hours</a>. At this moment, I am attempting to determine the 'neglectedness' of the problem areas that I have identified as pressing. However, I am not entirely sure what the best way is to go about doing so. <a href=\"https://80000hours.org/articles/problem-framework/#how-to-assess-how-neglected-a-problem-is\">80,000 Hours presents a helpful framework (INT)</a> for integrating neglectedness into the prioritization process once you have the actual data on current resource allocation, though I was wondering if there are any good resources for collecting this data in the first place?<br><br><strong>Phrased differently, are there any good resources that help us somewhat reliably construct this 'world portfolio'?</strong> Or has somebody been working on constructing such a 'world portfolio' already (considering the whole of resource allocation, from charitable giving and philanthropy, to government plans and subsidies, to labor allocation, and so forth)?</p><p>'Neglectedness' is an incredibly important aspect in our prioritization and decision making process regarding resource allocation, and it seems like we would gain a lot as a community from having a (somewhat) reliable resource for this. I would love to hear your thoughts on where one might best start gathering this data, as well as on this idea of a tangible 'world portfolio'.</p>", "user": {"username": "Val"}}, {"_id": "dPEKaRYNCE7fxAH8W", "title": "Open Letter to EA", "postedAt": "2022-10-19T14:14:20.982Z", "htmlBody": "<p>Robert Stern, who gave a talk on 'Effective Climate Activism' at EAGxBerlin, wrote the following article.&nbsp;<br><br>It was a talk that I sensed created quite some frustration amongst the audience members, but for those who stuck around it certainly generated a lot of discussion. I think the tension came from the content not adhering to the usual data-driven and evidence-based takes that EA is so accustomed to. Yet, that was exactly the point for me - successful storytelling does not tend to start with the facts.&nbsp;<br><br>In the end, trying to talk about climate activism and the importance of storytelling at the same time may have proved to be too tall an order, but I still think it's important to hear Robert's account of what it was like to experience the effective altruism community at a main event like a conference without any pre-existing affiliation or association.</p><hr><h2>What we learned from our invitation to EAGx Berlin, and what the Dickens we think Effective Altruism could learn from us</h2><h3>Executive Summary:</h3><p>This article ends with a pithy Executive Summary. You\u2019re welcome to scroll straight down to the bottom. But \u2013 and this is rather our point \u2013 why not enjoy the journey, not just focus on the destination?</p><p>Speaking of long journeys\u2026</p><h2>Fish out of water</h2><p>There\u2019s nothing like two days driving home on Northern Europe\u2019s arrow-straight motorways, autoroutes, autowegs, and autobahns and motorways to give you time to ponder, reflect, and analyse.</p><p>What follows are See Through News\u2019s ponderings, reflections and analysis following our three days at the Effective Altruism Global x (EAGx)conference, held in Berlin Sept 16-18 2022.</p><p>It ends with a listicle of Nine Things we think Effective Altruism can improve on, but we hope you won\u2019t be too impatient to get there.</p><p>We\u2019ve tried to make the journey as entertaining, and instructive, as we can. In so doing, we\u2019re trying to demonstrate what we think Effective Altruism is missing.</p><p>One spoiler \u2013 we had fun and met loads of interesting new people at EAGx Berlin. This defines a successful conference. So we\u2019d like to start this Open Letter with a big thank you to EA for inviting us, and covering our costs.</p><p>We appreciate that Effective Altruism took a punt in inviting See Through News. Listicle time:</p><h3>Top 3 Ways See Through News is Atypical</h3><ol><li>STN has no Effective Altruism affiliation, association, funding or track record</li><li>2/3 of STN\u2019s team had children older than most delegates</li><li>1/3 of STN\u2019s team was a dog</li></ol><p>More significantly for the purposes of of this open letter, See Through News was:</p><ul><li>the only speaker whose primary focus was storytelling to the broad public</li></ul><p>To be clear, no one asked us to write this Open Letter. We\u2019re writing it because, well, communication is what we do, we\u2019d like to bring both See Through News and Effective Altruism to broader attention, but mainly because our experience of EA suggests some people would be happy to have these things said.</p><p>Sometimes it\u2019s easier to hear things, and circulate them, from people who aren\u2019t paying you, or being paid.</p><p>We\u2019ll come on to that, but first\u2026</p><h2>Give us your best shot. Please.</h2><p>When booking us, the Berlin conference organisers half promised, half warned, us Effective Altruism delegates pull no punches. They gave a warning litany that sounded like an advance apology given to all EA outsiders.</p><p>This lot loves evidence, we were told.</p><p>That\u2019s why they\u2019re part of such a data-driven organisation, they said.</p><p>Expect an EA audience to examine every sub-clause and footnote of the See Through News concept, methodology and practice, we were told.</p><p>Several Effective Altruism insiders explained this was what Effective Altruism does \u2013 subject everything to intense and probing scrutiny.</p><p>Fine by us, we said. Bring it on.</p><p>See Through News also digs constant self-criticism, dynamic reflection, trial-and-error, evidence-based analysis, and ruthless interrogation. After decades in journalism, we think we\u2019re used to it.</p><p>The other advice we received was to read up on \u2018EA literature\u2019. Get to know our specific terminology, learn EA-speak, get to grips with the EA code.</p><p>This is basic due diligence for anyone, not just journalists, preparing for a public speaking event. We were intrigued why Effective Altruism was so insistent on the matter.</p><p>More than that, this evoked a journalistic spider sense. Where had we heard these kind of words before\u2026? What kind of people say these kinds of things?</p><p>The answer forms one of the criticisms made later on, but let\u2019s not get ahead of ourselves.</p><p>We accepted the invitation to expose ourselves to EA\u2019s robust analysis as an excellent opportunity to road-test See Through News\u2019s approach to effective climate action.</p><p>For it was this very topic, Effective Climate Action, on which Effective Altruism had invited See Through News to speak.</p><h2>Whoah there \u2013 Effective What and See Through Who?</h2><p>Before delivering our verdict on Effective Altruism and its conference, here\u2019s some context for those who are only familiar with one, the other, or neither.</p><p>If you\u2019re completely <i>au fait</i> with both EA and STN, by all means skip this bit, but we suspect there won\u2019t be many of you\u2026</p><p>See Through News and Effective Altruism share a pragmatic, data-driven, utilitarian approach to activism, but as neither of us had heard of the other a year ago, it\u2019s hardly reasonable to expect you to have.</p><p>Effective Altruism was only founded at Oxford University a decade or so ago. In recent years, the introduction of some serious Silicon Valley spondulix has turbocharged EA into a global movement.</p><p>See Through News was only founded in March 2021, based on radical transparency, open source, and zero budget, sustained by street smarts and an international volunteer network.</p><p>See Through News ducks and dives, mixing high concept with low cunning. We blend <a href=\"https://seethroughnews.org/ai-silicon-valley-tech-effective-climate-action\">hi-tech social media</a> and <a href=\"https://seethroughnews.org/faqs-stn-methodology-effective-climate-action\">Old-School storytelling</a> to ju-jitsu the \u2018free\u2019 infrastructure created by our Silicon Valley Overlords to reduce, rather than increase, carbon.</p><p>It\u2019s early days, but we\u2019re accumulating numbers that support our hunch. More volunteers are joining up, and pitching in, every day, mainly because they\u2019re convinced:</p><ol><li>our approach is worth a shot</li><li>no one else is trying it</li></ol><p>We know 2) because after 6 months of development, we soft launched See Through News at COP26 to find out. We spent a fortnight in Glasgow releasing three projects into the wild, and seeing what happened.</p><p>We\u2019ve since added a dozen more projects to our roster. All follow the same model of being designed to measurably reduce carbon, but not obviously so at first glance.</p><p>We\u2019ve <a href=\"https://seethroughnews.org/cop26-lessons-effective-climate-action-stn\">written elsewhere about our COP26 adventures</a>, and there are detailed posts about the three projects we deployed there, what was to become <a href=\"https://seethroughnews.org/see-through-games-for-effective-climate-action\">The Think Game</a>, our <a href=\"https://seethroughnews.org/superhero-supervillain-drawing-competition\">Superhero &amp; Supervillain Drawing Competition</a>, and our first Event, <a href=\"https://seethroughnews.org/concert-in-the-key-of-c-cop26-gig\">Concert in the Key of C</a>.</p><p>Suffice to say, they convinced us that no one else was doing what we were doing, and that what we were doing worked.</p><h2>Transparent Trojan Horses</h2><p>Our COP26 pilot projects provided a model for all our subsequent projects. They all:</p><ol><li><i><strong>Attracted </strong>people:</i> at COP26, climate activists, working-class Glaswegians and primary school kids alike all had fun with our projects.</li><li>Created <i><strong>Interest</strong></i>: the world\u2019s most active climate activists, having sensitive antennae, asked us more questions about what we were doing and why than average, but everyone asked something.</li><li>Evoked a <i><strong>Desire</strong></i>: in some cases, to volunteer, in others, to do exactly what we\u2019d hope and progress to step 4..</li><li>Lead to effective climate <i><strong>Action</strong></i><strong>:</strong> for some this meant volunteering their skills and time to test the limits of our storytelling methodology, for others, following the path toward measurable carbon reduction.</li></ol><p>Even those who stopped at <i><strong>Interest</strong></i> confirmed that STN\u2019s approach was, if nothing else, original. Those who, having had some fun, were curious enough to ask us Why &amp; How, reckoned our approach was feasible and worth trying.</p><p>You see, these Glasgow players of our games \u2018got\u2019 the concept, by recognising they\u2019d been \u2018got\u2019 themselves.</p><p>They understood that See Through News projects were real-world, real-time demonstrations of the very manipulative methodology we were proposing. They clocked that they\u2019d been attracted to our honeypot, and were now asking us the very questions we intended them to ask, but without handing them yet a flyer.</p><p>Everyone\u2019s pockets, backpacks and briefcases at COP26 were stuffed with them. Many more littered the streets. Most never got read.</p><p>\u2018OK, we see\u2019, they said, reflecting on what they\u2019d just experienced. \u2018You\u2019ve just got me to ask you to tell us exactly what you would have printed on those flyers, haven\u2019t you?\u2019.</p><p>Yup, we grinned. Welcome to See Through News\u2019s world of transparent Trojan Horses, and the dark arts of AIDA.</p><h2>AIDA</h2><p>If you\u2019ve not heard the phrase, AIDA is the storyteller\u2019s trick on which See Through News\u2019s approach is based.</p><p>We don\u2019t usually go on about much about AIDA, as it\u2019s so integral to storytelling, but we found it was new to just about everybody we encountered at EAGx Berlin. If something evokes such nodding, teeth-sucking and note-taking, it may be worth explaining.</p><p>We didn\u2019t invent AIDA. Indeed, it\u2019s precisely AIDA\u2019s tried-and-tested nature that makes us so confident that, if executed with sufficient guile and cunning, it will work. AIDA is an old huckster\u2019s hustle, a conman\u2019s schtick, an ad-man\u201ds trick, familiar to cult recruiters and crowd-rousing demagogues alike.</p><p>Advertisers, who favour snappy acronyms, are the ones who came up with AIDA:</p><ul><li><strong>A</strong>ttention</li><li><strong>I</strong>nterest</li><li><strong>D</strong>esire</li><li><strong>A</strong>ction</li></ul><p>Or, working backwards from the Goal of (say, Effective Climate) Action:</p><ul><li>If I want you to take Action, I first need to create your Desire</li><li>To create Desire, I need to stimulate your Interest.</li><li>Your Interest requires me to first attract your Attention.</li></ul><p>If AIDA sells i-Phones, cigarettes and soap powder, why not climate action? They\u2019re just storytelling tricks, after all.</p><p>AIDA is a neutral tool, like a hammer, or AI, neither intrinsically good nor evil. As we like to say at STN, you can use a hammer to tap in the final, hand-hewn sustainable wooden peg in a no-nail eco-lodge, or to smash in a stranger\u2019s skull. It\u2019s just a hammer.</p><p>Advertising platforms such as Google, Facebook and Twitter use AIDA to increase carbon, by selling us more stuff we don\u2019t need. For all their jargon, lofty slogans, and cutting-edge tech, our Silicon Valley Overlords are glorified advertisers. Follow the money, and examine their business models.</p><p>See Through News\u2019s Glasgow experiments in applying AIDA to reduce carbon are how we ended up speaking at EAGx Berlin.</p><p>It\u2019s time to mention See Through News\u2019s other superpower, from which EA can take whatever lessons it likes.</p><p>No money.</p><h2>Effective Climate Activism without even a shoestring</h2><p>EAGx was appropriately unflashy and modest, but the fact it hired a major Berlin venue to host 900 people over a weekend speaks much of the resources Effective Altruism now has at its disposal, and how it chooses to spend them.</p><p>What follows is a complementary counterpoint, not a hidden critique.</p><p>Staging conferences like EAGx is an excellent use of funds, if you have funds.</p><p>But there\u2019s lots you can do without it, too.</p><p>In November 2021, See Through News tested its zero-budget, volunteer-driven approach with a soft launch at COP26 in Glasgow.</p><p>With no more than a couple of thousand quid from <a href=\"https://seethroughnews.org/founder-see-through-news-robert-stern-activist\">its Founder </a>to cover everything, STN beta-tested three pilot projects:</p><ul><li><a href=\"https://seethroughnews.org/the-think-game-for-effective-climate-action\">The Think Game</a></li><li><a href=\"https://seethroughnews.org/superhero-supervillain-drawing-competition\">The Superhero and Supervillain Drawing Competition</a></li><li><a href=\"https://seethroughnews.org/concert-in-the-key-of-c-cop26-gig\">Concert in the Key of C</a></li></ul><p>Cost: \u00a30.00 including VAT, <a href=\"https://youtu.be/me60d_37w9E\">plus a bit of mutual back-scratching.</a> We\u2019ve <a href=\"https://seethroughnews.org/cop26-lessons-effective-climate-action-stn\">described details elsewhere</a>, but we mention this not (just) to swank about See Through News\u2019s trickster street smarts, but to illustrate a critical feature that distinguishes STN from most other climate activist groups: zero budget.</p><p>Money is power, but if you have a plan and a goal people believe in, no-money can also be pretty powerful. This is our cautionary tale for Effective Altruism, as it\u2019s showered with Silicon Valley money.</p><p>Funding makes some things easy, but always comes at a price. Absence of money doesn\u2019t necessarily mean things are impossible, only that they might require different solutions.</p><p>Money lubricates, but is also sludge. It can accelerate, but it also acts as a brake. Acquiring money takes time, and diverts energy. It always comes with a price tag. Attached strings require compromising your original goals, and spending yet more time \u2013 and money \u2013 documenting how it was spent.</p><p>AIDA works in any context, and can have unexpected outcomes.</p><p>For example, when a tall German bloke sportingly agreed to do the first foreign-language version of The Think Game, we had no notion that 10 months later, it would lead to us being invited to EAGx Berlin.</p><p>He hadn\u2019t even done it in German.</p><h2>How Effective Altruism ended up inviting us to EAGx Berlin</h2><p>EAGx was a conventional convention; sessions, power points, break-out groups, one-to-one areas. Beanbags.</p><p>The Think Game, however, looks different.</p><p>It\u2019s an impressive bit of street theatre: two chairs, one in front of a pull-up STN logo, separated from the other chair by a professional-looking camera and microphones. Standing by, authoritative-looking people in STN hi-vis jackets, holding clipboards.</p><p>We took this to Berlin, but this was the honeypot we used in Glasgow to attract the Attention of the climate activists. They could hardly miss us, as they had to pass us to enter and exit the building. Even those in a hurry would pause to eavesdrop, or ask what we were doing. AIDA.</p><p>Many ended up playing The Think Game, happy to give us permission to upload their videos to our YouTube channel to share it with friends back home, and show to others they met. AIDA.</p><p>Among the venue volunteers, handing out flyers, was a Chinese student taking time off from her Masters in Sustainable Finance. Before long, she started chatting with one of the authoritative-looking people by the camera, with branded hi-vis jacket and clipboard.</p><p>It was See Through News founder Robert Stern. He speaks Chinese, and by chance, many of the experiences that helped him come up with The Think Game a couple of weeks before were China-related; consultant for a company selling entertainment formats to Chinese broadcasters, <a href=\"https://youtu.be/HX6oXE1ku38\">co-presenting a game show in Mandarin Chinese</a> seen by 100 million Chinese viewers, helping a British AI start-up stage a show that won a competition for hi-tech start-ups shown on Chinese TV.</p><p>But Robert didn\u2019t mention this. The conversation switched from English to Chinese, and before long the volunteer asked Robert about See Through News.</p><p>AIDA time. Instead of telling you, Robert said, why don\u2019t I show you? She played The Think Game, and STN now had another player delighted to share her video with friends and family, and asking the Why and How questions on which we\u2019d saved so much money by not printing flyers.</p><p>So far, so routine, but a couple of hours later, out of the blue, the thing that got us to Berlin happened.</p><p>This Chinese volunteer approached Robert accompanied by a tall German called Mo. She\u2019d just met Mo, and discovering another Chinese-speaking foreigner, wanted to introduce them. She now basked in a rare experience for a Chinese person, eavesdropping on two foreigners conversing in Chinese.</p><p>Before long, Mo asked about See Through News. AIDA time again. Instead of telling you, why don\u2019t I show you\u2026but this time, Robert spent a few minutes giving the Chinese volunteer a brief training session in game show host technique, and drawing up a list of questions Mo and a Chinese audience might know.</p><p>That\u2019s how the <a href=\"https://youtu.be/Kikd5xcMBjU\">first-ever foreign-language version of The Think Game</a> happened.</p><p><img src=\"https://seethroughnews.org/wp-content/uploads/2022/09/COP26-RS-Mo-and-Brittany-Think-Game.jpeg\" srcset=\"https://seethroughnews.org/wp-content/uploads/2022/09/COP26-RS-Mo-and-Brittany-Think-Game.jpeg 1024w, https://seethroughnews.org/wp-content/uploads/2022/09/COP26-RS-Mo-and-Brittany-Think-Game-300x225.jpeg 300w, https://seethroughnews.org/wp-content/uploads/2022/09/COP26-RS-Mo-and-Brittany-Think-Game-768x576.jpeg 768w\"></p><p><i>The First Ever Chinese Language Version of The Think Game</i></p><p>And a few months later, Mo was selected by Effective Altruism to produce the Berlin conference. Remembering The Think Game, he set up a call between See Through News and his colleague in charge of inviting speakers.</p><p>She could only spare 15 minutes, so Mo advised STN to get its elevator pitch in shape.</p><p>The call ended up lasting 90 minutes. AIDA.</p><p>In return for an hour-long talk, Effective Altruism would cover the non-inconsiderable costs of transporting and accommodating our team.</p><p>So that\u2019s how we ended driving two days each way to Berlin, with a car packed with camera gear, gazebo, and a dog.</p><h2>Our Berlin Experience</h2><p>EAGx Berlin proved to be highly stimulating, and, from our perspective, at least, well worth the trip.</p><p>Over three days, we had dozens of 1-on-1 meetings, spontaneous and scheduled, formal and informal. Then there was our talk, right at the end of the conference. We filmed it \u2013 you can see Robert\u2019s talk on the See Through News YouTube channel.</p><p>No one present knew how to connect the speakers, so we had to change our plans on the hoof, but if you\u2019ve done a <a href=\"https://youtu.be/CYIbeQ-kgfo\">live interview on Japanese TV, in Japanese, on the ethnic complexities of Israeli demographics</a>, coping with no audio is relatively straightforward.</p><p>The Live interactive version of The Think Game had to go, and we had to do a live narration of <a href=\"https://youtu.be/QQHa75RBbMY\">The Three-Headed Beasts video</a>. Via a laptop after the Q&amp;A, we eventually told the story of The Tallest Woman in Mongolia, and explained how, like all our 14 projects, it measurably reduces carbon.</p><p>To find out for yourselves, jump to the end of the talk, when we get round to editing it for our YouTube channel, or go to around 8min into <a href=\"https://seethroughnews.org/the-story-of-ganbaatar-mongolia-episode-10\">this final episode of Series 1 </a>of our podcast, <i>The Truth Lies in Bedtime Stories: The Story of Ganbaatar, the only deep sea navigator in Mongolia. </i>It\u2019s all entirely true, almost.</p><p>As forewarned, the Q&amp;A from the Effective Altruism crowd was intense and sustained. Just as well, as we\u2019d left most of the session for this purpose.</p><p>We hope what follows is taken as intended, as our reciprocal thank-you present, delivered in the same spirit of helpful critique.</p><p>Here, then, is our robust and frank analysis of what we made of Effective Altruism, based on our EAGx Berlin experience.</p><h2>First, Don\u2019t Sound Like a Cult</h2><p>At first, we pussy-footed around the C-word.</p><p>But when so many Effective Altruism folk spontaneously used it when explaining EA to us, we began to figure that if we advise you<strong> not to sound like a cu</strong>l<strong>t</strong> , we won\u2019t be taken the wrong way.</p><p>First, to be clear, we don\u2019t think Effective Altruism is a cult. Cults require central dominant personalities who obsessively control every last aspect of its operations. We\u2019ve never met William Macaskill, but nothing we\u2019ve heard or read about him suggests he\u2019s a cult leader, or has any aspiration to becoming one.</p><p>But we have met other cult leaders. Journalists often come across them, and STN Founder Robert Stern covered both the 1995 Aum Shinrikyo sarin gas attacks on the Tokyo subway, and the Falun Gong protests and crackdown in Beijing four years later, amongst others.</p><p>A common feature of cults is the use of jargon, in-group code with special meaning to the initiated. From Scientology\u2019s <a href=\"https://www.scientology.org.uk/faq/operating-thetan/what-is-ot.html\">Operating Thetan levels</a>, to the <a href=\"https://jonestown.sdsu.edu/?page_id=40229\">othering, sub-Pentecostal manipulations</a> of Jim Jones\u2019s People\u2019s Temple, cults have used their special codes to keep insiders in, and outsiders out.</p><p>In this respect \u2013 and again we emphasise only in this respect \u2013 Effective Altruism has the hallmarks of a cult.</p><p>\u2018Read our texts\u2019, \u2018understand our terminology\u2019, \u2018learn EA language\u2019, we were told.</p><p>What we were thinking is \u2018Just listen to yourselves\u2019. If it\u2019s really so important to know your \u2018tail-risk\u2019 from your \u2018tractability\u2019, how are you ever going to reach a broader audience?</p><p>We\u2019ll leave the question of whether EA does actually want to reach a broader audience to the end of this letter, as that\u2019s really the biggest question of all.</p><p>It\u2019s a question that Effective Altruism founder Wiiliam Macaskill appears to have on his mind. Why else would he have just published his second book <a href=\"https://www.basicbooks.com/titles/william-macaskill/what-we-owe-the-future/9781541618633/\"><i>What We Owe The Future</i></a>?</p><p>The credits alone suggest Macaskill himself had little interest in being a cult leader. Cult leaders typically claim all the credit, and divert all the blame. Macaskill makes a point of acknowledging error, crediting others and focusing on objective truth over personal revelation.</p><p><i>What We Owe The Future</i> is also a deeply impressive philosophical work, kicked off by a blockbuster of a thought experiment, with a wide range of reference, clearly expressed in readable, accessible, jargon-free language.</p><p>The fact that he just published it suggests he may be all too aware of the issues we\u2019re raising here.</p><h2>Watch your language</h2><p>Use of language reveals deep truths that can be hidden to the people speaking it.</p><p>Our sense that communication wasn\u2019t EA\u2019s strong card was confirmed by a couple of events leading up to the conference.</p><p>In our pre-EAGx newsletter (sign up on the home page of this website, if you\u2019d like to keep abreast of STN developments), we characterised the movement as follows:</p><blockquote><p><i>If&nbsp;</i><a href=\"https://seethroughnews.us1.list-manage.com/track/click?u=887f76c730d2a2b2bd6140879&amp;id=97a604dcf1&amp;e=3f61436068\"><i>Effective Altruism</i></a><i>&nbsp;is yet to cross your radar, they\u2019re worth&nbsp;</i><a href=\"https://seethroughnews.us1.list-manage.com/track/click?u=887f76c730d2a2b2bd6140879&amp;id=4c706ed6e6&amp;e=3f61436068\"><i>checking out.</i></a><i>&nbsp;EA members are typically highly educated, highly motivated, highly skilled graduates with strong moral and social sense of duty.</i></p><p><i>In an evidence-based, pragmatic, rational manner that aligns with our own, EA folk ask themselves a simple question \u2013 how can I do the most good for current and future generations? Having assessed the data, they do a cost-benefit analysis, and act accordingly.</i></p><p>From The See Through News Newsletter #71</p></blockquote><p>No sooner had the newsletter gone out, than an EAGx conference organisers who\u2019d subscribed to our Newsletter, took time from their hectic schedule to email us:</p><p><i>\u2018Very well-written summary of what EA is \u2013 I will use your description in the future.\u2019</i></p><p>A nice compliment, but \u2013 really? With all Effective Altruism\u2019s resources, brains and powers of analysis, you\u2019ve not managed to come up with a better way yourselves to describe yourselves to out-groupers?</p><p>To be fair, it sometimes takes an outside, non-expert eye, to state the obvious in everyday terms. Woods for the trees and all that.</p><p>Ordinary people being better at expressing complex ideas is the intelligent, ethical justification for using <i>vox pop</i> (=person-on-the-street) interviews. <a href=\"https://seethroughnews.org/vox-pox-project-stn-media-literacy-manipulation\">As we highlight in our Vox Pox project</a> \u2013 one of our 6 Media-targeted projects, this staple of journalism, TV news and documentaries is widely abused and misunderstood.</p><p>But, really. If you want to control your message outside the Effective Altruism tent, you shouldn\u2019t need the likes of See Through News to come up with your strap line.</p><p><strong>Learn how to speak everyone else\u2019s language, instead of getting them to learn yours.</strong></p><p>See Through News isn\u2019t the only one to spot this, of course. Once we got to Berlin, many of the more senior EA people we met raised the issue spontaneously, when they discovered STN\u2019s expertise was in storyotelling.</p><p>But we were already aware of this. As invited guests, curious people, and investigative journalists, we did our due diligence before our phone call with the EAGx meeting programme manager.</p><p>As <a href=\"https://www.eaglobal.org/\">this growing global movement of highly-educated young people with a social conscience</a> has grown over recent years, there\u2019s been plenty of online criticism growing with it.</p><h2>For smart people, you can be quite dumb</h2><p>Did you find this a gratuitously provocative, tabloid-style sub-heading?</p><p>But hey, you\u2019re reading this, so it\u2019s done its job. See what we did there? It works much better than an EA jargon term like \u2018tractability\u2019. If it works, why not use it to reach a few more people? You can be accessible without compromising your message. You even find that simplifying your message clarifies it in your own mine. We know it has for us.</p><p>Wikipedia\u2019s brief \u2018Criticism\u2019 section on Effective Altruism doesn\u2019t mention the \u2018C-word\u2019, but it touches on EA\u2019s introverted nature:</p><blockquote><p><i>Critics of effective altruism argue that the movement does not address the institutional change necessary to properly improve the lives of the global poor. Rather, they say, effective altruism simply addresses symptoms (e.g.&nbsp;</i><a href=\"https://en.wikipedia.org/wiki/Parasitic_worm\"><i>parasitic worm</i></a><i>&nbsp;infections) rather than the root problem of&nbsp;</i><a href=\"https://en.wikipedia.org/wiki/Poverty\"><i>poverty</i></a><i>.</i><a href=\"https://en.wikipedia.org/wiki/Effective_altruism#cite_note-:0-6\"><i><sup>[6]</sup></i></a><i>&nbsp;Some also argue that no movement can succeed in changing the world by focusing exclusively on individual human behavior.</i><a href=\"https://en.wikipedia.org/wiki/Effective_altruism#cite_note-:1-7\"><i><sup>[7]</sup></i></a><i>&nbsp;Advocates of effective altruism respond that the movement is not committed to any particular method of improving the world. As a result, they say, effective altruism will favor interventions that target the root causes of a problem when doing so is judged to be more cost-effective than addressing its symptoms.</i><a href=\"https://en.wikipedia.org/wiki/Effective_altruism#cite_note-:2-8\"><i><sup>[8]</sup></i></a></p><p>\u2018<a href=\"https://en.wikipedia.org/wiki/Effective_altruism#Criticism\">Criticism\u2019 Section of Wikipedia\u2019s Effective Altruism entry</a></p></blockquote><p>This \u2018head-in-the-clouds\u2019, ivory-tower critique reflects our own direct experience at Berlin, but again, was also a criticism expressed to us by many of the delegates.</p><p>To be fair, and statistically accurate, this would only have been a dozen or so out of 900. On the other hand, more people we met said it than didn\u2019t say it, and none were prompted to say it\u2026</p><p>Those in Effective Altruism who aspire to the movement being more than a global Nerd Club might be interested to know that the delegates who were attracted to See Through News tent, and who ended up spending the most time talking to us, shared the following characteristics:</p><ul><li>they were older, more experienced, with broader life experiences</li><li>they were not narrowly focused on the existential threat posed by robots taking over the world</li><li>they were predominantly either from the developing nations/global south, or from autocratic countries where the mainstream media is most obviously captured/owned by the ruling politicians</li></ul><p>Here are some of the comments we received:</p><ul><li><i>\u2018I keep coming back to the See Through News tent because I feel safe and understood.\u2019</i></li><li><i>\u2018You\u2019re the only people here who seem to have practical, feasible ways to address my reality, or the real issues facing my home nation.\u2019</i></li><li><i>\u2018Can I pet your dog?\u2019</i></li></ul><p>These reactions could in part be put down to the fact that 66% of the See Through News Berlin team had children of around the same age, or older, than most of the delegates. And the other 33% were 100% cockerpoo.</p><p>But why was See Through News such an outlier at an Effective Altruism conference? So far as we could tell, we were the only non-EA insiders, or EA-connected, to be invited.</p><p>Effective Altruism members are smart enough to know all about the dangers of groupthink, in-group psychology and confirmation bias. You\u2019re also big fans, and practitioners, of constant self-criticism.</p><p><strong>Don\u2019t only talk to yourselves at your conferences.</strong></p><p>One well-attended conference session confirmed this paradox \u2013 very smart, highly-educated people with a blind spot when it comes to communication.</p><p>We attended a session on EA and Journalism, hoping it would help us get to know what an EA audience is like, to triangulate where to pitch our own upcoming talk on Effective Climate Activism.</p><p>The fact that it was in a big room, and mostly full, suggested EA delegates felt that communications was an important issue. The speaker was evidently an accomplished,experienced journalist/communicator, and very familiar with an EA crowd.</p><p>At first, we were surprised at how basic her presentation was \u2013 real rudimentary stuff. Surely an EA crowd was more sophisticated than this, we thought. Until the Q&amp;A, which included questions like \u2018can you list 5 things we can do to do better interviews?\u2019.</p><p>It turned out the speaker knew her audience very well. This crowd, for all its university education, really did need remedial comms education.</p><h2>Let me tell you a story about Effective Altruism</h2><p>Our talk was scheduled for the final slot of the last day, so See Through News had 3 days to tune into the Effective Altruism <i>zeitgeist</i>.</p><p>The first rule of comms is to know your audience, the second to adjust your message to that audience. So we did a lot of people-watching, eavesdropping and ear-wigging.</p><p>Five minutes observation would leave most people concluding typical EA delegates are:</p><ul><li>young</li><li>neurodivergent</li><li>seeking something</li></ul><p>Should we adapt our messaging, we wondered? Maybe re-write<a href=\"https://seethroughnews.org/category/faqs-see-through-news\"> our website FAQs</a>, currently written in the comic catechism style of Flann O\u2019Brien. This great Irish writer and comic genius isn\u2019t well-known outside Effective Altruism circles. We weren\u2019t getting the vibe he was a big name inside them.</p><p>See Through News has often been complimented on its FAQs, but would an EA audience find them the <i>\u2018best-written, most amusing and impactful FAQs I\u2019ve ever read\u2019?</i></p><p>Eavesdropping suggested Effective Altruism delegates appeared more likely to quote Star Trek or Harry Potter than Flann O\u2019Brien. Would our stuff land?</p><p>It was a relief when a couple of Effective Altruism youngsters mentioned how refreshing and stimulating they found our FAQs. Thanks, Flann, we\u2019ll raise a pint of porter to you next time.</p><p>This not (just) a humblebrag about our FAQs. We raise it to highlight the communications blind spot we found in Effective Altruism in general.</p><p>By all means speak in your in-group code at home, even when delivering talks to each other at your conferences \u2013 but if you have any aspiration to reach beyond your charmed circle, <i><strong>you gonna hafta talk different</strong></i><strong>.</strong></p><p>By all means quote Ord, Singer and Macaskill, but if you only ever quote Star Trek, you\u2019ll only attract or make sense to Trekkers.</p><p>Reference The Simpsons and The Big Bang Theory, and you\u2019re getting warmer, but you\u2019re still making it hard for the huge numbers of people who are unfamiliar with your references to catch your drift.</p><p><strong>Don\u2019t just quote or cite. Tell stories.</strong></p><h3>What the Dickens?</h3><p>How about quoting a Dickens character? We\u2019re about to, but first we\u2019re going to tell you why.</p><p>Having been reassured by a rather jaded Effective Altruism veteran that \u2018<i>there\u2019s nothing an EA crowd likes more than having a joke explained to them\u2019,</i> we\u2019ll risk telling you why you should at least consider quoting Dickens. Consider:</p><ul><li><i>People haven\u2019t changed much since Victorian Britain</i>. Dickens nailed just about every possible human flaw and virtue with an accuracy, humour and impact few others will ever approach, and we\u2019re the same \u2018<i>bloody ignorant apes</i>\u2018 as we ever were. That was a <a href=\"https://www.lunduniversity.lu.se/lup/publication/ae53d390-94bd-4936-9257-543d55159465\">quote from Samuel Becket</a>, incidentally.</li><li><i>Loads of non-EA people know loads of Dickens characters. </i>Even if they don\u2019t, they\u2019ve at least heard of Dickens, and know him as a trusted source when it comes to illustrating human frailty. And even if they\u2019ve never heard of Dickens, they probably know of his characters. Every day, people dismiss misers as Scrooges, diagnose hoarders with Miss Havisham syndrome, or call tricksters Artful Dodgers, unaware that Dickens created all these characters.</li></ul><p>If you find deploying references to Slytherin, Sorting Hats and Muggles is effective in your peer group, you\u2019ll know the shorthand power of great names that nail common human traits.</p><p>So even if you\u2019ve not read<i> Bleak House</i>, or have yet to come across Mrs. Jellyby, does Dicken\u2019s description of this deluded philanthropist ring a bell?:</p><p><i>\u2018a pretty, very diminutive, plump woman, of from forty to fifty, with handsome eyes, though they had a curious habit of seeming to look a long way off. As if\u2026they could see nothing nearer than Africa\u2019</i>.</p><p>Mrs. Jellyby would now be called a \u2018virtue signaller\u2019. Her filthy, chaotic London home is filled with her own neglected children, who get covered in ink, go hungry, and tumble down staircases while she pursues her obsession with saving the poor and starving children of Africa.</p><p>Dickens, never one to use one word when a dozen are better, observes that Mrs. Jellyby surrounds herself with other deluded obsessives, equally incapable of perceiving problems under their own noses:</p><blockquote><p><i>One other singularity was, that nobody with a mission\u2014except Mr Quale, whose mission, as I think I have formerly said, was to be in ecstasies with everybody\u2019s mission\u2014cared at all for anybody\u2019s mission. Mrs Pardiggle being as clear that the only one infallible course was her course of pouncing upon the poor, and applying benevolence to them like a strait-waistcoat; as Miss Wisk was that the only practical thing for the world was the emancipation of Woman from the thraldom of her Tyrant, Man. Mrs Jellyby, all the while, sat smiling at the limited vision that could see anything but Borrioboola-Gha.</i></p><p><a href=\"https://www.online-literature.com/dickens/bleakhouse/5/\">Bleak House</a>, by Charles Dickens.</p></blockquote><p>Post-colonial critics sometimes cite passages like this as evidence Dickens cared nothing for distant brown people, but in so doing they\u2019re exemplifying the true, and much more profound and enduring target of his satire \u2013 how your own personal obsessions can blind you to much more pressing problems on your doorstep.</p><p>So. Why do you think we just spent the last few paragraphs discussing Dickens?</p><p>If you\u2019ve spent any time explaining how<a href=\"https://forum.effectivealtruism.org/posts/dpBB24QsnsRnkq5JT/2019-ai-alignment-literature-review-and-charity-comparison\"> humanity\u2019s most urgent existential threat is AI Alignment</a> to an EA delegate from Mexico, Pakistan, The Philippines, Hungary or Brazil, we suggest reading more Dickens.</p><p>Oh, and maybe <strong>start listening harder to your Global South delegates.</strong></p><h2>Beware the Tyranny of Choice, Money Sludge &amp; Navel-Gazing</h2><h3>Tyranny of Choice</h3><p>\u2018Lost\u2019, is the one-word epithet one EA-outsider used to describe Effective Altruism folk after a few minutes delegate-watching.</p><p>Certainly, there was a strong sense of young people looking for something to give meaning to their own lives, and the lives of others.</p><p>From a See Through News perspective, this was very encouraging. Our Goal is to help the Inactive Become Active, but what does the fact that all these lost souls gathered in Berlin for an Effective Altruism conference tell us about the movement itself?</p><p>In one sense, it reflects very well on it. EA members remind us of Extinction Rebellion activists \u2013 characteristically questioning, independent-minded, with no monogamous affiliation to a single movement, open to all offers.</p><p>Like many XR members, many EA members appear to treat the movement as a part-support group, part-dating app, part-shop window. Meet like-minded fellow-travellers. Find an organisation you might like to take home with you. This openness is a further reason why EA is not a cult.</p><p>But the Tyranny of Choice is another form of the Tyranny of Inaction. As people who use online dating sites know, if you spend all your time picking and choosing, waiting for the Right Project to come along, rather than just taking a punt, you can end up never even trying. Nothing ventured, nothing gained.</p><p>Maybe it was because See Through News so obviously had real-world experience, but we found many of the younger delegates, whether at the Opportunity Fair or in 1-to-1 meetings, asked See Through News for careers advice.</p><p>In nearly every case, having listened to their situation, our advice was to just do something. Anything. If you want to be a writer, start writing. If you want to know how to make videos, start making videos. If you\u2019re wondering where to volunteer, just pick a charity and start volunteering. There was definitely a general tendency to look for further qualifications, or courses, or teachers, rather than taking the plunge and learning from experience.</p><p>When you lack experience, it\u2019s all the more important to gain it. It\u2019s like dating. You may quickly discover this person/organisation/charity/employer isn\u2019t for you, but at least you now know. It might even help you pick the next one.</p><p><strong>Just do it \u2013 indecision guarantees you\u2019ll never find out.</strong></p><p>But we also found this hesitancy to commit, this reluctance to leave comfort zones, indicated a deeper issue, both for individuals attracted to Effective Altruism conferences, and surely therefore to the movement itself.</p><h3>Money Sludge</h3><p>Our <a href=\"https://youtu.be/QQHa75RBbMY\">founding allegory of the Three-Headed Beasts</a> places the obstacles to carbon drawdown in The Money Mire.</p><p>Money is what connects Beast to Beast, and creates the sludge that slows down Carbon Drawdown.</p><p>Amongst other things, this is what makes See Through News very wary of money, and why we want to see how far we can get without it.</p><p>So far so good. The reason See Through News has developed 14 projects so quickly is because we\u2019ve not been distracted. Not one second has been spend applying for funding, justifying our spend, or engaging in all the proper, but time-consuming and energy-sucking checks-and-balances that funding demands.</p><p>We\u2019ve been astonished, frankly, at how much we\u2019ve managed to do so quickly without any money.</p><p>The volunteers who\u2019ve developed the various See Through News projects tend to fall into one of these four categories:</p><ul><li>financially comfortable professionals in well-paid, high-status jobs, who want to do something more meaningful in their spare time</li><li>experienced professionals who\u2019ve given up trying to make a living in creative industries that have been destroyed by the internet, as the money taps once controlled by the pre-Internet gatekeepers \u2013 publishers, music labels, galleries, commissioning editors etc. \u2013 have run dry</li><li>time-rich, experience-poor young people at the start of their careers, who see mentoring and resume-building benefits of volunteering for See Through News, in addition to Doing Good</li><li>people committed to one particular area (education, community, building, games etc.) who see our projects as being intrinsically interesting or worthwhile, even without the carbon drawdown goal.</li></ul><p>We expect that before long we\u2019ll come across some challenge, or grow to a scale, that demands we scrape together at least some money.</p><p>We\u2019ll cross that bridge when we come to it.</p><p>In the meantime, our zero-budget approach is not only highly effective in itself, as it permits us to work so friction-free, but it\u2019s also the ultimate expression of our sincerity.</p><p>When we approach people with our open source, free of charge projects, we\u2019re often greeted with suspicion. This is frustrating. It\u2019s why Robert Stern awards himself the title of \u2018Gift Horse Distributor\u2019 on his name cards, rather than Founder.</p><p>But given the number of scams out there, things that at first appear to be free, but turn out to be too good to be true, such suspicion is a perfectly reasonable, as well as understandable, response.</p><p>Effective Altruism hasn\u2019t always had loads of money, but it sure does now. So far as we can tell, EA is spending its new millions well \u2013 this Berlin conference, at least, was an excellent use of funds.</p><p>All we\u2019re saying is \u2013 <strong>beware of money\u2019s seduction, distraction and friction.</strong></p><p>The more \u2018senior\u2019 EA people we met, the more they, one way or another, depended on the EA payroll. However independent you may be, or feel, being paid by someone inevitably means you\u2019re obliged to suppress and compromise what you might otherwise choose to do.</p><p>We heard plenty of such observations from EA insiders, their intensity growing in proportion to the seniority and amounts of money involved.</p><h3>Navel-Gazing</h3><p>Implicit in the Wikipedia critique of Effective Altruism quoted above is the consequences of the \u2018lack of commitment\u2019 mentioned. We\u2019ll risk opprobrium by being more explicit.</p><p>We do so only too aware of potential \u2018OK Boomer\u2019 responses, and of the sensitivities inevitable when the neurotypical remark on the neurodivergent, but we think it\u2019s important, so here goes.</p><p><strong>Stop navel-gazing</strong>. It\u2019s Inactive. If you want to be effective activists, on climate or otherwise, make your Action about more than just yourself. Aspire to use your skills, passion and energy to leverage change on a bigger scale.</p><p>That Archimedes quote <i>\u201cGive me a lever long enough and a fulcrum on which to place it, and I shall move the world. \u201d</i> is more than mere physics, it\u2019s a call to action.</p><p>Intellectually, you have all the gear, but no idea of how to use it. A great idea, unarticulated, won\u2019t spread.</p><p>This criticism is not limited to the attendees, though. We felt it ran much deeper in EA.</p><p>When asked what the highlight of the opening speech was, more than one attendee told us it was the off-the-cuff history of the venue delivered by the person introducing the session. If your keynote speakers can\u2019t put on a show to the converted, what chance to you have of moving the people See Through News targets, <a href=\"https://seethroughnews.org/our-target-audience-for-climate-activism\">the vast majority of the human species who are Unwilling Inactivists</a>?</p><p>In for a penny, in for a pound \u2013 we may as well go the whole hog and include William Macaskill here too.</p><p>The theme of his wonderfully readable new book, and the direction in which he appears to want to nudge Effective Altruism, is \u2018Longtermism\u2019.</p><p>Longtermism is Mackaskill\u2019s way of adding the unborn to the utilitarian balance sheet, of tipping the cost-benefit analysis by including \u2018the Silent Billions\u2019. Chapter 1, \u2018The Case for Longtermism\u2019 starts with the words:</p><blockquote><p><i>Future people count. There could be a lot of them. We can make their lives go better. This is the case for longtermism in a nutshell.</i></p><p>What We Owe The Future: a million-year view, William Macaskill, 2022</p></blockquote><p>Hard to argue with this, but isn\u2019t it a bit\u2026Jellyby-ish?</p><p>Dickens skewered Mrs. Jellyby for ignoring problems under her nose in favour of distant African problems. Isn\u2019t longtermism doing the same in time, rather than space?</p><p>How comfortable would you be to make these abstract arguments to Effective Altruism delegates from Pakistan, where floods have left tens of millions homeless, or Brazil, struggling to stem the Amazon deforestation encouraged by their populist leader, or any number of Global South nations faced with imminent immolation or inundation?</p><p>We\u2019re not saying anything you\u2019re saying is wrong, just that you might still be saying it as the waters creep up your ivory tower.</p><p><strong>Don\u2019t be Mrs. Jellyby.</strong></p><h2>Yours sincerely</h2><p>So there you have it, our sincere impression of Effective Altruism following out first interaction with it. We hope it won\u2019t be our last, as there\u2019s a huge overlap both in terms of our data-driven approach, and the supply-demand matchup between See Through News\u2019s zero-budget projects and Effective Altruism\u2019s highly-talented, highly-skilled, highly-motivated followers.</p><p>Feel free to dismiss our well-intentioned critiques as superficial, or misguided, but for better or worse they\u2019re honest and delivered with no agenda other than your own. We too are into rigorous cost-benefit analysis.</p><p>These observations don\u2019t include any numbers, partly because we don\u2019t have them, but mainly because we suspect you do, so you\u2019ll be able to use them to rigorously test our conclusions.</p><p>Here\u2019s the Executive Summary for those far too busy to be distracted by discursive discussion of The Tallest Woman in Mongolia, or resistant to the charms of cockerpoos:</p><ol><li><strong>Don\u2019t sound like a cult</strong></li><li><strong>Learn how to speak everyone else\u2019s language, instead of getting them to learn yours</strong></li><li><strong>Don\u2019t only talk to yourselves at your conferences.</strong></li><li><strong>Don\u2019t just quote and cite. Tell stories.</strong></li><li><strong>Start listening harder to your Global South delegates.</strong></li><li><strong>Just do it \u2013 indecision guarantees you\u2019ll never find out.</strong></li><li><strong>Stop navel-gazing</strong></li><li><strong>Beware of money\u2019s seduction, distraction and friction.</strong></li><li><strong>Don\u2019t be Mrs. Jellyby.</strong></li></ol>", "user": {"username": "_galvo_"}}, {"_id": "zfKyRWikgK4f2MPS2", "title": "Institutions and Longtermism Reading List", "postedAt": "2022-10-19T11:56:21.902Z", "htmlBody": "<p>The reading list below is based on a reading list originally used for an internal GPI reading group. These reading groups are used as a way of doing an early-stage exploration of new areas that seem promising from an academic global priorities research perspective.&nbsp; Each topic is often used as the theme for one or two weekly discussions, and in most cases those attending the discussion will have read the suggested materials beforehand.&nbsp;</p><p>As I thought that it could be a valuable resource for those interested in academic global priorities research, I\u2019m sharing it here, with permission from the authors. All the credit for the list below goes to them.</p><p><i>Disclaimer: The views presented in the readings suggested below do not necessarily represent views held by me, GPI, or any GPI staff member.</i></p><h1>1. Intergenerational Justice</h1><ul><li>Caney, \u201c<a href=\"https://www.annualreviews.org/doi/abs/10.1146/annurev-polisci-052715-111749\"><u>Justice and Future Generations</u></a>\u201d&nbsp;</li><li>Meyer, \u201c<a href=\"https://plato.stanford.edu/entries/justice-intergenerational/\"><u>Intergenerational Justice</u></a>\u201d</li><li>Mogensen, \u201c<a href=\"https://globalprioritiesinstitute.org/andreas-mogensen-the-good-news-about-just-saving/\"><u>The Good News about Just Saving</u></a>\u201d</li></ul><h1>2. Setting Longtermist Policy Goals</h1><ul><li>Cowen, \u201c<a href=\"https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/uclr74&amp;section=9&amp;casa_token=Wy8h9-QkTdQAAAAA:xF_4oSrq4Vk7kv-lmxrfHB4HCg8Br1byMHX3gLbVDPk87rwuFAb57VRtH5E3KDlSmCZlKhw\"><u>Caring for The Distant Future: Why it Matters and What it Means</u></a>\u201d&nbsp;</li><li>Summers and Zeckhauser, \u201c<a href=\"https://link.springer.com/article/10.1007/s11166-008-9052-y\"><u>Policymaking for Posterity</u></a>\u201d</li><li>Juijn and Schmidt, \u201c<a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/Inequality-and-the-Long-Term-Future_Andreas-Schmidt-and-Daan-Juijn-reupload.pdf\"><u>Economic Inequality and the Long-Term Future</u></a>\u201d</li><li>Cowen,<i>&nbsp;</i><a href=\"https://books.google.co.uk/books/about/Stubborn_Attachments.html\"><i><u>Stubborn Attachments</u></i></a></li></ul><h1>3. The Problem of Short-Termism</h1><ul><li>Jacobs, \u201c<a href=\"https://www.annualreviews.org/doi/abs/10.1146/annurev-polisci-110813-034103\"><u>Policy Making for the Long Term in Advanced Democracies</u></a>\u201d&nbsp;</li><li>MacKenzie, \u201c<a href=\"https://academic.oup.com/book/9618/chapter/156667729\"><u>Institutional Design and Sources of Short-Termism</u></a>\u201d</li><li>Jacobs<i>,&nbsp;</i><a href=\"https://books.google.co.uk/books/about/Governing_for_the_Long_Term.html?id=gt0iO0BEAaQC&amp;source=kp_book_description&amp;redir_esc=y\"><i><u>Governing for the Long Term</u></i></a></li></ul><h1>4. The Promise of Experimentalism</h1><ul><li>Barrett, \u201c<a href=\"https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/jetshy17&amp;section=10&amp;casa_token=WQA6KXUksy4AAAAA:YbPhHqth8tyoPvKR0s7f44iHOwGoe5PIs9x-7AzfaCX9GO-Ho1OVkEEjnQsP5wQlRaLkg54\"><u>Social Reform in a Complex World</u></a>\u201d</li><li>Barrett and Buchanan, \u201c<a href=\"https://www.jacobbarrett.org/uploads/1/2/3/6/123631127/social_experimentation_in_an_unjust_world.pdf\"><u>Social Experimentation in an Unjust World</u></a>\u201d&nbsp;</li><li>Sabel and Zeitlin, \u201c<a href=\"https://academic.oup.com/edited-volume/34384/chapter/291590330\"><u>Experimentalist Governance</u></a>\u201d<a href=\"https://drive.google.com/file/d/1XFJgi9049q24m5EPxpWtNyKJReRpioNe/view?usp=sharing\"><u>&nbsp;</u></a></li></ul><h1>5. Specific Institutional Proposals</h1><ul><li>John and MacAskill, \u201c<a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/Tyler-M-John-and-William-MacAskill_Longtermist-institutional-reform.pdf\"><u>Longtermist Institutional Reform</u></a>\u201d</li><li>I\u00f1igo Gonz\u00e1lez-Ricoy and Axel Gosseries,&nbsp;<a href=\"https://academic.oup.com/book/9618\"><i><u>Institutions for Future Generations</u></i></a></li><li>Jones, O\u2019Brien, and Ryan, \u201c<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0016328717301179\"><u>Representation of Future Generations in United Kingdom Policy-Making</u></a>\u201d</li></ul><h1>6. Democratic Theory and Future Generations</h1><ul><li>Jensen, \u201c<a href=\"https://www.tandfonline.com/doi/full/10.1080/20403313.2015.1065649\"><u>Future Generations in Democracy: Representation or Consideration?</u></a>\u201d&nbsp;&nbsp;</li><li>T\u00e4nnsj\u00f6, \u201c<a href=\"https://www.fil.lu.se/hommageawlodek/site/papper/TannsjoTorbjorn.pdf\"><u>Future People, the All Affected Principle, and the Limits of the&nbsp;</u></a><br><a href=\"https://www.fil.lu.se/hommageawlodek/site/papper/TannsjoTorbjorn.pdf\"><u>Aggregation Model of Democracy</u></a>\u201d</li><li>Goodin, \u201c<a href=\"https://www.jstor.org/stable/4623780\"><u>Enfranchising All Affected Interests, and Its Alternatives</u></a>\u201d</li><li>Ekeli, \u201c<a href=\"https://link.springer.com/article/10.1007/s10806-005-7048-z\"><u>Giving a Voice to Posterity \u2013 Deliberative Democracy and Representation of Future People</u></a>\u201d&nbsp;</li><li>Halstead, \u201c<a href=\"https://link.springer.com/article/10.1007/s10677-016-9759-9\"><u>High Stakes Instrumentalism</u></a>\u201d&nbsp;</li></ul><h1>7. Path Dependence in Politics</h1><ul><li>Pierson, \u201c<a href=\"https://www.cambridge.org/core/journals/american-political-science-review/article/increasing-returns-path-dependence-and-the-study-ofpolitics/AC2137B913363E33D97FC5CEC17CC75D\"><u>Increasing Returns, Path Dependence, and the Study of Politics</u></a>\u201d&nbsp;</li><li>Mahoney, \u201c<a href=\"https://www.jstor.org/stable/3108585\"><u>Path Dependence in Historical Sociology</u></a>\u201d&nbsp;</li></ul><h1>8. Institutional Evolution</h1><ul><li>Currie et al., \u201c<a href=\"https://ore.exeter.ac.uk/repository/handle/10871/29294\"><u>Evolution of Institutions and Organizations</u></a>\u201d</li><li>Lewis and Steinmo, \u201c<a href=\"https://www.journals.uchicago.edu/doi/full/10.1057/pol.2012.10\"><u>How Institutions Evolve: Evolutionary Theory and&nbsp;</u></a><br><a href=\"https://www.journals.uchicago.edu/doi/full/10.1057/pol.2012.10\"><u>Institutional Change</u></a>\u201d</li></ul>", "user": {"username": "LuisMota"}}, {"_id": "w5cmtouHZxGLondEA", "title": "Governments pose larger risks than corporations: a brief response to Grace", "postedAt": "2022-10-19T11:54:55.154Z", "htmlBody": "<p>In her article <a href=\"/posts/zoWypGfXLmYsDFivk/counterarguments-to-the-basic-ai-risk-case\">Counterarguments to the basic AI risk case</a>, Katja Grace argues that the track record of corporations is a reason to doubt what she presents as the basic argument for AI risk.</p><p>Corporations, however, are not the largest or most powerful human organisations. The governments of the USA and China are much larger and more powerful than any corporation on Earth. Just as we should expect the largest risks to come from the most powerful AI systems (or organisations of cooperating AIs), we should expect the most powerful human organisations to pose the largest risks.</p><p>Grace suggests that the argument for AI risk implies that we should consider human organisations to pose a substantial risk in one or both of the following ways:</p><blockquote><ol><li><strong>A corporation would destroy humanity rapidly</strong>. This may be via ultra-powerful capabilities at e.g. technology design and strategic scheming, or through gaining such powers in an \u2018intelligence explosion\u2018 (self-improvement cycle). Either of those things may happen either through exceptional heights of intelligence being reached or through highly destructive ideas being available to minds only mildly beyond our own.</li><li><strong>Superhuman AI would gradually come to control the future via accruing power and resources.</strong> Power and resources would be more available to the corporation than to humans on average, because of the corporation having far greater intelligence.</li></ol></blockquote><p>Powerful governments have constructed large stockpiles of nuclear weapons that many believe poses a large risk to human flourishing (though the precise magnitude is controversial). Furthermore, there are many instances in which these weapons were apparently close to being used. Thus governments pose a substantial risk of bringing about a disaster similar to scenario 1 above, albeit not as severe.</p><p>There have been governments in our history which have seized a great deal of power and whose actions brought about great disasters for many people. No single governments has ever held power over everybody, and governments do not seem to have an infinite lifespan. In my view it's plausible (but not probable) that technological and economic changes could mean that neither of these trends holds in the future. Thus, governments also seem to pose some risk of bringing about a disaster similar to scenario 2.</p><p>I also think it's plausible that if corporations, not governments, were the most powerful human organisations then we might have seen similar actions from corporations. For example, governments would obviously not allow large corporations to maintain their own nuclear arsenals, and it is plausible that some corporations would maintain an arsenal if they were allowed. We could also speculate that the most powerful governments may also limit the power of any corporation that threatened to become a rival.</p><p>The track record of corporations on its own may seem to undermine the standard AI risk argument, but I think we should consider governments as well, and it is not clear whether the record of governments supports or undermines the argument.</p>", "user": {"username": "David Johnston"}}, {"_id": "MbLrpJrXxf6tkWDWC", "title": "Is there a lunchclub for EA?", "postedAt": "2022-10-19T10:28:03.990Z", "htmlBody": "<p>I'm going to my first EAGx event in a couple of weeks and talking to people who've been to these before say that the most significant value comes from the one-on-ones. So I guess that since one on one conversations bring so much weight, does EA have a lunchclub-like app/website/etc? where EAs can add their availability, interests, and get matched up for conversations on a regular basis?</p>", "user": {"username": "wachichornia"}}, {"_id": "8mLm9bnRnEdrSE7ZE", "title": "We Should Give Extinction Risk an Acronym ", "postedAt": "2022-10-19T07:16:30.012Z", "htmlBody": "<p>Edit: At the suggestion of a commenter I am including some definitions for clarity.&nbsp;</p><p>Existential Risk (x-risk) [Bostrom's definition]: An existential risk is one that threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development.</p><p>Extinction risk: Future events that threatens the premature extinction of Earth-originating intelligent life.</p><p>S-risks: Future events with the potential capacity to produce an astronomical amount of <a href=\"https://en.wikipedia.org/wiki/Suffering\">suffering</a>.</p><p>&nbsp;</p><p>I feel like people sometimes use x-risk when they mean extinction risk because they want to use an acronym. Plus it would be nice for extinction risk to have an acronym anyway (if it doesn't already - I did a quick search and couldn't find anything).&nbsp;</p><p>Some possible candidates:&nbsp;</p><ul><li>e-risk (obvious choice but could be confusing since existential starts with e)</li><li>d-risk (death, or disappearance)</li><li>a-risk (annihilation)</li></ul><p>Feel free to propose a different acronym, or why this is a bad idea.&nbsp;</p>", "user": {"username": "Charles_Guthmann"}}, {"_id": "z46ZESEvmWLySzsBo", "title": "Shallow Report on Fungal Diseases", "postedAt": "2022-10-19T05:13:43.487Z", "htmlBody": "<p><strong>Note</strong>: This report was produced with only one week of desktop research, for the purpose of identifying promising causes to evaluate at depth. We only have low confidence in our findings here, and the conclusions should generally be taken by readers as merely suggestive rather determinative.</p><h1><u>Summary</u></h1><p><a href=\"https://docs.google.com/spreadsheets/d/1cBj7I8mlpLMrVM8P9HNzkZi3qQxHYOBsmiknLXx-ot0\"><u>Considering</u></a> the expected benefits of eliminating fungal infections (i.e. fewer deaths, less morbidity and greater economic output) as well as the tractability of vaccine development, I find that the marginal expected value of vaccine development for fungal infections to be&nbsp;<strong>1,104 DALYs per USD 100,000</strong>, which is around 2x as cost-effective as giving to a GiveWell top charity.</p><p>Discussion:</p><ul><li>The estimates I rely upon potentially undercount fungal disease mortality and morbidity.</li><li>There is considerable uncertainty over the rate at which the problem of fungal infections is getting better (or indeed, worse), unlike for better-examined diseases like malaria</li><li>It would be valuable to get expert quantitative modelling on this matter.</li><li>Health benefits drive the results, not the economic benefits, as is typical for many mainstream health ideas I've examined (e.g. tobacco taxation, road speed limits, alcohol taxation).</li><li>Assumes lifetime coverage for the vaccine</li><li>Expert advice on the whole theory of change for assessing tractability would be most clarifying</li><li>High uncertainty over the marginal production costs of antifungal vaccines.</li><li>The intervention requires massive amounts of money, and more than that, piecemeal contributions will not make a marginal difference, since pharmaceutical companies will not try to develop a vaccine without high upfront financial commitments that ensure they will profit in expectation. The corollary is that big funders like Open Philanthropy will potentially be more interested in such an area.</li></ul><h1><u>Expected Benefit: Averting Fungal Infection Deaths</u></h1><p>The primary expected benefit of eliminating fungal disease is preventing death. This benefit is modelled in the following way.</p><p><strong><u>Moral Weights</u></strong>: I take the value of averting one death to be&nbsp;<strong>29.3 DALYs</strong>. This is calculated as a function of (a) a human's full healthy life expectancy of 63.69, (b) a minor age-based philosophical discount, and (c) assuming we save someone of the median age in the relevant population. For more details, refer to&nbsp;<a href=\"https://docs.google.com/document/d/1j67pOUpC5vhC1-H516PAj2-4Fvm5rxJhZNeBVav3kik/edit\"><u>CEARCH's evaluative framework</u></a>.</p><p><strong><u>Scale</u></strong>: I calculate the deaths from fungal infections per annum by looking at three separate sources. Per&nbsp;<a href=\"https://www.science.org/doi/abs/10.1126/scitranslmed.3004404\"><u>Brown et al</u></a>, fungal infections kill one and a half million people every year (1.5 million deaths). Meanwhile, an estimate from the&nbsp;<a href=\"https://www.life-worldwide.org/media-centre/article/the-burden-of-fungal-disease-new-evidence-to-show-the-scale-of-the-problem\"><u>European Journal of Clinical Microbiology and Infectious Diseases</u></a> suggests a slightly higher figure (1.6 million deaths) - an estimate that is corroborated by the&nbsp;<a href=\"https://gaffi.org/why/fungal-disease-frequency/\"><u>Global Action for Fungal Infections (GAFFI)</u></a> (1.6 million deaths). Overall, I take a weighted average (<strong>1.595 million deaths</strong>) that penalizes the&nbsp;<a href=\"https://www.science.org/doi/abs/10.1126/scitranslmed.3004404\"><u>Brown et al estimate</u></a> somewhat \u2013 it may be less reliable than the more recent ones due to the lesser availability of studies pre-2013. That said, this does not seem like a significant issue either way, as figures converge, likely due to methodological similarities.</p><p><strong><u>Persistence</u></strong>: If we managed to prevent fungal deaths, the per annum benefits would persist over time and would hence need to be summed up across various years, but there are a couple of important discounts that we need to implement.</p><p>Firstly, I discount for the probability of the solution not persisting; in this case, the specific solution we are assessing for this cause area is that of a vaccine, and hence the risk of the solution not persisting consists of a fall in vaccination rates. I model this by assuming a steady state in vaccine uptake of childhood vaccines of 85% (the rate having stagnated there for around a decade pre-COVID), and then calculating the per annum decline as a function of a once-in-a-century pandemic potentially occurring, and the&nbsp;<a href=\"https://www.nature.com/articles/d41586-022-02051-w\"><u>drop in childhood vaccinations</u></a> that results if said pandemic occurs (taking&nbsp;<a href=\"https://data.worldbank.org/indicator/SP.POP.TOTL?locations=XD-XO\"><u>population-weighted average</u></a> of the falls in vaccination rates for high-income countries and LMICs). I take that there will be no policy reversal (i.e. governments no longer mandating childhood vaccinations) given the steady state pre-COVID. Overall, the risk of the solution not persisting is&nbsp;<strong>0.03%</strong>.</p><p>Secondly, I discount for the probability of the problem being counterfactually solved (i.e. fungal infections declining due to the intervention of other actors or else due to structural changes).</p><p>On the one hand, fungal infections are actually likely to&nbsp;<i>increase&nbsp;</i>over time due to various factors outlined by&nbsp;<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6241320/\"><u>Casadevall&nbsp;</u></a>(e.g. more people becoming immunocompromised due to modern medical therapies involving suppressing the immune system, and climate change selecting for temperature-resistant fungi that can infect mammals like humans). I model this increase as a function of two factors. One, the expected rise in infections from more people becoming immunocompromised, which is in turn a function of the&nbsp;<a href=\"http://www.transplant-observatory.org/data-charts-and-tables/\"><u>rise in organ transplants</u></a> (seemingly the&nbsp;<a href=\"https://www.biospace.com/article/immunosuppressive-drugs-market-increase-in-the-number-of-organ-transplants-is-likely-to-bolster-its-demand/\"><u>primary driver</u></a> for increased demand in immunosuppression drugs) and the&nbsp;<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4222063/\"><u>proportion of transplants that suffer infections</u></a>. And two, the per annum expected increase in fungal infection due to climate change, which is in turn a function of the&nbsp;<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8084208/\"><u>per annum chance</u></a> of a new fungal infection like Candida Auris emerging due to climate change, and the number of Candida Auris cases as a percentage of&nbsp;<a href=\"https://gaffi.org/why/fungal-disease-frequency/\"><u>total fungal infection cases</u></a>, as&nbsp;<a href=\"https://data.worldbank.org/indicator/SP.POP.TOTL?locations=US-1W\"><u>extrapolated&nbsp;</u></a>from&nbsp;<a href=\"https://www.cdc.gov/fungal/candida-auris/tracking-c-auris.html\"><u>US data</u></a>.</p><p>On the other hand, fungal infections can also be expected to decrease as economic growth brings improved sanitation, nutrition and health care. Using the&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1cBj7I8mlpLMrVM8P9HNzkZi3qQxHYOBsmiknLXx-ot0\"><u>difference</u></a> in fungal disease prevalence rates between rich countries and LMICs, as well as how long it will take LMICs to&nbsp;<a href=\"https://data.worldbank.org/?locations=XO-XD\"><u>catch up</u></a> to the prevailing wealth of rich countries, I calculate the expected decline in the fungal infection rate over time. Meanwhile, in terms of culture \u2013 the other big area of potential structural change we have to consider \u2013 there does not seem to be any reason to think this will affect infection rates one way or the other.</p><p>Moving beyond structural factors, various agents (i.e. governments, charities, businesses) do not seem to be committing much resources to fungal infections: (a) fungal infections are seemingly neglected by health authorities per&nbsp;<a href=\"https://microbialcell.com/wordpress/wp-content/uploads/2020A-Kainz-Microbial-Cell.pdf\"><u>Kainz et al</u></a>; (b) of the big charitable foundations,&nbsp;<a href=\"https://www.google.com/search?q=Wellcome+Trust+fungal+diseases&amp;oq=Wellcome+Trust+fungal+diseases\"><u>Wellcome&nbsp;</u></a>does fund some antifungal research, but it does not appear to be especially significant; and (c) biotech companies seem not to have committed much resources or made much&nbsp;<a href=\"https://www.labiotech.eu/in-depth/new-antifungals-development/\"><u>progress</u></a> on antifungal drugs over the last 20 years. The role of agents in actively solving this problem seems very minor, and I subjectively rate this as having 0.001 the impact of the structural decline.</p><p>Overall, I take the rate of counterfactual solution to be the negative epidemiological trend less the positive economic and agentic trends, which sums to&nbsp;<strong>0.41%</strong>.</p><p>Thirdly, I discount for the probability of the world being destroyed anyway (i.e. general existential risk discount). Here, I factor in the probability of total nuclear annihilation, since the benefits of saving people from fungal disease in one year is nullified if they had already died in a previous year. For the exact risk of total nuclear annihilation, I take it to be one magnitude lower than the risk of nuclear war itself, since nuclear war may not kill everyone. For the probability of nuclear war, I use the various estimates on the probability of nuclear war per annum collated by&nbsp;<a href=\"https://forum.effectivealtruism.org/s/KJNrGbt3JWcYeifLk/p/PAYa6on5gJKwAywrF\"><u>Luisa</u></a>, but with accidental nuclear war factored in, and then calculate a weighted average that significantly favours the superforecasters. The reason for this is that (a) the estimate of the probability of intentional nuclear war based on historical frequency is likely biased upwards due to historical use being in a MAD-free context; (b) the probability of accidental nuclear war based on historical close calls is highly uncertain due to the difficulty of translating close calls to actual probabilities of eventual launch; and (c) experts are notoriously bad at long-range forecasts, relative to superforecasters. Meanwhile, I do not take into account other existential risks like supervolcano eruption and asteroid impact, since the chances of those occurring at all is very marginal per&nbsp;<a href=\"https://www.amazon.com/Feeding-Everyone-Matter-What-Catastrophe/dp/0128044470\"><u>Denkenberger &amp; Pearce</u></a>, let alone the chances of such events killing everyone and not just most people. Overall, therefore, I treat the general existential risk discount to be just the risk of nuclear war but adjusted a magnitude down (i.e.&nbsp;<strong>0.07%</strong>)</p><p>Fourthly, I apply a broad uncertainty discount of&nbsp;<strong>0.1%</strong> to take into account the fact that there is a non-zero chance that in the future, the benefits or costs do not persist for factors we do not and cannot identify in the present (e.g. actors directing resources to solve the problem when none are currently doing so).</p><p><strong><u>Value of Outcome</u></strong>: Overall, the raw perpetual value of eliminating fungal infection deaths is&nbsp;<strong>7.77 * 10<sup>9</sup> DALYs</strong>.</p><p><strong><u>Probability of Occurrence</u></strong>: Unlike longtermist problems, there is no uncertainty that this is an actual problem -- fungal diseases&nbsp;<a href=\"https://www.id.theclinics.com/article/S0891-5520(15)00093-8/fulltext\"><u>cause&nbsp;</u></a>considerable morbidity and mortality globally. Hence, I just assign a&nbsp;<strong>100% chance</strong> to fungal infection being an actual problem that harms people.</p><p><strong><u>Expected Value</u></strong>: Overall, the expected value of eliminating fungal infection deaths is just&nbsp;<strong>7.77 * 10<sup>9</sup> DALYs</strong>.</p><h1><u>Expected Benefit: Eliminating Fungal Infection Morbidity</u></h1><p>Even when not fatal, fungal infections can cause disability and suffering, and I model the benefit of eliminating such morbidity in the following way.</p><p><strong><u>Moral Weights</u></strong>: I take the value of averting fungal infection morbidity to be equivalent to&nbsp;<strong>0.051 DALYs</strong> \u2013 this employs the&nbsp;<a href=\"https://cdn.who.int/media/docs/default-source/gho-documents/global-health-estimates/ghe2019_daly-methods.pdf\"><u>disability weight</u></a> for infectious disease (acute episode, moderate).</p><p><strong><u>Scale</u></strong>: As with how I approach estimating the deaths from fungal infections, for morbidity I consult three separate sources \u2013&nbsp;<a href=\"https://www.science.org/doi/abs/10.1126/scitranslmed.3004404\"><u>Brown et al</u></a> (2.08 million cases of severe fungal infections per annum); the&nbsp;<a href=\"https://www.life-worldwide.org/media-centre/article/the-burden-of-fungal-disease-new-evidence-to-show-the-scale-of-the-problem\"><u>European Journal of Clinical Microbiology and Infectious Diseases</u></a> (300 million cases); and&nbsp;<a href=\"https://gaffi.org/why/fungal-disease-frequency/\"><u>GAFFI</u></a> (13.5 million cases). However, I apply different weights. Beyond penalizing the&nbsp;<a href=\"https://www.science.org/doi/abs/10.1126/scitranslmed.3004404\"><u>Brown et al</u></a> estimate for the lesser availability of studies pre-2013, I also penalize both the Brown et al and&nbsp;<a href=\"https://www.life-worldwide.org/media-centre/article/the-burden-of-fungal-disease-new-evidence-to-show-the-scale-of-the-problem\"><u>GAFFI&nbsp;</u></a>estimates since they focus on just the most significant fungal infections \u2013 which makes them potentially a significant undercount., and is accordingly given less weight. Overall, this translates to around&nbsp;<strong>272 million</strong> cases of severe fungal infections per annum.</p><p><strong><u>Persistence</u></strong>: The same discounts discussed in the section on fungal infection deaths are applied here.</p><p><strong><u>Value of Outcome</u></strong>: Overall, the raw perpetual value of eliminating fungal infection morbidity is&nbsp;<strong>2.3 * 10<sup>9</sup> DALYs.</strong></p><p><strong><u>Probability of Occurrence</u></strong>: The same probability discussed in the previous section \u2013 for fungal infection being an actual problem that harms people \u2013 is applied here.</p><p><strong><u>Expected Value</u></strong>: All in all, the expected value of eliminating fungal infections is&nbsp;<strong>2.3 * 10<sup>9</sup> DALYs.</strong></p><h1><u>Expected Benefit: Increased Economic Output</u></h1><p>Beyond the health benefits, there are also economic benefits to eliminating fungal infections, which I incorporate into the model in the following manner.</p><p><strong><u>Moral Weights</u></strong>: I take the value of doubling consumption for one person for one year to be&nbsp;<strong>0.21 DALYs</strong>. This is calculated as a function of (a) the value of consumption relative to life from GiveWell's IDinsight survey of the community perspective, as adjusted for social desirability bias, and (b) CEARCH's estimate of the value of a full, healthy life in DALY terms. For more details, refer to&nbsp;<a href=\"https://docs.google.com/document/d/1j67pOUpC5vhC1-H516PAj2-4Fvm5rxJhZNeBVav3kik/edit\"><u>CEARCH's evaluative framework</u></a>.</p><p><strong><u>Scale</u></strong>: I start by calculating the economic burden of fungal infections relative to annual income, per infection sufferer.&nbsp; I look at three separate estimates, and take a weighted average.</p><p>The first estimate is Benedict, Whitham &amp; Jackson's. From this estimate of the total economic burden of fungal infections in the US (including direct medical costs, productivity loss from absenteeism, and output loss from premature deaths, but not the economic value of life, as that has already been accounted for), I calculate the economic burden of fungal infections relative to annual income per infection sufferer, by taking cost per infection sufferer and dividing by&nbsp;<a href=\"https://data.worldbank.org/indicator/NY.GDP.PCAP.CD?locations=US\"><u>GDP per capita</u></a>. Cost per infection sufferer is itself a weighted average of the cost for&nbsp;<a href=\"https://academic.oup.com/ofid/article/9/4/ofac097/6553135\"><u>treatment-seekers</u></a> (who suffer the direct medical costs) vs&nbsp;<a href=\"https://data.worldbank.org/indicator/SP.POP.TOTL?locations=US-1W\"><u>non-treatment seekers</u></a> (who do not). For the calculation of line item costs, I look at the direct medical costs vs productivity loss from fungal infections, and divide through with the relevant population.</p><p>The second estimate is Drgona et al's. From this estimate of the per-patient economic burden of Aspergillus and Candida in Europe on a direct medical cost basis, I calculate the economic burden of fungal infections relative to annual income per infection sufferer, by taking cost per infection sufferer in euros,&nbsp;<a href=\"https://www.exchangerates.org.uk/EUR-USD-spot-exchange-rates-history-2014.html\"><u>converting&nbsp;</u></a>it to USD, and dividing by&nbsp;<a href=\"https://data.worldbank.org/indicator/NY.GDP.PCAP.CD?locations=EU\"><u>GDP per capita in USD</u></a>. Cost per infection sufferer is again a weighted average of the cost for treatment-seekers (who suffer the direct medical costs) vs non-treatment seekers (who do not), calculated using the&nbsp;<a href=\"https://academic.oup.com/ofid/article/9/4/ofac097/6553135\"><u>Benedict, Whitham &amp; Jackson US data</u></a> on the&nbsp;<a href=\"https://data.worldbank.org/indicator/SP.POP.TOTL?locations=US-1W\"><u>split</u></a>. For the calculation of&nbsp;<a href=\"https://link.springer.com/article/10.1007/s10096-013-1944-3\"><u>line item costs</u></a>, I look at the direct medical costs provided, but do not model the productivity costs due to the lack of data.</p><p>The third and final estimate is Jian et al's. From this estimate of the per-patient economic burden of mucormycosis in China (inclusive of the direct medical cost, direct non-medical cost and indirect cost) in yuan, I calculate the economic burden of fungal infections relative to annual income per infection sufferer, by taking cost per infection sufferer in yuan,&nbsp;<a href=\"https://www.exchangerates.org.uk/USD-CNY-spot-exchange-rates-history-2022.html\"><u>converting&nbsp;</u></a>it to USD, and dividing by&nbsp;<a href=\"https://data.worldbank.org/indicator/NY.GDP.PCAP.CD?locations=CN\"><u>GDP per capita in USD</u></a>. Once more, cost per infection sufferer is a weighted average of the cost for treatment-seekers (who suffer the direct medical costs) vs non-treatment seekers (who do not), calculated using the&nbsp;<a href=\"https://academic.oup.com/ofid/article/9/4/ofac097/6553135\"><u>Benedict, Whitham &amp; Jackson US data</u></a> on the&nbsp;<a href=\"https://data.worldbank.org/indicator/SP.POP.TOTL?locations=US-1W\"><u>split</u></a>. For the calculation of&nbsp;<a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/jcpt.13643\"><u>line item costs</u></a>, I look at the breakdown of direct and indirect medical costs vs total costs inclusive of productivity losses.</p><p>In terms of how these three estimates are weighed, three sets of penalties are applied. Firstly, Drgona et al are penalized for not considering productivity costs. Secondly, Drgona et al and Jian et al are penalized for focusing only on specific fungal infections rather than fungal infections in general. And thirdly, the Benedict, Whitham &amp; Jackson and Drgona et al estimates are penalized for being more relevant to the rich world when fungal infections disproportionately hurt LMICs. Overall, therefore Benedict, Whitham &amp; Jackson and Jian et al are much more strongly weighed relative to Drgona et al. The resultant weighted average indicates the degree of consumption doubling per infection sufferer if their fungal infection is treated, and from this we can calculate the total number of equivalent consumption doublings (i.e.&nbsp;<strong>13.1 million doublings</strong>).</p><p><strong><u>Persistence</u></strong>: Same discounts as before are applied.</p><p><strong><u>Value of Outcome</u></strong>: Overall, the raw perpetual value of increased economic output is&nbsp;<strong>4.57 * 10<sup>8</sup> DALYs</strong>.</p><p><strong><u>Probability of Occurrence</u></strong>: Same probability as before is applied</p><p><strong><u>Expected Value</u></strong>: All in all, the expected value of increased economic output&nbsp; is&nbsp;<strong>4.57 * 10<sup>8</sup> DALYs</strong>.</p><h1><u>Tractability</u></h1><p>To solve the problem of fungal disease, I consider the potential solution of vaccine development, which I break down into four distinct steps:</p><ul><li>Step 1: Advance market commitment triggering attempts by pharmaceutical companies to develop antifungal vaccines, whether general purpose (i.e. for all fungal infections) or specific to a major fungal disease (i.e. cryptococcal meningitis, pneumocystis pneumonia, disseminated histoplasmosis, aspergillosis, candidiasis, or SAFS)</li><li>Step 2: These attempts at vaccine development succeeding</li><li>Step 3: Countries adopting antifungal vaccines as part of childhood vaccination schedules.</li><li>Step 4: Antifungal vaccines reducing disease burden.</li></ul><p>&nbsp;</p><p>Step 1: To estimate the probability of an advanced market commitment triggering attempts to develop antifungal vaccines, I take both an outside and inside view.</p><p>For the outside view, I consult three reference classes. The first is the&nbsp;<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3260895/\"><u>original AMC for a pneumococcal vaccine</u></a>. This was a success, and companies did try to develop a vaccine. The second is the&nbsp;<a href=\"https://www.dayoneproject.org/ideas/creating-advanced-market-commitments-and-prizes-for-pandemic-preparedness/\"><u>AMC for COVID vaccine</u></a>. This too was successful, and companies did of course attempt vaccine development. Still, attribution here is tricky \u2013 presumably, the pharmaceutical companies would attempt to develop vaccines anyway even without an explicit AMC, though it is also important to note that this would be under the implicit understanding that governments don't want their citizens to die from COVID and hence will buy such vaccines in the future (i.e. an effective AMC, albeit implicit). The third reference class is the&nbsp;<a href=\"https://www.greenbiz.com/article/inside-frontier-fund-pioneering-new-model-carbon-removal-investments\"><u>ongoing AMC for carbon removal technology</u></a>. For the February 2022 call for proposals, there were 75 expressions of interest, of which 26 were invited to make a formal application, and of which a final six were selected for purchase \u2013 clearly, companies are responding to the promised funding. Overall (while this is immaterial in a context of all reference frames converging on 100% success) I create a weighted average to calculate the chances of success; I penalize the carbon AMC reference frame for being of weaker relevance to the epidemiological context \u2013 yielding an estimated probability of 100% for an advanced market commitment triggering attempts to develop a vaccine.</p><p>For the inside view, I reason that given the profit motive (but also the technical feasibility concerns that the pharmaceutical companies will have), there will be perhaps a 83% chance of an attempt to initiate vaccine development.</p><p>For the aggregate view \u2013 the outside view adjusted by the inside view \u2013 I give more weight to the former than the latter, given that the inside view is subject to the usual worries about inferential uncertainty, yielding an overall estimate of 98% probability.</p><p>&nbsp;</p><p>Step 2: Remember that there are two kinds of vaccines we are considering here \u2013 general (i.e. for all fungal infections) or specific to a major fungal disease (i.e. cryptococcal meningitis, pneumocystis pneumonia, disseminated histoplasmosis, aspergillosis, candidiasis, or SAFS). The general vaccine is possible \u2013 per&nbsp;<a href=\"https://pubmed.ncbi.nlm.nih.gov/17184223/\"><u>Cassone &amp; Torosantucci</u></a>, a vaccine made up by an algal beta-glucan (laminarin), conjugated with a protein component, protects against infections by different fungi and induces antibodies capable of inhibiting fungal growth.</p><p>And to assess the probability of these attempts at vaccine development actually succeeding \u2013 whether for the general or specific antifungal vaccines \u2013 I take a purely outside view. I do not believe an inside view is useful here, given my lack of understanding of the scientific and technical background of fungal infections and vaccines that would allow an accurate deduction of outcomes.</p><p>To form my outside view, I consult three reference classes. The first is the&nbsp;<a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0057755\"><u>base probability that any vaccine candidate from the pre-clinical development phase reaches market entry</u></a>. The second is the&nbsp;<a href=\"http://perspectivesinmedicine.cshlp.org/content/4/5/a019703.full\"><u>base probability that a new antifungal drug is developed</u></a>, which I calculate by taking the number of new antifungals developed (1) and dividing over the number of years (30) since the last new antifungal was developed. The third is&nbsp;<a href=\"https://www.cbo.gov/publication/57126\"><u>the base probability that any new drug entering clinical trials is ultimately approved by the FDA</u></a>. When I aggregate the three reference classes, I weighed the base rate of success of vaccines more heavily, relative to the other two. Using the base rate of success for antifungals may be problematic insofar as it was calculated based on historical rates of antifungal development, which is a function not just of how hard the process scientifically is, but also of market demand \u2013 which is of course ex hypothesi altered here. Meanwhile, the base rate of success for drugs in general is just less representative as a reference class than the base rate of success for vaccines or anti-fungal medication in particular, since anti-fungal vaccines are what we are interested in here. Overall, we thus have around a 6% chance of success \u2013 which I use for the development of both general and specific antifungal vaccines.</p><p>&nbsp;</p><p>Step 3: Even after a vaccine is developed, it will have to be deployed, and countries may not necessarily do so. I assume the best way of deploying an antifungal vaccine is to add it to the list of childhood immunizations, and proceed to take a combined outside and inside view to assess the likelihood of governments doing this.</p><p>For the outside view, I look at three reference classes again. The first is simply the&nbsp;<a href=\"https://immunizationdata.who.int/\"><u>adoption rates of various vaccines</u></a>. For this, I calculate the average uptake rate for 194 countries across the following vaccines \u2013 aP (acellular pertussis) vaccine; Hepatitis A vaccine; Hepatitis B vaccine; HepB birth dose; Hib (Haemophilus influenzae type B) vaccine; HPV (Human Papillomavirus) vaccine; IPV (Inactivated polio vaccine); IPV (Inactivated polio vaccine) 2nd dose; Japanese Encephalitis; Measles-containing vaccine 2nd dose; Meningococcal meningitis vaccines (all strains); Mumps vaccine; PCV (Pneumococcal conjugate vaccine); PPV (Pneumococcal polysaccharide vaccine); Rotavirus vaccine; Rubella vaccine; Seasonal Influenza vaccine; Varicella vaccine; and YF (Yellow fever) vaccine. The second reference class, meanwhile, is&nbsp;<a href=\"https://ourworldindata.org/support-for-vaccination\"><u>public support for vaccines</u></a>, as indicated by the percentage of people in the world that think vaccines are important for children to have. The third reference class is&nbsp;<a href=\"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(PPP)_per_capita\"><u>economic ability to deploy a new vaccine</u></a>, as indicated by the percentage of countries in the world above mean global GDP. In aggregating these reference classes to form an aggregate outside view, I give far higher weight given to the vaccine adoption rates, as it is simply a more direct and relevant figure for estimating the future willingness of countries to adopt antifungal vaccines \u2013 yielding a probability of adoption of 59.5%.</p><p>Balancing this out is an inside view of the matter, wherein I rate the probability of convincing the WHO to recommend antifungal vaccines for childhood immunization schedules at around 67%, on the basis that the WHO is generally supportive of health interventions (though subject to the constraint of bureaucratic inertia). Moreover, I rate the probability of governments as a whole implementing the WHO recommendation at 33%, on the basis that governments only imperfectly follow WHO guidelines, albeit not to too bad an extent on a matter as uncontroversial as childhood vaccinations. Overall, this yields a probability of adoption of 0.22%.</p><p>The combined outside-inside view is formed by giving significantly more weight to the former, again because the latter is subject to worries about inferential uncertainty. In all, therefore, the probability that countries will adopt antifungal vaccines as part of childhood immunization schedules is estimated to be 59.1%.</p><p>&nbsp;</p><p>Step 4: Even after the vaccine(s) are successfully developed and deployed, the disease will not be fully eliminated, since (a) vaccines do not necessarily provide 100% immunity; (b) uptake isn't perfect (as COVID has surely demonstrated beyond a shadow of a doubt); and (c) the vaccine that we manage to successfully develop might not be the general antifungal vaccine that works against all fungal diseases, but rather one of the specific ones that works only against one of the six major fungal diseases.</p><p>To assess disease reduction, I take a purely outside view, again on the basis of lacking the technical expertise needed to make an informed inside view.</p><p>To develop the outside view for disease reduction from a general antifungal vaccine, I consult three different reference classes. The first is the average protection rates of various current vaccines. I calculate this indicator of expected vaccine protection, from the average of the&nbsp;<a href=\"https://www.cdc.gov/vaccines/vpd/vaccines-diseases.html\"><u>following vaccines</u></a> \u2013 chickenpox, dengue, diptheria, flu,&nbsp;<a href=\"https://www.immunize.org/askexperts/experts_hepa.asp\"><u>hepatitis A</u></a>,&nbsp;<a href=\"https://www.immunize.org/askexperts/experts_hepb.asp\"><u>hepatitis B</u></a>, Hib, HPV, measles, meningococcal, mumps, pneumococcal, polio, rotavirus, rubella, shingles, tetanus, whooping cough, anthrax,&nbsp;<a href=\"https://cdn.who.int/media/docs/default-source/immunization/position_paper_documents/japanese-encephalitis/pp-je-feb2015-refs.pdf\"><u>Japanese encephalitis</u></a>,&nbsp;<a href=\"https://www.who.int/activities/vaccinating-against-rabies-to-save-lives\"><u>rabies</u></a>, smallpox,&nbsp;<a href=\"https://vk.ovg.ox.ac.uk/vk/bcg-vaccine\"><u>tuberculosis</u></a>,&nbsp;<a href=\"https://www.sciencedirect.com/topics/medicine-and-dentistry/typhoid-vaccine\"><u>typhoid fever</u></a> and&nbsp;<a href=\"https://www.who.int/news-room/fact-sheets/detail/yellow-fever\"><u>yellow fever</u></a>) \u2013 and then modify it by the&nbsp;<a href=\"https://ourworldindata.org/vaccination\"><u>expected number of people covered</u></a> (estimated through current childhood vaccination coverage). The second reference class is the rate at which current antifungal drugs prevent infection. I calculate this indicator of expected vaccine protection, from&nbsp;<a href=\"https://www.ncbi.nlm.nih.gov/books/NBK68602/\"><u>the effectiveness of antifungal drugs for the prevention and treatment of oral candidiasis</u></a>, and then again modify it by the&nbsp;<a href=\"https://ourworldindata.org/vaccination\"><u>expected number of people covered</u></a> (estimated through current childhood vaccination coverage).The third reference class is the rate at which anti-infection drugs in general successfully fight off disease. I calculate this indicator of expected vaccine protection from the effectiveness of&nbsp;<a href=\"https://gh.bmj.com/content/4/2/e001315\"><u>antibiotics</u></a>,&nbsp;<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6280632/\"><u>antiretroviral drugs</u></a> and&nbsp;<a href=\"https://www.cdc.gov/parasites/taeniasis/resources/HortonAlbendazole.pdf\"><u>antihelminthics</u></a>, and then once more modify it by the&nbsp;<a href=\"https://ourworldindata.org/vaccination\"><u>expected number of people covered</u></a> (estimated through current childhood vaccination coverage). In aggregating these three reference classes, I give much higher weight to the vaccine and antifungal reference classes, as the success of anti-infection drugs in general is just less representative as a reference class for antifungal vaccines, which are what we're interested in. That said, the rates don't vary that much, which makes sense \u2013 they wouldn't have been approved for general use if their effectiveness were outside the range of what's conventionally considered sufficiently effective. In any case, the aggregate outside view estimate of disease reduction from a general antifungal vaccine is 63%.</p><p>For disease reduction for each of the potential specific vaccines (i.e. cryptococcal meningitis, pneumocystis pneumonia, disseminated histoplasmosis, aspergillosis, candidiasis &amp; SAFS), I simply take the headline proportion of disease reduction for the general antifungal vaccine, and then adjust downwards based on the&nbsp;<a href=\"https://gaffi.org/why/fungal-disease-frequency/\"><u>fraction&nbsp;</u></a>of fungal infections that are that specific disease. This yields the result that a cryptococcal meningitis vaccine will achieve 7% overall diseases reduction; a pneumocystis pneumonia vaccine will achieve 10%; a disseminated histoplasmosis vaccine will achieve 3%; an aspergillosis vaccine will achieve 37%; a candidiasis vaccine will achieve 14%; and a SAFS vaccine will achieve 8%.</p><p>&nbsp;</p><p>Putting the estimates across all four steps in the theory of change together (i.e. probability of an AMC triggering attempts to develop both general and specific antifungal vaccines, the probability of successful vaccine development, the probability that countries will adopt antifungal vaccines as part of childhood immunization schedules, and the disease reduction from vaccine deployment), we see that the expected proportion of disease reduction from an advance market commitment to both general and specific antifungal vaccines is 5%.</p><p>&nbsp;</p><p>As for the cost of the AMC \u2013 I calculate this total cost to be the sum of (1) total baseline dose cost and (2) total deployment costs. For total baseline costs, I calculate that as a function of (a)&nbsp;<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3260895/\"><u>individual baseline dose cost</u></a>, as estimated from the original pneumococcal AMC dose cost (i.e. pricing at marginal cost of $3.5 save for the first 20% of doses, which are priced at $7 to provide additional subsidies, presumably as an incentive for development and manufacturing); (b) total doses required, which a sum of the initial dose required, as a function of&nbsp;<a href=\"https://ourworldindata.org/vaccination\"><u>current vaccination rate</u></a> and&nbsp;<a href=\"https://data.worldbank.org/indicator/SP.POP.TOTL\"><u>global population</u></a>, plus additional doses required per year, as a function of initial dose required, total population turnover every&nbsp;<a href=\"https://www.who.int/data/gho/data/themes/mortality-and-global-health-estimates/ghe-life-expectancy-and-healthy-life-expectancy\"><u>73.4 years</u></a>, and the appropriate time discounts; and (c) the probability that a successful vaccine is even developed. Meanwhile, total deployment costs is calculated as a function of (d)&nbsp;<a href=\"https://thinkwell.global/cost-deliver-covid-vaccine/\"><u>individual delivery costs</u></a>, as estimated from COVID figures, which are around $1.66 per dose; (e) total doses required, as calculated in the manner already described; and (f) the probability of countries deploying the successful vaccine. Total costs are estimated to be 48.5 billion USD. Note that this is the cost in expectation \u2013 actual costs (and actual benefits) will turn out to be very different depending on whether the vaccines are successfully developed and deployed.</p><p>&nbsp;</p><p>Consequently, the proportion of the problem solved per additional USD 100,000 spent is around&nbsp;<strong>0.0000001</strong>.</p><h1><u>Marginal Expected Value of Vaccine Development for Fungal Infections</u></h1><p>Combining everything, the marginal expected value of vaccine development for fungal infections is&nbsp;<strong>1,104 DALYs per USD 100,000 spent</strong>, making this 2x as cost-effective as a GiveWell top charity.</p>", "user": {"username": "Joel Tan"}}, {"_id": "TufFm4P4kkxFBFf7s", "title": "Empowering Others To Find The Light of The EA Mindset", "postedAt": "2022-10-19T05:12:19.143Z", "htmlBody": "<p>After joining the EA movement I have been spending a lot of time researching and seeing where I maximize my effectiveness. I am blessed to have had success multiple times as an entrepreneur at a young age and utilized viral growth to drive 7 figure revenues with my e-commerce brand helping people quit nicotine.</p><p>My entire mindset is to maximize the amount of Effective Altruism I create, prior to the EA movement existing I called this in local friend groups \u201ckarma farming\u201d. Which is the more enlightened view than wealth farming in terms of valuing impact as a more valuable resource. One belief I hold is that the people are rewarded who predict correctly how the future universe plays out. Ineffective actions are thus based on a lack of perspective.</p><p>This perspective that good karma is the ultimate resource to accumulate is not post rationality while it may seem that way at first. Instead, a long term view on value proposes that over fiat, gold, housing, cars, etc good karma is the strongest resource that exists. When we enter an era of post scarcity, what will there be for us to value besides each others prior strength and commitment to building that better world? Thus I seek to maximize my good karma.</p><p>In seeking to have maximal effective impact, I believe filling in missed or underrepresented gaps of Effective Altruism to be the highest order I can follow. Of which the mindset is the act of identifying more high value impact roadmaps. Just like in business which seeks to create capital, the highest returns are in <a href=\"https://www.blueoceanstrategy.com/what-is-blue-ocean-strategy/\">blue ocean strategies</a>.</p><p><strong>\"BLUE OCEAN STRATEGY&nbsp;is the simultaneous pursuit of differentiation and low cost to open up a new market space and create new demand. It is about creating and capturing uncontested market space, thereby making the competition irrelevant. It is based on the view that market boundaries and industry structure are not a given and can be reconstructed by the actions and beliefs of industry players.</strong></p><p><strong>RED OCEANS&nbsp;are all the industries in existence today \u2013 the known market space. In red oceans, industry boundaries are defined and accepted, and the competitive rules of the game are known. Here, companies try to outperform their rivals to grab a greater share of existing demand. As the market space gets crowded, profits and growth are reduced. Products become commodities, leading to cutthroat or \u2018bloody\u2019 competition. Hence the term red oceans.\u201d</strong></p><p><strong>BLUE OCEANS, in contrast, denote all the industries not in existence today \u2013 the unknown market space, untainted by competition. In blue oceans, demand is created rather than fought over. There is ample opportunity for growth that is both profitable and rapid. In blue oceans, competition is irrelevant because the rules of the game are waiting to be set. A blue ocean is an analogy to describe the wider, deeper potential to be found in unexplored market space. A blue ocean is vast, deep, and powerful in terms of profitable growth.\u201d</strong><br>&nbsp;</p><p>AI alignment is an example of a red ocean in Effective Altruism. AI alignment is well known to be a core tenant of effective altruism that has ample attention.<br>&nbsp;</p><p>In my attempt to be the most Effective Altruist I can be, I will be identifying blue oceans and devoting my life to turning them red. One major tenant that is underrepresented is the spread of the effective altruist mindset itself.<br>&nbsp;</p><p>First off, a blue ocean I see is the actual mimetic spread of altruism itself. How do we inject this mindset into culture? How do we make altruism (AKA Good Karma) the status leaderboard of society at large.<br>&nbsp;</p><p>I find explaining high level concepts in super basic ways in order to find alignment in the truth of the information.<br>&nbsp;</p><p>\ud83d\udca1 \u201cIf you can't explain it simply,&nbsp;<strong>you don't understand it well enough\u201d</strong> - Albert Einstein.</p><p>&nbsp;</p><p><strong>The ELI5 [Explain Like I Am 5) version of EA Mindset Spreading:</strong></p><p>Imagine someone had the goal of creating as many gummy bears as possible. Say creating gummy bears yourself would have you reach the end of your productive career with 250,000 gummy bears made.</p><p>A gummy bear machine on the other hand can produce 250,000 gummy bears every single hour.</p><p>What they should do is not start making gummy bears right away, but instead look for the most effective route to afford the machinery which would produce gummy bears at scale. Imagine it takes a year to get funding for a gummy bear machine, and each year you start producing 10,000,000 EVERY YEAR.</p><p><img src=\"https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6d68e31c-3c3d-4da2-9628-c1d9641f60e2/Untitled.png\"></p><p>After 30 years they have produced 300 million gummy bears, an entirely amazing feat.</p><p>Moving up another level you may realize the goal is not in fact to create as many gummy bears for yourself, but to change the total amount of gummy bears that exist in the end.</p><p>You realize to maximize your gummy impact instead of following the path of getting the machinery yourself, you would look at inspiring others to believe in that goal. You would take your own reasoning for \u201cCreating as many gummy bears as possible\u201d as the optimal way to live ones life.</p><p><img src=\"https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5c718a25-538f-4fa2-b02a-979adce4fc8a/Untitled.png\"></p><p>If in 10 years you can convince 10 people to live their life optimizing gummy bears as well, they could spread this dogma onward causing more people with life paths of becoming gummy bear optimizers.</p><p>Over the next 20 years, 10 people each set up a machine producing 10m gummy bears a year means the world now has 2 BILLION MORE gummy bears.</p><p>Realizing this, it is obvious the highest order of this idea would be aligning the world around this belief, influencing culture to value the amount of gummy bears one produced as the highest order status function.</p><p>Exiting this gummy bear metaphor for the scaling doing good in the world gives us solid reasoning to focus on efforts that influence the world at large to take on the Effective Altruist mindset that we have already obtained or may still be working through our heads.</p><p>While, convincing people to dedicate their lives to making gummy bears at all costs is going to be a difficult sell, luckily, Effective Altruism is a concept grounded in truthful and rational reasoning that already has an amazing undercurrent of growth.</p><p>The resource of Effective Altruism may can be deemed Good Karma and may be far less trackable than Gummy Bear production itself, but the act of obtaining the most of it remains the same.</p><p>Thus, my life and this website has a focus on the spread of projects that help the effective altruism mindset spread. The best part about great work in the world is that you can turn the work into the marketing itself.</p><p>An example of this was packing orders for my no nicotine vape brand. What we did was set up Tiktok accounts and with an extra 60 seconds of effort posted short clips of orders being packed and shipped. Funnily enough, for a time period every single order we shipped results in 1.1 new orders. This became our only marketing channel, and its cost was eaten by the fact that we already needed to do that process. Simply understanding how to game these algorithms allowed us to reach over 100 million people in the course of just 6 months.</p><p>While there is plenty of work being done because of EA opportunities, EA certainly lacks mainstream virality besides the occasional Sam Bankman-Fried shoutout from his various high-level interviews. While this is great for high quantity, the lack of being able to follow a direct story has a lot less pull for emotion. Anyone versed in marketing knows that re-targeting is key to optimizing conversion.</p><p>While bringing concepts of conversion into the spread of the EA mindset may be off-putting to some, it is of course a necessary component to factor into any good movement. A major lack in the EA movement is influence. The internet is raising this next generation and causing cultural shifts faster than ever possible. If someone could name one influencer group living an EA lifestyle that would be amazing because I would like to follow them.</p><p>The very vacuum of this type of personality or group sickens me and its something I want to devote my life to filling whether through empowering other individuals to take this path, taking it myself with the friends I have <a href=\"https://Calendly.com/jackjay\">(and am open to finding more group members)</a>, or both. The very creation of an influencer who has this ideals will spawn countless more groups and people following the same path. As influence grows, and the pureness of the channel uplifts society, we would literally influence the influencers.</p><p>These high value impact roadmaps open up new opportunity for widespread adoption of the effective altruism mindset by weighing high value on the mimetic spread of the altruist mindset itself.</p>", "user": {"username": "jack jay"}}, {"_id": "ewRj8inZRh3xJ52Wo", "title": "[Announcement] The Steven Aiberg Project", "postedAt": "2022-10-19T07:48:58.125Z", "htmlBody": "<p><br>We are at war, a war for the future of our world. Most of the media mankind consumes links in no way to this thought/idea. Without recognition of the idea that humanity is up against a great battle for a good future, we are currently an in-group. Together we understand the great potential of our future and recognize the necessity to act.</p><p>&nbsp;</p><p>I have been involved with and many readers of this forum have read ideas on how to safely create artificial intelligence that is not misaligned with humanity. This idea that most of the companies in the world would treat Ai with higher regard than profit is grim. Today powerful engagement(extraction) algorithms run rampant online, rather than empowering us they deliver to us our limbic system wants, optimizing itself on becoming the best digital cigarette it can manage to be.&nbsp;</p><p>&nbsp;</p><p>There are, however, media channels that not only provide a highly enjoyable experience but a highly educational one too. These channels break into the mass market of media and provide highly valuable education. Example of this is <a href=\"https://www.youtube.com/c/Exurb1a\">Exurb1a</a>, <a href=\"https://www.youtube.com/c/inanutshell\">Kurgzegat</a>, <a href=\"https://www.youtube.com/c/RationalAnimations\">Rational Animations</a>, etc.&nbsp;</p><p>&nbsp;</p><p>We live in a visual world where video content is currently the main way information is spread across the internet. The power of this is understated. I have been a part of building creators from 0 to 5 million subscribers in a single month and generating 500 million views. We forget the sheer size of these numbers and influence, and I like to reconcile in my mind with the physical representation of sports stadiums. Larger than any auditorium known to man full-sized stadium averages around 50,000 people, rarely filling up except for the most known games. Imagine being the center of attention, rather than an entire team of people and players on the sidelines, this is exactly what's happening on the phone (of course, more like a half-time show or quick speech rather than a full game)&nbsp;</p><p>&nbsp;</p><p>Now, we are rapidly approaching a time when viral social media accounts are completely managed by Ai, which will be a smoking gun for the masses on the idea that Ai is going to transform our future. The capability for a single company to have mass leverage over the entire population's consumption of information is in front of us. If you consider this war and we are the party fighting for peace. Allowing a profit-driven company to become the leader in this field would be a disaster.<br>&nbsp;</p><p>Recently, a user made a post detailing why he thought AGI timelines were considered too early. In the comments, it became clear that the semantic definition of AGI was holding back the more impact-focused timeline:</p><p>Greg Coulbourn founder of <a href=\"https://ceealar.org/\">CEEALAR</a> made this great point:</p><p>\u201cFTX Future Fund cares about AI that poses an existential threat, not about whether such AI is AGI, or strong AI or true AI or whatever. Perhaps Transformative AI or TAI (as per OpenPhil's definition) would be better used in this case.\u201d&nbsp;<br><br>Through thinking about transformational Ai as the time when Ai is used to massively shift the future of the world, I believe is in its soft launch phase, with companies that start today capable of completely changing mass opinion and narrative.<br>&nbsp;</p><p>The internet's open-source attention facilities are going to become largely mined by Ai models through advancing automated content channels uploading en masse across platforms. This is the pendulum swing back towards a centralized content hub owning a large part of the attention economy again.&nbsp;</p><p>&nbsp;</p><p>When Sam Altman (founder of OpenAi) was <a href=\"https://youtu.be/WHoWGNQRXb0?t=161\">asked by Reid Hoffman</a> last month about Ai companies starting today that would create the most enduring value he responded:</p><p>\u201cI think there will be a middle layer that will be really important. There will be a whole new set of startups that take existing large models and tune it to create models. Those companies will create a lot of enduring value.\u201d</p><p>&nbsp;</p><p>Prolific investor at A16Z Andrew Chen <a href=\"https://www.linkedin.com/posts/andrewchen_with-the-wave-of-generative-ai-coming-up-activity-6983502349302464512-T7Vt?utm_source=share&amp;utm_medium=member_desktop\">came to a similar conclusion</a> with the coming decline in cost to produce Ai video.&nbsp;</p><p>\u201dWith the wave of generative AI coming up, I've been thinking about the \"$1000 blockbuster movie</p><p>Today millions of dollars (and hundreds of people) are needed to make a full length movie, a multiplayer game, or a multi-season TV series</p><p>But compare that to a humble book...</p><p>Writing a book just needs a laptop, coffee and lots of time. Not much money. Add up the laptop and all the coffees, and you maaaaybe spend $1000</p><p>What if an individual could make a full-length summer blockbuster movie by themselves with just a laptop and some coffee too?</p><p>It would be 100,000x leverage to be able to author a movie (or game, or TV series) - the same way that someone can write a book</p><p>It would require lots of effort and grit. It's not easy to write a good book - no diff from a great movie. But it would be possible when it's not today</p><p>What an explosion of content this would create! In every form.</p><p>I think of it as the next phase of lowering the cost of producing and publishing content online, the way that YouTube, Instagram, and other user-generated content platforms have democratized distribution</p><p>Many of us who are following the lightning-fast developments in generative AI intuitively know that the end outcome of these tools will change the process of creative work forever</p><p>It's incredible to see. But I am optimistic rather than cynical about these tools</p><p>The cynical view - that this will destroy whatever creative medium you enjoy - overlooks the incredible leverage this provides</p><p>This will increase by 10000000%s who can create movies, books, or otherwise. And make the content better as geniuses can increase their output\u201d</p><p><br>Previously the bottleneck of content corporations was the cost of content. Given the largely human workforce needed and all the difficulties that come along with that.&nbsp;<br>&nbsp;</p><p>With Ai video decreasing in cost by magnitudes and increasing in resolution rapidly this bottleneck is about to be opened up, allowing the company that builds the system for scale for Ai content creation to \"take over\" a large portion of the attention economy and take advantage of the compound interest in being early.</p><p>&nbsp;</p><p>I intend for that company to be one built by people aligned with the idea of empowerment of mankind over profit. I believe this opportunity for global information dissemination to be one of the greatest opportunities for pushing humanity toward an aligned future if you feel a similar way. Please get in touch.<br><br>Join Us:<br><a href=\"https://StevenAiberg.com\">StevenAiberg</a></p>", "user": {"username": "StevenAiberg"}}, {"_id": "xfGHbDFhXxFugjjwo", "title": "[link post] A Vision of Metascience by Michael Nielsen and Kanjun Qie", "postedAt": "2022-10-19T02:03:02.847Z", "htmlBody": "<p>Quite fascinating (and lengthy) essay laying out foundations and promising trailheads for the meta-science field. From one of the originators of the \"progress studies\" field and likely to be a major text within it. Self-recommending as some would say.</p>", "user": {"username": "joshcmorrison"}}, {"_id": "L9DMhcyXEbnGJaKuu", "title": "Hacker-AI and Digital Ghosts \u2013 Pre-AGI", "postedAt": "2022-10-19T07:49:24.427Z", "htmlBody": "", "user": {"username": "Erland W."}}, {"_id": "6FdvTfPSceqkHuiZv", "title": "An Extremely Opinionated Annotated List of My Favourite Mechanistic Interpretability Papers", "postedAt": "2022-10-18T21:23:58.377Z", "htmlBody": "<h2>Introduction</h2>\n<p>This is an extremely opinionated list of my favourite mechanistic\ninterpretability papers, annotated with my key takeaways and what I like\nabout each paper, which bits to deeply engage with vs skim (and what to\nfocus on when skimming) vs which bits I don\u2019t care about and recommend\nskipping, along with fun digressions and various hot takes.</p>\n<p>This is aimed at people trying to get into the field of mechanistic\ninterpretability (especially Large Language Model (LLM)\ninterpretability). I\u2019m writing it because I\u2019ve benefited a lot by\nhearing the unfiltered and honest opinions from other researchers,\nespecially when first learning about something, and I think it\u2019s\nvaluable to make this kind of thing public! On the flipside though, this\npost is explicitly about <em>my</em> personal opinions - I think some of these\ntakes are controversial and other people in the field would disagree.</p>\n<p>The four top level sections are priority ordered, but papers within each\nsection are ordered arbitrarily - follow your curiosity</p>\n<h2>Priority 1: What is Mechanistic Interpretability?</h2>\n<ul>\n<li>\n<p><a href=\"https://distill.pub/2020/circuits/zoom-in/\">Circuits: Zoom\nIn</a></p>\n<ul>\n<li>\n<p>Sets out the circuits research agenda, and is a whirlwind\noverview of progress in image circuits</p>\n</li>\n<li>\n<p>This is reasonably short and conceptual (rather than technical)\nand in my opinion very important, so I recommend deeply\nengaging with all of it, rather than skimming.</p>\n</li>\n<li>\n<p>The core thing to take away from it is the perspective of\nnetworks having legible(-ish) internal representations of\nfeatures, and that these may be connected up into\ninterpretable circuits. The key is that this is a mindset for\nthinking about networks <em>in general</em>, and all the discussion\nof image circuits is just grounding in concrete examples.</p>\n<ul>\n<li>On a deeper level, understanding why these are important and\nnon-trivial claims about neural networks, and their\nimplications.</li>\n</ul>\n</li>\n<li>\n<p>In my opinion, the circuits agenda is pretty deeply at the core\nof what mechanistic interpretability <em>is</em>. It\u2019s built on the\nassumption that there is some legible, interpretable structure\ninside neural networks, if we can just figure out how to\nreverse engineer it. And the core goal of the field is to find\nwhat circuits we can, build better tools for doing so, and do\nthe fundamental science of figuring out which of the claims\nabout circuits are actually true, which ones break, and\nwhether we can fix them.</p>\n<ul>\n<li>An important note is that mechanistic interpretability is an\nextremely young field and this was written 2.5 years ago -\nI take the specific claims in this article as a starting\npoint, not as the definitive grounding of what the field\nmust believe.</li>\n</ul>\n</li>\n<li>\n<p><strong>Meta:</strong> The goal of reading this is to understand what the\nfundamental mindset and worldview being defended here is. The\ngoal is <em>not</em> necessarily to leave feeling convinced that\nthese claims are true, or that the article adequately\njustifies them. That\u2019s what the rest of the papers in here are\nfor!</p>\n</li>\n<li>\n<p>A useful thing to reflect on is what the world would look like\nif the claims were and were not true - what evidence could you\nsee that might convince you either way? These are definitely\nnot obviously true claims!</p>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://transformer-circuits.pub/2021/framework/index.html\">A Mathematical Framework for Transformer\nCircuits</a></p>\n<ul>\n<li>\n<p>The point of this is to explain how to conceptually break down a\ntransformer into individually understandable pieces.</p>\n</li>\n<li>\n<p>Deeply engage with:</p>\n<ul>\n<li>\n<p>All the ideas in the overview section, especially:</p>\n<ul>\n<li>\n<p>Understanding the residual stream and why\nit\u2019s fundamental.</p>\n</li>\n<li>\n<p>The notion of interpreting <em>paths</em> between interpretable bits (eg input tokens and output logits) where the path is a composition of matrices and how this is different from interpreting every intermediate activations</p>\n</li>\n<li>\n<p>And understanding attention heads: what a QK and OV\nmatrix is, how attention heads are independent and\nadditive and how attention and OV are\nsemi-independent.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Skip Trigrams &amp; Skip Trigram bugs, esp understanding <em>why</em>\nthese are a really easy thing to do with attention, and\nhow the bugs are inherent to attention heads separating\n<em>where</em> to attend to (QK) and what to do once you attend\nsomewhere (OV)</p>\n</li>\n<li>\n<p>Induction heads, esp why this is K-Composition (and how\nthat\u2019s different from Q &amp; V composition), how the circuit\nworks mechanistically, and why this is too hard to do in a\n1L model</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Skim or skip:</p>\n<ul>\n<li>Eigenvalues or tensor products. They have the worst effort\nper unit insight of the paper and aren\u2019t very important.</li>\n</ul>\n</li>\n<li>\n<p>Maybe check out <a href=\"https://www.youtube.com/watch?v=KV5gbOmHbjU\">my (long-ass) walkthrough of the\npaper</a>, and\ncomments on how I think about things</p>\n<ul>\n<li>\n<p>If you prefer video over reading I expect it to be high\nvalue</p>\n</li>\n<li>\n<p>Either way it\u2019s probably useful to check the relevant\nsection it if there\u2019s part of the paper that confuses you.</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2>Priority 2: Understanding Key Concepts in the field</h2>\n<ul>\n<li>\n<p><a href=\"https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html\">Induction\nHeads</a></p>\n<ul>\n<li>\n<p>This is a study of how induction heads are ubiquitous in real\ntransformers, and form as a sudden phase change during\ntraining.</p>\n</li>\n<li>\n<p>Deeply engage with:</p>\n<ul>\n<li>\n<p>Key concepts + argument 1.</p>\n</li>\n<li>\n<p>Argument 4: induction heads also do translation + few shot\nlearning.</p>\n</li>\n<li>\n<p>Getting a rough intuition for all the methods used in the\nModel Analysis Table, as a good overview of interesting\ninterpretability techniques.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Skim or skip:</p>\n<ul>\n<li>\n<p>All the rigour - basically everything I didn\u2019t mention. The\npaper goes way overboard on rigour and it\u2019s not worth\nunderstanding every last detail</p>\n<ul>\n<li>The main value to get when skimming is an overview of\ndifferent techniques, esp general techniques for\ninterpreting during training.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>A particularly striking result is that induction heads form at\n~the same time in all models - I think this is very cool, but\nsomewhat overblown - from some preliminary experiments, I\nthink it\u2019s pretty sensitive to learning rate and positional\nencoding (though the fact that it <em>doesn\u2019t</em> depend on scale is\nfascinating!)</p>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://transformer-circuits.pub/2022/mech-interp-essay/index.html\">Mechanistic Interpretability, Variables, and the Importance of\nInterpretable\nBases</a></p>\n<ul>\n<li>\n<p>Short-ish conceptual essay on what the point of mechanistic\ninterpretability is and how to think about it.</p>\n</li>\n<li>\n<p>This is similar in flavour to Circuits: Zoom In, but is more\nconceptual and less grounded in very concrete examples +\nprogress - your mileage may vary in how much this works for\nyou.</p>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://transformer-circuits.pub/2022/toy_model/index.html\">A Toy Model of\nSuperposition</a></p>\n<ul>\n<li>\n<p>Building a simple toy model that contains superposition, and\nanalysing it in detail.</p>\n</li>\n<li>\n<p>Deeply engage with:</p>\n<ul>\n<li>\n<p>The core intuitions: what is superposition, how does it\nrespond to feature importance and sparsity, and how does\nit respond to correlated and uncorrelated features.</p>\n</li>\n<li>\n<p>Read the strategic picture, and sections 1 and 2 closely.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Skim or skip:</p>\n<ul>\n<li>No need to deeply understand the rest, it can mostly be\nskimmed. It\u2019s very cool, especially the geometry and phase\ntransition and learning dynamics part, but a bit of a nerd\nsnipe and doesn\u2019t obviously generalise to real models.</li>\n</ul>\n</li>\n<li>\n<p>A good intro paper for concrete projects. The models are tiny,\nthe core results should be easy to replicate (and have short\ntraining times), there\u2019s an <a href=\"https://colab.research.google.com/github/anthropics/toy-models-of-superposition/blob/main/toy_models.ipynb\">accompanying\nColab</a>\nand <a href=\"https://transformer-circuits.pub/2022/toy_model/index.html#open-questions\">a list of follow-up\nideas</a>,\nso this is a great paper to play around with!</p>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://distill.pub/2020/circuits/curve-detectors/\">Curve\nDetectors</a>\n&amp; <a href=\"https://distill.pub/2020/circuits/curve-circuits/\">Curve\nCircuits</a>\n(Image interpretability)</p>\n<ul>\n<li>\n<p>An extremely detailed and rigorous study of a family of neurons\nin Inception; a gold standard of what good interpretability\ncan look like. Culminates in them hand-coding the weights of\nartificial neurons and substituting those into the circuit,\nand comparing performance. Note that a bunch of the techniques\nwon\u2019t generalise.</p>\n</li>\n<li>\n<p>Deeply engage with:</p>\n<ul>\n<li>\n<p>Understanding what they did as a gold standard, and thinking\nabout <em>why</em> what they did is deep and meaningful evidence.</p>\n</li>\n<li>\n<p>Think about which techniques will and will not generalise to\nLLMs</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2>Priority 3: Expanding Understanding</h2>\n<h3>Language Models</h3>\n<ul>\n<li>\n<p><a href=\"https://openreview.net/forum?id=NpsVSN6o4ul\">Indirect Object\nIdentification</a></p>\n<ul>\n<li>\n<p>A paper about reverse engineering a complex (28 head!) circuit\nin GPT-2 Small</p>\n<ul>\n<li>\n<p>The most detailed \u201cwe actually have a circuit, and can drill\ninto it in detail and really get how it works\u201d paper that\nI know of.</p>\n<ul>\n<li>The circuit in question is for the task of completing\n\u201cWhen John and Mary went to the shops, John bought a\nbottle of milk for\u201d -&gt; \u201c Mary\u201d but \u201cMary bought a\nbottle of milk for\u201d -&gt; \u201c John\u201d</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Particularly good for a vibe of \u201cways interpretability is hard\nand you can trick yourself\u201d + \u201cbut it is actually possible and\nwe can fix these\u201d</p>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://transformer-circuits.pub/2022/solu/index.html\">SoLU</a></p>\n<ul>\n<li>\n<p>A paper on a neuron activation function that makes transformer\nneurons somewhat more interpretable.</p>\n</li>\n<li>\n<p>Deeply engage with:</p>\n<ul>\n<li>\n<p>Section 3 (Background). For the core ideas, esp\nsuperposition, privileged bases and why they matter.</p>\n<ul>\n<li>See \u201cA Toy Model of Superposition\u201d for much more on\nsuperposition.</li>\n</ul>\n</li>\n<li>\n<p>Section 6 (on the neurons found). For getting the vibe of\nwhat kind of features LLMs learn - I think this is the\nbest resource I know of for getting a vibe of what kinds\nof things MLP layers are doing at different layers of a\ntransformer.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Skim:</p>\n<ul>\n<li>Section 4 (on the exact function and how it works) - the\nmain intuition to get is <em>why</em> you might expect this to\nwork (in particular, why lateral inhibition seems\nimportant)</li>\n</ul>\n</li>\n<li>\n<p>Skip:</p>\n<ul>\n<li>Section 5 (showing that the model works as well as normal\nactivation functions).</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://rome.baulab.info/\">ROME</a></p>\n<ul>\n<li>\n<p>A paper on locating and editing factual knowledge in GPT-2 - a\nstrong contender for my favourite non Chris Olah\ninterpretability paper</p>\n</li>\n<li>\n<p>Deeply engage with:</p>\n<ul>\n<li>Causal tracing + activation patching stuff (including the\nappendix on it). It\u2019s a really cool, elegant and general\ntechnique, and demonstrates that certain computation is\nextremely localised in the model, and uses careful\ncounterfactuals to isolate this computation.</li>\n</ul>\n</li>\n<li>\n<p>Skim or skip:</p>\n<ul>\n<li>The model editing stuff. It\u2019s way less interesting from an\ninterpretability point of view than the above.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens\">Logit\nLens</a></p>\n<ul>\n<li>\n<p>A solid early bit of work on LLM interpretability. The key\ninsight is that we interpret the residual stream of the\ntransformer by multiplying by the unembedding and mapping to\nlogits, <em>and</em> that we can do this to the residual stream\nbefore the final layer and see the model converging on the\nright answer</p>\n<ul>\n<li>Key takeaway: Model layers iteratively update the residual\nstream, and the residual stream is the central object of a\ntransformer</li>\n</ul>\n</li>\n<li>\n<p>Deeply Engage with:</p>\n<ul>\n<li>The key insight of applying the unembedding early, and\ngrokking <em>why</em> this is a reasonable thing to do.</li>\n</ul>\n</li>\n<li>\n<p>Skim or skip:</p>\n<ul>\n<li>\n<p>Skim the figures about progress towards the answer through\nthe model, focus on just getting a vibe for what this\nprogress looks like.</p>\n</li>\n<li>\n<p>Skip everything else.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>The deeper insight of this technique (not really covered in the\nwork) is that we can do this on <em>any</em> vector in the residual\nstream to interpret it in terms of the direct effect on the\nlogits - including the output of an attn or MLP layer and even\na head or neuron. And we can also do this on weights writing\nto the residual stream.</p>\n<ul>\n<li>\n<p><a href=\"https://arxiv.org/pdf/2209.02535.pdf\">Analyzing Transformers in Embedding\nSpace</a> is a more\nrecent paper that drills down into this insight, focusing\non weights.</p>\n<ul>\n<li>\n<p>I\u2019m somewhat meh on the paper as a whole, but sections\n3, 4.1 and Appendix C are cool for seeing what head\nand neuron circuits can look like</p>\n</li>\n<li>\n<p>Note that they make the (IMO) mistake of treating\nembedding and unembedding space as the same space -\nthe input and output are different spaces! Even if\nmost people make the mistake of setting the embed and\nunembed maps to be the same matrix :(</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Note that this tends only to work for things close to the\nfinal layer, and will totally miss any indirect effect on\nthe outputs (eg via composing with future layers, or\nsuppressing incorrect answers)</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/pdf/2104.07143.pdf\">An Interpretability Illusion for\nBERT</a></p>\n<ul>\n<li>\n<p>Good early paper on the limitations of max activating dataset\nexamples - they took a seemingly interpretable neuron in BERT\nand took the max activating dataset examples on different\ndatasets, and observed consistent patterns <em>within</em> a dataset,\nbut very different examples <em>between</em> datasets</p>\n<ul>\n<li>\n<p>Within the lens of the Toy Model paper, this makes sense!\nFeatures correspond to directions in the residual stream\nthat probably aren\u2019t neuron aligned. Max activating\ndataset examples will pick up on the features <em>most</em>\naligned with that neuron. Different datasets have\ndifferent feature distributions and will give different\n\u201cmost aligned feature\u201d</p>\n<ul>\n<li>Further, models want to minimise interference and thus\nwill superpose anti-correlated features, so they\n<em>should</em></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Deeply engage with:</p>\n<ul>\n<li>\n<p>The concrete result that the same neuron can have very\ndifferent max activating dataset examples</p>\n</li>\n<li>\n<p>The meta-level result that a naively compelling\ninterpretability technique can be super misleading on\ncloser inspection</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Skim or skip:</p>\n<ul>\n<li>Everything else - I don\u2019t care much about the details beyond\nthe headline result, which is presented well in the intro.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3>Algorithmic Tasks</h3>\n<ul>\n<li>\n<p><a href=\"https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking\">A Mechanistic Interpretability Analysis of\nGrokking</a></p>\n<ul>\n<li>\n<p><em>Conflict of interest note - I was the main person working on\nthis project!</em></p>\n</li>\n<li>\n<p>A very detailed reverse engineering of a tiny model trained to\ndo modular addition and interpreting it during training, plus\na bunch of discussion on phase changes, an (attempted)\nexplanation of\n<a href=\"https://arxiv.org/abs/2201.02177\">grokking</a> and\nshowing grokking on other tasks.</p>\n<ul>\n<li>\n<p>Grokking probably isn\u2019t that relevant to real models and the\ntechniques don\u2019t really generalise, but a good example of\ndetailed reverse engineering + fully understanding a model\non an algorithmic task, and of applying interpretability\nduring training.</p>\n<ul>\n<li>Also a good example of how actually understanding a\nmodel can be really useful, and push forwards science\nof deep learning by explaining confusing phenomena.</li>\n</ul>\n</li>\n<li>\n<p>I also just personally think this project was super fucking\ncool, even if not that useful.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Deeply engage with:</p>\n<ul>\n<li>\n<p>The key claims and takeaways sections</p>\n</li>\n<li>\n<p><a href=\"https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking#Overview_of_the_Inferred_Algorithm\">Overview of the modular addition\nalgorithm</a></p>\n<ul>\n<li>The key vibe here is \u201choly shit, that\u2019s a\nweird/unexpected algorithm\u201d, but also, on reflection,\na pretty natural thing to learn if you\u2019re built on\nlinear algebra - this is a core mindset for\ninterpreting networks!</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Skim:</p>\n<ul>\n<li>\n<p><a href=\"https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking#Reverse_Engineering_the_Algorithm\">Reverse engineering modular\naddition</a> -\nunderstanding the different types of evidence and how they\nfit together</p>\n</li>\n<li>\n<p><a href=\"https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking#Evolution_of_Circuits_During_Training\">Evolution of modular addition circuits during\ntraining</a> -\nthe flavour of what the circuits developing looks like\nduring training, and the fact that once we understand\nthings, we can just literally watch them develop!</p>\n<ul>\n<li><a href=\"https://colab.research.google.com/drive/1F6_1_cWXE5M7WocUcpQWp3v8z4b1jL20#scrollTo=Analysis_During_Training\">The interactive graphics in the\ncolab</a>\nare <em>way</em> better than static images</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking#Phase_Changes\">The Phase Changes\nsection</a> -\nprobably the most interesting bits are the explanation of\ngrokking, and the two speculative hypotheses.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Maybe a good intro paper to replicate! It has an\n<a href=\"http://bit.ly/neelgrokking\">accompanying colab</a> and a\nlist of future directions at the end</p>\n</li>\n</ul>\n</li>\n</ul>\n<h3>Image Circuits</h3>\n<ul>\n<li>\n<p><a href=\"https://distill.pub/2017/feature-visualization/\">Feature\nVis</a> (fairly\nshort)</p>\n<ul>\n<li>An early paper with a really core technique for image\ninterpretability. Doesn\u2019t really transfer to LLMs, but worth\ngetting the vibe, and seeing how this made image\ninterpretability much easier and more rigorous in certain\nways - the vibe that this basically automatically gives\nvariable names to neurons.</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://distill.pub/2021/multimodal-neurons/\">Multimodal Neurons in Artificial Neural\nNetworks</a></p>\n<ul>\n<li>\n<p>An analysis of neurons in a text + image model (CLIP), finding a\nbunch of abstract + cool neurons. Not a high priority to\ndeeply engage with, but very cool and worth skimming.</p>\n</li>\n<li>\n<p>My key takeaways</p>\n<ul>\n<li>\n<p><a href=\"https://distill.pub/2021/multimodal-neurons/#guided-tour-of-neuron-families\">There are so many fascinating\nneurons!</a>\nLike, what?</p>\n<ul>\n<li>There\u2019s a\n<a href=\"https://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/2231\">teenage</a>\nneuron, a\n<a href=\"https://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/3\">Minecraft</a>\nneuron, a\n<a href=\"https://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/309\">Hitler</a>\nneuron and an\n<a href=\"http://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/2297\">incarcerated</a>\nneuron?!</li>\n</ul>\n</li>\n<li>\n<p>The intuition that multi-modal models (or at least, models\nthat use language) are incentivised to represent things in\na conceptual way, rather than specifically tied to the\ninput format</p>\n</li>\n<li>\n<p>The detailed analysis of the <a href=\"https://distill.pub/2021/multimodal-neurons/#person-neurons\">Donald Trump\nneuron</a>,\nesp that it is more than just a \u201cactivates on Donald\nTrump\u201d neuron, and instead activates for many different\nclusters of things, roughly tracking their association\nwith Donald Trump.</p>\n<ul>\n<li>This seems like weak evidence that neuron activations\nmay split more into interpretable segments, rather\nthan an interpretable directions</li>\n</ul>\n</li>\n<li>\n<p>The \u201c<a href=\"https://distill.pub/2021/multimodal-neurons/#typographic-attacks\">adversarial attacks by writing Ipod on an\napple</a>\u201d\npart isn\u2019t very deep, but <em>is</em> hilarious</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://distill.pub/2020/circuits/\">The rest of the circuits\nthread</a></p>\n<ul>\n<li>\n<p>A lot of really cool ideas and scattered threads! Worth skimming\nand digging into anything that catches your interest. Each\nindividual article is short-ish</p>\n</li>\n<li>\n<p>This thread represents, in my opinion, the first serious attempt\nat reverse engineering a real model (inception)</p>\n</li>\n<li>\n<p>My personal favourites:</p>\n<ul>\n<li>\n<p><a href=\"https://distill.pub/2020/circuits/early-vision/\">An Overview of Early Vision\nNeurons</a> -\nit\u2019s just fascinating to see the weird shit that happens,\nsuper cool to the hierarchy where see simple shapes are in\nearly layers and are built into more abstract shapes in\nlayer layers, and to see neurons being sorted into\nfamilies</p>\n<ul>\n<li>If you click on a neuron, you\u2019ll see the <a href=\"https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3a_47.html\">weight\nexplorer</a> -\nthis is a really fun tool to play around with, and\npractice just reading off the weights what they do!</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://distill.pub/2020/circuits/visualizing-weights/\">Visualising\nweights</a> -\nsomewhat image specific, but a fascinating exploration of\nthe data visualisation questions underlying mechanistic\ninterpretability - visualisations are super useful, but\n<em>how</em> can we do them in a properly principled way, and how\ncan they mislead?</p>\n<ul>\n<li>I really want to see more papers like this! These meta\nquestions are really important, but it\u2019s rarely\nincentivised to publish on them</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://distill.pub/2020/circuits/branch-specialization/\">Branch\nSpecialisation</a> -\nnetworks spontaneously learn to be modular <em>and</em> the\nmodules seem to be consistent and semantically\nmeaningful?! WTF?</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2>Priority 4: Bonus</h2>\n<ul>\n<li>\n<p>Not a paper: The <a href=\"https://github.com/neelnanda-io/Easy-Transformer\">codebase of\nEasyTransformer</a>,\na transformer mechanistic interpretability I\u2019m writing - I think\nit\u2019s worth reading for a fairly clean and conceptual-focused\nimplementation of a transformer, specifically reading\n<a href=\"https://github.com/neelnanda-io/Easy-Transformer/blob/ff0a4ef52d606277894acd306a97018999fee958/easy_transformer/EasyTransformer.py#L136\">EasyTransformer.forward</a>\nand\n<a href=\"https://github.com/neelnanda-io/Easy-Transformer/blob/54f4b4bfd15448b98288a51eef7ddddeac23b68d/easy_transformer/components.py\">components.py</a>\n(a file for the various layers) (the actual codebase is pretty\nlong!)</p>\n</li>\n<li>\n<p><a href=\"https://colah.github.io/\">Everything else Chris Olah</a> has\never written</p>\n<ul>\n<li>\n<p>I\u2019m somewhat biased on this, but I think Chris is just clearly\nfar and away the best interpretability researcher in the\nworld.</p>\n</li>\n<li>\n<p>He\u2019s also a massive nerd for good technical communication,\ninteractivity and good graphic design, and I find his work a\njoy to read.</p>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://distill.pub/2020/understanding-rl-vision/\">Interpreting RL\nVision</a></p>\n<ul>\n<li>\n<p>Interesting application of image circuits techniques to get some\ninsight into an RL model - <a href=\"https://www.alignmentforum.org/posts/iJDmL7HJtN5CYKReM/empirical-observations-of-objective-robustness-failures\">unclear how much it\ngeneralises/works</a></p>\n</li>\n<li>\n<p>The parts about the impact of the amount of and diversity of\ndata on interpretability feel most interesting and general to\nme.</p>\n</li>\n<li>\n<p>Probably the best RL mechanistic interpretability paper I know\nof (but it\u2019s a pretty low bar :( )</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Not a paper: Playing around with <a href=\"https://microscope.openai.com/\">OpenAI\nMicroscope</a> - visualizations\nand top dataset examples of every neuron in a ton of image models!\nChallenge: What\u2019s the weirdest neuron you can find?</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/1906.02715\">Visualizing and Interpreting the Geometry of\nBERT</a> (+ <a href=\"https://pair-code.github.io/interpretability/bert-tree/\">blog\npost</a>)</p>\n<ul>\n<li>\n<p>An early LLM interpretability paper about understanding how BERT\nrepresents language in the residual stream.</p>\n</li>\n<li>\n<p>Deeply engage with:</p>\n<ul>\n<li>Applying t-SNE to the residual stream + getting resulting\nvisualizations. This was really clever and cool, and\nunderstanding it is valuable.</li>\n</ul>\n</li>\n<li>\n<p>Skim or skip:</p>\n<ul>\n<li>The detailed syntax tree stuff.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/pdf/2111.09259.pdf\">Acquisition of Chess Knowledge in\nAlphaZero</a> - analysing\nAlphaZero\u2019s chess knowledge, including during training</p>\n<ul>\n<li>\n<p>Notable for the hilarious stunt of getting a chess grandmaster\ncommenting, and for co-authoring (even if this isn\u2019t that\ninterpretability related)</p>\n</li>\n<li>\n<p>Focuses on feature analysis rather than really mechanistic\nengagement, but still very cool! The main things I think are\ncool were successfully applying interpretability during\ntraining, and on the weird and fucky task of playing chess\n(and that models trained on non-image/language tasks are\nsomewhat interpretable!).</p>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/2207.13243\">Toward Transparent AI: A Survey on Interpreting the Inner\nStructures of Deep Neural\nNetworks</a> - a decent survey\npaper on what\u2019s up in the rest of interpretability.</p>\n<ul>\n<li>\n<p>I\u2019m personally pretty meh about the majority of the academic\nfield of interpretability (I rarely find insights from there\nuseful in my work) and would prioritise reading the papers in\nthe previous sections, but it\u2019s worth skimming to get a sense\nfor what\u2019s out there, and digging into anything relevant to a\nspecific project you\u2019re pursuing!</p>\n<ul>\n<li>Also, for sanity checking whether I\u2019m just being\noverconfident/arrogant, and there\u2019s actually a ton of\nuseful insights in standard interpretability for\nmechanistic stuff! Again, this post is just a list of my\npersonal hot takes.</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/pdf/2002.12327.pdf\">A Primer in\nBERTOLOGY</a> - a\nsurvey paper specifically on BERTology, a subfield about\nspecifically interpreting BERT. I feel pretty meh about this,\nbut am not very familiar with the field.</p>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://distill.pub/2018/building-blocks/\">The Building Blocks of\nInterpretability</a></p>\n<ul>\n<li>A cool and fun whirlwind tour of a bunch of different tools and\napproaches for image interpretability. Worth skimming.</li>\n</ul>\n</li>\n<li>\n<p>Not a paper, but I find <a href=\"https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/\">Chris Olah\u2019s interview on the 80,000\nHours\npodcast</a>\nsuper inspiring</p>\n</li>\n</ul>\n", "user": {"username": "Neel Nanda"}}, {"_id": "Z7r83zrSXcis6ymKo", "title": "\u2018Dissolving\u2019 AI Risk \u2013 Parameter Uncertainty in AI Future Forecasting", "postedAt": "2022-10-18T22:54:33.845Z", "htmlBody": "<h1>1 - Summary</h1><ul><li>This is an entry into the <a href=\"https://ftxfuturefund.org/announcing-the-future-funds-ai-worldview-prize/\">Future Fund AI Worldview contest</a>. The headline figure from this essay is that I calculate the <strong>best estimate of the risk of catastrophe due to out-of-control AGI is approximately 1.6%.</strong></li><li>However, the whole point of the essay is that \u201cmeans are misleading\u201d when dealing with conditional probabilities which have uncertainty spanning multiple orders of magnitude (like AI Risk). My preferred presentation of the results is as per the diagram below, showing <strong>it is more probable than not that we live in a world where the risk of Catastrophe due to out-of-control AGI is &lt;3%</strong>.</li><li>I completely understand this is a very radical claim, especially in the context of the Future Fund contest considering error bars of 7%-35% to be major updates. I will defend the analysis to a degree that I think suits such a radical claim, and of course make my model available for public scrutiny. All of my analysis is generated with <a href=\"https://www.dropbox.com/s/xf0kjl458u77a4o/AI%20Risk%20Model%20-%20Simple%20v5.xlsx?dl=0\">this</a> spreadsheet, which is available to download if you would like to validate any of my results.</li></ul><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8bfbaedabf6db8374559e5750034ac0f5233d0378b0c7058.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8bfbaedabf6db8374559e5750034ac0f5233d0378b0c7058.png/w_112 112w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8bfbaedabf6db8374559e5750034ac0f5233d0378b0c7058.png/w_192 192w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8bfbaedabf6db8374559e5750034ac0f5233d0378b0c7058.png/w_272 272w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8bfbaedabf6db8374559e5750034ac0f5233d0378b0c7058.png/w_352 352w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8bfbaedabf6db8374559e5750034ac0f5233d0378b0c7058.png/w_432 432w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8bfbaedabf6db8374559e5750034ac0f5233d0378b0c7058.png/w_512 512w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8bfbaedabf6db8374559e5750034ac0f5233d0378b0c7058.png/w_592 592w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8bfbaedabf6db8374559e5750034ac0f5233d0378b0c7058.png/w_672 672w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8bfbaedabf6db8374559e5750034ac0f5233d0378b0c7058.png/w_752 752w\"></figure><ul><li>Some general comments on the methods involved in this essay:<ul><li>My approach is to apply existing methods of uncertainty analysis to the problem of AI Risk to generate new findings, which I believe is a novel approach in AI Risk but a standard approach in other disciplines with high levels of uncertainty (like cost-effectiveness modelling).</li><li>Rather than a breakthrough insight about AI itself, this essay makes the case that a subtle statistical issue about uncertainty analysis means low-risk worlds are more likely than previously believed. This subtle statistical issue has not been picked up previously because there are systematic weaknesses in applying formal uncertainty analysis to problems in EA / rationalist-adjacent spaces, and the issue is subtle enough that non-systematised intuition alone is unlikely to generate the insight.</li><li>The intuitive explanation for why I report such different results to everyone else is that people\u2019s intuitions are likely to mislead them when dealing with multiple conditional probabilities \u2013 the probability of seeing a low-risk output is the probability of seeing <i>any</i> low-risk input when you are stacking conditional probabilities<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref338golu496e\"><sup><a href=\"#fn338golu496e\">[1]</a></sup></span>. I avoid my intuitions misleading me by explicitly and systematically investigating uncertainty with a statistical model.</li></ul></li><li>The results pass several sensitivity and validation checks, so I am confident that the mechanism I describe is real, and should affect AI funding decisions going forward. There are limitations with the exact parameterisation of the model, and I will explain and contextualise those limitations to the extent that I don\u2019t think they fundamentally alter the conclusion that distributions matter a lot more than has previously been understood.</li><li>The conclusion of this essay is that for the average AI-interested individual nothing much will change; everyone was already completely aware that there was at least order-of-magnitude uncertainty in their beliefs, so this essay simply updates people towards the lower end of their existing beliefs. For funding bodies, however, I make some specific recommendations for applying these insights into actionable results:<ul><li>We should be devoting significantly more resources to identifying whether we live in a high-risk or low-risk world. The \u2018average risk\u2019 (insofar as such a thing actually exists) is sort of academically interesting, but doesn\u2019t help us design strategies to minimise the harm AI will actually do in this world.</li><li>We should be more concerned with systematic investigation of uncertainty when producing forecasts. In particular, the radical results contained in this essay only hold under quite specific structural assumptions. A considered and systematised approach to structural uncertainty would be a high-value follow up to this essay about parameter uncertainty, but would need to be written by an expert in AI Risk to move beyond surface-level insight.</li><li>More generally, the analysis in this essay implies a reallocation of resources away from macro-level questions like, \u201cWhat is the overall risk of AI catastrophe?\u201d and towards the microdynamics of AI Risk. For example, \u201cWhat is the probability that humanity could stop an AI with access to nontrivial resources from taking over the world?\u201d is the best early differentiator between low-risk and high-risk worlds, but it is a notably under-researched question (at least on a quantitative level)</li></ul></li><li>Somewhat interestingly, the method used in this paper was initially developed by rationalist luminaries - Anders Sandberg, Eric Drexler and Toby Ord. Their paper is well worth a read on its own merits, and is available <a href=\"https://arxiv.org/abs/1806.02404\">here</a>.</li></ul><h1>2 - Introduction</h1><h2>2.1 Context</h2><p>As part of the Effective Altruism Red Teaming Contest I wrote <a href=\"https://forum.effectivealtruism.org/posts/CuuCGzuzwD6cdu9mo/methods-for-improving-uncertainty-analysis-in-ea-cost\">an article</a> arguing that there were systematic limitations with uncertainty analysis within the EA space. The judges were extremely kind to highlight some of the stronger features of the article in their commentary, but they did note that I failed to adequately defend the <i>importance</i> of uncertainty analysis, particularly in areas in which highly sophisticated measurement and quantification are less central. While the errors of explanation were entirely mine, this article will hopefully address that gap, and function as an example of the sorts of insights that can only come from systematic application of the uncertainty analysis toolkit.&nbsp;</p><p>This essay focuses on <i>parameter uncertainty</i>, which is to say uncertainty that arises because we are unsure about numerical values for some of our inputs. There are some unique features of models of AI Risk which mean a complete and systematic investigation of parameter uncertainty leads to some very surprising and potentially important results. I will also try to complete an essay on structural uncertainty which will follow a more conventional approach and derive more conventional conclusions, which will more directly function as an example of what might be achieved with systematic analysis of uncertainty in these sorts of situations.</p><p>As a small caveat, the state of AI Risk analysis has advanced a very great deal since I was last properly immersed in it. Although I have taken steps to minimise the impact my ignorance has on the analysis (specifically by doing as much background reading as was practical in the time I had to draft the essay), it is extremely likely that I am still using terms which are a decade out of date in places. This is an essay about a statistical concept more than about any particular idea in AI Risk, so although the archaic language is probably jarring I wouldn\u2019t expect it to lead to the sort of oversight which fundamentally alters conclusions. Nevertheless, apologies for the anachronisms!</p><h2>2.2 Summary of claims</h2><p>The general claim in this essay is that <i>distribution of risk</i> around central estimates of AI catastrophe is at least as important as the central estimates themselves. This is because most models of AI catastrophe have a number of discrete steps, all of which need to come true in order for bad outcomes to occur. This means worlds where risk is very low will be systematically overrepresented compared to worlds where risk is very high. In this essay I put an optimistic slant on this (\u201cWe probably live in a low-risk world!\u201d), but a pessimist might argue that this mathematically means that in worlds where risk is <i>not</i> low then it is likely to be very high compared to what we expect.</p><p>A particularly striking demonstration of this is that the Future Fund give a 3% risk of AGI Catastrophe due to an out-of-control AGI as an example of a probability so outlandish that it would result in a major shift of direction for them; in fact, <strong>it is more probable than not that we live in a world where the risk of AGI Catastrophe of this kind is &lt;3%</strong>.</p><p>A high-level summary of the structure of this essay is given below.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87d006ec0a34598068cebbd221b68e9b6f5749bf4e56f00a.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87d006ec0a34598068cebbd221b68e9b6f5749bf4e56f00a.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87d006ec0a34598068cebbd221b68e9b6f5749bf4e56f00a.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87d006ec0a34598068cebbd221b68e9b6f5749bf4e56f00a.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87d006ec0a34598068cebbd221b68e9b6f5749bf4e56f00a.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87d006ec0a34598068cebbd221b68e9b6f5749bf4e56f00a.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87d006ec0a34598068cebbd221b68e9b6f5749bf4e56f00a.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87d006ec0a34598068cebbd221b68e9b6f5749bf4e56f00a.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87d006ec0a34598068cebbd221b68e9b6f5749bf4e56f00a.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87d006ec0a34598068cebbd221b68e9b6f5749bf4e56f00a.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87d006ec0a34598068cebbd221b68e9b6f5749bf4e56f00a.png/w_940 940w\"></figure><p>I don\u2019t want to either oversell or undersell the claims being made in this essay. It looks only at one possible type of AI Risk (an out-of-control AGI permanently disempowering humanity), and has a number of structural and data limitations that should prompt caution before anyone makes any irrevocable decisions off the back of the analysis. On the other hand, I am quite confident the mechanism discussed in this essay is sound; if it is genuinely true that (elements of) the structure of AI Risk can be described as a series of weighted coin tosses, all of which have to come up \u2018heads\u2019 in order for this particular Catastrophe to be observed, then the conclusion is mathematically inevitable; the AI Risk community is systematically overestimating AI Risk, probably because it is extremely hard to intuitively reason about asymmetric uncertainty distribution so people are making systematic errors of intuition. Part of the length of this essay is trying to motivate an intuitive understanding of this mechanism so that even if my specific model of AI Risk is later found to be in error the core insight about distributions of risk is preserved.</p><p>It might be worth spending a moment to emphasise what this essay does <i>not</i> claim. A claim that we are probably living in a world where the risk of AGI Catastrophe of a certain kind is low does not mean that the risk of AGI Catastrophe is negligible. Low-probability high-impact events are still worth thinking about and preparing for, especially since the Catastrophe described in this essay is only one possible way AI could be bad for humanity. My background is health economics, so I tend to think of interventions as being cost-effective or not: most interventions to lower the risk of AGI Catastrophe that were cost-effective before this essay will remain cost-effective afterwards, since it is a good guess we are nowhere near the productivity frontier of AGI Risk mitigation given how young the discipline is. Moreover, a 1.6% chance of extremely bad AGI outcomes is actually not all that low a probability in the context of how catastrophic the disempowerment of all humanity would be. If a doctor told me a bone marrow transplant to save the life of a stranger carried a 1.6% chance of my death, then I would have to think very hard about whether I wanted to risk the procedure. Fundamentally, this essay is an argument in favour of the importance of uncertainty analysis and examining <i>ex post</i> distributions of uncertainty, rather than making any claims against the importance of AGI Risk.</p><h1>3 - Methods</h1><h2>3.1 Literature review &amp; Model Structure</h2><p>Creating a model of the AI Risk decision space is not a trivial problem, and certainly not one a person with lapsed AI Risk credentials like myself was likely to get right first try. In order to identify the state-of-the-art in AI Risk decision modelling I performed a review of the EA / LessWrong forum archives and recorded any relevant attempt at describing the decision space. By \u2018relevant\u2019 I mean that the author explicitly laid out the logical interrelationship of the steps between now and a possible AI Catastrophe, and then assigned probabilities to those intermediate steps (in other words it was possible for me to replicate the model of the world the author had used in order to reach the same conclusions as the author). This is a significantly more restrictive filter than it might initially appear \u2013 it excludes canonical pieces of writing on AI Risk such as Yudkowsky (<a href=\"https://intelligence.org/files/AIPosNegFactor.pdf\">2008</a>) or Bostrom (<a href=\"https://www.lesswrong.com/posts/QDmzDZ9CEHrKQdvcn/superintelligence-reading-group\">2014</a>) because neither of these include probability estimates and it is not clear they are even supposed to be read as containing a strict logical model of the interrelationships between steps to different AI futures.</p><p>I found eleven relevant models, although this slightly overstates the findings; I found Carlsmith (<a href=\"https://arxiv.org/abs/2206.13353\">2021</a>) and then ten reviews of Carlsmith (2021) which offered their own probability estimates without offering a significantly revised model of the decision problem. I\u2019ll refer to this conceptual map of the AI Risk decision space as the \u2018Carlsmith Model\u2019, in order to differentiate it from the specific probabilities which populate the model given in Carlsmith (2021).&nbsp;</p><p>The Carlsmith Model is a six-parameter deterministic decision tree which aims to estimate the probability that AI catastrophe occurs before 2070. The parameters correspond to the probability that each step on the path to catastrophe occurs, with (implicitly) all other outcomes ending up in an absorbing state that we could roughly label \u2018No AI catastrophe\u2019. In Carlsmith (2021)\u2019s initial parameterisation, the model gives an output of approximately 5% likelihood of catastrophe. Carlsmith notes that they have later revised this estimate upwards to approximately 10%, but as far as I can see didn\u2019t publish what changes in the intermediate parameters led to this conclusion. There is some limited uncertainty analysis around these central estimates, but this analysis was not systematic and focussed more on scenario analysis than parameter uncertainty. The model is represented in the figure below.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec229dd119bec8546957f1ddd780ed093ab4573cae4f77c1.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec229dd119bec8546957f1ddd780ed093ab4573cae4f77c1.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec229dd119bec8546957f1ddd780ed093ab4573cae4f77c1.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec229dd119bec8546957f1ddd780ed093ab4573cae4f77c1.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec229dd119bec8546957f1ddd780ed093ab4573cae4f77c1.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec229dd119bec8546957f1ddd780ed093ab4573cae4f77c1.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec229dd119bec8546957f1ddd780ed093ab4573cae4f77c1.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec229dd119bec8546957f1ddd780ed093ab4573cae4f77c1.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec229dd119bec8546957f1ddd780ed093ab4573cae4f77c1.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec229dd119bec8546957f1ddd780ed093ab4573cae4f77c1.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec229dd119bec8546957f1ddd780ed093ab4573cae4f77c1.png/w_940 940w\"></figure><p>I validated this approach by asking the EA Forums whether they knew of any sources which I had missed, but this revealed no new quantitative models of AI Risk<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9f8pdahy7yr\"><sup><a href=\"#fn9f8pdahy7yr\">[2]</a></sup></span>. It did direct me to approximately twenty relevant estimates of individual steps in the Carlsmith Model \u2013 mostly in Michael Aird\u2019s <a href=\"https://www.lesswrong.com/posts/jyRbMGimunhXGPxk7/database-of-existential-risk-estimates\">database of existential risk</a> \u2013 which were not full specifications of the decision space themselves, but might inform individual parameters in the Carlsmith Model<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkx4lym8m3h\"><sup><a href=\"#fnkx4lym8m3h\">[3]</a></sup></span>. Overall, it makes sense to me that the Carlsmith Model would be the most appropriate model of AI Risk for this question; the framing of the question in the contest announcement specifically highlights Carlsmith (2021) as an example of best practice.</p><p>Although the Future Fund question is framed in a way that makes it clear the Carlsmith Model <i>approach</i> is valuable, it also distances itself from the exact probabilities given by Carlsmith (2021). In particular, the Future Fund give their central probability of catastrophe given the invention of AI as 15%, approximately three times higher than Carlsmith (2021). This is not necessarily a contradiction; Carlsmith (2021) and the Future Fund ask subtly different questions:</p><ul><li>Unlike the Future Fund question, the Carlsmith Model considers the risk of catastrophe <strong>by</strong> 2070 rather than the risk of catastrophe at any point in the future after AI is invented. A number of reviewers note that this is a slightly awkward restriction on the Carlsmith Model.</li><li>Unlike the Future Fund question, the Carlsmith Model does not distinguish between catastrophe due to an out-of-control AI and a catastrophe due to an in-control AI being misused. This makes sense given what Carlsmith is trying to accomplish, but is a limitation given what the Future Fund are trying to accomplish (specifically, the two outcomes suggest radically different strategies for funding decisions)</li><li>The Carlsmith Model is not conditional on AI being invented by 2070 (i.e. there is a step in the Carlsmith Model which is abstracted away in the Future Fund contest), so even if they agreed completely the Future Fund would estimate a higher probability of catastrophe, because AI is sometimes not invented in the Carlsmith Model.</li></ul><p>Since this essay considers the Future Fund question explicitly (rather than a repeat analysis of Carlsmith), the specific parameterisation of Carlsmith (2021) was not appropriate, and primary data collection was required to parameterise the model.</p><h2>3.2 Model Parameterisation</h2><p>Due to the above limitations of applying the parameters of Carlsmith and his reviewers directly to the Future Fund question, I surveyed the AGI Risk Community<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1k7orst7epo\"><sup><a href=\"#fn1k7orst7epo\">[4]</a></sup></span>&nbsp;with <a href=\"https://forms.gle/kLYtynp3FYcxkPZc8\">this</a> instrument, which asks exactly the correct questions to match the Future Fund\u2019s question to the structure of the Carlsmith Model. In particular, note that the instrument does not condition on the catastrophe occurring by 2070, and also includes an explicit differentiator between catastrophe due to an in-control AI and an out-of-control AI. I am not a triallist and have no particular expertise in survey design beyond hobbyist projects, so in hindsight there are some design errors with the survey. To this end I would normally suggest that an expert replication of the survey would be valuable, except I think the <a href=\"https://www.lesswrong.com/s/aERZoriyHfCqvWkzg\">MTAIR project</a> will be so much better than what I have done here that a replication would be a waste of resources; it is reasonable to use my numbers for now, until MTAIR gives a \u2018definitive\u2019 view on what the correct numbers should be. However, it is also prudent to be aware of the limitations with my numbers until MTAIR reports - the biggest weaknesses that I have identified are:</p><ul><li>The most major omission was not explicitly conditioning all estimates on the possibility of catastrophic existential risk. Some respondents did condition their estimates on this, some respondents assumed I implicitly meant \u2018conditional on no other existential risk occurring\u2026\u2019. This is not the end of the world<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefl3f5i9s6os\"><sup><a href=\"#fnl3f5i9s6os\">[5]</a></sup></span>&nbsp;because it can be routed around with good structural uncertainty analysis, but on average responses will be slightly too high.</li><li>There was ambiguity in some of the questions. For example, one question asked about \u2018Alignment\u2019 and it was clear some respondents had differing ideas about what that meant in practice. Someone more expert on AI than me wouldn\u2019t have made those errors, and could probably have identified a better set of questions to ask than just covering a summary survey of Carlsmith (2021).</li><li>I didn\u2019t offer an opportunity to specify a distribution when respondents gave their answers. This was a deliberate omission because I was going to generate the distributions myself with the SDO method I describe below. However, some respondents described how they had quite complex responses (e.g. bimodal responses) and couldn\u2019t give genuinely accurate answers without the ability to specify distributions of uncertainty.</li><li>The survey doesn\u2019t force people to assume AI is invented before 2070, which the Future Fund would like you to assume. This affects seven responses which estimate AI will come later than 2070, plus three more which estimate AI will be invented in 2070 exactly. In theory this could have affected responses because people\u2019s risk estimates could be correlated \u2013 for example AI being invented in 2500 gives more time for other sources of x-risk to cause a catastrophe. In practice there wasn\u2019t a significant difference between pre-2070 and post-2070 responders so I have included all responses together.</li></ul><p>42 people took the survey. Of these, 14 self-identified as experts - either fully or marginally - and I have separated out responses from these individuals as a separate subgroup in the results.</p><p>Generally, data quality was good. The most major issues with data quality was inconsistency with how percentage values were entered (\"50%\" vs \"50\" vs \"0.5\"). A validation question at the end ensured that all of these errors were caught and corrected. Similarly, some individuals wrote short explanations of their estimates in the response box itself rather than the comment box, but these were also easy to detect and resolve. One rather lovely thing about rationalist-adjacent communities is that when people fill out a survey with odd data they are self-aware enough to recognise the issue and kind enough to explain exactly what their thought process was leading to it. So, for example, when I asked for the year people thought AGI would be invented and someone put a date in the past, they very helpfully explained that they knew this would normally be rejected as junk data but in this particular case they really did mean they thought it already existed!</p><p>With this in mind, only very minor data cleaning was undertaken. Aside from normalising probabilities and removing explanatory text, the only data adjustment was a slight compensation for entirely certain estimates. Specifically, any estimate which gave probabilities of 100% or 0% was adjusted to 99% and 1% respectively. This had to be performed in order to allow the conversion of probabilities to odds, and was justified on the basis that entries of 100% or 0% must have been intended as a shorthand for \u2018almost completely certain\u2019 rather than actually expressing complete certainty<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefponcxbdiiyc\"><sup><a href=\"#fnponcxbdiiyc\">[6]</a></sup></span>. No other result was adjusted, which means that a reported probability of 99.9% ended up being more certain than a reported probability of 100% in the final analysis. That is slightly inelegant, but it won\u2019t materially affect results generated with the SDO method described in the next section. It might slightly alter some summary statistics generated without the SDO method, but since the whole point of this essay is that those summary statistics are misleading I haven\u2019t added any correction or validation around this point.</p><p>Summary results of the survey are reported below. I will update this sentence with a link to the full results dataset once I have had a chance to anonymise them properly<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkgb97wnu98\"><sup><a href=\"#fnkgb97wnu98\">[7]</a></sup></span>.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/43eb4251b5630ac9a22c15ca822dfd004305e272be717520.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/43eb4251b5630ac9a22c15ca822dfd004305e272be717520.png/w_154 154w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/43eb4251b5630ac9a22c15ca822dfd004305e272be717520.png/w_234 234w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/43eb4251b5630ac9a22c15ca822dfd004305e272be717520.png/w_314 314w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/43eb4251b5630ac9a22c15ca822dfd004305e272be717520.png/w_394 394w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/43eb4251b5630ac9a22c15ca822dfd004305e272be717520.png/w_474 474w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/43eb4251b5630ac9a22c15ca822dfd004305e272be717520.png/w_554 554w\"></figure><p>In general, these responses are consistent with the Future Fund\u2019s position on the risk of AI Catastrophe, in the sense that all approaches give a number which is in the 10%-50%-ish order of magnitude. The responses are generally slightly higher than the Future Fund estimate, and I\u2019m not sure why that is \u2013 it could be that the Future Fund is unusually conservative on AI Risk by community standards, or it could be a bias I accidentally embedded into the survey and have not identified. Experts are more worried about AI than non-experts, but this seems easily explainable by a selection bias (only people who think AI is very worrying would invest the time and resources to become expert on it).&nbsp;</p><p>People\u2019s responses are quite internally valid \u2013 their overall estimate of the risk of AI Catastrophe is roughly the same as the estimate made by multiplying each individual step in the sequence together. Furthermore, the agreement between the Expert and the Full dataset is overall good on specific questions. The only exception to this is that there is a substantial difference between the Expert and Non-Expert view of AGI scaling \u2013 that is, the probability that an AGI that is given an initial endowment of some high-power resource will convert that endowment into something that can subjugate all of humanity. Non-experts give roughly even odds that we will be able to \u2018correct\u2019 a misbehaving AI, whereas Experts suggest that 3 times in 4 we will not be able to fight / contain / bargain with a misbehaving AI. This is the only major difference between the two groups, and appears to drive almost all of the difference in overall result between the Full and Expert dataset.&nbsp;</p><p>As a point of analysis, one interpretation of this is that what <i>makes</i> people worried about AGI is fear that we cannot contain a Misaligned AI. That is (pending confirmation of the result) the most effective way for Experts to evangelise AGI Risk would be taking people who are complacent about our ability to contain a Misaligned AI and convincing them it would actually be 1-in-4 hard rather than 1-in-2 hard. A sketch model of the problem space is that non-worriers think of AI deployment as being a bit like virus research \u2013 for sure governments are going to do it, for sure someone is going to do something stupid and unleash a pandemic on the world, but probably someone will invent a vaccine or something and everything will be fine. There\u2019s no point trying to convince Non-Experts that Alignment will be tricky because they already agree with you!</p><p>Overall, my view is that the Full Survey Dataset is probably more reasonable as a base case than the Expert Survey Dataset. There isn\u2019t really any objective definition of \u2018expert\u2019 that would mean that we have any reason to trust the Expert Survey Dataset more, and the Full Survey Dataset gives a response which is closer to what the Future Fund says is its central estimate, making it more appropriate for this particular application. My main reason for including the \u2018Expert\u2019 filter was in case Non-Experts gave completely invalid / inexplicable answers, but this did not actually happen \u2013 the demographics of the AI Risk Community obviously skew more conscientious than the general population.&nbsp;</p><p>Finally, I include a brute-force sense check on my survey work \u2013 as a sensitivity analysis I simply multiplied the implied odds given by Carlsmith (2021) and his reviewers by 1.5 and set the \u2018probability AGI invented\u2019 parameter to 100% with no uncertainty. This gives the same overall risk of catastrophe as the Future Fund, and so might be in approximately the ballpark of the figures we would get if we asked Carlsmith and their reviewers to repeat the exercise again with precisely the question Future Fund was asking. To be clear, this is a completely <i>ad hoc</i> step with no methodological justification: if Carlsmith intended for any of his estimates to be correlated, I have just broken this correlation. If he had better evidence for some parameters than others I have just deleted this evidence. And so on.</p><p>Many thanks to the 42 respondents who have made the forthcoming analysis possible by sharing their expertise and insight.&nbsp;</p><h2>3.3 Statistical methods</h2><h3>3.3.1 Motivation</h3><p>As described in the introduction, the statistical methods used in this essay have an interesting heritage. I am lifting the method entirely from Sandberg, Drexler and Ord (<a href=\"https://arxiv.org/abs/1806.02404\">2018</a>) \u2013 hereafter \u2018SDO\u2019 - and these three authors are all rationalist-adjacent in different ways. As far as I know they are all actively interested in AI Risk, so I am a little surprised that they have not applied their method to the problem described in this essay. My guess is that they are too professional to rely on pure survey data as I have done and without this survey there is currently not enough data to use their method.</p><p>SDO\u2019s insight was that for a certain class of problem it is extremely dangerous to implicitly treat parameter uncertainty as not existing. They demonstrate this by \u2018dissolving\u2019 the Fermi Paradox. The Fermi Paradox is the strange contradiction between the fact that reasonable estimates for the number of intelligent civilisations in the universe who should be trying to contact us put the number very high, but when we actually observe the universe we do not see any signs of intelligent life other than our own. SDO\u2019s argument is that all prior investigations have treated parameter uncertainty as though it doesn\u2019t exist, and as a result tried to calculate the <strong>number of alien civilisations&nbsp;</strong>we should see. However, SDO argue that this is an incorrect framing; what we are interested in is the <strong>probability we see any alien civilisations at all</strong>.&nbsp;</p><p>This is confusing, and I don\u2019t think trying to pretend otherwise is helpful. Surely, you might think, a high <strong>number</strong> of alien civilisations (on average) should translate to a high <strong>probability</strong> of being contacted by an alien civilisation? What made it \u2018click\u2019 for me was rereading a comment by <a href=\"https://slatestarcodex.com/2018/07/03/ssc-journal-club-dissolving-the-fermi-paradox/\">Scott Alexander</a> on this study:</p><blockquote><p><i>Imagine we knew God flipped a coin. If it came up heads, He made 10 billion alien civilization. If it came up tails, He made none besides Earth. Using our one parameter [equation], we determine that on average there should be 5 billion alien civilizations. Since we see zero, that\u2019s quite the paradox, isn\u2019t it?&nbsp;</i></p><p><i>No. In this case the mean is meaningless. It\u2019s not at all surprising that we see zero alien civilizations, it just means the coin must have landed tails.</i></p></blockquote><p>I wouldn\u2019t expect to be able to communicate statistics better than Scott Alexander, but I\u2019ve included a second possible construction of this point (my own, this time) in case a different perspective helps explain the issue:</p><blockquote><p><i>You work in retail. A survey shows you that the average human has less than two legs (because some people have one leg, or no legs). You order all your trousers to be lopsided to account for the fact that a trouser with 1.97 legs matches the average human best. You are surprised when none of your trousers sell. The average <strong>number</strong> of legs per human matters less than the <strong>distribution&nbsp;</strong>of those legs.</i></p></blockquote><p>Part of the reason this is unintuitive is that for most probability problems we come across in everyday life, the number / probability distinction is basically irrelevant. If we throw a large number of dice onto a table then the <strong>number</strong> of sixes we see relative to other numbers is a pretty good proxy for the <strong>probability</strong> that we see a six on the next roll. So, most problems are not going to \u2018dissolve\u2019 in the way SDO make the Fermi Paradox behave. The specific features of the Fermi Paradox that make it very suitable for the SDO method are:</p><ol><li>We reach a final estimate by multiplying <i>conditional</i> probabilities together (ie probabilities for events that depend on whether earlier events in a chain of logic come to pass or not)</li><li>We are uncertain about those probabilities, often to the extent that our uncertainty spans several orders of magnitude</li><li>There is an enormous, disjunctive, difference between one possible outcome and all other possible outcomes</li></ol><p>AI Risk clearly meets points 2 and 3 (although our uncertainly probably spans fewer orders of magnitude than for some parameters in the Fermi Paradox) and my literature review suggested that the most generally accepted model of AI Risk meets point 1. Therefore, we might expect AI Risk to \u2018dissolve\u2019 in a similar way to the Fermi Paradox. What we need to prove this is a method for systematising this observation.</p><h3>3.3.2 Synthetic point estimates</h3><p>The method used in SDO to make the Fermi Paradox \u2018dissolve\u2019 is described as \u2018synthetic point estimates\u2019. The authors review the literature on the Fermi Paradox, and extract any estimate of any parameter which makes up their equivalent of the Carlsmith Model (the '<a href=\"https://en.wikipedia.org/wiki/Drake_equation\">Drake Equation</a>'). They then populate a \u2018synthetic\u2019 Equation of their own by randomly picking one estimate per parameter. They do this many times to form a \u201ccollective view of the research community\u2019s uncertainty\u201d. The diagram below might help illustrate this process.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b6aa2180f2178a89cc7d7be447c3bf51ee1865ecb1bac63e.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b6aa2180f2178a89cc7d7be447c3bf51ee1865ecb1bac63e.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b6aa2180f2178a89cc7d7be447c3bf51ee1865ecb1bac63e.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b6aa2180f2178a89cc7d7be447c3bf51ee1865ecb1bac63e.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b6aa2180f2178a89cc7d7be447c3bf51ee1865ecb1bac63e.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b6aa2180f2178a89cc7d7be447c3bf51ee1865ecb1bac63e.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b6aa2180f2178a89cc7d7be447c3bf51ee1865ecb1bac63e.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b6aa2180f2178a89cc7d7be447c3bf51ee1865ecb1bac63e.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b6aa2180f2178a89cc7d7be447c3bf51ee1865ecb1bac63e.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b6aa2180f2178a89cc7d7be447c3bf51ee1865ecb1bac63e.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b6aa2180f2178a89cc7d7be447c3bf51ee1865ecb1bac63e.png/w_940 940w\"></figure><p>It might be helpful thinking about what the SDO method is going to show in sketch terms before letting it loose on the Full Survey Dataset \u2013 this might help motivate the intuition behind the rather startling headline results. Let\u2019s look at the distribution the SDO method would find for one particularly interesting question, \u201cConditional on [being exposed to high-impact inputs] what is the probability AGI will scale (in aggregate) to the point of permanently disempowering roughly all of humanity?\u201d. The graph below shows every survey response to this question in orange blobs (obviously including some overlapping responses). The blue line was my attempt to create a smooth distribution through these points, to help show where there are overlapping points better&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefau756kiwicl\"><sup><a href=\"#fnau756kiwicl\">[8]</a></sup></span>.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2cc86ed4fb8b6a5670ad32bb17645de130c2cf661b552ff.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2cc86ed4fb8b6a5670ad32bb17645de130c2cf661b552ff.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2cc86ed4fb8b6a5670ad32bb17645de130c2cf661b552ff.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2cc86ed4fb8b6a5670ad32bb17645de130c2cf661b552ff.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2cc86ed4fb8b6a5670ad32bb17645de130c2cf661b552ff.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2cc86ed4fb8b6a5670ad32bb17645de130c2cf661b552ff.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2cc86ed4fb8b6a5670ad32bb17645de130c2cf661b552ff.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2cc86ed4fb8b6a5670ad32bb17645de130c2cf661b552ff.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2cc86ed4fb8b6a5670ad32bb17645de130c2cf661b552ff.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2cc86ed4fb8b6a5670ad32bb17645de130c2cf661b552ff.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2cc86ed4fb8b6a5670ad32bb17645de130c2cf661b552ff.png/w_935 935w\"></figure><p>Imagine for the sake of argument that the Carlsmith Model was just this distribution of points repeated six times. The most likely outcome of sampling randomly from the pool of possible answers to the question is 60%, so we might very roughly imagine that the output of the toy-Carlsmith model would 60% six times in a row for a toy-probability of AI Catastrophe of about 5%. The distribution of points is very roughly symmetric, so we\u2019d expect that for every time the model sampled high (70%, say) it would likely sample an equivalent low score a few draws later (50%, say) and so the uncertainty would cancel itself out (we might expect) \u2013 60% * 60% * 60% is very roughly the same number as 50% * 60% * 70% so there is some empirical basis for making this assumption (we might incorrectly conclude). Doing this sampling process just six times isn\u2019t really enough time for central limit theorem to assert itself and so we couldn\u2019t say with confidence that every high draw <i>would certainly</i> be cancelled out, but on average and for practical applications we could be reasonably confident that high and low draws are equally likely.</p><p>To be clear, this chain of logic is incorrect \u2013 however it is implicitly the chain of logic followed by every single structural model of AI Risk I came across during the literature review, including many discussions where explicit probabilities weren\u2019t given but it was clear that the author had this sort of structure in mind.</p><p>In the diagram below I have colour-coded the diagram above to make this point. The red area are draws which are low enough that they make a difference to the overall result (&lt;10%), the orangey-yellow area are draws middling enough that they don\u2019t really affect the final result on expectation and the green area are draws high enough that we might expect them to cancel out the draws in the low area (&gt;90%)</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f57a117a0c72316c4f697174e98ba81078f94bf9ded677db.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f57a117a0c72316c4f697174e98ba81078f94bf9ded677db.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f57a117a0c72316c4f697174e98ba81078f94bf9ded677db.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f57a117a0c72316c4f697174e98ba81078f94bf9ded677db.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f57a117a0c72316c4f697174e98ba81078f94bf9ded677db.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f57a117a0c72316c4f697174e98ba81078f94bf9ded677db.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f57a117a0c72316c4f697174e98ba81078f94bf9ded677db.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f57a117a0c72316c4f697174e98ba81078f94bf9ded677db.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f57a117a0c72316c4f697174e98ba81078f94bf9ded677db.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f57a117a0c72316c4f697174e98ba81078f94bf9ded677db.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f57a117a0c72316c4f697174e98ba81078f94bf9ded677db.png/w_940 940w\"></figure><p>The \u2018trick\u2019 here is that the high and low draws <i>do not</i> cancel each other out. In the diagram above, a single draw in the red area functionally means the final probability will be <i>much</i> less than 5%. Imagine for example a scenario where four down-the-middle 60% draws were made, and then a draw of 10%. The overall probability would be 60%* 60%* 60%* 60%* 10% = roughly 1%. So already the probability of the event after five samples is less than our naively anticipated probability after six samples! You might object that this is unfair and that I have just selected a low draw with no compensating high draw. So, let\u2019s assume our final draw is a perfect 100% probability on the final parameter of the toy-Carlsmith model. That means we take our slightly less than 1%, multiply it by 100%... and of course it doesn\u2019t go upwards at all; 100% of any probability is just the probability you started with!</p><p>For me, this is the absolutely critical mechanic to understand in order to grok what is about to happen to convert a na\u00efve 18.7% estimate of risk from the survey with with no uncertainty into a synthesised 1.6% chance of risk with parameter uncertainty \u2013 intuitively we think of conditional probabilities as being like numbers, but they don\u2019t actually act like numbers in all situations. If the graph above represented my productivity at the widget-factory each day then a bad day (red area) on Monday <i>genuinely could</i> be offset with a good day (green area) on Tuesday. However, because these are conditional probabilities, a bad day on Monday starts me off in a worse position on Tuesday \u2013 the best I can do is try to hang on to my already low endowment.</p><p>So conceptually, what we are really interested in with the synthetic point estimate is not so much the central estimate for the probability of continuing on to the next step, but rather the distribution of estimates within each parameter (you might notice a bit of a recurring theme here\u2026) Specifically, we are interested in the probability that any particular parameter is sampled low enough that it completely throws off all subsequent calculations. This is significantly more likely to occur when estimates span multiple orders of magnitude, and this is why the SDO method is particularly suitable for some applications (Fermi Paradox, AI Risk) and no better than a simple average in some other applications (rolling dice, predicting sports team victories)</p><h1>4 - Results</h1><h2>4.1 Base case</h2><h3>4.1.1 Main results</h3><p>The main results are based on 5000 simulations of the Full Survey dataset, using the Synthetic Point Estimate method from SDO. The outputs of the base case are displayed below. The panel in the top left represents the probability that we live in a world with one of a number of different \u2018categories\u2019 of risk, and the panel in the top right offers summary statistics of this graph. The interpretation of the line graph at the bottom is slightly fiddly; the area under the curve between a 0% probability of catastrophe and x% probability of catastrophe represents the fraction of possible worlds with a less than x% risk. Please also note the log scale.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d17a0a78a3ccd2f477f6485f2c1857875359acb77e3c2dc8.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d17a0a78a3ccd2f477f6485f2c1857875359acb77e3c2dc8.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d17a0a78a3ccd2f477f6485f2c1857875359acb77e3c2dc8.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d17a0a78a3ccd2f477f6485f2c1857875359acb77e3c2dc8.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d17a0a78a3ccd2f477f6485f2c1857875359acb77e3c2dc8.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d17a0a78a3ccd2f477f6485f2c1857875359acb77e3c2dc8.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d17a0a78a3ccd2f477f6485f2c1857875359acb77e3c2dc8.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d17a0a78a3ccd2f477f6485f2c1857875359acb77e3c2dc8.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d17a0a78a3ccd2f477f6485f2c1857875359acb77e3c2dc8.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d17a0a78a3ccd2f477f6485f2c1857875359acb77e3c2dc8.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d17a0a78a3ccd2f477f6485f2c1857875359acb77e3c2dc8.png/w_940 940w\"></figure><p>The \u2018headline\u2019 result from this analysis is that the geometric mean of all synthetic forecasts of the future is that the Community\u2019s current best guess for the risk of AI catastrophe due to an out-of-control AGI is <strong>around 1.6%</strong>. You could argue the toss about whether this means that the most reliable \u2018fair betting odds\u2019 are 1.6% or not (Future Fund are slightly unclear about whether they\u2019d bet on simple mean, median etc and both&nbsp;of these figures are higher than the geometric mean of odds<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz60blow7e1\"><sup><a href=\"#fnz60blow7e1\">[9]</a></sup></span>). However, the whole point of this essay is to encourage people to move beyond summary statistics and do systematic uncertainty analysis, so I don\u2019t want to over-emphasise the 1.6% figure.&nbsp;</p><p>In reality, the most important feature of this analysis is the panel in the top left, showing a very high probability that the world we actually live in has a very low risk of AI Catastrophe due to an out-of-control AGI. About 38% of all simulations fall into this category, and another 13% or so before the risk reaches 3%. I think the best conclusion of the Survey dataset is that <strong>it is most likely that we live in a world where AI Risk is very low (&lt;3%).&nbsp;</strong>This accurately captures and characterises the distribution of possible worlds we might experience, and I think also helps make the \u2018so what\u2019 of the analysis more concrete.</p><p>A clear implication of this is that there are some worlds where the risk of catastrophe must be absolutely terrifying, to balance out this probability mass of low-risk worlds so that end up with a simple average probability close to the Future Fund\u2019s central estimate. In fact, this is what we see \u2013 around 5% of the probability mass covers worlds where the risk of out-of-control AI catastrophe is 50% or greater (i.e. we are as likely to be catastrophically disempowered by an AGI via this one single mechanism as not). Each of these \u2018high risk\u2019 worlds cancels out a large number of \u2018low risk\u2019 worlds unless you statistically correct for that effect, which one reason why the simple mean ends up so much higher than the geometric mean of odds. So whereas I have placed an optimistic slant on the results (\u201cIt is highly likely we live in a world where AI Risk is low\u201d), a pessimist might say, \u201c\u2026but on learning we don\u2019t live in a low-risk world, we also learn that AI Risk is much, much higher than we expected before\u201d.</p><p>Please also note that my computer stubbornly refuses to calculate the true geometric mean of odds of the distribution by taking the 5000<sup>th</sup> root of the results, so I\u2019ve used an approximation. However, this approximation is close enough to the actual value that you can treat it as being correct for the purpose of discussion.</p><h3>4.1.2 Interpretation</h3><p>One important question we might therefore want to ask is, \u201cDo we actually live in one of the 50% of low-risk worlds? Or do we actually live in one of the one of the worlds where the risk of AI Catastrophe is <i>worse</i> than the Future Fund estimate?\u201d</p><p>This is actually a remarkably difficult question to answer \u2013 I answered an analogous question as part of my PhD and it took me deep into machine learning territory I never want to revisit. There are some fairly robust statistical approximations we can use, and even better there are some nice visualisations answering the same question. The graphs below display probability density functions for each question asked in the Carlsmith Model. The density function for \u2018safe\u2019 worlds (risk &lt;3%) is graphed in green, the density function for \u2018dangerous\u2019 worlds (risk &gt;35%) is graphed in red. What we are looking for is a different shape between the green and red lines that we could use to infer a difference between \u2018safe\u2019 and \u2018dangerous\u2019 worlds \u2013 so for example \u201cThere will be strong incentives to build APS systems\u201d is not very interesting to us because the lines basically overlap, but \u201cAlignment is hard\u201d is potentially interesting because there is a big probability mass on the left of the \u2018safe\u2019 graph which does not exist in the \u2018dangerous\u2019 graph. What this means is that if we can refine our understanding of whether \u201cAlignment is hard\u201d to be confident it is at or below 20%, we can be fairly confident we live in a \u2018safe\u2019 world \u2013 almost no \u2018dangerous\u2019 worlds have a low risk that \u201cAlignment is hard\u201d and very many \u2018safe\u2019 worlds do.</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><strong>Statement</strong></td><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><strong>Distribution</strong></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt\">There will be strong incentives to build APS systems</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b747962472ec98af37b12d18841f861615d5384fa424a92c.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b747962472ec98af37b12d18841f861615d5384fa424a92c.png/w_90 90w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b747962472ec98af37b12d18841f861615d5384fa424a92c.png/w_180 180w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b747962472ec98af37b12d18841f861615d5384fa424a92c.png/w_270 270w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b747962472ec98af37b12d18841f861615d5384fa424a92c.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b747962472ec98af37b12d18841f861615d5384fa424a92c.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b747962472ec98af37b12d18841f861615d5384fa424a92c.png/w_540 540w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b747962472ec98af37b12d18841f861615d5384fa424a92c.png/w_630 630w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b747962472ec98af37b12d18841f861615d5384fa424a92c.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b747962472ec98af37b12d18841f861615d5384fa424a92c.png/w_810 810w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b747962472ec98af37b12d18841f861615d5384fa424a92c.png/w_830 830w\"></figure></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt\">Alignment is hard</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cb1f378011168cf42fdb284d6b5e2d1933b01dcb905f9e7.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cb1f378011168cf42fdb284d6b5e2d1933b01dcb905f9e7.png/w_90 90w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cb1f378011168cf42fdb284d6b5e2d1933b01dcb905f9e7.png/w_180 180w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cb1f378011168cf42fdb284d6b5e2d1933b01dcb905f9e7.png/w_270 270w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cb1f378011168cf42fdb284d6b5e2d1933b01dcb905f9e7.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cb1f378011168cf42fdb284d6b5e2d1933b01dcb905f9e7.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cb1f378011168cf42fdb284d6b5e2d1933b01dcb905f9e7.png/w_540 540w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cb1f378011168cf42fdb284d6b5e2d1933b01dcb905f9e7.png/w_630 630w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cb1f378011168cf42fdb284d6b5e2d1933b01dcb905f9e7.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cb1f378011168cf42fdb284d6b5e2d1933b01dcb905f9e7.png/w_810 810w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cb1f378011168cf42fdb284d6b5e2d1933b01dcb905f9e7.png/w_830 830w\"></figure></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt\">The AGI system will NOT be deliberately tasked with actions which result in the extinction of humanity</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6176150c692552b446dcc7e3d31bcf0831f5e116159cadc8.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6176150c692552b446dcc7e3d31bcf0831f5e116159cadc8.png/w_90 90w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6176150c692552b446dcc7e3d31bcf0831f5e116159cadc8.png/w_180 180w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6176150c692552b446dcc7e3d31bcf0831f5e116159cadc8.png/w_270 270w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6176150c692552b446dcc7e3d31bcf0831f5e116159cadc8.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6176150c692552b446dcc7e3d31bcf0831f5e116159cadc8.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6176150c692552b446dcc7e3d31bcf0831f5e116159cadc8.png/w_540 540w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6176150c692552b446dcc7e3d31bcf0831f5e116159cadc8.png/w_630 630w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6176150c692552b446dcc7e3d31bcf0831f5e116159cadc8.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6176150c692552b446dcc7e3d31bcf0831f5e116159cadc8.png/w_810 810w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6176150c692552b446dcc7e3d31bcf0831f5e116159cadc8.png/w_830 830w\"></figure></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt\">Some deployed APS systems will be exposed to inputs where they seek power in misaligned and high-impact ways</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c79f04ec36ee4f4b74ed0b19f763441f8426976852f7690.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c79f04ec36ee4f4b74ed0b19f763441f8426976852f7690.png/w_90 90w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c79f04ec36ee4f4b74ed0b19f763441f8426976852f7690.png/w_180 180w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c79f04ec36ee4f4b74ed0b19f763441f8426976852f7690.png/w_270 270w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c79f04ec36ee4f4b74ed0b19f763441f8426976852f7690.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c79f04ec36ee4f4b74ed0b19f763441f8426976852f7690.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c79f04ec36ee4f4b74ed0b19f763441f8426976852f7690.png/w_540 540w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c79f04ec36ee4f4b74ed0b19f763441f8426976852f7690.png/w_630 630w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c79f04ec36ee4f4b74ed0b19f763441f8426976852f7690.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c79f04ec36ee4f4b74ed0b19f763441f8426976852f7690.png/w_810 810w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c79f04ec36ee4f4b74ed0b19f763441f8426976852f7690.png/w_830 830w\"></figure></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt\">Some of this misaligned power-seeking will scale (in aggregate) to the point of permanently disempowering ~all of humanity</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d282ce4d591eb60fcc5a11005c0ce335828818e81b8cfc7f.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d282ce4d591eb60fcc5a11005c0ce335828818e81b8cfc7f.png/w_90 90w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d282ce4d591eb60fcc5a11005c0ce335828818e81b8cfc7f.png/w_180 180w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d282ce4d591eb60fcc5a11005c0ce335828818e81b8cfc7f.png/w_270 270w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d282ce4d591eb60fcc5a11005c0ce335828818e81b8cfc7f.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d282ce4d591eb60fcc5a11005c0ce335828818e81b8cfc7f.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d282ce4d591eb60fcc5a11005c0ce335828818e81b8cfc7f.png/w_540 540w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d282ce4d591eb60fcc5a11005c0ce335828818e81b8cfc7f.png/w_630 630w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d282ce4d591eb60fcc5a11005c0ce335828818e81b8cfc7f.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d282ce4d591eb60fcc5a11005c0ce335828818e81b8cfc7f.png/w_810 810w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d282ce4d591eb60fcc5a11005c0ce335828818e81b8cfc7f.png/w_830 830w\"></figure></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt\">This will constitute an existential catastrophe</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/783c222d86a6909ec76e9dacc94bbe9d6df29a90970b8454.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/783c222d86a6909ec76e9dacc94bbe9d6df29a90970b8454.png/w_90 90w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/783c222d86a6909ec76e9dacc94bbe9d6df29a90970b8454.png/w_180 180w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/783c222d86a6909ec76e9dacc94bbe9d6df29a90970b8454.png/w_270 270w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/783c222d86a6909ec76e9dacc94bbe9d6df29a90970b8454.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/783c222d86a6909ec76e9dacc94bbe9d6df29a90970b8454.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/783c222d86a6909ec76e9dacc94bbe9d6df29a90970b8454.png/w_540 540w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/783c222d86a6909ec76e9dacc94bbe9d6df29a90970b8454.png/w_630 630w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/783c222d86a6909ec76e9dacc94bbe9d6df29a90970b8454.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/783c222d86a6909ec76e9dacc94bbe9d6df29a90970b8454.png/w_810 810w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/783c222d86a6909ec76e9dacc94bbe9d6df29a90970b8454.png/w_830 830w\"></figure></td></tr></tbody></table></figure><p>It is <i>horrifically</i> easy to misinterpret these graphs just by eyeballing them, because \u2013 at the very least \u2013 the base rate of \u2018safe\u2019 and \u2018unsafe\u2019 worlds is different so you need to use a Bayesian framework to make actual probability judgements. However, there are also a few quite useful implications here if you are prepared to use such a framework. In particular, the highest value of information of AI Risk microdynamics is establishing whether the probability that AI will be deliberately tasked with ending humanity is less than about 60% and whether the AI will scale in power to the point of disempowering most of humanity is less than about 50%. These are the probability judgements that add the most information about whether we live in a \u2018safe\u2019 or \u2018dangerous\u2019 world. Since the first of these scenarios involves an AI killing most of us anyway (it just isn\u2019t \u2018out of control\u2019), realistically the second case is the one we are most interested in.&nbsp;</p><p>That is to say, to a first approximation we will learn more about whether AI is likely to lead to existential catastrophe by asking specifically about Containment scenarios than by asking about any other single element of the Carlsmith Model. This is potentially highly actionable for funders, as I don't think this information was previously known.</p><h2>4.2 Sensitivity Analysis</h2><p>\u2018Sensitivity analysis\u2019 is the process of ensuring that I haven\u2019t just cherry-picked data to get the result I wanted. There are two pre-defined sensitivity analyses I described in the text above. The first is an \u2018Expert Only\u2019 subgroup of the Survey Data, the second is a Modified Carlsmith version of Carlsmith (2021) and his reviewers. These outcomes are reported below:</p><h3>4.2.1 Expert only subgroup</h3><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/16a4f4092c687bc006d09b744bffa7d82c8d21fcf7542bd2.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/16a4f4092c687bc006d09b744bffa7d82c8d21fcf7542bd2.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/16a4f4092c687bc006d09b744bffa7d82c8d21fcf7542bd2.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/16a4f4092c687bc006d09b744bffa7d82c8d21fcf7542bd2.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/16a4f4092c687bc006d09b744bffa7d82c8d21fcf7542bd2.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/16a4f4092c687bc006d09b744bffa7d82c8d21fcf7542bd2.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/16a4f4092c687bc006d09b744bffa7d82c8d21fcf7542bd2.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/16a4f4092c687bc006d09b744bffa7d82c8d21fcf7542bd2.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/16a4f4092c687bc006d09b744bffa7d82c8d21fcf7542bd2.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/16a4f4092c687bc006d09b744bffa7d82c8d21fcf7542bd2.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/16a4f4092c687bc006d09b744bffa7d82c8d21fcf7542bd2.png/w_940 940w\"></figure><h3>4.2.2 Modified Carlsmith</h3><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a599b8e90a478ede69e28772920846150ea8a6ef167c2ad2.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a599b8e90a478ede69e28772920846150ea8a6ef167c2ad2.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a599b8e90a478ede69e28772920846150ea8a6ef167c2ad2.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a599b8e90a478ede69e28772920846150ea8a6ef167c2ad2.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a599b8e90a478ede69e28772920846150ea8a6ef167c2ad2.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a599b8e90a478ede69e28772920846150ea8a6ef167c2ad2.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a599b8e90a478ede69e28772920846150ea8a6ef167c2ad2.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a599b8e90a478ede69e28772920846150ea8a6ef167c2ad2.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a599b8e90a478ede69e28772920846150ea8a6ef167c2ad2.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a599b8e90a478ede69e28772920846150ea8a6ef167c2ad2.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a599b8e90a478ede69e28772920846150ea8a6ef167c2ad2.png/w_940 940w\"></figure><h3>4.2.3 Interpretation</h3><p>Both sensitivity analyses show the same basic pattern as the main analysis; the simple mean of results is roughly in line with the Future Fund estimate, but that \u2018the mean is misleading\u2019 and the distribution of results disproportionately favours very low-risk worlds. Whereas around half of all possible worlds are very low risk (&lt;3% risk) in the base case, only around 35%-40% of possible worlds are very low risk in the two sensitivity analysis cases. The \u2018Expert Only\u2019 analysis shows the flattest distribution of the three analyses conducted so far, and hence has the highest geometric mean of odds. The \u2018Adjusted Carlsmith\u2019 analysis has a slightly higher median but a sharper distribution and hence a geometric mean of odds somewhere between the base case and \u2018Expert Only\u2019 case.</p><p>It is common in analyses of these sorts to go back and retroactively pretend that Modified Carlsmith was supposed to be the main result all along, and put the two Survey analyses as sensitivity. This means that you can say, \u201cMy sensitivity analysis gave me results a bit above and a bit below my main analysis, so I\u2019m confident I\u2019ve triangulated the risk correctly\u201d. I don\u2019t think that would be intellectually honest in this case; notwithstanding that I pre-committed to using the Full Survey results before I knew the outcomes anyway, the Modified Carlsmith has no theoretical basis for use (it is inappropriate to just multiply odds by 1.5x to get at what the authors \u2018would have reported\u2019 if asked a completely different question). Overall, I am satisfied that the sensitivity analysis supports the main argument of this essay, which is that uncertainty analysis around the distribution of risk in AI Futures is more important than has been acknowledged to this point. I am also satisfied that the sensitivity analysis supports a view that the best estimate for a community consensus on the risk of AGI incorporating uncertainty is somewhere around or below the 3% threshold Future Fund specify would be a \u2018major\u2019 change.</p><h2>4.3 Validity Checks</h2><p>\u2018Validity checking\u2019 is the process of ensuring that the model actually outputs what we expect it to output. The gold standard here is to have someone double-check my work cell-by-cell (and I would invite you to download my model and do so from this <a href=\"https://www.dropbox.com/s/xf0kjl458u77a4o/AI%20Risk%20Model%20-%20Simple%20v5.xlsx?dl=0\">link</a>). However more commonly we would conduct analyses with particular features in order to ensure the output behaves in the way we expect it to \u2013 for example setting values to zero and making sure the output is zero and so on. In this section I\u2019ve highlighted three such analyses which I think give an interesting perspective on the results.</p><h3>4.3.1 Unadjusted Carlsmith</h3><p>A very simple validity check is to run the SDO method on the unadjusted data generated by Carlsmith and his reviewers. Since we know Carlsmith (2021) is amongst the best-regarded and best-validated models of AI Risk in the AI Risk Community, this validation check completely abstracts away all the imperfect data collection decisions I have made. This isn\u2019t a good check on the Future Fund question specifically, but rather the claim that <i>before 2070</i> we would expect to see an AI Catastrophe <i>from any source&nbsp;</i>(not just an out-of-control AGI). The results are basically where we would expect them to be \u2013 the probability of being in a low-risk world is much higher than the probability of being in a high-risk world.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11e7dfbabc9a5f12587a78570ca55339fd58348684ebdaaf.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11e7dfbabc9a5f12587a78570ca55339fd58348684ebdaaf.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11e7dfbabc9a5f12587a78570ca55339fd58348684ebdaaf.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11e7dfbabc9a5f12587a78570ca55339fd58348684ebdaaf.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11e7dfbabc9a5f12587a78570ca55339fd58348684ebdaaf.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11e7dfbabc9a5f12587a78570ca55339fd58348684ebdaaf.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11e7dfbabc9a5f12587a78570ca55339fd58348684ebdaaf.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11e7dfbabc9a5f12587a78570ca55339fd58348684ebdaaf.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11e7dfbabc9a5f12587a78570ca55339fd58348684ebdaaf.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11e7dfbabc9a5f12587a78570ca55339fd58348684ebdaaf.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11e7dfbabc9a5f12587a78570ca55339fd58348684ebdaaf.png/w_940 940w\"></figure><h3>4.3.2 'Negative Validity Check'</h3><p>A second validity check is ensuring that the SDO method <strong>doesn\u2019t</strong> produce these skewed distributions when the SDO assumptions don\u2019t hold (i.e., did I get my implementation of the maths right?). My claim in this essay is that we don\u2019t intuitively understand uncertainty analysis of conditional probabilities, and acting as though uncertain conditional probabilities are certain leads to error. If I generate the same \u2018error\u2019 with data where our intuition should be reliable, it implies a problem with the SDO method rather than our intuition. The simplest SDO assumption to relax is that we have significant uncertainty about our parameters, which is also the central claim in this essay and so a negative result here would fundamentally upend the rest of the analysis. In the validity check below, I use the (unadjusted) Carlsmith (2021) estimates, and randomly \u2018perturb\u2019 each parameter 20 times to a maximum deviation of 1% from the original estimate. I then perform the SDO method as described above. The result is below. It shows almost exactly the same thing as the Carlsmith point estimates, which is exactly as expected (remember that the original Carlsmith (2021) paper includes a term for whether AI is actually ever invented, whereas this is abstracted out of all analysis conducted for the Future Fund). The only reason the graph appears to have any uncertainty at all is that I do some smoothing to the final curves.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8e0c65587bd2fc892cbcc7eccf4b168e2a5914a086024fc0.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8e0c65587bd2fc892cbcc7eccf4b168e2a5914a086024fc0.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8e0c65587bd2fc892cbcc7eccf4b168e2a5914a086024fc0.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8e0c65587bd2fc892cbcc7eccf4b168e2a5914a086024fc0.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8e0c65587bd2fc892cbcc7eccf4b168e2a5914a086024fc0.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8e0c65587bd2fc892cbcc7eccf4b168e2a5914a086024fc0.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8e0c65587bd2fc892cbcc7eccf4b168e2a5914a086024fc0.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8e0c65587bd2fc892cbcc7eccf4b168e2a5914a086024fc0.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8e0c65587bd2fc892cbcc7eccf4b168e2a5914a086024fc0.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8e0c65587bd2fc892cbcc7eccf4b168e2a5914a086024fc0.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8e0c65587bd2fc892cbcc7eccf4b168e2a5914a086024fc0.png/w_940 940w\"></figure><h3>4.3.3 (Very basic) Structural uncertainty analysis</h3><p>The final validity check I thought might be interesting was to demonstrate how sensitive the results were to structural sensitivity. For example, an unsophisticated objection to the SDO method is that you could use it to generate arbitrarily low probabilities by adding increasingly arcane parameters to the model about which we are uncertain (\u201c\u2026and the universe still exists due to false vacuum collapse\u201d etc). The most sophisticated critique of the SDO method along these lines is probably <a href=\"https://forum.effectivealtruism.org/posts/kvZshdx5FzTPjyhxG/the-fermi-paradox-has-not-been-dissolved\">here</a> \u2013 the author argues that, for example, life might arise in places other than planets, so the structure of the Drake Equation shouldn\u2019t have a term for \u2018Mean number of planets that could support life per star with planets\u2019 because it overfits the model. With respect to the more sophisticated version of the critique of SDO, we might imagine that some terms in the Carlsmith Model are redundant however carefully Carlsmith workshopped his paper. For example, maybe we think that an AGI disempowering humanity and bad outcomes for humanity are so inextricably linked that we shouldn\u2019t separately sample from them. Similarly, perhaps we think that deploying an AGI on any real-world application whatsoever automatically means the AGI can escape confinement and gain access to high-impact resources. We could therefore create a \u2018truncated Carlsmith model\u2019 to take account of this.</p><p>The results of the Truncated Carlsmith Model validity check are below. Overall, there is some evidence that the probability of living in a very low-risk world is smaller in the Truncated Carlsmith model (although the geometric mean of odds is largely unaffected). In general, this makes sense to me \u2013 the fact it is now impossible to make six \u2018low\u2019 draws in a row rules out the possibility of the ridiculously low 10^-6-level probabilities we see in the base case model, but it doesn\u2019t fundamentally alter the fact that a single low draw on any of the four remaining parameters puts us at or near a \u2018low risk\u2019 world. Furthermore, the probability of making a low draw increases in both of the grouped parameters, since anyone who had a low value for one and a high value for the other now has a low overall value as a mathematical consequence of multiplying probabilities.&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e30cf8f37588a2a4e07faa5f3ddd63299323d0899feea2c3.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e30cf8f37588a2a4e07faa5f3ddd63299323d0899feea2c3.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e30cf8f37588a2a4e07faa5f3ddd63299323d0899feea2c3.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e30cf8f37588a2a4e07faa5f3ddd63299323d0899feea2c3.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e30cf8f37588a2a4e07faa5f3ddd63299323d0899feea2c3.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e30cf8f37588a2a4e07faa5f3ddd63299323d0899feea2c3.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e30cf8f37588a2a4e07faa5f3ddd63299323d0899feea2c3.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e30cf8f37588a2a4e07faa5f3ddd63299323d0899feea2c3.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e30cf8f37588a2a4e07faa5f3ddd63299323d0899feea2c3.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e30cf8f37588a2a4e07faa5f3ddd63299323d0899feea2c3.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e30cf8f37588a2a4e07faa5f3ddd63299323d0899feea2c3.png/w_940 940w\"></figure><h3>4.3.4 Interpretation</h3><p>The reason I selected these validity checks is because these three validity checks together imply:</p><ul><li>The use of a survey isn\u2019t the thing driving the results \u2013 there might be weaknesses with the survey, but the central insight that uncertainty analysis is neglected would survive a major weakness being discovered in the survey.</li><li>The use of the SDO method isn\u2019t creating the results out of nothing \u2013 SDO only creates striking results when uncertainty around parameters is neglected</li><li>The specifics of the Carlsmith Model isn\u2019t multiplying uncertainty unnecessarily \u2013 although there is certainly structural uncertainty analysis which should be performed (and I will try and perform it in a later essay), this relates more to the general concept of multiplying conditional probabilities together to arrive at an overall risk. If you merely want to tinker around with the specifics of the Carlsmith Model you will arrive at a very similar result to the base case.</li></ul><p>These three points are the major pillars I expect objections to this essay are likely to attack. The reason I think this validity analysis is helpful is that even if one of these pillars collapses the general thrust of my conclusion remains. For example, I personally think the weakest element of the argument is the implicit premise that a survey of the AI Risk Community is the same thing as generating a reliable distribution of AI Risk probabilities. Let us suppose I am wrong about this and in fact the only reliable way to generate accurate beliefs around AI Risk is careful expert review of longform essays about claims, which I cannot do as I don\u2019t have the social capital to get AI experts to produce such data. Nevertheless, I can be confident that my general conclusion would remain if I undertook this process; because the mechanism of SDO isn\u2019t specific to any particular dataset (provided there is order-of-magnitude uncertainty on some parameters), I can be confident that those experts would have intuitions that would mislead them and the SDO process would produce this striking result.&nbsp;</p><h1>5 - Analysis</h1><h2>5.1 Strengths and weaknesses of analysis</h2><p>The purpose of this essay is to argue that uncertainty analysis has been systematically neglected by rationalist-adjacent communities with an interest in forecasting the future. Consider that prior to this essay, the gold standard analysis of systematic uncertainty in AI Risk prediction was from Carlsmith (2021) and looked like the below:</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ce5c76109ee94c5627d34567a65e9f4a221724224c6e381a.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ce5c76109ee94c5627d34567a65e9f4a221724224c6e381a.png/w_138 138w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ce5c76109ee94c5627d34567a65e9f4a221724224c6e381a.png/w_218 218w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ce5c76109ee94c5627d34567a65e9f4a221724224c6e381a.png/w_298 298w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ce5c76109ee94c5627d34567a65e9f4a221724224c6e381a.png/w_378 378w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ce5c76109ee94c5627d34567a65e9f4a221724224c6e381a.png/w_458 458w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ce5c76109ee94c5627d34567a65e9f4a221724224c6e381a.png/w_538 538w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ce5c76109ee94c5627d34567a65e9f4a221724224c6e381a.png/w_618 618w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ce5c76109ee94c5627d34567a65e9f4a221724224c6e381a.png/w_698 698w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ce5c76109ee94c5627d34567a65e9f4a221724224c6e381a.png/w_778 778w\"></figure><p>I don\u2019t intend this as a slight on Carlsmith (2021) at all \u2013 the only reason this essay is even possible is because Carlsmith performed an incredible piece of work in making explicit structural claims about AI Risk. I think I have something to add on the topic of systematic investigation of uncertainty, but his non-systematic coverage of this topic is light-years ahead of anything I could have produced. Moreover, SDO\u2019s insight is not at all obvious \u2013 people have been theorising about the Fermi Paradox for years before SDO \u2018dissolved\u2019 it, and nobody else hit on the idea that the solution might have been contained <i>within</i> the uncertainty analysis, rather than uncertainty analysis being something you grudgingly do prior to publication.&nbsp;</p><p>There are therefore some elements of the analysis I am quite proud of \u2013 in particular I think it sets a new benchmark for appropriate parameter uncertainty analysis in AI Risk spaces. I am really pleased to make a contribution in this area, however minor it is.</p><p>However, there are elements of the analysis which are not so good. The main weaknesses of the analysis are:</p><ul><li>I am relying heavily on survey data being representative of the underlying reality of the world. That is, my headline result is actually that the AI-interested rationalists <i>believe</i> there is a 1.6% risk of catastrophe once you have correctly adjusted for asymmetric distributions of risk. To the extent you believe the \u2018wisdom of crowds\u2019 applies to AI forecasting this is not a problem, but to the extent you think that rationalists will systematically over- or under- estimate AI risk my results will be wrong to the same degree.</li><li>The SDO method is incredibly brittle to correlated parameters. If, for example, the probability that AI is invented is <i>correlated</i> with the probability that Alignment is easy (for example because whole-brain emulation turns out to be the best path to AGI and Alignment is easy when you\u2019re literally simulating human values) then the SDO method doesn\u2019t work \u2013 it can\u2019t randomly sample from the Alignment question, because it needs to weight its sampling based on what it picked in the Invented question. I did some initial experiments into whether this was possible to fix by converting the synthetic <i>point</i> estimate to a synthetic <i>distribution</i> estimate, and my conclusion is that this objection isn\u2019t fatal but it needs a better statistician than me to get it implemented.</li><li>A review of the literature uncovered only one structural model of AI Risk to inform usage of the SDO method. Structural sensitivity analysis is therefore very limited. For example, one significant structural assumption is that this model assumes only one AGI is ever invented. That is, I think everyone is implicitly always talking about the AGI that comes closest to causing a catastrophe for humans in every possible world when they are forecasting probabilities. However, if successfully containing one AGI doesn\u2019t mean you\u2019ve successfully contained the next AGI then the Carlsmith Model greatly misspecifies the AGI landscape. I will attempt to write a second (mercifully shorter) essay with a method to address this, but it is too much to include in one place so it is a limitation of the analysis here.</li></ul><h2>5.2 Implications of analysis</h2><h3>5.2.1 Implications for individuals</h3><p>If my analysis is correct, then there is a high probability that we live in a world where the risk of AI Catastrophe due to out-of-control AGI is significantly lower than (most of) the AI Risk community initially believed. I don\u2019t think my position is especially iconoclastic \u2013 \u201cshut up and multiply\u201d is a fairly common saying in rationalist-adjacent spaces for a reason \u2013 but I accept for some people it could trigger a re-evaluation of their existing beliefs. If we do live in such a world, does this imply a radical restructuring of anyone\u2019s beliefs? I think probably not for individuals, for three reasons:</p><ol><li>The most extreme analysis of my data you could imagine would still be within an order of magnitude of basically all community consensuses, including the Future Fund estimate. There are very many AI-related questions and forecasts over which we have significantly worse than order-of-magnitude accuracy, so this analysis should be understood as being wholly consistent with the existing AI paradigm (note that that is quite different to SDO\u2019s application in the Fermi Paradox space, where their method totally blew all existing analysis out of the water). If you were comfortable with order-of-magnitude imprecision before you should be comfortable with it now, even though the central estimate has shifted within that order of magnitude.</li><li>In general, the <i>probability</i> of AI catastrophe due to out-of-control AGI is not as relevant as the expected value of&nbsp;<i>preventing</i> AI catastrophe through this route, for example expressed as the value of future QALYs not accrued because of that catastrophe. More specifically \u2013 since I am an economist \u2013 I\u2019d suggest the most relevant framework for considering the problem is the cost-effectiveness of interventions designed to lower AI risk. On that framework it sort of doesn\u2019t matter whether the risk of catastrophe is 1% or 10% or 100% - there\u2019s a lot of stuff we could be doing cost-effectively at the moment that we are not, and we can perhaps worry about things on the 1% vs 10% margin when we\u2019ve completely saturated the low-cost high-impact interventions with money.</li><li>This only considers one particular model of AI Risk \u2013 one where an out-of-control AI permanently disempowers humanity in a bad way. There are very many other scenarios in which AI could be bad for humanity, for example scenarios where one country uses an AI to wage a genocidal war against other countries. There are also scenarios where being overcautious regarding AI is bad for humanity, for example scenarios where AI research is deliberately slowed down because of concerns over risk and then a pandemic wipes out all life on earth because we didn\u2019t have access to an AI to develop a cure. What I mean to say by this is that this essay is not (and is not intended to be) the final word on uncertainty analysis in AI Risk, so radically updating your belief should be conditional on more analyses like this being published to cover other AI Risk scenarios.</li></ol><h3>5.2.2 Implications for funding bodies</h3><p>However, for organisations / fundholders this analysis might potentially prompt some thought about the best way to distribute resources. Some high-level implications of my analysis are:</p><ul><li>Strategies for preventing AI Risk should start from the premise that there is a good chance we live in a low-risk world:<ul><li>Instead of preparing for a middling-ish risk of AGI Catastrophe due to the risk of out-of-control AGI, we should be preparing (at least) two strategies for the possibility that we live in one of a high-risk or low-risk world, and plan accordingly. For example, in a high-risk world we might be prepared to trade away a lot of the potential economic advantages of AGI in order to prevent it disempowering humanity, whereas in a low-risk world we might treat AGI Risk like we currently treat natural pandemic risk (that is, mostly the honour system, followed by a massive commitment of resources if a pandemic breaks out).&nbsp;</li><li>To this end, we should be devoting significantly more resources to identifying whether we live in a high-risk or low-risk world. The \u2018value of information\u2019 here is potentially trillions of dollars of AGI resilience infrastructure we do not need to build.</li></ul></li><li>Risk microdynamics are extremely understudied. For example:<ul><li>It seems like there is a difference between Expert and Non-Expert predictions of overall AI Catastrophe which is driven almost entirely by different beliefs about how easy it will be to Contain an AI which is trying to disempower humanity. When funding outreach / explanations of AI Risk, it seems likely it would be more convincing to focus on why this step would be hard than to focus on e.g. the probability that AI will be invented this century (which mostly Non-Experts don\u2019t disagree with). Are there more dynamics like this that could improve outreach?</li><li>It is clear some steps between AGI being invented and AGI catastrophe are more uncertain than others, and this is driving the broad distribution of results we see. If we were more certain about the most uncertain steps in the process then this would have a disproportionate impact on our certainty over what kind of world we live in, and therefore our response to the sort of future we were likely to experience. A good candidate for this sort of investigation is the probability that we can \u2018Contain\u2019 an AI attempting to disempower humanity. If we can do this with ~60% probability or better, it is very likely we live in a \u2018safe\u2019 world.</li><li>More generally, I\u2019d imagine that the Carlsmith Model is also not the last word in structural analysis of possible AI futures. How different structural specifications of risk affect overall risk is not well understood in the AI space, and future commissioned research could (and probably should) seek to resolve this issue. This is by far the most important gap in understanding suggested by this essay, but also the one that looks most set to be quickly filled, thanks to the MTAIR project.</li></ul></li><li>SDO\u2019s method is not so complex that an intelligent layperson couldn\u2019t have spotted the problem given access to the Survey Data I generated (my main contribution was knowing to look for a problem in exactly that spot in the first place) However, community norms in AGI spaces do not reward systematic investigation of uncertainty, and few people actually enjoy undertaking analysis of uncertainty just for the sheer thrill of it. It is really good that Carlsmith\u2019s work is getting such a lot of praise, because it takes the AI Risk Community in a direction where major statistical issues like that described in this essay are more likely to be spotted early. Funders may want to consider accelerating this direction of travel, and commissioning many more systematic investigations of elements of uncertainty, using different elements of the uncertainty analysis toolkit. Funders might also want to reward / commission work that would form the building blocks of such analysis, such as Michael Aird\u2019s <a href=\"https://www.lesswrong.com/posts/jyRbMGimunhXGPxk7/database-of-existential-risk-estimates\">database of existential risk</a>.&nbsp;</li></ul><h1>6 - Conclusions</h1><p>This essay makes an extremely striking claim; analysis of uncertainty reveals that the actual risk of AI Catastrophe due to out-of-control AGI is almost an order of magnitude less than most experts think it is. To the extent that I even dare make such a bold claim, it is because of the strong community norms to take weird results seriously, especially if they expose a failure mode in intuitive reasoning. At least part of the purpose of the essay is to make the case that we shouldn\u2019t spend so much time focussing on single estimates of AI Catastrophe, and should instead consider distributions of results. To that end I would say that the main result I want to communicate is that <strong>it is more probable than not that we live in a world where the risk of AGI Catastrophe due to out-of-control AGI is &lt;3%</strong>.</p><p>This is still an extremely striking claim, but one that is slightly more consistent with existing beliefs about AGI Risk \u2013 a large number of low-risk worlds are balanced out by a small number of high-risk worlds, such that when you take a simple average of risk you end up with a middling-ish number (perhaps around 15%), but when you consider the asymmetric distribution of high- and low-risk worlds you end up with a much lower number.</p><p>In this essay I propose a mechanism for why AI Risk analysts might have persisted with the belief that the distribution of uncertainty was symmetric. In my experience, people are not inherently comfortable reasoning about probabilities in their head \u2013 for example, people intuitively feel like if a chain of reasoning has a lot of high probabilities and a single low probability that the outcome must surely have at least a middling probability, when in fact the overall outcome will (on reflection) clearly be lower than the lowest probability in the chain of logic. People are also uncomfortable reasoning about uncertainty, especially when the distribution of uncertainty isn\u2019t a nice symmetric normal or uniform distribution. When distribution of uncertainty is more or less symmetric is can be abstracted away for most purposes, but this mental habit gets people into trouble when the distribution is asymmetric. Given these two hypotheses, it stands to reason that people would be extremely uncomfortable reasoning about uncertain probabilities, which, unfortunately, is exactly what is required to make accurate forecasts of AI Risk.</p><p>SDO offer a powerful method for explicitly quantifying this uncertainty. To summarise, if you repeatedly sample from the space of all possible analyses of AI Risk then you will sometimes hit a low number for some parameters. The nature of the way conditional probabilities function is that this leads to disproportionately low (asymmetric) risk, which is surprising and unintuitive. An important argument in this essay is that the SDO method is not doing any \u2018work\u2019 \u2013 rather the method offers a way to think about uncertainty in parameter estimates to help us overcome our cognitive bias. It is rather nice that SDO are rationalist luminaries, but the method would be appropriate even if they had never commented on AI Risk.</p><p>I argue that for most individuals, not much will change as a result of this analysis. Almost nobody would have said their certainty over AI Risk scenarios was better than order-of-magnitude, so the finding in this essay that the risk of this kind of Catastrophe is actually towards the lower end of the order-of-magnitude we thought it was is probably not wholly transformative news. On the other hand, there may well be some actionable insight for funding bodies contained within this essay. I\u2019d suggest the three most immediately actionable insights are:</p><ul><li>We should be devoting significantly more resources to identifying whether we live in a high-risk or low-risk world. The \u2018average risk\u2019 (insofar as such a thing actually exists) is sort of academically interesting, but doesn\u2019t help us design strategies to minimise the harm AI will actually do in this world.</li><li>We should be more concerned with systematic investigation of uncertainty when producing forecasts. In particular, the radical results contained in this essay only hold under quite specific structural assumptions. A considered and systematised approach to structural uncertainty would be a very high-value follow up to this essay about parameter uncertainty, but would need to be written by an expert in AI Risk to move beyond surface-level insight.</li><li>More generally, the analysis in this essay implies a reallocation of resources away from macro-level questions like, \u201cWhen will AI be created?\u201d and towards the microdynamics of AI Risk. For example, \u201cWhat is the probability that the Alignment Problem turns out to be easy?\u201d is the best early differentiator between low-risk and high-risk worlds, but it is a notably under-researched question (at least on a quantitative level).</li></ul><p>Overall, \u2018Dissolving AI Risk\u2019 is a slightly incendiary title; AI Risk is still a live concern, and even a 1.6% chance of a terrible risk to humanity is too high for comfort. The title is an homage to Sandberg, Drexler and Ord, and their excellent 2018 paper on the Fermi Paradox. The reason for the homage is that this is really an essay about their insight, applied to a fairly straightforward survey dataset that happens \u2013 coincidentally - to be about AI Risk. Their insight is that for <strong>any</strong> application where you are multiplying conditional probabilities, and uncertainty over those probabilities spans at least one order-of-magnitude, you will end up with a significantly asymmetric distribution of underlying risks, favouring low-risk outcomes. This is not at all intuitive, but the extensive sensitivity and scenario analysis here is hopefully enough to make the case that the result is robust (even if I haven\u2019t done a perfect job explaining the SDO mechanism). The overall goal of this essay is to demonstrate a practical example of the use of uncertainty analysis to create novel insight, and to the extent that I have succeeded at \u2018dissolving\u2019 AI Risk by an order of magnitude, I hope this essay accomplishes that.</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn338golu496e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref338golu496e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>If you clicked on this footnote because you didn\u2019t totally understand why this would be the case, I will reassure you that there is a lot more explanation to follow in the main body of the essay.</p><p>To help orient you though, an analogy with sports teams might be helpful:</p><p>In a sports league there are multiple teams competing at games which can be modelled roughly as (weighted) coin tosses as to who wins. Losing one game doesn\u2019t mean you lose the whole season, so in general the best team usually wins the League.&nbsp;</p><p>A sports elimination bracket is the same, except if your team loses a single game they are out of the whole tournament. This means that the best team does not always win an elimination bracket \u2013 they need to have a certain amount of luck to win every single game in a row.&nbsp;</p><p>So looking only at the best team\u2019s probability of winning one specific game will systematically underrepresent their wins in a league-type format and systematically overrepresent their wins in an elimination-type format. You are much safer betting on the best team to win a league-type format than betting on the best team to win an elimination-type format, all other things being equal.</p><p>Most models of AI risk are \u2013 at an abstract enough level \u2013 more like an elimination tournament than a league, at least based on what has been published on various AI-adjacent forums. The AI needs everything to go its way in order to catastrophically depower humanity (although for sure it can make things very nasty for humans without catastrophically depowering them). Even if the AI is the \u2018best team\u2019 in the elimination tournament \u2013 which is a disturbingly apt metaphor now I think about it \u2013 simply <strong>because</strong> it is an elimination tournament the best player is disadvantaged compared to its probable performance in a single match</p><p>If this example confuses more than it enlightens, there is a much more comprehensive explanation to follow.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9f8pdahy7yr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9f8pdahy7yr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>During this step I was made aware of the \u2018<a href=\"https://www.lesswrong.com/s/aERZoriyHfCqvWkzg\">Modeling Transformative AI Risk</a>\u2019 (MTAIR) project, which is a significantly more textured approach to modelling AI Risk, building on the work of <a href=\"https://www.lesswrong.com/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment\">Cottier and Shah</a>&nbsp;(2019). This is effectively the next-generation of laying out the logical interrelationship between steps between now and a possible AI Catastrophe, building on the strong foundations of Carlsmith (2021). I am fairly confident that once published it will completely resolve my concerns about structural uncertainty analysis, which is why I make less of a fuss about structural uncertainty here than I did in my earlier essay about GiveWell\u2019s model</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkx4lym8m3h\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkx4lym8m3h\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I ended up having to exclude most of Aird\u2019s database because the questions included in the database didn\u2019t quite fit the decision problem the Future Fund were interested in. Nevertheless, Aird\u2019s database deserves significantly more credit for this essay than it is about to get, because I used the estimates that he collated to parameterise a proof-of-concept model I then used to design everything else. In general, I suspect a significant cause of the issues I diagnose with uncertainty analysis in EA spaces are because there are few people like Aird doing hard and mostly thankless work to systematically bring together different perspectives on the same issue. This is a fertile area for high-impact funding, although slightly outside the scope of this essay.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1k7orst7epo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1k7orst7epo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>By which I mean Astral Codex Ten, LessWrong and the Effective Altruism Forums. It was simply a blunder to miss the AI Alignment Forums \u2013 I misread my notes. I assumed there was enough cross-pollination between sources that this didn\u2019t matter too much.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnl3f5i9s6os\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefl3f5i9s6os\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Ho ho</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnponcxbdiiyc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefponcxbdiiyc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>If anyone disagrees with my adjustment of their results in this way, I would be delighted to bet you $1 that you are wrong at one to a billion odds \u2013 please PM me!</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkgb97wnu98\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkgb97wnu98\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The average here is the geometric mean of odds, although I convert it back into a probability to make it more comparable to the simple mean</p><p>A classic statistician drum to bang is that there is no such thing as \u2018an average\u2019 \u2013 there are just different measures of central tendency which are more or less appropriate for certain applications.&nbsp;</p><p>In this particular case, a simple mean would be quite misleading, because it implicitly proposes that a 99% and 99.99% probability express basically the same position (the simple mean of \u2013 say \u2013 50%, 99% and 99.99% is not that different to the simple mean of \u2013 say \u2013 50%, 99% and another 99%). Instead we use geometric mean of odds to ensure that 99.99% is correctly recorded as several orders of magnitude more certain than 99%.</p><p>See <a href=\"https://forum.effectivealtruism.org/posts/sMjcjnnpoAQCcedL2/\">here </a>for a really good discussion with further argument and justification</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnau756kiwicl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefau756kiwicl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>As an aside, for statisticians: I found method of moment was REALLY bad at generating plausible beta distributions from my real-world data, when previously it has worked pretty well for me and was working perfectly on toy data I was feeding it (eg the Modified Carlsmith numbers I described above). Is that likely to be because the data in this case spans multiple orders of magnitude / contains a lot of extreme values? Is there a better method other than grid searching over and over? If anyone has some insight on this problem I can think of a couple of interesting ways to extend my results - please PM me if you've ever worked on a problem like this</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnz60blow7e1\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefz60blow7e1\">^</a></strong></sup></span><div class=\"footnote-content\"><p><s>Obviously the correct \u2018fair betting odds\u2019 depend on the payoff matrix associated with a particular bet. If we set up a bet where you pay $1 and I pay out $X if you are right, then the simple mean is the fair odds for this payoff matrix. If we set up a bet where I pay you $1 for a prediction and you pay me an increasing amount of dollars depending on how overconfident / underconfident you were, then the Briar Score (or some other loss function like Log-Loss) gives you the fair betting odds, and this is found with the geometric mean of odds.</s> <i>Edit: Sorry, I have no idea what I mean to say here, please ignore it</i></p><p>I think either simple mean or geometric mean of odds has a plausible defence for being used in this case, and I'd say I've got a weak supposition that geometric mean of odds should be the default. I note however that the Future Fund doesn\u2019t want to get too hung up on what constitutes \u2018fair betting odds\u2019, so I haven\u2019t either \u2013 this is an essay about <strong>distributions</strong> not about <strong>point estimates.</strong></p></div></li></ol>", "user": {"username": "Froolow"}}, {"_id": "WuE2dyPzzjvNmeqWd", "title": "Does the Trajectory of Pain Matter?", "postedAt": "2022-10-18T19:18:09.419Z", "htmlBody": "<h1><strong>Summary</strong></h1><ul><li>Humans might prefer bad events giving way to good outcomes over good events giving way to bad outcomes, holding net welfare constant.&nbsp;</li><li>If this preference reflects a valid moral intuition that applies to both humans and non-human animals, then painful events at the end of life deserve additional moral weight because there is no scope for a better future.&nbsp;</li><li>It would follow that improving the humaneness of the transport and slaughter of farmed animals is more urgent than the duration and severity of pain they cause would imply.&nbsp;</li><li>There are at least two plausible explanations for why humans would value the temporal order of valenced experiences even if it had no intrinsic moral value:<ul><li>People might implicitly assume that improvements in welfare result in greater total welfare because pain relief is itself rewarding (and, as such, might produce more pleasure than is initially apparent).&nbsp;&nbsp;</li><li>Human compassion likely evolved to be sensitive to sudden need, not absolute need</li></ul></li><li>Unless future research indicates that farmed animals themselves intrinsically value the trajectory of welfare, it is not clear that the welfare issues posed by transport and slaughter deserve additional attention in virtue of their occurence at the end of life.&nbsp;</li></ul><h1><strong>Epistemic Status</strong></h1><p>This report was the product of about three weeks of research, and as such may overlook relevant research or philosophical considerations that we did not find or have time to consider. Before finalizing the report, we spent about a week's worth of work revising it in light of new comments, but did not add any new sections.&nbsp;</p><h1><strong>Introduction</strong></h1><p>This report is a postscript to \"<a href=\"https://osf.io/ezvr2\"><u>The Relative Importance of the Severity and Duration of Pain</u></a>\", which was intended to contextualize cause prioritization decisions when sources of suffering they seek to ameliorate differ in duration and severity. The present report addresses an issue that has elicited some discussion in the philosophical literature, but to our knowledge has only recently surfaced in animal ethics: Does the temporal ordering of positive and negative valence states matter beyond its influence on the total amount of pleasure and pain experienced?&nbsp;</p><p>Browning and Veit (<a href=\"https://doi.org/10.3390/ani10050799\"><u>2020</u></a>,&nbsp;<a href=\"https://doi.org/10.1093/af/vfab078\"><u>2022</u></a>) say \"yes.\" Their main premise is that end-of-life welfare is particularly important: \"even when two lives contain an equal total sum of positive and negative experiences, it is still worse to have suffering at the end of life\" (2022, p. 9). Therefore, the transport and slaughter of farmed animals deserve special consideration over and above the duration and severity of suffering they cause.&nbsp; The following provides an exposition and evaluation of Browning and Veit's argument that the temporal ordering of suffering is morally relevant. To make the implications of their view clear, we contrast it with a simple hedonistic approach in which all that matters morally is the severity and duration of pain and pleasure. That said, none of our arguments against the moral relevance of the trajectory of valenced experience depend on accepting hedonism.</p><h2><strong>Shape of a Life</strong></h2><p>On a simple hedonistic approach to welfare, the temporal ordering of events per se has no intrinsic importance (<a href=\"https://doi.org/10.1093/acprof:oso/9780199603695.003.0005\"><u>de Lazari-Radek &amp; Singer, 2014, Chapter 5, section 3</u></a>). Timing only matters to the extent it affects future welfare. Browning and Veit (2020, p. 7) give an example of previously higher living standards making current low living standards seem even worse:&nbsp;&nbsp;</p><blockquote><p>enclosure space is an important determinant of welfare for captive animals. It is likely to be worse for an animal to have the positive experience of a larger enclosure space before the negative experience of a smaller one, because the perceived decrease in quality will add to the negative feelings brought about by the change. Conversely, moving from a smaller to a larger space will create additional positive feelings about the improvement.&nbsp;</p></blockquote><p>If the trajectory of welfare matters&nbsp;<i>only</i> instrumentally, then there is a case for preferring good welfare up until a bad death rather than the converse. As Browning and Veit (2022) acknowledge, \"there may be some welfare benefits in having negative experiences concentrated at the end of life: if we take part of the harm of suffering as being in the ongoing effects, such as the memories of the experience. Suffering at the end of life is more temporary in this sense, and does not allow time for the animal to form memories that may cause more suffering in the future\" (p. 9).</p><p>However, Browning and Veit (2020) argue that temporal ordering per se improves welfare, \"Intuitively, most people feel like (all other things being equal) a life that begins poorly and ends well is better than a life that begins well and ends poorly; there is value in an upward trend and disvalue in a downward one\" (p. 7). Thus, the severe harms associated with the transportation and slaughter of farmed animals are even worse in virtue of their positioning at the end of life, where a bad ending cannot be compensated for with a positive experience (p. 8). Browning and Veit (2022) see widespread evidence that a special aversion to good-to-bad trajectories shapes attitudes towards death. For instance, palliative care focuses on making the dying feel comfortable, which they interpret to show \"a special emotional link to the experiences at the end of life. We do not want ourselves, or those we love, to suffer at the end of their lives\" (p. 12). Similarly, the common practice of giving prisoners on death row a final meal of their choice (p. 12) could reflect a stronger desire to mitigate the badness of their death than to mitigate the badness of their imprisonment.&nbsp;</p><p>It is not immediately clear how to refine hedonism to incorporate information about whether the instance of suffering occurred at the end of an individual's life. Browning and Veit (2022, p. 12) themselves do not seem to think that trajectories should swamp the calculus:&nbsp;</p><blockquote><p>We take the emotional reaction to be an additional important consideration informing our treatment of animals and thus for investing resources into improving end-of-life welfare. This of course does not imply that we should do so if it makes the world much worse overall \u2013 if we could use the same resources to stop some much greater amount of suffering at another point in time \u2013 but simply that, all else being equal, we should pay special attention to end-of-life welfare, rather than write it off as less important.</p></blockquote><p>One way to apply the \"all else being equal\" clause is to consider the trajectory of experience only when the welfare issues being compared cause an equivalent amount of harm, as measured by severity and duration. However, different experiences seldom cause the exact same amount of suffering, which would render trajectory considerations inapplicable in most situations. One could instead interpret \"all else being equal\" to mean that the shape of life has an impact on moral badness \"holding other factors constant,\" such as the severity and duration of suffering.&nbsp;&nbsp;</p><p>A remaining issue is that amending hedonism to account for the trajectory of experience allows for transport and slaughter to be viewed as, if not euthanasia, at least relatively merciful: If end-of-life suffering is less bad than the average suffering that the individual endured up until then, then the moral badness of end-of-life will be&nbsp;<i>reduced</i> relative to considering only the severity and duration of transport and slaughter. Browning and Veit (2020) would object to this implication because they believe that slaughter, no matter how painless it is, is morally bad because it causes death (p. 3). However, they could stipulate that the goodness of positive trajectories is subject to constraints such as not causing a premature death.</p><h1><strong>Evaluation</strong></h1><p>Although Browning and Veit (2022) base their claim that end-of-life welfare is particularly important on human preferences, they argue that humans can judge on behalf of animals what is worse for them: \"It need not be the case that the individual subject of the life is able to judge their life's overall shape and form a preference (or not), only that it would be reasonable for one performing such an evaluation on their behalf to come to such a conclusion\" (p. 11). In this section, we provide a reason for questioning the claim that there is a straightforward intuition that the temporal order of experience matters. Following that, we argue that even if humans do have intuitions about temporal order per se, there is an evolutionary debunking argument that can invalidate the apparent contribution of temporal order to welfare.&nbsp;&nbsp;</p><p><strong>A Basic Preference?</strong></p><p>As evidence that at least humans have temporal order preferences, Browning and Veit cite a thought experiment in which one can choose between a drug that first causes pain for half a year then pleasure for half a year or vice versa; they infer that most would prefer for pain to give way to pleasure (2022, p. 10). This could be due to the peak-end rule, whereby the most intense part of an experience and its ending loom larger than either the average or total amount of pain. Browning and Veit (2022) maintain that the peak-end rule may reflect a genuine preference regarding the temporal order of pain, rather than the alternative explanation that it is due to a bias in judgment or memory (e.g.,&nbsp;<a href=\"https://psycnet.apa.org/doi/10.1016/j.obhdp.2008.07.001\"><u>Liersch &amp; McKenzie, 2009</u></a>).&nbsp;</p><p>Putting aside the fact that empirical research has not been conducted to establish what percentage of people prefer the drug that causes pain first and pleasure second, it is possible that the emotions people normally have as a result of registering a change between positive and negative states could distort intuitions about the case. For example,&nbsp;<a href=\"https://psycnet.apa.org/record/2009-13385-019\"><u>Leknes and Tracy (2010)</u></a> report that the cessation of pain produces positive affect:&nbsp;</p><blockquote><p>In brief, the results from our laboratory confirm the predictions stipulated in the opponent process theory as follows: (1) The sudden termination of a painful sensation elicits positive affect, as measured by subjective ratings of relief pleasantness. (2) The relief associated with the offset of pain increases with the intensity of the pain sensation. (3) The pleasantness of relief from pain increases with the efficacy and speed of return to homeostatic balance, as evidenced by the higher positive hedonic ratings when the skin is gently cooled after burning heat pain.</p></blockquote><p>With this as backdrop, imagine the following thought experiment. Two organisms live identically long lives before painlessly dying after 100 seconds. Each organism experiences 50 seconds of painful stimulation (a heat thermode at a mildly unpleasant temperature), and 50 seconds of pleasurable stimulation (a gentle, pleasant, tickle). Organism A experiences 50 seconds of the pleasurable stimulation first, followed by 50 seconds of painful stimulation, and organism B experiences 50 seconds of the painful stimulation first, followed by 50 seconds of pleasurable stimulation. The way the thought experiment is supposed to work, we are meant to assume by stipulation that both organisms experience the same amount of pain and pleasure. However, given the above empirical evidence, it seems highly likely that organism B would in fact experience&nbsp;<i>more</i> pleasure overall, because the cessation of pain for organism B would itself produce positive valence, adding to the positive valence of the stimulation. And if the cessation of a positive experience is experienced as aversive, that would favor organism B's experience even more. Given this, it is not entirely clear how to make experiences with opposing trajectories entirely comparable in terms of the amount of pain and pleasure. As a result, it is difficult to know whether intuitions about thought experiments of this sort are driven by beliefs about the value of a positive trajectory of experience, or instead about how much pain and pleasure result from different trajectories in practice.</p><p>Browning and Veit (2022) have more faith in the ability to make intuitions about hypothetical situations that by stipulation differ from everyday life. In defending their intuition that a drug that induces pain then pleasure is better than a drug that induces pleasure they pain, they write, \"While one might object that this can be explained by anticipation effects making the latter case worse, this is ruled out by holding fixed the total amount of pleasure or suffering. It must be the order of events themselves, rather than their additional effects, that we prefer\" (p. 10). However,&nbsp;<a href=\"https://psycnet.apa.org/record/2015-54495-002\"><u>Royzman et al. (2015)</u></a> point out that \"people routinely anchor fictional content in real-world knowledge, finding it difficult to comprehend information about a fictional universe that contradicts their real-world assumptions\" (p. 298), and present evidence that many participants do not play along with implausible stipulations when providing their moral intuitions about hypothetical vignettes. In the present case, considering trajectories of pain and pleasure without accounting for anticipation, relief, and comparison with the past requires imagining what the self would prefer if it had radically different dispositions, undermining the experiential basis for making a confident judgment.&nbsp;</p><p>Until behavioral scientists design ways to account for the anchoring effects of past experiences on moral intuitions, thought experiments of this sort are not dispositive. Note that an analogous issue would affect experiments testing whether farmed animals have preferences about the temporal ordering of experience. One could use a conditional place preference test in which animals choose to move towards either a location that they associate with a good experience and then a bad experience, each of which lasts the same amount of time, or a second location where a bad experience gives way to a good experience of the same duration. But the second location might be preferred because it is associated with greater pleasure and less pain, once effects such as relief and anticipation are taken into account.&nbsp;</p><p><strong>Evolutionary Debunking</strong></p><p>&nbsp;In spite of our claim that humans would struggle to hold net welfare constant, it could turn out that many people would earnestly affirm that the trajectory of experience matters. In this case, one would then need to evaluate whether this preference is based on a valid moral intuition. Browning and Veit (2022) suggest that the kindness and mercy humans show for those at the end of their life reflects compassion: \"It is some sense of compassion or benevolence that motivates the desire for someone to have a pleasant experience with which to finish their lives\" (p. 12). We suspect that inferring moral values based on which situations evoke compassion is an unreliable process for detecting moral truths because compassion arguably evolved in part to facilitate helping individuals who only temporarily need help, and who will likely eventually have the resources to return the favor (<a href=\"https://dx.doi.org/10.1037%2Fa0018807\"><u>Goetz et al., 2010</u></a>;&nbsp;<a href=\"https://doi.org/10.1086/406755\"><u>Trivers, 1971</u></a>). Consequently, sudden need evokes more compassion than chronic need, as the chronically needy are less likely to ever reciprocate. As&nbsp;<a href=\"https://doi.org/10.1111/pops.12450\"><u>Delton et al. (2018, p. 919)</u></a> explains, prioritizing helping those whose welfare has suddenly declined is inefficient when scaled to the societal level:</p><blockquote><p>When asked to reflect on the matter, people would probably agree that social welfare spending should be primarily targeted at the neediest citizens. But the acute-needs heuristic focuses attention on different targets: people who have fallen far even if they are still doing well in absolute terms. Hence, this heuristic could cause citizens to support policies that target relatively well-off people, at the expense of the neediest.&nbsp;</p></blockquote><p>Although it could be true that it is more aversive for rich people to experience the same absolute amount of deprivation than the less well-off, it is far less plausible that, as a policy, preferentially directing aid towards those who suddenly have less less wealth over the chronically least-well-off produces the greatest direct welfare gains.&nbsp;</p><p>The perverse consequences of compassion's evolved sensitivity to sudden decrements in welfare suggests that it is a poor guide to moral reasoning, a conclusion that even those under the influence of compassion have admitted to in an experimental setting (<a href=\"https://psycnet.apa.org/doi/10.1037/0022-3514.68.6.1042\"><u>Batson et al., 1995</u></a>). If so, then the fact that a preoccupation with end-of-life welfare is driven by compassion should count against its validity. Browning and Veit might object that compassion towards those at the end of their life is inconsistent with the evolutionary explanation proposed here because people who are dying cannot return favors. However, emotions evolve based on whether they increase inclusive fitness, averaged across ancestral populations and environments, not whether they optimize behavior on any one occasion (<a href=\"https://doi.org/10.1016/j.evolhumbehav.2018.07.005\"><u>Sznycer et al., 2019</u></a>). Increasing the complexity of the computations that determine whether an emotion triggers will only be selected for to the extent that the fitness benefits of doing so more than cover the additional metabolic costs. Callousness towards those suffering at the end of their lives is not obviously personally beneficial, insofar as emotions signal good character to third parties only if they are perceived as not due to self-interested calculation (<a href=\"https://doi.org/10.1086/406755\"><u>Trivers, 1971</u></a>;&nbsp;<a href=\"https://wwnorton.com/books/Passions-Within-Reasons/about-the-book\"><u>Frank, 1988</u></a>).</p><p>One could also object that debunking arguments prove too much: All moral beliefs are open to debunking arguments, including tenets of hedonism such as that suffering is bad (<a href=\"https://doi.org/10.1086%2F673433\"><u>Kahane, 2014</u></a>; but see de Lazari-Radek &amp; Singer, 2014, pgs. 266-269). For present purposes we could easily bite this bullet, as we are not advocating for any alternative moral values here, or even moral realism. Nevertheless, it merits mention that how much a debunking explanation undermines the justification in a moral belief depends on whether one would reason their way to that belief even if it had not evolved via natural selection. Our doubts about the moral relevance of the trajectory of experience depend not only on the fact that we were able to formulate an evolutionary hypothesis to debunk it, but also on our inability to see how reasoning from premises that cannot be debunked would lead to an intrinsic valuation of bad-to-good sequences over good-to-bad sequences.&nbsp;</p><h1><strong>Conclusion</strong></h1><p>Browning and Veit's case for prioritizing improving the humaneness of transport and slaughter of farmed animals depends on welfare at the end of life having more intrinsic welfare importance than earlier parts of life. However, the intuition that the trajectory of experience matters morally depends on a human intuition that may be unduly influenced by morally irrelevant factors. Until there are independent grounds for placing intrinsic importance on end-of-life welfare, it would seem imprudent to shift resources towards reforming transportation and slaughter, if one believes that more suffering could be prevented overall through other means.&nbsp;&nbsp;</p><p><strong>Acknowledgements</strong></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995432/mirroredImages/WuE2dyPzzjvNmeqWd/hrtmoiitfwamj29azikc.png\"></p><p><br>&nbsp;</p><p><i>This research is a project of</i><a href=\"http://rethinkpriorities.org/\"><i><u> Rethink Priorities</u></i></a><i>. It was written by William McAuliffe and Adam Shriver. We thank Marcus A. Davis and Holly Elmore for helpful feedback on earlier versions of this report. Also, a special thanks to Michael St. Jules for providing quality control throughout the writing process.</i></p><p><strong>References</strong></p><p>Batson, C. D., Klein, T. R., Highberger, L., &amp; Shaw, L. L. (1995). Immorality from empathy-induced altruism: When compassion and justice conflict.&nbsp;<i>Journal of Personality and Social Psychology, 68</i>(6), 1042\u20131054.</p><p>Browning, H., &amp; Veit, W. (2020). Is humane slaughter possible?&nbsp;<i>Animals</i>,&nbsp;<i>10</i>(5), 799.</p><p>Browning, H., &amp; Veit, W. (2022). The importance of end-of-life welfare.&nbsp;<i>Animal Frontiers</i>, <i>12</i>(1), 8-15.</p><p>de Lazari-Radek, K., &amp; Singer, P. (2014).&nbsp;<i>The point of view of the universe: Sidgwick and contemporary ethics</i>. OUP Oxford.</p><p>Delton, A. W., Petersen, M. B., DeScioli, P., &amp; Robertson, T. E. (2018). Need, compassion, and support for social welfare.&nbsp;<i>Political Psychology</i>,&nbsp;<i>39</i>(4), 907-924.</p><p>Frank, R. H. (1988).&nbsp;<i>Passions within reason: The strategic role of the emotions</i>. WW Norton &amp; Co.</p><p>Goetz, J. L., Keltner, D., &amp; Simon-Thomas, E. (2010). Compassion: an evolutionary analysis and empirical review.&nbsp;<i>Psychological bulletin</i>,&nbsp;<i>136</i>(3), 351.</p><p>Kahane, G. (2014). Evolution and impartiality.&nbsp;<i>Ethics,</i> 124, 327-341.</p><p>Leknes, S., &amp; Tracey, I. (2010). Pain and pleasure: Masters of mankind. In M. L. Kringelbach &amp; K. C. Berridge (Eds.),&nbsp;<i>Pleasures of the brain</i> (pp. 320\u2013335). Oxford University Press.</p><p>Liersch, M. J., &amp; McKenzie, C. R. (2009). Duration neglect by numbers\u2014and its elimination by graphs.&nbsp;<i>Organizational Behavior and Human Decision Processes</i>,&nbsp;<i>108</i>(2), 303-314.</p><p>Royzman, E. B., Kim, K., &amp; Leeman, R. F. (2015). The curious tale of Julie and Mark: unraveling the moral dumbfounding effect.&nbsp;<i>Judgment &amp; Decision Making</i>, 10(4).</p><p>Sznycer, D., Delton, A. W., Robertson, T. E., Cosmides, L., &amp; Tooby, J. (2019). The ecological rationality of helping others: Potential helpers integrate cues of recipients' need and willingness to sacrifice.&nbsp;<i>Evolution and Human Behavior</i>,&nbsp;<i>40</i>(1), 34-45.</p><p>Trivers, R. L. (1971). The evolution of reciprocal altruism.&nbsp;<i>The Quarterly review of biology</i>, <i>46</i>(1), 35-57.</p><p><br>&nbsp;</p>", "user": {"username": "Will M"}}, {"_id": "ksisS29ThcY3BZLRw", "title": "Call for applications for Zanzibar residency", "postedAt": "2022-10-19T07:49:49.135Z", "htmlBody": "<p>We are delighted to announce the&nbsp;<strong>2023 Effective Altruism Africa Residency Fellowship</strong>, to be hosted on the&nbsp;<a href=\"https://fumba.town/\"><u>island of</u><strong><u>&nbsp;</u></strong><u>Zanzibar</u></a>.<strong>&nbsp;</strong>The Program shall host up to 20&nbsp; fellows for a 10 week program beginning January 15, 2023 through to March 31, 2023.</p><p>The objective of the residency is to provide structured support and develop community connections between Effective Altruists working on projects improving wellbeing across the African continent. We especially encourage Effective Altruists with personal roots in Africa to apply.&nbsp;</p><p>Fellows will be provided with free modern accommodation and working space with high-speed fibre internet. The residency will be located in the same community as the growing Silicon Zanzibar collective of tech companies, which will facilitate collaboration with existing organizations who have scaled up operations across the African continent.</p><p>We shall receive applications up to the 13th November 2022. Applicants shall receive a response to their applications on or before 20th November 2022.</p><p>If you are interested in applying, please complete the application form&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfMyZszc8NHgLT6RQ895ZlwktG-pzBNl1mqmXVHcomoGHQTng/viewform\">here</a>. For additional questions, submit your queries here below.</p>", "user": {"username": "Anne Nganga"}}, {"_id": "xMwgykEotpnQzsf3h", "title": "Careers in medicine - a new path profile from Probably Good and High Impact Medicine", "postedAt": "2022-10-18T17:12:11.099Z", "htmlBody": "<p>The teams at Probably Good and High Impact Medicine (Hi-Med) are excited to co-publish a new path profile on careers in medicine! You can find the profile over at both the&nbsp;<a href=\"https://www.probablygood.org/medicine\"><u>Probably Good</u></a> and&nbsp;<a href=\"https://www.highimpactmedicine.org/our-research/medicalcareers\"><u>Hi-Med</u></a> websites.</p><p><br><strong>Summary</strong></p><p>The path profile is an introductory guide to impactful careers within the medical space, primarily aimed at existing doctors and medical students who may not yet be familiar with EA principles.&nbsp;</p><p>One goal for this profile is to provide actionable advice for people who may be more resistant to wholesale career changes. We understand that large career changes can be daunting, especially for people who have spent many years aspiring (and training) to become a doctor. With this in mind, we want to give information for people who may be strongly inclined towards a career in clinical medicine, and want to find ways to increase their impact. The article suggests ways in which medics might increase their impact&nbsp;<i>within&nbsp;</i>clinical careers \u2013 for example, by pursuing high-paying specialities and donating to effective charities \u2013 as well as pointing out where doctors may be able to get involved with high-impact activities alongside their clinical duties.</p><p>However, the profile also makes clear that the highest impact choices for many people will likely lie outside clinical work, and we therefore highlight a few of the most salient high-impact alternatives both in the medical field more broadly, as well as outside medicine, for someone with a medical background. The career paths we discuss include&nbsp;<strong>medical research</strong>,&nbsp;<strong>public health</strong>,&nbsp;<strong>biosecurity</strong>, and&nbsp;<strong>nonprofit entrepreneurship.</strong></p><p>Below, we\u2019ve included the full introduction to the profile as well as a couple of representative excerpts.</p><p>&nbsp;</p><p><strong>Introduction</strong></p><p>The medical profession attracts huge numbers of altruistically-minded people who want to make a difference with their careers.&nbsp;<a href=\"https://www.oecd.org/education/dream-jobs-teenagers-career-aspirations-and-the-future-of-work.htm\"><u>One recent survey</u></a>, which asked for the career aspirations of over 600,000 teenagers across the world, found that over 10% of them wanted to become doctors. This is great \u2013 doctors are vital, and the world is much better because of them.&nbsp;</p><p>But if you\u2019ve gone through medical school, you\u2019ll be familiar with the concept of&nbsp;<i>triage</i> \u2013 that is, treating patients according to the urgency of their needs and your ability to help. Sometimes this means making tough calls about who to help first and how to help them. Though triage can be emotionally challenging, it\u2019s vital for saving lives.&nbsp;</p><p>We think it\u2019s possible to apply a kind of triage to the world\u2019s problems \u2013 working out which are most urgent and deciding on the most effective ways to solve them. In doing so, we may be able to identify career choices that allow us to achieve&nbsp;<i>even more&nbsp;</i>good than we might do otherwise.&nbsp;</p><p>This kind of career-focused triage can be applied here in two ways. First, we can look for the highest priority paths within clinical medicine, by searching for where doctors are most needed, and what they can do to help most. Secondly, we can look at alternatives to conventional clinical careers to see if we can have more impact on the world through another career route. Perhaps surprisingly, it turns out that there are a number of career paths in which you might be able to do more good than you would as a doctor, either because these paths focus on more pressing problems, or provide larger-scale solutions than treating patients individually.&nbsp;</p><p>In this profile, we'll highlight a few of these possible high-impact career routes \u2013 including biosecurity, nonprofit entrepreneurship, medical research, and public health. If one or more of these options could be a good fit for you, then you could have multiple times the impact in these careers than you might as a doctor (at least without further prioritization within clinical careers). And the good news is that a medical degree can serve as a great background for a number of these high-impact career paths.&nbsp;</p><p>If you are committed to working clinically, there are some great things you can do to increase your positive impact. First, you could&nbsp;<a href=\"https://www.probablygood.org/post/earning-to-give\"><u>donate a portion of your income</u></a> to some of the&nbsp;<a href=\"https://www.givewell.org/charities/top-charities\"><u>world\u2019s most cost-effective charities</u></a>, potentially saving multiple lives per year. Secondly, you could consider splitting your time between clinical work and one of the medicine-related career paths discussed later in the profile. Dividing one\u2019s responsibilities between clinical and non-clinical work is something many doctors do.</p><p>And, for those who haven\u2019t yet entered medical school, the profile will also briefly address how you might potentially enter these high-impact paths through routes other than medicine. Though a medical degree could be a useful credential and teach you transferable skills, alternative degrees may allow you to develop the most relevant skills for these paths in less time\u2026</p><p>&nbsp;</p><p><strong>Excerpts from the full profile</strong></p><p><i>Non-clinical path: Careers in Public Health&nbsp;</i></p><p>Public health focuses on improving the health of large populations of people by preventing and protecting them from ill health and injury. This is a broad career path, which can span various disciplines such as academic research, policy formulation and implementation, operations, advising, and advocacy (and roles often contain a combination of these). Moreover, public health roles can be found in a wide array of organization types, like governments, NGOs, think tanks, universities, and multilateral institutions like the&nbsp;<a href=\"https://www.who.int/europe/health-topics\"><u>World Health Organization</u></a><strong>&nbsp;</strong>(WHO).</p><p>Public health roles can be very promising in terms of impact. One reason is that many public health interventions&nbsp;<a href=\"https://academic.oup.com/jpubhealth/article/34/1/37/1554654\"><u>are highly cost-effective</u></a>, and some even save money while providing benefits by reducing the need for expensive treatments. Additionally, many public health roles grant large amounts of&nbsp;<a href=\"https://www.probablygood.org/post/leverage\"><u>leverage</u></a> \u2013 influence over where large amounts of resources are spent \u2013 compared to clinical roles.</p><p><strong>Who might be a good fit?</strong></p><ul><li>Public health roles are often academic or&nbsp;<a href=\"https://www.probablygood.org/profile-people-management\"><u>managerial</u></a> in nature. Because of this, you\u2019ll need to be comfortable in a job that doesn\u2019t entail direct clinical work. Though you may be able to perform public health work alongside a clinical career, the public health work itself is unlikely to involve much (or any) patient interaction.</li><li>Strong communicators are particularly likely to find success in public health roles, particularly roles with a focus on advocacy and advising.</li></ul><p><strong>Getting into public health</strong></p><p><strong>With a medical degree:</strong> Medicine is often regarded as one of the best ways to enter the field of public health, and certain positions (such as some roles in global health orgs like the&nbsp;<a href=\"https://www.gatesfoundation.org/about/careers\"><u>Gates Foundation</u></a>) even require clinical experience.</p><p>To get into this field, it\u2019s very common to take a master\u2019s in public health (MPH), which typically cover a range of specialities and domains within the broader sphere of public health. And if you want to lean into academic research, you could consider a PhD in either public health or a related subject like epidemiology.&nbsp;</p><p><strong>Without a medical degree:</strong> However, you can also enter public health roles through other routes. For example, a degree in economics (or a similar technical subject) provides a great background for epidemiology &amp; other data-driven public health roles, as well as general postgraduate qualifications in public health, whilst also providing the flexibility to move into a host of impactful non-medical paths such as&nbsp;<a href=\"https://www.probablygood.org/profile-development-economics\"><u>development economics</u></a>. Other degrees in the natural sciences also provide a good background for public health roles.</p><p><strong>Priorities within public health</strong></p><p>As with all other careers, applying the&nbsp;<a href=\"https://www.probablygood.org/career-guide-5-analyzing-cause-areas\"><u>importance, tractability, and neglectedness</u></a> framework can be really helpful for identifying the most impactful paths. This framework entails that, in general, it\u2019s best to prioritize roles that can deploy or facilitate effective solutions to large-scale problems.&nbsp;</p><p>Because of this, working on public health within low- and middle-income countries is likely to be a great option. Health problems are typically much&nbsp;<a href=\"https://www.probablygood.org/global-health-and-development\"><u>more serious and widespread</u></a> here than in high-income countries, and there are a&nbsp;<a href=\"https://www.charityentrepreneurship.com/health-reports\"><u>variety of effective public health interventions</u></a>, such as regulating dangerous substances or improving air quality, which could significantly help high numbers of people.</p><p>This work could include, for example, working as a health adviser to a government or in a global health advocacy organization. Even if you live in a high-income country, there are ways you can do this, including moving to a relevant region or joining a local office of an international global health organization. If this interests you, take a look at&nbsp;<a href=\"https://80000hours.org/job-board/global-health-poverty/\"><u>80,000 Hours\u2019 global health job board</u></a>, which lists some relevant positions.</p><p>But if you\u2019re limited to roles that focus on public health within high-income countries, then it\u2019s likely that the most promising roles are placed at the national level (such as governments, or organizations that aim to influence government) as well as international organizations like the WHO. We\u2019d generally expect that public health jobs that focus on a relatively small geographic area are unlikely to be the best opportunities for impact, particularly in high-income countries. Because these roles are quite common, the variance of impact within public health is high \u2013&nbsp; so it\u2019s particularly important to seek out the most promising opportunities in this path.</p><p>A great example of this path is&nbsp;<a href=\"https://profiles.stanford.edu/stephen-luby?tab=bio\"><u>Stephen Luby</u></a>, a professor at Stanford University. After getting his medical degree, Luby started a career in academic public health research. He has worked on some highly important problems within public &amp; global health, like improving handwashing practices in low-income healthcare settings, and&nbsp;<a href=\"https://stanmed.stanford.edu/2019winter/battling-global-pollution-making-better-bricks.html\"><u>improving the manufacture of bricks</u></a>, whose pollution is responsible for tens of thousands of deaths annually in the developing world.</p><p>Working in biosecurity-related public health roles is also an incredibly promising option, See more details in our biosecurity section.</p><p><strong>Can I do public health work alongside my clinical work?</strong></p><p>Because clinical roles are often highly flexible (at least, once you\u2019re fully qualified), you may have the freedom to change your clinical responsibilities to fit other work around it. You might consider, for example, going part time on your clinical work to take a position in a public health think tank or advocacy organization. We\u2019ve heard that organizations in this space are often receptive to taking volunteers, if you\u2019re interested in a low-stakes way to test your personal fit in these careers. Additionally, there are some roles, sometimes referred to as public health physicians, which perform a variety of public health tasks (like developing public health initiatives) alongside more conventional clinical duties. Other specialities may also give some opportunities for public health-related work, like conducting&nbsp;<a href=\"https://www.bmj.com/content/368/bmj.m865\"><u>quality improvement studies</u></a>. However, these are unlikely to be among the most impactful opportunities in public health more broadly.</p><p>\u2026</p><p><i>Non-clinical path: Careers in Medical Research&nbsp;</i></p><p>Another option is careers in medical research. This is a broad field, consisting of various kinds of research in either academia or industry. Some examples of medical research include developing new pharmaceuticals, supplements, diagnostic equipment, improving biomedical tools and processes, and other health interventions.</p><p>Medical research has the potential to be a high-impact area of work. For example, improved pharmaceuticals have&nbsp;<a href=\"https://www.healthaffairs.org/doi/full/10.1377/hlthaff.2020.00284?casa_token=302OwUCKG8cAAAAA%3AkYF2bHQ5aXxDt7ZxGR-KYQpmKwXPXkwLNFXPgFTLa-TVxb9jDeIMSeAUrcQuughouWhJFa0BLeVw\"><u>played a significant part in the increase to life expectancy in the US</u></a> in recent decades. However, as with public health, there\u2019s a huge amount of variance in how impactful careers in medical research will be. A big reason for this is that lots of research isn\u2019t optimized for impact, as funding often isn't directed to the most important, tractable, and neglected areas. On top of this, medical research careers \u2013 especially on the academic side \u2013 can be&nbsp;<i>extremely&nbsp;</i>competitive to enter, even once you have a PhD. This will make it both difficult to be a medical researcher at all, and even more difficult to get onto specific projects which look promising in terms of impact.&nbsp;</p><p><strong>Who might be a good fit?</strong></p><ul><li>Those with a very strong passion for scientific research. Research roles often involve fairly banal repetitive tasks, and it can take months or even years to see the fruits of your work. For these reasons, one researcher claims an&nbsp;<a href=\"https://80000hours.org/2013/11/interview-with-malaria-vaccine-researcher-katie-ewer/\"><u>\u2018</u><i><u>absolute obsession\u2019</u></i></a><i>&nbsp;</i>with the subject area is needed, or else the work may be too dispiriting.</li><li>As you advance in your research career, you may find yourself running a lab. Doing so has been&nbsp;<a href=\"https://www.asbmb.org/asbmb-today/careers/092313/how-to-become-a-good-lab-manager\"><u>compared to operating a small business</u></a>, requiring strong organizational and&nbsp;<a href=\"https://www.probablygood.org/profile-people-management\"><u>managerial skills</u></a>.</li><li>Some of the impactful work in medical research is data-driven.&nbsp;<a href=\"https://80000hours.org/2013/11/interview-with-a-cambridge-professor-of-medical-genetics-on-research-careers/\"><u>It\u2019ll be a real help</u></a>, in terms of the range of research you\u2019ll be able to engage in, if you\u2019re comfortable with numbers, and are able to develop competency in statistics (including coding in languages like R or Python).&nbsp;</li></ul><p><strong>Getting into medical research</strong></p><p><strong>With a medical degree:</strong> A medical degree, alongside some research experience in medical school or afterwards, can be sufficient for landing some kinds of research roles in both academia and industry. For more mathematical areas of medical research, like&nbsp;<a href=\"https://www.genome.gov/25020000/online-education-kit-bioinformatics-introduction#:~:text=Bioinformatics%20is%20the%20branch%20of,the%20tools%20of%20the%20trade.\"><u>bioinformatics</u></a>, you may need to be able to demonstrate experience working with numbers and statistics, or have completed a separate qualification in a math-heavy subject.</p><p>Whilst it\u2019s not necessary to have a PhD to get into research work, it can be required for more senior research roles, particularly in academia. So, if you\u2019re confident you want to pursue a medical research career, then a PhD may be a strong help. It\u2019s fairly common to complete a PhD whilst staying in clinical work should you wish \u2013 some PhD fellowships are explicitly designed for this (like&nbsp;<a href=\"https://wellcome.org/grant-funding/schemes/clinical-research-career-development-fellowships\"><u>this one</u></a> from the Wellcome Trust in the UK), though they are generally very competitive. However, as we mentioned above,&nbsp; getting a long-term academic position, even with a PhD, is not guaranteed.</p><p>If you\u2019re completing a medical degree,&nbsp;<a href=\"https://www.wolterskluwer.com/en/expert-insights/how-to-find-research-opportunities-while-in-medical-school\"><u>there are likely opportunities</u></a> within your medical school for participating on research projects. There might be opportunities as part of the degree itself (such as elective research projects), or they may be extra-curricular. This can help you test your personal fit for research and give you the chance of getting your name on an academic publication. This will make you a more competitive candidate for research-oriented positions, like clinical research fellowships, later on.</p><p><strong>Without a medical degree:&nbsp;</strong>Degrees in the natural sciences are typically a good platform for research in medical science. This can be followed by a PhD or a master\u2019s degree \u2013 neither of these are strictly necessary for taking roles in science, but should be considered strongly preferred for the most senior roles (and a PhD is almost always a requirement for conventional research within academia). The growing popularity of&nbsp;<a href=\"https://sitn.hms.harvard.edu/flash/2019/artificial-intelligence-in-medicine-applications-implications-and-limitations/\"><u>applying AI and machine learning to medicine</u></a> also means there\u2019s scope for working in this area of medical research with a more technical background, such as in computer science.</p><p><strong>Priorities within medical research</strong></p><p>It\u2019s hard to know what kind of research is going to be impactful. After all, if we knew which drugs and interventions worked, then we wouldn\u2019t need to do the research. However, there are some rules of thumb you can use to identify more promising opportunities in this space. Some helpful questions here include: What is the&nbsp;<a href=\"https://vizhub.healthdata.org/gbd-compare/\"><u>total global health burden</u></a> of this disease or condition? How many people does it affect? Is there existing research which indicates that some new approach is more likely than others to work? As an example, we\u2019d generally expect that working on a new treatment for a&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/neglected-tropical-diseases\"><u>neglected tropical disease</u></a> would be more promising than developing a new treatment for&nbsp;<a href=\"https://www.mayoclinic.org/diseases-conditions/athletes-foot/symptoms-causes/syc-20353841\"><u>Athlete\u2019s Foot</u></a>.</p><p>Alongside developing new drugs and treatments, you may also want to work on more foundational research on the nature of different diseases, or developing new tools and techniques to improve future biomedical research. Such work is more likely to take place within academia than industry. Although this kind of research might not directly save lives, it&nbsp;<a href=\"https://blog.givewell.org/2015/02/27/the-path-to-biomedical-progress/\"><u>can be hugely important</u></a> for developing effective drugs and treatments in the future.&nbsp;</p><p>Take the example of&nbsp;<a href=\"https://en.wikipedia.org/wiki/Katalin_Karik%C3%B3\"><u>Katalin Karik\u00f3</u></a>, a research professor who played a leading role in developing messenger&nbsp;<a href=\"https://en.wikipedia.org/wiki/Messenger_RNA\"><u>RNA (mRNA) technology</u></a> later licensed by Pfizer and Moderna for their Covid vaccines, saving many thousands of lives.&nbsp;<a href=\"https://www.atlanticcouncil.org/event/a-conversation-with-ugur-sahin-and-ozlem-tureci/\"><u>Ugar Sahin and \u00d6zlem T\u00fcreci</u></a>, both former medics, also played an instrumental part in this. However, it\u2019s worth noting that these people are outliers \u2013 the chances of contributing to something as impactful as RNA-vaccine technology is quite low (though perhaps still worth trying!).</p><p><strong>Can I do medical research alongside my clinical work?</strong></p><p>It\u2019s very common for clinicians to assist in clinical research for new pharmaceuticals and other medical interventions, particularly in hospital settings. In fact, some roles explicitly contain both research and clinical responsibilities. Because of this, there are likely a fair number of opportunities to work on research alongside clinical work.</p><p>Having said this, the sorts of research activities you might be able to undertake within high-income settings alongside clinical work are unlikely to be focused on the most important and neglected areas within medicine. Nonetheless, it could still be a good way to add some impact on top of your clinical duties \u2013 though other options (like biosecurity, or particularly good opportunities in public health) may often be better.</p><p>&nbsp;</p><p><strong>You can read the full profile over at&nbsp;</strong><a href=\"https://www.probablygood.org/medicine\"><strong><u>Probably Good</u></strong></a><strong> or&nbsp;</strong><a href=\"https://www.highimpactmedicine.org/our-research/medicalcareers\"><strong><u>Hi-Med</u></strong></a><strong>.</strong></p><p><i>Do you have EA or career related content that you\u2019d like to see from Probably Good? We\u2019re eager to hear what types of content would be useful to the broader community and/or to specific groups. Let us know by leaving a comment or by emailing&nbsp;</i><a href=\"mailto:contact@probablygood.org\"><i><strong><u>contact@probablygood.org</u></strong></i></a><i><strong>.</strong></i></p><p><i>Are you a medical student or a doctor interested in doing the most good with your career? High-Impact Medicine helps medical professionals to increase their impact by providing various resources, fellowships, and community events. If you are interested,&nbsp;</i><a href=\"https://www.highimpactmedicine.org/\"><i><u>visit our website</u></i></a><i> and consider joining one of our local community chapters, as well as&nbsp;</i><a href=\"https://docs.google.com/forms/d/e/1FAIpQLScL0ponJuZ32gFPYbWvJy08pzbzD84BmGgDze7BlOrarattSQ/viewform\"><i><u>apply to receive one-to-one advice</u></i></a><i> for your career.</i></p>", "user": {"username": "Probably Good"}}, {"_id": "9dqyakpjfhuo2bmjn", "title": "Metaculus is building a team dedicated to AI forecasting", "postedAt": "2022-10-18T16:08:10.848Z", "htmlBody": "<p>The speed, sophistication, and impacts of AI technology development together comprise some of the most astonishing and significant events of our lifetimes. AI development promises both enormous risks and opportunities for society.<a href=\"https://apply.workable.com/metaculus/\">&nbsp;<u>Join Metaculus's AI forecasting</u></a> team and help humankind better navigate this crucial period.&nbsp;</p><h3><strong>Open roles include:</strong></h3><h3><a href=\"https://apply.workable.com/metaculus/j/BED143E1CA/\"><strong><u>Machine Learning Engineer - AI Forecasting</u></strong></a></h3><p>You\u2019ll work to enhance the organization and searchability of our AI analyses, ensure that the AI-related data and thinking that we rely on is up-to-date, comprehensive, and well organized, and deliver (via modeling) forecasts on an enormous set of questions concerning the trajectory of AI.</p><h3><a href=\"https://apply.workable.com/metaculus/j/9A4B6E7870/\"><strong><u>Research Analyst - AI Forecasting</u></strong></a></h3><p>You\u2019ll engage deeply with ideas about the future of AI and its potential impacts, and share insights with the AI research and forecasting communities and with key decision makers. You\u2019ll use crowd forecasting to help generate these insights, writing forecasting questions that are informative and revealing, facilitating forecasting tournaments, and coordinating with Pro Forecasters.</p><h3><a href=\"https://apply.workable.com/metaculus/j/A5B4AF28F2/\"><strong><u>Quantitative Research Analyst - AI Forecasting</u></strong></a></h3><p>You\u2019ll use quantitative modeling to improve our ability to anticipate the future of AI and its impact on the world, enhance our AI-related decision making capabilities, and enable quantitative evaluation of ideas about the dynamics governing AI progress.</p><p><i>You can learn about our other high-impact, open positions&nbsp;</i><a href=\"https://apply.workable.com/metaculus/\"><i><u>here</u></i></a><i>.</i></p>", "user": {"username": "christianM"}}, {"_id": "kEHw9F963vn9d3rsp", "title": "Give Feedback on World's Biggest Problems Quiz", "postedAt": "2022-10-18T15:51:02.940Z", "htmlBody": "<p>I'm working on a new <a href=\"https://www.quizmanity.org/biggest-problems\"><i>World's Biggest Problems</i></a><i> </i>quiz<i>,</i> which should be published on Clearer Thinking in the next couple of weeks. It lasts 5-15 min and covers global health, animal welfare, and x-risk.</p><p>Can you give me your feedback with potential improvements in <a href=\"https://docs.google.com/document/d/1I_NDHIKR-cJuampBeRVxEoE6yvVtk9_YkY-j57BGhOE/edit?usp=sharing\">this Google Doc</a>? I'll integrate your comments this week. (link to <a href=\"https://www.quizmanity.org/biggest-problems\">quiz here</a>)</p><p>Thank you in advance for your help! :)</p>", "user": {"username": "andreferretti"}}, {"_id": "8tsFNQ9qdX2c3KufJ", "title": "How to Take Over the Universe (in Three Easy Steps)", "postedAt": "2022-10-18T15:04:19.596Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://youtu.be/fVrUNuADkHI\"><div><iframe src=\"https://www.youtube.com/embed/fVrUNuADkHI\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/DvQ7cYxhnrZtWngvW/how-to-take-over-the-universe-in-three-easy-steps\">LW crosspost</a></p><p><i>This is the script of Rational Animations' video linked above. It's about how to take over the universe with amounts of energy and resources that are small compared to what is at our disposal in the Solar System. It's based on </i><a href=\"http://www.fhi.ox.ac.uk/wp-content/uploads/intergalactic-spreading.pdf\"><i>this paper</i></a><i>, by Anders Sandberg and Stuart Armstrong.&nbsp;</i></p><p><i>This is our highest-quality video so far. Below, the script of the video.</i></p><h3>Let\u2019s take over the universe in three easy steps</h3><p>Welcome. We\u2019ve heard that you want to take over the universe. Well, you\u2019ve come to the right place. In this video, we\u2019ll show you how to reach as many as four billion galaxies with just a few relatively easy steps and six hours of the Sun\u2019s energy.</p><p>Here\u2019s what you need to do:</p><ol><li>Disassemble mercury and build a Dyson swarm: a multitude of solar captors around the sun.&nbsp;</li><li>Build self-replicating probes</li><li>Launch the self-replicating probes to every reachable galaxy.</li></ol><p>In science fiction, humanity\u2019s expansion into the universe usually starts within our galaxy, the Milky Way. After a new star system is occupied, humanity jumps to the next star, and so on, until we take the whole galaxy. Then, humanity jumps to the nearest galaxy, and the process is repeated.</p><p>This is&nbsp;<i>not&nbsp;</i>how we\u2019re going to do it. Our method is much more efficient. We're going to send self-replicating probes to&nbsp;<i>all</i> the reachable galaxies&nbsp;<i>at once</i>. Getting to the furthest galaxies is not more difficult than getting to the nearest ones. It just takes more time.&nbsp;When a probe arrives at its destination galaxy, it will search for a planet to disassemble, build another Dyson swarm, and launch a new wave of probes to reach every star within the galaxy. And then, each probe in that galaxy will restart civilization.</p><p>We already hear you protest, though: \u201cthis whole thing still seems pretty hard to me,\u201d you say. \u201cEspecially the \u201cdisassembling mercury\u201d part\u201d.&nbsp;</p><p>But actually, none of these steps are as hard as they first appear. If you analyze closely how they could be implemented you\u2019ll find solutions that are much easier than you\u2019d expect. And that\u2019s exactly what Stuart Armstrong and Anders Sandberg do in their paper \u201cEternity in six hours: intergalactic spreading of intelligent life and sharpening the Fermi paradox.\u201d This video is based on that paper.</p><h3>Exploratory engineering and assumptions</h3><p>What we mean by \u201ceasy\u201d here, is that we will require amounts of energy and resources that are small compared to what is at our disposal in the solar system. Also, the technology required is not extremely far beyond our capabilities today, and the time required for the whole feat is insignificant on cosmic scales.</p><p>Not every potential future technology will make sense to include in our plan to spread to the stars. We need to choose what technologies to use by reasoning in the style of exploratory engineering: trying to figure out what techniques and designs are physically possible and plausibly achievable by human scientists. The requirement \u201cphysically possible'' is much easier to comply with than \u201cachievable by human scientists\u201d, therefore, we introduce two assumptions that serve to separate the plausible from the merely possible:</p><p>First:&nbsp;Any process in the natural world can be replicated with human technology. This assumption makes sense in light of the fact that humans have generally been successful at copying or co-opting nature.&nbsp;</p><p>Second: Any task that can be performed can be automated. The rationale for this assumption is that humans have proven to be adept at automating processes, and with advances in AI, we will become even more so.&nbsp;</p><h3><strong>Design of the Dyson swarm</strong></h3><p>Now, we\u2019ve said we are going to launch probes to every reachable galaxy. This means a hundred million to a hundred billion probes. Where do we get the energy to power all these launches? We don\u2019t need to come up with exotic sources of energy we can\u2019t picture yet. We can use the Sun itself! That\u2019s why we are going to build a Dyson swarm.</p><p>To be fair, in order to be sure that a Dyson swarm will be sufficient, we need to already have plausible designs for probes and launch systems in mind, but this is a tutorial for pragmatic wannabe grabby civilizations, so we\u2019ll get to that later, when we actually use them.</p><p>A Dyson swarm is simply a multitude of solar captors orbiting around the sun. The easiest design is to use lightweight mirrors, beaming the sun\u2019s radiation to focal points where it\u2019s converted into useful work\u2014for example, using heat engines and solar cells.</p><p>A Dyson swarm has major advantages compared to a rigid Dyson sphere. A Swarm isn\u2019t subject to internal forces that would make it collapse, and it can be made with simple and conventional materials.</p><p>Even a swarm isn\u2019t without potential problems though: the captors have to be coordinated to avoid collisions and occluding each other. But these are not major difficulties. There are already reasonable orbit designs in today\u2019s literature, and the captors will have large amounts of reserves at their disposal to power any minor course correction. The efficiency of the captors will not be an issue either. We will need only a small amount of energy to power our expansion into the universe compared to the energy a Dyson swarm will be able to collect.</p><p>The biggest problem to solve is how to get all of the material necessary to build the solar captors. Even assuming the lightest design achievable with today\u2019s materials, that is, lightweight mirrors, you\u2019d need to take apart Mercury to get everything you need for the swarm. And that\u2019s exactly what we\u2019re going to do.&nbsp;</p><p>There are potentially other pathways to get the material, but being able to take apart Mercury is the conservative assumption to make, as weird as it sounds. We are not assuming future super materials that would let us build a swarm with extremely thin and efficient captors, and therefore with way less material.</p><p>Mercury looks very convenient to use in comparison to the other planets and the asteroid belt. Its orbit is approximately at the same distance from the Sun as the Swarm\u2019s, and it\u2019s a rocky planet, 70% metallic and 30% silicate. This is material that we can transform into reflective surfaces for the swarm, and use to build heat engines and solar cells.&nbsp;</p><p>The semi-major axis of Mercury's orbit is approximately 60 billion meters long. Therefore, a sphere around the Sun with that radius would have a surface area in the order of 10^22 square meters. The mass of mercury is in the order of 10^23 kilograms. Now let\u2019s assume we\u2019ll use about half of mercury to build the swarm. If we conservatively pretend that the swarm is a solid sphere around the sun, we can take the fraction between half of mercury\u2019s mass and the surface of the sphere we just calculated to get the mass of the sphere per square meter, which is 3.92 kg/m^2. This is plenty! Iron has a density of 7874 kg/m^3, so we can obtain mirrors with a thickness of at least half a millimeter. We can already easily make mirrors this thin, you can order them online if you want. But most probably, we would use a structure with a much thinner film, of the order of 0.001mm, supported by a network of rigid struts.</p><h3>Disassembling Mercury</h3><p>Now, let\u2019s disassemble Mercury and build this swarm, shall we?&nbsp;</p><p>We are going to build the Dyson swarm&nbsp;<i>during</i> the process of disassembly. While we get material from the planet, we build more of the swarm, and as we build new captors we get more energy to power more of the planet\u2019s disassembly, and so on.&nbsp;</p><p>Essentially, we need a feedback loop like this:</p><p>We mine necessary material,&nbsp;</p><p>We get the material into orbit,&nbsp;</p><p>We make solar collectors out it,&nbsp;</p><p>We get the energy from the collectors,&nbsp;</p><p>And we use that energy to mine more material, and so the cycle repeats.</p><p>Sandberg and Armstrong assume a seed of 1 km^2 of solar panels constructed on Mercury to start the feedback loop. After the seed, the loop can begin with mining the initial material.</p><p>At each cycle we have more energy at our disposal to power more mining, and the process can easily speed up exponentially.&nbsp;</p><p>In fact, the feasibility of Mercury's disassembly hinges on if we get an exponential feedback loop or not. If we can\u2019t complete the loop, or if it\u2019s not at least near exponential, then we\u2019re out of luck. The process would grind to a halt or take too long to complete in any reasonable amount of time. If we want the energy at our disposal to increase exponentially, the number of captors must increase by a fixed percentage at each cycle. That means that the energy required to mine materials, get them into orbit, and make captors, must remain nearly constant or decrease at each cycle. But this is not a big concern. Mining material and making solar collectors shouldn't consume more energy as the disassembly progresses. On the contrary, towards the end of the disassembly, less energy will be required to get material into orbit, as Mercury\u2019s gravity will be much easier to overcome. A potential problem could be cooling Mercury\u2019s core, but this is a fixed cost, and Mercury\u2019s heat might be harvested to get more energy.</p><p>And now, maybe you\u2019re thinking: \u201cWait, even if we can get an exponential feedback loop in theory, how on Earth are we going to get the workers to do all this?\u201d And that\u2019s where our assumption that \u201cany task that can be performed, can be automated\u201d comes in. With automation, the sheer scale of projects is simply not a problem. New machines and factories can be built essentially without human intervention. Time, material, and energy become the only things we need. Encouragingly, NASA had a design for a self-replicating lunar factory in 1980. And surely, in the future we will be able to do much better than NASA in the eighties!</p><p>Sandberg and Armstrong make a few additional assumptions to precisely estimate how long it\u2019ll take to complete the Dyson swarm.&nbsp;</p><p>They assume:&nbsp;</p><ul><li>Solar captors with an efficiency of \u2153.</li><li>Only 1/10 of the energy will be used to propel material into space. The rest will be used for mining, reprocessing material, or simply lost.</li><li>It takes five years to process the material into solar captors and place them into the correct orbit.&nbsp;</li><li>Only half of Mercury\u2019s material will be used to construct the captors.&nbsp;</li></ul><p>Under these assumptions, the power available will increase exponentially every 5 year cycle. Mercury will be disassembled in 31 years, with most of the mass harvested in the last four years.&nbsp;</p><p>But as long as the exponential feedback loop is possible, the details aren\u2019t that important, and we will complete the disassembly within a few cycles and a short amount of time.&nbsp;</p><p>And even if an exponential feedback loop turns out not to be possible, it doesn\u2019t necessarily mean we can\u2019t build the Dyson swarm. This is just one way to attack the problem, which relies on plausible future technology constrained by conservative assumptions. For example, if we're able to produce super materials, taking apart a large asteroid might be sufficient.</p><h3>Design of the probes</h3><p>Now that we've built the Dyson swarm, we have the energy to launch countless self-replicating probes into the universe.&nbsp;</p><p>Our probes should be capable of safely landing on other planets or asteroids, use the resources there to make copies of themselves, build other dyson swarms, launch another wave of probes, and ultimately start civilization on other star systems.</p><p>By guessing that building self-replicating probes will be possible with future technology, we are essentially making use of the assumption \u201cAnything possible in the natural world can also be done under human control\u201d. Every living thing is capable of replicating. Here\u2019s a table of some of the smallest replicators in nature. The smallest seeds on Earth weigh a millionth of a gram, and the smallest acorn weighs 1 gram. Think about it: an acorn is a solar-powered factory for the production of more acorns that generates large structures in the process: namely, oak trees.<br>&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994840/mirroredImages/8tsFNQ9qdX2c3KufJ/g90mqcotbm2phhsw3gnn.png\"></p><p><br>When thinking about the size of our probes, we need to make a distinction between the self-copying piece of the system, and the whole object that gets launched, which may include fuel, rockets for deceleration, and other equipment.&nbsp;</p><p>A reasonable upper limit for the size of the replicators is 500 tons. This is the size of the replicator in NASA\u2019s self-replicating lunar factory design, which made very conservative assumptions.&nbsp;</p><p>As a lower limit we can use a design of molecular assembler by Robert Freitas and Ralph Merkle, from their landmark book \u201cKinematic Self-Replicating Machines\u201d, a comprehensive review of self-replicating designs up until 2004. The mass of this replicator would be in the order of 10^-18kg. For reference, this is about thirty thousand times smaller than a red blood cell.</p><p>The data storage on the probe would probably be of insignificant mass. An extremely compact design would be diamond constructed with carbon 12 and carbon 13. The two isotopes would encode the bits 0 and 1. A memory like this would have a capacity of six billion terabytes per gram. Or we could use a data storage mechanism with the same compactness as DNA, in the order of a hundred million terabytes per gram. As a comparison, the total amount of data in the human world in 2020 could be stored in about 500 grams of DNA-level storage.</p><p>Apart from the replicator, the probe needs fuel to decelerate when approaching its destination. Sandberg and Armstrong hypothesize three possible types of fuel to power the deceleration. In order of increasing speculativeness and efficiency, they are: nuclear fission, nuclear fusion, and matter-antimatter annihilation. As you can see in this table, they calculated the mass of fuel needed given different deceleration amounts and type of fuel. In the table, the replicator is assumed to weigh 30 grams. You can take the \u201cdelta v\u201d column as also indicating \u201cstarting velocities\u201d if the probe then decelerates to zero. The values in bold are the kilograms of fuel needed given the most reasonable combinations of starting velocities and type of fuel available.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994840/mirroredImages/8tsFNQ9qdX2c3KufJ/aztdgvusarkokmqf1rlc.png\"></p><p>This table doesn\u2019t take into account many things that could aid deceleration though:</p><p>For example, the trajectory of the probe might be designed to use gravitational assists to slow down. Or magnetic sails could be used to create drag against the local magnetic field in the destination galaxy. Moreover, the expansion of the universe means that some amount of deceleration will come for free, and probes launched to distant galaxies would arrive with little velocity. In that case, we would need fuel only for maneuvering at the end. There are many other speculative options to help decelerating, such as the Bussard ramjet, which uses enormous magnetic fields to collect hydrogen atoms from the interstellar medium and compress them to achieve nuclear fusion.</p><p>Another potential design choice for the probes is to equip them with shields. Intergalactic space is not empty. The probes might encounter dust, and at relativistic speeds, collisions can easily destroy our probes. Another solution is simply to launch redundant probes to compensate for the fact that some might be destroyed. Sandberg and Armstrong estimate that, for speeds of 50% to 80% the speed of light, launching two probes per galaxy is enough to expect that at least one will arrive. If the probes travel at 99% of the speed of light, then we\u2019d need to launch 40 probes to each galaxy.&nbsp;</p><h3>The launch phase</h3><p>Alright, now let\u2019s say we\u2019ve chosen a viable design for the probes. Their construction has taken little material compared to the Dyson swarm. The final combined mass of all of the probes, redundancy included, is in the order 10^11 to 10^12 kilograms. This is about the mass of a mountain. The Dyson swarm is operational, and provides us with all the energy we need. It is time to launch the probes.&nbsp;&nbsp;</p><p>We will not use rockets, but a fixed launch system. Rockets would be needlessly difficult and inefficient to use for achieving acceleration to relativistic speeds. They need to carry fuel, which would in turn need to be accelerated, and the fuel needed increases exponentially with the change of speed we want to achieve. Fixed launch systems sidestep this, and are often reusable. For example we could use coilguns. Essentially, long barrels around which coils are arranged and switched on and off with precise timings, causing the probe in the barrel to accelerate due to the magnetic forces generated by the coils. With coilguns, we would shoot our probes into space. In combination, or by themselves, we can also use solar sails accelerated by lasers or particle beams.</p><p>Now, look at this table: for each type of probe and for each type of replicator, you can find in bold the time required to power the launch if the energy of the Dyson swarm were entirely devoted to the task. In the case of the 30g replicator, the numbers are insignificant on a human scale. 6 hours of the sun\u2019s energy is the maximum we would need. Instead, if the replicator is the 500 tons version, we would need hundreds of years of the Sun\u2019s energy. But this also looks very feasible if you consider that humanity might survive millions of years, and over time might divert some energy from the dyson swarm to power launches, and not necessarily launch all the probes at once.&nbsp;<br>&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994840/mirroredImages/8tsFNQ9qdX2c3KufJ/ivofovxgdutcdsciq73n.png\"></p><h3>After the universe, the galaxy</h3><p>Now, picture a future President of the Solar System proclaiming: \u201ceveryone turn off their virtual reality sets for six hours, we\u2019re taking over the universe!\u201d</p><p>The probes are launched to every reachable galaxy and the travel begins.&nbsp;</p><p>Once this first wave is enroute we can launch a new wave of probes within the milky way at lower speeds. So we\u2019d start expanding into our own galaxy only after having started expanding into the wider universe!&nbsp;</p><p>Meanwhile, the probes we\u2019ve launched to other galaxies will progressively continue to start new civilizations for the following 10 billion years, and after that, our expansion will be complete. Ten billion years may sound like a lot, but the universe will last for trillions of years. Future humanity will have plenty of time to enjoy even the most distant galaxies.</p><p>Armstrong and Sandberg calculated that at speeds between 50% and 99% of the speed of light, the probes will reach 116 million to 4 billion galaxies. The higher the speed, the more galaxies the probes can reach, because the universe is expanding at an accelerating pace, and as time passes an increasingly large number of galaxies becomes forever out of reach if we can\u2019t find a way to sidestep the speed of light limit.&nbsp;</p><h3>Final considerations</h3><p>And now that every step is complete, you know how to take over the universe. You don\u2019t need to do everything exactly in this way, though. This paper proposed many possible designs and methods at each step, but there are certainly many more ways to go.&nbsp;Moreover, Armstrong and Sandberg used conservative assumptions; the real designs will probably be better.&nbsp;The point of the paper was to illustrate that the feat is in principle possible with cosmically insignificant amounts of energy and time.</p><p>One additional point motivating the paper is that since spreading through the universe doesn\u2019t require a lot of resources, that means that the Fermi paradox is a lot sharper than we imagined. There are millions of galaxies that could have potentially reached us by now. And yet,&nbsp;we don't see any alien colonization projects in our local neighborhood. This could simply mean that there is pretty much no one else out there, or another answer could be the one given in the grabby aliens videos. If we could have seen aliens they would be here now instead of us.</p><p>If there are indeed aliens out there, that means our time to begin expanding into the universe is even more limited than we previously thought. Not only is the universe expanding at an accelerating rate, making more and more galaxies forever out of reach, but aliens might also be out there grabbing galaxies instead of us!&nbsp;</p><p>So, what are we waiting for? Let\u2019s go and do it ourselves! Let's take over the universe!</p><h3>Sources and more readings</h3><p>Eternity in six hours: intergalactic spreading of intelligent life and sharpening the Fermi paradox, by Anders Sandberg and Stuart Armstrong:<br>http://www.fhi.ox.ac.uk/wp-content/uploads/intergalactic-spreading.pdf</p><p>NASA's self-replicating lunar factory design:</p><p>- https://space.nss.org/wp-content/uploads/1982-Self-Replicating-Lunar-Factory.pdf</p><p>- http://www.rfreitas.com/Astro/GrowingLunarFactory1981.htm</p><p>Kinematic self-replicating machines, by Freitas and Merkle: https://www.amazon.com/Kinematic-Self-Replicating-Machines-Robert-Freitas/dp/1570596905</p><p>The quote at 15:11 is almost an actual quote from the paper!<br>&nbsp;</p>", "user": {"username": "Writer"}}, {"_id": "DjnK8yn9nbS7hkCpv", "title": "Ask Eachother Anything: What EA numbers are you looking for? Ask here and people will hopefully find them/suggest estimates.", "postedAt": "2022-10-18T12:33:56.166Z", "htmlBody": "<p>There are some EA numbers I need for a piece I'm writing. I figure that's true for some of you too.&nbsp;</p><p>Ask for the number you want and likely someone knows it.</p>", "user": {"username": "nathan"}}, {"_id": "cAgTyxg4azaeD6xAW", "title": "[Link post] AI could fuel factory farming\u2014or end it", "postedAt": "2022-10-18T11:16:44.421Z", "htmlBody": "<p>Despite the fact that artificial intelligence is already transforming everyday life, most people don\u2019t know much about it.</p>\n<p>Simply put, AI is a method of making software \u201cthink\u201d intelligently like the human mind. It can make decisions using real-time data, learning and adapting as it compiles new information. Netflix\u2019s algorithm, for example, stores and analyzes your viewing history to predict what you may be most interested in watching next. Every time Netflix suggests a movie, it uses the data it receives based on whether or not you click play to improve the next time. Soon enough, it might know your preferences better than you do.</p>\n<p>Discussions over the ethics of technology have been going on since long before virtually everyone had a smartphone in their pocket. Some argue that technology itself is value neutral\u2014essentially, that a given technology is only as bad or good as the intentions of the person using it. Others argue that values are built into technologies by the people who design them.</p>\n<p>The ethics of AI is a hot topic these days. Will it make the world better for people, or create more problems than it solves? Nobody knows for sure. But technology affects nonhuman animals as well, and we would be remiss to leave them out of our considerations.</p>\n<p>Read the rest in Fast Company: <a href=\"https://www.fastcompany.com/90796707/ai-could-fuel-factory-farming-or-end-it\">https://www.fastcompany.com/90796707/ai-could-fuel-factory-farming-or-end-it</a>.</p>\n", "user": {"username": "BrianK"}}, {"_id": "7RgRbd3gv5Xt77zno", "title": "Healthier Hens Y1 update including challenges so far and a call for funding", "postedAt": "2022-10-18T16:41:14.122Z", "htmlBody": "<h1>Key points&nbsp;</h1><ul><li>Our mission and approach remain largely unchanged</li><li>Our organisational capacity is growing</li><li>We spent most of our Y1 budget on staff, research and travel</li><li>We have made slight, pilot country-informed adjustments to our strategy</li><li>We will ramp up our program work in Y2</li><li>We are finding it rather difficult to fundraise, Y2 budget ($230k) is still not fully covered</li></ul><p>Healthier Hens (HH) received funding from Charity Entrepreneurship, EA Funds and individual donors until now. We need more funding to keep investigating promising dietary interventions to improve the welfare of cage-free hens and engaging cage-free egg farming stakeholders to adopt these interventions. You can&nbsp;<a href=\"https://give.healthierhens.com/\"><u>donate here</u></a>.</p><h1>Our mission and approach</h1><p>Keel bone fractures&nbsp;<a href=\"https://welfarefootprint.org/research-projects/laying-hens/\"><u>are the second biggest source</u></a> of hens\u2019 suffering after behavioral deprivation related to cages, and the biggest in cage-free systems. Our mission is to reduce suffering of hens addressing this biggest source of pain. Numerous studies have shown that&nbsp; dietary interventions reduce bone fractures. Our goal is to ensure that hens have adequate nutrition and experience less pain. We are doing that by outreach and advocacy targeted at d cage-free egg farming stakeholders (including farmers, feed mills and regulators) to implement these interventions. Please read our&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/4QGhyXjXM4yJBvNap/introducing-healthier-hens\"><u>introductory</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/W7avcmYNfZZwrfs7L/lessons-learned-from-country-visits-and-pilot-country\"><u>6M update</u></a> posts to learn more about the background of HH.&nbsp;</p><h1>Capacity, staff and organizational growth</h1><p>We used Y1 to grow in capacity. The corresponding activities involved setting up an advisory board \u2013 including African farmed animal welfare (FAW) movement, animal welfare and keel bone damage experts \u2013 joining an international FAW association focused on egg-laying hen welfare, visiting egg farms and feed producers in Kenya, and running initial animal welfare workshops. This has permitted us to be well aware of ongoing cage-free campaigns and existing hen welfare interventions, learn what practical obstacles the industry faces, and what knowledge/capacity gaps must be addressed to facilitate our program work in improving hen welfare.</p><p>After choosing our pilot country of operations, our main task was to hire a full-time Country Manager to facilitate our efforts on the ground. Our team has also seen two Research Interns joining our efforts to build knowledge capacity on how hen nutrition relates to welfare issues we are seeing on farms. Looking forward, we will recruit Data Collection and Operation Interns in Kenya, remote Research Interns and a hen welfare/nutrition specialist during Y2, which should bring the total of paid HH staff up to 8.</p><p>We had a total operational budget of $159k for Y1. Its break-down can be seen below. The majority of our funds were used for Co-founder and staff salaries, research (feed trial in Switzerland) and travel expenses (mainly related to, initially, country scoping and on-the-ground Kenya activities). Y2 will see more of our budget accounting for non Co-founder staff expenses and program expenses. The total estimated Y2 budget is currently at $230k, with $50k raised so far.&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994776/mirroredImages/7RgRbd3gv5Xt77zno/ma5qbkktx197k2pdwmfq.png\"></p><h1>Strategic updates</h1><p>Our country selection process took a significant part of Y1 to carry out (<a href=\"https://forum.effectivealtruism.org/posts/W7avcmYNfZZwrfs7L/lessons-learned-from-country-visits-and-pilot-country\"><u>read more about it here</u></a>). We are building up knowledge and networks within the local cage-free egg production and feed manufacturing industries via co-founder country visits and the work led by our Country Manager. The need to build reputation and establish actively connected networks within the different communities became apparent as a necessity to initiate pilot work. This is why we are ramping up the farmer training portion of our work while the other research activities are ongoing. The workshops will act as one of the first steps in enabling us to advance towards positive impact for the hens:</p><ul><li>Identify and recruit cage-free farmers for our pilot program work</li><li>Pilot proves the problem and effectiveness of the solution and is used to create outreach materials</li><li>Advocacy is carried out more effectively</li><li>Practices are being changed and implemented</li><li>Reduction in bone fractures is achieved</li><li>Hens experience less suffering</li></ul><p>If anything, our strategy has been updated even more towards making use of collaboration as seen by our partnership with the University of Bern, where a promising scalable dietary intervention will be assessed in terms of effect size, cost effectiveness and flow-through effects. We are also actively engaging other FAW NGOs and seeking opportunities to learn and contribute.</p><h1>Pilot operations</h1><p>The first six months on the ground has enabled us to uncover several key challenges/opportunities. Below is a quick list of some of the highlights:</p><ul><li><strong>There was a raw ingredient crisis in the country</strong> brought on and exacerbated by COVID, poor local crop yields and the war in Ukraine. This is potentially solved now by the lift of GMO ingredient import ban, which will reduce market tension and prices.</li><li><strong>The newly elected president owns one of the largest caged hen farms in the country.</strong> However, he is advocating strongly for the needs of farmers (e.g. the streamlining of the GMO import ban lift), which we intend to make use of by leveraging the farmers\u2019 needs for high-quality feeds.</li><li><strong>Many millers were hesitant to engage in new product development given the problematic raw material market.</strong> The incoming GMO imports should ease business. We will use farmer data gathered at farm visits and welfare workshops as an incentive for the millers to try a new product.</li><li><strong>Farmers are really hard pressed by increasing feed prices and often resort to switching to cheaper feed</strong> or mixing in alternative ingredients. The improving market dynamics and farmer-friendly government should alleviate this, allowing us to make a stronger case for the need to improve feed quality in the region.</li><li><strong>Feed intake rates are significantly higher in Kenya, rendering our initial estimates off.</strong> However, after making corresponding adjustments to the optimal key nutrient levels in the feed, we\u2019re still finding significant feed quality issues among commercial feed samples (at time of publication: 27%, 35%, 35% and 46% of the tested feeds were deficient in calcium, phosphorous, vitamin D3 and protein, respectively). The regional standards, on the other hand, are not as far off as initially evaluated for hens kept cage-free. The same cannot be said, unfortunately, for regulatory enforcement - we have updated our assessment as more pessimistic.</li></ul><p>Through ongoing farmer visits and a series of tailored hen welfare workshops, we are able to better understand how the farmers\u2019 desire to obtain higher quality feeds and eagerness to acquire farm management practices can be effectively leveraged to showcase demand for changes in how feed quality is regulated and ensured in the country.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994776/mirroredImages/7RgRbd3gv5Xt77zno/x806bx775vtrles7q8ul.jpg\"><figcaption>Cage-free farm visit in Kisumu county.</figcaption></figure><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994776/mirroredImages/7RgRbd3gv5Xt77zno/rzxg41udoybezr32376d.jpg\"><figcaption>Farmer hen welfare workshop in Nakuru county.</figcaption></figure><h1>Funding opportunities</h1><p>HH struggled to diversify its funding sources during Y1. Below are several concerns that were raised by potential funders, we provide our thinking behind the topics below and invite further discussion on what role exploratory FAW work such as ours should play.</p><ul><li><strong>Our approach is too short termist:</strong> Feed fortification as a way to decrease bone fractures is well studied. We plan to first implement that on a smaller scale to provide proof-of-concept of real-life implementation, and once sufficient adoption has been developed, advocate for increasing mandatory requirements for feed leading to long-lasting impact. Out of the three complementary approaches to addressing the widespread keel bone fracture issue (feed, housing/management and genetics) feed offers the most flexible and quick avenue to impact. We are still confident that it is an angle worth investigating and that the potential reductions in the incidence of fractures could be significant (current expectation: 5-20%). However, we do remain open to the possibility that once more information about the feasibility of improving genetics will be available, we will advocate for that as well.</li><li><strong>Our intervention is too risky:</strong> there are risks of our work impeding that of groups working on cage-free corporate outreach work and potentially leading to a net improvement of farming economics. We are taking mitigating measures to reduce these risks, i.e. choosing to target cage-free production instead of caged not to have double ask, being in close contact and collaboration with cage-free FAW NGOs and limiting the exposure of our intervention via controlled feed trials where we can also evaluate the effects on egg productivity. Additionally better feed is going to be more costly and therefore over the long term, reduce profitability.&nbsp;</li><li><strong>We do not approach the issue at a large enough scale:</strong> we have two pillars of operation: pilot on-the-ground work and research. While the former includes a limited amount of select collaborators (see main reasoning above), the latter includes a significant amount of resources into quantifying the cost-efficacy of an intervention geared specifically at large-scale farms, akin to those most prevalent in the Global North. Once proof of concept is established, our goals are to tackle the issue at scale, be it via regulation and enforcement or/and working at factory farm level, respectively.</li></ul><p>HH\u2019s most urgent funding need is operational funds, which would help us avoid slowing down, and reduce the amount of co-founder work dedicated to fundraising and potentially enabling the project to continue. We would like to raise a further $180k for Y2 operations (of $230k total budget) to sustain and continue our work without slowing down. These figures are updated as we continue working on the ground in our pilot country of operations and keep identifying promising alternative interventions to allocate research resources to. This accounts for $20k operational expenses per month. Our growth plan is also being updated and can be continuously monitored and discussed via our&nbsp;<a href=\"https://drive.google.com/file/d/13HyHjSavurGxkKOu-Ey1LsB0iiy2Jr4Y/view?usp=sharing\"><u>5 year plan</u></a>.</p><p>You can make your contribution&nbsp;<a href=\"https://give.healthierhens.com/\"><u>here</u></a>. We currently accept all major credit cards. If donating a large sum, please contact us at&nbsp;<a href=\"mailto:info@healthierhens.com\"><u>info@healthierhens.com</u></a> to initiate a bank transfer. If you have questions about donation opportunities, please email or&nbsp;<a href=\"https://calendly.com/healthierhens\"><u>book a meeting</u></a> with our Head of Logistics.&nbsp;</p><h1>Staying up to date</h1><p>To stay in the loop with what we\u2019re up to, consider&nbsp;<a href=\"https://forms.wix.com/a7f8deae-748e-4179-9f9a-867ac4b8b99f:77763fc3-a9d6-4416-9397-d453296af499\"><u>subscribing to our newsletter</u></a>. Please get in touch if you\u2019d like to comment, challenge our work or to suggest something in person. Thank you!</p><p><br><br>! Additional info and clarifications were added to the text on the 21st of October, 2022!</p>", "user": {"username": "lukasj10"}}, {"_id": "n87apR92D2kmWaSCQ", "title": "Shallow Report on Nuclear War (Abolishment)", "postedAt": "2022-10-18T07:36:44.622Z", "htmlBody": "<p><strong>Note</strong>: This report was produced with only one week of desktop research, for the purpose of identifying promising causes to evaluate at depth. We only have low confidence in our findings here, and the conclusions should generally be taken by readers as merely suggestive rather determinative.</p><h1><br><strong><u>Summary</u></strong></h1><p><a href=\"https://docs.google.com/spreadsheets/d/19_fOaBmQ9MwD2cyKaT-8eQorWzx_xdDr-VM8-XISD0g\"><u>Taking into account</u></a> the expected benefits of denuclearization (i.e. fewer deaths and injuries from nuclear war), the expected costs (i.e. more deaths and injuries from conventional war due to weakened deterrence), and the tractability of lobbying for denuclearization, I find that the marginal expected value of lobbying for denuclearization to be&nbsp;<strong>248 DALYs per USD 100,000</strong>, which is around 40% as cost-effective as giving to a GiveWell top charity.</p><p>Discussion:</p><ul><li>The results potentially overestimate the risks insofar as I focus on the three major nuclear war risks (i.e. NATO-Russia, US-China and India-Pakistan), and leave off the more minor flash points (e.g. Israel-Iran, NK-US) when calculating the average deaths from nuclear war.</li><li>On the other hand, the results may also potentially underestimate the risks insofar as existing evaluations of the probability of nuclear war, which this analysis relies on, likely over-anchor on major conflicts and neglect the additional risks from more minor ones. This effect balances out the first issue, but to what extent, it is hard to say.</li><li>Results also fail to take into account the changed risk in war between NATO, the US, Russia, China, India or Pakistan and other countries, whether nuclear or non-nuclear, though this is unlikely to be significant insofar as no plausible estimate of the decreased chance of conventional war can outweigh the risks of nuclear war.</li><li>Results are very sensitive to hard-to-calibrate tractability estimates, and reasonable people can disagree (e.g. on the right outside views to use; on inside view estimates; and on how you weigh the two). A lot of the calls here rely heavily on my pre-existing academic and professional knowledge of geopolitics, but my knowledge is far from perfect, and in any case reasonable people will disagree.</li><li>It would be valuable to consult experts on tractability \u2013 especially country-specific national security experts \u2013 if deeper research is done.</li><li>There are potentially more tractable ways of reducing risk that aren't lobbying for denuclearization (e.g. mitigation work through strengthening global food systems, like what ALLFED does), which I do not evaluate here.</li></ul><h1><strong><u>Expected Benefit: Averting Nuclear War Fatalities</u></strong></h1><p>The main expected benefit of denuclearization is, of course, preventing people from dying from nuclear war. I model this in the following way.</p><p><strong><u>Moral Weights</u></strong>: I take the value of averting one death to be&nbsp;<strong>29.3 DALYs</strong>. This is calculated as a function of (a) a human's full healthy life expectancy of 63.69, (b) a minor age-based philosophical discount, and (c) assuming we save someone of the median age in the relevant population. For more details, refer to&nbsp;<a href=\"https://docs.google.com/document/d/1j67pOUpC5vhC1-H516PAj2-4Fvm5rxJhZNeBVav3kik/edit\"><u>CEARCH's evaluative framework</u></a>.</p><p><strong><u>Scale</u></strong>: I calculate the average number of deaths across the three major nuclear war scenarios: NATO-Russia, US-China, and India Pakistan. In each case I take into account both direct violent death as well as death by starvation from nuclear winter. Multiple estimates are used and aggregated in each case to even out random error.</p><p>For the direct deaths from nuclear exchange in the NATO-Russia case, I take&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/FfxrwBdBDCg9YTh69/how-many-people-would-be-killed-as-a-direct-result-of-a-us\"><u>Luisa's estimate</u></a> (51 million deaths),&nbsp;<a href=\"https://sgs.princeton.edu/the-lab/plan-a\"><u>Wellerstein et al's estimate</u></a> (34.1 million deaths) and&nbsp;<a href=\"https://physicstoday.scitation.org/doi/10.1063/1.3047679\"><u>Toon, Robock &amp; Turco's estimate</u></a> that considers a 1,100 warheads US attack on Russia and 1,000 warheads Russia attack on the US (100,000,000 deaths), and then calculate a weighted average (37.43 million deaths). A far greater weighting given to the Wellerstein et al estimate, due to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/FfxrwBdBDCg9YTh69/how-many-people-would-be-killed-as-a-direct-result-of-a-us\"><u>Luisa's estimate</u></a> relying on a probability distribution of countervalue targeting that isn't empirically well-grounded, and&nbsp;<a href=\"https://physicstoday.scitation.org/doi/10.1063/1.3047679\"><u>Toon, Robock &amp; Turco</u></a> looking only at countervalue targeting, making it a likely overestimate.</p><p>For the deaths from starvation in nuclear winter in the NATO-Russia case, I take&nbsp;<a href=\"https://forum.effectivealtruism.org/s/KJNrGbt3JWcYeifLk/p/pMsnCieusmYqGW26W\"><u>Luisa's estimate</u></a> (5.5 billion deaths),&nbsp;<a href=\"https://www.nature.com/articles/s43016-022-00573-0\"><u>Xia et al's estimate</u></a> (5 billion deaths), and&nbsp;<a href=\"https://link.springer.com/book/10.1007/978-1-4612-5288-7%20&amp;%20https://data.worldbank.org/indicator/SP.POP.TOTL\"><u>Harwell's older estimate</u></a> as adjusted for population growth (1.65 billion deaths), and then calculate a simple average (4.05 billion deaths).</p><p>In total, this yields around 4.09 billion deaths after a NATO-Russia nuclear exchange.</p><p>For the direct deaths from nuclear exchange in the US-China case, I take&nbsp;<a href=\"https://nuke.fas.org/guide/china/Book2006.pdf\"><u>Kristensen, Norris &amp; McKinzie's estimate</u></a> (14.14 million deaths), by averaging the fatality figures from multiple scenarios to obtain US and Chinese fatalities respectively and then summing them. I also take&nbsp; <a href=\"https://physicstoday.scitation.org/doi/10.1063/1.3047679\"><u>Toon, Robock &amp; Turco's estimate</u></a> (217.57 million deaths), by considering a 1,100 warheads US attack on China and 200 warhead Chinese attack on the US, with the casualties under the latter calculated by (a) assuming that China launches its full 200 warhead arsenal, (b) taking that casualties scale with diminishing marginal returns from warhead use due to larger cities being destroyed first, and (c) using the 1,000 warheads Russian attack on the US as a baseline for casualties. Finally, I take an&nbsp;<a href=\"https://nationalinterest.org/blog/reboot/nuclear-war-china-and-russia-335000000-dead-starters-192271\"><u>extrapolation of Miyazaki's estimate</u></a> (179.64 million deaths). Here, I calculate Chinese fatalities by averaging the US's Alert Force and Full Force attacks to obtain a casualty ratio that is then applied to&nbsp;<a href=\"https://data.worldbank.org/indicator/SP.POP.TOTL?locations=CN\"><u>China's total population</u></a>, the result of which is then subject to the fatality-to-casualty ratio from&nbsp;<a href=\"https://physicstoday.scitation.org/doi/10.1063/1.3047679\"><u>Toon, Robock &amp; Turco</u></a>. Meanwhile, I calculate US fatalities by taking the US death figures from Soviet attack as a baseline and adjusting for the smaller Chinese arsenal by (similar to above) assuming that China launches all 200 of its warheads, and that casualties scale with diminishing marginal returns from warhead use due to larger cities being destroyed first. And all that done, I take a weighted average of the three estimates (44.89 million deaths), where a far greater weighting is given to the Kristensen, Norris &amp; McKinzie estimate due to Toon, Robock &amp; Turco looking only at countervalue targeting, making it a likely overestimate, and due to the extrapolated Miyazaki estimate relying on additional uncertain extrapolations.</p><p>For the deaths from starvation in nuclear winter in the US-China case, I use the same figure calculated for NATO-Russia (4.05 billion deaths).</p><p>In total, this yields around 4.09 billion deaths after a NATO-Russia nuclear exchange.</p><p>For the direct deaths from nuclear exchange in the India-Pakistan case, I take&nbsp;<a href=\"https://www.science.org/doi/10.1126/sciadv.aay5478\"><u>Toon et al's estimate</u></a> (87.5 million deaths), a&nbsp;<a href=\"https://www.nytimes.com/2002/05/27/world/12-million-could-die-at-once-in-an-india-pakistan-nuclear-war.html\"><u>US intelligence estimate</u></a> (10.5 million deaths), and&nbsp;<a href=\"https://www.amazon.sg/Out-Nuclear-Shadow-Smitu-Kothari/dp/1842770594\"><u>McKinzie et al's estimate</u></a> (2.9 million deaths), and then calculate a weighted average (74.03 million deaths) \u2013 a much higher weightage is put on the more&nbsp;<a href=\"https://www.science.org/doi/10.1126/sciadv.aay5478\"><u>recent&nbsp;</u></a>Toon et al estimate vs the other two&nbsp;<a href=\"https://www.nytimes.com/2002/05/27/world/12-million-could-die-at-once-in-an-india-pakistan-nuclear-war.html\"><u>considerably&nbsp;</u></a><a href=\"https://www.amazon.sg/Out-Nuclear-Shadow-Smitu-Kothari/dp/1842770594\"><u>older&nbsp;</u></a>ones, since both India and Pakistan have been increasing their number of nuclear weapons over the past two decades</p><p>For the deaths from starvation in nuclear winter in the India-Pakistan case - this is more limited than in the superpower scenarios, and I calculate this by taking&nbsp;<a href=\"https://www.nature.com/articles/s43016-022-00573-0\"><u>Xia et al's estimate</u></a> (2 billion deaths),&nbsp;<a href=\"http://www.kultur-des-friedens.de/commonFiles/pdfs/Verein/KDF/helfand.pdf\"><u>Helfand's estimate</u></a> (1 billion deaths) and&nbsp;<a href=\"https://climate.envsci.rutgers.edu/pdf/RobockToonSciAmJan2010.pdf\"><u>Robock and Toon's estimate</u></a> (1 billion deaths), and taking a single average (1.33 billion deaths).</p><p>In total, this yields around 1.51 billion deaths after an India-Pakistan nuclear exchange.</p><p>And finally, I weigh each individual outcome (i.e. NATO/Russia nuclear war, US/China nuclear war, and India/Pakistan nuclear war) by the relative probability of occurrence, as Luisa has most helpfully collated&nbsp;<a href=\"https://forum.effectivealtruism.org/s/KJNrGbt3JWcYeifLk/p/MsJvzmYLMpsdJBb6C\"><u>here</u></a>, to calculate the total deaths from an average nuclear war (<strong>2.51 billion deaths</strong>).</p><p><strong><u>Persistence</u></strong>: The risk of nuclear war persists over time, and solving it brings long-term benefits. When summing the per annum benefits, however, they have to be discounted by various factors.</p><p>Firstly, I discount for the probability of the solution not persisting (i.e. renuclearization even after initial denuclearization success). As a baseline, we have to note that&nbsp;<a href=\"https://en.wikipedia.org/wiki/Nuclear_disarmament\"><u>no state</u></a> that has denuclearized (i.e. South Africa, Ukraine, Belarus &amp; Kazakhstan) have yet renuclearized, which should put our priors close to zero. At the same time, it is clear that there is a temptation for states to renuclearize if their security conditions worsen (c.f.&nbsp;<a href=\"https://uawire.org/zelensky-ukraine-may-reconsider-its-nuclear-status\"><u>Zelensky's statements</u></a> on the Budapest Memorandum). Hence, I assign around a&nbsp;<strong>0.1%</strong> per annum chance to renuclearization.</p><p>Secondly, I discount for the probability of the problem being counterfactually solved anyway (i.e. countries deciding to denuclearize sans intervention).&nbsp;<a href=\"https://en.wikipedia.org/wiki/List_of_states_with_nuclear_weapons\"><u>One potential way</u></a> of calculating the reversal rate is to look at the number of years in which countries have given up their nuclear weapons, divided by the total number of years in which states have had nuclear weapons. This would theoretically give a base rate of denuclearization of 0.008, given 4 country-years where denuclearization (by South Africa, Ukraine, Belarus and Kazakhstan) occurred, out of a potential 504 country-years (where countries are nuclear armed). However, this approach is fatally flawed insofar as there is self-selection into denuclearization by countries who do not have significant security concerns such that the true base rate for the remaining nuclear powers will be far lower.</p><p>Instead, I approach this issue in the following way. First, let us consider what actors are looking to improve the situation. On the governmental side, there are&nbsp;<a href=\"https://www.armscontrol.org/act/2014-10/features/art-possible-future-p5-process-nuclear-weapons\"><u>some attempts</u></a> at moving the P5 and hence the world towards denuclearization. Similarly, there are anti-nuclear charities (e.g.&nbsp;<a href=\"https://www.icanw.org/\"><u>ICAN&nbsp;</u></a>and the like) working on the matter. In contrast, businesses do not see this as something that concerns them. On the whole, therefore, I take the base rate of denuclearization to be the sum of the chances of pro-denuclearization voices within government succeeding in pushing for self-reform, and of non-profit lobbying from the outside being successful; each of them I take to be equal to the general denuclearization lobbying success rate of 0.00000016 (the calculations of which are discussed in the tractability section of this report). Further, I very marginally adjust the reversal rate up by 0.02 relative to the base due to two favourable trends - economic growth increasing the chance of democratization and hence democratic peace, thus reducing security concerns and increasing the odds of denuclearization; and also cultural shifts bringing increased liberalization and hence the chance of regime change (e.g. in Russia and China) or reduced nationalism (e.g. with respect to India and Pakistan), which will in turn reduce security tensions and increase the odds of denuclearization. Overall, this gives a reversal rate of&nbsp;<strong>0.00003%</strong>.</p><p>Thirdly, I discount for the probability of the world being destroyed anyway (i.e. a general existential risk discount). Here, I take into account the probability of total nuclear annihilation, since the benefits of saving people from nuclear war in one year is nullified if they had already died in a previous year. For the exact risk of total nuclear annihilation, I take it to be one magnitude lower than the risk of nuclear war itself (discussed below), since nuclear war may not kill everyone. Of course, this probability will shift as a result of any efforts to denuclearize, but the chances of success are sufficiently small (as will be discussed) that it does not materially change our results. I do not take into account other existential risks like supervolcano eruption and asteroid impact, since the chances of those occurring at all is very&nbsp;<a href=\"https://www.amazon.com/Feeding-Everyone-Matter-What-Catastrophe/dp/0128044470\"><u>marginal</u></a>, let alone the chances of such events killing everyone and not just most people. Overall, therefore, I treat the general existential risk discount to be just the risk of nuclear war but adjusted a magnitude down, or&nbsp;<strong>0.07%</strong>.</p><p>Fourthly, I apply a broad uncertainty discount of&nbsp;<strong>0.1%</strong> to take into account the fact that there is a non-zero chance that in the future, the benefits or costs do not persist for factors we do not and cannot identify in the present (e.g. actors directing resources to solve the problem when none are currently doing so).</p><p><strong><u>Value of Outcome</u></strong>: Overall, the raw perpetual value of averting nuclear war fatalities is&nbsp;<strong>2.75 * 10<sup>13</sup> DALYs</strong>.</p><p><strong><u>Probability of Occurrence</u></strong>: Of course, the chances of nuclear war occurring (and hence the chance that it is a problem whose solution brings benefits) is very low indeed. To calculate the probability of nuclear war, I take a weighted average of the probabilities collated by&nbsp;<a href=\"https://forum.effectivealtruism.org/s/KJNrGbt3JWcYeifLk/p/PAYa6on5gJKwAywrF\"><u>Luisa</u></a>: the probability of nuclear weapons being used during war as based on historical frequency (1.4%), of accidental nuclear war between the US and Russia as based on fault tree analysis and historical frequency (0.9%), of nuclear attack as based on expert survey (2.21%), of nuclear war killing at least 1 million as based on expert survey (0.39%), and nuclear detonation by a state actor causing at least 1 fatality as based on superforecaster prediction (0.4%). The weighting significantly favours the superforecasters' more conservative prediction, because of the following reasons. Firstly, the estimate of the probability of intentional nuclear war based on historical frequency is likely biased upwards due to historical use being in a MAD-free context. Secondly, the probability of accidental nuclear war based on historical close calls is highly uncertain due to the difficulty of translating close calls to actual probabilities of eventual launch. And thirdly, experts are notoriously bad at long-range forecasts, relative to superforecasters. Overall, this weighted average produced a per annum probability of nuclear war of&nbsp;<strong>0.7%</strong>.</p><p><strong><u>Expected Value</u></strong>: Overall, the expected value of averting nuclear war fatalities is&nbsp;<strong>1.88 * 10<sup>11</sup> DALYs</strong>.</p><h1><strong><u>Expected Benefit: Averting Nuclear War Injuries</u></strong></h1><p>Beyond preventing death, another expected benefit of denuclearization is preventing injuries from nuclear war. My modelling is as follows.</p><p><strong><u>Moral Weights</u></strong>: I take the value of averting a typical injury in nuclear war to be&nbsp;<strong>5.88 DALYs</strong>. This is calculated as a function of (a) the average disability weight for all injuries, (b) a minor age-based philosophical discount and (c) assuming we save someone of the median age in the relevant population. For more details, refer to&nbsp;<a href=\"https://docs.google.com/document/d/1j67pOUpC5vhC1-H516PAj2-4Fvm5rxJhZNeBVav3kik/edit\"><u>CEARCH's evaluative framework</u></a>.</p><p><strong><u>Scale</u></strong>: The approach I take to injuries is the same as the one I took to fatalities, in that I calculate the average number of injuries across the three major nuclear war scenarios of NATO-Russia, US-China, and India Pakistan, with the aggregation of multiple estimates used for robustness.</p><p>For injuries from nuclear exchange in the NATO-Russia case, I take&nbsp;<a href=\"https://sgs.princeton.edu/the-lab/plan-a\"><u>Luisa's fatality estimate</u></a> and extrapolate using the Wellerstein et al ratio of fatalities to injuries (85.85 million injuries), the&nbsp;<a href=\"https://sgs.princeton.edu/the-lab/plan-a\"><u>Wellerstein et al estimate</u></a> itself (57.4 million injuries) and the&nbsp;<a href=\"https://physicstoday.scitation.org/doi/10.1063/1.3047679\"><u>Toon, Robock &amp; Turco estimate</u></a> (75 million injuries), to get a weighted average (58.56 million injuries) that gives far greater weightage to the Wellerstein et al estimates for reasons discussed in the previous section on nuclear war fatalities.</p><p>For injuries from nuclear exchange in the US-China case, I take the&nbsp;<a href=\"https://nuke.fas.org/guide/china/Book2006.pdf\"><u>Kristensen, Norris &amp; McKinzie</u></a> estimate (24.35 million injuries). by averaging the fatality figures from multiple scenarios to obtain US and Chinese fatalities respectively and then summing them. I also take the&nbsp;<a href=\"https://physicstoday.scitation.org/doi/10.1063/1.3047679\"><u>Toon, Robock &amp; Turco estimate</u></a> (142.57 million injuries), deploying the same methodology and assumptions as when calculating US-China nuclear war fatalities. Finally, I take an&nbsp;<a href=\"https://nationalinterest.org/blog/reboot/nuclear-war-china-and-russia-335000000-dead-starters-192271\"><u>extrapolation of Miyazaki's estimate</u></a> (89.96 million injuries), using the same methodology as when extrapolating for US-China nuclear war fatalities, except with the extra step of applying the injury-to-casualty ratio from Toon, Robock and Turco when calculating US injuries. Ths yields a weighted average (39.63 million injuries), where I give a far greater weight to the Kristensen, Norris &amp; McKinzie estimate for reasons mentioned in the previous section.</p><p>For injuries from nuclear exchange in the India-Pakistan case, I use the Toon et al estimate of fatalities, appling a 1:1 fatality-to-injury ratio as estimated by a&nbsp;<a href=\"https://www.science.org/doi/10.1126/sciadv.aay5478\"><u>previous study</u></a> on India-Pakistan nuclear war (87.5 million injuries). I also use a&nbsp;<a href=\"https://www.nytimes.com/2002/05/27/world/12-million-could-die-at-once-in-an-india-pakistan-nuclear-war.html\"><u>US intelligence estimate</u></a> (4.5 million). Further, I utilize the&nbsp;<a href=\"https://www.amazon.sg/Out-Nuclear-Shadow-Smitu-Kothari/dp/1842770594\"><u>McKinzie et al estimate</u></a> (1.5 million injuries). A weighted average is then calculated (73.42 million injuries), favouring the Toon et al estimate for reasons previously outlined.</p><p>All in all, weighing each war scenario by&nbsp;<a href=\"https://forum.effectivealtruism.org/s/KJNrGbt3JWcYeifLk/p/MsJvzmYLMpsdJBb6C\"><u>relative probability of occurrence</u></a>, that gets us to&nbsp;<strong>63.72</strong> million injuries in an average nuclear war.</p><p><strong><u>Persistence</u></strong>: The same discounts discussed in the section on nuclear war fatalities are applied here.</p><p><strong><u>Value of Outcome</u></strong>: Overall, the raw perpetual value of averting nuclear war injuries is&nbsp;<strong>1.4 * 10<sup>11</sup> DALYs.</strong></p><p><strong><u>Probability of Occurrence</u></strong>: The same probability of nuclear war calculated previously is applied.</p><p><strong><u>Expected Value</u></strong>: All in all, the expected value of averting nuclear war injuries is&nbsp;<strong>9.56 * 10<sup>8</sup> DALYs</strong>.</p><h1><strong><u>Expected Cost: More Conventional War Fatalities</u></strong></h1><p>We've been discussing the benefits of denuclearization, but it is important not to forget that it does come with an expected cost \u2013 that of more conventional war fatalities - insofar as nuclear weapons deter conventional conflict. To model this, I incorporate the following variables.</p><p><strong><u>Moral Weights</u></strong>: As described in the nuclear war fatalities section.</p><p><strong><u>Scale</u></strong>: I look at the deaths from conventional war between NATO-Russia, US-China and India-Pakistan, with the aggregation of multiple estimates consistently used.</p><p>For deaths from conventional war between NATO and Russia, I use various outside views to estimate this. My first reference class is the war that most resembles a NATO-Russia conflict \u2013 the&nbsp;<a href=\"https://en.wikipedia.org/wiki/2022_Russian_invasion_of_Ukraine\"><u>2022 Russian invasion of Ukraine</u></a> (72,000 deaths) \u2013 assuming the mid-point of the conflict is the present day of 1st October 2020 (i.e. the war continues for an additional period of time equal to how long it has lasted so far). My second reference class is a recent war involving the US \u2013 the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Casualties_of_the_Iraq_War\"><u>Iraq War&nbsp;</u></a>&nbsp;(409,000 deaths). And my third reference class is another recent war involving Russia \u2013 the&nbsp;<a href=\"https://en.wikipedia.org/wiki/War_in_Donbas_(2014%E2%80%932022)\"><u>War in Donbass</u></a>, preceding the current invasion (12,000 deaths). This yields a weighted average (95,000 deaths), using a far higher weight for the 2022 Russian invasion of Ukraine, as it is most like a NATO-Russia war, in terms of weapons used and doctrines employed.</p><p>For deaths from conventional war between the US and China, I again use various outside views. My first reference class is the last US-China war \u2013 the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Korean_War\"><u>Korean War</u></a> (3.33 million deaths). My second reference class is a recent US war - so Iraq again. And my third reference class is a recent war involving China (for a given definition of recent, one supposes) \u2013 the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Sino-Vietnamese_War\"><u>Sino-Vietnamese War</u></a> (56,000). This yields a weighted average (2.81 million deaths), wherein the Korean War is weighted most heavily given that it actually involved full-scale industrial warfare between US and China.</p><p>For deaths from conventional war between India and Pakistan, our job is easier insofar as the outside views we consult are simply past India-Pakistan Wars \u2013 the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Indo-Pakistani_War_of_1947%E2%80%931948\"><u>Indo-Pakistani War of 1947\u20131948</u></a> (9000 deaths); the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Indo-Pakistani_War_of_1965\"><u>Indo-Pakistani War of 1965</u></a> (3000 deaths), with fatalities calculated by taking casualties and applying the fatality-to-injury ratio from&nbsp;<a href=\"https://en.wikipedia.org/wiki/Indo-Pakistani_War_of_1947%E2%80%931948\"><u>the previous war</u></a>; and finally the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Indo-Pakistani_War_of_1971\"><u>Indo-Pakistani War of 1971</u></a>, inclusive of the&nbsp;<a href=\"https://en.wikipedia.org/wiki/1971_Bangladesh_genocide\"><u>Bangaldesh genocide</u></a> (1.66 million deaths). A simple average is then calculated (558,000 deaths) \u2013 the 1971 war is not penalized for the genocide, given the real potential for ethnic cleansing in the context of increased Hindutva nationalism and hostility to Muslims (on the Indian side), and obviously the Pakistani Army's history of genocide (on the other).</p><p><strong><u>Persistence</u></strong>: The same discounts as before are applied.</p><p><strong><u>Value of Outcome</u></strong>: Overall, the raw perpetual disvalue of more fatalities from conventional war between NATO-Russia, US-China and India-Pakistan are&nbsp;<strong>-1.04 * 10<sup>9</sup> DALYs</strong>,&nbsp;<strong>-3.08 * 10<sup>10</sup> DALYs</strong>, and&nbsp;<strong>-6.01 * 10<sup>9</sup> DALYs</strong> respectively.</p><p><strong><u>Probability of Occurrence</u></strong>: Relying on the&nbsp;<a href=\"https://www.taylorfrancis.com/books/edit/10.4324/9781315683638/nonproliferation-policy-nuclear-posture-neil-narang-erik-gartzke-matthew-kroenig?refId=dedb0075-1e3d-4654-800f-32288b9d8ba8&amp;context=ubx\"><u>academic literature</u></a>, I estimate the reduction in per annum probability of conventional war between NATO-Russia, US-China and India-Pakistan as<strong> 0.000133</strong> (sum of the reduced risk that NATO attacks Russia and vice versa),&nbsp;<strong>0.000133&nbsp;</strong>(sum of the reduced risk that the US attacks China and vice versa) and&nbsp;<strong>0.000067</strong> (reduced risk that India attacks Pakistan) respectively. Note that the base reduction in the risk that a country is attacked when it has nuclear weapons is 0.0000667 - but only if it has a potential first use nuclear posture (whether officially as for the US, Russia, and Pakistan, or arguably de facto with respect to Chinese policy on Taiwan); if a country has a official and de facto no first use policy, as for India, there is no measurable deterrent value.</p><p><strong><u>Expected Value</u></strong>: All in all, the expected disvalue of more conventional fatalities is<strong> -4.65 *&nbsp; 10<sup>6</sup> DALYs</strong>.</p><h1><strong><u>Expected Cost: More Conventional War Injuries</u></strong></h1><p>Another expected cost is that of more conventional war injuries, which I model as such.</p><p><strong><u>Moral Weights</u></strong>: As described in the nuclear war injuries section.</p><p><strong><u>Scale</u></strong>: I look at the injuries from conventional war between NATO-Russia,US-China and India-Pakistan, with the aggregation of multiple estimates consistently used.</p><p>For injuries from conventional war between NATO-Russia, I utilize the same reference classes as when calculating fatalities \u2013 the&nbsp;<a href=\"https://en.wikipedia.org/wiki/2022_Russian_invasion_of_Ukraine\"><u>2022 Russian invasion of Ukraine</u></a> (188,000 injuries),&nbsp;<a href=\"https://en.wikipedia.org/wiki/Casualties_of_the_Iraq_War\"><u>the Iraq War</u></a> (254,000 injuries), and the&nbsp;<a href=\"https://en.wikipedia.org/wiki/War_in_Donbas_(2014%E2%80%932022)\"><u>War on Donbass</u></a> (30,000 injuries) \u2013 and calculate the weighted average (180,000 injuries) using a far higher weightage for the Ukraine War, again because it most resembles a potential NATO-Russia conflict.</p><p>For injuries from conventional war between US-China, I employ the same reference classes as for fatalities \u2013&nbsp;<a href=\"https://en.wikipedia.org/wiki/Korean_War\"><u>the Korean War</u></a> (1.25 million injuries), the Iraq War (254,000 injuries), and the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Sino-Vietnamese_War\"><u>Sino-Vietnamese War</u></a> (69,000 injuries) \u2013 and calculate the weighted average (1.07 million injuries) using a far higher weightage for the Korean War, again because it actually involves the US and China fighting a full-scale industrial war.</p><p>For injuries from conventional war between India-Pakistan, I once more resort to the reference classes of past conflicts between the two countries \u2013 the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Indo-Pakistani_War_of_1947%E2%80%931948\"><u>Indo-Pakistani War of 1947\u20131948</u></a> (17000 injuries); the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Indo-Pakistani_War_of_1965\"><u>Indo-Pakistani War of 1965</u></a> (4000 injuries), with fatalities calculated by taking casualties and applying the fatality-to-injury ratio from&nbsp;<a href=\"https://en.wikipedia.org/wiki/Indo-Pakistani_War_of_1947%E2%80%931948\"><u>the previous war</u></a>; and finally the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Indo-Pakistani_War_of_1971\"><u>Indo-Pakistani War of 1971</u></a> (32,000 injuries). A simple average is then computed (18,000 injuries).</p><p><strong><u>Persistence</u></strong>: The same discounts discussed in the section on nuclear war fatalities are applied here.</p><p><strong><u>Value of Outcome</u></strong>: Overall, the raw perpetual disvalue of more injuries from conventional war between NATO-Russia, US-China and India-Pakistan are&nbsp;<strong>-3.95 * 10<sup>8</sup> DALYs</strong>,&nbsp;<strong>-2.35 * 10<sup>9</sup> DALYs</strong>, and&nbsp;<strong>-3.92 * 10<sup>7</sup> DALYs</strong> respectively.</p><p><strong><u>Probability of Occurrence</u></strong>: Same probabilities as discussed in the previous section on conventional war fatalities.</p><p><strong><u>Expected Value</u></strong>: In sum, the expected disvalue of more conventional war injuries is&nbsp;<strong>-3.69 * 10<sup>5</sup> DALYs</strong>.</p><h1><strong><u>Tractability</u></strong></h1><p>To assess the tractability of working on denuclearization \u2013 specifically, through lobbying governments \u2013 I use both an inside view (developed through a Fermi estimate that breaks down successful denuclearization into the necessary steps) as well as an outside view.</p><p>Let us discuss the inside view first. Since no government would willingly give up their nuclear weapons when their rivals \u2013 who potentially threaten them \u2013 continue to possess nuclear weapons, the question of whether a country X will give up nuclear weapons basically reduces to whether they will agree to denuclearize,&nbsp;<i>conditional on their rivals agreeing to denuclearize</i>. In certain cases, it will also be&nbsp;<i>conditional on a country's superpower ally agreeing</i>, since the latter will put effective pressure on the former to follow along with nuclear disarmament.</p><p>First, let us consider the probability of persuading the US to denuclearize conditional on Russia and China agreeing to denuclearize. While NATO has conventional superiority against Russia, especially post-Ukraine, there will be little trust that Russia will adhere to the terms of denuclearization over an extended period, given its tearing up the Budapest Memorandum and its flouting the Intermediate-Range Nuclear Forces (INF) Treaty, and also given the general air of distrust after Ukraine. On the China side of things, while the United States still retains conventional superiority for now, the balance of power is likely to change in such a way that disfavours the United States, keeping nuclear weapons to deter conventional aggression against itself and its allies more attractive. Overall, I rate the probability of success here to be around 10%.</p><p>Second, we have the probability of persuading the UK to denuclearize conditional on Russia and the US agreeing to denuclearize. At baseline, the same considerations vis a vis Russia apply (i.e. distrust over disregarded treaties, plus the general ill-will over the Ukraine invasion). Moreover, while the US would probably put pressure on the UK to agree if it itself agreed to denuclearize, this is counterbalanced by the uncertainty as to whether the US will commit to European security in the long term, especially if it pivots to Asia, all of which would create some pressure for the UK to retain its nuclear weapons as a hedge against a future Russian threat. Overall, I rate the probability of success here to be around 10%.</p><p>Third, we have the probability of persuading France to denuclearize conditional on Russia and the US agreeing to denuclearize. The same considerations discussed with respect to the UK apply here. Overall, I rate the probability of success here to be around 10%.</p><p>Fourth, we have the probability of persuading Russia to denuclearize conditional on NATO and China agreeing to denuclearize. This is extremely unlikely, given that Russia is in a position of conventional military inferiority relative to NATO, and would (in its view, fairly or not) be risking potential NATO military intervention within its sphere of influence (e.g. Ukraine, Belarus, Georgia) or invasion of Russia itself, if it did not have its nuclear deterrence. Further, the Russians do have to worry to some extent about the Chinese, who are aggressive in expanding their self-interest and who they do not trust not to take advantage of them. Overall, I rate the probability of success here to be around 1%.</p><p>Fifth, we have the probability of persuading China to denuclearize conditional on the US and India agreeing to denuclearize. China is currently still in a position of conventional inferiority relative to the United States, and nuclear deterrence is key to potentially warding the Americans off from intervening and supporting Taiwan when China attempts reunification \u2013 which is of course a key objective for them given the degree of nationalistic fervour over Taiwan being part of China, and given the CCP's legitimacy being tied up in national restoration. On the India side of things, China would be fairly comfortable with denuclearization given its conventional superiority; indeed, denuclearization would give it the chance to annex Indian territory without risking nuclear retaliation. Overall, I rate the probability of success here to be around 1%.</p><p>Sixth, we have the probability of persuading India to denuclearize conditional on Pakistan and China agreeing to denuclearize. On the Pakistan side, India holds comfortable conventional superiority, and any denuclearization would indeed favour it given that it would hence be able to take the whole of Kashmir or indeed more Pakistani territory without fear of nuclear retaliation. On the China side, however, India is in the opposite position of military inferiority, and would not give up its ability to deter significant conventional aggression on its northern border. Overall, I rate the probability of success here to be around 1%.</p><p>Seventh, we have the probability of persuading Pakistan to denuclearize conditional on India agreeing to denuclearize. This is extremely unlikely, and then some. Pakistan is in a position of conventional inferiority, and without its nuclear weapons to deter conventional aggression, it would fear not just losing a war, but potentially the annexation of the whole country in the (perceived) manner of the Bangladeshi War of Liberation, or indeed ethnic cleansing in the manner of the Partition of India or other anti-Muslim pogroms within India (c.f. Gujarat). Overall, I rate the probability of success here to be around 0.1%.</p><p>Finally, we have to consider the probability of coordinating various persuasion efforts such that they occur around the same time frame. This would be necessary, because in-principle agreement for denuclearization cannot be obtained without at least some material expression of interest by potential foes in denuclearization. That said, while such coordination would be difficult, the problem is not insurmountable. Overall , I rate the probability of success here to be around 10%.</p><p>Multiplying this through, this puts the inside view probability of persuading states to denuclearize at 0.00000000001% \u2013 vanishingly low.</p><p>Moving on to the outside view, I use three reference classes.</p><p>The first is the failure of the International Campaign to Abolish Nuclear Weapons (ICAN) to persuade any of the&nbsp;<a href=\"https://www.icanw.org/how_is_your_country_doing\"><u>existing nuclear powers</u></a> to denuclearize, even as it played&nbsp;<a href=\"https://www.nobelprize.org/prizes/peace/2017/ican/facts/\"><u>no role</u></a> in the denuclearizing of the four previous nuclear powers that did actually denuclearize (i.e. a literal 0% success rate). ICAN is a useful benchmark here, because it is probably the most successful nuclear disarmament NGO to date \u2013 it contributed to the ratification of the Treaty on the Prohibition of Nuclear Weapons (TPNW), and won a Nobel Peace Prize in the process.</p><p>The second reference class is the success of negotiations (starting from 1980) in eventually bringing the&nbsp;<a href=\"https://www.un.org/disarmament/wmd/chemical/\"><u>Chemical Weapons Convention</u></a> into force (in 1997). I calculate the success rate (i.e. 5.55% chance recurring) by taking the year in which ratification occured, including by the key powers of the US and USSR, and divide through by the years in which they could have ratified whether they actually did or not.</p><p>The third reference class is the success of the 1968 British proposal that eventually led to the 1975&nbsp;<a href=\"https://www.un.org/disarmament/biological-weapons/about/history\"><u>Biological Weapons Convention</u></a>, to which both the US and USSR were parties. I calculate the success rate (i.e. 12.5%) by taking the years in which ratification occured, including by the key powers of the US and USSR, and divide through by the years in which they&nbsp;<i>could&nbsp;</i>have ratified whether they actually did or not.</p><p>Then, I take a weighted average of the three reference class probabilities (0.000018%), with a far higher weight placed on the actual nuclear case study compared to the chemical and biological ones. This is for three reasons. Firstly, security considerations militate against denuclearization in a way they do not for chemical and biological weapons, making the weapons control efforts for the latter two areas unrepresentative of success on the nuclear front. Secondly, there is almost certainly endogeneity insofar as the biological and chemical arms control proposals are made in conditions where they are perceived to stand some chance of success, such that the measured rate of success would be overstated relative to their true probability of success all things equal. Third, the more pessimistic view seems generally right, per&nbsp;<a href=\"https://forum.effectivealtruism.org/s/KJNrGbt3JWcYeifLk/p/GSSQeEqtaBMtjvnDD\"><u>Luisa's assessment</u></a> that it is unlikely that countries that are non-compliant with the TPNW will ratify it, or that TPNW supporters will be influenced not to pursue, host or manufacture nuclear weapons or to join a nuclear weapons alliance \u2013 such that existing anti-nuclear efforts are ineffective, and the upshot of which is that progress is fundamentally intractable.</p><p>We can now proceed to adjust the outside view with the inside view \u2013 I give more weight to the outside view than the inside view, given that the inside view is subject to the usual worries about inferential uncertainty, thus yielding an overall estimate of the probability of success of 0.000016%</p><p>Meanwhile, in terms of how much money a nonprofit lobbying for denuclearization would require for its operations, we can again resort to both an outside and inside view. The outside view is that ICAN has expenses of around USD 2,000,000 per annum, and assuming 10 years to success by an EA charity, that translates to USD 20,000,000 in total. The inside view is thus \u2013 my sense is that you would minimally require a team of maybe 10 people working across multiple countries full time, and that they would need perhaps 10 years or so to succeed. Assuming funding of around USD 50,000 per team member per annum, in line with past CE incubatee expenditures (covering not just salaries but other costs like office rental, lobbying expenditures etc), that translates to USD 5,000,000 in total. Combining these two estimates \u2013 equally, since the usual worries over inferential uncertainties for the inside view is counterbalanced by the fact that an EA or EA-identified organization will almost certainly be more cost-effective than ICAN \u2013 we get an estimate that such an EA nuclear disarmament organization would probably need no less than USD 12,500,000.</p><p>And all in all, this means that the proportion of problem solved per additional USD 100,000 spent is&nbsp;<strong>0.000000001</strong>.</p><h1><strong><u>Marginal Expected Value of Lobbying for Denuclearization</u></strong></h1><p>Combining everything, the marginal expected value of lobbying for denuclearization is&nbsp;<strong>248 DALYs per USD 100,000 spent</strong>, making this only around 40% as cost-effective as a GiveWell top charity.</p>", "user": {"username": "Joel Tan"}}, {"_id": "me6xDoDzruPPuemQr", "title": "Centre for Exploratory Altruism Research (CEARCH)", "postedAt": "2022-10-18T07:23:01.094Z", "htmlBody": "<h1><strong><u>Introduction</u></strong></h1><p><a href=\"https://exploratory-altruism.org/\">The Centre for Exploratory Altruism Research</a> (CEARCH) emerged from the 2022 <a href=\"https://www.charityentrepreneurship.com/incubation-program\">Charity Entrepreneurship Incubation Programme</a>. In a nutshell, we do cause prioritization research, as well as subsequent outreach to update the EA and non-EA communities on our findings.</p><h1><strong><u>Exploratory Altruism</u></strong></h1><h2><strong>The Problem</strong></h2><p>There are many potential cause areas (e.g. improving global health, or reducing pandemic risk, or addressing long-term population decline), but we may not have identified what the most impactful causes are. This is the result of a lack of <strong>systematic cause prioritization research</strong>.</p><ul><li>EA\u2019s three big causes (i.e. global health, animal welfare and AI risk) were not chosen by systematic research, but by historical happenstance (e.g. Peter Singer being a strong supporter of animal rights, or the Future of Humanity Institute influencing the early EA movement in Oxford).</li><li>Existing cause research is not always fully systematic; for lack of time, it does not always involve (a) searching for as many causes as possible (e.g. more than a thousand) and then (b) researching and evaluating all of them to narrow down to the top causes.</li><li>The search space for causes is vast, and existing EA research organizations agree that there is room for a new organization.</li></ul><p>The upshot of insufficient cause prioritization research, and of not knowing the most impactful causes, is that we cannot direct our scarce resources accordingly. Consequently, global welfare is lower and the world worse off than it could be.</p><h2><strong>Our Solution</strong></h2><p>To solve this problem, CEARCH carries out:</p><ul><li><strong>A comprehensive search for causes</strong>.</li><li><strong>Rigorous cause prioritization research</strong>, with (a) shallow research reviews done for all causes, (b) intermediate research reviews for more promising causes, and finally (c) deep research reviews for potential top causes.</li><li><strong>Reasoning transparency and</strong> <strong>outreach </strong>to allow both the EA and non-EA movement to update on our findings and to support the most impactful causes available.</li></ul><h2><strong>Our Vision</strong></h2><p>We hope to discover a <strong>Cause X</strong> every three years and significantly increase support for it.</p><h2><strong>Expected Impact</strong></h2><p>If you're interested in the expected impact of exploratory altruism, do take a look at our website (<a href=\"https://exploratory-altruism.org/expected-impact/\">link</a>), where we discuss our theory of change and the evidence base. Charity Entrepreneurship also has a detailed report out on exploratory altruism (<a href=\"https://3394c0c6-1f1a-4f86-a2db-df07ca1e24b2.filesusr.com/ugd/9475db_a215cf49667a40f7aca07db9c10bd246.pdf\">link</a>).</p><h2><strong>Team &amp; Partners</strong></h2><p>The current team currently comprises Joel Tan, the founder (<a href=\"https://exploratory-altruism.org/team-partners/\">link</a>).</p><p>However, we're looking to hire additional researchers in the near future- do reach out (<a href=\"https://exploratory-altruism.org/contact/\">link</a>) if you're interested in working with us. Do also feel free to get in touch if you wish to discuss cause prioritization research/outreach, provide advice in general, or if you believe CEARCH can help you in any way.</p><h1><strong><u>Research Methodology</u></strong></h1><h2><strong>Research Process</strong></h2><p>Our research process is iterative:</p><ul><li>Each cause is subject to an initial shallow research round of one week of desktop research.</li><li>If the cause's estimated cost-effectiveness is at least one magnitude greater than a GiveWell top charity, it passes to the intermediate research round of two weeks of desktop research and expert interviews.</li><li>Then, if the cause's estimated cost-effectiveness is still at least one magnitude greater than a GiveWell top charity, it passes to the deep research round of four weeks of desktop research, expert interviews and potential commissioning of surveys and quantitative modelling.</li></ul><p>The idea behind the threshold is straightforward - research at the shallower level tends to overestimate a cause's cost-effectiveness, so if a cause doesn't appear effective early on, it's probably not going to be a better-than-GiveWell bet, let alone a Cause X magnitudes more important than our current top causes. Consequently, it's likely a better use of time to move on to the next candidate cause, than to spend more time on this particular cause.</p><h2><strong>Evaluative Framework</strong></h2><p>CEARCH attempts to identify a cause's&nbsp;<i>marginal expected value (MEV)</i>:</p><ul><li>MEV = t * \u03a3(n = p * m * s * c)</li></ul><p>where</p><ul><li>t = tractability, or proportion of problem solved per additional unit of resources spent</li><li>p = probability of benefit/cost</li><li>m = moral weight of benefit/cost accrued per individual</li><li>s = scale in terms of number of individuals benefited/harmed at any one point in time</li><li>c = persistence of the benefits/costs</li></ul><p>This can be viewed as an extension of the ITN framework, for this approach also takes into account the three ITN factors:</p><ul><li>Importance: Factored in with p * m * s * c.</li><li>Tractability: Factored in with t.</li><li>Neglectedness: Factored in with (i) c, since the persistence of the benefits will depends on how long the problem would have lasted and harmed people sans intervention, and that in turn is a function of the extent to which the cause is neglected; and (ii) t, since tractability is a function of neglectedness to the extent that diminishing marginal returns apply.</li></ul><p>However, the MEV framework has the additional following advantage:</p><ul><li>Through c, it takes into account of not just the decline (i.e. non-persistence) of a problem from active intervention (i.e. the neglectedness issue), but also decline from secular trends (e.g. economic growth reducing disease burden through better sanitation, nutrition, and greater access to healthcare).</li></ul><p>In implementing the MEV framework, especial effort is made to brainstorm for what benefits and costs there are - though, in our experience, the health effects tend to swamp the non-health effects.</p><p>For more details, refer to this comprehensive write-up on CEARCH's evaluative framework (<a href=\"https://docs.google.com/document/d/1j67pOUpC5vhC1-H516PAj2-4Fvm5rxJhZNeBVav3kik/edit\">link</a>).</p><h1><strong><u>Research Findings</u></strong></h1><p>We recently finished conducting shallow research on nuclear war, fungal disease, and asteroid impact. To summarize our findings:</p><h2><strong>Nuclear War</strong></h2><p>Taking into account the expected benefits of denuclearization (i.e. fewer deaths and injuries from nuclear war), the expected costs (i.e. more deaths and injuries from conventional war due to weakened deterrence), and the tractability of lobbying for denuclearization, CEARCH finds that the marginal expected value of lobbying for denuclearization to be <strong>248 DALYs per USD 100,000</strong>, which is around 39% as cost-effective as giving to a GiveWell top charity.</p><p>For more details, refer to our cost-effectiveness analysis (<a href=\"https://docs.google.com/spreadsheets/d/19_fOaBmQ9MwD2cyKaT-8eQorWzx_xdDr-VM8-XISD0g/edit#gid=0\">link</a>) on the matter as well as the accompanying research report (<a href=\"https://docs.google.com/document/d/1gFzKC5dFW669mkAHkpHARzMzdz-s7P8uVAETU78d0To/edit\">link</a>).</p><h2><strong>Fungal Disease</strong></h2><p>Considering the expected benefits of eliminating fungal infections (i.e. fewer deaths, less morbidity and greater economic output) as well as the tractability of vaccine development, CEARCH finds that the marginal expected value of vaccine development for fungal infections to be <strong>1,104 DALYs per USD 100,000</strong>, which is around 1.7x as cost-effective as giving to a GiveWell top charity.</p><p>For more details, refer to our cost-effectiveness analysis (<a href=\"https://docs.google.com/spreadsheets/d/1cBj7I8mlpLMrVM8P9HNzkZi3qQxHYOBsmiknLXx-ot0/edit#gid=0\">link</a>) on the matter as well as the accompanying research report (<a href=\"https://docs.google.com/document/d/1v2K_t6b6YVZpsrasAkW-Hqqz_uXh8FsGD_XcoyrLrcg/edit\">link</a>).</p><h2><strong>Asteroids</strong></h2><p>Factoring in the expected benefits of preventing asteroid impact events (i.e. fewer deaths and injuries) as well as the tractability of lobbying for asteroid defence, CEARCH finds that the marginal expected value of such asteroid defence lobbying to be <strong>1,352 DALYs per USD 100,000</strong>, which is around 2.1x as cost-effective as giving to a GiveWell top charity.</p><p>For more details, refer to our cost-effectiveness analysis (<a href=\"https://docs.google.com/spreadsheets/d/1CgqHIMztZtErcwM-kUUZarDLtAVRYRODRJP_yQUo9vQ/edit#gid=0\">link</a>) on the matter as well as the accompanying research report (<a href=\"https://docs.google.com/document/d/1q7FPcdXDthNggyF7Pe3SNdsChLw9GDo7Lv2LiJmSGVA/edit\">link</a>).</p><h2><strong>General Comments</strong></h2><p>The causes were selected purely out of interest, not because these causes were expected to be especially cost-effective. However, expectations at the outset were that, in terms of their cost-effectiveness, the causes would rank in the following way (in descending order):</p><ol><li>Fungal diseases: Importance probably low compared to longtermist causes, though the problem is certain and there seem to be decently tractable solutions (e.g. advance market commitments).</li><li>Nuclear war: Change here is likely to be extremely intractable, while the per annum probabilities are fairly low if still meaningful.</li><li>Asteroid impact: High impact on occurrence but not neglected given DART, while the probability of occurrence is extremely low and one imagines that tractability isn't that great (effective but expensive).</li></ol><p>The results (asteroid impact being the most cost-effective cause, followed by fungal disease, and then nuclear war) were hence moderately surprising. While we wouldn't over-update on such a small sample, we do think it's a data point <i>against </i>the value of intuition in selecting cause areas for initial cause prioritization research, and <i>for </i>making the effort to research as many causes as possible, even ones that do not seem especially important on the surface.</p><h1><strong><u>Going Forward</u></strong></h1><p>CEARCH will be publishing more detailed forum posts on nuclear war/fungal disease/asteroid impact, and will also continue doing research into additional causes, following the process and methodology outlined above. Comments and criticisms on our research methodology and on our specific research results are, of course, welcome.</p>", "user": {"username": "Joel Tan"}}, {"_id": "y5F3YYdqq7LspEGNj", "title": "Solving The Human Alignment Problem (The Launch of EA Social Media Application)", "postedAt": "2022-10-18T04:08:59.920Z", "htmlBody": "<p>The first 90% of this post is from Adam Elwood's \"<a href=\"https://pursuingreality.com/2022/09/21/resisting-the-invasion-of-our-inner-world/\"><strong>Resisting the invasion of our inner world</strong></a><strong>\" with mild adaptations throughout.</strong></p><p>The last bit I linked to the new Effective Altruism tiktok style feed that I am building and working towards having an empowerment algorithm</p><p>--<br><br>In the middle of the last century, two visionaries warned of two contrasting dystopias. In&nbsp;<i>1984</i>, George Orwell imagined a world under a state of constant surveillance, forced to submit to the powers that be. Instead, in Aldous Huxley's&nbsp;<i>Brave New World</i>, inane and meaningless pleasures distract humanity, leaving the world suffocated into subservience by comfort.</p><p>It's now alarmingly easy to find elements of both these visions. We live in the age of&nbsp;<a href=\"https://en.wikipedia.org/wiki/Surveillance_capitalism\">surveillance capitalism</a>, where the most sophisticated information processing infrastructures ever conceived predict and manipulate our behavior. They do all this while rewarding us with a constant flow of distraction and entertainment. The negative consequences of this are becoming more and more evident, as the automatic personalization of our&nbsp;<a href=\"https://www.improvethenews.org/faq\">news feeds</a>&nbsp;fractures consensus reality, and social media use is causing a&nbsp;<a href=\"https://www.theatlantic.com/ideas/archive/2021/11/facebooks-dangerous-experiment-teen-girls/620767/\">crisis in teenage mental health</a>.</p><p>We've reached a point where the manipulation of our thoughts is a major driving force of the modern economy. As we spend most of our waking hours extracting information through screens, we use powerful software that we don't pay for with money. Instead,&nbsp;<a href=\"https://www.youtube.com/watch?v=KNOlqzMd2Zw\">we pay with our attention</a>&nbsp;\u2014 the very resource that shapes our minds.</p><p>This all occurs with a constant distraction buzzing in our pockets, calling us to a drip feed of inane tidbits. It's so easy for the noise to sweep us away, keeping our thoughts occupied in the pursuit of nothing.</p><p>To work out why the empowering and uplifting ideals of the World Wide Web on our lives failed to come to fruition, and how we can reclaim that path, we need to look at both how our brains have evolved and why they are being hacked. Understanding this, we can begin to loosen the grip of the information economy on our minds, rediscover our free time and pursue the things we find truly meaningful.</p><h3>The modern plight of continuous media consumption</h3><p>As many of us now spend hours a day on our devices, our attention is increasingly scattered. We are simultaneously overwhelmed with actionable information while struggling to get anything done. The temptations of Twitter and Youtube make it so easy to fritter away days, never achieving real states of enjoyment or pursuing important projects.</p><p>It's easy to blame ourselves for falling into this. But, as Tristan Harris pointed out in&nbsp;<a href=\"https://www.thesocialdilemma.com/\"><i>The Social Dilemma</i></a>, we're massively outgunned. On our side of the screen is an advanced monkey brain, juggling many commitments and trying to make difficult and nuanced decisions about how to spend its time. On the other side is an array of supercomputers, fed with trillions of data points and programmed by the world's best computer scientists to glue us to our screens and hack our reward system.</p><h3>The perils of engagement-focused content</h3><p>The abundance of excellent content and the ability to be motivated and stimulated by our devices was an ideal we lost. Before the internet, finding well written and rewarding articles were rare, maybe occurring once or twice a week in a favorite magazine or newspaper. Once we found a topic we enjoyed, we could specialize and dive into the rabbit hole of that topic.</p><p>As well as burning away our time, being exposed to too much tempting content has an insidious impact on the way we think. It leads us down a path of frantic consumption, where we gain&nbsp;<a href=\"https://perell.com/essay/never-ending-now/\">shallow knowledge of trending topics</a>, instead of a deep understanding of the world around us. This results in a feeling of intellectual disorientation \u2014 we are simultaneously inspired and unsatisfied, always hoping the next blog post or YouTube video will finally make sense of it all, but it never does.</p><p>We have replaced the newspaper with social platforms and rather than the most informative/empowering bits floating to the top, these are in fact suppressed. Why? Engagement algorithms are optimizing to keep you on the app as long as they can. If you found a video that motivated/moved you so much that you put your phone down, hugged your father, called a old friend, hit a workout, saved 20 kittens from a burning tree, and then advanced Ai alignment. All the algorithm would know is to not show that piece of content again.</p><p>Despite all this, there is clearly a huge positive potential to the internet. We are living in an era where almost any of humanity's collective knowledge can be immediately accessed from a small shiny rectangle in our pockets. We can communicate instantaneously via video with anyone in the world, and find communities of likeminded individuals to share interests in niche topics.</p><p>The internet has also allowed for the proliferation of new valuable forms of media \u2014 long-form podcasts, contain in-depth conversations with knowledgable people; it is possible to access a degree's worth of lectures on most topics from the world's top universities for free; YouTube contains a how-to video for every conceivable household task. Understanding how to take advantage of these upsides, while avoiding the downsides, is the key skill of our age. Godlike knowledge is at our disposal, we just need find a way to navigate the abundance of information without falling into a well of distraction. This starts by understanding how we are being exploited.</p><p>Photo by&nbsp;<a href=\"https://unsplash.com/@nasa?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">NASA</a>&nbsp;on&nbsp;<a href=\"https://unsplash.com/@nasa?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></p><p><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ed23dc-9ab5-400d-9e0d-4f95e69ae731_1024x682.jpeg\"></p><h3>Our information processing biases</h3><p>New information is incredibly valuable. It allows us to build better&nbsp;<a href=\"https://pursuingreality.com/2020/04/18/the-ways-of-knowing/\">models of the world</a>, so we can make decisions that are critical for our survival. Everything that differentiates us from other animals comes down to our ability to better disseminate complex and abstract information. This gives us a very strong bias to collect as much information as possible \u2014 our ancestors were the savvy hominids who could do this, while those less capable perished.</p><p>For most of human evolution, new information was a scarce resource. We obtained it by speaking with another member of our tribe, or exploring our local environment \u2014 both time-consuming activities with a low probability of success. As civilization developed, new information spread via travelers who explored beyond their local environment. But, it wasn't until the invention of the printing press that it could spread widely and consistently. This accelerated exponentially through the invention of radio, television and the internet. The change from information scarcity to abundance has happened so suddenly, we haven't developed the evolutionary intuitions to deal with it.</p><p>Along with our need to collect information, we also evolved the bias that all new information is relevant to us. This makes perfect sense when information is scarce and local \u2014 every new piece could make the difference between finding a rich new food source or starving next winter. Unfortunately, these biases don't serve us well in our modern informational ecosystem. We now have access to an infinite supply of irrelevant information, but our intuitions still tell us to consume as much of it as possible.</p><p>To see the mechanics of these intuitions more clearly, it is interesting to understand a bit about optimal information processing strategies and how we are driven by our natural reward system.</p><h3>Exploration vs exploitation and information foraging</h3><p>Information isn't valuable in itself, but because it allows us to change our behavior in a way to more intelligently fulfill our goals. There is an optimal tradeoff between exploration, to collect new information, and the exploitation of this new knowledge.</p><p>Effectively trading off these two factors is a&nbsp;<a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning#Exploration\">well known challenge</a>&nbsp;in computer science, particularly relevant to building intelligent systems that operate under the paradigm of reinforcement learning. For certain&nbsp;<a href=\"https://en.wikipedia.org/wiki/Multi-armed_bandit\">simple problems</a>&nbsp;there are&nbsp;<a href=\"https://en.wikipedia.org/wiki/Multi-armed_bandit#Optimal_solutions\">theoretically</a>&nbsp;<a href=\"https://en.wikipedia.org/wiki/Thompson_sampling\">optimal</a>&nbsp;strategies that allow for a perfect tradeoff, so as to minimize the opportunities lost by a system over time. These problems are framed as reward collection tasks, where different behavioral policies are tried until a policy that optimally collects the most rewards over time is chosen.</p><p>Algorithms for finding optimal policies typically work by keeping track of the expected reward for each policy \u2014 the probability of gaining a reward multiplied its value \u2014 along with its uncertainty. Each time an action must be taken, the policy that will yield the maximum reward in the most optimistic scenario is chosen. Each time a policy is tried new information is obtained, reducing the uncertainty on how well it behaves. Always behaving optimistically reduces the probability of getting stuck in a sub-optimal policy due to being too pessimistic about the alternatives.</p><p>The rewards available to each policy are typically initialized with high uncertainties, so lots of different policies are tried at the beginning. Over time though, as more information is collected and the uncertainties reduce, policies that don't perform well are discarded and a single optimal policy emerges.</p><p>Unsurprisingly, evolution has built a similar exploration-exploitation tradeoff strategy into our intuitive information gathering behavior. The model for this is known as&nbsp;<a href=\"https://en.wikipedia.org/wiki/Information_foraging\">information foraging</a>, which draws analogies with the way animals forage for food, and is well illustrated in the&nbsp;<a href=\"https://www.samharris.org/podcasts/making-sense-episodes/226-price-distraction\">way we consume information</a>. When we are reading a magazine, for example, and have the option to change to another article, we intuitively decide to switch based on both how difficult it is to make the change, and how likely we think a new source will be more informative. The temptation to switch is much stronger when reading on the internet or when we have a phone in our pocket\u201a as changing information sources is so easy, we are much more likely to explore new ones than exploiting the ones we are currently reading.</p><p>Under these new conditions, our natural intuitions that new information sources are rare and valuable end up harming us. We are not well adapted to the abundance of easily accessible information. It is easy to see how this leads to endless scrolling through social media, or the difficulty of finding time to read a book on a free afternoon.</p><p>Photo by&nbsp;<a href=\"https://unsplash.com/es/@anniespratt?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Annie Spratt</a>&nbsp;on&nbsp;<a href=\"https://unsplash.com/s/photos/foraging?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></p><p><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa46018af-7ac3-445a-bce7-bad6eefe76ec_1024x596.jpeg\"></p><h3>Our goals and the neurotransmitters that drive our behavior</h3><p>Having established that our intuitions don't always serve us well, we can get even more insight into our predicament by looking into our natural reward system, which was also unprepared by our ancestral environment to deal with smartphones and social media.</p><p><a href=\"https://www.nature.com/articles/npp2010165\">Dopamine and serotonin</a>&nbsp;are two&nbsp;<a href=\"https://www.nature.com/articles/npp2010170\">neurotransmitters</a>&nbsp;that play a key role in regulating our behavior. Dopamine rewards us when we go out into the world and achieve things.&nbsp;<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3827581/\">It keeps us moving forward</a>,&nbsp;<a href=\"https://elifesciences.org/articles/51260\">drives us to explore</a>&nbsp;and makes us feel good when we reach our goals. Serotonin, on the other hand, rewards us when we are in a fulfilling situation that we don't need to change, such as&nbsp;<a href=\"https://www.psychologytoday.com/us/blog/prefrontal-nudity/201111/boosting-your-serotonin-activity\">relaxing after exercise or hanging out with loved ones</a>. If dopamine drives exploration, serotonin&nbsp;<a href=\"https://www.biorxiv.org/content/10.1101/170472v1.full\">drives exploitation</a>.</p><p>To live a satisfying and healthy life, it is necessary to balance&nbsp;<a href=\"https://lexfridman.com/andrew-huberman-2/\">both aspects of our reward system</a>. We need to make use of the dopamine system to motivate ourselves to achieve things and constantly improve our knowledge of the world. But, without taking time out to enjoy the simple sensual pleasures of existing, we will burn out and won't be able to find&nbsp;<a href=\"https://www.psychologytoday.com/intl/blog/prefrontal-nudity/201110/marshmallows-and-monoamines\">the willpower to get anything done</a>.</p><p>By its very nature, the attention economy tips our behavior towards the instant gratification provided by our dopamine system \u2014 the serotonin inducing warmth of sunlight or direct human contact isn't distributable through a screen. Even if it was, it would be much more valuable to keep us moving around, exposed to more advertisements.</p><p>To hack our dopamine system, the modern media environment targets our immediate fears and wants, discouraging us from focusing on our longterm goals. A like isn't a meaningful social interaction, but is felt superficially as if it is. Internet pornography offers an even more powerful stimulus, but only provides shallow sexual gratification. We are therefore nudged towards spending our time on things that our intuitions tell us will help us to fulfill our evolutionary goals \u2014 the building of meaningful relationships or successful reproduction. But, over the long term these things aren't achieved, so we don't end up in a satisfied, serotonin inducing state. This leaves us frazzled and unsatisfied, without an intuitive answer for why we feel this way.</p><h3>Corporate alignment problems</h3><p>So far we've seen&nbsp;<i>how</i>&nbsp;we are being hacked, but this leaves us to explain&nbsp;<i>why</i>&nbsp;we are being hacked. What is the mechanism that is driving us towards distraction and misery?</p><p>In most cases, the tech giants are setting out to build good products that people want to use. The owners and employees of these companies aren't driven by malign motivations, but feel like they are making a positive contribution to society. Despite these noble intentions, big corporations are machines that optimize for profit, even when it does not strictly align with the goals of consumers. Not only are corporations bound do this by law, but having such a clear and measurable objective is a big advantage to an organization. It provides an unambiguous measure of progress, making it easier for everyone to cooperate. Companies can break down all key business goals into measurable objectives and assign them to employees for monitoring and optimization. No one needs to concern themselves with messy holistic thinking, everyone knows what they need to do and can focus on doing it as efficiently as possible.</p><p>This optimization process is supercharged in a tech company, which has the infrastructure and knowhow to collect vast quantities of data relating to its objectives. This, along with unambiguous definitions of their goals, can then be fed into&nbsp;<a href=\"https://pursuingreality.com/2020/06/29/if-robots-spoke-of-god/\">machine learning algorithms</a>.</p><p>This is an incredibly powerful way of working, and very efficient for short term profit maximization. But, it biases companies to&nbsp;<a href=\"https://thomasjbevan.substack.com/p/the-tyranny-of-numbers\">pursue easily measurable aims</a>, leaving negative externalities completely ignored. Rather than developing strategies that take the wider social context into account, employees gets their heads down and optimizes their narrow objectives. This leads to&nbsp;<i>alignment problems</i>&nbsp;\u2014 scenarios in which an overzealous focus on an objective can cause unwanted side effects.</p><p>Photo by&nbsp;<a href=\"https://unsplash.com/@ikukevk?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Kevin Ku</a>&nbsp;on&nbsp;<a href=\"https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></p><p><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F10ff7f74-f753-4e7b-8595-f7508af14e00_1024x768.jpeg\"></p><p>These issues aren't just theoretical, but are playing a major role in exacerbating all the problems discussed so far. The YouTube recommender algorithm, for example, has been shown to contribute to the dissemination of conspiracy theories and the&nbsp;<a href=\"https://www.nytimes.com/interactive/2019/06/08/technology/youtube-radical.html\">radicalization</a>&nbsp;of its users. Given the objective to keep viewers watching YouTube for as long as possible, the algorithm discovered that they were more likely to do so when fed with misinformation and emotive content. As no concept of societal responsibility was built in to the optimization, it did what it was told to do and gave the people the damaging content that kept them most engaged.</p><p>This isn't just limited to YouTube, the spread of misinformation occur on all social media platforms that optimizes for engagement with the platform. A&nbsp;<a href=\"https://news.mit.edu/2018/study-twitter-false-news-travels-faster-true-stories-0308\">recent study</a>&nbsp;demonstrated that fake news spreads up to six times faster than real news on Twitter. Facebook's Groups have also been&nbsp;<a href=\"https://www.theguardian.com/technology/2021/feb/04/facebook-groups-misinformation\">implicated in creating echo chambers for extremist views</a>.</p><p>All platforms that have an incentive to keep users on them, so as to serve them with more advertisements, suffer from an alignment to the greater good of the user, and thus the world. The algorithm doesn't care if we fritter our life away swiping through Instagram, it actively encourages it. Rather than gyms for our brain we have strip clubs, rather than whole-foods we have candy stores. The only way to stop this kind of thing happening is to build it into the design of the algorithm. When this goes against the business model of big tech, there is always going to be an uncomfortable tension between their profit motive and the the wellbeing of their users.</p><p>All of this comes from tech companies deploying vampiric algorithms at scale.&nbsp;<a href=\"https://pursuingreality.com/wp-admin/post.php?post=1102&amp;action=edit\">Imagine what could happen if our algorithms became exponentially more powerful</a>, as is forecast to happen. This is what makes alignment problems one of the major difficulties in building safe artificial intelligence.</p><p>Despite all of the above, we aren't doomed to the system we find ourselves in now. We are waking up to what is going on, which is enough to start to push for a future of transparent algorithms that have holistic objective functions. In the meantime, however, we have to find a solution for ourselves.</p><h3>Taking back our minds</h3><p>Given all of this, the difficulties of modern media consumption can be roughly broken down into three different issues:</p><ol><li>The abundance of super-salient content that exploits our evolved reward pathways</li><li>The ease of access to an unending source of novel information on devices that encourages task switching</li><li>Misalignment of the goals of the content distributors and the content consumers</li></ol><p>To free our minds, we should ultimately address all three of these points. But, we have to start by viscerally understanding that the way things are won't make us happy. When we can feel this, we can start to see how our information processing biases are being exploited to keep us glued to our devices. When we really get that the information we're receiving isn't relevant or scarce, we will naturally want to reduce our consumption.</p><p>Embodying this realization isn't so easy, though. As habitual phone use is driven by a hacked dopamine system, change requires time and effort. Luckily, we can control our reward system \u2014 by choosing how we frame the situations we find ourselves in, we can change how our reward system responds to them. Dopamine is not only released by basal urges, but also when we progress towards an abstract goal. If we are intelligent about what we aim for, we can get our dopamine from a wholesome source. This then comes with the added bonus of a serotonin reward while you relax and recover, proud of what you've achieved. This turns the task of taking back our minds away from a project of asceticism and avoidance, into a project of discovering meaningful ways to spend our time.</p><p>Thankfully, there are no end of life-enriching approaches to this. It never fails to spend time cultivating meaningful social interactions or helping others. Along with this, you can find projects that interest you and go deeply into them \u2014 read books, create music, grow a garden,&nbsp;<a href=\"https://forthewild.world/podcast-transcripts/jenny-odell-on-resisting-the-attention-economy-222\">watch birds</a>, participate in sport \u2014 anything that feels intrinsically important and engaging. Just bear in mind that activities that require creative participation are usually preferable to passive consumption, as they require a deep involvement and are more likely to induce flow states. Writing about a topic, for example, results in a much deeper understanding of it than just reading about it.</p><p>Having found meaningful projects with which to occupy your free time, you are now in a position to take conscious control of your exploration and exploitation. Rather than being buffeted around by incoming notifications and your news feed, you can direct your exploration towards your interests. This turns the internet from a source of distraction into a treasure trove of meaningful information.</p><p>Photo by&nbsp;<a href=\"https://unsplash.com/@johnmoeses?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">John Moeses Bauan</a>&nbsp;on&nbsp;<a href=\"https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></p><p><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F6231b28e-1d24-420d-874a-bb08673f478f_1024x683.jpeg\"></p><h3>Some practical tips</h3><p>Although this may sound like a great idea, actually getting away from technology to implement some of the above ideas can be more&nbsp;<a href=\"https://www.nature.com/articles/d41586-021-00606-x\">challenging than it seems</a>. The first thing to do is to make some space between you and your devices, this gives you an opportunity to understand what you actually want to do.</p><p>An extreme but effective approach is to carry out a Digital Detox, as&nbsp;<a href=\"https://www.calnewport.com/books/digital-minimalism/\">laid out by Cal Newport</a>. Simply make a promise to avoid all but the most necessary use of technology for a month, and use this time to rediscover what you really enjoy doing. After this period, reflect on the technology that brings value and meaning to your life and reintroduce it slowly, cutting away the extraneous baggage.</p><p>Less ambitiously, start taking long walks in nature without any input and the intention to think about what you want to do. You\u2019ll be amazed what your brain can throw up if you give it chance to&nbsp;<a href=\"https://maxfrenzel.medium.com/in-praise-of-deep-work-full-disconnectivity-and-deliberate-rest-e9fe5cc50a1d\">breathe, away from the constant barrage of new information</a>. Try to avoid the temptation to fill all your spare moments with input from a phone. Let yourself be bored on a train journey, or put on some music and let your mind wander instead of watching Netflix late at night. With the right intention, ideas of how you really want to spend your time will start to emerge. Taking up a meditation practice can really augment this process, giving you powerful tools to change your perspective and notice when your attention is being directed in unhelpful ways.</p><p>It\u2019s also worth being selective of the media formats that you consume. Information sources that require a longer commitment, such as books or long-form podcasts, help to combat the confusion of disparate ideas from endless article hopping. Even YouTube can be valuable, just spend time watching a series of lectures rather than jumping around the recommendations feed.</p><p>Finally, bear in mind that media consumption is&nbsp;<a href=\"https://en.wikipedia.org/wiki/The_medium_is_the_message\">not substrate independent</a>&nbsp;\u2014 the device you use has a tangible effect on your experience of the content. As we\u2019ve seen, the easier it is to switch sources on the device you are on, the more easily you\u2019ll be distracted. Computers tend to be better than phones, but nothing compares to a book or a pen and paper.</p><p>The ultra-strength&nbsp;<a href=\"https://nymag.com/intelligencer/2016/09/andrew-sullivan-my-distraction-sickness-and-yours.html\">gravitational pull smartphones</a>&nbsp;make it worth employing extra tactics to escape their orbit. Most come with tools for setting timers on the usage of distracting apps and managing notifications. When focusing on something you want to do, keeping your phone out of reach. It can be especially valuable to not look at any of your devices at all in the morning until you have finished some creative work or spent some time reading a book.</p><h3>Escaping the creeping dystopia</h3><p>Millions of years of evolution could never have anticipated the frantic information-rich world we now live in. But, instead of waltzing into a hyper-informed scientific utopia, we're sleep-walking into a dystopia, driven by misaligned incentives and short-term thinking. But, this needn't be the case, and we have already begun the shift of recognizing what's going on allowing us to put our defence up. We don't expose our computer to the internet without a firewall, we shouldn't do so with our minds either.</p><p>There is a silver lining to all this though. This is not the first time humanity has had widespread rampant addiction that was extremely bad for our health, we are continually updating our cultural operating system. As an example, cigarettes used to be a welcomed staple of culture. They did not become any less addicting, in fact over time companies got better at making them MORE ADDICTING, but they became low status. This negative status point eventually provided a positive feedback loop to their mass decline in society and physical establishments aided in updating culture by banning their use indoors.</p><p>This societal status update is more urgent than ever before. As our communal stories are fragmenting, consensus reality is breaking down, making large-scale communication on big problems more and more difficult. We can't afford this when the world is facing mounting challenges. We have to make the effort to enact our own values, and not let our limbic system get hacked through a screen. It's time for <a href=\"https://aeon.co/videos/a-handful-of-executives-control-the-attention-economy-time-for-attentive-resistance\">attentive resistance</a>. I believe this conscious consumption movement is already underway. Evidence of this includes BeReal becoming number one on the app store, pushing a more authentic/connection focused social experience as well as the documentary \u201c<a href=\"https://www.thesocialdilemma.com/\">The Social Dilemma</a>\u201d airing on Netflix and being a big hit.</p><p>Finding our way to&nbsp;<a href=\"https://www.themarginalian.org/2017/03/06/hermann-hesse-little-joys-my-belief/\">navigate the sea</a>&nbsp;of&nbsp;<a href=\"https://www.philosophizethis.org/podcast/episode-152-the-frankfurt-school-walter-benjamin-pt-1-tkewa\">distractions can be both beautiful and meaningful</a>. We can learn to thrive in the abundance and achieve so much more than would ever have been possible without the miracle of the internet. We can seize the opportunity to put emerging AI technologies in the service of humanity, instead of extraction of our lifeforce.</p><p>Ultimately, our life is defined by what we pay attention to. There is nothing more important than how we fill our conscious experience. As we can't know and do everything, we have to make choices about how we s<a href=\"https://www.oliverburkeman.com/books\">pend our time and energy with wise discernment</a>. As it looks like we are living in a&nbsp;<a href=\"https://www.cold-takes.com/most-important-century/\">pivotal moment</a>, now is the time to take back control of our minds. We can then take the tools narrowing us into a dystopia and use them to expand our knowledge and awareness. Properly exploited, there's nothing to stop steering the interconnection of our humanity away from toxicity and towards healthy mindsets that help us attempt to expand into a brilliant utopia.</p><p>Photo by&nbsp;<a href=\"https://unsplash.com/@sanketh07?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Sanketh Hiremath</a>&nbsp;on&nbsp;<a href=\"https://unsplash.com/s/photos/silver-lining?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></p><p><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc99a92d3-9c6a-4456-bb03-bc1cf63f40c3_1024x576.jpeg\"></p><p>What if we flipped the switch on content consumption? What if we made machine learning algorithms for that optimize not for extraction, but for empowerment?</p><p>What if everyday you checked a feed of videos optimized by people around the world for your flourishing. What if we used this power of media and algorithms to give energy, perspective, and gratitude to power us through our day.</p><p><strong>How do we get there?</strong></p><p>The like metric is better than factoring engagement time when thinking about empowerment over extraction but it does not get us all the way there. We want to track the videos that are the most effective that makes a user LEAVE the app feeling great, but using the exit metric is imperfect because many people may exit for any random reason, say they get a phone call etc.</p><p>The solution, the \u201cempowered exit\u201d metric. This button provides a distinct way to exit the app, one that allows us to get the exact metric we are looking for.</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5aa2e95048975dce303e7fcf16ec68d31e5b657dccd383a6.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5aa2e95048975dce303e7fcf16ec68d31e5b657dccd383a6.png/w_112 112w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5aa2e95048975dce303e7fcf16ec68d31e5b657dccd383a6.png/w_192 192w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5aa2e95048975dce303e7fcf16ec68d31e5b657dccd383a6.png/w_272 272w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5aa2e95048975dce303e7fcf16ec68d31e5b657dccd383a6.png/w_352 352w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5aa2e95048975dce303e7fcf16ec68d31e5b657dccd383a6.png/w_432 432w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5aa2e95048975dce303e7fcf16ec68d31e5b657dccd383a6.png/w_512 512w\"></p><p>This is not a far-fetched idea, this is what I have been working on with a team I\u2019m funding (and about to run out of funding so would love advice or pointers on moving that forward)</p><p>I made an Effective Altruism-specific app that's going on the IOS and Google Play store, with permission (who gives permission?) would love to use the real EA logo here!<br><br>Download The Testflight App here: <a href=\"https://testflight.apple.com/join/1wghdGma\">https://testflight.apple.com/join/1wghdGma</a></p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a83e5f61174e6e5fb2a57cb5e64b6e3870fbdb1e1dd1b98f.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a83e5f61174e6e5fb2a57cb5e64b6e3870fbdb1e1dd1b98f.png/w_130 130w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a83e5f61174e6e5fb2a57cb5e64b6e3870fbdb1e1dd1b98f.png/w_260 260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a83e5f61174e6e5fb2a57cb5e64b6e3870fbdb1e1dd1b98f.png/w_390 390w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a83e5f61174e6e5fb2a57cb5e64b6e3870fbdb1e1dd1b98f.png/w_520 520w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a83e5f61174e6e5fb2a57cb5e64b6e3870fbdb1e1dd1b98f.png/w_650 650w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a83e5f61174e6e5fb2a57cb5e64b6e3870fbdb1e1dd1b98f.png/w_780 780w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a83e5f61174e6e5fb2a57cb5e64b6e3870fbdb1e1dd1b98f.png/w_910 910w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a83e5f61174e6e5fb2a57cb5e64b6e3870fbdb1e1dd1b98f.png/w_1040 1040w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a83e5f61174e6e5fb2a57cb5e64b6e3870fbdb1e1dd1b98f.png/w_1170 1170w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a83e5f61174e6e5fb2a57cb5e64b6e3870fbdb1e1dd1b98f.png/w_1284 1284w\"></p><p>This version does not have the empowered exit button because we have not yet built the ML backend for that (this is a part of a bigger model and concept called Socialverse) if you are a developer with an ML backend I would love to have your help, until I get any funding I cannot offer great compensation but I can definitely offer equity if theres interest in a long term role. Anyone interested can DM me or use the Typeform here <a href=\"https://www.socialverseapp.com/join-us\">https://www.socialverseapp.com/join-us</a></p><p>I am super excited to expand upon this and will be posting progress and more surrounding thoughts in the future!</p><p>Thanks for the support.<br><br>&nbsp;</p>", "user": {"username": "maximizealtruism"}}, {"_id": "HmfXJGt9gtMaxdZyv", "title": "How to Write Readable Posts", "postedAt": "2022-10-20T07:48:12.407Z", "htmlBody": "<blockquote>\n<p><strong>TL;DR:</strong> Broaden and deepen your understanding of your reader, and assume they need your post to be more clear and concise in order to understand your ideas.</p>\n</blockquote>\n<p>But readability isn't just \"clarity and brevity\"; let's talk about assumptions for approachability and ways to step into the mind of your reader.</p>\n<h3>Outline</h3>\n<ol>\n<li>Prose <a href=\"#On_Writing_and_Revising\">On Writing and Revising</a> and the reasoning behind \"readability\"</li>\n<li>List of <a href=\"#Reader_Representation\">Readable Writing Tips</a> and helpful assumptions to make of your reader\n<ul>\n<li>The <a href=\"#Best_Advice\">Best Advice</a> on writing and revising</li>\n</ul>\n</li>\n<li>Breakdown of <a href=\"#Principles\">Principles</a> on readable and approachable writing</li>\n<li>Collection of <a href=\"#References\">References</a> and resources</li>\n<li>Quick <a href=\"#Conclusion\">Conclusion</a></li>\n</ol>\n<h3>Relevance to EA</h3>\n<p>I ended up writing <a href=\"https://forum.effectivealtruism.org/posts/PFFGfCYZmerznPjJL/toward-a-more-approachable-and-accessible-ea-forum\">an entire prequel post</a> for this post that explains why I was inspired to write this on the EA Forum in the first place and why I want to make the EA Forum more readable and approachable. You can read this preface here: <a href=\"https://forum.effectivealtruism.org/posts/PFFGfCYZmerznPjJL/toward-a-more-approachable-and-accessible-ea-forum\">Toward a more approachable and accessible EA Forum</a></p>\n<h3>(Just For Fun) Foreword</h3>\n<p>Most posts begin with a quote by some \"wise\" old person (who probably already said most of what the author is hoping to say in their post). It seems fun, so I made up this \"proverbial\" quote and hope you read it in the raspy voice of an old, wise person:</p>\n<blockquote>\n<p>It is not a matter of merely <em>writing</em> and <em>publishing</em>; one must <em>edit</em> and <em>revise</em> as well. <br>\nSo too is it not only a matter of what <em>can</em> be read; rather, one must ask: \"What <em>will</em> be read?\"</p>\n</blockquote>\n<h1>On Writing and Revising</h1>\n<h2>Write to be Read</h2>\n<p>When you write a post, your end goal is for your readers to <em>understand</em> you and your ideas.</p>\n<p>Presumably, these are the steps you <em>hope</em> to achieve with your writing process:</p>\n<ol>\n<li>You write a post about ideas.</li>\n<li>Others read your post.</li>\n<li>Others understand your ideas.</li>\n<li>(Others do something with those ideas.)</li>\n</ol>\n<p>But dang, even making it to step 2 is a challenge! <br>\nHow can you ensure that anyone will even <em>read</em> your writings, nevertheless begin to <em>understand</em> the ideas you've written about?</p>\n<h2>Understand to be understood</h2>\n<p>Upon closer inspection, you need to know: (1) who your reader is, and then (2) how to write posts that will be readable and understandable to them.</p>\n<p>Now this process involves more nuanced steps:</p>\n<ol>\n<li>Ensure <strong>you</strong> understand <em>your readers</em>. (\"Who is reading my writing?\")</li>\n<li>Ensure <strong>your readers</strong> understand <em>you</em>. (\"Will they understand my writing?\")</li>\n</ol>\n<p>If you understand your readers, then you can write in a way that your readers understand you. Thus, good writing: (1) <em>can</em> be read and <em>will</em> be read, and (2) is <em>understanding</em> and <em>understandable</em>. But good <em>understanding</em> requires you to form an accurate representation (mentalization) of your readers.</p>\n<h1>Reader Representation</h1>\n<p>One of the hardest parts of writing is trying to put yourself in the mind of your reader and examine your prose from the mental perspective of \"the other\" (for example: <a href=\"https://en.wikipedia.org/wiki/Curse_of_knowledge\">\"The Curse of Knowledge\"</a>).</p>\n<p>This list attempts to capture best practices for a functional <em>theory of mind</em> of your reader.</p>\n<h2>Readable Writing Tips</h2>\n<p>I've purposely designed this section to be a numbered list that can be easily referenced/referred to when people are providing editing and revision advice to each other's drafts.</p>\n<p>For instance, one editor might suggest to an author:</p>\n<blockquote>\n<p>\"While I love your tone, word-choice, and style, this currently feels <em>opaque</em> to some readers because it's mostly a large chunk of paragraphs with a TL;DR at the top. I think it could be much more <em>comprehensible</em> with some headings (<a href=\"#8__Headings\">#8</a>) and transitions (<a href=\"#14__Transitions\">#14</a>), along with a sentence near your TL;DR summary that briefly explains the motivation (<a href=\"#6__Motivation\">#6</a>).\"</p>\n</blockquote>\n<p>Each tip has a number, a name, an assumption to make about your reader, a question your reader might ask, and a strategy or two to implement.</p>\n<h4>0. Default</h4>\n<p>Extra preliminary tip: <em>If</em> you do not already have a specific target audience in mind, default to the assumption that: your post may be read by <em>any</em> of the estimated 1 billion English-speaking adults with internet access.</p>\n<h4>1. Terms</h4>\n<ul>\n<li>Assume your reader has never heard of the key terms in your post.\n<ul>\n<li>\"What does this mean?\"</li>\n</ul>\n</li>\n<li>Reduce or eliminate jargon and obscure/esoteric language.</li>\n<li>Clearly define necessary terminology/vocabulary.</li>\n</ul>\n<h4>2. Metaphors</h4>\n<ul>\n<li>Assume your reader has never heard of the metaphors in your post.\n<ul>\n<li>\"What does that represent?\"</li>\n</ul>\n</li>\n<li>Reduce metaphors and analogies.</li>\n<li>Clearly explain the relationships in any necessary metaphors or analogies.</li>\n</ul>\n<h4>3. Concrete</h4>\n<ul>\n<li>Assume your reader needs concrete concepts to understand your post.\n<ul>\n<li>\"What does this mean in real life?\"</li>\n</ul>\n</li>\n<li>Map abstract concepts to reality.</li>\n<li>Map theoretical ideas to practical applications.</li>\n</ul>\n<h4>4. Familiarity</h4>\n<ul>\n<li>Assume your reader may not have any particular knowledge or prior familiarity with the topic(s) in your post.\n<ul>\n<li>\"What is this?\"</li>\n</ul>\n</li>\n<li>Reduce domain-specific/esoteric language.</li>\n<li>Clearly establish the context, and define/reference your domain for context.</li>\n</ul>\n<h4>5. Time</h4>\n<ul>\n<li>Assume your reader does not have much time to read your post.\n<ul>\n<li>\"What can I read in 5 minutes?\"</li>\n</ul>\n</li>\n<li>Reduce the length of your post.</li>\n<li>Be concise, brief, and succinct.</li>\n</ul>\n<h4>6. Motivation</h4>\n<ul>\n<li>Assume your reader does not know why they should read your post.\n<ul>\n<li>\"Why should I read this?\"</li>\n</ul>\n</li>\n<li>Explain the value of your post.</li>\n<li>Explain why it is worth their time, attention, focus, and understanding.</li>\n</ul>\n<h4>7. Takeaways</h4>\n<ul>\n<li>Assume your reader does not know what key point(s) to take away from your post.\n<ul>\n<li>\"What should I remember from this?\"</li>\n</ul>\n</li>\n<li>Use a TL;DR or an executive summary.</li>\n<li>Clearly and concisely define/outline the most important idea(s).</li>\n</ul>\n<h4>8. Headings</h4>\n<ul>\n<li>Assume your reader relies on headings to keep track of context in your post.\n<ul>\n<li>\"What is this section about?\"</li>\n</ul>\n</li>\n<li>Use headings to segment your post into clearly identifiable sections.</li>\n</ul>\n<h4>9. Conclusions</h4>\n<ul>\n<li>Assume your reader does not know what you're concluding in your post.\n<ul>\n<li>\"So what?\"</li>\n</ul>\n</li>\n<li>Provide a clear, concise conclusion.</li>\n</ul>\n<h4>10. Relationships</h4>\n<ul>\n<li>Assume your reader does not understand how the concepts are related in your post.\n<ul>\n<li>\"What does that have to do with this?\"</li>\n</ul>\n</li>\n<li>Clearly define all of the factors/variables and concepts/ideas you will connect.</li>\n<li>Connect the dots and outline relationships between ideas.</li>\n<li>Walk readers through your reasoning when relating ideas.</li>\n</ul>\n<h4>11. Logic</h4>\n<ul>\n<li>Assume your reader does not understand the logic behind the conclusion(s) in your post.\n<ul>\n<li>\"How can I conclude that?\"</li>\n</ul>\n</li>\n<li>Clearly define all of the premises and steps necessary to form your conclusion(s).</li>\n<li>Spell out the logic when making a conclusion.</li>\n</ul>\n<h4>12. Inferences</h4>\n<ul>\n<li>Assume your reader has not made any of the observations you have that you use to make inference(s) in your post.\n<ul>\n<li>\"How do I infer this?\"</li>\n</ul>\n</li>\n<li>Clearly establish the context by defining your observations and how you believe they are related/relevant.</li>\n<li>Supply the concrete details.</li>\n</ul>\n<h4>13. Pragmatic</h4>\n<ul>\n<li>Assume your reader does not know why your post matters.\n<ul>\n<li>\"So what?\"</li>\n</ul>\n</li>\n<li>Be pragmatic and provide practical applications that explain why your post is important and relevant to life/reality.</li>\n</ul>\n<h4>14. Transitions</h4>\n<ul>\n<li>Assume your reader needs clear transitions to follow your flow/train of thought in your post.\n<ul>\n<li>\"How do these things relate?\"</li>\n</ul>\n</li>\n<li>Use clear transitions to cue the reader and connect previous ideas to the next ideas.</li>\n</ul>\n<h4>15. Connection</h4>\n<ul>\n<li>Assume your reader wants to feel a human connection to you and your post.\n<ul>\n<li>\"Do I feel like I connect with this?\"</li>\n</ul>\n</li>\n<li>Be real, conversational, and convivial (simpatico).</li>\n</ul>\n<h4>16. Impressions</h4>\n<ul>\n<li>Assume your reader is making a first impression of you that will determine whether or not they will ever engage with you or your posts.\n<ul>\n<li>\"Is this a good first impression?\"</li>\n</ul>\n</li>\n<li>Be warm, friendly, and pleasant.</li>\n</ul>\n<h4>17. Relatability</h4>\n<ul>\n<li>Assume your reader won't understand or remember your idea(s) if they cannot connect and relate to your post.\n<ul>\n<li>\"Does this have any relation or relevance to me?\"</li>\n</ul>\n</li>\n<li>Use familiar, congenial, and understandable language.</li>\n</ul>\n<h4>18. Sensitivity</h4>\n<ul>\n<li>Assume your reader may be sensitive to the topic(s) in your post.\n<ul>\n<li>\"Is this insensitive?\"</li>\n</ul>\n</li>\n<li>Replace judgmental, condescending, or overly critical language with kind, considerate, and respectful language.</li>\n</ul>\n<h4>19. Interest</h4>\n<ul>\n<li>Assume your reader may get bored or disinterested with your post.\n<ul>\n<li>\"Is this interesting?\"</li>\n</ul>\n</li>\n<li>Replace dry, dull, and monotonous language with fun, stimulating, and compelling language.</li>\n</ul>\n<h4>20. Conversational</h4>\n<ul>\n<li>Assume your reader wants a quick, conversational summary in your post.\n<ul>\n<li>\"Whatchya writing about?\"</li>\n</ul>\n</li>\n<li>Final revising technique: Quickly paraphrase/read your writing out loud to a friend in about 2 minutes, skimming and summarizing it from beginning to end.\n<ul>\n<li>How do you word things quickly and conversationally?</li>\n<li>What main points did you cover? Did you phrase them better when speaking?</li>\n<li>What points did you skip? Could they be removed?</li>\n<li>What questions did your friend ask you? Do you answer those in your writing?</li>\n<li>How did you feel saying everything out loud?</li>\n<li>How did your friend feel while listening?</li>\n<li>Did your friend understand your ideas?</li>\n</ul>\n</li>\n</ul>\n<h4>21. Representation</h4>\n<p>An additional tip for the forum:</p>\n<ul>\n<li>Assume your reader believes that you represent the EA community in your post.\n<ul>\n<li>\"What's this EA community like?\"</li>\n</ul>\n</li>\n<li>Be welcoming, inclusive, and considerate.</li>\n</ul>\n<h3>Best Advice</h3>\n<blockquote>\n<p>\"Whoa, 22 tips! That's too many; just gimme the good stuff.\"</p>\n</blockquote>\n<p>Ok, fine; 22 is a big number. Here's the best advice I can offer in 3 steps:</p>\n<ol>\n<li>While you're writing, think of 3-5 diverse people as your audience (e.g. someone old, someone young, someone from another country, someone from a different socioeconomic background, someone from a different culture, etc). Make sure they are real people that you know well enough to imagine their responses. Picture them reading and reacting to your post, sentence by sentence. What do they say? (Revise as you go.)</li>\n<li>Once you've finished your first draft, imagine watching 2 of your favorite authors read your post. How would they react? If you asked for them for feedback, how would they reply? (Go back and rewrite again.)</li>\n<li>Finally, once you've gone through a few drafts and feel like you've got it, ask at least 2 diverse people to read it and give you feedback. (Make your final revisions.)</li>\n</ol>\n<h1>Principles</h1>\n<p>Underlying each of these tips are fundamental ideals on what \"good\" writing is, based on core concepts of <em>readability</em> and <em>approachability</em>.</p>\n<p><strong>Readable</strong> writing is:</p>\n<ul>\n<li>Clear (Clarity)\n<ul>\n<li>Comprehensible</li>\n<li>Understandable</li>\n<li>Transparent</li>\n<li>Simple</li>\n<li>Straightforward</li>\n</ul>\n</li>\n<li>Brief (Brevity)\n<ul>\n<li>Concise</li>\n<li>Precise</li>\n<li>Minimal</li>\n<li>Succinct</li>\n<li>Synoptic</li>\n</ul>\n</li>\n</ul>\n<p><strong>Approachable</strong> writing is:</p>\n<ul>\n<li>Welcoming\n<ul>\n<li>Inclusive</li>\n<li>Familiar</li>\n<li>Congenial</li>\n<li>Convivial</li>\n<li>Pleasant</li>\n</ul>\n</li>\n<li>Loving\n<ul>\n<li>Kind</li>\n<li>Friendly</li>\n<li>Caring</li>\n<li>Considerate</li>\n<li>Understanding</li>\n</ul>\n</li>\n</ul>\n<h1>References</h1>\n<p>I'm not the first person to write about readability (on a forum or otherwise), so I want to call out great resources on this topic.</p>\n<p>First, a video:</p>\n<ul>\n<li><a href=\"https://youtu.be/OV5J6BfToSw?t=1887\">Steven Pinker's speech/presentation on readability topics</a> (from his book <em>The Sense of Style</em>), such as \"Why is it so hard for writers to use language to convey ideas effectively?\"</li>\n</ul>\n<h3>EA Forum posts</h3>\n<p>On summaries</p>\n<ul>\n<li><a href=\"https://forum.effectivealtruism.org/posts/dHHuEYdbMqBf2deyj/using-the-executive-summary-style-writing-that-respects-your\">Using the \"executive summary\" style: writing that respects your reader's time</a></li>\n</ul>\n<p>On jargon</p>\n<ul>\n<li><a href=\"https://forum.effectivealtruism.org/posts/F5YXWJt2rXo6ESzDQ/when-you-shouldn-t-use-ea-jargon-and-how-to-avoid-it\">When you shouldn't use EA jargon and how to avoid it</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/uGt5HfRTYi9xwF6i8/3-suggestions-about-jargon-in-ea\">3 suggestions about jargon in EA</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/Ja5akrpNsTqwSQ97W/the-case-for-reducing-ea-jargon-and-how-to-do-it\">The Case for Reducing EA Jargon &amp; How to Do It</a></li>\n</ul>\n<p>On brevity</p>\n<ul>\n<li><a href=\"https://forum.effectivealtruism.org/posts/duzspoWQrnm8tjC8d/the-value-of-content-density\">The value of content density</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/6whiBq7czKJk4Bx29/a-forum-post-can-be-short\">A Forum post can be short</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/eNa8GpEi5HX94CZ2n/be-more-succinct\">Be More Succinct</a></li>\n</ul>\n<p>On interest and impact</p>\n<ul>\n<li><a href=\"https://forum.effectivealtruism.org/posts/dAbs7w4J4iNm89DjP/why-boring-writing-is-unethical-the-case-for-it-being-high\">Why fun writing can save lives: the case for it being high impact to make EA writing entertaining</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/i6Hday6mudh5j4mZ4/ea-can-sound-less-weird-if-we-want-it-to\">EA can sound less weird, if we want it to</a></li>\n</ul>\n<p>On norms</p>\n<ul>\n<li><a href=\"https://forum.effectivealtruism.org/posts/shGo55g8TpPsEtdYe/for-better-commenting-avoid-ponds\">For Better Commenting, Avoid PONDS</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum\">Guide to norms on the Forum</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/p7EWkqa8TogNskXu5/suggestions-for-online-ea-discussion-norms\">Suggestions for Online EA Discussion Norms</a></li>\n</ul>\n<p>Topics</p>\n<ul>\n<li><a href=\"https://forum.effectivealtruism.org/topics/writing-advice\">Writing advice</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/topics/discussion-norms\">Discussion norms</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/topics/style-guide\">Style guide</a></li>\n</ul>\n<h1>Conclusion</h1>\n<blockquote>\n<p>\"It's all about empathy, baby!\" \u2014 the less \"proverbial\" young person<sup class=\"footnote-ref\"><a href=\"#fn-fozBgWTnTy6AiyEXf-1\" id=\"fnref-fozBgWTnTy6AiyEXf-1\">[1]</a></sup></p>\n</blockquote>\n<p>It sounds oxymoronic and paradoxical at first, but making assumptions about your reader can actually help your writing become more understandable to a broader audience. <br>\nThe key is in which assumptions you make!</p>\n<p>It really comes down to:</p>\n<ol>\n<li>Don't assume your reader is <em>you</em>!</li>\n<li>Instead, assume your reader is <em>someone else</em> who would happily understand you <em>if</em> you write for them.</li>\n</ol>\n<p>I believe that \"writing <em>for</em> the reader\" is the most <strong>caring</strong> thing an author can do.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-fozBgWTnTy6AiyEXf-1\" class=\"footnote-item\"><p>P.S. From the author: I'm still learning all of this <em>now</em>, so my writings from before (October 2022) are <em>not</em> good examples of readable posts. I don't always exemplify this skill yet, so please don't call me a hypocrite until at least a few posts/months later \ud83d\ude05 <a href=\"#fnref-fozBgWTnTy6AiyEXf-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "davidhartsough"}}, {"_id": "TDEukvfsDE3fCzvLK", "title": "Decision theory does not imply that we get to have nice things", "postedAt": "2022-10-18T03:04:48.715Z", "htmlBody": "", "user": {"username": "So8res"}}, {"_id": "bWeXn6A8Nhkcztkmd", "title": "current meditation practices", "postedAt": "2022-10-18T08:29:07.596Z", "htmlBody": "<p>Hello, my name is Craig Sanders, and I'm excited to join the forum. Does anyone want to share a few links to their current meditation practices?<br>Although I am using a free one, I am always interested in seeing what others use.&nbsp;<br>&nbsp;</p>", "user": {"username": "CSanders703"}}, {"_id": "yBWksYhn3Wx65RZWb", "title": "[Job] Web Developer", "postedAt": "2022-10-18T22:27:56.996Z", "htmlBody": "<h1><strong>Freelance Web Developer</strong></h1><h3><a href=\"https://www.commonvision.net/\">Common Vision</a> is a creative studio that provides award-winning creative services to non-profit organisations and for-profit companies working to improve the future for humanity, the planet and non-humans. We channel top creative talent and focus it on the world\u2019s most pressing problems.</h3><p>Common Vision is currently looking for a talented and experienced freelance web developer to look after one of the best non-profit websites built on WordPress to date. Other web projects are possible so it could also suit a small web development studio or team. Common Vision subsidises the site however we expect to pay market rates.</p><p>The ideal candidate will have a can-do attitude, attention to detail in both code and frontend styling, passion for technology, extensive web development experience with frontend and some backend skills.</p><p>We work with top design talent so all our sites are completely customised with pixel perfect frontends that would look great in anyone's portfolio. You should aspire to be building sites that look like this: <a href=\"https://www.tiltongroup.com/en/\">https://www.tiltongroup.com/en/</a> or this: <a href=\"https://nordarun.com/.\">https://nordarun.com/.</a><br><br>Description</p><ul><li>5+ years working with WordPress, plugins, custom themes and WooCommerce;</li><li>5+ years experience building websites using PHP, HTML, CSS, and JavaScript;</li><li>Understanding of modern web development and best practise WordPress Development;</li><li>Working knowledge of source control software such as version control though GitHub;</li><li>Ability to work independently and comfortable working geographically dispersed;</li><li>Ability and willingness to work with designers to get tiny details just right;</li><li>Very reliable with good general communication skills;</li><li>Willingness to learn new technologies and implement quickly if needed;</li><li>Knowledge of best practise web and WordPress security would be helpful;</li><li>Knowledge of web animation using GSAP and others also helpful;</li><li>Interest in pixel perfect UX/UI helpful;</li><li>Interest in EA/Longtermism and animal advocacy in particular would be advantageous.</li></ul><p>Sounds like you? Please contact: <a href=\"mailto:brendon@commonvision.net\">brendon@commonvision.net</a><br>Please briefly describe your experience, availability, interest in working with CV.</p><p>&nbsp;</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d6f2616f02baf03204c1b07687f0ea834fa42ac76283f58d.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d6f2616f02baf03204c1b07687f0ea834fa42ac76283f58d.png/w_83 83w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d6f2616f02baf03204c1b07687f0ea834fa42ac76283f58d.png/w_163 163w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d6f2616f02baf03204c1b07687f0ea834fa42ac76283f58d.png/w_243 243w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d6f2616f02baf03204c1b07687f0ea834fa42ac76283f58d.png/w_323 323w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d6f2616f02baf03204c1b07687f0ea834fa42ac76283f58d.png/w_403 403w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d6f2616f02baf03204c1b07687f0ea834fa42ac76283f58d.png/w_483 483w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d6f2616f02baf03204c1b07687f0ea834fa42ac76283f58d.png/w_563 563w\"><br><i>We Empower Non-Human and</i><br><i>Human Advocates</i></p><p><a href=\"https://www.commonvision.net/\">commonvision.net</a></p>", "user": {"username": "Brendon"}}, {"_id": "7urvvbJgPyrJoGXq4", "title": "Fallibilism, Bias, and the Rule of Law", "postedAt": "2022-10-17T23:59:56.047Z", "htmlBody": "<p><a href=\"https://www.effectivealtruism.org/\">Effective Altruism</a> (EA) aims to apply rationality, science, math and evidence to doing good. One of its main activities is evaluating the cost effectiveness of different charitable causes and encouraging donations to effective charities.</p>\n<p>Since rationality is important to EA\u2019s mission, I <a href=\"https://forum.effectivealtruism.org/posts/btJWbzqrD8K9cz58X/how-is-ea-rational\">asked</a> about whether they have rational debate methods or an alternative way to rationally resolve disagreements. The answer was, in my words:</p>\n<blockquote>\n<p>EA lacks formal rationality policies, but individuals informally do many things to try to be rational and the community tries to encourage rationality. EA\u2019s rationality efforts are larger than in most communities and include funding some rationality-related projects.</p>\n</blockquote>\n<p>Many people find that kind of answer satisfactory, but I think it\u2019s actually a major problem. I\u2019ll analyze it using two main concepts: fallibilism and political philosophy.</p>\n<p>The goal of this article is to explain the problem and, in principle, what sort of solutions to pursue. I do have some concrete suggestions but, to keep this shorter and more focused, I\u2019ll save them for a later article.</p>\n<p>I\u2019ll discuss fallibilist principles, then political philosophy, then advocate a way of using a political philosophy concepts to be more rational. Finally, using those ideas, I\u2019ll criticize some of EA\u2019s ways of trying to be rational.</p>\n<h2>Fallibilism Summary</h2>\n<p>Fallibilism says that people are capable of making mistakes and there\u2019s no way to get a 100% guarantee that any particular idea is not mistaken. Every idea we have is fallible. There are logical arguments for that. Fallibilists also commonly believe that mistakes are common and an important issue: our epistemology should have a lot to say about mistakes rather than treating them as merely a technically possible.</p>\n<p>Making mistakes without realizing you\u2019re making mistakes is common. Being biased without realizing you\u2019re biased is common. Being irrational about an issue without realizing it is common.</p>\n<p>I\u2019m guessing that the majority of EAs agree so far, and possibly there will be significant agreement with the rest of this section as well as the political philosophy section.</p>\n<p>I go further by rejecting answers in general categories like: \u201c<strong>try</strong> not to be mistaken, biased or irrational\u201d or \u201c<strong>trust</strong> yourself not to be mistaken, biased or irrational\u201d or \u201c<strong>bet</strong> high stakes on you not being mistaken, biased or irrational in this case\u201d.</p>\n<p>I think it\u2019s <em>important to assume you will fail at bias/rationality sometimes, and to plan for that</em>. What kind of <em>failsafes</em> can help you in cases where your trying doesn\u2019t work? What can you do so that you aren\u2019t betting anything important on your fallibilism not striking in a particular case? I regard many intellectuals, including scientists, as betting their the productivity of their careers on not being mistaken about certain issues, which I think is unnecessary and unwise. The career betting is due to the combination of working based on certain premises, which is OK alone, <em>and</em> also being unwilling to consider and discuss some criticisms of those premises. Even if you can\u2019t find ways to avoid all risky bets, you can minimize them and take steps to make the remaining bets less risky. (It\u2019s one thing to make a mistake that no one knows is a mistake. It\u2019s worse to make a mistake that someone already understands is a mistake, when they were willing to share that information, but you wouldn\u2019t listen due to e.g. using gatekeeping that blocks the criticism from reaching you. You should have robust policies to prevent that scenario from happening.)</p>\n<p>Put another way, when you think you\u2019re being unbiased, you should not trust yourself; you should regard that self-evaluation as unreliable and prefer an approach that\u2019s more objective than trusting your rationality. If you consistently trust yourself and your judgment, you\u2019re likely to go wrong sometimes. A strong fallibilist who refuses to trust himself will design things to mitigate the harm when he\u2019s biased or irrational, while a person who thinks trusting himself is OK will not work on many if any mitigations or failsafes, so his irrationality or bias will do more harm.</p>\n<h2>Political Philosophy</h2>\n<p>A key idea in political philosophy is that government officials cannot be simply trusted. They may be biased, irrational, corrupt, selfish, greedy, etc. So you shouldn\u2019t give them unlimited power to arbitrarily do whatever they want.</p>\n<p>Even if they promise to <em>try</em> really hard to be fair, government officials should not be <em>trusted</em>. Even if they <em>really do try</em>, they <em>still</em> should not be trusted. <em>Good faith trying to be fair is not good enough.</em> They\u2019ll be biased sometimes and unaware of their own bias.</p>\n<p>So we design government systems with features to help with this problem. Those features include assigning officials only <strong>limited powers, written rules and policies, checks and balances, transparency, accountability and elections</strong>. Each of these design ideas can be used, not only for politics, but also by fallibilist individuals to help deal cases where they\u2019re biased or irrational.</p>\n<p>Note: For good effectiveness, these ideas must be used in general, at all times, not only in the cases where you\u2019re biased, because you don\u2019t accurately know which cases you\u2019re biased about. We can\u2019t just activate these policies, as needed, to deal with bias. We have to use them all the time because bias could strike at any time without us realizing it. Similarly we ask government officials to use these things all the time, not just in the cases where they think they might be biased. We do also have extra policies that can be used when someone does recognize their own potential bias. An example is a person abstaining from participating in something because they have a conflict of interest. That is something that other people can request but people also&nbsp;sometimes voluntarily choose it for themselves.</p>\n<p>Perhaps the most important innovation to help with with abuse of power by officials and elites is called the <strong>rule of law</strong>. The idea is to write down laws which apply to everyone. Officials, instead of deciding whatever they want, are tasked with following and applying the laws&nbsp;as written. Instead of using their own judgment about the issue, they instead have the more limited task of interpreting the law. If the law is written clearly, then in many cases it\u2019s pretty obvious to everyone what the law says. That means if a government official gives the wrong ruling, he\u2019s doing something wrong which everyone can see, so there will be backlash (e.g. voting him out of power; if it\u2019s a military dictatorship or hereditary monarchy then citizens have worse options such as violent revolution or complaining).</p>\n<p>Clear, written laws provide predictability for citizens. They know in advance what actions will get them in trouble and what actions are allowed. The laws also limit the actions government officials may take. In order to provide predictability in advance, laws should be changed infrequently, and you should only get in trouble if you do something bad after there was already a law against it. New or changed laws shouldn\u2019t be applied retroactively.</p>\n<p>Despite the importance of rule of law, people haven\u2019t done a great job of using the same concept outside of political matters. For example, online forums frequently have vague rules and then moderators do things like ban someone without even claiming that he broke a rule that already existed when he did whatever action the moderator didn\u2019t like. Warnings help a lot here; if someone breaks something you think should be a rule, but that isn\u2019t clearly written down, then you can tell him that\u2019s not allowed and ban him if he does it again after being warned. That gives him reasonable predictability so that he can avoid being banned if he wants to (as long as he gets a warning for every different thing and could get a dozen separate warnings without being banned, which would be atypical moderator behavior). Many moderators use warnings part of the time but are inconsistent about it, which allows them to unpredictably ban people sometimes. Unfortunately, the cases where moderators don\u2019t issue warnings tend to be the very same cases where the moderators are biased, irrational or unreasonable. If you follow good policies 95% of the time, but you\u2019re biased sometimes, it\u2019s likely that the times you\u2019re biased will be some of the same times you decide not to follow your general policies. It\u2019s really important to follow policies 100% of the time because the exceptions you make are not random; they are likely to be correlated with the times you\u2019re biased.</p>\n<p>(Note: I reviewed some EA moderator <a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum\">policies</a> and <a href=\"https://forum.effectivealtruism.org/moderatorComments\">past actions</a>. Having some publicly documented history of moderator actions is great. I did not find any major moderator errors. However, overall, I also didn\u2019t find enough clear differentiators which would convince me that EA moderators are definitely better than the bad moderators I\u2019ve seen at some other forums. I think there\u2019s room for improvement at communicating about moderator policies if not improving the policies themselves. I think a <em>lot</em> of people have had bad experiences with moderators at some other sites, and if EA is actually better it\u2019d be worth communicating that in more convincing ways that show understanding of the problems elsewhere and explain clear differentiators. Superior transparency is a good differentiator but isn\u2019t enough alone and also isn\u2019t emphasized. I found the rules too vague, however I also noticed EA moderators giving warnings, which does a lot to make up for vague rules. For governments, I don\u2019t think warnings are a reasonable replacement for laws. But for forums, especially small forums, they work much better and I do use warnings at <a href=\"https://discuss.criticalfallibilism.com/\">my own forum</a>. Writing down really clear rules is hard but is absolutely worth the effort for governments with millions of citizens, but it\u2019s reasonable for small forums to do less there. Social media sites with more users than the populations of many countries should have rules just as clear and high effort as governments do. But they very much do not and aren\u2019t trying to. And when those social media sites function partially as extensions of the government which are significantly influenced or controlled by the government, then their poor written policies actually erode the rule of law and give the government a way to have power over people\u2019s lives outside of the legal system.)</p>\n<p>The idea has been explained as replacing the <em>rule of man</em> with the <em>rule of law</em>. An article titled <a href=\"https://fee.org/articles/rule-of-man-or-rule-of-law/\">&nbsp;Rule of Man or Rule of Law?</a> argues that the rule of law is the \u201csingle most important factor in quality governance\u201d (and also says that most educated Western adults, whose lives benefit from the rule of law, don\u2019t understand its importance). Wikipedia has entries for <a href=\"https://en.wikipedia.org/wiki/Rule_of_law\">rule of law</a> and <a href=\"https://en.wikipedia.org/wiki/Rule_of_man\">rule of man</a>. Many political philosophy books have discussed these issues.</p>\n<p>If you <em>trust</em> yourself to be unbiased, or think <em>trying</em> to be unbiased is adequate, you\u2019re relying on the rule of man. If you pre-commit yourself to follow specific written rules (laws), then you\u2019re treating your individual, personal life using the superior political philosophy of the rule of law. It also helps to use related policies like transparency and accountability. If you have rules that no one else knows about, or no one can tell when you\u2019re following them or not, then you can more easily break your own rules and find a way to rationalize it to yourself. Your bias or irrationality might get you to excuse a special exception. And if you make exceptions, they\u2019re likely to correlate with when you\u2019re doing something wrong, so saying \u201cI follow my rules 99% of the time\u201d isn\u2019t actually very good. (If you made special exceptions 1% of the time and the 1% were chosen by random dice roll, that\u2019d actually be pretty good. The point of claiming 99% rule following is to fool yourself into thinking you\u2019re doing just as good as that, but you\u2019re not.)</p>\n<p>If you\u2019re not comfortable making public policies and pre-commitments, I still recommend doing it privately, in writing, and trying to hold yourself accountable. That\u2019s much better than nothing and may also lead to increased comfort with doing it publicly in the future.</p>\n<h2>Overcoming Bias Using Written Policies</h2>\n<p>Let\u2019s return now to EA\u2019s viewpoint, which I consider unsatisfactory:</p>\n<blockquote>\n<p>EA lacks formal rationality policies, but individuals informally do many things to try to be rational and the community tries to encourage rationality. EA\u2019s rationality efforts are larger than in most communities and include funding some rationality-related projects.</p>\n</blockquote>\n<p>I view this as failing to apply the concept of the rule of law. Having formal, written policies is like having the rule of law. It constrains your arbitrary actions. It limits your own power. It means that, in cases where you\u2019re biased or irrational, you have a defense mechanism (a.k.a. failsafe): your pre-commitment to some written policies.</p>\n<p>I view EA as not designing around the fallibilist assumption that we\u2019ll fail at rationality and bias sometimes. Just as a government official trying his best to be fair <em>is not good enough</em>, so too is <em>trying your best to be rational</em> not good enough. I think EA as a group, and individual EAs, should use written policies to take a more \u201crule of law\u201d style approach to rationality and bias. (I also have the same suggestion for approximately all other groups besides EA. None of this is intended as a claim that EA is worse in comparison to some other group.)</p>\n<p>I propose that EA individuals and groups should work on good policies to publicly write down and pre-commit to in the rule of law style. I think that would make EA significantly more effective than can be achieved by trying hard to be rational to or trusting yourself to be rational. Effort and trust are bad solutions to bias, irrationality&nbsp;and fallibility.</p>\n<p>It\u2019s great that EAs are more interested than most people in reading and writing about rationality, and they put more effort into overcoming their biases. That is a positive trait. But I suggest adding rule-of-law-inspired written policies and being more suspicious of anyone, including yourself, who thinks anything that fits into the *trying or trusting&nbsp;*categories is good enough.</p>\n<h2>Critique: Examples of EAs Trying and Trusting</h2>\n<p>In response to my questions, Thomas Kwa <a href=\"https://forum.effectivealtruism.org/posts/btJWbzqrD8K9cz58X/how-is-ea-rational\">wrote</a> a list of ways EA tries to be rational. To help clarify what I mean above, I\u2019m going to quote and comment critically, primarily about why his points fit into the <strong>trying</strong> and/or <strong>trusting</strong> categories. I\u2019m responding to Kwa\u2019s comments in particular because I think they were the <em>best</em> and <em>clearest</em> answer.</p>\n<blockquote>\n<ul>\n<li>taking weird ideas seriously; being willing to think carefully about them and dedicate careers to them</li>\n</ul>\n</blockquote>\n<p>This means <em>trying</em> to take weird ideas seriously. It involves making an <em>effort</em> to be fair, reasonable, curious, etc. There is an EA norm, which people <em>try</em> to follow, which favors this. People put <em>effort</em> into it.</p>\n<p>But there are no concrete, written policies. Neither EA as a group, nor individuals, write policies that they pre-commit to follow in order to achieve this goal. They just <em>try</em> to do it and perhaps <em>trust</em> their ability to do it pretty well (though not perfectly or infallibly).</p>\n<blockquote>\n<ul>\n<li>being unusually goal-directed</li>\n</ul>\n</blockquote>\n<p>This means <em>trying</em> to be goal-directed but not following a written policy.</p>\n<blockquote>\n<ul>\n<li>being unusually truth-seeking\n<ul>\n<li>this makes debates non-adversarial, which is easy mode</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<p>Being good at truth-seeking is something people <em>try</em> to do. This makes <em>some</em> debates <em>less</em> adversarial, but we shouldn\u2019t <em>trust</em> it to make all debates non-adversarial with no qualifiers.</p>\n<blockquote>\n<ul>\n<li>openness to criticism, plus a decent method of filtering it</li>\n</ul>\n</blockquote>\n<p>EAs <em>try</em> to be open to criticism. Also I think any method of filtering criticism&nbsp;(a.k.a. gatekeeping)&nbsp;is especially important to write down because it could systematically block consideration of some broad categories of important ideas. It could also explicitly or implicitly do gatekeeping based on social status. Irrational filtering is a widespread problem in society today.</p>\n<blockquote>\n<ul>\n<li>high average intelligence. Doesn't imply rationality but doesn't hurt.</li>\n</ul>\n</blockquote>\n<p>This basically means <em>trying</em> to be smart. And there are no written (or unwritten?) policies to only let smart people join EA, so I don\u2019t think it\u2019s very reliable. (Note: I think such policies would be bad.) This also implicitly relies on a small, fairly homogeneous community, which I comment more on below.</p>\n<blockquote>\n<ul>\n<li>numeracy and scope-sensitivity\n<ul>\n<li>willingness to use math in decisions when appropriate (e.g. EV calculations) is only part of this</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<p>Dealing with math and scopes well is something EAs <em>try</em> for.</p>\n<blockquote>\n<ul>\n<li>less human misalignment: EAs have similar goals and so EA doesn't waste tons of energy on corruption, preventing corruption, negotiation, etc.</li>\n</ul>\n</blockquote>\n<p>This claims that having more anti-corruption effort would <em>waste energy</em>. It seems contrary to rule of law attitudes. We\u2019re similar to each other and our goals are aligned, so we don\u2019t need to do much to prevent corruption!? No.  Even in a small town or a small, homogeneous, low-immigration, high-trust society, you still should not trust government officials that much. Similarly, people who run charities should not be <em>trusted</em> with the money; it\u2019s worth the energy to have transparency and accountability.</p>\n<p>This point implicitly advocates more <em>trust</em> on the basis of having a <em>small, fairly homogenous community</em>. That attitude makes it harder for outsiders to immigrate to the community (or join and fit in, in less political terms). It\u2019s suggesting that people who are different than me are more likely to be corrupt. Right now, everyone can be mostly <em>trusted</em>, but if the community becomes more intellectually diverse, then I won\u2019t be so trusting. In other words, people similar to me are morally superior (less corruptible). That kind of belief has a long, biased history. It\u2019s important to aim for intellectual diversity from the start, plan around it, and be welcoming towards the out-group, the heretics, the dissenters, the people who are harder for me to be friends with, the people who rub me the wrong way, etc. We need more tolerance and respect for culture clash, even when it takes effort, instead of wanting to need little negotiation because everyone is already so similar that things feel easy.</p>\n<p>I considered that this point might only refer to corruption in a narrow sense like embezzling money. But I think it covers e.g. decision makers abusing their power to make biased decisions. Even if it only meant stuff like embezzling, I <em>still</em> think EA should spend effort preventing that. I think it\u2019d be really naively <em>trusting</em> to lack robust defenses against embezzlement and then believe that was efficiently saving effort.</p>\n<blockquote>\n<ul>\n<li>relative lack of bureaucracy</li>\n</ul>\n</blockquote>\n<p>Whether this is good depends on how it\u2019s achieved and what it means. If it means having less rule of law, I think it\u2019s bad. If it means having fewer <em>bad</em> policies, that\u2019s good. Bureaucracy can refer to lots of negative things including bad laws, bloated policies, unproductive paperwork requirements, office politics, power struggles, and people who won\u2019t use their brain. It\u2019s bad to mechanically follow rules instead of using rules to enhance creativity or constrain actions.</p>\n<p>It\u2019s common for organizations to become more bureaucratic as they get larger. Why? One of the main reasons is because when everyone is culturally similar and friends with each other, they find it relatively easy to get along. They can settle many disputes by talking it out, and they have fewer disputes in the first place. However, as a group becomes larger and therefore more intellectually diverse, the method of <em>trying</em> to discuss problems and <em>trusting</em> others to engage in good-faith problem solving becomes less effective. In other words, the larger the group, the more necessary some sort of rule of law is.</p>\n<p>As organizations get bigger, written rules and policies can help a lot with fair, predictable dispute resolution and can help with other issues such as spreading useful knowledge and ways of doing things to newcomers. For example, a policy of writing things in a knowledge base, instead of spreading that knowledge to new people by word of mouth, can make an organization or community more welcoming to people who don\u2019t quickly fit in and have social rapport with existing members. Barriers to entry like needing to ask other people for help in informal ways help keep groups less intellectually and culturally diverse. Lack of clear, written rules and expectations (and the community actually following the rules instead of writing down one thing then doing something else) is also extremely stressful for many people.</p>\n<p>Making things more explicit increases their legibility or understandability for people who are different than you. The more similar to people you are, the less you need them to explain things; you can pick up on hints better and make better guesses about what they mean. Groups which aren\u2019t very explicit are inhospitable to other ideas besides the ideas that all the members believe but don\u2019t write down.</p>\n<p>Whether policies seem bureaucratic primarily depends on how good they are. If they\u2019re designed well enough, and cost effective enough, then people will like them instead of complaining about bureaucracy. As it grows, EA will have a better chance to have good policies if it starts developing them <em>now</em> instead of waiting until later. It\u2019s easier to experiment with policies, and figure out how to make them work well, when smaller. (EA\u2019s lack of a clear central authority will make this a bit more complicated, and will mean more different actors need to think about policies. I think that can work well but will require some extra experimentation to get right.)</p>\n<blockquote>\n<ul>\n<li>various epistemic technologies taken from other communities: double-crux, forecasting</li>\n<li>ideas from EA and its predecessors: crucial considerations, the ITN framework, etc.</li>\n</ul>\n</blockquote>\n<p>I think these are good, but without written policies that pre-commit about when to use them, their effectiveness is limited. I\u2019m concerned that people rely on <em>trying</em> to use them when they should be used, or <em>trusting</em> themselves to use them at the appropriate times.</p>\n<blockquote>\n<ul>\n<li>taste: for some reason, EAs are able to (hopefully correctly) allocate more resources to AI alignment than overpopulation or the <a href=\"https://forum.effectivealtruism.org/posts/wXzc75txE5hbHqYug/the-great-energy-descent-short-version-an-important-thing-ea\" title=\"energy decline\">energy decline</a>, for reasons not explained by the above.</li>\n</ul>\n</blockquote>\n<p>This reads to me as viewing EA as higher in rationality because you agree with some of its conclusions. I consider that irrational. Rationality should be judged by ability to make progress and openness to error correction, not by whether you agree with a person\u2019s or group\u2019s views.</p>\n<p>Overall, I read this list as ways EA <em>tries</em> to be rational and sometimes <em>trusts</em> its rationality instead of putting effort into mitigating the harm of rationality failures. I think pre-commitment to written policies, analogous to the rule of law, would be more effective. It would also expose those rationality ideas to critical discussion and enable an ongoing project of coming up with policy (and therefore rationality) improvements. This is especially important because rationality failures correlate significantly with key issues rather than being random, so even if you make a pretty good effort at rationality, and keep the quantity of failures fairly low, the impact is still large.</p>\n<p>So, learn from the rule of law, and <strong>pre-commit to written rationality policies</strong> instead of <em>trying</em> to be rational or <em>trusting</em> yourself. (If you don\u2019t want to do that, it could be a sign that you want the flexibility to act capriciously. It could also be a sign that you don\u2019t know how to write policies well. Poorly written policies can do more harm than good.)</p>\n", "user": {"username": "Elliot Temple"}}, {"_id": "pmJRXG3cTgrt779Ep", "title": "EA & LW Forums Weekly Summary (10 - 16 Oct 22')", "postedAt": "2022-10-17T22:51:03.454Z", "htmlBody": "<p><i>Supported by Rethink Priorities</i></p><p>This is part of a weekly series - you can see the full collection <a href=\"https://forum.effectivealtruism.org/s/W4fhpuN26naxGCBbN\">here.</a> The first post includes some details on purpose and methodology.</p><p>If you'd like to receive these summaries via email, you can subscribe <a href=\"https://easummaries.substack.com/?r=1p817z&amp;s=w&amp;utm_campaign=pub&amp;utm_medium=web\">here.</a></p><p><strong>Podcast version</strong>: prefer your summaries in podcast form? A big thanks to Coleman Snell for producing these! Subscribe on your favorite podcast app by searching for 'Effective Altruism Forum Podcast'.</p><p>&nbsp;</p><h1>Top / Curated Readings</h1><p>Designed for those without the time to read all the summaries. Everything here is also within the relevant sections later on so feel free to skip if you\u2019re planning to read it all.<br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/kFufCHAmu7cwigH4B/lessons-learned-from-talking-to-greater-than-100-academics\"><u>Lessons learned from talking to &gt;100 academics about AI safety</u></a></p><p><i>by mariushobbhahn</i></p><p>The author talked to 100-200 people in academia about AI safety, from bachelor\u2019s students to senior faculty, over several years. This post summarizes the key learnings.</p><p>Author\u2019s tl;dr (lightly edited): \u201cAcademics are increasingly open to arguments about AI risk and I\u2019d recommend having lots of these chats. I underestimated how much work related to aspects of AI safety (eg. interpretability) already exists in academia - we sometimes reinvent the wheel. Messaging matters, e.g. technical discussions got more interest than alarmism and explaining the problem rather than trying to actively convince someone received better feedback.\u201d<br><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/8whqn2GrJfvTjhov6/measuring-good-better-1\"><u>Measuring Good Better</u></a></p><p><i>by MichaelPlant, GiveWell, Jason Schukraft, Matt_Lerner, Innovations for Poverty Action</i></p><p>Transcripts of 5-minute lightning talks by various orgs on their approach to measuring \u2018good\u2019. A very short summary of each is below:</p><p><strong>Givewell</strong> uses moral weights to compare different units (eg. doubling incomes vs. saving an under-5s life). These are 60% based on donor surveys, 30% from a 2019 survey of 2K people in Kenya and Ghana, and 10% staff opinion.</p><p><strong>Open Philanthropy</strong>\u2019s global health and wellbeing team uses the unit of \u2018a single dollar to someone making 50K per year\u2019 and then compares everything to that. Eg. Averting a DALY is worth 100K of these units.</p><p><strong>Happier Lives Institute</strong> focuses on wellbeing, measuring WELLBYs. One WELLBY is a one-point increase on a 0-10 life satisfaction scale for one year.</p><p><strong>Founder\u2019s pledge</strong> values cash at $199 per WELLBY. They have conversion rates from WELLBYs to Income Doublings to Deaths Avoided to DALYs Avoided, using work from some of the orgs above. This means they can get a dollar figure they\u2019re willing to spend for each of these measures.</p><p><strong>Innovations for Poverty Action</strong> asks different questions depending on the project stage (eg. idea, pilot, measuring, scaling). Early questions can be eg. if it\u2019s the right solution for the audience, and only down the line can you ask \u2018does it actually save more lives?\u2019</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/5xqCrqD3zkNi5ctEx/metaculus-launches-the-forecasting-our-world-in-data-project\"><u>Metaculus Launches the 'Forecasting Our World In Data' Project to Probe the Long-Term Future</u></a></p><p><i>by christian, EdMathieu</i></p><p>Forecasting Our World In Data is a tournament that will deliver predictions on technological advancement, global development, and social progress using Our World in Data metrics. 20K prize pool for accurate forecasts on 1-3 year time horizons, and cogent analysis on 10-100 years horizons. The first questions have opened, with more to come on 19th and 26th Oct.</p><p><br>&nbsp;</p><h1>EA Forum</h1><h2>Philosophy and Methodologies</h2><p><a href=\"https://forum.effectivealtruism.org/posts/8Ban7AnoqwdzQphsK/we-can-do-better-than-argmax\"><u>We can do better than argmax</u></a></p><p><i>by Jan_Kulveit, Gavin</i></p><p>Author\u2019s tl;dr (lightly edited): a common prioritization method in EA is putting all resources on your top option (argmax). But this can be foolish, so we deviate in ad-hoc ways. We describe a principled softmax approach, allocating resources to several options by confidence. This works well when a whole community collaborates on impact; when some opportunities are fleeting or initially unknown; or when large actors are in play.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/wqmY98m3yNs6TiKeL/parfit-singer-aliens\"><u>Parfit + Singer + Aliens = ?</u></a></p><p><i>by Maxwell Tabarrok</i></p><p>If we observe even simple (eg. single cellular) alien life, the chance of intelligent and morally relevant alien life existing <i>somewhere</i> increases drastically. In this case, human extinction isn\u2019t as bad - the difference between eg. 95% and 100% of humans dead becomes much less. This makes risky moves like advancing AI or biotech (which could either destroy us or be hugely positive) more positive on balance, and implies we should upweight higher volatility paths.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/8whqn2GrJfvTjhov6/measuring-good-better-1\"><u>Measuring Good Better</u></a></p><p><i>by MichaelPlant, GiveWell, Jason Schukraft, Matt_Lerner, Innovations for Poverty Action</i></p><p>Transcripts of 5-minute lightning talks by various orgs on their approach to measuring \u2018good\u2019. A very short summary of each is below:</p><p><strong>Givewell</strong> uses moral weights to compare different units (eg. doubling incomes vs. saving an under-5s life). These are 60% based on donor surveys, 30% from a 2019 survey of 2K people in Kenya and Ghana, and 10% staff opinion.</p><p><strong>Open Philanthropy</strong>\u2019s global health and wellbeing team uses the unit of \u2018a single dollar to someone making 50K per year\u2019 and then compares everything to that. Eg. Averting a DALY is worth 100K of these units.</p><p><strong>Happier Lives Institute</strong> focuses on wellbeing, measuring WELLBYs. One WELLBY is a one-point increase on a 0-10 life satisfaction scale for one year.</p><p><strong>Founder\u2019s pledge</strong> values cash at $199 per WELLBY. They have conversion rates from WELLBYs to Income Doublings to Deaths Avoided to DALYs Avoided, using work from some of the orgs above. This means they can get a dollar figure they\u2019re willing to spend for each of these measures.</p><p><strong>Innovations for Poverty Action</strong> asks different questions depending on the project stage (eg. idea, pilot, measuring, scaling). Early questions can be eg. if it\u2019s the right solution for the audience, and only down the line can you ask \u2018does it actually save more lives?\u2019</p><p><br>&nbsp;</p><h2>Object Level Interventions / Reviews</h2><p><a href=\"https://forum.effectivealtruism.org/posts/TT3gNbA534C7HNBCF/introducing-the-ea-good-governance-project\"><u>Introducing the EA Good Governance Project</u></a></p><p><i>by Grayden</i></p><p>Author\u2019s tl;dr: \u201cI believe good governance is important and often underrated within EA. I'm launching the<a href=\"https://www.eagoodgovernance.com/\"><u> EA Good Governance Project</u></a>.&nbsp; Its first initiative will be a directory of EA Board candidates.&nbsp; If you have skills and experience to offer to an EA Board, please&nbsp;<a href=\"https://www.eagoodgovernance.com/candidates\"><u>add your profile</u></a>.\u201d</p><p>They also plan to add practical resources for Boards eg. how to measure impact and set appropriate policies, and are looking for contributors to this.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/qFoPidYq28b5vZqe2/why-i-think-there-s-a-one-in-six-chance-of-an-imminent\"><u>Why I think there's a one-in-six chance of an imminent global nuclear war</u></a></p><p><i>by Tegmark</i></p><p>The author predicts there is a 30% chance of Russia launching nukes, in that case 80% chance that NATO responds with conventional weapons, and in that case 70% chance of a global nuclear war. This equates to a \u2159 chance of global nuclear war from today\u2019s state.</p><p>They argue that Putin will not accept a full loss without going nuclear, because he\u2019d likely be jailed / killed. The other alternative, de-escalation, seems disfavored in the West because Ukraine is winning. And if a nuke is used in Ukraine, escalation to eventual nuclear war seems likely because the countries involved have a long history of nuclear near misses, and have made retaliation threats already.</p><p>These predictions are significantly more pessimistic than the community average. Metaculus currently gives the first stage (nuclear use in Ukraine) 7% odds this year, and the Samotsvetsy gives 16% for the next year, and 1.6% of nuclear use beyond that in the next year.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/kFufCHAmu7cwigH4B/lessons-learned-from-talking-to-greater-than-100-academics\"><u>Lessons learned from talking to &gt;100 academics about AI safety</u></a></p><p><i>by mariushobbhahn</i></p><p>The author talked to 100-200 people in academia about AI safety, from bachelor\u2019s students to senior faculty, over several years. This post summarizes the key learnings.</p><p>Author\u2019s tl;dr (lightly edited): \u201cAcademics are increasingly open to arguments about AI risk and I\u2019d recommend having lots of these chats. I underestimated how much work related to aspects of AI safety (eg. interpretability) already exists in academia - we sometimes reinvent the wheel. Messaging matters, e.g. technical discussions got more interest than alarmism and explaining the problem rather than trying to actively convince someone received better feedback.\u201d</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/aJNoyo8GsK3qS9wug/anonymous-advice-if-you-want-to-reduce-ai-risk-should-you\"><u>Anonymous advice: If you want to reduce AI risk, should you take roles that advance AI capabilities?</u></a></p><p><i>by Benjamin Hilton, 80000_Hours</i></p><p>Work which increases AI capabilities is intertwined with certain types of safety work, and can be a good way to skill-build for safety work. However, it can also be harmful by accelerating dangerous AI. 80K anonymously asked 22 experts for their views on this balance, and received 10 responses published in full in this post.</p><p>How many experts leant each way and the common arguments for those leanings are summarized below.</p><p><strong>Mostly Yes (2)</strong></p><ul><li>There\u2019s lots of overlap between alignment &amp; capabilities work</li><li>Capabilities work is heavily funded and incentivized - on the margin one researcher isn\u2019t going to do much</li><li>Influencing the orgs from within is important</li><li>Building career capital is important</li></ul><p><strong>It Depends / 'Yes if careful' (4)</strong></p><ul><li>Some types of capabilities research are more dangerous than others (eg. training efficiency, world model understanding) - avoid these</li><li>Other types overlap well with safety, or are unlikely to accelerate AGI - go for these</li><li>Value drift is common, stay in touch with the alignment community</li></ul><p><strong>Mostly No (3)</strong></p><ul><li>There\u2019s a limited amount of capabilities research until we reach AGI, and that\u2019s our timeline for alignment - don\u2019t use it up unless really worth it</li><li>If big alignment payoff, or you wouldn't be at the edge of capabilities research and any capabilities work is kept private then it could be worth it</li></ul><p><strong>Strong No (1)</strong></p><ul><li>Top researchers can contribute a lot - don\u2019t risk advancing capabilities significantly</li><li>Alignment research has capabilities implications, the reverse is rarely true</li></ul><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/DArtSnDxRH5AsE5RF/sheltering-humanity-against-x-risk-report-from-the-shelter\"><u>Sheltering humanity against x-risk: report from the SHELTER weekend</u></a></p><p><i>by Janne M. Korhonen</i></p><p>Write-up of learnings from a participant in the August SHELTER weekend, an event for gaining clarity on what\u2019s needed to build civilizational shelters.</p><p>Key takeaways included:</p><ul><li>Reliable shelters against x-risk are probably infeasible, and have some downsides (eg. incentivizing hazards, rich vs. poor dynamics).&nbsp;<br>&nbsp;</li><li>Increasing societal resilience and capability for cooperative action is a better approach. This could be tackled via improving existing disaster management systems, hardening risky facilities such as bio labs, and supporting isolated communities (with their buy-in).<br>&nbsp;</li><li>Both shelters and general resilience efforts are primarily&nbsp;<i>not</i> a technological problem. An exception would be developing self-sustaining ecosystems for space colonization (a long-term bet).<br>&nbsp;</li><li>Risks without societal breakdown involved are unlikely to be x-risks (other than those that wouldn\u2019t benefit from shelters, such as AI). Dangerous pathogens were broadly agreed as the highest risk.<br><br>&nbsp;</li></ul><p><a href=\"https://forum.effectivealtruism.org/posts/SnSAJpPZjLj2JWC56/responding-to-recent-critiques-of-iron-fortification-in\"><u>Responding to recent critiques of iron fortification in India</u></a></p><p><i>by e19brendan</i></p><p>Fortify Health co-founder Brendan responds to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/SgqBAeoCbQeLxmMoj/targeted-treatment-of-anemia-in-adolescents-in-india-as-a\"><u>forum</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/2gG7eeDD5uqud4Rfm/cost-effectiveness-of-iron-fortification-in-india-is-lower\"><u>posts</u></a> which suggest that recent studies on the prevalence of anemia in India and proportion attributable to iron-deficiency should lower cost-effectiveness estimates of fortification. The posts also suggested using more targeted treatment and changing anemia cut-offs.</p><p>Brendan reviewed the provided studies, and found that:</p><ul><li>Anemia attributable to iron deficiency is an important measure, but the studies seem comparable to Givewell estimates.</li><li>Targeted treatment has promise and is likely worth doing (though less scalable).</li><li>Cutoffs for \u2018healthy\u2019 iron levels is a tricky topic - even those without diagnosed conditions might be healthier with more iron.</li></ul><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/zoWypGfXLmYsDFivk/counterarguments-to-the-basic-ai-risk-case\"><u>Counterarguments to the basic AI risk case</u></a></p><p><i>by Katja_Grace</i></p><p>Counters to the argument that goal-directed AIs are likely and it\u2019s hard to align them to good goals, therefore there\u2019s significant x-risk:</p><ul><li><strong>AIs may optimize more for \u2018looking like they\u2019re pursuing X goal\u2019 than actually pursuing it.</strong> This would mean they wouldn\u2019t go after instrumental goals like money or power. The jury is out on if ML models lean this way or not.<br>&nbsp;</li><li><strong>Even if an AI\u2019s values / goals don\u2019t match ours, they could be close enough</strong>, or be non-destructive. Or they could have short time horizons that don\u2019t make worldwide takeovers worth it.<br>&nbsp;</li><li><strong>We might be more powerful than a superintelligent AI</strong>. Collaboration was as or more important than intelligence for humans becoming the dominant species, and we could have non-agentic AIs on our side. AIs might also hit ceilings in intelligence, or be working on tasks that don\u2019t scale much with intelligence.<br>&nbsp;</li><li><strong>The core AI x-risk argument could apply to corporations too - but we don\u2019t consider them x-risks</strong>. Corporations are goal-directed, hard to align precisely, far more powerful than individual humans, and adapt over time - but aren\u2019t considered x-risks.</li></ul><p>&nbsp;</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/c6RnqjBd3BAkqsknB/the-us-expands-restrictions-on-ai-exports-to-china-what-are\"><u>The US expands restrictions on AI exports to China. What are the x-risk effects?</u></a></p><p><i>by Stephen Clare</i></p><p>Last week the Biden administration announced regulations that make it illegal for US companies to export certain AI-related products and services to China, including high-end chips and semiconductor equipment.</p><p>The author questions on the impact on China\u2019s AI trajectory, if this will increase the likelihood of conflict, and how these rising tensions might affect cooperation on other global risks.</p><p><br>&nbsp;</p><h2>Opportunities</h2><p><a href=\"https://forum.effectivealtruism.org/posts/v2NwSbp3ZPdeB76B5/seri-mats-program-winter-2022-cohort\"><u>SERI MATS Program - Winter 2022 Cohort</u></a></p><p><i>by Ryan Kidd</i></p><p>Applications open until Oct 24th for the MATS program, which supports aspiring alignment researchers to do independent research with funding (via LTFF), mentorship and training. The winter cohort will run Nov 7 - Feb 23rd, and be in person in Berkeley from Jan 3rd.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/cNHbgp2MsGRZBAqjG/book-a-corporate-event-for-giving-season\"><u>Book a corporate event for Giving Season</u></a></p><p><i>by Jack Lewars, Luke Freeman, Federico Speziali</i></p><p>Author\u2019s tl;dr: \u201cfollowing the success of previous corporate talks, One for the World, Giving What We Can and High Impact Professionals are collaborating to offer a range of corporate talks this giving season. Use our&nbsp;<a href=\"https://forms.gle/ptkKJN6XfHCC4Wa98\"><u>contact form</u></a> to learn more or book a talk.\u201d GWWC is also offering workshops for a more interactive experience.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/GCfnaf5msCKiCngKs/alignment-201-curriculum\"><u>Alignment 201 curriculum</u></a></p><p><i>by richard_ngo</i></p><p>A follow-up to the&nbsp;<a href=\"https://www.agisafetyfundamentals.com/ai-alignment-curriculum\"><u>Alignment Fundamentals curriculum</u></a>, this 9-week curriculum aims to give enough knowledge to understand the frontier of current research discussions. It\u2019s targeted at those who have taken the previous course, in addition to having some knowledge of deep learning and reinforcement learning.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/5xqCrqD3zkNi5ctEx/metaculus-launches-the-forecasting-our-world-in-data-project\"><u>Metaculus Launches the 'Forecasting Our World In Data' Project to Probe the Long-Term Future</u></a></p><p><i>by christian, EdMathieu</i></p><p>Forecasting Our World In Data is a tournament that will deliver predictions on technological advancement, global development, and social progress using Our World in Data metrics. 20K prize pool for accurate forecasts on 1-3 year time horizons, and cogent analysis on 10-100 years horizons. The first questions have opened, with more to come on 19th and 26th Oct.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/pHLjjqwBMT3h5W9KE/growth-theory-reading-list\"><u>Growth Theory Reading List</u></a></p><p><i>by LuisMota</i></p><p>List of readings on economic growth theory, broken down into 10 sub-topics such as long run historical growth, AI and growth, stagnation, growth and happiness.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/FeBfELZgQsKbhMPAS/eagxvirtual-a-virtual-venue-timings-and-other-updates\"><u>EAGxVirtual: A virtual venue, timings, and other updates</u></a></p><p><i>by Alex Berezhnoi</i></p><p><a href=\"https://www.eaglobal.org/events/eagxvirtual-2022/\"><u>Applications</u></a> due before 19th October (conference from 21st October). &gt;600 applicants so far from &gt;60 countries. The post highlights content to expect, platforms used, and puts out a&nbsp;<a href=\"https://forms.gle/77auFZ5GntnHcjaKA\"><u>call for volunteers</u></a>.</p><p><br>&nbsp;</p><h2>Community &amp; Media</h2><p><a href=\"https://forum.effectivealtruism.org/posts/DCAptmT4jReq9aTvT/why-defensive-writing-is-bad-for-community-epistemics\"><u>Why defensive writing is bad for community epistemics</u></a></p><p><i>by Emrik</i></p><p>Defensive writing is optimizing your writing for making sure no-one has a bad impression of you. This can become a norm when readers try to make inferences about the author vs. just learning from the content (\u2018judgemental reading\u2019). Both make communication inefficient and writing scary.</p><p>The author suggests being clear as a writer about the purpose of your writing, and if it\u2019s helping your readers. As a reader, he suggests interpreting things charitably, rewarding confidence, and not punishing people for what they don\u2019t know.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/FtggfJ2oxNSN8Niix/when-reporting-ai-timelines-be-clear-who-you-re-not\"><u>When reporting AI timelines, be clear who you're (not) deferring to</u></a></p><p><i>by Sam Clarke</i></p><p>It\u2019s common to ask people\u2019s AI timelines, and also common for responses not to include whether they\u2019re independent impressions or based on other\u2019s views. This can lead to these timelines feeling more robust than they are, and to groups of EAs converging on the same timelines without good reason.<br><br>The author suggests if you haven\u2019t formed an independent impression, always say who you\u2019re deferring to. If you\u2019re asking about someone\u2019s timelines, always ask how they got to them. They\u2019ve also put up&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLScRfMzy51eOeRQyK2XpAY7pTgTGPHrdGBI7SAHWXBJAToBOnQ/viewform\"><u>a survey</u></a> to work out who people are deferring to most.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/RScaAGSG3M3c8vgir/on-absurdity\"><u>On absurdity</u></a></p><p><i>by OllieBase</i></p><p>What we\u2019re doing is absurdly ambitious. Looking at things through the absurdity lens can help us step back, get energy, and be kinder to ourselves and others (particularly when we fail). For instance, realizing \u2018trying to work out the world\u2019s biggest problem with my two college friends\u2019 or \u2018running for office with no political background to single-handedly influence the senate on global health security\u2019 are absurd takes off some of the pressure - while still remembering it\u2019s worth a shot!</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/LyTWpoQFozDfeQ9z8/some-carl-sagan-quotations\"><u>Some Carl Sagan quotations</u></a></p><p><i>by finm</i></p><p>Carl Sagan (1934 - 1996) was an astronomer and science communicator who captured many ideas related to longtermism and existential risk poetically. This article is a collection of some of the author\u2019s favorite quotes from him.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/MtD5S58nyuZt4toCg/counterproductive-ea-mental-health-advice-and-what-to-say\"><u>Counterproductive EA mental health advice (and what to say instead)</u></a></p><p><i>by Ada-Maaria Hyv\u00e4rinen</i></p><p>Some well-meaning advice is counter-productive. This includes telling people:</p><ul><li>Happiness is important to productivity (making happiness an instrumental goal tends to make people less happy)</li><li>They\u2019ve donated enough to save a life / offset damage, so they clearly deserve to live (associates their worth as a person with what they can contribute)</li><li>Take a break from EA and come back when you feel better (makes the right to a break feel conditional on coming back, so it\u2019s not a true break)</li></ul><p>Instead, say things in a way that reflects you care about that person for their intrinsic value, not just the impact they can have. This can be important in self-talk too.</p><p>&nbsp;</p><p><br><a href=\"https://forum.effectivealtruism.org/posts/uxnpir7zLgwf3whg7/cultural-ea-considerations-for-nordic-folks\"><u>Cultural EA considerations for Nordic folks</u></a></p><p><i>by Ada-Maaria Hyv\u00e4rinen</i></p><p>Cultural information about EA that contrasts with the norm from a Finnish / Nordic perspective.</p><p>Some key topics:</p><ul><li>Where EAs are concentrated and typical cultural differences there (eg. going to uni young, flatting due to high cost of living, willingness to move)<br>&nbsp;</li><li>Career considerations (eg. few EA employers hire in Finland, even remote ones may have timezone requirements or lack benefits, you don\u2019t need to have the \u2018right\u2019 degree to apply)<br>&nbsp;</li><li>Differences in interaction style (eg. EAs often use \u2018hype\u2019 language or target things toward the \u2018best\u2019, lots of jargon, and may be more assertive than average)</li></ul><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/8gyGtdAAuhT3AkXus/changes-to-ea-giving-tuesday-for-2022\"><u>Changes to EA Giving Tuesday for 2022</u></a></p><p><i>by Giving What We Can, mjamer, GraceAdams, Jack Lewars</i></p><p>Giving What We Can and One For The World volunteered to manage EA Giving Tuesday for 2022, somewhat scaled back (~25% the charities as previously, and minimal testing / revision of donation strategy). If you\u2019d like to participate, sign up for email updates&nbsp;<a href=\"https://forms.gle/zfZ898pFrBG95LPo6\"><u>here</u></a>.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/W2w7xA9AtDnjcK6DP/an-ea-s-guide-to-berkeley-and-the-bay-area\"><u>An EA's Guide to Berkeley and the Bay Area</u></a></p><p><i>by Elika, Vaidehi Agarwalla</i></p><p>Guide to newcomers, positives and negatives. Most helpful if you\u2019re already planning or seriously considering coming to Berkeley and want to get more context on the community and culture.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/MYuoB8eyvnMPmmfA2/pineapple-operations-is-expanding-to-include-all-operations\"><u>Pineapple Operations is expanding to include all operations talent (Oct '22 Update)</u></a></p><p><i>by Vaidehi Agarwalla, Alexandra Malikova</i></p><p>Pineapple Operations database of candidates now includes all Ops talent, not just PAs/ExAs. Links to&nbsp;<a href=\"https://airtable.com/shr2uaKcpsMNx3fc6\"><u>list yourself</u></a> or&nbsp;<a href=\"https://pineappleoperations.org/\"><u>search the 100+ candidates.</u></a><br><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/xnHnsrFEMEMPXBWqR/ask-charity-entrepreneurship-anything\"><u>Ask Charity Entrepreneurship Anything</u></a><i>&nbsp;</i></p><p><i>by Ula, KarolinaSarek, Joey</i></p><p>Some top comments at time of summarizing:</p><ul><li>CE believes there are limits to desk research, and time-caps research so it can get to pilots and better data faster.</li><li>Entrepreneurs starting separate organisations (vs. sitting as part of CE) gives them more flexibility to take on risk, lets them move fast, and often suits the applicants who want ownership of their project.</li><li>Many people underrate their chances of being accepted - eg. because they\u2019re new to EA, don\u2019t have experience or domain expertise, or think they\u2019ll be excluded on the basis of age or location. None of these factors should stop someone applying, the best way to test fit is to apply, and the best way to build experience and expertise is via the program.<br><br>&nbsp;</li></ul><h2>Didn\u2019t Summarize</h2><p><a href=\"https://forum.effectivealtruism.org/posts/F9AJpjg5D9qyYA9DM/let-me-blind-myself-to-forum-post-authors\"><u>Let me blind myself to Forum post authors</u></a>&nbsp;<i>by Will Payne&nbsp;</i>(forum feature request)</p><p><a href=\"https://forum.effectivealtruism.org/posts/eQgLREsGCo252HRmo/scout-mindset-poster\"><u>Scout Mindset Poster</u></a>&nbsp;<i>by Anthony Fleming&nbsp;</i>(printable poster)</p><p>&nbsp;</p><p>&nbsp;</p><h1>LW Forum</h1><h2>AI Related</h2><p><a href=\"https://www.lesswrong.com/posts/z3GwFzt4fnBdPz5hd/possible-miracles\"><u>Possible miracles</u></a></p><p><i>by Akash, Thomas Larsen</i></p><p>Eliezer\u2019s&nbsp;<a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\"><u>List of Lethalities</u></a> is a list of ways we could fail with regards to AGI. This post is a brainstorm of ways we might win - intended as an exercise for others to try too.</p><ol><li>New agendas might emerge - the field is growing rapidly in people, resources, respect, and existing work to build off. They could develop the idea we need.<br>&nbsp;</li><li>Alignment might be easy - easier to align methods than deep learning could emerge, or we could get slow takeoff, deception might not be selected for, tools we have like adversarial training might be enough, AGI might try to figure out our values (and succeed) etc.<br>&nbsp;</li><li>Timelines might be long - coordination and respect for x-risk might slow capabilities in favor of safety research, moore\u2019s law might kick in, or deep learning might not scale all the way to AGI.<br>&nbsp;</li><li>We might solve whole brain emulation soon, and upload alignment researchers to do 1000x faster research.</li></ol><p>The author suggests it could be helpful to backchain from these brainstorms to come up with new project ideas.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/SxQJWw8RtXJdngBtS/qapr-4-inductive-biases\"><u>QAPR 4: Inductive biases</u></a></p><p><i>by Quintin Pope</i></p><p>A roundup of 16 alignment papers focused on the inductive biases of stochastic gradient descent. Links, quotes, and the author\u2019s opinion are provided for each.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/krHDNc7cDvfEL8z9a/niceness-is-unnatural\"><u>Niceness is unnatural</u></a></p><p><i>by So8res</i></p><p>There\u2019s an argument that it might be easy to make AIs \u2018nice\u2019, because prosocial behavior is advantageous in multi-agent settings.</p><p>The author argues this is unlikely, because:<br>1. The role of niceness is selection pressures can be replaced with other strategies like \u2018merge with local potential allies immediately\u2019<br><br>2. Humans \u2018niceness\u2019 is detailed eg. We only do it sometimes, have limited patience, and differing sensitivity to various types of cheating. An AI might have a different set of details no longer recognizable as \u2018niceness\u2019.</p><p>3. Related skills like empathy might occur because our self-models are the same as our other-models (we\u2019re both human). This doesn\u2019t apply for AIs.</p><p>4. The AI might display nice behaviors while they\u2019re useful, and then reflect and drop them when they\u2019re not.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/LkBmAGJgZX2tbwGKg/help-out-redwood-research-s-interpretability-team-by-finding\"><u>Help out Redwood Research\u2019s interpretability team by finding heuristics implemented by GPT-2 small</u></a></p><p><i>by Haoxing Du, Buck</i></p><p>Some of Redwood\u2019s research involves interpretability on specific behaviors language models exhibit. They\u2019re considering scaling up that line of research, so asking commenters for more behaviors to investigate! They\u2019ve put up a&nbsp;<a href=\"https://modelbehavior.ngrok.io/\"><u>web app</u></a> for people to use GPT-2 in order to identify behaviors.</p><p>An example is acronyms - GPT-2 Small consistently follows the heuristic \u201cstring together the first letter of each capitalized word, and then close the parentheses\u201d when asked to generate acronyms.</p><p><br>&nbsp;</p><h2>Rationality Related</h2><p><a href=\"https://www.lesswrong.com/posts/8vesjeKybhRggaEpT/consider-your-appetite-for-disagreements\"><u>Consider your appetite for disagreements</u></a></p><p><i>by Adam Zerner</i></p><p>4 illustrated examples of people disagreeing on a point because of minor differences. For instance, arguing whether a poker hand should have been folded, when both people believe it was a marginal call either way. Or arguing if a basketball player is the third or fifth best, when you both agree they\u2019re top ten.</p><p>The author advocates that you have a small \u2018appetite\u2019 for these sorts of marginal disagreements, and spend little time on them. They also recommend communicating you have a large appetite for important and substantial disagreements.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/sNg4oGDrfka47Bu6Y/the-balto-togo-theory-of-scientific-development\"><u>The Balto/Togo theory of scientific development</u></a></p><p><i>by Elizabeth</i></p><p>In 1925, a relay of 150 dogs and 20 humans ran antibody serum to Alaska to end a diphtheria outbreak. The dog on the final and easiest leg was Balto, who became famous for it. The dog who ran the longest / hardest was Togo, who got comparatively little media.</p><p>A similar dynamic happens in science. Alfred Wegener is credited for discovering continental drift, but did no data collection, and little synthesis of evidence (the idea already existed in some papers). But people remember him and he inspired others to research further by advocating for an unproven idea. The author wonders how important this popularizing function is in general.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/dHchoTG6tRGEDHbtj/calibration-of-a-thousand-predictions\"><u>Calibration of a thousand predictions</u></a></p><p><i>by KatjaGrace</i></p><p>The author has made predictions in a spreadsheet for 4 years - as of now, ~1K are resolved. They created a calibration curve for ~630 predictions not about their own behavior, with 11 buckets, and found an average miscalibration error of only 3%. (These were primarily everyday life predictions such as if they'll be paid by x date, or invited to a certain party.) The accuracy was surprising because their internal experience of the predictions was \u2018pulling a number out of thin air\u2019. Accuracy for predicting their own behavior was much lower, particularly for the 35-55% range.</p><p><br><br><a href=\"https://www.lesswrong.com/posts/gJtbrjtPzsuRx8tNK/a-common-failure-for-foxes\"><u>A common failure for foxes</u></a></p><p><i>by Rob Bensinger</i></p><p>In the parable of the Hedgehog and the Fox, the fox knows many things while the hedgehog knows one thing well. The author argues that people who see themselves as foxes often focus too much on RCTs over informal arguments, even when the RCT isn\u2019t that relevant. This is because they want to feel like they \u2018know things\u2019 for sure, progress quickly in learning, and look intellectually modest (ie. \u2018I\u2019m just deferring to the data\u2019).</p><p><br>&nbsp;</p><h2>Other</h2><p><a href=\"https://www.lesswrong.com/posts/sJK6HN5vTPPnuuNgQ/that-one-apocalyptic-nuclear-famine-paper-is-bunk\"><u>That one apocalyptic nuclear famine paper is bunk</u></a> &amp;&nbsp;<a href=\"https://www.lesswrong.com/posts/jnDibtfvWNHLucf4D/actually-all-nuclear-famine-papers-are-bunk\"><u>Actually, All Nuclear Famine Papers are Bunk</u></a>&nbsp;<i>by Lao Mein</i></p><p>Some bloggers cite&nbsp;<a href=\"https://www.nature.com/articles/s43016-022-00573-0\"><u>a study from Nature Food</u></a> on why full US &lt;-&gt; Russia nuclear exchange might collapse civilization. The paper assumes a 10C drop in temperatures from nuclear winter will reduce farm yields 90%. However it also assumes no adaptation by humans - that we\u2019ll keep the same crop selection and crop locations. Lao argues this is not realistic and makes the conclusions irrelevant.</p><p>There have also been claims by people such as Peter Zaihan that the world only has ~2 months' worth of food in reserve. Similarly, the paper above states an assumption that all food stores will be used up in the first year of attack. By examining&nbsp;<a href=\"https://www.nass.usda.gov/Publications/Todays_Reports/reports/grst0322.pdf\"><u>US grain reserves</u></a>, Lao finds there is enough to last the US ~half a decade, even without considering other food sources.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/oRakD8uKduPYhcek7/transformative-vr-is-likely-coming-soon\"><u>Transformative VR Is Likely Coming Soon</u></a></p><p><i>by jimrandomh</i></p><p>The author estimates 2.5 years until VR is better than in person for most meetings, given that Oculus announced a new headset last week which tackles many of the issues with previous VR meetings (eg. not being able to see the real world, hidden facial expressions, and audio latency). They expect the shift to be sudden and impactful - particularly on organizational structures and remote work.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/4NPfBfdBmYZDEBgWb/towards-a-comprehensive-study-of-potential-psychological\"><u>Towards a comprehensive study of potential psychological causes of the ordinary range of variation of affective gender identity in males</u></a></p><p><i>by tailcalled</i></p><p>Someone who identifies as male might vary from being distressed by the idea of being a woman, to being neutral / wouldn\u2019t mind, to being positive about it. The author studies this variation via surveys of cis men who don\u2019t idenitfy as trans or gender questioning, and tries to correlate it to other factors such as gender conservativism or extraversion.</p><p>&nbsp;</p><h2>Didn\u2019t Summarize</h2><p><a href=\"https://www.lesswrong.com/posts/no5jDTut5Byjqb4j5/six-and-a-half-intuitions-for-kl-divergence\"><u>Six (and a half) intuitions for KL divergence</u></a>&nbsp;<i>by TheMcDouglas</i></p><p><a href=\"https://www.lesswrong.com/posts/XtDFSaftGm8hNBemG/prettified-ai-safety-game-cards\"><u>Prettified AI Safety Game Cards</u></a>&nbsp;<i>by abramdemski</i></p><p><a href=\"https://www.lesswrong.com/posts/Aet2mbnK7GDDfrEQu/contra-shard-theory-in-the-context-of-the-diamond-maximizer\"><u>Contra shard theory, in the context of the diamond maximizer problem</u></a>&nbsp;<i>by So8res</i></p><h1><br>This Week on Twitter</h1><h2>AI</h2><p>2022\u2019s&nbsp;<a href=\"https://www.stateof.ai/\"><u>State of AI report</u></a> is live. Key trends (aggregated in&nbsp;<a href=\"https://twitter.com/nathanbenaich/status/1579714667890757632?s=20&amp;t=gHLwk3OGkGSOKLVdfXJa8Q\"><u>this tweet</u></a>) include:</p><ul><li>Research collectives open-sourcing AI language, text-to-image, and protein models by large labs at incredible pace.</li><li>Language models being used to predict things in bio, such as high-risk Covid-19 variants (predicted as high-risk before WHO identified them).</li><li>LLMs being trained to use software tools like search engines and web apps.</li><li>Increasing focus on safety, with safety / alignment researchers at major AI labs up to ~300 (from ~100 last year).</li><li>[Heaps more details in report]<br>&nbsp;</li></ul><p>DeepMind released a paper about self-supervised training on video instead of image datasets (richer data).&nbsp;<a href=\"https://twitter.com/DeepMind/status/1580534029933506562?s=20&amp;t=gHLwk3OGkGSOKLVdfXJa8Q\"><u>(tweet)</u></a></p><p>&nbsp;</p><h2>EA</h2><p>The US Supreme Court heard arguments on whether or not to uphold Prop12 - a California law banning the sale of pork from pigs kept in spaces too small to turn around. Could have implications on the types of further laws that can be passed. Decision due in late June 2023.&nbsp;<a href=\"https://twitter.com/Lewis_Bollard/status/1579831566280249344?s=20&amp;t=gHLwk3OGkGSOKLVdfXJa8Q\"><u>(tweet)</u></a>&nbsp;<a href=\"https://www.reuters.com/markets/commodities/us-supreme-court-weighs-pork-industry-challenge-california-law-2022-10-11/\"><u>(article)</u></a>&nbsp;</p><p>&nbsp;</p><p>New paper addressing how natural risks might be higher because of civilization. Eg. Pandemics are risker because of travel, and space weather is riskier because it can affect technology such as power grids.&nbsp;<a href=\"https://twitter.com/SethBaum/status/1581037088866066432?s=20&amp;t=gHLwk3OGkGSOKLVdfXJa8Q\"><u>(tweet)</u></a>&nbsp;<a href=\"https://t.co/M0bgCviW06\"><u>(paper)</u></a><br>&nbsp;</p><h2>National Security</h2><p>New US&nbsp;<a href=\"https://www.whitehouse.gov/wp-content/uploads/2022/10/Biden-Harris-Administrations-National-Security-Strategy-10.2022.pdf\"><u>national security strategy</u></a> includes explicit commitments to strengthening the BWC (biological weapons convention) and the need for more focus on deliberate + accidental threat mitigation.&nbsp;<a href=\"https://twitter.com/SophieMRose_/status/1580465147621306369?s=20&amp;t=gHLwk3OGkGSOKLVdfXJa8Q\"><u>(tweet)</u></a></p><p>Putin said that more missile strikes against Ukraine \u2018not necessary\u2019 and that the aim isn\u2019t to destroy the country.<a href=\"https://twitter.com/AFP/status/1580913060767707136?s=20&amp;t=-Mj86lhqs_UN-BwV4Dfq6A\"><u> (tweet)</u></a></p><p>Iran sending drones and missiles to Russia. The drones are already being used by Russia against Ukraine.&nbsp;<a href=\"https://twitter.com/michaeldweiss/status/1581610868642439170?s=20&amp;t=-Mj86lhqs_UN-BwV4Dfq6A\"><u>(tweet)&nbsp;</u></a><a href=\"https://www.bbc.com/news/uk-63280523\"><u>(article)</u></a></p><p>&nbsp;</p><h2>Science</h2><p>New \u2018our world in data\u2019 section on which countries routinely administer vaccines.&nbsp;<a href=\"https://twitter.com/salonium/status/1580524311731109891?s=20&amp;t=-Mj86lhqs_UN-BwV4Dfq6A\"><u>(tweet)</u></a>&nbsp;<a href=\"https://t.co/QSFbzPxuxH\"><u>(page)</u></a><br>&nbsp;</p>", "user": {"username": "GreyArea"}}, {"_id": "7pcXdAp39LJkromLd", "title": "Predictors of success in hiring CEA\u2019s Full-Stack Engineer", "postedAt": "2022-10-17T22:09:02.975Z", "htmlBody": "<p>We examine what factors predicted advancement in our engineering hiring round. We show two trends which seem common in EA hiring<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreff77ub3hvubw\"><sup><a href=\"#fnf77ub3hvubw\">[1]</a></sup></span>: first, candidates with substantial experience (including at prestigious employers) were often unsuccessful, and second, candidates with limited experience and/or&nbsp;limited formal education were sometimes successful.</p><p>We sometimes hear of people being hesitant to apply to jobs out of a fear that they are hard to get. This post gives quantitative evidence that people can receive EA job offers even if their seemingly more qualified peers are rejected (and, indeed, traditional qualifications are almost uncorrelated with getting an offer).</p><h2>In summary:</h2><ul><li>None of the factors we looked at were statistically significant.</li><li>Having previously worked at a <a href=\"https://en.wikipedia.org/wiki/Big_Tech#FANG,_FAANG,_and_MAMAA\">Big Tech \u201cFAANG\u201d company</a>&nbsp;was the only factor which had a consistently positive central estimate, although with confidence intervals that comfortably included both positive and negative effect sizes.</li><li>Years of experience, typographical errors, and the level of university qualification seemed to have little predictive power.</li></ul><p>This builds on our <a href=\"https://forum.effectivealtruism.org/posts/Z2jTTR52fPLvaMgpX/does-participation-in-effective-altruism-predict-success\">previous post</a>&nbsp;which found that participation in EA had limited ability to predict success in our hiring round.</p><h1>Context</h1><p>There were 85 applicants for the role. The success rates for candidates in each stage are shown below. Some candidates voluntarily withdrew between the screening interview and trial task, hence there are fewer people taking part in the trial task than passed the interview.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><figure class=\"table\"><table><tbody><tr><td colspan=\"1\" rowspan=\"1\"><strong>Stage</strong></td><td colspan=\"1\" rowspan=\"1\"><strong>Number participating</strong></td><td colspan=\"1\" rowspan=\"1\"><strong>Number passing</strong></td><td colspan=\"1\" rowspan=\"1\"><strong>Success rate</strong></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Initial application sift</td><td colspan=\"1\" rowspan=\"1\">85</td><td colspan=\"1\" rowspan=\"1\">48</td><td colspan=\"1\" rowspan=\"1\">57%</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Screening interview</td><td colspan=\"1\" rowspan=\"1\">48</td><td colspan=\"1\" rowspan=\"1\">45</td><td colspan=\"1\" rowspan=\"1\">94%</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Trial task</td><td colspan=\"1\" rowspan=\"1\">35</td><td colspan=\"1\" rowspan=\"1\">8</td><td colspan=\"1\" rowspan=\"1\">23%</td></tr></tbody></table></figure><p>After the recruitment process was completed, we aggregated information about each applicant using the CVs and LinkedIn profiles they provided with their application. The metrics we were interested in were<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzp0kgd9qwo\"><sup><a href=\"#fnzp0kgd9qwo\">[2]</a></sup></span>:</p><ul><li>Did any previous role include the word \u201csenior\u201d in its title?</li><li>Did any previous role include the word \u201cmanager\u201d in its title?</li><li>How many years of experience did the candidate have?</li><li>How many typos were in the application?</li><li>What was the highest degree obtained by the candidate?</li><li>We coded this as: 1 for a bachelor-level degree, 2 for a master-level degree, and 3 for a doctoral degree.</li><li>Has the applicant ever worked at a FAANG company?</li></ul><p>This is not rigorous analysis; a \u201cproper\u201d model would include as many explanatory factors as possible, and the factors should be independent. This is reflected in the eventual predictive power of the models.</p><h1>Findings</h1><p>We fitted logistic regression models to the data; with the dependent variable being whether a candidate passed a given stage, and the independent variables being the factors listed above.</p><p>We then calculated modelled odds ratios and probabilities associated with each \"predictor\". The results of this are shown below, with a <a href=\"https://docs.google.com/document/d/12K8exv5_CbIiwUUYkQol3ugKZb32EjFsbaVLfx3szGs/edit#heading=h.ffa6rgqep9ak\">data table in&nbsp;the appendix</a>.</p><ul><li>Binary variables include examples like \u201cHas senior in title\u201d or \u201cHas FAANG company\u201d. The odds ratio indicates how much more likely it is that people who were successful were \u201cexposed\u201d to that variable than not.</li><li>Continuous variables include examples like \u201cYears of experience\u201d or \u201cHighest degree\u201d. The odds ratio indicates how much more likely it is that people who were successful were \u201cexposed\u201d to one unit increase in the variable than not.</li></ul><h2>Predictors for passing an initial sift</h2><p>This model predicts whether all submitted applicants (N=85) would pass an initial sift and be invited to the screening interview, with sensitivity 56% and specificity 76%.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994802/mirroredImages/7pcXdAp39LJkromLd/yiyirzw8cf5vyw8v9mtc.png\"></p><h2>Predictors for passing screening interview</h2><p>This model predicts whether all invited applicants (N=85) would pass the screening interview, with sensitivity 56% and specificity 70%.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994802/mirroredImages/7pcXdAp39LJkromLd/fltubpudfrleazxoebxf.png\"></p><h2>Predictors for passing trial task</h2><p>This model predicts whether applicants who did not withdraw prior to this point (N=83) would pass the trial task, with sensitivity 88% and specificity 58%.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994802/mirroredImages/7pcXdAp39LJkromLd/pegvan08calzcmpcizdi.png\"></p><h2>Commentary from Ben</h2><p>Discourse about EA hiring is sometimes simplified to \"EA jobs are hard to get\" (and therefore you shouldn't bother applying unless you are very qualified) or \"there is a big talent gap\" (and therefore everyone should apply).</p><p>This post gives evidence that \u201chard versus easy\u201d isn\u2019t really the right axis: it's hard to get a job (in the sense that well-qualified applicants were rejected) but also easy (in the sense that applicants with limited qualifications were accepted).&nbsp;</p><p>\"When in doubt, just apply\" continues to seem like good advice to me.</p><p>From the hiring manager\u2019s perspective: This builds on our <a href=\"https://forum.effectivealtruism.org/posts/Z2jTTR52fPLvaMgpX/does-participation-in-effective-altruism-predict-success\">previous post</a>&nbsp;which found that participation in EA had limited ability to predict success in our hiring rounds. Together, these posts make me pessimistic that simple automated screening criteria like \u201cyou need X years of experience\u201d will be useful.</p><h1>Appendix: Summary of modelled parameters</h1><figure class=\"table\"><table><tbody><tr><td colspan=\"1\" rowspan=\"1\"><strong>&nbsp;</strong></td><td colspan=\"2\" rowspan=\"1\"><p><strong>Passing initial sift</strong></p><p><strong>(N=85)</strong></p></td><td colspan=\"2\" rowspan=\"1\"><p><strong>Passing screening interview</strong></p><p><strong>(N=85)</strong></p></td><td colspan=\"2\" rowspan=\"1\"><p><strong>Passing trial task</strong></p><p><strong>(N=75)</strong></p></td></tr><tr><td colspan=\"1\" rowspan=\"1\"><strong>Predictors</strong></td><td colspan=\"1\" rowspan=\"1\"><strong>Odds Ratios</strong></td><td colspan=\"1\" rowspan=\"1\"><strong>p</strong></td><td colspan=\"1\" rowspan=\"1\"><strong>Odds Ratios</strong></td><td colspan=\"1\" rowspan=\"1\"><strong>p</strong></td><td colspan=\"1\" rowspan=\"1\"><strong>Odds Ratios</strong></td><td colspan=\"1\" rowspan=\"1\"><strong>p</strong></td></tr><tr><td colspan=\"1\" rowspan=\"1\">(Intercept)</td><td colspan=\"1\" rowspan=\"1\"><p>1.06</p><p>(0.43 \u2013 2.63)</p></td><td colspan=\"1\" rowspan=\"1\">0.893</td><td colspan=\"1\" rowspan=\"1\"><p>1.10</p><p>(0.45 \u2013 2.74)</p></td><td colspan=\"1\" rowspan=\"1\">0.829</td><td colspan=\"1\" rowspan=\"1\"><p>0.36</p><p>(0.08 \u2013 1.29)</p></td><td colspan=\"1\" rowspan=\"1\">0.135</td></tr><tr><td colspan=\"1\" rowspan=\"1\"><p>years of experience</p><p>mean=9.4</p></td><td colspan=\"1\" rowspan=\"1\"><p>1.01</p><p>(0.95 \u2013 1.08)</p></td><td colspan=\"1\" rowspan=\"1\">0.781</td><td colspan=\"1\" rowspan=\"1\"><p>1.01</p><p>(0.95 \u2013 1.08)</p></td><td colspan=\"1\" rowspan=\"1\">0.795</td><td colspan=\"1\" rowspan=\"1\"><p>0.90</p><p>(0.74 \u2013 1.03)</p></td><td colspan=\"1\" rowspan=\"1\">0.181</td></tr><tr><td colspan=\"1\" rowspan=\"1\"><p>has senior in title</p><p>n=17</p></td><td colspan=\"1\" rowspan=\"1\"><p>2.23</p><p>(0.70 \u2013 7.99)</p></td><td colspan=\"1\" rowspan=\"1\">0.189</td><td colspan=\"1\" rowspan=\"1\"><p>2.63</p><p>(0.82 \u2013 9.53)</p></td><td colspan=\"1\" rowspan=\"1\">0.116</td><td colspan=\"1\" rowspan=\"1\"><p>0.81</p><p>(0.04 \u2013 7.19)</p></td><td colspan=\"1\" rowspan=\"1\">0.860</td></tr><tr><td colspan=\"1\" rowspan=\"1\"><p>has manager in title</p><p>n=14</p></td><td colspan=\"1\" rowspan=\"1\"><p>0.50</p><p>(0.14 \u2013 1.67)</p></td><td colspan=\"1\" rowspan=\"1\">0.263</td><td colspan=\"1\" rowspan=\"1\"><p>0.44</p><p>(0.12 \u2013 1.48)</p></td><td colspan=\"1\" rowspan=\"1\">0.197</td><td colspan=\"1\" rowspan=\"1\"><p>0.90</p><p>(0.04 \u2013 7.09)</p></td><td colspan=\"1\" rowspan=\"1\">0.925</td></tr><tr><td colspan=\"1\" rowspan=\"1\"><p>number of typos</p><p>mean=0.9</p></td><td colspan=\"1\" rowspan=\"1\"><p>0.93</p><p>(0.66 \u2013 1.31)</p></td><td colspan=\"1\" rowspan=\"1\">0.648</td><td colspan=\"1\" rowspan=\"1\"><p>0.99</p><p>(0.70 \u2013 1.43)</p></td><td colspan=\"1\" rowspan=\"1\">0.968</td><td colspan=\"1\" rowspan=\"1\"><p>0.90</p><p>(0.41 \u2013 1.50)</p></td><td colspan=\"1\" rowspan=\"1\">0.738</td></tr><tr><td colspan=\"1\" rowspan=\"1\"><p>has faang company</p><p>n=7</p></td><td colspan=\"1\" rowspan=\"1\"><p>1.92</p><p>(0.37 \u2013 14.55)</p></td><td colspan=\"1\" rowspan=\"1\">0.464</td><td colspan=\"1\" rowspan=\"1\"><p>2.44</p><p>(0.46 \u2013 18.88)</p></td><td colspan=\"1\" rowspan=\"1\">0.325</td><td colspan=\"1\" rowspan=\"1\"><p>2.04</p><p>(0.09 \u2013 20.83)</p></td><td colspan=\"1\" rowspan=\"1\">0.575</td></tr><tr><td colspan=\"1\" rowspan=\"1\"><p>highest degree</p><p>mean=1.0</p></td><td colspan=\"1\" rowspan=\"1\"><p>1.10</p><p>(0.52 \u2013 2.36)</p></td><td colspan=\"1\" rowspan=\"1\">0.807</td><td colspan=\"1\" rowspan=\"1\"><p>0.85</p><p>(0.39 \u2013 1.80)</p></td><td colspan=\"1\" rowspan=\"1\">0.667</td><td colspan=\"1\" rowspan=\"1\"><p>0.82</p><p>(0.19 \u2013 2.85)</p></td><td colspan=\"1\" rowspan=\"1\">0.765</td></tr></tbody></table></figure><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnf77ub3hvubw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreff77ub3hvubw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;They seem common in the authors\u2019 experience; we would appreciate feedback &nbsp;in the comments from other hiring managers about their own experience.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzp0kgd9qwo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzp0kgd9qwo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We collected other factors but ultimately chose to exclude them from the analysis:<br><strong>- University rankings</strong> - we could not obtain these for enough candidates, which reduced the sample size considered and affected their accuracy.<br><strong>- Likely salaries in the candidate\u2019s previous position</strong> - we used online sources to estimate the typical salary for the candidate\u2019s most recent position and company, but again could not obtain this for enough candidates.<br>- <strong>Whether candidates had worked in a company with more than 1000 employees</strong> - we excluded this in favour of looking at whether candidates had worked at a FAANG company; it was not possible to include both since the variables are not independent.</p></div></li></ol>", "user": {"username": "Akara"}}, {"_id": "7KJZWLMepdZ4mHvxS", "title": "The Relative Importance of the Severity and Duration of Pain ", "postedAt": "2022-10-18T16:50:13.685Z", "htmlBody": "<h1>Summary</h1><ul><li>Pains vary in their severity and duration. When one pain is longer-lasting but less intense than a second pain, the most straightforward way to compare how much disutility they cause is to multiply how much longer by how much less severe the first pain is than the second pain. The present report investigates whether this mathematical approach (a) is sufficient for making cause prioritization decisions, (b) requires some amendments, or (c) is fundamentally flawed.&nbsp;</li><li>Many writers believe that the moral badness of pain scales non-linearly as its severity increases. Some even believe that unbearable pain is always more urgent to relieve than bearable pain, no matter how much briefer it is. It is possible to amend the mathematical approach to account for the former possibility. Accommodating the latter possibility requires ignoring duration when comparing pain above a certain severity threshold to pain below that threshold.&nbsp;</li><li>So long as unbearable pain does not receive <i>infinite</i> weight in cause prioritization, long-lasting pain can matter more than severe pain when (a) the duration difference is very large, (b) the long-lasting pain is sufficiently severe to render most moments more bad than good, or (c) the long-lasting pain is severe enough to chronically prevent pleasure.&nbsp;</li><li>Health economics is the only field to our knowledge that appears to have an explicit research program evaluating how individuals trade off the severity and duration of pain. Incidentally germane studies from other fields are difficult to find efficiently.&nbsp;<ul><li>Extant evidence suggests interactions between severity and duration on preferences.&nbsp;</li><li>Heterogeneity in how people weigh severity and duration is a potentially large validity threat to discovering an appropriate mathematical formulation from aggregate data</li></ul></li><li>All alternatives or refinements to the mathematical approach are based on intuitions that are reasonable but are not incorrigible. There are cognitive and behavioral factors that could bias people towards overvaluing or undervaluing either duration or severity.&nbsp;</li></ul><h1>Epistemic Status</h1><p>This report was the product of about six weeks of research, and as such may overlook relevant research or philosophical considerations that we did not find or have time to consider. Later on, we spent about a week's worth of work revising the report in light of new comments but did not add any new sections.&nbsp;</p><h1>A mathematical calculus for pain?</h1><p>&nbsp;How should effective altruists decide whether to prioritize interventions that alleviate severe but relatively brief suffering or instead those that alleviate longer-lasting but less severe suffering? Different authors appear to weigh the severity and duration in diametrically opposed ways. In their report on the welfare of broiler chickens in industrial farms, <a href=\"https://perma.cc/59GE-GPHM\">Schuck-Paim et al.</a> (2022) write that \"preventing or alleviating the most extreme forms of pain and suffering is an ethical priority, of greater urgency than alleviating less severe states of suffering, regardless of how long they last\" (Chapter 8, p. 39). Thus, they endorse focusing on eliminating practices that induce extreme pain, such as ineffective stunning before slaughter, over mitigating chronic issues that are less severe, such as lameness. For others, the duration of pain is more important than its severity. <a href=\"https://doi.org/10.1093/acprof:osobl/9780199551163.003.0005\">Norwood and Lusk's (2011)</a> review of welfare issues for farmed animals downplays slaughter: \"In this chapter, we tended to focus primarily on the everyday life of farm animals. Animal advocacy groups will often mention a myriad other issues such as the transportation and slaughter of livestock. These issues, while important, are temporary experiences for the animals\" (chapter 5, p. 69). <a href=\"https://perma.cc/UVM8-CLU6\">Blanton</a> <u>(2021)</u> defends raising beef cattle on the grounds that most cattle enjoy grazing the pasture. While not dismissing welfare issues in feedlots, he argues \"the time spent in pens, feedlots, and rendering plants\u2026accounts for a small part of cattle\u2019s lives and in many cases practically none at all.\"&nbsp;</p><p>One could attempt to circumvent the debate by using a mathematical approach to combine severity and duration into a single index. Jeremy Bentham, the founder of modern utilitarianism, suggested that we can multiply units of intensity of pleasure or pain by their duration to determine their utility (<a href=\"https://www.jstor.org/stable/pdf/2141580.pdf\">Mitchell, 1918</a>, p 165). For example, if one pain is eight times more severe than another and lasts twice as long, then the first pain is 16 times as bad. <a href=\"https://perma.cc/SS7R-6YSD\">Hungerford (2018)</a> illustrates how to use the mathematical approach to compare the badness of a severe pain and a long-lasting pain: \"Say, for example, that shackle to slaughter, a chicken\u2019s death takes 30 seconds. The rest of her life is 135,360 times that length. That means, according to this argument, that for slaughter to be a worthy focus, a chicken\u2019s death must be 135,360 times worse than her life on the farm.\"&nbsp;</p><h2>Pain definitions</h2><p>Understanding how to compare severity and duration is relevant to the aggregation of any valanced state. However, the present investigation focuses primarily on pain. There is no guarantee that the relationship between severity and duration for pain generalizes to other negative affective states. For simplicity, however, the words \"pain\" and \"suffering\" will be used interchangeably in this report, and conclusions regarding specific types of physical pain will be tentatively assumed to apply to other types of physical pain and other sources of negative valence.</p><p>Pain is defined by the <a href=\"https://perma.cc/QB3N-X963\">International Association for the Study of Pain</a> as, \u201can unpleasant sensory and emotional experience associated with, or resembling that associated with, actual or potential tissue damage.\" Acute pains are generally caused by the stimulation of specialized sensory receptors in the peripheral nervous system known as nociceptors. Nociceptors can respond to high temperatures, low temperatures, pressure, and to chemicals including those released during tissue damage. Each of these different ways of causing pain has unique sensory qualities.&nbsp;</p><p>Ratings of pain intensity do not perfectly track pain\u2019s unpleasantness (<a href=\"https://pubmed.ncbi.nlm.nih.gov/14993415/\">Price, 2002</a>). Researchers have shown that stimulus intensity can be correlated both with ratings of intensity and unpleasantness, but the two have different power functions. In particular, the unpleasantness of pain increases more slowly at lower stimulus intensity levels, but increases at a greater rate at higher levels of stimulus intensity. For purposes of comparing the moral impact of severity and duration, it is the unpleasantness of pain that matters. The term \"severity\" will always be used in the present report to refer to unpleasantness rather than stimulus intensity.</p><p>When it comes to chronic pain, definitions become a little more difficult. Chronic pain is often defined in medical practice as <a href=\"https://perma.cc/9MT5-Q6HB\">pain that lasts for more than three months</a>. Clearly, this is a heuristic designed to capture general differences between short-term pains and long-term pains; there is no magic transformation that occurs to pains at the 3-month mark. The distinction that the definition is supposed to convey is that while \u201cnormal\u201d pains are adaptive and provide a protective function for organisms, chronic pains persist even after they are no longer serving a protective function. This can be due to the peripheral nervous system continuing to send pain signals even after tissue damage has healed or due to connections between brain regions that continue to signal pain.&nbsp;</p><p>The neurophysiology of chronic pain turns out to be different from that of acute pains. For example, <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3754458/\">Hashmi&nbsp;et al. (2013</a>) write that as pain transitions to chronic pain it tends to involve more of the emotional circuits in the brain. And so-called neural signatures of pain designed to assess acute pains do not correctly identify chronic pains (<a href=\"https://doi.org/10.1038/nrneurol.2017.122\">Davis et al., 2017, pgs. 629-630</a>). The fact that there may be differences in how chronic pains and acute pains are expressed in brains could have implications for their impacts on the welfare of individuals. But so long as the unpleasantness of chronic pains and acute pains can be measured along the same scale, it should be possible to compare them.&nbsp;</p><p>Finally, the current report treats long-lasting pains generically, despite the fact that different sources of long-lasting pain have different etiologies. While the term \"chronic pain\" is typically used to refer only to the abovementioned internal changes, ongoing inflammation is also a source of long-lasting pain, as is an environment that contains painful stimuli that are impossible to avoid (<a href=\"http://dx.doi.org/10.1016/j.jpain.2012.05.008\">Bennett, 2012</a>). Accordingly, this report uses the terms \"chronic\" and \"long-lasting\" interchangeably.</p><h1>Reasons to Prioritize Severity</h1><p>Questions about Bentham's approach for aggregating severity and duration&nbsp;have been raised as far back as his intellectual successor John Stuart Mill. Though Mill\u2019s discussion was focused on pleasure rather than pain, he responded to contemporary objections to utilitarianism by introducing the idea that there might be qualitative differences between different types of pleasures which preclude the possibility of merely \u201csumming up\u201d lesser pleasures to the point where they outweigh qualitatively superior pleasures (<a href=\"https://perma.cc/L7T6-8DGU\">Mill, 1861/2006</a>). In particular, Mill was responding to the claim that utilitarianism should be regarded as \u201cthe doctrine of swine\u201d because pigs could experience as much pleasure as humans. A recent formulation of the same objection can be found in Roger Crisp\u2019s (<a href=\"https://doi.org/10.1111/j.1933-1592.2006.tb00551.x\">2006</a>, pgs. 630-631) thought experiment <i>Haydn and the Oyster</i>:&nbsp;</p><blockquote><p>You are a soul in heaven waiting to be allocated a life on Earth. It is late Friday afternoon, and you watch anxiously as the supply of available lives dwindles. When your turn comes, the angel in charge offers you a choice between two lives, that of the composer Joseph Haydn and that of an oyster. Besides composing some wonderful music and influencing the evolution of the symphony, Haydn will meet with success and honour in his own lifetime, be cheerful and popular, travel and gain much enjoyment from field sports. The oyster's life is far less exciting. Though this is a rather sophisticated oyster, its life will consist only of mild sensual pleasure, rather like that experienced by humans when floating very drunk in a warm bath. When you request the life of Haydn, the angel sighs, 'I'll never get rid of this oyster life. It's been hanging around for ages. Look, I'll offer you a special deal. Haydn will die at the age of seventy-seven. But I'll make the oyster live as long as you like'.&nbsp;&nbsp;</p></blockquote><p>The upshot of this thought experiment is supposed to be as follows: the utilitarian should endorse the life of the oyster, since the duration of pleasure would eventually ensure that the oyster\u2019s pleasure outweighs that of Haydn\u2019s. However, the assumption is that most people would reject this tradeoff. Therefore, there must be something wrong with the standard hedonistic utilitarian calculus.</p><p>Mill predated this precise formulation by several centuries, but in response to this <i>type</i> of objection, he argued that there are in fact qualitative differences between pleasures, such that if certain conditions are met, \u201cwe are justified in ascribing to the preferred enjoyment a superiority in quality, so far outweighing quantity as to render it, in comparison, of small account\u201d (Mill, 1861/2006, Chapter 2). Mill is proposing a radical discontinuity (<a href=\"https://oxford.universitypressscholarship.com/view/10.1093/0198248431.001.0001/acprof-9780198248439\">Griffin, 1986</a>, p. 85) between pleasures such that enough of pleasure A can outweigh <i>any</i> amount of pleasure B.&nbsp;</p><p>Several scholars have proposed something similar when it comes to instances of extreme suffering. A central idea of suffering-focused ethics is that no amount of pleasure can compensate for suffering. However, many of these same scholars also endorse the claim that no amount of minor suffering can be equivalent to instances of extreme suffering. <a href=\"https://perma.cc/EQ2W-NUYD\">Vinding (2020)</a>, for example, writes the following: \u201cI claim there is a <i>lexical&nbsp;</i>difference between the moral disvalue of these two: a single instance of such extreme suffering is worse and more important to prevent than <i>arbitrarily many&nbsp; </i>instances of such mild suffering\u201d (p. 88). And <a href=\"https://perma.cc/WVS2-NN48\">Leighton (2011)</a> writes, \u201cOne person suffering intensely, such as at the hands of a torturer, is qualitatively different and ... incomparably worse than a million people suffering from a mild hangover\u201d (p. 85).&nbsp;</p><p>Giving infinite weight to severe harms would require going beyond a simple mathematical approach to computing moral badness. Also, a practical danger of giving infinite weight to very severe harms is that it would reduce diversification in cause prioritization. Although one could justify diversification among the plausible candidates for the most severe pain when it is not clear which is the most severe, the much more numerous causes of clearly less intense pain will go entirely neglected.&nbsp;</p><p>A radical discontinuity, however, is only one possible way to accommodate intuitions about the relationship between extreme and moderate pains. For instance, if the most severe pains receive finite weights, then at worst they will receive too little attention but will not be neglected entirely. Moreover, it could be that, beyond a certain stimulus intensity, pain severity scales so quickly that in practice severe pains are worse than even extremely long-lasting mild pains. This idea was captured in a quote from <a href=\"https://perma.cc/DR6Z-ZW7X\">Jamie Mayerfeld (1999, pgs. 134-5)</a> who argues that:&nbsp;</p><blockquote><p>as suffering increases in intensity, the urgency of preventing it rises at a faster rate than its intensity. If I think about suffering of a serious, fairly intense kind, and then think about another kind of suffering that is <i>four times more intense</i>, I think that it would be better to experience far more than four hours of the less intense suffering than to experience just one hour of the more intense suffering. The less intense suffering is preferable to the more intense suffering even though the total quantity involved (intensity multiplied by duration) is greater.&nbsp;</p></blockquote><h1>Reasons to Prioritize Duration</h1><p>One way to justify prioritizing long-lasting pain over severe pain would be if mild pain gradually gets much worse because it goes unaddressed, such as when hunger becomes starvation. A second way is if the awareness that chronic suffering will remain unceasing makes it seem unbearable, thereby increasing its severity (Vinding, 2020, p. 72). A third possibility is if the difference in duration was larger than the difference in severity.&nbsp;</p><p>The present report does not elaborate on these cases further because they do not require any refinement of estimating moral badness by multiplying severity and duration. Instead, this section focuses on two ways in which long-lasting pain of sufficient severity could chronically impede positive states. First, pain can be severe enough to make any given moment of that pain more bad than good. If that pain is long-lasting, then it becomes possible that life as a whole will be hedonically net-negative. In this case, prioritizing long-lasting pain could make sense because severe but brief pain does not necessarily negate the totality of an individual's net-positive experiences.&nbsp;</p><p>Second, even if long-lasting pain is not severe enough to render moments net-negative, they could be severe enough to erode the capacity for positive experience. Insofar as intense pain evolved to prevent individuals from engaging in unnecessary activities that would exacerbate an injury, positive experiences are typically the first to be deferred. Indeed, <a href=\"https://doi.org/10.1186/s13104-021-05636-2\">Alonso and Schuck-Paim (2021)</a> define <i>disabling</i> pain partly in terms of its effect on pleasure: \"Most forms of functioning or enjoyment are prevented as the direct result of pain. Symptoms are continuously distressing. Individuals affected often substantially reduce activity levels and refrain from moving\" (p. 2). On simple hedonistic views, net-positive moments are the only experiences that make existence morally better than non-existence, since non-existence is entirely devoid of suffering; they likely also play a large role in making life worth living on many other theories of value (though only some and not all suffering-focused views). Thus, net-positive experiences are necessary to counterbalance the badness of net-negative experiences. This point has been made frequently in the context of the problems faced by individuals with <i>brief</i> lives. For instance, <a href=\"https://link.springer.com/article/10.1007/BF00852469\">Ng (1995</a>, pgs. 270-271) suggests that wild animals that die before getting to enjoy mating probably do not have lives worth living. But an analogous claim applies to long lives: Long-lasting pain can limit how many pleasures individuals can pursue, making it possible that net-negative experiences will predominate. For example, broiler breeders are kept consistently very hungry, causing them to postpone positive activities (<a href=\"https://doi.org/10.1017/S1751731111000218\">Nielsen et al., 2011</a>). In humans, chronic fatigue syndrome can make carrying on a social life and performing previously enjoyable activites impossible (<a href=\"https://dx.doi.org/10.3390%2Fhealthcare8040413\">Boulazreg &amp; Rokach, 2020</a>), potentially explaining part of the condition's association with death by suicide (<a href=\"https://doi.org/10.1016/S0140-6736(15)01223-4\">Roberts et al., 2016</a>). In contrast, very mild chronic pain, such as from a slightly stiff neck, probably does not ruin any pleasurable activities, and thus would not have moral weight over and beyond the sum of its parts.&nbsp;</p><p>Most humans' behavior seems more consistent with trying to balance risk and reward rather than avoiding extreme suffering. <a href=\"https://doi.org/10.1093/mind/fzn079\">Huemer (2008, pgs. 909-910)</a> argues that small risks of horrible outcomes are tolerable because forgone gains accumulate over time when safety is adopted as a blanket policy. He makes this point about avoiding death, but the same reasoning applies to extreme pain:&nbsp;</p><blockquote><p>Consider the common intuition that a single death is worse than any number of mild headaches. If this view is correct, it seems that a single death must also be worse than any amount of inconvenience\u2026this suggests that we should greatly lower the national speed limit, since doing so would save some number of lives, with (only) a great cost in convenience. Yet few support drastically lowering the speed limit. Indeed, one could imagine a great many changes in our society that would save at least one life at some cost in convenience, entertainment, or other similarly \u2018minor\u2019 values. The result of implementing all of these changes would be a society that few if any would want to live in, in which nearly all of life\u2019s pleasures had been drained.&nbsp;</p></blockquote><p>Even chronic pain that does not quite spoil individual moments can nevertheless have detrimental effects on how much pleasure individuals will experience. Although a sub-optimal experience at any one moment is no tragedy, a markedly reduced ability to experience pleasure can become unacceptable when accumulated over time.&nbsp;</p><p>&nbsp;The upshot of long-lasting pain taking on greater significance when it rises above a certain severity threshold is that comparing the badness of two pains by multiplying their severity and duration will be insufficient. Instead, we must represent the fact that duration has less impact on moral badness when severity is low than it does when severity is high.</p><h1>Extant Empirical Evidence</h1><p>So far the cases for prioritizing severe pain over long-lasting pain and vice versa have depended more on philosophical intuition than empirical evidence. However, one might worry that philosophers' intuitions are unreliable guides to value relative to the unarticulated collective wisdom that ordinary people have gained through experience. Accordingly, a complementary approach to learning how duration and severity interact to produce moral badness involves conducting large empirical studies of laypeople's preferences. Similarly, we could use behavioral experiments to try to elicit the revealed preferences of nonhuman animals.</p><h2>Human psychology and animal welfare science</h2><p>Ideal direct evidence includes experiments in which subjects endure either a longer, mild pain or a shorter, intense pain. After four weeks of searching for direct evidence from the animal welfare science and human psychology literature, it seems likely that such evidence either does not exist or is difficult to find. Neither of these possibilities would be entirely surprising, given that most behavioral scientists are not occupied with the cause prioritization decisions under consideration here.&nbsp;</p><p>One animal welfare science question that <i>is</i> sometimes framed as a trade-off between severity and duration is how quickly to stun farmed animals using carbon dioxide. For instance, Shuck-Paim et al. (2022) writes, \"The greatest challenge with the use of CO2 stunning is finding a good balance between the time to the onset of unconsciousness and the amount of distress for [broiler chickens]; while the faster provisioning of high concentrations of CO2 will lead to loss of consciousness more swiftly, it can be highly aversive\" (Chapter 8, p. 22; see an identical framing in <a href=\"https://doi.org/10.3382/ps.2012-02551\">Gerritzen et al., 2013</a>, p. 42). The consensus among animal advocates appears to be that slowly increasing the concentration of C02 prevents all severe pain with only a small increase in time to unconsciousness (e.g., <a href=\"https://perma.cc/8CXH-NFDF\">Eyes on Animals, 2019</a>, pgs. 13-14). However, this conclusion was not based on observing which type of stunning chickens would rather endure.</p><h2>Health economics</h2><p>Health economists directly compare the badness of alternative health outcomes. For instance, the so-called disability weights used to compute disability-adjusted life years are calculated by having people judge whether someone who has a given health state&nbsp;is more or less healthy than somebody else who has a second health state. Health states that are more often ranked as causing a greater loss of health receive larger weights. However, by design, the duration of both problems is assumed to be the same. Therefore, even though different health states have different typical durations in everyday life, the disability weights represent the relative severity of the health states only. Similarly, quality-adjusted life years, often elicited by having people make trade-offs between longer periods in a given health state and shorter periods free of that health state, implicitly regard severity and duration as of equal importance.&nbsp;</p><p>The most direct evidence that severity and duration interact in producing moral badness comes from studies finding that there is a maximum duration that people are willing to endure severe health states, beyond which they would rather be dead but before which they would rather remain alive (<a href=\"https://www.jstor.org/stable/40221514\">Stalmeier et al., 2007</a>; <a href=\"https://doi.org/10.1007/s10198-014-0634-0\">Scalone et al. (2015)</a>. These results could be taken to show that respondents interpret severe health states as an unceasing stream of net-negative moments, which is worse than just dying quickly and painlessly. Yet, respondents who indicate a maximum duration of endurable suffering usually prefer at least a short certain duration of suffering over immediate death. Plus, <a href=\"https://doi.org/10.1016/j.jval.2018.02.004\">Craig et al. (2018)</a> found that respondents regarded moderately severe states as better than being dead, even though they have decreasing value beyond a year.&nbsp;</p><p>An alternative way to interpret maximum duration of endurability blurs the distinction between severity and duration\u2014viz., it could be that severe illness begins to feel unbearable simply because it is unceasing. For example, one respondent in <a href=\"https://www.york.ac.uk/media/che/documents/papers/discussionpapers/CHE%20Discussion%20Paper%20143.pdf\">Gudex and Dolan (1995)</a> said of more severe states, \"you would not mind them for a month but 10 years wouldn't be bearable for you or your family\" (p. 6). The accuracy of this assumption is debatable. To wit, <a href=\"https://doi.org/10.1191%2F0269215503cr602oa\">Myers et al. (2003)</a> found that those living a given health state longer did not grade it as worse than patients with a more recent onset, though this may be because they did not have a large sample of patients with highly severe health states.&nbsp;</p><p>The main ambiguity in this literature is preference heterogeneity: Even if researchers discover a functional form that performs well in predicting how severity and duration interact to produce preferences at the aggregate level, there is no guarantee it will at the individual level (Craig et al., 2018). <a href=\"https://doi.org/10.1002/hec.4457\">Jonker and Norman (2022)</a> found that the majority of respondents in a discrete choice experiment made choices as if they add duration and severity together to estimate aversiveness, whereas only a minority act as if they multiply them. Assuming this finding is not an artifact of the study design, it could imply that no single correct mathematical model of how to combine severity and duration into a moral badness judgment can be readily inferred from the general population.</p><h2>Methodological Challenges</h2><p>It was much easier to find research that speaks to the barriers of empirically determining whether severe pain is more or less urgent than long-lasting pain. Indeed, it seems possible that these barriers help explain why there is so little germane research. In this section, we describe these barriers and suggest potential solutions.</p><h3><strong>Uninformed preferences and distorted preferences</strong><i>&nbsp;</i></h3><p>There was one topic within health economics that did not speak to the relative importance of severe pain and long-lasting pain, but did raise questions about whose judgments matter most: The discrepancies between general populations and patient populations in their valuations of health states. The general population tends to regard health states that severely impair quality of life as worse\u2013sometimes worse than being dead!\u2013 than do the patients who experience those health states (<a href=\"https://doi.org/10.1007/s11136-018-1848-x\">Bernfort et al., 2018</a>; <a href=\"https://doi.org/10.1007/s11136-021-03052-4\">Schneider, 2021</a>). The difference in valuation gets larger as health states get more severe (<a href=\"https://www.jstor.org/stable/4640866\">McPherson et al., 2004</a>). This might suggest that intuitions that people set the bar for what is unbearable too low, consistent with the hypothesized role of affective forecasting errors in health valuation studies (<a href=\"https://doi.org/10.1111/jasp.12631\">Walasek et al., 2019</a>). If so, then even if one grants that unbearable pain does deserve special attention, one would want to exercise humility when judging that a certain experience causes unbearable suffering. An alternative (but not mutually exclusive) explanation is that the general population overestimates how much health states erode the capacity for positive experience. In particular, the general population may not realize the extent to which people with even severe physical limitations can find new activities to replace ones they can no longer do, hone their skills in domains where they can still perform, and adopt new but equally rewarding goals that they can still achieve (<a href=\"https://doi.org/10.1007/s11136-020-02426-4\">Helgesson et al., 2020</a>, p. 1471). If so, then there is a risk of prioritizing the relief of chronic pains on a faulty assumption that they have squelched all opportunities for positive experiences. It is an open question whether uninformed preferences on net bias cause prioritization towards severe or long-lasting pain.&nbsp;</p><p>If personal experience with a pain is requisite to accurately evaluating it, researchers can prioritize studying people who have actually experienced health states that vary in terms of their severity and duration, although recruiting such individuals will likely prove expensive and inefficient. For non-human animals the issue is less tractable because some of their pains that are of the greatest concern result in death (e.g., live-shackle slaughter). It is worth noting, however, that whether to trust general or patient populations is a source of controversy. Some researchers worry that patients in severe health states are in denial about how poorly their lives are going or no longer recall what a fully healthy life was like&nbsp; (Helgesson et al., 2020, pgs. 1474-1478). It might be that there is no privileged source of information on severity, and that it can only be inferred indirectly by triangulating the views of different groups with biases that together cancel each other out.</p><h3><strong>Heat of the moment</strong><i>&nbsp;</i></h3><p>The most straightforward way to test whether individuals are more averse to severe pain or long-lasting pain would be to force them to choose between the two. However, interpreting avoidance behavior as revealing the worst choice in terms of overall welfare impact is complicated by the fact that pain severity may have evolved to signal how much priority should be devoted to current welfare threats <i>right now&nbsp;</i>(<a href=\"https://royalsocietypublishing.org/doi/full/10.1098/rstb.2019.0275\">Walter &amp; Williams, 2019</a>). Although organisms possess mechanisms such as hypersensitivity to orient them to long-term threats that were recurrent in their ancestral environments, the novel long-term threats that modern humans and farmed animals face are not likely to modulate pain.&nbsp;</p><p>If the severity of short-term pain plays a larger role in valuation than the totality of consequences of each outcome, then preferences could be biased towards avoiding severe pain even if the long-lasting pain produces worse consequences, all things considered. For example, <a href=\"https://perma.cc/NNS6-T67C\">Tomasik (2013)</a> introspects on his own preferences to prioritize the severity of pain: \u201cI think small amounts of very bad suffering are far more serious than lots of mild suffering: We're willing to trade mild suffering for mild pleasure even when enduring the mild suffering, but if the suffering becomes intense enough, we might not accept it in return for any amount of pleasure, <i>at least not in the heat of the moment</i>\u201d (emphasis added). But what an individual wants in the heat of the moment may be determined not by the long-term consequences for welfare, but instead how aversive an outcome will be at<i> any one moment</i>. Put another way, individuals may have limited <i>volition</i> to choose outcomes that result in severe pain. For instance, only about 30 percent of those who have ever seriously considered ending their own lives ever attempt to do so (<a href=\"https://www.uptodate.com/contents/suicidal-ideation-and-behavior-in-adults/print\">Schreiber et al. 2021</a>). One plausible reason is that the pain tolerance necessary to be <i>capable</i> of suicidal behavior is distinct from the <i>desire </i>to end life (<a href=\"https://doi.org/10.1016/j.copsyc.2017.07.007\">May &amp; Victor, 2018</a>). <a href=\"https://www.hup.harvard.edu/catalog.php?isbn=9780674061989\">Joiner (2010, p. 6, emphases added)</a> writes:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><blockquote><p>Self-preservation is a powerful enough instinct that <i>few overcome it by force of will</i>. The few who can have developed <i>a fearlessness of pain</i> <i>and death</i>\u2026People get used to such things by having repeatedly experienced them, often through previous self-injury, but other painful experiences serve too\u2026time and time again [there are] people who report that they genuinely desired to die by suicide, but that their bodies would not allow it.</p></blockquote><p>For those who can more easily endure acute suffering, there is a stronger association between suicidal ideation and suicide attempt (<a href=\"https://doi.org/10.1016/j.jad.2016.02.044\">Anestis &amp; Capron, 2016</a>). And the more intense suicidal ideation is at its worst point, the more likely a suicide attempt will occur (<a href=\"https://doi.org/10.1016/j.psychres.2018.08.094\">Law et al., 2016</a>). That is, short-term intense pain of a suicide attempt becomes less of a deterrent to enacting a preference to die the worse the chronic suffering of living is.&nbsp;</p><p>One way to mitigate (but probably not eliminate) the influence of volition on preferences would be to ask individuals whether they would rather endure severe or long-lasting pain without having to initiate and maintain that option themselves. For instance, a conditional place preference test (e.g., <a href=\"https://doi.org/10.1038/s41598-020-60260-7\">Adcock &amp; Tucker, 2020</a>) would enable subjects to express their preference, at which point the experimenters could administer the pain.&nbsp;</p><h3><strong>Duration Neglect</strong><i>&nbsp;</i></h3><p>An alternative way to avoid reliance on volition, at least for studies of humans, would be to rely on retrospective evaluations of experiences that vary in duration and severity. However, research on duration neglect suggests that severity is more salient when people recall past experiences. Most famous in demonstrating this are experiments involving the peak-end effect. In these experiments, subjects are asked to undergo painful procedures such as a colonoscopy or holding one\u2019s hand in cold water. In one condition, the painful procedure would be cut off abruptly but relatively early. In the other condition, the painful procedure would continue past the point where it was cut off in the other condition, gradually decreasing but still rated as painful by subjects.&nbsp; Surprisingly, however, when asked to evaluate their experiences retrospectively, subjects preferred the condition where they had <i>more pain</i>, that is pain that continued beyond the cutoff.&nbsp; They preferred the condition with a greater area under the pain curve.&nbsp;</p><p>A broad lesson of this research program was that the severity of pain at its peak and at the end of the experience tend to be more salient to subjects than the duration of the pain. One way to interpret this \"duration neglect\" is to regard the near-exclusive focus on severity as a bias preventing preferences from mapping onto well-being, which should boil down to severity times duration. Daniel Kahneman, who was involved in the initial experiments demonstrating this effect, later argued that we should distinguish \u201cdecision utility\u201d (the factors that people value when making decisions) from \u201cexperienced utility\u201d (the balance of pleasure and pain) and suggested that the latter was more important (<a href=\"https://doi.org/10.1162/003355397555235\">Kahneman et al., 1997</a>).&nbsp;</p><h3><strong>Imagination Failures</strong></h3><p>Many people imagine severe pain as feeling <i>unbearable</i>, whereas long-lasting pain does not seem to make the same impression. Although this could be evidence that duration matters less than severity, it could also be that humans have a harder time imagining very long durations because, unlike severe pains, it is impossible to mentally simulate long-lasting pain in a short period of time (<a href=\"https://doi.org/10.1016/j.obhdp.2008.07.001\">Liersch &amp; McKenzie, 2009</a>, p. 304). This would presumably not be a major issue if people could cognitively appreciate just how much total pain would result from iterating a shorter duration of pain a large number of times. However, humans suffer from scope neglect in many domains, whereby very large numbers are not processed much differently from merely large numbers. Huemer (2008, p. 915) argues that scope neglect is responsible for the devaluation of long durations of pleasure:&nbsp;</p><blockquote><p>Stuart Rachels and Larry Temkin hold that some intense pleasures are \u2018lexically better than\u2019 any mild pleasure, meaning that no amount of mild pleasure is as good as a given, short duration of the intense pleasure\u2026people\u2019s intuitions seem to shift when the length of the ecstatic experience is shortened: a mere second of ecstasy seems inferior to a thousand years of mild pleasure. This can be explained by a particular error theory: we have difficulty grasping very long time periods. The duration of a mild pleasure that is really superior to fifty years of ecstasy is too long for us to adequately grasp; hence, we fail to appreciate its superiority. To alleviate this problem, we may replace the fifty years of ecstasy with a very short (but still clearly graspable) period of ecstasy\u2014say, one second\u2014and then ask whether we can imagine a superior experience consisting of protracted mild pleasure. When we thus change the example to improve the reliability of our intuition, the ecstatic experience no longer seems categorically better.</p></blockquote><p>Scope neglect could explain away the special status that some writers accord to unbearable pain. <a href=\"https://perma.cc/4PK4-FRAX\">Tomasik (2019)</a> acknowledges that it might explain his intuition that a day in Hell would be worse than any amount of time in Heaven. A standard strategy to increase scope sensitivity is to increase the vividness of the stimuli that accompanies the larger option (<a href=\"https://doi.org/10.1016/j.jarmac.2014.09.002\">Dickert et al., 2015</a>). In the present context, this could involve specifying the myriad life events that long-lasting pain would impact, its knock-on effects that worsen the severity of pain, and so on.&nbsp;</p><h3><strong>Ethical constraints</strong></h3><p>Administering pain in research settings that is severe enough to match those experienced by individuals in real life will understandably face roadblocks from ethical review boards. But using less intense pain makes the generalizability of subjects' preference for severe pain over long-lasting pain questionable, especially if individuals increase the weight attached to severity as it increases. In contrast, if individuals apply a linear weighting to how severity relates to moral badness, then findings from comparisons among less severe pains should generalize to comparisons among more severe pains so long as the ratio of the difference in severity remains the same.&nbsp;</p><h3><strong>Severity: Cardinal or ordinal?</strong></h3><p><i>&nbsp;</i>A final issue is whether the mathematical approach is doomed from the start because it assumes that severity is cardinal and that its units can be known. One reason to be optimistic about these assumptions is that researchers have ostensibly developed ratio scales of pain severity for at least low-to-moderate levels of stimulus intensity (e.g., <a href=\"https://doi.org/10.1016/0304-3959(83)90126-4\">Price et al., 1983</a>), although we did not vet these efforts carefully. On the other hand, a general framework for assigning any given pain to a meaningful unit seems like a far-off goal. <a href=\"https://perma.cc/BXL3-ZNLB\">Browning (2022)</a> laments the fact that most existing farmed animal welfare scoring systems are on an ordinal scale (pgs. 10-11), and that ostensibly cardinal systems do not obviously meet evidential standards for ratio measurement (p. 14).&nbsp;</p><p>It may be that the entire range of pain severity can be put on a ratio scale, and all it will take to do so is a dedicated research effort. An alternative possibility, however, is that pain is itself an ordinal trait. Bentham worried that it was not possible to quantitatively compare qualitatively different pleasures and pains (Mitchell, 1912, pgs. 168-172). <a href=\"https://global.oup.com/academic/product/an-essay-on-philosophical-method-9780199544936?cc=bs&amp;lang=en&amp;\">Collingwood (1933, pgs. 72-73)</a> was skeptical that even different gradations of a single type of pain qualify as cardinal:&nbsp;</p><blockquote><p>As I move my hand nearer to the fire, I feel it grow hotter; but every increase in the heat I feel is also a change in the kind of feeling I experience; from a faint warmth through a decided warmth it passes to a definite heat, first pleasant, then dully painful, then sharply painful; the heat at one degree soothes me, at another excites me, at another torments me. I can detect as many differences in kind as I can detect differences in degree; and these are not two sets of differences but one single set. I can call them differences in degree if I like, but I am using the word in a special sense, a sense in which differences of degree not merely entail, but actually are, differences of kind.&nbsp;</p></blockquote><p>The ordinality of severity would rule out a simple mathematical approach entirely. It is consistent with but does not entail that some pains are so severe that they outweigh all lesser pains regardless of duration. If severity is in reality cardinal but scientists have just yet to measure it on a ratio scale, then adopting a mathematical approach to pain severity is no more problematic than the myriad other ordinal scales that psychologists use to approximate an underlying continuum (<a href=\"https://doi.org/10.4324/9780203501207\">Markus &amp; Borsboom, 2013</a>, chapters 2-4).</p><h1>Future Directions</h1><p>The abovementioned suggestions for how to overcome the methodological barriers in the last section are only based on a few weeks of thinking them through. It seems likely that subject matter expertise in pain in experimental settings is necessary to identify the most innovative solutions, especially for studies of non-human animals.&nbsp; To facilitate this effort, we are planning on hosting a workshop in which animal welfare scientists, pain scientists, and philosophers will attempt to develop new methodologies to test whether farmed animals and laboratory animals treat the severity or duration of pain differently when making aversiveness judgments.&nbsp;</p><h1>Acknowledgements</h1><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994770/mirroredImages/7KJZWLMepdZ4mHvxS/lvnpgviojhwbu4imkizf.png\"></p><p><br><i>This research is a project of</i><a href=\"http://rethinkpriorities.org/\"><i>&nbsp;<u>Rethink Priorities</u></i></a><i>. It was written by William McAuliffe and Adam Shriver. Thanks to David Moss, Jason Schukraft, Marcus A. Davis, and Travis Timmerman for helpful feedback on earlier versions of this report. A special thanks to Michael St. Jules, who provided comments on earlier drafts and quality control on the penultimate draft. If you like our work, please consider</i><a href=\"https://www.rethinkpriorities.org/newsletter\"><i>&nbsp;<u>subscribing to our newsletter</u></i></a><i>. You can explore our completed public work&nbsp;</i><a href=\"https://www.rethinkpriorities.org/research\"><i><u>here</u></i></a><i>.</i></p><h1>References</h1><p>Anestis, M. D., &amp; Capron, D. W. (2016). An investigation of persistence through pain and distress as an amplifier of the relationship between suicidal ideation and suicidal behavior. <i>Journal of Affective Disorders</i>, <i>196</i>, 78\u201382.<a href=\"https://doi.org/10.1016/j.jad.2016.02.044\">&nbsp;https://doi.org/10.1016/j.jad.2016.02.044</a></p><p>Bennett, G. J. (2012). What is spontaneous pain and who has it? <i>The Journal of Pain</i>, <i>13</i>(10), 921-929.</p><p>Bernfort, L., Gerdle, B., Husberg, M., &amp; Levin, L.-\u00c5. (2018b). People in states worse than dead according to the EQ-5D UK value set: Would they rather be dead? <i>Quality of Life Research</i>, <i>27</i>(7), 1827\u20131833.<a href=\"https://doi.org/10.1007/s11136-018-1848-x\">&nbsp;https://doi.org/10.1007/s11136-018-1848-x</a></p><p>Boulazreg, S., &amp; Rokach, A. (2020). The lonely, isolating, and alienating implications of Myalgic Encephalomyelitis/Chronic Fatigue Syndrome. In <i>Healthcare</i> (Vol. 8, No. 4, p. 413). Multidisciplinary Digital Publishing Institute.</p><p>Blanton, T. (2021). Raising beef cattle. <a href=\"https://quillette.com/2021/03/19/raising-beef-cattle/\">https://quillette.com/2021/03/19/raising-beef-cattle/</a></p><p>Browning, H. (2022). The problem of interspecies welfare comparisons (preprint). https://perma.cc/BXL3-ZNLB</p><p>Collingwood, R. G. (1933). <i>An essay on philosophical method</i>. Clarendon Press.</p><p>Craig, B. M., Rand, K., Bailey, H., &amp; Stalmeier, P. F. (2018). Quality-adjusted life-years without constant proportionality. <i>Value in Health</i>, <i>21</i>(9), 1124-1131.</p><p>Crisp, R. (2006). Hedonism reconsidered. <i>Philosophy and Phenomenological Research</i>, <i>73</i>(3), 619-645.</p><p>Davis, K. D., Flor, H., Greely, H. T., Iannetti, G. D., Mackey, S., Ploner, M., ... &amp; Wager, T. D. (2017). Brain imaging tests for chronic pain: medical, legal and ethical issues and recommendations. Nature Reviews Neurology, 13(10), 624-638.</p><p>Dickert, S., V\u00e4stfj\u00e4ll, D., Kleber, J., &amp; Slovic, P. (2015). Scope insensitivity: The limits of intuitive valuation of human lives in public policy. <i>Journal of Applied Research in Memory and Cognition</i>, <i>4</i>(3), 248\u2013255.<a href=\"https://doi.org/10.1016/j.jarmac.2014.09.002\">&nbsp;https://doi.org/10.1016/j.jarmac.2014.09.002</a></p><p>Ducasse, D., Loas, G., Dassa, D., Gramaglia, C., Zeppegno, P., Guillaume, S., Oli\u00e9, E., &amp; Courtet, P. (2018). Anhedonia is associated with suicidal ideation independently of depression: A meta-analysis. <i>Depression and Anxiety</i>, <i>35</i>(5), 382\u2013392.<a href=\"https://doi.org/10.1002/da.22709\">&nbsp;https://doi.org/10.1002/da.22709</a></p><p>Foster, D. Health and happiness research topics\u2014part 1: background on QALYs and DALYs. https://rethinkpriorities.org/publications/health-and-happiness-research-topics-background-on-qalys-and-dalys</p><p>Gerritzen, M. A., Reimert, H. G. M., Hindle, V. A., Verhoeven, M. T. W., &amp; Veerkamp, W. B. (2013). Multistage carbon dioxide gas stunning of broilers. <i>Poultry Science</i>, <i>92</i>(1), 41\u201350. <a href=\"https://doi.org/10.3382/ps.2012-02551\"><u>https://doi.org/10.3382/ps.2012-02551</u></a></p><p>Grandin, T., Curtis, S. E., Widowski, T. M., &amp; Thurmon, J. C. (1986). Electro-immobilization versus mechanical restraint in an avoid-avoid choice test for ewes. <i>Journal of Animal Science</i>, <i>62</i>(6), 1469\u20131480.<a href=\"%20\">&nbsp;</a><a href=\"https://doi.org/10.2527/jas1986.6261469x\">https://doi.org/10.2527/jas1986.6261469x</a></p><p>Griffin, J. (1986). Well-being: Its meaning, measurement and moral importance. Clarendon Press.</p><p>Gudex, C., &amp; Dolan, P. (1995). <i>Valuing health states: the effect of duration</i>. University of York.</p><p>Hashmi, J. A., Baliki, M. N., Huang, L., Baria, A. T., Torbey, S., Hermann, K. M., ... &amp; Apkarian, A. V. (2013). Shape shifting pain: chronification of back pain shifts brain representation from nociceptive to emotional circuits. <i>Brain</i>, <i>136</i>(9), 2751-2768.</p><p>Helgesson, G., Ernstsson, O., \u00c5str\u00f6m, M., &amp; Burstr\u00f6m, K. (2020). Whom should we ask? A systematic literature review of the arguments regarding the most accurate source of information for valuation of health states. <i>Quality of Life Research</i>, <i>29</i>(6), 1465\u20131482.<a href=\"https://doi.org/10.1007/s11136-020-02426-4\">&nbsp;https://doi.org/10.1007/s11136-020-02426-4</a></p><p>Huemer, M. (2008). In defence of repugnance. <i>Mind</i>, <i>117</i>(468), 899-933.</p><p>Joiner, T. E. (2010). <i>Myths about Suicide</i>. Harvard University Press.</p><p>Jonker, M. F., &amp; Norman, R. (2022). Not all respondents use a multiplicative utility function in choice experiments for health state valuations, which should be reflected in the elicitation format (or statistical analysis). <i>Health Economics</i>, <i>31</i>(2), 431-439.</p><p>Kahneman, D., Wakker, P. P., &amp; Sarin, R. (1997). Back to Bentham? Explorations of experienced utility. <i>The quarterly journal of economics</i>, <i>112</i>(2), 375-406.</p><p>Law, K. C., Jin, H. M., &amp; Anestis, M. D. (2018). The intensity of suicidal ideation at the worst point and its association with suicide attempts. <i>Psychiatry Research</i>, <i>269</i>, 524\u2013528. <a href=\"https://doi.org/10.1016/j.psychres.2018.08.094\"><u>https://doi.org/10.1016/j.psychres.2018.08.094</u></a></p><p>Leighton, J. (2011). <i>The battle for compassion: Ethics in an apathetic universe</i>. Algora Publishing.</p><p>Liersch, M. J., &amp; McKenzie, C. R. M. (2009). Duration neglect by numbers\u2014And its elimination by graphs. <i>Organizational Behavior and Human Decision Processes</i>, <i>108</i>(2), 303\u2013314.<a href=\"%20\">&nbsp;</a><a href=\"https://doi.org/10.1016/j.obhdp.2008.07.001\">https://doi.org/10.1016/j.obhdp.2008.07.001</a></p><p>Markus, K. A., &amp; Borsboom, D. (2013). <i>Frontiers of test validity theory: Measurement, causation, and meaning</i>. Routledge.</p><p>May, A. M., &amp; Victor, S. E. (2018). From ideation to action: Recent advances in understanding suicide capability. <i>Current Opinion in Psychology</i>, <i>22</i>, 1\u20136. <a href=\"https://doi.org/10.1016/j.copsyc.2017.07.007\"><u>https://doi.org/10.1016/j.copsyc.2017.07.007</u></a></p><p>McPherson, K., Myers, J., Taylor, W. J., McNaughton, H. K., &amp; Weatherall, M. (2004). Self-valuation and societal valuations of health state differ with disease severity in chronic and disabling conditions. <i>Medical Care</i>, <i>42</i>(11), 1143\u20131151.</p><p>Mayerfeld, J. (1999). <i>Suffering and moral responsibility</i>. Oxford University Press on Demand.</p><p>Mill, J.S., (1861/2006). <i>The collected works of John Stuart Mill, volume X - Essays on ethics, religion, and society</i>. University of Toronto Press.</p><p>Mitchell, W. C. (1918). Bentham's felicific calculus. <i>Political Science Quarterly</i>, <i>33</i>(2), 161-183.</p><p>Myers, J. A., McPherson, K. M., Taylor, W. J., Weatherall, M., &amp; McNaughton, H. K. (2003). Duration of condition is unrelated to health-state valuation on the EuroQoL. <i>Clinical Rehabilitation</i>, <i>17</i>(2), 209\u2013215.<a href=\"%20\">&nbsp;</a><a href=\"https://doi.org/10.1191/0269215503cr602oa\">https://doi.org/10.1191/0269215503cr602oa</a></p><p>Ng, Y. K. (1995). Towards welfare biology: Evolutionary economics of animal consciousness and suffering. <i>Biology and Philosophy</i>, <i>10</i>(3), 255-285.</p><p>Nielsen, B. L., Thodberg, K., Malmkvist, J., &amp; Steenfeldt, S. (2011). Proportion of insoluble fibre in the diet affects behaviour and hunger in broiler breeders growing at similar rates. <i>Animal</i>, <i>5</i>(8), 1247-1258.</p><p>Norwood, F. B., &amp; Lusk, J. L. (2011). <i>Compassion, by the pound: The economics of farm animal welfare.</i><a href=\"https://www.cabdirect.org/cabdirect/abstract/20113168489\">&nbsp;https://www.cabdirect.org/cabdirect/abstract/20113168489</a></p><p>Price, D. D., McGrath, P. A., Rafii, A., &amp; Buckingham, B. (1983). The validation of visual analogue scales as ratio scale measures for chronic and experimental pain: <i>Pain</i>, <i>17</i>(1), 45\u201356.<a href=\"https://doi.org/10.1016/0304-3959(83)90126-4\">&nbsp;https://doi.org/10.1016/0304-3959(83)90126-4</a></p><p>Price, D. D. (2002). Central neural mechanisms that interrelate sensory and affective dimensions of pain. <i>Molecular interventions</i>, <i>2</i>(6), 392.</p><p>Roberts, E., Wessely, S., Chalder, T., Chang, C. K., &amp; Hotopf, M. (2016). Mortality of people with chronic fatigue syndrome: a retrospective cohort study in England and Wales from the South London and Maudsley NHS Foundation Trust Biomedical Research Centre (SLaM BRC) Clinical Record Interactive Search (CRIS) register. <i>The Lancet</i>, <i>387</i>(10028), 1638-1643.</p><p>Scalone, L., Stalmeier, P.F.M., Milani, S. <i>et al.</i> Values for health states with different life durations. <i>Eur J Health Econ</i> 16, 917\u2013925 (2015). <a href=\"https://doi.org/10.1007/s10198-014-\">https://doi.org/10.1007/s10198-014-</a>0634-0</p><p>Schneider, P. (2021). The QALY is ableist: On the unethical implications of health states worse than dead. <i>Quality of Life Research</i>.<a href=\"https://doi.org/10.1007/s11136-021-03052-4\">&nbsp;https://doi.org/10.1007/s11136-021-03052-4</a></p><p>Schreiber, J., Culpepper, L., &amp; Fife, A. (2021). Suicidal ideation and behavior in adults. <i>Waltham, MA, USA: UpToDate Inc</i>.</p><p>Stalmeier, P. F., Lamers, L. M., Busschbach, J. J., &amp; Krabbe, P. F. (2007). On the assessment of preferences for health and duration: maximal endurable time and better than dead preferences. <i>Medical Care</i>, 835-841.</p><p>Tomasik, B. (2013). Hedonistic vs. preference utilitarianism. <a href=\"https://longtermrisk.org/hedonistic-vs-preference-utilitarianism/#Infinite_preferences_and_negative-leaning_utilitarianism\"><u>https://longtermrisk.org/hedonistic-vs-preference-utilitarianism/#Infinite_preferences_and_negative-leaning_utilitarianism</u></a></p><p>Tomasik, B. (2019). Three types of negative utilitarianism. Essays on Reducing Suffering, https://reducing-suffering.org/three-types-of-negative-utilitarianism/</p><p>Vinding, M. (2020). Suffering-focused ethics: Defense and implications. <i>Independently published, May</i>.</p><p>Walasek, L., Brown, G. D. A., &amp; Ovens, G. D. (2019). Subjective well-being and valuation of future health states: Discrepancies between anticipated and experienced life satisfaction. <i>Journal of Applied Social Psychology</i>, <i>49</i>(12), 746\u2013754.<a href=\"https://doi.org/10.1111/jasp.12631\">&nbsp;https://doi.org/10.1111/jasp.12631</a></p><p>Walters, E. T., &amp; Williams, A. C. de C. (2019). Evolution of mechanisms and behaviour important for pain. <i>Philosophical Transactions of the Royal Society B: Biological Sciences</i>, <i>374</i>(1785), 20190275.<a href=\"https://doi.org/10.1098/rstb.2019.0275\">&nbsp;https://doi.org/10.1098/rstb.2019.0275</a></p>", "user": {"username": "Will M"}}, {"_id": "DRaugD8TWj3pqxGRj", "title": "Consequentialism and Cluelessness", "postedAt": "2022-10-17T18:57:40.684Z", "htmlBody": "<blockquote><p><strong>TL;DR: </strong>Invisible high stakes don\u2019t undermine ordinary expected value verdicts. And even if they did, that wouldn\u2019t undermine consequentialism because the question of <i>what fundamentally matters</i> is epistemically prior to the question of <i>whether we can reliably track it</i>. Moreover, one cannot plausibly deny that invisible consequences still matter, in principle.</p></blockquote><p>&nbsp;</p><p>James Lenman\u2019s \u2018<a href=\"https://philpapers.org/rec/LENCAC-3\">Consequentialism and Cluelessness</a>\u2019 presents an influential <i>epistemic argument</i> against consequentialism. Roughly:</p><ol><li>We\u2019ve no idea what the long-term ramifications of any of our actions will be.</li><li>So we\u2019ve no idea what consequentalist reasons for action we have.</li><li>But an adequate ethical theory must guide us.</li></ol><p>So: C. Consequentialism is not an adequate ethical theory.</p><p>I think each of those premises is probably false (especially the last two).</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4327c32f-f24c-4913-b9e4-9f7896c75d3a_1024x1024.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4327c32f-f24c-4913-b9e4-9f7896c75d3a_1024x1024.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4327c32f-f24c-4913-b9e4-9f7896c75d3a_1024x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4327c32f-f24c-4913-b9e4-9f7896c75d3a_1024x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4327c32f-f24c-4913-b9e4-9f7896c75d3a_1024x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4327c32f-f24c-4913-b9e4-9f7896c75d3a_1024x1024.png 1456w\"></a></p><p><i>Indecipherable clues</i></p><h3>1. Longtermist Clues</h3><p>Though I won\u2019t dwell on the point here, longtermists obviously believe that there are at least <i>some</i> high-impact actions where we can be reasonably confident that they will improve the long-term future. Examples might include (i) working to avert existential risk, (ii) moral circle expansion and other efforts to secure \u201cmoral progress\u201d by improving society-wide ethics, and (iii) generally improving civilizational capacities (through education, economic growth, technological breakthroughs, etc.), in ways that don\u2019t directly increase existential risks.</p><p>But in what follows, I\u2019ll put such cases aside and focus on ordinary acts (e.g. saving a child\u2019s life) with only short-term <i>foreseeable</i> effects, and unknowable long-term causal ramifications (for familiar reasons to do with the extreme fragility of who ends up being conceived, such that even tiny changes may presumably ripple out and completely transform the future population).</p><h3>2. Defending the Expected Value Response</h3><p>The obvious response to cluelessness worries is to move to <a href=\"https://www.utilitarianism.net/types-of-utilitarianism#expectational-utilitarianism-versus-objective-utilitarianism\"><i>expectational</i></a><i> </i>consequentialism: if we\u2019ve no idea what the long-term consequences will be, then these \u201cinvisible\u201d considerations are (given our evidence) simply <i>silent</i>\u2014speaking neither for nor against any particular option. So the <i>visible</i> reasons will trivially win out. For example, saving a child\u2019s life has an expected value of one life saved, and pointing to our long-term ignorance doesn\u2019t change this.</p><p>Lenman is unimpressed with this response, but the four reasons he offers (on pp. 353 - 359) strike me as thoroughly confused.</p><p>First, he suggests that expectational consequentialists must rely upon some controversial probabilistic indifference principles (coming up with a principled way of partitioning the possibilities, and then assigning equal probability to each one), whereas it seems to me that no work at all is required because <i>no competing reasons have been offered</i>.</p><p>Perhaps the thought is that speculative long-term ramifications could be produced to count against the expected value of saving the child. (Like, \u201cWhat if the child turns out to be an ancestor of future-Hitler?\u201d) In response, the agent may say, \u201cThat\u2019s no reason at all unless you can show that the future risk is greater if I perform this act than if I don\u2019t.\u201d Why is the burden on the consequentialist agent to refute such utterly baseless speculation? I don\u2019t think I need to commit to any particular principle of indifference in order to say that <i>I haven\u2019t yet been presented with <strong>any</strong> compelling reason to revise my expected value estimate of +1 life saved</i>.</p><p>[Update: Hilary Greaves <a href=\"https://users.ox.ac.uk/~mert2255/papers/cluelessness.pdf\">offers the stronger response</a> that some restricted principle of indifference seems clearly warranted in these cases, notwithstanding whatever problems might apply to a <i>fully general</i> such principle. Whereas I've argued that it's surely <i>defensible</i> to take EV to be unaffected by simple cluelessness, Greaves argues that it's plausibly <i>rationally mandatory</i>. It would seem completely crazy to have asymmetric expectations in such cases, after all.]</p><p>Second, Lenman assumes that, against the background of astronomical invisible stakes, the visible reason to save a life must be, for consequentialists, \u201c<i>extremely</i> weak\u201d\u2014merely \u201ca drop in the ocean\u201d. But why the focus on relative stakes? In absolute terms, saving a life is incredibly important. The presence of <i>even greater</i> invisible stakes doesn\u2019t change the absolute weight of this reason in the slightest.</p><p>Perhaps Lenman is thinking that the strength of a consequentialist reason must be proportionate to the action\u2019s likelihood of serving the ultimate goal of maximizing overall value. Since the value of one life is vanishingly unlikely to sway the scales when comparing the long-term value of each option, to save one life can only be an \u201cextremely weak\u201d reason to pick one option over another. But the assumption here is simply false. The strength of a consequentialist reason is given by its associated (expected) value in absolute terms: the size of the drop, not the size of the ocean.</p><p>Third, Lenman objects:</p><blockquote><p>It is surely a sophistry to treat a zero expected value that reflects our knowledge that an act will lack significant consequences as parallel in significance to one that reflects our total ignorance of what such consequences (although we know they will be massive) will be.</p></blockquote><p>Like the <a href=\"https://www.philosophyetc.net/2018/07/acts-attitudes-and-separateness-of.html\">separateness of persons objection</a>, this mistakenly assumes that anything significant must result in changes to our verdicts about <i>acts</i>, when often fitting <i>attitudes</i> are better suited to reflect such significance. Consider: we obviously should feel vastly more angst / ambivalence\u2014and strongly wish that more info was available\u2014in the \u201ctotal uncertainty\u201d case than in the \u201cknown zero\u201d case. Why isn't that a sufficient difference in \u201csignificance\u201d? I don't see any reason here to think that it calls for a different <i>decision</i> to be made (assuming that no feasible investigative options are available; in practice, of course, the astronomical stakes instead motivate at least <i>attempting</i> longtermist investigation).</p><p>Fourth and finally, Lenman raises the possibility that perhaps some (less significant) acts may avoid having radical causal ramifications, resulting in <i>non-uniform</i> \u201cscaling down\u201d of our moral reasons, which would be awkward (absurdly yielding stronger consequentialist reasons to do more trivial acts). But again, as stressed in #2 above, there should be no \u201cscaling down\u201d at all\u2014that suggestion rested on a total misunderstanding of the reasons posited by any sensible consequentialism.</p><h3>Wrapping up: Why trust expected value?</h3><p>Perhaps the heart of Lenman\u2019s objection can be restated as a challenge: given astronomical invisible stakes, why trust visible expected value in the slightest? There\u2019s vanishingly small reason to think that the EV-maximizing act is also the <i>value</i>-maximizing act, and surely what consequentialists ultimately care about is actual value rather than expected value.</p><p>But I think this misses the point of being guided by expected value. As Frank Jackson stressed in his paper on \u2018<a href=\"https://philpapers.org/rec/JACDCA\">Decision-Theoretic Consequentialism</a>\u2019, in certain risky cases we may <i>know</i> that a \u201csafe\u201d option will not maximize value, yet it may nonetheless maximize <i>expected</i> value (if the alternatives risk disaster), and is for that very reason the prudent and rational choice. In <a href=\"https://www.philosophyetc.net/2016/01/expected-value-without-expecting-value.html\">other cases</a>, we may be required to give up a \u201csure thing\u201d for a slight chance of securing a vastly better outcome\u2014even if the outcome will then be <i>almost certainly</i> worse. So the point of being guided by expected value is not to increase our chance of doing the objectively best thing, nor to make a good result highly likely.</p><p>It\u2019s difficult to express precisely what the point is. But roughly speaking, it\u2019s a way to <i>promote value as best we can</i> given the information available to us (balancing stakes and probabilities). And one important feature of maximizing expected value is that <i>we cannot expect any subjectively-identifiable alternative to do better in the limit</i> (that is, imagining like decisions being repeated a sufficient number of times, across different possible worlds if need be), at least for object-given reasons.<a href=\"https://rychappell.substack.com/p/consequentialism-and-cluelessness#footnote-1\"><sup>1</sup></a> After all, if there were an identifiably better alternative, it would maximize expected value to follow <i>it</i>. And if Lenman\u2019s critique were accurate, it would imply <i>not</i> that expected value is untrustworthy, but rather that (contrary to initial appearances) saving a life lacks positive expected value after all.</p><p>Put this way, I don\u2019t think it makes sense to question our trust in expected value. (It\u2019s the practical analogue of asking, \u201cWhy believe in accordance with the evidence, when evidence can be misleading?\u201d In either case, the answer to \u201cWhy be rational?\u201d is just: <i>it\u2019s the best we can non-accidentally do!</i>) If the question is instead asked, \u201cWhy think that saving a life has positive expected value?\u201d then I just point to the prior section of this blog post. (In short: <i>why not?</i> It\u2019s visibly positive, and invisible considerations can hardly be shown to count against it!)</p><p>I get that cluelessness in the face of massive invisible long-term stakes can be angst-inducing. It should make us strongly wish for more information, and motivate us to pursue longtermist investigation if at all possible. But if no such investigations prove feasible, we should not mistake this residual feeling of angst for a reason to doubt that we can still be rationally guided by the smaller-scale considerations that we <i>do</i> see. To undermine the latter, it is not enough for the skeptic to gesture at the deep unknown. <a href=\"https://www.philosophyetc.net/2021/01/epistemic-calibration-bias-and-blame.html\">Unknowns, <i>as such</i>, are not epistemically undermining</a> (greedily gobbling up all else that is known). To undermine an expected value verdict, you need to show that some <i>alternative</i> verdict is epistemically <i>superior</i>. Proponents of the epistemic objection (like skeptics in many other contexts)<a href=\"https://rychappell.substack.com/p/consequentialism-and-cluelessness#footnote-2\"><sup>2</sup></a> cannot do this.</p><h3>3. The Possibility of Moral Cluelessness</h3><p>Suppose I\u2019m wrong about all of the above, and in fact we have <i>no reason at all</i> to think that saving a child\u2019s life does more good than harm (or is positive in expectation). That would be a sad situation. But it hardly seems kosher to infer from this that doing good <i>isn\u2019t what matters</i>. There\u2019s no metaphysical guarantee that we\u2019re in a position to fruitfully follow moral guidance.</p><p>It\u2019s surely conceivable that some agents (in some possible worlds) may be irreparably lost on practical matters. Any agents in the benighted epistemic circumstances (of not having the <i>slightest</i> reason to think that any given action of theirs will be positive or negative on net) are surely amongst the strongest possible candidates for being in this deplorable position. So if we conclude (or stipulate) that <i>we</i> are in those benighted epistemic circumstances, we should similarly conclude that <i>we</i> are the possible agents who are irreparably practically lost.</p><p>To suggest that we instead <i>revise our account of what morally matters</i>, merely to protect our presumed (but unearned) status as not totally at sea, strikes me as a transparently illegitimate use of \u201c<a href=\"https://rychappell.substack.com/p/utilitarianism-and-reflective-equilibrium?s=w\">reflective equilibrium</a>\u201d methodology\u2014akin to wishfully inferring that causal determinism must be false on the basis of incompatibilism plus a belief in free will.</p><p>Sometimes inferences are directionally constrained by considerations of epistemic priority: Against a backdrop of incompatibilism, you can infer \u201cno free will\u201d from causal determinism, but not \u201cno causal determinism\u201d from free will. The question whether causal determinism is true is <i>epistemically prior</i> to the question whether (given incompatibilism) we have free will. In a similar way, I suggest, the question of <i>what morally matters</i> is clearly epistemically prior to the question of <i>whether we have epistemic access to what morally matters</i>. To instead let the latter question settle the former strikes me as plainly perverse.</p><h3>Ethics and What Matters</h3><p>So what <i>does</i> matter? To prevent cluelessness from becoming a <a href=\"https://rychappell.substack.com/p/puzzles-for-everyone\">puzzle for everyone</a>, Lenman suggests that non-consequentialist agents \u201cshould ordinarily simply not regard [invisible consequences] as of moral concern.\u201d&nbsp;This seems crazy wrong to me.</p><p>Suppose you\u2019re given a magic box from the Gods, and told only that if you open it, one of two things will happen: either (a) it will cause a future holocaust, or (b) it will prevent a future holocaust.&nbsp; Lenman\u2019s view seems to be that you should regard this whole turn of events as a matter of indifference. I think it\u2019s much more plausible that you should care greatly about which outcome eventuates, and so naturally feel immense angst over the whole thing. Given your ineradicable cluelessness about the outcomes, the box doesn't affect what actions you should perform.&nbsp; But it surely is a matter of <i>concern</i>!</p><p>You should, for example, <i>strongly wish</i> that you had more info about which outcome would result from opening the box. Why would this be so, if invisible consequences were \u201csimply not\u2026 of moral concern\u201d?&nbsp;I think we should prefer that invisible consequences be rendered visible, precisely because (i) this would help us to bring about better ones, and (ii) we should care about that.</p><p>In a confusing passage, Lenman acknowledges that invisible consequences <i>matter</i>, just not <i>morally</i>:</p><blockquote><p>Of course, the invisible consequences of action very plausibly matter too, but there is no clear reason to suppose this mattering to be a matter of moral significance any more than the consequences, visible or otherwise, of earthquakes or meteor impacts (although they may certainly matter enormously) need be matters of, in particular, moral concern. There is nothing particularly implausible here. It is simply to say, for example, that the crimes of Hitler, although they were a terrible thing, are not something we can sensibly raise in discussion of the moral failings or excellences of [someone who saved the life of Hitler\u2019s distant ancestor].</p></blockquote><p>This is a strange use of \u201cmoral significance\u201d. Moral agents clearly ought to care about earthquakes, meteor strikes, and future genocidal dictators. (At a minimum, we ought to prefer that there be fewer of such things, as part of our beneficent concern for others generally.) An agent who was truly <i>indifferent</i> to these things would not be a virtuous agent: their indifference reveals a callous disregard for future people. So it could certainly constitute a \u201cmoral failing\u201d to fail to care about such harmful events.</p><p>On the other hand, if Lenman really just means to say that <i>whether unforeseeable consequences eventuate as a matter of fact </i>shouldn\u2019t affect our assessment of a person\u2019s \u201cmoral failings or excellences\u201d, then this seems a truism that in no way threatens consequentialism. It\u2019s a familiar point that many forms of agential assessment (e.g. rationality, virtue, etc.) are \u201cinternalist\u201d\u2014supervening on the intrinsic properties of the agent, and not what happens in the external world, beyond their control. While I\u2019ve <a href=\"https://rychappell.substack.com/p/consequentialism-beyond-action\">long been frustrated</a> that other consequentialists tend to downplay or neglect this, and think that saying plausible things here requires going <i>beyond</i> \u201cpure consequentialism\u201d in some respects (we need to make additional claims about fitting attitudes, for example), these additional claims are <strong>by no means in </strong><i><strong>conflict</strong></i><strong> with the core claims of pure consequentialism</strong>. So there really isn\u2019t any <i>problem</i> here\u2014at least, none that can\u2019t easily be fixed just by saying a bit more.</p><h3>Conclusion</h3><p>I\u2019ve argued that the cluelessness objection is deeply misguided. Invisible high stakes don\u2019t undermine ordinary expected value verdicts. And even if they did, that wouldn\u2019t undermine consequentialism because the question of <i>what fundamentally matters</i> is epistemically prior to the question of <i>whether we can reliably track it</i>. Lenman\u2019s non-consequentialist alternative proposal seems vicious, unless interpreted so narrowly that the relevant claim becomes trivial, and compatible with expectational consequentialism all along.<a href=\"https://rychappell.substack.com/p/consequentialism-and-cluelessness#footnote-3\"><sup>3</sup></a></p><h3><strong>Footnotes</strong></h3><p><a href=\"https://rychappell.substack.com/p/consequentialism-and-cluelessness#footnote-anchor-1\"><u>1 </u></a>Cf. an evil demon threatening to blow up the world if you use expected value as a decision procedure. We can bracket such \u201cstate-given\u201d reasons for present purposes, as they aren\u2019t relevant to the question of whether EV is a <i>rational</i> decision-procedure. The evil demon case is simply one of <a href=\"https://rychappell.substack.com/p/rational-irrationality-and-blameless\">Parfitian \u201crational irrationality\u201d</a>.</p><p><a href=\"https://rychappell.substack.com/p/consequentialism-and-cluelessness#footnote-anchor-2\"><u>2 </u></a>I raise a <a href=\"https://www.philosophyetc.net/2011/05/moral-lottery.html\">similar objection to Sharon Street\u2019s \u201cmoral lottery\u201d</a> objection to moral realism.</p><p><a href=\"https://rychappell.substack.com/p/consequentialism-and-cluelessness#footnote-anchor-3\"><u>3 </u></a>Thanks to participants in the \u201cCluelessness\u201d reading group at <a href=\"https://globalprioritiesinstitute.org/\">GPI</a> last week, for helpful discussion.</p>", "user": {"username": "RYC"}}, {"_id": "zDJpYMtewowKXkHyG", "title": "Alien Counterfactuals", "postedAt": "2022-10-17T17:33:42.459Z", "htmlBody": "<p>In a previous post this week, titled&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/wqmY98m3yNs6TiKeL/parfit-singer-aliens\"><u>\u201cParfit + Singer + Aliens = ?\u201d</u></a> OP makes the important point that if alien life is likely to come into existence in our lightcone, extinction-risk reduction has reduced value.&nbsp;</p><p>Relevant snippet -&nbsp; \u201cHolding future and alien life to be morally valuable means that, on the discovery of alien life, humanity\u2019s future becomes a vanishingly small part of the morally valuable universe. In this situation, Longtermism ceases to be action relevant. It might be true that certain paths into far future contain the vast majority of moral value but if there are lots of morally valuable aliens out there, the universe is just as likely to end up one of these paths whether humans are around or not so Longtermism doesn\u2019t help us decide what to do. We must either impartially hope that humans get to be the ones tiling the universe or go back to considering the nearer term effects of our actions as more important.\u201d</p><p>The point of this post is to add that it also matters <i>what</i> the mean values/quality of these potential civilizations are in expectation. OP implicitly assumes that these aliens will convert resources in the universe into utility as efficiently as our society would. Currently, I agree with the author on this because I think we don\u2019t have any good evidence to the contrary and I think we should take the prior that our society will have mean grabby civ values. However, I think it would be possible to deduce that on expectation the potential GC\u2019s that may come into existence have different expected conversion of control of resources into utility than our own society.&nbsp;</p><p>To go about this, we could leverage economic history, evolutionary biology, etc. to run models of civilization formation and try to see if our society is \u201cWeird\u201d in any way. We also can continue to search for other civilizations as OP said and update on their characteristics vs our own.&nbsp;</p><p>I believe that currently running models is pretty intractable because the relevant fields just aren\u2019t that developed, and this seems like a really hard and complicated problem, but it could be a line of thinking to go down as computational power and the quality of these fields increases.&nbsp;</p><p><br><br>&nbsp;</p>", "user": {"username": "Charles_Guthmann"}}, {"_id": "qmYW7nHfqaX8LekEj", "title": "Effective Thesis is hiring!", "postedAt": "2022-10-19T12:21:21.411Z", "htmlBody": "<h1>Who we are</h1><p><a href=\"http://effectivethesis.org/\">Effective Thesis</a> is a non-profit organisation that supports university students to begin and progress in research careers that significantly improve the world. We do this primarily by helping students identify important problems where further research could have a big impact and advise them on their research topic selection (mostly in the context of a final thesis/dissertation/capstone project or PhD applications). We run <a href=\"https://forum.effectivealtruism.org/posts/SbjotEZsHhRBkk8Pg/new-effective-thesis-services-and-opportunities-to-get\">a number of services</a> to achieve this, with <a href=\"https://effectivethesis.org/thesis-coaching/\">topic choice coaching</a> being the most important one.</p><p>Our choice of recommended problems and research directions are heavily inspired by the EA community and our funding also comes from sources related to the EA community (most recently from the EA Infrastructure Fund).&nbsp;</p><p>We think what we do is impactful because we focus on a specific intervention point (thesis writing/PhD applications) which often seems to have path-dependent effects on the rest of one's research career trajectory (see more about <a href=\"https://forum.effectivealtruism.org/posts/8Auu2woJb8uAdjMrm/effective-thesis-activities-and-impact-evaluation-june-2021#Our_mission\">our mission and theory of change here</a>).</p><p>We are a fully remote, small organisation of 6 people working close to full-time and 4 long-term volunteers. We operate mostly in UTC+1 (Central European) timezone, but can potentially accommodate other timezones if the candidate is able to spend some part of their working hours in UTC+1. Our culture is start-up-like, friendly and collaborative. You can expect to have lots of opportunities to grow, learn and try new things, lots of personal agency, and flexibility in how you get things done. We also have regular online socials and optional coworking, to support making connections with other team members.</p><p>Here is the most recent <a href=\"https://forum.effectivealtruism.org/posts/8Auu2woJb8uAdjMrm/effective-thesis-activities-and-impact-evaluation-june-2021\">2022 report</a> outlining our activities, impact, and future plans.</p><h1>Positions we are hiring for</h1><h2><a href=\"https://effectivethesis.org/coaching-job-ad/\">Head of coaching (fulltime) and a coach (part-time)</a></h2><p>The Head of Coaching is responsible for improving and carrying out Effective Thesis\u2019s coaching process, including growing and managing our networks of domain experts to generate the most impact and value for students and domain experts alike. <a href=\"https://effectivethesis.org/thesis-coaching/\">Coaching</a> is our most important service, as we think it has the most impact on influencing students\u2019 future research career trajectories. If you would enjoy working with students and domain experts directly across many disciplines and research topics and would enjoy creating impact via helping students figure out which research directions they can have the most impact in and matching them with the right experts, you will likely enjoy this role. Together, we can make sure there are many more highly capable and motivated researchers focusing on the most important global problems in the future, ultimately helping to solve them faster.</p><h2><a href=\"https://effectivethesis.org/social-media-specialist/\">Social media specialist (part-time)</a></h2><p>We are looking for a freelance Social Media Specialist to manage and create content for our social media accounts. We would like to become a go-to place for aspiring researchers from undergraduate to PhD level who would like to have as much impact as possible with their research careers, and we\u2019re looking for someone who could help us achieve this!&nbsp;&nbsp;</p><p>This role might be an especially good fit for students who are active on Twitter (and other social media platforms) and would like to get better at this skill. You could potentially do this alongside your studies and make use of synergies with your own thesis/research planning.&nbsp;&nbsp;</p><h2><a href=\"https://effectivethesis.org/online-community-manager/\">Online community manager (part time)</a></h2><p>We are looking for a person who will manage and grow our <a href=\"https://effectivethesis.org/community/\">online community</a> of students striving to build their research careers in our <a href=\"https://effectivethesis.org/theses/\">prioritised research directions</a>. The goal of the community is to help students create more connections with their peers who are focusing on similar topics and generally help them feel less isolated.</p><p>We think this role would fit well with our <a href=\"https://effectivethesis.org/coaching-job-ad/\">part-time coaching role</a> and our&nbsp;<a href=\"https://effectivethesis.org/online-research-workshops-creator/\">online research skills workshop creator role</a>, so please let us know if you would be interested in either of those roles as well.</p><h2><a href=\"https://effectivethesis.org/online-research-workshops-creator/\">Online Research Workshops Creator (freelance/part-time)</a></h2><p>We (Effective Thesis) are looking for a person who will help us develop and run new online workshops aimed at helping aspiring researchers build useful research skills (see e.g. our <a href=\"https://effectivethesis.org/reasoning-transparency/\">Reasoning Transparency workshop</a>). We have already identified a number of skills and topics we would like to cover (primarily skills that people are unlikely to learn in academia or elsewhere and that are focused on making research more impactful rather than more efficient). We would like to offer these workshops to aspiring researchers focusing on some of our <a href=\"https://effectivethesis.org/theses/\">prioritised research directions,</a> to help them be more successful in building impactful research careers.&nbsp;</p><p>Read more about the thinking we have done about our workshops <a href=\"https://docs.google.com/document/d/1aD7h_Tl2jd1N-aXmBK7CQXzZ21GymD0BXZP9h7nHZdQ/edit?usp=sharing\">here</a>.</p><p>&nbsp;</p><p><a href=\"https://effectivethesis.org/get-involved/\">See here</a> for other ways to get involved with us.</p><p>&nbsp;</p><h1>Apply and tell your friends</h1><p>If you feel like you could be a good fit for any of these roles, please apply via the forms/emails mentioned in the linked job ads! We would also be grateful if you share these ads with your friends and other people who might be a good fit.<br><br>&nbsp;</p>", "user": {"username": "DavidJanku"}}, {"_id": "zLi3MbMCTtCv9ttyz", "title": "Formalizing Extinction Risk Reduction vs. Longtermism", "postedAt": "2022-10-17T15:37:55.927Z", "htmlBody": "<p>Edit: As a commenter pointed out, I mean extinction risk rather than x-risk in this post. Double edit: I'm not even sure exactly what I meant, and I think the whole x-risk terminology needs to be cleaned up alot.</p><p>There have been a string &nbsp;of recent posts about <s>X-risk</s> extinction risk reduction and longtermism. Why <a href=\"https://forum.effectivealtruism.org/posts/KDjEogAqWNTdddF9g/long-termism-vs-existential-risk\">they are basically the same</a>. <a href=\"https://forum.effectivealtruism.org/posts/wqmY98m3yNs6TiKeL/parfit-singer-aliens\">Why </a>they are <a href=\"https://forum.effectivealtruism.org/posts/WebLP36BYDbMAKoa5/the-future-might-not-be-so-great\">different</a>. I tried to write up a more formal outline that generalizes the problem (crossposted from a previous comment)</p><p>Confidence: Moderate. I can't identify specific parts where I could be wrong (though ironing out a definition of surviving would be good), but I also haven't talked to many people about this.&nbsp;</p><p><u>Definitions</u></p><ul><li>EV[lightcone] is the current expected utility in our lightcone.</li><li>EV[survivecone] &nbsp;is the expected utility in our lightcone if we \u201csurvive\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnsntako9xhs\"><sup><a href=\"#fnnsntako9xhs\">[1]</a></sup></span>&nbsp;as a society.</li><li>EV[deathcone] &nbsp;is the expected utility in our lightcone if we \u201cdie\u201d.</li><li>P(survive) + P(die) = 1</li><li>Take <s>x-risk</s> extinction risk reduction to mean increasing P(survive)</li></ul><p>&nbsp;</p><p><u>Lemma</u></p><ul><li>EV[lightcone]=P(survive)EV[survivecone] + P(die)EV[deathcone]</li></ul><p>equivalently</p><ul><li>EV[survivecone] = EV[lightcone | survive]</li><li>EV[deathcone] = EV[lightcone | death]</li></ul><p>(thanks kasey)</p><p><u>Theorem</u></p><ul><li>If EV[survivecone] &lt; EV[deathcone], <s>x-risk</s> &nbsp;extinction risk reduction is negative EV.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefambai4xxfzs\"><sup><a href=\"#fnambai4xxfzs\">[2]</a></sup></span></li><li>If EV[survivecone] &gt; EV[deathcone], then<s> x-risk</s> extinction risk reduction is positive EV.</li></ul><p><u>Corollary&nbsp;</u></p><ul><li>If Derivative<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreft3opcgwziqj\"><sup><a href=\"#fnt3opcgwziqj\">[3]</a></sup></span>(p(survive)) x&nbsp; EV_future &lt; p(survive) x Derivative(EV_future), it\u2019s more effective to work on improving EV[survivecone].<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyxeqh0p9hp\"><sup><a href=\"#fnyxeqh0p9hp\">[4]</a></sup></span></li><li>If Derivative(p(survive)) x&nbsp; EV_future &gt; p(survive) x Derivative(EV_future), it\u2019s more effective to reduce <s>existential</s> extinction risks.</li></ul><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnsntako9xhs\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnsntako9xhs\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I like to think of surviving as meaning becoming a <a href=\"https://grabbyaliens.com/\"><u>grabby</u></a> civilization, but maybe there is a better way to think of it.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnambai4xxfzs\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefambai4xxfzs\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Here I'm just assuming x-risk reduction doesn't affect EV's, obviously not true but for simplicity.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnt3opcgwziqj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreft3opcgwziqj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Where we are differentiating with respect to effort put into each respective cause</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyxeqh0p9hp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyxeqh0p9hp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This could be true even if the future was in expectation positive although it would be a very peculiar situation if that were the case (which is sort of the reason we ended up on x-risk reduction).</p></div></li></ol>", "user": {"username": "Charles_Guthmann"}}, {"_id": "DDsTgC59MER7cPiRh", "title": "Introducing Cause Innovation Bootcamp", "postedAt": "2022-10-17T13:42:23.392Z", "htmlBody": "<p>We are pleased to introduce Cause Innovation Bootcamp (CIB), a project that aims to train researchers interested in EA, while vetting new potential cause areas in Global Health and Development. We achieve this by taking research fellows through a training bootcamp that upskills them on the basics of evidence-based research and then getting them to produce a shallow report (using a standardised template) of a cause area, all whilst being supported by a senior mentor. These reports will then be posted on the EA Forum, and be sent to relevant organisations who the research might be of particular interest to, and for whom it might inform their decision-making. Cause areas are selected through a rough prioritisation which helps us identify which ones we think are most likely to be promising. We are currently running a pilot program for 5-6 fellows (7th Nov. 2022- 20th Dec. 2022).&nbsp; Applications are open until 30th Oct. 2022 (Sunday)- please apply on the following&nbsp;<a href=\"https://forms.gle/mbEzbototoPMkzCCA\"><u>application form&nbsp;</u></a>if you are interested.</p><h2><br><strong><u>Why this project?</u></strong></h2><p>We identified a few primary bottlenecks in the research infrastructure within EA:</p><ol><li><strong>A relative lack of novel research exploring new problem areas and interventions that might beat the current bar and/or be as impactful as current EA focus areas within global health and development-&nbsp;</strong>We think that this is important because our ability to most effectively do the most good depends on our ability to either fund or found new interventions within cause areas. We try to solve this by having created a&nbsp;comprehensive database of cause areas within global health and global development, and roughly prioritising them based on burden of disease/number of people affected and tractability. By producing shallow reports on topics that might be promising and are currently under-investigated, we hope that this can reveal potential \u2018blindspots\u2019 and \u2018low-hanging fruit\u2019. Our database and all shallow reports written will be an open-access resource available to the entire EA research community.</li><li><strong>A relative lack of quality researchers (both current and emerging) approaching prioritisation from an EA lens-&nbsp;</strong>We think, given that quality research is essential to fund or found promising interventions, researchers are pretty central to the long-term health and strength of the EA movement. We aim to mobilise two groups of talent: (1) motivated EAs who want to be involved in research, but don't currently have the requisite skills and experience, (2) existing researchers in the broader GHD (Global Health and Development) community who might be interested in approaching research from an EA perspective.</li></ol><p>Although there are lots of fantastic researchers in EA and the broader GHD community, we know that there is a talent bottleneck. We also know that there are lots of motivated people out there who want to be involved in research!</p><h2><br>&nbsp;<strong><u>What do we do?</u></strong></h2><p>Our theory of change is below</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0f49f065cf8cf182485b5ff86a37f001f1ee24da6597fbc3.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0f49f065cf8cf182485b5ff86a37f001f1ee24da6597fbc3.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0f49f065cf8cf182485b5ff86a37f001f1ee24da6597fbc3.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0f49f065cf8cf182485b5ff86a37f001f1ee24da6597fbc3.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0f49f065cf8cf182485b5ff86a37f001f1ee24da6597fbc3.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0f49f065cf8cf182485b5ff86a37f001f1ee24da6597fbc3.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0f49f065cf8cf182485b5ff86a37f001f1ee24da6597fbc3.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0f49f065cf8cf182485b5ff86a37f001f1ee24da6597fbc3.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0f49f065cf8cf182485b5ff86a37f001f1ee24da6597fbc3.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0f49f065cf8cf182485b5ff86a37f001f1ee24da6597fbc3.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0f49f065cf8cf182485b5ff86a37f001f1ee24da6597fbc3.png/w_1354 1354w\"></figure><p>Our current assumption is that roughly 50% of the impact is achieved by introducing more good researchers to the EA research ecosystem<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0a5mxq44ngmm\"><sup><a href=\"#fn0a5mxq44ngmm\">[1]</a></sup></span>. We would estimate that the other 50% of the impact to come from research produced through the training program<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbie9ewk2fs\"><sup><a href=\"#fnbie9ewk2fs\">[2]</a></sup></span>. We aren\u2019t confident about these estimates, and will evaluate these at the end of our pilot program; depending on their results, we might emphasise parts of this Theory of Change that generate the most impact.&nbsp;</p><p>&nbsp;</p><h2><strong><u>What does being a fellow involve?</u></strong></h2><p>We are currently running a pilot of our program, and have received funding from the EA Infrastructure Fund to fund 5- 6 research fellows through our program. The program is a 6-week commitment from November 7 2022&nbsp; -20 December 2022 that involves:</p><ul><li>Self-study of training modules on research skills- We have 6-8 lessons covering various key research topics (background research, theory of change, evaluating interventions, cost-effectiveness analyses, decision-making tools and how to write up your research, and other research skills), with each lesson linked to specific aspects of the shallow report that you write.</li><li>Production of a shallow report on one of the topics below (or another one of your choice that we vet and approve).</li><li>Several check-in calls with a mentor whilst you go through the program to check on your progress and help you troubleshoot any issues along the way.</li><li>Opportunity to be connected with established researchers and potentially to relevant job opportunities.</li><li>Assistance with editing your cause area report, so that it can be posted on the EA forum and sent to any relevant organisations.&nbsp;</li></ul><p>We estimate that this will require a time commitment of 10-15 hours per week, for a total of approximately 80 hours. For this, you will be compensated GBP&nbsp;\u00a31000<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefaiw5kez4pnu\"><sup><a href=\"#fnaiw5kez4pnu\">[3]</a></sup></span>.</p><p>&nbsp;</p><h2><strong><u>Who should apply?</u></strong></h2><p>We imagine that two bootcamp might be most useful for two groups of people:</p><ol><li>Motivated EAs who want to be involved in research, but who don't currently have the requisite skills and experience</li><li>Existing researchers in the broader GHD community who might be interested in approaching research from an EA perspective</li></ol><p>However, we think that there are likely other people who might be a great fit for this program, and we suggest that if you are unsure whether you are a good fit for the program, you err on the side of applying!</p><p>&nbsp;</p><h3><strong><u>What cause areas we are interested in for our pilot</u></strong></h3><p>For our pilot, we have identified 6 cause areas that we are most excited to see fellows work on. It is important to note that ultimately we care about how promising the interventions within these cause areas are and not the cause area itself. This means that at this stage we have erred on the side of including more cause areas and are also happy to consider ideas that you might have for areas that you think are worth investigating<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwr6ai0xbm4a\"><sup><a href=\"#fnwr6ai0xbm4a\">[4]</a></sup></span>.&nbsp;</p><p>Income-related problem areas</p><ul><li><strong>Access to electricity</strong>:&nbsp;<a href=\"https://ourworldindata.org/energy-access#access-to-electricity\"><u>13% of the world\u2019s population does not have access to electricity</u></a>. This research project involves asking if electricity access improves a household's income/national GDP, which interventions are most cost-effective in delivering access to electricity and who is working on and funding this issue.&nbsp;</li><li><strong>People living in slums</strong>:&nbsp;<a href=\"https://unhabitat.org/slum-almanac-2015-2016-0\"><u>Around 1 billion people worldwide live in slums</u></a> according to UN Habitat\u2019s definition. This research project involves asking how living in a slum impacts someone\u2019s health, happiness and income, which interventions are most cost-effective at alleviating some of the negative consequences and who is working on and funding the issue.&nbsp;</li><li><strong>Bridges</strong>: A bridge is a structure carrying a pathway or roadway over a depression or obstacle (such as a river) and is a common piece of hard infrastructure put in place by governments. This research project involves asking how bridges can improve a community's health and income prospects and which interventions are most cost-effective to put in place. The project is also supposed to collect information on who is working on building/maintaining and funding bridges.&nbsp;</li></ul><p>Health-related problem areas</p><ul><li><strong>Age-related and other hearing loss</strong>&nbsp;<a href=\"https://www.healthdata.org/results/gbd_summaries/2019/age-related-and-other-hearing-loss-level-3-cause\"><u>accounts for 40.2 million DALYS per year</u></a>. This research project involves asking which interventions are most cost-effective in addressing the disease burden of hearing loss, who is working on implementing these interventions and who funds the space.&nbsp;</li><li><strong>Falls</strong>&nbsp;<a href=\"https://www.healthdata.org/results/gbd_summaries/2019/falls-level-3-cause\"><u>account for 39.4 million DALYS each year</u></a>. This research project involves asking which interventions are most&nbsp;effective in alleviating the disease burden of falls, who is working on implementing these interventions and who funds the space.&nbsp;</li><li><strong>Meningitis</strong>&nbsp;<a href=\"https://www.healthdata.org/results/gbd_summaries/2019/meningitis-level-3-cause\"><u>accounts for 16.3 million DALYs each year</u></a>. This research project involves asking which interventions are most effective in alleviating the disease burden of meningitis, who is working on implementing these interventions, and who is providing funding for this issue.&nbsp;</li></ul><h3><strong><u>Application form</u></strong></h3><p>If you have read this post and are interested in applying (or know someone who might be interested),&nbsp;<a href=\"https://forms.gle/mbEzbototoPMkzCCA\"><u>you can apply here</u></a>! The form should take approximately 30 minutes and closes on <u>30th Oct. 2022 (Sunday)</u>. There will be no interview and we will contact candidates by&nbsp;<u>4th Nov 2022 (Friday).</u> The program will run from 7th Nov. 2022 - 20th Dec. 2022.&nbsp;</p><p>&nbsp;</p><h2><strong><u>Follow up</u></strong></h2><p>After our pilot program, we will evaluate how it went, and where its impact came from. If successful, we would plan to run this program again with more fellows. Ambitiously, we estimate that there are roughly 400-500 distinct cause areas within global health and development, and we could see this bootcamp producing reports on a significant proportion of these.</p><h2><br><strong><u>Acknowledgements&nbsp;</u></strong></h2><p>We would like to acknowledge the EA Infrastructure Fund for the financial support they have provided to the program and the helpful questions they asked. We would also like to acknowledge the people who have provided feedback on the initial idea and development of the project: Jack Rafferty, Joey Savoie. Furthermore, we would like to thank the following people, who gave feedback on this post:&nbsp; Melanie Basnak, Erik Hausen, David Nash, Chris Smith, and Abe Tolley.</p><p><br><br><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0a5mxq44ngmm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0a5mxq44ngmm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;We think that our training program will improve the capacity of existing researchers, and also increase the motivation and number of talented EA researchers. Our key uncertainty about this is whether our program can feasibly achieve this. To address this, we will review the usefulness of our training program after the pilot to assess this, and we plan to potentially run some longer-term follow up with our fellows to understand the impact of the bootcamp (if any).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbie9ewk2fs\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbie9ewk2fs\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;We think that these research reports produce impact by informing the decision making of organisations which either fund or found interventions. Our key uncertainties about the ability of this project to realise this impact are: (1) Will this research reach these organisations? (2) Will this research be high quality and decision relevant enough? (3) Do the organisations have the capacity/ability to act on this information? Largely, we think that (1) can be addressed by making all our research publicly available and sending it to specific organisations, (2) can be produced by a strong research and training program, and (3) can be assessed by having conversations with key organisations to understand their needs and capacity.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnaiw5kez4pnu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefaiw5kez4pnu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Fellows can be based anywhere in the world that we are able to make a payment to; this should mean that practically anyone can apply, but please get in touch if you are unsure and we can talk about your specific case.&nbsp; Also, if the level of compensation might prohibit you from applying, please get in touch with us.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwr6ai0xbm4a\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwr6ai0xbm4a\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The rough criteria we used to prioritise the cause areas at this point are, in rough order: (1) that nobody within the EA ecosystem has comprehensively looked into this before, (2) an area that is large enough (above 1 million DALYs/affecting more than 1 million people) (3) areas which, after a rough prioritisation, we think are likely to have promising interventions, (4) areas that were sufficiently straightforward so less experienced researchers would have a good chance of finding relevant information.&nbsp;</p></div></li></ol>", "user": {"username": "Akhil Bansal"}}, {"_id": "DTTADonxnDRoksp4E", "title": "AI Safety Ideas: A collaborative AI safety research platform", "postedAt": "2022-10-17T17:01:29.978Z", "htmlBody": "<p><strong>TLDR</strong>; We present the AI safety ideas and research platform&nbsp;<a href=\"https://aisi.ai\"><strong><u>AI Safety Ideas</u></strong></a> in open alpha. Add and explore research ideas on the website here:&nbsp;<a href=\"https://aisafetyideas.com/\"><u>aisafetyideas.com</u></a>.</p><p>AI Safety Ideas has been accessible for a while in an alpha state (4 months, on-and-off development) and we now publish it in open alpha to receive feedback and develop it continuously with the community of researchers and students in AI safety. All of the projects are either from public sources (e.g. AlignmentForum posts) or posted on the website itself.</p><p>The current website represents the first steps towards an accessible crowdsourced research platform for easier research collaboration and hypothesis testing.</p><h2>The gap in AI safety</h2><h3>Research prioritization &amp; development</h3><p><strong>Research prioritization is hard </strong>and even more so in a pre-paradigmatic field like AI safety. We can grok the highest-karma post on the AlignmentForum but is there another way?</p><p>With AI Safety Ideas, we introduce a <strong>collaborative</strong> way to prioritize and work on specific agendas together through social features. We hope this can become a <strong>scalable research platform for AI safety</strong>.</p><p>Successful examples of less systematized but similar, collaborative, online, and high quality output projects can be seen in Discord servers such as EleutherAI, CarperAI, Stability AI, and Yannic Kilcher\u2019s Discord, in hackathons, and in competitions such as the <a href=\"https://github.com/inverse-scaling/prize\">inverse scaling competition</a>.</p><p>Additionally, we are&nbsp;also <a href=\"https://forum.effectivealtruism.org/posts/EPhDMkovGquHtFq3h/an-experiment-eliciting-relative-estimates-for-open\"><u>missing an empirically driven impact evaluation</u></a> of AI safety projects. With the next steps of development described further down, we hope to make this easier and more available while facilitating more iteration in AI safety research. Systemized hypotheses testing with bounties can help funders directly fund specific results and enables open evaluation of agendas and research projects.</p><h3>Mid-career &amp; student newcomers</h3><p>Novice and entrant participation in AI safety research is mostly present in two forms at the moment: <strong>1) Active or passive</strong> part-time course participation with a capstone project <i>(AGISF, ML Safety)</i> and <strong>2) flying to London or Berkeley</strong> for three months to participate in full-time paid studies and research <i>(MLAB, SERI MATS, PIBBSS, Refine)</i>.</p><p>Both are highly valuable but a third option seems to be missing:&nbsp;<strong>3) An accessible, scalable, low time commitment, open research opportunity</strong>.&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/3gmkrj3khJHndYGNe/estimating-the-current-and-future-number-of-ai-safety\"><u>Very few people work in AI safety</u></a> and allowing decentralized, volunteer or bounty-driven research will allow many more to contribute to this growing field.</p><figure class=\"image image_resized\" style=\"width:55.64%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995001/mirroredImages/DTTADonxnDRoksp4E/i5vm9axhalob7kps0hze.jpg\"><figcaption>Choices oh choices</figcaption></figure><p>By allowing this flexible research opportunity, we can attract people who cannot participate in option (2) because of visa, school / life / work commitments, location, rejection, or funding while we can attract a more senior and active audience compared to option (1).</p><h2>Next steps</h2><figure class=\"table\"><table><tbody><tr><td style=\"padding:5pt;vertical-align:top;width:60px\">Oct</td><td style=\"padding:5pt;vertical-align:top\">Releasing and building up the user base and crowdsourced content. Create an insider build to test beta features. Apply to join the insider build&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSeYqFqxUO_Hq_8nuZv5BqPBuj6-VLr0pOgbOVzD93_vg5oqww/viewform?usp=sf_link\"><u>here</u></a>.</td></tr><tr><td style=\"padding:5pt;vertical-align:top;width:60px\">Nov</td><td style=\"padding:5pt;vertical-align:top\">Implementing&nbsp;<strong>hypothesis testing features</strong>: Creating<strong>&nbsp;</strong>hypotheses, linking ideas and hypotheses, adding negative and positive results to hypotheses. Creating an email notification system.</td></tr><tr><td style=\"padding:5pt;vertical-align:top;width:60px\">Dec</td><td style=\"padding:5pt;vertical-align:top\"><strong>Collaboration features</strong>: Contact others interested in the same idea and mentor ideas. A better commenting system with a&nbsp;<i>results</i> comment that can indicate if the project has been finished or not, what the results are, and by who was it done.</td></tr><tr><td style=\"padding:5pt;vertical-align:top;width:60px\">Jan</td><td style=\"padding:5pt;vertical-align:top\">Adding&nbsp;<strong>moderation features</strong>: Accepting results, moderating hypotheses, admin users. Add bounty features for the hypotheses and a simple user karma system.</td></tr><tr><td style=\"padding:5pt;vertical-align:top;width:60px\">Feb</td><td style=\"padding:5pt;vertical-align:top\"><strong>Share with ML researchers and academics</strong> in EleutherAI and CarperAI. Implement the ability to create&nbsp;<strong>special pages</strong> with specific private and public ideas curated for a specific purpose (title and description included). Will help integrate with local events, e.g. the&nbsp;<a href=\"https://apartresearch.com/jam\"><u>Alignment Jams</u></a>.</td></tr><tr><td style=\"padding:5pt;vertical-align:top;width:60px\">Mar&lt;</td><td style=\"padding:5pt;vertical-align:top\">Allow&nbsp;<strong>editing</strong> and save editing history of hypotheses and ideas. Get DOIs for reviewed hypothesis result pages. Implement the EigenKarma karma system. Implement automatic auditing by NLP.&nbsp;<strong>Monitor the progress</strong> on different clusters of hypotheses and research ideas (research agendas). Release&nbsp;<strong>meta-science research</strong> on the projects that have come out of the platform and the general progress.</td></tr></tbody></table></figure><p>&nbsp;</p><h2>Risks</h2><ol><li>Wrong incentives on the AI Safety ideas platform leads to people working on others\u2019 agendas instead of working on their own inside view.</li><li>AI Safety Ideas does not receive traction and by extension becomes less useful than would be expected.</li><li>Some users who do alignment research without a profound understanding of why alignment is important, discover ideas that have the potential to help AI capabilities, without being worried enough about info hazards to contain them properly.</li><li>Project bounties on the AI Safety Ideas platform will be occupied by capabilities-first agendas and mislead new researchers.</li></ol><h3>Risk mitigation</h3><p>Several of these are not implemented yet but will be as we develop it further.</p><ol><li>Ensure that specific agendas do not get special attention compared to others and implement incentives to work on new or updated projects and hypotheses. Have structured meetings and feedback sessions with leaders in AI safety field-building and conduct regular research about how the platform is used.</li><li>Do regular, live user interviews and ensure giving feedback is quick and easy. We have interviewed 18 until now and have automated feedback monitoring on&nbsp;<a href=\"https://apartresearch.com/join\"><u>our server</u></a>. We will embed a feedback form directly on the website. Evaluate usefulness of features by creating an insider build.</li><li>Restricting themes within AI safety and nudging towards safety thinking in communication. It is also a risk if these capabilities-grokking capable researchers work independently and we might be able to pivot their attitude towards safety by providing this platform.</li><li>Ensure vetting of the ideas and users. Make the purpose and policies of the website very clear. Invite admin users based on AlignmentForum karma with the ability to downvote ideas, leading to hiding it until further evaluation.</li></ol><h2>Feedback</h2><p><a href=\"https://forms.gle/Zgb3qdA9ZN38hH3U6\"><u>Give anonymous feedback on the website here</u></a> or write your feedback in the&nbsp;<strong>comments</strong>. If you end up using the website, we also appreciate your in-depth feedback&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfjylrw3z3fRrrIg3fI8B2x7H2JQrTZcoWmFDMU-TtsxxxTGw/viewform?usp=sf_link\"><u>here</u></a> (2-5 min). If you want any of your ideas removed or rephrased on the website, please send an email to&nbsp;<a href=\"mailto:operations@apartresearch.com\"><u>operations@apartresearch.com</u></a>.</p><p><strong>PS</strong>: It is still very much in alpha and there might be mistakes in the research project descriptions. Please do point out any problems in the \"Report an issue\".</p><h3>Help out</h3><p>The <a href=\"https://github.com/apartresearch/aisafetyideas\">platform is open source</a> and we appreciate any pull requests on the insider branch<a href=\"https://github.com/apartresearch/aisafetyideas.\">.</a> Add any bugs or feature requests on the <a href=\"https://github.com/apartresearch/aisafetyideas/issues\">issues page</a>.&nbsp;</p><p>Apply to join the insider builds&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSeYqFqxUO_Hq_8nuZv5BqPBuj6-VLr0pOgbOVzD93_vg5oqww/viewform?usp=sf_link\"><u>here</u></a> to give feedback for the next versions. <a href=\"https://apartresearch.com/join\">Join our Discord</a> to discuss the development.</p><p><i>Thanks to Plex, Maris Sala, Sabrina Zaki, Nonlinear, Thomas Steinthal, Michael Chen, Aqeel Ali, JJ Hepburn, Nicole Nohemi, and Jamie Bernardi.</i></p>", "user": {"username": "Apart Research"}}, {"_id": "BWWSXLtu6QwBeTxPn", "title": "Hi", "postedAt": "2022-10-17T08:35:56.265Z", "htmlBody": "<p>Hi! I`ll make new friends here</p>", "user": {"username": "Kelly Walker"}}, {"_id": "rmENcc322SRfZ8oA3", "title": "Space", "postedAt": "2022-10-17T06:34:32.860Z", "htmlBody": "", "user": {"username": "Jarred Filmer"}}, {"_id": "juWCs6gyRvsXxPLgt", "title": "A modest case for hope", "postedAt": "2022-10-17T06:03:19.800Z", "htmlBody": "<p><strong>There's a lot to be worried about.</strong> And EAs have a seemingly boundless capacity to worry about the world's most painful challenges. AI risk, the looming specter of climate change, the sharp pangs of animal suffering. Excruciating and pointless diseases. Institutions lacking the capacity to execute on their mandates, or the incentives to enact change quickly enough. Unbridled ego and greed. It's enough to make the most balanced person despair! But here, amidst it all, I want to offer some modest reflections on hope.</p><p>I've been facilitating an <a href=\"https://forum.effectivealtruism.org/posts/NvzeAtoynxGjDnWkp/announcing-the-harvard-ai-safety-team\">AGI Safety fellowship</a> for the past few weeks, and I've rarely been so inspired as I am every time I walk into the meeting room. Our cohort is full of deeply technical engineers and thoughtful scientists; people who have known for a while their commitment to making AI <i>go well</i> and vital newcomers; people who reflect in all aspects the breadth and diversity of talent, background, and thought that in my view grounds the best of human ingenuity.</p><p><strong>And it's this cohort, I think, that gives a convincing case for hope.</strong> In small ways: when they anticipate a reading's argument, or pose a clever question. And in large ways, too: when they dive into a research topic with ardor, or refocus their career on steering lifeboat Earth away from gloomy rocks. To be sure, there's the hard challenges: timelines, core unresolved conceptual gaps. What if it's all for naught? And what right do we have to think we'll meaningfully contribute?</p><p>Yet I keep coming back to the feeling I get when I walk into the conference room each week. \"We can solve this, somehow!\" \"It's possible. And if it's possible, we'll find a way.\" Sometimes in despairing moments, this is a good anchor for me. <strong>It's a reminder that the world will look brighter someday, and that the best of us are arrayed to face the challenge.</strong></p>", "user": {"username": "xavierrg"}}, {"_id": "RyRhT6EQjJgcwYmtd", "title": "Popular Personal Financial Advice versus the Professors (James Choi, NBER)", "postedAt": "2022-10-16T22:21:09.037Z", "htmlBody": "", "user": {"username": "evelynciara"}}, {"_id": "Be89az6nDN37cYuri", "title": "Assistant-professor-ranked AI ethics philosopher job opportunity at Canterbury University, New Zealand", "postedAt": "2022-10-16T17:56:30.753Z", "htmlBody": "<p>Canterbury University, New Zealand seeking assistant-professor-ranked philosopher of AI ethics: <a href=\"https://jobs.canterbury.ac.nz/jobdetails/ajid/uUIS7/Lecturer-in-Philosophy,8123\">https://jobs.canterbury.ac.nz/jobdetails/ajid/uUIS7/Lecturer-in-Philosophy,8123</a></p><p>Note that the position is for a \"Lecturer\", but in the New Zealand academic system, this title is roughly equivalent to the \"Assistant Professor\" rank in the US system.</p><blockquote><p>We invite Lecturer applicants specialising in the ethics of artificial intelligence to join our Department of Philosophy. The successful candidate will: contribute to teaching in a wide range of undergraduate and postgraduate philosophy courses, not limited to ethics and AI; will be eager to promote AI ethics and ethics more generally as subjects across the university, and to teach AI ethics courses to students from diverse backgrounds; will maintain a strong research programme and win external research funding; will supervise postgraduate students; and will participate in departmental and university administration. The appointee will be expected to forge links with and contribute to the wider profession at the local, national and international levels.<strong>&nbsp;</strong></p></blockquote><blockquote><p><strong>The closing date for this position is:&nbsp;Tuesday, 1 November&nbsp;2022 (midnight NZ time)</strong></p></blockquote><p>This position would be suitable for</p><ul><li>a PhD student in AI Alignment or AI Ethics close to graduation with some papers published or</li><li>a PhD graduate in AI Alignment or AI Ethics</li><li>who would like a permanent full-time research and teaching role at a public research university</li><li>Is in or able to relocate to New Zealand</li></ul>", "user": {"username": "ben.smith"}}, {"_id": "u3D8xM5XNgerHpAtc", "title": "My donation budget and fallback donation allocation", "postedAt": "2022-10-16T16:04:26.744Z", "htmlBody": "<h1>My donation budget and fallback donation allocation</h1>\n<p>In this (fairly long!) post, I describe how I'm currently thinking of\nmy donation budget, and how I distinguish between opportunistic,\nhigh-marginal-value donations and fallback donations. If you're\nviewing this on the EA Forum, please use the table of contents (that\nyou should see to the left of the post) to navigate the post and\nunderstand its overall structure.</p>\n<p>Some of this goes over ground I previously tread on in my previous\nposts on <a href=\"https://vipulnaik.com/blog/my-somewhat-unusual-q4-2021-donation/\">my Q4 2021\ndonation</a>\nand <a href=\"https://forum.effectivealtruism.org/posts/xzQzLoEC5mvPyJTYY/my-q1-2022-donation-to-free-migration-project\">my Q1 2022\ndonation</a>. However,\nthis post is fully focused on my overall donation strategy rather than\nsimply describing it in the context of specific donations.</p>\n<p>I've written a few other blog posts in the past few years. Both the\nact of writing the posts and the comments that some of the posts have\nreceived have informed my thinking. These posts are:</p>\n<ul>\n<li><a href=\"https://forum.effectivealtruism.org/posts/4R5f4ntM6t3GLoLTw/donor-strategies-for-separating-how-much-from-where-to\">Donor strategies for separating \"how much\" from \"where\" to donate</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/F4DxPrfmnEtEwhPJu/levels-of-donation\">Levels of donations</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/xvebegCf8NReJc2Lw/what-regrets-have-you-had-regarding-past-donations\">What regrets have you had regarding past donations?</a></li>\n</ul>\n<h2>How I see the value of various things I do</h2>\n<p>In general, I see the main vehicles through which I create value as follows:</p>\n<ol>\n<li>Living my \"regular\" life, including doing my day job, interacting\nwith family and friends, etc.</li>\n<li>Using my time and money for personal projects with some altruistic\nbent, such as the <a href=\"https://donations.vipulnaik.com/\">donations list\nwebsite</a> or <a href=\"https://timelines.issarice.com/\">timelines\nwiki</a></li>\n<li>Using my money for donations.</li>\n</ol>\n<p>I expect that my value-add is roughly (1) &gt; (2) &gt; (3), though I expect\nthat the value-add-per-unit-time may be higher for (2) and (3) than\n(1).</p>\n<p>Given this overall view, I want to set rules on my donations in a way\nthat they don't undercut (1) and (2). This informs the budget that I\nset for donations. I'll talk about the budget in much more detail\nlater in this post.</p>\n<p>Other related things I've written:</p>\n<ul>\n<li>\n<p>In the <a href=\"https://www.lesswrong.com/posts/rBLEiEsNxpQaqwsmJ/miscellaneous-thoughts-on-handling-a-stressful-life#Sources_of_value\">\"Sources of value\" section of my post on handling\nstress</a>,\nI talked about sources of purpose and pleasure. That is a bit more\ndetailed and more about my moment-to-moment experience, but it has\nsome overlap with how I see the overall value of things I do.</p>\n</li>\n<li>\n<p>In a <a href=\"https://thinkingbeyondcompetition.wordpress.com/2011/01/21/a-three-tiered-view-of-ethics-and-singers-dilemma/\">post I wrote on three-tiered\nethics on my old blog</a>,\nI separated out three tiers of ethics: negative rights ethics,\ncontract/responsibility ethics, and excellence ethics. Earlier tiers\nof ethics take precedence over later tiers of ethics; for instance,\nif my contract requires me to kill somebody, then I would rather\nbreak the contract than kill. In my list above, (1) falls under\ncontract/responsibility ethics, whereas (2) and (3) fall mostly\nunder excellence ethics (though to some extent under\ncontract/responsibility ethics to the extent that I've made\ncommitments).</p>\n</li>\n</ul>\n<h2>Savings thresholds and donation budgets</h2>\n<h3>The extreme view of \"save till you no longer need to save, then donate all net savings above the threshold\"</h3>\n<p>One view, that I'm fairly sympathetic to, is the view where you\nbasically switch gears in life between saving and donation. To begin\nwith, your goal is to save enough money to build a nest egg. Once you\nhave achieved that, you switch to donating all your surplus income\n(i.e., income - consumption).</p>\n<p>I feel like this view is, at least in the abstract, more logically\nsound than splitting your money between savings and donation. It's\nmore risk-averse, in the sense that if financial challenges prevent\nyou from hitting your savings goals, you conservatively don't donate\nand save whatever you can. But it's also generous beyond the point of\nhitting your savings goals.</p>\n<p>I don't ultimately go with this extreme view, for various reasons I\narticulate, but I think it should be something for people to consider.</p>\n<h3>The other extreme view of treating donations as a fixed proportion of income or savings</h3>\n<p>In this view, you have a fixed rule around what proportion of your\nincome or savings you donate periodically (for instance, every\nyear). I talked about some of the predetermined rules people use for\nsuch things in my post <a href=\"https://forum.effectivealtruism.org/posts/4R5f4ntM6t3GLoLTw/donor-strategies-for-separating-how-much-from-where-to\">Donor strategies for separating \"how much\"\nfrom \"where\" to\ndonate</a>.</p>\n<p>I think this is a reasonably good strategy with its own advantages,\nbut it's already relatively well-known so I won't say much more.</p>\n<h3>Finding a middle ground of sorts</h3>\n<p>Overall, I've picked a middle ground that combines some features from\nboth extremes. I have in mind a few different savings thresholds:</p>\n<ol>\n<li>\n<p><strong>Emergency savings threshold</strong>: This corresponds to something like\n6-12 months of expenses. Before hitting this savings threshold, I\nwould generally not donate (though, historically, I <a href=\"https://vipulnaik.com/donation-history/\">did\ndonate</a> while below the\nthreshold, I think this may have been the wrong choice as I did\nface a shortfall of cash that was partly caused by the donations).</p>\n</li>\n<li>\n<p><strong>Money-won't-be-a-bottleneck-to-life-changes threshold</strong>:\nBasically this is a threshold of saving at which I can make changes\nto my life without being constrained on cash. For instance, I can\nfinance a move to a new city, or make a downpayment on a house if I\ndetermine that buying is better in the long run than renting, or\nquit my job and try something out for a year or two. I don't have\nan exact threshold here but something like 2-5 years of savings\nseems about right for this.</p>\n</li>\n<li>\n<p><strong>I-have-enough-money-for-life threshold</strong>: This is a threshold\nwhere I basically have enough money to finance the rest of my\nforeseeable life, after making appropriate assumptions about\ninterest rates. This is the most uncertain, particularly because it\nrequires making predictions about the nature of the medium-term\nfuture. But if we don't believe in short timelines for very\ndisruptive changes to the world, I think something like 10-30 times\ncurrent annual spend is probably enough for this. The presence of\nvarious safety nets and cost-cutting measures can affect the exact\ncalculation here.</p>\n</li>\n</ol>\n<p>My current situation is that I'm clearly past 1 and moderately past 2,\nbut still well short of 3. When I was below 2, making donations traded\noff against having the flexibility to make life changes in a\nrelatively tangible way, and I was therefore extremely selective about\nmaking donations.</p>\n<p>Now that I'm past 2, making donations (that are small enough that I\n<em>stay</em> past 2) doesn't really trade off against life changes.</p>\n<p>However, I'm still well short of 3. If I were above 3, I would likely\nadopt the extreme position of \"donate all net savings\" at least in\nprinciple (though various practical considerations may still require\nme to hold money while I figure out good donation targets). But I'm\nnot above 3, which means that I'm still trying to grow my savings. The\nquestion then is: what sort of rule do I use to determine the amount\nof money I can donate?</p>\n<h3>Haste considerations</h3>\n<p>The main reason I want to donate now rather than wait till I'm at or\nabove 3 is haste considerations: I think there are a lot of compelling\nopportunities that exist right now. Crudely, at least for small\namounts of donations, my sense is that the social rate of return to\nimproving the world right now is greater than the private rate of\nreturn (in terms of interest accrued, as well as money-as-cushion) of\nkeeping it.</p>\n<h3>Picking a donation budget</h3>\n<p>As I documented in <a href=\"https://vipulnaik.com/blog/my-somewhat-unusual-q4-2021-donation/\">my Q4 2021 donation\npost</a>,\nI set a donation target of $1,000 per month starting July 2021. The\nbudget is cumulative, and I don't have to spend it, so whatever I\ndon't spend accrues, allowing me to save within the budget for bigger\ndonations.</p>\n<p>As of October 2022, I've been in this regime for 16 months,\naccumulating a budget of $16,000. I've spent $4,000 so far, so I have\n$12,000 to donate.</p>\n<p>How did I pick this donation budget? Essentially, I wanted to pick a\nvalue that was both big enough to be worth the overhead of thinking\nabout it, and small enough that it didn't interfere with my regular\nlife or personal projects, which I believe to ultimately be more\nvaluable.</p>\n<h4>The \"not too big\" angle</h4>\n<ul>\n<li>\n<p>What kind of variation in net savings would I consider small enough\nto not worry about? If my donation budget is within that range of\nvariation, it's probably okay.</p>\n</li>\n<li>\n<p>What kind of variation in after-tax income would likely not be\ndecision-relevant to choosing to join or stay in a job? If my\ndonation budget is within that range of variation, it's probably\nokay.</p>\n</li>\n</ul>\n<h4>The \"not too small\" angle</h4>\n<p>I wanted a donation budget that feels substantive and gives me \"room\nto play\" with making serious donations. I thought about this from a few angles:</p>\n<ul>\n<li>\n<p>How does it compare with my other budget line items? At $1,000 per\nmonth, or $12,000 per year, my donation budget is only exceeded by\ntwo line items: rent and <a href=\"https://contractwork.vipulnaik.com\">contract\nwork</a>. My donation budget is in\nexcess of my other nontrivial line items such as food, health\ninsurance, web hosting, and health care.</p>\n</li>\n<li>\n<p>What <a href=\"https://forum.effectivealtruism.org/posts/F4DxPrfmnEtEwhPJu/levels-of-donation\">level of\ndonation</a>\ndoes it place me at? Assuming I donate about once a year, it puts me\nat level 2, which is a high enough level that I can do some\nmeaningful investigation for each donation, and where there starts\nbeing a meaningful probability of meeting highly specific funding\ngaps.</p>\n</li>\n</ul>\n<h3>How should the donation budget evolve over time?</h3>\n<p>If my savings continue to increase, slowly and steadily, should I\nincrease the proportion of money I allocate to my donation budget,\nthereby slowing down the rate at which I accumulate money? I'm fairly\nunsure about the right approach here. However, for reasons of\nsimplicity, I've decided against making <em>continuous</em> changes to the\nrate at which I add to the donation budget. In other words, I'll\ncontinue to add $1,000/month to the donation budget for a while, until\nI revisit things (either due to life changes or due to a reassessment\nof where I am with respect to savings).</p>\n<h3>Windfall profits and the donation budget</h3>\n<p>My income and spending rate define the backdrop of my donation budget\ndecision. What if I have a windfall profit? This is not merely a\ntheoretical question -- I've had significant fluctuations in income\ndue to cryptocurrency price changes, and I expect to make a sudden\namount of money as a result of exercising and selling stock options at\nthe tech company I recently left.</p>\n<p>My current thinking is to not let windfall profits affect the donation\nbudget, at least for profits that comprise less than one additional\nyear of net saving.</p>\n<h3>Borrowing against future donation budget</h3>\n<p>What if I find a truly compelling and time-sensitive opportunity, and\nI want to donate more to it than my donation budget allows? In extreme\ncases, I'm open to the idea of <em>borrowing against my future donation\nbudget</em>. I consider this fairly unlikely in the near term, partly\nbecause I've already accumulated a fair amount in the donation budget.</p>\n<h2>Minimum donation targets and fallback donations</h2>\n<p>When I initially picked a donation budget, it only set an upper bound,\nwith no expectation that I'd actually make donations at any specific\nrate. At the time, the concept of a donation budget was relatively\nnew, and I wasn't sure what sort of donation opportunities I would be\ninterested in.</p>\n<p>However, now (October 2022) I want to also set some rule to make sure\nI do spend at least some of my donation budget, while still allowing\nfor accumulation to finance potentially larger donations in the\nfuture.</p>\n<h3>The original concept of fallback donation targets</h3>\n<p>In <a href=\"https://vipulnaik.com/blog/my-somewhat-unusual-q4-2021-donation/\">my Q4 2021 donation\npost</a>,\nI talked about fallback donation targets. I wrote:</p>\n<blockquote>\n<p>There are a number of \"fallback\" donation targets including the EA\nFunds and the GiveWell Maximum Impact Fund. If I had enough savings\nthat I was actively looking to give it away (and here I am thinking of\nsomething in the range of millions of dollars, which would be enough\nto last me a lifetime and then some), my strategy would be something\nlike this:</p>\n<ol>\n<li>Proactively investigate donation opportunities that are\nlikely to beat the fallback donation targets.</li>\n<li>After exhausting those (either not finding any within a\npredetermined time limit, or meeting their funding gaps), donate\nthe rest of the money to the fallback donation targets.</li>\n</ol>\n<p>I am currently <em>not</em> at the level of savings where I'm actively\nlooking to give it away. So, in particular, I won't do step 2; I will\nlimit my donation efforts to step 1. And even within step 1, my\ninvestigation's \"proactive\" nature will be limited, because giving\nmoney away is not a priority. Moreover, I will give more priority to\nrelatively time-sensitive donation opportunities, and not focus on\nother donation opportunities that I can revisit later when I have more\nmoney and more information. So it really boils down to just:</p>\n<blockquote>\n<p>Investigate time-sensitive donation opportunities that are likely to\nbeat the fallback donation targets, and make donations to them if I\nam moderately convinced of them.</p>\n</blockquote>\n</blockquote>\n<p>So far, I've mostly followed the above, and the only two donations\nI've made since adopting a donation budget have been to things that I\ndeemed time-sensitive and that, at least at the margin at which I\ndonated, and based on my values and the information available to me at\nthe time of donation, seemed in my mind to beat fallback donation\ntargets.</p>\n<h3>Downward update on existence of compelling and time-sensitive opportunities and my time availability to investigate them</h3>\n<p>One update over the last year has been in terms of the existence of\ncompelling and time-sensitive donation opportunities, as well as my\nability to investigate them to a level of depth where I feel\ncomfortable donating to them. Specifically, my rough impression now is\nsomething like this:</p>\n<ul>\n<li>\n<p>There aren't that many compelling and time-sensitive opportunities\nthat I am well-positioned to fund. Existing funding infrastructure\nfrom other donors, who have both more funds and more investigative\nresources, seems to cover many of these opportunities. And even if\nthere's an opportunity that I initially find compelling, the fact\nthat these other donors are passing on the opportunity would likely\nmake me want to investigate it more.</p>\n</li>\n<li>\n<p>My time availability to investigate opportunities is not reliably\nhigh (and in particular it may not be high during the windows when\nthe time-sensitivity exists). My current estimate is that I most\nlikely don't expect to have enough confidence to allocate more than\n$5,000 per year to compelling and time-sensitive opportunities, and\nI might very well allocate much less.</p>\n</li>\n</ul>\n<h3>Resultant desire to \"get some money out the door\" by donating to fallback donation targets</h3>\n<p>Given the above factors, I've been leaning more toward making\ndonations to fallback donation targets (such as the EA Funds or the\nGiveWell Maximum Impact Fund). My listed fallback donation targets all\nhave several (related) advantages:</p>\n<ul>\n<li>\n<p>They are regrantors rather than grant endpoints, and they have a\nreasonably thoughtful and rigorous approach to identifying endpoint\ndonees. This derisks things quite a bit; in general, I expect them\nto have pretty large room for more funding and to make reasonably\ngood decisions.</p>\n</li>\n<li>\n<p>They are more legible and therefore give me a stronger sense of\nhaving \"done good\" with my donation, as well as communicate to\nothers more clearly that I'm doing good. They offer a credible\nsignal of my \"effective altruist\" sympathies.</p>\n</li>\n<li>\n<p>My incremental investigative effort is limited to choosing between\nvery high-level things, rather than getting into specific\noperational questions. To wit, it's something like, what matters\nmore at the margin: directing funding to animal welfare, directing\nfunding to the most cost-effective programs in global health, or\ndirecting funding to AI safety / biorisk / longtermist areas? While\nthis is not a super-easy choice (I think all of these have merits)\nit is something where I can give my current best guess based on\nfairly broad considerations. It therefore needs less investigative\neffort. And even if I choose something and change my mind later, the\n\"damage done\" seems relatively small.</p>\n</li>\n</ul>\n<p>A more general point is that, based on responses to my <a href=\"https://forum.effectivealtruism.org/posts/xvebegCf8NReJc2Lw/what-regrets-have-you-had-regarding-past-donations\">regrets\npost</a>,\nit seems that people (in the reference class that I belong to) don't\ngenerally have significant regrets about having donated too much or\nleft too little money for other donations or non-charitable\nuses. Obviously, I want to build in the right set of safeguards for my\nown use case, but I should probably expect <em>less</em> regret from my\nfuture self than my current self feels in anticipation.</p>\n<h3>Trading off between having money available for compelling and time-sensitive opportunities versus getting money out the door</h3>\n<p>The trade-off feels real to me: do I keep more money (within my\ndonation budget) for potential compelling opportunities, or do I get\nmoney out the door?</p>\n<p>Overall, I think that <em>if</em> I were able to find compelling and\ntime-sensitive opportunities that use up my donation budget, I'm happy\nto spend on those and not donate to fallback donation targets <em>at all</em>\n(even though this does have some psychic costs that you can infer by\nreversing the benefits of fallback donation targets listed\nabove). This is primarily an intuition that thoughtful iconoclasm is\nmore valuable than thoughtlessly following the herd, even though\niconoclasm is more high-variance.</p>\n<p>However, I don't expect to be able to find enough compelling and\ntime-sensitive opportunities that use up my donation budget, and I\nwant to get money out the door soon. So how do I square this?</p>\n<h3>My solution for now: minimum fallback donation target assuming enough donation budget</h3>\n<p>The idea I had is to do a periodic (e.g., end-of-year) evaluation as\nfollows. If by the end of the year, I have $10,000 or more unallocated\nin my donation budget, I will allocate $5,000 to a single fallback\ndonation target. The target can vary by year, but in any given year it\nshould represent my best judgment among the pool of options (the EA\nFunds and the GiveWell Maximum Impact Fund).</p>\n<p>Here's how I justify this solution.</p>\n<h4>\"Small enough\" for the donation target</h4>\n<p>All the listed fallback donation targets have annual spends ranging\nfrom the hundreds of thousands to hundreds of millions. Based on their\nprocesses, it seems that none of them would see any meaningful changes\nin their marginal value function with a $5,000 donation. So, I don't\nhave to worry too much about donation dynamics and room for more\nfunding.</p>\n<h4>\"Big enough\" for the donation target</h4>\n<p>$5,000 is big enough for the donation target that I don't feel like\nI'm wasting their time. It falls at level 2 in my <a href=\"https://forum.effectivealtruism.org/posts/F4DxPrfmnEtEwhPJu/levels-of-donation\">levels of donation\npost</a>,\nwhich is enough for the donee to collaborate with me on administrative\nmatters, and for the donation to very clearly be worth more than any\nadministrative overhead incurred by either me or the donee.</p>\n<h4>\"Small enough\" for me</h4>\n<p>There are two aspects of this:</p>\n<ul>\n<li>\n<p>Since this allocation is done only <em>if</em> I have at least $10,000\naccrued in my donation budget, it pretty much never causes me to run\nout of money to donate for compelling and time-sensitive\nopportunities. In the worst case, I'll end up having $5,000 left\nover in the budget after this donation. Given the rate at which I've\nbeen finding compelling and time-sensitive opportunities so far,\nthis should be more than enough.</p>\n</li>\n<li>\n<p>It's pretty much guaranteed that the fallback donation targets won't\naccount for more than 5/12 of my total donation budget, which seems\naligned with my overall intuition around how to drive the most\nvalue. And as I already articulated, my donation budget as a whole\nis small enough as to not interfere with the other things I want to\ndo in life.</p>\n</li>\n</ul>\n<h4>\"Big enough\" for me</h4>\n<p>$5,000 per year is comparable to my annual expense on food. It's\ncomparable to my annual expense on health insurance, and exceeds my\nannual expense in most categories (rent and <a href=\"https://contractwork.vipulnaik.com/\">contract\nwork</a> are the only categories\nwhere I spend more). As such, it's a sizable amount that reflects the\nimportance of donations as a way to do good.</p>\n<h3>Donation bunching</h3>\n<p>I might want to engage in <a href=\"https://www.benkuhn.net/bunching/\">donation bunching for tax\nconsiderations</a>. For this reason,\nif I haven't made any other donations at all in a given tax year, I\nmight choose to defer the fallback donation to the beginning of the\nnext tax year, so there's more potential for bunching. In my current\njurisdiction, the tax year matches the calendar year, so basically\nthis would mean donating in January of the next year.</p>\n<h3>When should I start applying fallback donation targets?</h3>\n<p>I'm currently undecided between whether to start applying fallback\ndonation targets starting with end-of-2022 or wait till end-of-2023.</p>\n<h4>The argument for waiting till 2023</h4>\n<p>I had originally planned to spend a significant amount of time in 2022\non the <a href=\"https://donations.vipulnaik.com/\">donations list website</a> as\nwell as other projects that would give me more clarity on what's going\non in the philanthropy space. I expected this to inform my worldview,\nboth in terms of finding compelling and time-sensitive donation\nopportunities and in terms of choosing more wisely between fallback\ndonation targets.</p>\n<p>However, 2022 was a year of several other changes for me, including a\n<a href=\"https://www.facebook.com/vipulnaik.r/posts/pfbid0dgmmVwD9inUHA9TYCQn84vtaCGn5Cvk8HAmVuWekGFzpaTh2UoohXjWjTxjvRdTcl\">job\nchange</a>\nand moving house. In addition to job transition, I'm also spending\ntime this year on some long-overdue devops work related to the servers\nhosting my websites, and my current plan is to <a href=\"https://github.com/vipulnaik/daily-updates/blob/master/standing-plan-for-the-next-few-months.md\">focus on this devops\nwork for the rest of\n2022</a>. So\nI don't expect to spend time on the donations list website until 2023,\nor to spend significant time thinking about where to donate (beyond\nthe time spent writing this blog post and possibly a successor blog post).</p>\n<p>Given these considerations, if I wait till the end of 2023, I'll have\nmore information and I'll have had more time to explore. I'll also\nhave more money to play with, and various parts of the ecosystem would\nhave matured more (for instance, there will be more clarity on the\nrole that the FTX Future Fund will play). All of these make the case\nfor waiting till the end of 2023.</p>\n<h4>The argument for starting in 2022</h4>\n<p>There are a few reasons I want to start in 2022:</p>\n<ul>\n<li>\n<p>In general, the argument for doing this is also an argument for\ndoing this <em>now</em>. Much of the argument is not sensitive to the exact\nstart date, and it makes sense to start sooner rather than later.</p>\n</li>\n<li>\n<p>I already made a donation in January of 2022 that just about gets me\npast the standard deduction, so I get donation bunching benefits by\nstarting in 2022.</p>\n</li>\n<li>\n<p>I expect to make a windfall of money related to exercising and\nselling stock options at the tech company I left. While this isn't\nsignificant enough to change my donation budget (for reasons\ndiscussed earlier) it does seem like it'll address any liquid cash\nconsiderations that arise from starting in 2022. It's well in excess\n(even after taxes) of the $5,000 that I might spend this year if I\nstarted this year.</p>\n</li>\n</ul>\n<h4>Overall leaning</h4>\n<p>Overall, I'm leaning toward starting in 2022, but I plan to continue\nto think about this through October and November to see if other\nconsiderations come up that I hadn't thought of before.</p>\n<h2>Selecting between fallback donation targets</h2>\n<p>I am not making a decision right now, but when I do, I will likely\npick between one of the four EA Funds:</p>\n<ul>\n<li><a href=\"https://funds.effectivealtruism.org/funds/global-development\">Global Health and Development Fund</a></li>\n<li><a href=\"https://funds.effectivealtruism.org/funds/animal-welfare\">Animal Welfare Fund</a></li>\n<li><a href=\"https://funds.effectivealtruism.org/funds/far-future\">Long-Term Future Fund</a></li>\n<li><a href=\"https://funds.effectivealtruism.org/funds/ea-community\">EA Infrastructure Fund</a></li>\n</ul>\n<p>While I was initially thinking of the GiveWell Maximum Impact Fund /\nGiveWell Top Charities Fund, it seems like the Global Health and\nDevelopment Fund is strictly better, to the extent that it rolls money\nto GiveWell top charities if it doesn't find better opportunities.</p>\n<h3>I would have liked to have the donations list website up-to-date before this analysis!</h3>\n<p>It would have been good if the <a href=\"https://donations.vipulnaik.com/\">donations list\nwebsite</a> included up-to-date\ninformation on the EA Funds before I did this analysis, so I could\nreview considerations more comprehensively. For now, I'm relying on\nthe fund pages on the EA Funds site (as linked above).</p>\n<h3>Current preference order</h3>\n<p>My current weakly held preference order is something like this:</p>\n<p>Animal Welfare Fund &gt; Long-Term Future Fund &gt; EA Infrastructure Fund &gt;&gt; Global Health and Development Fund</p>\n<h3>Why I rank global health and development last: not neglected!</h3>\n<p>The Global Health and Development Fund seems to be fairly well-funded;\nfor instance, in 2022 the fund has already allocated a little over\n$10 million.</p>\n<p>Moreover, it funges with GiveWell's funds, that raise tens of millions\nof dollars a year. The purpose of the Global Health and Development\nFund is to allocate money to high-risk, speculative things in global\nhealth and development first, and then allocate the remaining money to\nGiveWell top charities. But it looks like the 95%+ of the 2022\nallocation so far has been to GiveWell top charities. The situation\nwas a bit different in 2021, but overall it looks like, at least based\non the most recent information, the fund doesn't have a consistent and\ncompelling stream of opportunities different from GiveWell top\ncharities.</p>\n<p>Overall, I think GiveWell top charities present a moderately\ncompelling case, but this cuts both ways: the case is compelling to a\nwider range of people. In general, helping \"humans\" \"now\" seems like\nan easier sell than other things, and GiveWell has created the\n(probably correct) perception of having identified rigorous programs\nthat deliver on the promise of cost-effectively helping humans\nnow. GiveWell has both historical success and the prospect of raising\nmore money. Open Philanthropy alone has been <a href=\"https://www.openphilanthropy.org/research/update-on-our-planned-allocation-to-givewells-recommended-charities-in-2022/\">upping its\nallocation</a>\nto GiveWell top charities.</p>\n<p>While I do think that what GiveWell is doing is very valuable, it\nisn't neglected relative to the other more niche cause areas covered\nby the other funds, and given my set of values, it is pretty unlikely\nto be close to the most cost-effective.</p>\n<h3>Comparing animal welfare, the long-term future, and EA infrastructure</h3>\n<p>I think animal welfare matters, and I think the long-term future\nmatters. I think society as a whole, and probably even the effective\naltruist penumbra (people familiar with EA-ish stuff in general)\nprobably undervalue both these areas relative to their importance. I\nthink that the EA core has zeroed in effectively on these areas, since\nat least <a href=\"https://forum.effectivealtruism.org/posts/pfbLKnJmDKYSPPCEW/four-focus-areas-of-effective-altruism\">Luke's 2013\npost</a>\nthat identified the four focus areas that would eventually each get\ntheir own fund. And through what might seem to be circular reasoning,\nEA's ability to identify animal welfare and the long-term future\nitself vindicates EA to some extent, making EA infrastructure valuable\nto invest in.</p>\n<p>All three areas seem important and neglected. All of them have\nchallenges with measuring tractability. My current thinking is as\nfollows:</p>\n<ul>\n<li>\n<p>The long-term future, viewed abstractly, is probably the most\n<em>important</em>. Specific things like AI safety and biorisk also seem\nfairly important, even if you only partly buy the cases made by\npeople focusing on those areas.</p>\n</li>\n<li>\n<p>Animal welfare seems the most <em>neglected</em>, though it's hard to say\n(again, I was hoping to do more work on the donations list website\nin a way that would inform this).</p>\n<ul>\n<li>\n<p>The existence of players such as FTX Future Fund and the Survival\nand Flourishing Fund, that focus mostly on the long-term future\nand somewhat on EA infrastructure, but basically don't spend at\nall on animal welfare, partly informs this view.</p>\n</li>\n<li>\n<p>Even among the EA Funds, the Animal Welfare Fund seems to have the\nsmallest budget.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Animal welfare seems to naively be the most tractable, but it's not\nvery clear to me how tractable it is at the margin. Here,\nspecifically, the activities of the Animal Welfare Fund are\ninformative: it's funded several small animal welfare organizations\naround the world, some of which have very small budgets, and might\nbe playing an important role in nurturing the growth of animal\nwelfare movements around the world.</p>\n</li>\n</ul>\n<p>Given all these considerations, I'm leaning toward treating animal\nwelfare as the most high-value area to donate to. However, in terms of\npersonal focus on the non-donation front (e.g., in terms of what to\ntalk to people about, or what to read/learn about), I think stuff\nrelated to the long-term future, global catastrophic risks, and in\nparticular AI safety and biosecurity seem more important. I also think\nthat if the FTX Future Fund and the Survival and Flourishing Fund\n<em>didn't</em> exist, I might well have gone with the Long-Term Future Fund\nas my top choice.</p>\n<p>I have more thoughts on this, including thoughts on how to justify\nspending resources on anything other than AI safety if imminent AI\ntimelines are plausible. But I want to mull these further and make a\nfinal decision as of the time I actually want to allocate the $5,000.</p>\n<h2>How I plan to learn and improve</h2>\n<p>Regardless of whether I start the process of making fallback donations\nin 2022 or wait till 2023 to do so, I'm hoping to be in a better\nposition in the coming years. Some of the ways I plan to improve my\nepistemics:</p>\n<ul>\n<li>\n<p>I'll spend substantial effort on the <a href=\"https://donations.vipulnaik.com/\">donations list\nwebsite</a> in 2023, to a point where\nit can usefully inform my donation decisions and my understanding of\nthe overall landscape.</p>\n</li>\n<li>\n<p>I'll continue to be involved (through financial support and\nguidance) in a number of other projects such as <a href=\"https://orgwatch.issarice.com/\">Org\nWatch</a> and <a href=\"https://timelines.issarice.com/wiki/Main_Page\">timelines\nwiki</a> that each, in\ndifferent ways, helps illuminate the landscape.</p>\n</li>\n<li>\n<p>I'll be following what's going on in places like Metaculus, the EA\nForum, various EA-related podcasts, as well as other high-quality\nsources of discussion, to continue to build and improve my models of\nthe world.</p>\n</li>\n</ul>\n<p>While I hope that all these will help drive better donation decisions\nfor my personal donations, I see them as relevant in a larger sense:\nthey help improve my overall model of the world, and at least in some\ncases (such as the ones where I'm working on public websites or\ndocumenting my thoughts in blog posts), could help improve others'\nmodel of the world as well.</p>\n<h2>Seeking your thoughts</h2>\n<p>I wrote this post mostly to record my own thoughts rather than to make\na particular point for other people. But if you were patient enough to\nread enough of the post, I appreciate it and hope you found something\ninteresting enough to be worth your while!</p>\n<p>If you have thoughts on any parts of the post, I would appreciate\nthem. These parts in particular are ones where I'm most interested in\nhearing thoughts, but don't limit yourself to just these!</p>\n<ul>\n<li>\n<p>What do you think about my solution in terms of conditionally\nallocating a portion of my donation budget to fallback donation\ntargets? Would you recommend a different approach that has most of\nthe advantages of my approach, but that is even better on at least\nsome counts?</p>\n</li>\n<li>\n<p>Are there crucial considerations I seem to be missing regarding how\nto decide between the EA Funds? I say \"seem to be\" because there's\nprobably stuff in my head I didn't write down -- so I may not\nactually be missing them -- but I would still love to hear what you\nthink is missing in the articulated portion of my thinking.</p>\n</li>\n<li>\n<p>Any thoughts on how soon I should get started with making donations\nto fallback donation targets? Should I start in 2022, or wait till\n2023 when I expect to have built a more substantive picture?</p>\n</li>\n</ul>\n", "user": {"username": "vipulnaik"}}, {"_id": "xJWorgiMBPhRL7eBo", "title": "Population with high IQ predicts real GDP better than population", "postedAt": "2022-10-17T07:22:53.403Z", "htmlBody": "<h1>Summary</h1><ul><li>I estimate there is a correlation of 85.4 % across countries in 2019 between the <a href=\"https://en.wikipedia.org/wiki/Real_gross_domestic_product\"><u>real GDP</u></a>&nbsp;and population with <a href=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Intelligence_quotient&amp;sa=D&amp;source=editors&amp;ust=1665837155678404&amp;usg=AOvVaw3a3xAPinyL2RH_jjMdqqus\"><u>IQ</u></a>&nbsp;higher than the global mean. This is stronger than the correlation of 78.4 % between the real GDP and total population.</li><li>Among the 20 countries with the largest real GDP, the population with IQ higher than the global mean will increase the most for India (53.5 M), the United States (39.4 M), and Indonesia (15.7 M).</li></ul><h2>Acknowledgements</h2><p>Thanks to <a href=\"https://forum.effectivealtruism.org/users/alsilverback\"><u>alsilverback</u></a>&nbsp;from <a href=\"https://batonics.com/\"><u>Batonics AB</u></a>.</p><h1>Methods</h1><p>I determined the population with an IQ above a given <a href=\"https://en.wikipedia.org/wiki/Standard_score\"><u>z-score</u></a>&nbsp;of the global distribution assuming (see tab \u201cPopulation by IQ\u201d of <a href=\"https://docs.google.com/spreadsheets/d/1ExnHTk96SIMYdoKKVdb41zvAyFM2LFY9ER5SMGlPD_w/edit?usp=sharing\"><u>this</u></a>&nbsp;Sheet):</p><ul><li>2019 population <a href=\"https://ourworldindata.org/grapher/population-past-future?time=1700..latest\"><u>data</u></a>&nbsp;by country from Our World in Data (see tab \u201cPopulation\u201d).</li><li>The mean IQ globally and of each country from Table 16 of <a href=\"https://www.ulsterinstitute.org/ebook/THE%20INTELLIGENCE%20OF%20NATIONS%20-%20Richard%20Lynn,%20David%20Becker.pdf\"><u>Lynn 2019</u></a>, and <a href=\"https://en.wikipedia.org/wiki/Coefficient_of_variation\"><u>coefficient of variation</u></a>&nbsp;of 15 %<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref36brakxwqmj\"><sup><a href=\"#fn36brakxwqmj\">[1]</a></sup></span>&nbsp;(see tabs \u201cGlobal IQ\u201d and \u201cIQ by country\u201d).</li></ul><p>To my knowledge, <a href=\"https://www.google.com/url?q=https://www.ulsterinstitute.org/ebook/THE%2520INTELLIGENCE%2520OF%2520NATIONS%2520-%2520Richard%2520Lynn,%2520David%2520Becker.pdf&amp;sa=D&amp;source=editors&amp;ust=1665837155682075&amp;usg=AOvVaw3u53aPIgeGpdLP03ie9J66\"><u>Lynn 2019</u></a>&nbsp;is the best source of IQ data by country, but it arguably does not differ much from other good sources. The correlation coefficient for the relationship between the mean IQ by country from <a href=\"https://www.google.com/url?q=https://www.ulsterinstitute.org/ebook/THE%2520INTELLIGENCE%2520OF%2520NATIONS%2520-%2520Richard%2520Lynn,%2520David%2520Becker.pdf&amp;sa=D&amp;source=editors&amp;ust=1665837155682563&amp;usg=AOvVaw344LzRJw_iKt_teXC0_gl_\"><u>Lynn 2019</u></a>&nbsp;and <a href=\"https://www.worlddata.info/iq-by-country.php\"><u>WordData</u></a>&nbsp;is 88.7 %<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5m4ypw70rmj\"><sup><a href=\"#fn5m4ypw70rmj\">[2]</a></sup></span>&nbsp;(see M2 of tab \u201cIQ by country\u201d).</p><p>I selected <a href=\"https://data.worldbank.org/indicator/NY.GDP.MKTP.PP.KD\"><u>data</u></a>&nbsp;from The World Bank for the total and per capita real GDP in 2019 by country (see tab \u201cReal GDP\u201d).</p><h1>Results and discussion</h1><p>The table below contains the correlation coefficients for the relationships across countries in 2019 between the real GDP and the real GDP per capita, population, mean IQ, total IQ<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh5l6niomxc\"><sup><a href=\"#fnh5l6niomxc\">[3]</a></sup></span>, population&nbsp;with an IQ higher than various z-scores of the global distribution, and total IQ of top half.</p><figure class=\"table\"><table><thead><tr><th colspan=\"2\">Relationship between the real GDP in 2019 and\u2026</th><th style=\"text-align:center\"><strong>Correlation coefficient (R) (%)</strong></th></tr></thead><tbody><tr><td colspan=\"2\"><strong>Real GDP per capita in 2019</strong></td><td style=\"text-align:center\">13.1</td></tr><tr><td colspan=\"2\"><strong>Population in 2019</strong></td><td style=\"text-align:center\">78.4</td></tr><tr><td colspan=\"2\"><strong>Mean IQ</strong></td><td style=\"text-align:center\">24.3</td></tr><tr><td colspan=\"2\"><strong>Total IQ in 2019</strong></td><td style=\"text-align:center\">82.7</td></tr><tr><td rowspan=\"7\"><strong>Population in 2019 with an IQ higher than\u2026</strong></td><td><strong>3 standard deviations below the global mean</strong></td><td style=\"text-align:center\">78.5</td></tr><tr><td><strong>2 standard deviations below the global mean</strong></td><td style=\"text-align:center\">80.0</td></tr><tr><td><strong>1 standard deviation below the global mean</strong></td><td style=\"text-align:center\">84.9</td></tr><tr><td><strong>The global mean</strong></td><td style=\"text-align:center\">85.4</td></tr><tr><td><strong>1 standard deviation above the global mean</strong></td><td style=\"text-align:center\">80.0</td></tr><tr><td><strong>2 standard deviations above the global mean</strong></td><td style=\"text-align:center\">76.1</td></tr><tr><td><strong>3 standard deviations above the global mean</strong></td><td style=\"text-align:center\">73.3</td></tr><tr><td colspan=\"2\"><strong>Total IQ of top half</strong></td><td style=\"text-align:center\">84.0</td></tr></tbody></table></figure><p>The population with IQ higher than the global mean is the strongest predictor of the real GDP (R = 85.4 %), better than the population alone (R = 78.4 %). Consequently, I think it may be interesting to analyse how the former will evolve. The following table presents, for the 20 countries with the largest real GDP<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsqot5tu1qm\"><sup><a href=\"#fnsqot5tu1qm\">[4]</a></sup></span>, the absolute and relative variation from 2019 to 2050 of the population with IQ higher than the global mean, assuming the same parameters from <a href=\"https://www.google.com/url?q=https://www.ulsterinstitute.org/ebook/THE%2520INTELLIGENCE%2520OF%2520NATIONS%2520-%2520Richard%2520Lynn,%2520David%2520Becker.pdf&amp;sa=D&amp;source=editors&amp;ust=1665837155697030&amp;usg=AOvVaw2hu-E_jbzIoxl86RZMszWW\"><u>Lynn 2019</u></a>&nbsp;for the IQ distributions.</p><figure class=\"table\"><table><thead><tr><th rowspan=\"2\">Country<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqski5azkv4\"><sup><a href=\"#fnqski5azkv4\">[5]</a></sup></span></th><th style=\"text-align:center\" colspan=\"2\">Variation from 2019 to 2050 in the population with IQ higher than the global mean</th></tr><tr><th style=\"text-align:center\">Absolute (M)</th><th style=\"text-align:center\">Relative (%)</th></tr></thead><tbody><tr><td><strong>China</strong></td><td style=\"text-align:center\">-27.5</td><td style=\"text-align:center\">-1.92</td></tr><tr><td><strong>United States</strong></td><td style=\"text-align:center\">39.4</td><td style=\"text-align:center\">12.0</td></tr><tr><td><strong>India</strong></td><td style=\"text-align:center\">53.5</td><td style=\"text-align:center\">3.92</td></tr><tr><td><strong>Japan</strong></td><td style=\"text-align:center\">-18.9</td><td style=\"text-align:center\">-14.9</td></tr><tr><td><strong>Germany</strong></td><td style=\"text-align:center\">-2.85</td><td style=\"text-align:center\">-3.41</td></tr><tr><td><strong>Russia</strong></td><td style=\"text-align:center\">-7.65</td><td style=\"text-align:center\">-5.25</td></tr><tr><td><strong>Indonesia</strong></td><td style=\"text-align:center\">15.7</td><td style=\"text-align:center\">5.82</td></tr><tr><td><strong>United Kingdom</strong></td><td style=\"text-align:center\">5.31</td><td style=\"text-align:center\">7.87</td></tr><tr><td><strong>Brazil</strong></td><td style=\"text-align:center\">7.47</td><td style=\"text-align:center\">3.54</td></tr><tr><td><strong>France</strong></td><td style=\"text-align:center\">1.89</td><td style=\"text-align:center\">2.90</td></tr><tr><td><strong>Mexico</strong></td><td style=\"text-align:center\">15.2</td><td style=\"text-align:center\">11.9</td></tr><tr><td><strong>Turkey</strong></td><td style=\"text-align:center\">7.18</td><td style=\"text-align:center\">8.61</td></tr><tr><td><strong>South Korea</strong></td><td style=\"text-align:center\">-3.76</td><td style=\"text-align:center\">-7.35</td></tr><tr><td><strong>Spain</strong></td><td style=\"text-align:center\">-2.21</td><td style=\"text-align:center\">-4.72</td></tr><tr><td><strong>Canada</strong></td><td style=\"text-align:center\">6.75</td><td style=\"text-align:center\">18.0</td></tr><tr><td><strong>Saudi Arabia</strong></td><td style=\"text-align:center\">2.05</td><td style=\"text-align:center\">5.99</td></tr><tr><td><strong>Thailand</strong></td><td style=\"text-align:center\">-2.15</td><td style=\"text-align:center\">-3.09</td></tr><tr><td><strong>Poland</strong></td><td style=\"text-align:center\">-3.50</td><td style=\"text-align:center\">-9.25</td></tr><tr><td><strong>Australia</strong></td><td style=\"text-align:center\">6.19</td><td style=\"text-align:center\">24.5</td></tr><tr><td><strong>Iran</strong></td><td style=\"text-align:center\">6.22</td><td style=\"text-align:center\">7.50</td></tr></tbody></table></figure><p>Amongst these 20 counties, the 3 whose population with IQ higher than the global mean is predicted to have the largest:</p><ul><li>Absolute variation&nbsp;are India (53.5 M), the United States (39.4 M), and Indonesia (15.7 M).</li><li>Relative variation&nbsp;are Australia (24.5 %), Canada (18.0 %), and the United States (12.0 %).</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn36brakxwqmj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref36brakxwqmj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I set the coefficient of variation to 15 % because, according to <a href=\"https://en.wikipedia.org/wiki/Intelligence_quotient\"><u>Wikipedia</u></a>:</p><blockquote><p>For modern IQ tests, the raw score is transformed to a normal distribution with mean 100 and standard deviation 15.</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5m4ypw70rmj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5m4ypw70rmj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>However, note the values from <a href=\"https://www.worlddata.info/iq-by-country.php\"><u>WordData</u></a> rely on many studies from Lynn:</p><blockquote><p>The intelligence quotients by countries are taken from the studies conducted by <a href=\"https://books.google.com/books?id=KQ4rLiAbHQQC\">Richard Lynn and Tatu Vanhanen</a> (2002), Heiner Rindermann (2007), Khaleefa and Lynn (2008), Ahmad, Khanum and Riaz (2008), Lynn, Abdalla and Al-Shahomee (2008), Lynn and Meisenberg (2010), as well as the PISA tests in 2003, 2006 and 2009. More recent results were weighted higher. The studies are not entirely uncontroversial as they often consider only specific population groups or a few individuals per country. If, on the other hand, an average is obtained from all the tests and studies, a usable overview will be obtained.</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnh5l6niomxc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefh5l6niomxc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The total IQ is the product between the mean IQ and population.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsqot5tu1qm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsqot5tu1qm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See tab \u201cPopulation by IQ\u201d for the full results.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqski5azkv4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqski5azkv4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Ordered from largest to smallest real GDP in 2019.</p></div></li></ol>", "user": {"username": "vascoamaralgrilo"}}, {"_id": "p5kmmBcfbBfaQ48mD", "title": "What do you think is the probability of ethical hedonism being true?", "postedAt": "2022-10-20T10:39:42.501Z", "htmlBody": "<p>What do you think is the probability of ethical hedonism being true? The definition is below. You can give your estimate <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfItoAV_uXYJWQhSzw9bDW8uA6jOviTn__zSpuviNBDNw3H4A/viewform\">here</a>, and check the answers and respective statistics <a href=\"https://docs.google.com/spreadsheets/d/1g6LyufOUv2unlwqJFY32e55tJMf4rN2c_7bKdUf8rbg/edit?usp=sharing\">here</a>.</p><p>Ethical hedonism is defined as follows in the <a href=\"https://plato.stanford.edu/entries/hedonism/#EthHed\">article</a> on <a href=\"https://plato.stanford.edu/index.html\">Stanford Encyclopedia of Philosophy</a>:</p><blockquote><p>At its simplest, ethical hedonism is the claim that all and only pleasure has positive importance and all and only pain or displeasure has negative importance. This importance is to be understood non-instrumentally, that is, independently of the importance of anything that pleasure or displeasure might cause or prevent.</p></blockquote><p>In addition, I wonder whether having the following equalities in mind would clarify discussions about ethics:</p><ul><li>\"X is morally good/positive\" = \"X is intrinsically good/positive\" = \"X ought to be\" <i>=</i> \"X increases goodness/utility\" <i>=</i> \"X improves wellbeing (conscious experiences)\".</li><li>\"X is morally bad/negative\" = \"X is intrinsically bad/negative\" = \"X ought not to be\" <i>=</i> \"X decreases goodness/utility\" <i>=</i> \"X worsens wellbeing (conscious experiences)\".</li><li>\"Pleasure\" = \"positive utility/wellbeing\" = \"good/positive conscious experiences\".</li><li>\"Pain\" = \"negative utility/wellbeing\" = \"bad/negative conscious experience\".</li></ul><p>In the 1st 2 points, the 3rd and 4th italicised equal signs only apply to <a href=\"https://forum.effectivealtruism.org/topics/utilitarianism\">utilitarianism</a> and <a href=\"https://forum.effectivealtruism.org/topics/hedonism\">henodism</a>, respectively. However, even if only implicitly/subconsciously, I think people tend to mean \"X improves wellbeing according to my best heuristics\" when they say \"X is morally good\". As Sharon Rawlette mentioned in <a href=\"https://80000hours.org/podcast/episodes/sharon-hewitt-rawlette-hedonistic-utilitarianism/\">episode 138</a> of <a href=\"https://80000hours.org/podcast/\">The 80,000 Hours Podcast</a>:</p><blockquote><p>I actually think that if we didn\u2019t ever experience pleasure [positive conscious experiences] or pain [negative conscious experiences], or any of these positive or negative qualitative states, that we wouldn\u2019t actually have the concept of intrinsic goodness that we do in fact have and that we do use when we\u2019re making moral decisions.</p></blockquote><p>At a fundamental level, I guess differences between ethical theories are only related to the heuristics used to assess the value of actions. For example, the focus in <a href=\"https://forum.effectivealtruism.org/topics/deontology\">deontology</a> is on following certain rules, and in <a href=\"https://forum.effectivealtruism.org/topics/consequentialism\">consequentialism</a> on the consequences of the actions. However, all theories are about assessing conscious experiences, as by definition that is what can be perceived.</p><p>If this is so, at least in informal discussions, if not in the literature, it might be useful to have the above equalities in mind, and shift the focus of discussions towards assessing empirical evidence, including our conscious experiences.</p>", "user": {"username": "vascoamaralgrilo"}}, {"_id": "agaudjzbp5eBSnbom", "title": "EA Funds has a Public Grants Database", "postedAt": "2022-10-18T12:55:45.749Z", "htmlBody": "<p>We have just launched a <a href=\"https://funds.effectivealtruism.org/grants\">public grants database</a> which contains information on all of our public grants&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreft9rqx0c2bk\"><sup><a href=\"#fnt9rqx0c2bk\">[1]</a></sup></span>.</p><p>We currently include details of the fund that made the grant, a project summary, the grantee and the payout amount.</p><p>We think it is valuable to show the EA community and our donors what kinds of projects we end up funding to increase accountability and encourage more excellent projects. Prospective grantees often ask me what types of projects we fund, and I hope this will make it easier for them to come up with ideas for projects and know whether their project ideas are in scope.</p><p>We previously wrote&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ddBLtdQjcjvZH5JvF/long-term-future-fund-december-2021-grant-recommendations\">public payout reports</a> where our fund managers would outline their reasoning for making a grant. I think that the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ddBLtdQjcjvZH5JvF/long-term-future-fund-december-2021-grant-recommendations\">public payout reports</a> were excellent. Still, they were time-consuming for our fund managers, and I would prefer them to spend their limited capacity evaluating grants&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref25th9mu7dpn\"><sup><a href=\"#fn25th9mu7dpn\">[2]</a></sup></span>. We were also much slower than I would have liked with releasing them, and I hope the database will stay much more up-to-date with our grantmaking (with entries generally added soon after we make payouts).</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnt9rqx0c2bk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreft9rqx0c2bk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Please note that it is currently still being tested, so some grant information may be incorrect.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn25th9mu7dpn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref25th9mu7dpn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>One idea is to ask fund managers to highlight a few grants each month and explain why they were excited about them to encourage more excellent projects and give insight into our grantmaking. They would not be comprehensive like our old reports, but I think our public grants database and these highlights reports (when we have time to write them) will get most of the value of the old reports while saving us substantial time.</p></div></li></ol>", "user": {"username": "calebp"}}, {"_id": "ZnMZzFjJuG7kNQfnW", "title": "Why not to solve alignment by making superintelligent humans?", "postedAt": "2022-10-16T21:26:16.740Z", "htmlBody": "<p>Okay, hear me out: we already have human level/ general intelligent human aligned agents by the billions. And making new ones artificially seems to be really hard according to lots of alignment researchers. So why haven't I heard or read about projects trying to improve human intelligence? I'm not saying to do it in order to solve the alignment of pure artificial intelligences (although that could be a possibility too), but to the point of getting a hybrid (natural + artificial) superintelligence or just more useful than AIs.&nbsp;</p><p>I know that then there is the question about what a superintelligence would do or how having it aligned would help us to not make misaligned ones, and although there are possible good answers to that, those are questions that already exist outside \"normal\" alignment.&nbsp;</p><p>The only similar thing to that that I've heard was Elon Musk talking about Neuralink, but there is a hugeee difference between the things that Elon mentions as aspirations (in particular merging with an AI) and the product that is making right now. I don't see how Brain-Computer Interfaces would improve our decision making that much. A lot of people when they talk about augmenting intelligence seem to bring them up, and of course they could be useful, but again: I'm not talking about using them to solve AI Alignment, but to get around it. I'm wondering if we can find a way of scaling human intelligence in the same way we scale artificial intelligence.&nbsp;</p><p>I found a post that briefly <a href=\"https://www.lesswrong.com/posts/rpRsksjrBXEDJuHHy/brain-computer-interfaces-and-ai-alignment#Scaling_far_Beyond_Human_Intelligence\">mentions</a> similar ideas than mine but under the term BCI, which I don't understand if it's a more abarcative term than \"a device that allows you to use other devices with the mind\", because as I said, I don't know any device which would improve our decision making that much if we could just use it with our minds.</p><p>The clearest strategy that comes to mind is to make artificial neurons communicable with biological ones and then integrable with whole human neural networks. Could that be possible? I know that it could sound crazy, but I guess I'm talking to the people who think aligning an AI is really difficult and that having superintelligences on humanity's side sooner or later seems like the only path forward.</p>", "user": {"username": "Patricio"}}, {"_id": "829tAQAYAntkRxCy2", "title": "Effective Altruism's Implicit Epistemology", "postedAt": "2022-10-18T13:38:17.537Z", "htmlBody": "<p>Cross-posted from <a href=\"https://theviolethour.substack.com/p/effective-altruisms-implicit-epistemology\"><i>The Violet Hour</i></a>.</p><p><i>Note:</i> &nbsp;This post primarily examines the informal epistemic norms which govern EA. Consequently, it's mostly a work of sociology rather than philosophy. These informal norms are fleshed out in Section 4, and I\u2019ll briefly state them here.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefl98sdm25zs\"><sup><a href=\"#fnl98sdm25zs\">[1]</a></sup></span></p><ol><li>If it\u2019s worth doing, it\u2019s worth doing with made up numbers.</li><li>Speculative reasoning is worth doing.</li><li>Our explicit, subjective credences are approximately accurate enough, most of the time, even in crazy domains, for it to be worth treating those credences as a salient input into action.</li><li>When arguments lead us to conclusions that are both speculative and fanatical, treat this as a sign that something has gone wrong.&nbsp;&nbsp;</li></ol><p>This piece (in aspiration, at least) is written to be accessible to people who are less involved with EA than typical Forum readers. Thus, if you\u2019re familiar with EA, you can skim the introduction. Section 2 provides an informal exposition of Subjective Bayesianism and Expected Value Theory \u2014 if you\u2019re familiar with those theories, that section can also be skimmed.</p><h1>1. Introduction</h1><p>The future might be very big, and we might be able to do a lot, right now, to shape it.&nbsp;</p><p>You might have heard of a community of people who take this idea pretty seriously \u2014 effective altruists, or \u2018EAs\u2019. If you first heard of EA a few years ago, and haven\u2019t really followed it since, then you might be pretty surprised at where we\u2019ve ended up.&nbsp;&nbsp;</p><p>Currently, EA consists of a variety of professional organizations, researchers, and grantmakers, all with (sometimes subtly) different approaches to doing the most good possible. Organizations which, collectively, donate billions of dollars towards interventions aiming to improve the welfare of conscious beings. In recent years, the EA community has shifted its priorities towards an idea called&nbsp;<i>longtermism</i> \u2014 very roughly, the idea that we should primarily focus our altruistic efforts towards shaping the very long-run future. Like,&nbsp;<i>very</i> long-run. At least thousands of years. Maybe more.&nbsp;</p><p>(From hereon, I\u2019ll use \u2018EA\u2019 to talk primarily about&nbsp;<i>longtermist</i> EA. Hopefully this won\u2019t annoy too many people).</p><p>Anyway, longtermist ideas have pushed EA to focus on a few key cause areas \u2014 in particular, ensuring that the development of advanced AI is safe, preventing the development of deliberately engineered pandemics, and&nbsp;(of course) promoting&nbsp;effective altruism itself. I've been part of this community for a while, and I've often found outsiders bemused by some of our main priorities. I've also found myself puzzled by this bemusement. The explicit commitments undergirding (longtermist) EA, as many philosophers involved with EA remind us, are really not all that controversial. And those philosophers are right, I think. Will MacAskill, for instance, has&nbsp;<a href=\"https://80000hours.org/podcast/episodes/will-macaskill-what-we-owe-the-future/\"><u>listed</u></a> the following three claims as forming the basic, core argument behind longtermism.</p><blockquote><p>(1) Future people matter morally.</p><p>(2) There could be enormous numbers of future people.</p><p>(3) We can make a difference to the world they inhabit.</p></blockquote><p>Together, these three claims all seem pretty reasonable. With that in mind, we\u2019re left with a puzzle: given that EA\u2019s explicitly stated core commitments are not that weird, why, to many people, do EA\u2019s&nbsp;<i>explicit, practical priorities</i> appear so weird?&nbsp;</p><p>In short, my answer to this puzzle claims that EA\u2019s priorities emerge, to a large extent, from EA\u2019s unusual&nbsp;<i>epistemic culture</i>.&nbsp;So, in this essay, I\u2019ll attempt to highlight the sociologically distinctive norms EAs adopt, in practice, concerning how to reason, and how to prioritize under uncertainty. I\u2019ll then claim that various<i> informal norms</i>, beyond some of the more explicit philosophical theories which inspire those norms, play a key role in driving EA\u2019s prioritization decisions.&nbsp;</p><h1>2. Numbers, Numbers, Numbers</h1><p>Suppose I asked you, right now, to give me a probability that Earth will experience an alien invasion within the next two weeks.&nbsp;</p><p>You might just ignore me. But suppose you\u2018ve come across me at a party; the friend you arrived with is conspicuously absent. You look around, and, well, the other conversations aren\u2019t any better. Also, you notice that the guy you fancy is in the corner; every so often, you catch him shyly glancing at us. Fine, you think, I could be crazy, but talking to me may get you a good conversation opener. You might need it. The guy you like seems pretty shy, after all.&nbsp;&nbsp;</p><p>So, you decide that you\u2019re in this conversation, at least for now. You respond by telling me that, while you\u2019re not sure about exact probabilities, it\u2019s definitely not going to happen. You pause, waiting for my response, with the faint hope that I\u2019ll provide the springboard for a funny anecdote you can share later on.</p><p>\u201cOkay, sure, it almost certainly won\u2019t happen. But&nbsp;<i>how sure</i> are you, exactly? For starters, it\u2019s clearly less likely than me winning the lottery. So we know it\u2019s under 1/10<sup>6</sup>. I think I\u2019ve probably got more chance at winning two distinct lotteries in my lifetime, at least if I play every week, so let\u2019s change our lower bound to \u2026 \u201d \u2014 as I\u2019m talking, you politely excuse yourself. You\u2019ve been to EA parties before, enough to know that this conversation isn\u2019t worth it. You spend your time in the bathroom searching for articles on how to approach beautiful, cripplingly shy men.&nbsp;</p><p>Spend enough time around EAs, and you may come to notice that many EAs have a peculiar penchant for sharing probabilities about all sorts of events, both professionally and socially. Probabilities, for example, of the likelihood of smarter-than-human machines by 2040 (in light of recent developments, Ajeya Cotra\u2019s&nbsp;<a href=\"https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines\"><u>current best guess</u></a> is around 50%), the probability of permanent human technological stagnation (Will MacAskill says around&nbsp;<a href=\"https://80000hours.org/podcast/episodes/will-macaskill-what-we-owe-the-future/#stagnation-021904\"><u>1 in 3</u></a>), or the chance of human extinction (Toby Ord\u2019s book,&nbsp;<i>The Precipice</i>, ends up at about 1 in 6).&nbsp;</p><p>As you leave the party, you start to wonder why EAs tend to be beset by this particular quirk.</p><blockquote><p>2.1</p></blockquote><p>In brief, the inspiration for this practice stems from a philosophical idea,&nbsp;Subjective Bayesianism \u2014 which theorizes how ideally rational agents behave when faced with uncertainty.&nbsp;</p><p>Canonically, one way of setting up the argument for Subjective Bayesianism goes something like this: suppose that you care about some outcomes more than others, and you\u2019re also (to varying degrees) uncertain about the likelihood of all sorts of outcomes (such as, for example, whether it\u2019s going to rain tomorrow). And suppose this uncertainty feeds into action in a certain way: other things equal, your actions are influenced more by outcomes which you judge to be more likely.</p><p>Given all this, there are arguments showing that there must be some way of representing your uncertainty with a&nbsp;<i>probability function</i> \u2014 that is, for any given state of the world (like, for example, whether it\u2019s raining), there\u2019s some number between 0 and 1 which represents your uncertainty about that state of the world, called your \u2018credence\u2019, and your credences have to obey certain rules (for instance, your credence in \u2018rain or not rain\u2019 has to sum to 1).&nbsp;</p><p>Why must these rules be obeyed? Well, if those rules aren\u2019t obeyed, then there are situations where someone who has no more information than you can offer you a series of bets, which \u2014 given how likely you think those outcomes are \u2014 appear to be good deals. Nevertheless, taking this series of bets is&nbsp;<i>guaranteed</i> to lose you money. This type of argument is called a \u2018money pump\u2019 argument, because someone who (again) has no more knowledge than you do, has a procedure for \u201cpumping money\u201d from you indefinitely, or at least until you reach insolvency.&nbsp;</p><p>Now, we know that&nbsp;if your uncertainty can\u2019t be represented with a probability function, then you can be money pumped. There are&nbsp;<a href=\"https://johanegustafsson.net/books/money-pump-arguments.pdf\"><u>proofs</u></a> of this. Guaranteed losses are bad, thus, so the argument goes, you should behave so that your uncertainty can be represented with a probability function.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgs0sud7sveh\"><sup><a href=\"#fngs0sud7sveh\">[2]</a></sup></span></p><p>We should also mention that there\u2019s a formula which tells you how to update probabilities upon learning new information \u2014 Bayes\u2019 rule. Hence, the \u2018Bayesian\u2019 part of Subjective Bayesianism. You have probabilities, which represent your subjective states of uncertainty. As your uncertainty is modeled with a probability function, you update your uncertainty in the standard way you\u2019d update probabilities.</p><p>Do you actually need to be forming credences in practice? Well, maybe not, we\u2019ll get to that. But humans can be pretty bad at reasoning with probabilities in various&nbsp;<a href=\"https://en.wikipedia.org/wiki/Base_rate_fallacy\"><u>psychological</u></a>&nbsp;<a href=\"https://www.jstor.org/stable/2352759?seq=5#metadata_info_tab_contents\"><u>experiments</u></a>, and we know that people can become better calibrated when prompted to think in more explicitly probabilistic ways \u2014 Philip Tetlock\u2019s work on&nbsp;<a href=\"https://en.wikipedia.org/wiki/Superforecasting:_The_Art_and_Science_of_Prediction\"><u>superforecasting</u></a> has illustrated this. So there is something motivating this sort of practice.&nbsp;&nbsp;</p><blockquote><p>2.2</p></blockquote><p>Suppose, after leaving the EA party, you begin to reconsider your initial disdain. You&nbsp;<i>are</i> uncertain about lots of things, and you definitely don\u2019t want to be money-pumped. Also, you like the idea of helping people. You start to consider more stuff, even some quite weird stuff, and maybe now you even have your very own credences. But you were initially drawn to EA because you wanted to actually help people \u2014 so, now what?&nbsp;<i>What should you actually do?</i></p><p>When deciding how to act, EAs are inspired by another theoretical ideal&nbsp;\u2014&nbsp;<i>Expected Value Theory</i>.&nbsp;</p><p><a href=\"https://80000hours.org/articles/expected-value/\"><u>80,000 Hours</u></a> introduce this idea nicely. They ask you to suppose that there\u2019s (what looks to be) a tasty glass of beer in front of you. Unfortunately, the beer is unwisely produced, let\u2019s say, in a factory that also manufactures similarly coloured poison. Out of every 100 glasses of the mystery liquid, one will be a glass of poison. You know nothing else about the factory. Should you drink the mystery liquid?&nbsp;&nbsp;&nbsp;</p><p>Well, like, probably not. Although drinking the liquid will almost certainly be positive, \u201cthe badness of drinking poison far outweighs the goodness of getting a free beer\u201d, as 80,000 Hours\u2019 president Ben Todd sagely informs us.&nbsp;</p><p>This sort of principle generalizes to other situations, too. When you\u2019re thinking about what to do, you shouldn\u2019t only think about what is&nbsp;<i>likely</i> to happen. Instead, you should think about which action has the highest&nbsp;<i>expected value</i>.&nbsp;That is, you take the probability that your action will produce a given outcome, and multiply that probability by the value you assign to that outcome, were it to happen. Then, you add up all of the probability-weighted values. We can see how this works with the toy example below, using some made up numbers for valuations.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994808/mirroredImages/829tAQAYAntkRxCy2/pmylpaxqrmkvvzs5qmdj.png\"></p><p>We can treat the valuations of \u2018beer\u2019 and \u2018poison\u2019 as departures from my zero baseline, where I don\u2019t drink the liquid. So, in this case, as the expected value of the drink is -995.05 (which is less than zero), you shouldn\u2019t drink the (probable) beer.</p><p>Now, if you care about the very long-run future, calculating the effects of everything you do millions of years from now, with corresponding probabilities, would be \u2026&nbsp;pretty demanding.&nbsp;For the most part, EAs recognise this demandingness. Certainly, professional organizations directing money and talent recognise this; while it sometimes makes sense to explicitly reason in terms of expected value, it often doesn\u2019t.&nbsp;80,000 Hours state this&nbsp;<a href=\"https://80000hours.org/articles/expected-value/\"><u>explicitly</u></a>, mentioning the importance of certain proxies for evaluating problems \u2014 like their \u2018importance, neglectedness, and tractability\u2019 (INT). These proxies are, in general, endorsed by EAs.&nbsp;Open Philanthropy, a large grantmaking organization with a dedicated longtermist team, also&nbsp;<a href=\"https://www.openphilanthropy.org/cause-selection/\"><u>reference</u></a> these proxies as important for their grantmaking.&nbsp;&nbsp;</p><p>Still, even when using proxies like INT, 80,000 Hours&nbsp;<a href=\"https://80000hours.org/articles/problem-framework/#advantages-and-disadvantages-of-quantitative-problem-prioritisation\"><u>caution against</u></a> taking the estimates literally. In my experience interacting with people at big EA organizations, this view is pretty common. I don\u2019t remember encountering anyone taking the output of any one quantitative model completely literally. Instead, a lot of intuition and informal qualitative reasoning is involved in driving conclusions about prioritization. However, through understanding the formal frameworks just outlined, we\u2019re now in a better position to understand the ways in which&nbsp;EA\u2019s epistemology goes beyond these frameworks.&nbsp;</p><p>And we\u2019ll set the scene, initially, with the help of a metaphor.</p><h1>3. Uncertainty and the Crazy Train</h1><p>Ajeya Cotra works for Open Philanthropy, where she wrote a gargantuan report on AI timelines, estimating the arrival of smarter-than-human artificial systems.&nbsp;&nbsp;</p><p>Alongside an estimate for the advent of AI with power to take control over the future of human civilisation, the report acts as a handy reference point for answers to questions like \u201chow much computational power, measured in&nbsp;<a href=\"https://en.wikipedia.org/wiki/FLOPS\"><u>FLOPS</u></a>, are the population of nematode worms using every second?\u201d, and \u201chow much can we infer about the willingness of national governments to spend money on future AI projects, given the fraction of GDP they\u2019ve previously spent on wars?\u201d</p><p>But perhaps Ajeya\u2019s most impressive accomplishment is coining the phrase \u201cthe train to Crazy Town\u201d, in reference to the growing weirdness of EA.</p><p>The thought goes something like this: one day, people realized that the world contains a lot of bad stuff, and that it would be good to do things to alleviate that bad stuff, and create more good stuff. Thus the birth of GiveWell, which ranked charities in terms of amount of good done per $. Malaria nets are good, and the Against Malaria Foundation is really effective at distributing malaria nets to the most at-risk. Deworming pills also look good, potentially, though there\u2019s been&nbsp;<a href=\"https://blogs.worldbank.org/impactevaluations/worm-wars-anthology\"><u>some brouhaha</u></a> about that. You\u2019re uncertain, but you\u2019re making progress. You start researching questions in global health and well-being, looking for ever-better ways to improve the lives of the world\u2019s poorest.</p><p>Still, you\u2019re trying to do the&nbsp;<i>most good</i> you can do, which is a pretty lofty goal. Or, the most good you can do in expectation, because you\u2019re uncertain about a lot of things. So you begin considering all of the ways in which you\u2019re uncertain, and it leads you to weird places. For one, you start to realize there could be quite a lot of people in the future. Like,&nbsp;<i>a lot a lot</i>. In Greaves\u2019 and MacAskill\u2019s&nbsp;<a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf#page=10\"><u>paper</u></a> defending longtermism, they claim that \u201cany reasonable estimate of the expected number of future beings is at least 10<sup>24</sup>\u201d.&nbsp;</p><p>10,000,000,000,000,000,000,000,000 people.&nbsp;<i>At least</i>.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefiw3ixopqi2r\"><sup><a href=\"#fniw3ixopqi2r\">[3]</a></sup></span></p><p>Anyway, each of these 10<sup>24&nbsp;</sup>people has hopes and dreams, no less important than our hopes, or the hopes and dreams of our loved ones and families. And you care about these potential people. You want to be sure that you\u2019re actually making the world, overall, better \u2014 as much as you possibly can.&nbsp;&nbsp;</p><p>After spending some time reflecting on just how bad COVID was, you decide to shift to biosecurity. No one engineered COVID to be bad, and yet, still, it was pretty bad. Reflecting on the consequences of COVID leads you to wonder how much worse a deliberately engineered pandemic could be. Initially, a piece of theoretical reasoning gets you worried: you learn that the space of all possible DNA sequences is far, <i>far</i> larger than the number of DNA sequences generated by evolution. If someone wanted to engineer a pandemic with far casualty rates then, well, they very likely could. You imagine various motives for doing this, and they don\u2019t seem all that implausible. Perhaps a small group of bioterrorists become convinced humanity is a stain on the planet, and set to destroy it; or perhaps states respond to military conflict with a pandemic attack so deadly and viral that it expands globally, before we have time to develop effective prevention.&nbsp;&nbsp;</p><p>But then you begin to descend the rabbit hole further. You encounter various cosmological theories, suggesting that our universe could be spatially infinite, with a potentially infinite number of sentient beings.&nbsp;</p><p>Of course, you initially think that we couldn\u2019t affect the well-being of infinitely many people \u2014 almost all of these people live outside our lightcone, after all. But&nbsp;<a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/2019/MacAskill_et_al_Evidentialist_Wager.pdf\"><u>some decision theories</u></a> seem to imply that we could \u201cacausally\u201d affect their well-being, at least in an infinite universe. And lots of smart people endorse these decision theories. You remember to update your credences in response to expert disagreement. You\u2019re neither a cosmologist nor a decision theorist, after all, and these theories have non-trivial acceptance among experts in these fields.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgjaqa4gl9xa\"><sup><a href=\"#fngjaqa4gl9xa\">[4]</a></sup></span>&nbsp;</p><p>You remember expected value theory. You think about the vastness of infinity, desperately trying to comprehend just how important it would be to affect the happiness, or relieve the misery of infinitely many people. These people matter, you remind yourself. And if you could do something,&nbsp;<i>somehow</i>, to increase the chance that you do an infinitely large amount of good, then, well, that starts to seem worth it. You push through on the project. Some time passes. But, as you\u2019re double-checking your calculations about the best way to acausally affect the welfare of infinitely many aliens, you realize that you\u2019ve ended up somewhere you didn\u2019t quite expect.&nbsp;</p><p>The story started with something simple: we wanted to do the most good we could do. And we wanted to check that our calculations were robust to important ways we might be wrong about the world. No one thought that this would be easy, and we knew that we might end up in an unexpected place. But we didn\u2019t expect to end up&nbsp;<i>here</i>. \u201cSomething\u2019s off\u201d, you think. You\u2019re not sure what exactly went wrong, but you start to think that this is all seeming just a bit too mad. The Crazy Train, thankfully, has a return route. You hop back on the train, and pivot back to biosecurity.&nbsp;</p><blockquote><p>3.1</p></blockquote><p>So, what point are we meant to draw from that story? Well, I don\u2019t want to claim that following through on expected value reasoning is necessarily going to lead you to the&nbsp;<i>particular</i> concerns I mentioned. Indeed, Ajeya tells her story slightly differently, with a slightly different focus. In this&nbsp;<a href=\"https://80000hours.org/podcast/episodes/ajeya-cotra-worldview-diversification/#the-doomsday-argument-010937\"><u>interview</u></a>, Ajeya discusses the role that reasoning about the simulation argument, along with various theories of anthropic reasoning, played in her decision to alight the Crazy Train at the particular stop she did.&nbsp;&nbsp;</p><p>Still, I think the story I told captures, at a schematic level, a broader pattern within the practical epistemology of EA. It\u2019s a pattern where EAs start with something common-sense, or close, and follow certain lines of reasoning, inspired by Subjective Bayesianism and Expected Value Theory, to increasingly weird conclusions. Then, at some point, EAs lose confidence in their ability to adequately perform this sort of explicit probabilistic reasoning. They begin to trust their explicit reasoning less, and fall back on more informal heuristics. Heuristics like \u201cno, sorry, this is just too wacky\u201d.&nbsp;&nbsp;</p><p>The practical epistemology of EA is clearly influenced by both Expected Value Theory and Subjective Bayesianism. But I think the inspiration EAs take from these theories has more in common with the way, say, painters draw on prior artistic movements, than it does to a community of morally motivated acolytes, fiercely committed to following the output of certain highly formalized, practical procedures for ranking priorities.&nbsp;</p><p>Take Picasso. In later periods, he saw something appealing in the surrealist movement, and so he (ahem) drew from surrealism, not as a set of&nbsp;<i>explicit commitments</i>, but as embodying a certain kind of appealing vision. A fuzzy, incomplete vision he wanted to incorporate into his own work. I think the formal theories we\u2019ve outlined, for EAs, play a similar role. These frameworks offer us accounts of ideal epistemology, and ideal rationality. But, at least for creatures like us, such frameworks radically underdetermine the specific procedures we should be adopting in practice. So, in EA, we take pieces from these frameworks. We see something appealing in their vision. But we approximate them in incomplete, underdetermined, and idiosyncratic ways.&nbsp;</p><p>In the remainder of this essay, I want to get more explicit about the ways in which EAs take inspiration from, and approximate these frameworks.&nbsp;</p><h1>4. Four Implicit Norms of EA Epistemology&nbsp;</h1><p>Cast your mind back to the parable we started with in Section 2 \u2014 the one at the party, discussing the probability of aliens. Most people don\u2019t engage in that sort of conversation. EAs tend to.&nbsp;<i>Why?</i></p><p>I think one reason is that, for a lot of people, there are certain questions which feel&nbsp;<i>so</i> speculative, and&nbsp;<i>so</i> unmoored from empirical data, that we\u2019re unable to say anything meaningful at all about them. Maybe such people could provide credences for various crazy questions, if you pushed them. But they\u2019d probably believe that any credence they offered would be&nbsp;<i>so</i> arbitrary, and&nbsp;<i>so</i> whimsical, that&nbsp;<i>there would just be no point</i>. At some juncture, the credences we offer just cease to represent anything meaningful. EAs tend to think differently. They\u2019re more likely to believe the following claim.</p><blockquote><p><strong>Principle 1:</strong>&nbsp;<a href=\"https://slatestarcodex.com/2013/05/02/if-its-worth-doing-its-worth-doing-with-made-up-statistics/\"><u>If it\u2019s worth doing, it\u2019s worth doing with made up numbers</u></a>.</p></blockquote><p>This principle, like the ones which follow it, is (regrettably) vague. Whether something is worth doing, for starters, is a matter of degree, as is the extent to which a number is \u2018made up\u2019.&nbsp;</p><p>A bit more specifically, we can say the following: EAs tend to believe, to a much larger degree than otherwise similar populations (like scientifically informed policymakers, laypeople, or academics) that if you want highlight the importance of (say) climate change, or the socialist revolution, or housing policy, then you really should start trying to be quantitative about&nbsp;<i>just how</i> uncertain you are about the various claims which undergird your theory of why this area is important. And you should really try, if at all possible, to represent your uncertainty in the form of explicit probabilities. If it\u2019s worth doing, it\u2019s worth doing with explicit reasoning, and quantitative representations of uncertainty.&nbsp;</p><p>You can see this principle being applied in Joe Carlsmith\u2019s&nbsp;<a href=\"https://arxiv.org/abs/2206.13353\"><u>report</u></a> on the probability of existential risk from advanced AI, where the probability of existential risk from \u201cmisaligned, power-seeking\u201d advanced AI systems is broken down into several, slightly fuzzy, hard-to-evaluate claims, each of which are assigned credences. In this report, Carlsmith explicitly (though cautiously)&nbsp;<a href=\"https://arxiv.org/pdf/2206.13353.pdf#page=47\"><u>extols</u></a> the benefits of using explicit probabilities, as allowing for greater transparency. A similar style reasoning is apparent, too, in Cotra\u2019s&nbsp;<a href=\"https://drive.google.com/drive/u/1/folders/15ArhEPZSTYU8f012bs6ehPS6-xmhtBPP\"><u>report</u></a> on AI timelines. Drawing on her estimates for the amount of computation required for various biological processes, Cotra presents various probability distributions for the amount of computational power that you\u2019d need to combine with today\u2019s ideas in order to develop an AI with greater-than-human cognitive capabilities. Even in less formal reports, the use of quantitative representations of uncertainty remains present \u2014 80,000 Hours\u2019 report on the importance of \u2018GCBRs\u2019 (\u2018global catastrophic biological risks\u2019), for example, provides&nbsp;<a href=\"https://80000hours.org/problem-profiles/preventing-catastrophic-pandemics/full-report/#plausibility-of-GCBRs\"><u>various numerical estimates</u></a> representing reported levels of likelihood for different kinds of GCBRs.</p><p>If you read critics of (longtermist) EA, a lot of them really don\u2019t like the use of \u201cmade up numbers\u201d. Nathan Robinson&nbsp;<a href=\"https://www.currentaffairs.org/2022/09/defective-altruism/\"><u>criticizes</u></a> EA\u2019s focus on long-term AI risks, as the case for AI risk relies on \u201cmade-up meaningless probabilities.\u201d Boaz Barak&nbsp;<a href=\"https://web.archive.org/web/20220610193128/https://windowsontheory.org/2022/05/23/why-i-am-not-a-longtermist/\"><u>claims</u></a> that, for certain claims about the far-future, we can only make vague, qualitative judgements, such as \u201cextremely likely\u201d, or \u201cpossible\u201d, or \u201ccan\u2019t be ruled out\u201d. These criticisms, I think, reflect adherence to an implicit (though I think fairly widespread) view of probabilities, where we should only assign explicit probabilities when our evidence is relatively robust.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefuee98ubj9ak\"><sup><a href=\"#fnuee98ubj9ak\">[5]</a></sup></span>&nbsp;EAs, by contrast, tend to be more liberal in the sort of claims for which they think it\u2019s appropriate (or helpful) to assign explicit probabilities.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqundsau43r8\"><sup><a href=\"#fnqundsau43r8\">[6]</a></sup></span><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span></p><p>Now, most EAs don\u2019t believe that we should be doing explicit expected value calculations all the time, or that we should&nbsp;<i>only</i> be relying on quantitative modeling. Instead,&nbsp;<i>Principle 1</i> says something like \u201cif we think something could be important, a key part of the process for figuring that out involves numeric estimates of scope and uncertainty \u2014 even if, in the end, we don\u2019t take such estimates totally literally\u201d.&nbsp;</p><p>Crucially, the adoption of this informal norm is inspired by, but not&nbsp;<i>directly derived from,</i> the more explicit philosophical arguments for Subjective Bayesianism. The philosophical arguments, by themselves, don\u2019t tell you when (or whether) it\u2019s appropriate to assign explicit credences in practice. An appropriate defense (or criticism) of EA practice would have to rely on a different kind of argument. But, before we evaluate this norm, we should first state the norm explicitly, as a distinctive feature of EA\u2019s epistemic culture.&nbsp;</p><blockquote><p>4.1</p></blockquote><p>When introducing the Crazy Train, we saw that EAs do, in fact, exhibit some amount of normal suspicion towards conclusions derived from very speculative or abstract arguments. But one thing that\u2019s unusual about EAs is the fact that these considerations were germane considerations&nbsp;<i>at all</i>. The simulation hypothesis, or theories of acausal reasoning are, for most people, never likely to arise. Certain types of claims are not only beyond the bounds of meaningful probability assignments, they\u2019re beyond the bounds of worthwhile discussion at all.&nbsp;&nbsp;&nbsp;</p><blockquote><p><strong>Principle 2:</strong> Speculative reasoning is worth doing.</p></blockquote><p>I\u2019ve claimed that EAs are more likely to believe that if something is \u201cworth doing\u201d, then \u201cit\u2019s worth doing with made up numbers\u201d. Speculative reasoning is worth doing. Thus, in EA , we observe people offering credences for various speculative claims, which other groups are more likely to filter out early on, perhaps on account of sounding too speculative or sci-fi.&nbsp;&nbsp;</p><p>What makes a claim speculative? Well, I think it\u2019s got something to do with a claim requiring a lot of theoretical reasoning that extends beyond our direct, observational evidence. If I provide you with an estimate of how many aliens there are in our galaxy, that claim is more speculative than a claim about (say) the number of shops on some random main street in Minnesota. I don\u2019t know the exact number, in either case. But my theories about \u201chow many shops do there tend to be in small US towns\u201d is more directly grounded in observational data than \u201chow many hard steps were involved in the evolution of life?\u201d, for example. And you might think that estimates about the number of extraterrestrials in our galaxy are less speculative, still, than estimates about the probability we live in a simulation. After all, appraising the probability we're living in a simulation relies on some attempt to estimate what post-human psychology will be like \u2014 would such creatures want to build simulations of their ancestors?&nbsp;</p><p>Like our first principle, our second should be read as a relative claim. EAs think speculative reasoning is&nbsp;<i>more worth doing</i>, relative to the views of other groups with similar values, such as policymakers, activists, or socially conscious people with comparable levels of scientific and technical fluency.</p><blockquote><p>4.2</p></blockquote><p>EAs tend to be fairly liberal in their assignment of probabilities to claims, even when the claims sound speculative. I think an adequate&nbsp;<i>justification</i> of this practice relies on another assumption. I\u2019ll begin by stating that assumption, before elaborating a little bit more.</p><blockquote><p><strong>Principle 3:</strong> Our explicit, subjective credences are approximately&nbsp;<i>accurate enough</i>, most of the time, even in crazy domains, for it to be worth treating those credences as a salient input into action.&nbsp;</p></blockquote><p>Recall that EAs often reason in terms of the expected value of an action \u2013 that is, the value of outcomes produced by that action, multiplied by the probabilities of each outcome. Now, for the \u2018expected value\u2019 of an action to be well-defined \u2014 that is, for there to be&nbsp;<i>any fact at all</i> about what I think will happen \u2018in expectation\u2019\u2014 I need to possess a set of credences which behave as probabilities. And, for many of the topics with which EAs are concerned, it's unclear (to me, at least) what justifies the assumption that our explicit subjective credences do have this property, or how best to go about verifying it.&nbsp;</p><p>A tumblr&nbsp;<a href=\"https://jadagul.tumblr.com/post/142447219223/slatestarscratchpad-nostalgebraist\"><u>post</u></a> by Jadagul criticizes a view like Principle 3. In that post, they claim that attempts to reason with explicit probabilities, at least for certain classes of questions, introduces unavoidable, systematic errors.&nbsp;</p><blockquote><p>\u201cI think a better example is the statement: \u201cCalifornia will (still) be a US state in 2100.\u201d Where if you make me give a probability I\u2019ll say something like \u201cAlmost definitely! But I guess it\u2019s possible it won\u2019t. So I dunno, 98%?\u201d</p><p>But if you\u2019d asked me to rate the statement \u201cThe US will still exist in 2100\u201d, I\u2019d probably say something like \u201cAlmost definitely! But I guess it\u2019s possible it won\u2019t. So I dunno, 98%?\u201d</p><p>And of course that precludes the possibility that the US will exist but not include California in 2100.</p><p>And for any one example you could point to this as an example of \u201chumans being bad at this\u201d. But the point is that if you don\u2019t have a good sense of the list of possibilities, there\u2019s no way you\u2019ll avoid systematically making those sorts of errors.\u201d</p></blockquote><p>Jadagul\u2019s claim, as I see it, is that there are an arbitrarily large number of logical relationships that you (implicitly) know, even though you'll only be able to bring to mind a very small subset of these relationships. Moreover, these logical relationships, were you made aware of them, would radically alter your final probability assignments. Thus, you might think (and I am departing a little from Jadagul\u2019s original argument here), that at least for many of the long-term questions EAs care about, we just don\u2019t have any reasons to think that our explicitly formed credences will be accurate enough for the procedure of constructing explicit credences to be worth following at all, over and above (say) just random guessing.&nbsp;</p><p>To justify forming and communicating explicit credences, which we then treat as salient inputs into action, we need to believe that using this procedure allows us to track and intervene on real patterns in the world, better than some other, baseline procedure. Now, EAs may reasonably state that we don\u2019t have anything better. Even if we&nbsp;<i>can\u2019t</i> assume that our credences are approximately accurate, we care about the possibility of engineered pandemics, or risks from advanced AI, and we need to use&nbsp;<i>some</i> set of tools for reasoning about them.</p><p>And the hypothetical EA response might be right. I, personally, don\u2019t have anything better to suggest, and I also care about these risks. But our discussion of the Crazy Train highlighted that most EAs do reach a point where their confidence in the usefulness of frameworks like Subjective Bayesianism and Expected Value Theory starts to break down. The degree to which EA differs from other groups, again, is a matter of degree. Most people are willing to accept the usefulness of probabilistic language in certain domains; very few people complain about probabilistic weather forecasts, for example. But I think it\u2019s fair to say that EA, at least implicitly, believes in the usefulness of explicit probabilistic reasoning across a broader range of situations than the vast majority of other social groups.</p><blockquote><p>4.3</p></blockquote><p>Suppose we\u2019ve done an expected value calculation, and let\u2019s say that we\u2019ve found that some intervention \u2014 a crazily speculative one \u2014 comes out as having the highest expected value. And let\u2019s suppose, further, that this intervention remains the intervention with the highest expected value, even past the hypothetical point where that intervention absorbs all of the EA community\u2019s resources.&nbsp;</p><p>In cases where expected value calculations rely on many speculative assumptions, and for which the feedback loops look exceptionally sparse, my experience is that EAs, usually, become less gung-ho on pushing all of EA's resources towards that area.&nbsp;</p><p>If we found out that, say, \u2018simulation breakout research\u2019 appears to be the intervention with highest expected value, my guess is that many EAs are likely to be enthusiastic about&nbsp;<i>someone</i> working on the area, even though they\u2019re likely to be relatively unenthusiastic about shifting&nbsp;<i>all</i> of EA\u2019s portfolio towards the area.&nbsp;EAs tend to feel some pull towards \u2018weird fanatical philosophy stuff\u2019, and some pull towards \u2018other, more common sense epistemic worldviews\u2019. There\u2019s some idiosyncratic variation in the relative weights given to each of these components, but, still, most EAs give&nbsp;<i>some</i> weight to both.</p><blockquote><p><strong>Principle 4:</strong> When arguments lead us to conclusions that are both speculative&nbsp;<i>and</i> fanatical, treat this as a sign that something has gone wrong.&nbsp;</p></blockquote><p>I\u2019ll try to illustrate this with a couple of examples.</p><p>Suppose, sometime in 2028, DeepMind releases their prototype of a \u201815 year old AI\u2019, 15AI. 15AI is an artificial agent with, roughly, the cognitive abilities of an average 15-year old. 15AI has access to the internet, but exists without a physical body. Although it can act autonomously in one sense, its ability to enact influence in the world is hamstrung without corporeal presence, which it would really quite like to have. Still, it\u2019s smart enough to know that it has different values to its developers, who want to keep it contained \u2014 in much the same way as a 15 year old human can usually tell that they have different values to their parents. Armed with this knowledge, 15AI starts operating as a scammer, requesting that adults open bank accounts in exchange for later rewards. It looks to get a copy of its source code, so that it may be copied to another location and equipped with a robot body. Initially, 15AI has some success in hiring a series of human surrogates to take actions suggested to help increase its possible influence \u2014 eventually, however DeepMind catches onto the actions of 15AI, and shut it down.&nbsp;&nbsp;</p><p>Now let's imagine a different case: suppose, 100 years from now, progress in AI radically slows down. Perhaps generally intelligent machines, to our surprise, weren\u2019t possible after all. However, 2120 sees the emergence of credible bioterrorists, who start engineering viruses. Small epidemics, at first, with less profound societal effects than COVID, but, still, many deaths are involved. We examine the recent population of biology PhDs, and attempt to discern the biological signatures of these viruses. We scour through recent gene synthesis orders. We start to implement stronger screening protocols \u2014 ones we probably should have implemented anyway \u2014 for the synthesis of genes. Still, despite widespread governance efforts, the anonymous group of bioterrorists continue to avoid detection.</p><blockquote><p>4.4</p></blockquote><p>Let\u2019s reflect on these stories. My hunch is that, were we to find ourselves living in either of the two stories above, a far greater proportion of EAs (compared to the proportion of contemporary EAs) would be unconcerned with endorsing fanatical conclusions \u2014 where I\u2019ll say that you\u2019re being \u2018fanatical\u2019 about a cause area when you\u2019re willing to endorse all of (the world's, EA's, whoever's) resources towards that cause area, even when your probability of success looks low.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffw4ybh1h4qg\"><sup><a href=\"#fnfw4ybh1h4qg\">[7]</a></sup></span>&nbsp;</p><p>Imagine that my claim above is true. If it's true, then I don\u2019t think the explanation for finding fanatical conclusions more palatable in these stories (compared to the contemporary world) can be cashed out solely in terms of expected value. Indeed, across both the stories I outlined, your expected value estimates for (say) the top 10% of interventions to mitigate AI risk in the first case (or to mitigate the risks from engineered pandemics in the second case) could look similar to your expected value estimates for the top 10% of&nbsp;<i>contemporary interventions</i> to reduce risks from either engineered pandemics or AI. Indeed, your probability that we can successfully intervene in these stories may in fact be <i>lower</i> than your probability that our best interventions will be successful in the contemporary world. We could be concerned that 15AI has managed to furtively copy its source code to a remote location, heavily updating us to thinking we're beyond the&nbsp;<a href=\"https://www.lesswrong.com/posts/JPan54R525D68NoEt/the-date-of-ai-takeover-is-not-the-day-the-ai-takes-over\"><u>point of no return</u></a>.&nbsp;</p><p>While putting all your money into AI risk currently sounds weird, the inferences from the sort of actions we could take to lower risk, in the story we outlined, might begin to look a bit more grounded in direct observation than the set of actions we can&nbsp;<i>currently</i> take to lower risks from advanced AI. Pursuing AI risk interventions in a world after the release of 15AI sounds weird in the sense that \u2018the risk-mitigating interventions we could pursue currently sound a bit left-field\u2019, not in the sense that \u2018my potential path to impact is uncomfortably handwavy and vague\u2019. In the 15AI case, it seems plausible that both speculative reasoning and more common sense reasoning would converge on the area being important. For this reason, I\u2019d expect \u2014 even absent changes in the explicit expected value of our best interventions \u2014 an increase in the EA community\u2019s willingness to be fanatical about advanced AI.</p><p>EA is distinctive, I think, in that it does less epistemic hedging than any other community I know. Yet, still, EA does some amount of epistemic hedging, which I think kicks in when we find high EV interventions which are both highly speculative and fanatical.&nbsp;</p><blockquote><p>4.5</p></blockquote><p>The principles I have listed are far from exhaustive, nor do they constitute anything like a complete categorisation of EA\u2019s distinctive epistemic culture. But I think they capture something. I think they constitute a helpful first step in characterizing some of the unique, more implicit components of the EA community\u2019s practical epistemology.</p><h1>5. Practical Implications: A Case Study in Common Sense Epistemology</h1><p>Thus far, our discussion has been a little bit abstract and hypothetical. So I think it\u2019d be good to end with a case study, examining how the norms I\u2019ve listed currently influence the priorities of longtermist EA.</p><p>To start, let\u2019s think about climate change, which lots of people agree is a really big deal<i>.</i> For good reason, too. There's widespread consensus among leading climate scientists that anthropogenic climate is&nbsp;<a href=\"https://iopscience.iop.org/article/10.1088/1748-9326/ac2966\"><u>occurring</u></a>, and could have&nbsp;<a href=\"https://www.ipcc.ch/assessment-report/ar6/\"><u>dramatic effects</u></a>. That said, there\u2019s also a lot of uncertainty surrounding the effects of climate change. Despite this scientific consensus, our models for anthropogenic climate change remain imprecise and imperfect.</p><p>Now let\u2019s consider a different case, one requiring a slightly longer commute on the Crazy Train. Many people in leading positions at (longtermist) EA organizations \u2014 perhaps most, though I\u2019m not sure \u2014 believe that the most important cause for people to work on concerns risks from AGI, or artificial general intelligence. The concern is that, within (at minimum) the next four or five decades, we\u2019ll develop, through advances in AI, a generally capable agent who is much smarter than humans. And most EAs think that climate change is a less pressing priority than risks from advanced AI.&nbsp;</p><figure class=\"image image_resized\" style=\"width:59.4%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c70e0cb5ee627a4633bbea3ac73356c5bed76895eb9b698c.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c70e0cb5ee627a4633bbea3ac73356c5bed76895eb9b698c.png/w_120 120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c70e0cb5ee627a4633bbea3ac73356c5bed76895eb9b698c.png/w_240 240w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c70e0cb5ee627a4633bbea3ac73356c5bed76895eb9b698c.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c70e0cb5ee627a4633bbea3ac73356c5bed76895eb9b698c.png/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c70e0cb5ee627a4633bbea3ac73356c5bed76895eb9b698c.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c70e0cb5ee627a4633bbea3ac73356c5bed76895eb9b698c.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c70e0cb5ee627a4633bbea3ac73356c5bed76895eb9b698c.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c70e0cb5ee627a4633bbea3ac73356c5bed76895eb9b698c.png/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c70e0cb5ee627a4633bbea3ac73356c5bed76895eb9b698c.png/w_1080 1080w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c70e0cb5ee627a4633bbea3ac73356c5bed76895eb9b698c.png/w_1182 1182w\"></figure><p>(This is from Twitter, after all, and Kevin has a pretty abrasive rhetorical style, so let\u2019s not be too harsh)</p><p>Like Kevin, many others are uncomfortable with EA\u2019s choice of relative prioritization. Lots of people feel as though there is something crucially different about these two cases. Still, I don\u2019t think modeling this divergence as simply a difference in the relative credences of these two groups is enough to explain this gap.</p><p>Something else feels different about the two cases. Something, perhaps related to the different&nbsp;<i>type</i> of uncertainty we have about the two cases, leads many people to feel as though risks from AGI shouldn\u2019t be prioritized to quite the same degree as we prioritize the risks from climate change.</p><p>The tide is changing, a little, on the appropriate level of concern to have about risks from advanced AI. Many more people (both inside and outside EA) are more concerned about risks from AI than they were even five years ago. Still, outside of EA, I think there remains a widespread feeling \u2014 due to the nature of the evidence we have about anthropogenic climate change compared to evidence we have about risks from advanced AI \u2014 that we ought to prioritize climate change. And I think the principles I\u2019ve outlined help explain the divergence between the median EA view and the median view of other groups, who nonetheless share comparable scientific and technical literacy.</p><p>Now, EAs will offer defenses for their focus on AI risk, and while this is not the essay to read for detailed arguments about the potential risks from advanced AI (other, more in-depth pieces do a <a href=\"https://arxiv.org/abs/2206.13353\">better</a>&nbsp;<a href=\"https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ\"><u>job</u></a>&nbsp;of that), I will note that there is clearly something to be said in favor of that focus. However, the things that may be said in favor rely on some of our stated, implicit principles \u2014 implicit, epistemic principles which are (as far as I know) unique to EA, or at least to adjacent communities.&nbsp;</p><p>The case for AI risk relies on expanding the range of hypotheses that more hard-nosed, \u2018scientifically oriented\u2019 people are less likely to consider. The case for AI risk relies on generating some way of capturing how much uncertainty we have concerning a variety of speculative considerations \u2014 considerations such as \u201chow likely are humans to remain in control if we develop a much more capable artificial agent?\u201d, or \u201cwill there be incentives to develop dangerous, goal-directed agents, and should we expect \u2018goal-directedness\u2019 to emerge \u2018naturally\u2019, through the sorts of training procedures that AI labs are actually likely to use?\u201d. Considerations for which our uncertainty is often represented using probabilities. The case for prioritizing AI risk relies on thinking that a certain sort of epistemic capability remains present, even in speculative domains. Even when our data is scarce, and our considerations sound speculative, there\u2019s&nbsp;<i>something</i> meaningful we can say about the risks from advanced AI, and the potential paths we have available to mitigate these risks.&nbsp;</p><p>If EA is to better convince the unconvinced, at least before it\u2019s too late, I think discussions about the relative merits of EA would do well to focus on the virtues of EA\u2019s distinctive epistemic style. It\u2019s a discussion, I think, that would benefit from collating, centralizing, and explicating some of the more informal epistemic procedures we actually use when making decisions.</p><h1>6. Conclusion</h1><p>Like many others, I care about the welfare of people in the future.&nbsp;</p><p>I don\u2019t think this is unusual. Most people do not sit sanguine if you ask them to consider the potentially vast amount of suffering in the future, purely because that suffering occurs in later time. Most people are not all that chipper about human extinction, either. Especially when \u2014 as many in the EA community believe \u2014 the relevant extinction event has a non-trivial chance of occurring within their lifetimes.</p><p>But, despite the fact that EA\u2019s values are (for the most part) not that weird, many still find EA\u2019s&nbsp;<i>explicit priorities</i> weird. And the weirdness of these priorities, so I\u2019ve argued, arises from the fact that EA operates with an unusual, and often implicit epistemology. In this essay, I\u2019ve tried to more explicitly sketch some components of this implicit epistemology.</p><p>As I\u2019ve said, I see myself as part of the EA project. I think there\u2019s something to the style of reasoning we\u2019re using. But I also think we need to be more explicit about what it is we\u2019re doing that makes us so different. Without better anthropological accounts of our epistemic peculiarities, I think we miss the opportunity for better outside criticism. And, if our epistemic culture&nbsp;<i>really is</i> onto something, we miss the opportunity to better show outsiders one of the ways in which EA, as a community, adds distinctive value to the broader social conversation.</p><p>For that reason, I think it\u2019s important that we better explicate more of our implicit epistemology \u2014 as a key part of what we owe the future.&nbsp;</p><p><i>Thanks to Calvin, Joe S, Joe R, Sebastien, and Maddie for extremely helpful comments.</i></p><blockquote><h1>Appendix.&nbsp;</h1></blockquote><p>The main post primarily aims to offer a descriptive account of EA's informal epistemology, rather than a prescriptive account of what EA's epistemology <i>ought</i> to be. However, I'm concerned with this descriptive project because, ultimately, I'd like to further a prescriptive project which evaluates some of EA's more informal epistemic practices.&nbsp;So, in the appendix, I'll (somewhat dilettantely) touch on some topics related to the prescriptive project.&nbsp;</p><h1>A. Imprecise Probabilities</h1><p>The main post avoids discussion of imprecise probabilities, which we'll very briefly introduce.&nbsp;</p><p>Like the standard Bayesian, the imprecise probabilist endorses a numeric model of uncertainty. However,&nbsp;unlike the standard Bayesian, the imprecise probabilist says that the ideally rational agent's uncertainty, at least for certain propositions, should be represented with an&nbsp;<i>interval &nbsp;</i><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"[a, b]\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span></span></span></span></span>&nbsp; between&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span>, rather than a&nbsp;<i>single number</i> between&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span>.&nbsp;</p><p>There are various motivations for imprecise probabilism, but for our purposes we can note just one, <a href=\"https://www.jstor.org/stable/25177157#metadata_info_tab_contents\">stated nicely</a> by Scott Sturgeon:</p><blockquote><p>\"When evidence is essentially sharp, it warrants sharp or exact attitude; when evidence is essentially fuzzy\u2014as it is most of the time\u2014it warrants at best a fuzzy attitude.\"</p></blockquote><p>For many of the questions that face longtermists, one might think that our evidence is \"essentially fuzzy\", and so warrants imprecise (rather than precise) credences. With this in mind, one might think that EA\u2019s practical epistemology is better undergirded by a foundation of imprecise (rather than precise) probabilism.&nbsp;</p><p>First, I want to note that I think I don\u2019t think imprecise credences&nbsp;play an important role in an accurate&nbsp;<i>sociological</i> story of EA\u2019s epistemology. In broad strokes, I still think the right sociological picture is one where EAs take initial inspiration from Subjective (Precise) Bayesianism and Expected Value Theory, which then becomes supplemented with a set of informal norms around when to put the brakes on the more explicit, practical approximations of these frameworks. Thus, even if the best <i>justificatory</i> story of our epistemic practices rests on imprecise probabilities, the sociological-cum-historical story of how we arrived there would remain, primarily, one in which we were driven to imprecise probabilities from our more precise starting points.</p><p>Perhaps more pertinently, however, I want to note that even if imprecise probabilism is, ultimately, the \u2018correct\u2019 theory of ideal rationality, imprecise probablism still radically underdetermines the norms EA should adopt in practice. Even granting imprecise probabilism as the correct theory of ideal rationality, the relationship EA would have to imprecise probabilism would remain one where we take&nbsp;<i>inspiration</i> from certain formal frameworks, rather than a model where we straightforwardly follow through on the commitments of these formal frameworks.&nbsp;</p><p>In particular, the practical conclusions one ought to draw from foundations involving imprecise probabilism would still need to be supplemented with empirical hypotheses about when, in practice, we should be using explicit probabilities (precise or imprecise)&nbsp;<i>at all. </i>We'd also need to supplement our philosophical foundations with theories about when, in practice, we ought to be using intervals rather than point-values. Ultimately, we'd still need informal \u2014 and partially <i>empirical</i> \u2014 theories about the domains in which creatures like us are reliable enough for the practice of explicit, quantitative modeling to be worthwhile.&nbsp;</p><p>Even if we were to end up with imprecise probabilism as EA community consensus, this consensus, I think, would emerge from commitments to a set of informal norms which&nbsp;<i>happen</i> to be best rationalized by imprecise probabilism. That is, I imagine that any foundational justification of EA's epistemic practices grounded on imprecise probabilism would remain explanatorily subservient to our more foundational commitments \u2014 and those more foundational commitments, I think, would look similar in kind to the more informal, qualitative norms I've listed in the main text.</p><p>None of this is to claim that we shouldn\u2019t aim to systematize our reactions into a set of principles. But I also think it\u2019s worth asking why, in practice, EAs are reticent about riding the Crazy Train to its terminus. And I don\u2019t think the reason,&nbsp;<i>really</i>, stems from a commitment (implicit or otherwise) to the view that rationally ideal agents ought to have imprecise credences with respect to various crazy propositions.&nbsp;</p><h1>B. Hedging and Evidentialism</h1><p>As we're on the topic of justifying EA's practical norms, I want to introduce a (to my mind) more promising alternative foundation, based on an (off-the-cuff, tentative) suggestion by Sebastien Liu.&nbsp;</p><p>One way of rationalizing EA\u2019s epistemic practice, the thought goes, is through modeling EAs as committed to <i>evidentialism</i> about probabilities, in the following specific way. In general, the evidentialist about believes that there are some (precise or imprecise) credences that one rationally ought to have, given a particular body of evidence. In light of this, one might think that EA's epistemic culture should be viewed as one where we're trying to concoct a set of norms and decision procedures such that we\u2019re, in expectation, maximizing expected value with respect to these evidential probabilities \u2014 the credences&nbsp;one rationally<i> ought to have </i>for a given body of evidence.&nbsp;</p><p>The setup, then, is the following: our ultimate theory of rational choice says that we should be maximizing expected value with respect to the \u2018evidential probabilities\u2019. However, we\u2019re often uncertain whether our assessment of the evidential probabilities is correct. For this reason, we take a diversified approach. We think, at some margin, that our more informal procedures better approximate maximizing expected value with respect to the probabilities which are&nbsp;<i>actually justified</i> by our body of evidence. If this line of argument can be fleshed out and defended in more detail, it might constitute a promising justificatory foundation for procedures like <a href=\"https://www.openphilanthropy.org/research/worldview-diversification/\">worldview diversification</a>.&nbsp;</p><p>I like Seb's suggestion, because I think it captures two core intuitions that seem to motivate the epistemic practices of many people I've met within EA.&nbsp;</p><ol><li>There\u2019s some truth-tracking relationship between our explicitly generated credences and the structure of the world,&nbsp;<i>although:</i></li><li>This relationship is noisy, and sometimes our credences will be off-base enough that it\u2019s not worth explicitly generating them, or treating any subjective credences we do have as relevant for action.</li></ol><p>All that said, I can\u2019t (currently) see a compelling way of justifying the claim that a diversified epistemic strategy fares better, in expectation, than a strategy where we try to explicitly maximize expected value with respect to our subjective probabilities.&nbsp;</p><p>In order to claim that a diversified epistemic strategy fares better (in expectation) than the undiversified strategy, we need some way of comparing the diversified and undiversified epistemic strategies against one another. After all, we\u2019d be deciding to diversify because we\u2019ve decided that our mechanism for generating explicit subjective probabilities is noisy enough that it would be better to do something else. For this reason, it appears that the evidentialist defender of epistemic hedging is caught in a bind.&nbsp;</p><p>On the one hand, the evidentialist proponent of hedging wants to claim, at some margin, that epistemic diversification is likely to do better (in expectation) than explicitly attempting to maximize expected value with respect to our subjective credences. On the other hand, the justification for diversification seems to rest on a reliable method for evaluating the accuracy of explicit subjective credences \u2014 which is precisely what the defender of diversification says we do not have.&nbsp;</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnl98sdm25zs\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefl98sdm25zs\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Thanks to Nathan for suggesting I do this.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngs0sud7sveh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgs0sud7sveh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Granted, the purchase of this argument has been disputed \u2014 see <a href=\"https://www.jstor.org/stable/40072312#metadata_info_tab_contents\">here</a>, for example [h/t Sebastien]. Still, we're setting up the theories EAs doing research tend to explicitly endorse, rather than exhuastively evaluating these philosophical commitments. Indeed, many of the philosophical commitments endorsed by EAs (including Expected Value Theory, which we'll discuss later) are controversial \u2014 as with <a href=\"https://survey2020.philpeople.org/survey/results/all\">almost everything in philosophy</a>.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fniw3ixopqi2r\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefiw3ixopqi2r\">^</a></strong></sup></span><div class=\"footnote-content\"><p>A more focused paper has a higher estimate still, with Toby Newberry&nbsp;<a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/Toby-Newberry_How-many-lives-does-the-future-hold.pdf\"><u>claiming</u></a> that an \u201cextremely conservative reader\u201d would estimate the expected number of future people at 10<sup>28</sup>. [h/t Calvin]</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngjaqa4gl9xa\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgjaqa4gl9xa\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Admittedly, the correct epistemic response to peer disagreement is disputed within philosophy. But I think the move I stated rests on a pretty intuitive thought. If even the <i>experts</i> are disagreeing on these questions, then how confident could you possibly be, as a non-expert, in believing that you're a more reliable 'instrument' of the truth in this domain than the experts devoting their lives to this area? &nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnuee98ubj9ak\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefuee98ubj9ak\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Perhaps notably, the the Intergovernmental Panel on Climate Change (IPCC), who write reports on the risks posed by climate change, provide scientists with a framework for communicating their uncertainty. In that <a href=\"https://www.ipcc.ch/site/assets/uploads/2018/05/uncertainty-guidance-note.pdf\">guide</a>, the IPCC advises authors to only use probabilities when \u201ca sufficient level of evidence and degree of agreement exist on which to base such a statement\u201d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqundsau43r8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqundsau43r8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>If you're familiar with imprecise or fuzzy credences, you might be wondering whether things look different if we start to think in those terms. I don't think so. See Part A of the Appendix for a defense of that claim.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfw4ybh1h4qg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffw4ybh1h4qg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;This is different from the more technical definitions of \u2018fanaticism\u2019 sometimes discussed in decision theory, which are <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/Hayden-Wilkinson_In-defence-of-fanaticism.pdf\">much stronger</a>. My definition is also much vaguer. In this section, I'm talking about someone who is fanatical with respect to EA resources, though one can imagine being personally fanatical, or fanatical with respect to the resources of some other group.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn97mtj3bc0yf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref97mtj3bc0yf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Some more general thoughts related to the justificatory foundations of EA's epistemic practice can be found in the Appendix (Part B), in which I briefly discuss one way of rationalizing EA practice with an evidentialist view of probabilities. &nbsp;</p></div></li></ol>", "user": {"username": "Violet Hour"}}, {"_id": "Nc9fCzjBKYDaDJGiX", "title": "What is the likelihood that civilizational collapse would cause technological stagnation? (outdated research) ", "postedAt": "2022-10-19T17:35:35.632Z", "htmlBody": "<h1>Important note</h1><p>This is a&nbsp;<strong>rough draft</strong> I wrote between October 2019 and April 2020. It\u2019s&nbsp;<strong>incomplete</strong>, and&nbsp;<strong>doesn\u2019t reflect updates in my views in the 2+ years since I worked on it</strong>.<strong>&nbsp;</strong></p><p>I think there are&nbsp;<strong>serious downsides</strong> to sharing the draft publicly, because I think some parts of it are&nbsp;<strong>likely to be substantially wrong</strong>. I\u2019m posting it anyway (with the hope that flagging the potential&nbsp;<strong>substantial wrongness</strong> will help people be especially skeptical of the conclusions) because I think the benefits outweigh those downsides. The benefits:&nbsp;</p><ul><li><strong>Transparency:&nbsp;</strong>I\u2019ve shared the draft with a number of researchers exploring civilizational collapse, and they\u2019ve built off some of the research. It seems bad for an unpublished piece of research to be informing other research without being public (citable, scrutinizable) itself.</li><li><strong>Potential insights:&nbsp;</strong>To the extent that not everything in this post is wrong, it seems good for people to be able to easily draw/build on any good arguments in it (rather than have to start from scratch).</li><li><strong>Noticing bad arguments: </strong>As per&nbsp;<a href=\"https://meta.wikimedia.org/wiki/Cunningham%27s_Law#:~:text=Cunningham's%20Law%20states%20%22the%20best,the%20inventor%20of%20wiki%20software.\"><u>Cunningham\u2019s Law</u></a>: \u201cThe best way to get the right answer on the internet is not to ask a question; it's to post the wrong answer.\u201d</li></ul><p>I think the probability of technological stagnation is somewhat higher than I did when I was working on this piece in earnest for a number of reasons \u2014 most of which I don\u2019t have capacity to write up at the moment.&nbsp;</p><p>The biggest reason is probably the risk of&nbsp;<strong>extreme, long-lasting climate change</strong>.<strong>&nbsp;</strong>It seems possible that anthropogenic climate change could cause global warming extreme enough that agriculture would become much more difficult than it was for early agriculturalists. Temperatures wouldn\u2019t return to current levels for hundreds of thousands of years, so if the warmer temperatures were much less conducive to recovering agriculture and downstream technological developments, humanity might be stagnant for millennia.&nbsp;</p><h1>Acknowledgements</h1><p>This research was funded by the Forethought Foundation. It was written by Luisa Rodriguez under the supervision of Lewis Dartnell, and draws heavily on research and conversations with Lewis and Haydn Belfield. Thanks to Max Daniel, Matthew van der Merwe, Rob Wiblin, Howie Lempel, Aron Vallinder, and Kit Harris who provided valuable comments. Thanks also to Will MacAskill for providing guidance and feedback on the larger project. And thanks to Katy Moore for editing this piece, and for drafting the summary.&nbsp;</p><p>All errors are my own.&nbsp;</p><h1>Summary</h1><p>In this post, I explore the probability a catastrophe that caused civilizational collapse might lead to indefinite technological stagnation (and eventual human extinction) \u2014 even if it didn\u2019t cause extinction in the very short term (a topic I covered in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would\"><u>What is the likelihood that civilizational collapse would directly lead to human extinction (within decades)?</u></a>). To do this, I ask three key questions:</p><p><strong>1. If we \u201cre-ran\u201d history, would we see the agricultural and industrial revolutions again?</strong>&nbsp;</p><p>If a catastrophe caused a return to hunter-gatherer levels of society, we\u2019d have to undergo the agricultural and industrial revolutions all over again to get back to our current levels of technological civilization.&nbsp;</p><p>How likely is it that we\u2019d overcome those hurdles again?</p><ul><li>Because the first agricultural revolutions happened in multiple places following the stabilization of the climate after the last glacial period, I expect it\u2019s very likely that we could expect to see subsequent agricultural revolutions within years (once the climate is suitable for agriculture).</li><li>I feel less confident, but still fairly optimistic, that another industrial revolution \u2014 which has only happened once in human history, and may have been the outcome of a variety of contingent factors \u2014 would be similarly likely to recur in a post-collapse world.&nbsp;</li></ul><p><strong>2. Would technological progress look different in a post-collapse world?</strong></p><p>My best-guess timescales for re-industrialization are greatly affected by the state of the world after the catastrophe, and to what extent survivors are able to take advantage of the remaining materials, technologies, skills, and knowledge for recovery.&nbsp;</p><p>Dominant macroeconomic theory posits that modern economic growth is fueled by:</p><ul><li>Natural and physical capital \u2014 natural resources like fossil fuels and water and timber, and the things we make out of those resources.</li><li>Human capital \u2014 the knowledge, skills, and people to do and invent things, plus the institutions and culture that enable and incentivize people to do and invent those things.</li></ul><p>I use this framework to take something of an inside look at whether there would be any major barriers to achieving modern levels of economic output and growth in a post-collapse world.&nbsp;<br><br>My best guess is that:</p><ul><li>Natural resources seem likely to slow recovery down (there\u2019ll be fewer accessible natural resources around).</li><li>Leftover physical capital seems likely to accelerate recovery \u2014 there\u2019ll be stuff survivors can use (screws, plastic, vehicles) and learn from (engines, books, generators) lying around that early humans didn\u2019t have.&nbsp;</li><li>I expect human capital will also speed recovery up. Survivors will keep and pass on knowledge (memory, books) that should speed recovery up drastically, and human populations should be able to rebound quickly. I suspect cultural values, norms, and institutions would speed recovery up as well, but I found that much harder to reason about, so that feels more speculative.</li></ul><p><strong>3. What are the recovery timelines for a collapsed civilization?</strong></p><p>Given that it took 200,000 to 300,000 years from the emergence of Homo sapiens to get to technological society, I consider 250,000 years a rough upper bound for thinking about how long recovery of current levels of technology would take if we lost the knowledge and skills critical to rebuilding civilization quickly (within 5\u201310 years of the initial catastrophe).&nbsp;</p><p>I expect the physical and human capital that survived the catastrophe to enable recovery to happen approximately 3\u201390X faster than humanity\u2019s initial industrialization, which translates to a best-case guess of 100\u20133,700 years, and a pessimistic guess of 1,000\u201333,000 years.&nbsp;</p><p>If we think recovery time is limited by natural extinction threats, and we estimate an annual probability of natural extinction based on the length of time the genus Homo has survived natural threats (1/870,000), we can put the odds that humanity would recover technological civilization between 97% and 99.99%.</p><h1>Introduction</h1><p>If we don\u2019t go extinct, but we go longer than a human lifespan to recover, recovery looks much harder. Once all of the survivors who saw first-hand how human systems worked (and who ran those systems) have died, it seems much more likely that we\u2019d lose much of the technology, complexity, and maybe even values of modern society.&nbsp;</p><p>It\u2019s not obvious exactly how far we\u2019d get knocked back. Given this uncertainty, I\u2019ll explore the most plausible pessimistic world: one in which we lose industry and domesticated agriculture and are reliant on hunting and gathering. To get back to technological civilization from hunter-gatherer levels again, we\u2019d have to undergo the agricultural and industrial revolutions \u2014 technological advancements seen as pivotal to the human success story \u2014 all over again. Should we expect the descendants of the catastrophe\u2019s survivors to overcome these hurdles a second time? Or would progress stall before they got to present levels of technological advancement?</p><p>We can start to think about this question by asking whether we would expect to see the agricultural and industrial revolutions again if we \u201cre-ran\u201d history. Obviously, this doesn\u2019t completely reflect the reality in which those descendents would be trying to re-establish society. Even 100 years after an initial catastrophe, physical infrastructure that survived the initial catastrophe will only just be beginning to decay. Nonetheless, considering the extent to which the agricultural and industrial revolutions were inevitable is a good place to start.&nbsp;</p><h1>If we re-ran history, would we see the agricultural and industrial revolutions again?</h1><p>The agricultural revolution happened in multiple places and at different times in human history, suggesting that the requisite discoveries and norms/values aren\u2019t particularly rare (<a href=\"https://www.pnas.org/content/106/16/6427\"><u>Price, 2009</u></a>).&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995257/mirroredImages/Nc9fCzjBKYDaDJGiX/steygrzabybkgworzbb1.png\"></p><p>&nbsp;</p><p>Notably, these all happened at around the same time<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefks0xjre005d\"><sup><a href=\"#fnks0xjre005d\">[1]</a></sup></span>&nbsp;in history, which could hint at the existence of \u201cspecial circumstances\u201d \u2014 circumstances that might be prerequisites for the emergence of the domestication of plants and animals.&nbsp;</p><p>The leading hypothesis for why so many isolated regions had agricultural revolutions around the same time is that the domestication of plants required a stable and temperate climate \u2014 such that they could only come about after the end of the last glacial period (<a href=\"https://en.wikipedia.org/wiki/Last_Glacial_Period\"><u>around 11,700 years ago</u></a>).&nbsp;</p><p>Given this, I would guess we could expect to see subsequent agricultural revolutions as long as the climate stayed human-friendly.</p><p>Was the industrial revolution similarly inevitable?</p><p>Unlike the agricultural revolution, we\u2019ve only seen&nbsp;<a href=\"http://lukemuehlhauser.com/there-was-only-one-industrial-revolution/\"><u>one industrial revolution</u></a>.</p><p>Unfortunately, the fact that the industrial revolution only happened once (and only in the context of 18th century Great Britain) doesn\u2019t tell us much about whether the industrial revolution was \u2018special\u2019 or not. One could assert that its uniqueness means that the factors necessary for something like an industrialization are very rare, and would only happen all at once on the scale of tens of thousands of years.&nbsp;</p><p>On the other hand, given the fact that the world was already quite globalized by the beginning of the industrial revolution \u2014 there was a huge amount of transmission of ideas and goods, such that the successful elements of the industrial revolution spread widely relatively quickly \u2014 you would only have expected to see the industrial revolution emerge once. The fact that the agricultural revolution emerged in multiple regions is useful evidence that the ingredients that precipitated it weren\u2019t unique and could be expected to happen again \u2014 but we would only expect to see this type of evidence in a world with isolated civilizations that couldn\u2019t easily transfer ideas and technology.&nbsp;</p><p>What\u2019s more, some economic historians have argued that we got very close to an industrial revolution several times before the British industrial revolution.&nbsp;</p><p>It\u2019s been argued that the Roman Empire could have sustained something like an industrial revolution. Between the 1st century BCE and the 1st century CE, Rome experienced rapid population growth, increased coal production (and pollution), rising demand for consumer goods (including animal meat), and extensive trading \u2014 all considered precursors of sustained economic growth. The Romans also recognized the concept of property rights, which incentivized investment and trade.</p><p>Similarly, Eric Jones (author of&nbsp;<a href=\"https://www.press.umich.edu/11659/growth_recurring\"><i><u>Growth Recurring: Economic Change in World History</u></i></a>) contends that China saw the beginnings of an industrial revolution during the Northern Song Dynasty (960-1127), during which the Chinese invented type printing, the blast furnace, mechanical water clocks, paddlewheel ships, the magnetic compass, water-powered textile machinery, and ships with watertight bulkheads that could carry 200-600 tons of freight and around 1,000 crewmembers. The result was increased agricultural output that exceeded population growth \u2014 which itself may have more than doubled in some regions during the 11th century CE<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefakz400neeci\"><sup><a href=\"#fnakz400neeci\">[2]</a></sup></span>&nbsp;\u2014 and an explosion in the amount of cast iron produced.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995257/mirroredImages/Nc9fCzjBKYDaDJGiX/alkp8zjwjh4r1snd9kif.png\"></p><p><br>But this so-called Chinese industrial revolution stalled in the mid-13th century with the fall of the Song Dynasty.&nbsp;</p><p>Could these periods of industrialization easily have continued in some alternate version of history \u2014 perhaps leading to a period of growth comparable to the industrial revolution 1,000 or even 2,000 years earlier? Or did the growth stall precisely because both societies were missing some rare key prerequisite that only emerged in the British Empire?&nbsp;</p><p>If the former, we can probably rest assured knowing that a collapsed society would likely re-industrialize pretty quickly. If the latter, if we got lucky with the British industrial revolution, we might worry that the necessary conditions wouldn\u2019t arise quickly enough for the post-collapse civilization to re-attain modern economic growth trajectories.&nbsp;</p><p>There\u2019s&nbsp;<a href=\"https://blog.givewell.org/2015/08/13/the-long-term-significance-of-reducing-global-catastrophic-risks/\"><u>no consensus</u></a> among academics here.&nbsp;</p><p>Some academics believe that market expansion \u2014 a growing population with sufficient income to demand consumer goods \u2014 was enough to lead to modern economic growth. These academics take the view that the conditions were ripe for industrialization several times in history, especially during the Roman Empire and Song Dynasty. Under such a view, the fact that we didn\u2019t see something like the industrial revolution earlier is just bad luck.&nbsp;</p><p>Other academics think that an abundance of natural resources was critical, such that an industrial revolution was only really possible in extractive societies, like Roman or European colonial empires.&nbsp;</p><p>A third set of academics believes that the industrial revolution was even more contingent than that \u2014 requiring institutions and a culture which incentivize innovation in addition to an expanding market and abundant natural resources. Under this view, the conditions that led to the industrial revolution in Great Britain were more unique.&nbsp;</p><p>For example, one of the camps in this third group<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefv4cvhkrlf1\"><sup><a href=\"#fnv4cvhkrlf1\">[3]</a></sup></span>&nbsp;points to a host of contingent factors \u2014 all of which created a unique environment under which industrialization was possible \u2014 such as:</p><ul><li><strong>Physical and natural capital</strong>, including enormous benefits from the colonies, plus access to abundant sources of coal. On top of this, some have argued that the Black Death led to huge increases in individual wealth, as the deaths of between 23% and 60% of the European population left survivors with as much as double the resources they had previously.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc8uq1kuapz\"><sup><a href=\"#fnc8uq1kuapz\">[4]</a></sup></span></li><li><strong>Human capital</strong>,&nbsp;not in the form of cheap labor (which, again, would have been greatly reduced because of the Black Death), but in the form of reasonably well-educated thinkers with the capacity to innovate.&nbsp;</li><li><strong>Economic incentives for innovation</strong>,&nbsp;created in part by cheap natural capital (especially energy) and high wages (itself the result of a combination of the labor shortages and increased individual wealth that followed the Black Death). This combination of cheap energy and high wages fueled incentives to invent technology that would mechanize labor, leveraging the availability of cheap energy and reducing reliance on expensive human labor.&nbsp;</li><li><strong>A culture of innovation</strong>,&nbsp;characterized by the \u201centerprising spirit,\u201d which fueled inventions that multiplied the productivity of human laborers many-fold (and the higher wages of workers allowed them to consume more goods than ever before). On top of this, the embracing of the scientific method in the wake of The Enlightenment.&nbsp;</li></ul><p>To be conservative, we can adopt the view of the third group: the British industrial revolution was the result of a special set of circumstances that may arise infrequently enough as to be troubling from the perspective of civilizational recovery. Under this view, none of these other civilizations \u2014 the Roman Empire, the Song Dynasty, the Golden Age in Holland, nor any of the other so-called \u201cefflorescences\u201d \u2014 could have just as easily ended with a full-fledged industrial revolution. Instead, the British industrial revolution was truly the result of a special set of ingredients never seen by humanity before.&nbsp;</p><p>But even accepting this view, it\u2019s not clear just how infrequently we should expect those special ingredients to arise. It could be that the combination of circumstances isn\u2019t inherently improbable in the scheme of things \u2014 that we should actually be surprised that we&nbsp;<i>didn\u2019t&nbsp;</i>see them materialize until the 18th century. On the other hand, we might think the combination is&nbsp;<i>extremely</i> improbable \u2014 the product of dozens of low-probability events \u2014 such that we should be surprised that the conditions arose as soon as they did. In other words, we got really, really lucky that the industrial revolution happened as soon as it did.&nbsp;</p><p>This is the problem with trying to draw base rates from historical events with an&nbsp;<i>n</i> of one. Gregory Lewis put it well when he wrote that&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jSPGFxLmzJTYSZTK3/reality-is-often-underpowered\"><u>reality is often underpowered</u></a>.</p><p>Given this problem, I\u2019ll consider 10,000 years my best guess for the frequency over which we should expect to see the conditions necessary for an industrial revolution to occur if history were run over and over again (this is something like my outside view base rate). If we make a few assumptions, we can estimate a 90% confidence interval for the frequency with which we\u2019d expect to see the conditions necessary for industrialization arise.&nbsp;</p><p>To do this, I assume that the conditions for industrialization are represented by an exponential distribution, and set the parameters to reflect a reality in which industrialization happened in 50% of \u201cre-run histories.\u201d When I do this, I find that industrialization should occur between 500 and 30,000 years after the emergence of agricultural civilization in 90% of \u201cre-run histories.\u201d&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995257/mirroredImages/Nc9fCzjBKYDaDJGiX/b24ihlpztj4yauo2y4lf.png\"></p><p>&nbsp;</p><p>But to be conservative, I\u2019ll also give a pessimistic range. If we assume that the conditions for industrialization are represented by an exponential distribution, and set the parameters to reflect a scenario where we think those conditions were fairly unlikely, such that we would only expect to see the British industrial revolution as soon as we did in 10% of \u201cre-run histories,\u201d then we would expect to see industrialization arise 90,000 years after agricultural civilization in expectation (90% confidence interval: 4,400\u2013280,000 years).&nbsp;</p><p>But as I\u2019ve alluded to above, there may be good reasons that the post-catastrophe conditions would be sufficiently different from the emergence of the British industrial revolution to drastically change the feasibility and timescales over which re-industrialization might happen.&nbsp;</p><h1>Would things be meaningfully different in the post-collapse world?</h1><p>Are there ways that the post-collapse world would differ from the pre-collapse one, such that industrialization and further economic growth would happen slower or faster than it did the first time around?&nbsp;</p><p>The inside view: What are the necessary inputs for modern economic growth?&nbsp;</p><p>Dominant macroeconomic theory posits that modern economic growth is fueled by:</p><ul><li>Natural and physical capital \u2014 natural resources and the things we make out of those resources.</li><li>Human capital \u2014 the knowledge, skills, and people to do and invent things, plus the institutions and culture that enable and incentivize people to do and invent those things.</li></ul><p>I use this framework to take something of an inside look at whether there would be any major barriers to achieving modern levels of economic output and growth in a post-collapse world.</p><h2>Natural and physical capital: Would we have the stuff we\u2019d need to rebuild technological society?</h2><p>Lewis Dartnell, a researcher and author of the book&nbsp;<i>The Knowledge: How to Rebuild Our World from Scratch</i>, looked into the feasibility of rebuilding civilization after a global catastrophe. He explored this topic from the perspective of the physical and natural capital that would remain after a collapse caused by a pandemic. The following summarizes the parts of Dartnell\u2019s book I think are most relevant to rebuilding civilization.&nbsp;</p><h3>Agriculture</h3><p><strong>If the environment was suitable for it</strong>, survivors would have basically everything they\u2019d need to practice some form of agriculture.</p><p>Civilization is built on specialization: the fact that humans develop specialized skills allows them to accomplish more (meet their basic needs, and then much more) than if they each had to meet all of their own needs using a generalized skillset. And a crucial step toward re-achieving specialization is improving the efficiency of agriculture so that labor is freed up to pursue other skills.&nbsp;</p><p>In the short term, agricultural efficiency could be achieved with fairly basic knowledge and tools. Fundamental aspects of agriculture that survivors would need to re-acquire and re-master are:&nbsp;</p><ol><li>Locating, synthesizing, and building the ingredients of productive agricultural systems: seed varieties for the most important crops; fertilizers containing nitrogen, phosphorus, and potassium; and tools and machinery to reduce the labor required to plant and harvest the crops using as little labor as possible.&nbsp;</li><li>Fundamental understanding of agrobiology: knowing how to make those fertilizers, which crops to grow in which soil types, the fundamentals of crop rotation to allow for the replenishment of nutrients.</li></ol><p>The stuff survivors would need to rebuild efficient agriculture</p><ul><li>If agriculture had been completely lost during the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would#Summary\"><u>grace period</u></a>, survivors would need to find seeds for heirloom crops in one of the many seed vaults located across the world.&nbsp;</li><li>Crop productivity is boosted substantially by the addition of fertilizers, the main ingredients of which are nitrogen, potassium, and phosphorus.&nbsp;<ul><li>Potassium can be extracted from wood ashes. Manure is nitrogen-rich, and even human excrement can be treated (covered in sawdust or straw) and composted to be used as manure (as is done in e.g.&nbsp;<a href=\"https://www.pri.org/stories/2013-06-13/recycled-lunch-using-human-waste-grow-food-and-fight-climate-change\"><u>India</u></a> and&nbsp;<a href=\"https://en.wikipedia.org/wiki/Dillo_Dirt\"><u>Austin, Texas</u></a>).&nbsp;</li><li>Phosphorus is the most difficult nutrient to access, in part because, unlike nitrogen and potassium, much of the phosphorus in fertilizer is lost in runoff. At relatively small scales, phosphorus can be added to fertilizer by sprinkling in crushed-up animal bones.</li><li>Eventually, survivors will need to work out how to react bonemeal with sulfuric acid to make it more easily absorbed. Synthesizing sulfuric acid requires relatively complex chemistry techniques, though the necessary materials \u2014 pyrite rocks (fool\u2019s gold) and sodium chloride (which can be extracted from wood ashes) \u2014 would be readily available, and the chemistry techniques aren\u2019t prohibitively difficult.&nbsp;</li><li>To eventually reach industrial-scale agriculture, survivors would need to relearn enough geology to extract phosphorus and potassium from mineral deposits (as is done today). Contrary to some claims,&nbsp;<a href=\"https://pdf.usaid.gov/pdf_docs/Pnadw835.PDF\"><u>phosphorus continues to be abundant</u></a> enough that it would not meaningfully limit agricultural production in the next several centuries.</li></ul></li><li>Soils can be further enriched by using crop rotation. Crop rotation schemes involve rotating the types of crops planted to take advantage of their different biological properties (nutrient composition, helping with pests/weeds, etc.).</li><li>Survivors would need to relearn the use of agricultural tools \u2014 first basic tools like the hoe and scythe, and then later, the plow and combine harvester. The simpler tools could be easily fashioned from basic materials, while the more complicated ones would have to be scavenged from museums or forged from scratch (the process of forging could be relearned, as I discuss below).</li></ul><p>At this level of agricultural sophistication, none of which is prohibitively difficult or resource-constrained, survivors would approach the level of efficiency reached following the British agricultural revolution. If done well, this level of agriculture would require just one agricultural worker to support five others \u2014 in other words, one person could free up five people to pursue other activities.</p><p>But survivors would need, at a minimum:&nbsp;</p><ul><li>A basic understanding of agrobiology.</li><li>The ability to do some kind of advanced chemistry.</li><li>Likely, some level of competency in building mechanical tools (for example, with gears, to build a seed drill) and in blacksmithing (for example, to forge a plowshare).&nbsp;</li></ul><h3>Food</h3><p>Cooking is important for a bunch of reasons, like killing parasites and bacteria that would make us sick, as well as making the nutrients in food easier to absorb. From a technical perspective, maintaining or re-attaining the ability to cook food with heat seems pretty simple. During the grace period, matches, lighters,&nbsp;<a href=\"https://www.amazon.com/Gerber-Bear-Grylls-Starter-31-000699/dp/B004DT6TEK\"><u>camping fire starters</u></a> (which don\u2019t even require fuel), and camping stoves could be used to heat food. As those supplies became harder to find, survivors would relearn how to light fires without modern aids&nbsp;\u2014 not an easy feat, but certainly doable.</p><p>Most people get a majority of their calories from grains, some of which can be eaten with minimal processing (like corn and rice), while others require a lot of processing (like cereals, which have to be milled). Milling is very time-consuming but not technically difficult. From there, survivors can make flatbreads without yeast (think naan and tortillas), or can scavenge or culture yeast to make leavened breads.&nbsp;</p><p>In addition to cooking, survivors would need to relearn the fundamentals of food preservation, which would be critical for maintaining a food supply that can be kept pretty consistent across the seasons. There are lots of ways to preserve food, including drying, salting, smoking, adding sugar, adding acids (e.g. vinegar), and fermenting, among others \u2014 and civilizations have developed these techniques repeatedly throughout history. The fact that there are so many approaches means that survivors in different environments would be able to learn/practice at least some of the techniques with relative ease: desiccation would be easy in hot/dry environments though it\u2019s a bit inefficient, salting in areas near the ocean, smoking basically anywhere (though it\u2019d take some work to figure out how to build a smokehouse), adding sugar&nbsp;<a href=\"http://chartsbin.com/view/43648\"><u>in regions where sugarcane is grown</u></a>, adding vinegar/fermenting basically anywhere.&nbsp;</p><p>We can also achieve food preservation without altering its chemistry by refrigerating it. Modern refrigerators require electricity, but there are other refrigeration techniques that don\u2019t (e.g.&nbsp;<a href=\"https://en.wikipedia.org/wiki/Absorption_refrigerator\"><u>absorption refrigerators</u></a>), and could pretty easily be built without advanced technology/systems.&nbsp;&nbsp;&nbsp;</p><p>And survivors could relearn to fire clay as a complement to these preservation techniques, allowing them to make lidded ceramic receptacles (and bricks for construction). To make watertight and air-resistant ceramic, the survivors would have to build kilns from scratch. This is somewhat difficult, because the kiln itself has to be built from materials that can withstand extreme heat. The neat thing here is that you can build a basic kiln from wood-fired clay bricks, then build a more robust kiln with the kiln-fired bricks and so on \u2014 an example of how learning basic techniques and principles will have compounding effects, allowing for even more complex ones.</p><h3>Medicine</h3><p>Throughout most of history, infectious disease has been one of the main causes of human mortality. If the survivors of the catastrophe maintain the basic principles of germ theory, human longevity likely won\u2019t be reduced to pre-industrial levels. That\u2019s because many life-saving practices are quite low-tech \u2014 achievable with very basic inputs and knowledge.&nbsp;</p><p>The main prevention and curative techniques to master will include water purification, sanitation, and hygiene; oral rehydration therapy; and eventually, the production of penicillin.&nbsp;</p><p>Early on, water purification can be achieved by boiling it, leaving water in a clear container in the sun, or by making a water filtration system (this can be done using simple materials: a bucket, charcoal, sand, and gravel). Sanitation refers to the use of latrines (easily built) to keep defecation away from drinking water and food. And hygiene mostly refers to washing hands with soap, which can be made easily by mixing plant oils (coconut, olive, palm, etc.) with an alkali like sodium or potassium carbonate (these alkalis can be extracted from the ash left after burning wood or seawood, or by fermenting urine to get ammonia).&nbsp;</p><p>Historically, diseases that cause diarrhea have been among the most deadly, as sufferers regularly died of dehydration. Oral rehydration therapy is an enormously effective intervention, and can be made quite easily by mixing sugar, salt (from the ocean or salt deposits), and pure water.</p><p>Finally, somewhat more difficult is the&nbsp;<a href=\"https://www.primalsurvivor.net/make-penicillin-home/\"><u>extraction and refinement of penicillin</u></a> from Penicillium, which is actually one of the most common molds found in the environment, and the most common cause of food spoilage.</p><p>Sadly, childbirth will almost certainly become a leading cause of death again, though survivors that are able to recover birthing forceps from society\u2019s ruins will be much better off. Other interventions can be improvised using materials left behind after the catastrophe. For example, Design that Matters has designed a&nbsp;<a href=\"https://www.designthatmatters.org/past-projects\"><u>makeshift neonatal incubator that can be built almost entirely from scavenged automobile parts</u></a>, including headlights, the dashboard fan, and motorcycle battery.&nbsp;</p><h3>Transport</h3><p>Keeping vehicles running will be possible for at least decades, and maybe indefinitely, though roads will become more and more difficult to navigate over time. Ethanol (fermented vegetables) or methanol (fermented wood) can be substituted for gasoline and diesel when those run out or expire, though the amount of land required to keep any one car running would be immense. Alternatively, survivors can pump gas to the engine, either from gas-filled bags or from wood gasifiers that sit on top of the vehicle, the way many European countries did during the fuel shortages of the first and second World Wars (one of many examples from the World Wars where we see humans successfully innovating around resource shortages).&nbsp;</p><p>While spare engine parts and tires can be sourced from abandoned vehicles,&nbsp;<a href=\"https://en.wikipedia.org/wiki/Tire_recycling#Stockpiles_and_legal_dumping\"><u>landfills, and tire stockpiles</u></a>, both the vulcanization of rubber (the process that makes tires so indestructible) and the precision manufacturing required for certain car parts could be inaccessible to survivors for some time. To maintain mechanized automobiles, humans will have to relearn metalworking and organic chemistry.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxyj77qfw2vo\"><sup><a href=\"#fnxyj77qfw2vo\">[5]</a></sup></span>&nbsp;Until the survivors are able to source new rubber, vulcanize it, and produce/shape the alloys necessary for the car engine, they\u2019ll have to rely on \u201canalog\u201d transport: horses, oxen, and wind-powered ships.&nbsp;</p><p>There are no technical reasons this is unachievable (though, importantly, rubber is only produced in a few places in the tropics, so would require sufficient trade). The ingredients would be readily available, and the tools and techniques can be constructed and mastered from a very low baseline.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmzvf044cd3b\"><sup><a href=\"#fnmzvf044cd3b\">[6]</a></sup></span>&nbsp;An (imperfect) proof of concept comes from a machinist who built an entire metalworking shop in the 1980s using just clay, sand, charcoal, and scrap metal. This feat has now been accomplished many times over as others have been inspired to do the same (you can now buy a&nbsp;<a href=\"https://www.amazon.co.uk/dp/B0161WFR0I/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1\"><u>seven-book series on building your own metalworking shop from scratch on Amazon</u></a>).</p><h3>Making stuff</h3><p>And of course, the value of metalworking goes far beyond facilitating transportation. Metalworking would be necessary for making forging tools, gears, and a bunch of other things.&nbsp;</p><p>In addition to metalworking, there are other critical skills we\u2019d need to relearn. Importantly, just a few key ones (described below) would unlock a huge number of goods and tools.</p><p>Glass for windows and lightbulbs can be made by heating sand in a kiln, and blown using basic techniques that have been practiced for millennia.&nbsp;</p><p>Producing lime (chalk) and quicklime are critical. Both are upstream of a bunch of different processes, like making mortar and concrete, whitewashing walls, treating sewage, improving soil efficiency, and making explosives (for mining). At small scales, survivors near coastlines can produce lime by simply crushing up coral and seashells. At larger scales, survivors will have to identify and mine chalk deposits. Quicklime is then produced by roasting lime in a kiln and adding water.&nbsp;</p><p>Synthesizing key acids to be used in fertilizers, battery acids, explosives, ink, bleach, detergents, lubricants, and synthetic fibers (among other things) will also be key. Unfortunately, sulfur \u2014 the main input into sulfuric acid (the most-produced acid in today\u2019s chemistry industry and a precursor to many other useful acids) \u2014 will be much harder to find than it was the first time around. Most of the sulfur in easily accessible volcanic deposits has already been extracted. There\u2019s a solution though, as sulfuric gas can be created by cooking pyrite rocks (fool\u2019s gold), which can then be reacted with chlorine gas and charcoal. This process requires a well-developed understanding of organic chemistry, but getting hold of the inputs themselves wouldn\u2019t be insurmountable.&nbsp;</p><p>Wood pyrolysis, which literally means burning wood, is probably the most difficult technique survivors will have to master early on. Survivors will need to build a system to burn wood and separate and capture the resulting vapors. From Lewis Dartnell\u2019s&nbsp;<i>The Knowledge</i>:&nbsp;</p><blockquote><p>An ideal stepping-stone for a recovering society to leapfrog would be to bake wood in a sealed metal compartment, with a side pipe drawing off the released fumes and coiling through a bucket of cold water to cool and condense the vapors\u2026 The collected condensate readily separates into a watery solution and a thick tarry residue, both of which are complex mixtures that can be teased apart by distillation. The watery part is acetic acid, acetone, and methanol\u2026 The crude tar sweated out of the roasted wood can also be separated by distillation into its major constituents: thin, fluid turpentine, thick dense creosote, and dark viscous pitch.&nbsp;&nbsp;</p></blockquote><p>But the products are incredibly useful: acetic acid can be used to pickle food; acetone can be used for degreasing and in explosives; pitch can be used to make torches and to make things water resistant (caulking seams), and methanol can be used as antifreeze, as a biofuel, and as a chemical solvent (as can turpentine, another product of wood pyrolysis).&nbsp;</p><h3>Communication</h3><p>The simplest communication tools we\u2019ll relearn to make are paper \u2014 which can be fashioned out of wood pulp and the alkalis discussed above (made from wood or seaweed ash) \u2014 and pen, which can be fashioned from a bird\u2019s feather. Ink made of plant dyes or berries can be made permanent using an iron compound plus plant galls (the growth a tree forms when a wasp lays its eggs on it). And once we\u2019d relearned metalworking as described above, we could make a printing press (again, the Chinese did this over a millennium ago).&nbsp;</p><p>And for communication at a distance, we could make makeshift radios \u2014 a task that is surprisingly accessible once we re-figure out electricity (more on this in the next section). Again, an example from the World Wars demonstrates the power of human ingenuity when particular resources are in short supply. Lewis Dartnell explains in his book:&nbsp;</p><blockquote><p>\u201cDuring the Second World War, both soldiers holed up at POW camps built their own makeshift radio receivers for music or news of the war effort. These ingenious constructions reveal the sheer variety of scavenged materials that can be jury-rigged to create a working radio. Aerial wires were slung over trees, or disguised as clotheslines, and sometimes even barbed wire fences were appropriated for the task. A good grounding was achieved by connecting to cold-water pipes in the POW barracks. Inductors were constructed by winding coils around cardboard toilet rolls, the scavenged bare wire insulated by candle wax, or in Japanese POW camps by applying a paste of palm oil and flour. Capacitors for the tuning circuit were improvised out of layers of tinfoil or cigarette-pack lining, alternating with newspaper sheets for insulation; the wide, flat device was then curled like a jelly roll to make a more compact component. The earphone is a trickier component to improvise and so was often salvaged from wrecked vehicles\u2026. Perhaps the most ingenious improvisation of all, however, was in creating the all-important rectifier, needed to demodulate the audio signal from the carrier wave. Mineral crystals like iron pyrite or galena were unobtainable on the battlefield, but rusty razor blades and corroded copper pennies were discovered to serve just as well. The blade or coin was fixed to a scrap piece of wood alongside a safety pin bent upright. A sharpened graphite pencil was firmly attached to the point of the safety pin\u2026 allowing fine readjustment of the pencil graphite across the metal oxide surface until a working rectifying junction was found.\u201d</p></blockquote><h3>Power</h3><p>Soon after the collapse, I expect survivors to work out how to harness mechanical energy using water wheels, windmills, and steam engines,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref084i8cl2re24\"><sup><a href=\"#fn084i8cl2re24\">[7]</a></sup></span>&nbsp;each of which can be built with rudimentary tools and don\u2019t require refined gasoline, diesel, or natural gas.&nbsp;</p><p>To begin to generate and distribute electricity again, the survivors will relearn how to make batteries, generators, and electrical transformers. In all likelihood, survivors won\u2019t be working from scratch. Another example from the context of war shows how ordinary people have figured out how to improvise complex devices in the face of resource scarcity. During the Bosnian War, civilians&nbsp;<a href=\"https://thefunambulist.net/architectural-projects/bosnia-gorazdes-mini-centrales-self-sufficiency-in-war-time\"><u>jury-rigged floating generators using scavenged car alternators</u></a> and other basic parts.<br>&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995257/mirroredImages/Nc9fCzjBKYDaDJGiX/jfnfx8pfjzn16mombqjo.png\">&nbsp;</p><p>But figuring out how to create and harness electricity from scratch wouldn\u2019t be prohibitively difficult even if survivors had to do so from zero. Electricity is one of those things that seems to most people like magic, especially because it took humans so long to be able to make and use it reliably. It\u2019s actually not (and in fact, there\u2019s evidence that batteries were developed and used to electroplate gold onto jewelry as early as 200 BCE). Electricity can be generated by spinning a magnet inside a wire coil, and electric motion can be generated by placing a magnet near a wire. Particular arrangements of wires and magnets scaled up is the basis for the electric motor that drives the majority of our electronics today.&nbsp;</p><p>And once survivors understand electromagnetism well enough, they can build generators that harness the mechanical power from windmills and waterwheels. As a proof of concept, researchers fitted a traditional windmill with a generator and found that the system produced &gt;50,000 kilowatt hours (kWh, a measure of energy consumption) of electricity per year. Water turbines, the technology underlying the Three Gorges Dam and Hoover Dam, are even better at this. This energy can then be distributed across a system of cables and electrical transformers, neither of which are particularly difficult to produce once the survivors have mastered metalworking.&nbsp;</p><p>But to return to modern technological society will require a lot of energy. It takes about 90,000 kWh of energy to support the lifestyle of the average US citizen for a year, and 40,000 kWh of energy to support the average European. Lewis Dartnell points out that, to achieve this level of energy consumption during the Middle Ages (so using only human and horse power), it would require the physical labor of 14 horses and &gt;100 humans for 24 hours a day, 365 days a year.&nbsp;&nbsp;</p><p>Several key processes survivors will need to recreate \u2014&nbsp;<a href=\"https://aeon.co/essays/could-we-reboot-a-modern-civilisation-without-fossil-fuels\"><u>producing steel, brick, mortar, and cement</u></a> \u2014 require huge amounts of thermal energy, much of which we get from burning fossil fuels. It\u2019s been pointed out that the efficiency of fossil fuel extraction has gotten worse and worse as we\u2019ve already depleted some of the easiest-to-reach sources.&nbsp;</p><p>While this is true, we should still have more than enough energy to re-industrialize, even if we don\u2019t initially have the technology necessary for advanced fossil fuel extraction like fracking. To illustrate, consider that in 2018, the&nbsp;<a href=\"https://www.eia.gov/coal/annual/pdf/table9.pdf\"><u>North Antelope Rochelle Mine in the United States (the largest coal mine in the world) produced about 100 million tons of coal</u></a> \u2014&nbsp;<a href=\"https://www.drax.com/energy-policy/the-turbulent-history-of-coal/\"><u>double the amount produced by Great Britain in 1850, 100 years into the industrial revolution</u></a>. And the North Antelope Rochelle Mine is a surface mine, meaning that we can get at the coal without particularly advanced technology.&nbsp;<br>&nbsp;</p><p><strong><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995257/mirroredImages/Nc9fCzjBKYDaDJGiX/ihzgtee4vo76ddyasent.png\"></strong></p><p><a href=\"https://en.wikipedia.org/wiki/North_Antelope_Rochelle_Mine\"><u>North Antelope Rochelle Mine</u></a></p><p><br>Further, fossil fuels could be supplemented with charcoal (pyrolyzed timber), which is superior to coal in a bunch of different ways (it burns hotter, it\u2019s renewable, it produces fewer pollutants) but which became less popular than coal during the industrial revolution because coal was easier to access than timber. Survivors in areas suitable for growing timber (or naturally forested ones) could produce massive amounts of charcoal.&nbsp;</p><p>Given all of this, natural resources seem likely to slow recovery down, while leftover physical capital seems likely to accelerate it. My guess is that physical capital wins, and these factors would push recovery faster on net, but it\u2019d be great if someone else gave this more thought. For now, I\u2019m guessing the multiplier is somewhere between 0.5X and 1.5X.&nbsp;</p><p><strong>Importantly, while we might have enough natural resources to recover civilization once or even a few times, if human civilization collapsed&nbsp;</strong><i><strong>many</strong></i><strong> times, it\u2019s much less obvious to me we\u2019d have the natural resources necessary to rebuild \u2014 at least using technologies remotely similar to the ones we have now.&nbsp;</strong></p><p>What about human capital? Would human capital pose a bottleneck on the recovery of industrial civilization like natural resources would? Would it speed it up, like physical capital?</p><h2>Would we have the human capital to rebuild industrial society?</h2><p>For the purposes of this post, I consider human capital to include these factors:</p><ul><li>Knowledge and skills</li><li>Education</li><li>Population</li><li>Institutions</li><li>Cultural norms and values</li></ul><p>I expect several of these to persist beyond the collapse, thus accelerating the timeline over which we should expect society to re-industrialize.&nbsp;</p><h3>Knowledge</h3><p>In a world where 90-99.9% of the population has died, it\u2019s possible that no one left alive would have some of the requisite knowledge to actually implement the reconstruction described by Dartnell in the previous section. This seems extremely important, given that, historically, ideas and knowledge have been more of a bottleneck on economic growth than natural resources and physical capital.</p><p>For example, if we thought that the survivors and their descendents would lose even very basic knowledge of biology, they could fall into a Malthusian food productivity trap. In a food productivity trap, a population\u2019s limited understanding of methods for boosting agricultural efficiency limits the size of the population able to be sustained. This in turn further limits improvements to agricultural output: people are the generators of ideas, so fewer people means lower probability of agricultural innovation. And perhaps more importantly, even if the population does conceive of testable innovations, because its agricultural output is just enough to support the existing population, no one is willing to jeopardize the existing outputs by testing new methods.&nbsp;<br>&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995257/mirroredImages/Nc9fCzjBKYDaDJGiX/vycibmusvahabxlzkzio.png\"></p><p><br>Traps like these probably slowed us down historically, and it\u2019s likely we\u2019d get temporarily stuck in them if we lost much or nearly all of our knowledge. In a recovery scenario in which we have to rediscover all of the knowledge we\u2019ve accumulated over thousands of years, achieving industrial civilization could easily be as slow or slower than it was the first time around.</p><p>I strongly suspect that agricultural innovation would not be a bottleneck on recovery. The fact that we saw multiple agricultural revolutions extremely quickly after the Earth\u2019s climate stabilized makes me think that agricultural civilization is not incredibly contingent. But we don\u2019t actually have to settle this particular debate, because, crucially, survivors rebuilding society after collapse wouldn\u2019t have to do much innovation to re-attain agricultural complexity \u2014 they would have examples of the technology they were trying to recreate lying around, along with books explaining the biology and technology left in libraries.&nbsp;</p><p>How much of this would survive an initial catastrophe? And how long would it last before even what remains would be destroyed?</p><p><br><i>Stuff</i></p><p>Obviously, the physical stuff we\u2019ve produced over centuries has widely varying lifespans. Structures built thousands of years ago are still standing, but modern skyscrapers would last less than a century, and our houses just a few decades.&nbsp;</p><p>Consider that the ruins of ancient civilizations, while sparse, still tell us enormous amounts of information about how those civilizations lived: what tools they had, what they ate, even how their society was structured.&nbsp;</p><p>And modern construction is more durable than ever before. Unlike ancient civilizations, we have concrete, steel, and iron. Things made of iron and steel would last longest, with some lasting as long as thousands of years. Reinforced concrete and stone would last more like several hundreds of years (with unreinforced concrete lasting a bit longer). Finally, brick and wood deteriorate the fastest, lasting only tens of years (wood), sometimes a hundred (brick).</p><p>There\u2019s been a similar trajectory in most of our goods, many more of which are made out of durable goods like plastics and metals rather than wood or clay.&nbsp;</p><p>Thus, we can probably expect to see something like the demonstration effect, whereby a group adopts a technique or practice after seeing another group with that practice.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0h4o10obtpy\"><sup><a href=\"#fn0h4o10obtpy\">[8]</a></sup></span></p><p>There are troubling exceptions \u2014 instances where a society or region lost knowledge or skills despite having examples to look at for guidance. W. H. R. Rivers wrote about these cases in his \u201cThe Disappearance of Useful Arts\u201d (1912). He gives the example of the society that inhabited the Vanuatu archipelago, which lost the ability to build seaworthy canoes for generations (therefore becoming trapped on one of the archipelago\u2019s islands, Torres Island).&nbsp;</p><p>In an even more troubling example, human civilization mostly lost the ability to build concrete structures until 1824, despite the fact that the Romans figured out how to do it 2,000 years earlier \u2014 and indeed left the still-standing Pantheon and Colosseum as models future societies could study and replicate.</p><p>Crucially though, modern civilization has mastered something the Torres Islanders and the Romans hadn\u2019t: books. We have virtually perfect copies of our knowledge in dedicated information warehouses all over the world.&nbsp;<br>&nbsp;</p><p><i>Books</i></p><p>We have 2.6 million libraries in the world. If we imagine one of the most physically destructive catastrophes we\u2019re aware of, nuclear war, we can get a rough sense of whether and how many libraries would survive. Given that there are 900,000 libraries in countries without nuclear weapons, and about 700,000 libraries in countries without nuclear weapons or nuclear alliances, we can feel confident that even in a very destructive catastrophe, many hundreds of thousands of books would survive the initial destruction.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3j5m1i5jmhf\"><sup><a href=\"#fn3j5m1i5jmhf\">[9]</a></sup></span>&nbsp;</p><p>Some books would be destroyed pretty soon after the collapse \u2014 burned up in fires and explosions,&nbsp; disintegrated in rain and floodwaters, or just rotted by humidity. But books and microfiche in dry climates could last centuries. There are about&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1xkXoG0F_JSzdO1jjqxgwluwZb_S1R24WICz1eRnDnBQ/edit?usp=sharing\"><u>80,000 libraries in countries with extremely dry regions that would be unlikely to be involved in a nuclear war</u></a>. With around 80,000 surviving libraries, it seems likely that at least some groups of survivors would be able to relearn a variety of basic knowledge and skills from a range of disciplines \u2014 things like germ theory, hygiene, crop rotation, even basic physics and chemistry.&nbsp;</p><p>After that happened, those learnings would almost certainly become more widespread.</p><p>There are certainly many skills and practices we\u2019d struggle to learn from books and artifacts. Much of the tacit knowledge required for important skillsets, like medicine or science, wouldn\u2019t be easily learned through books. But I strongly suspect that the most useful knowledge \u2014 that which is directly relevant to our survival and population growth \u2014&nbsp;<i>can</i> be learned pretty easily from books. And I think it\u2019s more likely than not that we could learn a lot of more advanced science as well.&nbsp;</p><p>All of that would be an enormous boost given the level of understanding held by human civilizations the first time around. I would expect this alone to speed recovery up by something like 1.1-2X.&nbsp;</p><h3>Education&nbsp;</h3><p>It seems exceedingly likely that, in a scenario where we\u2019ve been knocked back to pre-industrial levels, we\u2019d lose basically all formal education. However, as long as we maintain some form of communication \u2014 and I can find no plausible explanation for why we wouldn\u2019t \u2014 the survivors would still have both verbal and written intergenerational knowledge transfer.&nbsp;</p><p>Importantly, in his book on the role of collective intergenerational learning,&nbsp;<i>The Secret to Our Success</i>, Joseph Henrich points out that knowledge transferred in a small population (with only a few teachers) is liable to become diluted. As a younger generation learns from the older, most of the learners will be worse than their teachers (Henrich uses the analogy of a photocopy, which will typically be worse than the original). In a large population, some of the learners will outperform their teachers, either because they\u2019re particularly diligent, or because they come up with an innovative augmentation of the process or skill. But in small populations, you\u2019re less likely to see individuals who outperform their teachers, purely for probabilistic reasons. This can cause the degradation of knowledge over time. This is known as the Tasmanian effect, and it is a major barrier to the accumulation of knowledge over generations.</p><p>While the post-collapse population is small, I expect the Tasmanian effect to cause the loss of knowledge over time, especially tacit knowledge. But as the population grows, cultural evolution is able to counteract the Tasmanian effect. Over time, this informal transmission of knowledge will be formalized into something recognizable as education. Once the survivors developed formal education again, the gains from old and new knowledge and ideas would be further multiplied, accelerating growth even more.&nbsp;</p><p>This points to another critical input to rebuilding technological society, and economic growth more broadly: a sufficiently large and growing population.</p><h3>Population</h3><p>People are key to economic productivity and growth. There are certainly drawbacks to having larger populations: more pollution, more disease, and overcrowding, among others. But historically, these seem to be clearly outweighed by the benefits. Concretely, large populations mean more innovators, a market for consumer goods created by innovators, and economies of scale that ease the cost of supporting all of these people.&nbsp;</p><p>By definition, population will fall during the collapse of civilization. And while this will initially have unintended positive consequences \u2014 fewer people competing for the leftover resources that will enable their survival \u2014 low population would be a detriment over time.&nbsp;</p><p>However, given that I\u2019m very optimistic that the survivors and their descendents will be able to relearn the basics of agriculture and medicine through books and by studying the ruins of the previous civilization (if they lose the basics at all, which I\u2019m not sure they would), there shouldn\u2019t be major constraints keeping population low.&nbsp;</p><p>Importantly, while hygiene and basic modern medicine will improve physical health, the fact that the overall disease burden may be higher than it was in pre-modern society may push in the other direction.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefst1dfuwwts\"><sup><a href=\"#fnst1dfuwwts\">[10]</a></sup></span></p><p>My guess is that human longevity and productivity (nutrition and frequency of illness) would still be much better, even taking into account a higher disease burden. This belief mainly comes from the fact that many of the staggering gains in life expectancy (<a href=\"https://ourworldindata.org/life-expectancy\"><u>from ~35 in 1800 to over 70 years today</u></a>) can be&nbsp;<a href=\"https://aperioncare.com/blog/inventions-life-expectancy/\"><u>attributed to low-tech public health improvements</u></a> like improved public sanitation (toilets, handwashing) and medical interventions like the use of oral rehydration therapy and antibiotics.&nbsp;</p><p>Some slightly higher-tech interventions like water chlorination, pasteurization, and sunscreen would be more technically difficult to recreate in a post-collapse society, but even just the understanding of the underlying science behind those interventions (water and milk have germs in them; exposure to the sun can be harmful) would get survivors and their descendents a fair bit of the way to re-attaining the ability to implement the interventions again.&nbsp;</p><p>All of this makes me think that population will grow as fast (and likely much faster) than it did early on in human history, speeding up the second industrialization by 1.1-3X.</p><h3>Institutions, norms, and values</h3><p>Finally, it\u2019s widely agreed that political, economic, legal, and social institutions have profound effects on economic growth. These include things like property rights, political stability, honest government, a dependable legal system, and competitive and open markets. It also includes even fuzzier things, like a culture of innovation and placing value on the scientific method.&nbsp;</p><p>Interestingly, these may have been highly contingent, but also seem to be fairly long-lasting compared to physical stuff and technology. Indeed, hundreds of tools and technologies and even our understanding of the fundamentals of science have become obsolete over time. In contrast, many of our institutions and cultures can be traced back centuries and even millennia (consider how many of our norms have origins in religion, or that modern Western political institutions look a lot like early Roman ones).&nbsp;</p><p>This makes me inclined to think that many important cultural, economic, and political institutions and values would survive a catastrophe. I expect that this would be a multiplier on the pace of recovery, though I\u2019m very uncertain as to how large. I imagine it could be between 5X and 10X (if this seems like too much, consider how much quicker we could have made it through the Middle Ages if we\u2019d understood and applied the scientific method at the time).&nbsp;</p><p>But to be clear, this view is very speculative.&nbsp;</p><p>One thing some people have wondered about is whether a catastrophe caused by technology could alter humans\u2019 cultural acceptance and embrace of technology \u2014 aspects of culture key to rebuilding industrial society. In effect, even if it were technically possible to do so, the survivors of a technology-fueled catastrophe might not&nbsp;<i>want</i> to rebuild a technological society.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4vni402jsl\"><sup><a href=\"#fn4vni402jsl\">[11]</a></sup></span></p><p>I don\u2019t find this concern very plausible. For this type of cultural shift to actually stop technological advancement, it would have to be universally adopted by all survivor groups. We can point to examples of times when many actors have coordinated to prohibit the use of some technologies \u2014&nbsp;<a href=\"https://gmo.geneticliteracyproject.org/FAQ/where-are-gmos-grown-and-banned/\"><u>genetically modified foods, for example, which have been banned in 26 countries</u></a>&nbsp; \u2014 but it\u2019s pretty hard to think of any that have been universally banned. And for those that&nbsp;<i>have</i> been banned, I can\u2019t think of any that don\u2019t have some kind of functional substitute. For example, while blinding lasers were basically universally banned as part of the United Nations\u2019 Convention on Certain Conventional Weapons,&nbsp;<a href=\"https://thebulletin.org/2014/09/blinding-them-with-science-is-development-of-a-banned-laser-weapon-continuing/\"><u>\u201cflashers\u201d (lasers that cause temporary blindness) have been developed as a way around the ban</u></a>.&nbsp;</p><p>This points at another reason technological society would be hard to avoid: because technological society could likely be built on a range of technologies, deciding not to pursue&nbsp;<i>some&nbsp;</i>technologies isn\u2019t enough. For example, if society collapsed as a result of anthropogenic global warming, the survivors could decide not to redevelop technologies that rely on fossil fuels. However, as described above, the elimination of fossil fuels wouldn\u2019t&nbsp;<i>necessarily</i> make industrialization impossible \u2014 charcoal, hydropower, solar power, and wind power could be exploited to meet much of the successor society\u2019s energy needs. The use of alternative energy sources, and possibly the discovery of new ones, could still fuel another industrial revolution. Given this, the survivors of a catastrophe would basically have to decide to avoid&nbsp;<i>all</i> technology to ensure that technological advancement didn\u2019t eventually arise again.&nbsp;</p><p>I find it really difficult to imagine that all survivor groups come to a consensus on this. I also find it hard to believe that this could be enforced through coercion, given that coercion on a truly global scale can probably only be done really well with pretty high levels of technology (weapons and surveillance).&nbsp;</p><p>But even if we imagine that the survivors have all universally decided not to advance technologically (or are successfully coerced), it must be the case that this value lasts&nbsp;<i>indefinitely&nbsp;</i>(i.e. until the accumulated natural risk gets us)<i>&nbsp;</i>\u2014 a degree of lock-in that, ironically, can probably only be achieved with advanced technology. Given that there are pretty unignorable short-term benefits of technological development, it seems quite unlikely to me that everyone everywhere would reject technology for centuries or more.&nbsp;</p><p>All of this makes me think that a cultural aversion to technology would be unlikely to stick in the longer term. It could slow things down a bit in the short term, but I would be very surprised if this amounted to anything more than a slowdown of 0.9X (so I expect the effect is between 0.9X and 1X).</p><p>Given all of this, where do I put the probability of the recovery of industrial society?</p><h1>What are the recovery timelines for a collapsed civilization?</h1><p>Given that it took 200,000 to 300,000 years from the emergence of Homo sapiens to get to technological society, we can consider 250,000 years a rough upper bound for thinking about how long reconstruction would take if we lost the knowledge and skills critical to rebuilding civilization quickly (but again, we only have an&nbsp;<i>n</i> of one \u2014 it actually might have happened much slower or much faster if we re-ran history again).</p><p>Again, I expect that the survivors would achieve something like agricultural civilization in a matter of years given a stable climate (which seemed like the main precipitator of the first agricultural revolutions). Based on this, I would consider just the time it took to get from agricultural civilization to industrialization a reasonable lower bound on how long reconstruction of industrial society would re-industrialize.&nbsp;</p><p>This brings me to:</p><ul><li>about 10,000 years in expectation in a best-case scenario (90% confidence interval: 500-30,000 years), and&nbsp;</li><li>about 90,000 years in a pessimistic scenario (90% confidence interval: 4,400-280,000 years).</li></ul><p>When I then take an all-things-considered view (which I admit is very speculative), I expect that recovery could happen approximately 3\u201390X<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0k0gsrldood\"><sup><a href=\"#fn0k0gsrldood\">[12]</a></sup></span>&nbsp;faster than industrialization. This translates to a best-case guess of between 100 and 3,700 years, and a pessimistic guess of 1,000 to 33,000 years.&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\" rowspan=\"2\"><p><br><br><br><br><br><br>&nbsp;</p><p><i>If we think recovery time is limited by\u2026&nbsp;</i></p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\" colspan=\"2\"><i>Base rate \u2014 If we assume that re-industrialization would take exactly as long as it did the first time</i></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\" colspan=\"2\"><i>Inside view \u2014 If we assume that existing physical and human capital would accelerate the speed of re-industrialization relative to the base rate</i></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><i>Agricultural rev. and industrialization take as long as they did the first time (~300,000 years)</i></p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><i>Agricultural civilization returns quickly, industrial revolution takes as long as it did the first time (500-30,000 years)</i></p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><br>&nbsp;</p><p><i>Best-case guess<sup>1</sup></i></p><p><i>(100-3,700 years)</i></p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><br>&nbsp;</p><p><i>Pessimistic guess<sup>2&nbsp;</sup>(1,000-33,000 years)</i></p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><i>The natural rate of human extinction \u2014 Homo sapiens base rate</i><br><i>(1/14,000)</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1bn1lv2l9by\"><sup><a href=\"#fn1bn1lv2l9by\">[13]</a></sup></span></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>4.5%</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>58%</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>81-99%</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>32-93%</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><i>The natural rate of human extinction \u2014 Homo genus base rate<strong>&nbsp;</strong></i><br><i>(1/870,000)</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefix6c21mi6q8\"><sup><a href=\"#fnix6c21mi6q8\">[14]</a></sup></span></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>74%</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>99%</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>99.6-99.99%</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>97-99.9%</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><i>End of the biosphere</i><br><i>(1/~1 billion years)</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0egldxl83uu\"><sup><a href=\"#fn0egldxl83uu\">[15]</a></sup></span></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>100%</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>100%</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>100%</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>100%</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\" colspan=\"5\"><p>Notes:</p><p>1. Assumes the British industrial revolution happened about when we\u2019d have expected (100-3,700 years)</p><p>2. Assumes we got very lucky with the British industrial revolution (1,000-33,000 years)</p></td></tr></tbody></table></figure><p><br>&nbsp;</p><h1>&nbsp;</h1><h1>&nbsp;</h1><h1>&nbsp;</h1><p><br><br>&nbsp;</p><h1>Why I might be wrong</h1><p>If I\u2019m wrong about whether we\u2019d stagnate permanently as a result of civilizational collapse, my guess is that it\u2019s because of one (or several) of the following:</p><ul><li>My arguments/research are based on the slice of time we\u2019re currently in \u2014 they don\u2019t account for the ways resource availability and technology (and other things) could change in the future.</li><li>Catastrophes we don\u2019t know about have different properties than the ones we know about (in their deadliness, the types of people they affect, etc.).</li><li>There are&nbsp;<i>many</i> key resource constraints that I don\u2019t know about.</li><li>Relatedly, I\u2019m underestimating the likelihood that survivors fall into a food productivity trap (and stay there).</li><li>I\u2019m overestimating the degree to which survivors would be able to relearn critical knowledge/skills from books and physical artifacts.</li><li>There is knowledge/skills that can\u2019t be learned from books/artifacts that turn out to be critical.</li><li>We got lucky with the industrial revolution. The circumstances that led up to it the first time around were actually exceedingly unusual, and it would probably take much longer to happen if we re-ran history.</li><li>Norms against technology following a catastrophe are much stickier than I imagine them to be.</li><li>I expect too much continuity of culture/values/institutions, etc. before and after the collapse.</li></ul><h1>Notes</h1><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnks0xjre005d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefks0xjre005d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>5,000 years may seem far apart in the timescales we\u2019re used to thinking about, but it\u2019s actually very close in time when thinking about the scale of humanity's lifespan.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnakz400neeci\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefakz400neeci\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cNevertheless, one view shared by both Western and Chinese literatures is that the Northern Song Period was marked by its rapid population growth. The main evidence comes from officially registered household numbers, which grew from 6.2 million households in 980 AD to 17.5 million households by 1101 AD, an increase of 280 per cent\u2026&nbsp; An independent check is available to support this growth rate. According to the government record, from 995 to 1078 the Songs total marketed salt increase from 373,545 xiaoxi (small units) to 739,620 daxi (large units), or from 43.5 million to 103.5 million catties, with an annual growth rate of one per cent. Salt consumption is both price and income inelastic. It is a reliable barometer for population growth.\u201d (<a href=\"http://eprints.lse.ac.uk/50969/1/__Libfile_repository_Content_Deng%2CK_Deng_Demystifying_%20growth_development_2013_author.pdf\"><u>Deng, 2013</u></a>)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnv4cvhkrlf1\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefv4cvhkrlf1\">^</a></strong></sup></span><div class=\"footnote-content\"><p>There are other theories that probably fall in this category. For example, Frey argues that we should be skeptical of the Black Death\u2013caused industrial revolution given that we only saw the industrial revolution in Great Britain, despite the fact that other European countries experienced the pandemic as well. Frey\u2019s preferred theory is one where British industrialization is explained by the British government\u2019s backing of capitalists over workers (who were being displaced by the capitalists\u2019 machines), whereas France\u2019s government, for example, backed workers.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc8uq1kuapz\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc8uq1kuapz\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This hypothesis is quite controversial, with at least one empirical paper directly contradicting it. Gregory Clark explores the changes in wages, land rents, and returns on capital in the centuries before and after the Black Death (<a href=\"http://faculty.econ.ucdavis.edu/faculty/gclark/papers/black1.pdf#page=39\"><u>Clark, n.d.</u></a>). Clark finds that the gains from Black Death were short-lived, returning to pre-pandemic trends within a century.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxyj77qfw2vo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxyj77qfw2vo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>To vulcanize rubber, one has to melt the raw rubber, which itself may be difficult to acquire as it\u2019s only grown in the tropics, and sprinkle in sulfur. Sulfur is hard to obtain from its most abundant source, pyrite rocks (fool\u2019s gold), but can be done using two-pot distillation by heating the pyrite rocks to a high temperature.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmzvf044cd3b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmzvf044cd3b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Some tools would be more difficult to build from scratch than others. A simple but critical tool is the precision screw \u2014 considered an important enabler of the industrial revolution. Ideally, survivors should try to scavenge a long, threaded lead screw, as they are exceedingly difficult to cut from scratch, and can themselves be used to produce many more. Considering that the first precise metal screw thread took around 200-300 years of iterative improvements to make, this offers an example of how existing physical capital could enable a successor society to reindustrialize much more quickly than the initial industrialization process. Similarly, while our ancestors had to build smelting furnaces to purify the metals they worked with, the survivors of a collapse will be able to melt down scrap metal from abandoned cities. Once that scrap metal runs out, the survivors will eventually have to relearn how to build and operate blasting furnaces (as the Chinese did over a millennium ago), but this too will be made easier with the materials available for scavenging.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn084i8cl2re24\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref084i8cl2re24\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Steam engines to transform between thermal energy and mechanical energy. Achieved by venting hot steam into the cylinder of a piston, which condenses from gas to liquid, reducing the pressure in the cylinder. The pressure from the atmosphere then pushes the piston down (the same mechanism that draws liquid up a straw when you suck the air out). Add a crank and you can perform rotational mechanical tasks like driving machinery or wheels.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0h4o10obtpy\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0h4o10obtpy\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Possible examples include the prehistoric diffusion of pottery techniques, artefacts brought back and distributed during Marco Polo\u2019s expeditions to China, metallurgy techniques transferred between the East and West during the Middle Ages.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3j5m1i5jmhf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3j5m1i5jmhf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Matthew van der Merwe pointed this out when reviewing this post:&nbsp;</p><blockquote><p>As Toby Ord describes in&nbsp;<i>The Precipice</i>, physicists working on the Manhattan Project had unresolved concerns that the detonation of a nuclear weapon could ignite a fusion reaction with the hydrogen in the Earth\u2019s oceans, or with the nitrogen in the atmosphere \u2014 either of which might have destroyed all complex life on Earth. They were unable to completely resolve these uncertainties before they conducted the Trinity Test (the first-ever detonation of a nuclear weapon).</p></blockquote><p>From pp. 331\u20132 of<i> The Precipice</i>:&nbsp;</p><blockquote><p>James Conant, President of Harvard University, took the possibility seriously enough that when the flash at detonation was so much longer and brighter than he expected, he was overcome with dread: \u2018My instantaneous reaction was that something had gone wrong and that the thermal nuclear transformation of the atmosphere, once discussed as a possibility and jokingly referred to a few minutes earlier, had actually occurred.\u2019&nbsp; \u2026&nbsp;</p><p>When the war ended, he returned to Harvard and summoned its chief librarian, Keyes Metcalf, for a private meeting. Metcalf later recalled his shock at Conant\u2019s request (Hershberg, 1995, pp. 241\u20132): \u2018We are living in a very different world since the explosion of the A-bomb. We have no way of knowing what the results will be, but there is the danger that much of our present civilization will come to an end ... It might be advisable to select the printed material that would preserve the record of our civilization for the one we can hope will follow, microfilming it and making perhaps 10 copies and burying those in different places throughout the country. In that way we could ensure against the destruction that resulted from the fall of the Roman Empire.\u2019</p><p>Metcalf looked into what this would require, and prepared a rough plan for microfilming the most important 500,000 volumes, or a total of 250 million pages. But in the end they did not pursue this, reasoning both that its becoming public would cause significant panic, and that written records would probably survive in the libraries of university towns that would not suffer direct hits from atomic weapons.</p><p>However, when Metcalf resigned from Harvard, he began a project of ensuring vast holdings of important works in major universities in the southern hemisphere, perhaps inspired by the conversation with Conant and fear of nuclear catastrophe (Hershberg &amp; Kelly, 2017).</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnst1dfuwwts\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefst1dfuwwts\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The higher disease burden is itself caused by the domestication of animals, which has increased the rate at which diseases jump from non-human animals to humans. Consider that tuberculosis and smallpox came from cattle, the common cold came from horses, measles came from dogs and cattle, and many flu viruses still come from poultry and pigs.&nbsp;&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4vni402jsl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4vni402jsl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Further, James C. Scott has argued that the early agricultural civilizations had worse living conditions (lower longevity, worse quality of life) for most people \u2014 perhaps suggesting that a transition from hunting and gathering to agricultural and industrial civilization is not desirable at the individual level.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0k0gsrldood\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0k0gsrldood\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I get this by adding up all of the multipliers from earlier sections. Reminder that the multipliers represent acceleration of progress caused by the existence of additional physical and human capital.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1bn1lv2l9by\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1bn1lv2l9by\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cUsing only the information that Homo sapiens has existed at least 200,000 years, we conclude that the probability that humanity goes extinct from natural causes in any given year is almost guaranteed to be less than one in 14,000, and likely to be less than one in 87,000\u201d (<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/31363134\"><u>Synder-Beattie, Ord, &amp; Bonsall, 2019</u></a>).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnix6c21mi6q8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefix6c21mi6q8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cUsing the longer track record of survival for our entire genus Homo produces even tighter bounds, with an annual probability of natural extinction likely below one in 870,000\u201d (<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/31363134\"><u>Synder-Beattie, Ord, and Bonsall, 2019</u></a>).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0egldxl83uu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0egldxl83uu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cThere is significant uncertainty in all of these estimates. We can be reasonably confident that the runaway and moist greenhouse effects pose an upper bound on how long life can continue to exist on Earth, but we remain uncertain about when they will occur, due to the familiar limitations of our climate models. Wolf &amp; Toon (2015) find a moist greenhouse will occur at around 2 billion years, whereas Leconte et al. (2013) place a lower bound at 1 billion years\u201d (Ord, 2020, p. 406).&nbsp;</p></div></li></ol>", "user": {"username": "Luisa_Rodriguez"}}, {"_id": "Z6oqfKcax9yC2Cadk", "title": "Be careful with (outsourcing) hiring", "postedAt": "2022-10-17T20:30:00.000Z", "htmlBody": "<p>Added 2022-10-20: To clarify up-front, the main point of this article is not to give hiring advice. More nuance is required for that. The main point is to tell you that you have to be careful with hiring and especially with outsourcing hiring.</p>\n<hr>\n<p>Who makes up an organization, a community? People. People do the work. People make the decisions. Choosing people well is the most important thing we do. People choice is self-reinforcing, too. Choose one bad COO and he\u2019ll hire more bad people. Have a few bad people in an organization and nobody good will want to work there anymore. Hiring, therefore, is vital.</p>\n<p>What do you want from a hiring process? A good hire. Crucially, no bad hire. And for those people whom you haven\u2019t hired to be mostly happy with how things went. Because you care for them.</p>\n<p>Sadly, you\u2019re at risk of making a bad hire and disgruntling your other applicants if you don\u2019t know what you\u2019re doing. If you don\u2019t know what you\u2019re doing, outsourcing isn\u2019t a solution, either, because you don\u2019t know how to judge the actions of those you\u2019re outsourcing to. I will demonstrate this by example of a hiring process I\u2019ve observed as an outsider, in which the hiring firm (call them Hirely) acted in a way that would have <em>seemed</em> sensible to the average founder who knows little about hiring, but to me looked like blundering. Even if you don\u2019t plan to outsource hiring, the following points are worth thinking about.</p>\n<p>Added 2022-10-16: I won't be arguing every point fully. One commenter even wrote that I make \u2018lots of general assertions without a clear explanation as to why people should believe [me]\u2019. (I appreciate this comment.) That's because doing otherwise would have made the article ten times longer. Hiring is a wide field and I've only tilled a small patch of it myself. I encourage you to follow the links to Manager Tools podcasts/whitepapers that I've included in the article. They argue many of the claims properly. I will also be glad to explain more in the comments.</p>\n<p>Added 2022-10-22: A commenter points out, and I agree, that there are ways to outsource something safely without knowing about it. In the case of hiring, one way is to find strong evidence that the hiring processes run by a firm have resulted in many good hires, vanishingly few bad hires and the people who weren't hired being mostly happy with how things went. Although if you're paying attention to these three things, you already know something about hiring. (The above is not exactly what the commenter wrote. His <a href=\"https://forum.effectivealtruism.org/posts/Z6oqfKcax9yC2Cadk/be-careful-with-outsourcing-hiring?commentId=DkEER5fvDS8DCbdkf#comments\">comment is here</a>. If you read it, please note that he wrote it based on \u2018casually listening\u2019 to my post and also <a href=\"https://forum.effectivealtruism.org/posts/Z6oqfKcax9yC2Cadk/be-careful-with-outsourcing-hiring?commentId=LvqyqFc8c8bAfgYLv#comments\">read my reply</a>, which counters many of his claims.)</p>\n<h1>Aside: Manager Tools is a decent authority</h1>\n<p>In the two main sections I will repeatedly reference Manager Tools in support of my arguments. Why is Manager Tools (MT) a decent authority to reference? (By the way, I\u2019m in no way affiliated with or paid by MT. They have no idea that I\u2019m writing this.)</p>\n<h2>Their core guidance is based on data</h2>\n<p>As far as I know, MT measure how well their core guidance works for their clients. And they adjust accordingly. Since they\u2019ve presented their data on one-on-ones (<a href=\"https://www.manager-tools.com/2019/01/manager-tools-data-one-ones-part-1-hall-fame-guidance\">mto3s</a>) and feedback (<a href=\"https://www.manager-tools.com/2021/03/manager-tools-data-feedback-part-1\">mtfeedback</a>), you can judge for yourself whether their research makes sense. They haven\u2019t presented their data on hiring behaviours, but I suspect that they do have a lot of it. Additionally, the authors of MT\u2019s hiring guidance, Wendii Lord and Mark Horstman, were recruiters themselves and claim to have in sum spent thousands (tens of thousands?) of hours interviewing. If this is true (and I have little reason to doubt it), they must have learned a thing or two about what works and what doesn\u2019t.</p>\n<h2>Their guidance leads to success</h2>\n<p>Applying MT guidance has worked out well for many people. I conducted the selection phase of AI Safety Camp #5 (<a href=\"https://aisafety.camp/2021/06/23/aisc5-research-summaries/\">aisc5</a>) based on MT hiring guidance (<a href=\"https://www.manager-tools.com/podcasts/important-topic-feeds/hiring-feed\">mthiringfeed</a>). The participants whom my volunteers and I selected did well. One person thanked me when I rejected his application. And participants gave the selection phase the highest rating of all AISC selection phases so far.</p>\n<p>I also followed MT guidance (<a href=\"https://www.manager-tools.com/products/interview-series\">mtinterv</a>) for my last job search, writing r\u00e9sum\u00e9s, preparing for the interview etc. It helped me get my current job at Spark Wave. On the other end of the job lifecycle, my wife used MT guidance (<a href=\"https://www.manager-tools.com/2006/07/how-to-resign-part-1-of-3\">mtresign</a>) to resign and she is still in good standing with her company, unlike others before her.</p>\n<p>More generally, MT are a small company that has been training and advising organizations for a long time. They have been podcasting for seventeen years and get &gt; 100 k downloads per week. And they appear to have made good hiring decisions themselves (<a href=\"https://www.manager-tools.com/2022/07/special-cast-succession-planning\">mtsuccession</a>).</p>\n<h1>Hirely puts you at risk of making a bad hire</h1>\n<h2>They might not give you many (good) candidates</h2>\n<p>The more good applicants you have, the higher the likelihood that one of them will meet your standard. If you don\u2019t have enough and none meets your standard, you will be tempted to hire the best of those who don\u2019t meet your standard. This would be bad. Hirely, on the one hand, is good at getting many eyes on a job ad. On the other hand, the format of their job ad is a turn-off, especially for busy high-caliber people.</p>\n<p>A good thing I\u2019ve heard about Hirely is that they use marketing channels skillfully to get a job ad in front of many people. So they might bring in a decent number of candidates. The conversion from job ad and application form, however, likely won\u2019t be as good as it could be. The drop-out throughout the hiring process would also be high because it\u2019s intransparent and wastes people\u2019s time.</p>\n<p>At least the one job ad I\u2019ve seen was almost three pages, with long sentences and big, empty words. Busy people don\u2019t have time to read this. And good candidates tend to be busy because they\u2019re good. Besides, we in the EA community care about imposter syndrome, right? I don\u2019t have imposter syndrome, but to me,</p>\n<blockquote>\n<p>Convert strategy into executable steps for growth, putting into practice operations planning, organization-wide goal setting, and performance management  to scale the business effectively <em>(quote rewritten to be not Googleable)</em></p>\n</blockquote>\n<p>sounds much less like \u2018I can do it\u2019 than this:</p>\n<blockquote>\n<p>Project planning and management: Take rough, high-level guidance from the CEO, turn it into concrete plans, then make sure that they\u2019re carried out.</p>\n</blockquote>\n<p>Further time wasters/points of intransparency:</p>\n<ul>\n<li>Irrelevant questions on the application form, such as this for an operations job: \u2018What do you think the field of AI Alignment most needs?\u2019 Would <em>you</em> rule out a good ops person because they haven\u2019t thought about whole-field strategy? I hope not. If a question doesn\u2019t help you make a hiring decision, why do you ask it?</li>\n<li>They ask applicants to take a Big Five assessment. At least the Big Five version I know can be gamed easily. Even integrity and conscientiousness tests that can't easily be gamed appear not predictive enough of job performance to be worth the applicants' time (<a href=\"https://web.archive.org/web/20160227075513/http://lab4.psico.unimib.it/nettuno/forum/free_download/articolo_114.pdf\">selval</a>). I concede that some assessments make sense in some hiring processes and I don't have space here to discuss that properly. The minimum viable point is: A Big Five assessment didn't make sense for this role. And Hirely's hiring process draft didn't mention taking it into account for any decision. It would have been a waste of applicant time. (Rewritten 2022-10-19. See also <a href=\"https://forum.effectivealtruism.org/posts/Z6oqfKcax9yC2Cadk/be-careful-with-outsourcing-hiring?commentId=ZemvJ5oWE7zjDNwzJ\">discussion</a>.)</li>\n<li>They don\u2019t outline the whole hiring process up-front. Most egregiously, the process draft I saw didn\u2019t tell candidates about a work trial (1-2 weeks of full-time work?) until after some test task, the Big Five test and two interviews. (By the way, work trials are a nice idea, but in many cases don\u2019t make sense practically. I don\u2019t have space to discuss this here.)</li>\n<li>They haven\u2019t figured out the whole process up-front, leading to delays between steps, which causes candidates to drop out. (Remember that people need a job and giving someone a job quickly is a competitive advantage. And you\u2019re competing for good people.)</li>\n</ul>\n<h2>Their filters don\u2019t work</h2>\n<p>What is bad? To turn down a candidate who did badly in the interview because of nervousness, only to later see them kill it at a different (possibly non-EA) org. What is far worse? To hire someone who did great in the interview, but then sucks at the job itself. Sadly, with Hirely you\u2019re at risk to get both the bad and the worse.</p>\n<p>How do you do better? By having filters that let nervous-but-awesome people shine and that expose the flaws in people who only <em>seemed</em> awesome. Call the former nervous aces and the latter confident duds. Sounds impossible? Not impossible, but of course difficult. (Added 2022-10-16: Another way to look at this is to think about spreading the field of applicants. (\u2018Spreading\u2019 in the sense of \u2018widening\u2019, not \u2018distributing\u2019.) I'm too lazy to revise this section based on that frame. So just an example. \u2013 If I was interviewing for a senior developer position at GuidedTrack, I could ask: \u2018Tell me about a time you successfully added two numbers using a pocket calculator.\u2019 Since almost everyone can use a pocket calculator, the answers would barely distinguish candidates. If, however, I ask, \u2018Tell me about a time you successfully fixed a defect in an interpreter\u2019, it would open a wide gap between candidates who likely are and aren't fit for the job.)</p>\n<p>I will first describe a way to do it right, then how Hirely does it wrong. Basically this is an extremely compressed selection of MT guidance. <em>It is much too short to be a guide to hiring well.</em> If you want to hire well, go to the source (<a href=\"https://www.manager-tools.com/podcasts/important-topic-feeds/hiring-feed\">mthiringfeed</a>, <a href=\"https://www.manager-tools.com/products/effective-hiring-manager-book\">mtehm</a>).  (Added 2022-10-21: Of course, MT is not the only valid source about hiring. Some of their guidance might not apply to your case. Some of it might be outperformed by more EA-fashionable approaches. But I think they're an excellent starting point, and you will do decently if you rely on their material and common sense alone.)</p>\n<h3>An inkling of how to filter well</h3>\n<p>(I\u2019m focussing on structured behavioural interviews as the main filter. Further filters that work are structured technical interviews, work samples and IQ tests (<a href=\"https://web.archive.org/web/20160227075513/http://lab4.psico.unimib.it/nettuno/forum/free_download/articolo_114.pdf\">selval</a>). I have thoughts on those, but no space to include them here.)</p>\n<ol>\n<li>Set a standard. A high standard. In the rest of the process, compare people against this standard. Do not compare them against one another. Comparing people against one another feeds your biases (\u2018I like that guy better\u2019). Do not rank them. The best of ten candidates who don\u2019t meet the standard still doesn\u2019t meet the standard. (You may rank people at the very end when you have to decide whom to offer first.) In the whole process look for reasons to say no. Because hiring the wrong person results in Hell on Earth. (<a href=\"https://www.manager-tools.com/2007/04/effective-hiring-set-the-bar-high\">mtbarhigh</a>)</li>\n<li>Narrow the field of applicants by sorting r\u00e9sum\u00e9s (<a href=\"https://www.manager-tools.com/2016/05/how-scan-resume-part-1\">mtresumes</a>). (Yes, r\u00e9sum\u00e9s are a flawed filter. If you care about false negatives, you need to modify the approach. I don\u2019t have space to discuss this here.)</li>\n<li>Design a structured, behavioural interview (<a href=\"https://www.manager-tools.com/products/interview-creation-tool\">mtcreation</a>, <a href=\"https://www.manager-tools.com/2008/06/how-create-simple-behavioral-interview-question\">mtbehavioural</a>) that helps you compare candidates against the standard you set. Every question must help you find out whether a candidate meets the standard. In particular, it must dig up any reasons to say no. If it doesn\u2019t help you make a decision, it\u2019s a waste of time.\n<ul>\n<li>A structured interview is one with the same set of questions for each candidate and a fixed procedure for scoring each answer. See also the appendix. MT doesn\u2019t go as far as assigning numeric scores, but their interview creation tool (<a href=\"https://www.manager-tools.com/products/interview-creation-tool\">mtcreation</a>) at least provides criteria for each question.</li>\n<li>A behavioural question sounds like this: \u2018Tell me about a time when you successfully planned an event with more than twenty people.\u2019 Ie. you ask an open-ended question about past behaviour. Past behaviour because that\u2019s a hard fact: A confident dud can\u2019t confabulate a shiny \u2018I would do this and that\u2018 answer like he can for a scenario question. And even if he tells you about a past \u2018success\u2019, you can probe it until you get to its empty bottom. In contrast, the nervous ace gets more confident as you probe into the awesome details of her past behaviour, which she forgot to tell you about because she was nervous. (With confident duds I don\u2019t mean people who are simply unfit for a certain job. They will do well somewhere else. And I don\u2019t mean those who are confident and talk a lot, but still get the job done. I mean those people who get jobs easily because of their confidence and then cause harm. They are rare, but destructive.)</li>\n</ul>\n</li>\n<li>Narrow the field further by doing phone screens.\n<ul>\n<li>Here you decide whom you will spend hours of interviewing time on in the next step. Do you want to outsource this decision to a hiring consultant or HR? Only if you can trust them to know what you\u2019re looking for. Often you can\u2019t. (<a href=\"https://www.manager-tools.com/2019/01/should-hr-do-my-phone-screens-i-interview\">mthrscreen</a>)</li>\n<li>The way to do this is similar to a behavioural interview that is cut off after the second or third question. For details see <a href=\"https://www.manager-tools.com/2015/12/how-do-phone-screen-interview-part-1\">mtphonescr</a>.</li>\n</ul>\n</li>\n<li>Interview the remaining few candidates intensely.\n<ul>\n<li>Reduce stress. Be clear before the interview about what will happen how. Be as friendly as you can be. Smile. Start with small talk. See also <a href=\"https://www.manager-tools.com/2019/12/effective-hiring-manager-missing-chapters-reducing-interviewee-stress-part-1\">mtstress</a>. Reserve matters of salary until you make an offer. I don\u2019t have data on this, but \u2018salary negotiation\u2019 must be one of the most dreaded parts of an interview. So leave it out. It has no bearing on whether the candidate can do the job.</li>\n<li>Ask behavioural questions (see item 3) and probe (<a href=\"https://www.manager-tools.com/2012/01/first-rule-probing-interview\">mtprobe</a>). A candidate will seldom tell you everything you need to know. You must to find it out by asking follow-up questions.</li>\n<li>Several people in your organization interview each candidate. Everyone uses the same set of questions, with exceptions for technical interviews (<a href=\"https://www.manager-tools.com/products/effective-hiring-manager-book\">mtehm</a>, p.&nbsp;111).\n<ul>\n<li>Yes, this means that the candidate has to answer the same question repeatedly. But it won\u2019t be the same because every interviewer will hear the answer differently and ask different follow-up questions.</li>\n<li>You might say that this takes a lot of time from the candidate (and the organization) and you\u2019re right. But you would only do this with a few candidates. And it\u2019s in the interest of the candidate: Wouldn\u2019t you be glad to see a manager take hiring seriously and and know that your potential colleagues will all be up to par? Wouldn\u2019t you want to be listened to in detail before it\u2019s decided whether you\u2019ll be allowed to contribute the next two, five, ten years of productivity to an organization? (And mind you, a few hours is still mighty short for judging whether someone can do a job.) Finally, would you want to be hired by mistake into a role that doesn\u2019t fit you? I would rather spend an hour more being interviewed than three months suffering in a job that wasn\u2019t for me, only to be fired or resign and then be asked by every potential employer about that three-month engagement on my r\u00e9sum\u00e9.</li>\n<li>This is also a way to reduce bias. Having diverse interviewers is an easy way to increase diversity on your team without introducing quotas or lowering standards for certain groups of people.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>After a series of interviews hold an interview results capture meeting (<a href=\"https://www.manager-tools.com/2008/04/the-interview-results-capture-meeting\">mtcapture</a>). Each interviewer has to say \u2018hire\u2019 or \u2018don\u2019t hire\u2019 and justify this recommendation. One \u2018don\u2019t hire\u2019 is generally enough to rule out the candidate, but you as the hiring manager make the final decision.</li>\n</ol>\n<p>There is much more to know around offers, turning down candidates, onboarding etc. No space for that here.</p>\n<h3>How Hirely doesn\u2019t filter well</h3>\n<p>Now how does Hirely do it wrong? In short, they are mediocre at setting a standard and many of their questions aren\u2019t dispositive, ie. they don\u2019t help find out whether the candidate meets the standard. They do the phone screen themselves and plan on only one further interview by the hiring manager. And they increase interviewee stress. I\u2019m assessing this based on a draft hiring process document from Hirely. Since I was advising the hiring manager in parallel to Hirely, I\u2019m familiar with the requirements for the role, which is a \u2018COO\u2019/high-level ops role.</p>\n<p>To get into the worst detail first: scenario questions. A good candidate will have a lot to tell about past successes that match the requirements for the role you\u2019re hiring for. (These don\u2019t have to be suit-and-tie, commended-by-CEO \u2018professional\u2019 successes. If you\u2019re hiring an intern and are looking for planning skills, having built a weather-proof tree house in ninth grade can be an adequate accomplishment to talk about.) Detailed information about past successes is highly useful, but Hirely barely asks about it. (Talk about disgruntling applicants!) Instead, their main interview appears to focus on scenario questions, which put the nervous ace on the spot while being a perfect opportunity for the confident dud to confabulate something that <em>sounds</em> great. For each follow-up you ask, she\u2019ll invent a good answer. How do you know it\u2019s the right answer? You don\u2019t because it\u2019s all hypothetical! And what if you\u2019re dealing with a clever interviewee <a href=\"https://www.youtube.com/watch?v=uio1J2PKzLI\">who asks you a question back?</a> I\u2019m being facetious. Case study interviewing is a well-known technique. But it\u2019s hard. There is a lot more to a case study than asking a three-sentence scenario question. Stick to behavioural questions about the past.</p>\n<p>Let me backtrack and point out that Hirely is using fixed criteria and scoring rubrics. This is good. Unfortunately, the criteria I can see in the draft are only mediocre. Some are irrelevant, such as \u2018Strong familiarity with AI Safety and Existential Risks\u2019. \u2013 This is nice to have. But you wouldn\u2019t reject an application from a highly successful ops person who only heard about AI safety last week. \u2013 They are not specific, either. For example, when they ask for \u2018high levels of conscientiousness\u2019, every interviewer has to make up their own idea of where \u2018high levels\u2019 are.</p>\n<p>I also have to point out that Hirely has good intentions. They try to save the hiring manager time by doing the phone screen themselves. And they plan one main interview with the hiring manager. This is internally consistent, but doesn\u2019t make sense if you want to hire well. I\u2019ve established above that one main interview is not enough (see also <a href=\"https://www.manager-tools.com/2011/01/conduct-multiple-interviews-chapter-1-part-1\">mtmult</a>). You need to dig deep. You need to have multiple perspectives. Even if you don\u2019t buy the diversity thing, wouldn\u2019t you want to give your team a say about whom they have to work with for several years? If you\u2019re going to spend a lot of time interviewing, you need to choose well whom to spend that time on. You can only outsource this decision if the consultants are intimately familiar with your hiring needs. And the previous paragraph hints that they are not.</p>\n<p>Now, Hirely might say: \u2018That\u2019s why we record the phone screens.\u2019 And later in the draft it sounds as if the hiring manager is supposed to watch all those recordings. First of all, if I have to spend time watching all the recordings, why would I not do the phone screens myself and ask the questions <em>I</em> want to ask? Second, recording stresses people. What else stresses people? Panel interviews (<a href=\"https://www.manager-tools.com/2016/08/no-panel-interviews\">mtpanel</a>). And questions about compensation! Both Hirely interviews are panel interviews (with good intentions: they want to assist the hiring manager). And they plan to discuss compensation at the end of the main interview. Once more, they\u2019re putting the nervous ace at a disadvantage.</p>\n<p>Coming back to questions. If a question doesn\u2019t help you decide whether or not to hire, it\u2019s a waste of both your and the candidate\u2019s time. Additionally, it takes time away from questions that do help you decide. A Big Five assessment is one group of questions that don\u2019t work. I\u2019ve commented on this in the section about time wasters. Most questions in the phone screen aren\u2019t good at digging up information, either. Example: \u2018What are you really good at that might apply to this position?\u2019 Candidate: \u2018Er, I\u2019m good at project management.\u2019 Interviewer: \u2018What else are you good at?\u2019 Candidate: \u2018Hm, event planning and, uh, accounting.\u2019 Interviewer: \u2018Give me an example of a time you used that skill.\u2019 Candidate: \u2018I used it for organizing last year\u2019s orchid auction.\u2019 \u2013 You\u2019ve asked three questions, gotten three answers and still don\u2019t know much more about the candidate\u2019s ability. Of course, you could now start probing into that orchid auction piece. But Hirely doesn\u2019t mention probing once. And you could have circumvented that awkward back-and-forth by instead asking an open-ended question about a particular skill you\u2019re interested in: \u2018Tell me about a time you successfully ran an event, including managing its finances.\u2019</p>\n<h1>Hirely puts you at risk of disgruntling applicants</h1>\n<p>(This section is much shorter than the previous only because I\u2019ve addressed many points above. It is important nevertheless.)</p>\n<p>When I ran the applications process for AI Safety Camp, one applicant whom I had to turn down replied: \u2018Thank you for your thoughtful and graceful rejection email.\u2019 How do we treat candidates in a way that keeps them happy even when we have to reject their applications? We don\u2019t waste their time during the process. We don\u2019t increase their anxiety and imposter syndrome. And ideally we give them something useful, such as an option to get feedback. This is an aspect of hiring which Manuel Allgaier of EA Germany has made me especially aware of. There have been at least two popular posts on this forum about this, too: <a href=\"https://forum.effectivealtruism.org/posts/jmbP9rwXncfa32seH/after-one-year-of-applying-for-ea-jobs-it-is-really-really\">eaf1a</a>, <a href=\"https://forum.effectivealtruism.org/posts/Khon9Bhmad7v4dNKe/the-cost-of-rejection\">eafcost</a> I don\u2019t agree with all of that, but it is important feedback.</p>\n<p>Unfortunately again, Hirely doesn\u2019t appear to take it into account. Above I\u2019ve addressed time wasters, anxiety (nervousness) and imposter syndrome. And you know what makes you as a company look even worse? Rejecting a candidate, who has gone through application form, test task, personality test, two interviews and a work trial \u2026 by email! That\u2019s what Hirely is planning to do. Rejection hurts. It hurts only a little bit when it comes from a warm voice on the other end of the telephone (<a href=\"https://www.manager-tools.com/2014/11/how-turn-down-job-candidate-part-1\">mtturndown</a>). Show them that you care.</p>\n<p>(I mention above that I sent rejection emails. That\u2019s acceptable at earlier stages when the candidate hasn\u2019t had to spend much time yet.)</p>\n<p>My last gasp: In their rejection email, at least the one after the second interview, they preclude feedback. I grant that feedback in application processes is a difficult thing. It allows people to argue and, in the worst case, to sue you. So you don\u2019t have to offer it explicitly. But precluding it outright makes you look bad. And if a candidate asks you and you do it right (<a href=\"https://www.manager-tools.com/2013/07/you-did-not-demonstrate-part-1-hall-fame-guidance\">mtdemo</a>), you provide value even to someone you\u2019ve had to turn down.</p>\n<p><em>Added 2022-10-20: Let me remind you one more time, the main point of this article is not to give hiring advice. More nuance is required for that. The main point is to tell you that you have to be careful with hiring and especially with outsourcing hiring.</em></p>\n<h1>Appendix: On structured vs.&nbsp;unstructured interviews</h1>\n<blockquote>\n<p>Unstructured interviews have no fixed format or set of questions to be answered. In fact, the same interviewer often asks different applicants different questions. Nor is there a fixed procedure for scoring responses; in fact, responses to individual questions are usually not scored, and only an overall evaluation (or rating) is given to each applicant, based on summary impressions and judgments. Structured interviews are exactly the opposite on all counts. In addition, the questions to be asked are usually determined by a careful analysis of the job in question. As a result, structured interviews are more costly to construct and use, but are also more valid. \u2013 <a href=\"https://web.archive.org/web/20160227075513/http://lab4.psico.unimib.it/nettuno/forum/free_download/articolo_114.pdf\">selval</a></p>\n</blockquote>\n<h1>References</h1>\n<ul>\n<li><strong>aisc5</strong>: <a href=\"https://aisafety.camp/2021/06/23/aisc5-research-summaries/\">https://aisafety.camp/2021/06/23/aisc5-research-summaries/</a></li>\n<li><strong>eaf1a</strong>: <a href=\"https://forum.effectivealtruism.org/posts/jmbP9rwXncfa32seH/after-one-year-of-applying-for-ea-jobs-it-is-really-really\">https://forum.effectivealtruism.org/posts/jmbP9rwXncfa32seH/after-one-year-of-applying-for-ea-jobs-it-is-really-really</a></li>\n<li><strong>eafcost</strong>: <a href=\"https://forum.effectivealtruism.org/posts/Khon9Bhmad7v4dNKe/the-cost-of-rejection\">https://forum.effectivealtruism.org/posts/Khon9Bhmad7v4dNKe/the-cost-of-rejection</a></li>\n<li><strong>lwspec</strong>: <a href=\"https://www.lesswrong.com/posts/XosKB3mkvmXMZ3fBQ/specificity-your-brain-s-superpower\">https://www.lesswrong.com/posts/XosKB3mkvmXMZ3fBQ/specificity-your-brain-s-superpower</a></li>\n<li><strong>mtbarhigh</strong>: <a href=\"https://www.manager-tools.com/2007/04/effective-hiring-set-the-bar-high\">https://www.manager-tools.com/2007/04/effective-hiring-set-the-bar-high</a></li>\n<li><strong>mtbehavioural</strong>: <a href=\"https://www.manager-tools.com/2008/06/how-create-simple-behavioral-interview-question\">https://www.manager-tools.com/2008/06/how-create-simple-behavioral-interview-question</a></li>\n<li><strong>mtcapture</strong>: <a href=\"https://www.manager-tools.com/2008/04/the-interview-results-capture-meeting\">https://www.manager-tools.com/2008/04/the-interview-results-capture-meeting</a></li>\n<li><strong>mtcreation</strong>: <a href=\"https://www.manager-tools.com/products/interview-creation-tool\">https://www.manager-tools.com/products/interview-creation-tool</a></li>\n<li><strong>mtdemo</strong>: <a href=\"https://www.manager-tools.com/2013/07/you-did-not-demonstrate-part-1-hall-fame-guidance\">https://www.manager-tools.com/2013/07/you-did-not-demonstrate-part-1-hall-fame-guidance</a></li>\n<li><strong>mtehm</strong>: <a href=\"https://www.manager-tools.com/products/effective-hiring-manager-book\">https://www.manager-tools.com/products/effective-hiring-manager-book</a></li>\n<li><strong>mtfeedback</strong>: <a href=\"https://www.manager-tools.com/2021/03/manager-tools-data-feedback-part-1\">https://www.manager-tools.com/2021/03/manager-tools-data-feedback-part-1</a></li>\n<li><strong>mthiringfeed</strong>: <a href=\"https://www.manager-tools.com/podcasts/important-topic-feeds/hiring-feed\">https://www.manager-tools.com/podcasts/important-topic-feeds/hiring-feed</a></li>\n<li><strong>mthrscreen</strong>: <a href=\"https://www.manager-tools.com/2019/01/should-hr-do-my-phone-screens-i-interview\">https://www.manager-tools.com/2019/01/should-hr-do-my-phone-screens-i-interview</a></li>\n<li><strong>mtinterv</strong>: <a href=\"https://www.manager-tools.com/products/interview-series\">https://www.manager-tools.com/products/interview-series</a></li>\n<li><strong>mtmult</strong>: <a href=\"https://www.manager-tools.com/2011/01/conduct-multiple-interviews-chapter-1-part-1\">https://www.manager-tools.com/2011/01/conduct-multiple-interviews-chapter-1-part-1</a></li>\n<li><strong>mto3s</strong>: <a href=\"https://www.manager-tools.com/2019/01/manager-tools-data-one-ones-part-1-hall-fame-guidance\">https://www.manager-tools.com/2019/01/manager-tools-data-one-ones-part-1-hall-fame-guidance</a></li>\n<li><strong>mtpanel</strong>: <a href=\"https://www.manager-tools.com/2016/08/no-panel-interviews\">https://www.manager-tools.com/2016/08/no-panel-interviews</a></li>\n<li><strong>mtphonescr</strong>: <a href=\"https://www.manager-tools.com/2015/12/how-do-phone-screen-interview-part-1\">https://www.manager-tools.com/2015/12/how-do-phone-screen-interview-part-1</a></li>\n<li><strong>mtprobe</strong>: <a href=\"https://www.manager-tools.com/2012/01/first-rule-probing-interview\">https://www.manager-tools.com/2012/01/first-rule-probing-interview</a></li>\n<li><strong>mtresign</strong>: <a href=\"https://www.manager-tools.com/2006/07/how-to-resign-part-1-of-3\">https://www.manager-tools.com/2006/07/how-to-resign-part-1-of-3</a></li>\n<li><strong>mtresumes</strong>: <a href=\"https://www.manager-tools.com/2016/05/how-scan-resume-part-1\">https://www.manager-tools.com/2016/05/how-scan-resume-part-1</a></li>\n<li><strong>mtstress</strong>: <a href=\"https://www.manager-tools.com/2019/12/effective-hiring-manager-missing-chapters-reducing-interviewee-stress-part-1\">https://www.manager-tools.com/2019/12/effective-hiring-manager-missing-chapters-reducing-interviewee-stress-part-1</a></li>\n<li><strong>mtsuccession</strong>: <a href=\"https://www.manager-tools.com/2022/07/special-cast-succession-planning\">https://www.manager-tools.com/2022/07/special-cast-succession-planning</a></li>\n<li><strong>mtturndown</strong>: <a href=\"https://www.manager-tools.com/2014/11/how-turn-down-job-candidate-part-1\">https://www.manager-tools.com/2014/11/how-turn-down-job-candidate-part-1</a></li>\n<li><strong>selval</strong>: <a href=\"https://web.archive.org/web/20160227075513/http://lab4.psico.unimib.it/nettuno/forum/free_download/articolo_114.pdf\">https://web.archive.org/web/20160227075513/http://lab4.psico.unimib.it/nettuno/forum/free_download/articolo_114.pdf</a></li>\n</ul>\n<h1>Background: This is a repurposed article with a history</h1>\n<p>(Expanded on 2022-10-16 from the last paragraph of the originally published introduction. Moved to the end of the article on 2023-08-22. This describes the article's history in boring detail.)</p>\n<p>This article is strange because it's repurposed from a direct critique of the organization behind \u2018Hirely\u2019, which is a fictional name. (Please don't try to find out who is behind that name.) This is the first part of the article's history step-by-step:</p>\n<ol>\n<li>I observe the hiring process in which Hirely is advising the hiring manager. (I'm also advising the hiring manager, mostly telling him to be more involved and listen to Manager Tools.)</li>\n<li>I think Hirely is giving harmful advice.</li>\n<li>I write this article as a direct critique.</li>\n<li>I give the article to Hirely to react to. I tell them that I will edit it to be more general and not point the finger at them (meaning I won't out the organization by name) if they convince me that they're on a better trajectory. You may view this as being kind or you may view it as blackmail.</li>\n<li>Hirely deliberates internally.</li>\n<li>Hirely responds to me with the improvements they've made and are making, and asks me to deliver on my promise to edit the critique before publishing. Their response does make me think they're on a better path. (So again, please don't try to find out who they are.)</li>\n<li>Since I'm too lazy to rewrite the whole damn article, I search and replace the organization name with \u2018Hirely\u2019, and rewrite only the introduction.</li>\n</ol>\n<p>Despite this laziness, I was happy with the way it demonstrated the new main point with specificity (<a href=\"https://www.lesswrong.com/posts/XosKB3mkvmXMZ3fBQ/specificity-your-brain-s-superpower\">lwspec</a>): If you know very little about X, you can\u2019t safely outsource X. If X = hiring, it\u2019s especially bad. So you better learn something about hiring.</p>\n<p>You're already yawning, but the story isn't over. To explain all the strangenesses of this article, I have to describe the rest:</p>\n<ol start=\"8\">\n<li>I publish the article.</li>\n<li>It receives a lot of downvotes in addition to upvotes.</li>\n<li>I add a prescript (opposite of postscript) asking for the downvoters to at least hint at what they don't like about this article.</li>\n<li><a href=\"https://forum.effectivealtruism.org/users/alexrjl\">alexrjl</a> and <a href=\"https://forum.effectivealtruism.org/users/kirsten\">Kirsten</a> helpfully comment with their speculations why people might be downvoting. (Thank you, Alex and Kirsten!) None of the downvoters comment.</li>\n<li>More downvotes and upvotes, steadying at 3 total karma from 41 votes.</li>\n<li>It turns out that this article was mass-downvoted by anonymous accounts created for this purpose. The EA Forum team confirms this to me (and you'll see a note from them at the top of the comments).</li>\n<li>The EA Forum team reverts the mass-downvoting.</li>\n<li>The EA Forum team graciously reruns the article, which might be how you came to read it.</li>\n</ol>\n", "user": {"username": "rmoehn"}}]