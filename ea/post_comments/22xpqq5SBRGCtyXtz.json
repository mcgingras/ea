[{"_id": "Rag4MQKgyQmiq8oY8", "postedAt": "2022-10-07T19:29:26.531Z", "postId": "22xpqq5SBRGCtyXtz", "htmlBody": "<blockquote><p>AI operates in the single-minded pursuit of a goal that humans provide it. This goal is specified in something called the <i>reward function</i>.</p></blockquote><p>It turns out the problem is a lot worse than this -- even if we knew of a safe goal to give AI, we would have no idea how to build an AI that pursues that goal!&nbsp;</p><p>See <a href=\"https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-targe\">this post</a> for more detail. Another way of saying this using the inner/outer alignment framework: reward is the outer optimization target, but this &nbsp;does &nbsp;automatically induce inner optimization in the same direction.&nbsp;</p>", "parentCommentId": null, "user": {"username": "thomas-larsen"}}, {"_id": "XaJGEqHrtJuvHmKui", "postedAt": "2022-10-07T19:56:51.429Z", "postId": "22xpqq5SBRGCtyXtz", "htmlBody": "<p>People interested in AI risk and this post might be interested in applying to&nbsp;the <a href=\"https://efctv.org/3CjycyF\">researcher or software engineer roles</a> at&nbsp;the <a href=\"https://alignment.org/\">Alignment Research Center</a>, a non-profit organization focused on theoretical research to align future machine learning systems with human interests.</p><p><i>This is a test by the EA Forum Team to gauge interest in job ads relevant to posts -&nbsp; </i><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSez8LIh4zKj26Ok63mmvB2mueYFrvtCHvG5tVq3uPYHZO3Mgg/viewform?usp=pp_url&amp;entry.1792284488=ARC\"><i>give us feedback here</i></a><i>.</i></p>", "parentCommentId": null, "user": {"username": "syc"}}, {"_id": "PPKfaitxFs9bYKDny", "postedAt": "2022-10-07T22:09:49.071Z", "postId": "22xpqq5SBRGCtyXtz", "htmlBody": "<p>[I'm a contest organizer but I'm recusing myself for this because I personally know Andrew.]</p><p>Thanks for writing! A few minor points (may leave more substantive points later).</p><blockquote><p>In 2014, one survey asked the 100 most cited living AI scientists by what year they saw a 10%, 50%, and 90% chance that HLMI would exist</p></blockquote><p>There is updated research on this <a href=\"https://arxiv.org/abs/2206.04132?context=cs\">here</a> (survey conducted 2019) and <a href=\"https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/\">here</a> (2022; though it's not a paper yet, so might not be palatable for some people).</p><blockquote><p>Only 17% of respondents said they were <i>never</i> at least 90% confident HLMI would exist.</p></blockquote><p>I think this is a typo.</p><blockquote><p>Considering all of these scenarios together, 80,000 Hours\u2019 team of AI experts estimates that \u201cthe risk of a severe, even existential catastrophe caused by machine intelligence within the next 100 years is something like 10%.\u201d</p></blockquote><p>I don't think I would cite 80,000 hours, as that particular article is older. There is a <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\">newer one</a> recently, but it still seems better for ethos to cite something that looks like a paper. You could possibly cite <a href=\"https://arxiv.org/abs/2206.13353\">Carlsmith</a> or the survey above, which I think says the median researcher assigns 5% chance of extinction-level catastrophe.</p>", "parentCommentId": null, "user": {"username": "ThomasWoodside"}}, {"_id": "fy58GSyKKNhNct4qb", "postedAt": "2022-10-08T17:13:00.689Z", "postId": "22xpqq5SBRGCtyXtz", "htmlBody": "<p>Thanks Thomas - appreciate the updated research. And that wasn't a typo, just a poorly expressed idea. I meant to say, \"Only 17% of respondents reported less than 90% confidence that HLMI will eventually exist.\"</p>\n", "parentCommentId": "PPKfaitxFs9bYKDny", "user": {"username": "AndrewDoris"}}]