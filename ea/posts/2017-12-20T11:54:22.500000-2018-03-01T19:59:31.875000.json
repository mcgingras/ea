[{"_id": "niWhrjgQLcqysPMzA", "title": "The Concentric Circles model (CEA)", "postedAt": "2018-01-01T23:33:26.232Z", "htmlBody": "<p>When describing the target audience of our projects, it is useful to have labels for different parts of the community. Common language can help us communicate better internally, and so make better&nbsp;decisions.</p><p>What follows is simply a description of this language - unlike some of our other models, it doesn\u2019t make substantive claims about how the community is, or should&nbsp;be.</p><p>There are many ways to partition the EA community. One way is by&nbsp;engagement.</p><p>By \u201cengagement\u201d we mean a combination of three highly correlated factors: engagement with our ideas, dedication to act, and engagement with our&nbsp;community.</p><ul><li>Someone is engaged with effective altruism ideas if they are more familiar and favorable towards research from the community, and with the mindset that generated some of that&nbsp;research.</li><li>They are dedicated to act on those ideas if they devote more of their resources to helping in accordance with the&nbsp;ideas.</li><li>They are more engaged with the community if they have spent more time reading online content, commenting on the effective altruism forum, attending effective altruism events or carrying out related&nbsp;projects.</li></ul><p>These three factors are likely to be correlated, but imperfectly. (For instance, Bill Gates has read and endorsed some of the community\u2019s research, but hasn\u2019t significantly engaged with the community in person.) In what follows, we have tried to consider all three of these facets. A precise definition would be impossible, and we think that discussing all three factors gives a better sense of what is meant by each group&nbsp;discussed.</p><p>We can think of different levels of engagement as a series of concentric circles, where the smaller, inner circles are made up of the people who are most&nbsp;engaged.</p><figure class=\"image image_resized\" style=\"width:778.324px\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/niWhrjgQLcqysPMzA/mw8qkwn65gcag2vuyd7z\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/niWhrjgQLcqysPMzA/mw8qkwn65gcag2vuyd7z 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/niWhrjgQLcqysPMzA/rmxmy4mpvhniit3sq2og 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/niWhrjgQLcqysPMzA/wb8drnaqvvmvq1cwgvcs 768w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/niWhrjgQLcqysPMzA/x40ubocdy3ze1bujhfdi 992w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/niWhrjgQLcqysPMzA/koku2dpve1r7mlddwbc2 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/niWhrjgQLcqysPMzA/bekraqkvgpho3t6cu9xf 1800w\"></figure><p>Obviously, in reality these circles aren\u2019t perfectly concentric: there are several different, overlapping communities in effective altruism, focused around different ideas - e.g. there are communities of people who focus on AI safety, biosecurity, and animal welfare respectively. However, the notion of concentric circles is often a useful&nbsp;shorthand.</p><p>Also note that the concentric circles model is tracking engagement, not the total impact of an individual. For instance, Bill Gates has done incredibly impactful things: he has probably had a greater impact than nearly everyone in the EA community. However, he is not very highly engaged. Therefore, despite his high impact, he is not in the core of effective altruism. However, using the framework of the <a href=\"https://www.centreforeffectivealtruism.org/a-three-factor-model-of-community-building\">three-factor model</a>, dedication appears to be one of the determinants of impact, and engagement with ideas and engagement with the community may be correlated with&nbsp;realization.</p><p>Let us define the following levels of&nbsp;engagement:</p><ol><li><strong>Audience</strong><ol><li>Have not yet engaged with effective altruism, but might be partial to&nbsp;it</li></ol></li><li><strong>Followers</strong><ol><li>Understand some of the core ideas of effective&nbsp;altruism</li><li>Find the ideas plausible, but don\u2019t act to&nbsp;help</li><li>Have generally spent a couple of hours engaging with effective altruism, perhaps by:<ol><li>Joining the EA&nbsp;newsletter</li><li>Engaging with EA&nbsp;content</li><li>Attending one or two local&nbsp;events</li></ol></li></ol></li><li><strong>Participants</strong><ol><li>Understand the core ideas of effective altruism&nbsp;well</li><li>Are motivated to act on those&nbsp;ideas</li><li>Have generally spent a couple of days engaging with effective altruism, or made another significant commitment to effective altruism, for instance by<ol><li>Taking the Try Giving&nbsp;pledge</li><li>Using research from the EA community to inform their giving&nbsp;decisions</li><li>Attending EA Global X&nbsp;conferences</li></ol></li></ol></li><li><strong>Contributors</strong><ol><li>Have a detailed understanding of core ideas, and an understanding of more in-depth&nbsp;ideas</li><li>Are willing to make significant sacrifices to act on those&nbsp;ideas</li><li>Have spent weeks or months engaging with effective altruism, for instance by doing the above and:<ol><li>Taking the Giving What We Can&nbsp;pledge</li><li>Attending EA Global</li><li>Interning at aligned&nbsp;organizations</li><li>Running significant independent projects like a local group or a research&nbsp;project</li></ol></li></ol></li><li><strong>Core</strong><ol><li>Have an understanding of most ideas in effective altruism, normally with an expert-level understanding in some of the&nbsp;ideas</li><li>Have devoted most of their resources to acting on those&nbsp;ideas</li><li>Have dedicated their career to doing the most good in line with effective altruist principles, for instance by<ol><li>Working directly for an aligned organization or&nbsp;project</li><li>Earning to give while continuing to participate in the community\u2019s&nbsp;ideas</li></ol></li></ol></li><li><strong>Leadership</strong><ol><li>Have an understanding and devotion similar to the core, but they are also leaders of major effective altruist organizations, or are leaders of the intellectual development of the community. (It is likely to remain the case that some organizations and individuals have a much larger impact on the community than others, so there will be gradations within the leadership.)</li></ol></li></ol><p>The terms in the concentric circle model are most useful in helping us to communicate internally about the goals for each of our projects. For instance, when we were discussing what the audience for EA Global should be this year, we were able to use the terms \u201ccontributors\u201d and \u201cparticipants\u201d to set out different possibilities. Similarly, when we are writing a piece, we might have one of the above audiences in mind (this piece, for example, is mostly aimed at&nbsp;contributors).</p><p>Of course, these terms remain starting points for discussion. Normally, we will have a more specific or complicated audience in mind for any given&nbsp;project.</p>", "user": {"username": "EA Forum Archives"}}, {"_id": "J876n6AskHjXrTyfm", "title": "The funnel model (CEA)", "postedAt": "2018-01-01T23:35:49.497Z", "htmlBody": "<p>What is CEA trying to do, and what are the purposes of its individual projects? And how might we identify important new projects that we\u2019re&nbsp;missing?</p><p>A standard approach in business is to think of a <a href=\"https://en.wikipedia.org/wiki/Purchase_funnel\">sales funnel</a>. The idea is that there is a large number of people interested in a particular product or service, some of them can be convinced to read more about a particular version of the product, and some of those can be convinced to purchase the product. Finally, existing customers can be persuaded to become repeat&nbsp;customers.</p><p>The funnel model helps to suggest different projects for the business, each with different goals and target audiences. For instance, the first step in a sales process might be to get customers to visit a certain website - perhaps by having an interesting blog. But closer to a sale, the focus will be on providing answers to last-minute worries, and making the payment process&nbsp;easy.</p><figure class=\"image image_resized\" style=\"width:778.324px\"><img src=\"https://images.ctfassets.net/es8pp29e1wp8/3F77czAHnyyQiKcKggyYCC/66b8de9decd9ea0b16e217d614f9b7f0/mg_CEA_sitedesigns_ef-model_movementmodels_v1.1d.png?w=100&amp;q=80\" srcset=\"https://images.ctfassets.net/es8pp29e1wp8/3F77czAHnyyQiKcKggyYCC/66b8de9decd9ea0b16e217d614f9b7f0/mg_CEA_sitedesigns_ef-model_movementmodels_v1.1d.png?w=100&amp;q=80 100w, https://images.ctfassets.net/es8pp29e1wp8/3F77czAHnyyQiKcKggyYCC/66b8de9decd9ea0b16e217d614f9b7f0/mg_CEA_sitedesigns_ef-model_movementmodels_v1.1d.png?w=480&amp;q=70 480w, https://images.ctfassets.net/es8pp29e1wp8/3F77czAHnyyQiKcKggyYCC/66b8de9decd9ea0b16e217d614f9b7f0/mg_CEA_sitedesigns_ef-model_movementmodels_v1.1d.png?w=768&amp;q=70 768w, https://images.ctfassets.net/es8pp29e1wp8/3F77czAHnyyQiKcKggyYCC/66b8de9decd9ea0b16e217d614f9b7f0/mg_CEA_sitedesigns_ef-model_movementmodels_v1.1d.png?w=992&amp;q=70 992w, https://images.ctfassets.net/es8pp29e1wp8/3F77czAHnyyQiKcKggyYCC/66b8de9decd9ea0b16e217d614f9b7f0/mg_CEA_sitedesigns_ef-model_movementmodels_v1.1d.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/es8pp29e1wp8/3F77czAHnyyQiKcKggyYCC/66b8de9decd9ea0b16e217d614f9b7f0/mg_CEA_sitedesigns_ef-model_movementmodels_v1.1d.png?w=1800&amp;q=70 1800w\"></figure><p>The metaphor also helps businesses think about what to focus on. If you have a slick payment process, but no traffic to your site, you should probably focus further up the&nbsp;funnel.</p><p>CEA is not in the business of selling products. Instead, we are trying to foster a community, which means a more cooperative approach than some businesses take in making sales. A community is also different than a business in that community members interact with each other in a way that customers tend not to. This means that community members help determine how likely it is that other community members become more engaged. (This makes it even more important to make sure that community members at all stages have as accurate an understanding of effective altruism as possible, so that they can convey an accurate and convincing understanding to other community members.)</p><p>Nevertheless, we find the funnel metaphor useful. We are trying to build a community, and one aspect of this project is encouraging people to become more deeply engaged with the community. The funnel metaphor helps us to think about the appropriate goals and audiences for our different&nbsp;projects.</p><figure class=\"image image_resized\" style=\"width:778.324px\"><img src=\"https://images.ctfassets.net/es8pp29e1wp8/1NiZ50oQ3OU6q8iqwsyeeG/824bfd77ef987237030ba5f90f60d5f1/mg_CEA_sitedesigns_ef-model_movementmodels_v1.3b__1_.jpg?w=100&amp;q=80\" srcset=\"https://images.ctfassets.net/es8pp29e1wp8/1NiZ50oQ3OU6q8iqwsyeeG/824bfd77ef987237030ba5f90f60d5f1/mg_CEA_sitedesigns_ef-model_movementmodels_v1.3b__1_.jpg?w=100&amp;q=80 100w, https://images.ctfassets.net/es8pp29e1wp8/1NiZ50oQ3OU6q8iqwsyeeG/824bfd77ef987237030ba5f90f60d5f1/mg_CEA_sitedesigns_ef-model_movementmodels_v1.3b__1_.jpg?w=480&amp;q=70 480w, https://images.ctfassets.net/es8pp29e1wp8/1NiZ50oQ3OU6q8iqwsyeeG/824bfd77ef987237030ba5f90f60d5f1/mg_CEA_sitedesigns_ef-model_movementmodels_v1.3b__1_.jpg?w=768&amp;q=70 768w, https://images.ctfassets.net/es8pp29e1wp8/1NiZ50oQ3OU6q8iqwsyeeG/824bfd77ef987237030ba5f90f60d5f1/mg_CEA_sitedesigns_ef-model_movementmodels_v1.3b__1_.jpg?w=992&amp;q=70 992w, https://images.ctfassets.net/es8pp29e1wp8/1NiZ50oQ3OU6q8iqwsyeeG/824bfd77ef987237030ba5f90f60d5f1/mg_CEA_sitedesigns_ef-model_movementmodels_v1.3b__1_.jpg?w=1200&amp;q=70 1200w, https://images.ctfassets.net/es8pp29e1wp8/1NiZ50oQ3OU6q8iqwsyeeG/824bfd77ef987237030ba5f90f60d5f1/mg_CEA_sitedesigns_ef-model_movementmodels_v1.3b__1_.jpg?w=1800&amp;q=70 1800w\"></figure><p>Using the language of the <a href=\"https://www.centreforeffectivealtruism.org/the-concentric-circles-model\">Concentric Circles model</a>, we can view the outer circles as upper layers of the funnel. In this case, we can see that&nbsp;different parts of CEA operate to bring people into different parts of the&nbsp;funnel:</p><ol><li>Audience to followers:&nbsp;It is not currently a focus of CEA to help people who have only just heard about effective altruism. However, we support some of the first resources that people will encounter when they do express an interest, such as<ol><li>The EA Newsletter</li><li>Local groups</li></ol></li><li>Followers to Participants:<ol><li>Local groups</li><li>Giving What We&nbsp;Can</li><li>EA Global X&nbsp;conferences</li><li>Content</li></ol></li><li>Participants to Contributors:<ol><li>EA Global</li><li>CEA internships</li></ol></li><li>Contributors to Core:<ol><li>Effective Altruism Grants</li><li>Individual Outreach events</li><li>Individual Outreach coaching</li></ol></li><li>Core to leadership:<ol><li>The number of intellectual and organizational leaders will grow as new organizations are created, and new research areas&nbsp;established.</li><li>CEA does not try to directly affect the size or makeup of the leadership, but runs some events, such as the Leaders\u2019 Forum, catered to the&nbsp;leadership.</li></ol></li></ol><p>We are continuing to use this metaphor to identify parts of the above process that are working less smoothly than we would like, and thinking about how we can improve our operations in this&nbsp;area.</p><p>Currently, we are focusing mostly on Step 4: Contributors to Core. We think that there are many contributors in the community who are willing and able to work full-time on some of the world\u2019s most pressing problems, and we are trying to help them build the skills and network to do&nbsp;so.</p>", "user": {"username": "EA Forum Archives"}}, {"_id": "gZPvFgyjFbfrAGewN", "title": "History of Philanthropy Case Study: Clinton Health Access Initiative\u2019s Role in Global Price Drops for Antiretroviral Drugs", "postedAt": "2018-01-10T16:44:28.840Z", "htmlBody": "<p>A little over a year ago, the <a href=\"https://histphil.org/\">HistPhil blog</a> put up a <a href=\"https://histphil.org/2016/09/14/the-clinton-foundation-and-the-declining-price-of-antiretroviral-drugs-a-cautionary-success-story/\">post by Tamara Mann Tweel</a> about a now-<a href=\"https://www.openphilanthropy.org/files/History_of_Philanthropy/CHAI/Creating_The_World_Market_for_Lifesaving_AIDS_Drugs.docx\">published</a> report we commissioned her to work on, regarding the Clinton Health Access Initiative (CHAI)\u2019s role in global price drops for antiretroviral drugs (which can be crucial in treating HIV/AIDS).</p><p>The HistPhil post states:</p><blockquote><p><i>Antiretroviral drugs (ARVs) went down from 10,000 \u2013 $15,000 per person per year to $140 per person per year between 2000 and 2005. This price drop inspired governments and international bodies to purchase ARVs and administer therapy to millions of individuals stricken with HIV/AIDS.</i></p><p><i>While the Clinton Foundation often receives credit for the entirety of the ARV price drop, my report affirmed scholarship that claimed the price drop actually occurred in three stages. The first, from $15,000 per person per year to approximately $1000 per person per year in specific cases, can be attributed to activists persuading pharmaceutical companies to offer philanthropic prices to discreet pilot projects; the second price drop, from approximately $1000 per person per year to approximately $350 per person per year, can be attributed to the active creation of an international generic drug market; and the final drop, from $350 to $140, can be attributed to deliberate market interventions into the generic market by the Clinton Health Access Initiative (CHAI).</i></p></blockquote><p>As discussed in <a href=\"https://www.openphilanthropy.org/files/History_of_Philanthropy/CHAI/Creating_The_World_Market_for_Lifesaving_AIDS_Drugs.docx\">the full report</a>, this three-stage price drop corresponded to a massive increase in the purchases of antivirals (especially by governments and nonprofits); we haven\u2019t specifically estimated the deaths averted by this development, but feel confident that it qualifies as the sort of <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">hit</a> we\u2019re interested in.</p><p>We don\u2019t feel fully confident that any particular funder or nonprofit was crucial to the price drop. The key things we learned from the report were that:</p><ul><li>In the first stage, activists targeted a lawsuit by pharmaceutical companies against South Africa (initially with the support of the U.S. government).<ul><li>The South African Parliament had given its Ministry of Health authority for <a href=\"https://www.wto.org/english/tratop_e/trips_e/public_health_faq_e.htm\">compulsory licensing</a> and <a href=\"https://en.wikipedia.org/wiki/Parallel_import\">parallel imports</a> of antiretrovirals, and the pharmaceutical companies were suing it for breach of international intellectual property protections.</li><li>The report states: \u201cOxfam, ACT UP, and MSF issued press releases and took out ads critical of the pharmaceutical industry. MSF started an online petition demanding that the drug companies drop the lawsuit. ACT UP organized demonstrations in New York City against the pharmaceutical companies outside their corporate offices, featuring slogans like \u2018Stop Medical Apartheid of AIDS\u2019 and \u2018Drug Company Greed Kills.\u2019 The access NGOs\u2019 \u2018name and shame\u2019 campaign was widely considered a success, and the lawsuit became a \u2018public relations nightmare\u2019 for the pharmaceutical companies [<a href=\"http://online.wsj.com/news/articles/SB983487988418159849\">source</a>].\u201d</li><li>The US government reversed its position on the lawsuit (though the report cites <a href=\"https://cyber.harvard.edu/people/tfisher/South%20Africa.pdf\">Fisher and Rigamonti 2005, pg 35-38</a> for evidence that this wasn\u2019t necessarily purely a function of the activism). Pharma companies dropped the lawsuit, took the \u201cunusual step of reimbursing the South African government for its legal expense,\u201d and began selling drugs at steeply discounted prices to nonprofits such as the UN\u2019s Drug Access Initiative.</li><li>We\u2019d guess it\u2019s more likely than not that the activists played a crucial role in these changes - as well as in a later agreement at Doha that \u201cforeclosed the possibility of WTO sanctions over compulsory licensing\u201d - but the report did not focus heavily on this phase.</li></ul></li><li>In the second stage, the report credits <a href=\"https://en.wikipedia.org/wiki/James_Love_(NGO_director)\">Jamie Love</a> for a major role in arranging for generic manufacturers to sell antiretrovirals at much lower prices, and in much greater volumes, than they would have otherwise. He studied the requirements for manufacturing the drugs, \u201csuggested that a $350 annual per patient ARV price might be possible\u201d due to the possibility of far more sales at this price, and helped a major buyer (MSF) coordinate with manufacturers to plan and execute this change.</li><li>The third phase is where the report focused, and the <a href=\"https://histphil.org/2016/09/14/the-clinton-foundation-and-the-declining-price-of-antiretroviral-drugs-a-cautionary-success-story/\">HistPhil post</a> summarizes the activities and role of CHAI in working toward further price reductions.<ul><li>Working with a similar \u201clower prices, more volume\u201d goal that Jamie Love had had, CHAI created a \u201cbuying club\u201d to pool purchases - and, perhaps crucially, guarantee growing purchases over time - for developing countries, and used the promise of greater and better-known volume to negotiate price drops. It also \u201chelped source cheaper ingredients, and eventually they even paid chemists to develop less expensive manufacturing and synthesizing techniques.\u201d</li><li>I felt less convinced than the report\u2019s author that CHAI had clearly been crucial to the final stage of price drops, given the extent of price reduction that had already occurred prior to CHAI\u2019s involvement, but would guess that it at least sped up the reduction.</li></ul></li></ul><p>For more, see the <a href=\"https://www.openphilanthropy.org/files/History_of_Philanthropy/CHAI/Creating_The_World_Market_for_Lifesaving_AIDS_Drugs.docx\">full report</a>, the <a href=\"https://histphil.org/2016/09/14/the-clinton-foundation-and-the-declining-price-of-antiretroviral-drugs-a-cautionary-success-story/\">HistPhil post</a>, and a <a href=\"https://www.vox.com/policy-and-politics/2016/9/22/12893444/clinton-foundation-effectiveness\">Vox.com article that drew on the HistPhil post</a>.</p>", "user": {"username": "HoldenKarnofsky"}}, {"_id": "PsiiMHvHoHqzGu6Qm", "title": "Update on Cause Prioritization at Open Philanthropy", "postedAt": "2018-01-26T16:40:08.648Z", "htmlBody": "<p>Last year, we <a href=\"https://www.openphilanthropy.org/blog/our-progress-2016-and-plans-2017#Cause_prioritization_framework_and_last_dollar_analysis\">wrote</a>:</p>\n<p><em>A <strong>major goal of 2017</strong> will be to reach and publish better-developed views on:</em></p>\n<ul>\n<li><em>Which <a href=\"http://www.openphilanthropy.org/blog/worldview-diversification\">worldviews</a> we find most plausible: for example, how we allocate resources between giving that primarily focuses on present-day human welfare vs. present-day animal welfare vs. <a href=\"http://www.openphilanthropy.org/focus/global-catastrophic-risks\">global catastrophic risks</a>.</em></li>\n<li><em>How we allocate resources among worldviews.</em></li>\n<li><em>How we determine whether it\u2019s <a href=\"http://www.openphilanthropy.org/blog/good-ventures-and-giving-now-vs-later-2016-update\">better to make a given grant or save the money for a later date</a>.</em></li>\n</ul>\n<p>This post gives an update on this work.</p>\n<p>The questions we\u2019re tackling here are complex, and we are still far from having a fully developed framework.</p>\n<p>However, we do have a tentative high-level approach to these questions, and some rough expectations about a few high-level conclusions (at least as far as the next few years are concerned). Hopefully, laying these out will clarify - among other things - (a) why we continue to work on multiple highly disparate causes; (b) ranges for what sort of budgets we expect in the next few years for each of the focus areas we currently work in; (c) how we decided how much to recommend that Good Ventures donate to <a href=\"http://www.givewell.org/\">GiveWell\u2019s</a> top charities for 2017.</p>\n<p>In brief:</p>\n<ul>\n<li>When choosing how to allocate capital, we need to decide between multiple <strong>worldviews.</strong> We use \u201cworldview\u201d to refer to a highly debatable (and perhaps impossible to evaluate) set of beliefs that favor a certain kind of giving. Worldviews can represent a mix of moral values, views on epistemology and methodology, and heuristics. Worldviews that are particularly salient to us include:\n<ul>\n<li>a \u201clong-termist\u201d worldview that ascribes very high <a href=\"https://www.openphilanthropy.org/blog/moral-value-far-future\">value to the long-term future</a>, such that it assesses grants by how well they advance the odds of favorable long-term outcomes for civilization;</li>\n<li>a \u201cnear-termist, human-centric\u201d worldview that assesses grants by how well they improve the lives of humans (excluding all animal welfare considerations) on a relatively shorter time horizon;</li>\n<li>a \u201cnear-termist, animal-inclusive\u201d view that focuses on a similarly short time horizon but ascribes significant moral weight to animals.</li>\n</ul>\n</li>\n<li>If we evaluated all grants\u2019 cost-effectiveness in the same terms (e.g., \u201cPersons helped per dollar, including animals\u201d or \u201cPersons helped per dollar, excluding animals\u201d), this would likely result effectively putting all our resources behind a single worldview. For reasons given below (and <a href=\"https://www.openphilanthropy.org/blog/worldview-diversification\">previously</a>), we don\u2019t want to do this. Instead, we\u2019re likely to divide the available capital into buckets, with different buckets operating according to different worldviews and in some cases other criteria as well. E.g., we might allocate X% of capital to a bucket that aims to maximize impact from a \u201clong-termist\u201d perspective, and Y% of capital to a bucket that aims to maximize impact from a \u201cnear-termist\u201d perspective, with further subdivisions to account for other worldview splits (e.g., around the moral weight of animals) and other criteria. These buckets would then, in turn, allocate \u201ctheir\u201d capital to causes in order to best accomplish their goals; for example, a long-termist bucket might allocate some of its X% to <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/biosecurity\">biosecurity and pandemic preparedness</a>, some to causes that seek to improve decision-making generally, etc.</li>\n<li>In setting these allocations, we will need to:\n<ul>\n<li>Consider how much credence/weight we assign to each worldview, which in turn will be informed by a number of investigations and writeups that are in progress.</li>\n<li>Consider how appropriate it is to (a) model the different worldviews as different \u201cagents\u201d that have fundamentally different goals (and can make trades and agreements with each other), vs. (b) modeling them as different empirical beliefs that we can assign probabilities to and handle in an expected-value framework.</li>\n<li>To the extent we model the different worldviews as different \u201cagents\u201d: consider what deals and agreements they might make with each other - such as trying to arrange for worldviews to get more capital in situations where the capital is especially valuable. (In other words, we don\u2019t want to allocate capital to worldviews in a vacuum: more should go to worldviews that have more outstanding or \u201coutlier\u201d giving opportunities by their own lights.)</li>\n<li>Allocate some capital to buckets that might not be directly implied by the goals of any of our leading worldviews, for purposes of accomplishing a number of practical and other goals (e.g., <a href=\"https://www.openphilanthropy.org/blog/worldview-diversification\">practical benefits of worldview diversification</a>).</li>\n</ul>\n</li>\n<li>This process will require a large number of deep judgment calls about moral values, difficult-to-assess heuristics and empirical claims, etc. As such, we may follow a similar process to <a href=\"https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/cost-effectiveness-models\">GiveWell\u2019s cost-effectiveness analysis</a>: having every interested staff member fill in their inputs for key parameters, discussing our differences, and creating a summary of key disagreements as well as median values for key parameters. However, ultimately, this exercise serves to inform funders about how to allocate their funds, and as such the funders of <a href=\"http://www.goodventures.org/\">Good Ventures</a> will make the final call about how to allocate their capital between buckets.</li>\n<li>We don\u2019t expect to set the capital allocation between buckets all at once. We expect a continuing iterative process in which - at any given time - we are making enough guesses and tentative allocations to set our working budgets for existing focus areas and our desired trajectory for total giving over the next few years. We expect the detail and confidence of our capital allocation between buckets to increase in parallel with total giving, and to be fairly high by the time we reach peak giving (which could be 10+ years from now).</li>\n<li>This post includes some high-level outputs from this framework that are reasonably likely to guide our giving over the next 1-5 years, though they could still easily change. These are:\n<ul>\n<li>We will probably recommend that a cluster of \u201clong-termist\u201d buckets collectively receive the largest allocation: at least 50% of all available capital. Grants in these buckets will be assessed by how well they advance the odds of favorable long-term outcomes for civilization. We currently believe that <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks\">global catastrophic risk reduction</a> accounts for some of the most promising work by this standard (though there are many focus areas that can be argued for under it, e.g. work to promote international peace and cooperation).</li>\n<li>We will likely want to ensure that we have substantial, and somewhat diversified, programs in <a href=\"https://www.openphilanthropy.org/focus/us-policy\">policy-oriented philanthropy</a> and <a href=\"https://www.openphilanthropy.org/focus/scientific-research\">scientific research funding</a>, for a variety of practical reasons. I expect that we will recommend allocating at least $50 million per year to policy-oriented causes, and at least $50 million per year to scientific-research-oriented causes, for at least the next 5 or so years.</li>\n<li>We will likely recommend allocating something like 10% of available capital to a \u201cstraightforward charity\u201d bucket (described more below), which will likely correspond to supporting <a href=\"http://www.givewell.org/\">GiveWell</a> recommendations for the near future.</li>\n<li>Other details of our likely allocation are yet to be determined. We expect to ultimately recommend a substantial allocation (and not shrink our current commitment) to <a href=\"https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare\">farm animal welfare</a>, but it\u2019s a very open question whether and how much this allocation will grow. We also note that making informed and thoughtful decisions about capital allocation is very valuable to us, and we\u2019re open to significant capital allocations aimed specifically at this goal (for example, funding research on the relative merits of the various worldviews and their implicit assumptions) if we see good opportunities to make them.</li>\n</ul>\n</li>\n<li>A notable outcome of the framework we\u2019re working on is that we will <strong>no longer have a single \u201cbenchmark\u201d for giving now vs. later,</strong> as we <a href=\"https://blog.givewell.org/2015/11/25/good-ventures-and-giving-now-vs-later/#Benchmark\">did in the past</a>. Rather, each bucket of capital will have its own standards and way of assessing grants to determine whether they qualify for drawing down the capital in that bucket. For example, there might be one bucket that aims to maximize impact according to a long-termist worldview, and another that aims to maximize impact according to a near-termist worldview; each would have different benchmarks and other criteria for deciding on whether to make a given grant or save the money for later. We think this approach is a natural outcome of <a href=\"https://www.openphilanthropy.org/blog/worldview-diversification\">worldview diversification</a>, and will help us establish more systematic benchmarks than we currently have.</li>\n</ul>\n<h2>Key worldview choices and why they might call for diversification</h2>\n<p>Over the coming years, we expect to increase the scale of our giving significantly. Before we do so, we\u2019d like to become more systematic about how much we budget for each of our different <a href=\"https://www.openphilanthropy.org/focus\">focus areas</a>, as well as about what we budget for one year vs. another (i.e., how we decide when to <a href=\"https://www.openphilanthropy.org/blog/good-ventures-and-giving-now-vs-later-2016-update\">give immediately vs. save the money for later</a>).</p>\n<p>At first glance, the ideal way to tackle this challenge would be to establish some common metric for grants. To simplify, one might imagine a metric such as \u201clives improved (adjusted for degree of improvement) per dollar spent.\u201d We could then use this metric to (a) compare what we can accomplish at different budget sizes in different <a href=\"https://www.openphilanthropy.org/focus\">areas</a>; (b) make grants when they seem better than our \u201clast dollar\u201d (more discussion of the \u201clast dollar\u201d concept <a href=\"https://www.openphilanthropy.org/blog/good-ventures-and-giving-now-vs-later-2016-update#Benchmark\">here</a>), and save the money instead when they don\u2019t. Our past discussions of our approach to \u201cgiving now vs. later\u201d (<a href=\"https://www.openphilanthropy.org/blog/good-ventures-and-giving-now-vs-later-2016-update\">here</a> and <a href=\"http://blog.givewell.org/2015/11/25/good-ventures-and-giving-now-vs-later/#Budget\">here</a>) have implied an approach along these lines. I will refer to this approach as the \u201cdefault approach\u201d to allocating capital between causes (and will later contrast it with a \u201cdiversifying approach\u201d that divides capital into different \u201cbuckets\u201d using different metrics).</p>\n<p>A major challenge here is that <strong>many of the comparisons we\u2019d like to make hinge on very debatable questions involving deep uncertainty</strong>. In <a href=\"https://www.openphilanthropy.org/blog/worldview-diversification\">Worldview Diversification</a>, we characterized this dilemma, gave examples, and defined a \u201cworldview\u201d (for our purposes) as follows:</p>\n<p><em>I\u2019ll use \u201cworldview\u201d to refer to a set of highly debatable (and perhaps impossible to evaluate) beliefs that favor a certain kind of giving. One worldview might imply that evidence-backed charities serving the global poor are far more worthwhile than [other options]; another might imply that farm animal welfare is; another might imply that global catastrophic risk reduction is. A given worldview represents a combination of views, sometimes very difficult to disentangle, such that uncertainty between worldviews is constituted by a mix of empirical uncertainty (uncertainty about facts), normative uncertainty (uncertainty about morality), and methodological uncertainty (e.g. uncertainty about how to handle uncertainty \u2026)</em></p>\n<p>Below, we list some of what we view as the most crucial worldview choices we are facing, along with notes on why they might call for some allocation procedure other than the \u201cdefault approach\u201d described above - and a brief outline (fleshed out somewhat more in later sections) on what an alternative procedure might look like.</p>\n<h3>Animal-inclusive vs human-centric views</h3>\n<p>As we stated in our <a href=\"https://www.openphilanthropy.org/blog/worldview-diversification\">earlier post on worldview diversification</a>:</p>\n<p><em>Some people think that animals such as chickens have essentially no moral significance compared to that of humans; others think that they should be considered comparably important, or at least 1-10% as important. If you accept the latter view, <a href=\"https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare\">farm animal welfare</a> looks like an extraordinarily outstanding cause, potentially to the point of dominating other options: billions of chickens are treated incredibly cruelly each year on factory farms, and we estimate that <a href=\"https://www.openphilanthropy.org/blog/initial-grants-support-corporate-cage-free-reforms\">corporate campaigns</a> can spare over 200 hens from cage confinement for each dollar spent. But if you accept the former view, this work is arguably a poor use of money.</em></p>\n<p>(Note: this quote leaves out the caveat that this picture could change dramatically if one adopts the \u201clong-termist\u201d view discussed in a later section. For simplicity, the rest of this section will assume that one is focused on relatively near-term good accomplished rather than taking the \u201clong-termist\u201d view discussed below.)</p>\n<p>We\u2019ve since published an <a href=\"https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood\">extensive report on moral patienthood</a> that grew out of our efforts to become better informed on this topic. However, we still feel that we have relatively little to go on, to the point where the report\u2019s author wasn\u2019t comfortable publishing even his roughest guesses at the relative moral weights of different animals. Although he did publish his subjective <a href=\"https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood#Probabilities\">probabilities</a> that different species have \u201cconsciousness of a sort I intuitively morally care about,\u201d these are not sufficient to establish relative weight, and one of the main inputs into these probabilities is simple ignorance/agnosticism.</p>\n<p>There are many potential judgment calls to be made regarding moral weight - for example, two people might agree on the moral weight of cows (relative to humans) while strongly disagreeing on the moral weight of chickens, or agree on chickens but disagree on fish. For our purposes, we focus on one high-level disagreement. The disagreement is between two views:</p>\n<ul>\n<li>\n<p>An \u201canimal-inclusive\u201d view that assigns moral weight using explicit subjective estimates with a high degree of agnosticism, and generally considers members of most relevant<sup class=\"footnote-ref\"><a href=\"#fn-QaGgPdaXAwPJc9hSp-1\" id=\"fnref-QaGgPdaXAwPJc9hSp-1\">[1]</a></sup> species of fish, birds, and mammals to carry at least 1% as much moral weight as humans. At a high level, taking an \u201canimal-inclusive view\u201d allows - and, given the current state of the world, is likely to often result in<sup class=\"footnote-ref\"><a href=\"#fn-QaGgPdaXAwPJc9hSp-2\" id=\"fnref-QaGgPdaXAwPJc9hSp-2\">[2]</a></sup> - <strong>letting the interests of nonhuman animals <em>dominate</em> giving decisions.</strong> Beyond that, different versions of this view could have different consequences.</p>\n</li>\n<li>\n<p>A \u201chuman-centric\u201d view that effectively treats all non-human animals as having zero moral weight. This view could be driven by something like: (a) being suspicious, methodologically speaking, of estimating moral weights in an explicit (and in practice largely agnosticism-based) framework, and therefore opting for a conventional/\u201dnon-radical\u201d set of views in one\u2019s state of ignorance; (b) having criteria for <a href=\"https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood\">moral patienthood</a> that revolve around something other than consciousness, such as the idea that we have special obligations to humans (relative to animals).<sup class=\"footnote-ref\"><a href=\"#fn-QaGgPdaXAwPJc9hSp-3\" id=\"fnref-QaGgPdaXAwPJc9hSp-3\">[3]</a></sup>\nThis \u201canimal-inclusive\u201d vs. \u201chuman-centric\u201d split is a crude simplification of the many possible disagreements over moral weights, but in practice I believe most people fall into one camp or the other, and that the two camps can have radically different implications (at least when one focuses on near-term good accomplished rather than taking the \u201clong-termist\u201d perspective discussed below).</p>\n</li>\n</ul>\n<h4>Handling uncertainty about animal-inclusive vs. human-centric views</h4>\n<p>If we were taking the \u201cdefault approach\u201d noted above, we could handle our uncertainty on this front by assigning a subjective probability to each of the \u201canimal-inclusive\u201d or \u201chuman-centric\u201d views (or, for more granularity, assigning subjective probability distributions over the \u201cmoral weights\u201d of many different species relative to humans) and making all grants according to whatever maximizes some metric such as \u201cexpected years of life improved,<sup class=\"footnote-ref\"><a href=\"#fn-QaGgPdaXAwPJc9hSp-4\" id=\"fnref-QaGgPdaXAwPJc9hSp-4\">[4]</a></sup> adjusted for moral weight.\u201d For example, if one thinks there\u2019s a 50% chance that one should be weighing the interests of chickens 1% as much as those of humans, and a 50% chance that one should not weigh them at all, one might treat this situation as though chickens have an \u201cexpected moral weight\u201d of 0.5% (50% * 1% + 50% * 0) relative to humans. This would imply that (all else equal) a grant that helps 300,000 chickens is better than a grant that helps 1,000 humans, while a grant that helps 100,000 chickens is worse.</p>\n<p>This default approach has several undesirable properties. We discussed these somewhat in <a href=\"https://www.openphilanthropy.org/blog/worldview-diversification\">our previous post on worldview diversification</a>, but since our thinking has evolved, we list the main issues we see with the default approach below.</p>\n<h5>Issue 1: normative uncertainty and philosophical incommensurability</h5>\n<p>The \u201canimal-inclusive\u201d vs. \u201chuman-centric\u201d divide could be interpreted as being about a form of \u201cnormative uncertainty\u201d: uncertainty between two different views of morality. It\u2019s not entirely clear how to create a single \u201ccommon metric\u201d for adjudicating between two views. Consider:</p>\n<ul>\n<li>\n<p><em>Comparison method A:</em> say that \u201ca human life improved<sup class=\"footnote-ref\"><a href=\"#fn-QaGgPdaXAwPJc9hSp-5\" id=\"fnref-QaGgPdaXAwPJc9hSp-5\">[5]</a></sup>\u201d is the main metric valued by the human-centric worldview, and that \u201ca chicken life improved\u201d is worth &gt;1% of these (animal-inclusive view) or 0 of these (human-centric view). In this case, a &gt;10% probability on the animal-inclusive view would lead chickens to be valued &gt;0.1% as much as humans, which would likely imply a great deal of resources devoted to animal welfare relative to near-term human-focused causes.<sup class=\"footnote-ref\"><a href=\"#fn-QaGgPdaXAwPJc9hSp-6\" id=\"fnref-QaGgPdaXAwPJc9hSp-6\">[6]</a></sup></p>\n</li>\n<li>\n<p><em>Comparison method B:</em> say that \u201ca chicken life improved\u201d is the main metric valued by the animal-inclusive worldview, and that \u201ca human life improved\u201d is worth &lt;100 of these (animal-inclusive view) or an astronomical number of these (human-centric view). In this case, a &gt;10% probability on the human-inclusive view would be effectively similar to a 100% probability on the human-centric view.</p>\n</li>\n</ul>\n<p>These methods have essentially opposite practical implications. Method A is the more intuitive one for me (it implies that the animal-inclusive view sees \u201cmore total value at stake in the world as a whole,\u201d and this implication seems correct), but the lack of a clear principle for choosing between the two should give one pause, and there\u2019s no obviously appropriate way to handle this sort of uncertainty. One could argue that the two views are \u201cphilosophically incommensurable\u201d in the sense of dealing with fundamentally different units of value, with no way to identify an equivalence-based conversion factor between the two.</p>\n<p>This topic is further discussed in Chapter 4 of <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/03/MacAskill-Normative-Uncertainty.pdf\">MacAskill 2014</a>.</p>\n<h5>Issue 2: methodological uncertainty and practical incommensurability</h5>\n<p>As stated above, a major potential reason for taking the human-centric view is \u201cbeing suspicious, methodologically speaking, of estimating moral weights in an explicit (and in practice largely agnosticism-based) framework, and therefore opting for a conventional/\u2019non-radical\u2019 set of views in one\u2019s state of ignorance.\u201d Yet the default approach essentially comes down to evaluating <em>this concern</em> using an explicit (and in practice largely agnosticism-based) framework and embracing whatever radical implications result. It therefore seems like a question-begging and inappropriate methodology for handling such a concern.</p>\n<p>It\u2019s not clear what methodology could adjudicate a concern like this in a way that is \u201cfair\u201d both to the possibility this concern is valid and the possibility that it isn\u2019t. Because of this, one might say that the two views are \u201cpractically incommensurable\u201d: there is no available way to reasonably, practically come up with \u201ccommon metrics\u201d and thus make apples-to-apples comparisons between them.</p>\n<h5>Issue 3: practical considerations against \u201cputting all our eggs in one basket\u201d</h5>\n<p>We believe that if we took the default approach in this case, there\u2019s a strong chance that we would end up effectively going \u201call-in\u201d on something very similar to the animal-inclusive view.<sup class=\"footnote-ref\"><a href=\"#fn-QaGgPdaXAwPJc9hSp-7\" id=\"fnref-QaGgPdaXAwPJc9hSp-7\">[7]</a></sup> This could mean focusing our giving on a few cause areas that are currently extremely small, as we believe there are very few people or organizations that are both (a) focused on animal welfare and (b) focused on having highly cost-effective impact (affecting large numbers of animals per dollar). Even if these fields grew in response to our funding, they would likely continue to be quite small and idiosyncratic relative to the wider world of philanthropic causes.</p>\n<p>Over time, we aspire to become the go-to experts on impact-focused giving; to become powerful advocates for this broad idea; and to have an influence on the way many philanthropists make choices. Broadly speaking, we think our odds of doing this would fall greatly if we were all-in on animal-focused causes. We would essentially be tying the success of our broad vision for impact-focused philanthropy to a concentrated bet on animal causes (and their idiosyncrasies) in particular. And we\u2019d be giving up many of the <a href=\"https://www.openphilanthropy.org/blog/worldview-diversification#Capacity_building_and_option_value\">practical benefits we listed previously for a more diversified approach</a>. Briefly recapped, these are: (a) being able to provide tangibly useful information to a large set of donors; (b) developing staff capacity to work in many causes in case our best-guess worldview changes over time; (c) using lessons learned in some causes to improve our work in others; (d) presenting an accurate public-facing picture of our values; and (e) increasing the degree to which, over the long run, our expected impact matches our actual impact (which could be beneficial for our own, and others\u2019, ability to evaluate how we\u2019re doing).</p>\n<h5>Issue 4: the \u201coutlier opportunities\u201d principle</h5>\n<p>We see a great deal of intuitive appeal in the following principle, which we\u2019ll call the \u201coutlier opportunities\u201d principle:</p>\n<p><em>if we see an opportunity to do a huge, and in some sense \u201cunusual\u201d or \u201coutlier,\u201d amount of good according to worldview A by sacrificing a relatively modest, and in some sense \u201ccommon\u201d or \u201cnormal,\u201d amount of good according to worldview B, we should do so (presuming that we consider both worldview A and worldview B highly plausible and reasonable and have deep uncertainty between them).</em></p>\n<p>To give a hypothetical example, imagine that:</p>\n<ul>\n<li>We are allocating $100 million between <a href=\"https://www.openphilanthropy.org/blog/initial-grants-support-corporate-cage-free-reforms\">cage-free reforms</a> and <a href=\"http://www.givewell.org/charities/top-charities\">GiveWell\u2019s top charities</a>, and estimate that: (a) work on cage-free reforms could improve 1000 chicken-life-years per dollar spent (e.g. improve 200 chickens\u2019 lives for 5 years each), while (b) GiveWell\u2019s top charities could improve one human-life-year by an equivalent amount for every $500 spent. Suppose further that we imagine these figures to apply to large numbers of giving opportunities, enough to absorb the full $100 million easily, and to be broadly representative of the best giving opportunities one encounters in normal circumstances.</li>\n<li>Along the lines of the default approach, we value chicken-life-years at 0.5% as much as human-life-years. This would imply that cage-free reforms could improve the equivalent of 5 human-life-years per dollar spent, making them 2500x as cost-effective as GiveWell\u2019s top charities from our perspective.</li>\n<li>We then notice a particular opportunity to improve 2 million human-life-years (by an equivalent amount) for $1 million, perhaps by funding a highly promising treatment for a rare disease, and believe that this opportunity would not get funded in our absence. Call this an \u201coutlier opportunity,\u201d notable for its unusual cost-effectiveness in the broader scheme of giving opportunities for the human-centric worldview.</li>\n</ul>\n<p>In this hypothetical, the outlier opportunity would be ~1000x as cost-effective as the other top human-centric opportunities, but still &lt;50% as cost-effective as the vast amount of work to be funded on cage-free reforms. In this hypothetical, I think there\u2019s a strong intuitive case for funding the outlier opportunity nonetheless. (I think even more compelling cases can be imagined for some other worldview contrasts, as in the case of the \u201clong-termist\u201d vs. \u201cnear-termist\u201d views discussed below.)</p>\n<p>The outlier opportunities principle could be defended and debated on a number of grounds, and some version of it may follow straightforwardly from handling \u201cincommensurability\u201d between worldviews as discussed above. However, we think the intuitive appeal of the principle is worth calling out by itself, since one might disagree with specific arguments for the principle while still accepting some version of it.</p>\n<p>It\u2019s unclear how to apply the outlier opportunities principle in practice. It\u2019s occurred to us that the first $X we allocate according to a given worldview might, in many cases, be an \u201coutlier opportunity\u201d for that worldview, for the minimum $X that allows us to hire staff, explore giving opportunities, make sure to fund the very best ones, and provide guidance to other donors in the cause. This is highly debatable for any given specific case. More broadly, the outlier opportunities principle may be compelling to some as an indication of <em>some</em> sort of drawback in principle to the default approach.</p>\n<h5>A simple alternative to the default approach</h5>\n<p>When considering the animal-inclusive vs. human-centric worldview, one simple alternative to the default approach would be to split our available capital into two equally sized buckets, and have one bucket correspond to each worldview. This would result in allocating half of the capital however one needs to in order to maximize humans helped, and half however one needs to in order to maximize a metric more like \u201cspecies-adjusted persons helped, where chickens and many other species generally count more than 1% as much as humans.\u201d</p>\n<p>I\u2019ll note that this approach is probably intuitively appealing for misleading reasons (more <a href=\"https://www.openphilanthropy.org/blog/worldview-diversification#The_case_against_worldview_diversification\">here</a>), i.e., it has less going for it than one might initially guess. I consider it a blunt and unprincipled approach to uncertainty, but one that does seem to simultaneously relieve many of the problems raised above:</p>\n<ul>\n<li>If the two views are incommensurable (issues 1 and 2 above), such that one cannot meaningfully reach \u201ccommon units\u201d of value between them, one way of imagining this situation (metaphorically) might be to imagine the two views as though they are two different agents with fundamentally different and incommensurable goals, disagreeing about how to spend capital. A first-pass way of allocating capital \u201cfairly\u201d in such a situation would be to divide it evenly between the two agents; the approach described here is largely equivalent to that solution.</li>\n<li>This approach would probably ensure that most of the \u201cpractical benefits of worldview diversification\u201d (being able to provide tangibly useful information to a large set of donors; developing staff capacity to work in many causes; etc.) are realized.</li>\n<li>It would also ensure that \u201coutlier opportunities\u201d according to both worldviews are funded.</li>\n</ul>\n<p>As discussed <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#Allocating_capital_to_buckets_and_causes\">later in this post</a>, I think this approach can be improved on, but I find it useful as a starting-point alternative to the default approach.</p>\n<h3>Long-termist vs. near-termist views</h3>\n<p>We\u2019ve <a href=\"https://www.openphilanthropy.org/blog/moral-value-far-future\">written before</a> about the idea that:</p>\n<p><em>most of the people we can help (with our giving, our work, etc.) are people who haven\u2019t been born yet. By working to lower <a href=\"https://www.openphilanthropy.org/blog/potential-global-catastrophic-risk-focus-areas\">global catastrophic risks</a>, speed <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">economic development and technological innovation</a>, and generally improve people\u2019s resources, capabilities, and values, we may have an impact that (even if small today) reverberates for generations to come, helping more people in the future than we can hope to help in the present.</em></p>\n<p>In the &gt;3 years since that post, I\u2019ve come to place substantially more weight on this view, for several reasons:</p>\n<ul>\n<li>In my experience, it seems that the people who seem most knowledgeable and reflective about how to apply the principles of <a href=\"https://www.centreforeffectivealtruism.org/what-is-effective-altruism/\">effective altruism</a> disproportionately favor this view.</li>\n<li>I\u2019ve learned more about the case for this view. In particular:\n<ul>\n<li>I now believe that there is a substantial chance (easily over 10%) that if civilization survives the next few hundred years, it could lead to an extremely large, overwhelmingly positive world that is much more robust to <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks\">global catastrophic risks</a> than today\u2019s world. We expect to publish analysis on this topic in the future.</li>\n<li>My intuitions, like those of many others, say it is far more important to improve the lives of persons who do (or will) exist regardless of our actions, than it is to increase the odds that a large number of positive lives exist in the future. Because of this, I\u2019m instinctively skeptical that the possibility of even a very large positive world should carry too much weight in our moral calculations. However, I now believe there are good arguments to the contrary, such that I assign a substantial chance (easily over 10%) that I will eventually change my mind and come to believe that, e.g., preventing extinction is worth at least a trillion times more (due to the future generations it allows to exist) than a simple \u201clives saved today\u201d calculation would imply. The arguments on this topic are in the <a href=\"https://en.wikipedia.org/wiki/Population_ethics\">population ethics</a> literature; we expect to publish a review and summary in the future.</li>\n<li>I\u2019ve come to believe that there is highly <a href=\"https://www.openphilanthropy.org/focus\">important, neglected, tractable</a> work to do that is suited to improving long-run outcomes for large numbers of generations. Most of this falls under <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks\">global catastrophic risk reduction</a>. (Note: I don\u2019t mean to imply that global catastrophic risk reduction can <em>only</em> be justified by appealing to its impact on future generations. Sufficiently significant global catastrophic risk reduction could be justified by its benefits for the current generation alone, and thus could be strong according to the \u201cnear-termist\u201d view discussed below.)</li>\n</ul>\n</li>\n</ul>\n<p>I characterize the \u201clong-termist view\u201d as combining: (a) population ethics that assigns reasonably high moral weight to the outcome of \u201ca person with high well-being, who otherwise would not have existed\u201d (at least 1% relative to e.g. \u201ca person has a high well-being, who would otherwise have had low well-being\u201d);<sup class=\"footnote-ref\"><a href=\"#fn-QaGgPdaXAwPJc9hSp-8\" id=\"fnref-QaGgPdaXAwPJc9hSp-8\">[8]</a></sup> (b) methodological comfort with statements such as \u201cGood philanthropy may have a nontrivial impact on the likelihood that future civilization is very large (many generations, high population per generation) and very high in well-being; this impact would be very high relative to any short- or medium-term impact that can be had.\u201d I believe that in practice, one who is comfortable with the basic methodological and moral approach here is likely to end up assessing grants primarily based on how well they advance the odds of favorable long-term outcomes for civilization.</p>\n<p>(One could also reach a long-termist conclusion (assessing grants primarily based on how well they advance the odds of favorable long-term outcomes for civilization) without accepting the population ethics laid out in (a). However, I feel that this would likely require an even greater degree of (b), methodological willingness to give based on speculation about the long-term future. For example, one might devote all of one\u2019s effort to minimizing the odds of a very large long-term future filled with suffering or attempting to otherwise improve humanity\u2019s long-term trajectory. From a practical perspective, I think that\u2019s a much narrower target to hit than reducing the odds of human extinction.)</p>\n<p>An alternative perspective, which I will term the \u201cnear-termist view,\u201d holds some appeal for me as well:</p>\n<ul>\n<li>I have substantial uncertainty about population ethics. My personal intuitions still lean against placing too-high moral weight on the outcome of \u201ca very large positive world, which would otherwise not have existed\u201d relative to the outcome of \u201ca moderate number of persons with high well-being, who otherwise would have had low well-being.\u201d</li>\n<li>I find it reasonable to be suspicious, from a heuristic perspective, of reaching fairly unusual and \u201cradical\u201d conclusions based on speculation about the long-term future, even when such speculation is discounted based on subjective probability estimates. One reason is that such speculation seems (or at least is widely believed) to have a very poor historical track record. (We have some ongoing analysis aiming to examine whether the track record is really as poor as seems to be widely believed.)</li>\n</ul>\n<p>A \u201cnear-termist\u201d view might call for assessing grants based on the amount of good done per dollar <em>that could be observable, in principle, within the next ~50 years;</em><sup class=\"footnote-ref\"><a href=\"#fn-QaGgPdaXAwPJc9hSp-9\" id=\"fnref-QaGgPdaXAwPJc9hSp-9\">[9]</a></sup> or perhaps might be willing to count benefits to future generations, but with a cap of something like 10-100x the number of persons alive today. I think either of these versions of \u201cnear-termism\u201d would reduce the consequences of the above two concerns, while having the obvious drawback of excluding important potential value from the assessment of grants.</p>\n<p>Similarly to the \u201canimal-inclusive\u201d vs. \u201chuman-centric\u201d split, the \u201clong-termist\u201d vs. \u201cnear-termist\u201d split is a crude simplification of many possible disagreements. However, in practice, I believe that most people either (a) accept the basic logic of the \u201clong-termist\u201d argument, or (b) reject its conclusions wholesale (often for ambiguous reasons that may be combining moral and methodological judgments in unclear ways) and could reasonably be classified as \u201cnear-termist\u201d according to something like the definitions above.</p>\n<p>The two views have radically different implications, and I think all four issues listed <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#Issue_1:_normative_uncertainty_and_philosophical_incommensurability\">previously</a> apply, in terms of reasons to consider something other than the default approach to allocation:</p>\n<ul>\n<li><strong>Issue 1: normative uncertainty and philosophical incommensurability</strong>. I think a key question for long-termism vs. near-termism is \u201cHow should we weigh the outcome of \u2018a very large positive world, which would otherwise not have existed\u2019 relative to the outcome of \u2018a moderate number of persons with high well-being, who otherwise would have had low well-being\u2019?\u201d Answering this question alone doesn\u2019t resolve whether one should be a long-termist or near-termist, but I think people who weigh the former highly compared to the latter are likely to see a much stronger case that there are many promising long-termist interventions. And like the question about how to weigh animal vs. human welfare, this question could be interpreted as a question of \u201cnormative uncertainty\u201d (uncertainty between two different views of morality), in which case it is not clear how to create a single \u201ccommon metric\u201d for adjudicating between two views. (See <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#Issue_1:_normative_uncertainty_and_philosophical_incommensurability\">above</a> for more detail on the challenges of a common metric. I think a similar analysis applies here.)</li>\n<li><strong>Issue 2: methodological uncertainty and practical incommensurability.</strong> As stated above, a major potential reason for taking the near-termist view is being \u201csuspicious, from a heuristic perspective, of reaching fairly unusual and \u2018radical\u2019 conclusions based on speculation about the long-term future.\u201d Yet the default approach to allocation essentially comes down to evaluating <em>this concern</em> via an expected-value calculation that is likely (in practice) to be dominated by a highly speculation-based figure (the amount of value to be gained via future generations), and embracing whatever radical implications result. It therefore seems like a question-begging and inappropriate methodology for handling such a concern.</li>\n<li><strong>Issue 3: practical considerations against \u201cputting all our eggs in one basket.\u201d</strong> We believe that if we took the default approach in this case, there\u2019s a strong chance that we would end up effectively going \u201call-in\u201d on something very similar to the long-termist view. I think this would likely result in our entire portfolio being comprised of causes where most of our impact is effectively unobservable and/or only applicable in extremely low-probability cases (e.g., <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks\">global catastrophic risk reduction</a> causes). I think this would be a problem for our ability to continually learn and improve, a problem for our ability to build an informative track record, and a problem on other fronts as discussed <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#Issue_3:_practical_considerations_against_putting_all_our_eggs_in_one_basket\">above</a>.</li>\n<li><strong>Issue 4: the \u201coutlier opportunities\u201d principle.</strong> I think the \u201coutlier opportunities\u201d principle described <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#Issue_4:_the_outlier_opportunities_principle\">above</a> is quite relevant in this case, even more so than in the \u201canimal-inclusive vs. human-centric\u201d case. Due to the very large amount of potential value attributable to the long-term future, a long-termist view is likely to de-prioritize even extremely outstanding opportunities to do near-term good. That isn\u2019t necessarily the wrong thing to do, but could certainly give us pause in many imaginable cases, as discussed above.</li>\n</ul>\n<p>For these reasons, I think there is some appeal to handling long-termism vs. near-termism using something other than the \u201cdefault approach\u201d (such as the simple approach mentioned <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#A_simple_alternative_to_the_default_approach\">above</a>).</p>\n<h3>Some additional notes on worldview choices</h3>\n<p>We consider each possible combination of stances on the above two choices to be a \u201cworldview\u201d potentially worthy of consideration. Specifically, we think it\u2019s worth giving serious consideration to each of: (a) the animal-inclusive long-termist worldview; (b) the animal-inclusive near-termist worldview; (c) the human-centric long-termist worldview; (d) the human-centric near-termist worldview.</p>\n<p>That said, I currently believe that (a) and (c) have sufficiently overlapping practical implications that they can likely be treated as almost the same: I believe that a single metric (impact on the odds of civilization reaching a highly enlightened, empowered, and robust state at some point in the future) serves both well.</p>\n<p>In addition, there may be other worldview choices that raise similar issues to the two listed above, and similarly call for something other than the \u201cdefault approach\u201d to allocating capital. For example, we have considered whether something along the lines of <a href=\"https://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">sequence vs. cluster thinking</a> might call for this treatment. At the moment, though, my best guess is that the main worldviews we are deciding between (and that raise the most serious issues with the default approach to allocation) are \u201clong-termist,\u201d \u201cnear-termist animal-inclusive,\u201d and \u201cnear-termist human-centric.\u201d</p>\n<h2>Some other criteria for capital allocation</h2>\n<p>Our default starting point for capital allocation is to do whatever maximizes \u201cgood accomplished per dollar\u201d according to some common unit of \u201cgood accomplished.\u201d The first complication to this approach is the set of \u201cworldview choices\u201d discussed above, which may call for dividing capital into \u201cbuckets\u201d using different criteria. This section discusses another complication: there are certain types of giving we\u2019d like to allocate capital to in order to realize certain practical and other benefits, even when they otherwise (considering only their direct effects) wouldn\u2019t be optimal from a \u201cgood accomplished per dollar\u201d perspective according to any of the worldviews discussed above.</p>\n<h3>Scientific research funding</h3>\n<p>We seek to have a strong scientific research funding program (focused for now on life sciences), which means:</p>\n<ul>\n<li>Retaining top scientists as advisors (both part-time and <a href=\"https://www.openphilanthropy.org/about/team\">full-time</a>).</li>\n<li>Accumulating significant experience making and evaluating grants for scientific research, which ought to include the ability to identify and evaluate <a href=\"https://blog.givewell.org/2015/04/14/breakthrough-fundamental-science/\">breakthrough fundamental science</a>.</li>\n<li>Having a strong sense of what is scientifically promising, particularly but not exclusively among science relevant to our <a href=\"https://www.openphilanthropy.org/focus\">focus areas</a>, at a given time.</li>\n</ul>\n<p>We think the benefits of such a program are cross-cutting, and not confined to any one of the worldviews from the previous section:</p>\n<ul>\n<li>When there is promising scientific research relevant to any of the worldviews named above, we will be in position to quickly and effectively identify it and support it.</li>\n<li>Being knowledgeable about scientific research funding will, in my view, greatly improve our long-term potential for building relationships with donors beyond the ones we currently work most closely with.</li>\n</ul>\n<p>These benefits are similar to those described in the <a href=\"https://www.openphilanthropy.org/blog/worldview-diversification#Capacity_building_and_option_value\">capacity building and option value section</a> of our previous post on worldview diversification.</p>\n<p>In order to realize these benefits, I believe that we ought to allocate a significant amount of funding to scientific research (my current estimate is around $50 million per year, based on conversations with scientific advisors), with a reasonable degree of diversity in our portfolio (i.e., not all on one topic) and a substantial component directed at <a href=\"https://blog.givewell.org/2015/04/14/breakthrough-fundamental-science/\">breakthrough fundamental science</a>. (If we lacked any of these, I believe we would have much more trouble attracting top advisors and grantees and/or building the kind of general organizational knowledge we seek.)</p>\n<p>Currently, we are supporting a significant amount of scientific research that is primarily aimed at reducing <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/biosecurity\">pandemic risk</a>, while also hopefully qualifying as top-notch, cutting-edge, generically impressive scientific advancement. We have also made a substantial <a href=\"https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare/impossible-foods\">investment in Impossible Foods</a> that is primarily aiming to improve animal welfare. However, because we seek a degree of diversity in the portfolio, we\u2019re also pursuing a number of other goals, which we will lay out at another time.</p>\n<h3>Policy-oriented philanthropy</h3>\n<p>We seek to have a strong policy-oriented philanthropy program, which means:</p>\n<ul>\n<li>Retaining top policy-oriented staff.</li>\n<li>Accumulating significant experience making and evaluating policy-oriented grants.</li>\n<li>Having a sense of what policy-oriented opportunities stand out a given time.</li>\n</ul>\n<p>We think the benefits of such a program mirror those discussed in the previous section, and are similarly cross-cutting.</p>\n<p>In order to realize these benefits, I believe that we ought to allocate a significant amount of funding to policy-oriented philanthropy, with some degree of diversity in our portfolio (i.e., not all on one topic). In some causes, it may take ~$20 million per year to be the kind of \u201cmajor player\u201d who can attract top talent as staff and grantees; for some other causes, we can do significant work on a smaller budget.</p>\n<p>At the moment, we have a substantial allocation to <a href=\"https://www.openphilanthropy.org/focus/us-policy/criminal-justice-reform\">criminal justice reform</a>. I believe this cause is currently very promising in terms of our practical goals. It has relatively near-term ambitions (<a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#Causes_with_reasonable-length_feedback_loops\">some discussion of why this is important below</a>), and Chloe (the Program Officer for this cause) has made notable progress on connecting with external donors (more on this in a future post). At this point, I am inclined to recommend continuing our current allocation to this work for at least the next several years, in order to give it a chance to have the sorts of impacts we\u2019re hoping for (and thus contribute to some of our goals around self-evaluation and learning).</p>\n<p>We have smaller allocations to a number of other policy-oriented causes, all of which we are reviewing and may either de-emphasize or increase our commitment to as we progress in our cause prioritization work.</p>\n<h3>Straightforward charity</h3>\n<p><a href=\"https://www.openphilanthropy.org/blog/good-ventures-and-giving-now-vs-later-2016-update#Some_considerations_regarding_conventional_vs._unconventional_giving\">Last year</a>, I wrote:</p>\n<p><em>I feel quite comfortable making big bets on unconventional work. But at this stage, given how uncertain I am about many key considerations, I would be uncomfortable if that were all we were doing \u2026 I generally believe in trying to be an ethical person by a wide variety of different ethical standards (not all of which are consequentialist). If I were giving away billions of dollars during my lifetime (the hypothetical I generally use to generate recommendations), I would feel that this goal would call for some significant giving to things on the more conventional side of the spectrum. \u201cSignificant\u201d need not mean \u201cexclusive\u201d or anything close to it. But I wouldn\u2019t feel that I was satisfying my desired level of personal morality if I were giving $0 (or a trivial amount) to known, outstanding opportunities to help the less fortunate, in order to save as much money as possible for more speculative projects relating to e.g. artificial intelligence.</em></p>\n<p>I still feel this way, and my views on the matter have solidified to some degree. I now would frame this issue as a desire to allocate a significant (though not majority) amount of capital to <strong>\u201cstraightforward charity\u201d:</strong> giving that is clearly and unambiguously driven by a desire to help the less fortunate in a serious, rational, reasonably optimized manner.</p>\n<p>Note that this would\u2019t necessarily happen simply due to having a \u201cnear-termist, human-centric\u201d allocation. The near-termist and human-centric worldviews are to some extent driven by a suspicion of particular methodologies as justifications for \u201cradicalism,\u201d but both could ultimately be quite consistent with highly unconventional, difficult-to-explain-and-understand giving (in fact, it\u2019s a distinct possibility that some <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks\">global catastrophic risk reduction work</a> could be justified solely by its impact according to the near-termist, human-centric worldview). It\u2019s possible that optimizing for the worldviews discussed above, by itself, would imply only trivial allocations to straightforward charity, and if so, I\u2019d want to ensure that we explicitly set aside some capital for straightforward charity.</p>\n<p>I still haven\u2019t come up with a highly satisfying articulation of why this allocation seems important, and what is lost if we don\u2019t make it. However:</p>\n<ul>\n<li>My intuition on this front is strongly shared by others who sit on the <a href=\"https://www.openphilanthropy.org/about/who-we-are#Howwework\">Board of Managers of the Open Philanthropy Project LLC</a> (Alexander, Elie, Cari and Dustin).</li>\n<li>I personally see some appeal in an explanation that revolves around \u201cclear, costly, credible signaling of the right values.\u201d I tend to trust people\u2019s values and motives more when I see them taking actions that have real costs and are clearly and unambiguously driven by a desire to help the less fortunate in a serious, rational, reasonably optimized manner. The fact that some alternate version of myself (less steeped in the topics I\u2019m currently steeped in, such as <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence\">potential risks of advanced AI</a>) would instinctively distrust me without the \u201cstraightforward charity\u201d allocation seems important - it seems to indicate that a \u201cstraightforward charity\u201d allocation could have both direct instrumental benefits for our ability to connect with the sorts of people we\u2019re hoping to connect with, and a vaguer benefit corresponding to the idea of \u201ctaking actions that make me the sort of person that people like me would rationally trust.\u201d</li>\n<li>Other Board of Managers members generally do not share the above reasoning, but they do agree with the overall intuition that a significant \u201cstraightforward charity\u201d allocation is important, and would likely agree to a broader statement along the lines of \u201cWithout a significant allocation to straightforward charity, I\u2019d be far more nervous about the possibility and consequences of self-deception, especially since many of our <a href=\"https://www.openphilanthropy.org/focus\">focus areas</a> are now quite ambitious.\u201d</li>\n</ul>\n<p>I think it\u2019s likely that we will recommend allocating some capital to a \u201cstraightforward charity\u201d bucket, which might be described as: \u201cAssess grants by how many people they help and how much, according to reasonably straightforward reasoning and estimates that do not involve highly exotic or speculative claims, or high risk of self-deception.\u201d (Note that this is not the same as prioritizing \u201chigh likelihood of success.\u201d) <a href=\"http://www.givewell.org/\">GiveWell</a> was largely created to do just this, and I see it as the current best source of grant recommendations for the \u201cstraightforward charity\u201d bucket.</p>\n<p>My interest in \u201cstraightforward charity\u201d is threshold-based. The things I\u2019m seeking to accomplish here can be fully accomplished as long as there is an allocation that feels \u201csignificant\u201d (which means something like \u201cdemonstrates a serious, and costly, commitment to this type of giving\u201d). Our current working figure is 10% of all available capital. Hence, if the rest of our process results in less than 10% of capital going to straightforward charity, we will likely recommend \u201ctopping up\u201d the straightforward charity allocation.</p>\n<p>In general, I feel that the ideal world would be full of people who focus the preponderance of their time, energy and resources on a relatively small number of bold, <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">hits-based</a> bets that go against established conventional wisdom and the status quo - while also aiming to \u201ccheck boxes\u201d for a number of other ethical desiderata, some of which ask for a (limited) degree of deference to established wisdom and the status quo. I\u2019ve written about this view before <a href=\"https://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/#Sec5\">here</a> and <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving#Hits-based_mentality_vs._arrogance\">here</a>. I also generally am in favor of people going \u201ceasy on themselves\u201d in the sense of doing things that make their lives considerably easier and more harmonious, even when these things have large costs according to their best-guess framework for estimating good accomplished (as long as the costs are, all together, reducing impact by &lt;50% or so). Consistent with these intuitions, I feel that a &lt;1% allocation to straightforward charity would be clearly too small, while a &gt;50% allocation would be clearly too large if we see non-straightforward giving opportunities that seem likely to do far more good. Something in the range of 10% seems reasonable.</p>\n<h3>Causes with reasonable-length feedback loops</h3>\n<p>As noted above, one risk of too much focus on long-termist causes would be that most of our impact is effectively unobservable and/or only applicable in extremely low-probability cases. This would create a problem for our ability to continually learn and improve, a problem for our ability to build an informative track record, and problems on other fronts as discussed <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#Issue_3:_practical_considerations_against_putting_all_our_eggs_in_one_basket\">above</a>.</p>\n<p>Ensuring that we do a significant amount of \u201cnear-termist\u201d work partially addresses this issue, but even when using \u201cnear-termist\u201d criteria, many appealing causes involve time horizons of 10+-years. I think there is a case for ensuring that some of our work involves shorter feedback loops than that.</p>\n<p>Currently, I am relatively happy with Open Philanthropy\u2019s prospects for doing a reasonable amount of work with short (by philanthropic standards) feedback loops. Our work on <a href=\"https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare\">farm animal welfare</a> and <a href=\"https://www.openphilanthropy.org/focus/us-policy/criminal-justice-reform\">criminal justice reform</a> already seems to have had some impact (<a href=\"https://www.openphilanthropy.org/blog/our-progress-2016-and-plans-2017#Criminal_Justice_Reform\">more</a>), and seems poised to have more (if all goes well) in the next few years. So I\u2019m not sure any special allocations for \u201cshorter-term feedback loops\u201d will be needed. But this is something we\u2019ll be keeping our eye on as our allocations evolve.</p>\n<h2>Allocating capital to buckets and causes</h2>\n<p>Above, we\u2019ve contrasted two approaches to capital allocation:</p>\n<ul>\n<li>The \u201cdefault approach\u201d: establish a common metric for all giving, estimate how all causes would likely perform on this metric, and prioritize this ones that perform best. As noted above, this approach seems prone to effectively going \u201call-in\u201d on one branch of the \u201canimal-inclusive vs. human-centric\u201d <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#Animal-inclusive_vs_human-centric_views\">choice</a>, as well as the \u201clong-termist vs. near-termist\u201d <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#Long-termist_vs._near-termist_views\">choice</a>.</li>\n<li>The \u201cdiversifying approach\u201d: divide capital into buckets, each of which follows different criteria. There could be a \u201clong-termist\u201d bucket, a \u201cnear-termist human-centric\u201d bucket, and a \u201cnear-termist animal-inclusive\u201d bucket, each of which is then allocated to causes based on the goals and assumptions of the corresponding worldview. (These could be further divided into buckets with more specific criteria, as discussed <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#MethodOutline\">below</a>.)</li>\n</ul>\n<p>The <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#A_simple_alternative_to_the_default_approach\">simplest version</a> of the \u201cdiversifying approach\u201d would be to divide capital equally between buckets. However, we think a better version of the diversifying approach would also take into account:</p>\n<p><strong>The credence/weight we place on different worldviews relative to each other.</strong> Simply put, if one thinks the long-termist worldview is significantly more plausible/appealing than the near-termist worldview, one should allocate more capital to the long-termist bucket (and vice versa). One way of approaching this is to allocate funding in proportion to something like \u201cthe probability that one would endorse this worldview as correct if one went through an extensive reflective process like the one described <a href=\"https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood#ExtremeEffort\">here</a>.\u201d This is of course a major and subjective judgment call, and we intend to handle it <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#MethodOutline\">accordingly</a>. We also think it\u2019s important to complete writeups that can help inform these judgments, such as a review of the literature on population ethics and an analysis of some possibilities for the number and size of future generations (relevant to the long-termist vs. near-termist choice), as well as <a href=\"https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood\">our already-completed report on moral patienthood</a> (relevant to the animal-inclusive vs. human-centric choice).</p>\n<p><strong>Differences in \u201ctotal value at stake.\u201d</strong> Imagine that one is allocating capital between Worldview A and Worldview B, and that one\u2019s credences in the two worldviews are 80% and 20%, respectively - but <em>if</em> worldview B is correct, its giving opportunities are 1000x as good as the best giving opportunities if worldview A is correct. In this case, there would be an argument for allocating more capital to the buckets corresponding to worldview B, even though worldview B has lower credence, because it has more \u201ctotal value at stake\u201d in some sense.<sup class=\"footnote-ref\"><a href=\"#fn-QaGgPdaXAwPJc9hSp-10\" id=\"fnref-QaGgPdaXAwPJc9hSp-10\">[10]</a></sup></p>\n<p>Put another way, one might want to increase the allocation to worldviews that would be effectively favored under the \u201cdefault\u201d (as opposed to \u201cdiversifying\u201d) approach. We expect to make some degree of compromise between these two approaches: worldviews favored by the default approach will likely receive more capital, but not to a degree as extreme as the default approach alone would imply.</p>\n<p><strong>Deals and fairness agreements.</strong> We suggest <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#A_simple_alternative_to_the_default_approach\">above</a> that the different worldviews might be thought of as different agents with fundamentally different and incommensurable goals, disagreeing about how to spend capital. This metaphor might suggest dividing capital evenly, or according to credence as stated immediately above. It also raises the possibility that such \u201cagents\u201d might make deals or agreements with each other for the sake of mutual benefit and/or fairness.</p>\n<p>For example, agents representing (respectively) the long-termist and near-termist worldviews might make a deal along the following lines: \u201cIf the risk of permanent civilizational collapse (including for reasons of extinction) in the next 100 years seems to go above X%, then long-termist buckets get more funding than was originally allocated; if the risk of permanent civilizational collapse in the next 100 years seems to go <em>below</em> Y%, near-termist buckets get more funding than was originally allocated.\u201d It is easy to imagine that there is some X and Y such that both parties would benefit, in expectation, from this deal, and would want to make it.</p>\n<p>We can further imagine deals that might be made behind a \u201cveil of ignorance\u201d (discussed <a href=\"https://www.openphilanthropy.org/blog/worldview-diversification#The_ethics_of_the_veil_of_ignorance\">previously</a>). That is, if we can think of some deal that might have been made while there was little information about e.g. which charitable causes would turn out to be <a href=\"https://www.openphilanthropy.org/focus\">important, neglected, and tractable</a>, then we might \u201cenforce\u201d that deal in setting the allocation. For example, take the hypothetical deal between the long-termist and near-termist worldviews discussed above. We might imagine that this deal had been struck before we knew anything about the major <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks\">global catastrophic risks</a> that exist, and we can now use the knowledge about global catastrophic risks that we have to \u201cenforce\u201d the deal - in other words, if risks are larger than might reasonably have been expected before we looked into the matter at all, then allocate more to long-termist buckets, and if they are smaller allocate more to near-termist buckets. This would amount to what we term a \u201cfairness agreement\u201d between agents representing the different worldviews: honoring a deal they would have made at some earlier/less knowledgeable point.</p>\n<p>Fairness agreements appeal to us as a way to allocate more capital to buckets that seem to have \u201cespecially good giving opportunities\u201d in some sense. It seems intuitive that the long-termist view should get a larger allocation if e.g. tractable opportunities to reduce global catastrophic risks seem in some sense \u201csurprisingly strong relative to what one would have expected,\u201d and smaller if they seem \u201csurprisingly weak\u201d (some elaboration on this idea is <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#GCRsFairness\">below</a>).</p>\n<p>Methods for coming up with fairness agreements could end up making use of a number of other ideas that have been proposed for making allocations between different agents and/or different incommensurable goods, such as allocating according to <a href=\"https://plato.stanford.edu/entries/contractarianism/#AnswMoraSkep\">minimax relative concession</a>; <a href=\"http://users.ox.ac.uk/~ball1714/Variance%20normalisation.pdf\">allocating in order to maximize variance-normalized value</a>; and allocating in a way that tries to account for (and balance out) the allocations of other philanthropists (for example, if we found two worldviews equally appealing but learned that 99% of the world\u2019s philanthropy was effectively using one of them, this would seem to be an argument - which could have a \u201cfairness agreement\u201d flavor - for allocating resources disproportionately to the more \u201cneglected\u201d view). The \u201ctotal value at stake\u201d idea mentioned above could also be implemented as a form of fairness agreement. We feel quite unsettled in our current take on how best to practically identify deals and \u201cfairness agreements\u201d; we could imagine putting quite a bit more work and discussion into this question.</p>\n<p><strong>Practical considerations.</strong> We are likely to recommend making some allocations for various practical purposes - in particular, creating buckets for scientific research funding, policy-oriented philanthropy, and straightforward charity, as discussed <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#Some_other_criteria_for_capital_allocation\">above</a>.</p>\n<p><strong>How will we incorporate all of these considerations?</strong> We\u2019ve considered more than one approach to allocation, and we haven\u2019t settled on a definite process yet. For now, a few notes on properties we expect our process to have:</p>\n<ul>\n<li>We will likely list a number of buckets we could allocate capital to, each with its own criteria for grants, and each with its own notes on what the best giving opportunities by these criteria will probably look like at different levels of funding. Our take on what each bucket\u2019s best giving opportunities look like will evolve considerably over time, and could become the focus of significant ongoing research. Some buckets might have quite broadly defined criteria (e.g. \u201cmaximize impact according to the long-termist worldview\u201d), while some buckets might have considerably more tightly defined criteria (e.g., \u201cfund the best possible <a href=\"https://www.openphilanthropy.org/blog/breakthrough-fundamental-science\">breakthrough fundamental science\u201d</a> or even \u201creduce pandemic risk as much as possible subject to supporting interventions with certain properties\u201d); we will likely experiment with different approaches and think about which ones seem most conducive to highlighting key disagreements.</li>\n<li>Interested staff members will consider both the giving opportunities that we\u2019re guessing correspond to each bucket, and more abstract considerations (e.g., how much credence they place in long-termism vs. near-termism) and write down their own preferred working bucket allocations and total giving trajectory over the next few years. We will likely discuss our differences, and make observations about what each others\u2019 working allocations imply for a number of practical and philosophical considerations, e.g., \u201cThis allocation implies very little in the way of redistribution via fairness agreements\u201d or \u201cThis allocation misses out on a lot of value according to Worldview X (including by increasing giving too quickly or too slowly).\u201d We\u2019ll try to create a summary of the most important disagreements different staff members have with each other.</li>\n<li>We may try to create something similar to what <a href=\"https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/cost-effectiveness-models\">GiveWell uses for its cost-effectiveness analysis</a>: a spreadsheet where different people can fill in their values for key parameters (such as relative credence in different worldviews, and which ones they think should benefit from various fairness agreements), with explanations and links to writeups with more detail and argumentation for each parameter, and basic analytics on the distribution of inputs (for example, what the median allocation is to each worldview, across all staff members).</li>\n<li>Ultimately, this exercise serves to inform funders about how to allocate their funds, and as such the funders of <a href=\"https://www.openphilanthropy.org/about/who-we-are\">Good Ventures</a> will make the final call about how they are allocating their capital. Their decision will be informed by summaries of the judgments and reasoning of staff.</li>\n</ul>\n<h2>Likely outputs</h2>\n<p>This section discusses some reasonably likely outputs from the above process. All of these could easily change dramatically, but the general outputs listed here seem likely enough to help give readers a picture of what assumptions we are tentatively allowing to affect our planning today.</p>\n<h3>Global catastrophic risks and other long-term-oriented causes</h3>\n<p>I see several reasons to expect that we will recommend a very large allocation to <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks\">global catastrophic risks</a> and other causes primarily aimed at raising the odds of good long-term outcomes for civilization:</p>\n<ul>\n<li>\n<p>My impression is that this work is likely to be very strong according to the goals/metrics of both the \u201clong-termist animal-inclusive\u201d and \u201clong-termist human-centric\u201d worldviews. In fact, I suspect that for practical purposes, it may be reasonable to treat these as a single hybrid worldview.</p>\n</li>\n<li>\n<p>As discussed above, I suspect that the \u201cdefault approach\u201d to allocation would result in a heavy allocation to long-termist causes. In particular, long-termism has \u201cthe most value at stake\u201d in some sense, as the potential value to be had via future generations is extremely large.<sup class=\"footnote-ref\"><a href=\"#fn-QaGgPdaXAwPJc9hSp-11\" id=\"fnref-QaGgPdaXAwPJc9hSp-11\">[11]</a></sup></p>\n</li>\n<li>\n<p>I also feel that long-termism is likely to perform quite well according to a variety of ways of implementing the \u201cfairness agreements\u201d framework. If our <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence\">views on transformative artificial intelligence</a> are correct, for example, that would seem to make this point in time something of an outlier, in terms of opportunities to positively affect likely long-term outcomes for civilization. (I think there are further, more general arguments that this point in time is particularly high-leverage.) Furthermore, I believe that <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks\">global catastrophic risk reduction</a> is quite neglected in the scheme of things. For both of these reasons, it seems that giving opportunities are in some sense \u201cbetter than one might have initially expected\u201d (more neglected, more tractable) for long-termism.</p>\n</li>\n</ul>\n<p>Put differently, despite my own current skepticism about the population ethics that seems most conducive to long-termism,</p>\n<ul>\n<li>It seems to me that there are strikingly strong (perhaps even historical outlier) opportunities to improve long-term outcomes for civilization.</li>\n<li>If (as seems likely enough) the corresponding population ethics is more reasonable than it currently seems to me, such improvement could have enormous stakes, far greater than the value at stake for any other worldview.</li>\n<li>This is true regardless of one\u2019s position on the moral weight of nonhuman animals.</li>\n<li>Such opportunities are largely neglected by other philanthropists.</li>\n</ul>\n<p>Given such a situation, it seems reasonable to me to devote a very large part of our resources to this sort of giving.</p>\n<p>I note that the case for long-termism has largely been brought to our attention via the <a href=\"https://en.wikipedia.org/wiki/Effective_altruism\">effective altruism</a> community, which has emphasized similar points in the past.<sup class=\"footnote-ref\"><a href=\"#fn-QaGgPdaXAwPJc9hSp-12\" id=\"fnref-QaGgPdaXAwPJc9hSp-12\">[12]</a></sup> I think the case for this sort of giving is initially unintuitive relative to e.g. focusing on global health, but I think it\u2019s quite strong, and that gives some illustration of the value of effective altruism itself as an intellectual framework and community.</p>\n<p>I think it is reasonably likely that we will recommend allocating &gt;50% of all available capital to giving directly aimed at improving the odds of favorable long-term outcomes for civilization. This could include:</p>\n<ul>\n<li>Expanding our current work on <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence\">potential risks from advanced AI</a>, <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/biosecurity\">biosecurity and pandemic preparedness</a> and other <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks\">global catastrophic risks</a>.</li>\n<li>Giving aimed more broadly at promoting international peace and cooperation (in particular, reducing the odds of war between major military powers), which could reduce a number of global catastrophic risks, including those we haven\u2019t specifically identified.</li>\n<li>Giving aimed at improving the process by which high-stakes decisions (particularly with respect to emerging technologies, such as synthetic biology and artificial intelligence) are made. This could include trying to help improve the functioning of governments and democratic processes generally.</li>\n<li>A variety of other causes, including supporting long-termist <a href=\"https://www.openphilanthropy.org/focus/other-areas#EffectiveAltruism\">effective altruist</a> efforts.</li>\n</ul>\n<h3>Policy-oriented philanthropy and scientific research funding</h3>\n<p>As indicated <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#Some_other_criteria_for_capital_allocation\">above</a>, we will likely want to ensure that we have substantial, and somewhat diversified, programs in <a href=\"https://www.openphilanthropy.org/focus/us-policy\">policy-oriented philanthropy</a> and <a href=\"https://www.openphilanthropy.org/focus/scientific-research\">scientific research funding</a>, for a variety of practical reasons. I expect that we will recommend allocating at least $50 million per year to policy-oriented causes, and at least $50 million per year to scientific-research-oriented causes, for at least the next 5 or so years.</p>\n<p>Many details remain to be worked out on this front. When possible, we\u2019d like to accomplish the goals of these allocations while also accomplishing the goals of other worldviews; for example, we have funded scientific research that we feel is among the best giving opportunities we\u2019ve found for <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/biosecurity\">biosecurity and pandemic preparedness</a>, while also making a major contribution to the goals we have for our scientific research program. However, there is also some work that will likely not be strictly optimal (considering only the direct effects) from the point of view of any of the worldviews listed in <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#Some_additional_notes_on_worldview_choices\">this section</a>. We choose these partly for reasons of inertia from previous decisions, preferences of specialist staff, etc. as well as an all-else-equal preference for <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#Causes_with_reasonable-length_feedback_loops\">reasonable-length feedback loops</a> (though we will always be taking <a href=\"https://www.openphilanthropy.org/focus\">importance, neglectedness, and tractability</a> strongly into account).</p>\n<h3>Straightforward charity</h3>\n<p>As discussed <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#Straightforward_charity\">above</a>, we will likely recommend allocating something like 10% of available capital to a \u201cstraightforward charity\u201d worldview, which in turn will likely correspond (for the near future) to following <a href=\"http://www.givewell.org/\">GiveWell</a> recommendations. The implications for this year\u2019s allocation to GiveWell\u2019s top charities are discussed <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#allocation_to_GiveWell_top_charities\">below</a>.</p>\n<h3>Other outputs</h3>\n<p>I expect to recommend a significant allocation to near-termist animal-inclusive causes, and I expect that this allocation would mostly go to <a href=\"https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare\">farm animal welfare</a> in the near to medium term.</p>\n<p>Beyond the above, I\u2019m quite unsure of how our allocation will end up.</p>\n<p>However, knowing the above points gives us a reasonable amount to work with in planning for now. It looks like we will maintain (at least for the next few years), but not necessarily significantly expand, our work on <a href=\"https://www.openphilanthropy.org/focus/us-policy/criminal-justice-reform\">criminal justice reform</a>, <a href=\"https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare\">farm animal welfare</a>, and <a href=\"https://www.openphilanthropy.org/focus/scientific-research\">scientific research</a>, while probably significantly expanding our work on <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks\">global catastrophic risk reduction</a> and related causes.</p>\n<h3>Smoothing and inertia</h3>\n<p>When working with cause-specific specialist staff, we\u2019ve found it very helpful to establish relatively stable year-to-year budgets. This helps them plan; it also means that we don\u2019t need to explicitly estimate the cost-effectiveness of every grant and compare it to our options in all other causes. The latter is relatively impractical when much of the knowledge about a grant lives with the specialist while much of the knowledge of other causes lives with others. In other words, rather than decide on each grant separately using information that would need to be integrated across multiple staff, we try to get an overall picture of how good the giving opportunities tend to be within a given focus area and then set a relatively stable budget, after which point we leave decisions about which grants to make mostly up to the specialist staff.</p>\n<p>We\u2019ve written before about a number of other benefits to <a href=\"https://www.openphilanthropy.org/blog/importance-committing-causes\">committing to causes</a>. In general, I believe that philanthropy (and even more so <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">hits-based</a> philanthropy) operates best on long time frames, and works best when the philanthropist can make use of relationships and knowledge built over the course of years.</p>\n<p>For these reasons, the ultimate output of our framework is likely to incorporate aspects of <a href=\"https://www.openphilanthropy.org/blog/good-ventures-and-giving-now-vs-later-2016-update#Conservatism\">conservatism</a>, such as:</p>\n<ul>\n<li>A tendency to stay in causes for which we have built knowledge and connections, hired staff, and started having impact, for at least long enough that we have the opportunity to have significant impact and perform a fair assessment of our work as a whole, as well as fulfill any implicit commitments we\u2019ve made to grantees and potential grantees. We will wind down areas for which we no longer see any strong justification, but when a cause performs well on many of our practical goals, it will often be easier to stay in the cause than to switch to causes that are slightly better in the abstract (but incur large switching costs for ourselves and for others).</li>\n<li>A tendency to avoid overly frequent changes in our allocations to causes. We might tend toward taking a rough guess at what causes seem most promising for a given set of goals, using this to determine likely cause allocations and the accompanying plans around staffing and budgeting, and then sticking with these plans for significant time, revisiting only periodically.</li>\n<li>A practice of \u201csmoothing\u201d major budgetary changes, especially when we are decreasing the budget to a given cause. When we determine that the optimal allocation to a cause would be $X/yr, and it\u2019s currently $Y/yr, we might move from $Y/yr to $X/yr gradually over the course of several years.</li>\n</ul>\n<h3>Funding aimed directly at better informing our allocation between buckets</h3>\n<p>Making informed and thoughtful decisions about capital allocation is very valuable to us, and we expect it to be an ongoing use of significant staff time over the coming years.</p>\n<p>We\u2019re also open to significant capital allocations aimed specifically at this goal (for example, funding research on the relative merits of the various worldviews and their implicit assumptions) if we see good opportunities to make them. Our best guess is that, by default, there will be a relatively small amount (in dollars) of such opportunities. It\u2019s also possible that we could put significant time into <a href=\"https://www.openphilanthropy.org/blog/new-report-early-field-growth\">helping support the growth of academic fields</a> relevant to this topic, which could lead to more giving opportunities along these lines; I\u2019m currently unsure about how worthwhile this would be, relative to other possible uses of the same organizational capacity.</p>\n<h3>2017 allocation to GiveWell top charities</h3>\n<p>For purposes of our 2017 year-end recommendation, we started from the assumption that 10% of total available capital will eventually go to a \u201cstraightforward charity\u201d bucket that is reasonably likely to line up fairly well with <a href=\"http://www.givewell.org/\">GiveWell\u2019s</a> work and recommendations. (Note that some capital from other buckets could go to GiveWell recommendations as well, but since the \u201cstraightforward charity\u201d bucket operates on a \u201cthreshold\u201d basis as described <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#Straightforward_charity\">above</a>, this would not change the allocation unless the total from other worldviews exceeded 10%; it is possible that this will end up happening, but we aren\u2019t currently planning around that.)</p>\n<p>We further split this 10% into two buckets of 5% each:</p>\n<ul>\n<li>One, the \u201cfixed percentage of total giving\u201d bucket, allocates 5%*(total Open Philanthropy giving) to straightforward charity each year. Because some of the goals of this worldview relate to signaling and to virtue-ethics-like intuitions, they can\u2019t all be met by having given to straightforward charity in the past; they are best met by giving significantly each year. The \u201cfixed percentage of total giving\u201d bucket seeks to ensure that the \u201cstraightforward charity\u201d allocation is at least 5% for each year that we are active.</li>\n<li>The other is the \u201cflexible\u201d bucket: 5% of all available capital, restricted to \u201cstraightforward charity\u201d but otherwise allocated in whatever way maximizes impact, which could include spending it all during a particularly high-impact time. For this bucket, we presented GiveWell with a range of possible 2017 allocations ranging from \u201caggressive\u201d (2017 giving that, if repeated, would spend down the whole \u201cflexible\u201d 5% allocation - net of investment returns - in 5 years) to \u201cconservative\u201d (2017 giving that, if repeated, would spend down the whole \u201cflexible\u201d 5% allocation - net of investment returns - in 50+ years).</li>\n<li>GiveWell settled close to the \u201caggressive\u201d end of the spectrum. It reasoned that the advantages of giving now (<a href=\"https://blog.givewell.org/2011/12/20/give-now-or-give-later/\">mostly listed here</a>) are larger than the expected financial returns, though it wanted to spend down slowly enough to preserve some option value in case it finds unexpectedly strong giving opportunities (something that seems most likely sometime in the next 10 years).</li>\n<li>We took the figure GiveWell had chosen and the figure we had estimated for the \u201cfixed percentage of total giving\u201d bucket, and added these to arrive at an estimated total budget for all GiveWell-related 2017 giving, including general operating support for GiveWell itself, and support for <a href=\"https://www.givewell.org/research/incubation-grants\">GiveWell Incubation Grants</a>. We then subtracted the amount that has already been allocated to the latter two to arrive at a tentative figure for 2017 giving to <a href=\"http://www.givewell.org/\">GiveWell\u2019s top charities</a>. Finally, we checked to make sure the change wasn\u2019t too big compared to last year\u2019s giving (in line with the \u201csmoothing\u201d idea discussed above) and rounded off to $75 million.</li>\n</ul>\n<p>The result of all this was a $75 million allocation to GiveWell\u2019s top charities for 2017. As GiveWell <a href=\"https://blog.givewell.org/2017/11/27/our-top-charities-for-giving-season-2017/\">stated</a>, \u201cthe amount was based on discussions about how to allocate funding across time and across cause areas. It was not set based on the total size of top charities\u2019 funding gaps or the projection of what others would give.\u201d</p>\n<h2>No more unified benchmark</h2>\n<p>A notable outcome of the framework we\u2019re working on is that we will <strong>no longer have a single \u201cbenchmark\u201d for giving now vs. later,</strong> as we <a href=\"https://blog.givewell.org/2015/11/25/good-ventures-and-giving-now-vs-later/#Benchmark\">did in the past</a>. Rather, grants will be compared to the \u201clast dollar\u201d spent <em>within the same bucket</em>. For example, we will generally make \u201clong-termist\u201d grants when they are better (by \u201clong-termist\u201d criteria) than the last \u201clong-termist\u201d dollar we\u2019d otherwise spend.</p>\n<p>We think this approach is a natural outcome of <a href=\"https://www.openphilanthropy.org/blog/worldview-diversification\">worldview diversification</a>, and will make it far more tractable to start estimating \u201clast dollar\u201d values and making our benchmarks more systematic. It is part of a move from (a) Open Philanthropy making decisions grant-by-grant to (b) Open Philanthropy\u2019s cross-cause staff recommending allocations at a high level, followed by its specialist staff deciding which grants to make within a given cause.</p>\n<h2>Our future plans for this work</h2>\n<p>This post has given a broad outline of the framework we are contemplating, and some reasonably likely outputs from this framework. But we have a lot of work left to do before we have a solid approach to cause prioritization.</p>\n<p>Over the coming year, we hope to:</p>\n<ul>\n<li>Complete some public content on key inputs into the case for and against particular worldviews we\u2019ve highlighted above. These will include writeups on population ethics and on possible properties (particularly size) of civilization in the future, both of which relate to the long-termist vs. near-termist choice.</li>\n<li>Complete an investigation into the literature on normative uncertainty and procedures for handling it appropriately, including the possibility of modeling different normative views as different agents with incommensurable goals who can make deals and \u201cfairness agreements.\u201d</li>\n<li>Further consider (though not settle) the question of which worldview choices we want to handle with something other than the \u201cdefault approach\u201d to capital allocation.</li>\n<li>Go through an iteration of the exercise sketched <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#Allocating_capital_to_buckets_and_causes\">above</a> and reach a working guess about our giving trajectory over the next few years, as well as how we will allocate capital to different buckets and how we will think about the threshold for making a given grant vs. saving the money for later over that time frame.</li>\n</ul>\n<p>This work has proven quite complex, and we expect that it could take many years to reach reasonably detailed and solid expectations about our long-term giving trajectory and allocations. However, this is arguably the most important choice we are making as a philanthropist - how much we want to allocate to each cause in order to best accomplish the many and varied values we find important. We believe it is much easier to increase the budget for a given cause than to decrease it, and thus, we think it is worth significant effort to come to the best answer we can before ramping up our giving to near-peak levels. This will hopefully mean that the main thing we are deciding on is not what parts of our work to cut (though there may be some of this), but rather which parts of our work are to grow the most.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-QaGgPdaXAwPJc9hSp-1\" class=\"footnote-item\"><p>Here \u201crelevant\u201d means \u201canimals that we see significant opportunities to cost-effectively help.\u201d <a href=\"#fnref-QaGgPdaXAwPJc9hSp-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-QaGgPdaXAwPJc9hSp-2\" class=\"footnote-item\"><p>See <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#AnimalVsHumanInPractice\">this footnote</a>. <a href=\"#fnref-QaGgPdaXAwPJc9hSp-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-QaGgPdaXAwPJc9hSp-3\" class=\"footnote-item\"><p>I\u2019ll note that I do <em>not</em> feel there is much of a place for a human-centric view based on (c) the empirical view that nonhuman animals are not \u201cconscious\u201d in a morally relevant way: I have become convinced that there is presently no evidence base that could reasonably justify high confidence in this view, and I think the \u201cdefault approach\u201d to budget allocation would be appropriate for handling one\u2019s uncertainty on this topic. <a href=\"#fnref-QaGgPdaXAwPJc9hSp-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-QaGgPdaXAwPJc9hSp-4\" class=\"footnote-item\"><p>Adjusted for the degree of improvement. <a href=\"#fnref-QaGgPdaXAwPJc9hSp-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-QaGgPdaXAwPJc9hSp-5\" class=\"footnote-item\"><p>Adjusted for the degree of improvement. <a href=\"#fnref-QaGgPdaXAwPJc9hSp-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-QaGgPdaXAwPJc9hSp-6\" class=\"footnote-item\"><p>Supporting this statement is outside the scope of this post. We\u2019ve <a href=\"https://www.openphilanthropy.org/blog/initial-grants-support-corporate-cage-free-reforms\">previously written about grants</a> that we estimate can spare over 200 hens from cage confinement for each dollar spent; contrasting this estimate with <a href=\"https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/cost-effectiveness-models\">GiveWell\u2019s cost-effectiveness figures</a> can give an idea of where we\u2019re coming from, but we plan to make more detailed comparisons in the future. <a href=\"#fnref-QaGgPdaXAwPJc9hSp-6\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-QaGgPdaXAwPJc9hSp-7\" class=\"footnote-item\"><p>As noted above, a &gt;10% probability on the animal-inclusive view would lead chickens to be valued &gt;0.1% as much as humans (using the \u201cmethod A\u201d approach I find most intuitive), which would likely imply a great deal of resources devoted to animal welfare relative to near-term human-focused causes. For now, we bracket the debate over a \u201clong-termist\u201d worldview that could make this distinction fairly moot, though we note it in a later section of this post. <a href=\"#fnref-QaGgPdaXAwPJc9hSp-7\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-QaGgPdaXAwPJc9hSp-8\" class=\"footnote-item\"><p>Other assumptions could have similar consequences to (a). (a) has the relevant consequences because it implies that improving the expected long-term trajectory of civilization - including reducing the risk of extinction - could do enormous amounts of good. Other population ethics frameworks could have similar consequences if they hold that the size of the intrinsic moral difference between \u201chumanity has an extremely good future (very large numbers of people, very excellent lives, filling a large fraction of available time and space)\u201d and \u201chumanity does not survive the coming centuries\u201d is sufficiently greater than the intrinsic moral importance of nearer-term (next century or so) events. <a href=\"#fnref-QaGgPdaXAwPJc9hSp-8\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-QaGgPdaXAwPJc9hSp-9\" class=\"footnote-item\"><p>And barring sufficiently radical societal transformations, i.e., one might exclude impact along the lines of \u201cThis intervention could affect a massive number of people if the population explodes massively beyond what is currently projected.\u201d <a href=\"#fnref-QaGgPdaXAwPJc9hSp-9\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-QaGgPdaXAwPJc9hSp-10\" class=\"footnote-item\"><p><a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy#Issue_1:_normative_uncertainty_and_philosophical_incommensurability\">Philosophical incommensurability</a> is a challenge for making this sort of determination. We think it\u2019s unclear where and to what extent philosophical incommensurability applies, which is one reason we are likely to end up with a solution between what the assumption of incommensurability applies and what the assumption of commensurabillity implies. <a href=\"#fnref-QaGgPdaXAwPJc9hSp-10\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-QaGgPdaXAwPJc9hSp-11\" class=\"footnote-item\"><p>See <a href=\"https://www.jesp.org/index.php/jesp/article/view/223\">Greaves and Ord 2017</a> for an argument along these lines. <a href=\"#fnref-QaGgPdaXAwPJc9hSp-11\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-QaGgPdaXAwPJc9hSp-12\" class=\"footnote-item\"><p>Examples <a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/Unprecedented-Technological-Risks.pdf\">here</a>, <a href=\"http://www.existential-risk.org/concept.html\">here</a>, <a href=\"https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/\">here</a>. <a href=\"#fnref-QaGgPdaXAwPJc9hSp-12\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "HoldenKarnofsky"}}, {"_id": "swnPKJMf3suurGawn", "title": "History of Philanthropy Case Study: The Center on Budget and Policy Priorities and State EITC Programs", "postedAt": "2018-02-02T16:25:56.097Z", "htmlBody": "<p>Suzanne Kahn, a consultant who has been working with us as part of our <a href=\"https://www.openphilanthropy.org/research/history-of-philanthropy\">History of Philanthropy project</a>, recently finished a case study on the role of the Center on Budget and Policy Priorities (CBPP) in state-level Earned Income Tax Credit (EITC) programs. This report is a follow-up to her <a href=\"https://www.openphilanthropy.org/blog/history-philanthropy-case-study-founding-center-budget-and-policy-priorities\">earlier report</a> on CBPP\u2019s founding and early growth, and investigates CBPP\u2019s claim that CBPP \u201ccreated the concept of state EITCs and\u2026 developed state issue campaigns to secure their adoption. Before we started this work, no state had its own EITC; today, 26 do.\u201d</p><p>The report finds that:</p><ul><li>CBPP and its network of state-level organizations have probably been the largest advocate for, and defender of, state EITC programs. It seems likely that CBPP is an important cause of some states\u2019 EITC programs, though causal attribution was more difficult in this case study than it has been in some previous studies.</li><li>CBPP wasn\u2019t involved in the very earliest state EITC programs. However, when the Center began working on state EITCs, no state in the country had a state EITC in effect that it had knowingly adopted. Moreover, the earliest state EITC programs (in Rhode Island and Wisconsin) were non-refundable, meaning that (unlike the federal EITC program) these programs didn\u2019t pay tax filers the amount of the credit they earned beyond the taxes they would\u2019ve otherwise paid, but instead only zeroed-out the filer\u2019s taxes. CBPP has tried to ensure that state programs offer refundable tax credits, and 6 of 7 states that have switched from a nonrefundable to a refundable credit had a CBPP state partner working in the state at the time.</li><li>In half of the 30 states that now have a state EITC, there were no state organizations affiliated with CBPP\u2019s efforts before the adoption of that state\u2019s EITC. However, in some of the states without a CBPP affiliate, the Center worked with others on creating a state EITC program.</li></ul><p>The report is also an interesting case study with respect to the question of <a href=\"https://www.openphilanthropy.org/blog/challenges-passive-funding\">\u201cactive\u201d vs. \u201cpassive\u201d funding</a>. For the most part, funders of CBPP\u2019s work on state EITC programs followed CBPP\u2019s lead with respect to strategy in the space. However, the Rockefeller Foundation played a somewhat more active role at one point by commissioning a 1990 evaluation of CBPP\u2019s work, which concluded that CBPP\u2019s state-level work was not comparable to its federal work, and recommended abandoning the state work or hiring someone to rethink the approach. In response, CBPP hired Iris Lav, who radically rethought CBPP\u2019s strategy for state policy work \u2014 including EITC programs \u2014 and secured funding for the new strategy from the Ford Foundation and others.</p><p><a href=\"https://www.openphilanthropy.org/files/History_of_Philanthropy/CBPP/EITC_Report_Kahn_FINAL_10.7.17.pdf\">Read the full case study here (.pdf)</a></p>", "user": {"username": "lukeprog"}}, {"_id": "qwZj8f7eq4HxmBzus", "title": "2017 LEAN Impact Assessment: Evaluation & Strategic Conclusions", "postedAt": "2018-02-28T07:14:36.604Z", "htmlBody": "<p>&nbsp;</p>\n<p><span><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995988/mirroredImages/qwZj8f7eq4HxmBzus/xbot3sfupz3cuitsfb7k.png\"></span></p>\n<p><span>This is the third post in the LEAN Impact Assessment series. </span></p>\n<p>&nbsp;</p>\n<p><span>Quantitative Findings</span></p>\n<p><span>Qualitative Findings</span></p>\n<p><span>Evaluation &amp; Strategic Conclusions</span></p>\n<p><span>Methodology</span></p>\n<p>&nbsp;</p>\n<p><span>In the previous </span><a title=\"Quantitative\" href=\"https://rtcharity.org/2017-lean-impact-assessment-quantitative-findings/\"><span>Quantitative</span></a><span> and </span><a title=\"Qualitative\" href=\"https://rtcharity.org/2017-lean-impact-assessment-qualitative-findings/\"><span>Qualitative</span></a><span> reports we aimed to describe our findings with minimal commentary in order to allow the data to \u2018speak for itself\u2019 and allow readers to draw their own impressions without being influenced by our own interpretations and conclusions. In this report we aim to offer more interpretation and analysis of the implications of our findings for LEAN strategy and for EA local groups. </span></p>\n<p>&nbsp;</p>\n<p><span>The LEAN Impact Assessment has aimed to provide insight both into the status and value of EA local groups as a whole, and the efficacy of efforts (by LEAN and others) to support these groups. These topics occupy the first two sections of this report, respectively, and in the final section we outline LEAN\u2019s strategic plans formed in the light of these findings.</span></p>\n<p>&nbsp;</p>\n<p><span>Our findings represent only a first step in researching EA Local Groups and we plan to conduct further, more specific, research into local groups and LEAN\u2019s services in the future.</span></p>\n<p>&nbsp;</p>\n<p><a href=\"https://docs.google.com/document/d/17FmUyeliFyvYs61D-DFQWTYC27TBcHS8lWbV2tlh0Qk/edit#heading=h.wapnv9j1pcs0\"><span>Executive Summary</span><span>&nbsp;&nbsp;&nbsp; </span></a></p>\n<p><a href=\"https://docs.google.com/document/d/17FmUyeliFyvYs61D-DFQWTYC27TBcHS8lWbV2tlh0Qk/edit#heading=h.arlf4hk6lngq\"><span>Group Size and Activity</span><span>&nbsp;&nbsp;&nbsp; </span></a></p>\n<p><a href=\"https://docs.google.com/document/d/17FmUyeliFyvYs61D-DFQWTYC27TBcHS8lWbV2tlh0Qk/edit#heading=h.9h8mnadnjj4\"><span>EA Survey Data</span><span>&nbsp;&nbsp;&nbsp; </span></a></p>\n<p><a href=\"https://docs.google.com/document/d/17FmUyeliFyvYs61D-DFQWTYC27TBcHS8lWbV2tlh0Qk/edit#heading=h.c7uqtjwnza6u\"><span>Age of Groups</span><span>&nbsp;&nbsp;&nbsp; </span></a></p>\n<p><a href=\"https://docs.google.com/document/d/17FmUyeliFyvYs61D-DFQWTYC27TBcHS8lWbV2tlh0Qk/edit#heading=h.xx7fcdqb5go7\"><span>Impact</span><span>&nbsp;&nbsp;&nbsp; </span></a></p>\n<p><a href=\"https://docs.google.com/document/d/17FmUyeliFyvYs61D-DFQWTYC27TBcHS8lWbV2tlh0Qk/edit#heading=h.8rgommwjq7ff\"><span>Is it all the largest groups?</span><span>&nbsp;&nbsp;&nbsp; </span></a></p>\n<p><a href=\"https://docs.google.com/document/d/17FmUyeliFyvYs61D-DFQWTYC27TBcHS8lWbV2tlh0Qk/edit#heading=h.gahb8gjah3jk\"><span>Further Evidence of Group Impact</span><span>&nbsp;&nbsp;&nbsp; </span></a></p>\n<p><a href=\"https://docs.google.com/document/d/17FmUyeliFyvYs61D-DFQWTYC27TBcHS8lWbV2tlh0Qk/edit#heading=h.mkan4yfx9mpr\"><span>Value of existing programme services and resources</span><span>&nbsp;&nbsp;&nbsp; </span></a></p>\n<p><a href=\"https://docs.google.com/document/d/17FmUyeliFyvYs61D-DFQWTYC27TBcHS8lWbV2tlh0Qk/edit#heading=h.d09ilxnwbkun\"><span>Alternative Analytic Strategies</span><span>&nbsp;&nbsp;&nbsp; </span></a></p>\n<p><a href=\"https://docs.google.com/document/d/17FmUyeliFyvYs61D-DFQWTYC27TBcHS8lWbV2tlh0Qk/edit#heading=h.xiboqkasglv6\"><span>Evaluations of Group Support and Services</span><span>&nbsp;&nbsp;&nbsp; </span></a></p>\n<p><a href=\"https://docs.google.com/document/d/17FmUyeliFyvYs61D-DFQWTYC27TBcHS8lWbV2tlh0Qk/edit#heading=h.scqamk2tikrv\"><span>Technical support</span><span>&nbsp;&nbsp;&nbsp; </span></a></p>\n<p><a href=\"https://docs.google.com/document/d/17FmUyeliFyvYs61D-DFQWTYC27TBcHS8lWbV2tlh0Qk/edit#heading=h.7ytnxz29nh1l\"><span>Personal Support and Expertise</span><span>&nbsp;&nbsp;&nbsp; </span></a></p>\n<p><a href=\"https://docs.google.com/document/d/17FmUyeliFyvYs61D-DFQWTYC27TBcHS8lWbV2tlh0Qk/edit#heading=h.ymnmembh7dw5\"><span>Group Communication</span><span>&nbsp;&nbsp;&nbsp; </span></a></p>\n<p><a href=\"https://docs.google.com/document/d/17FmUyeliFyvYs61D-DFQWTYC27TBcHS8lWbV2tlh0Qk/edit#heading=h.kjutusxs73kp\"><span>Group Calls</span><span>&nbsp;&nbsp;&nbsp; </span></a></p>\n<p><a href=\"https://docs.google.com/document/d/17FmUyeliFyvYs61D-DFQWTYC27TBcHS8lWbV2tlh0Qk/edit#heading=h.f6zgr6axcmes\"><span>EA Group Newsletter</span><span>&nbsp;&nbsp;&nbsp; </span></a></p>\n<p><a href=\"https://docs.google.com/document/d/17FmUyeliFyvYs61D-DFQWTYC27TBcHS8lWbV2tlh0Qk/edit#heading=h.sjx14s8o44px\"><span>EA Mentoring Programme</span><span>&nbsp;&nbsp;&nbsp; </span></a></p>\n<p><a href=\"https://docs.google.com/document/d/17FmUyeliFyvYs61D-DFQWTYC27TBcHS8lWbV2tlh0Qk/edit#heading=h.ew80pvqzqb9r\"><span>Strategic Summary</span><span>&nbsp;&nbsp;&nbsp; </span></a></p>\n<p><a href=\"https://docs.google.com/document/d/17FmUyeliFyvYs61D-DFQWTYC27TBcHS8lWbV2tlh0Qk/edit#heading=h.ye0dnlo6cw6f\"><span>Acknowledgments</span><span>&nbsp;&nbsp;&nbsp; </span></a></p>\n<h2><span>Executive Summary</span></h2>\n<ul>\n<li>\n<p><span>EA groups report producing significant impact e.g. counterfactual pledges and donations influenced</span></p>\n</li>\n<li>\n<p><span>Most members report that groups are a large or very large factor in their engagement with EA</span></p>\n</li>\n<li>\n<p><span>Organisers express a strong demand for personal support and feedback, written guides and research on group impact</span></p>\n</li>\n</ul>\n<h2><span>Group Size and Activity</span></h2>\n<p>&nbsp;</p>\n<p><span>The LEAN Impact Assessment offers the most comprehensive empirical investigation into EA local groups to date. Prior to the LEAN Impact Assessment there was little, if any, systematic data about EA local groups as a whole, though minimal related data was gathered through the EA Survey (also run by Rethink Charity). As such, the first stage was to provide basic information about the number and size of EA groups and their activities.</span></p>\n<p>&nbsp;</p>\n<p><span>Our sample (disseminated with help from CEA and EAF) included 98 discrete local groups and a larger number of organisers and members. This sample likely does not include all EA groups. Nor did respondents answer every question. However, it seems reasonable to assume that non-respondents were, on the less, disproportionately less active groups or groups who had less impact to report for particular metrics.</span></p>\n<p>&nbsp;</p>\n<p><span>Reported group size ranged from 1 to 1350, with a median of 10. This means that the reported group size numbers are heavily dominated by a small number of very large groups. Of a total 4280 reported group members, across all groups, more than half the reported members come from the largest 3 groups, almost 70% from the largest 5 groups and almost 78% from the largest 10 groups. By contrast, more than 50% of groups contained 10 members or fewer and slightly more than 76% reported 20 members or fewer. </span></p>\n<p>&nbsp;</p>\n<p><span>As noted previously, the </span><span>extent</span><span> to which a small number of very large groups account for almost all EA local group members is likely somewhat overstated due to differences in how group \u201cmembers\u201d were counted by different organisers. For example, the number of group members reported by the largest group in our sample corresponds to the number of group members they have in their Facebook group, but their reported number of people who have attended multiple of their events is much lower (&gt;1300 and 160, respectively). While it is possible that smaller groups may also be \u2018over-reporting\u2019 their numbers in this way, it does not seem possible that the smaller groups could be inflating their numbers to the same extent (unless we suppose that the majority of groups have 1 member or less).</span></p>\n<p>&nbsp;</p>\n<p><span>Accounting for this, the gap between the typical group and the very largest groups is less astronomical, though still substantial. Of course, the number of group members is not </span><span>in itself</span><span> important, but rather the </span><span>impact</span><span> that each group can have, which follows later in the report.</span></p>\n<h3><span>EA Survey Data</span></h3>\n<p>&nbsp;</p>\n<p><span>It is worth briefly comparing this data with the results of the EA Survey on local group membership, to see whether they are plausible, and to gain insight from a distinct source on the scale and scope of EA local groups. We would not expect these numbers to match up particularly closely, given the different questions asked and very different sampling techniques (the EA Survey was distributed widely and answered by EAs and non-EAs alike and had several thousand responses), the Local Groups Survey was distributed directly to group organisers (as well as posted in relevant facebook groups) and was specifically for local group organisers and members. Nevertheless it seems worthwhile to compare the results across these surveys to see whether EAs in a broader sample report being influenced by EA groups, rather than in a sample specifically targeting group organisers and members.</span></p>\n<p>&nbsp;</p>\n<p><span>The EA Survey found 469 EA respondents indicating that \u201cYes\u201d they were a member of a local group, compared to 953 answering \u201cNo\u201d and 430 non-responses for that question. This means slightly more than 25% of EA respondents (including those who did not answer that question at all in order to give a more conservative estimate) reported being in an EA group. </span></p>\n<p><span>We cannot assume this proportion applies across the whole EA population though, since it seems plausible that those EAs who answer the survey may be more likely to be in an EA group (assuming they are generally more active and connected). On the other hand, the absolute number of EAs in local groups is likely to be higher than the absolute count reported here, since the EA Survey doubtless did not include </span><span>all</span><span> EAs. Regrettably, no-one knows precisely how many EAs (by any definition) there actually are in the EA population (or </span><a href=\"/ea/1ef/is_ea_growing_some_ea_growth_metrics_for_2017/\"><span>whether or how far EA is growing</span></a><span>). Nevertheless, almost 500 self-reported EA local group members from the EA Survey sample of ~1800 seems an appreciable number. </span></p>\n<p>&nbsp;</p>\n<p><span>The EA Survey also provided data on whether individuals reported that they would attend an EA group if there were one near their home. In addition to the 469 EAs who said they do attend an EA local group, a further 271 reported that they would if there was one near their home and a further 310 who do not attend suggested that they were \u201cunsure\u201d whether they would attend or not. This is suggestive of fairly significant demand for EA local groups from individuals who presently cannot attend one due to lacking one nearby.</span></p>\n<p>&nbsp;</p>\n<p><span>The <a href=\"/ea/1h5/ea_survey_2017_series_how_do_people_get_into_ea/\">EA Survey</a> also offers data regarding the importance of EA local groups in getting EAs into and more involved in EA. 3.6% of the sample reported first hearing about EA from a local group, slightly exceeding Doing Good Better (3.4%) and Facebook (2.8%). Due to the wide variety of different answers indicated in the survey few \u2018routes\u2019 received very high percentages (the highest were Personal Contact and LessWrong on 15.5% and 15.3% respectively, followed by \u2018Other blog post\u2019 and SlateStarCodex on 9.4% and 7.4%).</span></p>\n<p><span><br></span><span>A <a href=\"/ea/1h5/ea_survey_2017_series_how_do_people_get_into_ea/\">survey sample</a> drawn specifically from the EA Facebook group (using different methodology) returned higher numbers reporting that they first heard about EA from an EA local group. Here 7% reported hearing about EA first from a local group, beaten only by Friends (15%) Peter Singer/TLYCS (13%) and 80,000 Hours (13%). Including the 5% who reported first hearing about EA from a </span><span>university</span><span> group, the total % hearing about EA from groups was 12% (much closer to the top options) and comfortably beating the other options, including Facebook, SlateStarCodex, LessWrong and Doing Good Better.</span></p>\n<p>&nbsp;</p>\n<p><span>These findings are suggestive of EA local groups playing a significant role as the first place that many EAs encounter EA, albeit as one among a wide variety of different sources each making up a minority of the whole.</span></p>\n<p>&nbsp;</p>\n<p><span>The EA Survey also offers data regarding how many EAs thought that EA groups were important in getting them into or more involved with EA. Here respondents could select multiple options as each being important. 261 respondents indicated that local groups were important for getting them more involved in EA, whereas the most commonly cited factors GiveWell and \u2018Book or blog\u2019 were cited 532 and 519 times. 261 EAs in the sample getting more involved in EA seems like a significant source of potential impact. It is significantly smaller than the number citing other factors, but this is presumably somewhat due to the fact that almost every EA has probably encountered GiveWell or an EA book or blog, whereas likely fewer than 50% of EAs have encountered a local group.</span></p>\n<p>&nbsp;</p>\n<p><span>Overall, we take this findings from the EA Survey to offer reassurance that EA groups are reaching and influencing a substantial number of EAs, even in a broader sample not specifically targeting group organisers.</span></p>\n<h3><span>Age of Groups</span></h3>\n<p>&nbsp;</p>\n<p><span>Readers may be concerned by the fact that so many individual groups appear to be so small (for example, 9/85 contain fewer than 4 members). Likewise, looking at various metrics of impact, it is reliably the case that a number of groups report little impact. If many EA groups are largely quiescent or inefficacious, then this may be a matter for concern.</span></p>\n<p>&nbsp;</p>\n<p><span>To try to understand this better, we looked at how long groups had been running. A large number of EA groups reported being very new.</span><span>&nbsp;&nbsp;&nbsp; </span></p>\n<p>&nbsp;</p>\n<p><span><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995988/mirroredImages/qwZj8f7eq4HxmBzus/lmuw061zvp5zexyubj9s.png\"></span></p>\n<p>&nbsp;</p>\n<p><span>This may be reassuring context when considering why some groups have not had much of an impact. They may not have had much impact </span><span>yet</span><span>, but many groups have not even seen a complete academic year or two seasonal pledge drives. This may be a neglected feature to consider in EA movement building discussions: a large number of EA groups (see the left hand side of the chart above) are still in their infancy and in the coming years may be coming into their own. </span></p>\n<p>&nbsp;</p>\n<p><span>If we look at the number of groups of certain sizes at different ages, we see that while a majority (68%) of groups founded less than a year ago have fewer than 10 members, this proportion declines substantially for older groups (38% for groups 1-2 years old, 33% for groups 2-3 years old, and 14% for groups 3-4 years old). </span></p>\n<p>&nbsp;</p>\n<p><span>Similarly, the median group size increases with group age (we have excluded the 5-7 year old groups from this graph because with only 2 members in these categories, the median is fairly unindicative):</span></p>\n<p><span><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995988/mirroredImages/qwZj8f7eq4HxmBzus/km0kyrmxwaayaoiyjnlw.png\"></span></p>\n<p>&nbsp;</p>\n<p><span>A consequence of this is that even though most groups are younger groups (58% less than 2 years old, &gt;80% less than 3 years old), most members are in </span><span>older</span><span> groups. </span></p>\n<p>&nbsp;</p>\n<p><span><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995988/mirroredImages/qwZj8f7eq4HxmBzus/kntdzub00qxlgnsfwv2k.png\"></span></p>\n<p>&nbsp;</p>\n<p><span>Another possibility we considered is that EA group size and activity might be quite variable across time, with groups, for example, being moderately active with a number of members, then falling away when a core organiser leaves or in between academic years, and then gaining more members once an academic year has started. If so, then while at any one time (such as the snapshot our survey provides), a number of groups may appear to be all but non-existent, the same groups may range across time through periods of activity and inactivity. Our qualitative data also included city-based group organisers reporting that they had high turnover of young professional members who would stay in a city only a short period of time before moving. This is partly suggested by our data on the perceived \u2018precarity\u2019 of groups, with a significant minority (30/89) of organisers thinking their group was unlikely or very unlikely to continue to function after the current organisers left. This is a trope which we heard from a number of organisers, where groups</span><span> actually</span><span> fell into abeyance after previously being very active. We presently lack data on how common this is, but this, and how to handle transitions better, as well as how to keep groups could be kept active, is a potential topic for future research.</span></p>\n<h2><span>Impact</span></h2>\n<p>&nbsp;</p>\n<p><span>Calculating the impact of local groups is exceedingly difficult in a number of ways. EA groups aim to have an impact in a variety of different ways: attracting new members to EA, encouraging members to become more engaged with EA (through providing social connection, motivation and information) and (directly and indirectly) encouraging members to take more effective actions (such as donating to effective charities, taking the pledge, considering their career based on EA principles), and increasing member retention.</span></p>\n<p>&nbsp;</p>\n<p><span>Some of these things are intrinsically difficult to measure and quantify. For example, many EAs report that being in a local group increases their motivation and engagement with EA. Some impacts which we think EA groups are likely to have, are all but impossible to measure. It seems plausible that EA groups contribute to retention of EAs, as some EAs report that they likely would have left the movement but for their involvement in a local group: but EAs who </span><span>do</span><span> leave the movement and their reasons for leaving are typically inaccessible to EA data-gathering, as, <em>a fortiori</em>, are people who never join the movement but would have, counterfactually, had they encountered a local group.</span><span> As such, much of the true impact of local groups may be excluded from our analysis.</span></p>\n<p>&nbsp;</p>\n<p><span>Further, it is exceedingly difficult to discern the </span><span>counterfactuality</span><span> of each of these outputs. While we can ask members and organisers for their self-reported estimations of what they would have done were they not in a group, it will often be hard for them to genuinely know. Many report that they would not have taken certain actions were they </span><span>not</span><span> in a local group, but counterfactually, it is possible that they would have been moved by something else to, for example, take the pledge had it been impossible for them to join a group. Conversely, an individual in a group may feel confident that they would have continued to take effective actions if they weren\u2019t involved in a local group, but it is possible that they may be mistaken about this. This is particularly difficult given the wide variety of influences on any given EA\u2019s actions: their interactions with EAs face to face in a local group, their contact with other EAs online, their reading EA literature, their attendance at an EA conference, may all influence their EA actions and attitudes and which of these factors are important may vary substantially across different individuals and the influence of all of these factors may not be transparent even to the individual in question. </span></p>\n<p>&nbsp;</p>\n<p><span>Likewise, EAs may reasonably disagree quite radically about how much value to attach to each of these things (for example, the value of a given person taking the Giving What We Can pledge) or getting an additional person involved in the EA movement. We do not aim to settle such fundamental debates here, but rather report figures for a variety of metrics which will be considered of value to a wide range of EAs with different values. </span></p>\n<p>&nbsp;</p>\n<p><span>Below we report figures for the collective \u2018outputs\u2019, for various measures, across all the groups in our sample:</span></p>\n<p>&nbsp;</p>\n<div>\n<table><colgroup><col><col></colgroup>\n<tbody>\n<tr>\n<td>\n<p><span>Total members: </span></p>\n</td>\n<td>\n<p><span>4280</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Total events, meetups, etc.:</span></p>\n</td>\n<td>\n<p><span>1835</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Total new event attendees (unfamiliar with EA):</span></p>\n</td>\n<td>\n<p><span> 5994</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Total </span><span>counterfactual </span><span>active commitments to EA: </span></p>\n</td>\n<td>\n<p><span>617</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Total </span><span>counterfactual </span><span>pledges: </span></p>\n</td>\n<td>\n<p><span>227.5</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Total EA informed career decisions: </span></p>\n</td>\n<td>\n<p><span>517.5</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Total funds raised (group fundraisers):</span></p>\n</td>\n<td>\n<p><span>$51,495.36</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Total </span><span>counterfactual </span><span>private donations influenced:</span></p>\n</td>\n<td>\n<p><span>$704,390.92</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>&nbsp;</p>\n<p><span>Clearly these measures cannot capture </span><span>all</span><span> the impact of EA groups, since many potential effects of EA groups were not or could not be measured by the survey e.g. less tangible benefits resulting from reducing EA attrition, value drift or of connecting EA group organisers to valuable opportunities. </span></p>\n<p>&nbsp;</p>\n<p><span>Even looking solely at the estimated (by group organisers) </span><span>counterfactual</span><span> impact of local EA groups, these figures seem quite substantial. As, at present, most EA groups are run by unpaid volunteers with little or no funding, the direct costs associated with local groups are quite low. The costs associated with volunteer or paid staff time, are likely to be larger, but here there will be higher variance. For many organisers the opportunity costs of their time spent running an EA group may be low and there may even be net personal gains from their involvement in terms of experience, career capital, boosts to motivation and connection with others in EA. </span><span>Of course, for other EAs, with higher impact elsewhere, running a group would be a net loss. We expect that, for the most part, individual organisers may be best placed to make these judgements themselves. We do, however, think that further research into the activities and opportunity costs of EA group organisers would be valuable. </span></p>\n<p>&nbsp;</p>\n<p><span>Having looked at the </span><span>total</span><span> outputs across all EA local groups it is, of course, important to try to look at this more at an individual group level. The </span><span>mean</span><span> impact per group continues to appear to be quite high, but these numbers are driven upwards by a small number of very successful groups. </span></p>\n<p>&nbsp;</p>\n<p><span>Therefore we present the </span><span>median </span><span>figures across a variety of metrics:</span></p>\n<p>&nbsp;</p>\n<div>\n<table><colgroup><col><col></colgroup>\n<tbody>\n<tr>\n<td>\n<p><span>Median number of members: </span></p>\n</td>\n<td>\n<p><span>10</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Median new event attendees (unfamiliar with EA):</span></p>\n</td>\n<td>\n<p><span> 30</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Median </span><span>counterfactual </span><span>active commitments to EA: </span></p>\n</td>\n<td>\n<p><span>5</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Median </span><span>counterfactual </span><span>pledges: </span></p>\n</td>\n<td>\n<p><span>3</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Median EA informed career decisions: </span></p>\n</td>\n<td>\n<p><span>4</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Median funds raised collectively (group fundraising):</span></p>\n</td>\n<td>\n<p><span>$232.11</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Median </span><span>counterfactual </span><span>private donations influenced: </span></p>\n</td>\n<td>\n<p><span>$1386.4</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>&nbsp;</p>\n<p><span>While these figures are much lower than the figures reported by the most successful groups on each metric or the mean figures, they still seem plausibly to represent significant impact. As noted, it is impossible to attach a definitive value to these various outputs, but 3 </span><span>counterfactual</span><span> GWWC pledges, and 5 people becoming </span><span>counterfactually </span><span>actively committed to EA may represent significant impact. </span></p>\n<p>&nbsp;</p>\n<p><span>Of course, looking at the median figures across different metrics only captures one facet of the distribution of impact across different individual groups. If one wants to get a picture of how much impact a \u2018typical\u2019 active EA group\u2019s impact, one might think that the median figures under-represent this, given that it includes a number of groups who are very new or quiescent (having one organiser, but no group). For example, if you want to know how much groups typically raise through group fundraisers, you might want to exclude from these figures the many groups which didn\u2019t even attempt to run a fundraiser. By contrast, if you want to estimate the expected value of starting a group, this might be a more appropriate metric; or you might want to identify a more specific reference class by looking at groups in comparable situations (e.g. in cities which you judge to be similar) and draw inferences from that. We are wary about digging into these different ways of representing the data too much, because it introduces too much freedom for judicious selection flattering results, and we recommend that the reader looks at the specific results we report in previous sections. </span></p>\n<p>&nbsp;</p>\n<h3><span>Is it all the largest groups?</span></h3>\n<p>&nbsp;</p>\n<p><span>One of the trends that appeared most clearly in the results we posted in the quantitative data report, was how far the total figures (e.g. for members or money raised) appeared to be dominated by a small number of \u2018super groups.\u2019 Much of our data exhibited a very strong power law distribution.</span></p>\n<p>&nbsp;</p>\n<p><span><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995988/mirroredImages/qwZj8f7eq4HxmBzus/ljhevocajjpap537mv4m.png\"></span></p>\n<p>&nbsp;</p>\n<p><span><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995988/mirroredImages/qwZj8f7eq4HxmBzus/udiwses6775lsu4f2ixn.png\"></span></p>\n<p>&nbsp;</p>\n<p><span>We think this is likely to lead to the impression that since it seems almost all the impact from local groups, as a whole, is coming from a small number of \u2018top\u2019 groups, this is where attention and investment should be focused. This is worth considering further.</span></p>\n<p>&nbsp;</p>\n<p><span>Firstly, it should be noted that </span><span>strategically </span><span>this doesn\u2019t necessarily follow. If we can, at very low cost (for example, time valued at $500), cause a smaller group to bring about 3 counterfactual pledges (valued at, say, 3 x $10,000 each), this may represent a better investment than trying to help a large group producing 100 pledges produce yet more pledges, as even though their </span><span>mean</span><span> impact may great, producing extra </span><span>marginal</span><span> impact may be very costly. Even if we suppose that the number of members in a group is all that matters (for producing impact), it may be that it is most effective to focus investment in the smaller groups, in order to increase their size to that of the larger groups. This may hold true for other metrics as well. Suppose we are interested in recruiting top EA talent. It may be more likely that talented individuals will come from </span><span>larger </span><span>groups (since there are more members), but it may be that such individuals would likely be found anyway (due to already being surrounded by and connected with many EAs in an EA Hub). It might therefore be more important to support smaller local groups in connecting EAs with promising opportunities, as they are more likely to be missed. None of this is intended as an argument that smaller groups actually </span><span>are</span><span> more important to invest in, merely that it does not automatically follow that if certain groups produce most impact, it is more important to attend to and invest in these groups relative to the more numerous smaller groups. </span></p>\n<p>&nbsp;</p>\n<p><span>However, there is also more to be said in terms of understanding the data and which groups are producing most impact. Given the striking distributions of group size and various measures of impact describes above, it would be tempting to conclude that it is the largest groups (with the most members) that are producing most of the impact. Further, one might suspect that it is largely simply having more members that drives increases success on various metrics (more counterfactual pledges, more donations influenced and so on). </span></p>\n<p>&nbsp;</p>\n<p><span>Further consideration of the data suggests that this is only somewhat true. As noted in the quantitative report, for most measures there was a positive correlation between number of group members and various outputs, however this varied. For example, there was a strong correlation between numbers of group members basing their career on EA principles, but little relationship between the size of a group and the number of new attendees to events who were unfamiliar with EA.</span></p>\n<p>&nbsp;</p>\n<p><span><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995988/mirroredImages/qwZj8f7eq4HxmBzus/uyvlybyyvlsgmldy59vf.png\"></span></p>\n<p>&nbsp;</p>\n<p><span><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995988/mirroredImages/qwZj8f7eq4HxmBzus/qxagleuwuidrdlrxuelr.png\"></span></p>\n<p>&nbsp;</p>\n<p><span>Moreover, as we noted in the quantitative report, the data did not suggest that larger groups were </span><span>better </span><span>at producing outputs from their members (e.g. getting members to make career choices based on EA) - in fact, there appeared to be a weak negative relationship between the size of a group and the proportion of members who were basing career choices on EA or taking the pledge (plausibly explained by the fact that larger groups may contain relatively more new members and smaller groups may contain, as a proportion of their total size, relatively more core organisers).</span></p>\n<p>&nbsp;</p>\n<p><span>It is also important to note that these correlations and various positive outputs do not necessarily suggest that having more group members </span><span>causes</span><span> higher impact in terms of pledges taken, funds raised etc. It may be that a third factor (group activity, propitious environment, good organisation) reliably causes groups to be larger and to have more pledges, funds raised etc.</span></p>\n<p>&nbsp;</p>\n<p><span>To gain more insight into these questions we looked at the highest performing groups across different metrics (e.g. number of members, amount raised in group fundraisers, number of members becoming actively committed to EA, number of pledges, number of career changes based on EA principles, and number of new attendees at events who were unfamiliar with EA). Just because each metric seemed to be dominated by a small number of groups performing exceedingly well on that metric, it didn\u2019t necessarily follow that it was the same groups dominating across each metric: it might be that the groups accounting for almost all the total group members were different from the groups accounting for almost all the pledges, for example.</span></p>\n<p>&nbsp;</p>\n<p><span>Our analysis suggested that there was quite a lot of overlap between the groups dominating across different metrics. We don\u2019t name specific groups because we wish to avoid publicly ranking groups in terms of success. Of the 5 groups with the highest member count, 4 recurred multiple times (3-4) over the other 4 categories, with the largest group topping the categories for active commitments, pledges and career changes. Notably, the other group in the top 5 for size did not recur across any of the other categories. </span></p>\n<p>&nbsp;</p>\n<p><span>However there was scope for divergence from this pattern of dominance by the \u2018top\u2019 groups. Across the \u2018top 5\u2019 for the other 5 categories, each time at least 2 of the top 5 didn\u2019t appear in the top 5 for </span><span>any </span><span>other category (e.g. at least two of the groups reporting the most pledges did not report the most members, most funds raised, most EA career choices or most new attendees). Moreover, one local group appeared in the top 5 for all but one of the remaining categories (funds raised, active commitments, pledges and new event attendees) despite not being one of the groups with the most members (indeed, they only have 35 members, placing them outside the top 10 for size). </span></p>\n<p>&nbsp;</p>\n<p><span>Two of the categories examined were also striking outliers. The top \u2018group fundraisers\u2019 did not include the \u2018top\u2019 group across most categories and 3 of the 5 places were taken by groups which were not among the largest groups. As this graph shows, relatively little of the variance here was explained by group size:</span></p>\n<p><span><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995988/mirroredImages/qwZj8f7eq4HxmBzus/uqkwerfg9s6mivto0yuv.png\"></span></p>\n<p>&nbsp;</p>\n<p><span>The number of new event attendees, for a given group, who were not familiar with EA beforehand was even more striking. Here </span><span>none</span><span> of the largest groups were among the top groups (though the largest group, tied for 5th place) and only 1 of the 5 groups recurred across any other categories. This is quite noteworthy because this metric seems like the best measure we have (from this survey) for groups reaching out to non-EAs and succeeding in at least introducing them (via an event) to EA, yet it seemed to bear little connection to success on any other metrics. One other striking feature of this category is that all of the top groups (except for the largest group, tying in 5th place, and with only half as many new event attendees as the top group in this category) were from non-Anglo-American countries. While this is purely speculative, an explanation for this pattern might be that these groups are aggressively reaching out to people unfamiliar with EA in their areas, getting them to attend events, but largely not seeing success in transferring this into increased group membership. This issue probably bears further research, as it seems plausible that EA groups outside of the traditional geographical areas may face distinct challenges and require more tailored support (such as translation of materials).</span></p>\n<p>&nbsp;</p>\n<p><span>These findings suggest that though a small number of groups tend to be very successful across different categories, there is still clear scope for other groups, which are not particularly large to produce great impact, competitive with the largest and highest performing groups. </span></p>\n<p>&nbsp;</p>\n<p><span>It is also notable that many of the groups in the top 5 for various metrics were not from obviously propitious environments (e.g. elite universities or major cities). The fact that there is a fair amount of variance in success across different metrics, including success outside of the largest groups and outside of obvious EA \u2018hubs\u2019, suggests that there are influences on group impact beyond size and being in propitious locations. Further research seems needed to investigate how the most impactful groups attain their success and to see how far this can be propagated as best practice.</span></p>\n<p>&nbsp;</p>\n<p><span>A final observation worth noting is that the much larger influence of the biggest group(s) may be a result of particular, contingent policies, rather than an inevitable feature of the way that EA groups\u2019 impact will be distributed. For example, EA London reliably dominates most metrics by a substantial amount, but is unusual among groups in having had a full-time paid organiser, since 2016 (and now two dedicated staff). Looking at it from the present vantage point, it may appear inevitable that London would have grown to the size and influence that it has now. However, despite having been running since 2013, until quite recently, EA London was much smaller, averaging around 5 social event attendees every other month in 2014 compared to slightly more than 50 at the end of 2017. Of course, not all of this growth should be attributed to the presence of a funded organiser, and nor does it suggest that an organiser would have been equally successful in a different city, but it does somewhat count against the view that certain groups were simply inevitably going to be very large.</span></p>\n<h3><span>Further Evidence of Group Impact</span></h3>\n<p>&nbsp;</p>\n<p><span>Above we noted the median results for various metrics of group impact (e.g. pledges, donations etc.). However, as noted, groups also aim to have impact in a variety of ways which are harder to measure and quantify or which simply can\u2019t be translated into a median value per group. We note some of these here:</span></p>\n<p>&nbsp;</p>\n<p><span>We asked individual organisers and group members to indicate how much of a factor being involved with an EA group was for their involvement with EA. As noted in the quantitative report, a majority reported that being involved in an EA group had been a \u201clarge\u201d or \u201cvery large\u201d factor for their engagement with EA. Similarly, 89% of organisers and 78% of members reported that the way they thought about the world and/or their behaviour had changed since becoming a member of a local group, with large majorities of these reporting that they expected to have more social impact as a result of these changes.</span></p>\n<p>&nbsp;</p>\n<p><span><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995988/mirroredImages/qwZj8f7eq4HxmBzus/sqrxa11njuyhas0vhhr1.png\"></span></p>\n<p>&nbsp;</p>\n<p><span><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995988/mirroredImages/qwZj8f7eq4HxmBzus/qobubn5oyjylc6jnncpk.png\"></span></p>\n<p>&nbsp;</p>\n<p><span>Though hard to attach a precise value to, and reliant on self-reports, this is strongly suggestive that local groups are having a positive impact on EAs and increasing their engagement with EA. While this is not direct evidence of impact, it seems likely that increasing people\u2019s engagement with EA may lead to impact by, on the whole, making individuals more motivated and promoting EA actions as norms. </span></p>\n<p>&nbsp;</p>\n<p><span>Our qualitative data also strongly supported this, with many individuals explicitly reporting the importance of \u201cpersonal interaction\u201d with other EA for their motivation and engagement. It seems plausible that, for at least a subset of EAs, face-to-face interaction with other EAs, rather than only online contact, may be important. </span></p>\n<p>&nbsp;</p>\n<p><span>There was also a general indicator that local groups are valuable to EAs, as a large majority (93%) of members rated their group\u2019s activities as \u201cvaluable\u201d or \u201cvery valuable.\u201d Interestingly, group </span><span>organisers</span><span>, while still having a majority (57%) rating their group\u2019s activities as \u201cvaluable\u201d or \u201cvery valuable\u201d and 89% rating them between \u201cmoderately\u201d or \u201cvery valuable\u201d were noticeably less positive than group members. Over half (55%) of all members who responded rated the group\u2019s activities \u201cvery valuable.\u201d We speculate that this may be partly explained by organisers and members interpreting the question of how valuable their group\u2019s activities are slightly differently, with members taking it to be about how valuable the activities are </span><span>to them</span><span> and organisers taking it to be asking for an evaluation of how valuable the group is as a whole. It is also possible that organisers felt the need to be more self-critical about </span><span>their</span><span> group\u2019s activities and achievements, even though members near-uniformly found the groups to be extremely valuable. </span></p>\n<p>&nbsp;</p>\n<p><span>Groups also introduced a fairly large number of individuals who were unfamiliar with EA to EA via events (nearly 6000 in total, and a median of 30 per group- which is quite substantial given a median group size of only 10). This cannot be directly identified with impact (as we do not know what proportion if any of these individuals became more engaged with EA- this would be an avenue for potential future research). However, there is suggestive evidence from our findings that slightly more than half (138/254) of our respondents did not consider themselves an EA prior to joining an EA group. This may be suggestive of non-EAs joining groups and coming to identify with EA, though it may also reflect individuals applying a very stringent definition of EA to themselves and only identifying as EAs once they have become actively involved.</span></p>\n<h2><span>Value of existing programme services and resources</span></h2>\n<p>&nbsp;</p>\n<p><span>Where identifying the impact of local groups is, as we note above, exceedingly messy, identifying the impact of efforts to </span><span>support</span><span> local groups is substantially messier.</span></p>\n<p>&nbsp;</p>\n<p><span>Where local groups aim to produce impact through a wide variety of means: introducing people to EA, making EAs more motivated and engaged, making them more informed and connected to opportunities, encouraging impactful actions, increasing retention and reducing dropout, efforts to support and promote the work of EA groups in a similarly broad set of ways with a broad set of means.</span></p>\n<p>&nbsp;</p>\n<p><span>For example, LEAN has aimed to support EA groups in a host of ways:</span></p>\n<p><span><br></span><span>- Providing technical infrastructure services (e.g. group e-mails, group websites, meetup.com etc)</span></p>\n<p><span>- Providing informational resources (e.g. guides to running groups, the map of EA &nbsp;groups)</span></p>\n<p><span>- Personal support and feedback to group leaders</span></p>\n<p><span>- Supporting group communication (newsletter, group calls etc.)</span></p>\n<p>&nbsp;</p>\n<p><span>Taking even the more straightforward of these services - providing a group with e-mails and websites - these services may potentially have positive effects on a host of outcomes via a variety of different mechanisms. They aim to make the group appear more professional, which may attract more members and/or make the group (or wider EA movement) seem more appealing, and it may allow the group to access opportunities (in virtue of as one of our qualitative respondents put it \u201c</span><span>showing people that we\u2019re not some fringe thing that\u2019s only locally run\u201d</span><span>). They may also make organisers and current members feel better about their group, increasing motivation. Providing them as a service may make running the group more convenient and less costly for organisers (who might otherwise feel they need to set up these solutions themselves), encouraging people to run groups and increasing retention. (For diverse other positive effects of support offered to local groups, see the qualitative report).</span><span><br></span><span><br></span><span>However, for each of these putative mechanisms and metrics, there are innumerable other factors influencing how professional an EA group appears, making EA groups more or less convenient or costly to run, and influencing how many people are attracted to or engage with the group. And likewise for every other means of supporting groups: providing leaders with mentoring or guidance, making written guides available, providing video calls for leaders to discuss issues and so on. Counterfactuals here are, of course, particularly difficult to discern, given the mutual influence of so many different factors on the same outcomes.</span></p>\n<p>&nbsp;</p>\n<p><span>As such, identifying the causal impact of any particular service on the performance of groups (whose own impact is itself very difficult to identify, as we noted in the previous section) is all but impossible. As a consequence, in this report, we rely on the reports of group leaders on how useful they found different services offered to support groups. These evaluations certainly have their limitations, reliant as they are on the self-reports of organisers, but we think they are among the best evidence available to us of whether services are actually helping support groups.</span></p>\n<h3><span>Alternative Analytic Strategies</span></h3>\n<p>&nbsp;</p>\n<p><span>It is worth briefly sketching out some alternative means of calculating the impact of efforts to support local groups and why they are either unworkable or severely limited:</span></p>\n<p>&nbsp;</p>\n<ul>\n<li>\n<p><span>Randomised assignment of local groups to receive support or not (for example, one set of groups might receive websites or personal support, and another set would not): this is a gold standard of experimental design highly familiar to EAs. It may be impractical, in this case, to aid some groups and not others though. Given very small samples of highly heterogeneous groups, it may be very difficult to ensure that the treatment and control groups are comparable. Furthermore, some services provided are easily accessible public goods (e.g. online resources and guides). Also not treating a \u2018control\u2019 group may have its own effects: e.g. severely dispiriting group organisers who are denied assistance or encouraging organisers to adopt their own ersatz solutions (for example, if the treatment group are, very visibly, being given new websites, this may move those assigned to the control group to set up their own website). </span></p>\n</li>\n<li>\n<p><span>Comparing pre/post treatment metrics: for example, seeing how a group performs before/after receiving support. We already have some data along these lines: for example, groups reporting increases in response rates after using an official-looking group e-mail address. We aim to expand this kind of small scale experimentation, where practicable, next year. However, in many case, where there are not discrete metrics (such as e-mail response rates), this will also be limited, as multiple other changes over the course of a year will confound the changes observed after the implementation of a particular service.</span></p>\n</li>\n<li>\n<p><span>Comparison of outcomes from observational data for different kinds of groups e.g. CEA groups vs LEAN groups: this particular suggestion would be essentially impossible, because many groups receive support from a number of different sources, meaning there are few, if any, \u201cpure CEA\u201d or \u201cpure LEAN\u201d groups. It may be possible to make informative comparisons of groups of other types.</span></p>\n</li>\n<li>\n<p><span>Comparing EA </span><span>individuals</span><span> who are or are not in EA local groups (based on the EA Survey) to see their impact, rates of attrition etc. This suggestion seems hamstrung by the fact that these differences would likely be severely confounded. Individuals who are (or are not) in an EA group are likely to systematically differ in other ways due to self-selection, among other things. For example, more motivated individuals may be more likely to attend an EA group. Alternatively people who are not in an EA group may be more likely to live in isolated areas away from an EA hub compared to those in a group, which may independently exert an effect. Efforts could be made in an analysis to \u2018match\u2019 comparable people and overcome this in other ways, however we are not sure this would be reliable or worthwhile. (The EA Survey data is freely available to anyone who wishes to make an attempt.)</span></p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<h2><span>Evaluations of Group Support and Services</span></h2>\n<p>&nbsp;</p>\n<p><span>During the interviews and the Local Group Survey, we received feedback about a range of resources and services, some of which had been provided by LEAN exclusively and some of which had been jointly offered by LEAN and other EA organisations or independent EA individuals. Disaggregating the impact of different organisations\u2019 services is therefore difficult to do systematically, though it is sometimes possible to identify individual cases of impact from the qualitative interviews. Indeed, often group organisers themselves couldn\u2019t identify which services were provided by which organisations. Nevertheless, we think these results offer a reliable indication of the value which organisers attach to different services and thus, at least to some extent, of the degree to which they are likely to help support EA groups\u2019 work.</span></p>\n<p>&nbsp;</p>\n<h3><span>Technical support</span></h3>\n<p>&nbsp;</p>\n<p><span>Technical support is the area where we can connect respondent experiences most directly to the LEAN\u2019s investment. This is because, during the time frame considered, most of these facilities were not made available by any other organisation. Some organisers supplied their own websites and Meetup.com accounts, but these are in a minority. Furthermore, organisers came to LEAN over time and arranged for us to take on hosting for existing websites, or payment for existing Meetup.com accounts.</span></p>\n<p>&nbsp;</p>\n<p><span>Services Provided</span></p>\n<div>\n<table><colgroup><col><col></colgroup>\n<tbody>\n<tr>\n<td>\n<p><span>Domain name registration</span></p>\n</td>\n<td>\n<p><span>Meetup.com subscription</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Web hosting</span></p>\n</td>\n<td>\n<p><span>Custom email addresses</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Wordpress templates</span></p>\n</td>\n<td>\n<p><span>Basic user support</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>EA Hub Group Profiles</span></p>\n</td>\n<td>\n<p><span>Map of EAs and EA Groups</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Website deployment</span></p>\n</td>\n<td>\n<p><span>Mailchimp and Facebook management</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>&nbsp;</p>\n<p><span>Our quantitative report offers ratings of the usefulness and impact of various of the tech support services. Combined with our qualitative data, these generally follow a pattern of services being found to be highly useful by a small number of groups, but not very useful or not used at all by others. That said, \u201c</span><span>technical support\u201d as a whole \u201cfor instance, subscriptions for online services, free websites, group email addresses\u201d was rated as useful or very useful by &gt;70% of respondents. </span><span>Qualitative responses bore out a pattern which recurs across our tech services, which is that these services were invaluable to some groups and minimally or not at all valuable to others, with a large number of groups not using the service at all. </span></p>\n<p>&nbsp;</p>\n<p><span>Websites</span><span>: organisers were asked whether their group website \u201c</span><span>makes a non-trivial difference in the effectiveness of your group's outreach efforts\u201d</span><span> and websites were rated as \u201csignificantly useful\u201d rather than \u201cno more than trivially useful\u201d by a slim majority of respondents. Response rates were low though (39/98), in line with the fact that relatively few groups have a website.</span></p>\n<p>&nbsp;</p>\n<p><span>Meetup.com subscription:</span><span> We asked organisers to estimate how much of a counterfactual increase (%) in attendees they had as a result of using meetup.com. There was an exceedingly wide range, with estimates literally ranging from 0% to 100%, with a median of 15%, but a small number of groups gaining very large increases (50-100%) from using the services.</span></p>\n<p>&nbsp;</p>\n<p><span>E-mails</span><span>: we did not directly survey individuals about the usefulness of group e-mail addresses, but some indication of interest came from the fact that a majority (29/54) respondents indicated that they would like one for their groups. Qualitative data offered further confirmation, with a number of organisers noting that they preferred having a group e-mail as it looked more \u201cprofessional\u201d and that they noticed higher response rates after using them.</span></p>\n<p>&nbsp;</p>\n<p><span>We are not surprised by the fact that these services were extremely valuable to some groups and of little value to others, since we find that EA groups differ extremely in a wide variety of ways. As such, a tailored approach may be necessary to direct specific service to the specific groups which find them useful. It does not automatically follow from the fact that many groups find relatively little benefit from using meetup.com, that it is not worth continuing to provide it if a small number of groups gain substantial benefits from it. &nbsp;This is especially true in cases such as Meetup.com where it is cheap and easy to provide accounts to groups who want one, and where the service allows three different groups to share one subscription. </span></p>\n<h3><span>Personal Support and Expertise</span></h3>\n<p><span>LEAN provides expertise and support to groups through the following services and activities:</span></p>\n<p>&nbsp;</p>\n<ul>\n<li>\n<p><span>Personal Feedback</span></p>\n</li>\n<li>\n<p><span>Practical support and ideas</span></p>\n</li>\n<li>\n<p><span>Written guides</span></p>\n</li>\n<li>\n<p><span>Connecting and introducing</span></p>\n</li>\n<li>\n<p><span>Impact assessment and research support</span></p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p><span>The LEAN Impact Assessment quantitative report found that group organisers attached extremely high value to receiving </span><span>personal feedback</span><span>, </span><span>practical support and ideas</span><span> and guidance in the form of </span><span>written guides</span><span>.</span></p>\n<p>&nbsp;</p>\n<p><span>Personal feedback and support:</span><span> 78% useful or very useful</span></p>\n<p><span>Practical support and new ideas for group activities:</span><span> 94% useful or very useful</span></p>\n<p><span>Written guides</span><span>: 83% useful or very useful</span></p>\n<p>&nbsp;</p>\n<p><span>Our qualitative data filled out the details about exactly what and why organisers found valuable. A recurring theme was that some organisers felt insecure about running a group, as to whether or not what they were doing was effective, felt isolated from the broader community and felt that personal contact for feedback, reassurance and advice was beneficial for motivation. </span></p>\n<p>&nbsp;</p>\n<p><span>Increased guidance and provision of EA materials (for example, stock content which could be posted to their Facebook group) were also cited as things which would make running a local group easier. Some organisers reported that they felt that they were having to invent things themselves, even though they knew that other groups must have worked on similar problems before.</span></p>\n<p>&nbsp;</p>\n<p><span>Other related factors which came up in our qualitative data, but which were not captured in terms of our quantitative metrics, involved helping connect EAs and groups with relevant people (either directly, through our networks, or indirectly through the use of the Effective Altruism Map.)</span></p>\n<p>&nbsp;</p>\n<p><span>A further recurring theme was a demand from group organisers for more research which would inform how they should be optimally running their groups. Many EAs wished to know whether they were acting effectively and having an impact, and wished to see centralised data from other groups to better discern what would be effective.</span></p>\n<h3><span>Group Communication</span></h3>\n<ul>\n<li>\n<p><span>Group Calls</span></p>\n</li>\n<li>\n<p><span>EA Group Newsletter</span></p>\n</li>\n<li>\n<p><span>Mentoring Programme</span></p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p><span>LEAN, in some of these cases in collaboration with CEA and EAF, support a number of platforms for EA group organisers to discuss with each other and with orgs supporting movement building, to share expertise and information. There was substantial variability in how useful each of these platforms was rated as being, as some were new or little used or organisers were not yet aware they existed, whereas others were widely valued.</span></p>\n<h4><span>Group Calls</span></h4>\n<p><span>Slightly more than half (54.3%) of respondents rated group calls as being useful or very useful.</span></p>\n<p><span>Qualitative data suggested that organisers were broadly supportive of measures to improve interconnectivity between groups and group leaders, though there was little specific mention of group calls. Furthermore, some interviewees mentioned that the one-size-fits all nature of group calls meant that they had not found the issues discussed particularly relevant for their own groups. </span></p>\n<h4><span>EA Group Newsletter</span></h4>\n<p><span>LEAN published eleven newsletters for group organisers between 2015 and May 2016. The group organisers newsletter was relaunched as an inter-organizational service during 2017, and four editions have been published (note that this is not the same as the general EA Newsletter, which is also provided in collaboration between CEA, LEAN and EAF).</span></p>\n<p>&nbsp;</p>\n<p><span>Feedback on the concept is limited due to the fact that few respondents were in receipt of the newsletter. 32% indicated that they receive the newsletter, whereas 61.6% of respondents expressed an interest in being added to the newsletter. 53% of respondents to a question about the usefulness of the newsletter rated it as useful or very useful, while 40% rated it as neither useful nor useless.</span></p>\n<h4><span>EA Mentoring Programme</span></h4>\n<p><span>A pilot mentoring programme was initiated by CEA in late 2015. In 2017 the concept was rebooted by LEAN in beta form. As with the rebooted EA Groups Newsletter, few participants in the sample would have direct experience of the Mentoring programme so far. At of the time of writing, 9 mentoring pairs are included in the programme. Given this, it is not surprising that the majority (61%) of respondents rated the programme as \u2018neither useful nor useless,\u2019 though 30.7% deemed the programme to be either \u2018useful\u2019 or \u2018very useful\u2019. We received two or three comments from survey participants explicitly stating that they had not heard of the Mentoring Programme and the Newsletter, and asking to be sent further information. We will be following up on the pairs of individuals involved in the mentoring program to investigate how impactful it is.</span></p>\n<h2><span>Strategic Summary</span></h2>\n<p><span>This section summarises the strategic plans that LEAN has for 2018 based on these findings.</span></p>\n<p>&nbsp;</p>\n<p><span>Tech support and group communication services will continue, via more streamlined techniques. Our tech resources will be focused on developing a high quality interface for written content and resources. In response to demand from organisers for these services, we will expand our investment in offering personal support and feedback to organisers, and will consolidate our online resources for group leaders. We think that these written guides and personalised support will be synergistic with LEAN\u2019s expanded work researching EA groups and how they can best produce impact. More broadly, we hope to work more on discerning the specific needs of groups in different contexts and offering tailored support and advice.</span></p>\n<p>&nbsp;</p>\n<p><span>Tech support</span><span>: </span></p>\n<p>&nbsp;</p>\n<p><span>Having found that our tech services are highly useful to a minority of groups we have decided to continue, but streamline our service provision. While it might seem that services which are useful only to a minority of groups are not worth providing, we have concluded that the high level of usefulness to a small number of groups justifies providing these services. A significant consideration is that, in many cases, if LEAN ceased providing these services, the financial and time costs would likely simply fall to individual organisers, making running groups more costly for individuals. We have, however, switched to employing simpler, more automated processes for providing services (for example, a static site generator for EA group websites, and an improved email host) which requires less time and financial investment. At the same time we have also invested more in dedicated tech staff to ensure our systems are more reliable in the future.</span></p>\n<p>&nbsp;</p>\n<p><span>Tech development</span></p>\n<p>&nbsp;</p>\n<p><span>Based on the evident popularity of existing written content, and the widespread wish to see existing content consolidated, we are investing in a new web interface for organisers. This will involve editing existing community generated content, and assimilating it into a central, visually appealing interface. The new interface will be based on the EA Hub, which will itself be modernised and restructured. Where possible, we will facilitate editorial additions from the community, in order to make the tool a logical home for sharing resources.</span></p>\n<p>&nbsp;</p>\n<p><span>Group Communication</span></p>\n<p>&nbsp;</p>\n<p><span>We will continue to facilitate group calls alongside other organisations, as there is still demand for these, though we will work on promoting more targeted calls (and, in particular, offering groups more individual calls where requested). We will continue to run the group newsletter (which now has a larger number of organisers added) to keep organisers up to date on relevant development. We will also continue to facilitate the mentoring program for interested organisers. </span></p>\n<p>&nbsp;</p>\n<p><span>Personal support and feedback</span></p>\n<p>&nbsp;</p>\n<p><span>In response to very high ratings of usefulness of these kinds of services, we have decided to dedicate more attention to providing personal support and feedback to organisers where requested. Much of this will take the form of one-to-one calls with individual organisers to talk through challenges their group is facing, connect them with relevant individuals, information and resources. We hope this kind of tailored support will help guide organisers, and also increase their motivation and confidence in their actions. This will also allow a closer determination of i) how much counterfactual impact groups are having, ii) how far particular services are supporting groups. </span></p>\n<p>&nbsp;</p>\n<p><span>Written guides and resources</span></p>\n<p>&nbsp;</p>\n<p><span>Similarly, in response to a strong indication of organiser interest in this, we will dedicate time to consolidating and adding to existing written resources aimed at helping organisers to launch and run groups. This will contribute to the new content tool for organisers on the EA Hub, mentioned above. We aim to update these resources on an ongoing basis using our ongoing research into how EA groups are functioning and how they might function better. </span></p>\n<p>&nbsp;</p>\n<p><span>Research</span></p>\n<p>&nbsp;</p>\n<p><span>Many organisers expressed an interest in using metrics to determine their impact and in having access to evidence-based research on how best to run EA groups. We have also noted, throughout this report, various questions about EA movement building where further empirical research is needed. Some of this research may be conducted in concert with the EA Survey (also run by Rethink Charity). We aim to help individual groups to measure their impact, and to produce general research regarding EA groups for the wider community.</span></p>\n<p>&nbsp;</p>\n<p><span>Grants</span></p>\n<p>&nbsp;</p>\n<p><span>Recognising that funds are important for groups\u2019 success, and having heard from a number of group organisers that lack of funds was a bottleneck, we are planning to make small grants to groups on a case by case basis to help facilitate group growth and are exploring ways to collaborate with CEA to direct funds to groups. Where appropriate we will align this with our ongoing research into EA groups, to try to discern how far targeted grants boost EA groups\u2019 impact.</span></p>\n<p>&nbsp;</p>\n<h2><span>Acknowledgments </span></h2>\n<p>&nbsp;</p>\n<p><span>This report was analysed and authored by Richenda Herzig and David Moss.</span></p>\n<p><span>Special thanks to Tee Barnett for editorial feedback, and for coordinating those involved in the Impact Assessment, both internal and external. Further thanks to Peter Hurford for input as an internal advisor.</span></p>\n<p>&nbsp;</p>\n<p><span>We are very grateful to Greg Lewis for his ongoing input as our external advisor. </span></p>\n<p>&nbsp;<br>&nbsp;</p>", "user": {"username": "Richenda"}}, {"_id": "rhidnFTcWufnzypfh", "title": "How effective and efficient is the funding policy of Open Philanthropy concerning projects on AI risks?", "postedAt": "2018-02-27T23:33:54.632Z", "htmlBody": "<html><body><div>\n<div>\n<p><em>This was originally posted as a&#xA0;<a href=\"/ea/14c/why_im_donating_to_miri_this_year/dce\">comment</a>&#xA0;on an old thread. However, I think the topic is important enough to deserve a discussion of its own. I would be very interested in hearing your opinion on this matter. I am an academic working in the field of philosophy of science, and I am interested in the criteria used by funding institutions to allocate their funds to research projects.</em></p>\n<p>A recent trend of providing relatively high research grants (relative to some of the most prestigious research grants across EU, such as for instance ERC starting grants ~ 1.5 mil EUR) to projects on AI risks and safety made me curious, and so I looked a bit more into this topic. What struck me as especially curious is the lack of transparency when it comes to the criteria used to evaluate the projects and to decide how to allocate the funds.</p>\n<p>Now, for the sake of this article, I&#xA0;will assume that the research topic of AI risks and safety&#xA0;<em>is important</em>&#xA0;and <em>should be funded</em> (to which extent it actually is, is beside the point and deserves a discussion of its own; so let&apos;s just say it is among the most pursuit-worthy problems in view of both epistemic and non-epistemic criteria).</p>\n<p>Particularly surprising was a sudden <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support-2017\">grant of 3.75 mil USD by Open Philanropy Project (OPP) to MIRI</a>. Note that the funding is more than double the amount given to ERC starting grantees. Previously, OPP awarded MIRI with 500.000 USD and provided <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support\">an extensive explanation of this decision</a>. So, one would expect that for a grant more than 7 times higher, we&apos;d find at least as much. But what we do find is an extremely brief explanation saying that an anonymous expert reviewer has evaluated MIRI&apos;s work as highly promising in view of their paper &quot;Logical Induction&quot;.</p>\n<p>Note that in the last 2 years since I first saw this paper online, the very same paper has not been published in any peer-reviewed journal. Moreover, if you check <a href=\"https://intelligence.org/all-publications/\">MIRI&apos;s publications</a>&#xA0;you find not a single journal article since 2015 (or an article published in prestigious AI conference proceedings, for that matter -- *correction:* there are five papers published as conference proceedings in 2016, some of which seem to be <a href=\"https://www.aaai.org/Press/Reports/reports.php\">technical reports</a>, rather than actual publications, so I am not sure how their quality should be assessed;&#xA0;I see no such proceedings publications in 2017). It suffices to say that I was surprised. So I decided to contact both MIRI asking if perhaps their publications haven&apos;t been updated on their website, and OPP asking for the evaluative criteria used when awarding this grant.</p>\n<p>MIRI has never replied (email sent on February 8). OPP took a while to reply, and&#xA0;last week I received the following email:</p>\n<p>&quot;Hi Dunja,</p>\n<p>Thanks for your patience. Our assessment of this grant was based largely on the expert reviewer&apos;s reasoning in reviewing MIRI&apos;s work. Unfortunately, we don&apos;t have permission to share the reviewer&apos;s identity or reasoning. I&apos;m sorry not to be more helpful with this, and do wish you the best of luck with your research.</p>\n<p>Best,</p>\n<p>[name blinded in this public post; I explained in my email that my question was motivated by my research topic]&quot;</p>\n<p>All this is very surprising given that OPP prides itself on transparency. As stated <a href=\"https://www.openphilanthropy.org/what-open-means-us\">on their website</a>:</p>\n<p>&quot;We work hard to make it easy for new philanthropists and outsiders to learn about our work. We do that by:</p>\n<ul>\n<li>Blogging about major decisions and the reasoning behind them, as well as what we&#x2019;re learning about how to be an effective funder.</li>\n<li>Creating detailed reports on the causes we&#x2019;re investigating.</li>\n<li>Sharing notes from our information-gathering conversations.</li>\n<li>Publishing writeups and updates on a number of our grants, <strong>including our reasoning</strong> and reservations before making a grant, and any setbacks and challenges we encounter.&quot; (emphasis added)</li>\n</ul>\n<p>However, the main problem here is not the mere lack of transparency, but the lack of&#xA0;<em>effective and efficient funding policy</em>.</p>\n<p>The question, how to decide which projects to fund in order to achieve effective and efficient knowledge acquisition has been researched within philosophy of science and science policy for decades now. Yet, some of the basic criteria seem absent from cases such as the above mentioned one. For instance, establishing that the given research project is worthy of pursuit cannot be done merely in view of the pursuit-worthiness of the research topic. Instead, the project has to show a viable methodology and objectives, which have been assessed as apt for the given task by a <em>panel of experts</em> in the given domain (rather than by a single expert reviewer). Next, the project initiator has to show expertise in the given domain (where one&apos;s publication record is an important criterion). Finally, if the funding agency has a certain topic in mind, it is much more effective to make an open call for project submissions, where the expert panel selects the most promising one(s).</p>\n<p>This is not to say that young scholars, or simply scholars without an impressive track record wouldn&apos;t be able to pursue the given project. However, the important question here is not &quot;<em>Who could pursue this project?&quot;&#xA0;</em>but<em>&#xA0;&quot;Who could pursue this project in the most effective and efficient way?&quot;.</em></p>\n<p>To sum up: transparent markers of reliability, over the course of research, are extremely important if we want to advance effective and efficient research. The panel of experts (rather than a single expert) is extremely important in assuring procedural objectivity of the given assessment.</p>\n<p>Altogether, this is not just surprising, but disturbing. Perhaps the biggest danger is that this falls into the hands of press and ends up being an argument for the point that organizations close to effective altruism are not effective at all.</p>\n</div>\n</div>\n<div>&#xA0;</div></body></html>", "user": {"username": "Dunja"}}, {"_id": "877YnBmZXZzhxDyEv", "title": "How much does it cost to roll-out a vaccine?", "postedAt": "2018-02-26T15:33:03.710Z", "htmlBody": "<html><body><p><em>This essay was jointly written by Peter Hurford and Marcus A. Davis. This is part of <a href=\"/ea/1o6/what_is_the_costeffectiveness_of_researching/\">a series exploring the cost-effectiveness of vaccines</a>.</em></p>\n<p><a href=\"/ea/1l4/how_much_does_it_cost_to_research_and_develop_a\"><br>We previously estimated the cost of developing a vaccine</a>&#xA0;from scratch to be $460M to $1.9B with a mean of $960M. However, this still does not tell us the full cost of a vaccine, because developing a vaccine but then not ever using it accomplishes nothing. Instead, you need to roll out the vaccine to people, which costs more money. Thus, vaccine R&amp;D could be thought of as &#x201C;unlocking&#x201D; the opportunity to roll-out a vaccine, and the hope is that the high cost-effectiveness of rolling out a vaccine will help offset the high cost of vaccine R&amp;D.</p>\n<h2><br> Understanding Roll-out Costs</h2>\n<p>Rolling out a vaccine costs more than just the price of manufacturing the vaccine. As <a href=\"http://www.who.int/bulletin/volumes/86/1/07-045096/en/\"> Wolfson, et. al. (2008)</a>&#xA0;notes (see Table 3), there is a significant logistics component to the costs -- you have to ship the vaccine to its destination; keep it stored correctly (e.g., using a &#x201C;cold chain&#x201D; to make sure the vaccine is consistently stored at the correct temperature at all times); pay for waste management; train, supervise, and compensate volunteers and support staff; and monitor and evaluate vaccine performance. You may also need to pay for media to promote the vaccine and help the populace understand and accept it.</p>\n<p>The price per dose of a vaccine is aimed not just at recovering variable manufacturing costs, but also aimed at recovering fixed costs including R&amp;D (<a href=\"http://blogs.plos.org/speakingofmedicine/2013/04/23/vaccines-in-developing-countries-why-the-high-prices/\">Elder &amp; Cohn, 2013</a>) and, possibly, the cost of trials (<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3551877/\">Weinberg et al., 2012</a>). If we accounted for R&amp;D spending both separately and in the per dose costs of vaccines during roll-out, we could be overestimating vaccine costs.</p>\n<p>When thinking of costs, it is useful to separate &#x201C;fixed costs&#x201D; (one-time costs incurred from setting up a vaccine project) from &#x201C;variable costs&#x201D; (costs that will be incurred on every vaccine regardless of scale). However, with economies of scale, it is difficult to tell where fixed costs end and variable costs begin. For example, many modern vaccines today are cheaper to roll out because they can rely on infrastructure and technology built during previous rollouts. Even simple and apparently unrelated infrastructure like building roads and bridges could be conceived as an investment in reducing the rollout costs of vaccines. Estimating roll-out costs of vaccines is very tricky because it is unclear to what degree should we include these investments. <a href=\"http://www.who.int/immunization/sage/1_Bishai_Economic_analysis.pdf\"> Bishai, Johns, Lefevre, and Nair (2010)</a>&#xA0;found that distributing a vaccine could be up to 38x as expensive in highly remote areas compared to core, easily accessible areas with strong pre-existing infrastructure.</p>\n<h2>&#xA0;</h2>\n<h2>Vaccine Case-Studies</h2>\n<p>At this time, I no longer include the ebola vaccine, because it is too early stage to know how much it will cost to roll out. For the remaining vaccines, I find the following rollout costs and elaborate more below:</p>\n<ul>\n<li>\n<p>Smallpox - <strong>$0.73 - $47.62 per child</strong></p>\n</li>\n<li>\n<p>Measles - <strong>$1 - $38 per child</strong></p>\n</li>\n<li>\n<p>Rotavirus - <strong>$3 - $28 per child</strong></p>\n</li>\n<li>\n<p>HPV - <strong>$2.55 - $22.71 per child</strong></p>\n</li>\n<li>\n<p>HIV - <strong>$50 - $160 per child</strong></p>\n</li>\n<li>\n<p>Malaria - <strong>$22 per child</strong></p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<h4>Smallpox Vaccine</h4>\n<p>Smallpox represents the pinnacle of rolling out a vaccine -- continuing to deliver it until the disease itself is eliminated. Prior to the eradication campaign, however, the smallpox vaccine had to be rolled out like any vaccine. This rollout cost $0.10 per child per vaccination in the developing world and $6.50 per vaccination in the US (<a href=\"https://drive.google.com/file/d/0B3lpvu9ww5atZlJoNDl1WjRDSzA/view\">Fenner, 1988</a>, p1363-1368). Costs for developed world countries outside the US were not available. <a href=\"https://docs.google.com/spreadsheets/d/1kBlG98jsDD2nsXRXLdPJXRgR30-8dnQPF_OQfDXPY_g/edit?usp=sharing\"> Adjusting for inflation</a>, this suggests a comparable rollout cost of $0.73 to $47.62 per vaccination.</p>\n<h4>&#xA0;</h4>\n<h4>Measles Vaccine</h4>\n<p>Rolling out single-dose measles vaccinations costs marginally $1 per child in areas with pre-existing strong vaccine infrastructure, $18-$28 in core areas of low income countries, and $27-$38 in outlying satellite areas that are hard to reach (Bishai, Johns, Lefevre, &amp; Nair, 2010, p3). For one example, <a href=\"https://www.researchgate.net/publication/275528205_Examining_the_cost_of_delivering_routine_immunization_in_Honduras\"> Janusz, et. al. (2015)</a>&#xA0;found a cost of $1.60 per dose of the MMR (measles, mumps, and rubella) vaccine in Honduras.</p>\n<p>&#xA0;</p>\n<h4>Rotavirus Vaccine</h4>\n<p>As of 2007, the Brazilian government struck a deal with GSK to pay $7 per dose for a two-dose rotavirus vaccine (<a href=\"http://screening.iarc.fr/doc/IAVI_PATH_HPV_financing.pdf\">Saxenian, 2007</a>, p10), or $14/child. With delivery and other costs, this would be $28/child assuming vaccine dosage costs are 50% of costs (<a href=\"https://www.sciencedirect.com/science/article/pii/S0264410X15000225\">Brenzel, 2015</a>). Later, GAVI negotiated a price of ~$3 per dose (<a href=\"http://rotacouncil.org/wp-content/uploads/2016/03/White-paper-FINAL-v2.pdf\">RotaCouncil, 2016</a>, p28), but paying up to $7 per dose in the developing world is still common ( <a href=\"http://erc.msh.org/dmpguide/resultsdetail.cfm?language=english&amp;code=ROTX&amp;s_year=2014&amp;year=2014&amp;str=&amp;desc=Vaccine%2C%20Rotavirus&amp;pack=new&amp;frm=VIAL&amp;rte=PO&amp;class_code2=19%2E3%2E&amp;supplement=&amp;class_name=%2819%2E3%2E%29Vaccines%3Cbr%3E\"> Drug Price Search</a>). For one example, Janusz, et. al. (2015) found $5 per dose in Honduras.</p>\n<p>&#xA0;</p>\n<h4>HPV Vaccine</h4>\n<p>The HPV vaccine is assumed to cost about $48 for developing countries (<a href=\"http://erc.msh.org/dmpguide/resultsdetail.cfm?language=english&amp;code=HPVX&amp;s_year=2014&amp;year=2014&amp;str=&amp;desc=Vaccine%2C%20HPV&amp;pack=new&amp;frm=VIAL&amp;rte=INJ&amp;class_code2=19%2E3%2E&amp;supplement=&amp;class_name=%2819%2E3%2E%29Vaccines%3Cbr%3E\">MSH Drug Price Search</a>), though in 2013, GAVI was able to negotiate $4.50 per dose (<a href=\"http://www.uicc.org/gavi-roll-out-record-low-price-hpv-vaccines\">UICC, 2013 </a> ; <a href=\"http://www.gavi.org/library/news/press-releases/2013/hpv-price-announcement/\"> GAVI, 2013</a>), down from the prior price of $13 per dose (<a href=\"http://www.gavi.org/library/news/press-releases/2013/hpv-price-announcement/\">GAVI, 2013</a>). However the manufacturing cost of Gardasil for developing countries is currently between $0.48 and $0.59 per dose (<a href=\"https://www.sciencedirect.com/science/article/pii/S0264410X16308568\">Clendinen, Zhang, Warburton, &amp; Light, 2016</a>), which is much lower than the current price per dose, suggesting the price has room to decrease significantly over the future.</p>\n<p>These prices and manufacturing costs also do not include delivery costs. Delivery costs between $2.40 and $3.37 per three dose schedule according to one estimate, between $3.10 and $9.21 according to a second estimate, and between $1.11 and $2.74 according to a third estimate (<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/28691329\">LaMontagne, et. al., 2017</a>).</p>\n<p>Based on this, the total cost may range between $2.55 ($0.48 per dose for three doses plus $1.11 to deliver) and $22.71 ($4.50 per dose for three doses plus $9.21 to deliver) per child.</p>\n<p>&#xA0;</p>\n<h4>HIV Vaccine</h4>\n<p>Hecht and Jameson (2011) estimate the HIV vaccine would cost $60-$150 per person to administer. However, as <a href=\"http://www.copenhagenconsensus.com/sites/default/files/forsythe.pdf\"> Forsythe (2011)</a>, also writing for the Copenhagen Consensus, points out, there may be even more costs to create demand for the vaccine and to manage the supply chain, and suggests that the vaccine could cost up to $465 per person or more. On the other hand, Hecht and Jameson (2011) includes costs to the manufacturer to recoup R&amp;D spending in the vaccination costs, which risks double counting. Adjusting for this could result in a lower cost per person.</p>\n<p>&#xA0;</p>\n<h4>Malaria Vaccine</h4>\n<p>The pilot of the malaria vaccine is supposed to roll out to 360,000 children by 2022 (<a href=\"http://www.gavi.org/about/governance/gavi-board/minutes/2016/22-june/minutes/09---malaria-vaccine-pilots---appendices/\">GAVI, 2016</a>, p2). The long-term plan is to set a price per dose of $5 (Ibid., p6) -- with four doses per child (<em>Ibid.</em>, p1), that would be $20/child. There would be an additional cost of $2 per child (Ibid., p22), for $22/child total.</p>\n<p>For the pilot itself, there is an expectation of $101M (<em>Ibid.</em>, p2) in non-vaccine costs and $200M in future vaccine development costs (Ibid., p10). At the long-term $22/child rate, vaccinating 360,000 children would cost $7.92M. Thus the other $93M in non-vaccine costs must be transitional costs to set up the pipeline for the vaccine. With $605M in R&amp;D spent, $200M in future R&amp;D, and $93M in transition costs, that is $898M in fixed spending to unlock the ability to roll out the vaccine for $22/child.</p>\n<p>&#xA0;</p>\n<h2>Conclusion</h2>\n<p>In our historical sample, the rollout cost of a vaccine ranged between $0.73 to $160 per child vaccinated, and this is even before taking into account other large sources of variation, such as level of pre-existing infrastructure and remoteness of region. Furthermore, the price, particularly in lower and middle income countries, depends somewhat on the negotiation of large groups, NGOs and governments and as such it&#x2019;s difficult to generalize these estimate to future prices. Additionally, it&#x2019;s not clear that our six ranges can be directly compared, given that they were collected at different times in history with different methodologies, taking into account different factors.</p>\n<p>Disregarding that and taking the mean of our six ranges produces a mean range of $13.21 to $53.05 and a median range of $2.78 to $33 (though again it&#x2019;s unclear how meaningful these numbers are). <a href=\"https://www.sciencedirect.com/science/article/pii/S0264410X15000225\"> Brenzel (2015)</a>&#xA0;finds that routine immunization costs on average $27-42 per fully immunized child including all costs to deliver all routine vaccines, with significant regional variation, though this would be biased downwards compared to our estimate, as it would not include the more expensive HIV and HPV vaccines.</p>\n<p>Thus it is difficult to figure out what it would mean to estimate the &#x201C;typical&#x201D; roll-out cost for a new vaccine, should one exist. For example, if we were to roll out the ebola or malaria vaccines widely, how expensive would we expect them to be? Understanding this number, along with the price of research and development, is important for understanding the marginal costs and benefits of new vaccination.<br><br></p>\n<p><em>Next in the series: <a href=\"/ea/1nu/how_beneficial_have_vaccines_been/\">How Beneficial Have Vaccines Been?</a></em></p></body></html>", "user": {"username": "Peter_Hurford"}}, {"_id": "BjBmcfwg2awqPJLin", "title": "How much does it cost to research and develop a vaccine?", "postedAt": "2018-02-24T01:23:33.601Z", "htmlBody": "<html><body><p><em>This essay was jointly written by Peter Hurford and Marcus A. Davis.</em><em>&#xA0;This is part of&#xA0;<a href=\"/ea/1o6/what_is_the_costeffectiveness_of_researching/\">a series exploring the cost-effectiveness of vaccines</a>.</em><em><br><br></em></p>\n<p>Previously, <a href=\"/ea/1c5/how_long_does_it_take_to_research_and_develop_a/\"> we estimated how long it takes to research and develop a vaccine</a>&#xA0;and came up with a conclusion that it would take &#x201C;an average of 29 years [to develop a] typical vaccine, though with high uncertainty based on uncertainties in each approach and on many particular vaccines not being typical&#x201D;. However, if we want to know the cost-effectiveness of vaccine research, it&#x2019;s not enough to know how long a vaccine takes, but how much total money it would cost.</p>\n<p>&#xA0;</p>\n<h2>Literature</h2>\n<p>Like figuring out vaccine timelines, figuring out vaccine costs is also very difficult. <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/24160863\"> Waye, Jacobs, and Schryvers (2013)</a>&#xA0;state that the best answer to the average cost of vaccine R&amp;D is that &#x201C;the cost of vaccine R&amp;D is unknown&#x201D;. However, <a href=\"https://80000hours.org/2011/11/estimation-is-the-best-we-have/\"> we&#x2019;re not without our ability to estimate</a>.</p>\n<p>Waye, Jacobs, and Schryvers (2013) state that it costs an average of $1.2B to bring a drug to market in the US, though it&#x2019;s unclear how well that average generalizes to vaccines specifically. Another estimate by <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/26928437\"> DiMasi, Grabowski, and Hansen (2016)</a>&#xA0;found an average cost of $1.39B.</p>\n<p>To use a concrete example of the rotavirus vaccine, Light, Andrus, &amp; Warburton (2009) estimate using publicly available data that the Phase I-III trials for two different rotavirus vaccine candidates were $137M-$206M (Merck) and $128M-$192M (GSK). But the costs of a vaccine are more than just the costs of the Phase I-III trials.</p>\n<p><a href=\"https://www.researchgate.net/publication/241411254_Cost_lifecycle_in_vaccine_industry\"> Moalla, Bouras, Ouzrout, and Neubert (2009)</a>&#xA0;outline a more holistic cost lifecycle for vaccines, which includes payment for researcher salaries and equipment throughout the research and design phase in addition to the trial phase, plus the payment for preclinical trials before Phase I-III trials, plus the costs of registration. Dr. Michel Greco, the President and Chief Operating Officer of Aventis Pasteur, testified to WHO that vaccine trials and registration are about 70% of the total costs of developing a vaccine (<a href=\"http://apps.who.int/iris/bitstream/10665/67906/1/WHO_V-B_02.24_eng.pdf\">WHO 2001</a>, p19).</p>\n<p>Additionally, Waye, Jacobs, and Schryvers (2013) point out that this does not include the costs of the failed vaccines -- Light, Andrus, &amp; Warburton (2009) are only looking at the costs of the successes. If we consider the rotavirus vaccine to have taken four total attempts, with $200M each spent on clinical trials and $86M each spent on all other costs, that would require $1.14B (which is pretty close to the $1.2B initial average for drugs in the US).</p>\n<p>However, it&#x2019;s also not clear how well these long-run trends will predict the future. Andrew Witty, the CEO of GSK, <a href=\"http://www.reuters.com/article/us-glaxosmithkline-prices-idUSBRE92D0RM20130314\"> said in 2013 </a> that the $1B price tag for new vaccine development was &#x201C;one of the great myths of the industry&#x201D; and expected to bring future vaccines to market at lower prices. On the other hand, <a href=\"http://blogs.plos.org/speakingofmedicine/2013/04/23/vaccines-in-developing-countries-why-the-high-prices/\"> Elder and Cohn (2013) </a> observe that more modern vaccines being distributed have higher per dose prices than earlier vaccines, which could (but does not necessarily) suggest that vaccine costs are increasing.</p>\n<p>&#xA0;</p>\n<h2>Vaccine Breakdown</h2>\n<p>Any kind of historical data on the costs of individual vaccines, let alone reliable data is quite difficult to come by. Also, the costs of vaccines has dramatically changed with the invention and standardization of modern clinical trials and licensing, which limits the amount we can infer from historical vaccines.</p>\n<p>To get a good benchmark, we chose to look at smallpox because it was the first vaccine and the only disease successfully eradicated through vaccination; measles as a historical vaccine as there is a good prima facie chance it is the most cost-effective vaccine<strong><a href=\"#en1\">[1]</a></strong>; the HIV, malaria, and ebola vaccines because they are currently under development and total R&amp;D spending is relatively easy to find; and the rotavirus and HPV vaccines because they finished licensing the most recently.</p>\n<p>Based on this, I find the following costs and elaborate more below:<br><br></p>\n<ul>\n<li>\n<p>Smallpox - <strong>$5.5M</strong></p>\n</li>\n<li>\n<p>Measles - <strong>$38.3M</strong></p>\n</li>\n<li>\n<p>Rotavirus - <strong>$1,140M</strong></p>\n</li>\n<li>\n<p>HPV - <strong>???</strong></p>\n</li>\n<li>\n<p>HIV - <strong>$24,500M</strong></p>\n</li>\n<li>\n<p>Malaria - <strong>$605M</strong></p>\n</li>\n<li>\n<p>Ebola - <strong>$1,500M</strong></p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<h4>Smallpox Vaccine</h4>\n<p>The total cost of developing the smallpox vaccine is pretty difficult to estimate. While Edward Jenner interacted with, studied under, and was inspired by a wide variety of other people, he more or less appeared to work on the smallpox vaccine entirely on his own (<a href=\"http://www.jameslindlibrary.org/articles/the-origins-of-vaccination-no-inoculation-no-vaccination/\">Boylston, 2012</a>). If we assume his total costs was a modern salary of $120K<strong><a href=\"#en2\">[2]</a></strong> for each of the 26 years he was working on the smallpox vaccine (though he likely wasn&#x2019;t working on the vaccine full time), the cost of the labor would be ~$3.1M in today&#x2019;s money.</p>\n<p>Since Jenner worked in a time before modern clinical trials (and even before a theory of germs), Jenner was able to complete his work with very minimal overhead (Boylston, 2012)<strong><a href=\"#en3\">[3]</a></strong>. He only tested his theory on a small handful of test subjects (<em>Ibid</em>.) -- if we assume that there was 50% overhead from HR and equipment<strong><a href=\"#en4\">[4]</a></strong> and there were ~100 people with a modern cost of $9K per person in the trials<strong><a href=\"#en5\">[5]</a></strong>, this would add $2.45M, for a total of $5.55M. Accordingly, it looks like the first smallpox vaccine was developed for very little money.</p>\n<h4><br>Measles Vaccine</h4>\n<p>John Enders, the inventor of the first measles vaccine, was able to develop a vaccine in a time before modern clinical trials, and the vaccine was tested and licensed based on testing the vaccine on a few thousand people (<a href=\"http://www.nytimes.com/2010/10/05/health/05first.html\">Bakalar, 2010</a>), which made the development of the vaccine a lot cheaper than if it were being done today.</p>\n<p><a href=\"http://www.nobelprize.org/nobel_prizes/medicine/laureates/1954/enders-bio.html\"> The Nobel Prize biography of Enders</a>&#xA0;implies Enders had a vaccine team of at least seven and Enders himself credited six other scientists <a href=\"http://www.nytimes.com/2010/10/05/health/05first.html\"> in his announcement of the vaccine</a>. If we assume a salary of $120K each, same as we did for smallpox, for each of the nine years it took to make the vaccine (<a href=\"https://timelines.issarice.com/wiki/Timeline_of_measles\">Rice, 2017</a>), that there was 50% overhead from HR and equipment, and that it cost $9K per person for ~3000 people in the trials, that would be a total cost of $38.3M.<br>&#xA0;</p>\n<h4>Rotavirus Vaccine</h4>\n<p>Earlier, citing Waye, Jacobs, and Schryvers (2013) and Light, Andrus, &amp; Warburton (2009), we considered the rotavirus vaccine to have taken four total attempts, with $200M each spent on clinical trials and $86M each spent on all other costs, requiring $1.14B.</p>\n<p>&#xA0;</p>\n<h4>HIV Vaccine</h4>\n<p>The HIV vaccine <a href=\"https://en.wikipedia.org/wiki/HIV_vaccine#Difficulties_in_developing_an_HIV_vaccine\"> has proven difficult to develop</a>&#xA0;and has costed many times that of a typical vaccine. <a href=\"http://www.resultsfordevelopment.org/sites/resultsfordevelopment.org/files/Rethink%20HIV%202011%20Assessment%20Paper%20-%20Benefit-cost%20analysis%20of%20AIDS%20Vaccine%20Research.pdf\"> Hecht and Jameson (2011)</a>, writing for the Copenhagen Consensus&#x2019;s <a href=\"https://smile.amazon.com/Rethink-HIV-Bj%C3%B8rn-Lomborg-ebook/dp/B009ZRNSU8\"> Rethink HIV</a>&#xA0;(Chapter 6), outlines an estimate that having an &#x201C;effective HIV/ AIDS vaccine available for introduction by 2030 could cost as much as twenty times the $1B typically required to develop a new drug&#x201D;, citing <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/19247981\"> Adams and Brantner (2010)</a>&#xA0;for calculating typical vaccine costs. Existing work on an HIV vaccine is cited to be at $9B to date, growing at $800M-900M per year (Hecht &amp; Jameson, 2011).</p>\n<p>Hecht and Jameson (2011) convened a panel of leading AIDS vaccine scientists who, as of 2011, believed a prototype vaccine could achieve proof of 50% efficacy by 2020-2025, to be available for large-scale introduction by 2025-2030. This would mean that the total expected R&amp;D costs of the HIV vaccine would be $21B to $28B<strong><a href=\"#en6\">[6]</a></strong>, or $24.5B as the average guess.</p>\n<h4><br>Malaria Vaccine</h4>\n<p>While no malaria vaccine has been licensed yet, the malaria vaccine <a href=\"https://en.wikipedia.org/wiki/RTS,S\">RTS,S</a>, developed by GSK, has recently completed all clinical trials and has <a href=\"https://www.bostonglobe.com/news/world/2017/04/24/african-countries-chosen-test-first-malaria-vaccine/3gbWvBWQ3k7b4mP40gWbpL/story.html\"> started large-scale pilots to 100,000 children in Africa</a>. The total funding by GSK and the Bill &amp; Melinda Gates Foundation to get the malaria vaccine from conception through clinical trials has been ~$605M (<a href=\"http://www.gavi.org/about/governance/gavi-board/minutes/2016/22-june/minutes/09---malaria-vaccine-pilots---appendices/\">GAVI, 2016</a>, p10; see also <a href=\"https://web.archive.org/web/20140407081405/http://www.gsk.com/media/press-releases/2013/malaria-vaccine-candidate-reduces-disease-over-18-months-of-foll.html\"> GSK, 2013</a>).</p>\n<h4><br>Ebola Vaccine</h4>\n<p>There are eight current vaccine candidates for Ebola being tested (<a href=\"https://en.wikipedia.org/wiki/Ebola_vaccine\">Wikipedia</a>). Merck has a vaccine, originally developed by the Canadian government, that has undergone a Phase III trial which demonstrated efficacy above 60% but that trial had several limitations and has not yet been licensed (<a href=\"https://www.genengnews.com/gen-news-highlights/merck-co-licenses-newlink-s-ebola-vaccine-candidate/81250631\"> GEN news, 2014</a>; <a href=\"https://www.wired.com/2015/08/100-percent-effective-means-ebola-vaccine/\">Palmer, 2015</a>; <a href=\"http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(16)32621-6/fulltext\">Henao-Restrepo et al., 2017</a>; <a href=\"https://www.nap.edu/read/24739/chapter/6\">NAP, 2017</a>). Several parallel vaccines are in different stages of development (<a href=\"https://www.sciencedirect.com/science/article/pii/S1521661616305368?via%3Dihub\">Pavot, 2016</a>). Total spending by GAVI, international governments, and pharmaceutical companies on vaccine development has been around $1.5B depending on accounting<strong><a href=\"#en7\">[7]</a></strong>.</p>\n<p>&#xA0;</p>\n<h4>HPV Vaccine</h4>\n<p>Despite thoroughly searching as much as we did for other vaccines, we were not able to find sufficiently credible information about the HPV vaccine such that we felt comfortable making an estimate<strong><a href=\"#en8\">[8]</a></strong>.<br><br></p>\n<h2>Guesstimate Model</h2>\n<p>We generally assume that the costs of developing a vaccine are equal to paying for salaries, paying for clinical trials, and paying for equipment and other overhead costs. Salaries and overhead costs would be a function of the number of employees, while clinical trial costs would be a function of the number of vaccines tried. When we attempt to do a very rough Fermi calculation of this, we end up with <a href=\"https://www.getguesstimate.com/models/8751\"> a model pointing to the cost of developing a vaccine</a>&#xA0;$460M to $1.9B with a mean of $960M.</p>\n<p>&#xA0;</p>\n<p><em>Next in the series: <a href=\"/ea/1l5/how_much_does_it_cost_to_rollout_a_vaccine/\">How much does it cost to roll-out a vaccine?</a></em></p>\n<p>&#xA0;</p>\n<h2>Endnotes&#xA0;</h2>\n<p><span><strong>[1]:</strong> Because the measles vaccine was developed so quickly by a single person in a time prior to massive costs from the modern clinical trials and licensing process, the costs of the measles vaccine were likely exceptionally low compared to all other vaccines. Additionally, the high rate of contagiousness from measles (<a href=\"https://www.cdc.gov/measles/about/transmission.html\">CDC, 2017</a>) combined with a moderate DALY penalty (<a href=\"http://www.who.int/healthinfo/global_burden_disease/GBD2004_DisabilityWeights.pdf\">WHO, 2004</a>) makes eliminating cases of measles particularly high value. The high benefit combined with the abnormally low cost makes the measles vaccine the best prima facie case for impact in vaccination.</span></p>\n<p>&#xA0;</p>\n<p><span><strong>[2]:</strong> According to <a href=\"https://www.glassdoor.com/Salary/GlaxoSmithKline-Salaries-E3477.htm\"> Glassdoor</a>,&#xA0;Associate Scientists at GSK make $62K/yr, Senior Scientists make $88K/yr, PMs and investigators and principal scientists make $110K/yr, and the director makes $160K/yr. At Merck, <a href=\"https://www.glassdoor.com/Salary/Merck-Salaries-E438.htm\"> Glassdoor says</a>&#xA0;assistant scientists make $62K-70K, scientist makes $81K/yr, principal scientist makes $139K/yr, and the director makes $175K/yr.</span></p>\n<p>&#xA0;</p>\n<p><span><strong>[3]:</strong> Of course, all of this overhead is there for good reason and there may have been considerable risk at testing a vaccine so quickly!</span></p>\n<p>&#xA0;</p>\n<p><span><strong>[4]:</strong> This is a complete guess, but I&#x2019;m unsure how this guess could be improved.</span></p>\n<p>&#xA0;</p>\n<p><span><strong>[5]:</strong> <a href=\"http://phrma-docs.phrma.org/sites/default/files/pdf/biopharmaceutical-industry-sponsored-clinical-trials-impact-on-state-economies.pdf\"> Battelle (2015)</a>&#xA0;finds that a typical trial done today costs $36,500 per participant when considering all costs holistically, but trials for infectious diseases were found to be half as expensive. Since I already account for staff and overhead costs in my estimate, I&#x2019;d cut this estimate in half again, and guess a &#x201C;modern day cost&#x201D; of ~$9K per participant. <a href=\"https://jscholarship.library.jhu.edu/bitstream/handle/1774.2/37064/MATHENY-DISSERTATION-2013.pdf\"> Matheny (2013)</a>&#xA0;(of EA fame!) finds a cost of $9500 per participant for vaccines in particular (p20), which after re-factoring out staff costs could support an estimate even lower than $9K per participant.</span></p>\n<p>&#xA0;</p>\n<p><span><strong>[6]:</strong> $9B to date as of 2011, plus $800-900M per year from 2011 to 2025-2030 implies a range of $20.2B to $26.1B in 2011 US dollars, or $21.6 to $27.9 in 2016 US dollars.</span></p>\n<p>&#xA0;</p>\n<p><span><strong>[7]:</strong> From the records we could find $1.5B in contributions for vaccine research and development -- GAVI contributed $300M (<a href=\"https://drive.google.com/open?id=14wOsVSXAHyJKNmmQhny_EJrrScjqHDf6\">GAVI, 2015</a>), Merck contributed $50M ( <a href=\"https://www.reuters.com/article/us-health-ebola-merck/merck-buys-rights-to-newlinks-experimental-ebola-vaccine-idUSKCN0J814A20141124\"> Pierson, 2004</a>), CanSino contributed $65M ( <a href=\"http://www.sixthtone.com/news/1000081/ebola-vaccine-maker-cansino-raises-%2465-million\"> Hruby, 2017</a>; <a href=\"http://www.bioworld.com/content/china-approves-ebola-vaccine-co-developed-cansino-biologics-and-military\">Liu, 2017</a>), the European Union contributed $282.4M (<a href=\"https://ec.europa.eu/research/health/pdf/ebola_research_overview.pdf\"> European Commission, 2015</a>), the Bill and Melinda Gates Foundation contributed $34M (Ibid.), Johnson &amp; Johnson contributed $287M (<a href=\"http://www.bavarian-nordic.com/media/media/news.aspx?news=4241\">Bavarian Nordic, 2014</a>; <a href=\"https://www.jnj.com/media-center/press-releases/johnson-johnson-advances-investigational-ebola-prime-boost-vaccine-regimen-with-new-partnership\"> Johnson &amp; Johnson, 2017</a>), USAID contributed $515M (European Commision, 2015), China has contributed $8.3M (<a href=\"http://www.telegraph.co.uk/news/worldnews/ebola/11179135/What-countries-have-pledged-to-fight-Ebola...-and-how-much-theyve-paid-into-the-fund.html\"> Sanchez, 2014</a>), and Russia has contributed $8M (<a href=\"https://www.theguardian.com/world/2016/jan/14/vladimir-putin-claims-ebola-virus-vaccine-has-been-developed-by-russia\">Presse, 2016</a> ; see also&#xA0;<a href=\"http://www.russianembassy.org/article/on-russias-contribution-to-international-efforts-to-fight-ebola\">Russian Federation press release</a>). Another aggregation confirms this $1.5B over 2014-2015 (<a href=\"https://fts.unocha.org/appeals/453/flows?order=directional_property&amp;sort=asc\">Financial Tracking Service</a>). Notably this likely does not include any funding from before 2010, which likely could mean we are missing funds and creating an underestimate, but we decided not to investigate this further.</span></p>\n<p>&#xA0;</p>\n<p><span><strong>[8]:</strong> The initial research into HPV vaccines was done by the University of Queensland, the University of Georgetown, the University of Rochester, and by the U.S. National Cancer Institute (<a href=\"https://academic.oup.com/jnci/article/98/7/433/2522050\">McNeil, 2006</a>). The U.S. National Institute of Health (the parent organization to the National Cancer Institute) contributed to all of these universities, and approximately 13% of HPV vaccine trials, granting ~<a href=\"https://docs.google.com/spreadsheets/d/1rqnVyMN3mi3g6thzqcRbeYcGIByaGF6hJ_Asm0BoDPM/edit#gid=0\">$34.9 million </a> (inflation unadjusted) since 1995 (<a href=\"http://www.dpcpsi.nih.gov/eo/intranet/opa/documents/C-15_Pritty_Joshi_Gardasil_poster_Final_C15/pdf\">Joshi, et. al.</a>). Building on this research two vaccines were licensed, Gardasil, marketed by Merck, and Cervarix, sold by GSK (<a href=\"https://www.cdc.gov/mmwr/preview/mmwrhtml/mm5920a4.htm\">CDC, 2010</a>). We are uncertain of how much spending Merck and Gardasil contributed to the development of these vaccines. Before the release of Cervarix, GSK purchased a company, Corixa, with relevant technology for some of their vaccines, including Cervarix, for ~$300 million but it&#x2019;s unclear how much of that valuation should be attributed to HPV (<a href=\"https://www.icis.com/resources/news/2005/05/06/675042/corixa-a-booster-for-gsk-s-vaccines-unit/\">ICIS, 2005</a>; <a href=\"https://www.businesswire.com/news/home/20050429005567/en/Corixa-Announces-Agreement-Acquired-GlaxoSmithKline-Shareholders-Receive\"> BusinessWire, 2005</a>). There are public estimates of how much R&amp;D cost recovery was anticipated for Merck and GSK through their HPV vaccines but this is based in part on how large the market was expected to be for the vaccines, how much money these companies typically spend on R&amp;D, and how much they were anticipating they would charge per vaccine, instead of an estimate of how much money they actually spent developing the vaccines ( <a href=\"http://www.cptech.org/ip/health/prizefund/files/outterson-buyouts.pdf\"> Outterson, 2006</a>; <a href=\"https://www.healthaffairs.org/doi/full/10.1377/hlthaff.27.1.130\"> Outterson and Kesselheim, 2006</a>). Finally, the Bill and Melinda Gates Foundation spent $27.8 million to help LMIC understand how to implement the vaccine effectively after it was first rolled out (<a href=\"http://www.path.org/news/press-room/601/\">PATH, 2006</a>). Unfortunately, we&#x2019;re not able to figure out how to aggregate all this raw data into an accounting we feel captures all the costs without leaving out anything significant.</span></p></body></html>", "user": {"username": "Peter_Hurford"}}, {"_id": "6kFcr6L2jZhcsZnao", "title": "Announcing Effective Altruism Community Building Grants", "postedAt": "2018-02-22T17:09:18.593Z", "htmlBody": "<html><body><h1><span>Announcing Effective Altruism Community Building Grants<br></span></h1>\n<p>I&#x2019;m announcing a new project from the Centre for Effective Altruism: Effective Altruism Community Building Grants. This program will provide grants of between $5,000 and $100,000 to individuals and groups doing local effective altruism community building work. We expect to have a particular emphasis on funding groups aiming to transition from being run by volunteers to being run by full-time, paid organizers.&#xA0;We are currently in talks with LEAN about how we will collaborate on administering/implementing grants.</p>\n<p>&#xA0;</p>\n<p>The aim of this project is to build the capacity of the effective altruism community to produce value, by strengthening a small number of high-potential effective altruism groups. We believe that community building grants will help achieve this aim for the following reasons:</p>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p>Providing these grants will enable the people with the most expertise and personal fit for local community building to significantly increase the time they can dedicate to building the highest potential effective altruism groups.</p>\n</li>\n<li>\n<p>Increasing the groups&#x2019; organizational capacity in this way will significantly increase the number of highly talented people dedicated to working on the most important problems, as a result of understanding the ideas and methodology of effective altruism (<a href=\"http://beta.charitycommission.gov.uk/charity-details/?regid=1149828&amp;subid=0\">furthering CEAs charitable object No. 2</a>).</p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<p>Providing grants for effective altruism community builders promises to be a scalable way of using funding to increase the talent pool available to the effective altruism community. Upon completing the first round of effective altruism community building grants, we expect to further increase our understanding of both the specific contribution that effective altruism groups can make to the aims of the community, and the role that additional funding can play in strengthening effective altruism groups.</p>\n<p>&#xA0;</p>\n<p>Though we expect the typical application to be for a grant to cover full-time or part-time work organizing an effective altruism group, we are open to grant applications which are outside of this scope but are focused on the aim of creating successful local effective altruism communities.</p>\n<p>&#xA0;</p>\n<h1>Current Landscape</h1>\n<p>CEA&#x2019;s mission is to improve humanity&#x2019;s long-term trajectory by building a community that can work together to solve humanity&#x2019;s biggest problems. Right now, there are three main mechanisms by which individual EAs can come together as a community: online (e.g. <a href=\"/\">EA Forum</a>, <a href=\"https://www.facebook.com/groups/effective.altruists/\">Facebook</a>), yearly conferences (e.g. <a href=\"https://www.eaglobal.org/\">EA Global</a>) and local groups (e.g. university groups like <a href=\"http://www.harvardea.org/\">Harvard EA</a> and community groups like <a href=\"http://ealondon.com/\">EA London</a>).</p>\n<p>&#xA0;</p>\n<p>Local groups provide the only option for regular, in-person interaction within the EA community. We think this is likely to be essential to a thriving EA community for at least three reasons:</p>\n<p>&#xA0;</p>\n<ul>\n<li>Regular in-person interactions are essential to establishing informal information networks, which are particularly useful as a way to learn things you didn&#x2019;t know you needed to know.</li>\n<li>Regular in-person interactions facilitate strong connections within community, and these connections are likely to be essential to helping many people remain stably motivated to work on improving the world.</li>\n<li>Regular in-person interaction is a good method for introducing new people to the community.<br><br></li>\n</ul>\n<p>The impact of local groups appears to follow a heavy-tailed distribution, with some of the larger, more established groups producing substantially more value than the average local group. Historically, much of the local group support provided by CEA, Rethink Charity, EAF and others has focused on either seeding local groups or providing a small amount of support to a large number of different groups. EA Community Building Grants are an experiment in shifting our focus towards making some of the most promising local groups even better.</p>\n<p>&#xA0;</p>\n<p>In the past, CEA has considered similar plans, but ultimately decided not to push forward for three reasons:</p>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p>EA community size</p>\n</li>\n<ul>\n<li>\n<p>The effective altruism community used to be much smaller than it is now, both in terms of the number of people who identified with the aims of the community, and the reach of the ideas of effective altruism.</p>\n</li>\n<li>\n<p>Local groups need to be able to reach a critical mass of regular attendees to succeed. As the number of people in the community has grown, reaching critical mass has become easier, resulting in more groups gaining significant traction.</p>\n</li>\n</ul>\n</ul>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p>High opportunity cost for group leaders</p>\n</li>\n<ul>\n<li>\n<p>The best way for group leaders to attract excellent people to their group is for them to be personally excellent themselves.</p>\n</li>\n<li>\n<p>However, this means that the best group leaders are also likely to have the highest opportunity costs. We weren&#x2019;t sure that we knew enough about the overall value of local groups to know when a group leader should focus on their local group full time instead of pursuing other options.</p>\n</li>\n</ul>\n<li>\n<p>Entrenchment</p>\n</li>\n<ul>\n<li>\n<p>When a stable local group forms, it&#x2019;s typically the case that no one else attempts to form a competing local group. This is generally quite positive and represents good norms of cooperation rather than defection.</p>\n</li>\n<li>\n<p>However, this creates a lock-in effect whereby the first stable local group crowds out the possibility of different groups forming in the future. Funding full-time organizers exacerbates this effect.</p>\n</li>\n<li>\n<p>As the EA community grows, the importance of local groups will grow as will the quality of the local organizer talent pool.</p>\n</li>\n<li>\n<p>This means that funding a local group today could mean that we can&#x2019;t fund a better instantiation of that group in the future.</p>\n</li>\n</ul>\n</ul>\n<p>&#xA0;</p>\n<p>A few things have changed such that we think now is a good time to provide funding for local groups that want to professionalize.</p>\n<p>&#xA0;</p>\n<p>First, the size of the EA community has grown. Local groups serve as an early contact point for people who learn about EA. Therefore as the reach of the ideas of effective altruism grows larger, more people are learning about EA and the more valuable it is for local groups to serve as a first point of contact with the community.</p>\n<p>&#xA0;</p>\n<p>Second, we&#x2019;ve been increasingly impressed with the productive output of some of the local groups we&#x2019;ve talked to. This makes it more likely that running a local group is better than the alternative.</p>\n<p>&#xA0;</p>\n<p>Third, we&#x2019;ve gained a better understanding of what constitutes a promising local group and <a href=\"https://app.effectivealtruism.org/groups/resources/effective-altruism-community-building\">how local groups can produce value</a>. For example, we&#x2019;ve come to believe that a major comparative advantage of effective altruism groups is in supporting group members to develop high-value future plans, and that groups can achieve this aim more effectively by building strong (rather than simply large) communities.</p>\n<p>&#xA0;</p>\n<p>Fourth, the available funding in the EA community has increased whereas <a href=\"https://80000hours.org/2017/11/talent-gaps-survey-2017/\">important talent gaps remain</a>. This project may be able to turn money into talent which makes it especially promising.</p>\n<p>&#xA0;</p>\n<p>Finally, we think we can set clear expectations with the groups that we fund such that the project is viewed as an experiment and that part of the goal of the experiment is to determine whether the local group leaders should continue working on community-building full time.</p>\n<p>&#xA0;</p>\n<h1>The Ideal Candidate</h1>\n<p>In making funding decisions, we&#x2019;re looking for two main things: exceptional organizers, and high-potential groups.</p>\n<p>&#xA0;</p>\n<h2>The Organizers</h2>\n<p>In the simplest terms, the heuristic for evaluating organizers is &#x201C;Would we be excited to replicate these people 10x or 100x in the EA community?&#x201D; This heuristic is inspired by <a href=\"http://startupclass.samaltman.com/courses/lec10/\">YC&#x2019;s advice on hiring and culture</a>, and we think it encapsulates the importance of local community builders in setting the culture and trajectory of the EA community.</p>\n<p>&#xA0;</p>\n<p>More specifically, we expect the organizers to have the following characteristics:</p>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p>EA expertise</p>\n</li>\n<ul>\n<li>\n<p>If you want to spread EA you should first be an expert on EA.</p>\n</li>\n<li>\n<p>This doesn&#x2019;t mean that you need to be a cutting edge researcher. It does mean that you should have a thorough, nuanced understanding of the core EA ideas, and that you should be able to map out the considerations surrounding questions like what cause area to support, or what careers to consider.</p>\n</li>\n</ul>\n<li>\n<p>Adherence to the <a href=\"https://www.centreforeffectivealtruism.org/ceas-guiding-principles/\">guiding principles of effective altruism</a></p>\n</li>\n<ul>\n<li>\n<p>We want to fund organizers who adhere to the core principles of commitment to others, scientific mindset, openness, integrity and collaborative spirit.</p>\n</li>\n</ul>\n<li>\n<p>Judgment</p>\n</li>\n<ul>\n<li>\n<p>We aim to take a relatively hands-off approach to the groups we fund. This means that we will tend to prefer organizers who we trust will tend to make reasonable, well-considered decisions and who display appropriate caution about difficult or risky decisions.</p>\n</li>\n</ul>\n<li>\n<p>Skill at EA community building</p>\n</li>\n<ul>\n<li>\n<p>We&#x2019;ll also look for evidence that organizers possess skill at EA community building. The core heuristic for assessing this is &#x201C;Will this person be able to attract talented, dedicated people to make major contributions on the most important problems?&#x201D;</p>\n</li>\n<li>\n<p>There are many different ways of meeting this heuristic, but we expect to evaluate areas like social skill, executional competence, enthusiasm, and thoughtfulness, among others.</p>\n</li>\n</ul>\n<li>\n<p>Understanding of EA community building</p>\n</li>\n<ul>\n<li>\n<p>EA community building has the potential for being both highly valuable, but also harmful if done poorly. Organizers should have a very strong understanding of the relevant considerations determining the value of EA community building.</p>\n</li>\n</ul>\n</ul>\n<p><br><br></p>\n<h2>The Group</h2>\n<p>We&#x2019;re looking for groups that are likely to have a strong potential to flourish given a significant increase in their resources.</p>\n<p>&#xA0;</p>\n<p>Building on our experience at Y Combinator and their emphasis on building services for your 100 most dedicated users, we think one of the most important qualities of high-potential groups is the depth of engagement and understanding of the groups&#x2019; core members. This is intended to be a departure from assessing groups based on metrics related to size, such as event attendance or number of members.</p>\n<p>&#xA0;</p>\n<p>We are looking for groups with the potential to facilitate the development of people who contribute valuable work to high-priority areas, Specifically, the features that we expect to see are:</p>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p>An inner core of well-coordinated organizers</p>\n</li>\n<ul>\n<li>\n<p>Successful local groups need to be able to attract a small inner core of people who can help make the group succeed. The inner core typically consists of people who aren&#x2019;t working on the group full time, but attend events regularly, are willing to pitch in to help when needed and who the leaders can turn to for support and advice.</p>\n</li>\n</ul>\n<li>\n<p>Stable, highly-engaged group members</p>\n</li>\n<ul>\n<li>\n<p>This is a group of people who attend fairly regularly, don&#x2019;t generally help with organizing events, but who are otherwise highly engaged and derive significant value from the group.</p>\n</li>\n</ul>\n<li>\n<p>A goal-oriented activity portfolio</p>\n</li>\n<ul>\n<li>\n<p>This will consist of activities and projects which have been chosen on the basis of a developing understanding of the goals of the effective altruism community, and how they will contribute to achieving these goals.</p>\n</li>\n</ul>\n<li>\n<p>A record of past success in attracting talented, dedicated people to make major contributions to the most important problems.</p>\n</li>\n<ul>\n<li>\n<p>This includes looking at whether people currently in the group seem like they could make such a contribution in the future.</p>\n</li>\n</ul>\n</ul>\n<p>&#xA0;</p>\n<h1>The Grants</h1>\n<p>The grant period lasts for between three months and a year, and applicants may choose to delay the start of the grant period until the start of September 1st, 2018. By default, grants will be paid out on a quarterly basis, although there is flexibility in the payment schedule to accommodate the needs of grantees. Grant recipients will be required to write a brief quarterly report, including an itemized list of expenditures, as well as periodically checking in with CEA. Groups will also be required to write a full report at the end of the grant period.</p>\n<p>&#xA0;</p>\n<p>For applications for a grant to cover full-time work organizing an effective altruism group, the default grant will be $35,000 to cover the living costs of one organizer over the course of a year, in additional to $10,000 for general expenses related to the group itself. For applications for funding below $5,000, see the <a href=\"https://app.effectivealtruism.org/groups/resources/mentorship-and-funding\">effective altruism groups funding guide</a> instead. Individual or joint applications may be submitted.</p>\n<p>&#xA0;</p>\n<p>CEA will only be able to make grants which further some of its charitable objects, which are stated within the application form. CEA is not an employer of those to whom it awards grants, and reserves the right to terminate payment of the grants should they deviate from the agreed-upon terms.</p>\n<p>&#xA0;</p>\n<h1>Applications</h1>\n<p>Applications will open on February 21st and close on March 21st 2018. Applicants will be asked to include their CV, an overview of their group (where relevant), and a proposal for their project or group. &#xA0;Applicants will receive a confirmation of submission email, and will be contacted within two weeks letting them know whether they will be invited to interview, which will take place within three further weeks. Interviewees will receive notification of the status of their application within three weeks of interview, and successful applicants will receive a grant offer specifying the terms and expectations of the grant.</p>\n<p>&#xA0;</p>\n<p>If an application is particularly urgent, or you have any queries, please contact groups@effectivealtruism.org<br><br>We&apos;re happy to&#xA0;answer questions from potential applicants about the process, either via email or in a call.&#xA0;</p>\n<p><br><br></p>\n<p><a href=\"https://cea-core.typeform.com/to/CJLJCS?source=xxxxx\">Apply here<br><br></a></p>\n<p>Edits:</p>\n<p>1) &apos;We are currently in talks with LEAN about how we will collaborate on administering/implementing grants&apos; added to paragraph 1.&#xA0;<br>2) &apos;We&apos;re happy to&#xA0;answer questions from potential applicants about the process, either via email or in a call.&apos; Added to the final paragraph</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "hbesceli"}}, {"_id": "W9BauyoRzdcyzt32M", "title": "Save the dates for EA Global: San Francisco & London", "postedAt": "2018-02-22T16:16:11.142Z", "htmlBody": "<html><body><p><span>There will be two Effective Altruism Global conferences this year:</span></p>\n<p><span>-</span><span> June 8-10:</span><span> Bespoke</span><span>, 845 Market Street, San Francisco, CA</span><span>, USA</span></p>\n<p><span>-</span><span> End of October:</span><span> London, UK</span></p>\n<p><span>The goals of both these events are to increase attendees&#x2019; knowledge, skills and networks to enable them to do the most good possible. </span></p>\n<p><span>Content will be aimed at existing EA community members who already have a solid understanding of effective altruism, but would like to gain skills, network, master more complex problems, or move into new roles. </span></p>\n<p><span>Programming will include:</span></p>\n<p><span>- Presentations of papers on the latest ideas at the frontiers of the EA movement</span><br><span>- Whiteboard sessions (collaborative discussions between experts)</span><br><span>- Seminars and workshops to help improve your thinking and execution </span><br><span>- Facilitated networking sessions, office hours and social activities</span></p>\n<p><span>Who is EA Global a good fit for?</span></p>\n<p><span>We think the following kinds of attendees are particularly likely to find the conference valuable:</span></p>\n<p><span>- People already familiar with the key ideas of effective altruism </span><br><span>- People seeking the opportunity to master more complex problems in effective altruism, learn new skills or get help from the community</span><br><span>- People actively seeking work focused on high-priority global issues (examples here: </span><a href=\"http://e-a.io/causes\"><span>http://e-a.io/causes</span></a><span>). </span></p>\n<p><span>We use an application process to match our available spaces with the people we think will gain the most from the conferences. If you are new to EA, regional </span><a href=\"https://www.eaglobal.org/eagx/\"><span>EA Global X</span></a><span> conferences are likely to be a better fit.</span></p>\n<p><span>You can check out some other people&#x2019;s takes here:</span></p>\n<p><span>- </span><span><a href=\"/ea/19v/why_you_should_consider_going_to_ea_global/\">Why you should consider going to EA Global</a></span><span>, Konrad Seifert</span></p>\n<p><span>- </span><span><a href=\"/ea/11n/reflections_on_ea_global_from_a_firsttime_attendee/\">Reflections on EA Global from a first-time attendee</a></span><span>, Ian David Moss</span></p>\n<p><span>- </span><a href=\"http://slatestarcodex.com/2017/08/16/fear-and-loathing-at-effective-altruism-global-2017/\"><span>Fear and Loathing at Effective Altruism Global 2017</span></a><span>, Scott Alexander</span></p>\n<p>&#xA0;</p>\n<p><span>Next steps</span></p>\n<p><span>We will open applications for these events shortly.</span></p>\n<p><span>In the meantime:</span></p>\n<p><span>- Subscribe to our </span><a href=\"https://www.eaglobal.org/join/\"><span>newsletter</span></a><span> to be notified when applications are open </span><br><span>- Join the </span><a href=\"https://www.facebook.com/groups/eaglobal/about/\"><span>EA Global group on Facebook</span></a><br><span>- Submit topic and speaker recommendations </span><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfCfGs2T-Fp3ZgCc9rSGjwAY2wThes7XPY21nxR8u8nannK-Q/viewform\"><span>here</span></a><span> &#x2014; please feel free to nominate yourself!</span><br><span>- Stay tuned for details about the community-hosted </span><a href=\"https://www.eaglobal.org/eagx/\"><span>EA Global X events</span></a><span> taking place in Australia, Europe, and on the east coast of the US in 2018</span></p>\n<p><span>If you have any questions you can reach me and Katie Glass at </span><a href=\"mailto:hello@eaglobal.org\"><span>hello@eaglobal.org</span></a><span>.</span></p></body></html>", "user": {"username": "AmyLabenz"}}, {"_id": "BY8gXSpGijypbGitT", "title": "Why I prioritize moral circle expansion over reducing extinction risk through artificial intelligence alignment", "postedAt": "2018-02-20T18:29:12.819Z", "htmlBody": "<p><i>This blog post was written on an old version of the EA Forum, so it has formatting issues. The title was also updated for clarity.</i></p><p><i>Many thanks for helpful feedback to Jo Anderson, Tobias Baumann, Jesse Clifton, Max Daniel, Michael Dickens, Persis Eskander, Daniel Filan, Kieran Greig, Zach Groff, Amy Halpern-Laff, Jamie Harris, Josh Jacobson, Gregory Lewis, Caspar Oesterheld, Carl Shulman, Gina Stuessy, Brian Tomasik, Johannes Treutlein, Magnus Vinding, Ben West, and Kelly Witwicki. I also forwarded Ben Todd and Rob Wiblin a small section of the draft that discusses an 80,000 Hours article.</i></p><h1>Abstract</h1><p>When people in the effective altruism (EA) community have worked to affect the far future, they\u2019ve typically focused on reducing extinction risk, especially risks associated with superintelligence or general artificial intelligence alignment (AIA). I agree with the arguments for the far future being extremely important in our EA decisions, but I tentatively favor improving the quality of the far future by expanding humanity\u2019s moral circle more than increasing the likelihood of the far future or humanity\u2019s continued existence by reducing AIA-based extinction risk because: (1) the far future seems to not be very good in expectation, and there\u2019s a significant likelihood of it being very bad, and (2) moral circle expansion seems highly neglected both in EA and in society at large. Also, I think considerations of bias are very important here, given how necessarily intuitive and subjective judgment calls make up the bulk of differences in opinion on far future cause prioritization. I find the argument in favor of AIA that technical research might be more tractable than social change to be the most compelling counterargument to my position.</p><h1>Context</h1><p>This post largely aggregates existing content on the topic, rather than making original arguments. I offer my views, mostly intuitions, on the various arguments, but of course I remain highly uncertain given the limited amount of empirical evidence we have on far future <a href=\"https://causeprioritization.org/\">cause prioritization</a>.</p><p><a href=\"/ea/1fi/have_ea_priorities_changed_over_time/\">Many</a> in the effective altruism (EA) community think the <a href=\"https://ea-foundation.org/blog/the-importance-of-the-far-future/\">far future</a> is a very important consideration when working to do the most good. The basic argument is that humanity could continue to exist for a very long time and could expand its civilization to the stars, creating a very large amount of moral value. The main narrative has been that this civilization could be a very good one, and that in the coming decades, we face sizable risks of extinctions that could prevent us from obtaining this <a href=\"http://www.existential-risk.org/concept.html\">\u201ccosmic endowment.\u201d</a> The argument goes that these risks also seem like they can be reduced with a fairly small amount of additional resources (e.g. time, money), and therefore extinction risk reduction is one of the most important projects of humanity and the EA community.</p><p>(This argument also depends on a moral view that bringing about the existence of sentient beings can be a morally good and important action, comparable to helping sentient beings who currently exist live better lives. This is a contentious view in academic philosophy. See, for example, <a href=\"https://dash.harvard.edu/bitstream/handle/1/13064981/Frick_gsas.harvard.inactive_0084L_11842.pdf\">\u201c'Making People Happy, Not Making Happy People': A Defense of the Asymmetry Intuition in Population Ethics.\u201d</a>)</p><p>However, one can accept the first part of this argument \u2014 that there is a very large amount of expected moral value in the far future and it\u2019s relatively easy to make a difference in that value \u2014 <a href=\"http://lesswrong.com/lw/hjb/a_proposed_adjustment_to_the_astronomical_waste/\">without deciding</a> that extinction risk is the most important project. In <a href=\"/ea/t3/some_considerations_for_different_ways_to_reduce/\">slightly different terms</a>, one can decide not to work on reducing population risks, risks that could reduce the number of morally relevant individuals in the far future (of course, these are only risks of harm if one believes more individuals is a good thing), and instead work on reducing quality risks, risks that could reduce the quality of morally relevant individuals\u2019 existence. One specific type of quality risk often discussed is a <a href=\"https://foundational-research.org/risks-of-astronomical-future-suffering/\">risk of astronomical suffering</a> (s-risk), defined as \u201cevents that would bring about suffering on an astronomical scale, vastly exceeding all suffering that has existed on Earth so far.\u201d</p><p>This blog post makes the case for focusing on quality risks over population risks. More specifically, though also more tentatively, it makes the case for focusing on reducing quality risk through <a href=\"/ea/1b1/introducing_sentience_institute/\">moral circle expansion</a> (MCE), the strategy of impacting the far future through increasing humanity\u2019s concern for sentient beings who currently receive little consideration (i.e. widening our moral circle so it includes them), over <a href=\"https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/\">AI alignment</a> (AIA), the strategy of impacting the far future through increasing the likelihood that humanity creates an artificial general intelligence (AGI) that behaves as its designers want it to (known as the alignment problem).[1][2]</p><p>The basic case for MCE is very similar to the case for AIA. Humanity could continue to exist for a very long time and could expand its civilization to the stars, creating a very large number of sentient beings. The sort of civilization we create, however, seems highly dependent on our moral values and moral behavior. In particular, it\u2019s uncertain whether many of those sentient beings will receive the moral consideration they deserve based on their sentience, i.e. whether they will be in our <a href=\"https://press.princeton.edu/titles/9434.html\">\u201cmoral circle\u201d</a> or not, like the many sentient beings who have suffered intensely over the course of human history (e.g. from torture, genocide, oppression, war). It seems the moral circle can be expanded with a fairly small amount of additional resources (e.g. time, money), and therefore MCE is one of the most important projects of humanity and the EA community.</p><p>Note that MCE is a specific kind of <a href=\"http://reducing-suffering.org/values-spreading-often-important-extinction-risk/\">values spreading</a>, the parent category of MCE that describes any effort to shift the values and moral behavior of humanity and its decendants (e.g. intelligent machines) in a positive direction to benefit the far future. (Of course, some people attempt to spread values in order to benefit the near future, but in this post we\u2019re only considering far future impact.)</p><p>I\u2019m specifically comparing MCE and AIA because AIA is probably the most favored method of reducing extinction risk in the EA community. AIA seems to be the default cause area to favor if one wants to have an impact on the far future, and I\u2019ve been asked several times why I favor MCE instead.</p><p>This discussion risks conflating AIA with reducing extinction risk. These are two separate ideas, since an unaligned AGI could still lead to a large number of sentient beings, and an aligned AGI could still potentially cause extinction or population stagnation (e.g. if according to the designers\u2019 values, even the best civilization the AGI could help build is still worse than nonexistence). However, most EAs focused on AIA seem to believe that the main risk is something quite like extinction, such as the textbook example of an AI that seeks to maximize the number of paperclips in the universe. I\u2019ll note when the distinction between AIA and reducing extinction risk is relevant. Similarly, there are sometimes important prioritization differences between MCE and other types of values spreading, and those will be noted when they matter. (This paragraph is an important qualification for the whole post. The possibility of unaligned AGI that involves a civilization (and, less so because it seems quite unlikely, the possibility of an AGI that causes extinction) is important to consider for far future cause prioritization. Unfortunately, elaborating on this would make this post far more complicated and far less readable, and would not change many of the conclusions. Perhaps I\u2019ll be able to make a second post that adds this discussion at some point.)</p><p>It\u2019s also important to note that I\u2019m discussing specifically AIA here, not all AI safety work in general. AI safety, which just means increasing the likelihood of beneficial AI outcomes, could be interpreted as including MCE, since MCE plausibly makes it more likely that an AI would be built with good values. However, MCE doesn\u2019t seem like a very plausible route to increasing the likelihood that AI is simply aligned with the intentions of its designers, so I think MCE and AIA are fairly distinct cause areas.</p><p>AI safety can also include work on reducing s-risks, such as specifically reducing the likelihood of an unaligned AI that causes astronomical suffering, rather than reducing the likelihood of all unaligned AI. I think this is an interesting cause area, though I am unsure about its tractability and am not considering it in the scope of this blog post.</p><p>The post\u2019s publication was supported by Greg Lewis, who was interested in this topic and donated $1,000 to <a href=\"https://www.sentienceinstitute.org/\">Sentience Institute</a>, the think tank I co-founded which researches effective strategies to expand humanity\u2019s moral circle, conditional on this post being published to the Effective Altruism Forum. Lewis doesn\u2019t necessarily agree with any of its content. He decided on the conditional donation prior to the post being written, and I did ask him to review the post prior to publication and it was edited based on his feedback.</p><h1>The expected value of the far future</h1><p>Whether we prioritize reducing extinction risk partly depends on how good or bad we expect human civilization to be in the far future, given it continues to exist. In my opinion, the assumption that it will be very good is a tragically unexamined assumption in the EA community.</p><h2>What if it\u2019s close to zero?</h2><p>If we think the far future is very good, that clearly makes reducing extinction risk more promising. And if we think the far future is very bad, that makes reducing extinction risk not just unpromising, but actively very harmful. But what if it\u2019s near the middle, i.e. close to zero?[3] 80,000 Hours <a href=\"https://80000hours.org/articles/extinction-risk/#who-shouldnt-prioritise-safeguarding-the-future\">wrote</a> that to believe reducing extinction risk is not an EA priority on the basis of the expected moral value of the far future,</p><pre><code>...even if you\u2019re not sure how good the future will be, or suspect it will be bad, you may want civilisation to survive and keep its options open. People in the future will have much more time to study whether it\u2019s desirable for civilisation to expand, stay the same size, or shrink. If you think there\u2019s a good chance we will be able to act on those moral concerns, that\u2019s a good reason to leave any final decisions to the wisdom of future generations. Overall, we\u2019re highly uncertain about these big-picture questions, but that generally makes us more concerned to avoid making any irreversible commitments...</code></pre><p>This reasoning seems mistaken to me because wanting \u201ccivilisation to survive and keep its options open\u201d depends on optimism that civilization will do research, make good[4] decisions based on that research, and be capable of implementing those decisions.[5] In other words, while preventing extinction keeps options open for good things to happen, it also keeps options open for bad things to happen, and desiring this option value depends on an optimism that the good things are more likely. In other words, the reasoning assumes the optimism (thinking the far future is good, or at least that humans will make good decisions and be able to implement them[6]), which is also its conclusion.</p><p>Having that optimism makes sense in many decisions, which is why keeping options open is often a good heuristic. In EA, for example, people tend to do good things with their careers, which means career option value is a useful thing. This doesn\u2019t readily translate to decisions where it\u2019s not clear whether the actors involved will have a positive or negative impact. (Note 80,000 Hours isn\u2019t making this comparison. I\u2019m just making it to explain my own view here.)</p><p>There\u2019s also a sense in which preventing extinction risk decreases option value because if humanity progresses past certain civilizational milestones that make extinction more unlikely \u2014 say, the rise of AGI or expansion beyond our own solar system \u2014 it might become harder or even impossible to press the \u201coff switch\u201d (ending civilization). However, I think most would agree that there\u2019s more overall option value in a civilization that has gotten past these milestones because there\u2019s a much wider variety of non-extinct civilizations than extinct civilizations.[7]</p><p>If you think that the expected moral value of the far future is close to zero, even if you think it\u2019s slightly positive, then reducing extinction risk is a less promising EA strategy than if you think it\u2019s very positive.</p><h2>Key considerations</h2><p>I think the considerations on this topic are best represented as questions where people\u2019s beliefs (mostly just intuitions) vary on a long spectrum. I\u2019ll list these in order of where I would guess I have the strongest disagreement with people who believe the far future is highly positive in expected value (shortened as HPEV-EAs), and I\u2019ll note where I don\u2019t think I would disagree or might even have a more positive-leaning belief than the average such person.</p><ol><li>I think there\u2019s a significant[8] chance that the moral circle will fail to expand to reach all sentient beings, such as artificial/small/weird minds (e.g. a sophisticated computer program used to mine asteroids, but one that doesn\u2019t have the normal features of sentient minds like facial expressions). In other words, I think there\u2019s a significant chance that powerful beings in the far future will have low <a href=\"https://en.wikipedia.org/wiki/Willingness_to_pay\">willingness to pay</a> for the welfare of many of the small/weird minds in the future.[9]</li><li>I think it\u2019s likely that the powerful beings in the far future (analogous to humans as the powerful beings on Earth in 2018) will use large numbers of less powerful sentient beings, such as for recreation (e.g. safaris, war games), a labor force (e.g. colonists to distant parts of the galaxy, construction workers), scientific experiments, threats, (e.g. threatening to create and torture beings that a rival cares about), revenge, justice, religion, or even pure sadism.[10] I believe this because there have been less powerful sentient beings for all of humanity\u2019s existence and well before (e.g. predation), many of whom are exploited and harmed by humans and other animals, and there seems to be little reason to think such power dynamics won\u2019t continue to exist.</li><li>Alternative uses of resources include simply working to increase one\u2019s own happiness directly (e.g. changing one\u2019s neurophysiology to be extremely happy all the time), and constructing large non-sentient projects like a work of art. Though each of these types of project could still include sentient beings, such as for experimentation or a labor force.</li><li>With the exception of threats and sadism, the less powerful minds seem like they could suffer intensely because their intense suffering could be instrumentally useful. For example, if the recreation is nostalgic, or human psychology persists in some form, we could see powerful beings causing intense suffering in order to see good triumph over evil or in order to satisfy curiosity about situations that involve intense suffering (of course, the powerful beings might not acknowledge the suffering as suffering, instead conceiving of it as simulated but not actually experienced by the simulated entities). For another example, with a sentient labor force, punishment could be a stronger motivator than reward, as indicated by the history of evolution on Earth.[11][12]</li><li>I place significant moral value on artificial/small/weird minds.</li><li>I think it\u2019s quite unlikely that human descendants will find the correct morality (in the sense of <a href=\"https://en.wikipedia.org/wiki/Moral_realism\">moral realism</a>, finding these mind-independent moral facts), and I don\u2019t think I would care much about that correct morality even if it existed. For example, I don\u2019t think I would be compelled to create suffering if the correct morality said this is what I should do. Of course, such moral facts are very difficult to imagine, so I\u2019m quite uncertain about what my reaction to them would be.[13]</li><li>I\u2019m skeptical about the view that technology and efficiency will remove the need for powerless, high-suffering, instrumental moral patients. An example of this predicted trend is that factory farmed animals seem unlikely to be necessary in the far future because of their <a href=\"http://awfw.org/feed-ratios/\">inefficiency at producing animal products</a>. Therefore, I\u2019m not particularly concerned about the factory farming of biological animals continuing into the far future. I am, however, concerned about similar but less inefficient systems.</li><li>An example of how technology might not render sentient labor forces and other instrumental sentient beings obsolete is how humans seem motivated to have power and control over the world, and in particular seem more satisfied by having power over other sentient beings than by having power over non-sentient things like barren landscapes.</li><li>I do still believe there\u2019s a strong tendency towards efficiency and that this has the potential to render much suffering obsolete; I just have more skepticism about it than I think is often assumed by HPEV-EAs.[14]</li><li>I\u2019m skeptical about the view that human descendants will optimize their resources for happiness (i.e. create hedonium) relative to optimizing for suffering (i.e. create dolorium).[15] Humans currently seem more deliberately driven to create hedonium, but creating dolorium might be more instrumentally useful (e.g. as a threat to rivals[16]).</li><li>On this topic, I similarly do still believe there\u2019s a higher likelihood of creating hedonium; I just have more skepticism about it than I think is often assumed by EAs.</li><li>I\u2019m largely in agreement with the average HPEV-EA in my moral exchange rate between happiness and suffering. However, I think those EAs tend to greatly underestimate how much the empirical tendency towards suffering over happiness (e.g. wild animals seem to endure <a href=\"https://foundational-research.org/the-importance-of-wild-animal-suffering/\">much more suffering than happiness</a>) is evidence of a future empirical asymmetry.</li><li>My view here is partly informed by the capacities for happiness and suffering that have evolved in humans and other animals, the capacities that seem to be driven by cultural forces (e.g. corporations seem to care more about downsides than upsides, perhaps because it\u2019s easier in general to destroy and harm things than to create and grow them), and speculation about what could be done in more advanced civilizations, such as my best guess on what a planet optimized for happiness and a planet optimized for suffering would look like. For example, I think a given amount of dolorium/dystopia (say, the amount that can be created with 100 joules of energy) is far larger in absolute moral expected value than hedonium/utopia made with the same resources.</li><li>I\u2019m unsure of how much I would disagree with HPEV-EAs about the argument that we should be highly uncertain about the likelihood of different far future scenarios because of how highly speculative our evidence is, which pushes my estimate of the expected value of the far future towards the middle of the possible range, i.e. towards zero.</li><li>I\u2019m unsure of how much I would disagree with HPEV-EAs about the persistence of evolutionary forces into the future (i.e. how much future beings will be determined by fitness, rather than characteristics we might hope for like altruism and happiness).[17]</li><li>From the historical perspective, it worries me that many historical humans seem like they would be quite unhappy with the way human morality changed after them, such as the way Western countries are less concerned about previously-considered-immoral behavior like homosexuality and gluttony than their ancestors were in 500 CE. (Of course, one might think historical humans would agree with modern humans upon reflection, or think that much of humanity\u2019s moral changes have been due to improved empirical understanding of the world.)[18]</li><li>I\u2019m largely in agreement with HPEV-EAs that humanity\u2019s moral circle has a track record of expansion and seems likely to continue expanding. For example, I think it\u2019s quite likely that powerful beings in the far future will care a lot about charismatic biological animals like elephants or chimpanzees, or whatever beings have a similar relationship to those powerful beings as humanity has to elephants and chimpanzees. (As mentioned above, my pessimism about the continued expansion is largely due to concern about the magnitude of bad-but-unlikely outcomes and the harms that could occur due to MCE stagnation.)</li></ol><p>Unfortunately, we don\u2019t have much empirical data or solid theoretical arguments on these topics, so the disagreements I\u2019ve had with HPEV-EAs have mostly just come down to differences in intuition. This is a common theme for prioritization among far future efforts. We can outline the relevant factors and a little empirical data, but the crucial factors seem to be left to speculation and intuition.</p><p>Most of these considerations are about how society will develop and utilize new technologies, which suggests we can develop relevant intuitions and speculative capacity by studying social and technological change. So even though these judgments are intuitive, we could potentially improve them with more study of big-picture social and technological change, such as Sentience Institute\u2019s <a href=\"https://www.sentienceinstitute.org/research\">MCE research</a> or Robin Hanson\u2019s book on <a href=\"https://en.wikipedia.org/wiki/The_Age_of_Em\">The Age of Em</a> that analyzes what a future of brain emulations would look like. (This sort of empirical research is what I see as the most promising future research avenue for far future cause prioritization. I worry EAs overemphasize armchair research (like most of this post, actually) for various reasons.[19])</p><p>I\u2019d personally be quite interested in a survey of people with expertise in the relevant fields of social, technological, and philosophical research, in which they\u2019re asked about each of the considerations above, though it might be hard to get a decent sample size, and I think it would be quite difficult to debias the respondents (see the Bias section of this post).</p><p>I\u2019m also interested in quantitative analyses of these considerations \u2014 &nbsp;calculations including all of these potential outcomes and associated likelihoods. As far as I know, this kind of analysis has only been attempted so far by Michael Dickens in <a href=\"/ea/xr/a_complete_quantitative_model_for_cause_selection/\">\u201cA Complete Quantitative Model for Cause Selection,\u201d</a> in which Dickens notes that, \u201cValues spreading may be better than existential risk reduction.\u201d While this quantification might seem hopelessly speculative, I think it\u2019s <a href=\"https://en.wikipedia.org/wiki/Superforecasting\">highly useful</a> even in such situations. Of course, rigorous debiasing is also very important here.</p><p>Overall, I think the far future is close to zero in expected moral value, meaning it\u2019s not nearly as good as is commonly assumed, implicitly or explicitly, in the EA community.</p><h1>Scale</h1><h2>Range of outcomes</h2><p>It\u2019s difficult to compare the scale of far future impacts since they are all astronomical, and I find the consideration of scale here to overall not be very useful.</p><p>Technically, it seems like MCE involves a larger range of potential outcomes than reducing extinction risk through AIA because, at least from a classical consequentialist perspective (giving weight to both negative and positive outcomes), it could make the difference between some of the worst far futures imaginable and the best far futures. Reducing extinction risk through AIA only makes the difference between nonexistence (a far future of zero value) and whatever world comes to exist. If one believes the far future is highly positive, this could still be a very large range, but it would still be less than the potential change from MCE.</p><p>How much less depends on one\u2019s views of how bad the worst future is relative to the best future. If the absolute value is the same, then MCE has a range twice as large as extinction risk.</p><p>As mentioned in the Context section above, the change in the far future that AIA could achieve might not exactly be extinction versus non-extinction. While an aligned AI would probably not involve the extinction of all sentient beings, since that would require the values of its creators to prefer extinction over all other options, an unaligned AI might not necessarily involve extinction. To use the canonical AIA example of a \u201cpaperclip maximizer\u201d (used to illustrate how an AI could easily have a harmful goal without any malicious intention), the rogue AI might create sentient beings as a labor force to implement its goal of maximizing the number of paperclips in the universe, or create sentient beings for some other goal.[20]</p><p>This means that the range of AIA is the difference between the potential universes with aligned AI and unaligned AI, which could be very good futures contrasted with very bad futures, rather than just very good futures contrasted with nonexistence.</p><p>Brian Tomasik has written out a thoughtful (though necessarily speculative and highly uncertain) <a href=\"https://foundational-research.org/artificial-intelligence-and-its-implications-for-future-suffering#Would_a_human-inspired_AI_or_rogue_AI_cause_more_suffering\">breakdown</a> of the risks of suffering in both aligned and unaligned AI scenarios, which weakly suggests that an aligned AI would lead to more suffering in expectation.</p><p>All things considered, it seems that the range of quality risk reduction (including MCE) is larger than that of extinction risk reduction (including AIA, depending on one\u2019s view of what difference AI alignment makes), but this seems like a fairly weak consideration to me because (i) it\u2019s a difference of roughly two-fold, which is quite small relative to the differences of ten-times, a thousand-times, etc. that we frequently see in cause prioritization, (ii) there are numerous fairly arbitrary judgment calls (like considering reducing extinction risk from AI versus AIA versus AI safety) that lead to different results.[21]</p><h2>Likelihood of different far future scenarios[22][23]</h2><p>MCE is relevant for many far future scenarios where AI doesn\u2019t undergo the sort of \u201cintelligence explosion\u201d or similar progression that makes AIA important; for example, if AGI is developed by an institution like a foreign country that has little interest in AIA, or if AI is never developed, or if it\u2019s developed slowly in a way that makes safety adjustments quite easy as that development occurs. In each of these scenarios, the way society treats sentient beings, especially those currently outside the moral circle, seems like it could still be affected by MCE. As mentioned earlier, I think there is a significant chance that the moral circle will fail to expand to reach all sentient beings, and I think a small moral circle could very easily lead to suboptimal or dystopian far future outcomes.</p><p>On the other hand, some possible far future civilizations might not involve moral circles, such as if there is an egalitarian society where each individual is able to fully represent their own interests in decision-making and this societal structure was not reached through MCE because these beings are all equally powerful for technological reasons (and no other beings exist and they have no interest in creating additional beings). Some AI outcomes might not be affected by MCE, such as an unaligned AI that does something like maximizing the number of paperclips for reasons other than human values (such as a programming error) or one whose designers create its value function without regard for humanity\u2019s current moral views (\u201ccoherent extrapolated volition\u201d could be an example of this, though I agree with <a href=\"https://foundational-research.org/dealing-with-moral-multiplicity/#What_about_reflective_equilibrium\">Brian Tomasik</a> that current moral views will likely be important in this scenario).</p><p>Given my current, highly uncertain estimates of the likelihood of various far future scenarios, I would guess that MCE is applicable in somewhat more cases than AIA, suggesting it\u2019s easier to make a difference to the far future through MCE. (This is analogous to saying the risk of MCE-failure seems greater than the risk of AIA-failure, though I\u2019m trying to avoid simplifying these into binary outcomes.)</p><h1>Tractability</h1><p>How much of an impact can we expect our marginal resources to have on the probability of extinction risk, or on the moral circle of the far future?</p><h2>Social change versus technical research</h2><p>One may believe changing people\u2019s attitudes and behavior is quite difficult, and direct work on AIA involves a lot less of that. While AIA likely involves influencing some people (e.g. policymakers, researchers, and corporate executives), MCE is almost entirely influencing people\u2019s attitudes and behavior.[24]</p><p>However, one could instead believe that technical research is more difficult in general, pointing to potential evidence such as the large amount of money spent on technical research (e.g. by Silicon Valley) with often very little to show for it, while huge social change seems to sometimes be effected by small groups of advocates with relatively little money (e.g. organizers of revolutions in Egypt, Serbia, and Turkey). (I don\u2019t mean this as a very strong or persuasive argument, just as a possibility. There are plenty of examples of tech done with few resources and social change done with many.)</p><p>It\u2019s hard to speak so generally, but I would guess that technical research tends to be easier than causing social change. And this seems like the strongest argument in favor of working on AIA over working on MCE.</p><h2>Track record</h2><p>In terms of EA work explicitly focused on the goals of AIA and MCE, AIA has a much better track record. The past few years have seen significant technical research output from organizations like MIRI and FHI, as <a href=\"/ea/1iu/2018_ai_safety_literature_review_and_charity/\">documented</a> by user Larks on the EA Forum for 2016 and 2017. I\u2019d defer readers to those posts, but as a brief example, MIRI had an acclaimed paper on <a href=\"https://intelligence.org/2016/09/12/new-paper-logical-induction/\">\u201cLogical Induction,\u201d</a> which used a financial market process to estimate the likelihood of logical facts (e.g. mathematical propositions like the Riemann hypothesis) that we aren\u2019t yet sure of. This is analogous to how we use probability theory to estimate the likelihood of empirical facts (e.g. a dice roll). In the bigger picture of AIA, this research could help lay the technical foundation for building an aligned AGI. See Larks\u2019 post for a discussion of more papers like this, as well as non-technical work done by AI-focused organizations such as the Future of Life Institute\u2019s <a href=\"https://futureoflife.org/ai-open-letter/\">open letter</a> on AI safety signed by leading AI researchers and cited by the White House\u2019s <a href=\"https://obamawhitehouse.archives.gov/blog/2016/10/12/administrations-report-future-artificial-intelligence\">\u201cReport on the Future of Artificial Intelligence.\u201d</a></p><p>Using an analogous definition for MCE, EA work explicitly focused on MCE (meaning expanding the moral circle in order to improve the far future) basically only started in 2017 with the founding of Sentience Institute (SI), though there were various blog posts and articles discussing it before then. SI has basically finished four research projects: (1) <a href=\"https://www.sentienceinstitute.org/foundational-questions-summaries\">Foundational Question Summaries</a> that summarize evidence we have on important effective animal advocacy (EAA) questions, including a survey of EAA researchers, (2) a case study of the <a href=\"https://www.sentienceinstitute.org/british-antislavery\">British antislavery movement</a> to better understand how they achieved one of the first major moral circle expansions in modern history, (3) a case study of <a href=\"https://www.sentienceinstitute.org/nuclear-power-clean-meat\">nuclear power</a> to better understand how some countries (e.g. France) enthusiastically adopted this new technology, but others (e.g. the US) didn\u2019t, (4) a <a href=\"https://www.sentienceinstitute.org/animal-farming-attitudes-survey-2017\">nationally representative poll</a> of US attitudes towards animal farming and animal-free food.</p><p>With a broader definition of MCE that includes activities that people prioritizing MCE tend to think are quite indirectly effective (see the Neglectedness section for discussion of definitions), we\u2019ve seen EA achieve quite a lot more, such as the work done by The Humane League, Mercy For Animals, Animal Equality, and other organizations on corporate <a href=\"https://www.openphilanthropy.org/blog/initial-grants-support-corporate-cage-free-reforms#Corporate_cage-free_campaigns_are_extremely_cost-effective\">welfare reforms</a> to animal farming practices, and the work done by The Good Food Institute and others on supporting a shift away from animal farming, especially through supporting new technologies like so-called <a href=\"http://cleanmeat.org/\">\u201cclean meat.\u201d</a></p><p>Since I favor the narrower definition, I think AIA outperforms MCE on track record, but the difference in track record seems largely explained by the greater resources spent on AIA, which makes it a less important consideration. (Also, when I personally decided to focus on MCE, SI did not yet exist, so the lack of track record was an even stronger consideration in favor of AIA (though MCE was also more neglected at that time).)</p><p>To be clear, the track records of all far future projects tend to be weaker than near-term projects where we can directly see the results.</p><h2>Robustness</h2><p>If one values robustness, meaning a higher certainty that one is having a positive impact, either for instrumental or intrinsic reasons, then AIA might be more promising because once we develop an aligned AI (that continues to be aligned over time), the work of AIA is done and won\u2019t need to be redone in the future. With MCE, assuming the advent of AI or similar developments won\u2019t fix society\u2019s values in place (known as \u201cvalue lock-in\u201d), then MCE progress could more easily be undone, especially if one believes there\u2019s a social setpoint that humanity drifts back towards when moral progress is made.[25]</p><p>I think the assumptions of this argument make it quite weak: I\u2019d guess an \u201cintelligence explosion\u201d has a significant chance of value lock-in,[26][27] and I don\u2019t think there\u2019s a setpoint in the sense that positive moral change increases the risk of negative moral change. I also don\u2019t value robustness intrinsically at all or instrumentally very much; I think that there is so much uncertainty in all of these strategies and such weak prior beliefs[28] that differences in certainty of impact matter relatively little.</p><h2>Miscellaneous</h2><p>Work on either cause area runs the risk of backfiring. The main risk for AIA seems to be that the technical research done to better understand how to build an aligned AI will increase AI capabilities generally, meaning it\u2019s also easier for humanity to produce an unaligned AI. The main risk for MCE seems to be that certain advocacy strategies will end up having the opposite effect as intended, such as a confrontational protest for animal rights that ends up putting people off of the cause.</p><p>It\u2019s unclear which project has better near-term proxies and feedback loops to assess and increase long-term impact. AIA has technical problems with solutions that can be mathematically proven, but these might end up having little bearing on final AIA outcomes, such as if an AGI isn\u2019t developed using the method that was advised or if technical solutions aren\u2019t implemented by policy-makers. MCE has metrics like public attitudes and practices. My weak intuition here, and the weak intuition of other reasonable people I\u2019ve discussed this with, is that MCE has better near-term proxies.</p><p>It\u2019s unclear which project has more historical evidence that EAs can learn from to be more effective. AIA has previous scientific, mathematical, and philosophical research and technological successes and failures, while MCE has previous psychological, social, political, and economic research and advocacy successes and failures.</p><p>Finally, I do think that we learn a lot about tractability just by working directly on an issue. Given how little effort has gone into MCE itself (see Neglectedness below), I think we could resolve a significant amount of uncertainty with more work in the field.</p><p>Overall, considering only direct tractability (i.e. ignoring information value due to neglectedness, which would help other EAs with their cause prioritization), I\u2019d guess AIA is a little more tractable.</p><h1>Neglectedness</h1><p>With neglectedness, we also face a challenge of how broadly to define the cause area. In this case, we have a fairly clear goal with our definition: to best assess how much low-hanging fruit is available. To me, it seems like there are two simple definitions that meet this goal: (i) organizations or individuals working explicitly on the cause area, (ii) organizations or individuals working on the strategies that are seen as top-tier by people focused on the cause area. How much one favors (i) versus (ii) depends largely on whether one thinks the top-tier strategies are fairly well-established and thus (ii) makes sense, or whether they will change over time such that one should favor (i) because those organizations and individuals will be better able to adjust.[29]</p><p>With the explicit focus definitions of AIA and MCE (recall this includes having a far future focus), it seems that MCE is much more neglected and has more low-hanging fruit.[30] For example, there is only one organization that I know of explicitly committed to MCE in the EA community (SI), while numerous organizations (MIRI, CHAI, part of FHI, part of CSER, even parts of AI capabilities organizations like Montreal Institute for Learning Algorithms, DeepMind, and OpenAI, etc.) are explicitly committed to AIA. Because MCE seems more neglected, we could learn a lot about MCE through SI\u2019s initial work, such as how easily advocates have achieved MCE throughout history.</p><p>If we include those working on the cause area without an explicit focus, then that seems to widen the definition of MCE to include some of the top strategies being used to expand the moral circle in the near-term, such as farmed animal work done by Animal Charity Evaluators and it\u2019s top-recommended charities, which have a <a href=\"https://animalcharityevaluators.org/transparency/financials/\">combined</a> <a href=\"https://animalcharityevaluators.org/charity-review/the-good-food-institute/#c1\">budget</a> <a href=\"https://animalcharityevaluators.org/wp-content/uploads/2017/11/the-humane-leagues-accomplishments-and-budget-2016-2017.pdf\">of</a> <a href=\"https://animalcharityevaluators.org/wp-content/uploads/2017/11/animal-equalitys-expenses-2016-2017.pdf\">around</a> $7.5 million in 2016. The combined budgets of top-tier AIA work is harder to estimate, but the Centre for Effective Altruism <a href=\"https://www.centreforeffectivealtruism.org/blog/changes-in-funding-in-the-ai-safety-field/\">estimates</a> all AIA work in 2016 was around $6.6 million. The AIA budgets seem to be increasing more quickly than the MCE budgets, especially given the grant-making of the Open philanthropy project. We could also include EA movement-building organizations that place a strong focus on reducing extinction risk, and even AIA specifically, such as 80,000 Hours. The categorization for MCE seems to have more room to broaden, perhaps all the way to mainstream animal advocacy strategies like the work of People for the Ethical Treatment of Animals (PETA), which might make AIA more neglected. (It could potentially go even farther, such as advocating for human sweatshop laborers, but that seems too far removed and I don\u2019t know any MCE advocates who think it\u2019s plausibly top-tier.)</p><p>I think there\u2019s a difference in aptitude that suggests MCE is more neglected. Moral advocacy seems like a field which, while quite crowded, seems relatively easy for deliberate, thoughtful people to vastly outperform the average advocate,[31] which can lead to surprisingly large impact (e.g. EAs have already had far more success in publishing their writing, such as books and op-eds, than most writers hope for).[32] Additionally, despite centuries of advocacy, very little quality research has been done to critically examine what advocacy is effective and what\u2019s not, while the fields of math, computer science, and machine learning involve substantial self-reflection and are largely worked on by academics who seem to use more critical thinking than the average activist (e.g. there\u2019s far more skepticism in these academic communities, a demand for rigor and experimentation that\u2019s rarely seen among advocates). In general, I think the aptitude of the average social change advocate is much lower than that of the average technological researcher, suggesting MCE is more neglected, though of course other factors also count.</p><p>The relative neglectedness of MCE also seems likely to continue, given the greater self-interest humanity has in AIA relative to MCE and, in my opinion, the net biases towards AIA described in the Biases section of this blog post. (This self-interest argument is a particularly important consideration for prioritizing MCE over AIA in my view.[33])</p><p>However, while neglectedness is typically thought to make a project more tractable, it seems that existing work in the extinction risk space has made marginal contributions more impactful in some ways. For example, talented AI researchers can find work relatively easily at an organization dedicated to AIA, while the path for talented MCE researchers is far less clear and easy. This alludes to the difference in tractability that might exist between labor resources and funding resources, as it currently seems like MCE is much more funding-constrained[34] while AIA is <a href=\"http://lesswrong.com/lw/p5e/announcing_aasaa_accelerating_ai_safety_adoption/\">largely talent-constrained</a>.</p><p>As another example, there are already solid inroads between the AIA community and the AI decision-makers, and AI decision-makers have already expressed interest in AIA, suggesting that influencing them with research results will be fairly easy once those research results are in hand. This means both that our estimation of AIA\u2019s neglectedness should decrease, and that our estimation of its non-neglectedness tractability should increase, in the sense that neglectedness is a part of tractability. (The definitions in this framework vary.)</p><p>All things considered, I find MCE to be more compelling from a neglectedness perspective, particularly due to the current EA resource allocation and the self-interest humanity has, and will most likely continue to have, in AIA. When I decided to focus on MCE, there was an even stronger case for neglectedness because no organization existed committed to that goal (SI was founded in 2017), though there was an increased downside to MCE \u2014 the even more limited track record.</p><h1>Cooperation</h1><p>Values spreading as a far future intervention has been criticized on the following grounds: People have very different values, so trying to promote your values and change other people\u2019s could be seen as uncooperative. Cooperation seems to be useful both directly (e.g. how willing are other people to help us out if we\u2019re fighting them?) and in a broader sense because of superrationality, an argument that one should help others even when there\u2019s no causal mechanism for reciprocation.[35]</p><p>I think this is certainly a good consideration against some forms of values spreading. For example, I don\u2019t think it\u2019d be wise for an MCE-focused EA to disrupt the <a href=\"https://www.eaglobal.org/\">Effective Altruism Global</a> conferences (e.g. yell on stage and try to keep the conference from continuing) if they have an insufficient focus on MCE. This seems highly ineffective because of how uncooperative it is, given the EA space is supposed to be one for having challenging discussions and solving problems, not merely advocating one\u2019s positions like a political rally.</p><p>However, I don\u2019t think it holds much weight against MCE in particular for two reasons: First, because I don\u2019t think MCE is particularly uncooperative. For example, I never bring up MCE with someone and hear, \u201cBut I like to keep my moral circle small!\u201d I think this is because there are many different components of our attitudes and worldview that we refer to as values and morals. People have some deeply-held values that seem strongly resistant to change, such as their religion or the welfare of their immediate family, but very few people seem to have small moral circles as a deeply-held value. Instead, the small moral circle seems to mostly be a superficial, casual value (though it\u2019s often connected to the deeper values) that people are okay with \u2014 or even happy about \u2014 changing.[36]</p><p>Second, insofar as MCE is uncooperative, I think a large number of other EA interventions, including AIA, are similarly uncooperative. Many people even in the EA community are concerned with, or even opposed to, AIA. For example, if one believes an aligned AI would create a worse far future than an unaligned AI, or if one thinks AIA is harmfully distracting from more important issues and gives EA a bad name. This isn\u2019t to say I think AIA is bad because it\u2019s uncooperative \u2014 on the contrary, this seems like a level of uncooperativeness that\u2019s often necessary for dedicated EAs. (In a trivial way, basically all action involves uncooperativeness because it\u2019s always about changing the status quo or preventing the status quo from changing.[37] Even inaction can involve uncooperativeness if it means not working to help someone who would like your help.)</p><p>I do think it\u2019s more important to be cooperative in some other situations, such as if one has a very different value system than some of their colleagues, as might be the case for the Foundational Research Institute, which <a href=\"https://foundational-research.org/reasons-to-be-nice-to-other-value-systems/\">advocates strongly</a> for cooperation with other EAs.</p><h2>Cooperation with future do-gooders</h2><p>Another argument against values spreading goes something like, \u201cWe can worry about values after we\u2019ve safely developed AGI. Our tradeoff isn\u2019t, \u2018Should we work on values or AI?\u2019 but instead \u2018Should we work on AI now and values later, or values now and maybe AI later if there\u2019s time?\u2019\u201d</p><p>I agree with one interpretation of the first part of this argument, that urgency is an important factor and AIA does seem like a time-sensitive cause area. However, I think MCE is similarly time-sensitive because of risks of value lock-in where our descendants\u2019 morality becomes much harder to change, such as if AI designers choose to fix the values of an AGI, or at least to make them independent of other people\u2019s opinions (they could still be amenable to self-reflection of the designer and new empirical data about the universe other than people\u2019s opinions)[38]; if humanity sends out colonization vessels across the universe that are traveling too fast for us to adjust based on our changing moral views; or if society just becomes too wide and disparate to have effective social change mechanisms like we do today on Earth.</p><p>I disagree with the stronger interpretation, that we can count on some sort of cooperation with or control over future people. There might be some extent to which we can do this, such as via superrationality, but that seems like a fairly weak effect. Instead, I think we\u2019re largely on our own, deciding what we do in the next few years (or perhaps in our whole career), and just making our best guess of what future people will do. It sounds very difficult to strike a deal with them that will ensure they work on MCE in exchange for us working on AIA.</p><h1>Bias</h1><p>I\u2019m always cautious about bringing considerations of bias into an important discussion like this. Considerations easily turn into messy, personal attacks, and often you can fling roughly-equal considerations of counter-biases when accusations of bias are hurled at you. However, I think we should give them serious consideration in this case. First, I want to be exhaustive in this blog post, and that means throwing every consideration on the table, even messy ones. Second, my own cause prioritization \u201cjourney\u201d led me first to AIA and other non-MCE/non-animal-advocacy EA priorities (mainly EA movement-building), and it was considerations of bias that allowed me to look at the object-level arguments with fresh eyes and decide that I had been way off in my previous assessment.</p><p>Third and most importantly, people\u2019s views on this topic are inevitably driven mostly by intuitive, subjective judgment calls. One could easily read everything I\u2019ve written in this post and say they lean in the MCE direction on every topic, or the AIA direction, and there would be little object-level criticism one could make against that if they just based their view on a different intuitive synthesis of the considerations. This subjectivity is dangerous, but it is also humbling. It requires us to take an honest look at our own thought processes in order to avoid the subtle, irrational effects that might push us in either direction. It also requires caution when evaluating \u201cexpert\u201d judgment, given how much experts could be affected by personal and social biases themselves.</p><p>The best way I know of to think about bias in this case is to consider the biases and other factors that favor either cause area and see which case seems more powerful, or which particular biases might be affecting our own views. The following lists are presumably not exhaustive but lay out what I think are some common key parts of people\u2019s journeys to AIA or MCE. Of course, these factors are not entirely deterministic and probably not all will apply to you, nor do they necessarily mean that you are wrong in your cause prioritization. Based on the circumstances that apply more to you, consider taking a more skeptical look at the project you favor and your current views on the object-level arguments for it.</p><h2>One might be biased towards AIA if...</h2><p>They eat animal products, and thus are assign lower moral value and <a href=\"http://journals.sagepub.com/doi/abs/10.1177/0146167211424291\">less mental faculties</a> to animals.</p><p>They haven\u2019t accounted for the bias of <a href=\"https://luciuscaviola.com/s/Caviola-Everett-and-Faber-2018-Speciesism-JPSP-Pre-Print.pdf\">speciesism</a>.</p><p>They lack personal connections to animals, such as growing up with pets.</p><p>They are or have been a fan of science fiction and fantasy literature and media, especially if they dreamed of being the hero.</p><p>They have a tendency towards technical research over social projects.</p><p>They lack social skills.</p><p>They are inclined towards philosophy and mathematics.</p><p>They have a <a href=\"http://www.slate.com/blogs/moneybox/2014/01/14/activists_are_unpopular_social_psychologists_say_people_want_them_to_calm.html\">negative perception of activists</a>, perhaps seeing them as hippies, irrational, idealistic, \u201csocial justice warriors,\u201d or overly emotion-driven.</p><p>They are a part of the EA community, and therefore drift towards the status quo of EA leaders and peers. (The views of EA leaders can of course be genuine evidence of the correct cause prioritization, but they can also lead to bias.)</p><p>The idea of \u201csaving the world\u201d appeals to them.</p><p>They take pride in their intelligence, and would love if they could save the world just by doing brilliant technical research.</p><p>They are competitive, and like the feeling/mindset of doing astronomically more good than the average do-gooder, or even the average EA. (I\u2019ve argued in this post that MCE has this astronomical impact, but it lacks the feeling of literally \u201csaving the world\u201d or otherwise having a clear impact that makes a good hero\u2019s journey climax, and it\u2019s closely tied to lesser, near-term impacts.)</p><p>They have little personal experience of extreme suffering, the sort that makes one pessimistic about the far future, especially regarding s-risks. (Personal experience could be one\u2019s own experience or the experiences of close friends and family.)</p><p>They have little personal experience of oppression, such as due to their gender, race, disabilities, etc.</p><p>They are generally a happy person.</p><p>They are generally optimistic, or at least averse to thinking about bad outcomes like how humanity could cause astronomical suffering. (Though some pessimism is required for AIA in the sense that they don\u2019t count on AI capabilities researchers ending up with an aligned AI without their help.)</p><h2>One might be biased towards MCE if...</h2><p>They are vegan, especially if they went vegan for non-animal or non-far-future reasons, such as for better personal health.</p><p>Their gut reaction when they hear about extinction risk or AI risk is to judge it nonsensical.</p><p>They have personal connections to animals, such as growing up with pets.</p><p>They are or have been a fan of social movement/activism literature and media, especially if they dreamed of being a movement leader.</p><p>They have a tendency towards social projects over technical research.</p><p>They have benefitted from above-average social skills.</p><p>They are inclined towards social science.</p><p>They have a positive perception of activists, perhaps seeing them as the true leaders of history.</p><p>They have social ties to vegans and animal advocates. (The views of these people can of course be genuine evidence of the correct cause prioritization, but they can also lead to bias.)</p><p>The idea of \u201chelping the worst off\u201d appeals to them.</p><p>They take pride in their social skills, and would love if they could help the worst off just by being socially savvy.</p><p>They are not competitive, and like the thought of being a part of a friendly social movement.</p><p>They have a lot of personal experience of extreme suffering, the sort that makes one pessimistic about the far future, especially regarding s-risks. (Personal experience could be one\u2019s own experience or the experiences of close friends and family.)</p><p>They have a lot of personal experience of oppression, such as due to their gender, race, disabilities, etc.</p><p>They are generally an unhappy person.</p><p>They are generally pessimistic, or at least don\u2019t like thinking about good outcomes. (Though some optimism is required for MCE in the sense that they believe work on MCE can make a large positive difference in social attitudes and behavior.)</p><p>They care a lot about directly seeing the impact of their work, even if the bulk of their impact is hard to see. (E.g. seeing improvements in the conditions of farmed animals, which can be seen as a proxy for helping farmed-animal-like beings in the far future.)</p><h2>Implications</h2><p>I personally found myself far more compelled towards AIA in my early involvement with EA before I had thought in detail about the issues discussed in this post. I think the list items in the AIA section apply to me much more strongly than the MCE list. When I considered these biases, in particular speciesism and my desire to follow the status quo of my EA friends, a fresh look at the object-level arguments changed my mind.</p><p>From my reading and conversations in EA, I think the biases in favor of AIA are also quite a bit stronger in the community, though of course some EAs \u2014 mainly those already working on animal issues for near-term reasons \u2014 probably feel a stronger pull in the other direction.</p><p>How you think about these bias considerations also depends on how biased you think the average EA is. If you, for example, think EAs tend to be quite biased in another way like <a href=\"https://www.psy.ox.ac.uk/publications/477895\">\u201cmeasurement bias\u201d or \u201cquantifiability bias\u201d</a> (a tendency to focus too much on easily-quantifiable, low-risk interventions), then considerations of biases on this topic should probably be more compelling to you than they will be to people who think EAs are less biased.<br>&nbsp;</p><h1>Notes</h1><p>[1] This post attempts to compare these cause areas overall, but since that\u2019s sometimes too vague, I specifically mean the strategies within each cause area that seem most promising. I think this is basically equal to \u201cwhat EAs working on MCE most strongly prioritize\u201d and \u201cwhat EAs working on AIA most strongly prioritize.\u201d</p><p>[2] There\u2019s a sense in which AIA is a form of MCE simply because AIA will tend to lead to certain values. I\u2019m excluding that AIA approach of MCE from my analysis here to avoid overlap between these two cause areas.</p><p>[3] Depending on how close we\u2019re talking about, this could be quite unlikely. If we\u2019re discussing the range of outcomes from dystopia across the universe to utopia across the universe, then a range like \u201cbetween modern earth and the opposite value of modern earth\u201d seems like a very tiny fraction of the total possible range.</p><p>[4] I mean \u201cgood\u201d in a \u201cpositive impact\u201d sense here, so it includes not just rationality according to the decision-maker but also value alignment, luck, being empirically well-informed, being capable of doing good things, etc.</p><p>[5] One reason for optimism is that you might think most extinction risk is in the next few years, such that you and other EAs you know today will still be around to do this research yourselves and make good decisions after those risks are avoided.</p><p>[6] Technically one could believe the far future is negative but also that humans will make good decisions about extinction, such as if one believes the far future (given non-extinction) will be bad only due to nonhuman forces, such as aliens or evolutionary trends, but has optimism about human decision-making, including both that humans will make good decisions about extinction and that they will be logistically able to make those decisions. I think this is an unlikely view to settle on, but it would make option value a good thing in a \u201cclose to zero\u201d scenario.</p><p>[7] Non-extinct civilizations could be maximized for happiness, maximized for interestingness, set up like Star Wars or another sci-fi scenario, etc. while extinct civilizations would all be devoid of sentient beings, perhaps with some variation in physical structure like different planets or remnant structures of human civilization.&nbsp;</p><p>[8] My views on this are currently largely qualitative, but if I had to put a number on the word \u201csignificant\u201d in this context, it\u2019d be somewhere around 5-30%. This is a very intuitive estimate, and I\u2019m not prepared to justify it.</p><p>[9] Paul Christiano made a general argument in favor of humanity reaching good values in the long run due to reflection in his post <a href=\"https://rationalaltruist.com/2013/06/13/against-moral-advocacy/\">\u201cAgainst Moral Advocacy\u201d</a> (see the \u201cOptimism about reflection\u201d section) though he doesn\u2019t specifically address concern for all sentient beings as a potential outcome, which might be less likely than other good values that are more driven by cooperation.\"</p><p>[10] Nick Bostrom has considered some of these risks of artificial suffering using the term \u201cmind crime,\u201d which specifically refers to harming sentient beings created inside a superintelligence. See his book, <a href=\"https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/1501227742\">Superintelligence</a>.</p><p>[11] The Foundational Research Institute has written about risks of astronomical suffering in <a href=\"https://foundational-research.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/\">\u201cReducing Risks of Astronomical Suffering: A Neglected Priority.\u201d</a> The TV series Black Mirror is an interesting dramatic exploration of how the far future could involve vasts amounts of suffering, such as the episodes \u201cWhite Christmas\u201d and \u201cUSS Callister.\u201d Of course, the details of these situations often veer towards entertainment over realism, but their exploration of the potential for dystopias in which people abuse sentient digital entities is thought-provoking.</p><p>[12] I\u2019m highly uncertain about what sort of motivations (like happiness and suffering in humans) future digital sentient beings will have. For example, is punishment being a stronger motivator in earth-originating life just an evolutionary fluke that we can expect to dissipate in artificial beings? Could they be just as motivated to attain reward as we are to avoid punishment? I think this is a promising avenue for future research, and I\u2019m glad it\u2019s being discussed by some EAs.</p><p>[13] Brian Tomasik discusses this in his essay on <a href=\"http://reducing-suffering.org/values-spreading-often-important-extinction-risk/\">\u201cValues Spreading is Often More Important than Extinction Risk,\u201d</a> suggesting that, \u201cthere's not an obvious similar mechanism pushing organisms toward the things that I care about.\u201d However, Paul Christiano notes in <a href=\"https://rationalaltruist.com/2013/06/13/against-moral-advocacy/\">\u201cAgainst Moral Advocacy\u201d</a> that he expects \u201c[c]onvergence of values\u201d because \u201cthe space of all human values is not very broad,\u201d though this seems quite dependent on how one defines the possible space of values.</p><p>[14] This efficiency argument is also discussed in Ben West\u2019s article on <a href=\"/ea/1cl/an_argument_for_why_the_future_may_be_good/\">\u201cAn Argument for Why the Future May Be Good.\u201d</a></p><p>[15] The term \u201cresources\u201d is intentionally quite broad. This means whatever the limitations are on the ability to produce happiness and suffering, such as energy or computation.</p><p>[16] One can also create hedonium as a promise to get things from rivals, but promises seem less common than threats because threats tend to be more motivating and easier to implement (e.g it\u2019s easier to destroy than create). However, some social norms encourage promises over threats because promises are better for society as a whole. Additionally, threats against powerful beings (e.g. other citizens in the same country) do less than threats against less powerful, or more distant beings, and the latter category might be increasingly common in the future. Additionally, threats and promises matter less when one considers that they are often unfulfilled because the other party doesn\u2019t do the action that was the subject of the threat or promise.</p><p>[17] Paul Christiano\u2019s blog post on <a href=\"https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/\">\u201cWhy might the future be good?\u201d</a> argues that \u201cthe future will be characterized by much higher influence for altruistic values [than self-interest],\u201d though he seems to just be discussing the potential of altruism and self-interest to create positive value, rather than their potential to create negative value.</p><p>Brian Tomasik discusses Christiano\u2019s argument and others in <a href=\"http://reducing-suffering.org/the-future-of-darwinism/\">\u201cThe Future of Darwinism\u201d </a>and concludes, \u201cWhether the future will be determined by Darwinism or the deliberate decisions of a unified governing structure remains unclear.\u201d<br>[18] One discussion of changes in morality on a large scale is Robin Hanson\u2019s blog post, <a href=\"http://www.overcomingbias.com/2012/05/forager-vs-farmer-morality.html\">\u201cForager, Farmer Morals.\u201d</a></p><p>[19] Armchair research is relatively easy, in the sense that all it requires is writing and thinking rather than also digging through historical texts, running scientific studies, or engaging in substantial conversation with advocates, researchers, and/or other stakeholders. It\u2019s also more similar to the mathematical and philosophical work that most EAs are used to doing. And it\u2019s more attractive as a demonstration of personal prowess to think your way into a crucial consideration than to arrive at one through the tedious work of research. (These reasons are similar to the reasons I feel most far-future-focused EAs are biased towards AIA over MCE.)</p><p>[20] These sentient beings probably won\u2019t be the biological animals we know today, but instead digital beings who can more efficiently achieve the AI\u2019s goals.</p><p>[21] The neglectedness heuristic involves a similar messiness of definitions, but the choices seem less arbitrary to me, and the different definitions lead to more similar results.</p><p>[22] Arguably this consideration should be under Tractability rather than Scale.</p><p>[23] There\u2019s a related framing here of \u201cleverage,\u201d with the basic argument being that AIA seems more compelling than MCE because AIA is specifically targeted at an important, narrow far future factor (the development of AGI) while MCE is not as specifically targeted. This also suggests that we should consider specific MCE tactics focused on important, narrow far future factors, such as ensuring the AI decision-makers have wide moral circles even if the rest of society lags behind. I find this argument fairly compelling, including the implication that MCE advocates should focus more on advocating for digital sentience and advocating in the EA community than they would otherwise.</p><p>[24] Though plausibly MCE involves only influencing a few decision-makers, such as the designers of an AGI.</p><p>[25] Brian Tomasik discusses this in, <a href=\"http://reducing-suffering.org/values-spreading-often-important-extinction-risk/\">\u201cValues Spreading is Often More Important than Extinction Risk,\u201d</a> arguing that, \u201cVery likely our values will be lost to entropy or Darwinian forces beyond our control. However, there's some chance that we'll create a singleton in the next few centuries that includes goal-preservation mechanisms allowing our values to be \"locked in\" indefinitely. Even absent a singleton, as long as the vastness of space allows for distinct regions to execute on their own values without take-over by other powers, then we don't even need a singleton; we just need goal-preservation mechanisms.\u201d</p><p>[26] Brian Tomasik discusses the likelihood of value lock-in in his essay, <a href=\"http://reducing-suffering.org/will-future-civilization-eventually-achieve-goal-preservation/\">\u201cWill Future Civilization Eventually Achieve Goal Preservation?\u201d</a></p><p>[27] The advent of AGI seems like it will have similar effects on the lock-in of values and alignment, so if you think AI timelines are shorter (i.e. advanced AI will be developed sooner), then that increases the urgency of both cause areas. If you think timelines are so short that we will struggle to successfully reach AI alignment, then that decreases the tractability of AIA, but MCE seems like it could more easily have a partial effect on AI outcomes than AIA could.</p><p>[28] In the case of near-term, direct interventions, one might believe that <a href=\"https://80000hours.org/articles/effective-social-program/\">\u201cmost social programmes don\u2019t work,\u201d</a> which suggests that we should have low, strong priors for intervention effectiveness that we need robustness to overcome.</p><p>[29] Caspar Oesterheld discusses the ambiguity of neglectedness definitions in his blog post, <a href=\"https://casparoesterheld.com/2017/06/25/complications-in-evaluating-neglectedness/\">\"Complications in evaluating neglectedness.\"</a> Other EAs have also raised concern about this commonly-used heuristic, and I almost included this content in this post under the \u201cTractability\u201d section for this reason.</p><p>[30] This is a fairly intuitive sense of the word \u201cmatched.\u201d I\u2019m taking the topic of ways to affect the far future, dividing it into population risk and quality risk categories, then treating AIA and MCE as subcategories of each. I\u2019m also thinking in terms of each project (AIA and MCE) being in the category of \u201ccause areas with at least pretty good arguments in their favor,\u201d and I think \u201cput decent resources into all such projects until the arguments are rebutted\u201d is a good approach for the EA community.</p><p>[31] I mean \u201cadvocate\u201d quite broadly here, just anyone working to effect social change, such as people submitting op-eds to newspapers or trying to get pedestrians to look at their protest or take their leaflets.</p><p>[32] It\u2019s unclear what the explanation is for this. It could just be demographic differences such as high IQ, going to elite universities, etc. but it could also be exceptional \u201crationality skills\u201d like finding loopholes in the publishing system.</p><p>[33] In Brian Tomasik\u2019s essay on <a href=\"http://reducing-suffering.org/values-spreading-often-important-extinction-risk/\">\u201cValues Spreading is Often More Important than Extinction Risk,\u201d</a> he argues that \u201c[m]ost people want to prevent extinction\u201d while, \u201cIn contrast, you may have particular things that you value that aren't widely shared. These things might be easy to create, and the intuition that they matter is probably not too hard to spread. Thus, it seems likely that you would have higher leverage in spreading your own values than in working on safety measures against extinction.\u201d</p><p>[34] This is just my personal impression from working in MCE, especially with my organization Sentience Institute. With indirect work, The Good Food Institute is a potential exception since they have struggled to quickly hired talented people after their large amounts of funding.</p><p>[35] See <a href=\"https://foundational-research.org/reasons-to-be-nice-to-other-value-systems/#Superrationality\">\u201cSuperrationality\u201d</a> in \u201cReasons to Be Nice to Other Value Systems\u201d for an EA introduction to the idea. See \u201cIn favor of \u2018being nice\u2019\u201d in <a href=\"https://rationalaltruist.com/2013/06/13/against-moral-advocacy/\">\u201cAgainst Moral Advocacy\u201d</a> as example of cooperation as an argument against values spreading. In <a href=\"https://foundational-research.org/files/Multiverse-wide-Cooperation-via-Correlated-Decision-Making.pdf#page=76\">\u201cMultiverse-wide Cooperation via Correlated Decision Making,\u201d</a> Caspar Oesterheld argues that superrational cooperation makes MCE more important.</p><p>[36] This discussion is complicated by the widely varying degrees of MCE. While, for example, most US residents seem perfectly okay with expanding concern to vertebrates, there would be more opposition to expanding to insects, and even more to some simple computer programs that some argue should fit into the edges of our moral circles. I do think the farthest expansions are much less cooperative in this sense, though if the message is just framed as, \u201cexpand our moral circle to all sentient beings,\u201d I still expect strong agreement.</p><p>[37] One exception is a situation where everyone wants a change to happen, but nobody else wants it badly enough to put the work into changing the status quo.</p><p>[38] My impression is that the AI safety community currently wants to avoid fixing these values, though they might still be trying to make them resistant to advocacy from other people, and in general I think many people today would prefer to fix the values of an AGI when they consider that they might not agree with potential future values.</p>", "user": {"username": "Jacy"}}, {"_id": "5ynh5xL6JLj4Qri6g", "title": "Prioritization Consequences of \"Formally Stating the AI Alignment Problem\"", "postedAt": "2018-02-19T21:31:36.942Z", "htmlBody": "<html><body><p>I&apos;ve just completed a <a href=\"https://mapandterritory.org/formally-stating-the-ai-alignment-problem-fe7a6e3e5991\">comprehensive presentation of my current thinking on AI alignment</a>. In it I give a formal statement of the AI alignment problem that is rigorous and makes clear the philosophical assumptions being made when we try to say we want to create AI aligned with human values. In the process I show the limitations of a decision theory based approach to AI alignment, similarly show the limitations of an axiological approach, and end up with a noematological approach (an account given in terms of noemata or the&#xA0;objects of conscious phenomena) that is better able to encompass all of what is necessarily meant by &quot;align AI with human values&quot; given we are likely to need to align non-rational agents.</p>\n<p>I have yet to&#xA0;flesh out my thoughts on the prioritization consequences of this as it relates to what work we want to pursue to achieve AI alignment, but I have some initial thoughts I&apos;d like to share and solicit feedback on. Without further ado, those thoughts which I believe deserve deeper discussion:</p>\n<ol>\n<li>Non-decision theory based research into AI alignment is under explored.</li>\n<li>Noematological research into AI alignment is almost completely ignored until now.</li>\n<ol>\n<li>There are a handful of examples where people consider how humans align themselves with their goals and use this to propose techniques for AI alignment, but they are preliminary.</li>\n<li>How non-decision theory based approaches could contribute to solving AI alignment was previously poorly understood, but now has a framework in which to work.</li>\n<li>This framework is poorly explored though so it&apos;s not yet clear exactly how to put insights from human and animal behavior and thought alignment into forms that might present specific solutions to AI alignment that could be tried.</li>\n</ol>\n<li>Existing decision theory based research, like MIRIs, is well funded and well attended to relative to non-decision theory based research.</li>\n<ol>\n<li>Non-decision theory based research into AI alignment is a neglected area ripe to benefit from additional funding and attention.</li>\n<li>Decision theory based research is in absolute terms still an area where more work can be done and so remains underfunded and underattended to relative to the amount it could carry.</li>\n</ol>\n<li>AI alignment research with different simplifying assumptions from those of decision theory are underexplored (compare 2 above).</li>\n<li>Noematological AI alignment research may be a &quot;dangerous attractor&quot;.</li>\n<ol>\n<li>Draws ideas from disciplines that have less focus on rigor and so may attract distraction to the field from those who are unprepared for the level of care AI safety research demands.</li>\n<li>May give more of an angle for crackpots to access the field where a heavy decision theory focus helps weed them out.</li>\n</ol>\n</ol>\n<p>Comments on these statements welcome here. If you have feedback on the original work I recommend you leave them as <a href=\"https://mapandterritory.org/formally-stating-the-ai-alignment-problem-fe7a6e3e5991\">comments there</a> or on the <a href=\"https://www.lesserwrong.com/posts/7dvDgqvqqziSKweRs/formally-stating-the-ai-alignment-problem\">LW post about it</a>.</p></body></html>", "user": {"username": "gworley3"}}, {"_id": "PkSncEsBgHkz5iD5E", "title": "What is Animal Farming in Rural Zambia Like? A Site Visit", "postedAt": "2018-02-19T20:49:45.024Z", "htmlBody": "<html><body><p>Factory farming in the United States and other developed countries <a href=\"http://lesswrong.com/lw/i3s/why_eat_less_meat/\"> is no doubt terrible</a>. And <a href=\"http://us14.campaign-archive1.com/?u=66df320da8400b581cbc1b539&amp;id=1096590fac&amp;e=a6c8a51ace\"> while other developing countries are catching up in their sophistication</a>, it is not clear yet what factory farming is like in the least developed parts of the developing world where most aid is focused. Instead, most rural livestock in the developing world <a href=\"http://igrow.org/livestock/beef/raising-livestock-in-kenya-comparing-and-contrasting-animal-health-issues/\"> is kept in a pastoral farming system</a>&#xA0;that is relatively &#x201C;free range&#x201D;. However, little information is publicly available on the internet about what this is like or how much animal suffering is involved relative to the developed country horror of factory farming. <br> <br> One such site visit exploring what this looks was seen in <a href=\"https://www.givewell.org/research/site-visits/november-2012#GiveDirectly\"> GiveWell&#x2019;s 2012 Kenyan site visit</a>&#xA0;(for example, <a href=\"https://get.google.com/albumarchive/109427789386001570492/album/AF1QipMGHhECE2tuMXz1-OkgdN7zoMT0GVpC_C6d-88u/AF1QipOcrWvwkOBRMb1ZFjfme8mpTqKJWdm2TjYv8KLX?authKey=CL6w6q2Sl6WbLg\"> this photo</a>). To gather a bit more qualitative information on what animal farming could be like, I commissioned a different site visit to Zambia. <a href=\"https://contractwork.vipulnaik.com/payer.php?payer=Peter%20Hurford\"> Using Vipul Naik as an intermediary</a>, I paid $570 to Sebastian Sanchez (&#x201C;Sebas&#x201D;) to visit and report on farming in Zambia[1] during the course of his already pre-planned trip to Africa. This money was to compensate Sebastian for his time and his costs of travel and doing business. I explained the project to Sebas as &#x201C;consisting in visiting facilities and qualitatively assessing farming and agricultural practices for nonhuman animals&#x201D;. Sebas arranged his visits with the help of a local guide. His guide also connected him with a taxi driver. Both of these people were compensated by Sebas via the money I provided.</p>\n<p>&#xA0;</p>\n<h2>First Visit: Abattoir</h2>\n<p><strong>Media here (pictures and video!): <a href=\"https://www.facebook.com/seba.sanchez.3998/media_set?set=a.10156676891004377.1073741837.634404376&amp;type=3&amp;pnref=story\"> https://www.facebook.com/seba.sanchez.3998/media_set?set=a.10156676891004377.1073741837.634404376&amp;type=3&amp;pnref=story <br><br></a></strong></p>\n<p>On 1 September 2017, with the help of his guide, David, who introduced him to a taxi driver named Muulu, Sebas visited one of two slaughterhouses in Livingstone, Zambia where he was staying. Sebas was able to get permission to enter the abattoir after tipping the wife of one of the workers. The other slaughterhouse was more modern and could be seen from this slaughterhouse.</p>\n<p>In the slaughterhouse, chickens can be seen roaming outside freely. In a Facebook comment, David clarifies that this slaughterhouse is only for slaughtering cattle, whereas pigs would typically be slaughtered by the owner at home. Throughout the videos, Muluu describes that cattle owners would bring their own cattle in to be slaughtered. The slaughter process which is said to involve shooting cattle with a bullet that is supposed to kill instantly. Sebas observes that the place is generally unhygienic and full of flies. While animal skin of ill animals is burnt in a furnace, decaying animal skin from healthier animals can be seen in the open.</p>\n<p>&#xA0;</p>\n<h2>Second Visit: Large Farm in Countryside</h2>\n<p><strong>Media here: <a href=\"https://www.facebook.com/seba.sanchez.3998/media_set?set=a.10156677566689377.1073741838.634404376&amp;type=3&amp;pnref=story\"> https://www.facebook.com/seba.sanchez.3998/media_set?set=a.10156677566689377.1073741838.634404376&amp;type=3&amp;pnref=story </a></strong></p>\n<p><br>On 3 September 2017, Sebastian visited the countryside with his guide, David, his taxi driver, Muuluu, and one other unrelated person and headed to a farm which had several facilities scattered across a large state. This farm belonged to a white man of Afrikaner descent who claimed to have about 7,000 hectares of land in Southern Zambia. The farmer was hostile toward David and Muulu as he claimed to be concerned about robbery from locals. The farm owner was informed that the video recording was part of a research project. The farm owner did not accept any tips.</p>\n<p>Sebas tipped an employee of the farm about five dollars, and the employee showed him a large poultry area. The area was said to contain 3700 chicken in 1750m^2 (0.43 acre), which is about 0.5 square meter (~5 square feet) per chicken. This is comparable to, if not a little better&#xA0;than, <a href=\"https://www.huffingtonpost.com/bruce-friedrich/eggs-from-caged-hens_b_2458525.html\"> the density of an American factory farm</a>, however, here the chickens were not in cages and could freely move around. (For further comparison, a standard sheet of paper is about 0.7 square feet.) Later on, the farm owner demonstrates intensely confining three chickens to a small cage, but says that the confinement is actually slightly better than the European standard.</p>\n<p>Sebastian notes that in his travels he felt like chickens were the most frequently farmed animal and the animal people most frequently eat. Separately, Sebas saw live chicken being carried by their wings while the person holding them was riding a bike.</p>\n<p>Later, Sebas is shown by the farm owner a bunch of cows that freely range with a good deal of space. The farmer claims to care about avoiding animal cruelty and avoids needlessly beating his animals. The farmer says he avoids putting too many chickens in one cage mainly to avoid robbery. The farmer told Sebas a story about robbers who inexpertly killed a cow for the meat and the cow agonized for a long period of time. The farmer says he shoots his animals himself to avoid excess cruelty.</p>\n<p>The farmer invited Sebas to stay longer afterwards, but Sebas turned him down.</p>\n<p>&#xA0;</p>\n<h2>Third Visit: Rural Village</h2>\n<p><strong>Media here: <a href=\"https://www.facebook.com/seba.sanchez.3998/media_set?set=a.10156685699114377.1073741839.634404376&amp;type=3&amp;pnref=story\"> https://www.facebook.com/seba.sanchez.3998/media_set?set=a.10156685699114377.1073741839.634404376&amp;type=3&amp;pnref=story </a></strong></p>\n<p>After visiting the farm, Sebas and David went to the village of Siyakasipa, which is very poor and not English speaking. (English is widely spoken in Zambia, even by kids.) For the most part, the cattle range free. However, during a drive, Sebas could see cattle kept in a pen. While the pen was not particularly intense confinement, the cattle were definitely limited in their range of motion and it did not appear like they had access to adequate food or water. David explains that the cattle in the village are killed with an axe.</p>\n<p>&#xA0;</p>\n<h2>Endnote</h2>\n<p>[1]: We originally had talked about visiting Tanzania, but Zambia ended up being more convenient for Sebastian.</p></body></html>", "user": {"username": "Peter_Hurford"}}, {"_id": "iZp7TtZdFyW8eT5dA", "title": "A generalized strategy of \u2018mission hedging\u2019: investing in 'evil' to do more good", "postedAt": "2018-02-18T17:41:31.873Z", "htmlBody": "<h1>Video abstract</h1><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=uepoRvgny54\"><div><iframe src=\"https://www.youtube.com/embed/uepoRvgny54\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><h2>Disclaimer</h2><p>This is not to be construed as financial advice.</p><h2>Acknowledgments</h2><p>Thanks to Ben Todd, Kit Harris, Alexander Gordon Brown, and James Snowden for helpful discussion on this manuscript. Any mistakes are my own.</p><h2>Introduction to \u201cMission Hedging\u201d</h2><p>How should a foundation whose only mission is to prevent dangerous climate change invest their endowment? Surprisingly, in order to maximize expected utility, it might use \u2018mission hedging\u2019 investment principles and invest in fossil fuel stocks. This way it has more money to give to organisations that combat climate change when more fossil fuels are burned, fossil fuel stocks go up and climate change will get particularly bad. When fewer fossil fuels are burnt and fossil fuels stocks go down - the foundation will have less money, but it does not need the money as much. Under certain conditions the mission hedging investment strategy maximizes expected utility.</p><p>So generally, if you want to do more good, should you invest in \u2018evil\u2019 corporations with negative externalities? Corporations that cause harm such as those that sell arms, tobacco, factory farmed meat, fossil fuels, or advance potentially dangerous new technology? Here I argue that, though perhaps counterintuitively, that this might be the optimal investment strategy.</p><p>In this note, I extend the special case of an investment strategy for foundation endowments called 'Mission hedging', originally introduced by Brigitte Roth Tran. The generalized strategy proposed here suggests that, under certain conditions, agents should invest resources in entities that cause activity they want to prevent. I will focus only on the conceptual extension of mission hedging here, but more technical details, caveating and mathematical formalism can be found in Roth Trans original paper, all of which are also relevant to the more generalized theory.</p><p>Roth Tran&nbsp;<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt2\"><sup>[2]</sup></a>&nbsp;summarizes the basic mechanics of mission hedging for foundations as follows:</p><p>\u201c\u201d[M]ission hedging,\" [is] a new strategy in which the endowment \u201cdoubles down,\" skewing investments toward firms it opposes. If increased objectionable activities coincide with both higher firm returns and greater foundation revenue needs (with which to counteract the objectionable activities), then the foundation can align funding availability with need by increasing exposure to objectionable firms beyond that of a typical portfolio. Increasing investment in objectionable firms creates a hedge around the foundation's mission, maximizing expected utility.\u201d</p><p>In other words, the basic idea is that, surprisingly, it might be optimal for an altruist whose mission is to combat global poverty, factory farming, mass unemployment, or existential risks from artificial intelligence, to invest in stocks of corporations that might make the problem worse and then give the profits to organisations that will counteract the problem.</p><p>For example, it might be a good strategy for donors or even other entities such as governmental organisations that are concerned with global health to invest in tobacco corporations and then give the profits to tobacco control lobbying efforts. Another example might be that animal welfare advocates should invest in companies engaged in factory farming such as those in meat packing industry, and then use the profits to invest in organisations that work to create lab grown meat. A final example: it might be optimal for donors who think that emerging risks from artificial intelligence is a pressing cause to invest in the technology companies that might speed up such dangerous technologies, and then donate the profits to organisations that work on guarding against such risks.</p><p>The basic mechanics of the mission hedging investment strategy are the following: when a bad industry does well, you as an investor can use your increased profits or dividends to counteract this trend. In other words, the more harm the industry does, e.g. increases CO2 emissions, sells more meat, or is on more on track to create technology that destroys jobs or is otherwise dangerous, the more you will profit and you can then use these profits to prevent the industry from doing bad things (potentially through donations or grants). If the bad industry is not doing well, then you will not profit as much, but you don't need to donate as much anyway because there is less bad activity. So either way, you will always donate a more optimal amount of money in proportion to the bad activity level.</p><p>Here is a simple toy model that illustrates the climate change case.</p><p>First consider, the following figure (taken from<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt3\"><sup>[3]</sup></a>) that shows that there is uncertainty about the world\u2019s emission pathway and how how high warming will be above pre industrial levels:</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995763/mirroredImages/iZp7TtZdFyW8eT5dA/l9knrp9y3ui3mh1yyinw.png\"></figure><p>Now consider how the oil price will develop correspondingly:</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995763/mirroredImages/iZp7TtZdFyW8eT5dA/wzz44jsazib3mdpkmrti.png\"></figure><p>Now consider the following toy model (<a href=\"https://www.google.com/url?q=https://docs.google.com/spreadsheets/d/1B83J_PKQln6F9V7shDE-1zpvex7no3pgIZyBb3OgqhA/edit?usp%3Dsharing&amp;sa=D&amp;ust=1519131522082000&amp;usg=AFQjCNGpexQ845IOrSzQs6NECM1VxEEm1w\">The spreadsheet can be found here</a>):</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995763/mirroredImages/iZp7TtZdFyW8eT5dA/htcferveikq5xa8aki0f.png\"></figure><p>As you can see a $1 million endowment will reap different profits&nbsp;if mission hedging investment principles are used under two different warming scenarios. If warming is relatively mild, at e.g. 1 degrees celsius above preindustrial levels, that means that fewer fossil fuels were burned, and fossil fuel stock didn\u2019t do as well: the resulting profits are below market rate returns at a meager $10,000. However, in the second scenario with 4 degrees of warming above preindustrial levels, it\u2019s likely that more fossil fuel were burned than expected and fossil fuel stocks did very well. The dividends in this case might be above market rate returns at $60,000. Note that the average return with mission hedging ($35,000) of these equally probable scenarios (p=0.5), is lower than the average financial returns if no mission hedging is used ($50,000) because the portfolio is optimally diversified (historically average rate of return is about 5%&nbsp;<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt4\"><sup>[4]</sup></a>).</p><p>Now note how the cost-effectiveness of climate change interventions (e.g. saving the rainforest, geoengineering,lobbying for more renewable energy R&amp;D is different in the two warming scenarios): if warming is relatively mild then the cost-effectiveness of the average climate change intervention won\u2019t be as high (here I assign 10 QALYs per $1000 spent). However, if warming gets really severe as in the 4 degrees warming scenario, then the cost-effectiveness of climate change interventions might be much higher at 100 QALYs per $1000 spent. In this case the average number of QALYs gained under mission hedging is higher (3,050 QALYs gained) than the number of QALYs gained without mission hedging (2,750).</p><p>However, mission hedging is not necessarily limited to investing endowments on the stock market. Someone interested in career choice, political campaign contributions, or budget allocation amongst different parts of government might also be advised to use mission hedging. For instance, someone worried about the risks associated with a, say, authoritarian leaning political party coming into power or a corporation having large negative externalities and outsized regulatory capture, might invest career capital into that said political party or company and later use it prevent the bad activity.</p><p>I list more examples in the&nbsp;table 1&nbsp;below.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995763/mirroredImages/iZp7TtZdFyW8eT5dA/mevp0np14nfquzhby3ck.png\"></figure><p>Table 1: Examples of mission hedging investments for different cause areas or missions an altruistic agent might pursue</p><p>Mission hedging and related concepts might also be a useful tool to provide partial answers to the following questions:</p><ul><li>When should one invest to tackle a certain problem? Now or later?</li><li>How much of its endowment should a foundation spend within a given year?&nbsp;<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt5\"><sup>[5]</sup></a></li><li>How much should a government agency or international organisation tasked with a certain mission spend within a given time frame?</li><li>What should we do with regards to uncertainty in the timeline of existential risks from climate change or artificial intelligence?&nbsp;<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt6\"><sup>[6]</sup></a></li><li>How should you allocate your resources between different mission objectives or causes due to uncertainty of whether some objectives or causes are more important than others?</li></ul><h2>Conditions for mission hedging to be optimal</h2><h3>Markets must be efficient</h3><p>There are some obvious objections to this strategy. Intuitively, you might think it's better to divest from companies that do things that you think are making the problem worse, because you increase the capital supply to a company that will then create more supply of the socially harmful good. In contradistinction to divestment, the 'mission hedging' strategy explicitly asks you to actually double down on investments in companies you believe to be harmful. However, divestment is unlikely to affect the stock price or the cash flow of the targeted companies at all.</p><p>This argument has been made elsewhere in much detail&nbsp;2,<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt7\"><sup>[7]</sup></a>. In brief, if you divest from say, fossil fuel for ethical reasons, then an investor who doesn't care about climate change will happily buy the now undervalued fossil fuel stocks. She might do so by selling her stocks of socially neutral companies (e.g. medical technology corporations) that you now need to buy because you don't want to invest in fossil fuels. The end result is no difference in the allocation in capital or stock price.</p><p>Thus, in efficient markets, such as the stock market, no matter how you invest, the companies and their share prices will likely not be affected substantially. Interestingly, the divestment strategy, which has become popular in the context climate change advocacy, is now also adopted in other areas such as open science, where socially responsible investment managers divest from pharma corporations which do not publish all their findings&nbsp;<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt8\"><sup>[8]</sup></a>. Thus, it might be increasingly important to prevent attention and resources to be diverted to ineffective divestment strategies.</p><p>In any case, the first condition that is crucially important for any area where mission hedging is successfully applied: the market, construed in the broadest sense, that you are investing in must be efficient or else a mission hedging investor risks doing harm instead of good.</p><p>So crucially, mission hedging will backfire if markets are inefficient and resources such as time, money and effort would counterfactually enable bad activity. For example, outside of the stock market where markets are less efficient it is easier to cause actually cause bad activity, by, for instance, providing seed funding for a startup to create a new harmful tobacco product . Unlike on the stock market, in this example, the seed investment might not have occurred otherwise and harm is caused by the investor through the invention of technology that might otherwise not have been invented.</p><h3>Investment must be in entities that cause the bad activity and not merely covary</h3><p>The second condition is that, ideally, your investment should not just covary with the bad activity but actually be causing the bad activity<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt9\"><sup>[9]</sup></a>. For example, one might have the intuition that an organisation tasked with ensuring global health such as the WHO, should invest their endowment in the medical device industry: as global health gets worse, more medical devices will be sold and so the WHO will profit, which gives them more resources to improve global health. As global health gets better, fewer medical devices are sold and so the WHO does not need as many resources to fulfill its mission of ensuring good global health. However, medical devices do not cause global ill-health. Even though medical device sales and the stock prices of corporations selling these devices should often covary, they are merely correlated. One can imagine cases where medical devices sell poorly, and yet global health is poor, or cases where medical devices sell very well, but global health is good. This is why it\u2019s better to invest in corporations that directly cause the bad activity, in this case tobacco. However, a portfolio that strongly covaries with the bad activity level might still be good and have fewer reputational risks.</p><h3>Corporation that one invests in must not themselves hedge against risks excessively</h3><p>Corporations often hedge against their own mission risks by investing in things that could hurt their profits and they need to report the use of derivatives in their financial statements, including their purpose and its hedging effectiveness&nbsp;<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt10\"><sup>[10]</sup></a>. For instance, PHW \u2014 a billion-dollar German poultry company \u2014 recently invested in a lab grown meat startup and Tyson, a big meat packing corporation invests in plant-based burgers<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt11\"><sup>[11]</sup></a>. Similarly, oil companies often invest in solar. &nbsp;If these companies hedge to the extent that an investment in them just represents investing in a diversified portfolio of the whole industry (e.g. the food industry or the energy industry), then mission hedging will not work. However, it seems that in the cases above, the utility maximizing effect will simply be diluted.</p><h3>Mission hedging might work well for extreme risks</h3><p>The mission hedging investment principle might also work for risks from emerging technology such artificial intelligence, especially in a slow takeoff scenario<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt12\"><sup>[12]</sup></a>&nbsp;. However, note that the case has been made that hedging against extreme risks might be difficult (\u201cYou cannot short the apocalypse\u201d<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt13\"><sup>[13]</sup></a>).</p><p>Also, the relationship between the stock price of the objectionable asset class and the cost-effectiveness of the intervention might be nonlinear. For instance, it might have an inverted U shape, in climate change scenario where at first mission hedging applies, the stock price of fossil fuel goes up, and the cost-effectiveness of interventions goes up, but then the stock price might go up even further, yet the cost-effectiveness of doing something against climate change might decrease, because it is too late to do something against climate change.</p><p>More complicated financial products might be used to hedge against a risk such as strangles, which are option derivatives that allows the holder to profit based on how much the price of the underlying security moves, with relatively minimal exposure to the direction of price movement.</p><h3>Mission hedging might be&nbsp;non trivial&nbsp;in practise</h3><p>It might be difficult to find the right corporations to invest in practise. Hedging is a research field within finance that is extensively studied: for instance, there is still active research around what the best hedge against inflation is&nbsp;<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt14\"><sup>[14]</sup></a>. It might be non-trivial to figure out how to properly hedge against mission risks.</p><h2>Extension of the strategy to shareholder activism</h2><h3>A mission hedging collective might bring an harmful industry to its knees</h3><p>Interestingly, unlike with divestment strategies where potentially a large majority of total global capital would be needed to divest from companies, collective action of a much smaller percentage of the market using the mission hedging strategy could in some cases actually hurt harmful industries in terms of their stock price.</p><p>A big investor or a like-minded, value aligned investor collective could use mission hedging instead of divestment to damage industries that cause bad activity. Recall that if a harmful industry does well the investor collective (or large foundation endowment) will profit relatively more and can ramp up their anti-industry activities (such as lobbying against it). If an investor collective and their activity became widely known, such that the market would anticipate its actions, stock prices would go down inevitably. Stock prices would not even go up in the face of \u2018positive shocks\u2019 in that industry. Why is that? Imagine a new oil reservoir being discovered. Normally, the market would respond by buying more shares in the company that has discovered this reservoir and the price of shares would go up. However, if the market knows that there's an investor collective that will profit from this and actually use these profits to fight the industry from making further profits of fossil fuels, they might not invest despite the positive shock. There would be nowhere for the stock price to go but down. The rest of the market might divest from industries that are known to be under 'attack' of such a collective.</p><p>However, it is important to carefully consider whether the combined capital of such a collective is really large enough to affect the industry and are not dwarfed by counter lobbying activities of the harmful corporations that are trying to maximize shareholder value of socially neutral investors.</p><p>By definition, \u2018big X- industries\u2019, i.e. Big Oil, Big Pharma, Big Tobacco etc., that have negative externalities make billions in profits. However, one can use a bit of game theory in order to not need to take on the entire industry: a mission hedging investor (collective) could publically commit to always take on the current market leader with investment and lobbying and thus threaten a larger share of the market at once. This is similar to the strategies employed by some animal advocacy groups, which declare that they will not stop putting public pressure on a fast food chain until they only use cage free eggs in their products<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt15\"><sup>[15]</sup></a>.</p><p>In any case, the size of the mission hedging collective would have to be much smaller to drive down stock prices than the size of a divestment collective. This is because a small percentage of socially neutral investors with access to capital could always buy up sin stocks by using leverage, even if trillions of dollars of assets under management have already been divested from fossil fuel stocks<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt16\"><sup>[16]</sup></a>. They can do so with because they have the prospect of slightly higher than market rate returns due the exploitation of investors who use the divestment strategy and they can borrow for less than the market rate returns, thus clearing the market.</p><p>Another example: governments could use a fixed percentage of their tobacco tax revenue to counteract smoking (e.g. by running public health campaigns). This would make the industry less lucrative for investors, because the industry is working against itself - the better the companies do in terms of selling tobacco, the higher the tax receipts the government can use to educate smokers to quit.</p><p>One last example, one could buy life insurance for political dissidents in authoritarian regimes that receive death threats and announce that one would donate the proceeds to efforts to fight the regime.</p><p>One way to implement this mechanism would be using self-executing smart contracts on the blockchain, that would pay out in these cases.</p><h3>Shareholder activism and inside information</h3><p>When investing in a company that causes bad activity one can also use shareholder activism and try to reduce bad activity from within the industry. This is another advantage as compared to divestment.</p><h2>Applications for resource allocation between different causes</h2><p>Some foundations have trying to reduce some uncertainty on how to allocate resources amongst different causes to different buckets&nbsp;<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt17\"><sup>[17]</sup></a>. Investment using mission hedging might also be a way to optimize resource allocation between different missions or causes that an altruist thinks are likely to be important.</p><p>Imagine you want to make your donations or decide on your foundations strategy on how much you're going to spend on different causes. Let\u2019s say you decide based on moral and cost-effectiveness grounds that you will allocate 60% of your resources to animal welfare causes, 30% to try to prevent risks from advanced technology and 10% to global health at the end of the year.</p><p>You might&nbsp;not&nbsp;want to simply allocate the profits of your endowment by allocating, 60% to animal welfare causes, 30% to technological risks and 10% global health, based on how pressing and important each of the causes you think are. Rather, you could use the wisdom of the crowds (here the crowd is the market) to finesse those estimates and decide at the beginning of the year to invest 60% in meat industry stocks, 30% in technology stock, and 10% in tobacco stock. At the end of the year you allocate the profits or dividends to organisations in the respective cause areas. Then, relative to how well the different industries do that year, you will assign relatively more or less money to each cause (e.g. because tobacco stocks did very well and you have relatively more money in the global health portfolio, you end up with 15% of your overall donations going to tobacco control and because technology profits were poor this year, you only donate 20% of your overall donation to nonprofits working on technological risk prevention).</p><p>Why is this be a more optimal strategy? There are two major reasons:</p><p>&nbsp;</p><ol><li>There are also diminishing marginal returns to investing into a problem that becomes smaller and thus the cost-effectiveness of one's efforts to tackle a problem becomes less favourable. Take the global health example from above: as global health becomes better and there are fewer smokers, tobacco control efforts will be less effective at trying to get the last smokers to quit (e.g mass media interventions will not be as effective). Fewer resources should be invested relative to other causes or mission objective an investor might have.</li><li>It reduces the uncertainty about the (future) scale of the problem by tapping into the \u2018wisdom of the markets\u2019. Mission hedging is theoretically equivalent to using a very efficient prediction market (e.g. the stock market) to inform your decisions. Given that an agent might be uncertain about the (future) scale of the problem, using the stock market as a prior might shine light on this issue. However, the stock market is not a perfect prediction device for cost-effectiveness of an intervention and can only serve as prior or parameter estimate in one\u2019s model of the cost-effectiveness of an intervention</li></ol><p>This strategy can be added on top off other prioritization considerations (e.g. scenario analyses as described in<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt18\"><sup>[18]</sup></a>).</p><p>Mission hedging might not only work for endowments of foundations, but also for budgets of big international organisations or government departments with multiple mandates, or even with government budgets. As a foundation one could also offer this as a \u2018banking service\u2019 where one pays the grant into an account that is invested according to mission hedging principles and the charity can spend based on this.</p><h2>International organisations: example of the International Monetary Fund</h2><p>It might be good to convince not only foundation to skew their endowments, but also convince international institutions or governments to use mission hedging for their \u2018endowments\u2019 or accounts.</p><p>Consider for instance the International Monetary Fund (IMF). The IMFs stated mission is \u201cto foster global monetary cooperation, secure financial stability, facilitate international trade, promote high employment and sustainable economic growth, and reduce poverty around the world.\"&nbsp;<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt19\"><sup>[19]</sup></a>.</p><p>The IMF has billions in assets<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt20\"><sup>[20]</sup></a>. However, even though the IMF diversifies its assets by geography and asset classes against market risks, and hedges against foreign exchange volatility etc., it seems to to not hedge against mission risks.</p><p>The IMF could consider skewing their portfolio towards technology companies which have been suggested to create technological unemployment, in order to have more money so that it has more resources to ensure its mission of promoting high employment in case technology companies do well.</p><p>There appear to be many such international funds which might want to invest their endowments using mission hedging principles (e.g. another is the Strategic Climate Fund&nbsp;<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt21\"><sup>[21]</sup></a>).</p><h2>Applications outside of investment</h2><h3>Career choice</h3><p>Mission hedging&nbsp;might&nbsp;also be a strategy for career choice, where time is invested to hedge against risks that threaten one's mission or cause. For instance, one might believe that tobacco companies are particularly harmful, but choose to work at a tobacco corporation. After investing career capital one can use this to work against the harmful corporation in case it becomes particularly harmful. This might occur in the form lobbying for more socially responsible behaviour within the company, whistleblowing, industrial sabotage or earning to give with higher wages or equity in a harmful industry. Potentially this behaviour might be particularly effective if the company is on the verge of performing a particularly harmful action that other competitors would likely not perform. Note that this related, but slightly different and extended argument from the classical \u2018earning to give in socially harmful industries\u2019<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt22\"><sup>[22]</sup></a>&nbsp;and not simply an argument for high replaceability.</p><p>As mentioned above, whether mission hedging will work when it comes to careers will crucially depend on whether the job market is efficient in the sector. Big publicly traded company or big bank has many job applicants and when one takes a job with them, one only replaces a competitor who is only marginally less capable worker from working at the company by taking a job in a harmful industry (this is of course not say that all work at big corporations or in finance is socially harmful)<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt23\"><sup>[23]</sup></a>. However, joining a smaller, socially harmful business that has a harder time finding competent workers and less efficient hiring, might actively cause harm because there is a better chance that contributing to the success of the business will cause counterfactual harm over and above what the next candidate in line would have caused.</p><p>Finally, this strategy might be particularly effective when dealing with extreme, high stakes, low probability risks. For instance, someone might have the highest impact in terms of expected value by having a career in the military or the secret service. Even though the modal outcome of this career might not be very impactful, in the unlikely event of a war, military government etc., one\u2019s impact might be very high. Thus joining the military might have the highest counterfactual impact in terms of expected value. Historical analysis suggests that soldiers have sometimes single handedly prevented nuclear war<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt24\"><sup>[24]</sup></a>. Another example would be working at a leading artificial intelligence company that could create dangerous technology and using whistleblowing when they are on the verge of a breakthrough.</p><h3>Political donations</h3><p>One could argue that political donations occur in \u2018efficient markets\u2019 as well. In the past US elections, there have been cases in which billionaires donated a few millions, i.e. a tiny fraction of their networth (0.1% percent) to other billionaires campaigns (for instance, Peter Thiel donated to Donald Trump)<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt25\"><sup>[25]</sup></a>. These billionaires might have funded their campaigns themselves if it had not been for the donation. In exchange these billionaires might have gained political influence due to their contribution, which can be used to further their mission and to counteract bad activity from a bad politician, even though counterfactually the donation didn\u2019t have any influence as the politician would have been funded anyway.</p><h3>Retirement saving and hedging against unemployment in \u2018earning to give\u2019</h3><p>Someone who does \u2018earning to give' might want to skew their personal retirement fund towards technology companies, if they feel their job is at risk due to technological unemployment. They might also want to invest outside of the economy they work in&nbsp;<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt26\"><sup>[26]</sup></a>, in case it does poorly and the person doing earning to give is unable to relocate. They might also want to buy disability insurance.</p><p>It would be interesting to know whether big sovereign wealth fund / pension funds such as the Norwegian pension fund already do this (hedge against risks to their economy, technological unemployment, shifting demographics to a population with fewer people in working age). Much thought goes into<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt27\"><sup>[27]</sup></a>&nbsp;how to invest $25 trillion in private pensions funds in OECD countries<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt28\"><sup>[28]</sup></a>&nbsp;in a way that is best for their investors. The answer to this question might elucidate whether this mission hedging strategy has flaws, is difficult to implement in practise, etc.</p><h2>Limitations</h2><p>Interestingly, mission hedging decidedly skews the portfolio away from diversification and thus does not maximize risk-adjusted financial returns. Most endowments are trying to maximize financial returns, but with mission hedging one moves away from an optimally diversified, (passively invested) global market portfolio<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt29\"><sup>[29]</sup></a>. Thus, one will likely sacrifice financial returns. For a \u2018cause neutral\u2019 agent, who doesn\u2019t have a mission, favourite cause, or mandate, it might be best to not sacrifice the flexibility of switching causes and rather invest purely as to maximize financial returns. This is similar to the strategy of building up career capital in the face of uncertainty about what the most important cause is.</p><p>Also, because mission hedging is somewhat counterintuitive and people might be repulsed by being associated as profiting from e.g. evil corporations, there might be reputational risks associated with it that need to be factored in. If this is a decisive factor, then it might be better to buy stocks that are merely correlate with bad activity - such as buying stock in the pharmaceutical industry rather than the tobacco industry. One could also buy derivatives that merely track the stock price, and which would be&nbsp;de facto&nbsp;stocks for all intents and purposes, but an investor would not \u2018own\u2019 part the company.</p><p>However, there might be a way of using some upsides of mission hedging without investing in what might be seen as morally reprehensible companies. Say your foundation focuses on two rather than one focus areas (which in itself might be suboptimal<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt30\"><sup>[30]</sup></a>). Say area 1 is animal welfare and area 2 is global poverty. Your prior intuition is that these areas are equally important and you assign a 50-50 split of your annual disbursements to these two cause areas. Now, you could invest your entire endowment in&nbsp;stock in the developing world countries where your foundation supports say cash-transfer program. We will assume that corporations in those country doing well causes&nbsp;poverty reduction and they are not seen as morally reprehensible. Now, relative to how the developing world stock portfolio does you decide to invest excess profits (over the standard stock market return) to the other area (here animal welfare). If the developing world stock does well, you might not need to invest as much in poverty reduction, and have more funds for the more neglected animal welfare area. If the stock doesn\u2019t do as well, and there are no excess profits, it is better to stick closer to the original 50-50 split.</p><h2>Other quick thoughts</h2><ul><li>The process of hedging might also help to put clarify one's mission itself. If there would be a lot of resistance for the IMF to skew their portfolio towards technology companies because they state their mission is to keep unemployment low, and the hedge is accepted to work, then maybe the IMF is not really pursuing its mission or mandate. Mission hedging might clarify the mission of an organisation or agent, because one \u2018puts their money where their mouth is\u2019.</li><li>Catastrophe (CAT) bonds are now a $29 billion dollar market, and provide coverage against hurricanes, earthquakes, and pandemics&nbsp;<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt31\"><sup>[31]</sup></a>. There are new developments in this area of disaster insurance<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt32\"><sup>[32]</sup></a>&nbsp;with developments such as the creation over the counter catastrophe swaps<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt33\"><sup>[33]</sup></a>.There is also ongoing research on cyber insurance and catastrophes on the cyberspace&nbsp;<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt34\"><sup>[34]</sup></a>&nbsp;and on liability of future robotics technology&nbsp;<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt35\"><sup>[35]</sup></a></li><li>\u201cBetterment Investing just added a no-cost automatic donation feature. Using their existing tax-optimized system, they allow you to donate your most appreciated shares directly to any of their many connected charities. This gives you the maximum tax deduction right now, while reducing your taxes further when you later withdraw from your account later in life\u201d&nbsp;<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt36\"><sup>[36]</sup></a></li><li>There are some artificial intelligence ETFs<a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt37\"><sup>[37]</sup></a>&nbsp;that one could look into to hedge against risk from emerging technologies</li></ul><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><hr><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref1\">[1]</a>&nbsp;email:&nbsp;<a href=\"mailto:h@ea.do\">h@ea.do</a>.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref2\">[2]</a>&nbsp;\"Divest, Disregard, or Double Down? by Brigitte Roth Tran :: SSRN.\" 13 Apr. 2017,&nbsp;<a href=\"https://www.google.com/url?q=https://papers.ssrn.com/sol3/papers.cfm?abstract_id%3D2952257&amp;sa=D&amp;ust=1519131522095000&amp;usg=AFQjCNE-KcR-dI5xU5p8GR3r1E20W_VaOg\">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2952257</a>. Accessed 2 Jun. 2017.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref3\">[3]</a>&nbsp;\"Betting on negative emissions | Nature Climate Change.\" 21 Sep. 2014,&nbsp;<a href=\"https://www.google.com/url?q=https://www.nature.com/articles/nclimate2392&amp;sa=D&amp;ust=1519131522114000&amp;usg=AFQjCNFXO1Qqw-gttA2zjm8sBGsxWb_SgQ\">https://www.nature.com/articles/nclimate2392</a>. Accessed 20 Feb. 2018.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref4\">[4]</a>&nbsp;\"What is the average annual return for the S&amp;P 500? | Investopedia.\"&nbsp;<a href=\"https://www.google.com/url?q=https://www.investopedia.com/ask/answers/042415/what-average-annual-return-sp-500.asp&amp;sa=D&amp;ust=1519131522115000&amp;usg=AFQjCNFTjfk8CRGD4kymRpNB5fecuF8z3Q\">https://www.investopedia.com/ask/answers/042415/what-average-annual-return-sp-500.asp</a>. Accessed 20 Feb. 2018.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref5\">[5]</a>&nbsp;\"Should the Open Philanthropy Project be Recommending More/Larger ...\" 2015. 19 Sep. 2016 &lt;<a href=\"https://www.google.com/url?q=http://www.openphilanthropy.org/blog/should-open-philanthropy-project-be-recommending-morelarger-grants&amp;sa=D&amp;ust=1519131522095000&amp;usg=AFQjCNFkFuNMFN-f950AGOQdFPDVCCWewg\">http://www.openphilanthropy.org/blog/should-open-philanthropy-project-be-recommending-morelarger-grants</a>&gt;</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref6\">[6]</a>&nbsp;\"What Do We Know about AI Timelines? | Open Philanthropy Project.\" 2015. 19 Sep. 2016 &lt;<a href=\"https://www.google.com/url?q=http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-timelines&amp;sa=D&amp;ust=1519131522096000&amp;usg=AFQjCNHGZbfbETURYcrLmIDa8rs9Lefs7g\">http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-timelines</a>&gt;</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref7\">[7]</a>&nbsp;\"How Investors Can (and Can't) Create Social Value | Stanford Social ....\" 8 Dec. 2016,&nbsp;<a href=\"https://www.google.com/url?q=https://ssir.org/up_for_debate/article/how_investors_can_and_cant_create_social_value&amp;sa=D&amp;ust=1519131522097000&amp;usg=AFQjCNHxVipdJSWMiRObGlF_TBXG_57DEA\">https://ssir.org/up_for_debate/article/how_investors_can_and_cant_create_social_value</a>. Accessed 2 Jun. 2017.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref8\">[8]</a>&nbsp;\"Investment managers back greater transparency of clinical trials | The ....\" 23 Jul. 2015,&nbsp;<a href=\"https://www.google.com/url?q=http://www.bmj.com/content/351/bmj.h4002&amp;sa=D&amp;ust=1519131522097000&amp;usg=AFQjCNEB79g0_nCzjgvu6u9ZJT2c1a2i8Q\">http://www.bmj.com/content/351/bmj.h4002</a>. Accessed 5 Jun. 2017.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref9\">[9]</a>&nbsp;\"Selecting investments based on covariance with the value of charities ....\" 4 Feb. 2017,&nbsp;<a href=\"https://www.google.com/url?q=http://effective-altruism.com/ea/16u/selecting_investments_based_on_covariance_with/&amp;sa=D&amp;ust=1519131522107000&amp;usg=AFQjCNEI2SO2y814vIUzd6ZdsmGtBm6wVw\">http://effective-altruism.com/ea/16u/selecting_investments_based_on_covariance_with/</a>. Accessed 17 Feb. 2018.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref10\">[10]</a>&nbsp;<a href=\"https://www.google.com/url?q=https://link.springer.com/article/10.1007/s12197-017-9399-5&amp;sa=D&amp;ust=1519131522112000&amp;usg=AFQjCNEtREJjKhpvwdbIJAxzNXKIc7aABQ\">https://link.springer.com/article/10.1007/s12197-017-9399-5</a>&nbsp;</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref11\">[11]</a>&nbsp;\"Taxes on Meat Could Join Carbon and Sugar to Help Limit Emissions ....\" 11 Dec. 2017,&nbsp;<a href=\"https://www.google.com/url?q=http://www.fairr.org/news-item/taxes-meat-join-carbon-sugar-help-limit-emissions/&amp;sa=D&amp;ust=1519131522109000&amp;usg=AFQjCNFEYq3M9Ux-iBJ5xZwSSuEqakTeEg\">http://www.fairr.org/news-item/taxes-meat-join-carbon-sugar-help-limit-emissions/</a>. Accessed 17 Feb. 2018.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref12\">[12]</a>&nbsp;\"Strategic considerations about different speeds of AI takeoff - Future of ....\" 12 Aug. 2014,&nbsp;<a href=\"https://www.google.com/url?q=https://www.fhi.ox.ac.uk/strategic-considerations-about-different-speeds-of-ai-takeoff/&amp;sa=D&amp;ust=1519131522098000&amp;usg=AFQjCNGSny7So8trwiAh7jFPJy6qownA8w\">https://www.fhi.ox.ac.uk/strategic-considerations-about-different-speeds-of-ai-takeoff/</a>. Accessed 8 Oct. 2017.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref13\">[13]</a>&nbsp;\"Can You Short the Apocalypse? - Marginal REVOLUTION.\" 12 Aug. 2017,&nbsp;<a href=\"https://www.google.com/url?q=http://marginalrevolution.com/marginalrevolution/2017/08/can-short-apocalypse.html&amp;sa=D&amp;ust=1519131522098000&amp;usg=AFQjCNEOnntEi3SuOwHNSKaj19co51LNiw\">http://marginalrevolution.com/marginalrevolution/2017/08/can-short-apocalypse.html</a>. Accessed 8 Oct. 2017.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref14\">[14]</a>&nbsp;\"IJFS | Free Full-Text | A Study of Perfect Hedges - MDPI.\" 14 Nov. 2017,&nbsp;<a href=\"https://www.google.com/url?q=http://www.mdpi.com/2227-7072/5/4/28&amp;sa=D&amp;ust=1519131522110000&amp;usg=AFQjCNEwf1_je1o-k8BhajC0UKG4MbtCOw\">http://www.mdpi.com/2227-7072/5/4/28</a>. Accessed 17 Feb. 2018.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref15\">[15]</a>&nbsp;\"Ending factory farming as soon as possible - 80,000 Hours.\" 27 Sep. 2017,&nbsp;<a href=\"https://www.google.com/url?q=https://80000hours.org/2017/09/lewis-bollard-end-factory-farming/&amp;sa=D&amp;ust=1519131522105000&amp;usg=AFQjCNFNQzSvN3yxEfY3UKrxCj79s7M6FQ\">https://80000hours.org/2017/09/lewis-bollard-end-factory-farming/</a>. Accessed 17 Feb. 2018.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref16\">[16]</a>&nbsp;\"Investment Funds Worth Trillions Are Dropping Fossil Fuel Stocks ....\" 12 Dec. 2016,&nbsp;<a href=\"https://www.google.com/url?q=https://www.nytimes.com/2016/12/12/science/investment-funds-worth-trillions-are-dropping-fossil-fuel-stocks.html&amp;sa=D&amp;ust=1519131522099000&amp;usg=AFQjCNEjfsDQhodLA71pWFDNOgEBR6OSUw\">https://www.nytimes.com/2016/12/12/science/investment-funds-worth-trillions-are-dropping-fossil-fuel-stocks.html</a>. Accessed 5 Jun. 2017.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref17\">[17]</a>&nbsp;\"Update on Cause Prioritization at Open Philanthropy | Open ....\" 26 Jan. 2018,&nbsp;<a href=\"https://www.google.com/url?q=https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy&amp;sa=D&amp;ust=1519131522108000&amp;usg=AFQjCNGQzq8ARWSvCEKbzHA_KN-UsillgA\">https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy</a>. Accessed 17 Feb. 2018.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref18\">[18]</a>&nbsp;<a href=\"https://www.google.com/url?q=http://benjaminrosshoffman.com/givewell-case-study-effective-altruism-2&amp;sa=D&amp;ust=1519131522100000&amp;usg=AFQjCNE5cgFtVo45P5lVN2xNYAQ6qKha5w\">http://benjaminrosshoffman.com/givewell-case-study-effective-altruism-2</a></p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref19\">[19]</a>&nbsp;\"International Monetary Fund - Wikipedia.\"&nbsp;<a href=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/International_Monetary_Fund&amp;sa=D&amp;ust=1519131522100000&amp;usg=AFQjCNFJdXWAAvjggtKQDgarjbDLSadyDw\">https://en.wikipedia.org/wiki/International_Monetary_Fund</a>. Accessed 8 Oct. 2017.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref20\">[20]</a>&nbsp;\"The IMF at a Glance.\"&nbsp;<a href=\"https://www.google.com/url?q=http://www.imf.org/en/About/Factsheets/IMF-at-a-Glance&amp;sa=D&amp;ust=1519131522101000&amp;usg=AFQjCNExzC5orfzhemI6hPZ-kEHm5GvEyQ\">http://www.imf.org/en/About/Factsheets/IMF-at-a-Glance</a>. Accessed 8 Oct. 2017.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref21\">[21]</a>&nbsp;\"Clean Technology Fund | Climate Investment Funds.\"&nbsp;<a href=\"https://www.google.com/url?q=https://www.climateinvestmentfunds.org/fund/clean-technology-fund&amp;sa=D&amp;ust=1519131522101000&amp;usg=AFQjCNHwMbLYjL60GViYjxHEMmDgAMm_8Q\">https://www.climateinvestmentfunds.org/fund/clean-technology-fund</a>. Accessed 8 Oct. 2017.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref22\">[22]</a>&nbsp;\"Just how bad is being a CEO in big tobacco? - 80,000 Hours.\" 21 Jan. 2016,&nbsp;<a href=\"https://www.google.com/url?q=https://80000hours.org/2016/01/just-how-bad-is-being-a-ceo-in-big-tobacco/&amp;sa=D&amp;ust=1519131522105000&amp;usg=AFQjCNE0Z9dZYksgVQYVk2YU4LT0aB7STg\">https://80000hours.org/2016/01/just-how-bad-is-being-a-ceo-in-big-tobacco/</a>. Accessed 17 Feb. 2018.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref23\">[23]</a>&nbsp;\"The difference between true and tangible impact - 80,000 Hours.\"&nbsp;<a href=\"https://www.google.com/url?q=https://80000hours.org/articles/true-vs-tangible-impact/&amp;sa=D&amp;ust=1519131522102000&amp;usg=AFQjCNExQz2RFv8XdK6_wThxex4PXAOXEg\">https://80000hours.org/articles/true-vs-tangible-impact/</a>. Accessed 5 Jun. 2017.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref24\">[24]</a>&nbsp;\"Why the long-term future of humanity matters more than anything else ....\"&nbsp;<a href=\"https://www.google.com/url?q=https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/&amp;sa=D&amp;ust=1519131522102000&amp;usg=AFQjCNH1nG6XDjNOxE9QgT-z9LwBzI8FJQ\">https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/</a>. Accessed 8 Oct. 2017.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref25\">[25]</a>&nbsp;\"Peter Thiel Has Been Hedging His Bet On Donald Trump - BuzzFeed.\" 7 Aug. 2017,&nbsp;<a href=\"https://www.google.com/url?q=https://www.buzzfeed.com/ryanmac/peter-thiel-and-donald-trump&amp;sa=D&amp;ust=1519131522103000&amp;usg=AFQjCNHLC4YcvNFN6s-S78Oe3BEMYKAlQg\">https://www.buzzfeed.com/ryanmac/peter-thiel-and-donald-trump</a>. Accessed 20 Aug. 2017.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref26\">[26]</a>&nbsp;\"Do Investors Put Too Much Stock in the U.S.? | Michael Dickens.\" 26 Mar. 2017,&nbsp;<a href=\"https://www.google.com/url?q=http://mdickens.me/2017/03/26/do_investors_put_too_much_stock_in_the_us/&amp;sa=D&amp;ust=1519131522104000&amp;usg=AFQjCNF60GB4Fwr-lQ9s0OUoKhzaIhwISg\">http://mdickens.me/2017/03/26/do_investors_put_too_much_stock_in_the_us/</a>. Accessed 8 Oct. 2017.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref27\">[27]</a>&nbsp;\"A two-step hybrid investment strategy for pension funds - ScienceDirect.\"&nbsp;<a href=\"https://www.google.com/url?q=https://www.sciencedirect.com/science/article/pii/S1062940816301887&amp;sa=D&amp;ust=1519131522112000&amp;usg=AFQjCNGZq9IBym6gv43R8xabmszdYfHQ_Q\">https://www.sciencedirect.com/science/article/pii/S1062940816301887</a>. Accessed 18 Feb. 2018.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref28\">[28]</a>&nbsp;\"Macro- and micro-dimensions of supervision of large pension ... - IOPS.\"&nbsp;<a href=\"https://www.google.com/url?q=http://www.iopsweb.org/WP-30-Macro-Micro-Dimensions-Supervision-LPFs.pdf&amp;sa=D&amp;ust=1519131522108000&amp;usg=AFQjCNG9c52AnnR5P8A6JKO-lz2pqE8kwQ\">http://www.iopsweb.org/WP-30-Macro-Micro-Dimensions-Supervision-LPFs.pdf</a>. Accessed 17 Feb. 2018.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref29\">[29]</a>&nbsp;\"Meet the Global Market Portfolio -- The 'Optimal Portfolio For ... - Forbes.\" 30 Jul. 2014,&nbsp;<a href=\"https://www.google.com/url?q=https://www.forbes.com/sites/phildemuth/2014/07/30/meet-the-global-market-portfolio-the-optimal-portfolio-for-the-average-investor/&amp;sa=D&amp;ust=1519131522106000&amp;usg=AFQjCNF2Cmc7ogRSHPb-560n2wjW6U45vg\">https://www.forbes.com/sites/phildemuth/2014/07/30/meet-the-global-market-portfolio-the-optimal-portfolio-for-the-average-investor/</a>. Accessed 17 Feb. 2018.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref30\">[30]</a>James Snowden - Does risk aversion give us a good reason to diversify our charitable portfolio? ceppa.wp.st-andrews.ac.uk/files/2016/04/snowden_eac.ppt &nbsp;</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref31\">[31]</a>&nbsp;\"Pandemic bonds, a new idea - Fighting disease with finance.\" 27 Jul. 2017,&nbsp;<a href=\"https://www.google.com/url?q=https://www.economist.com/news/finance-and-economics/21725589-world-bank-creates-new-form-finance-pandemic-bonds-new-idea&amp;sa=D&amp;ust=1519131522106000&amp;usg=AFQjCNE7TCsbAvWiny67sqI869WU57kQaQ\">https://www.economist.com/news/finance-and-economics/21725589-world-bank-creates-new-form-finance-pandemic-bonds-new-idea</a>. Accessed 17 Feb. 2018.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref32\">[32]</a>&nbsp;\"Economic instruments - IIASA PURE.\"&nbsp;<a href=\"https://www.google.com/url?q=http://pure.iiasa.ac.at/13904/1/Chapter4-ENHANCE.pdf&amp;sa=D&amp;ust=1519131522114000&amp;usg=AFQjCNFDkDJga0czFcnh4sOPUeoTDJH5lg\">http://pure.iiasa.ac.at/13904/1/Chapter4-ENHANCE.pdf</a>. Accessed 18 Feb. 2018.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref33\">[33]</a>&nbsp;\"Loading Pricing of Catastrophe Bonds and Other Long-Dated ....\" 31 Oct. 2016,&nbsp;<a href=\"https://www.google.com/url?q=https://arxiv.org/abs/1610.09875&amp;sa=D&amp;ust=1519131522113000&amp;usg=AFQjCNEkV4V7tr8qgZiw6aU7QMVUa38zJA\">https://arxiv.org/abs/1610.09875</a>. Accessed 18 Feb. 2018.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref34\">[34]</a>&nbsp;\"Cyber Insurance - Springer Link.\" 27 Jun. 2017,&nbsp;<a href=\"https://www.google.com/url?q=https://link.springer.com/10.1007/978-3-319-06091-0_25-1&amp;sa=D&amp;ust=1519131522110000&amp;usg=AFQjCNEUJTeD-v43T1Vd-Mv1h-C8hM0l4g\">https://link.springer.com/10.1007/978-3-319-06091-0_25-1</a>. Accessed 18 Feb. 2018.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref35\">[35]</a>&nbsp;Baum (2017):&nbsp;<a href=\"https://www.google.com/url?q=https://books.google.co.uk/books?hl%3Den%26lr%3D%26id%3DN3QzDwAAQBAJ%26oi%3Dfnd%26pg%3DPT73%26ots%3Dtknk3i2--K%26sig%3DFbSwIynpOmYuYae-IXB2XdJgslE%26redir_esc%3Dy%23v%3Donepage%26q%26f%3Dfalse&amp;sa=D&amp;ust=1519131522111000&amp;usg=AFQjCNGAuFVdCp2iKCivoSoIAf_OyKT9Xw\">https://books.google.co.uk/books?hl=en&amp;lr=&amp;id=N3QzDwAAQBAJ&amp;oi=fnd&amp;pg=PT73&amp;ots=tknk3i2--K&amp;sig=FbSwIynpOmYuYae-IXB2XdJgslE&amp;redir_esc=y#v=onepage&amp;q&amp;f=false</a>&nbsp;</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref36\">[36]</a>&nbsp;\"How to Give Money (and Get Happiness) More Easily.\" 4 Dec. 2017,&nbsp;<a href=\"https://www.google.com/url?q=http://www.mrmoneymustache.com/2017/12/04/how-to-give-money-and-get-happiness-more-easily/comment-page-3/&amp;sa=D&amp;ust=1519131522109000&amp;usg=AFQjCNGMSG5ZrbkqzqhlM7-_oVujIkCGMQ\">http://www.mrmoneymustache.com/2017/12/04/how-to-give-money-and-get-happiness-more-easily/comment-page-3/</a>. Accessed 17 Feb. 2018.</p><p><a href=\"https://docs.google.com/document/d/e/2PACX-1vTXgIxXyF3DX9LP8OfgNWFMVzBfPf1aRokJ4F1eav_rrzhZAdHDqpEOxqyuuBq6619nqcFI-LqWFUH5/pub#ftnt_ref37\">[37]</a>&nbsp;\"Top 15 Artificial Intelligence ETFs - ETF ....\" 18 Feb. 2018,&nbsp;<a href=\"https://www.google.com/url?q=http://etfdb.com/themes/artificial-intelligence-etfs/&amp;sa=D&amp;ust=1519131522113000&amp;usg=AFQjCNGgZTuw1iqoFaoDH1mm-ieeRnZWGw\">http://etfdb.com/themes/artificial-intelligence-etfs/</a>. Accessed 18 Feb. 2018.</p>", "user": {"username": "HaukeHillebrandt"}}, {"_id": "X2n6pt3uzZtxGT9Lm", "title": "Doing good while clueless", "postedAt": "2018-02-15T05:04:25.291Z", "htmlBody": "<p>This is the fourth (and final) post in a series exploring <a href=\"https://flightfromperfection.com/cluelessness-what-to-do.html\">consequentialist cluelessness</a> and its implications for effective altruism:</p><ul><li>The <a href=\"https://forum.effectivealtruism.org/ea/1hh/what_consequences/\">first post</a> describes cluelessness &amp; its relevance to EA; arguing that for many popular EA interventions we don\u2019t have a clue about the intervention\u2019s overall net impact.</li><li>The <a href=\"https://forum.effectivealtruism.org/ea/1ix/just_take_the_expected_value_a_possible_reply_to/\">second post</a> considers a potential reply to concerns about cluelessness.</li><li>The <a href=\"https://forum.effectivealtruism.org/ea/1j4/how_tractable_is_cluelessness/\">third post</a> examines how tractable cluelessness is \u2013 to what extent we can grow more clueful about an intervention through intentional effort?</li><li><strong>This post</strong> discusses how we might do good while being clueless to an important extent.</li></ul><p>Consider reading the previous posts (<a href=\"https://forum.effectivealtruism.org/ea/1hh/what_consequences/\">1</a>, <a href=\"https://forum.effectivealtruism.org/ea/1ix/just_take_the_expected_value_a_possible_reply_to/\">2</a>, <a href=\"https://forum.effectivealtruism.org/ea/1j4/how_tractable_is_cluelessness/\">3</a>) first.</p><hr class=\"dividerBlock\"/><p>The <a href=\"https://forum.effectivealtruism.org/ea/1j4/how_tractable_is_cluelessness/\">last post</a>\u00a0looked at whether we could grow more clueful by intentional effort. It\u00a0concluded that, for the foreseeable future, we will probably remain clueless about the long-run impacts of our actions to a meaningful extent, even after taking measures to improve our understanding and foresight.</p><p>Given this state of affairs, we should act cautiously when trying to do good. This post outlines a framework for doing good while being clueless, then looks at\u00a0what this framework\u00a0implies\u00a0about current EA cause prioritization.</p><p>The following only make sense if you already believe that the far future matters a lot; this argument has been made <a href=\"https://forum.effectivealtruism.org/ea/6l/a_relatively_atheoretical_perspective_on/\">elegantly elsewhere</a> so we won\u2019t rehash it here.[1]</p><h1>An analogy:\u00a0interstellar\u00a0travel</h1><p>Consider a spacecraft, journeying out into space. The occupants of the craft are searching for a star system to settle. Promising destination systems are all very far away, and the voyagers don\u2019t have a complete map of how to get to any of them. Indeed, they know very little about the space they will travel through.</p><p>To have a good journey, the voyagers will have to successfully steer their ship (both literally &amp; metaphorically). Let&#x27;s use &quot;steering capacity&quot; as an umbrella term that refers to the capacity needed to have a successful journey.[2] &quot;Steering capacity&quot; can be broken down into the following five attributes:[3]</p><ul><li>The voyagers must have a clear idea of what they are looking for. (<strong>Intent</strong>)</li><li>The voyagers must be able to reach agreement about where to go. (<strong>Coordination</strong>)</li><li>The voyagers must be discerning enough to identify promising systems as promising, when they encounter them. Similarly, they must be discerning enough to accurately identify threats &amp; obstacles. (<strong>Wisdom</strong>)</li><li>Their craft must be powerful enough to reach the destinations they choose. (<strong>Capability</strong>)</li><li>Because the voyagers travel through unmapped territory, they must be able to see far enough ahead to avoid obstacles they encounter. (<strong>Predictive power</strong>)</li></ul><p>This spacecraft is a useful analogy for thinking about our civilization\u2019s trajectory. Like us, the space voyagers are somewhat clueless \u2013 they don\u2019t know quite where they should go (though they can make guesses), and they don\u2019t know how to get there (though they can plot a course and make adjustments along the way).</p><p>The five attributes given above \u2013 intent, coordination, wisdom, capability, and predictive power \u2013 determine how successful the space voyagers will be in arriving at a suitable destination system. These same attributes can also serve as a useful framework for considering which altruistic interventions we should prioritize, given our present situation. \u00a0</p><h1>The basic point</h1><p>The basic point here is that interventions whose main known effects do not improve our steering capacity (i.e. our intent, wisdom, coordination, capability, and predictive power) are not as important as interventions whose main known effects do improve these attributes.</p><p>An implication of this is that\u00a0interventions whose effectiveness is driven mainly by their <a href=\"https://forum.effectivealtruism.org/ea/1hh/what_consequences/\">proximate impacts</a> are less important than interventions whose effectiveness is driven mainly by increasing our steering capacity.</p><p>This is because any action we take is going to have indirect &amp; long-run consequences that bear on our civilization\u2019s trajectory. Many of the long-run consequences of our actions are unknown, so the future is unpredictable. Therefore, we ought to prioritize interventions that improve the wisdom, capability, and coordination of future actors, so that they are better positioned to address future problems that we did not foresee.</p><h1>What being clueless means for altruistic prioritization</h1><p>I think\u00a0the steering capacity framework implies a portfolio approach to doing good \u2013 simultaneously pursuing a large number of diverse hypotheses about how to do good, provided that\u00a0each approach\u00a0maintains <a href=\"https://www.centreforeffectivealtruism.org/blog/hard-to-reverse-decisions-destroy-option-value/\">reversibility</a>.[4]</p><p>This approach is similar to the Open Philanthropy Project\u2019s <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">hits-based giving framework</a> \u2013 invest in many promising initiatives with the expectation that most will fail.</p><p>Below, I look at how this framework interacts with focus areas that effective altruists are already working on. Other causes that EA has not looked into closely (e.g. improving education) may also perform well under this framework; assessing causes of this sort is beyond the scope of this essay.</p><p>My\u00a0thinking here is preliminary, and very probably contains errors &amp; oversights.</p><h1>EA focus areas to prioritize</h1><p>Broadly speaking, the steering capacity framework suggests prioritizing interventions that:[5]</p><ul><li>Further our understanding of what matters</li><li>Improve governance</li><li>Improve prediction-making &amp; foresight</li><li>Reduce existential risk</li><li>Increase the number of well-intentioned, highly capable people</li></ul><p></p><h3>To prioritize \u2013 better understanding what matters</h3><p>Increasing our understanding of what\u2019s worth caring about is important for clarifying our intentions about what trajectories to aim for. For many moral questions, there is already broad agreement in the EA community (e.g. the view that all currently existing human lives matter is uncontroversial within EA). On other questions, further thinking would be valuable (e.g. how best to compare human lives to the lives of animals).</p><p>Myriad thinkers have done valuable work on this question. Particularly worth mentioning is the work of the <a href=\"https://foundational-research.org/\">Foundational Research Institute</a>, the <a href=\"http://globalprioritiesproject.org/\">Global Priorities Project</a>, the <a href=\"https://qualiaresearchinstitute.org/\">Qualia Research Institute</a>, as well the <a href=\"https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood\">Open Philanthropy Project\u2019s work on consciousness &amp; moral patienthood</a>.</p><p></p><h3>To prioritize \u2013 improving governance</h3><p>Improving governance is largely aimed at improving coordination \u2013 our ability to mediate diverse preferences, decide on collectively held goals, and work together towards those goals.</p><p>Efficient governance institutions are robustly useful in that they keep focus oriented on solving important problems &amp; minimize resource expenditure on zero-sum competitive signaling.</p><p>Two routes towards improved governance seem promising: (1) improving the functioning of existing institutions, and (2) experimenting with alternative institutional structures (Robin Hanson\u2019s <a href=\"http://mason.gmu.edu/~rhanson/futarchy.html\">futarchy proposal</a> and <a href=\"https://www.seasteading.org/\">seasteading</a> initiatives are examples here).</p><p></p><h3>To prioritize \u2013 improving foresight</h3><p>Improving foresight &amp; prediction-making ability is important for informing our decisions. The further we can see down the path, the more information we can incorporate into our decision-making, which in turn leads to higher quality outcomes with fewer surprises.</p><p>Forecasting ability can definitely be improved from baseline, but there are probably hard limits on how far into the future we can extend our predictions while remaining believable.</p><p>Philip Tetlock\u2019s <a href=\"https://www.goodjudgment.com/\">Good Judgment Project</a> is a promising forecasting intervention, as are prediction markets like <a href=\"https://www.predictit.org/\">PredictIt</a> and polling aggregators like <a href=\"http://fivethirtyeight.com/\">538</a>.</p><p></p><h3>To prioritize \u2013 reducing existential risk</h3><p>Reducing existential risk can be framed as \u201cavoiding large obstacles that lie ahead.\u201d Avoiding extinction and \u201clock-in\u201d of suboptimal states is necessary for realizing the full potential benefit of the future.</p><p>Many initiatives are underway in the x-risk reduction cause area. <a href=\"https://forum.effectivealtruism.org/ea/1iu/2018_ai_safety_literature_review_and_charity/\">Larks\u2019 annual review of AI safety work</a> is excellent; Open Phil has good material about <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks\">projects focused on other x-risks</a>.</p><p></p><h3>To prioritize \u2013 increase the number of well-intentioned, highly capable people</h3><p>Well-intentioned, highly capable people are a scarce resource, and will almost certainly continue to be highly useful going forward. Increasing the number of well-intentioned, highly capable people seems robustly good, as such people are able to diagnosis &amp; coordinate together on future problems as they arise.</p><p>Projects like <a href=\"http://rationality.org/\">CFAR</a> and <a href=\"https://sparc-camp.org/\">SPARC</a> are in this category.</p><p>In a different vein, <a href=\"https://www.enthea.net/\">psychedelic experiences hold promise as a treatment</a> for treatment-resistant depression, and may also improve the intentions of highly capable people who have not reflected much about what matters (\u201cthe betterment of well people\u201d).</p><p></p><h1>EA focus areas to deprioritize, maybe</h1><p>The steering capacity framework suggests deprioritizing animal welfare &amp; global health interventions, to the extent that these interventions\u2019 effectiveness is driven by their proximate impacts.</p><p>Under this framework, prioritizing animal welfare &amp; global health interventions may be justified, but only on the basis of improving our intent, wisdom, coordination, capability, or predictive power.</p><h3>To deprioritize, maybe \u2013 animal welfare</h3><p>To the extent that animal welfare interventions expand our civilization\u2019s <a href=\"http://www.stafforini.com/docs/Singer%20-%20The%20expanding%20circle.pdf\">moral circle</a>, they may hold promise as interventions that improve our intentions &amp; understanding of what matters (the <a href=\"https://www.sentienceinstitute.org/\">Sentience Institute</a> is doing work along this line).</p><p>However, following this framework, the case for animal welfare interventions has to be made on these grounds, not on the basis of cost-effectively reducing animal suffering in the present.</p><p>This is because the animals that are helped in such interventions cannot help \u201csteer the ship\u201d \u2013 they cannot contribute to making sure that our civilization\u2019s trajectory is headed in a good direction.</p><p></p><h3>To deprioritize, maybe \u2013 global health</h3><p>To the extent that global health interventions improve coordination, or reduce x-risk by increasing socio-political stability, they may hold promise under the steering capacity framework.</p><p>However, the case for global health interventions would have to be made on the grounds of increasing coordination, reducing x-risk, or improving another steering capacity attribute. Arguments for global health interventions on the grounds that they cost-effectively help people in the present day (without consideration of how this bears on our future trajectory) are not competitive under this framework.</p><p></p><h1>Conclusion</h1><p>In sum, I think the fact that we are intractably clueless implies a portfolio approach to doing good \u2013 pursuing, in parallel, a large number of diverse hypotheses about how to do good.</p><p>Interventions that\u00a0improve\u00a0our understanding of what matters, improve governance, improve prediction-making ability, reduce existential risk, and increase the number of well-intentioned, highly capable people are all promising. Global health &amp; animal welfare interventions may hold promise as well, but the case for these cause areas needs to be made on the basis of improving our steering capacity, not on the basis of their proximate impacts.</p><p></p><p><em>Thanks to members of the Mather essay discussion group and an anonymous collaborator for thoughtful feedback on drafts of this post. Views expressed above\u00a0are my own. Cross-posted to <a href=\"https://www.lesserwrong.com/posts/iZBK6PboA66Zb6dmt/doing-good-while-clueless\">LessWrong</a> &amp; my <a href=\"https://flightfromperfection.com/doing-good-while-clueless.html\">personal blog</a>.</em></p><hr class=\"dividerBlock\"/><h2>Footnotes</h2><p>[1]: Nick Beckstead has done the best work I know of on the topic of why the far future matters. <a href=\"https://forum.effectivealtruism.org/ea/6l/a_relatively_atheoretical_perspective_on/\">This post</a> is a good introduction; for a more in-depth treatment see his PhD thesis, <a href=\"https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxuYmVja3N0ZWFkfGd4OjExNDBjZTcwNjMxMzRmZGE\">On the Overwhelming Importance of Shaping the Far Future</a>.</p><p>[2]:\u00a0I&#x27;m grateful to Ben Hoffman for discussion that fleshed out the &quot;steering capacity&quot; concept; see <a href=\"http://benjaminrosshoffman.com/seeding-productive-culture/\">this comment thread</a>.\u00a0</p><p>[3]: Note that this list of attributes is not exhaustive &amp; this metaphor isn&#x27;t perfect. I&#x27;ve found the space travel metaphor useful for\u00a0thinking about cause prioritization given our uncertainty about the far future, so am deploying it here.</p><p>[4]: Maintaining reversibility is important because given our cluelessness, we are unsure of the net impact of any action. When uncertain about overall impact, it\u2019s important to be able to walk back actions that we come to view as net negative.</p><p>[5]: I&#x27;m not sure of how to\u00a0prioritize these things amongst themselves. Probably improving our understanding of what matters &amp; our predictive power are highest priority, but that&#x27;s a very weakly held view.</p><p></p>", "user": {"username": "Milan_Griffes"}}, {"_id": "CbYgMbEPDWgafPfyL", "title": "On funding medical\u00a0research", "postedAt": "2018-02-15T03:07:36.649Z", "htmlBody": "<html><body><p>&#x201C;Which medical research should be receive more funding&#x201D; is a hugely important question, with hundreds of *billions* of dollars spent on medical research yearly. It&#x2019;s also one that is not always approached in a rational and evidence-based way, which is understandable, as most people think about this question when they or their loved ones ends up suffering from a particular disease.</p>\n<p>As an EA, it&#x2019;s somewhat ironic that I started thinking about medical research because someone close to me became disabled from a horrible illness, but researching this illness gave me lots of ideas that could be more widely applicable to cause prioritisation within medical research. I&#x2019;ll use this illness as a case study, using the <a href=\"https://concepts.effectivealtruism.org/concepts/importance-neglectedness-tractability/\">ITN framework</a>.</p>\n<h1>Neglectedness</h1>\n<p>I&#x2019;ve deliberately not named the illness yet, as there is <a href=\"http://nationalacademies.org/hmd/~/media/Files/Report%20Files/2015/MECFS/MECFS_KeyFacts.pdf\">significant controversy about its name</a>. In fact, there are various controversies around this illness, which we&#x2019;ll look at in more detail. The controversies both contribute to and are a symptom of its neglectedness, which makes it quite interesting for researchers and EAs to objectively investigate.</p>\n<p>Here I&#x2019;ll use the name myalgic encephalomyelitis, or ME. It&#x2019;s an <a href=\"https://www.cdc.gov/me-cfs/about/index.html\">illness characterised by extreme fatigue</a>, which in the <a href=\"https://www.youtube.com/watch?v=si_JJf-SVnQ\">most severe cases</a> can lead to not being able to move the body at all, beyond basic functions like breating. It is commonly accompanied by pain. There is no cure, very few medicine to manage symptoms, and the cause is not understood at all. Researchers have observed immunological and neurological abnormalities, but that&#x2019;s about it. <a href=\"https://www.researchgate.net/profile/Leonard_Jason/publication/236995875_The_Face_of_CFS_in_the_U.S/links/00b7d51acf6823bccb000000.pdf\">Tens of millions of people</a>&#xA0;worldwide fit the diagnosis, with <a href=\"http://www.nationalacademies.org/hmd/Reports/2015/ME-CFS.aspx\">about a quarter of them</a> being homebound or even unable to leave their bed.</p>\n<p>The history of controversy is a nasty one. It is now known that it&#x2019;s a disease that affects the immune system, and diseases like that <a href=\"http://ajp.amjpathol.org/article/S0002-9440(10)61635-5/abstract\">disproportionally affect women</a>, for reasons still unknown. This is a well-established, empirical fact. But in the past, the condescending, patriarchal society would label this &#x201C;mass hysteria&#x201D;. The revised name became &#x201C;Chronic Fatigue Syndrome&#x201D;, which is a bit better, but still trivialises this crippling disease (&#x201C;I&#x2019;m always tired too&#x201D;). This history continues in different forms, with some insurance companies and governmental bodies still <a href=\"http://apt.rcpsych.org/content/aptrcpsych/8/5/359.full.pdf\">labelling the disease</a> as a &#x201C;psycho-social disorder&#x201D;. This is an obvious cop-out for a disease that has virtually no objective lab tests to confirm it, but which is very real nonetheless.</p>\n<p>Diseases can be real&#x200A;&#x2014;&#x200A;causing symptoms and suffering&#x200A;&#x2014;&#x200A;and yet have no objective tests, simply because science has not advanced enough. This then causes a downward spiral: victims are shamed and blamed (&#x201C;just get off your ass&#x201D;), insurance companies see a way to improve their bottom line (&#x201C;we don&#x2019;t cover psychological conditions&#x201D;), doctors are powerless at best and <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3691015/\">harmful at worst</a> (&#x201C;do some exercise, that&#x2019;s good for everyone&#x201D;), patients get desperate and turn to alternative medicine and conspiracy theories, people see that and decide that this disease really must be a sham, and finally researchers stay away in fear of their funding drying up or even losing their jobs.</p>\n<p>This is very interesting to EAs. It&#x2019;s like mis-pricing of stocks, where investors act emotionally instead of rationally. If we can identify diseases for which this happens, and &#x201C;correct the valuation&#x201D;, we could not only correct the amount of funding a cause receives, but potentially even break this downward spiral. We could attract funding from traditional sources which have eschewed these causes because of stigma. I don&#x2019;t know of where to allocate funds to most effectively make that happen, be it in basic research, or public campaigns, or lobbying, or something else, but it seems a huge opportunity to me.</p>\n<h1>Importance</h1>\n<p>A standard measure of disease burden is <a href=\"https://concepts.effectivealtruism.org/concepts/the-global-burden-of-disease/\">DALY</a>, Disability-Adjusted Life Years, where you count the number of premature deaths because of an illness, plus the number of &#x201C;equivalent&#x201D; years lost due to disability. What equivalent means here <a href=\"https://oxpr.io/blog/2017/3/2/on-daly-disability-weights\">is tricky</a>, and there are various ways of measuring it, based on quality of life, or loss of function, or how many years with a disability people are willing to trade in for one year of premature death, and so on. In the end a disease gets a &#x201C;disability weight&#x201D; (DW) to indicate how much suffering it causes, between 0 (perfect health) and 1 (death).</p>\n<p>Because ME gets neglected by researchers, good data for determining the disease burden of ME is scarce. For example, ME was not included in&#xA0;the <a href=\"http://www.thelancet.com/gbd\">2016 Global Burden of Disease (GBD) study</a>. A <a href=\"http://www.oatext.com/Estimating-the-disease-burden-of-MECFS-in-the-United-States-and-its-relation-to-research-funding.php\">recent study</a> looked at two methods of estimating disability weights to come up with a weight of 0.46 (and for the one quarter of patients with severe ME, 0.76). It&#x2019;s fascinating how this is estimated, and I highly recommend reading the underlying studies!</p>\n<p>For comparison, here are the ME disability weights put in context (based on the 2016 GBD study). There are a ton of caveats in comparing these, but this should give you a rough idea.</p>\n<table>\n<tbody>\n<tr>\n<th>Condition</th>\n<th>Disability weight (DW)</th>\n</tr>\n<tr>\n<td>ME (weighted average)</td>\n<td>0.46</td>\n</tr>\n<tr>\n<td>ME (severe)</td>\n<td>0.76</td>\n</tr>\n<tr>\n<td>Multiple sclerosis (moderate)</td>\n<td>0.463</td>\n</tr>\n<tr>\n<td>Multiple sclerosis (severe)</td>\n<td>0.719</td>\n</tr>\n<tr>\n<td>Dementia (severe)</td>\n<td>0.449</td>\n</tr>\n<tr>\n<td>AIDS (without treatment)</td>\n<td>0.582</td>\n</tr>\n<tr>\n<td>HIV/AIDS (with treatment)</td>\n<td>0.078</td>\n</tr>\n<tr>\n<td>Anxiety disorder (severe)</td>\n<td>0.523</td>\n</tr>\n<tr>\n<td>Autism</td>\n<td>0.262</td>\n</tr>\n<tr>\n<td>Loss of one leg (without treatment)</td>\n<td>0.173</td>\n</tr>\n<tr>\n<td>Asthma (controlled)</td>\n<td>0.015</td>\n</tr>\n</tbody>\n</table>\n<p>The other part is premature deaths. Suicide is at a ridiculously high rate for ME patients, about 17 times the national US average (compared with 2 times the average for cancer and MS patients and 6 times the average for patients with depression). There also seems to be a link with heart disease and cancer. Taking all that into account, plus the disability weights, plus a conservative estimate of about 1 million patients in the United States with ME, the estimated disease burden in the US is <a href=\"http://www.oatext.com/Estimating-the-disease-burden-of-MECFS-in-the-United-States-and-its-relation-to-research-funding.php\">714,000 DALY</a>.</p>\n<p>To get a sense of context here, compare this with <a href=\"https://report.nih.gov/info_disease_burden.aspx\">2015 US DALYs and NIH spending</a>:</p>\n<table>\n<tbody>\n<tr>\n<th>DALY</th>\n<th>NIH spending</th>\n<th>spending per DALY</th>\n</tr>\n<tr>\n<td>ME: 0.71M</td>\n<td>$7M</td>\n<td>$9.8</td>\n</tr>\n<tr>\n<td>Multiple sclerosis: 0.28M</td>\n<td>$94M</td>\n<td>$331</td>\n</tr>\n<tr>\n<td>Cancer: 13.5M</td>\n<td>$5389M</td>\n<td>$399</td>\n</tr>\n<tr>\n<td>Mental illness: 7.7M</td>\n<td>$2263M</td>\n<td>$292</td>\n</tr>\n<tr>\n<td>Heart disease: 7.8M</td>\n<td>$426M</td>\n<td>$55</td>\n</tr>\n</tbody>\n</table>\n<p>Looking at the raw data, there is quite a bit of variation in funding, but estimated funding for ME per DALY is just insanely low. Using the previous analogy, this is almost literally mispricing in the market!</p>\n<p>All of this is still too limited for well-weighted decision for EA standards. For example, how does this extrapolate to *global* disease burdens instead of just the US? How robust&#xA0;are the analyses and the underlying studies? What kind of error bars are we talking about when comparing numbers here? I couldn&#x2019;t find answers to those questions, so this is fairly speculative, but the several-orders-of-magnitudes underfunding based on DALYs by the NIH does suggest that there might be something interesting here.</p>\n<h1>Tractability</h1>\n<p>Now, underfunding doesn&#x2019;t mean that it&#x2019;s a good use of your dollars. Is there &#x201C;<a href=\"https://concepts.effectivealtruism.org/concepts/room-for-more-funding/\">room for more funding</a>&#x201D;?</p>\n<p>I have two conflicting intuitions about this: first of all, since ME is so dramatically underfunded, any kind of research into the fundamental causes of the disease should find at least something, because there are such severe symptoms that basically no-one has looked into yet using modern methods. On the other hand, it could be hard to find someone competent because of the stigma and misunderstanding.</p>\n<p>In the case of ME though, there seems to be some momentum. A few years ago, the director of the Stanford Genome Technology Center, Ron Davis, saw his son fell severely ill with ME, <a href=\"https://stanmed.stanford.edu/2016spring/the-puzzle-solver.html\">after which he started a research group</a> to look into the disease. He also started a foundation and got endorsements for ME research from his extensive network, including several Nobel prize winners and NAS members. When you start looking, you find similar stories, such as the Cornell ME/CFS center, which was founded by <a href=\"http://me-pedia.org/wiki/Maureen_Hanson\">someone whose family member has ME</a>.</p>\n<p>These people are highly skilled, and might never have gotten into ME research if it weren&#x2019;t for family members getting it. But now they are highly motivated, and in the position to break out of the downward spiral of stigma. If they can even find a *hint* of a physical cause (&#x201C;biomarker&#x201D;), the names of institutions like Stanford or Cornell would be behind a legitimisation of the disease. This could lead to more funding, more decent research, and even better outcomes for patients in the short term, through increased awareness and education among doctors.</p>\n<p>Timing is crucial here. You want to have a good team with a research proposal that has some chance of finding something, so you want to wait for that. But you also don&#x2019;t want to wait too long, or the effort might lose momentum&#x200A;&#x2014;&#x200A;researchers might give up or retire, a group of well-intentioned Nobel Prize winners may fall apart, and so on.</p>\n<p>In the Stanford case, it might be the perfect time for an intervention by EAs. They have a <a href=\"https://www.omf.ngo/the-end-mecfs-project-2/\">plan</a> of doing a wide range of state-of-the-art tests on a small number of very severe patients, like the son of the director. This both saves money and increases the likelihood of finding something, as any biological effects will probably be more pronounced in such sick patients. They can do these tests cheaply because their lab has (co-)invented&#xA0;some of the technology that they&#x2019;re testing with. They raised enough funding from grants and private funds to collect the samples and get preliminary results, but need more money to continue doing tests and analysis. Money is actually the blocker, so each additional dollar would immediately be put to use.</p>\n<h1>Conclusion</h1>\n<p>For an underfunded, misunderstood, and important disease like ME, and others like it, it&#x2019;s hard to say where exactly to spend your money to make a difference. There are a number of different things to target, and they all reinforce each other: more funding for research, more public awareness, better education of doctors, and so on. For ME, a lot of groups focus on public awareness, with online campaigns, protests, and a <a href=\"https://www.unrest.film/\">recent award-winning documentary</a> by a Harvard student with ME. It looks like those efforts are helping, with recent results including revised <a href=\"https://www.nice.org.uk/guidance/cg53/resources/surveillance-report-2017-chronic-fatigue-syndromemyalgic-encephalomyelitis-or-encephalopathy-diagnosis-and-management-2007-nice-guideline-cg53-4602203537/chapter/how-we-made-the-decision\">government</a> <a href=\"https://www.npr.org/sections/health-shots/2017/10/02/554369327/for-people-with-chronic-fatigue-syndrome-more-exercise-isnt-better\">guidelines</a> and <a href=\"https://www.omf.ngo/2018/02/02/pineapple-fund/\">a surprisingly huge&#xA0;anonymous cryptocurrency donation</a>. That said, I&#xA0;suspect that&#xA0;additional dollars would be best spent on medical research.</p>\n<p>When learning about ME, I had a lot of ideas relevant for EA. Some might be already known, or perhaps even debunked, but maybe there are some new ones in here, too:</p>\n<ul>\n<li>Stigma could be an indicator of neglected diseases.</li>\n<li>Association with alternative medicine and questionable practices could be such an indicator too. There could be more to it when you dig deeper, as people can get desperate for a solution.</li>\n<li>Stigma and association with questionable practices can amplify the neglectedness of a disease.</li>\n<li>Basic research could have an outsized impact on such diseases, by breaking through stigma.</li>\n<li>When removing stigma from a field of research, that could attract funding and scientists, thus amplifying our dollars.</li>\n<li>This effect might be bigger than that of activist campaigns, per dollar contributed.</li>\n<li>In the long term there could be a lot of room for funding in diseases with low funding per DALY.</li>\n<li>In the short term however, it might be hard to find good research worth contributing to.</li>\n<li>It might be worth finding top researchers who have a family member suffering from a disease, as they will be motivated.</li>\n<li>This will be easier the more widespread a disease is, thus correlating with its importance.</li>\n<li>I think proper timing in selecting good research is key, in order to accelerate something that has some momentum.</li>\n</ul>\n<p>I&#x2019;m still an EA noob, but I suspect that a small amount of funding towards top research into diseases like this could be quite competitive, especially compared to other interventions in the developed world. What do you think?</p></body></html>", "user": {"username": "JanPaul123"}}, {"_id": "Z6FoocxsPfQdyNX3P", "title": "Where Some People Donated in 2017", "postedAt": "2018-02-11T21:55:09.730Z", "htmlBody": "<html><body><p><em>Edited 2018-02-14 to include myself.</em></p>\n<p><em>Edited soon after publication to include ACE staff members&#x2019; donations. Thanks to the Facebook commenter who pointed me to them.</em></p>\n<p>This is a collection of writings on where people are donating. It only includes writings that I am aware exist (obviously) and that are written by effectiveness-minded people.</p>\n<p>My descriptions are paraphrased from the linked writings as much as possible. The writing in this post includes combinations of my own and the linked writers&#x2019; words. My summaries often do not do the original writers justice, so I recommend reading all of the linked articles if you are interested.</p>\n<h3><a href=\"http://mdickens.me/2016/10/31/where_i_am_donating_in_2016/\">Michael Dickens</a></h3>\n<p>In 2017 I donated to the <a href=\"http://www.gfi.org/\">Good Food Institute</a> because I believed it presented a particularly good opportunity for reducing animal suffering and antispeciesism in expectation. (Original writeup says I donated in 2016 but I actually donated in early 2017.)</p>\n<h3><a href=\"http://www.zachgroff.com/2017/12/why-i-am-donating-to-wild-animal.html\">Zach Groff</a></h3>\n<p>Zach is donating to <a href=\"https://was-research.org/\">Wild-Animal Suffering Research</a> (WASR) because he believes that: (1) animals in the wild suffer terror and pain on a massive scale, and virtually nobody even considers trying to address this problem; (2) he has been thoroughly impressed with his personal interactions with WASR staff as well as their output to date.</p>\n<h2><a href=\"/ea/1iu/2018_ai_safety_literature_review_and_charity/\">Ben Henry</a></h2>\n<p>Ben reviewed the research produced by a bunch of AI safety organizations. He <a href=\"http://www.rot13.com/\">rot13&#x2019;d</a> his donation decision to allow readers to come to their own conclusions; below is his own rot13&#x2019;d description.</p>\n<blockquote>\n<p>Fvtavsvpnag qbangvbaf gb gur Znpuvar Vagryyvtrapr Erfrnepu Vafgvghgr naq gur Tybony Pngnfgebcuvp Evfxf Vafgvghgr. N zhpu fznyyre bar gb NV Vzcnpgf.</p>\n</blockquote>\n<h2><a href=\"/ea/1ig/four_organizations_eas_should_fully_fund_for_2018/\">Peter Hurford</a></h2>\n<p>Peter used three main criteria:</p>\n<blockquote>\n<ol>\n<li>Have a clear &#x201C;room for more funding&#x201D;</li>\n<li>Have a clear risk of not meeting their funding goal</li>\n<li>Clear a bar of being &#x201C;impactful enough&#x201D; for the EA community to be worth funding</li>\n</ol>\n</blockquote>\n<p>He selected four organizations that he believed met these criteria: (1) Charity Science Health, (2) Rethink Charity, (3) Wild-Animal Suffering Research, and (4) Sentience Institute.</p>\n<h2><a href=\"https://medium.com/thinking-about-animals/thinking-behind-2017-donations-to-reduce-animal-use-9259253dbcbb\">Jason Ketola</a></h2>\n<p>Jason donated primarily to <a href=\"http://www.new-harvest.org/\">New Harvest</a> because he believes he believes their efforts will support the development of animal product alternatives and thus reduce animal suffering. He also considered donations to Good Food Institute, Plant Based Foods Association, Wild Animal Suffering Research, and Animal Ethics. He believes that New Harvest has a strong track record and substantial room for more funding &#x2013; &#x201C;New Harvest seems especially well poised to [improve animal product alternatives] with marginal donations&#x201D;.</p>\n<p>(Note: Jason frequently refers to &#x201C;we&#x201D; in his article, but I don&#x2019;t know who the other person is.)</p>\n<h3><a href=\"https://www.jefftk.com/p/2017-donations\">Jeff Kaufman and Julia Wise</a></h3>\n<ul>\n<li>50-50 split between things that directly do good and more speculative options</li>\n<li>For things that directly do good, they follow GiveWell&#x2019;s recommendations &#x2013; 70% to the <a href=\"https://www.givewell.org/charities/amf\">Against Malaria Foundation</a> and 30% to <a href=\"https://www.givewell.org/charities/Schistosomiasis-Control-Initiative\">Schistosomiasis Control Initiative</a></li>\n<li>For more speculative things, they want to put part of the money towards a project that a friend is starting, and the rest to the <a href=\"https://app.effectivealtruism.org/funds/ea-community\">EA Community Fund</a></li>\n</ul>\n<h2><a href=\"https://www.benkuhn.net/giving-2017\">Ben Kuhn</a></h2>\n<ul>\n<li>80% to a donor-advised fund, to be allocated later</li>\n<li>18% to GiveWell top charities (to be allocated at GiveWell&#x2019;s discretion)</li>\n<li>2% to GiveWell&#x2019;s operations</li>\n</ul>\n<p>Ben saved 80% to donate later because he believes most top candidate organizations are not particularly cash constrained right now, but he wanted to make a large donation in 2017 due to the recent tax bill.</p>\n<h2><a href=\"https://thezvi.wordpress.com/2017/12/17/i-vouch-for-miri/\">Zvi</a></h2>\n<p>Zvi donated to the Machine Intelligence Research Institute. He believes AI safety is &#x201C;the most important, urgent and under-funded cause.&#x201D; His private information and personal experience point to MIRI being a capable organization. Zvi was particularly impressed by MIRI&#x2019;s paper on <a href=\"https://intelligence.org/2017/10/22/fdt/\">Functional Decision Theory</a>.</p>\n<p>His post explains (i) why he believes AI safety matters, (ii) his analysis of MIRI&#x2019;s organizational quality, (iii) his analysis of MIRI&#x2019;s research, and (iv) general comments on donation.</p>\n<h2><a href=\"https://animalcharityevaluators.org/blog/where-the-ace-staff-members-are-giving-in-2017-and-why/\">Animal Charity Evaluators staff members</a></h2>\n<p>John Bockman:</p>\n<ul>\n<li>24% to Animal Charity Evaluators</li>\n<li>24% to Good Food Institute (GFI)</li>\n<li>20% to <a href=\"https://veganoutreach.org/\">Vegan Outreach</a></li>\n<li>8% to <a href=\"https://awfw.org/\">A Well-Fed World</a></li>\n<li>8% to <a href=\"https://bettereating.org/\">Better Eating International</a></li>\n<li>16% to <a href=\"http://encompassmovement.org/\">Encompass</a></li>\n</ul>\n<p>Allison Smith split her donation evenly between <a href=\"https://www.directactioneverywhere.com/\">Direct Action Everywhere</a> (DxE) and ACE&#x2019;s <a href=\"https://animalcharityevaluators.org/donate/\">Recommended Charity Fund</a>.</p>\n<p>Toni Adleberg donated or directed donations to GFI, Against Malaria Foundation, and Encompass, plus some small donations elsewhere.</p>\n<p>Sofia Davis-Fogel donated to:</p>\n<ul>\n<li><a href=\"http://www.animalequality.net/\">Animal Equality</a></li>\n<li><a href=\"https://www.nonhumanrights.org/\">The Nonhuman Rights Project</a></li>\n<li>Encompass</li>\n<li><a href=\"http://www.animanaturalis.org/es\">AnimaNaturalis Internacional</a></li>\n</ul>\n<p>Kieran Greig planned on donating to ACE because he believes donations to ACE have a higher expected value than donations to ACE top charities. He was interested in funding some other projects but was unaware of any highly promising donation targets for them:</p>\n<ul>\n<li>Corporate campaigns aimed at improving farmed fish welfare</li>\n<li>Humane insecticides</li>\n<li>Feature length documentaries advocating for farmed animals</li>\n<li>Genetic interventions to improve animal welfare</li>\n<li>Establishing wild animal suffering as an academic field</li>\n<li>Funding talented researchers to do effective animal advocacy research</li>\n</ul>\n<p>Jamie Spurgeon cycled through supporting various causes throughout the year, including ACE and GiveWell top charities. Later in the year he became increasingly confident that animal advocacy was particularly neglected; at the end of the year he gave all his donations to the ACE Recommended Charity Fund.</p>\n<p>Erika Alonso donated to:</p>\n<ul>\n<li>Animal Charity Evaluators</li>\n<li>Animal Equality</li>\n<li><a href=\"http://www.foodispower.org/\">Food Empowerment Project</a></li>\n<li>Good Food Institute</li>\n<li>Humane League</li>\n<li>Mercy for Animals</li>\n<li>New Harvest</li>\n<li><a href=\"https://strongminds.org/\">StrongMinds</a></li>\n</ul>\n<p>Eric Herboso donated to:</p>\n<ul>\n<li><a href=\"https://www.centreforeffectivealtruism.org/\">Centre for Effective Altruism</a></li>\n<li>GiveDirectly (for their Universal Basic Income initiative)</li>\n<li>Effective Altruism Funds (specifically the animal welfare and far future funds)</li>\n<li><a href=\"https://www.givingwhatwecan.org/\">Giving What We Can</a></li>\n<li>Machine Intelligence Research Institute</li>\n<li><a href=\"https://app.effectivealtruism.org/lotteries/\">EA donor lottery</a></li>\n<li>Encompass</li>\n</ul>\n<p>He made some additional donations for <a href=\"http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/\">fuzzies, not utilons</a>, including gifts intended to encompass the virtue of generosity.</p>\n<p>Gina Stuessy donated primarily to ACE&#x2019;s Recommended Charity Fund. She also donated to the Centre for Effective Altruism, <a href=\"https://www.sentienceinstitute.org/\">Sentience Institute</a>, and <a href=\"https://was-research.org/\">Wild-Animal Suffering Research</a>.</p>\n<h2><a href=\"https://blog.givewell.org/2017/12/11/staff-members-personal-donations-for-giving-season-2017/\">GiveWell staff members</a></h2>\n<p>Josh Rosenberg:</p>\n<ul>\n<li>80% to GiveWell for re-granting to GiveWell&#x2019;s top charities</li>\n<li>10% to the <a href=\"https://app.effectivealtruism.org/funds/far-future\">Long-Term Future Fund</a></li>\n<li>10% to farm animal welfare, probably based on <a href=\"https://animalcharityevaluators.org/\">Animal Charity Evaluators&#x2019;</a> recommendations</li>\n</ul>\n<p>Sophie Monahan is donating to <a href=\"https://www.givewell.org/charities/no-lean-season\">No Lean Season</a>, a GiveWell top-rated charity.</p>\n<p>Catherine Hollander:</p>\n<ul>\n<li>90% to Against Malaria Foundation</li>\n<li>10% to No Lean Season</li>\n</ul>\n<p>Isabel Arjmand:</p>\n<ul>\n<li>75% to GiveWell for re-granting</li>\n<li>5% to GiveDirectly</li>\n<li>20% to promote justice-focused causes, including <a href=\"https://cjjc.org/\">Causa Justa : Just Cause</a>, <a href=\"https://www.plannedparenthood.org/\">Planned Parenthood</a>, <a href=\"https://www.propublica.org/\">ProPublica</a>, and <a href=\"https://earthjustice.org/\">Earthjustice</a></li>\n</ul>\n<p>James Snowden (research consultant):</p>\n<ul>\n<li>80% to <a href=\"https://app.effectivealtruism.org/funds/global-development\">Global Health and Development Fund</a></li>\n<li>10% to <a href=\"https://app.effectivealtruism.org/funds/animal-welfare\">Animal Welfare Fund</a></li>\n<li>10% to <a href=\"https://app.effectivealtruism.org/funds/far-future\">Long-Term Future Fund</a></li>\n</ul>\n<p>The following staff members are donating to GiveWell for regranting to GiveWell&#x2019;s top charities:</p>\n<ul>\n<li>Ellie Hassenfeld</li>\n<li>Natalie Crispin</li>\n<li>Andrew Martin</li>\n<li>Chelsea Talbart</li>\n<li>Christian Smith</li>\n</ul>\n<h2><a href=\"https://www.openphilanthropy.org/blog/staff-members-personal-donations-giving-season-2017\">Open Philanthropy Project staff members</a></h2>\n<p><em>Note: Two of the people listed said that they were not donating anything this year, so I have excluded them.</em></p>\n<p>Alexander Berger:</p>\n<ul>\n<li>80% to GiveWell for regranting</li>\n<li>5% to GiveWell for operations</li>\n<li>5% to <a href=\"https://www.givedirectly.org/\">GiveDirectly</a></li>\n<li>10% to farm animal welfare groups recommended by Lewis Bollard</li>\n</ul>\n<p>Nick Beckstead is giving money to his personal donor-advised fund, which he will re-grant in broadly similar ways to how he makes grants with the <a href=\"https://app.effectivealtruism.org/funds/ea-community\">EA Community Fund</a> and <a href=\"https://app.effectivealtruism.org/funds/far-future\">EA Long-Term Future Fund</a>.</p>\n<p>Helen Toner:</p>\n<ul>\n<li>Most of her money to a non-public organization started by a couple of friends</li>\n<li>Has not decided what to do with the remaining money, but will probably give it to GiveWell for regranting</li>\n</ul>\n<p>Lewis Bollard is donating his personal money to further support the animal welfare organizations that he has made grants to through Open Philanthropy.</p>\n<p>Ajeya Cotra is participating in a <a href=\"https://app.effectivealtruism.org/lotteries\">donor lottery</a>. If she wins, she will probably donate the money toward the same early stage organization mentioned by Helen Toner.</p>\n<p>Morgan Davis is giving to the <a href=\"https://app.effectivealtruism.org/funds\">Effective Altruism Funds</a>:</p>\n<ul>\n<li>5% to Animal Welfare</li>\n<li>5% to Global Development</li>\n<li>15% to EA Community</li>\n<li>75% to Long-Term Future</li>\n</ul>\n<p>Mike Levine:</p>\n<ul>\n<li>70% to <a href=\"https://www.givewell.org/charities/amf\">Against Malaria Foundation</a></li>\n<li>30% to <a href=\"https://www.givewell.org/charities/give-directly\">GiveDirectly</a></li>\n<li>Small donations to some of the organizations mentioned in the blog post <a href=\"https://www.openphilanthropy.org/blog/giving-suggestions-pertaining-recent-executive-actions\">Giving Suggestions Pertaining to Recent Executive Actions</a></li>\n</ul></body></html>", "user": {"username": "MichaelDickens"}}, {"_id": "Xiv8g92M4JMTFdfva", "title": "Lessons for Building Up a Cause", "postedAt": "2018-02-10T08:25:53.644Z", "htmlBody": "<html><body><p><span><span><span>Introduction/Summary</span><strong><br></strong><br></span></span><span>2015 is the year when </span><a href=\"https://futureoflife.org/2015/12/31/2015-a-year-in-review/\"><span>AI alignment and AI safety took off</span></a><span> as causes taken seriously by the academic and industry worlds of AI research and the media. This has largely been attributed to influential world leaders in science and technology like Bill Gates, Stephen Hawking and Elon Musk. They all cited Nick Bostrom&#x2019;s book </span><a href=\"https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies\"><span>Superintelligence</span></a><span> as&#xA0;having changed brought the serious issue of AI alignment to their attention. Published in 2014, </span><span>Superintelligence</span><span> gets the credit it deserves for having put </span><a href=\"https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/\"><span>AI alignment</span></a><span> on the map like never before, along with significant help from the </span><a href=\"https://futureoflife.org/\"><span>Future of Life Institute</span></a><span>. However, the AI alignment, rationality and effective altruism (EA) communities at large aren&#x2019;t credited enough for what they did to advance serious attention to the cause. My impression having talked to many people is that the popular history is AI alignment lucked into hitting a tipping point. There&#x2019;s little written up about the history of AI alignment as a cause developed by the rationality and effective altruism communities, and associated organizations. This is important because if we don&apos;t keep track of how much of AI alignment&apos;s rise to prominence is due to deliberate effort from within the community, we won&apos;t know how lessons of what worked can be applied to other fields.This observation has been made by effective altruists wanting to develop other fields, such as <a href=\"https://thingofthings.wordpress.com/2017/11/16/creating-welfare-biology-a-research-proposal/\">welfare biology</a> Having been involved in the rationality and EA communities for years before the publication of </span><span>Superintelligence</span><span>, I saw the build-up of AI alignment as a cause first-hand. Between this experience and a few key sources which highlight important points in the history of AI alignment as a cause, I&#x2019;ve noticed its development can be broken down into multiple stages. In this post I aim to explain how various strategies for growth and development over the course of AI alignment&#x2019;s history can be generalized to other causes. </span><span><br></span><span><br></span><span><span>A Brief Look Back At AI Alignment<br></span></span><span><br>As mentioned above, the Future of Life Institute (FLI) played a crucial role in working with Nick Bostrom and capitalizing on the publication of his book </span><span>Superintelligence</span><span>. Much of FLI&#x2019;s work when it was first founded was focused on organizing </span><a href=\"https://futureoflife.org/2015/10/12/ai-safety-conference-in-puerto-rico/\"><span>a conference on AI alignment</span></a><span> a month after the publication of </span><span>Superintelligence</span><span>, to which were invited journalists, academic and industry leaders, and public figures such as Elon Musk. The work of relating and communicating AI alignment to the public while trying to ensure fidelity of messaging FLI has done behind the scenes in 2015 and before is illustrated in this </span><a href=\"https://futureoflife.org/2015/12/31/2015-a-year-in-review/\"><span>2015 overview</span></a><span> on their website.</span><span><br></span><span><br></span><span>Max Tegmark, co-founder of FLI, </span><a href=\"http://www.rationality.org/about/media-and-mentions\"><span>explains the role</span></a><span> the </span><a href=\"http://rationality.org/\"><span>Center for Applied Rationality</span></a><span> (CFAR) played in FLI&#x2019;s creation:</span></p>\n<blockquote>\n<p><span>CFAR was instrumental in the birth of the Future of Life Institute: 4 of our 5 co-founders are CFAR alumni, and seeing so many talented idealistic people motivated to make the world more rational gave me confidence that we could succeed with our audacious goals.&quot;</span><span><br></span></p>\n</blockquote>\n<p><span><span>CFAR in turn owes its existence to </span><a href=\"http://lesswrong.com/\"><span>Less Wrong</span></a><span> and the rationality community. LessWrong started with </span><a href=\"https://www.readthesequences.com/\"><span>the Sequences</span></a><span> by Eliezer Yudkowsky, which also contains his foundational contributions to the theory underpinning AI alignment. Anna Salamon, co-founder and president of CFAR, describes in her post <a href=\"https://lesswrong.com/lw/o5z/on_the_importance_of_less_wrong_or_another_single/\">On the Importance of Less Wrong</a>, the crucial role Less Wrong has played in the existential risk (x-risk) and AI alignment communities:<br><br></span></span></p>\n<blockquote>\n<p><span><span>One feature that is pretty helpful here, is if we somehow maintain a single &quot;conversation&quot;, rather than a bunch of people separately having thoughts and sometimes taking inspiration from one another. &#xA0;By &quot;a conversation&quot;, I mean a space where people can e.g. reply to one another; rely on shared jargon/shorthand/concepts; build on arguments that have been established in common as probably-valid; point out apparent errors and then have that pointing-out be actually taken into account or else replied-to).</span><span><br></span><span><br></span><span>One feature that really helps things be &quot;a conversation&quot; in this way, is if there is a single Schelling set of posts/etc. that people (in the relevant community/conversation) are supposed to read, and can be assumed to have read. &#xA0;Less Wrong used to be a such place; right now there is no such place; it seems to me highly desirable to form a new such place if we can.</span></span></p>\n</blockquote>\n<p><span>Looking at all these historical efforts to build up a community around existential risk reduction and AI alignment as a single long-term project, I&#x2019;ve noticed multiple stages of community build-up around these fields. <br><br><span>Stages of Developing a Movement or Field</span></span><span><br></span><span>In hindsight, we can look at these historical efforts to build up a community around the ideas of existential risk reduction and AI alignment as a single, long-term project. Viewed this way, it appears there are multiple distinct stages of community development in the history of AI alignment. These stages can be generalized to examples from other social and intellectual movements as well, such as animal advocacy and effective altruism.</span></p>\n<ol>\n<li>\n<p><span><em>Knowledge Production.</em> The development of a knowledge base which can be used to generate common knowledge across a large number of people who have skills or interests relevant to a field who previously have gained little exposure to it. Examples of this for AI alignment include Bostrom&#x2019;s book </span><span>Superintelligence</span><span>; &#x201C;</span><a href=\"https://intelligence.org/files/AIPosNegFactor.pdf\"><span>Artificial Intelligence as a Positive and Negative Factor in Global Risk</span></a><span>&#x201D; by Eliezer Yudkowsky and more broadly the <a href=\"https://wiki.lesswrong.com/wiki/Sequences\">LessWrong Sequences;</a> and in general research produced by organizations such as the </span><span><a href=\"https://intelligence.org/\">Machine Intelligence Research Institute</a></span><span> (MIRI) and the </span><span><a href=\"https://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a></span><span> (FHI). For a cause, this body of knowledge serves as a research canon new supporters can read to get up to speed and involved. By becoming common knowledge this research canon provides a productive foundation for discussing the problems the field is currently facing. </span><span><br><br></span></p>\n</li>\n<li>\n<p><span><em>Community-Building. </em>Publish this knowledge base to gain public input and inform and educate others about the problem(s) you&#x2019;re trying to solve. From here, create a community with common knowledge and concern to transform the effort into an intellectual community on which progress towards solutions can begin. Historically, for AI alignment </span><a href=\"https://www.lesserwrong.com/\"><span>Less Wrong</span></a><span> has served this purpose. For more on this, I recommend reading the post </span><a href=\"https://www.lesserwrong.com/posts/8rYxw9xZfwy86jkpG/on-the-importance-of-less-wrong-or-another-single\"><span>On the importance of Less Wrong, or another single conversation locus</span></a><span> by Anna Salamon, in full. Other examples of this can be spotted in effective altruism. Effective animal advocacy as a cause has been able to develop relatively quickly for a young social movement, with effective altruism being preceded by the modern </span><a href=\"https://en.wikipedia.org/wiki/Animal_rights#20th_century:_Animal_rights_movement\"><span>animal rights movement</span></a><span>, with public exposure leading to the development and growth of the movement, and its ideas, largely due to </span><a href=\"https://en.wikipedia.org/wiki/Animal_rights#Publication_of_Animal_Liberation\"><span>publication</span></a><span> of </span><a href=\"https://en.wikipedia.org/wiki/Animal_rights#Publication_of_Animal_Liberation\"><span>Animal Liberation</span></a> <span>by Peter Singer in 1970. For the effective altruism movement itself, </span><a href=\"https://www.effectivealtruism.org/doing-good-better/\"><span>Doing Good Better</span></a><span> by William MacAskill; </span><a href=\"https://yalebooks.yale.edu/book/9780300219869/most-good-you-can-do\"><span>The Most Good You Can Do</span></a><span> by Peter Singer; and the </span><a href=\"http://www.careyryan.com/files/EA_Handbook.pdf\"><span>Effective Altruism Handbook</span></a><span> are recent examples of creating common knowledge from a set of ideas while simultaneously building a movement around them. The growth and development of an intellectual community/field seems like it can be organized and accelerated given control over an online platform to track and steer growth. </span><span><br></span><span><br></span><span>The example of Less Wrong stands out to me as being able to bootstrap a set of important ideas which had support of a relatively small group of people to a worldwide network in only a few years. Pairing local organization with online coordination has worked well for effective altruism and existential risk reduction. It&#x2019;s my experience various causes and communities adjacent to effective altruism have benefited from doing the same. Using social media like Facebook has served this function well for more purely social movements. A highly intellectual movement like EA or x-risk reduction seems to have strongly benefited from having control over an online platform with features social media lacks, and which promotes higher-quality discourse and epistemics. This is similar to the role peer-reviewed journals play in science. </span><span><br><br></span></p>\n</li>\n<li>\n<p><span><em>Project &amp; Resource Mobilization. </em>After a significant period of time, seeding decentralized organization can create multiple nodes in a network which autonomously specialize and advance the growth and development of a field. This specialization of labour is present in AI alignment: organizations like MIRI do technical research in the Bay Area where they can build bridges with AI researchers while, because of their connection to universities at Cambridge, FHI has a greater focus on AI policy. Likewise for EA movement, the Centre for Effective Altruism (CEA) has set up offices at the heart of the EA movement in the San Francisco Bay Area for movement growth purposes, while maintaining an office for research at the University of Oxford as well. The absolute level of global growth of a field will generate an effective network which can pool resources and begin long-term strategizing and the pursuit of larger collective goals. <br><br>What makes fields of interest to effective altruism different than other intellectual movements is its internal community and economy spanning the globe. Research organizations have benefited from a common pool of resources that is effective altruism as a social movement, receiving millions of dollars from thousands of individuals; having a constant source of potential candidates for an organization; and a vocal and eager supporter base. Unlike other fields of research, e.g., in the social sciences, EA organizations primarily rely on charitable donation from an association of private individuals, as opposed to similar research typically being sponsored by a large corporation, a university or government department. Large foundations like the <a href=\"https://en.wikipedia.org/wiki/GiveWell#Open_Philanthropy_Project\">Open Philanthropy Project</a> have a major influence over the EA community as a whole, but otherwise EA organizations have access to more grassroots support for non-profit efforts trying to approximate something like academic research in the private sphere. This means EA organizations which do work similar to think tanks mobilize resources more like a charitable or <a href=\"https://en.wikipedia.org/wiki/Resource_mobilization#Theory\">social movement</a> organization.<br><br></span></p>\n</li>\n<li><em><span>Global Coordination. </span></em><span>As projects and organizations in a worldwide community mature, they can form institutions which can influence public opinion and public policy; professionalize a research field; and more. Having a large and diverse support base to rely on allows individuals and teams within a cause to specialize and take risks with what projects they pursue as they can be more confident their projects will receive support and be part of a broader overarching movement. For AI alignment this has been organizations such as FHI, FLI, MIRI and CFAR collaborating on projects together which have advanced their common cause, with universities and communities around the world. An example of what the AI alignment community was with allies able to achieve at this stage of development was putting impressing the global significance of the cause on the world, and putting it on the map, as described above. <br><br>At this stage of community development for social coordination and intellectual problem-solving worldwide, we&apos;re at the current stage for any cause in effective altruism. In addition to AI alignment, effective altruism has overlapped with the pre-existing animal welfare/liberation movement to form a movement focused on effective animal advocacy. Over the last decade, effective animal advocacy organizations like <a href=\"http://www.animalequality.net/\">Animal Equality</a> and <a href=\"https://thehumaneleague.org/\">The Humane League</a>, research organizations like The <a href=\"http://www.gfi.org/\">Good Food Institute</a> and <a href=\"http://www.new-harvest.org/\">New Harvest</a>, have built upon <a href=\"https://en.wikipedia.org/wiki/Animal_rights\">existing movements</a> to coordinate around the world approaches from grassroots activism to policy reform to biotech development to mitigate harm to animals due to consumer industries worldwide. Effective giving and evidence-based charity are ideas in effective altruism most closely associated publicly with global poverty reduction, which with the growth of EA are gradually spreading in non-profit circles and among charities outside the movement. These examples of success in the early stages of a community&apos;s or field&apos;s effort to build an influential movement bring us to the present day in effective altruism. <br><br></span></li>\n</ol>\n<p><span><span>Applying This</span><span> Approach<br></span></span><span><span>Each of the three major focus areas of effective altruism:</span><br></span></p>\n<ul>\n<li><span>Global poverty alleviation; primary means: public health &amp; economic development<br></span></li>\n<li><span><span>Existential risk reduction; primary means: AI alignment &amp; global coordination<br></span></span></li>\n<li><span><span><span>Effective animal advocacy/welfare; primary means: multi-pronged approach to mitigating animal suffering due to factory farming<br></span></span></span></li>\n</ul>\n<p><span><span><span>benefited from another community focused on similar goals before effective altruism existed. Movements like transhumanism, the rationality community, veganism, animal welfare, and animal rights/liberation focused on EA causes for years before EA existed. Global poverty reduction has been a goal of charity worldwide for thousands of years, with its transformation into modern philanthropy benefiting from decades of research from fields like economics and epidemiology. Since its inception several years ago, EA has internally fostered the growth and development of some causes which to this day outside the movement receive little public interest. Notable examples include the focus areas of <a href=\"https://en.wikipedia.org/wiki/Wild_animal_suffering\">wild animal suffering</a>/welfare and <a href=\"https://en.wikipedia.org/wiki/Life_extension\">life extension</a>. Foci like existential risks in addition to AI alignment; emerging technology R&amp;D; public policy reform; and mass mental health interventions have begun receiving more attention from the EA movement in the last couple years. <br><br>As effective altruism has been constructed from the confluence of so many prior social movements, intellectual fields and online communities, as a community we have the collective experience to deliberately repeat this process. EA as a sizable coordinated network has the capacity to devote more resources to grow and develop smaller causes more rapidly than it was before. With the benefit of hindsight we also have more knowledge and ability to steer the intellectual development and project coordination for all kinds of causes. Looking back at the history of AI alignment and existential reduction, its been over a decade to get to its current stage. Other EA causes benefited from decades of public interest to get to their current stages of development. Ideally, smaller causes will be able to employ the above strategies in time to develop even more rapidly than the biggest focus areas in EA at present.<br><br><span>What&apos;s Next?</span><br>Having recognized these common and effective strategies for community-building, I&apos;ve begun collaborating with the intellectual communities around various causes on the first stage of field development, <em>Knowledge Production</em>. I&apos;ve been doing this for the following focus areas in the following respective Facebook groups. <br></span></span></span></p>\n<ul>\n<li>\n<p><a href=\"https://www.facebook.com/groups/1292009927521451/permalink/1462438213811954/\">Wild Animal Suffering/Welfare</a>; Wild Animal Welfare Project Discussion</p>\n</li>\n<li>\n<p><a href=\"https://www.facebook.com/groups/CausePrioritization/permalink/1556680921113128/\">Cause Prioritization</a>; Cause Prioritization Discussion Group</p>\n</li>\n<li>\n<p><a href=\"https://www.facebook.com/groups/1398473810454996/permalink/1800771240225249/\">Movement-Building and Social Movement Research</a>; EA Movement-Building</p>\n</li>\n<li>\n<p><a href=\"https://www.facebook.com/groups/307795212983019/permalink/408119462950593/\">Global Coordination</a>; Global Coordination Project Discussion</p>\n</li>\n<li><a href=\"https://www.facebook.com/groups/EAMentalHealth/permalink/1565599033523439/\">Mental Health</a>; Effective Altruism, Mental Health, and Happiness</li>\n</ul>\n<p>Over the last couple months, I&apos;ve begun compiling research materials on these subjects to form bodies of research I intend to post on the Effective Altruism Forum when complete. Depending on the focus area, they may be cross-posted on other websites as well. You&apos;re invited to join any of these groups to follow or contribute to any of these projects.&#xA0;Also feel free to suggest&#xA0;any other cause which would benefit from such a project. Having compiled a comprehensive spread of quality research for various fields and posted then to the EA Forum, I hope this information will be used to further advance and develop causes within effective altruism.</p>\n<p><br><br></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "Evan_Gaensbauer"}}, {"_id": "7ccsecGA75va8fzRN", "title": "[Link] OpenPhil's Update on Cause Prioritization at Open Philanthropy", "postedAt": "2018-02-07T15:49:24.551Z", "htmlBody": "<html><body><p>The Open Philanthropy Project have posted an <a href=\"https://www.openphilanthropy.org/blog/update-cause-prioritization-open-philanthropy\">update on cause prioritization</a> on their website.&#xA0;</p></body></html>", "user": {"username": "Crosspost"}}, {"_id": "SovfPFFePJ4LJETd7", "title": "Global catastrophic financial risks?", "postedAt": "2018-02-05T23:26:57.041Z", "htmlBody": "<html><body><p>Have there been any good analyses of possible global catastrophic financial risks?</p>\n<p>I&apos;m thinking of issues such as:</p>\n<p>1) narrow AI traders cornering global capital markets through more efficient predictions, trades, and arbitrage, so ordinary folks are left with near-zero equity and pensions</p>\n<p>2) blockchain and cryptocurrency technologies disrupting fiat-currency-denominated investments enough that most folks lose their net worth</p>\n<p>3) AIs plus human traders finding leveragable vulnerabilities in insurance markets and engineering medium-sized physical catastrophes that turn into global financial catastrophies</p>\n<p>4) sudden regulatory or tax-policy changes that have unanticipated snowballing effects, e.g. eliminating the US mortgage interest tax deduction suddenly crashing the US housing market.</p>\n<p>I&apos;d appreciate any pointers to analyses of situations like this from an EA perspective. Cheers -- G</p></body></html>", "user": {"username": "geoffreymiller"}}, {"_id": "eBLqrugTuc2nSWfh9", "title": "Would it be a good idea to create a 'GiveWell' for U.S. charities?", "postedAt": "2018-02-04T21:29:41.564Z", "htmlBody": "<html><body><p>In 2007, GiveWell was founded with <a href=\"http://files.givewell.org/files/ClearFund/Business%20Plans%20and%20Reviews/2007%2004%2007/Clear%20Fund%20Business%20Plan.pdf#page=1\">the goal</a> of &quot;giving every donor convenient, usable access to the kind of intelligence and due diligence that is currently exclusive to large grantmakers.&quot; Under its original plan, GiveWell would evaluate charities within&#xA0;<a href=\"http://files.givewell.org/files/ClearFund/Business%20Plans%20and%20Reviews/2007%2004%2007/Clear%20Fund%20Business%20Plan.pdf#page=12\">seven distinct cause&#xA0;areas</a>&#xA0;and recommend to donors the best charities within each area. Three of those cause areas were related to global health and development while four of them were related to U.S. educational/economic opportunity. GiveWell largely followed this plan for four years, during which it recommended a variety of U.S. charities. In 2011, GiveWell decided to stop recommending charities working on U.S.&#xA0;educational/economic opportunity and focus exclusively on global health and development charities. <a href=\"https://blog.givewell.org/2011/02/04/givewells-annual-self-evaluation-and-plan-a-big-picture-change-in-priorities/\">Its reasoning</a>&#xA0;was that U.S. educational/economic opportunity was&#xA0;much less cost-effective than global health and development and that its recommendations were moving&#xA0;relatively little money to charities working on U.S.&#xA0;educational/economic opportunity.&#xA0;</p>\n<p>In this post, I will consider whether it would be a good idea to start a GiveWell-style evaluator focused exclusively on U.S. charities.* The evaluator could find cause areas&#xA0;with high impact interventions that are appealing to donors and then find the best charities within each cause area.** Possible cause areas include:</p>\n<p>1. Climate change mitigation.&#xA0;The goal would be to find&#xA0;the charity that is most effective at reducing or&#xA0;offsetting greenhouse gas emissions. (Potential recommendation(s):&#xA0;<a href=\"https://www.givingwhatwecan.org/report/cool-earth/\">Cool Earth</a>)</p>\n<p>2. Criminal justice. The goal would be to find&#xA0;the charity that is most effective at minimizing the burden of the criminal justice system on people&#xA0;accused of minor crimes. (Potential recommendation(s):&#xA0;<a href=\"http://slatestarcodex.com/2017/05/16/bail-out/\">various bail funds</a>)</p>\n<p>3. Immigration relief. The goal would be to find the charity that is most effective at securing immigration relief for people in removal proceedings. (Immigrants who cannot afford an attorney have&#xA0;<a href=\"https://www.law.cornell.edu/uscode/text/8/1362\">no right to an appointed attorney</a>&#xA0;in removal proceedings. Those who are provided&#xA0;an attorney are&#xA0;<a href=\"https://storage.googleapis.com/vera-web-assets/downloads/Publications/new-york-immigrant-family-unity-project-evaluation/legacy_downloads/new-york-immigrant-family-unity-project-evaluation.pdf#page=26\">much more likely</a>&#xA0;to obtain relief, and providing immigration&#xA0;attorneys is&#xA0;<a href=\"http://www.nera.com/content/dam/nera/publications/archive2/NERA_Immigration_Report_5.28.2014.pdf#page=34\">relatively inexpensive</a>&#xA0;on a per-case basis.) (Potential recommendation(s): unknown)&#xA0;</p>\n<p>4.&#xA0;Educational&#xA0;opportunity. The goal would be to find&#xA0;the charity that is best at helping disadvantaged youths. (Potential recommendation(s): <a href=\"/ea/1dt/how_much_further_does_your_dollar_go_overseas/\">Nurse Family Partnership</a>)</p>\n<p>(I have purposefully left out public health and mental health because I think they would be less appealing to donors.)</p>\n<p>&#xA0;</p>\n<p><strong>Would It Be Successful?</strong></p>\n<p>In my view, for the evaluator&#xA0;to be successful, it would&#xA0;have to have an impact that is larger than the impact its employees would have if they each pursued a high earning career and donated a large fraction of their salary to GiveWell top charities. Since GiveWell top charities are <a href=\"/ea/1dt/how_much_further_does_your_dollar_go_overseas/\">about 100 times as effective (under certain assumptions)</a> as Nurse Family Partnership and various bail funds, the evaluator would have to move 100 times as much money to its recommended charities as its employees would be donating to GiveWell top charities if they instead took high paying jobs.*** For example, if the evaluator eventually grows to the point where it has 20 employees (<a href=\"https://web.archive.org/web/20171205183238/https://www.givewell.org/about/people\">the current number of GiveWell employees</a>) and each of those employees would otherwise be donating $10,000 to GiveWell top charities, then the evaluator would have to move $20 million to its recommended charities (about half of <a href=\"https://blog.givewell.org/2017/09/29/interim-update-on-givewells-money-moved-and-web-traffic-in-2016/\">what GiveWell moved to its recommended charities in 2016</a> if you exclude Good Ventures).</p>\n<p>If GiveWell was only able to move <a href=\"https://blog.givewell.org/2011/02/04/givewells-annual-self-evaluation-and-plan-a-big-picture-change-in-priorities/\">a relatively small amount of money</a> to U.S. charities, what reason is there to think that a new evaluator would be able to move such a large amount of money?&#xA0;</p>\n<p>1. GiveWell focused on several closely-related&#xA0;cause areas, while this evaluator would focus on several&#xA0;quite distinct cause areas. This&#xA0;would not only broaden the direct appeal of the evaluator, but also allow awareness of it to spread more easily (since&#xA0;someone who cares about&#xA0;one&#xA0;cause could learn about the evaluator through someone who cares about a different cause).</p>\n<p>2. GiveWell promoted its recommended U.S. charities alongside more effective global health and development charities, and it took a more utility-oriented approach to marketing its&#xA0;recommended charities (under which giving to global health and development charities would make more sense). This evaluator could take a more justice-oriented approach, focusing&#xA0;mainly on the problem underlying each cause area and treating the high cost-effectiveness of the&#xA0;top charities as more of a bonus.</p>\n<p>3. Due to the current political environment, there is a&#xA0;greater desire to&#xA0;support various U.S. causes&#xA0;(<a href=\"https://www.facebook.com/groups/effective.altruists/permalink/1499287060127646/\">including among some EAs</a>), which expands the pool of potential donors. Since the first few years of an evaluator are the most critical for its long-term success, now would probably be an ideal time to start an evaluator like this.</p>\n<p>[There is also the possibility that the evaluator could influence institutional donors, such as foundations, corporations, and governments. Influencing foundations seems relatively unlikely since foundations <a href=\"http://files.givewell.org/files/ClearFund/Business%20Plans%20and%20Reviews/2007%2004%2007/Clear%20Fund%20Business%20Plan.pdf#page=43\">tend to focus on supporting riskier endeavors</a>. However, influencing corporations seems possible since they are often geographically limited and at least <a href=\"https://www.facebook.com/groups/effective.altruists/permalink/1455920331130986/\">one has reached out to an EA group</a>. Additionally, it seems possible that high quality research could influence governments (e.g.&#xA0;municipalities deciding how best to offset greenhouse gas emissions).]</p>\n<p>&#xA0;</p>\n<p><strong>Benefits (if successful)</strong></p>\n<p>1. Its recommendations could move large amounts of money to existing effective charities.&#xA0;</p>\n<p>2. It could move money to new effective charities by incubating and then recommending them.</p>\n<p>3. It could make it&#xA0;mentally easier for people to accept EA by allowing them to first accept prioritization within&#xA0;a cause area they like and later accept prioritization between cause areas.</p>\n<p>4. It could help GiveWell make the case for overseas giving by providing a basis for comparison.</p>\n<p>5. It could result in charities and donors&#xA0;placing greater value on&#xA0;evidence and transparency.&#xA0;</p>\n<p>&#xA0;&#xA0;</p>\n<p><strong>Costs (if successful)</strong></p>\n<p>1. There would be a large opportunity cost to&#xA0;the money and time invested in the evaluator.&#xA0;</p>\n<p>2. If U.S. charities end up being somewhat close in cost-effectiveness to global health charities, some donors may decide to shift their donations from global health charities to U.S. charities.</p>\n<p>3. It could shift money away from higher risk U.S. charities that have a higher expected impact.</p>\n<p>4. It could reinforce the&#xA0;<a href=\"https://www.reddit.com/r/EffectiveAltruism/comments/7m229f/does_anyone_else_feel_like_the_ea_movement_is_a/\">perception</a>&#xA0;that&#xA0;<a href=\"/ea/1ci/an_argument_for_broad_and_inclusive/bf4\">EA&#xA0;is misleading</a>&#xA0;since it gets people involved through a relatable cause and then tries to convince them that&#xA0;<a href=\"https://80000hours.org/2017/11/talent-gaps-survey-2017/#ea-leaders-believe-that-giving-focussed-on-the-ea-community-and-long-term-future-is-more-effective-than-that-on-global-poverty-or-animal-welfare\">other causes</a>&#xA0;are even more important.</p>\n<p>5. It could&#xA0;<a href=\"/ea/1ci/an_argument_for_broad_and_inclusive/bf4\">undermine cause neutrality</a>&#xA0;by promoting the idea that people should just choose their favorite cause area and then support the most cost effective charity working in that area.</p>\n<p>6. If its recommendations rely on non-rigorous research or non-rigorous assumptions, then it could harm GiveWell&apos;s reputation by undermining confidence in GiveWell-style evaluation.&#xA0;&#xA0;</p>\n<p>&#xA0;</p>\n<p><strong>Ways It Could Fail</strong></p>\n<p>The evaluator could fail if...</p>\n<p>1. none of the most appealing interventions are <a href=\"https://www.facebook.com/groups/effective.altruists/permalink/656749537714740/\">sufficiently&#xA0;cost-effective</a>&#xA0;and evidence-based.</p>\n<p>2. all of the charities implementing the most promising&#xA0;interventions decline to be evaluated.</p>\n<p>3. it cannot find donors who value cost-effectiveness&#xA0;but only&#xA0;want to give to U.S. charities.</p>\n<p>4. it lacks a core community of dedicated supporters that will promote it and give money to it.</p>\n<p>5. it is unable to find employees who are as competent and motivated as GiveWell employees.</p>\n<p>&#xA0;</p>\n<p><strong>Costs of Failure</strong></p>\n<p>1.&#xA0;There would be a large opportunity cost to&#xA0;the money and time invested in the evaluator.&#xA0;</p>\n<p>2. It could&#xA0;harm the reputation of&#xA0;EAs&#xA0;by making people think&#xA0;EAs cannot execute their ideas.</p>\n<p>3. It could result in people thinking&#xA0;that GiveWell-style evaluation only works in global health.</p>\n<p>4. Charities would probably be less likely to cooperate if someone tried&#xA0;again after a failure.</p>\n<p>&#xA0;</p>\n<p>*This is distinct from the question of whether GiveWell should have continued to evaluate U.S. charities&#xA0;since it&apos;s possible for it to be a good idea to evaluate U.S. charities but a bad idea for GiveWell to do it.</p>\n<p>**This proposal to&#xA0;identify the most effective charities within highly effective cause areas is narrower than&#xA0;<a href=\"/ea/120/all_causes_are_ea_causes/\">previous</a>&#xA0;<a href=\"/ea/1ci/an_argument_for_broad_and_inclusive/\">proposals</a>&#xA0;to&#xA0;identify the most effective charities within every cause area.</p>\n<p><em>***</em>This is somewhat of a simplification for a number of reasons. 1) If some of the money moved would otherwise have gone to other charities, then the evaluator would have to move more money to offset the reduced impact of those other charities. 2) If the evaluator has benefits other than moving money (see &quot;Benefits (if successful)&quot; above), then the evaluator would have to move less money to have the same overall impact. 3) If&#xA0;some of the employees of the evaluator donate to GiveWell, then the evaluator would only have to move 100 times <em>the additional amount</em> that&#xA0;its employees would have donated to GiveWell had they taken high paying jobs. However, it is quite possible that the employees of the evaluator will mostly donate to the evaluator&apos;s recommended charities since GiveWell employees <a href=\"https://blog.givewell.org/2017/12/11/staff-members-personal-donations-for-giving-season-2017/\">mostly donate to GiveWell recommended charities</a>.&#xA0;</p>\n<p><em>Disclaimer: I am not a current or former GiveWell employee.</em></p></body></html>", "user": {"username": "RandomEA"}}, {"_id": "ji5YNqCaRgWAsZHCr", "title": "How much further does your dollar go overseas?", "postedAt": "2018-02-04T21:28:01.733Z", "htmlBody": "<html><body><h2>Background</h2>\n<div>&#xA0;</div>\n<div>Let&apos;s say you&apos;re choosing between donating to a charity that helps people living in a developed country&#xA0;and donating to a charity that helps people living in a developing country. Where will your donation do more good?&#xA0;According to GiveWell,&#xA0;<a href=\"http://www.givewell.org/giving101/Your-dollar-goes-further-overseas\">your dollar will&#xA0;go&#xA0;further overseas</a>.</div>\n<div>&#xA0;</div>\n<div>As far as I&apos;m aware,&#xA0;there are four comparisons that are used to support GiveWell&apos;s claim.</div>\n<div>&#xA0;</div>\n<div>The&#xA0;<a href=\"https://80000hours.org/career-guide/world-problems/#why-issues-facing-rich-countries-arent-always-the-most-importantand-why-charity-shouldnt-always-begin-at-home\">first comparison</a>&#xA0;is between a) how much utility an American living at the U.S. poverty line would gain from a cash transfer and b) how much utility a&#xA0;Kenyan living&#xA0;at the global extreme poverty line would gain from the same transfer. Since&#xA0;the relationship between utility and income <a href=\"https://80000hours.org/articles/money-and-happiness/#are-richer-people-more-satisfied-with-their-lives\">roughly follows a logarithmic curve</a>,&#xA0;it can be modeled using&#xA0;the&#xA0;function&#xA0;u = ln(x) where u is utility and x is income. The&#xA0;derivative of this function,&#xA0;du/dx = 1/x, shows how much a person&apos;s utility changes as their income changes. For the American at the U.S. poverty line, who has an <a href=\"https://web.archive.org/web/20180123101750/https://poverty.ucdavis.edu/faq/what-are-poverty-thresholds-today\">income of around $12,000 USD</a>, an extra dollar&#xA0;results in a gain of 1/12,000 units of utility. For the Kenyan at the global extreme poverty line, who has an <a href=\"https://80000hours.org/career-guide/anyone-make-a-difference/#fn-2\">income of around $500 USD</a> (purchasing power parity adjusted), an extra dollar results in a gain of 1/500 units of utility.&#xA0;Thus, the Kenyan&#xA0;benefits&#xA0;24 times as much as the American [1/500 / 1/12,000 = 24]. The&#xA0;biggest problem with this comparison is that it is&#xA0;<a href=\"https://www.centreforeffectivealtruism.org/blog/the-value-of-money-going-to-different-groups/\">highly sensitive</a>&#xA0;to the function&#xA0;selected, and the function used above does not come from&#xA0;experiments involving cash transfers. The correct function could be quite different, meaning the true ratio may be much lower or higher.</div>\n<div>&#xA0;</div>\n<div>The&#xA0;<a href=\"https://80000hours.org/career-guide/world-problems/#global-health-a-problem-where-you-could-really-make-progress\">second comparison</a>&#xA0;is between a) how much a developed country&#xA0;is willing to&#xA0;pay to save the life of one of its citizens and b) how much it costs a highly effective charity to save a life in a developing country. The United Kingdom National Health Service is willing to pay approximately $1,000,000 to save the life of one of its citizens (click <a href=\"https://80000hours.org/career-guide/world-problems/#fn-10\">here</a> and go to footnote 10). By contrast, Malaria Consortium can save a life in a developing country for <a href=\"https://web.archive.org/web/20171230111405/https://www.givewell.org/giving101/Your-dollar-goes-further-overseas\">around $2,500</a>. Assuming that the&#xA0;United Kingdom&#xA0;fully funds every intervention that can save&#xA0;the life of one of its citizens for less than $1,000,000, the most effective intervention in the United Kingdom that is still in need of funding will cost at least as much as $1,000,000 for each life saved.&#xA0;This would mean that it costs at least&#xA0;400&#xA0;times as much to save a life in a developed country than a developing country [$1,000,000 / $2,500 = 400]. The primary&#xA0;issue with this comparison is that&#xA0;it is highly unlikely&#xA0;that developed countries have fully funded all interventions that can save the life of one of their citizens for less than their willingness to pay to save the life of one of their citizens.</div>\n<div>&#xA0;</div>\n<div>The&#xA0;<a href=\"https://www.youtube.com/watch?v=Diuv3XZQXyc&amp;t=11m45s\">third</a>&#xA0;<a href=\"https://books.google.com/books?id=OEBTDQAAQBAJ&amp;pg=PA61&amp;dq=For+example,+it+costs+about+%24+50,000+to+train+and+provide+one+guide+dog+for+one+blind+person,+something+that+would+significantly+improve+that+person%E2%80%99s+quality+of+life.+However,+if+we+could+use+that+%24+50,000+to+completely+cure+someone+of+blindness,+that+would+be+an+even+better+use+of+money,+since+it+provides+a+larger+benefit+for+the+same+cost.+Not+only+is+%24+50,000+enough+to+cure+one+person+of+blindness+in+the+developing+world,+it%E2%80%99s+enough+to+cure+five+hundred+people+of+blindness+if+spent+on+surgery+to+prevent&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjWt6bs6LXVAhUKVz4KHXmWC_QQ6AEIJzAA#v=onepage&amp;q=For%20example%2C%20it%20costs%20about%20%2450%2C000%20to%20train%20and%20provide%20one%20guide%20dog%20for%20one%20blind%20person%2C%20something%20that%20would%20significantly%20improve%20that%20person%E2%80%99s%20quality%20of%20life.&amp;f=false\">comparison</a>&#xA0;is between a) how much it costs to provide a blind person in a developed country&#xA0;with a guide dog and b) how much it costs to prevent blindness in a person living in a developing&#xA0;country through trachoma surgery. Since <a href=\"https://web.archive.org/web/20171213193954/https://puppyintraining.com/how-much-does-a-guide-dog-cost/\">it costs $50,000 to train a guide dog</a> but only $125 to prevent one case of blindness through trachoma surgery* (and assuming that curing blindness is at least as good as&#xA0;providing a guide dog), donating to trachoma surgery is at&#xA0;least 400 times as effective as&#xA0;donating to a guide dog charity [$50,000 / $125 = 400]. This comparison is flawed since it is highly unlikely that&#xA0;guide dog provision is one of the most cost-effective developed country interventions, whereas it is at least plausible that trachoma surgery is one of the most cost-effective developing country interventions. Additionally, this comparison may create the perception that effective altruism is ableist (which may be reason enough to avoid the comparison even if you believe the perception is unfair).</div>\n<div>&#xA0;</div>\n<div>(*It costs <a href=\"https://www.ncbi.nlm.nih.gov/books/NBK11759/pdf/Bookshelf_NBK11759.pdf#page=2\">$14.28 in 2004 USD</a>&#xA0;(~$18.50 in 2017 USD) to perform surgery on both eyes (with each eye being counted as a separate surgery). Since <a href=\"https://www.ncbi.nlm.nih.gov/books/NBK11759/pdf/Bookshelf_NBK11759.pdf#page=2\">77% of those surgeries are successful</a>, it costs ~$25 in 2017 USD per successful surgery [$18.50 / 0.77 = $24].&#xA0;<a href=\"/ea/19w/we_are_often_giving_wrong_facts_about_trachoma/ay8\">According to Bernadette Young</a>, it is reasonable to believe that one out of every five people cured would otherwise have developed blindness. $25 * 5 = $125.)</div>\n<div>&#xA0;</div>\n<div>The <a href=\"https://web.archive.org/web/20171230111405/https://www.givewell.org/giving101/Your-dollar-goes-further-overseas\">fourth comparison</a> is between a) how much it costs for&#xA0;a GiveWell top charity working in a developing country to save a life and b) how much it costs&#xA0;for&#xA0;various U.S. charities previously recommended by GiveWell to&#xA0;serve one person or family. Since the cost for Malaria Consortium,&#xA0;a GiveWell top charity, to save a life (<a href=\"https://web.archive.org/web/20171230111405/https://www.givewell.org/giving101/Your-dollar-goes-further-overseas\">$2,500</a>) is less than the cost for the U.S. charities&#xA0;to serve a person or family (<a href=\"https://web.archive.org/web/20171230111405/https://www.givewell.org/giving101/Your-dollar-goes-further-overseas\">generally around $10,000</a>) and since saving a life has a bigger impact than the impact of the U.S. charities serving one person or family, giving overseas is likely to be significantly more effective than giving in the United States. The biggest limitation to this comparison is that it does not say how much more impactful saving a life is than serving one U.S. person or family, making it difficult to estimate how much more cost effective the overseas charities are.</div>\n<div>&#xA0;</div>\n<div>Thus, none of the existing comparisons&#xA0;meet the following two criteria:&#xA0;1)&#xA0;the comparison&#xA0;measures the benefits of the two interventions using&#xA0;the same unit 2)&#xA0;the&#xA0;estimates of the impact for&#xA0;both interventions are&#xA0;based on experimental evidence. In this post, I will attempt two comparisons that meet both criteria. Specifically, I will compare Malaria Consortium&apos;s seasonal malaria chemoprevention program with a) anti-smoking mass media campaigns&#xA0;in developed countries and b) the Nurse Family Partnership program. I am choosing these two programs because&#xA0;I believe they are&#xA0;the two most cost-effective interventions among scalable developed country interventions that can absorb more funding and that benefit people in the same way that Malaria Consortium benefits people (by preventing premature death). (I came across anti-smoking&#xA0;mass media campaigns&#xA0;via&#xA0;<a href=\"/ea/1a8/the_value_of_money_going_to_different_groups/azc\">this comment</a>, and Nurse Family Partnership was a GiveWell top charity <a href=\"https://www.givewell.org/charities/top-charities/2008\">for</a>&#xA0; <a href=\"https://www.givewell.org/charities/top-charities/2009\">three</a>&#xA0; <a href=\"https://www.givewell.org/charities/top-charities/2010\">years</a>.)</div>\n<div>&#xA0;&#xA0;</div>\n<h2>Methodology&#xA0;</h2>\n<div>&#xA0;</div>\n<div>In this post, I will use three units of measurement: premature deaths prevented, life years gained, and discounted life years gained.</div>\n<div>&#xA0;</div>\n<div>The first metric, premature deaths prevented, is simply the number of people who did not prematurely die but would have&#xA0;prematurely died&#xA0;absent the intervention. One drawback of this metric is that it gives equal weight to preventing the premature death of a&#xA0;teenager with many years of life ahead and preventing the premature death of someone who is only likely to live a few additional years.</div>\n<div>&#xA0;</div>\n<div>The second metric, life years gained (LYG), corrects for this by estimating, for the people whose premature death was prevented by the intervention, the number of extra years they lived as a result of the intervention. One flaw of this metric is that it considers a life year gained today as&#xA0;equally valuable to a life year gained in the future&#xA0;despite the fact that a life year gained today could bring about greater benefits in the future because the benefits of living additional years (and being economically productive during those years)&#xA0;could <a href=\"https://docs.google.com/document/d/1_NhuiqbcdITySX8_fuU_RTJ3fXfQWoiLFL6_yIw97SQ/edit\">compound over time</a>.</div>\n<div>&#xA0;</div>\n<div>The third metric, discounted life years gained (DLYG), corrects for&#xA0;this flaw by treating life years gained in the future as being a certain percent less valuable for each year further in the future they occur. The percent is called the discount rate. In this post, I will use a discount rate of 3% because that is the rate typically used in the literature on anti-smoking mass media campaigns. (Note that the <a href=\"https://docs.google.com/spreadsheets/d/13b_qt-G_TQtoYNznNak3_5dzvzgCSUPJnk3l5dMisJo/edit#gid=2064365103\">median discount rate among GiveWell staff</a>&#xA0;is 4.5%.) [Some other <a href=\"https://docs.google.com/document/d/1_NhuiqbcdITySX8_fuU_RTJ3fXfQWoiLFL6_yIw97SQ/edit\">reasons for discounting</a>, such as&#xA0;the option of investing money now so that more can be donated later or&#xA0;the expectation&#xA0;that the cost per life year gained will be greater in the future, do not apply&#xA0;when deciding which of two interventions to fund in the present.]&#xA0;</div>\n<div>&#xA0;</div>\n<div>Additionally, there are some problems common to all three metrics.</div>\n<div>&#xA0;</div>\n<div>1. They treat developing country lives/life years as being as valuable as developed country lives/life years. This could be flawed if you think the quality of life is higher in developed countries. Even if you think quality of life is the same, you might still think lives/life years in developed countries are more important because developed country citizens&#xA0;tend to produce more economically and global economic growth&#xA0;<a href=\"https://reflectivedisequilibrium.blogspot.com/2014/01/what-portion-of-boost-to-global-gdp.html\">benefits the global poor</a>.</div>\n<div>&#xA0;</div>\n<div>2. They treat lives/life years gained by a 1 year old as&#xA0;being as valuable as lives/life years gained by an 11 year old (whereas you&#xA0;<a href=\"https://blog.givewell.org/2008/07/28/significant-life-change/\">might</a>&#xA0; <a href=\"https://books.google.com/books?id=lHnrWYT5aygC&amp;pg=PA165\">think</a> that lives/life years gained by infants are less valuable). (Indeed, nearly every GiveWell analyst <a href=\"https://docs.google.com/spreadsheets/d/13b_qt-G_TQtoYNznNak3_5dzvzgCSUPJnk3l5dMisJo/edit#gid=1362437801\">places greater value</a> on averting the death of someone over the age of 5 than on averting the death of someone under the age of 5.)</div>\n<div>&#xA0;</div>\n<div>3. They&#xA0;only consider whether an intervention is extending how long a person lives and not whether it is improving the quality of the years that person lives. One metric that does consider quality of life is the quality adjusted life year (QALY), which considers how much an intervention improves years that would have been lived anyways&#xA0;as well as how many years&#xA0;of life it adds (and the quality of those added years). I will not&#xA0;be using QALYs because calculating quality of life benefits is too complicated, but I&#xA0;do note some quality of life benefits below.</div>\n<div>&#xA0;</div>\n<div>4. The life years gained metric treats extending a life from 10 years to 15 years as being as valuable as extending a life from 60 years to 65 years, whereas you might think that life years at some ages should be given more weight. (The premature deaths prevented metric also shares this flaw.)</div>\n<div>&#xA0;</div>\n<div>5. The life years gained metric treats&#xA0;40 years gained for 5 people as equally valuable to 5 years gained for 40 people, whereas you might think that the former outcome is better. (The premature deaths prevented metric is&#xA0;even worse for this scenario because it treats the former outcome as being only an eighth as good.)</div>\n<h2>&#xA0;</h2>\n<h2>Seasonal Malaria Chemoprevention (Malaria Consortium)</h2>\n<div>&#xA0;&#xA0;</div>\n<div>The first intervention I will consider is seasonal malaria chemoprevention. GiveWell estimates that the seasonal malaria chemoprevention program run by Malaria Consortium prevents one premature death of a 3-59 month old child for every&#xA0;$2,359 it spends (before accounting for leverage and funging effects) (click <a href=\"https://docs.google.com/spreadsheets/d/13b_qt-G_TQtoYNznNak3_5dzvzgCSUPJnk3l5dMisJo/edit#gid=385523701\">here</a> and go to cell B81).&#xA0;&#xA0;\n<div>&#xA0;</div>\n<div><span><span>A&#xA0;1-4 year old African child can expect to live for another 61.2 years (click <a href=\"http://apps.who.int/gho/data/node.main.LIFEREGION?lang=en\">here</a> and&#xA0;on the x axis find &quot;2013&quot; then &quot;Both sexes&quot; and on the y axis find &quot;</span></span>expectation of life at age x&quot; then &quot;1-4 years&quot;). Since each premature death prevented by Malaria Consortium equates to 61.2 life years gained, one life year is gained for every $38.55 it spends [$2,359 / 61.2].</div>\n<div>&#xA0;</div>\n<div><span><span>Alternatively, using discounted life years, 27.87 discounted life years are gained for every premature death Malaria Consortium prevents</span></span>&#xA0;[(1 - (1 + 0.03)^-61.2) / 0.03]. This implies that one discounted life year is gained for every $84.64 it spends [$2,359 / 27.87].</div>\n</div>\n<div>&#xA0;</div>\n<div>Additionally, it&apos;s worth noting that&#xA0;preventing malaria prevents <a href=\"https://www.givewell.org/international/technical/programs/seasonal-malaria-chemoprevention#What_is_the_problem\">non-fatal conditions caused by malaria</a>&#xA0;and that preventing malaria&#xA0;<a href=\"https://www.givewell.org/international/technical/programs/seasonal-malaria-chemoprevention#Possible_developmental_effects\">may increase future income</a>.&#xA0;</div>\n<div>&#xA0;</div>\n<div>Finally, there is&#xA0;<a href=\"http://economics.usf.edu/PDF/Mort_Fert_Malaria.pdf\">research</a> that suggests that preventing malaria mortality could increase fertility, which may be relevant depending on your views on <a href=\"/ea/xo/givewells_charity_recommendations_require_taking/\">population</a>&#xA0; <a href=\"/ea/14k/are_you_sure_you_want_to_donate_to_the_against/\">ethics</a>.</div>\n<div>&#xA0;</div>\n<h2><strong>Anti-Smoking Mass Media Campaigns</strong></h2>\n<div>&#xA0;</div>\n<div>The second intervention I will consider is anti-smoking mass media campaigns (MMCs). According to <a href=\"http://tobaccocontrol.bmj.com/content/24/4/320\">a 2014 systematic review</a>, there are eleven studies that have produced estimates for anti-smoking MMCs. (You can find the search strategy for the review <a href=\"http://tobaccocontrol.bmj.com/content/tobaccocontrol/suppl/2014/07/01/tobaccocontrol-2014-051579.DC1/tobaccocontrol-2014-051579supp.pdf\">here</a>.) Of the eleven studies, three were estimates of the cost-effectiveness of&#xA0;hypothetical&#xA0;anti-smoking&#xA0;MMCs (<a href=\"http://www.ajpmonline.org/article/S0749-3797(05)00102-9/fulltext\">Fishman et al. 2005</a>, <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.542.3009&amp;rep=rep1&amp;type=pdf\">Raikou and McGuire 2008</a>, <a href=\"https://link.springer.com/article/10.2165%2F11539640-000000000-00000\">Higashi et al. 2011</a>) and one was an estimate based on an anti-smoking&#xA0;MMC directed at an&#xA0;ethnic community in London with a high smoking rate and low information about the health effects of smoking, meaning that it likely has limited external validity (<a href=\"https://academic.oup.com/heapro/article/17/1/43/550094\">Stevens et al. 2002</a>). After excluding those four studies, there are seven studies remaining. The table below displays the estimates from those seven studies and an eighth one that was published after the systematic review was published.</div>\n<div>&#xA0;</div>\n<div>\n<table>\n<tbody>\n<tr>\n<td>\n<p><span>Author(s) and year</span></p>\n</td>\n<td>\n<p><span>Target (goal)</span></p>\n</td>\n<td>\n<p><span>Cost per success* (2017USD)</span></p>\n</td>\n<td>\n<p><span>DLYG per success**</span></p>\n</td>\n<td>\n<p><span>Cost per DLYG* (2017USD)</span></p>\n<p>&#xA0;</p>\n</td>\n<td>\n<p><span>Discount rate</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"http://tobaccocontrol.bmj.com/content/17/6/379\"><span>Hurley and Matthews 2008</span></a></p>\n</td>\n<td>\n<p><span>Adults (cessation)</span></p>\n</td>\n<td>\n<p><span>53</span></p>\n</td>\n<td>\n<p><span>1.7</span></p>\n</td>\n<td>\n<p><span>32</span></p>\n</td>\n<td>\n<p><span>3%</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"http://tobaccocontrol.bmj.com/content/20/4/302\"><span>Kotz et al. 2010</span></a></p>\n</td>\n<td>\n<p><span>Adults (cessation)</span></p>\n</td>\n<td>\n<p><span>260</span></p>\n</td>\n<td>\n<p><span>1.39</span></p>\n</td>\n<td>\n<p><span>188</span></p>\n</td>\n<td>\n<p><span>3.5%</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"http://www.ajpmonline.org/article/S0749-3797(14)00615-1/abstract\">Xu et al. 2015</a></p>\n</td>\n<td>\n<p><span>Adults (cessation)</span></p>\n</td>\n<td>\n<p><span>509</span></p>\n</td>\n<td>\n<p><span>1.22</span></p>\n</td>\n<td>\n<p><span>417</span></p>\n</td>\n<td>\n<p><span>3%</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"http://www.drugandalcoholdependence.com/article/S0376-8716(13)00470-5/fulltext\">Brown et al. 2013</a></p>\n</td>\n<td>\n<p><span>Adults (cessation)</span></p>\n</td>\n<td>\n<p><span>1,106</span></p>\n</td>\n<td>\n<p><span>1.18</span></p>\n</td>\n<td>\n<p><span>938</span></p>\n</td>\n<td>\n<p><span>3.5%</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"http://tobaccocontrol.bmj.com/content/6/3/207\"><span>Secker-Walker&#xA0;et al, 1997</span></a></p>\n</td>\n<td>\n<p><span>Youths (prevention)</span></p>\n</td>\n<td>\n<p><span>1,152</span></p>\n</td>\n<td>\n<p><span>1.07</span></p>\n</td>\n<td>\n<p><span>1,081</span></p>\n</td>\n<td>\n<p><span>3%</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"http://tobaccocontrol.bmj.com/content/6/2/104\">Ratcliffe et al. 1997</a></p>\n</td>\n<td>\n<p><span>Adults (cessation)</span></p>\n</td>\n<td>\n<p><span>720</span></p>\n</td>\n<td>\n<p><span>0.55</span></p>\n</td>\n<td>\n<p><span>1,295</span></p>\n</td>\n<td>\n<p><span>6%</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"http://www.ajpmonline.org/article/S0749-3797(09)00075-0/fulltext\"><span>Holtgrave et al. 2009</span></a></p>\n</td>\n<td>\n<p><span>Youths (prevention)</span></p>\n</td>\n<td>\n<p><span>2,582</span></p>\n</td>\n<td>\n<p><span>0.67</span></p>\n</td>\n<td>\n<p><span>3,855</span></p>\n</td>\n<td>\n<p><span>3%</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"https://www.healthaffairs.org/doi/abs/10.1377/hlthaff.2012.0277\">Villanti et al. 2012</a></p>\n</td>\n<td>\n<p><span>Adults (cessation)</span></p>\n</td>\n<td>\n<p><span>10,065</span></p>\n</td>\n<td>\n<p><span>NA</span></p>\n</td>\n<td>\n<p><span>NA</span></p>\n</td>\n<td>\n<p><span>3%</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n<p>&#xA0;*This is the cost to the organization running the campaign, not the societal cost.<br>**Discounting has a larger effect when benefits are realized only after many years.</p>\n<p>All of the eight studies were based on&#xA0;anti-smoking&#xA0;MMCs in developed countries. The most favorable estimate shows anti-smoking MMCs targeted at adults being more cost-effective than seasonal malaria chemoprevention ($32 per DLYG vs. $84.64 per DLYG) and several estimates show them being&#xA0;no more than 16 times less&#xA0;cost-effective than seasonal malaria chemoprevention (&lt;$1,300 per DLYG vs. $84.64 per DLYG).</p>\n<p>However, there are several reasons why someone might favor seasonal malaria chemoprevention over anti-smoking MMCs targeted at adults (even if they believe the most favorable estimate for the cost-effectiveness of anti-smoking MMCs).</p>\n<div>\n<div>1. The evidence in favor of seasonal malaria chemoprevention (<a href=\"https://www.givewell.org/international/technical/programs/seasonal-malaria-chemoprevention#Evidence_from_randomized_controlled_trials\">seven randomized controlled trials</a>) is stronger than the evidence in favor of mass media anti-smoking&#xA0;MMCs (where none of the studies have a true control group). Additionally, the evidence for anti-smoking MMCs being more cost-effective is weak (only a single study).</div>\n</div>\n</div>\n<div>\n<div>&#xA0;</div>\n<div>\n<div>2. You might think that&#xA0;anti-smoking MMCs are only highly cost effective when the spending crosses some threshold and that the EA community is unlikely to be able to pool together enough money to fund a campaign that crosses that threshold.</div>\n<div>&#xA0;</div>\n</div>\n<div>\n<div>3. You might think that adding a small number of years to a large number of people&apos;s lives (what&#xA0;anti-smoking&#xA0;MMCs do) is less valuable than adding a larger number of years to a smaller number of people&apos;s lives (what seasonal malaria chemoprevention does). On the other hand, you could think that&#xA0;anti-smoking&#xA0;MMCs are more important because they benefit adults rather than young children.</div>\n<div>&#xA0;</div>\n4.* You might think that it is less morally important to help people who could quit smoking on their own than to help people who cannot do anything to prevent their death from malaria. You might also think that&#xA0;since people can&#xA0;quit smoking on their own and&#xA0;since the dangers of smoking are widely known, it would be paternalistic to run a campaign encouraging people to do so.</div>\n<div>&#xA0;</div>\n<div>5.* You might think that it is less morally important to help people who made the decision to start smoking than to help people who did not cause themselves to be at risk of malaria. You might also think that there are greater benefits to helping someone who did not cause themselves to be in a situation because it also makes other less fearful of what would happen if they end up in that condition. (In the case of seasonal malaria chemoprevention, that benefit would be to the parents, not the young children.)</div>\n<div>&#xA0;</div>\n<div>6. Even if you disagree with the above two points, you might think that others will agree with them. This could make you want to donate to seasonal malaria chemoprevention because it would be more likely to result in others donating when they hear about your decision.</div>\n<div>&#xA0;</div>\n<div>7.&#xA0;As far as I&apos;m aware, there is no charity that focuses on running mass media anti-smoking&#xA0;MMCs (though if this is the only objection, EAs could start one).&#xA0;</div>\n<div>&#xA0;</div>\n<div>*You might accept these two arguments as to cessation campaigns targeted at adults but reject them as to prevention campaigns targeted at youths. Since youth prevention campaigns are significantly less cost effective than seasonal malaria chemoprevention, you would still end up choosing seasonal malaria chemoprevention over anti-smoking MMCs.</div>\n<div>&#xA0;</div>\n<div>\n<div>[Note: It is difficult to compare the cost effectiveness of developed country&#xA0;anti-smoking&#xA0;MMCs and developing country&#xA0;anti-smoking&#xA0;MMCs because the systematic review cited above did not uncover any studies based on a developing country&#xA0;anti-smoking MMC. The one developing country study that it found&#xA0;was for a hypothetical&#xA0;anti-smoking MMC.&#xA0;That study,&#xA0;<a href=\"https://link.springer.com/article/10.2165%2F11539640-000000000-00000\">Higashi et al. 2011</a>, estimated that an anti-smoking&#xA0;MMC in Vietnam would result in one DLYG (discount rate = 3%) for every 78,300 VND (<a href=\"https://www.exchange-rates.org/Rate/USD/VND/12-31-2011\">about 4 USD</a>). Additionally,&#xA0;the&#xA0;<a href=\"https://www.givingwhatwecan.org/post/2015/09/tobacco-control-best-buy-developing-world/\">Giving What We Can report</a>&#xA0;that&#xA0;shows&#xA0;tobacco control in developing countries being highly cost effective is based on the cost-effectiveness of&#xA0;tobacco taxes, not the cost-effectiveness of anti-smoking MMCs, and the estimated cost-effectiveness of tobacco taxes is&#xA0;<a href=\"https://academic.oup.com/ntr/article-abstract/4/3/311/1028866\">based on the cost to the government</a>, not the cost to the organization lobbying for a tobacco tax.]</div>\n<div>&#xA0;</div>\n</div>\n</div>\n<h2>Nurse Home Visiting (Nurse Family Partnership)</h2>\n<div>&#xA0;</div>\n<div>From 1990 to 2011, a group of researchers conducted a&#xA0;randomized controlled trial&#xA0;in Memphis, Tennessee to study the effect of home visits from nurses on low-income first time mothers and their children. (This program later became known as Nurse Family Partnership (NFP).) Each mother was randomly assigned to one of four categories; those in categories one and two did not receive home visiting, while those in categories three and four did receive home visiting. Although maternal mortality data was available for all four groups, child mortality data was only available for groups two and four. (Click <a href=\"https://jamanetwork.com/journals/jamapediatrics/fullarticle/1886653\">here</a> for the study reporting the results. If you want to closely following along for the next few paragraphs, open&#xA0;two new browser tabs with the study in each. On one of the browser tabs, click on the&#xA0;&quot;Figures/Tables&quot; tab and scroll down to Table 2. For the other, stay in the tab with the text of the study.)</div>\n<div>&#xA0;</div>\n<div>The study found that preventable-cause child mortality rate for the first children of&#xA0;mothers who were assigned to group 2 (the control group for child mortality) was 1.84% [9 / 489] and that preventable-cause child mortality rate for the first children of&#xA0;mothers who were assigned to group 4 (the treatment group for child mortality) was 0.00% [0 / 217]. (The difference was statistically significant (see&#xA0;&quot;Child Mortality&quot; under the &quot;Results&quot; section of the study).) This implies that&#xA0;one&#xA0;premature death of a first child was prevented for every&#xA0;54.348&#xA0;families served [1&#xA0;/ (0.0184 - 0.0000)].</div>\n<div>&#xA0;</div>\n<div>I will assume that Nurse Family Partnership also reduced child mortality for subsequent children and that the effect on subsequent children was&#xA0;identical to the effect on the first child. (There is <a href=\"http://pediatrics.aappublications.org/content/93/1/89.short\">some evidence</a> that NFP has a lasting effect on parenting.) I will also assume that mothers who&#xA0;received nurse home visits (i.e. mothers in the treatment group)&#xA0;had an average of 2.08 children&#xA0;(since they had an average of <a href=\"http://pediatrics.aappublications.org/content/114/6/1550\">1.08 subsequent births</a> at six years of follow up). (Note that this is lower than 2.9 children, which is the <a href=\"http://www.pewsocialtrends.org/2015/05/07/family-size-among-mothers/\">average number of children</a> of mothers without a high school diploma who are currently in their mid-40s.) This implies that&#xA0;one premature child death was prevented for every 26.129&#xA0;families served [1 / (2.08 * (0.0184 - 0.0000))].</div>\n<div>&#xA0;</div>\n<div>The study also found that&#xA0;external-cause maternal mortality for mothers assigned to groups 1 or 2 (the control group for maternal mortality) was 1.62% [(0 + 11) / (166 + 514)] and that external-cause maternal mortality for mothers assigned to groups 3 or 4 (the treatment group for maternal mortality) was 0.22% [(0 + 1) / (230 + 228)]. (This difference was statistically significant (see &quot;Maternal Mortality&quot; under the &quot;Results&quot; section of the study).) This implies&#xA0;that one&#xA0;premature maternal death was prevented&#xA0;for every&#xA0;71.429 families served [1&#xA0;/ (0.0162 - 0.0022)].</div>\n<div>&#xA0;</div>\n<div>[Mothers in group 3 received home visiting during pregnancy and two postpartum visits; mothers in group 4 received home visiting during pregnancy and for two years after the child was born (see &quot;Treatment Conditions&quot; under the &quot;Methods&quot; section of the study). The current NFP program <a href=\"https://www.nursefamilypartnership.org/wp-content/uploads/2017/07/NFP_Overview.pdf\">provides for</a> visits during pregnancy and for two years after the child is born. Thus, group 4 would probably be a better treatment group than groups 3 and 4 combined. However, the difference between the external-cause mortality for mothers in groups 1 and 2 and mothers in group 4 was not statistically significant, perhaps due to group 4 being too small by itself (see &quot;Maternal Mortality&quot; under the &quot;Results&quot; section of the study).]</div>\n<div>&#xA0;</div>\n<div>Combining the two numbers, I estimate that, during the study, one&#xA0;premature death was prevented&#xA0;for every 19.131 families served [1 / (1 / 26.129&#xA0;+ 1 / 71.429)].</div>\n<div>&#xA0;</div>\n<div>To calculate the impact of NFP today, I need to take into account&#xA0;the decrease in infant mortality (death before age 1) and child mortality (death between age 1 and age 20) between when the study took place and today as well as the decrease in fertility between when the study took place and today. From 1990 to 2013, infant mortality decreased by about a third and child mortality decreased by half. (Click <a href=\"https://blogs.cdc.gov/nchs-data-visualization/deaths-in-the-us/\">here</a> and then download &quot;Infant and neonatal mortality rates: United States, 1915-2013&quot; and &quot;Childhood Mortality Rates, by Age at Death: United States, 1900-2013.&quot;) Since infant mortality accounted for 40% of&#xA0;infant child/deaths&#xA0;in the study (with child mortality accounting for the remaining 60%) (go to the &quot;Figures/Tables&quot; tab of the study and scroll down to Figure 2B), I will assume that the effect of NFP on&#xA0;infant/child&#xA0;mortality today is 0.567 of what it was during the study [2/3 * 0.4 + 1/2 * 0.6]. Additionally, since the average number of&#xA0;second and subsequent children born per first child born has&#xA0;<a href=\"https://www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr66_01.pdf#page=35\">decreased in the U.S. by about 10%</a> between 1990 and 2015, I will assume that the&#xA0;number of second and subsequent children born to mothers served by NFP has gone down by 10%. This implies that NFP today&#xA0;prevents one premature infant/child death&#xA0;for every&#xA0;48.606 families served [1 / ((1 + 0.9 * 1.08) *&#xA0;0.567 * (0.0184 - 0.0000))], which implies that it&#xA0;prevents one premature death (mother or infant/child) for every&#xA0;28.924 families served [1 / (1 /&#xA0;48.606 + 1 / 71.429)].&#xA0;(Mortality among females aged 15-44&#xA0;<a href=\"https://www.cdc.gov/nchs/data/nvsr/nvsr60/nvsr60_03.pdf#page=7\">has changed little</a>&#xA0;since 1990.)&#xA0;</div>\n<div>&#xA0;</div>\n<div>[In the above paragraph, the term &quot;child mortality&quot; refers to death between age 1 and age 20. Throughout the rest of the post, the term &quot;child mortality&quot; refers to death before age 20, including death before age 1.]</div>\n<div>&#xA0;</div>\n<div>According to an&#xA0;<a href=\"http://www.givewell.org/united-states/charities/nfp\">old GiveWell estimate</a>&#xA0;(go to &quot;What do you get for your dollar?&quot;), it costs Nurse Family Partnership $10,800 to serve one family. If&#xA0;this estimate remains accurate today, then, on average, each $312,379 [28.924&#xA0;* $10,800] spent today by Nurse Family Partnership on home visiting programs similar to the one studied in the randomized controlled trial&#xA0;prevents one premature death.</div>\n<div>&#xA0;</div>\n<div>\n<div>I turn now to calculating&#xA0;the cost per life year gained. Based on Figure 2B (go to&#xA0;the &quot;Figures/Tables&quot; tab of the study and scroll to Figure 2B), I estimate that of the children whose lives were saved by NFP, 40% would have otherwise died around age 0, 30% would have otherwise died around age 5, and 30% would have otherwise died around age 19. Since 0-20 year old children in the U.S. can&#xA0;<a href=\"https://www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr66_03.pdf#page=3\">expect to live</a>&#xA0;until they are 79, each premature child death prevented results, on average, in&#xA0;71.8 life years gained [0.4 * 79 + 0.3 * (79 - 5) + 0.3 * (79 - 19)]&#xA0;or around&#xA0;24.28 discounted life years gained [[(1 - (1 + 0.03)^-79) / 0.03] - 0.3 * [(1 - (1 + 0.03)^-5) / 0.03] - 0.3 * [(1 - (1 + 0.03)^-19) / 0.03] = 30.11 -&#xA0;1.37 - 4.29 = 24.44].&#xA0;Since one premature child death is prevented for every&#xA0;48.606 families served, one child life year is gained for every&#xA0;0.677 families served [48.606&#xA0;/ 71.8]. If discounted life years are used instead, around one discounted child life year is gained for every&#xA0;1.989 families served [48.606&#xA0;/ 24.44].</div>\n<div>&#xA0;</div>\n</div>\n<div>Based on Figure 1B (go to&#xA0;the &quot;Figures/Tables&quot; tab of the study and scroll to Figure 1B), I estimate that of mothers whose lives were saved by NFP,&#xA0;25% would have otherwise died around 4 years after pregnancy, 60% would have otherwise died around 13 years after pregnancy, and 15% would have otherwise died around 19 years after pregnancy. On average, mothers who survived the entire 20.9 year follow period were 39.4 years old at the end of the follow-up period, which means that mothers who survived had an average age of 18.5 at the beginning of the follow-up period (scroll to &quot;Mortality Outcomes&quot; under the &quot;Methods&quot; section of the study). Since&#xA0;20-40 year old&#xA0;females in the U.S. can <a href=\"https://www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr66_03.pdf#page=3\">expect to live</a>&#xA0;until they are 82, each premature child death prevented equates to&#xA0;51.85 life years gained [0.25 * (82 - (18.5 + 4)) + 0.6 * (82 - (18.5 + 13)) + 0.15 * (82 - (18.5 + 19))] or around&#xA0;22.74 discounted life years gained [[(1 - (1 + 0.03)^-82) / 0.03] - 0.25 * [(1 - (1 + 0.03)^-4) / 0.03] - 0.6 * [(1 - (1 + 0.03)^-13) / 0.03]&#xA0;- 0.15 * [(1 - (1 + 0.03)^-19) / 0.03] =&#xA0;30.38 -&#xA0;0.93 -&#xA0;6.38 - 2.15 = 20.92].&#xA0;Since one premature maternal death is prevented for every 71.429&#xA0;families served, one maternal life year is gained for every&#xA0;1.378 families served [71.429&#xA0;/ 51.85]. If discounted life years are used instead, around one discounted maternal life year is gained for every&#xA0;3.414 families served [71.429&#xA0;/ 20.92].</div>\n<div>&#xA0;</div>\n<div>Combining the results from the above two paragraphs, I estimate that one life year is gained for every 0.454 families served by NFP [1 / (1 / 0.677 + 1 / 1.378)]. This implies that&#xA0;one life year is gained for every $4,903 NFP spends on home visiting [0.454 * $10,800]. Alternatively, using discounted life years, I estimate that one discounted life year is gained for every 1.257 families served by NFP [1 / (1 /&#xA0;1.989 + 1 / 3.414)]. This implies that one discounted life year is gained for every $13,576 NFP spends on home visiting [1.257 * $10,800].</div>\n<div>&#xA0;</div>\n<div>(Note that NFP has <a href=\"https://www.nursefamilypartnership.org/about/proven-results/\">benefits</a> other than saving lives. See <a href=\"https://www.nursefamilypartnership.org/about/proven-results/published-research/\">here</a> for the research.)&#xA0;</div>\n<div>&#xA0;</div>\n<div>(It&apos;s also worth noting that mothers who participated in NFP had <a href=\"http://pediatrics.aappublications.org/content/114/6/1550.long\">0.2 fewer children</a> than mothers who did not (2.08 vs. 2.28). Under certain views of&#xA0;<a href=\"/ea/xo/givewells_charity_recommendations_require_taking/\">population</a>&#xA0;<a href=\"/ea/14k/are_you_sure_you_want_to_donate_to_the_against/\">ethics</a>, this could mean that NFP does more harm than good.)</div>\n<div>&#xA0;</div>\n<div>\n<div>The table below compares the cost effectiveness of Malaria Consortium and NFP.</div>\n<div>&#xA0;</div>\n<table>\n<tbody>\n<tr>\n<td>\n<p><span>&#xA0;</span></p>\n</td>\n<td>\n<p><span>Malaria Consortium</span></p>\n</td>\n<td>\n<p><span>NFP</span></p>\n</td>\n<td>\n<p><span>Ratio of NFP to&#xA0;Malaria Consortium</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Cost per premature death prevented</span></p>\n</td>\n<td>\n<p><span>$2,359</span></p>\n</td>\n<td>\n<p><span>$312,379</span></p>\n</td>\n<td>\n<p><span>132.42</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Cost per life year gained</span></p>\n</td>\n<td>\n<p><span>$38.55</span></p>\n<p><span>&#xA0;</span></p>\n</td>\n<td>\n<p><span>$4,903</span></p>\n</td>\n<td>\n<p><span>127.19</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Cost per discounted life year gained</span></p>\n</td>\n<td>\n<p><span>$84.64</span></p>\n</td>\n<td>\n<p><span>$13,576</span></p>\n</td>\n<td>\n<p><span>160.40</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n<p>[Note: According to <a href=\"http://slatestarcodex.com/2017/05/16/bail-out/\">one estimate</a>, bail funds prevent one day of pretrial detention for every $10 donated to them. This implies that a $3,650 donation would prevent a year of pretrial detention [$10 * 365]. If you consider&#xA0;preventing one year of pretrial detention to be equivalent to or better than extending a person&apos;s life by one year, then the cost per life year gained of bail funds is better than the cost per life year gained of NFP. If you discount future benefits, then bail funds are likely several times more cost effective because their detention prevention benefits occur more quickly than the&#xA0;mortality reduction benefits of NFP.]</p>\n<p>If the ratios are taken at face value, then Malaria Consortium is at least 100 times as cost effective as Nurse Family Partnership. (This lends some support to William MacAskill&apos;s <a href=\"https://books.google.com/books?id=-u0BDAAAQBAJ&amp;pg=PA23&amp;dq=doing+good+better+100+times+multiplier#v=snippet&amp;q=100x%20multiplier&amp;f=false\">100x Multiplier</a>.)</p>\n</div>\n<div>\n<p>Of course, there are many reasons why my estimates for NFP (and hence the ratios above) should not be taken at face value.</p>\n<div>\n<div>1. My estimate for NFP is based on&#xA0;a single randomized controlled trial, making it less likely that the NFP estimate is accurate. (GiveWell&apos;s estimate for Malaria Consortium is based on <a href=\"https://www.givewell.org/international/technical/programs/seasonal-malaria-chemoprevention#Evidence_from_randomized_controlled_trials\">seven randomized controlled trials</a>.)</div>\n<div>&#xA0;</div>\n2. My estimate for NFP does not consider on the ground realities such as attrition. (By contrast, GiveWell carefully&#xA0;adjusts for attrition in its <a href=\"https://docs.google.com/spreadsheets/d/13b_qt-G_TQtoYNznNak3_5dzvzgCSUPJnk3l5dMisJo/edit#gid=385523701\">Malaria Consortium&#xA0;estimate</a>.)</div>\n<div>&#xA0;</div>\n<div>3. You might think that one of my assumptions is wrong, such as my assumption that NFP affects second and subsequent children the same way it affects first born children.&#xA0;</div>\n<div>&#xA0;&#xA0;</div>\n</div>\n<div>4.&#xA0;GiveWell&apos;s <a href=\"https://www.givewell.org/united-states/charities/nfp\">estimate</a>&#xA0;(go to &quot;What do you get for your dollar?&quot;) of the cost of serving one family through NFP was based on limited information and may no longer be accurate.</div>\n<div>&#xA0;</div>\n<div>5. You may be uncertain about the extent to which donations to NFP would result in additional families being served. While there are <a href=\"https://www.nursefamilypartnership.org/wp-content/uploads/2017/11/NFP_Snapshot_Sept2017.pdf\">states without NFP</a> (where NFP could presumably expand&#xA0;if given more&#xA0;funding), the federal government and state governments could conceivably reduce their funding of NFP if there is more private funding.&#xA0;</div>\n<div>&#xA0;</div>\n<div>6. You might disagree with using the same discount rate for developed countries and developing countries.</div>\n<div>&#xA0;</div>\n<div>7. NFP has large quality of life benefits that may be more significant than its&#xA0;mortality reduction benefits. By contrast, Malaria Consortium&apos;s mortality reduction benefits account for the vast majority of its benefits (click <a href=\"https://docs.google.com/spreadsheets/d/13b_qt-G_TQtoYNznNak3_5dzvzgCSUPJnk3l5dMisJo/edit#gid=385523701\">here</a> and go to row 78). Thus, a comparison that looks only at&#xA0;benefits from mortality reduction&#xA0;favors Malaria Consortium over NFP.</div>\n<div>&#xA0;</div>\n<div>8. Malaria Consortium only reduces the mortality of young children, while NFP reduces the mortality of children of all ages and of mothers.&#xA0;You might think that reducing the mortality of young children is much less important, which could lead you to think that the cost per unit impact of NFP is significantly less than 130 times the cost per unit impact of Malaria Consortium.&#xA0;</div>\n<div>&#xA0;</div>\n<div>(The first five points are reasons why the ratio may be biased in favor of Nurse Family Partnership, while the last two points are reasons why the ratio may be biased in favor of Malaria Consortium.)</div>\n<div>&#xA0;</div>\n<div>In addition to the above limitations, please keep in mind&#xA0;the general&#xA0;<a href=\"https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness\">limits to cost effectiveness estimates</a>.</div>\n<div>&#xA0;&#xA0;</div>\n<h2>Conclusion</h2>\n<div>&#xA0;</div>\n<div>In this post, I have compared two of the most cost-effective scalable&#xA0;developed country interventions that can still absorb additional funding (anti-smoking mass media campaigns and nurse home visiting) to one of the most cost-effective scalable developing country interventions that can still absorb additional funding (seasonal malaria chemoprevention). There is a single study that shows anti-smoking mass media campaigns being more cost-effective than seasonal malaria chemoprevention and several studies showing them being&#xA0;no more than 16 times less cost-effective than seasonal malaria chemoprevention, but none of these studies have true control groups.&#xA0;Using data from a single randomized controlled trial on nurse home visiting, I estimate&#xA0;that, under certain assumptions, nurse home visiting is&#xA0;125-160 times less cost-effective than seasonal malaria chemoprevention (depending on which metric is used).&#xA0;Based on these two comparisons, it appears that your dollar probably goes&#xA0;10-150 times further overseas.</div>\n<div>&#xA0;</div>\n<p><em>Disclaimer: I am not a current or former GiveWell employee.</em></p></body></html>", "user": {"username": "RandomEA"}}, {"_id": "RM77XJj3xGLXZk2es", "title": "Effective Altruism Sweden plans for 2018", "postedAt": "2018-02-02T12:18:47.734Z", "htmlBody": "<p><span>Effective Altruism Sweden will have a full-time employee over the coming year (that\u2019s me, Markus Anderljung). This document outlines the plans of Effective Altruism Sweden and what we\u2019ll do with that extra labor over the coming year. </span></p>\n<p><span>I think that the biggest opportunities in Sweden that we ought to take advantage of are: potential for community-building, a vibrant startup community, the ability to test working on politics and a fairly large x-risk community. Given this context, our current plans are to spend a significant amount of resources on capacity-building efforts and on one larger project. </span></p>\n<p>&nbsp;</p>\n<p><span>The capacity-building efforts will focus on community-building, primarily using methods and reasoning largely common to most effective altruism groups, with some extra attention paid to diversity-issues. Our reasoning is laid out in more detail in Section 3. </span></p>\n<p>&nbsp;</p>\n<p><span>The larger project we currently plan on spending a significant amount of time on is the Representation of Future Generations. The idea is to implement mechanisms in the Swedish political system to ensure that there is representation of future generations: that the interests of future generations are taken into account in political decision-making. This work is modelled on work carried out over the past year by CSER and FUSE in the UK, leading to the formation of the All-Party Parliamentary Group on Future Generations in November. For more details, see Section 4. The section also includes descriptions of two projects we decided not to pursue. </span></p>\n<p>&nbsp;</p>\n<p><span>This document explicitly lays out the reasoning behind our current plan, to allow you to find mistakes and help make the plan better. To aid this further, I try to make show where we think our uncertainties are. I have also listed what I think are the biggest most crucial uncertainties - our crucial considerations - are, in 3.3. and 4.3. </span></p>\n<p>&nbsp;&nbsp;</p>\n<p><span>Thanks to Stefan Schubert, Carin Ism, Karolina Grundin, Stefan Einhorn, Gabriella Over\u00f6dder, Tobias Malm, Catherine Derieux, James Snowden, Harri Besceli, Holly Morgan, Jonas Vollmer and Beth Barnes for useful discussions on the plan so far. You can find a version of the document including footnotes <a href=\"https://docs.google.com/document/d/12yXDewSKi8Kt-3iSGm8e2GcZhShaOFrnb9ysQwIZxxI/edit?usp=sharing\">here</a>.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong><span>Table of contents</span></strong></p>\n<p>&nbsp;1. Background</p>\n<p>1.1. A brief (selective) history of effective altruism in Sweden</p>\n<p>1.2. Purpose of this document</p>\n<p>1.3. What are the opportunities available to EA Sweden?</p>\n<p>&nbsp;</p>\n<p>2. Goals for the coming year</p>\n<p>&nbsp;</p>\n<p>3. Capacity-building activities</p>\n<p>3.1. Community-building</p>\n<p>3.1.1. Getting people to positively impact the world</p>\n<p>3.1.2. Examples of planned activities</p>\n<p>3.1.3. Diversity and inclusiveness</p>\n<p>3.1.4. Other activities</p>\n<p>3.2. Organizational capacity</p>\n<p>3.3. Crucial considerations regarding capacity-building</p>\n<p>&nbsp;</p>\n<p>4. Direct work</p>\n<p>4.1. Representation of future generations</p>\n<p>4.1.1. What\u2019s the idea?</p>\n<p>4.1.2. Representation of future generations in Sweden</p>\n<p>4.1.3. Potential benefits</p>\n<p>4.1.4. Impact mechanism and failure modes</p>\n<p>4.1.5. Do we have the resources to carry out this project?</p>\n<p>4.1.6. Plan outline</p>\n<p>4.2. Some projects we have decided against for now</p>\n<p>4.2.1. EA Fact Checkers / The EA Reproducibility Project</p>\n<p>4.2.2. Aid effectiveness-ranking of political parties</p>\n<p>4.3. Crucial considerations regarding direct work</p>\n<p>\f</p>\n<h1><span>1. Background</span></h1>\n<h2><span>1.1. A brief (selective) history of effective altruism in Sweden</span></h2>\n<p><span>A brief history of the effective altruism-movement in Sweden may prove useful background to the below. In 2015, people in Sweden interested in growing the movement started meeting up. We did not set up Effective Altruism Sweden immediately, but instead focused on building some momentum initially. This was done by putting on some meet-ups, particularly in Stockholm, but mainly in setting up three student groups at universities in Stockholm: Stockholm University</span><span>, Stockholm School of Economics</span><span> and the Royal Institute of Technology</span><span>. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>In September 2016, we felt that there was enough of a momentum and people interested in effective altruism to warrant a national organization and EA Sweden was created. During 2017, EAS had a committee of five people. The main focus was setting up the infrastructure for the organization, putting on community-building events and outreach.</span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>A selection of activities over the year Oct 2016 - Oct 2017:</span></p>\n<ul>\n<li>\n<p><span>Organized a talk by Will MacAskill with approx. 180 attendees and organized accompanying interviews in Swedish media</span></p>\n</li>\n<li>\n<p><span>Carried out six external lectures/talks, e.g. at Almedalsveckan, a big event in Swedish politics, and the largest conference for charities in Sweden</span></p>\n</li>\n<li>\n<p><span>Organizing regular meetups and workshops</span></p>\n</li>\n<li>\n<p><span>The student groups carried out a number of events, including reading groups and career workshops</span></p>\n</li>\n<li>\n<p><span>An EA co-living space was set up: Neurora</span></p>\n</li>\n</ul>\n<h2><span>1.2. Purpose of this document </span></h2>\n<p><span>The purpose of this document is to explicitly outline the plans of Effective Altruism Sweden (EAS) during 2018 in order to clarify our plans internally and to enable other to provide useful feedback. I also aim to make explicit as many crucial considerations as possible. These are uncertainties and questions where the answers will be particularly impactful on the work we do. There are few statements in this documents that we\u2019re sure of, and so feedback and criticism is greatly appreciated. </span></p>\n<h2><span>1.3. What are the opportunities available to EA Sweden?</span></h2>\n<p><span>In order to determine what we ought to focus on over the coming year, it\u2019s useful to think about what the main opportunities for an EA organization to do good in Sweden are. In short, we think that the (not mutually exclusive) opportunities and related resources are:</span></p>\n<ul>\n<li>\n<p><span>Community building:</span><span> This seems like a good opportunity regardless of where your EA group is located. There may be reasons to think that Sweden is particularly good to do community-building in, but I would not take these particularly seriously: Anecdotally, there is a fairly large number of EAs from Sweden relative to the country\u2019s size, despite there having been little organizing over the past few years</span><span>. Within the x-risk community some of the most influential figures are Swedish, including Anders Sandberg, Nick Bostrom and Max Tegmark are all Swedish. The number of Giving What We Can pledgers might be another measure of how promising community-building in Sweden is. In terms of pledgers per capita, Sweden ranks 7th (5th if outliers with small populations and one pledger are removed)</span><span>. </span></p>\n</li>\n<li>\n<p><span>Startup community: </span><span>Relative to its size, Sweden has a fairly large startup community</span><span>, being the place where e.g. Spotify, Majong (creators of Minecraft), King, Klarna and iZettle started. To provide an indication regarding the size, $1.4 billion was invested into Stockholm-based startups in 2016</span><span>. In addition there is an opportunity in the high-impact startup space: 2017 saw the creation of Norrsken Foundation in Stockholm. The organization has an endowment of at least $50 million, \u201ca strong belief in Effective Altruism\u201d and aims to create high-impact startups</span><span>. Another indication of this being a promising opportunity is that Founders Pledge received an Open Philanthropy Project grant to expand to Berlin, Paris and Stockholm, Sweden</span><span>. </span></p>\n</li>\n<li>\n<p><span>Politics: </span><span>People within the effective altruism movement have typically been timid about getting involved with political issues, especially with an explicit association with effective altruism. However, there is a lot of potential to do good if effective altruism can be brought into the political sphere in the right way. Given this context, there are advantages to trying out political campaigning in a small country: Firstly, small countries, including Sweden, have a tighter connection between the political process and the people. This means that a political campaign has a higher chance of success allowing shorter feedback loops. Secondly, the risks are smaller. If a political campaign accidentally causes reputational damage to effective altruism (e.g. the ideas come to be seen as weird), damage may be contained to the Swedish-speaking world as opposed to the English-speaking world. Thirdly, it is easier to promote the implementation of a policy if it has already been implemented somewhere else. If it is less costly to implement new policy in smaller countries, then it is advantageous to let smaller countries be this \u201csomewhere else\u201d. </span></p>\n</li>\n<li>\n<p><span>X-risk: </span><span>Globally speaking, Sweden is likely a small contributor to global catastrophic risks. However, there are reasons to believe the country might be a useful place to do work in the area. As mentioned above, there are substantial numbers of x-risk researchers with a connection to Sweden, including Nick Bostrom, Max Tegmark, Anders Sandberg and Olle H\u00e4ggstr\u00f6m. There are also a number of Swedish organizations and institutions that provide capacity: the Institute for Future Studies and the Global Challenges Foundation. </span></p>\n</li>\n</ul>\n<p><strong>&nbsp;</strong></p>\n<h1><span>\f</span></h1>\n<h1><span>2. Goals for the coming year</span></h1>\n<p><span>The activities that EAS carries out can lead to a number of benefits. Broadly defined, these seem to fit into the categories below:</span></p>\n<ul>\n<li>\n<p><span>Capacity-building: </span><span>Capacity-building is about laying the groundwork for future impact. It can roughly be divided into </span><span>community-building</span><span> and </span><span>building</span> <span>organizational capacity</span><span>. Community-building is about fostering a community of people with a commitment to and capable of improving the world effectively, who also have an understanding of how to do so. A crucial part of this community-building will be to make sure that we are a diverse and inclusive community. Organizational capacity is about the organization, and not just the people within it, being able to do good. This includes the organization being good at activating people within the community, having full-time staff, having processes to support its work etc. </span></p>\n</li>\n<li>\n<p><span>Exploration value: </span><span>Sweden is a small part of the global EA movement. If the work EAS does can help others do good better in the future, that\u2019s very valuable. This can be by learning how to better build a community or carry out a specific kind of project. </span></p>\n</li>\n<li>\n<p><span>Direct good: </span><span>The work we do can also have direct benefits to the world. Examples of this would be through affecting policy, helping an organization work more effectively or through donations because of our community-building work. </span></p>\n</li>\n</ul>\n<p><strong>&nbsp;</strong></p>\n<p><span>In light of the above, we will focus on two types of activities: capacity-building activities (section 3) and direct work (section 4). The capacity-building activities are those that primarily focus on capacity-building, but they may nonetheless lead to some direct good. The direct work, similarly, primarily aims at doing direct good, but also provides exploration value and the building of capacity. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>We have also defined measurables to track our impact. The most important measurables are data concerning the funnel of 80k, in addition to donations and pledges through Giving What We Can/EA Funds. </span></p>\n<p><strong>&nbsp;</strong></p>\n<h1><span>\f</span></h1>\n<h1><span>3. Capacity-building activities</span></h1>\n<p><span>Below is a brief summary of our current plans related to capacity-building. As mentioned previously I categorise these in terms of activities chiefly aimed at community-building and those aimed at building organizational capacity. </span></p>\n<h2><span>3.1. Community-building</span></h2>\n<h3><span>3.1.1. Getting people to positively impact the world</span></h3>\n<p><span>The ultimate goal of community-building within effective altruism is not to grow effective altruism: it is to positively impact the world. We want capable people with an understanding of how to improve the world as much as possible to spend a significant amount of their resources on improving the world. This is a big ask. And so for most people, it is a stepwise process from hearing about effective altruism for the first time to dedicating a significant amount of your time and money to improving the world effectively. This step-wise nature needs to be taken into account in community-building. Depending on where they are, people will have different needs and different activities will be needed to cater to these different needs. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>A useful analogy to thinking about this is a sales funnel (see below):</span></p>\n<p><span><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995336/mirroredImages/RM77XJj3xGLXZk2es/tshfxwr2zyvnkbqxaqlr.png\"></span></p>\n<p><span>Illustration 1: A sales funnel for effective altruism. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>Over the past year, EAS has been shifting our efforts more towards the latter parts of the sales funnel. Over the coming year, we will continue to put a lot of our efforts in that area. I think this is a good idea since I tend to find that awareness and knowledge of effective altruism is not enough for people to take significant action on the ideas. For most people, in my experience, a social context and nudges towards action are needed. My sense is that this is the case even for people who find that they agreed with a lot of the core effective altruism principles right away. However, I acknowledge that given the importance of the question, this reasoning is likely not enough. Therefore, we are considering putting more energy into answering the question of how best to get people from being aware of the ideas to acting on them. </span></p>\n<p><strong>&nbsp;</strong></p>\n<h3><span>3.1.2. Examples of planned activities</span></h3>\n<p><span>Most of our events and activities will be explicitly put in one of the levels of the sales funnel. That way, we can adapt our events to the relevant target audience. For example, a Level 3-event can benefit from being in a somewhat private space, e.g. someone\u2019s home. However, if you have only just now heard about effective altruism, that might feel too personal. Below, I\u2019ll list some of the activities that we have planned for the different levels:</span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>Level 1: Awareness</span></p>\n<p><span>There are several ways in which we plan to make more people aware of effective altruism and EAS. The first is word-of-mouth or personal contacts, which was the most common way for people to find out about EA in the 2017 Effective Altruism Survey</span><span> (15.5% of respondents). I find that a lot of people within the EA movement find it difficult to talk about effective altruism with friends and family. Therefore, we have already and are planning to keep running events about how to do so.</span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>In addition to the above, we will do more conventional outreach, primarily lectures, talks and one-on-one meetings for individuals we think particularly receptive and impactful. We will also spend some resources on media outreach, but primarily in relation to our direct work. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>Level 2: Knowledge</span></p>\n<p><span>Our main activities on this level will be a few recurring activities, listed below:</span></p>\n<ul>\n<li>\n<p><span>Socials</span></p>\n</li>\n<li>\n<p><span>Introduction to effective altruism (2-4 hour workshop)</span></p>\n</li>\n<li>\n<p><span>Giving games</span></p>\n</li>\n<li>\n<p><span>Reading groups</span></p>\n</li>\n</ul>\n<p><strong><br><br><br></strong></p>\n<p><span>Level 3: Action</span></p>\n<p><span>Activities under this level aim to get people to take action on their knowledge of effective altruism. In short, these actions can be donating money, changing your career or doing direct work. By direct work, I mean for example helping organize a local group, but also running specific projects such as those discussed in section 4.</span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>Under this level, we plan to carry out some of the following activities:</span></p>\n<ul>\n<li>\n<p><span>Career workshops</span></p>\n</li>\n<li>\n<p><span>Workshops on other actionable topics, e.g. how to talk to your friends about effective altruism and how to make yourself live in accordance with your values</span></p>\n</li>\n<li>\n<p><span>One-on-one conversations with particularly high-potential members</span></p>\n</li>\n<li>\n<p><span>Providing easy ways for people to get involved in direct work</span></p>\n</li>\n</ul>\n<p><strong>&nbsp;</strong></p>\n<h3><span>3.1.3. Diversity and inclusiveness</span></h3>\n<p><span>Lack of diversity is a significant risk to the effective altruism movement. In short, a lack of diversity means that we are losing out on a lot of talent. Furthermore, there are reasons to think that a community not being diverse is self-reinforcing: people tend to know and be more comfortable getting to know people who are similar to themselves. As an example of how trivial this type of effect can be: when I lived in the UK, I was far more likely to befriend someone if they were Swedish. And so, if a group were to become predominantly students, they would tend to bring in other students and non-students would feel less at home in the setting. Because of the potential loss in value and the self-reinforcing nature of homogeneity in a group, it is important to explicitly put effort into promoting diversity and inclusiveness and it is important to do that at an early stage to avoid calcification of a certain demographic. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>We have put work into this issue over the past year and currently the EA Sweden board is 4/6 female. However, the majority of our members are still white men. Given this, more work in this area is warranted. We hope to be pioneers within the EA community in terms of diversity. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>Below is a short list of the types of actions that we will and have taken in the past to improve our diversity and inclusiveness:</span></p>\n<ul>\n<li>\n<p><span>Put extra effort in the \u201cAwareness\u201d-stage on groups that are currently underrepresented and would make valuable contributions to the community. This can include putting more energy into contacting those organizations whose members may improve our diversity and inclusiveness.</span></p>\n</li>\n<li>\n<p><span>Project the community we want to be. That is, make sure that the people who are visible within the community are diverse in terms of background, ethnicity and gender. This can be done when posting pictures online, but also when choosing who to put on a stage. Of course, this needs to be done with a lot of sensitivity and tact. </span></p>\n</li>\n<li>\n<p><span>Explicitly discuss the importance of diversity/inclusiveness and ways to improve on it. As an example, we ran a workshop on this issue a few months before setting up EA Sweden.</span></p>\n</li>\n<li>\n<p><span>Showing everyone that they are welcome. If, at some event, we suspect that someone might not feel as welcome as everyone else, we will put extra effort into changing that. </span></p>\n</li>\n</ul>\n<h3><span>3.1.4. Other activities</span></h3>\n<ul>\n<li>\n<p><span>Organizing EAGx Scandinavia: </span><span>We have been in discussions with EA Norway about putting on an EAGx Scandinavia and are currently fairly likely to go through with the project. The thought is that this provides a useful ways to involve people in an EA project and to further grow the community.</span></p>\n</li>\n<li>\n<p><span>Seeding new groups:</span><span> So far, EA Sweden has largely been synonymous with EA Stockholm</span><span>. This is something we will put some energy into changing over the coming year, primarily by connecting individuals who are excited about starting local groups with each other and giving them nudges and support in setting up.</span></p>\n</li>\n</ul>\n<h2><span>3.2. Organizational capacity</span></h2>\n<p><span>Organizational capacity is about the organization, and not just the people within it, being able to do good. This includes the organization being good at community-building and conducting direct work. Organizational capacity can be contributed to by having a valuable brand, having the right culture, having systems and processes for community-building, having full-time staff and having the capacity to support members in doing direct work. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>A big part of the plan for the year is to grow the organizational capacity of EAS. This will be done through a number of activities, ranging from: </span></p>\n<ul>\n<li>\n<p><span>Setting/improving on up our digital infrastructure: website, tracking of members, collecting data from events etc.</span></p>\n</li>\n<li>\n<p><span>Setting up processes and policies related to having staff, including payroll</span></p>\n</li>\n<li>\n<p><span>Learning more systematically how we can best carry out our community-building</span></p>\n</li>\n<li>\n<p><span>Providing members with ways to get involved in direct work</span></p>\n</li>\n<li>\n<p><span>Increasing the number of effective altruism leaders, capable of leading local groups and lecturing on effective altruism</span></p>\n</li>\n</ul>\n<p><strong>&nbsp;</strong></p>\n<p><span>The main outstanding question related to organizational capacity whether we should focus on growing the number of staff. Currently, the plan is to put effort into considering this question later during the spring. However, outside perspectives on this question would be very valuable. Crucial questions related to this include: </span></p>\n<ul>\n<li>\n<p><span>What would be the value of another EAF-, CEA- or RC-style organization? </span></p>\n</li>\n<li>\n<p><span>If such an organization would be valuable, what ought the role of EAS in the global EA ecosystem be?</span></p>\n</li>\n<li>\n<p><span>If the organization ought to expand, should only effective altruist donors be considered, or also additional sources of funding?</span></p>\n</li>\n</ul>\n<h2><span>3.3. Crucial considerations regarding capacity-building</span></h2>\n<p><span>Below are questions where we currently are both very uncertain and where the answer seems to be particularly important to how our work should be carried out:</span></p>\n<ul>\n<li>\n<p><span>What is the best way to get more people to act upon the principles of effective altruism? On one extreme, the best strategy is simply exposing a lot of people to a short pitch and on the other is simply focusing on the latter parts of the sales funnel, letting awareness of EA mainly come through online channels and word-of-mouth. EAS current answer is that a combination of the two is needed. </span></p>\n</li>\n<li>\n<p><span>Should we attempt to grow our number of staff?</span></p>\n</li>\n<li>\n<p><span>What are the best methods for us to involve our members in direct work?</span></p>\n</li>\n</ul>\n<ul>\n<li>\n<p><span>Should we put on EAGx Scandinavia? We think the most crucial questions to answer are: How valuable is it? Is the community large enough for a conference?</span></p>\n</li>\n</ul>\n<h1><span>\f</span></h1>\n<h1><span>4. Direct work</span></h1>\n<p><span>In addition to focusing on community-building, we will also focus on direct work, partly due to its importance to community-building. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>The current plan is to focus our efforts on a project related to x-risk, both due to the importance of the cause, the resources available in carrying out such a project (see section 1.3) and that the proposed project seems promising. Below, the planned project and projects that were considered but decided against, are outlined.</span></p>\n<h2><span>4.1. Representation of future generations</span></h2>\n<h3><span>4.1.1. What\u2019s the idea?</span></h3>\n<p><span>Future generations seldom make it to the voting station. Because of this, their interests are poorly represented in current political systems. By future generations here, I mean people other beings who have yet to come into existence. The aim of the project (\u201cRFG\u201d below) is to better represent future generations in the Swedish political system. The hope is that this representation can affect policy which in turn decreases e.g. x-risks. The idea replicates work that has been done in the primarily UK over the past year by among others FUSE and CSER, which for example lead to the formation of the All-Party Parliamentary Group on Future Generations</span><span>.</span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>The current strategy within work on x-risk has focused mainly on researching the risks and raising awareness. However, in order for this research to decrease x-risks, there needs to be a way for the research to affect behaviour of relevant actors. So far, this has mainly been done by raising awareness among those who are likely to develop the potentially harmful technology - i.e. researchers and companies. Another approach is to do this by affecting policy. The aim of this project is to create avenues for x-risk research to do so.</span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>There are many mechanism that could represent future generations. A forthcoming paper, written by members of CSER and FUSE, provides a good introduction to the topic, including different ways in which countries have represented future generations and the successes of these various mechanisms. The paper ends with concrete suggestions concerning how future generations ought to receive representation in a UK context. In short, the mechanisms can range from explicitly granting rights to future generations, requiring that future generations be taken into account in the legislative process, having a body with powers to veto legislation or groups where parliamentarians can discuss issues connected to future generations. One conclusion regarding the different attempts to represent future generations drawn in the paper is that soft power-mechanisms are preferable. Mechanisms where a body e.g. receives the power to veto legislation do not last very long. We are currently not sure what modes of representation are suitable for the Swedish context. Working on this question will be one of the first tasks of the project.</span></p>\n<h3><span>4.1.2. Representation of future generations in Sweden</span></h3>\n<p><span>There has been attempts to do something similar in Swedish government over the past few decades. Below is a short summary of some shallow research into the topic. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>Most of the previous attempts seem to have concerned creating a body that provides the government with strategic thinking, helps coordinate between departments in the government and focuses on questions such as the environment, the future of work and global cooperation. From October 2014 to May 2016, Kristina Persson served as Minister for Strategic Development and presided over the Council on Strategic and Future Issues</span><span>. The Council published three reports in June 2016 on </span><span>The Work of the Future</span><span>, </span><span>The Environmental Transition and Swedish Competitiveness</span><span>, in addition to </span><span>Global Cooperation</span><span>. Both the Council and the Minister for Strategic Development were disbanded in May 2016, during a larger reshuffle of the cabinet</span><span>. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>Currently, the institutions or mechanisms that most resemble a representation of future generations are the Future Commission</span><span> - which replaced the Council on Strategic and Future Issues and is part of the government - and the Agenda 2030-Delegation</span><span>. I have so far been unsuccessful in finding any information as to what the Future Commission has done since its start in June 2016, while the focus of the Agenda 2030-delegation has been to determine how the Sustainable Development Goals should be implemented in Swedish politics. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>In summary, unless the Future Commission is in fact active and will last a significant amount of time, there seems to be a need for a mechanism to represent future generations. Furthermore, given a cursory glance at the previous attempts suggests that most of them have suffered from becoming politicized. Most of the previous bodies focusing on the future seem to have been set up by governments rather than being independent. This needs to be taken into account in determining what type of mechanism for the representation of future generations should be advocated for. </span></p>\n<p><strong>&nbsp;</strong></p>\n<h3><span>4.1.3. Potential benefits</span></h3>\n<p><span>The potential benefits of this project would be:</span></p>\n<ul>\n<li>\n<p><span>Direct impact</span><span>: If the project is successful, it has the potential to improve the lot of future generations. One could be skeptical of the impact here seeing as Sweden is not a particularly large country. However, the smaller size likely means that there is a higher chance of success. In addition, policy in one country affects the policy of other countries. It is often easier to implement a new policy if it has already been done in another country. This suggests that a good strategy is to first implement new policies in small countries and leverage that in other countries. </span></p>\n</li>\n<li>\n<p><span>Community-building</span><span>: The project would be a good opportunity to heavily involve a small group of individuals, making them more knowledgeable and more capable of carrying out important work in the future. In addition to this, the project would be useful to other aspects of community-building, both in being able to reach out to a wider audience, and be able to show that effective altruism is more than a group of people discussing interesting ideas. </span></p>\n</li>\n<li>\n<p><span>Exploration value</span><span>: Potentially the largest impact from carrying out this project is learning about how to do projects of this kind in the future. That is, both projects concerning bringing research concerning x-risk to bear on policy and political processes, but also the ideas of effective altruism more generally. Many have worried that bringing effective altruism to bear on politics poses a risk of effective altruism being perceived as partisan. However, Sweden can act as a kind of testing ground here. If effective altruism does become perceived as partisan and that turns out to be costly, it would be far less costly than if that was the case in a larger or an English-speaking country. This is not to say that the risk of being perceived as partisan should not be mitigated, but merely that a negative outcome would be less costly. </span></p>\n</li>\n</ul>\n<h3><span>4.1.4. Impact mechanism and failure modes</span></h3>\n<p><span>A useful way to identify how the project might fail and finding ways to mitigate those risks is to think about the impact mechanism of the project, identify ways for it to go wrong and how to mitigate those risks. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>In order for the project to have a direct positive impact</span><span> the following needs to occur: (i) some mechanism of representation of future generations needs to be implemented and sustained, (ii) that mechanism must affect policy in some way and (iii) it affects policy positively. The failure modes for this kind of project are the inverse of the steps in the impact mechanism. There are therefore three failure modes: </span></p>\n<ol>\n<li>\n<p><span>No mechanism for the representation of future generations is implemented or alternatively, the mechanism does not last a long time</span></p>\n</li>\n<li>\n<p><span>The implemented mechanism does not affect policy</span></p>\n</li>\n<li>\n<p><span>The implemented mechanism affects policy negatively</span></p>\n</li>\n</ol>\n<p><strong>&nbsp;</strong></p>\n<p><span>For a graphical representation, see illustration 2 below.</span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995336/mirroredImages/RM77XJj3xGLXZk2es/bctikjlzoy0z2bjc3vc7.png\"></span></p>\n<p><span>Illustration 2. Impact mechanism and failure modes of the project</span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>Failure mode 1</span><span> receives the most attention from the forthcoming paper out of CSER and FUSE. They focus on a number of factors that might lead to this failure mode</span><span>. In particular, they argue that modes of representation that use informal power are preferable, mainly because they last longer. Examples of formal powers include being able to veto legislation. When these types of mechanisms have been implemented, they have often been removed within a short amount of time, presumably because the institution easily becomes politicized. Exemplifying this, the Israeli parliament established a Commission for Future Generations in 2001, with scope ranging across 12 policy areas, the ability to initiate bills and the ability to veto legislation that did not comply with the interests of future generations. The body was given a five-year mandate which was not renewed, officially due to budgetary reasons, but there are indications that parliamentarians felt the body had too much authority. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>In addition to the above, the Swedish context needs to be taken into account. There is a Swedish parliamentary election coming up in September. This means that the media and politicians will have their attention fixed on more near-term political issues: taxes, immigration, schooling etc. The election also means that many parliamentarians will be changing jobs in September. Therefore, resources being put into trying to implement mechanisms for the representation of future generations before September risk being wasted. Additionally, previous attempts at establishing mechanisms for the representation of future generations ought to be used as case studies to learn from. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>Failure mode 2 </span><span>seems like a substantial risk: if the mechanism utilizes soft power, it seems that it would be less capable of affecting policy. In addition, if the x-risk research field is not mature enough to make specific policy recommendations, there is a risk that the mechanism has no policy to discuss or put forward. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>Failure mode 3</span><span> is surely something to be avoided as it not only poses risks having direct negative effects, but it might also lead to severe reputational costs. This failure mode could potentially arise in one of two ways: the recommendations of x-risk researchers turn out to be harmful or the mechanism is co-opted. Both of these risks hinge on the question of whether the x-risk research field is mature enough to come up with useful recommendations. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>To avoid these failure modes the project ought to:</span></p>\n<ul>\n<li>\n<p><span>Concentrate resources into public advocacy after the election or a significant amount of time before the election</span></p>\n</li>\n<li>\n<p><span>Include high-profile academics who can lend credibility to the idea </span></p>\n</li>\n<li>\n<p><span>Ensure that there is a sustainable connection between the mechanism and the x-risk research community. This has for example been done in regards to the APPG by CSER providing the secretariat for the group</span></p>\n</li>\n<li>\n<p><span>Learn from people with significant experience of campaigning and the political process in Sweden</span></p>\n</li>\n<li>\n<p><span>Learn from previous mechanisms to represent future generations in Swedish politics</span></p>\n</li>\n</ul>\n<h3><span>4.1.5. Do we have the resources to carry out this project?</span></h3>\n<p><span>This project will require a lot of capabilities, of which we think we have some. Below is a list of needed resources and our access to these resources:</span></p>\n<ul>\n<li>\n<p><span>Enough research within the x-risk space to warrant the mechanism: </span><span>A mechanism for the representation of future generations can only provide direct value if we know what policy it ought to promote or speak out against. To further investigate this question we need to discuss the issue with more researchers in the x-risk field.</span></p>\n</li>\n<li>\n<p><span>Resources to carry out the project</span><span>: To carry out the project, we will need the right workforce. Between me working full-time and interested volunteers, we feel confident we will be able to carry it out (although with a high chance of failure). </span></p>\n</li>\n<li>\n<p><span>An understanding of and a relevant network within Swedish politics</span><span>: Carrying out the project successfully requires an understanding of how the Swedish political system works and a related network. We likely have access to this network. </span></p>\n</li>\n<li>\n<p><span>A network of x-risk researchers and academics</span><span>: We have access to a network of x-risk researchers through effective altruism. </span></p>\n</li>\n</ul>\n<h3><span>4.1.6. Plan outline</span></h3>\n<p><span>The outline of the plan is as follows: Before the election (September 2018) the groundworks for a campaign are created. After the elections, a public campaign will be rolled out. The nature of this campaign is yet to be determined. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>The activities to be carried out as part of setting the groundworks include: </span></p>\n<ul>\n<li>\n<p><span>Gather feedback on the project idea from relevant experts (within the EA community, but mainly those with expertise in Swedish politics)</span></p>\n</li>\n<li>\n<p><span>Assess interest in the topic from Swedish politicians</span></p>\n</li>\n<li>\n<p><span>Put together a working group of high-impact individuals who can work on and support the project</span></p>\n</li>\n<li>\n<p><span>Conduct an analysis of the history of representation of future generations in Swedish politics</span></p>\n</li>\n<li>\n<p><span>Develop a strategy for the representation of future generations in a Swedish context. This includes answering questions such as: What mechanisms for the representation of future generations are suitable? Should these mechanisms be implemented in a specific order? What are some specific policies that these mechanisms could examine?</span></p>\n</li>\n<li>\n<p><span>Develop a short-list of the issues and policies that we would like a mechanism to represent future generations to address</span></p>\n</li>\n<li>\n<p><span>Build interest in the issue, by putting on a number of workshops</span></p>\n</li>\n<li>\n<p><span>Develop a concrete plan of action for after the election</span></p>\n</li>\n<li>\n<p><span>Prepare for launch of campaign post-election</span></p>\n</li>\n</ul>\n<h2><span>4.2. Some projects we have decided against for now</span></h2>\n<p><span>A lot of project ideas have been considered. Below is a short explanation of each of them. If you\u2019re keen, they\u2019re yours for the taking!</span></p>\n<h3><span>4.2.1. EA Fact Checkers / The EA Reproducibility Project</span></h3>\n<p><span>To do good in the world, it is important to have accurate beliefs about the world: facts are important. Because of this it is particularly important to uphold high epistemic standards. One way in which this could be done would be to have an independent organization that checks on claims made within the effective altruism community. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>One way to implement this idea - the version I call EA Fact Checkers - would be to focus in on facts or memes that are commonly cited within the effective altruism community. Examples of such claims are: \u201cdonating to charity makes you happier\u201d, \u201csome charities are 1000x more effective than others\u201d</span><span> and the graph from Ord (2013)</span><span> showing the difference in cost-effectiveness between different interventions. Most of these claims are likely not acted upon directly, and so fact-checking them would not have a direct positive effect on the world. It would likely not lead to money being allocated more effectively. However, there likely would be positive effects in upholding epistemic standards within the community. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>Another way to implement this idea - EA Reproducibility Project</span><span> - would be to focus in on facts and claims on which people base decisions: e.g. recommendations on charity donations. The epistemic standards put on research within the effective altruism community is likely high. However, my impression is that the majority of checking on claims is done within EA organizations themselves. Checking from outside of organizations is primarily informal: asking people one knows for feedback, publicly and transparently communicating recommendations through e.g. the EA Forum. What does not exist is a systematic way in which recommendations are externally vetted. However, this might be needed as the EA movement grows larger. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>An initial sketch of how to carry out this EA Reproducibility Project is as follows: Initially, one would focus on the simplest, most mechanical way of vetting recommendations, where there is little room for interpretation. This would likely be by randomly choosing a recommendation made by an EA organization and checking whether its quantitative models include mistakes or the studies cited supported the claims made. If the first few iterations proved successful, the project could expand into either focusing on a more of the research within the EA community (e.g. vetting citations in blog post from EA organizations) or into carrying out more substantive reproductions (e.g. by looking at whether the research left out important studies or made dubious arguments). Further down the line, this independent organization or project could be a part of setting common epistemic standards or setting up systems of effective altruism peer-review.</span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>We decided against working on this project since we do not see ourselves as having a competitive advantage in carrying it out. However, I think it could be quite valuable, both in upholding epistemic standards and in becoming a part of the effective altruism researcher pipeline.</span></p>\n<h3><span>4.2.2. Aid effectiveness-ranking of political parties</span></h3>\n<p><span>A simple way to potentially affect Swedish aid policy would be to conduct a ranking of political parties in terms of the quality of their aid policies </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>Similar projects have been run in the past. The Effective Altruism Foundation did a similar ranking in connection with the Swiss election, but focusing on the politics of specific candidates, not parties, and looking at all policy, not just aid policy. EA NTNU in Norway did a similar project during the recent Norwegian elections, which prompted the Norwegian Aid Minister to discuss the suggestions at one of their events</span><span>. In both of these cases, all of the work was done by the local group itself. This is likely good for e.g. skill-building. However, to affect policy, there likely needs to be a greater involvement of academics with credibility, both to make sure the conclusions are more likely to be correct and to lend the project credibility. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>Another notable similar project, and which this type of project would likely take a lot of input from, is the Centre for Global Development\u2019s Commitment to Development Index, a ranking of countries in terms of their contribution to global development</span><span>. </span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>The potential benefits of carrying out such a project would be:</span></p>\n<ul>\n<li>\n<p><span>Exploration value in learning about how effective altruism can be applied to the political sphere and potentially providing a blueprint for carrying out the same project in other countries</span></p>\n</li>\n<li>\n<p><span>Direct impact through potentially affecting policy. Presumably, the biggest majority of the potential impact would not be by swaying voters to vote for parties with better policies, but rather through getting politicians to pay attention to the issue. This would be accomplished both through them believing that the issue will sway voters and by simply the project exposing them to concrete policy proposals. </span></p>\n</li>\n<li>\n<p><span>Capacity-building, especially by providing a good opportunity to engage high-profile academics about the ideas of effective altruism</span></p>\n</li>\n</ul>\n<h2><span>4.3. Crucial considerations regarding direct work</span></h2>\n<p><span>Below are questions where we currently are both very uncertain and where the answer seems to be particularly important to how or if the project should be carried out:</span></p>\n<p>&nbsp;</p>\n<ul>\n<li>\n<p><span>Regarding the representation of future generations project (RFG), is there research within the x-risk field that could be implemented through the proposed mechanism?</span></p>\n</li>\n<li>\n<p><span>Regarding RFG, should this kind of campaign for the representation of future generations be explicitly linked to effective altruism?</span></p>\n</li>\n<li>\n<p><span>Regarding RFG, how can the mistakes of previous similar attempts in Sweden be avoided?</span></p>\n</li>\n<li>\n<p><span>Regarding RFG, should the roll-out of the project wait until after the election? Should e.g. a declaration with signatures be made during the election campaign?</span></p>\n</li>\n<li>\n<p><span>Given the above, ought we to focus on RFG?</span></p>\n</li>\n</ul>", "user": {"username": "MarkusAnderljung"}}, {"_id": "uaJ4ZGeYSphFG4tbC", "title": "Proposal for the AI Safety Research Camp", "postedAt": "2018-02-02T08:07:31.869Z", "htmlBody": "<html><body><div>\n<div>\n<p><em>Ways in which you can contribute:<br></em></p>\n<ul>\n<li><em><em>Give your feedback below or in the&#xA0;<a href=\"https://docs.google.com/document/d/1QlKruAZuuc5ay0ieuzW5j5Q100qNuGNCTHgC4bIEXsg/edit?ts=5a651a00\">GDoc</a></em><br></em></li>\n<li><em><em><u><a href=\"https://docs.google.com/forms/d/e/1FAIpQLScL9QaM5vLQSpOWxxb-Y8DUP-IK4c8DZlYSkL6pywz8OSQY1g/viewform\">Apply</a></u>&#xA0;to take part in the Gran Canaria camp on 12-22 April&#xA0;</em><em>(deadline: 12 February)</em></em></li>\n<li><em><em><u><a href=\"https://www.facebook.com/groups/348759885529601/\">Join</a></u>&#xA0;the Facebook group</em></em></li>\n</ul>\n<p>&#xA0;</p>\n<p>Below, we share our plans to launch an AI Safety Research Camp this year (a repost from&#xA0;<u><a href=\"https://www.lesserwrong.com/posts/KgFrtaajjfSnBSZoH/ai-safety-research-camp-project-proposal/\">LessWrong</a></u>). <em><br></em></p>\n<h1>Summary</h1>\n<p><strong>Aim:&#xA0;</strong>Efficiently launch aspiring AI safety and strategy researchers into concrete productivity by creating an &#x2018;on-ramp&#x2019; for future researchers.</p>\n<p>Specifically:</p>\n<ol>\n<li>Get people started on and immersed into concrete research work intended to lead to papers for publication.</li>\n<li>Address the bottleneck in AI safety/strategy of few experts being available to train or organize aspiring researchers by efficiently using expert time.</li>\n<li>Create a clear path from &#x2018;interested/concerned&#x2019; to &#x2018;active researcher&#x2019;.</li>\n<li>Test a new method for bootstrapping talent-constrained research fields.</li>\n</ol>\n<p><strong>Method:&#xA0;</strong>Run an online research group culminating in a two week intensive in-person research camp. Participants will work in groups on tightly-defined research projects on the following topics:</p>\n<ul>\n<li>Agent foundations</li>\n<li>Machine learning safety</li>\n<li>Policy &amp; strategy</li>\n<li>Human values</li>\n</ul>\n<p>Projects will be proposed by participants prior to the start of the program. Expert advisors from AI Safety/Strategy organisations will help refine them into proposals that are tractable, suitable for this research environment, and answer currently unsolved research questions. This allows for time-efficient use of advisors&#x2019; domain knowledge and research experience, and ensures that research is well-aligned with current priorities.</p>\n<p>Participants will then split into groups to work on these research questions in online collaborative groups over a period of several months. This period will culminate in a two week in-person research camp aimed at turning this exploratory research into first drafts of publishable research papers. This will also allow for cross-disciplinary conversations and community building, although the goal is primarily research output. Following the two week camp, advisors will give feedback on manuscripts, guiding first drafts towards completion and advising on next steps for researchers.</p>\n<p><strong>Example:&#xA0;</strong>Multiple participants submit a research proposal or otherwise express an interest in interruptibility during the application process, and in working on machine learning-based approaches. During the initial idea generation phase, these researchers read one another&#x2019;s research proposals and decide to collaborate based on their shared interests. They decide to code up and test a variety of novel approaches on the relevant AI safety gridworld. These approaches get formalised in a research plan.</p>\n<p>This plan is circulated among advisors, who identify the most promising elements to prioritise and point out flaws that render some proposed approaches unworkable. Participants feel encouraged by expert advice and support, and research begins on the improved research proposal.</p>\n<p>Researchers begin formalising and coding up these approaches, sharing their work in a Github repository that they can use as evidence of their engineering ability. It becomes clear that a new gridworld is needed to investigate issues arising from research so far. After a brief conversation, their advisor is able to put them in touch with the relevant engineer at Deepmind, who gives them some useful tips on creating this.</p>\n<p>At the research camp the participants are able to discuss their findings and put them in context, as well as solve some technical issues that were impossible to resolve part-time and remotely. They write up their findings into a draft paper and present it at the end of the camp. The paper is read and commented on by advisors, who give suggestions on how to improve the paper&#x2019;s clarity. The paper is submitted to NIPS 2018&#x2019;s Aligned AI workshop and is accepted.</p>\n<p><strong>Expected outcome:&#xA0;</strong>Each research group will aim to produce results that can form the kernel of a paper at the end of the July camp. We don&#x2019;t expect every group to achieve this, as research progress is hard to predict.</p>\n<ol>\n<li>At the end of the camp, from five groups, we would expect three to have initial results and a first draft of a paper that the expert advisors find promising.</li>\n<li>Within six months following the camp, three or more draft papers have been written that are considered to be promising by the research community.</li>\n<li>Within one year following the camp, three or more researchers who participated in the project obtain funding or research roles in AI safety or strategy.</li>\n</ol>\n<p><strong>Next steps following the camp:</strong>&#xA0;When teams have produced promising results, camp organizers and expert advisors will endeavour to connect the teams to the right parties to help the research shape up further and be taken to conclusion.</p>\n<p>Possible destinations for participants who wish to remain in research after the camp would likely be some combination of:</p>\n<ol>\n<li>Full-time internships in areas of interest, for instance&#xA0;<u><a href=\"https://deepmind.com/careers/476630/\">Deepmind</a></u>,&#xA0;<u><a href=\"https://www.fhi.ox.ac.uk/vacancies/\">FHI</a></u>&#xA0;or&#xA0;<u><a href=\"http://humancompatible.ai/jobs#internship\">CHAI</a></u></li>\n<li>Full-time research roles at AI safety/strategy organisations</li>\n<li>Obtaining research funding such as&#xA0;<u><a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/open-philanthropy-project-ai-fellows-program\">OpenPhil</a></u>&#xA0;or&#xA0;<u><a href=\"https://futureoflife.org/2017/12/20/2018-international-ai-safety-grants-competition/\">FLI</a></u>&#xA0;research grants - successful publications may unlock new sources of funding</li>\n<li>Independent remote research</li>\n<li>Research engineering roles at technical AI safety organisations</li>\n</ol>\n<p>Research projects can be tailored towards participants&#x2019; goals - for instance researchers who are interested in engineering or machine learning-related approaches to safety can structure a project to include a significant coding element, leading to (for instance) a GitHub repo that can be used as evidence of engineering skill. This is also a relatively easy way for people who are unsure if research work is for them to try it out without the large time investment and opportunity cost of a PhD or masters program, although we do not see it as a full replacement for these.</p>\n<h3>Plan</h3>\n<p><strong><u><a href=\"https://docs.google.com/document/d/1l02CtyQo-sUiXv4RlwRB2DHOXyS6pC7_lGcx0vxOOtY/edit\">Timeline</a></u>:&#xA0;</strong>We anticipate this project having 4 main phases (dates are currently open for discussion):</p>\n<ol>\n<li>Plan and develop the project, recruit researchers and look for advisors - December 2017 to April 2018</li>\n<li>Testing and refinement of event design during a small-scale camp at Gran Canaria - April 12-22</li>\n<li>Project selection, refinement and exploration (online) - April 2018 to July 2018</li>\n<li>Research camp (in person) - July/August 2018</li>\n</ol>\n<p><strong>Recruiting:&#xA0;</strong>We plan to have approximately 20 researchers working in teams of 3-5 people, with projects in agent foundations, machine learning, strategy/policy and human values/cognition. Based on responses to&#xA0;the interest form we&#xA0;<u><a href=\"https://drive.google.com/open?id=1xj7ffjihLyIqtPPEMozwnHA3HsKhzFD7ZxblVkA1O5g\">initially posted online</a></u>,<span>&#xA0;we expect to be able to easily meet this number of participants.</span></p>\n<p>Each team will be advised by a more experienced researcher in the relevant area, however we expect this won&#x2019;t be as tightly-coupled a relationship as that between PhD students and their supervisors - the aim is to maximise the usefulness of the relatively scarce advisor time and to develop as much independence in researchers as possible.</p>\n<p><strong>Project selection and exploration:&#xA0;</strong>Once the initial recruitment phase is complete, researchers and advisors can choose a project to work on and refine it into a single question answerable within the timeframe. We recognise the need for strong project planning skills and careful project choice and refinement here, and this project choice is a potential point of failure (see Important Considerations below). Following project selection, researchers will begin exploring the research project they&#x2019;ve chosen in the months between project choice and the research camp. This would probably require five to ten hours a week of commitment from researchers, mostly asynchronously but with a weekly &#x2018;scrum&#x2019; meeting to share progress within a project team. Regular sharing of progress and forward planning will be important to keep momentum going.</p>\n<p><strong>Research camp:</strong>&#xA0;Following the selection and exploration, we will have a two-week intensive camp assembling all participants in-person at a retreat to do focused work on the research projects. Exploratory work can be done asynchronously, but finishing research projects can be hard work and require intensive communication which can more easily be done in person. This also makes the full-time element of this project much more bounded and manageable for most potential participants. An in-person meeting also allows for much better communication between researchers on different projects, as well as helping form lasting and fruitful connections between researchers.</p>\n<h3>Important Considerations</h3>\n<p><strong>Shaping the research question:&#xA0;</strong>Selecting good research questions for this project will be challenging, and is one of the main potential points of failure. The non-traditional structure of the event brings with it some extra considerations. We expect that most projects will be:</p>\n<ol>\n<li>Tractable to allow progress to be made in a short period of time, rather than conceptually complex or open-ended</li>\n<li>Closely related to current work, e.g. suggestions found in &#x2018;further work&#x2019; or &#x2018;open questions&#x2019; sections from recent papers</li>\n<li>Parallelisable across multiple researchers, e.g. evaluating multiple possible solutions to a single problem or researching separate aspects of a policy proposal</li>\n</ol>\n<p>This biases project selection towards incremental research, i.e. extending previous work rather than finding completely new approaches. This is hard to avoid in these circumstances, and we are optimising at least partly for the creation of new researchers who can go on to do more risky, less incremental research in the future. Furthermore, a look at the &#x2018;future work/open questions&#x2019; sections of many published safety papers will reveal a broad selection of interesting, useful questions that still meet the criteria above so although this is a tradeoff, we do not expect it to be overly limiting. A good example of this in the Machine Learning subfield would be evaluating multiple approaches to one of the problems listed in DeepMind&#x2019;s recent&#xA0;<u><a href=\"https://arxiv.org/abs/1711.09883\">AI Safety gridworlds paper</a></u>.</p>\n<p><strong>Finding advisors:&#xA0;</strong>Although we intend this to be relatively self-contained, some amount of advice from active researchers will be beneficial at both the project selection and research stages, as well as at the end of the camp. The most useful periods for advisor involvement will be at the initial project selection/shaping phase and at the end of the camp - the former allows for better, more tractable projects as well as conveying previously unpublished relevant information and a sense of what&#x2019;s considered interesting. The latter will be useful for preparing papers and integrating new researchers into the existing community. Informal enquiries suggest that it is likely to be possible to recruit advisors for these stages, but ongoing commitments will be more challenging.</p>\n<p>The expected commitment during project selection and shaping would be one or two sessions of several hours spent evaluating and commenting on proposed research projects. This could be done asynchronously or by video chat. Commitment at the end of the research camp is likely to be similar - responding to initial drafts of papers with suggestions of improvements or further research in a similar way to the peer review process.</p>\n<p><strong>Costs:&#xA0;</strong>The main costs for the Gran Canaria camp, the AirBnBs, meals and low-income travel reimbursements, have been covered now by two funders.</p>\n<p>The July camp will likely take place in the UK at the&#xA0;<u><a href=\"https://www.facebook.com/groups/1624791014242988/?ref=br_rs\">EA Hotel</a></u>, a co-working hub planned by Greg Colbourn (for other options, see&#xA0;<u><a href=\"https://docs.google.com/spreadsheets/d/1cX4yrEH4Kw8-CdT9zRhV7EHD1xXLO6T9NqiK5FQ5pKk/edit#gid=0\">here</a></u>). For this, we will publish a funding proposal around April. <br>Please see&#xA0;<u><a href=\"https://docs.google.com/spreadsheets/d/1P5W8u8czOp_MEaZtb2iwPmmFs1CFkkH6RuI0NHrfP_I/edit#gid=781421298\">here</a></u>&#xA0;for the draft budgets.</p>\n<h3>Long-term and wider impacts</h3>\n<p>If the camp proves to be successful, it could serve as the foundation for yearly recurring camps to keep boosting aspiring researchers into productivity. It could become a much-needed additional lever to grow the fields of AI safety and AI strategy for many years to come. The research camp model could also be used to grow AI safety research communities where none presently exist, but there is a strong need - in China, for instance. By using experienced coordinators and advisors in conjunction with local volunteers, it may be possible to organise a research camp without the need for pre-existing experts in the community. A camp provides a coordination point for interested participants, signals support for community building, and if previous camps have been successful provides social proof for participants.</p>\n<p>In addition, scaling up research into relatively new cause areas is a problem that will need to be solved many times in the effective altruist community. This could represent an efficient way to &#x2018;bootstrap&#x2019; a larger research community from a small pre-existing one, and so could be a useful addition to the tool set available to the EA community.</p>\n<p>This project serves as a natural complement to other AI safety projects currently in development such as&#xA0;<u><a href=\"https://wiki.lesswrong.com/wiki/Road_to_AI_Safety_Excellence\">RAISE</a></u>&#xA0;that aim to teach researchers the foundational knowledge they will need to begin research. Once an aspiring AI safety researcher completes one of these courses, they might consider a research camp as a natural next step on the road to become a practicing researcher.</p>\n<h3>Acknowledgements</h3>\n<p>Thanks to Ryan Carey, Chris Cundy, Victoria Krakovna and Matthijs Maas for reading and providing helpful comments on this document.</p>\n<h3>Organisers</h3>\n<p><u><a href=\"https://tommcgrath.github.io/\">Tom McGrath</a></u><br>Tom is a maths PhD student in the&#xA0;<u><a href=\"http://wwwf.imperial.ac.uk/~nsjones/\">Systems and Signals</a></u>&#xA0;group at Imperial College, where he works on statistical models of animal behaviour and physical models of inference. He will be interning at the&#xA0;<u><a href=\"https://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a></u>&#xA0;from Jan 2018, working with&#xA0;<u><a href=\"https://www.fhi.ox.ac.uk/team/owain-evans/\">Owain Evans</a></u>. His previous organisational experience includes co-running Imperial&#x2019;s&#xA0;<u><a href=\"http://mathshelpdesk.ma.ic.ac.uk/\">Maths Helpdesk</a></u><span>&#xA0;</span>and running a postgraduate deep learning study group</p>\n<p><u><a href=\"https://nl.linkedin.com/in/remmelt-ellen-19b88045\">Remmelt Ellen<br></a></u>Remmelt is the Operations Manager of&#xA0;<u><a href=\"https://effectiefaltruisme.nl/en/effective-altruism-netherlands/\">Effective Altruism Netherlands</a></u>, where he coordinates national events, works with organisers of new meetups and takes care of mundane admin work. He also oversees planning for the team at&#xA0;<u><a href=\"https://wiki.lesswrong.com/wiki/Accelerating_AI_Safety_Adoption_in_Academia\">RAISE</a></u>, an online AI Safety course. He is a Bachelor intern at the&#xA0;<u><a href=\"https://www.cwi.nl/research/groups/intelligent-and-autonomous-systems\">Intelligent &amp; Autonomous Systems</a>&#xA0;</u>research group. In his spare time, he&#x2019;s exploring how to improve the interactions within multi-layered networks of agents to reach shared goals &#x2013; especially approaches to collaboration within the EA community and the representation of persons and interest groups by&#xA0;<u><a href=\"https://homepages.cwi.nl/~baarslag/pub/When_Will_Negotiation_Agents_Be_Able_to_Represent_Us-The_Challenges_and_Opportunities_for_Autonomous_Negotiators.pdf\">negotiation agents</a></u>&#xA0;in&#xA0;<u><a href=\"http://www.oilcrash.com/articles/complex.htm\">sub-exponential</a></u>&#xA0;takeoff scenarios.</p>\n<p><u><a href=\"https://docs.google.com/document/d/1NkYDp3zns-cyasAk_WDrhj6DlJ9QjMP24fM7jTvWPqM/edit?usp=sharing\">Linda Linsefors<br></a></u>Linda has a PhD in theoretical physics, which she obtained at&#xA0;<u><a href=\"https://doctorat.univ-grenoble-alpes.fr/en/doctoral-studies/research-fields/physics-630344.htm\">Universit&#xE9; Grenoble Alpes</a></u>&#xA0;for work on loop quantum gravity. Since then she has studied AI and AI Safety online for about a year. Linda is currently working at&#xA0;<u><a href=\"http://www.org.umu.se/icelab/english/?languageId=3\">Integrated Science Lab</a></u>&#xA0;in Ume&#xE5;, Sweden, developing tools for analysing information flow in networks. She hopes to be able to work full time on AI Safety in the near future.</p>\n<p><u><a href=\"https://www.linkedin.com/in/nandi-schoots-70bba8125/?locale=en_US\">Nandi Schoots<br></a></u>Nandi has a research master in pure mathematics and a minor in psychology from Leiden University. Her master was focused on algebraic geometry and her&#xA0;<u><a href=\"https://www.universiteitleiden.nl/binaries/content/assets/science/mi/scripties/masterschoots.pdf\">thesis</a></u>&#xA0;was in category theory. Since graduating she has been steering her career in the direction of AI safety. She is currently employed as a data scientist in the Netherlands. In parallel to her work she is part of a study group on AI safety and involved with the reinforcement learning section of&#xA0;<u><a href=\"https://wiki.lesswrong.com/wiki/Accelerating_AI_Safety_Adoption_in_Academia\">RAISE</a></u>.</p>\n<p><u><a href=\"https://www.linkedin.com/in/davidkristoffersson/\">David Kristoffersson<br></a></u>David has a background as R&amp;D Project Manager at Ericsson where he led a project of 30 experienced software engineers developing many-core software development tools. He liaised with five internal stakeholder organisations, worked out strategy, made high-level technical decisions and coordinated a disparate set of subprojects spread over seven cities on two different continents. He has a further background as a Software Engineer and has a BS in Computer Engineering. In the past year, he has contracted for the&#xA0;<u><a href=\"https://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a></u>, has explored research projects in ML and AI strategy with FHI researchers, and is currently collaborating on existential risk strategy research with&#xA0;<u><a href=\"http://convergenceanalysis.org/\">Convergence</a></u>.</p>\n<p><u>Chris Pasek<br></u>After graduating from mathematics and theoretical computer science, Chris ended up touring the world in search of meaning and self-improvement, and finally settled on working as a freelance researcher focused on AI alignment. Currently also running a rationalist shared housing project on the tropical island of Gran Canaria and continuing to look for ways to gradually self-modify in the direction of a superhuman FDT-consequentialist entity with a goal to save the world.</p>\n<p>&#xA0;</p>\n<p><em>Ways in which you can contribute:<br></em></p>\n<ul>\n<li><em>Give your feedback below or in the&#xA0;<a href=\"https://docs.google.com/document/d/1QlKruAZuuc5ay0ieuzW5j5Q100qNuGNCTHgC4bIEXsg/edit?ts=5a651a00\">GDoc</a></em></li>\n<li><em>&#xA0;<u><a href=\"https://docs.google.com/forms/d/e/1FAIpQLScL9QaM5vLQSpOWxxb-Y8DUP-IK4c8DZlYSkL6pywz8OSQY1g/viewform\">Apply</a></u>&#xA0;to take part in the Gran Canaria camp on 12-22 April&#xA0;</em><em>(deadline: 12 February)</em></li>\n<li><em><u><a href=\"https://www.facebook.com/groups/348759885529601/\">Join</a></u>&#xA0;the Facebook group</em></li>\n</ul>\n</div>\n</div></body></html>", "user": {"username": "remmelt"}}, {"_id": "n8nXqqgbwp58wuo6a", "title": "How fragile was history?", "postedAt": "2018-02-02T06:23:54.282Z", "htmlBody": "<html><body><p><span>Elsewhere</span>&#xA0;(and better): <span><a href=\"https://www.google.com/url?q=https://www.openphilanthropy.org/blog/long-term-significance-reducing-global-catastrophic-risks&amp;sa=D&amp;ust=1517555783604000&amp;usg=AFQjCNHD6xdUAq4gXEvfNvOg2caJ1jTPDg\">1</a></span>, <span><a href=\"https://www.google.com/url?q=http://lukemuehlhauser.com/industrial-revolution/%23Negative&amp;sa=D&amp;ust=1517555783604000&amp;usg=AFQjCNGzNGzdLHzkwuFn5iQBk_afKx4aJQ\">2</a></span>.<span><br><br>If one could go back in time and make a small difference in the past, would one expect it to effect dramatic changes to the future? Questions like these are fertile soil for fiction writers (generally writing under speculative or alternative history) but receive less attention in the historical academy, which tends to focus on explaining what in fact happened, rather than what could have been. Yet general questions of historical fragility (e.g. Are events in human history &#x2018;generally&#x2019; fragile? In what areas is history particularly fragile? Are things getting more or less fragile over time?) are of particular interest to those interested in altering the course the long-run future by differences they make today.</span></p>\n<p>The small but reasonably sophisticated historiographic literature in this area (<span><a href=\"https://www.google.com/url?q=http://onlinelibrary.wiley.com/doi/10.1111/0018-2656.00090/abstract&amp;sa=D&amp;ust=1517555783605000&amp;usg=AFQjCNE8G2aNuWkDTU54XQn9WhRKtGg7wA\">e.g.</a></span>) often uses analogies to chaos theory: whether minor perturbations in antecedent conditions can flow through into huge consequences, akin to the archetypal &#x2018;butterfly flapping its wings causing a hurricane&#x2019; example. In defence of history being chaotic one can point to various events which seemed to have large consequences yet great uncertainty <span>ex ante</span><span>. Had Hitler been killed in the first world war, or was assassinated in 1944, it appears the subsequent course of the 20th century would have been very different. Although &#x2018;great man&#x2019; views of history find little favour, many events seem to be contingent on the traits and foibles of particular individuals: whether the Nazi party would have risen to power, whether they would have prompted a world war, and the precise conduct of any such war seem to vary a lot depending on whether Hitler was alive and in charge. </span></p>\n<h1><span>The chaos of coital counterparts</span></h1>\n<p>I suggest chaotic views of history can draw considerable support from the sensitivity to initial conditions and large variation that attends any human conception. Conception involves one female gamete (which is replaced with a genetically distinct one on a monthly basis or so), and one male gamete from a pool of 100 million genetically distinct sperm (themselves taken from an even larger pool with roiling substitution). All manner of trivial contingencies scramble which two would fuse to conceive a child: if the parents decide to have sex another month, another day (or another minute), when they last had sex, ambient temperature, and so on and so forth.<sup><a href=\"#ftnt1\">[1]</a></sup><span>&#xA0;Siblings give us an impression of the variability of (forgive me) coital counterparts, and they are far from identical.</span></p>\n<p>Moreover (somewhat like chaos theory) replacing one person with their coital-counterpart seems to cascade onwards to even more scrambling: not only do all the coital-counterparts descendants differ, but there is horizontal contagion if the counterpart&#x2019;s slightly different behaviour influences others around them to only minutely change the manner of them conceiving children. Maybe there would have been something like the Mongol empire without Genghis Khan, but the precise details seem likely to vary. Yet these precise details may be decisive into which pairings between potential parents occur, and especially for exactly when they conceive, and the ripples of this activity goes on to perturb further successors, and so on and so forth. It seems not too wild to suggest that none of us would be here if the parents of Genghis Khan decided to postpone having sex one night until the following morning.<sup><a href=\"#ftnt2\">[2]</a></sup></p>\n<h1><span>Constraints on chaos</span></h1>\n<p><span>Yet if whether an individual or a potential sibling exists is very fragile, many other parts of history seem fragile. Imagining the course of the 20th century where every individual is replaced with their sibling drawn from the genetic lottery seems nigh-impossible. Yet perhaps some very broad predictions (perhaps some technological developments, population growth, etc.) could still be confidently held. </span></p>\n<p><span>One may speculate there is an issue of scale. Although maybe even very large historical events (e.g. a given empire, a world war) are seen as fragile, one could zoom out even further. If one was asked to summarise all of human history in a sentence, one may be able to include little more than, &#x201C;Humans arose 300 000 years ago, and spent 290 000 years or so as hunter-gatherers. They developed agriculture 10000 years ago, and the industrial revolution happened about a hundred years ago.&#x201D; These details seem pretty robust. There are broader contours of the human condition that constrain the course the eddies of possible people may take.</span></p>\n<p>One may hypothesis more to the story than &#x2018;stable over long run, very fragile over short run&#x2019;. There seem some &#x2018;meso-level&#x2019; factors which alter the balance of contingency versus necessity, and so constrain the fan of possibilities on particular matters. The <span><a href=\"https://www.google.com/url?q=https://nickbostrom.com/papers/future.pdf&amp;sa=D&amp;ust=1517555783607000&amp;usg=AFQjCNEfHbjXx8GZdpfLh6gRrKizgTcwLQ\">technological completion conjecture</a></span><span>&#xA0;might be one of these: although whether, which, and when great scientists are born may alter the precise timing of discoveries, they do get discovered not too much sooner or later: if Einstein was never born, the theory of relativity would have eventually been discovered. </span></p>\n<p>By contrast, creative work does not have this property: if Dostoyevsky wasn&#x2019;t born, <span>The Brothers Karamazov </span>would not have been written by someone else. Where ideas fall may vary between these: perhaps (e.g.) liberalism, democracy, and communism would have been proposed by an entirely different set of thinkers; the character of major world religions seems likely to be substantially different in worlds without (e.g.) Jesus, Mohammad, Confucius, or Buddha. To what degree these things vary is unclear, but perhaps not wholly intractable with careful study. <span>&#xA0;</span></p>\n<h1><span>The goldilocks zone of future fragility</span></h1>\n<p><span>The value of efforts to shape the long-run future rely upon fragility being not too extreme either way. With very rigid laws governing history, the future is set regardless of what choices we make, and so directed efforts to change the future are as feckless as (on a Marxist view) a directed effort to prevent the eventual triumph of the proletariat. By contrast, ultra-fragile histories imply forecasting the effects of our actions on the future are effectively impossible, and concerted effort to achieve one end or another futile.</span></p>\n<p>Intermediate views are not implausible: the future seems neither cast in stone nor flotsam borne by the vicissitudes of chance. Yet degrees and variations are productive to investigate: perhaps political history is too chaotic to steer, but technological development less so&#xA0;(or vice versa).<span>&#xA0;I think these topics could reward further investigation.</span></p>\n<p>&#xA0;</p>\n<hr>\n<div>\n<p><a href=\"#ftnt_ref1\">[1]</a><span>&#xA0;I leave other sources of &#x2018;coital perturbation&#x2019; as a less-than-pleasantly-edifying exercise to the reader. </span></p>\n</div>\n<div>\n<p><a href=\"#ftnt_ref2\">[2]</a><span>&#xA0;A more proximal example is that whether a given person&#x2019;s sibling counterpart is male or female is about a coin-flip. Given now and in the past there were incentives to have a child of a particular sex (consider the importance of having a male heir in many cultures), it is plausible (e.g.) a family&#x2019;s male third child may never have been born if one of the first two was male instead of female.</span></p>\n</div>\n<div>&#xA0;</div>\n<div>&#xA0;</div></body></html>", "user": {"username": "Gregory_Lewis"}}, {"_id": "225Aq4P4jFPoWBrb5", "title": "Cause prioritization for downside-focused value systems", "postedAt": "2018-01-31T14:47:11.961Z", "htmlBody": "<p><em>Last updated: July 8th 2021. </em> </p><p>This post outlines my thinking on cause prioritization from the perspective of value systems whose primary concern is reducing disvalue. I\u2019m mainly thinking of <a href=\"https://foundational-research.org/the-case-for-suffering-focused-ethics/\">suffering-focused ethics (SFE)</a>, but I also want to include moral views that attribute substantial disvalue to things other than suffering, such as inequality or preference violation. I will limit the discussion to interventions targeted at improving the long-term future (see the reasons in section II). I hope my post will also be informative for people who do not share a downside-focused outlook, as thinking about cause prioritization from different perspectives, with emphasis on considerations other than those one is used to, can be illuminating. Moreover, understanding the strategic considerations for plausible moral views is essential for acting under moral uncertainty and cooperating with people with other values.&nbsp;<br><br>I will&nbsp;talk about the following topics:&nbsp;</p><ul><li>Which&nbsp;views qualify as downside-focused (given our empirical situation)</li><li>Why downside-focused views prioritize s-risk&nbsp;reduction over utopia creation</li><li>Why&nbsp;extinction risk reduction is unlikely to be a promising intervention according to&nbsp;downside-focused views</li><li>Why AI alignment is probably positive for downside-focused views, and especially&nbsp;positive if done&nbsp;with certain precautions</li><li>What to include in an EA&nbsp;portfolio that incorporates&nbsp;population ethical&nbsp;uncertainty and&nbsp;cooperation between value systems</li></ul><h1>Which views qualify as downside-focused?</h1><p>I\u2019m using the term <em>downside-focused</em> to refer to value systems that in practice (given what we know about the world) primarily recommend working on interventions that make bad things less likely.[1] For example, if one holds that what is most important is how things turn out for individuals (welfarist consequentialism), and that it is comparatively unimportant to add well-off beings to the world, then one should likely focus on preventing suffering.[2] That would be a downside-focused ethical view.&nbsp;</p><p>By contrast, other moral views place great importance on the potential upsides of very good futures, in particular with respect to bringing about a utopia where vast numbers of well-off individuals will exist. Proponents of such views may also believe it to be a top priority that a large, flourishing civilization exists for an extremely long time. I will call these views <em>upside-focused</em>.</p><p>Upside-focused views do not have to imply that bringing about good things is <em>normatively</em> more important than preventing bad things; instead, a view also counts as upside-focused if one has reason to believe that bringing about good things is easier in practice (and thus more overall value can be achieved that way) than preventing bad things.&nbsp;</p><p>A key point of disagreement between the two perspectives is that the upside-focused people might say that suffering and happiness are in a relevant sense symmetrical, and that downside-focused people are too willing to give up good things in the future, such as the coming into existence of many happy beings, just to prevent suffering. On the other side, downside-focused people feel that the other party is too willing to accept, say, that many people suffering extremely goes unaddressed, or is in some sense being accepted in order to achieve some purportedly greater good.&nbsp;</p><p>Whether a normative view qualifies as downside-focused or upside-focused is not always easy to determine, as the answer can depend on difficult empirical questions such as how much disvalue we can expect to be able to reduce versus how much value we can expect to be able to create. I feel confident however that views according to which it is not in itself (particularly) valuable to bring beings in optimal conditions into existence come out as largely downside-focused. The following commitments may lead to a downside-focused prioritization:&nbsp;</p><ul><li>&nbsp; Views based on <a href=\"https://foundational-research.org/tranquilism/\">tranquilist axiology</a></li><li>&nbsp; Views based on \u201c<a href=\"https://foundational-research.org/what-is-the-difference-between-weak-negative-and-non-negative-ethical-views/\">negative-leaning</a>\u201d variants of traditional consequentialist axiology (<a href=\"https://philpapers.org/archive/SINTEA-3.pdf\">hedonist axiology</a>)[3]</li><li>&nbsp; Prioritarianism or welfare-based egalitarianism[4]</li><li> <a href=\"https://en.wikipedia.org/wiki/Antifrustrationism\">Antifrustrationism</a></li><li>&nbsp; Views that incorporate some (asymmetric) <a href=\"https://en.wikipedia.org/wiki/Person-affecting_view\">person-affecting principle</a>[5]</li><li>&nbsp; (Non-welfarist) views that include considerations about suffering prevention or the prevention of rights violations as a prior or as (central) part of an objective list of what constitutes goodness</li></ul><p>For those who are unsure about where their beliefs may fall on the spectrum between downside- and upside-focused views, and how this affects their cause prioritization with regard to the long-term future, I recommend being on the lookout for interventions that are positive and impactful according to both perspectives. Alternatively, one could engage more with population ethics to perhaps cash in on the value of information from narrowing down one\u2019s uncertainty.&nbsp;</p><h1>Most expected disvalue happens in the long-term future</h1><p>In this post, I will only discuss interventions chosen with the intent of affecting the long-term future \u2013 which not everyone agrees is the best strategy for doing good. I want to note that choosing interventions that reliably reduce suffering or promote well-being in the short run also has <a href=\"http://reducing-suffering.org/altruists-focus-reducing-short-term-far-future-suffering/\">many arguments in its favor</a>.&nbsp;<br><br>Having said that, I believe that most of the expected value comes from the effects our actions have on the long-term future, and that our thinking about cause prioritization should explicitly reflect this. The future may come to hold astronomical quantities of the things that people value (<a href=\"https://nickbostrom.com/astronomical/waste.html\">Bostrom, 2003</a>). Correspondingly, for moral views that place a lot of weight on bringing about astronomical quantities of positive value (e.g., happiness or human flourishing), Nick Beckstead presented a strong case for <a href=\"https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxuYmVja3N0ZWFkfGd4OjExNDBjZTcwNjMxMzRmZGE\">focusing on the long-term future</a>. For downside-focused views, that case rests on similar premises. A simplified version of that argument is based on the two following ideas:&nbsp;&nbsp;</p><ol type=\"1\"><li>Some futures contain astronomically more disvalue than others, such as uncontrolled space colonization versus a future where compassionate and wise actors are in control.</li><li>It is sufficiently likely that our current actions can help shape the future so that we avoid worse outcomes and end up with better ones. For example, such an action could be to try to figure out which interventions best improve the long-term future.</li></ol><p>This does not mean that one should necessarily pick interventions one thinks will affect the long-term future through some specific, narrow pathway. Rather, I am saying (following Beckstead) that we should pick our actions based primarily on what we estimate their net effects to be on the long-term future.[6] This includes not only narrowly targeted interventions such as technical work in AI alignment, but also projects that improve the values and decision-making capacities in society at large to help future generations cope better with expected challenges.&nbsp;</p><h1>Downside-focused views prioritize s-risk reduction over utopia creation</h1><p>The observable universe has very little suffering (or inequality, preference frustration, etc.) compared to what could be the case; for all we know suffering at the moment may only exist on one small planet in a computationally inefficient form of organic life.[7] According to downside-focused views, this is fortunate, but it also means that things can become much worse. <a href=\"https://foundational-research.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/\">Suffering risks</a>&nbsp;(or \u201cs-risks\u201d) are risks of events that bring about suffering in cosmically significant amounts. By \u201csignificant,\u201d I mean significant relative to expected future suffering. (Note that it may turn out that the amount of suffering that we can influence is dwarfed by suffering that we can\u2019t influence. By \u201cexpectation of suffering in the future\u201d we mean \u201cexpectation of action-relevant suffering in the future.\u201d) Analogously and more generally, we can define <em>downside risks</em> as events that would bring about disvalue (including things other than suffering) at cosmically significant scales.</p><p>Why might this definition be practically relevant? Imagine the hypothetical future scenario \u201cBusiness as usual (BAU),\u201d where things continue onwards indefinitely exactly as they are today, with all bad things being confined to earth only. Hypothetically, let\u2019s say that we expect 10% of futures to be BAU, and we imagine there to be an intervention \u2013 let\u2019s call it <em>paradise creation</em> \u2013 that changed all BAU futures into futures where a suffering-free paradise is created. Let us further assume that another 10% of futures will be futures where earth-originating intelligence colonizes space and things go very wrong such that, through some pathway or another, creates suffering on a cosmically significant scale, and little to no happiness or good things. We will call this second scenario \u201cAstronomical Suffering (AS).\u201d</p><p>If we limit our attention to only the two scenarios AS and BAU (of course there are many other conceivable scenarios, including scenarios where humans go extinct or where space colonization results in a future filled with predominantly happiness and flourishing), then we see that the total suffering in the AS futures vastly exceeds all the suffering in the BAU futures. Successful paradise creation therefore would have a much smaller impact in terms of reducing suffering than an alternative intervention that averts the 10% s-risk from the AS scenario, changing it to a BAU scenario for instance. Even reducing the s-risk from AS in our example by a single percentage point would be vastly more impactful than preventing the suffering from all the BAU futures.</p><p>This consideration highlights why suffering-focused altruists should probably invest their resources not into making exceptionally good outcomes more likely, but rather into making dystopian outcomes (or dystopian elements in otherwise good outcomes) less likely. Utopian outcomes where all sources of significant suffering are abolished through technology and virtually all sentient beings get to enjoy lives filled with unprecedented heights of happiness are certainly something we should <em>hope</em> will happen. But from a downside-focused perspective, our own efforts to do good are, <em>on the margin</em>, better directed towards making it less likely that we get particularly bad futures.</p><p>While the AS scenario above was stipulated to contain little to no happiness, it is important to note that s-risks can also affect futures that contain more happy individuals than suffering ones. For instance, the suffering in a future with an astronomically large population count where 99% of individuals are very well off and 1% of individuals suffer greatly constitutes an s-risk even though upside-focused views may evaluate this future as very good and worth bringing about. Especially when it comes to the prevention of s-risks affecting futures that otherwise contain a lot of happiness, it matters a great deal <em>how</em> the risk in question is being prevented. For instance, if we envision a future that is utopian in many respects except for a subportion of the population suffering because of problem X, it is in the interest of virtually all value systems to solve problem X in highly targeted ways that move probability mass towards even better futures. By contrast, only few value systems (ones that are strongly or exclusively about reducing suffering/bad things) would consider it overall good if problem X was \u201csolved\u201d in a way that not only prevented the suffering, but also prevented all the happiness from the future scenario this suffering was embedded in.&nbsp;As I will argue in the last section, <a href=\"https://concepts.effectivealtruism.org/concepts/moral-uncertainty/\">moral uncertainty</a> and <a href=\"https://foundational-research.org/gains-from-trade-through-compromise/\">moral cooperation</a> are strong reasons to solve such problems in ways that most value systems approve of.</p><p>All of the above is based on the assumption that bad futures, i.e., futures with s-risks or downside risks, are reasonably likely to happen (and can tractably be addressed). This seems to be the case, unfortunately: We find ourselves on a civilizational trajectory with rapidly growing technological capabilities, and the ceilings from physical limits still far away. It looks as though large-scale space colonization might become possible someday, either for humans directly, for some successor species, or for intelligent machines that we might create. Life generally tends to spread and use up resources, and intelligent life or intelligence generally does so even more deliberately. As space colonization would so vastly increase the stakes at which we are playing, a failure to improve sufficiently alongside all the necessary dimensions \u2013 both morally and with regard to overcoming coordination problems or lack of foresight \u2013 could result in futures that, even though they may in many cases (different from the AS scenario above!) also contain astronomically many happy individuals, could still contain cosmically significant quantities of suffering. We can envision numerous conceptual pathways that lead to such suffering (<a href=\"http://www.informatica.si/index.php/informatica/article/view/1877\">Sotala &amp; Gloor, 2017</a>), and while each single pathway may seem unlikely to be instantiated \u2013 as with most specific predictions about the long-term future \u2013the risks are <em>disjunctive</em>, and people tend to underestimate the probability of disjunctive events (Dawes &amp; Hastie, 2001). In particular, our historical track record contains all kinds of factors that directly cause or contribute to suffering on large scales:&nbsp;</p><ul><li><strong>Darwinian competition</strong> (exemplified by the suffering of wild animals)</li><li><strong>Coordination problems and economic competition</strong> (exemplified by suffering caused or exacerbated by income inequality, both locally and globally, and the <a href=\"http://slatestarcodex.com/2014/07/30/meditations-on-moloch/\">difficulty</a>&nbsp;of reaching a long-term stable state where Malthusian competition is avoided)</li><li><strong>Hatred of outgroups</strong> (exemplified by the Holocaust)</li><li><strong>Indifference</strong> (exemplified by the suffering of animals in factory farms)</li><li><strong>Conflict</strong> (exemplified by the suffering of a population during/after conquest or siege; sometimes coupled with the promise that surrender will spare the torture of civilians)</li><li><strong>Sadism</strong> (exemplified by cases of animal abuse, which may not make up a large source of current-day suffering but could become a bigger issue with cruelty-enabling technologies of the future)&nbsp;</li></ul><p>And while one can make a case that there has been a trend for things to become better (see&nbsp;<a href=\"https://en.wikipedia.org/wiki/The_Better_Angels_of_Our_Nature\">Pinker, 2011</a>), this does not hold in all domains (e.g. not with regard to the number of animals directly harmed in the food industry) and we may, because of filter bubbles for instance, underestimate <a href=\"http://slatestarcodex.com/2015/12/24/how-bad-are-things/\">how bad things still are</a>&nbsp;even in comparatively well-off countries such as the US. Furthermore, it is easy to overestimate the trend for things to have gotten better given that the underlying mechanisms responsible for catastrophic events such as conflict or natural disasters may follow a <a href=\"https://en.wikipedia.org/wiki/Power_law\">power law</a>&nbsp;distribution where the vast majority of violent deaths, diseases or famines result from a comparatively small number of particularly devastating incidents. Power law distributions constitute a plausible (though tentative) candidate for modelling the likelihood and severity of suffering risks. If this model is correct, then observations such as that the world did not erupt in the violence of a third world war, or that no dystopian world government has been formed as of late, cannot count as very reassuring, because power law distributions become hard to assess precisely towards the tail-end of the spectrum where the stakes become altogether highest (<a href=\"https://arxiv.org/abs/cond-mat/0412004\">Newman, 2006</a>).&nbsp;</p><p>In order to now illustrate the difference between downside- and upside-focused views, I drew two graphs. To keep things simple, I will limit the example scenarios to cases that either uncontroversially contain more suffering than happiness, or <em>only</em>&nbsp;contain happiness. The BAU scenario from above will serve as reference point. I\u2019m describing it again as reminder below, alongside the other scenarios I will use in the illustration (note that all scenarios are <em>stipulated to last for equally long</em>):</p><h3><strong>Business as usual (BAU)</strong></h3><p>Earth remains the only planet in the observable universe (as far as we know) where there is suffering, and things continue as they are. Some people remain in extreme poverty, many people suffer from disease or mental illness, and our psychological makeup limits the amount of time we can remain content with things even if our lives are comparatively fortunate. Factory farms stay open, and most wild animals die before they reach their reproductive age.&nbsp;</p><h3><strong>Astronomical suffering (AS)</strong></h3><p>A scenario where space colonization results in an outcome where astronomically many sentient minds exist in conditions that are evaluated as bad by all plausible means of evaluation. To make the scenario more concrete, let us stipulate that 90% of beings in this vast population have lives filled with medium-intensity suffering, and 10% of the population suffers in strong or unbearable intensity. There is little or no happiness in this scenario.</p><h3><strong>Paradise (small or astronomical; SP/AP)</strong></h3><p>Things go as well as possible, suffering is abolished, all sentient beings are always happy and even experience <a href=\"https://nickbostrom.com/utopia.html\">heights of well-being</a>&nbsp;that are unachievable with present-day technology. We further distinguish <strong><em>small paradise (SP)</em></strong> from <strong><em>astronomical paradise (AP)</em></strong>: while the former stays earth-bound, the latter spans across (maximally) many galaxies, optimized to turn available resources into flourishing lives and all the things people value.&nbsp;</p><p>Here is how I envision typical suffering-focused and upside-focused views ranking the above scenarios from \u201ccomparatively bad\u201d on the left to \u201ccomparatively good\u201d on the right:<br><br></p><br><span><figure><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994642/mirroredImages/225Aq4P4jFPoWBrb5/rk5h9tjdqt9ig1kcri9r.png\" class=\"draft-image \" style=\"width:40%\"></figure></span><br><p>The two graphs represent the relative value we can expect from the classes of future scenarios I described above. The leftmost point of a graph represents not the worst possible outcome, but the worst outcome amongst the future scenarios we are considering. The important thing is not whether a given scenario is more towards the right or left side of the graph, but rather <em>how large the distance is between scenarios</em>. The yellow stretch signifies the highest importance or scope, and interventions that move probability mass across that stretch are either exceptionally good or exceptionally bad, depending on the direction of the movement. (Of course, in practice interventions can also have complex effects that affect multiple variables at once.)&nbsp;</p><p>Note that the BAU scenario was chosen mostly for illustration, as it seems pretty unlikely that humans would continue to exist in the current state for extremely long timespans. Similarly, I should qualify that the SP scenario may be unlikely to ever happen in practice because it seems rather unstable: Keeping value drift and <a href=\"http://reducing-suffering.org/the-future-of-darwinism/\">Darwinian dynamics</a>&nbsp;under control and preserving a small utopia for millions of years or beyond may require <a href=\"https://nickbostrom.com/fut/singleton.html\">technology</a>&nbsp;that is so advanced that one may as well make the utopia astronomically large \u2013 unless there are overriding reasons for favoring the smaller utopia. From any strongly or exclusively downside-focused perspective, the smaller utopia may indeed \u2013 factoring out concerns about cooperation \u2013 be preferable, because going from SP to AP comes with some risks.[8] However, for the purposes of the first graph above, I was stipulating that AP is completely flawless and risk-free.&nbsp;</p><p>A \u201cnear paradise\u201d or \u201cflawed paradise\u201d mostly filled with happy lives but also with, say, 1% of lives in constant misery, would for upside-focused views rank somewhere close to AP on the far right end of the first graph. By contrast, for downside-focused views on the second graph, \u201cflawed paradise\u201d would stand more or less in the same position as BAU in case the view in question is weakly downside-focused, and decidedly more on the way towards AS on the left in case the view in question is strongly or exclusively downside-focused. Weakly downside-focused views would also have a relatively large gap between SP and AP, reflecting that creating additional happy beings is regarded as morally quite important but not sufficiently important to become the top priority. A view would still count as suffering-focused (at least within the restricted context of our visualization where all scenarios are artificially treated as having the same probability of occurrence) as long as the gap between BAU and AS would remain larger than the gap between BAU/SP and AP.</p><p>In practice, we are well-advised to hold very large uncertainty over what the right way is to conceptualize the likelihood and plausibility of such future scenarios. Given this uncertainty, there can be cases where a normative view falls somewhere in-between upside- and downside-focused in our subjective classification. All these things are very hard to predict and other people may be substantially more or substantially less optimistic with regard to the quality of the future. My own estimate is that a more realistic version of AP, one that is allowed to contain some suffering but is characterized by containing near-maximal quantities of happiness or things of positive value, is ~40 times less likely to happen[9] than the vast range of scenarios (of which AS is just one particularly bad example) where space colonization leads to outcomes with a lot less happiness. I think scenarios as bad as AS or worse are also&nbsp;very rare, as most scenarios that involve a lot of suffering may also contain some islands of happiness (or even have a sea of happiness and some islands of suffering). See also <a href=\"https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/\">these</a> <a href=\"/ea/1cl/an_argument_for_why_the_future_may_be_good/\">posts</a>&nbsp;on why the future is likely to be net good in expectation according to views where creating happiness is similarly important as reducing suffering.&nbsp;</p><p>Interestingly, various upside-focused views may differ normatively with respect to how <a href=\"http://lesswrong.com/lw/y3/value_is_fragile/\">fragile</a>&nbsp;(or not) their concept of positive value is. If utopia is very fragile, but dystopia comes in vastly many forms (related: the <a href=\"https://en.wikipedia.org/wiki/Anna_Karenina_principle\">Anna Karenina principle</a>), this would imply greater pessimism regarding the value of the average scenario with space colonization, which could push such views closer to becoming downside-focused. On the other hand, some (idiosyncratic) upside-focused views may simply place an overriding weight on the <a href=\"https://en.wikipedia.org/wiki/Directed_panspermia#Motivation_and_ethics\">ongoing existence</a>&nbsp;of conscious life, largely independent of how things will go in terms of hedonist welfare.[10] Similarly, <em>normatively</em> upside-focused views that count creating happiness as more important than reducing suffering (though presumably very few people would hold such views) would always come out as upside-focused in practice, too, even if we had reason to be highly pessimistic about the future.&nbsp;&nbsp;</p><p>To summarize, what the graphs above try to convey is that for the example scenarios listed, downside-focused views are characterized by having the <em>largest gap</em> in relative importance between AS and the other scenarios. By contrast, upside-focused views place by far the most weight on making sure AP happens, and SP would (for many upside-focused views at least) not even count as all that good, comparatively.[11]</p><h1>Extinction risk reduction: Unlikely to be positive according to downside-focused views</h1><p>Some futures, such as ones where most people\u2019s quality of life is hellish, are worse than extinction. Many people with upside-focused views would agree. So the difference between&nbsp; upside- and downside-focused views is not about whether there can be net negative futures, but about how readily a future scenario is ranked as worth bringing about in the face of the suffering it contains or the downside risks that lie on the way from here to there.&nbsp;</p><p>If humans went extinct, this would greatly reduce the probability of space colonization and any associated risks (as well as benefits). Without space colonization, there are no s-risks \u201cby action,\u201d no risks from the <em>creation</em> of cosmically significant suffering where human activity makes things worse than they would otherwise be.[12] Perhaps there would remain some s-risks \u201cby omission,\u201d i.e. risks corresponding to a failure to <em>prevent</em> astronomical disvalue. But such risks appear unlikely given the apparent <a href=\"http://www.jodrellbank.manchester.ac.uk/media/eps/jodrell-bank-centre-for-astrophysics/news-and-events/2017/uksrn-slides/Anders-Sandberg---Dissolving-Fermi-Paradox-UKSRN.pdf\">emptiness</a>&nbsp;of the observable universe.[13] Because s-risks by action overall appear to be more plausible than s-risks by omission, and because the latter can only be tackled in an (arguably unlikely) scenario where humanity accomplishes the feat of installing compassionate values to robustly control the future, it appears as though downside-focused altruists have more to lose from space colonization than they have to gain.&nbsp;</p><p>It is however not obvious whether this implies that efforts to reduce the probability of human extinction indirectly increase suffering risks or downside risks more generally. It very much depends on the way this is done and what other effects are. For instance, there is a large and often underappreciated difference between existential risks from bio- or nuclear technology, and existential risks related to smarter-than-human artificial intelligence (superintelligence; see the next section). While the former set back technological progress, possibly permanently so, the latter drives it all the way up, likely \u2013 though maybe not always \u2013 culminating in space colonization with the purpose of benefiting whatever goal(s) the superintelligent AI systems are equipped with (<a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.393.8356&amp;rep=rep1&amp;type=pdf\">Omohundro 2008</a>; <a href=\"http://www.sciencedirect.com/science/article/pii/S0094576513001148\">Armstrong &amp; Sandberg, 2013</a>). Because there's a non-negligible chance that space colonization would come with systemically embedded sources of suffering in this way, this means that a failure to reduce existential risks from AI is often also a failure to prevent s-risks from AI misalignment. Therefore, the next section will argue that reducing such AI-related risks is valuable from both upside- and downside-focused perspectives. By contrast, the situation is much less obvious for <em>other</em> existential risks, ones that are not about artificial superintelligence.&nbsp;</p><p>Sometimes efforts to reduce these other existential risks also benefits s-risk reduction. For instance, efforts to reduce non-AI-related extinction risks may <a href=\"https://foundational-research.org/how-would-catastrophic-risks-affect-prospects-for-compromise/\">increase global stability</a>&nbsp;and make particularly bad futures less likely in those circumstances where humanity nevertheless goes on to colonize space. Efforts to reduce extinction risks from e.g. biotechnology&nbsp;or nuclear war&nbsp;in practice also reduce the risk of global catastrophes where a small number of humans survive and where civilization is likely to eventually recover technologically, but perhaps at the cost of a <a href=\"https://blog.givewell.org/2015/08/13/the-long-term-significance-of-reducing-global-catastrophic-risks/\">worse geopolitical situation or with worse values</a>, which could then lead to increases in s-risks going into the long-term future. This mitigating effect on s-risk reduction through a more stable future is substantial and positive according to downside-focused value systems, which has to be weighed against the effects of making&nbsp;s-risks from space colonization more likely.&nbsp;</p><p>Interestingly, if we care about the total number of sentient minds (and their quality of life) that can at some point be created, then because of some known facts about cosmology,[14] any effects that near-extinction catastrophes have on <em>delaying</em> space colonization are largely negligible in the long run when compared to affecting the quality of a future with space colonization \u2013 at least unless the delay becomes very long indeed (e.g. millions of years or longer).&nbsp;</p><p>What this means is that in order to determine how reducing&nbsp;the probability of extinction from things other than superintelligent AI&nbsp;in expectation affects downside risks, we can approximate the answer by weighing the following two considerations against each other:</p><ol type=\"1\"><li>How likely is it that the averted catastrophes merely delay space colonization rather than preventing it completely?</li><li>How much better or worse would a second version of a technologically mature civilization (after a global catastrophe&nbsp;thwarted the the first attempt) fare with respect to downside risks?[15]</li></ol><p>The second question involves judging where our current trajectory falls, quality-wise, when compared to the distribution of post-rebuilding scenarios \u2013 how much better or worse is our trajectory than a random resetted one? It also requires estimating the effects of post-catastrophe conditions on AI development \u2013 e.g., would a longer time until technological maturity (perhaps due to a lack of fossil fuels) cause a more uniform distribution of power, and what does that imply about the probability of arms races? It seems difficult to account for all of these considerations properly. It strikes me as more likely than not that things would be worse after recovery, but because there are so many things to consider,[16] I do not feel very confident about this assessment.&nbsp;</p><p>This leaves us with the question of how likely a global catastrophe is to merely delay space colonization rather than preventing it. I have not thought about this in much detail, but after having talked to some people (especially at FHI) who have investigated it, I updated that rebuilding after a catastrophe seems quite likely. And while a civilizational collapse would set a precedent and reason to worry the second time around when civilization reaches technological maturity again, it would take an unlikely constellation of collapse factors to get stuck in a loop of recurrent collapse, rather than at some point escaping the setbacks and reaching a stable plateau (<a href=\"https://nickbostrom.com/papers/future.html\">Bostrom, 2009</a>), e.g. through space colonization. I would therefore say that large-scale catastrophes related to biorisk or nuclear war are quite likely (~80\u201393%) to merely delay space colonization in expectation.[17] (With more uncertainty being not on the likelihood of recovery, but on whether some outlier-type catastrophes might directly lead to extinction.)</p><p>This would still mean that the successful prevention of all biorisk and risks from nuclear war makes space colonization 10-20% more likely. Comparing this estimate to the previous, uncertain estimate about the s-risks profile of a civilization after recovery, it tentatively seems to me that the effect of making cosmic stakes (and therefore downside risks) more likely is not sufficiently balanced by positive effects[18] on stability, arms race prevention and civilizational values (factors which would make downside risks less likely). However, this is hard to assess and may change depending on novel insights.&nbsp;</p><p>What looks slightly clearer to me is that making rebuilding after a civilizational collapse more likely comes with increased downside risks. If this was the sole effect of an intervention, I would&nbsp;estimate&nbsp;it&nbsp;as overall negative for downside-focused views (factoring out considerations of moral uncertainty or cooperation with other value systems) \u2013 because not only would it make it more likely that space will eventually be colonized, but it would also do so in a situation where s-risks might be higher than in the current trajectory we are on.[19]</p><p>However, in practice it seems as though any intervention that makes recovery after a collapse more likely would also have many other effects, some of which might more plausibly be positive according to downside-focused ethics. For instance, an intervention&nbsp;such as developing <a href=\"/ea/1g9/should_we_be_spending_no_less_on_alternate_foods/\">alternate foods</a>&nbsp;might merely speed up rebuilding after civilizational collapse rather than making it altogether more likely, and so would merely affect whether rebuilding happens from a low base or a high base. One could argue that rebuilding from a higher base is less risky also from a downside-focused perspective, which makes things more complicated to assess. In any case, what seems clear is that none of these interventions look&nbsp;<em>promising</em> for the prevention of downside risks.&nbsp;</p><p>We have seen that efforts to reduce extinction risks (exception: AI alignment) are unpromising interventions for downside-focused value systems, and some of the interventions available in that space (especially if they do not simultaneously also improve the quality of the future) may even be negative when evaluated purely from&nbsp;this perspective. This is a counterintuitive conclusion, maybe so much so that many people would rather choose to adopt moral positions where it does not follow. In this context, it is important to point out that valuing humanity not going extinct is definitely compatible with a high degree of priority for reducing suffering or disvalue. I view morality as including both considerations about duties towards other people (inspired by social contract theories or game theoretic reciprocity) as well as considerations of (unconditional) care or altruism. If both types of moral considerations are to be weighted similarly, then while the \u201ccare\u201d dimension could e.g. be downside-focused, the other dimension, which is concerned with respecting and cooperating with other people\u2019s life goals, would not be \u2013 at least not under the assumption that the future will be good enough that people want it to go on \u2013 and would certainly not welcome extinction.</p><p>Another way to bring together both downside-focused concerns and a concern for humanity not going extinct would be through a morality that evaluates states of affairs holistically, as opposed to using an additive combination&nbsp;for individual welfare and a global evaluation of extinction versus no extinction. Under such a model, one would have a bounded value function for the state of the world as a whole,&nbsp;so that a&nbsp;long history with great heights of discovery or continuity could improve the evaluation of the whole history, as would properties like&nbsp;highly favorable densities of good things&nbsp;versus bad things.&nbsp;</p><p>Altogether, because more people seem to come to hold upside-focused or at least strongly <em>extinction-averse</em> values after grappling with the arguments in population ethics, reducing extinction risk can be part of a fair compromise even though it is an unpromising and possibly negative intervention from a downside-focused perspective. After all, the reduction of extinction risks is <a href=\"https://80000hours.org/articles/extinction-risk/\">particularly important</a>&nbsp;from both an upside-focused perspective and from the perspective of (many) people\u2019s self-, family- or community-oriented moral intuitions \u2013 because of the short-term death risks it involves.[20] Because it is difficult to identify interventions that are robustly positive and highly impactful according to downside-focused value systems (as the length of this post and the uncertain conclusions indicate), it is however not a trivial issue that many commonly recommended interventions are unlikely to be positive according to these value systems. To the extent that downside-focused value systems are regarded as a plausible and frequently arrived at class of views, considerations from moral uncertainty and moral cooperation (see the last section) recommend some degree of offsetting expected harms through targeted efforts to reduce s-risks, e.g. in the space of AI risk (next section). Analogously, downside-focused altruists should not increase extinction risks and instead focus on more cooperative ways to reduce future disvalue.&nbsp;</p><h1>AI alignment: (Probably) positive for downside-focused views; high variance</h1><p>Smarter-than-human artificial intelligence <a href=\"https://foundational-research.org/altruists-should-prioritize-artificial-intelligence/\">will likely be</a>&nbsp;particularly important for how the long-term future plays out. There is a good chance that the goals of superintelligent AI would be much more <a href=\"https://nickbostrom.com/fut/singleton.html\">stable</a>&nbsp;than the values of individual humans or those enshrined in any constitution or charter, and superintelligent AIs would \u2013 at least with considerable likelihood \u2013 remain in control of the future not only for centuries, but for millions or even billions of years to come. In this section, I will sketch some crucial considerations for how work in AI alignment is to be evaluated from a downside-focused perspective.&nbsp;</p><p>First, let\u2019s consider a scenario with unaligned superintelligent AI systems, where the future is shaped according to goals that have <a href=\"https://www.jefftk.com/p/examples-of-superintelligence-risk#fb-886930452142_886983450932\">nothing to do</a>&nbsp;with what humans value. Because resource accumulation is instrumentally useful to most consequentialist goals, it is likely to be pursued by a superintelligent AI no matter its precise goals. Taken to its conclusion, the acquisition of ever more resources culminates in space colonization where accessible raw material is used to power and construct supercomputers and other structures that could help in the pursuit of a consequentialist goal. Even though random or \u201caccidental\u201d goals are unlikely to intrinsically value the creation of sentient minds, they may lead to the instantiation of sentient minds for <em>instrumental reasons</em>. In the absence of explicit concern for suffering reflected in the goals of a superintelligent AI system, that system would instantiate suffering minds for even the slightest benefit to its objectives. Suffering may be related to powerful ways of learning (<a href=\"https://arxiv.org/abs/1505.04497\">Daswani &amp; Leike, 2015</a>), and an AI indifferent to suffering might build vast quantities of <a href=\"http://reducing-suffering.org/what-are-suffering-subroutines/\">sentient subroutines</a>, such as robot overseers, robot scientists or subagents inside larger AI control structures. Another danger is that, either during the struggle over control over the future in a multipolar AI takeoff scenario, or perhaps in the distant future should superintelligent AIs ever encounter other civilizations, conflict or extortion could result in tremendous amounts of disvalue. Finally, superintelligent AI systems might create vastly many sentient minds, including very many suffering ones, by running simulations of evolutionary history for research purposes (\u201cmindcrime;\u201d Bostrom, 2014, pp. 125-26). (Or for other purposes; if humans had the power to run alternative histories in large and fine-grained simulations, probably we could think of all kinds of reasons for doing it.) Whether such history simulations would be fine-grained enough to contain sentient minds, or whether simulations on a digital medium can even qualify as sentient, are difficult and controversial questions. It should be noted however that the stakes are high enough such that even comparatively small credences such as 5% or lower would already go a long way in terms of the implied expected value for the overall severity of s-risks from artificial sentience (see also footnote 7).&nbsp;</p><p>While the earliest discussions about the risks from artificial superintelligence have focused primarily on scenarios where a single goal and control structure decides the future (<a href=\"https://nickbostrom.com/fut/singleton.html\">singleton</a>), we should also remain open for scenarios that do not fit this conceptualization completely. Perhaps what happens instead could be several goals either competing or acting in concert with each other, like an <a href=\"https://foundational-research.org/artificial-intelligence-and-its-implications-for-future-suffering#AI_More_like_the_economy_than_like_robots\">alien economy</a>&nbsp;that drifted further and further away from originally having served the goals of its human creators.[21] Alternatively, perhaps goal preservation becomes more difficult the more capable AI systems become, in which case the future might be controlled by unstable motivations (agent-internally) or coalitions (of agents) taking turns over the steering wheel. The scenarios where no proper singleton emerges may perhaps be especially likely to contain large numbers of sentient subroutines because navigating a landscape with other highly intelligent agents requires the ability to continuously model other actors and to react to changing circumstances under time pressure \u2013 all of which are things that are plausibly relevant for the development of sentience.&nbsp;</p><p>In any case, we cannot expect with confidence that a future controlled by non-compassionate goals will be a future that neither contains happiness nor suffering. In expectation, such futures are instead likely to contain vast amounts of both happiness and suffering, simply because these futures would contain astronomical amounts of goal-directed activity in general.</p><p>Successful AI alignment could prevent most of the suffering that would happen in an AI-controlled future, as a superintelligence with compassionate goals would be willing to make tradeoffs that substantially reduce the amount of suffering contained in any of its instrumentally useful computations. While a \u201ccompassionate\u201d AI (compassionate in the sense that its goal includes concern for suffering, though not necessarily in the sense of experiencing emotions we associate with compassion) might still pursue history simulations or make use of potentially sentient subroutines, it would be much more conservative when it comes to risks of creating suffering on large scales. This means that it would e.g. contemplate using fewer or slightly <a href=\"http://slatestarcodex.com/2015/03/15/answer-to-job/\">less fine-grained</a>&nbsp;simulations, slightly less efficient robot architectures (and ones that are particularly happy most of the time), and so on. This line of reasoning suggests that AI alignment might be highly positive according to downside-focused value systems because it averts s-risks related to instrumentally useful computations.&nbsp;</p><p>However, work in AI alignment not only makes it more likely that fully aligned AI is created and everything goes perfectly well, but it also affects the distribution of alignment <em>failure modes</em>. In particular, progress in AI alignment could make it more likely that failure modes shift from \u201cvery far away from perfect in conceptual space\u201d to \u201cclose but slightly off the target.\u201d There are some reasons why such <em>near misses</em> might sometimes end <a href=\"http://lesswrong.com/lw/p5v/srisks_why_they_are_the_worst_existential_risks/duaa\">particularly</a> <a href=\"https://arbital.com/p/hyperexistential_separation/\">badly</a>.&nbsp;</p><p>What could loosely be classified as a near miss is that certain work in AI alignment makes it more likely that AIs would share whichever values their creators want to install, but the creators could be unethical or (<a href=\"https://agentfoundations.org/item?id=1150\">meta-</a>)philosophically and strategically incompetent.&nbsp;</p><p>For instance, if those in power of the future came to follow some kind of ideology that is uncompassionate or even hateful of certain out-groups, or favor a distorted version of libertarianism where every person, including a few sadists, would be granted an astronomical quantity of future resources to use it at their disposal, the resulting future could be a bad one according to downside-focused ethics.&nbsp;</p><p>A related and perhaps more plausible danger is that we might prematurely lock in a definition of suffering and happiness into an AI\u2019s goals that neglects sources of suffering we would come to care about after deeper reflection, such as not caring about the mind states of insect-like digital minds (which may or may not be reasonable). A superintelligence with a random goal would also be indifferent with regard to these sources of suffering, but because humans value the creation of sentience, or at least value processes related to agency (which tend to <em>correlate</em> with sentience), the likelihood is greater that a superintelligence with aligned values would create unnoticed or uncared for sources of suffering. Possible such sources include the suffering of non-human animals in nature simulations performed for aesthetic reasons, or characters in sophisticated virtual reality games.&nbsp;(The sources of suffering only form proper \"s-risk\" if they make up a significant enough fraction of our expectation that it becomes worth focusing on them.) </p><p>A further danger is that, if our strategic or technical understanding is too poor, we might fail to specify a recipe for getting human values right and end up with <em>perverse instantiation</em>&nbsp;(Bostrom, 2014) or a failure mode where the reward function ends up flawed. This could happen e.g. to cases where an AI system starts to act in unpredictable but optimized ways due to conducting searches far outside its training distribution.[22] Probably most mistakes at that stage would result in about as much suffering as in the typical scenario where AI is unaligned and has (for all practical purposes) <em>random</em> goals. However, one possibility is that alignment failures surrounding utopia-directed goals have a higher chance of leading to dystopia than alignment failures around random goals. For instance, a failure to fully understand the goal 'make maximally many happy minds' could lead to a dystopia where maximally many minds are created in conditions that do not reliably produce happiness, and may even lead to suffering in some of the instances, or some of the time. This is an area for future research.&nbsp;</p><p>A final possible outcome in the theme of \u201c<em>almost</em>&nbsp;getting everything right\u201d is if one where we are able to successfully install human values into an AI, only to have the resulting AI compete with other, unaligned AIs for control of the future and be threatened with things that are bad according to human values, in the expectation that the human-aligned AI would then forfeit its resources and give up in the competition over controlling the future.&nbsp;</p><p>Trying to summarize the above considerations, I drew a (sketchy) map with some major categories of s-risks related to space colonization. It highlights that artificial intelligence can be regarded as a cause <em>or</em>&nbsp;cure for s-risks (Sotala &amp; Gloor, 2017). That is, if superintelligent AI is successfully aligned, s-risks stemming from indifference to suffering are prevented and a maximally valuable future is instantiated (green). However, the danger of near misses (red) makes it non-obvious whether efforts in AI alignment reduce downside risks overall, as the worst near misses may e.g. contain more suffering than the average s-risk scenario.&nbsp;</p><br><span><figure><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994642/mirroredImages/225Aq4P4jFPoWBrb5/bmdyjhwtbf4n3xpt0kxb.png\" class=\"draft-image \" style=\"width:40%\"></figure></span><br><p>Note that no one should quote the above map out of context and call it \u201cThe likely future\u201d or something like that, because some of the scenarios I listed may be highly improbable and because the whole map is drawn with a focus on things that could go wrong. If we wanted a map that also tracked outcomes with astronomical amounts of happiness, there would in addition be many nodes for things like \u201chappy subroutines,\u201d \u201cmindcrime-opposite,\u201d \u201csuperhappiness-enabling technologies,\u201d or \u201cunaligned AI trades with aligned AI and does good things after all.\u201d There can be futures in which several s-risk scenarios come to pass at the same time, as well as futures that contain s-risk scenarios but also a lot of happiness (this seems pretty likely).&nbsp;</p><p>To elaborate more on the categories in the map above: Pre-AGI civilization (blue) is the stage we are at now. Grey boxes refer to various steps or conditions that could be met, from which s-risks (orange and red), extinction (yellow) or utopia (green) may follow. The map is crude and not exhaustive. For instance \u201cNo AI Singleton\u201d is a somewhat unnatural category into which I threw both scenarios where AI systems play a crucial role and scenarios where they do not. That is, the category contains futures where space colonization is orchestrated by humans or some biological successor species without AI systems that are smarter than humans, futures where AI systems are used as tools or oracles for assistance, and futures where humans are out of the loop but no proper singleton emerges in the competition between different AI systems.&nbsp;</p><p>Red boxes are s-risks that may be intertwined with efforts in AI alignment (though not by logical necessity): If one is careless, work in AI alignment may exacerbate these s-risks rather than alleviate them. While dystopia from extortion would never be the result of the activities of an aligned AI, it takes an AI with aligned values, e.g. alongside the unaligned AI in a multipolar scenario or alien AI encountered during space colonization, to even provoke such a threat (hence the dotted line linking this outcome to \u201cvalue loading success\u201d). I coined the term \u201caligned-ish AI\u201d to refer to the class of outcomes that efforts in AI alignment shifts probability mass to. This class includes both very good outcomes (intentional) and neutral or very bad outcomes (accidental). Flawed realization \u2013 which stands for futures where flaws in alignment prevent most of the value or even create disvalue \u2013 is split into two subcategories in order to highlight that the vast majority of such outcomes likely contains no more suffering than the typical outcome with unaligned AI, but that things going wrong in a particularly unfortunate way could result in exceptionally bad futures. For views that care similarly strongly about achieving utopia than preventing very bad futures, this tradeoff seems most likely net positive, whereas from a downside-focused perspective, this consideration makes it less clear whether efforts in AI alignment are overall worth the risks.</p><p>Fortunately, not all work in AI alignment faces the same tradeoffs. Many approaches may be directed specifically against avoiding certain failure modes, which is extremely positive and impactful for downside-focused perspectives. <em>Worst-case AI safety</em>&nbsp;is the idea that downside-focused value systems recommend pushing differentially the approaches that appear <a href=\"https://foundational-research.org/suffering-focused-ai-safety/\">safest</a>&nbsp;with respect to particularly bad failure modes. Given that many approaches towards AI alignment are still at a very early stage, it may be hard to tell which components to AI alignment are likely to benefit downside-focused perspectives the most. Nevertheless, I think we can already make some informed guesses, and our understanding will improve with time.&nbsp;</p><p>For instance, approaches that make AI systems corrigible (see <a href=\"https://intelligence.org/files/Corrigibility.pdf\">here</a>&nbsp;and <a href=\"https://ai-alignment.com/corrigibility-3039e668638\">here</a>) would extend the window of time during which we can spot flaws and prevent outcomes with flawed realization. Similarly, <a href=\"https://ai-alignment.com/model-free-decisions-6e6609f5d99e\">approval-directed</a>&nbsp;approaches to AI alignment, where alignment is achieved by simulating what a human overseer would decide if they were to think about the situation for a very long time, would go further towards avoiding bad decisions than approaches with immediate, unamplified feedback from human overseers. And rather than trying to solve AI alignment in one swoop, a promising and particularly \u201cs-risk-proof\u201d strategy might be to first build a <a href=\"https://arxiv.org/pdf/1705.10720.pdf\">low-impact</a>&nbsp;AI systems that increases global stability and prevent arms races without actually representing fully specified human values. This would give everyone more time to think about how to proceed and avoid failure modes where human values are (partially) inverted.&nbsp;</p><p>In general, especially from a downside-focused perspective, it strikes me as very important that early and possibly flawed or incomplete AI designs should <em>not yet</em> attempt to fully specify human values. Eliezer Yudkowsky recently expressed the same point in this&nbsp;<a href=\"https://arbital.com/p/hyperexistential_separation/\">Arbital post on the worst failure modes in AI alignment</a>.</p><p>Finally, what could also be highly effective for reducing downside risks, as well as being important for many other reasons, is some of the foundational work in bargaining and decision theory for AI systems, done at e.g. the Machine Intelligence Research Institute, which could help us understand how to build AI systems that reliably steer things towards outcomes that are always positive-sum.&nbsp;</p><p>I have a general intuition that, at least as long as the AI safety community does not face a strong pressure from (perceived) short timelines where the differences between downside-focused and upside-focused views may become more pronounced, there is likely to be a lot of overlap in terms of the most promising approaches focused on achieving the highest probability of success (utopia creation) and approaches that are particularly robust against failing in the most regretful ways (dystopia prevention). Heuristics like \u2018Make AI systems corrigible,\u2019 \u2018Buy more time to think,\u2019 or \u2018If there is time, figure out some foundational issues to spot unanticipated failure modes\u2019 all seem as though they would more likely be useful from both perspectives, especially when all good guidelines are followed without exception. I also expect that reasonably many people working in AI alignment will gravitate towards approaches that are robust in all these respects, because making your approach multi-layered and foolproof simply is a smart strategy when the problem in question is unfamiliar and highly complex. Furthermore, I anticipate that more people will come to think more explicitly about the tradeoffs between the downside risks from near misses and utopian futures, and some of them might put deliberate efforts into finding AI alignment methods or alignment components that fail gracefully and thereby make downside risks less likely (worst-case AI safety), either because of intrinsic concern or for reasons of cooperation with downside-focused altruists.[23] All of these things make me optimistic about AI alignment as a cause area being roughly neutral or slightly positive when done with little focus on downside-focused considerations, and strongly positive when pursued with with strong concern for avoiding particularly bad outcomes.&nbsp;</p><p>I also want to mention that I think the entire field of <strong><a href=\"https://80000hours.org/articles/ai-policy-guide/\">AI policy and strategy</a></strong>&nbsp;strikes me as particularly positive for downside-focused value systems. Making sure that AI development is done carefully and cooperatively, without the threat of arms races leading to ill-considered, rushed approaches, seems like it would be exceptionally positive from all perspectives, and so I recommend that people who fulfill the requirements for such work should prioritize it very highly.&nbsp;</p><h1>Moral uncertainty and cooperation</h1><p>Population ethics, which is the area in philosophy most relevant for deciding between upside- and downside-focused positions, is a notoriously contested topic. Many people who have thought about it a great deal believe that the appropriate epistemic state with regard to a solution to population ethics is one of substantial moral uncertainty or of valuing further reflection on the topic. Let us suppose therefore that, rather than being convinced that some form of suffering-focused ethics or downside-focused morality is the stance we want to take, we consider it a plausible stance we very well <em>might</em> want to take, alongside other positions that remain in contention.&nbsp;</p><p>&nbsp;Analogous to situations with high empirical uncertainty, there are two steps to consider for deciding under moral uncertainty:&nbsp;</p><ul><li>(1) Estimate the value of information from attempts to reduce uncertainty, and the time costs of such attempts</li></ul><p>and compare that with&nbsp;</p><ul><li>(2) the uncertainty-adjusted value&nbsp;from pursuing those interventions that are best from our current epistemic perspective</li></ul><p>With regard to (1), we can reduce our moral uncertainty on two fronts. The obvious one is population ethics: We can learn more about the arguments for different positions, come up with new arguments and positions, and assess them critically. The second front concerns meta-level questions about the nature of ethics itself, what our uncertainty is exactly <em>about</em>, and in which ways more reflection or a sophisticated reflection procedure with the help of future technology would change our thinking. While some people believe that it is futile to even try reaching confident conclusions in the epistemic position we are in currently, one could also arrive at a view where we simply have to get started at some point, or else we risk getting stuck in a state of underdetermination and judgment calls all the way down.[24]&nbsp;</p><p>If we conclude that the value of information is insufficiently high to justify more reflection, then we can turn towards getting value from working on direct interventions (2) informed by those moral perspectives we have substantial credence in. For instance, a portfolio for effective altruists in the light of total uncertainty over downside- vs. upside-focused views (which may not be an accurate representation of the EA landscape currently, where upside-focused views appear to be in the majority) would include many interventions that are valuable from both perspectives, and few interventions where there is a large mismatch such that one side is harmed without the other side attaining a much greater benefit. Candidate interventions where the overlap between downside-focused and upside-focused views is high include AI strategy and AI safety (perhaps with a careful focus on the avoidance of particularly bad failure modes), as well as growing healthy communities around these interventions. Many other things might be positive from both perspectives too, such as (to name only a few) efforts to increase international cooperation,&nbsp;raising awareness and concern for for the suffering of non-human sentient minds, or <a href=\"https://80000hours.org/problem-profiles/improving-institutional-decision-making/\">improving institutional decision-making</a>.</p><p>It is sometimes acceptable or even rationally mandated to do something that is negative according to some plausible moral views, provided that the benefits accorded to other views are sufficiently large. Ideally, one would consider all of these considerations and integrate the available information appropriately with some decision procedure for acting under moral uncertainty, such as one that includes variance voting (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/03/MacAskill-Normative-Uncertainty.pdf\">MacAskill, 2014</a>, chpt. 3) and an imagined&nbsp;<a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">moral parliament</a>.[25] For instance, if someone leaned more towards upside-focused views, or had reasons to believe that the low-hanging fruit in the field of non-AI extinction risk reduction are exceptionally important from the perspective of these views (and unlikely to damage downside-focused views more than they can be benefitted elsewhere), or gives a lot of weight to the argument from option value (see the next paragraph), then these interventions should be added at high priority to the portfolio as well.</p><p>Some people have argued that even (very) small credences in upside-focused views, such as 1-20% for instance, would&nbsp;in itself already speak in favor of making extinction risk reduction a top priority because making sure there will still be decision-makers in the future provides high <em>option value</em>. I think this gives by far too much weight to the argument from option value. Option value does play a role, but not nearly as strong a role as it is sometimes made out to be. To elaborate, let\u2019s look at the argument in more detail: The naive argument from option value says, roughly, that our descendants will be in a much better position to decide than we are, and if suffering-focused ethics or some other downside-focused view is indeed the outcome of their moral deliberations, they can then decide to not colonize space, or only do so in an extremely careful and controlled way. If this picture is correct, there is almost nothing to lose and a lot to gain from making sure that our descendants get to decide how to proceed.&nbsp;</p><p>I think this argument to a large extent misses the point,&nbsp;but seeing that even some well-informed effective altruists seem to believe that it is very strong led me realize that I should write a post explaining the landscape of cause prioritization for downside-focused value systems. The problem with the naive argument from option value is that the decision algorithm that is implicitly being recommended in the argument, namely focusing on extinction risk reduction and leaving moral philosophy (and s-risk reduction in case the outcome is a downside-focused morality) to future generations, makes sure that people follow the implications of downside-focused morality&nbsp;in precisely the one instance where it is least needed, and never otherwise. If the future is going to be controlled by philosophically sophisticated altruists who are also modest and willing to change course given new insights, then most bad futures <em>will already have been averted in that scenario</em>. An outcome where we get long and careful reflection without downsides is far from the only possible outcome. In fact, it does not even seem to me to be the most <em>likely</em> outcome (although others may disagree). No one is most worried about a scenario where epistemically careful thinkers with their heart in the right place control the future; the discussion is instead about whether the probability that things will accidentally go off the rails warrants extra-careful attention. (And it is not as though it looks like we are particularly <em>on</em> the rails currently either.) Reducing non-AI extinction risk does not preserve much option value for downside-focused value systems because most of the expected future suffering probably comes not from scenarios where people deliberately implement a solution they think is best after years of careful reflection, but instead from cases where things unexpectedly pass a point of no return and compassionate forces do not get to have control over the future. Downside risks by action likely loom larger than downside risks by omission, and we are plausibly in a better position to reduce the most pressing downside risks now than later. (In part because \u201clater\u201d may be too late.)&nbsp;</p><p>This suggests that if one is uncertain between upside- and downside-focused views, as opposed to being uncertain between all kinds of things <em>except</em> downside-focused views, the argument from option value is much weaker than it is often made out to be. Having said that, non-naively, option value still does upshift the importance of reducing extinction risks quite a bit<em>&nbsp;\u2013 </em>just not by an overwhelming degree. In particular, arguments for the importance of option value that do carry force are for instance:&nbsp;</p><ul><li>There is still <em>some</em> downside risk to reduce after long reflection</li><li>Our descendants will know more about the world, and crucial considerations in e.g. infinite ethics or anthropics could change the way we think about downside risks (in that we might for instance realize that downside risks by omission loom larger than we thought)</li><li>One\u2019s adoption of (e.g.) upside-focused views after long reflection may correlate favorably with the expected amount of value or disvalue in the future (meaning: conditional on many people eventually adopting upside-focused views, the future is more valuable according to upside-focused views than it appears during an earlier state of uncertainty)</li></ul><p>The discussion about the benefits from option value is interesting and important, and&nbsp;a lot more could be said on both sides. I think it is safe to say that the&nbsp;non-naive case for option value is&nbsp;not strong enough to make extinction risk reduction a top priority given only small credences in upside-focused views, but it does start to become&nbsp;a highly relevant&nbsp;consideration once the credences become reasonably large. Having said that, one can also make a case that improving the <em>quality</em> of the future (more happiness/value and less suffering/disvalue) conditional on humanity not going extinct is probably going to be at least as important for upside-focused views <em>and</em> is more robust under&nbsp;population ethical uncertainty \u2013 which speaks particularly in favor of highly prioritizing existential risk reduction through AI policy and AI alignment.&nbsp;&nbsp;</p><p>We saw that integrating population-ethical uncertainty means that one should often act to benefit both upside- and downside-focused value systems \u2013 at least in case such uncertainty applies in one\u2019s own case and epistemic situation. <strong>Moral cooperation</strong> presents an even stronger and more universally applicable reason to pursue a portfolio of interventions that is altogether positive according both perspectives. The <a href=\"https://foundational-research.org/reasons-to-be-nice-to-other-value-systems/\">case for moral cooperation</a>&nbsp;is very broad and convincing, as it ranges from commonsensical heuristics to theory-backed principles found in Kantian morality or throughout Parfit\u2019s work, as well as in the literature on decision theory.[26] It implies that one should give extra weight to interventions that are positive for value systems different from one\u2019s own, and subtract some weight from interventions that are negative according to other value systems \u2013 all to the extent in which the value systems in question are endorsed prominently or endorsed by potential allies.[27]&nbsp;</p><p>Considerations from moral cooperation may even make moral reflection obsolete on the level of individuals: Suppose we knew that people tended to gravitate towards a small number of attractor states in population ethics, and that once a person tentatively settles on a position, they are very unlikely to change their mind. Rather than everyone going through this process individually, people could collectively adopt a decision rule where they value the outcome of a hypothetical process of moral reflection. They would then work on interventions that are beneficial for all the commonly endorsed positions, weighted by the probability that people would adopt them if they were to go through long-winded moral reflection. Such a decision rule would save everyone time that could be spent on direct work rather than philosophizing, but perhaps more importantly, it would also make it much easier for people to benefit different value systems cooperatively. After all, when one is genuinely uncertain about values, there is no incentive to attain uncooperative benefits for one\u2019s own value system.&nbsp;</p><p>So while I think the position that valuing reflection is always the epistemically prudent thing to do rests on dubious assumptions (because of the argument from option value being weak, as well as the reasons alluded to in footnote 24), I think there is an intriguing argument that a <em>norm for valuing reflection</em> is actually best from a moral cooperation perspective \u2013 provided that everyone is aware of what the different views on population ethics imply for cause prioritization, and that we have a roughly accurate sense of which attractor states people\u2019s moral reflection would&nbsp;seek out.&nbsp;</p><p>Even if everyone went on to primarily focus on interventions that are favored by their own value system or their best guess morality, small steps into the direction of cooperatively taking other perspectives into account can already create a lot of additional value for all parties. To this end, everyone benefits from trying to better understand and account for the cause prioritization implied by different value systems.&nbsp;</p><h1>Endnotes</h1><p>[1] Speaking of \u201cbad things\u201d or \u201cgood things\u201d has the potential to be misleading, as one might think of cases such as \u201cIt is good to prevent bad things\u201d or \u201cIt is bad if good things are prevented.\u201d To be clear, what I mean by \u201cbad things\u201d are states of affairs that are in themselves disvaluable, as opposed to states of affairs that are disvaluable only in terms of wasted opportunity costs. Analogously, by \u201cgood things\u201d I mean states of affairs that are in themselves worth bringing about (potentially at a cost) rather than just states of affairs that fail to be bad.&nbsp;</p><p>[2] It is a bit more complicated: If one thought that for existing people, there is a vast amount of value to be achieved by ensuring that people live very long and very happy lives, then even views that focus primarily on the well-being of currently existing people or people who will exist regardless of one\u2019s actions may come out as upside-focused. For this to be the case, one would need to have reason to believe that these vast upsides are realistically within reach, that they are normatively sufficiently important when compared with downside risks for the people in question (such as people ending up with long, unhappy lives or extremely unhappy lives), and that these downside risks are sufficiently unlikely (or intractable) empirically to count as the more pressing priority when compared to the upside opportunities at hand.&nbsp;</p><p>It is worth noting that a lot of people, when asked e.g. about their goals in life, do not state it as a priority or go to great lengths to live for extremely long, or to experience levels of happiness that are not sustainably possible with current technology. This may serve as a counterargument against this sort of upside-focused position. On the other hand, advocates of that position could argue that people may simply not believe that such scenarios are realistic, and that e.g.&nbsp;some people's longing to go to heaven does actually speak in favor of there being a strong desire for ensuring an exceptionally good, exceptionally long personal future.&nbsp;</p><p>[3] I estimate that a view is sufficiently \u201cnegative-leaning\u201d to qualify as downside-focused if it says that reducing extreme suffering is much more important (say 100 times or maybe 1,000 times more important) than creating optimized happiness. For normative views with lower exchange rates between extreme suffering and optimal happiness (or generally positive value), prioritization becomes less clear and will often depend on additional specifics of the view in question. There does not appear to be a uniquely correct way to <a href=\"https://foundational-research.org/measuring-happiness-and-suffering/\">measure happiness and suffering</a>, so talk about it being x times more important to prevent suffering than to create happiness always has to be accompanied with instructions for what exactly is being compared. Because of the possibility that advanced future civilizations may be able to efficiently instantiate mind states that are much more (dis)valuable than the highs and lows of our biology, <a href=\"http://reflectivedisequilibrium.blogspot.de/2012/03/are-pain-and-pleasure-equally-energy.html\">what seems particularly relevant</a>&nbsp;for estimating the value of the long-term future is the way one compares maximally good happiness and maximally bad suffering. Because presumably we have experienced neither one extreme nor the other, it may be difficult to form a strong opinion on this question, which arguably gives us grounds for substantial moral uncertainty. One thing that seems to be the case is that a lot of people \u2013 though not everyone \u2013 would introspectively agree that suffering is a lot stronger than happiness at least within the limits of our biology. However, one can draw different interpretations as to why this is the case. Some people believe that this difference would become a lot less pronounced if all possible states of mind could be accessed efficiently with the help of advanced technology. Evolutionary arguments lend some support to this position: It was plausibly more important for natural selection to prevent organisms from artificially maxing out their feelings of positive reward (<a href=\"http://reducing-suffering.org/how-likely-is-wireheading/\">wireheading</a>) than to prevent organisms from being able to be max out their negative rewards (self-torture). This line of reasoning suggests that us observing that it is rare and very difficult in practice \u2013 given our biology \u2013 to experience extremely positive states of mind (especially for prolonged periods of time) should not give us a lot of reason to think that such states are elusive <em>in theory</em>. On the other hand, another plausible interpretation of the asymmetry we perceive between suffering and happiness (as we can experience and envision them) would be one that points to an <em>underlying difference</em> in the nature of the two, one that won\u2019t go away even with the help of advanced technology. Proponents of this second interpretation believe that no matter how optimized states of happiness may become in the future, creating happiness will always lack the kind of <a href=\"https://foundational-research.org/tranquilism/#22_Cravings_Negative_states\">moral urgency</a>&nbsp;that comes with <a href=\"https://foundational-research.org/the-case-for-suffering-focused-ethics/#II_Torture-level_suffering_cannot_be_counterbalanced\">avoiding extreme suffering</a>.</p><p>[4] Note that prioritarianism or egalitarianism are not upside-focused views even though they may share the similarity with classical hedonistic utilitarianism that they <a href=\"http://www.fil.lu.se/hommageawlodek/site/papper/HoltugNils.pdf\">accept</a> <a href=\"https://www.cambridge.org/core/journals/utilitas/article/egalitarianism-and-the-putative-paradoxes-of-population-ethics/72AFFDF409AEE9BDBF435C14DFFDC0AF\">versions of</a>&nbsp;the repugnant conclusion. There can be downside-focused views which accept the repugnant conclusion. As soon as lives of negative welfare are at stake, prioritarianism and welfare-based egalitarianism accord especially high moral importance towards preventing these lives, which most likely makes the views downside-focused. (The bent towards giving priority to those worse off would have to be rather mild in order to not come out as downside-focused in practice, especially since one of the reasons people are drawn to these views in the first place might be that they incorporate downside-focused moral intuitions.)&nbsp;</p><p>[5] Person-affecting restrictions are often considered unpromising as a solution to population ethics. For instance, versions of person-affecting views that evaluate it as neutral to add well-off beings to the world, yet bad to add beings whose welfare is below zero, suffer from what Hilary Greaves (2017) has called a \u201c\u201cremarkabl[e] difficult[y] to formulate any remotely acceptable axiology that captures this idea of \u2018neutrality.\u2019\u201d Versions of person-affecting views that evaluate it as negative to add lives to the world with even just a little suffering (e.g. <a href=\"https://www.youtube.com/watch?v=1s88Ze41pRU\">Benatar\u2019s anti-natalism</a>) do not have this problem, but they appear counterintuitive to most people because of how strongly they count such suffering in otherwise well-off lives.&nbsp;All of that said, I think that person-affecting intuitions can be rescued within an anti-realist philosophical framework where there's no all-encompassing \"theory of welfare.\" Instead, there's just \"What do existing people want the future to be like?\" and \"What can we say should be the criteria that govern the creation of successor agents.\" On this alternative view, moral philosophy doesn't aim to specify everything everyone ought to do; it only provides constraints on what not to do, while leaving it to existing people to shape the future according to their goals. (These goals may be motivated by moral considerations, but there's no \"uniquely correct solution\" to population ethics.)</p><p>[6] Within the set of interventions that plausibly have a large positive impact on the long-term future, it is also important to consider one\u2019s comparative advantages. Talent, expertise and motivation for a particular type of work can have a vast effect on the quality of one\u2019s output and can make up for effectiveness differences of one or two orders of magnitude.&nbsp;</p><p>[7] Computationally inefficient in the sense that, with advanced computer technology, one could speed up what functionally goes on in biological brains by a vast factor and create large numbers of copies of such brain emulations on a computer substrate \u2013 see for instance <a href=\"https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf\">this report</a>&nbsp;or Robin Hanson\u2019s <em>The Age of Em</em>. One premise that strongly bears on the likelihood of scenarios where the future contains cosmically significant quantities of suffering is whether artificial sentience, sentience implemented on computer substrates, is possible. Because most philosophers of mind believe that digital sentience is possible, only having very small credences (say 5% or smaller) in this proposition is unlikely to be epistemically warranted. Moreover, even if digital sentience was impossible, another route to astronomical future suffering is that whatever substrates <em>can</em> produce consciousness would be used/recruited for instrumental purposes. I have also written about this issue <a href=\"https://foundational-research.org/altruists-should-prioritize-artificial-intelligence/#VII_Artificial_sentience_and_risks_of_astronomical_suffering\">here</a>.&nbsp;</p><p>[8] And some people might think that SP, under some speculative assumptions on the philosophy of mind and how one morally values different computations, may contain suffering that is <a href=\"http://reducing-suffering.org/what-are-suffering-subroutines/#Suffering_in_paradise\">tied up with the paradise requirements</a>&nbsp;and therefore hard to avoid.&nbsp;</p><p>[9] My inside view says they are two orders of magnitude less likely, but I am trying to account for some people being more optimistic. My pessimism derives from there being so many more macroscopically distinct futures that qualify as AS rather than AP, and while I do think AP represents a strong attractor, my intuition is that <a href=\"http://slatestarcodex.com/2014/07/30/meditations-on-moloch/\">Molochian</a>&nbsp;forces are difficult to overcome. (I'm very much unpersuaded by economic arguments about it being in people's self-interest to set things up in a smart way \u2013 I don't think humans have a good track record with that kind of foresight.) See also Brian Tomasik's discussion of utopia&nbsp;<a href=\"http://reducing-suffering.org/utopia/\">here</a>. Evidence that might contribute towards making me revise my estimate (up to e.g. 10% likelihood of some kind of utopia given space colonization) would be things such as:&nbsp;</p><ul><li>A country implementing a scheme to redistribute wealth in a way that eliminates poverty or otherwise generates large welfare benefits without really bad side-effects (including side-effects in other countries).</li><li>Technologically leading countries achieving a vastly consequential agreement regarding climate change or international coordination with respect to AI development.&nbsp;</li><li>Demographic trends suggesting a stable decrease in fertility rates for total population prognoses down to sustainability levels or below (especially if stable without highly negative side-effects on a long-term perspective of centuries and beyond).</li><li>Many countries completely outlawing factory farming in the next couple of decades, especially if non-economic reasons play a major role.</li><li>Any major, research-relevant jurisdiction outlawing or very tightly regulating the use of certain AI algorithms for fear of needlessly creating sentient minds.</li><li>Breakthroughs in AI alignment research that make most AI safety researchers (including groups with a track record of being on the more pessimistic side) substantially more optimistic about the future than they currently are.</li></ul><p>[10] Some people would say that as long as a being prefers existence over non-existence (and if they would not, there is usually the option of suicide), their existence cannot be net negative even if it contained a lot more suffering than happiness. I would counter that, while I see a strong case from cooperation and respecting someone\u2019s goals for not <em>terminating</em> the existence of a being&nbsp;who wants to continue to go on existing, this is not the same as saying that the being\u2019s existence, assuming it contains only suffering, adds value independently of the being's drives or goals. Goals may not be about only one\u2019s own welfare \u2013 they can also include all kinds of objectives, such as living a meaningful life or caring about personal achievements or the state of the world. After all, one would think that Darwinian forces would be unlikely to select for the kind of goals or evaluation&nbsp;mechanisms that easily evaluate one\u2019s olife as <em>all things considered</em>&nbsp;not worth living. So rather than e.g. only taking moment-by-moment indicators of one\u2019s experienced well-being, people\u2019s life satisfaction judgment may include many additional, generally life-supporting components that derive from our relation to others and to the world. Correspondingly, not committing suicide is insufficient evidence that, from a purely well-being-oriented perspective, someone\u2019s life is being \u2013 even just subjectively \u2013 evaluated as net valuable. (If alongside suicide, there was also the option to take a magic pill that turned people into p-zombies, how many people would take it?) We can also think of it this way: If a preference for continued life or no-longer-existence was enough to establish that starting a life is neutral or positive, then someone could engineer artificial beings that always prefer consciousness over non-consciousness, even if they experienced nothing but agony for every second of their existence. It personally strikes me as unacceptable to regard this situation as anything but very bad, but intuitions may differ.</p><p>[11] Technically, one could hold an upside-focused ethical view where SP is similarly good as AP. But this raises the question whether one would have the same intuition for a small dystopia versus a large dystopia. If the large dystopia is orders of magnitude worse than the small dystopia, yet the scope of a potential paradise runs into <a href=\"https://foundational-research.org/the-case-for-suffering-focused-ethics/#IV_Other_values_have_diminishing_returns\">diminishing returns</a>, then the view in question is either fully downside-focused if the large dystopia is stipulated to be much worse than SP is good, or implausibly indifferent towards downside risks in case SP is labelled as a lot better than a much larger dystopia. Perhaps one could construct a holistic view that is exactly in-between downside- and upside-focused, based on the stipulation that half of one\u2019s moral caring capacity goes into utopia creation and the other half into suffering prevention. This could in practice be equivalent to using a <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">moral parliament</a>&nbsp;approach to moral uncertainty while placing 50% credence on each view.</p><p>[12] One exception is that in theory, one could have a civilization that remains on earth yet uses up as much energy and resources as physically available to instantiate digital sentience at \u201castronomical\u201d scales on supercomputers built on earth. Of course, the stakes in this scenario are still dwarfed by a scenario where the same thing happens around every star in the accessible universe. Whether this scenario would fit the definition of \"s-risk\" depends on how likely it is that there are scenarios that would be so much worse that earthbound suffering remains insignificant by comparison.</p><p>[13] Though one plausible example might be if a value-aligned superintelligent AI could trade with AIs in other parts of the universe and bargain for suffering reduction.</p><p>[14] From Bostrom\u2019s <em><a href=\"https://nickbostrom.com/astronomical/waste.html\">Astronomical Waste</a></em>&nbsp;(2003) paper:&nbsp;</p><p>\u201cBecause the lifespan of galaxies is measured in billions of years, whereas the time-scale of any delays that we could realistically affect would rather be measured in years or decades, the consideration of risk trumps the consideration of opportunity cost. For example, a single percentage point of reduction of existential risks would be worth (from a utilitarian expected utility point-of-view) a delay of over 10 million years.\u201d</p><p>[15] For the purposes of this post, I am primarily evaluating the question from the perspective of minimizing s-risks. Of course, when one evaluates the same question according to other desiderata, it turns out that most plausible moral views very strongly favor the current trajectory. Any view that places overriding importance on avoiding involuntary deaths for currently-existing people automatically evaluates the current trajectory as better, because the collapse of civilization would bring an end to any hopes of delaying people\u2019s deaths with the help of advanced technology. Generally, a central consideration here is also that the more one values parochial features of our civilization, such as the survival of popular taste in art, people remembering and revering Shakespeare, or the continued existence of pizza as a food item, etc., the more obvious it becomes that the current trajectory would be better than a civilizational reset scenario.&nbsp;</p><p>[16] Further things to consider include that a civilization recovering with depleted fossil fuels would plausibly tend to be poorer for any given level of technology compared to&nbsp;our historical record. What would this mean for values? By the time they caught up with us in energy production, might they also have been civilized for longer? (This could add sophistication in some ways but also increase societal conformity, which could be both good or bad.)&nbsp;</p><p>[17] This estimate may&nbsp;appear surprisingly high, but that might be because when we think about our own life, our close ones, and all the things we see in the news and know about other countries and cultures, it makes little difference whether all of civilization ends and a few humans survive, or whether all of civilization ends and no one survives. So when experts talk about e.g. the dangers of nuclear war, they may focus on the \u201cthis could very well end civilization\u201d aspects, without emphasizing the part that <em>some</em> few humans at least are probably going <a href=\"http://reflectivedisequilibrium.blogspot.de/2012/11/nuclear-winter-and-human-extinction-q.html\">to survive</a>. And after the worst part <a href=\"http://reflectivedisequilibrium.blogspot.de/2012/05/what-to-eat-during-impact-winter.html\">is survived</a>, population growth could kick in again and, even though the absence of fossil fuels in such a scenario would likely mean that technological progress takes longer, this \u201cdisadvantage\u201d could quite plausibly be overcome given longer timescales and <a href=\"http://reflectivedisequilibrium.blogspot.de/2012/09/can-catch-up-growth-take-us-to-stars.html\">larger populations</a>&nbsp;per level of technology (closer to the Malthusian limits).&nbsp;</p><p>[18] One pathway for positive spillover effects would be if increased global coordination with respect to non-AI technologies also improves important coordination efforts for the development of smarter-than-human AI systems. And maybe lower risks of catastrophes generally lead to a more stable geopolitical climate where arms races are less pronounced, which could be highly beneficial for s-risk reduction. See also the points made&nbsp;<a href=\"https://foundational-research.org/how-would-catastrophic-risks-affect-prospects-for-compromise/\">here</a>.&nbsp;</p><p>[19] Furthermore, because a scenario where civilization collapsed would already be irreversibly very&nbsp;bad for all the people who currently exist or would exist at that time, making an eventual recovery more likely (after several generations of post-Apocalyptic hardship) would suddenly become something that is only favored by some people\u2019s altruistic concerns for the long-term future, and not anymore by self-oriented or family- or community-oriented concerns.</p><p>[20] I think the situation is such that extremely few positions are <em>comfortable</em> with the thought of extinction, but also few perspectives come down to regarding the avoidance of extinction as top priority. In <a href=\"https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/\">this podcast</a>, Toby Ord makes the claim that non-consequentialist value systems should place particular emphasis on making sure humanity does not go extinct. I agree with this only to the small extent that it is true that such views largely would not welcome extinction. However, overall I disagree and think it is hasty to pocket these views as having extinction-risk reduction as their primary priority. Non-consequentialist value systems admittedly are often somewhat underdetermined, but insofar as they do say anything about the long-term future being important, it strikes me as a more natural extension to count them as extinction-averse <em>but downside-focused</em>&nbsp;rather than extinction-averse and upside-focused. I think this would be most pronounced when it comes to downside risks by action, i.e. moral catastrophes humanity could be responsible for. A central feature distinguishing consequentialism from other moral theories is that the latter often treat harmful actions differently from harmful omissions. For instance, a rights-based theory might say that it is particularly important to prevent large-scale rights violations in the future (\u2018do no harm\u2019), though not particularly important to make large-scale positive scenarios actual (\u2018create maximal good\u2019). The intuition that causes some people to <a href=\"http://reducing-suffering.org/omelas-and-space-colonization/\">walk away from Omelas</a>, in this fictional dilemma that is structurally similar to some of the moral dilemmas we might face regarding the promises and dangers from advanced technology and space colonization, are largely non-consequentialist intuitions. Rather than fighting for Omelas, I think many non-consequentialist value systems would instead recommend a focus on helping those beings who are (currently) worst-off, and perhaps (if one extends the scope of such moralities to also include the long-term future) also a focus on helping to prevent future suffering and downside risks.&nbsp;</p><p>[21] For a pointer on how to roughly envision such a scenario, I recommend this <a href=\"https://ordinaryideas.wordpress.com/2015/11/30/driving-fast-in-the-counterfactual-loop/\">allegorical description</a>&nbsp;of a future where humans lose their influence over an automated economy.&nbsp;</p><p>[22] See the description under <em>Problem #2</em> <a href=\"https://ai-alignment.com/aligned-search-366f983742e9\">here</a>.&nbsp;</p><p>[23] Of course, every decent person has intrinsic concern for avoiding the worst failure modes in AI alignment, but the difference is that, from an downside-focused perspective, bringing down the probabilities from very small to extremely small is as important as it gets, whereas for upside-focused views, other aspects of AI alignment may look more promising to work on (on the margin) as soon as the obvious worst failure modes are patched.&nbsp;</p><p>[24] Valuing reflection means to value not one\u2019s current best guess about one\u2019s (moral) goals, but what one would want these goals to be after a long period of philosophical reflection under idealized conditions. For instance, I might envision placing copies of my brain state now into different virtual realities, where these brain emulations get to converse with the world\u2019s best philosophers, have any well-formed questions answered by superintelligent oracles, and, with advanced technology, explore what it might be like to go through life with different moral intuitions, or explore a range different experiences. Of course, for the reflection procedure to give a well-defined answer, I would have to also specify how I will get a moral view out of whatever the different copies end up concluding, especially if there won\u2019t be a consensus or if the consensus strongly depends on ordering effects of the thought experiments being presented. A suitable reflection procedure would allow the right amount of flexibility in order to provide us with a chance of actually changing our current best guess morality, but at the same time, that flexibility should not be dialed up too high, as this might then fail to preserve intuitions or principles we are \u2013 right now \u2013 certain we would want to preserve. The art lies in setting just the right amount of guidance, or selecting some (sub)questions where reflection is welcome, and others (such as maybe whether we want to make altruism a major part of our identity or instead go solely with rational egoism, which after all also has sophisticated philosophical adherents) that are <em>not</em> open for change. Personally, I find myself having much stronger and more introspectively transparent or reasoning-backed intuitions about population ethics than e.g. the question of which entities I morally care about, or to what extent I care about them. Correspondingly, I am more open for changing my mind on the latter. I would be more reluctant to trust my own thoughts and intuitions on population ethics if I thought it is likely that there was a single correct solution. However, based on my model of how people form consequentialist goals \u2013 which are a new and weird thing for animal minds to get hijacked by \u2013 I feel confident that at best, we will find several consistent solutions that serve as attractors for humans undergoing sophisticated reflection procedures, but no voluntarily-consulted reflection procedure can persuade someone to adopt a view that they rule out as something they would ex ante not want the selection procedure to select for them.&nbsp;<br><br>[25] An alternative to the parliamentary approach that is sometimes mentioned, at least for dealing with moral uncertainty between consequentialist views that seem comparable, would be to choose whichever action makes the largest positive difference in terms of <em>expected utility</em> between views, or choose the action for which \u201cmore seems at stake.\u201d While I can see some intuitions (related to <a href=\"https://casparoesterheld.com/2016/11/21/thoughts-on-updatelessnes/\">updatelessness</a>) for this approach, I find it overall not very convincing because it so strongly and predictably privileges upside-focused views, so that downside-focused views would get virtually no weight. For a related discussion (which may only make sense if people are familiar with the context of <a href=\"/ea/1gf/multiversewide_cooperation_in_a_nutshell/\">multiverse-wide cooperation</a>) on when the case for updatelessness becomes particularly shaky, see the section \u201cUpdateless compromise\u201d <a href=\"https://docs.google.com/document/d/1wiI2i7FGSASt_8Uvcd6SAEaghOKkzx01_2Y6JDKeeZE/edit\">here</a>. Altogether, I would maybe give some weight to expected value considerations for dealing with moral uncertainty, but more weight to a parliamentary approach. Note also that the expected value approach can create moral uncertainty wagers for idiosyncratic views, which one may be forced to accept for reasons of consistency. Depending on how one defines how much is at stake according to a view, an expected value approach in our situation gives the strongest boost to normatively upside-focused views such as positive utilitarianism (\u201conly happiness counts\u201d) or to views which affect as many interests as possible (such as counting the interests of <a href=\"http://reflectivedisequilibrium.blogspot.de/2012/07/rawls-original-position-potential.html\">all possible people</a>, which under some interpretations should swamp every other moral consideration by the amount of what is at stake).&nbsp;</p><p>[26] Both Kant and Parfit noticed that one\u2019s decisions may change if we regard ourselves as deciding over not just our own policy over actions, but over a collective policy shared by agents similar to us. This principle is taken up more formally by many of the <a href=\"https://casparoesterheld.com/a-comprehensive-list-of-decision-theories/\">alternatives to causal decision theory</a>&nbsp;that recommend cooperation (under varying circumstances) even in <em>one-shot</em>, prisoner-dilemma-like situations. In <em>On What Matters</em>&nbsp;(2011), Parfit in addition argued that the proper interpretation for both Kantianism and consequentialism suggests that they are describing the same kind of morality, \u201cKantian\u201d in the sense that naive acts-justify-the-means reasoning often does not apply, and consequentialist in that, ultimately, what matters is things being good.&nbsp;</p><p>[27] I am primarily talking about people we can interact with, though some approaches to thinking about decision theory suggest that potential allies may also include people in <a href=\"/ea/1gf/multiversewide_cooperation_in_a_nutshell/\">other parts of the multiverse</a>, or future people good at <a href=\"https://arbital.com/p/parfits_hitchhiker/\">Parfit\u2019s Hitchhiker</a>&nbsp;problem.&nbsp;</p><h1>Acknowledgements</h1><p>This piece benefitted from comments by Tobias Baumann, David Althaus, Denis Drescher, Jesse Clifton, Max Daniel, Caspar Oesterheld, Johannes Treutlein, Brian Tomasik, Tobias Pulver, Kaj Sotala (who also allowed me to use a map on s-risks he made and adapt it for my purposes in the section on AI) and Jonas Vollmer. The section on extinction risks benefitted from inputs by Owen Cotton-Barratt, and I am also thankful for valuable comments and critical inputs in a second round of feedback by Jan Brauner, Gregory Lewis and Carl Shulman.&nbsp;</p><h1>References</h1><p>Armstrong, S. &amp; Sandberg, A. (2003). Eternity in Six Hours: Intergalactic spreading of intelligent life and sharpening the Fermi paradox. <em>Ars Acta</em>&nbsp;89:1-13.&nbsp;</p><p>Bostrom, N. (2003). Astronomical Waste: The Opportunity Cost of Delayed Technological Development. <em>Utilitas</em>&nbsp;15(3):308-314.&nbsp;</p><p>Bostrom, N. (2014). <em>Superintelligence: Paths, Danger, Strategies</em>. Oxford: Oxford University Press.&nbsp;</p><p>Daswani, M. &amp; Leike, J. (2015). A Definition of Happiness for Reinforcement Learning Agents. <em>arXiv:1505.04497</em>.&nbsp;</p><p>Greaves, H. (2017). Population axiology. <em>Philosophy Compass</em>, 12:e12442. doi.org/10.1111/phc3.12442.</p><p>Hastie, R., &amp; Dawes, R. (2001). <em>Rational choice in an uncertain world: The psychology of judgment and decision making</em>. Thousand Oaks: Sage Publications.</p><p>MacAskill, W. (2014). <em>Normative Uncertainty</em>. PhD diss., St Anne\u2019s College, University of Oxford.&nbsp;</p><p>Newman, E. (2006). Power laws, Pareto distributions and Zipf\u2019s law.&nbsp;<em>arXiv:cond-mat/0412004</em>.</p><p>Omohundro, SM. (2008). The Basic AI Drives. In P. Wang, B. Goertzel, and S. Franklin (eds.). Proceedings of the First AGI Conference, 171,<em> Frontiers in Artificial Intelligence and Applications</em>. Amsterdam: IOS Press.</p><p>Parfit, D. (2011). <em>On What Matters</em>. Oxford: Oxford University Press.</p><p>Pinker, S. (2011). <em>The Better Angels of our Nature</em>. New York, NY: Viking.</p><p>Sotala, K. &amp; Gloor, L. (2017). Superintelligence as a Cause or Cure for Risks of Astronomical Suffering. <em>Informatica</em> 41(4):389\u2013400.</p>", "user": {"username": "Lukas_Gloor"}}, {"_id": "Bueoanm4d7tc2ht4F", "title": "EA Funds hands out money very infrequently - should we be worried?", "postedAt": "2018-01-31T08:10:24.954Z", "htmlBody": "<html><body><p>From the <a href=\"https://app.effectivealtruism.org/\">EA Funds</a> website, here is the amount of money unallocated in each fund:</p>\n<ul>\n<li><strong>Long-Term Future</strong>: $348,167</li>\n<li><strong>Global Development</strong>: $497,957</li>\n<li><strong>Animal Welfare</strong>: $75,109</li>\n<li><strong>EA Community</strong>: $206,271</li>\n</ul>\n<p>Some of them haven&apos;t disbursed any funds in quite a while. I asked the CEA team about this and got the following reply:</p>\n<blockquote>\n<p>The answer to this is that currently the fund manager have a great deal of discretion about when they give out grants, and they tend to do so in large chunks with somewhat low frequency.&#xA0; This is a situation we may be giving more consideration in the future, because indeed there are reasons to question whether the granting frequency as it currently is an optimal situation.</p>\n</blockquote>\n<p>It seems&#xA0;problematic to have ~$1.1million dollars&#xA0;in the bank&#xA0;and no commitment as to when this will be handed out.</p>\n<p>For the Animal Welfare fund, I mind less &#x2013; the fund manager makes many small donations to a bunch of charities, and it seems unlikely I&apos;d ever be able to match his skill in doing this. But&#xA0;in the case of the Global Development fund (which so far has mostly just handed money to AMF and has half a million dollars in the pot), I could just pay my monthly donation into a low-risk ETF, make a&#xA0;small yield on it, and then donate the total myself at the end of the year.</p>\n<p>Interested to hear people&apos;s thoughts on this!</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "HenryStanley"}}, {"_id": "BJrdKZHTkgmhmjDDC", "title": "How scale is often misused as a metric and how to fix it", "postedAt": "2018-01-30T01:36:45.368Z", "htmlBody": "<html><body><p><span>One of the big criteria used for cause area selection is scale and importance of the issue. This has been used by </span><a href=\"https://80000hours.org/articles/problem-framework/#how-to-assess-scale\"><span>80,000 Hours</span></a><span> and </span><a href=\"https://www.openphilanthropy.org/research/cause-selection\"><span>OpenPhil</span></a><span> amongst others. This is often defined as the size and intensity of the problem. For example, if an issue affects 100,000 people deeply, that would be considered higher scale than an issue that minorly affects 1,000 people. Although this version of scale is pretty common in EA, I think there are some major problems with it, the biggest of which being bottlenecking.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>The broad idea of measuring scale this way has an implication baked into it that the total scale of the problem is the factor to most consider. However, with almost all large problems this seems very unlikely to be true as they are almost always going to be capped or bottlenecked by something much faster than they will by the total capacity of the problem. Take for example bednets. If AMF only gets the funding to give out 10 million bednets a year it doesn&apos;t really matter if the total scale of the malaria burden would require 20 million or 500 million. Effectively AMF is capped by money before it hits other scaling considerations. If you were a billionaire perhaps you could give enough money to make money no longer the capping feature, but even in that situation it&#x2019;s likely another factor would cap before reaching all those in need of nets. In AMF&#x2019;s case it would likely be number of partners who can effectively be worked with or the political stability of remaining countries. </span><span><br></span><span><br></span><span>This concern of bottlenecking is even more dramatic in fields that have tighter caps or very large scale. To take an example in animal rights, if your organization can only raise $100,000 it doesn&apos;t really matter how big the population is from a scale perspective as long as it&#x2019;s much larger than you are likely to effectively help with $100,000. When comparing a cause like animal rights vs bednets, clearly the total size of the animal rights issues hits a lot more individuals, but its &#x201C;true scale,&#x201D; in many cases, will be more strictly capped than a more popular, better funded, and more well understood poverty intervention that affects fewer individuals. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Money is one of the most common capping features to scale but it&#x2019;s not the only one. Sometimes it can be logistical factors like partners or total production in the market of a certain good. Sometimes it can be people capped (it seems likely a charity focused on surgeries would run into a skilled people shortage before running into the problem of not having enough people to do surgeries on). A capping feature could also tied to crowdedness of a space. It could also be our understanding of the problem. For example, wild animals may suffer enormously, but if we don&#x2019;t know how to help all of them. In general, when looking at a cause area or charity, it seems one should consider what factor is most likely to cap scale first rather than just looking at the total size of the problem and assuming no other capping features happen. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>A counter argument to this might be that scale is just used as a proxy to narrow down cause selection. This use of scale I have far less concerns with, but many people, including major organizations, explicitly use scale in the way I described to make end line calls about what causes to support. </span></p>\n<p>&#xA0;</p>\n<p><span>Another counter argument is that if you think your intervention has a small chance of helping all the population. For example, if you think your action produces a 0.000001% increase in the chance of ending all factory farming, then a more normal understanding of scale makes sense. However given the huge scale of most problems EAs work on, few of our solutions are aimed at solving the whole problem (e.g. we cannot even fill fill AMF&#x2019;s room for funding which is only one of many charities working on malaria). We want to be careful not being far overconfident about our ability to affect change and let that change our cause selection. </span></p></body></html>", "user": {"username": "Joey"}}, {"_id": "Ez9m9ejbeYTJZwiMa", "title": "The almighty Hive will", "postedAt": "2018-01-28T17:59:07.040Z", "htmlBody": "<html><body><p>I&#x2019;ve been wondering whether EA can&#x2019;t find some strategic benefits from a) a peer-to-peer trust economy, or b) rational coordination towards various goals. These seem like simple ideas, but I haven&#x2019;t seen them publicly discussed.&#xA0;</p>\n<p>I&#x2019;ll start&#xA0;from the related and oversimplifying assumptions that&#xA0;</p>\n<p>a) there&#x2019;s a wholly fungible pool of EA money (for want of a better name, let&#x2019;s call it Gringotts) shared among EAs and EA organisations, and&#xA0;</p>\n<p>b) all EAs trust all other EAs as much as they trust themselves such that we form a megamind (the Hive), and&#xA0;</p>\n<p>c) all EAs consider all EA goals to be worthwhile and high value, even if they see some as substantially less so than others, such that we all have basically the same goal (collecting all the Pokemon)</p>\n<p>In some cases these assumptions are so flawed as to be potentially fatal, but I think they&#x2019;re an interesting starting point for some thought experiments - and we can focus on relevant problems with them as we go. But the EA movement is getting large enough that even if these assumptions were only to hold for microcosms of it, we might still be able to get some big wins. So here are some ideas for exploiting our Hivery, in two broad categories:</p>\n<p>&#xA0;</p>\n<h2>Building an EA social safety net</h2>\n<p>&#xA0;</p>\n<h3>1) Intra-Hive insurance</h3>\n<p>Normal insurance is both inherently wasteful (insurance companies have to spend ages assessing risk to ensure that they make a profit on their rates) and negative expected value for the insuree (who pays for the waste, plus the insurance profits). In a well functioning Hive seeking all the Pokemon, with a sufficiently sizable Gringotts, each EA could just register things of irreplaceable value to them personally, and if they ever broke/lost/accidentally swallowed the item, job, existential status etc, they would get some commensurate amount of money with few questions asked. That would save Gringotts from the negative <a href=\"https://en.wikipedia.org/wiki/Expected_value\">expected value</a>&#xA0;(EV) of almost all insurance, give everyone peace of mind, and avoid a lot of time and angst spent dealing with potentially unscrupulous or opaque insurers.</p>\n<p>In practice this is only as good an idea as our simplifying assumptions combined, and creates some dubious incentives, so might be a pipe dream. Still, if the EA community really is or could be a lot closer to the assumed ideal than society at large, it seems like there could be room for some small-scale operations like this - for example EA organisations offering such pseudo-insurance to their staff, and large-scale donors offering it to the EA organisations.</p>\n<p>One way to potentially strengthen the trust requirement would be to develop an opt-in EA reputation system on an EA app or website, much like the ratings for Uber drivers. If it felt uncomfortable, obviously you wouldn&#x2019;t have to get involved, but it could allow a fairly straightforward tier-based system of what you were eligible for based on your rating (probably weighted by other factors, like how many people had voted). You could also add some weighting to people currently working for EA organisations, though it might be too limiting to make that a strong prerequisite (Earn-to-Givers might want to insure themselves so they could safely give a higher proportion, for eg). As with normal insurance it would create <a href=\"https://www.investopedia.com/terms/m/moralhazard.asp\">moral hazard</a>&#xA0;problems, but hopefully with some intelligent but low-cost reputation management this could still be a&#xA0;big net positive for Gringotts.</p>\n<p>Personally, I think the reputation app is a really cool idea even if it never got used for anything substantial, but I&#x2019;m prepared to be alone in that.</p>\n<p>&#xA0;</p>\n<h3>1.1) Guaranteed income pool for entrepreneurial EAs</h3>\n<p>This is much like insurance, with similar limitations and much the same potential for mitigating them. Except here it&#x2019;s based on the idea that entrepreneurialism is one of the highest EV-earning pathways, but because of the ultra-high risks it&#x2019;s out of reach to anyone who can&#x2019;t get insta-VCed or support themselves for several months. As with insurance, Gringotts is big enough that it doesn&#x2019;t really suffer from risks that affect a single person. In this case though, a further factor is that the EA would need to demonstrate some degree of competence to ensure that they were actually doing something positive EV, and to the extent that they could do so they might be able to get funding from regular pathways.&#xA0;</p>\n<p>Something similar might also be useful for people interested in starting EA charities, before the stage where they might be eligible for a substantial grant from eg Givewell or OpenPhil. Again, I&#x2019;m not sure such a window exists, but it seems worth looking at for people from poorer backgrounds.</p>\n<p>&#xA0;</p>\n<h3>2) Low interest loans</h3>\n<p>Loans have all the waste and negative EV of insurance, except that you get the money straight away - and there&#x2019;s no question about whether you get it. This maybe makes them a stronger candidate for Gringotts-coverage, since it removes one risk factor. Relatedly, they also avoid the incentive-distorting effects of insurance, removing another.</p>\n<p>In the real world, loans also require a credit rating check, which can be based on some quite arbitrary criteria, such as not being able to guarantee repayments because you&#x2019;re poor, whether you use a credit card or debit card, or even <a href=\"https://www.theguardian.com/money/2010/apr/10/register-vote-improve-credit-rating\">whether you&#x2019;re registered to vote</a>. And given the relatively low number of factors the credit rating relies on, there would probably be a lot of random noise in it even if they were all sensible.</p>\n<p>Lastly, with a normal loan, something has necessarily gone wrong for the creditor if a repayment is missed. Gringotts, on the other hand, might sometimes be content for a debitor to miss repayments if the money nonetheless went towards gathering a lot of Pokemon, or even if it had been wisely spent on an ultimately doomed venture.</p>\n<p>&#xA0;</p>\n<h3>3) A Hive existential space network</h3>\n<p>Couchsurfing may already be A Thing, but there might be some opportunities for making it a smoother experience given a robust trust network. Also, since sleeping isn&#x2019;t the only mode of existence, &#x2018;living spaces&#x2019; aren&#x2019;t the only form of living spaces; given how many EAs work remotely, there&#x2019;s probably also a lot of demand for working spaces. EAs with more modest living accommodation could also offer solo- or duo- (etc) working spaces, in the former case if they would normally work elsewhere. It might even be helpful to have co-working spaces in a fairly close area, with explicitly differing cultures (eg one being mostly silent, the other having music or ambient sound, or freer conversation, or with people working on similar project types, people with similar - or deliberately disparate - skills, people bringing children etc)</p>\n<p>Given the psychological benefits for some of us of having a separate space for living and working combined with the <a href=\"http://webarchive.nationalarchives.gov.uk/20160131203938/http://www.ons.gov.uk/ons/rel/wellbeing/measuring-national-well-being/commuting-and-personal-well-being--2014/art-commuting-and-personal-well-being.html#tab-2--Key-Points\">emotional benefits of having a short commute</a>, EAs who live near each other might even benefit from just swapping homes for the working day.</p>\n<p>&#xA0;</p>\n<h3>4) EA for-profits offering discounts on VAT-applied goods and services</h3>\n<p>At the moment there are few EA for profits, and many of those mainly offer services to disadvantaged subgroups rather than to other EAs. Nonetheless in future we might see a proliferation of EA startups, even if the only sense in which they&#x2019;re EA is a strong effective giving culture among their founders. In such a case, if the goods/services they offer are VAT (or similar) taxable, it would provide an incentive for them to offer heavy discounts to other EA organisations and/or EAs - since the lower the cost, the less Gringotts would leak in service tax.</p>\n<p>Gringotts could incentivise this with one of the strategies above, though there might be legal implications. Nonetheless, the Hive would surely benefit from finding out exactly what the legal limits are and exploring the possibilities of going right up to them.&#xA0;</p>\n<p>&#xA0;</p>\n<h2>Maximising the value of EA employees</h2>\n<p>&#xA0;</p>\n<h3>5) EA organisations partially substituting salaries with benefits</h3>\n<p>Every time an EA working at an EA org buys something the org could have bought, Gringotts loses the income tax on whatever they&#x2019;ve bought. In the UK at least, there&#x2019;s a tax-free threshold of &#xA3;11,500, but in an ideal world everything EA employees would want to spend money on above that threshold would be bought for them by EA organisations. More realistically, to keep things relatively egalitarian and maintain sensible incentives, the ideal might be to pay for any things that EA employees would need to maintain a healthy lifestyle. An initial laundry list of candidates I put together:&#xA0;</p>\n<ul>\n<li>accommodation, (not necessarily just for employees - we might ultimately be able to build peer-to-org or org-to-org existential space networks, per point 3 above)</li>\n<li>bills,&#xA0;</li>\n<li>travel to and from work,&#xA0;</li>\n<li>a gym membership (or some equivalent physical activity for people who find the gym too sterile),</li>\n<li>out-of-work education,</li>\n<li>electronic equipment for unrestricted (within reason) personal use,</li>\n<li>clothes,</li>\n<li>pension contributions,</li>\n<li>toiletries,</li>\n<li>food,&#xA0;</li>\n<li>medical supplies</li>\n</ul>\n<p>&#xA0;&#xA0;</p>\n<p>I know some of these are already offered by some EA organisations (and many for-profits, come to that), and there will surely be legal restrictions on how much money you can spend on employees like this without it getting taxed. But the potential savings are so big that again the Hive should surely explore and share knowledge of the exact legal boundaries.</p>\n<p>&#xA0;</p>\n<h3>6) Employees of EA organisations <em>not</em> donating&#xA0;</h3>\n<p>Since every such donation is made with an EA&#x2019;s taxed income, the same considerations as in 5) apply. Every time an EA does this, Gringotts loses the tax value of the donation. The simplest way to avoid this would be for EAs to just ask for a 10% lower salary (or whatever donation proportion they would imagine themselves having made) than they would have done for a comparable job elsewhere .&#xA0;</p>\n<p>This would potentially redistribute money among causes since EAs working at one org might not think it&#x2019;s actually the best one. But unless the proportion redistributed would be more than the average income tax on an EA salary (somewhere in the vicinity of 20% seems like a plausible estimate), this would be an iterated prisoner&#x2019;s dilemma. Any individual could move more money to their cause of choice by requesting a higher income, but the fewer of us did so, the more money would end up with <em>all</em>&#xA0;the causes. And it feels like a Hive of cooperating altruists should be able to deal with one little wafer thin prisoner&#x2019;s dilemma&#x2026;&#xA0;</p>\n<p>In cases where individuals are working for an EA org but feel that other organisations are substantially more than 20% more effective than their own, it feels like they should often prefer just earning to give. There are numerous possible exceptions - for example if you feel like the multiplier on the other org is higher than 20% but you wouldn&#x2019;t earn enough in another to multiply to a net plus, or you&#x2019;re planning to move to another EA organisation but are working in your current job to gain skills and reputation. It seems like such motivations would have intra-EA signalling costs, though, since they imply both that you&#x2019;re defecting in the prisoner&#x2019;s dilemma and that you don&#x2019;t value the work of the people around you that highly. Ironically, it might actually look <em>bad</em> for an EA employee to admit to charitable donations.</p>\n<p>Even so, the extra-EA signalling costs of not giving could conceivably outweigh both the intra-EA signals and the tax savings from doing so. If we believe this, an alternative approach would be to have EA orgs explicitly run donation-directing schemes. Each org could contribute to a pool of money they planned to redirect whose size was dependent on their number of staff and staff salaries. Then each employee could direct some proportion of it to the cause of their choice; the weight of their direction could either be proportional to the difference between their salary and the max salary they could have asked for or, more diplomatically, just equal for each employee. That way the money would still be distributed in much the same proportion as it currently is, but without being taxed - and EAs could still be said to be donating in some sense at least (and would still have an incentive to keep abreast of what&#x2019;s going on elsewhere in the EA world).</p>\n<p>&#xA0;</p>\n<h3>7) Basing salary on expected career trajectory</h3>\n<p>Similarly to the previous idea, if I&#x2019;m working at an EA organisation but expect that in the near future I&#x2019;ll end up working in the private sector - either because I&#x2019;m earning to give, because I&#x2019;m trying to build career capital, or any number of other possible reasons - it doesn&#x2019;t make sense for me to get a substantial amount more than I need to live on at the EA org and then give a lot of money away after I transition. Better to earn less now and give slightly less later.</p>\n<p>Again, this follows from taxation - whether I later pay back the tax on the extra money I earned at the EA org or not, Gringotts will be that much poorer (because it partly comprises me). It also compounds to the extent that <a href=\"https://80000hours.org/2012/04/the-haste-consideration/\">you agree with</a>&#xA0;the <a href=\"https://80000hours.org/2012/04/the-haste-consideration/\">haste consideration</a>&#xA0;- the money saved now could be worth substantially more than the money you give later.</p>\n<p>If you&#x2019;re moving from the private sector into an EA org, the same strategy would probably make sense in reverse - if you&#x2019;re in commerce and transitioning into an EA organisation, you would keep more now and ask for a commensurately lower salary from the EA org - though it would be less clear/less pronounced an effect because of the haste consideration. Also, the haste consideration suggests that if you&#x2019;re never expecting to work at an EA organisation, it might be better to donate a declining proportion of your income (or rather, to donate in such a way as to increase the amount you keep for yourself over time, holding the net lifetime amount you expect to donate constant). Since this front-loads your donations, it also has the side-benefit of making future burnout less costly to Gringotts, and perhaps also less tempting for future-you.</p>\n<p>This strategy is fairly high risk for an individual, in that if you suddenly need to pay for urgent medical assistance or some other emergency expenditure in your younger life, you might find yourself unable to afford it - but that&#x2019;s just the sort of issue that could be mitigated or even resolved by Hive insurance.</p>\n<p>It would also &#x2018;lose&#x2019; the interest you&#x2019;d have earned on the money you&#x2019;d kept earlier, but you can account for that when calculating future donations. The effect will be dominated by the tax savings, and in any case, the money will still having been earning a (greater) return on investment through its EA use elsewhere in the Hive.</p>\n<p>One complicating factor is that sometimes commercial employers will offer a salary based on the size of your current one, so taking a low salary from an EA org might harm future earning prospects. A possible remedy for this, if it wasn&#x2019;t perceived as dishonest, and assuming the EA is leaving their organisation openly and on good terms with it, would be for them to briefly take a higher salary just as they started hunting for their next job. Personally I think this would be a poetic antidote to this obnoxious practice in the first place, but wider public opinion might disagree with me.</p>\n<p>&#xA0;</p>\n<h3>8) Offering clear financial security to all EA employees</h3>\n<p>Seemingly contrariwise to the above, but bear with me&#x2026;&#xA0;</p>\n<p>EA employees will be more productive if they aren&#x2019;t dealing with financial insecurity, since such insecurity has high costs in both time and mental health.&#xA0;</p>\n<p>According to 80K&#x2019;s <a href=\"https://80000hours.org/2017/11/talent-gaps-survey-2017/\">talent gap survey</a>, even a junior hire is worth about $83,000 per year on average (median - much higher mean) to their EA organisation. If we take this literally, then a) EA organisations could comfortably test the effect of doubling (or more) the offered salaries on the number and quality of applications and perhaps more realistically b) they could afford to offer sufficiently high rates to even their most junior employees that money isn&#x2019;t a substantial limiting factor in their lives.&#xA0;</p>\n<p>What &#x2018;isn&#x2019;t a substantial limiting factor&#x2019; means is obviously fairly vague, but it seems like if any EA is eg spending a lot of time commuting, waiting for dated hardware to run, or eating a lot of cheap unhealthy food or not participating in healthy hobbies, or otherwise losing time or health to save money, then it will impede their productivity. Again, taking the above survey at an admittedly naive face value, it would be worth the average EA org spending up to $830 more per year to increase their junior employees&#x2019; productivity by just 1% (perhaps more if the employee&apos;s productivity increase would compound over their career).</p>\n<p>We should probably be sceptical of such striking survey results - nonetheless, there&#x2019;s room to be more conservative and still see the potential for gain here. In an ideal world, the financial security offered could mostly come from the benefits and insurance discussed above - ie at a ~20% discount.</p>\n<p>Lastly, this reasoning argues only for the <em>option</em>&#xA0;of higher salaries/benefits - many EAs on very low salaries seem perfectly able and willing to get by on them - and only to people who would otherwise be below whatever financial threshold would allow them to stop feeling constrained or anxious in daily life.</p>\n<p>&#xA0;</p>\n<hr>\n<p>&#xA0;</p>\n<p>I&#x2019;m aware that some EA organisations are already implementing some form of these strategies, but they&#x2019;re far from universally adopted. Perhaps this is because they&#x2019;re bad ideas - this was quite an off-the-cuff post - but I haven&#x2019;t really heard substantial discussion of any of them, so let&#x2019;s have it now. And if there&#x2019;s any mileage in the core assumptions, I&#x2019;d hope such discussion will reveal several more ways we can use our almighty collective will.</p>\n<p>&#xA0;</p>\n<hr>\n<p><br><em>Full disclosure - I work for an EA organisation (Founders Pledge), so some of these strategies would potentially benefit me. But hopefully they&#x2019;d benefit FP still more.&#xA0;</em></p>\n<p><em>Thanks to Kirsten Horton and John Halstead for some great&#xA0;feedback on this post.</em></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "Arepo"}}, {"_id": "GhHirpH9uzxKCr3Lx", "title": "New Effective Altruism course syllabus", "postedAt": "2018-01-25T19:25:43.864Z", "htmlBody": "<html><body><p>I&apos;ve developed a new course called&#xA0;&apos;The Psychology of Effective Altruism&apos; (Psych450) that I&apos;m teaching this spring term here at University of New Mexico.&#xA0;</p>\n<p>The syllabus (including an extensive list of required and optional readings and videos) is here:&#xA0;</p>\n<p>https://www.primalpoly.com/s/syllabus-draft-jan24d.docx&#xA0;</p>\n<p>Feel free to&#xA0;borrow any of this material if you teach an EA-related course.</p>\n<p>This is&#xA0;an advanced undergraduate seminar with about 20 students, mostly psychology majors. It was designed to be suitable for diverse students at a large state university.&#xA0;Whereas other EA courses have focused mostly on technical moral philosophy, this is pitched more towards the psychology of EA -- both why it&apos;s appealing, and why it&apos;s difficult.</p>\n<p>The course&apos;s main topics are EA principles, cause prioritization, utilitarian ethics, moral psychology, charity evaluation, global poverty and health, existential risks, AI safety, moral &amp; cognitive enhancement, near-term technologies (robots, Ems, VR), animal sentience &amp; welfare, and career choices.&#xA0;</p>\n<p>&#xA0;</p>\n<p>I&apos;ll probably run this class at least once a year, so would welcome any other suggestions of good readings, videos, exercises, etc. for next time.</p>\n<p>-- Geoffrey</p></body></html>", "user": {"username": "geoffreymiller"}}, {"_id": "ScLHyCY6JCr5FtuiY", "title": "Effective Volunteering", "postedAt": "2018-01-24T16:09:11.731Z", "htmlBody": "<html><body><h2><strong>Summary</strong></h2>\n<h2>&#xA0;</h2>\n<p>What is &#x201C;effective volunteering&#x201D;? A group from EA London met biweekly from September to November 2017 to try to develop a straightforward method for someone to decide where s/he should volunteer. We thought Effective Altruists might want to volunteer for a combination of these reasons: (a) direct impact, (b) personal wellbeing, and/or (c) career capital. We suggest listing several volunteer opportunities that might do well and evaluating them against these criteria, weighted based on your volunteering goals and multiplied by personal fit. A copy of the spreadsheet we used is available here: <a href=\"https://docs.google.com/spreadsheets/d/1tJtJ6lpIjE7Dv5hUo_HCXb8mNGYvDgiQSQYvNfJGJyU/edit?usp=sharing\">https://docs.google.com/spreadsheets/d/1tJtJ6lpIjE7Dv5hUo_HCXb8mNGYvDgiQSQYvNfJGJyU/edit?usp=sharing</a></p>\n<p>The score you get from the spreadsheet can help you narrow down your options. We recommend asking questions and thinking more carefully about your top few options. Based on this project, I am now volunteering for three hours a week with EA London.</p>\n<h2>Introduction</h2>\n<p>Effective Altruism has a complicated relationship with volunteerism. On the one hand, our community recognizes that a good deal of volunteering is useless or even harmful; on the other hand, we have countless examples of valuable projects being completed by volunteers from the Effective Altruism, like running the annual survey or coordinating local groups.</p>\n<p>Unlike donations or careers, Effective Altruism doesn&#x2019;t have clear advice about how to volunteer effectively. I facilitated a group as part of EA London from September to November 2017 to try to develop a straightforward method of finding &#x201C;effective&#x201D; volunteering opportunities.</p>\n<p>Participants were: Mathilde Guittard, Stephen Ayres, Ellie Raison, Enrico Calvense, Victor Damian, and Colin McClure.</p>\n<h2>Goals and Agenda</h2>\n<p>At our first meeting, we defined &#x201C;effective volunteering&#x201D; and planned the next several weeks.</p>\n<p>We defined &#x201C;effective volunteering&#x201D; as &#x201C;using pre-allocated volunteering time to do as much good as possible, using evidence and reason.&#x201D; We recognised that there might be better things I could do with my time than volunteering, such as paid work or skill-building. Despite that, I was still committed to volunteering, and I thought other aspiring Effective Altruists might be as well. For many people, volunteering is an important part of their life and wellbeing, but they&#x2019;re open to changing where they volunteer.</p>\n<p>Everyone who attended the first meeting was already familiar with Effective Altruism. Many had participated in an earlier research project, the Equality and Justice project run by Sam Hilton. Drawing on this experience, we created a plan together for how we would answer the question of the most effective way for me to volunteer.</p>\n<p>The schedule we agreed on:<br> Week 1 - Define goals and make a plan<br> Week 2 - Consider possible priorities and clarify my priorities; make or find a &quot;personal fit&quot; tool and determine what kind of roles I would have a good personal fit with.<br> Week 3 - Determine criteria for evaluating different volunteering opportunities; limit what opportunities we&apos;ll look at (eg based on location).<br> Week 4 - Consider causes I could work on; find (or make!) as many volunteer opportunities for me as possible.<br> Week 5 - Narrow down volunteer opportunities; consider risks and costs of top 3-5.<br> Week 6 - Choose a volunteer opportunity!</p>\n<h2>Priorities and Personal Fit</h2>\n<p>In Week 2, we considered different priorities or goals Effective Altruists might have when volunteering. We identified three goals that Effective Altruists might have for volunteering:</p>\n<p><!-- [if !supportLists]--><span>&#xB7;<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span></span><!--[endif]-->Direct Impact: An Effective Altruist might want to use their unpaid work to make the world a better place.</p>\n<p><!-- [if !supportLists]--><span>&#xB7;<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span></span><!--[endif]-->Career Capital: An Effective Altruist might want to use their volunteering time to develop skills, reputation or relationships that will allow them to do more good with their career.</p>\n<p><!-- [if !supportLists]--><span>&#xB7;<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span></span><!--[endif]-->Self-Care: An Effective Altruist might choose to volunteer because they enjoy volunteering or to improve their mental health.</p>\n<p>Although some Effective Altruists might only focus on one of these (eg volunteering at a local animal shelter as a form of self-care), we hypothesized that many might prefer combining two or all three of these goals (eg building marketing career capital while also having direct impact by editing videos for AMF).</p>\n<p>I decided to weight my goals&#x2014;60% direct impact, 20% career capital, and 20% self-care. In other words, I care 3x as much about the direct impact I could have as I do about the career capital I could gain or the amount it would improve my happiness and wellbeing.</p>\n<p>Next, we tested out different methods of determining personal fit. We paired up and tried giving others advice about their strengths, comparing ourselves to the &#x201C;average person&#x201D; based on past achievements, and comparing ourselves to the &#x201C;average person&#x201D; based on intuition.</p>\n<p>We&#x2019;d advise identifying strengths based on past achievements and asking a friend or colleague for advice to determine &#x201C;personal fit.&#x201D; This might be more worthwhile after you&#x2019;ve listed possible roles.</p>\n<h2>Listing Possible Roles</h2>\n<p>We decided on a means of evaluation (described below) and then listed opportunities.</p>\n<p>I posted in the EA London Facebook group and received a wide range of suggestions. I also sought out opportunities that I thought might do well on a particular metric&#x2014;for example, I sought out a tutoring opportunity because I&#x2019;m a qualified teacher, and I asked Effective Altruism London about high impact volunteer opportunities.</p>\n<p>In the end, we listed 17 possible roles at 12 charities. We evaluated 15 roles, because we didn&#x2019;t have&#xA0; enough information about two of the suggestions.</p>\n<p>We think an independent project like research or blogging could have been high-impact, but we didn&#x2019;t think of any projects to list. If I were running the project again, I would spend more time thinking about independent projects I could run.</p>\n<h2>Short-Listing Opportunities</h2>\n<p>We created a <a href=\"https://docs.google.com/spreadsheets/d/1tJtJ6lpIjE7Dv5hUo_HCXb8mNGYvDgiQSQYvNfJGJyU/edit#gid=1264761950\">spreadsheet</a> to help me evaluate opportunities based on the metrics I cared about. We rated Self Care, Direct Impact and Career Capital on a scale from 1 (not good) to 5 (very good). We rate Personal Fit as 1 (no better than average), 2 (I have some advantage in this role), or 3 (better than most people I can think of).</p>\n<p>I filled in my thoughts about how much I would enjoy each role, which I used as a proxy for self care; how much I thought each role could improve my career prospects; and how replaceable I would be.</p>\n<p>Impact was more difficult. I filled in my best guess; then Colin and Ellie filled in their best guess about the impact of each role without seeing my answers. Most of our &#x201C;impact&#x201D; ratings were identical, and for the few that were different, we discussed which rating to include in the final decision.</p>\n<p>Our spreadsheet uses a formula inspired by 80,000 Hours:</p>\n<p>(Weighted Self-Care + Weighted Impact + Weighted Career Capital) x Personal Fit = Total Score</p>\n<p>After checking the answers to see if they lined up with our intuitions, we decided to shortlist the three highest-scoring roles and think about them more carefully. All three of the top-scoring roles were with EA London:</p>\n<p><!-- [if !supportLists]-->1.<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><!--[endif]-->Giving What We Can Pledge Drive Coordinator</p>\n<p><!-- [if !supportLists]-->2.<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><!--[endif]-->Volunteer Coordinator to encourage other EA London members to volunteer effectively</p>\n<p><!-- [if !supportLists]-->3.<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><!--[endif]-->Diversity and Inclusion Advisor to increase diversity of viewpoints and demographics within EA London</p>\n<h2>Choosing a Role</h2>\n<p>For our final week, we invited David Nash to consult because he&#x2019;s on staff with EA London and would supervise me in whichever role the group chose. He suggested a fourth opportunity&#x2014;that I could run a group for Londoners interested in a career in artificial intelligence strategy and policy.</p>\n<p>I tried to facilitate the conversation without influencing it too much, as I was aware the group was very sensitive to what I wanted and I had promised them that they could decide.</p>\n<p>First, the group eliminated the pledge drive coordinator role because I haven&#x2019;t yet taken the pledge. Then they eliminated the volunteer coordinator. They thought that most of the benefit of the volunteer coordinator would be me conversing with people looking for volunteering opportunities, and I would likely do that anyway. They struggled to decide whether I should host the AI group or work on improving diversity, so while I was out of the room they decided to ask me to split my time between both.</p>\n<p>As of December, I have been splitting three hours a week between setting up an AI Strategy and Policy group in London and making EA London more attractive to a more diverse range of individuals. I&#x2019;ll continue until May at least.</p>\n<h2>Suggestions for Effective Volunteering</h2>\n<p><!-- [if !supportLists]--><span>&#xB7;<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span></span><!--[endif]-->Consider how you&#x2019;d like to weight Direct Impact, Self Care, Career Capital, and any other criteria you&#x2019;d like to focus on.</p>\n<p><!-- [if !supportLists]--><span>&#xB7;<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span></span><!--[endif]-->Identify your personal fit/comparative advantage by looking at past achievements and asking a friend or colleague.</p>\n<p><!-- [if !supportLists]--><span>&#xB7;<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span></span><!--[endif]-->Make a list of a wide range of opportunities.</p>\n<p><!-- [if !supportLists]--><span>o<span>&#xA0;&#xA0; </span></span><!--[endif]-->Seek out opportunities that will do well on specific criteria.</p>\n<p><!-- [if !supportLists]--><span>o<span>&#xA0;&#xA0; </span></span><!--[endif]-->Ask around about opportunities.</p>\n<p><!-- [if !supportLists]--><span>o<span>&#xA0;&#xA0; </span></span><!--[endif]-->Consider independent projects that may be high-impact.</p>\n<p><!-- [if !supportLists]--><span>&#xB7;<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span></span><!--[endif]-->Make a copy of our <a href=\"https://docs.google.com/spreadsheets/d/1tJtJ6lpIjE7Dv5hUo_HCXb8mNGYvDgiQSQYvNfJGJyU/edit#gid=1264761950\">spreadsheet</a> and use it to narrow down your options.</p>\n<p><!-- [if !supportLists]--><span>&#xB7;<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span></span><!--[endif]-->Create a short-list of the best opportunities. (These might not be the highest-scoring opportunities on the spreadsheet.)</p>\n<p><!-- [if !supportLists]--><span>&#xB7;<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span></span><!--[endif]-->Try to get more information about your top options eg by talking to someone you&#x2019;d be working with.</p>\n<p><!-- [if !supportLists]--><span>&#xB7;<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span></span><!--[endif]-->Make a consistent, long-term commitment. Volunteering can be net negative if it wastes the time of a staff member or another volunteer at a high-impact charity.</p></body></html>", "user": {"username": "Khorton"}}, {"_id": "CQqxXFrY2dqbkCWKR", "title": "Ongoing lawsuit naming \"future generations\" as plaintiffs; advice sought for how to investigate", "postedAt": "2018-01-23T22:22:08.173Z", "htmlBody": "<html><body><p>Juliana vs. US is an ongoing lawsuit. Notably, it names &quot;FUTURE GENERATIONS&quot; as plaintiffs in the case.<br><br>I don&apos;t know much law, but I hear precedents are important, and so maybe EA&apos;s concerned about the long-term future should be especially interested in ensuring that this case sets a good one.<br><br>https://www.ourchildrenstrust.org/us/federal-lawsuit/</p>\n<p>I heard about this from someone I met yesterday who studies this case. I&apos;m going to meet with him someday soon and ask more questions. <strong>What questions should I ask?</strong><br><br>So far I intend to follow the importance/neglectedness/tractability framework and ask questions like &quot;What is the budget of this organization? Is there no other precedent, are they really the first case of this kind? Is it too late to change anything about their approach, or are there still decisions that need to be made?&quot; But I think people with more legal background than me (I have zero) could suggest better questions to ask...<br><br>Also, I&apos;m interested in hearing whether or not I&apos;ve completely misjudged the expected value of looking into this. Maybe this sort of thing is actually not that important or tractable?</p>\n<p>Thanks in advance.<br><br><br>&#xA0;</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "kokotajlod"}}, {"_id": "84utmyNmRqCS9Ytka", "title": "Could I have some more systemic change, please, sir?", "postedAt": "2018-01-22T16:26:30.577Z", "htmlBody": "<html><body><p><span>In this short post I outline a very simple (new?) way of estimating&#xA0;the expected value of systemic changes when you are very uncertain of how much they&apos;d cost. It seems to have the result of making systemic changes look much more viable than they otherwise would.</span></p>\n<p>Suppose we want to do something about poverty, and we&apos;re torn between Give Directly, an &apos;atomic&apos; intervention (prejoratively, &apos;sticking plaster&apos;) and international legal reform, a &apos;systemic&apos; intervention. I&apos;m not going to get tied down to any specifics of the latter, because it&apos;s only for illustration, but the sort of thing I have in mind are those mentioned by&#xA0;<a href=\"http://www.princeton.edu/rpds/seminars/pdfs/pogge_assistingpoor.pdf\">Thomas Pogge</a>&#xA0;- such as resource privilege (p11), and borrowing privilege (p13) - and <a href=\"https://www.ft.com/content/3d478af6-e0a0-11e5-8d9b-e88a2a889797\">Leif Weinar in Blood Oil</a>; we could also think about try to reduce&#xA0;<a href=\"https://www.theguardian.com/global-development/poverty-matters/2011/may/24/american-cotton-subsidies-illegal-obama-must-act\">subsidies to rich-world farmers</a>, such as the EU&apos;s <a href=\"https://ec.europa.eu/agriculture/sites/agriculture/files/cap-post-2013/graphs/graph1_en.pdf\">Common Agricultural Policy</a>, which make it much harder for poor-world farmers to compete.</p>\n<p><span>Let&apos;s say we know how cost-effective Give Directly are. Asssume</span><span>&#xA0;Give Directly spends $730 to take one person out of poverty for a year ($2x365 days). Suppose this increases someone&#x2019;s happiness by 0.5 units on a&#xA0;-1 to +1&#xA0;happiness scale. 0= death/neutral/unconsciousness, 1 = &#x2018;max&#x2019; happiness, where maximum is maximum average sustainable happiness, or something. So the cost per &#x2018;happiness adjusted life year&#x2019; (HALY) is $1,460.</span></p>\n<p><span>Now, suppose we&#x2019;re wondering how&#xA0;cost-effective it would be to run a campaign that&apos;s trying to bring about some of the systemic changes I just mentioned.&#xA0;This is tricky to do, because we really don&apos;t know what comparisons to draw on (which other campaigns are similar? how similar are they to ours?). It feels like we&apos;re engaged in whimsical speculation when we create the numbers.</span></p>\n<p><span>However, we can do things the other way around and create <em>ceiling-cost estimate</em>. We know how cost-effective Give Directly are, so we can ask: what&apos;s the maximum we could we expect to spend on the systemic change campaign and it still turn out to be <em>as cost-effective</em>?</span></p>\n<p>Some numbers: suppose there are&#xA0;<span>1bn people in poverty, and if we change the international system somehow (to be specified) it increases their happiness by 0.1 units a year, i.e we&apos;re assuming this has only 1/5<sup>th</sup> of the impact on each individual as Give Directly does. Therefore, if we pulled it off, it would do 100 million HALYs worth of good (i.e. 0.1 x 1bn). Give Directly produces 1 HALY for $1,460 (from above assumption). Therefore, if we spent&#xA0;</span><span><em>anything less </em>than<em>&#xA0;$146 billion</em> on a <em>successful</em> campaign/lobbying group, then it would be&#xA0;<em>more cost effective</em> to do that than give money to Give Directly directly (100m x 1,460; see maths in figure 1)</span></p>\n<table>\n<tbody>\n<tr>\n<td>&#xA0;</td>\n<td>\n<p><span>Cost of intervention</span></p>\n</td>\n<td>\n<p><span>People effected</span></p>\n</td>\n<td>\n<p><span>Year of happiness</span></p>\n</td>\n<td>\n<p><span>happiness increase/yr</span></p>\n</td>\n<td>\n<p><span>HALYS</span></p>\n</td>\n<td>\n<p><span>Cost per HALY</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Give Directly</span></p>\n</td>\n<td>\n<p><span>730.00</span></p>\n</td>\n<td>\n<p><span>1.00</span></p>\n</td>\n<td>\n<p><span>1.00</span></p>\n</td>\n<td>\n<p><span>0.50</span></p>\n</td>\n<td>\n<p><span>0.50</span></p>\n</td>\n<td>\n<p><span>$1,460.00</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>International reform</span></p>\n</td>\n<td>\n<p><span>146,000,000,000.00</span></p>\n</td>\n<td>\n<p><span>1,000,000,000.00</span></p>\n</td>\n<td>\n<p><span>1.00</span></p>\n</td>\n<td>\n<p><span>0.10</span></p>\n</td>\n<td>\n<p><span>100,000,000.00</span></p>\n</td>\n<td>\n<p><span>$1,460.00</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Int reform (more realistic)</span></p>\n</td>\n<td>\n<p><span>146,000,000,000.00</span></p>\n</td>\n<td>\n<p><span>1,000,000,000.00</span></p>\n</td>\n<td>\n<p><span>10.00</span></p>\n</td>\n<td>\n<p><span>0.01</span></p>\n</td>\n<td>\n<p><span>100,000,000.00</span></p>\n</td>\n<td>\n<p><span>$1,460.00</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n<p>Figure 1.</p>\n<p>What do we do with this $146bn figure we&apos;ve just made up?&#xA0;We ask ourselves if we believe the expected cost for the systemic change to be successful would be <em>higher or lower</em> than that number. This seems like a much easier, back-of-the-mental envelope subjective judgement to make.</p>\n<p>Suppose we fund this campaign and it costs $160bn before it&apos;s successful, rather than $146bn? Well, then our campaign turned out to be less cost-effective than Give Directly, but not by a lot. Suppose we&#xA0;reflect and conclude it would, on expectation, cost&#xA0;$100m for systemic change success.&#xA0;Then the systemic change would be 1,460 times more cost-effective than Give Directly.&#xA0;</p>\n<p>The above calculation was unrealistic in a couple of ways. First, to keep things simpler, I assumed the international legal reform would last for one year. Actually, that&apos;s unlikely. The reform would have an ongoing, annual effect. However, we might think the reform would have happened anyway, so the correct counterfactual is how many years earlier we make it occur. Let&apos;s guess it happens 10 years before it otherwise would. Second, I also assumed it would have an effect of 0.1 HALYs per person per year. This might be high, so let&apos;s assume it has a 1/10th of that impact. This scenario is represented in line three. Notice the ceiling cost is still the same, because that&apos;s how I rigged it. The upshot: even very expensive, long-term systemic changes with even a minor impact can look believably more cost-effective than atomic interventions.&#xA0;</p>\n<p>Does this mean EAs should abandon all atomic interventions in favour of systemic ones? Not so fast.</p>\n<p>First, you&apos;d need a plausible story of how you could spend the money to bring about the systemic change. I haven&apos;t provided more than a skeleton story here. But when we see how promising systemic changes <em>could be,&#xA0;</em>that should cause us to look a bit harder for potential systemic interventions. (For those interested, I discuss what seems to be a more plausible candidate for a systemic change campaign,&#xA0;drug policy reform, <a href=\"/ea/1dj/high_time_for_drug_policy_reform_part_44/\">in this post</a>. I use poverty here because it requires less explanation.)</p>\n<p>Second, you might think it would cost $146bn, on expectation, for success, but that a much smaller campaign than that would be <em>proportionally</em> much less likely to succeeed; a token effort would be a waste. For instance, the expected value of a campaign with $1.46bn of funding&#xA0;could be&#xA0;<em>less than 100 times</em> that of one with $146bn, i.e. 100 times its size. The expected value of additional resources may well not be constant; there might be <em>increasing marginal returns</em>. So donors would need to think what the effect of <em>their</em> extra contribution would be. Donors might realise they could do much more good together, targeting the same project, that they do targeting different projects. This would be a reason to co-ordinate.&#xA0;Curiously, this seems to work the other way around for atomic interventions. It&apos;s hard to believe, if you tried to give Give Directly $146bn, they wouldn&apos;t run into diminishing marginal returns.</p>\n<p>That said,&#xA0;these very simple calculations suggest EAs should think much harder about systemic interventions; at least, if you want to fund an atomic intervention, it would be an oversight to assume&#xA0;the&#xA0;systemic alternative won&apos;t be more cost-effective if you haven&apos;t even drawn up some simple cost-effectiveness guesses.</p>\n<p>EAs already tend&#xA0;to use expected value reasoning, and accept that small chances of very high value events are worth taking seriously, e.g. when it comes to extinction risk.&#xA0;There doesn&#x2019;t seem to be anything particularly suspicious about systemic changes <em>per se. </em>It&apos;s just a small change of affecting loads of people. Everyone should accept the abolition of slavery, which affected millions, was a substantial systemic change that had a large positive impact. It&apos;s at least hypothetically possible funding that, at the time, would have been the best thing to do.</p>\n<p>One result of this way of thinking is that systemic changes now look more effective, in general, that atomic ones. This could well be true. Certainly, one criticism of EA is that&apos;s it&apos;s ignored systemic changes.&#xA0;The next step would be comparing various types of systemic changes against each other to see which look the most plausible.</p>\n<p>(I&apos;m grateful to Will Rooney for reading this post in advance and providing some comments)</p></body></html>", "user": {"username": "MichaelPlant"}}, {"_id": "n6dtnP5babfNaz2bW", "title": "69 things that might be pretty effective to fund", "postedAt": "2018-01-21T22:47:32.094Z", "htmlBody": "<p><span>Opinions are mine. If you comment on one of these items, please quote the item in full at the start of your comment. Other people can then upvote and downvote other comments depending on whether you think it\u2019s worthy of funding.</span></p>\n<p><span>Below I list 69 pretty effective funding opportunities. These funding opportunities might be able to absorb a fair bit of money effectively on the margin (on the orders of tens to hundreds of thousands of dollars), </span><span>because even though there are diminishing returns to scale as funding increases, at first there are increasing returns to scale and constant returns to scale:</span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995875/mirroredImages/n6dtnP5babfNaz2bW/ymlrukicnx4bebfjmupt.png\"></span></p>\n<p><span>So the items below do not have funding gaps in the 10s of millions of dollars like some global development charities (in particular GiveDirectly) and might hit diminishing returns relatively quickly, especially because they might already received some funding. Yet they might have room for more funding and be effective if one were to give them a relatively small \u2018top up\u2019.</span></p>\n<p><span>These items are all more on the \u2018high risk / high reward\u2019 side (cf. </span><a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\"><span>hits-based giving</span></a><span>). It might be that good funding opportunities such as these can be found in many fields. The variance of the effectiveness within different causes is high, as is the case in </span><a href=\"https://www.cgdev.org/publication/moral-imperative-toward-cost-effectiveness-global-health\"><span>global health, where some health interventions seem to be many times more effective than others</span></a><span>. But it can also be true that cross cause variance effectiveness is lower than people might think and that the distributions of effectiveness of different causes are overlapping. In other words, it might be that there are some climate change interventions that are more effective to fund than than some biosecurity interventions, even though we might agree that </span><a href=\"https://80000hours.org/articles/cause-selection/\"><span>climate change is less neglected than biosecurity</span></a><span> and generally scores worse on the scale, neglectedness, solvability framework.</span></p>\n<p><span>Analogously, in the for profit world, even though investors such as YCombinator are often particularly interested </span><a href=\"https://www.ycombinator.com/rfs/\"><span>in investing in specific vertical markets that they find particularly promising</span></a><span>, they often don\u2019t restrict themselves to these markets and generally try to fund a small amount of money in many different startups that are trying to disrupt any wide variety verticals. Another finance analogy: investing in cause areas that are most neglected, such as global health or AI risk, is probably akin to investing in an index fund which tracks the biggest companies within a particular market, in the sense that it is a pretty safe bet, whereas finding good funding opportunities across causes might be more similar to active investing, which requires more research.</span></p>\n<p><span>Something one could do on each of these items is a </span><a href=\"https://en.wikipedia.org/wiki/Reversal_test\"><span>reversal test</span></a><span> to sanity check against any status quo bias. For example, you could ask yourself: \u2018If the organisation or project already had an extra $100,000, would I take it away from them and give it to, say, GiveDirectly (which is used as a benchmark in development and charitable giving)?\u2019 and \u2018Would I take away money from GiveDirectly on the margin (their last $100,000 for cash transfers) to give it to these projects?\u2019. Maybe this method can also guard against </span><a href=\"https://en.wikipedia.org/wiki/Precision_bias\"><span>precision bias</span></a><span> of conducting ever more elaborate, explicit cost-effectiveness modelling (which the following items lack), while neglecting </span><a href=\"https://concepts.effectivealtruism.org/concepts/the-importance-of-crucial-considerations/\"><span>crucial considerations</span></a><span>. Using this sort of reversal test myself, in many cases it becomes self-evidently clear to me that those items should be prioritized over direct global development interventions.</span></p>\n<p><span>Finally, any of the </span><a href=\"https://en.wikipedia.org/wiki/Ultra_high-net-worth_individual\"><span>world\u2019s 2,100 billionaires</span></a><span> could give each of these funding opportunities $1 million with the interest on their wealth in any given year and not touch the principal of their net worth. Any of the </span><a href=\"https://en.wikipedia.org/wiki/Ultra_high-net-worth_individual\"><span>world\u2019s 200,000 ultra high net worth individuals (+$30 million)</span></a><span> could give each of these funding opportunities $250,000 and they would have still have at least half their wealth left. </span><span>Some argue </span><a href=\"https://en.wikipedia.org/wiki/Capital_in_the_Twenty-First_Century#Contents\"><span>that wealth inequality will increase</span></a><span> and so in the future there might be even more people who could fund projects such as these. It might be useful to have a wide variety of things to pitch to particular high net worth individuals\u2019 interests.</span></p>\n<p><span>Suffice to say, I don\u2019t have a strong sense if each and every one of these items should really be funded, because I have not vetted them thoroughly, but I hope that they might serve as an inspiration for further research. The items are in no particular order.</span></p>\n<p>&nbsp;</p>\n<ol>\n<li>\n<p><span>Climate change:</span> <span>&nbsp;</span><a href=\"http://priceofoil.org\"><span>\u2019The Price of Oil\u2019 works to fade out fossil fuel subsidies</span></a><span> which </span><a href=\"https://www.wider.unu.edu/sites/default/files/Publications/Working-paper/wp2017-174.pdf\"><span>are bad and in the billions.</span></a> <span>It also seems tractable given that conservatives usually don\u2019t like subsidies. Also fund the </span><a href=\"http://www.sei-us.org/\"><span>Stockholm Environment Institute</span></a><span>\u2019s researchers and authors of </span><a href=\"http://www.nber.org/papers/w22105\"><span>this</span></a><span> and this </span><a href=\"https://www.nature.com/articles/s41560-017-0009-8\"><span>paper on the same topic</span></a><span> to continue their work.</span></p>\n</li>\n<li>\n<p><span>EA community building:</span><span> The </span><span>EA Norway</span><span> group \u201c</span><a href=\"https://www.stiftelseneffekt.no/\"><span>Stiftelsen effekt</span></a><span>\u201d. Norway is the richest country on earth and </span><a href=\"/ea/1js/project_report_on_the_potential_of_norwegian/\"><span>so their fundraising ratio for GiveWell recommended charities is very promising</span></a><span>. Also fund </span><a href=\"http://effectivealtruism.dk\"><span>Effective Altruism Denmark</span></a><span>.</span></p>\n</li>\n<li>\n<p><span>Global catastrophic risks: </span><span>Fund the authors of </span><a href=\"http://onlinelibrary.wiley.com/doi/10.1002/2017SW001691/full\"><span>this paper</span></a><span> on geomagnetic storms, </span><a href=\"https://www.openphilanthropy.org/blog/geomagnetic-storms\"><span>which are dangerous</span></a><span>. Lloyd's </span><a href=\"https://www.lloyds.com/~/media/lloyds/reports/emerging-risk-reports/solar-storm-risk-to-the-north-american-electric-grid.pdf\"><span>estimated the economic cost of a solar storm to be 0.6 - 2.6 Trillion USD</span></a><span> in the United States alone. Also, fund the researchers of this paper on </span><a href=\"https://link.springer.com/chapter/10.1007/978-3-319-54364-2_17\"><span>cosmic hazards and planetary defense</span></a><span> to continue their research into international coordinative efforts as well as legal and institutional mechanisms to protect humanity against cosmic hazards such as severe space weather and asteroid impacts. </span><span>Also, fund the </span><a href=\"http://www.sciencedirect.com/science/article/pii/S0094576513004670\"><span>researchers</span></a><span> of this article to do more research on the probability of asteroid impacts. Finally, fund the </span><a href=\"https://b612foundation.org/our-mission/\"><span>B612 foundation</span></a><span> to work more </span><a href=\"https://spectrum.ieee.org/aerospace/astrophysics/how-to-stop-killer-asteroids\"><span>research and advocacy</span></a><span> on asteroid detection and deflection. </span></p>\n</li>\n<li>\n<p><span>Animal welfare: </span><span>Get a \u2018</span><a href=\"http://www.econtalk.org/archives/2011/08/odonohoe_on_pot.html\"><span>Brendan O'Donohoe\u2019-type</span></a><span> to work at Quorn, a mycoprotein based meat replacement company. Quorn is now launching in the United States after posting \u201c</span><a href=\"https://www.foodbev.com/news/quorn-foods-unveils-refrigerated-meat-alternative-line-us/?utm_content=buffer918ac&amp;utm_medium=social&amp;utm_source=linkedin.com&amp;utm_campaign=buffer\"><span>19% growth globally in the first six months of the year, with 15% growth in its home market of the UK. \u201c</span></a><span> They are investing \u201c</span><a href=\"https://www.foodbev.com/news/quorn-foods-unveils-refrigerated-meat-alternative-line-us/?utm_content=buffer918ac&amp;utm_medium=social&amp;utm_source=linkedin.com&amp;utm_campaign=buffer\"><span>\u00a3150 million in its UK manufacturing facility, it is aiming to become a \u2018billion-dollar brand\u2019 by 2020</span></a><span>\u201d. The brand was acquired by Monde Nissin in 2015 for \u00a3550 million. One could also fund the authors of </span><a href=\"http://www.sciencenewsline.com/news/2017101103430082.html\"><span>this study showing that Quorn is on par with meat-based protein sources</span></a><span> to conduct more research on Quorn or other meat based alternatives.</span></p>\n</li>\n<li>\n<p><span>AI Safety: </span><span>Try to get </span><a href=\"http://www.kyunghyuncho.me/\"><span>Kyunghyun Cho</span></a><span> to do work on AI safety research. Also</span><span>, look into funding more </span><a href=\"https://aiwatch.issarice.com/\"><span>researchers and orgs collated on this page</span></a><span>. </span><span>These </span><a href=\"http://ieeexplore.ieee.org/abstract/document/8090235/\"><span>AI safety researchers from</span></a><span> Turkey and Malaysia might be more cost-effective to fund than US/UK researchers.</span></p>\n</li>\n<li>\n<p><span>US politics:</span> <a href=\"https://en.wikipedia.org/wiki/Vote.org\"><span>Vote.org</span></a><span>: </span><span>Uses voter registration to increase voter turnout in the United States to increase democratic participation. \u201cVote.org, a nonpartisan group, </span><a href=\"https://www.pbs.org/newshour/politics/the-months-long-campaign-that-drove-historic-black-turnout-in-alabama\"><span>spent $658,000 in the final four weeks of the [Alabama] race in a targeted effort to increase black turnout</span></a><span>, according to the group\u2019s founder, Debra Cleaver.\u201d </span></p>\n</li>\n<li>\n<p><span>Global health: </span><a href=\"http://www.sciencedirect.com/science/article/pii/S0264410X16308490\"><span>According to a Gates funded study</span></a><span>, GAVI, the vaccine alliance, the major international vaccine funder has a $7.6 billion funding gap over 2016\u201320 across 94 countries. The </span><a href=\"http://cdrwww.who.int/bulletin/volumes/95/9/16-178475.pdf\"><span>WHO estimates that vaccinations might prevent 14 million deaths from 2011\u20132020</span></a><span>. Another study estimates the </span><a href=\"https://www.sciencedirect.com/science/article/pii/S0264410X17317231\"><span>cost per death averted of Rotavirus vaccinations in the next years is approximately US$7000</span></a><span>. </span><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5012773/\"><span>Vaccination gaps against yellow fever can also lead to pandemic outbreaks</span></a><span>. One could look into funding the </span><a href=\"https://www.taskforce.org/center-for-vaccine-equity\"><span>Center for Vaccine Advocacy</span></a><span> to lobby for increased funding support for vaccines.</span></p>\n</li>\n<li>\n<p><span>Global health: </span><span>Fund the</span><a href=\"http://rspb.royalsocietypublishing.org/content/283/1841/20161903.long\"><span> authors of this study</span></a><span> to conduct a workshop using </span><a href=\"http://rspb.royalsocietypublishing.org/content/283/1841/20161903.long\"><span>weakly transmissible vaccines</span></a><span> to eradicate infectious diseases (similar to this </span><a href=\"http://www.openphilanthropy.org/focus/scientific-research/miscellaneous/foundation-national-institutes-health-working-group\"><span>workshop on gene drives</span></a><span> that was funded by OpenPhil).</span></p>\n</li>\n<li>\n<p><span>Global health: </span><a href=\"https://www.gaffi.org/\"><span>Global Action Fund for Fungal Infections:</span></a> <a href=\"http://rstb.royalsocietypublishing.org/content/371/1709/20150468\"><span>Minimizing fungal disease deaths has been estimated to reduce annual AIDS deaths below 500,000 by 2020</span></a><span>. Fungal infections deaths in AIDS were estimated at more than 700,000 deaths (47%) annually till 2020.. It has been argued that rapid diagnostic tools and antifungal agents are available for these diseases and would likely have a major impact in reducing deaths. </span><span>The cost per life saved by screening has been estimated at $20\u2013140 (this might be an overestimate, but even if off it\u2019s off by an order of magnitude, it\u2019s still worth looking into). One could also do some impact investing and see whether one </span><a href=\"http://www.f2g.com/antifungal-market/\"><span>can fund this company that works on antifungals</span></a><span>. </span></p>\n</li>\n<li>\n<p><span>Global health: </span><span>Antifungal resistance: antifungals, </span><a href=\"https://www.theguardian.com/society/2016/aug/27/millions-at-risk-as-deadly-fungal-infections-acquire-drug-resistance\"><span>just like antibiotics, can stop working due to antifungal resistance, which puts a lot of people at risk</span></a><span>. Give the </span><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4791369/\"><span>author of this study</span></a><span> a grant to look into it more. </span></p>\n</li>\n<li>\n<p><span>Global development:</span> <a href=\"http://kieranholmes.com/\"><span>Kieran Holmes</span></a><span>, is a tax consultant, who advises developing country governments to get better at tax collection. He has improved Burundi's </span><a href=\"http://www.econtalk.org/archives/2017/11/simeon_djankov.html\"><span>doing good business indicators</span></a><span>. He has also </span><a href=\"https://www.economist.com/news/finance-and-economics/21657433-poor-countries-need-get-better-raising-tax-and-multinational-firms-need\"><span>helped to increase annual tax revenue in Rwanda by 6.5 times after automating the collection process</span></a><span>, which reduced errors and opportunities for fraud. This is </span><a href=\"http://www.econtalk.org/archives/2017/05/lant_pritchett_1.html\"><span>likely to reduce poverty.</span></a><span> One could also look into </span><a href=\"https://pefa.org/research-impact\"><span>funding this initiative</span></a><span>.</span></p>\n</li>\n<li>\n<p><span>Global development: </span><span>Fund the Indian Council for Research on International Economic Relations (ICRIER). \u201cThere is a narrative in which Ford Foundation, a global philanthropy provides some millions of dollars of funding that play some role in creating a think tank [ICRIER] that itself then plays some role in providing the conditions in which good policy choices are made that then results in the creation of $3.6 trillion in additional output of Indians.\u201d [</span><a href=\"https://www.cgdev.org/publication/perils-partial-attribution-lets-all-play-team-development\"><span>Source</span></a><span>]. </span></p>\n</li>\n<li>\n<p><span>Global catastrophic risks: </span><span>Fund </span><a href=\"https://en.m.wikipedia.org/wiki/Coalition_for_Epidemic_Preparedness_Innovations\"><span>CEPI</span></a><span>, the Coalition for Epidemic Preparedness Innovations.</span></p>\n</li>\n<li>\n<p><span>Global catastrophic risks:</span><span> Fund the scientists working at the </span><a href=\"http://www.centerforhealthsecurity.org/our-staff/profiles/inglesby/\"><span>Johns Hopkins Center for Health Security</span></a><span> here working on pandemic preparedness. Also </span><a href=\"http://cisac.fsi.stanford.edu/people/megan_palmer\"><span>these</span></a> <a href=\"https://profiles.stanford.edu/david-relman\"><span>scientists</span></a> <a href=\"http://www.pnas.org/content/114/42/11006.short\"><span>who</span></a> <a href=\"http://online.liebertpub.com/doi/abs/10.1089/hs.2017.0081\"><span>are</span></a><span> also working on risks from synthetic biology.</span></p>\n</li>\n<li>\n<p><span>Global health: </span><a href=\"http://www.snakebiteinitiative.org/?page_id=791\"><span>Snakebite initiative. </span></a><span>Snake bites are </span><a href=\"http://link.springer.com/referenceworkentry/10.1007/978-3-319-20790-2_87-1\"><span>a big problem</span></a><span> and might have a </span><a href=\"http://test.snakebiteinitiative.org/wp-content/uploads/2012/06/Global-burden-of-snakebite.pdf\"><span>burden of disease</span></a><span> as high as 94,000 deaths and more than 1 million bites. Preventing them might be very cost-effective according to </span><a href=\"http://journals.plos.org/plosntds/article?id=10.1371/journal.pntd.0004568\"><span>one study</span></a><span>: \"The cost/death averted for the 16 countries of interest ranged from $1,997 in Guinea Bissau to $6,205 for Liberia and Sierra Leone.\u201d Key African anti-venom has been reported to </span><a href=\"http://globalhealthnow.us3.list-manage2.com/track/click?u=0a43ad874dbe00d8f0545cfef&amp;id=af3baf302f&amp;e=b264e2cc7c\"><span>permanently run out</span></a><span>, people say </span><a href=\"http://www.scidev.net/global/medicine/news/WHO-snakebites-africa.html?utm_medium=email&amp;utm_source=SciDevNewsletter&amp;utm_campaign=international%20SciDev.Net%20update%3A%2013%20June%202016\"><span>urgent action is needed to tackle snakebites </span></a><span>, and </span><a href=\"https://www.nature.com/news/vipers-mambas-and-taipans-the-escalating-health-crisis-over-snakebites-1.20495\"><span>Nature calls it an 'escalating health crisis'</span></a><span>, so it might be quite neglected. </span><a href=\"http://www.telegraph.co.uk/news/health/11850111/Charity-warns-world-is-running-out-of-snake-anti-venom.html\"><span>M\u00e9decins Sans Fronti\u00e8res say they\u2019re running out of anti venom.</span></a><span> Give the authors of this paper titled \u2018</span><a href=\"http://journals.plos.org/plosntds/article?id=10.1371/journal.pntd.0004565\"><span>\"A Simple and Novel Strategy for the Production of a Pan-specific Antiserum against Elapid Snakes of Asia\"</span></a><span> a grant for further research</span></p>\n</li>\n<li>\n<p><span>Suffering focus: </span><span>Fund the researchers studying this new </span><a href=\"http://science.sciencemag.org/content/355/6328/966\"><span>pain killer</span></a><span> to speed up further research</span></p>\n</li>\n<li>\n<p><span>Global catastrophic risks: </span><span>Fund </span><a href=\"http://link.springer.com/search?sortOrder=newestFirst&amp;facet-content-type=Article&amp;facet-journal-id=13753\"><span>a special issue</span></a><span> of the International Journal of Disaster Risk Science on GCR preparedness</span></p>\n</li>\n<li>\n<p><span>Global Health: </span><a href=\"http://www.ajog.org/article/S0002-9378(17)31439-4/fulltext\"><span>Delayed umbilical cord clamping might save newborns lives</span></a><span> and prevent </span><a href=\"http://www.bmj.com/content/343/bmj.d7157\"><span>anemia</span></a><span>. Fund researchers to do a risk assessment of whether this should be a widely promoted health message. Depending on the outcome, commission </span><a href=\"http://www.developmentmedia.net/\"><span>Development Media International</span></a><span> or Charity Science Health to spread the message.</span></p>\n</li>\n<li>\n<p><span>Global catastrophic risks: </span><span>Supervolcanoes:</span> <span>Fund Brian Wilcox</span> <span>for more academic work on the feasibility of a geothermal plant on the yellowstone that could </span><a href=\"http://www.thinkgeoenergy.com/cooling-down-a-supervolcano-and-generate-geothermal-power-a-possible-win-win/?jo\"><span>you prevent a future supervolcano eruption</span></a><span>. </span></p>\n</li>\n<li>\n<p><span>Global development: </span><span>There\u2019s much talk about open borders, but few people talk about within country migration. A </span><a href=\"http://www.un.org/en/development/desa/population/publications/policy/world-population-policies-2013.shtml\"><span>UN study found that 80% of countries had policies</span></a><span> to reduce rural-urban migration (up from only 38% in 1996). It is also more pronounced in poorer countries: 88% of the least developed countries reported policies to reduce migration to urban areas. Rural to urban migration is a crucial part in economic development for any country in human history. Some studies run simulation that suggest that \u201cacross the period 2011\u201322, if the labour participation rate of the non-agricultural sector increases by 1 percent per year, China will increase its potential growth rate by 0.88 percent (Cai and Lu </span><a href=\"http://onlinelibrary.wiley.com/doi/10.1111/1467-8462.12248/full#aere12248-bib-0004\"><span>2013</span></a><span>). [and] the most important measures to improve labour force participation include reform of the household registration system and improvements in urban public goods and services, both of which would help to stabilise and expand the employment of migrant workers in the urban economy and non-agricultural industries. [...] Traditional factor-driven growth can still contribute if reform increases labour force participation, most significantly through easing restrictions on rural\u2013urban migration through reform of the household registration system and other means.\u201d [</span><a href=\"http://onlinelibrary.wiley.com/doi/10.1111/1467-8462.12248/full\"><span>Study</span></a><span>] &nbsp;One could </span><a href=\"https://www.sciencedirect.com/science/article/pii/S0305750X13002258\"><span>fund more research on this topic</span></a><span> or </span><a href=\"https://www.theguardian.com/world/2014/jul/31/china-reform-hukou-migrant-workers\"><span>fund Fei-ling Wang</span></a><span>, who advocates for incremental reform in China and might be willing to convene a conference on lessons for other countries. </span><a href=\"https://mobile.nytimes.com/2017/12/11/world/asia/china-beijing-migrants-tech.html?action=click&amp;module=Editors%E2%80%99%20Picks&amp;pgtype=Homepage\"><span>More on this topic</span></a><span>.</span></p>\n</li>\n<li>\n<p><span>Climate Change: </span><span>Fund the </span><a href=\"http://onlinelibrary.wiley.com/doi/10.1002/2016RG000533/full\"><span>authors of this paper</span></a><span> to do more feasibility research on using ocean alkalinity for carbon sequestration. The </span><a href=\"https://wedocs.unep.org/bitstream/handle/20.500.11822/22070/EGR_2017.pdf\"><span>UN emissions gap report </span></a><span>cites studies on this topic that suggest this to be the cheapest way to prevent climate change for as little as $10 per tonne of CO2 averted.</span></p>\n</li>\n<li>\n<p><span>Ageing research: </span><span>The Open Philanthropy Project </span><a href=\"https://www.openphilanthropy.org/focus/scientific-research/miscellaneous/uc-berkeley-aging-related-research-conboy\"><span>recently funded Irina Conboy\u2019s lab to do anti-ageing research</span></a><span>. Similarly, one could </span><a href=\"http://randolab.stanford.edu/\"><span>fund the Randolab at Stanford</span></a><span>, which also looks into aging and does similar research. </span><a href=\"https://www.semanticscholar.org/author/Irina-Conboy/5755227\"><span>Rando seems to have heavily influenced Conboy\u2019s research according to Semantic Scholar</span></a><span>.</span></p>\n</li>\n<li>\n<p><span>Macroeconomic stability:</span><span> A </span><a href=\"http://www.sciencedirect.com/science/article/pii/S0140673616005778\"><span>recent Lancet study suggests</span></a><span> that the 2008 financial crisis was was associated with about </span><a href=\"https://www.yahoo.com/news/financial-crisis-may-caused-500-000-cancer-deaths-224552583.html\"><span>500,000 excess cancer-related deaths worldwide</span></a><span>. This is just cancer, which only contributes to about 15% of global mortality and so a naive extrapolation might suggest mortality figures in the millions. Other factors such </span><a href=\"http://carnegieendowment.org/2009/06/17/financial-transmission-of-crisis-what-s-lesson-pub-23284\"><span>trade and tourism suffered significantly due to the economic crisis</span></a><span> and thus poor countries were probably hit harder in terms of wellbeing. </span><span>Fund </span><a href=\"https://en.wikipedia.org/wiki/Riccardo_Rebonato\"><span>Riccardo Rebonato</span></a><span>, an expert on stress testing, to write more on banking regulation and stress testing banks. </span></p>\n</li>\n<li>\n<p><span>Climate Change: </span><span>Fund the </span><a href=\"http://www.eurocapacity.org/homepage.shtml\"><span>European Capacity Building Initiative</span></a><span>, to work more on their </span><a href=\"http://www.eurocapacity.org/downloads/2016_ecbi_Policy_Brief_Finance_final.pdf\"><span>\u201cUnconventional Options to Enhance Multilateral Climate Finance\u201d</span></a></p>\n</li>\n<li>\n<p><span>EA community building: </span><span>Fund \u201c</span><a href=\"http://efektivni-altruismus.cz/\"><span>Efektivni-altruismus\u201d</span></a><span>, the Czech Effective Altruism group. They run the </span><a href=\"http://effectivethesis.com/\"><span>effective thesis project</span></a><span>. You could do it in a </span><a href=\"https://en.wikipedia.org/wiki/Social_impact_bond\"><span>social impact bond</span></a><span> way, where you pay X &nbsp;dollars for every thesis on Effective Altruism.</span></p>\n</li>\n<li>\n<p><span>Global catastrophic risks: North Korea: </span><span>Fund \u2018</span><a href=\"https://flashdrivesforfreedom.org/\"><span>Flash Drives for freedom</span></a><span>\u2019, which smuggles flash drives with unbiased information into North Korea. Such an approach was implicitly endorsed in November by Thae Yong-ho, once number </span><a href=\"http://www.telegraph.co.uk/news/2017/12/23/north-korea-floodedwith-illicit-information-carried-hydrogen/\"><span>two at North Korea\u2019s London embassy and now defector</span></a><span>. There\u2019s also </span><a href=\"http://journals.sagepub.com/doi/pdf/10.1177/2347797017732227\"><span>academic analysis of this isolation being one of the reasons for the lack of uprising in North Korea</span></a><span>. &nbsp;</span></p>\n</li>\n<li>\n<p><span>Global Development: </span><span>Global Prioritization Research: </span><a href=\"http://www.copenhagenconsensus.com/\"><span>The Copenhagen Consensus Center</span></a><span> is </span><a href=\"http://www.copenhagenconsensus.com/bangladesh-priorities\"><span>helping Bangladesh to liberalize its economy</span></a><span> and prioritize which policies would have the highest social, economic and environmental benefits for every dollar spent. Pushing such policies could radically </span><a href=\"http://www.econtalk.org/archives/2017/05/lant_pritchett_1.html\"><span>reduce poverty at a faster level than \u2018direct interventions\u2019 such as cash transfers according to some development economists.</span></a><span> Bangladesh has a large population and many people living in extreme poverty, so there\u2019s maybe an opportunity to repeat what was done in India (see previous link). They have also recently started a programme in India funded by the Gates Foundation: </span><a href=\"https://copenhagenconsensus.us5.list-manage.com/track/click?u=40608ac93880d1cb2444f1d20&amp;id=839d4ec223&amp;e=281ed29584\"><span>India Consensus</span></a><span>, starting with the states Andhra Pradesh and Rajasthan.</span></p>\n</li>\n<li>\n<p><span>Global Public Goods / Global Development: </span><span>Global flood forecasting: </span><a href=\"http://onlinelibrary.wiley.com/doi/10.1002/wat2.1137/pdf\"><span>Flooding has the highest frequency of occurrence of all types of natural disasters globally</span></a><span>, accounting for 39% of all natural disasters since 2000, with more than 94 million people globally being affected by floods every year. This population is expected to continue to increase due to climate change and globalisation. Floods lead to displacement, unsafe drinking water, destruction of infrastructure, injury, and death. Producing forecasts at the global scale with high spatial and temporal resolution has only become possible in recent years with scientific and technological advances and the increasing integration of hydrological and meteorological communities. In general, though the following estimates are uncertain, the</span><a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1843258\"><span> cost\u2013benefit ratio of flood forecasting systems appears to compare extremely favourably to the cost\u2013benefit ratio of weather and climate services</span></a><span>, or other early warning systems in general. Forecasting systems in Bangladesh, for instance, </span><a href=\"https://www.gfdrr.org/sites/gfdrr/files/Teisberg_EWS.pdf\"><span>are estimated to have a benefit-cost ratio of 500</span></a><span>. Generally, in the developing world, saving between $300 million and $2 billion could be obtained, plus additional economic benefits for a total ranging between $3 and $30 billion and averting the death of an average 23,000 a year. The total benefits would reach between $4 and $36 billion per year. The investments are </span><a href=\"https://elibrary.worldbank.org/doi/abs/10.1596/1813-9450-6058\"><span>estimated at approximately $1 billion US per year</span></a><span>. This would imply a benefit-cost ratio between 4 and 36. &nbsp;Fund </span><a href=\"http://www.reading.ac.uk/h-l-cloke.aspx\"><span>researchers such as Hannah Cloke who are trying to improve global flood forecasting</span></a><span>. </span></p>\n</li>\n<li>\n<p><span>US politics / Global catastrophic risks: </span><span>\u2018Swing Left\u2019 \u201can online community that connects you with your nearest Swing District. This is a district where the winner, an elected official who is now serving a two-year term in the House of Representatives, won the November 2016 election by a thin margin, or is otherwise vulnerable in 2018.[...] It\u2019s goal is to flip the House of Representatives in the 2018 midterm elections and put a check on the Trump and GOP agenda\u201d [</span><a href=\"https://swingleft.org/faq\"><span>source</span></a><span>] It is a carey PAC that does not have a donation limit for their core activities. Here\u2019s </span><a href=\"https://www.nytimes.com/2017/12/24/us/democrats-house-control-2018-midterms.html?hp&amp;action=click&amp;pgtype=Homepage&amp;clickSource=story-heading&amp;module=first-column-region&amp;region=top-news&amp;WT.nav=top-news\"><span>a recent analysis on the democrats effort to win seats in the House.</span></a><span> There are also some superPACs that are </span><a href=\"http://facebook.com/story.php?story_fbid=10108541378111670&amp;id=7958447&amp;ref=bookmarks\"><span>recommended in this post that might be worth looking into</span></a><span>.</span></p>\n</li>\n<li>\n<p><span>Ageing: </span><span>Fund </span><a href=\"https://www.leafscience.org/\"><span>life extension advocacy</span></a></p>\n</li>\n<li>\n<p><span>EA Research:</span><span> Fund </span><a href=\"https://ieet.org/index.php/IEET2/bio/cirkovic\"><span>Milan M. Cirkovic</span></a><span> who has published \u201cGlobal Catastrophic Risks\u201d with Nick Bostrom</span></p>\n</li>\n<li>\n<p><span>Short term AI risk: </span><a href=\"https://icrac.net/\"><span>The International Committee for Robot Arms Control</span></a><span> \u2013 or ICRAC \u2013 is an international not-for-profit association committed to the peaceful use of robotics in the service of humanity and the regulation of robots. One could also donate to the </span><a href=\"http://www.stopkillerrobots.org/about-us/\"><span>campaign to stop killer robots by Human Rights Watch</span></a><span>.</span></p>\n</li>\n<li>\n<p><span>Wild animal suffering:</span><span> Donate to </span><a href=\"/ea/1i4/wildanimal_suffering_researchs_plans_for_2018_and/\"><span>Wild animal suffering research </span></a></p>\n</li>\n<li>\n<p><span>GCR preparedness: </span><span>Fund the researchers </span><a href=\"http://onlinelibrary.wiley.com/doi/10.1111/risa.12761/abstract\"><span>of this paper</span></a> <span>to do more research on global food security a la </span><a href=\"http://allfed.info/\"><span>All Fed</span></a><span>.</span></p>\n</li>\n<li>\n<p><span>Global catastrophic risks: </span><span>Cambridge Center for Existential Risk (CSER).</span></p>\n</li>\n<li>\n<p><span>Global catastrophic risks / Climate change:</span><span> Fund the</span><a href=\"https://www.bmsis.org/\"><span> Blue Marble Space Institute of Science </span></a><span>to do more research into the feasibility of </span><a href=\"https://arxiv.org/ftp/arxiv/papers/1507/1507.05495.pdf\"><span>geoengineering larger ice caps</span></a><span>.</span></p>\n</li>\n<li>\n<p><span>Global catastrophic risks: </span><span>Fund the \u2018</span><a href=\"https://armscontrolcenter.org/\"><span>Center for Arms Control and Non-Proliferation</span></a><span>\u2019 to do more </span><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4128296/\"><span>research</span></a><span> and advocacy on biosecurity issues.</span></p>\n</li>\n<li>\n<p><span>Global catastrophic risks / space weather: </span><span>Fund the </span><a href=\"https://arxiv.org/pdf/1709.05348.pdf\"><span>researchers of this paper</span></a><span> to work more on estimating the probability and economic losses of solar flares and work on a potential mitigation strategy to protect earth by setting up a magnetic shield to deflect solar flares. </span></p>\n</li>\n<li>\n<p><span>Global Health:</span><span> Tobacco Control in India: </span><a href=\"https://www.cgdev.org/page/case-14-curbing-tobacco-use-poland\"><span>Smoking is on course to lead to 1 billion premature deaths</span></a><span>, that could be prevented tobacco control efforts. Fund the </span><a href=\"http://www.rctfi.org/\"><span>Resource Center for a Tobacco Free India</span></a><span> to work on </span><a href=\"http://pubdocs.worldbank.org/en/39281440103918543/CGD-Policy-Paper-62-Savedoff-Alwang-Best-Health-Policy-Tobacco-Tax.pdf\"><span>increasing tobacco taxes which have been called the best public health policy</span></a><span>. Also fund </span><a href=\"http://www.dlsph.utoronto.ca/faculty-profile/jha-prabhat/\"><span>Prabhat Jha</span></a><span>, an advocate on tobacco control at the University of Toronto. He wrote the </span><a href=\"http://documents.worldbank.org/curated/en/914041468176678949/Curbing-the-epidemic-governments-and-the-economics-of-tobacco-control\"><span>seminal 1999 book at the World Bank</span></a><span> and has been actively promoting tobacco control in India as well as other places around the world. Tobacco control mass media campaigns in India </span><a href=\"http://tobaccocontrol.bmj.com/content/early/2017/08/10/tobaccocontrol-2016-053564?utm_content=buffer5106c&amp;utm_medium=social&amp;utm_source=facebook.com&amp;utm_campaign=buffer\"><span>have been suggested to be very cost-effective at US$9.2 per death averted</span></a><span>. </span></p>\n</li>\n<li>\n<p><span>Global catastrophic risks:</span><span> Fund Arturo Casadevall to do more work on </span><a href=\"http://online.liebertpub.com/doi/abs/10.1089/hs.2017.0048\"><span>Global Catastrophic Risks from fungi</span></a><span>. Some </span><a href=\"http://journals.plos.org/plospathogens/article?id=10.1371/journal.ppat.1002808\"><span>people say it might have killed the dinosaurs</span></a><span>.</span></p>\n</li>\n<li>\n<p><span>Ageing: </span><span>Fund </span><a href=\"http://www.sciencedirect.com/science/article/pii/S0167779917301713\"><span>the researchers of this paper</span></a><span> to do more research into the economics of ageing research.</span></p>\n</li>\n<li>\n<p><span>Global catastrophic risks: </span><span>Fund the researchers of </span><a href=\"http://online.liebertpub.com/doi/full/10.1089/hs.2016.0069\"><span>this paper on how find disease outbreaks faster to continue their research</span></a><span> and advocacy. </span></p>\n</li>\n<li>\n<p><span>Global development:</span><span> Fund the </span><a href=\"http://roadsafetyngos.org/\"><span>Global Alliance of NGOs for Road Safety</span></a><span> to reduce traffic accidents. This can be </span><a href=\"https://www.givingwhatwecan.org/post/2015/03/road-traffic-injuries-how-effective-prevention/\"><span>cost-effective</span></a><span> because more than 1 million people die due to road traffic accidents: for instance, average cost-effectiveness of better road safety enforcement in Uganda </span><a href=\"http://injuryprevention.bmj.com/content/14/4/223.short\"><span>has been estimated $603 per death averted</span></a><span>.</span></p>\n</li>\n<li>\n<p><span>Global Development: </span><span>Fund </span><a href=\"https://www.trademarkea.com/\"><span>Trademark East Africa</span></a><span> which tries to improve trade in East Africa. Or fund Cuts International, Geneva, which also works </span><a href=\"http://www.cuts-geneva.org/pdf/MC11Notes%20-%20Tariff%20Simplification.pdf\"><span>on improving trade for least developed countries</span></a><span>. Trade is important because it attracts investment, creates jobs, and reduces poverty. For instance, one recent study suggests that the African Growth and Opportunity Act, the </span><a href=\"https://editorialexpress.com/cgi-bin/conference/download.cgi?db_name=CSAE2016&amp;paper_id=832\"><span>US trade agreement with Sub-saharan African countries, has reduced infant mortality by about 9%</span></a><span>. Another </span><a href=\"http://documents.worldbank.org/curated/en/962781513281198572/pdf/WPS8277.pdf\"><span>study suggest that</span></a><span> an \u201cincrease in tariffs to average bound rates of 44.7 percent in highly protectionist countries such as India, Bangladesh, Pakistan and Sri Lanka would translate into a decline in real income in South Asia by 4.2 percent or welfare losses of close to US$125 billion relative to the baseline by 2020\u201d.</span></p>\n</li>\n<li>\n<p><span>Wild animal suffering: </span><span>Fund the researchers of </span><a href=\"http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0185809\"><span>this paper on the decline of world insect population</span></a><span> to conduct follow up research on causes.</span></p>\n</li>\n<li>\n<p><span>Near term AI risk: </span><span>It seems that there is a lot regulatory capture of technology companies going on. Many people </span><a href=\"https://medium.com/wordsthatmatter/merge-now-430c6d89d1fe\"><span>are</span></a> <a href=\"https://www.samharris.org/podcast/item/what-is-technology-doing-to-us\"><span>worried</span></a> <a href=\"http://www.econtalk.org/archives/2017/12/matt_stoller_on.html\"><span>about</span></a><span> how technology is hijacking our attention spans and more. Addictive internet technology probably wastes a lot of resources. </span><a href=\"http://www.timewellspent.io/\"><span>Time well spent</span></a><span> is an advocacy organisation that works on this issue. </span><a href=\"https://www.theguardian.com/commentisfree/2017/nov/02/facebook-google-monopoly-companies\"><span>Matt Stoller</span></a><span> is another advocate who\u2019s working on this.</span></p>\n</li>\n<li>\n<p><span>Global public goods / Global development: </span><a href=\"https://en.wikipedia.org/wiki/Air_pollution\"><span>Air pollution kills 7 million people</span></a><span> every year. Clean air is a global public and </span><a href=\"http://pubs.acs.org/doi/abs/10.1021/es034031g\"><span>cross continental air pollution exists</span></a><span>. Fund the </span><a href=\"http://www.catf.us/\"><span>Clean Air Taskforce</span></a><span>. </span></p>\n</li>\n<li>\n<p><span>Global catastrophic risks:</span><span> Fund the </span><a href=\"https://en.wikipedia.org/wiki/Svalbard_Global_Seed_Vault\"><span>Global Seed Vault</span></a><span>: \u201cThe Global Seed Vault is a frozen vault in the Arctic, which contains the seeds of many important crop varieties, reducing the chance we lose an important species. Melting water </span><a href=\"https://www.theguardian.com/environment/2017/may/19/arctic-stronghold-of-worlds-seeds-flooded-after-permafrost-melts\"><span>recently entered the tunnel leading to the vault</span></a><span> due, ironically, to climate change, so could probably use more funding. There are lots of other projects like this we could do to preserve knowledge.\u201d </span><span><br></span><span>[</span><a href=\"https://80000hours.org/articles/extinction-risk/\"><span>source</span></a><span>]</span></p>\n</li>\n<li>\n<p><span>Fostering global prioritization research: </span><span>Fund </span><a href=\"https://global.oup.com/academic/product/cost-effectiveness-in-health-and-medicine-9780195108248?cc=de&amp;lang=en&amp;\"><span>the authors of this textbook on \u201cCost-Effectiveness in Health and Medicine\u201d to run a MOOC</span></a></p>\n</li>\n<li>\n<p><span>Global catastrophic risks: Look into (!) </span><span>funding an </span><a href=\"http://empactamerica.org/\"><span>advocacy group working on warning against Electromagnetic Pulse (EMP)</span></a><span> attacks. </span><a href=\"https://www.theatlantic.com/international/archive/2011/07/the-campaign-to-terrify-you-about-emp/241971/\"><span>Critical commentary here</span></a><span>. Generally, it might be good to fund more research on this for </span><a href=\"https://concepts.effectivealtruism.org/concepts/value-of-information/\"><span>value of information</span></a><span>.</span></p>\n</li>\n<li>\n<p><span>Global public goods: </span><span>Fund the </span><a href=\"https://smw.ch/en/article/doi/smw.2017.14553\"><span>researchers of this paper on antimicrobial resistance</span></a><span> to do more advocacy on alternative to developing new antibiotics.</span><span>According </span><a href=\"https://amr-review.org/sites/default/files/AMR%20Review%20Paper%20-%20Tackling%20a%20crisis%20for%20the%20health%20and%20wealth%20of%20nations_1.pdf\"><span>to one report</span></a><span> antimicrobial resistance will result in more than 10 million annual deaths and cause up to $100 trillion in economic costs.</span></p>\n</li>\n<li>\n<p><span>Cancer: </span><span>Give </span><a href=\"https://www.vetmed.wisc.edu/people/vaild/\"><span>David Vail\u2019s lab</span></a><span> a grant to study a </span><a href=\"https://www.openphilanthropy.org/focus/scientific-research/miscellaneous/arizona-state-university-canine-clinical-trial-multivalent-cancer-vaccine\"><span>cancer vaccine similar to the grant for to this researcher,</span></a><span> whose research was heavily influenced by </span><a href=\"https://www.semanticscholar.org/author/Douglas-H-Thamm/2465310\"><span>David Vail according to Semantic Scholar. </span></a></p>\n</li>\n<li>\n<p><span>Global Development: </span><span>Fund organisations that work on finding the causes of growth in developing countries and trying to speed up developing country growth. For instance, fund </span><a href=\"http://growthdialogue.org\"><span>Growthdialogue.org</span></a><span> and </span><a href=\"https://www.theigc.org/\"><span>The International Growth Center.</span></a></p>\n</li>\n<li>\n<p><span>Global health: </span><span>Try to get </span><a href=\"http://ash.org.uk/\"><span>Action against Smoking</span></a><span>, </span><a href=\"http://www.actiononsalt.org.uk/\"><span>Action against Salt</span></a><span>, and </span><a href=\"http://www.actiononsugar.org/\"><span>Action on Sugar</span></a><span> to start international advocacy on their respective areas, which all kill several million people every year. For instance, </span><a href=\"http://www.nejm.org/doi/full/10.1056/NEJMoa1304127\"><span>one modeling study</span></a><span> suggests that </span><span>1.65 million deaths from cardiovascular causes can be attributed to excessive sodium consumption annually. Another </span><a href=\"http://www.bmj.com/content/356/bmj.i6699\"><span>study</span></a><span> suggest that sodium reduction strategies can be very cost-effective at around $100 per DALY averted. One could fund the </span><a href=\"https://www.omicsonline.org/open-access/to-live-long-eat-less-salt-salt-intake-reduction-promotion-and-hypertension-control-in-china-2375-4273-1000169.pdf\"><span>authors of a paper on how to lower sodium consumption</span></a><span> in China to do more research. &nbsp;</span></p>\n</li>\n<li>\n<p><span>Global public goods:</span><span> Fund </span><a href=\"https://policyexchange.org.uk/\"><span>Policy Exchange</span></a><span> to </span><a href=\"https://policyexchange.org.uk/wp-content/uploads/2017/11/Global-Britain.pdf?mc_cid=bea1a8fe72&amp;mc_eid=281ed29584\"><span>do more research</span></a><span> on how R&amp;D investment can benefit development.</span></p>\n</li>\n<li>\n<p><span>Global public goods:</span><span> Set up a </span><a href=\"https://mobile.nytimes.com/2017/12/04/upshot/health-research-lawsuits-chilling-effect.html?action=click&amp;module=Discovery&amp;pgtype=Homepage\"><span>fund similar to the one set up by Gates and Bloomsberg to cover legal fees when developing countries are sued by Big Tobacco.</span></a><span> But not for Big Tobacco lawsuits but for scientists that are </span><a href=\"https://mobile.nytimes.com/2017/12/04/upshot/health-research-lawsuits-chilling-effect.html?action=click&amp;module=Discovery&amp;pgtype=Homepage\"><span>sued by industry for conducting research in global health.</span></a><span> This might have a deterrent effect by just having the money sit there (of course it is not a free lunch).</span></p>\n</li>\n<li>\n<p><span>Global catastrophic risks / Global public goods:</span><span> General support for the </span><a href=\"http://gcrinstitute.org/\"><span>Global Catastrophic Risks Institute</span></a><span>.</span></p>\n</li>\n<li>\n<p><span>Global catastrophic risks:</span><span> Fund the researchers of this </span><a href=\"https://www.belfercenter.org/sites/default/files/2017-10/NK%20Bioweapons%20final.pdf\"><span>paper on North Korea\u2019s biological weapons programme</span></a><span> to continue their research. Also, fund the Rand Corporation to continue their research </span><a href=\"https://www.rand.org/pubs/testimonies/CT486.html\"><span>North Korea's chemical and biological weapons capabilities and which countermeasures to prioritize</span></a><span>.</span></p>\n</li>\n<li>\n<p><span>Global catastrophic risks: </span><span>Fund Chatham House, </span><a href=\"https://repository.upenn.edu/cgi/viewcontent.cgi?article=1011&amp;context=think_tanks\"><span>Think Tank of the Year 2016</span></a><span>, to continue their work on </span><a href=\"https://www.chathamhouse.org/publication/cybersecurity-nuclear-weapons-systems-threats-vulnerabilities-and-consequences\"><span>Cybersecurity of Nuclear Weapons Systems.</span></a> <span>&nbsp;Also fund the researchers of </span><a href=\"http://ieeexplore.ieee.org/document/7600218/\"><span>this paper on cyber terrorism</span></a><span>. </span></p>\n</li>\n<li>\n<p><span>Climate change: </span><span>Fund the authors of </span><a href=\"http://rsta.royalsocietypublishing.org/content/373/2054/20140429\"><span>this paper on the $10 trillion value of better information about the transient climate response</span></a><span>. More on </span><a href=\"https://www.effectivealtruism.org/articles/the-moral-value-of-information-amanda-askell/\"><span>Value of information</span></a><span>.</span></p>\n</li>\n<li>\n<p><span>Climate change: </span><span>Between 2010 and 2016, the world spent $2.3 trillion on renewable energy and only $10 billion on CCS. Fund the </span><a href=\"http://www.ccsassociation.org/\"><span>Carbon Capture and Storage Association</span></a><span> or the </span><a href=\"https://www.globalccsinstitute.com/\"><span>Global CCS Institute</span></a><span> for advocacy and the </span><a href=\"http://www.imperial.ac.uk/carbon-capture-and-storage/\"><span>carbon capture and storage (CCS) research program at Imperial College London</span></a><span>. </span></p>\n</li>\n<li>\n<p><span>Global Public Goods:</span><span> Fund the </span><a href=\"https://www.cambridge.org/core/journals/proceedings-of-the-nutrition-society/article/diet-nutrition-and-the-ageing-brain-current-evidence-and-new-directions/8378A6954F727E4680508A7DF5796D03\"><span>Nutrition society</span></a><span> to do more special issues on micronutrients. For instance, there seems to be </span><a href=\"https://www.theglobeandmail.com/life/health-and-fitness/health/the-vitamin-d-dilemma-how-much-should-we-be-taking/article23672033/\"><span>some controversy of how much Vitamin D humans need</span></a><span> - the value of information of figuring out information like this might be good because micronutrient fortification programmes seem to be very effective.</span></p>\n</li>\n<li>\n<p><span>Global Health: </span><span>Fund the researchers of </span><a href=\"http://www.sciencedirect.com/science/article/pii/S0166354216304582?via%3Dihub\"><span>this paper on broad spectrum antivirals</span></a><span> to continue their research. Their paper seems to have gathered </span><a href=\"https://www.semanticscholar.org/paper/Evaluation-of-anti-Zika-virus-activities-of-broad-Adcock-Chu/5804a023d5eb83a8290d38f488473f801e834568?tab=citations\"><span>interest according Semantic Scholar</span></a><span>.</span></p>\n</li>\n<li>\n<p><span>Global catastrophic risks: </span><span>Fund these </span><a href=\"https://www.nature.com/articles/nature14121\"><span>researchers</span></a><span> to do more work on </span><a href=\"https://www.nature.com/articles/nature14121\"><span>biocontainment of genetically modified organisms</span></a><span>. They are cited in </span><a href=\"https://www.cser.ac.uk/\"><span>CSER</span></a><span> paper on </span><a href=\"https://elifesciences.org/articles/30247\"><span>20 emerging issues in biological engineering</span></a><span> - there are other opportunities in this paper. </span></p>\n</li>\n<li>\n<p><span>Animal welfare: </span><span>Meat taxes: Fund the </span><a href=\"https://www.nature.com/articles/nclimate3155\"><span>researchers of this pape</span></a><span>r to do more research on emissions pricing of food commodities. Because meat has a high CO2 footprint, this might </span><a href=\"https://www.theguardian.com/environment/2017/dec/11/meat-tax-inevitable-to-beat-climate-and-health-crises-says-report\"><span>lead to a de facto meat tax</span></a><span>.</span></p>\n</li>\n<li>\n<p><span>Biomedical science: </span><span>Fund the </span><a href=\"https://galaxyproject.org\"><span>Galaxy Project</span></a><span>, a web-based genome analysis tool, which has </span><a href=\"https://www.semanticscholar.org/paper/Galaxy-a-web-based-genome-analysis-tool-for-experi-Blankenberg-Kuster/ca1ecace9b63f40136312874a74ee16e80109f81?tab=citations\"><span>been widely used in biological sciences according to Semantic Scholar</span></a><span>. </span><a href=\"https://www.semanticscholar.org/paper/Bioconductor-open-software-development-for-computa-Gentleman-Carey/608bcf0c7a9432458ba104b0147cddc73c416bc5?tab=citations\"><span>Similarly</span></a><span>, fund </span><a href=\"https://www.bioconductor.org\"><span>Bioconductor</span></a><span>, an open source, software project to provide tools for the analysis of high-throughput genomic data. </span></p>\n</li>\n<li>\n<p><a href=\"https://80000hours.org/problem-profiles/improving-institutional-decision-making/\"><span>Improving institutional decision making:</span></a> <span>The </span><a href=\"http://www.igmchicago.org/igm-economic-experts-panel\"><span>IGM Economic Experts Panel</span></a><span> regularly polls in how far renowned economists agree or disagree on major public policy issues. Fund them to </span><span>set up a prediction market with IGM booth with grant money.</span></p>\n</li>\n<li>\n<p><span>Global catastrophic risks: </span><span>Fund the </span><a href=\"https://www.sciencedirect.com/science/article/pii/S2212420916301753\"><span>researchers of this paper</span></a><span> to continue their research on cross cutting disaster risk reduction and resilience. &nbsp;</span></p>\n</li>\n<li>\n<p><span>Global development: </span><span>Fund the ADM Institute for the Prevention of Postharvest Loss at the University of Illinois at Urbana-Champaign. The authors of this paper on </span><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5296677/\"><span>reducing postharvest losses during storage of grain crops to strengthen food security in developing countries</span></a><span> summarize this issue: \u201cAs much as 50%\u201360% cereal grains can be lost during the storage stage due only to the lack of technical inefficiency. Use of scientific storage methods can reduce these losses to as low as 1%\u20132%.\u201d</span></p>\n</li>\n</ol>", "user": {"username": "HaukeHillebrandt"}}, {"_id": "HnxQF6kkLuyiSZjhN", "title": "AI alignment prize winners and next round [link]", "postedAt": "2018-01-20T12:07:16.024Z", "htmlBody": "<html><body><p>From <a href=\"https://www.lesserwrong.com/posts/4WbNGQMvuFtY3So7s/announcement-ai-alignment-prize-winners-and-next-round\">Announcement: AI alignment prize winners and next round</a>:&#xA0;</p>\n<blockquote>\n<p>We (Zvi Mowshowitz, Vladimir Slepnev and Paul Christiano) are happy to announce that the AI Alignment Prize is a success. From November 3 to December 31 we received over 40 entries representing an incredible amount of work and insight. That&apos;s much more than we dared to hope for, in both quantity and quality.</p>\n<p>In this post we name six winners who will receive $15,000 in total, an increase from the originally planned $5,000.</p>\n<p>We&apos;re also kicking off the next round of the prize, which will run from today until March 31, under the same rules as before.</p>\n</blockquote>\n<p>&#xA0;</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "RyanCarey"}}, {"_id": "BEb36kr8MoY835eKS", "title": "Project Report - On the Potential of Norwegian CSR-donations to Effective Charities", "postedAt": "2018-01-18T10:56:56.438Z", "htmlBody": "<html><body><pre><span><em>Pre-script</em><br></span><span>Postet on behalf of Bruno and Eirin since they lack the necessary karma. I&apos;m managing director of Stiftelsen Effekt and was part of the steering committee. Also see the <a href=\"/ea/e9/initial_research_into_corporate_fundraising/\">relevant post</a> by Tom Ash, Charity Science, from Jan 2015. As Markus Anderljung mentions in the comments, they also deem cooperating with EAs within a company a promising strategy.</span></pre>\n<p>&#xA0;</p>\n<p><span>Initial Research the potential of CSR-donations to effective charities</span></p>\n<p><span>By Bruno Seifert and Eirin Evjen on behalf of </span><a href=\"https://www.stiftelseneffekt.no/\"><span>Stiftelsen Effekt</span></a><span> and </span><a href=\"http://www.effektivaltruisme.no/\"><span>Effective Altruism Norway</span></a></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Effective Altruism Norway and the Effect foundation (Stiftelsen Effekt) started in September a project to reach out to companies and convince them to donate as part of their corporate social responsibility (CSR) efforts. Stiftelsen Effekt is a foundation started by a group of Norwegian Effective Altruists with the aim of promoting and simplifying donations to GiveWell&#x2019;s recommended charities while also cutting down transaction costs. </span></p>\n<p><span><br></span><span>The idea for this project came up after the leader of Stiftelsen Effekt (SE) was invited by an insurance company to present the Stiftelsen Effekt and its recommended charities, because they were looking for a new CSR-project. Especially GiveDirectly (GD) stood out because of </span><a href=\"https://live.givedirectly.org/\"><span>GDLive</span></a><span> and with it the possibility to put up a screen in offices showing a few example families to which &#x201C;their&#x201D; donations are being sent. The aim of this project is to explore if CSR is a worthwhile endeavour for EA Norway and to develop a strategy for it as well as an MVP-product for the display of GDLive.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>However, CSR is a difficult area for EA in general, because businesses usually have other goals than creating the greatest social impact with their CSR contribution. Often they want to use CSR in recruitment, get good PR or use it primarily as a motivation tool for their employees. Especially large companies prefer to donate their products or pro bono work rather than cash, which is harder to channel towards effective charities. This vastly limits the scalability of fundraising from businesses, although from a very high level. It is interesting to note that the total marked in donations from private individuals dwarfs the marked from businesses, at least in Norway. Here businesses make out just 2% of private donations to charity. The rest are funds from the government, institutions and foundations [58 %], individuals [32 %] and sales/other [8 %].</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>This being said, there are several advantages with fundraising from companies:</span></p>\n<ul>\n<li>The marginal value of one more company donor is - on average - much higher than for individuals, but the marginal effort is not necessarily that much higher.&#xA0;</li>\n<li>By reaching the employees though the company CSR initiative, we get a great channel to promote effective charities to resourceful individuals who may themselves donate.</li>\n<li>The funding cannibalism effect on other effective charities is low with companies as donors.</li>\n</ul>\n<p><span>If anyone is interested in more detail, see our more in-depth report </span><a href=\"https://docs.google.com/document/d/1Avh26cD-7H7HyYJceEBa8_gGaiGpFRWdHYzBlRBouuI/edit?usp=sharing\"><span>here</span></a><span>. T</span><span>his report is divided in two parts: a description of the CSR-project (pages 2-3) and a discussion of what we have learned in the first phase of the project and what we recommend going forward (page 4). </span></p>\n<h2>&#xA0;</h2>\n<h2><span>The CSR-project</span></h2>\n<h3><span>1. Preliminary research</span></h3>\n<p><span>We started by getting an overview of the CSR field in Norway through online researching and conversations with a few contacts at different companies. We also contacted companies of varying sizes and asked them about their CSR-efforts. We found that Norwegian CSR consists of four main objectives: human rights, labor rights, the environment and fight against corruption. These objectives are usually met through non-donation efforts, though it depends on the size of the company. Only a small part of larger companies&#x2019; CSR efforts are donations. Consultancies often do pro-bono work, while others focus more on enhancing eco-friendly production. Medium-sized companies had much more focus on donations, and the smaller companies even more. We also found that there were little incentives for donations in terms of tax deduction. It generally seems like </span><span>effectiveness is not a big focus when it comes to CSR in Norway</span><span>, but rather that the person in charge believes in the projects and that they feel like the charities fit with them. </span></p>\n<h3>&#xA0;</h3>\n<h3><span>2. Strategy</span></h3>\n<p><span>Based on the initial research, </span><span>we decided to focus on SMEs </span><span>onward. We also decided to focus on GiveDirectly due to the possibility of using GDLive, but be open to promote another organization if we thought that would suit the company more. As a next step we planned our contact and sales process more in detail and tried it on our existing leads. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>We expect much exploration value in testing out this concept as it is easily transferable to other countries if it works in Norway. In addition to this visual product used for motivation and keeping scores, it may be gamified further by employee competitions or events. This could raise employee engagement and productivity further, which has been shown to be a general effect of corporate philanthropy already (e.g. </span><a href=\"http://www.tandfonline.com/doi/citedby/10.1300/J054v16n01_03?scroll=top&amp;needAccess=true\"><span>Raman &amp; Zboja, 2008</span></a><span>).</span><span><br></span><span><br></span><span>As a part of the package we present the companies, we offer that Stiftelsen Effekt handles transactions, reporting and contact with GD free of charge. On a regular basis, or whenever the company wishes, SE may give presentations to employees or managing staff about the effect of their donations. This is also high value facetime with resourceful private individuals. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>What we want to to convince companies of are:</span></p>\n<p><span>i) Donating money has positive use to them even from an economic standpoint </span></p>\n<p><span>ii) We are a better partner for them to achieve this than other mega-charities.</span></p>\n<h3>&#xA0;</h3>\n<h3><span>3. Graphic design</span></h3>\n<p><span>To support our sales effort we made graphics that easily could be attached to emails. The pdf-file consisted of two pages: </span></p>\n<p><span>i) </span><span>A sales-pitch</span><span> with an overview of the agreement we were proposing.</span></p>\n<p><span>ii) </span><span>An introduction to GiveDirectly</span><span> and some information on Stiftelsen Effekt. </span></p>\n<p><span>iii) </span><span>An introduction to PHC</span><span> and some information on Stiftelsen Effekt.</span></p>\n<h3>&#xA0;</h3>\n<h3><span>4. Contacting</span></h3>\n<p><span>To find companies which seemed worthwhile contacting we tried multiple approaches. We used a general register of companies in Norway, searching Google with keywords like &#x2018;cost-effective&#x2019; or &#x2018;results-oriented&#x2019;, and looked on LinkedIn for employees responsible for CSR. The method that has provided the most traction this far is having an EA employee in the company as support while they try to convince the responsible persons. For this, </span><a href=\"http://www.eaworkplaceactivism.org/wp-content/uploads/2016/10/WA-Handbook-v1.pdf\"><span>GWWC&#x2019;s handbook</span></a><span> on workplace activism is a great resource. </span></p>\n<h3>&#xA0;</h3>\n<h3><span>5. Visual product</span></h3>\n<p><span>One of our main selling points is a visual solution based on GDLive which we can offer the companies free of charge. </span><span>We decided to go for a back-end solution on our own servers, as well as a lightweight front-end solution </span><span>for the MVP. There is no API for GDLive yet, so we have to scrape the website regularly. A separate front-end solution allows for more customization tailored for each firm. We are planning for two versions for the front-end solution: One banner for the firms&#x2019; own website or intranet; and a full-page on a designated screen. </span><span>We outsourced this element of the project to volunteers, which worked quite well.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>The current visual product, a banner which could be placed on the side of e.g. an intranet page, shows 2-3 families at a time, with the same information as on the frontpage of GDLive. It shows name, picture, how long ago it is that they received a transfer, how much they transferred, and a few sentences on what they used the transfer on. The product also shows how much the company has donated so far, or alternatively how much that department of the firm has donated so far. Through GiveDirectly&#x2019;s estimations, the product also shows a calculation of how many families the amount donated so far have gone to. There are additional ideas, mainly a version that spans across a full screen and shows a full profile, in development.</span></p>\n<h3>&#xA0;</h3>\n<h3><span>6. Organisational work</span></h3>\n<p><span>The project </span><span>lasted three months and we were two employees working 30%</span><span>. </span><span>A steering committee </span><span>made up of the managing director of the Effect foundation, board chair of EA Norway and two others with valuable experience</span><span> helped by giving feedback and supporting us with professional expertise.</span><span> We also got feedback from other people, both from within the EA-community and outside. We got in touch with them through a post on the EA Norway Facebook group, and through ours and the board&#x2019;s contacts. This was especially useful in the first phase, when we had a lot of uncertainties surrounding how CSR works in Norway and what our strategy should be.</span></p>\n<h2>&#xA0;</h2>\n<h2><span>Lessons learned and the way forward</span></h2>\n<h3><span>7. Uncertainties</span></h3>\n<p><span>There still are multiple things we are unsure about: </span></p>\n<p><span>i) Finding an effective process to search for and filter out good prospects</span></p>\n<p><span>ii) What size, industry or other identifier should we focus on</span></p>\n<p><span>iii) How to best make initial contact</span></p>\n<p><span>iv) How good are our chances at competing with mega charities like the Red Cross</span></p>\n<p><span>v) Is the visual product and concept an important selling point</span></p>\n<h3>&#xA0;</h3>\n<h3><span>8. CSR project 2.0</span></h3>\n<p><span>It still is our impression that CSR can be a worthwhile endeavour for EA, but in our current position contacting companies at near-random might not be the best strategy. We should focus more on setting a few high profile partnerships in place going through EAs who are already working instead of contacting as many companies as possible. This way we might get more attention and gather experience for future projects. So as a first step we&#x2019;d recommend exhausting our already existing personal network within EA / GWWC by pushing and supporting workplace activism as much as possible. If our capacities prove to be larger than our network needs we can focus on making new contacts either through a refined strategy of emails and cold calls, trying to build personal contact on fairs or through pro-bono networks before offering a partnership or new ideas not discussed this far. </span></p>\n<h3>&#xA0;</h3>\n<h3><span>9. Conclusion</span></h3>\n<p><span>Our conclusion is that there is a substantial potential in getting donations from companies in Norway and that the lowest hanging fruit is to get EA-aligned people to advocate for the charities from within the companies. At scale, the tractability is more uncertain. Although we have limited knowledge of firms in other countries, we would also recommend EA-organisations there to consider researching the opportunities for directing CSR-funds towards effective charities either through direct contact or through EA-aligned people in the job force.</span></p></body></html>", "user": {"username": "Jorgen_Ljones"}}, {"_id": "o7DxtCiRKa8vu7cNp", "title": "[Paper] Interventions that May Prevent or Mollify Supervolcanic Eruptions", "postedAt": "2018-01-15T21:46:27.407Z", "htmlBody": "<html><body><p>Supervolcanic eruptions are approximately 10 times as powerful as the eruption that caused the year without a summer in 1816. A supervolcanic eruption would cause local devastation, but the main problem is blocking the sun for years and starvation (with possible loss of civilization without recovery and other far future effects). Indeed, many people think that the genetic bottleneck of humans going to a population of only a few thousand was due to a super volcanic eruption 74,000 years ago. It is widely assumed that there is nothing we can do to prevent or mollify supervolcanic eruptions. However, I came up with over 50 possible interventions. In the paper, we could not get into economics. I have done some initial estimates that indicate that the most promising interventions of adding soil or water on top of the supervolcano to delay an eruption for 100 years would likely be cost-effective only considering the present generation. However, this is in isolation. In reality, the first thing we should do is get prepared with <a href=\"/ea/14y/saving_expected_lives_at_10_apiece/\">alternate foods</a> that are not dependent on sunlight. <a href=\"/ea/1g9/should_we_be_spending_no_less_on_alternate_foods/\">This </a>would protect against the majority of the damage associated with a supervolcanic eruption, making prevention of a supervolcanic eruption less cost-effective. Still, since people are already doing research on supervolcanic eruptions, it may make sense to nudge that research towards directions that would reduce global catastrophic/extistential risk.</p>\n<p><a href=\"https://www.academia.edu/35646065/Interventions_that_May_Prevent_or_Mollify_Supervolcanic_Eruptions\">Here </a>is the full paper and below is the abstract:</p>\n<p><span>A supervolcanic eruption of 10^1</span><span>5 kg could block the sun for years, causing mass starvation or </span><span>even extinction of some species, including humans. Despite awareness of this problem for </span><span>several decades, only five interventions have been proposed. In this paper, we increase the </span><span>number of total possible interventions by more than an order of magnitude. The 64 total </span><span>interventions involve changing magma characteristics, venting magma, strengthening the cap </span><span>(rock above the magma), putting more pressure on the magma, stopping an eruption in progress, </span><span>containing the erupted material, disrupting the plume, or provoking a less intense eruption. We </span><span>&#xA0;provide qualitative evaluations of the feasibility and risk of 38 of the more promising </span><span>interventions. The two most promising interventions involve putting more pressure on the </span><span>magma and delaying the eruption with water dams or soil over the magma chamber. We perform </span><span>a technical analysis, accurate to within an order of magnitude, and find that water dams and soil </span><span>and could statistically delay the eruption for a century with 1 and 15 years of effort, respectively. </span><span>All actions require essentially untested geoengineering challenges along with economic, political </span><span>and general public acceptance. Further work is required to refine the science, provide cost </span><span>estimates, and compare cost effectiveness with interventions focusing on adapting to a </span><span>supereruption.</span></p></body></html>", "user": {"username": "Denkenberger"}}, {"_id": "3MbPLi9GJgsdvfmW9", "title": "[Paper] Global Catastrophic and Existential Risks Communication Scale, similar to Torino scale ", "postedAt": "2018-01-14T10:07:26.123Z", "htmlBody": "<html><body><p>&#xA0;</p>\n<p>We (Alexey Turchin and David Denkenberger) have a new paper out where we suggest a scale to communicate the size of global catastrophic and existential risks.</p>\n<p>For impact risks, we have the <a href=\"https://en.wikipedia.org/wiki/Torino_scale\">Torino scale</a>&#xA0;of asteroid danger which has five color-coded levels. For hurricanes we have the <a href=\"https://en.wikipedia.org/wiki/Saffir%E2%80%93Simpson_scale\">Saffir-Simpson scale</a> of five categories. Here we present similar scale for communicating the size of the global catastrophic and existential risks.</p>\n<p>Typically, some vague claims about probability are used as a communication tool for existential risks, for example, some may say, &#x201C;There is a 10 per cent chance that humanity will be exterminated by nuclear war&#x201D;. But the probability of the most serious global risks is difficult to measure, and the probability estimate doesn&#x2019;t take into account other aspects of risks, for example, preventability, uncertainty, timing, relation to other risks etc. As a result, claims about probability could be misleading or produce reasonable skepticism.&#xA0;</p>\n<p>To escape these difficulties, we suggested creating a scale to communicate existential risks, similar to the Torino scale of asteroid danger.</p>\n<p>In our scale, there are six color codes, from white to purple. If hard probabilities are known, the color corresponds to probability intervals for a fixed timeframe of 100 years, which helps to solve uncertainty and timing.&#xA0;</p>\n<p>However, for most serious risks, like AI, their probabilities are not known, but the required levels of prevention action are known. For these cases, the scale communicates the risk&#x2019;s size through the required level of prevention action. In some sense, it is similar to Updateless Decision Theory, where an event&#x2019;s significance is measured, not by observable probabilities, but by the utility of corresponding actions. The system would work, because in many cases of x-risks, the required prevention actions are not very sensitive to the probability.</p>\n<p>How should the scale be implemented in practice? If probabilities are not known, a group of experts should aggregate available information and communicate it to the public and policymakers, saying something like: &quot;We think that AI is a red risk, a pandemic is a yellow risk and asteroid danger is a green risk.&#x201D; It would help to bring some order to the public perception of each risk&#x2014;where, currently, asteroid danger is clearly overestimated compared to the risks of AI risk&#x2014;without making unsustainable claims about unmeasurable probabilities.</p>\n<p>In the article we have already given some estimates for the most well-known existential risks, but clearly they are open to debate.&#xA0;</p>\n<p>Here&apos;s the abstract:</p>\n<p>Existential risks threaten the future of humanity, but they are difficult to measure. However, to communicate, prioritize and mitigate such risks it is important to estimate their relative significance. Risk probabilities are typically used, but for existential risks they are problematic due to ambiguity, and because quantitative probabilities do not represent some aspects of these risks. Thus, a standardized and easily comprehensible instrument is called for, to communicate dangers from various global catastrophic and existential risks. In this article, inspired by the Torino scale of asteroid danger, we suggest a color-coded scale to communicate the magnitude of global catastrophic and existential risks. The scale is based on the probability intervals of risks in the next century if they are available. The risks&#x2019; estimations could be adjusted based on their severities and other factors. The scale covers not only existential risks, but smaller size global catastrophic risks. It consists of six color levels, which correspond to previously suggested levels of prevention activity. We estimate artificial intelligence risks as &#x201C;red&#x201D;, while &#x201C;orange&#x201D; risks include nanotechnology, synthetic biology, full-scale nuclear war and a large global agricultural shortfall (caused by regional nuclear war, coincident extreme weather, etc.) The risks of natural pandemic, supervolcanic eruption and global warming are marked as &#x201C;yellow&#x201D; and the danger from asteroids is &#x201C;green&#x201D;.</p>\n<p>The paper is published in Futures https://www.sciencedirect.com/science/article/pii/S001632871730112X</p>\n<p>If you want to read the full paper, here&apos;s a link on preprint: https://philpapers.org/rec/TURGCA&#xA0;&#xA0;</p>\n<p>Two main pictures from the paper:&#xA0;</p>\n<p>&#xA0;</p>\n<p><span><img src=\"http://immortality-roadmap.com/xriskstab1.jpg\" alt=\"\"></span></p>\n<p>&#xA0;<img src=\"http://immortality-roadmap.com/xriskstab2.jpg\" alt=\"\"></p></body></html>", "user": {"username": "turchin"}}, {"_id": "vuQobY5yQvWdtPiab", "title": "EA #GivingTuesday Fundraiser Matching Retrospective", "postedAt": "2018-01-14T00:48:02.297Z", "htmlBody": "<p>by Avi Norowitz&nbsp;</p>\n<h1>Summary</h1>\n<p>For #GivingTuesday this year, Facebook had <a href=\"https://donations.fb.com/givingtuesday/\">announced</a>&nbsp;that Facebook and the Gates Foundation would match donations made on Facebook up to $2 million. A number of us saw this as a <a href=\"/ea/1hl/givingtuesday_counterfactual_donation_matching_is/\">rare opportunity</a>&nbsp;to get donations to EA nonprofits counterfactually matched, similar to employer matching programs that match employee donations to any registered nonprofit. We created a Facebook <a href=\"https://www.facebook.com/groups/eagivingtuesday/\">group</a>, a Facebook <a href=\"https://www.facebook.com/events/566463213697311/\">event</a>, and a <a href=\"https://docs.google.com/spreadsheets/u/1/d/18l2W7JFeura0JLSaRAlB9AS6_bDJSSp7B2TAXoePCaQ/edit?usp=sharing\">spreadsheet</a>&nbsp;to help coordinate our efforts to capture as much of the match as possible. In the end, we had ~$379k in donations. Unfortunately, because the matching funds were exhausted at 1 minute and 26 seconds into the match, only ~$48k (~13%) of these donations were matched. As a result, it\u2019s plausible that the most important effects of our efforts may have been indirect, and these effects may have been a mix of positive and negative. We consider some of the lessons we learned, whether we should try this again in 2018 (polls suggest yes), and some questions regarding the implementation in 2018. We also briefly discuss our follow-up work with nonprofits, which show that, in general, nonprofits received amounts similar to or greater than the amounts we had estimated.</p>\n<h1><span>Table of contents</span></h1>\n<p><span><a href=\"#h.yf9csqk5ru0h\">Summary</a></span></p>\n<p><span><a href=\"#h.mznd5k2jbi77\">Table of contents</a></span></p>\n<p><span><a href=\"#h.wn9jfciugnfw\">Acknowledgments</a></span></p>\n<p><span><a href=\"#h.jfh0jmpxkxv2\">The Facebook #GivingTuesday matching program</a></span></p>\n<p><span><a href=\"#h.x45yhx2y2vyc\">The opportunity</a></span></p>\n<p><span><a href=\"#h.8v6531atlozr\">Our strategy to capture the matching funds</a></span></p>\n<p><span><a href=\"#h.wrj423rynfcl\">The pledges and donations</a></span></p>\n<p><span><a href=\"#h.nnpw9882iz0k\">The matches</a></span></p>\n<p><span><a href=\"#h.wsxrhijh65vq\">Estimated amounts donated and matched, by cause area and nonprofit</a></span></p>\n<p><span><a href=\"#h.4ummb246sbsk\">Follow-up with nonprofits</a></span></p>\n<p><span><a href=\"#h.jzqigsk0hqmq\">Potential effects</a></span></p>\n<p><span><a href=\"#h.nla39y1abe2k\">Positive</a></span></p>\n<p><span><a href=\"#h.q961r8opyfm3\">~$48k in counterfactually valid donation matches</a></span></p>\n<p><span><a href=\"#h.ox9zzdopu97j\">Some EAs may have been encouraged to donate more</a></span></p>\n<p><span><a href=\"#h.pr83csflasmh\">Some non-EAs may have been encouraged to donate to EA nonprofits</a></span></p>\n<p><span><a href=\"#h.al5g2nrg44qe\">Increased openness about donating</a></span></p>\n<p><span><a href=\"#h.farzri75h4t0\">Learning experience</a></span></p>\n<p><span><a href=\"#h.qqfa3sjp5n7o\">Mixed</a></span></p>\n<p><span><a href=\"#h.pb6oory6jna4\">Facebook newsfeeds were flooded with fundraisers</a></span></p>\n<p><span><a href=\"#h.j3imy6yytr14\">Potential tax effects</a></span></p>\n<p><span><a href=\"#h.45qmazit611v\">Negative</a></span></p>\n<p><span><a href=\"#h.zaqle5bdxexx\">Some donations may have been made suboptimally</a></span></p>\n<p><span><a href=\"#h.i0qlc9li09h4\">Some productivity may have been lost</a></span></p>\n<p><span><a href=\"#h.95xyw2rm5a4g\">Lessons learned</a></span></p>\n<p><span><a href=\"#h.d03i262h9jbx\">EAs can work impressively well as a community</a></span></p>\n<p><span><a href=\"#h.i2z6scc19zri\">We should have focused more on donating fast</a></span></p>\n<p><span><a href=\"#h.c26v65xhweq7\">We should have prepared more for payment problems</a></span></p>\n<p><span><a href=\"#h.ysu4pq90u4fk\">We should have began working on this earlier</a></span></p>\n<p><span><a href=\"#h.u4w3cbj4n7dy\">We should have suggested that people not donate privately</a></span></p>\n<p><span><a href=\"#h.fz7rwzxdo8ty\">EAs outside of the US ran into problems</a></span></p>\n<p><span><a href=\"#h.ado41fzg6njw\">It might be worthwhile for the EA community to mobilize on other similar opportunities</a></span></p>\n<p><span><a href=\"#h.nwya08n8t39b\">Should we try again in 2018? Polls suggest yes</a></span></p>\n<p><span><a href=\"#h.ii6bzpdpp7t6\">Questions for #GivingTuesday 2018</a></span></p>\n<p><span><a href=\"#h.9kwzsyjgkkgy\">Should we encourage people to create multiple fundraisers for the same nonprofit for their own donations?</a></span></p>\n<p><span><a href=\"#h.xj5in8e9hauy\">How much time will we have?</a></span></p>\n<p><span><a href=\"#h.1kiqqfyiu4ab\">How should we keep track of fundraisers?</a></span></p>\n<p><span><a href=\"#h.lopkhtpi2f7q\">What other unexpected problems might we face?</a></span>&nbsp;</p>\n<h1><span>Acknowledgments</span></h1>\n<p><span>The following other members of the EA #GivingTuesday organizer team (listed in alphabetical order by last name) were enormously helpful in making this all happen, including providing valuable assistance with this post.</span></p>\n<ul>\n<li><span>Arushi Gupta</span></li>\n<li><span>William Kiely</span></li>\n<li><span>Angelina Li</span></li>\n<li><span>Bruno Parga</span></li>\n<li><span>Anisha Zaveri</span></li>\n</ul>\n<p>I\u2019m also grateful to the many other members of the EA community for their valuable contributions, and the 183 participants who created fundraisers, pledged donations to fundraisers, and did an admirable job maintaining the <span><a href=\"https://docs.google.com/spreadsheets/d/18l2W7JFeura0JLSaRAlB9AS6_bDJSSp7B2TAXoePCaQ/edit#gid=2040380990\">spreadsheet</a></span><span>.</span></p>\n<p><span>We also greatly appreciate the multiple nonprofits who provided us valuable information for our follow-up work.<br></span></p>\n<h1><span>The Facebook #GivingTuesday matching program</span></h1>\n<p>On October 27, 2017, Facebook had <span><a href=\"https://www.facebook.com/nonprofits/posts/10154901188335918\">announced</a></span><span>&nbsp;that Facebook and the Gates Foundation would match donations made on Facebook on #GivingTuesday (November 28, 2017) up to $2 million:</span></p>\n<p><span>On #GivingTuesday, Facebook and the Bill &amp; Melinda Gates Foundation will be matching up to $2 million of funds raised on Facebook for US nonprofits. Facebook is also waiving its fees for donations made to nonprofits on Facebook this #GivingTuesday.</span></p>\n<p><span>Donations to nonprofits made through Facebook\u2019s charitable giving tools on November 28th will be matched up to $50,000 per nonprofit or $1,000 per fundraiser or donate button, until the $2 million in matching funds run out. The match will begin at 8AM EST (5AM PST).</span></p>\n<p>Facebook ran a <span><a href=\"https://nonprofits.fb.com/2016/11/17/two-great-ways-your-followers-can-give-this-givingtuesday/\">similar matching program</a></span><span>&nbsp;for the previous year, and reported that the initial match of $500,000 was reached \u201cwithin hours,\u201d and that $6.79 million had been raised by Facebook fundraisers overall. Facebook also reported that after the initial $500,000 match had run out, the Gates Foundation had increased the matching funds to $900,000.</span></p>\n<h1><span>The opportunity</span></h1>\n<p>A number of us saw this as a <span><a href=\"/ea/1hl/givingtuesday_counterfactual_donation_matching_is/\">rare opportunity</a></span><span>&nbsp;to get donations to EA nonprofits counterfactually matched, similar to employer matching programs that match employee donations to any registered nonprofit. In particular, this opportunity appeared different than most matching opportunities in the following respects:</span></p>\n<ul>\n<li>The matching funds were available to nearly any US nonprofit registered as a 501(c)3<span>. This permitted donations to EA nonprofits, and reduced the risk that donation decisions would be distorted towards less effective nonprofits because of the match.<br><br></span></li>\n<li>Matching funds that were not directed to EA nonprofits would instead have gone to nonprofits of more average effectiveness. Although the match was in part funded by the Gates Foundation,<sup><a href=\"#ftnt1\">[1]</a></sup><span>&nbsp;the matching funds were reportedly limited to $2 million, and we expected that the entire $2 million would have been captured by other non-EA fundraisers without our efforts.</span></li>\n</ul>\n<p>We saw this donation matching opportunity as one that avoided most of the pitfalls described by this old GiveWell <span><a href=\"https://blog.givewell.org/2011/12/15/why-you-shouldnt-let-donation-matching-affect-your-giving/\">blog post</a></span>&nbsp;on donation matching.</p>\n<h1><span>Our strategy to capture the matching funds</span></h1>\n<p>The matching program had a limit of $1000 per Facebook <span><a href=\"https://donations.fb.com/\">fundraiser or donate button</a></span>&nbsp;and<span>&nbsp;a limit of $50,000 per nonprofit. Our plan was to encourage people in the EA community to create fundraisers for EA nonprofits, and encourage EAs to donate to up to $1000 to each of those fundraisers as soon as possible after 8am EST (5am PST) on #GivingTuesday.</span></p>\n<p>To help coordinate these efforts, we created an <span><a href=\"https://www.facebook.com/groups/eagivingtuesday/\">EA #GivingTuesday Fundraiser Matching</a></span><sup><a href=\"#ftnt2\">[2]</a></sup>&nbsp;group on Facebook. We realized that using a Facebook group alone to handle coordination between people creating fundraisers and donating to them would be difficult to manage and scale, so we created a <span><a href=\"https://docs.google.com/spreadsheets/d/18l2W7JFeura0JLSaRAlB9AS6_bDJSSp7B2TAXoePCaQ/edit#gid=2040380990\">Google Sheet</a></span>&nbsp;where some people could post fundraisers and other people could pledge to donate to them. We also created a Facebook <span><a href=\"https://www.facebook.com/events/566463213697311/\">event</a></span><span>&nbsp;for people to easily add to their calendars and invite their EA friends.</span></p>\n<p>To create awareness about the opportunity, we posted to the <span><a href=\"/ea/1hl/givingtuesday_counterfactual_donation_matching_is/\">EA Forum</a></span>&nbsp;and a few EA groups on Facebook. We also invited our EA friends to our Facebook <span><a href=\"https://www.facebook.com/events/566463213697311/\">event</a></span><span>, and encouraged people to invite their other EA friends as well.</span></p>\n<h1><span>The pledges and donations</span></h1>\n<p>On Monday morning, we had a total of $67k in pledges on our <span><a href=\"https://docs.google.com/spreadsheets/d/18l2W7JFeura0JLSaRAlB9AS6_bDJSSp7B2TAXoePCaQ/edit\">Google Sheet</a></span><span>. By Tuesday morning, that amount had increased to $365k.</span></p>\n<p>At Tuesday starting at 8am EST, more than 100 EAs -- including a large number of EAs awake at 5am on the west coast -- rushed to donate hundreds of thousands of dollars to EA nonprofits to capture as much of the matching funds as possible. EA Facebook was flooded with one $1000 donation after another. The mood was well captured in this <span><a href=\"https://www.facebook.com/groups/eagivingtuesday/permalink/1994153440866560/\">post</a></span>&nbsp;by Tyler<span>&nbsp;John:</span></p>\n<p><span><img title=\"\" src=\"http://res.cloudinary.com/cea/image/upload/v1667996120/mirroredImages/vuQobY5yQvWdtPiab/ovszk1gh9xaqbjtbtzqo.png\" alt=\"\"></span></p>\n<p>By 8:49am EST, Facebook had begun reporting to users that the $2 million in matching funds had run out.<sup><a href=\"#ftnt3\">[3]</a></sup></p>\n<p>By 11:47am EST, we had finished manually collecting donation data from the fundraisers on the spreadsheet. We documented $338k in donations, an impressive 93% of the amount pledged.<sup><a href=\"#ftnt4\">[4]</a></sup></p>\n<h1><span>The matches</span></h1>\n<p>We had expected Facebook to begin reporting match amounts to fundraisers and nonprofits the following day. Unfortunately, Facebook kept us in suspense and <span>didn\u2019t report match amounts until Friday afternoon.</span></p>\n<p>Although the amounts pledged and donated were very impressive, the amount matched was fairly disappointing. Through some <span><a href=\"https://www.facebook.com/groups/eagivingtuesday/permalink/1995654614049776/?comment_id=1995668864048351&amp;reply_comment_id=1995675610714343&amp;comment_tracking=%7B%22tn%22%3A%22R%22%7D\">impressive detective work</a></span><span>, some EAs had found that the matching funds ran out at 1 minute and 26 seconds into the match. We attempted to estimate the match amounts from a combination of information reported by people who had created fundraisers and our observations of donation timestamps. The following are our current estimates:</span></p>\n<ul>\n<li>$380k pledged<sup><a href=\"#ftnt5\">[5]</a></sup></li>\n<li>$379k donated (99.7% of the amount pledged)<sup><a href=\"#ftnt6\">[6]</a></sup></li>\n<li>$48k matched (13% of the amount donated)<sup><a href=\"#ftnt7\">[7]<br><br></a></sup></li>\n</ul>\n<h1><span>Estimated amounts donated and matched, by cause area and nonprofit</span></h1>\n<p>Most of the donations went to animal (42%) and global poverty (33%) nonprofits. This differs from donations and cause area preferences reported in the <span><a href=\"/ea/1e5/ea_survey_2017_series_cause_area_preferences/\">EA Survey 2017</a></span>&nbsp;results. The differences appear to be partially the result of a small number of large donors and<span>&nbsp;nonprofits who were actively involved in attempting to capture the matching funds.</span></p>\n<p><span>Donations to long-term future nonprofits were unusually successful at generating matching funds, with 22% of donations matched, compared to the average of 13% (weighted by donations).</span></p>\n<p><span><img title=\"Chart\" src=\"http://res.cloudinary.com/cea/image/upload/v1667996120/mirroredImages/vuQobY5yQvWdtPiab/t4fidllcarrggqhridvg.png\" alt=\"\"></span></p>\n<p>Machine Intelligence Research Institute (MIRI) received more donations than any other nonprofit ($49k),<sup><a href=\"#ftnt8\">[8]</a></sup>&nbsp;followed by the Against Malaria Foundation ($43k), GiveWell ($39k),<sup><a href=\"#ftnt9\">[9]</a></sup>&nbsp;<span>and The Humane League ($38k).</span></p>\n<p>The Animal Charities Evaluators\u2019 Recommended Charities Fund had a larger percent of donations matched than any other nonprofit, with 50<span>% of donations matched, compared to the average of 13% (weighted by donations).</span></p>\n<p><span><img title=\"Chart\" src=\"http://res.cloudinary.com/cea/image/upload/v1667996120/mirroredImages/vuQobY5yQvWdtPiab/u3gmpsquvu73oslnlwd7.png\" alt=\"\"></span></p>\n<h1><span>Follow-up with nonprofits</span></h1>\n<p>To ensure that nonprofits receive the expected amounts, and to help us estimate the effectiveness our our work, we\u2019ve been following up with all nonprofits that received donations according to our fundraiser worksheet. We\u2019ve received information from nonprofits on donation and match amounts for ~81% of the estimated amount donated. In general, we\u2019ve found that the amounts received by the nonprofits have been similar to or greater than the amounts we had estimated. (One notable exception is that Animal Ethics, which should have received a total of $150 in donations through one fundraiser, reported to us that they had received only $104. We haven\u2019t been able to determine the cause of this discrepancy.)</p>\n<p>For the nonprofits that reported amounts for both donations and matches to us, the table below shows the nonprofit name our estimated amounts, and the amounts reported by nonprofits.</p>\n<div>\n<table><colgroup><col><col><col><col><col></colgroup>\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>\n<p><span>Total donated (estimated via fundraisers)</span></p>\n</td>\n<td>\n<p><span>Total donated (reported by nonprofit)</span></p>\n</td>\n<td>\n<p><span>Total matched (estimated via fundraisers)</span></p>\n</td>\n<td>\n<p><span>Total matched (reported by nonprofit)</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>MIRI</span></p>\n</td>\n<td>\n<p><span>$48,526</span></p>\n</td>\n<td>\n<p><span>$48,422</span></p>\n</td>\n<td>\n<p><span>$11,272</span></p>\n</td>\n<td>\n<p><span>$11,372</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Against Malaria</span></p>\n</td>\n<td>\n<p><span>$43,126</span></p>\n</td>\n<td>\n<p><span>$67,716</span></p>\n</td>\n<td>\n<p><span>$5,979</span></p>\n</td>\n<td>\n<p><span>$7,986</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>GiveWell</span></p>\n</td>\n<td>\n<p><span>$39,225</span></p>\n</td>\n<td>\n<p><span>$40,484</span></p>\n</td>\n<td>\n<p><span>$0</span></p>\n</td>\n<td>\n<p><span>$1,254</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Humane League</span></p>\n</td>\n<td>\n<p><span>$37,915</span></p>\n</td>\n<td>\n<p><span>$48,228</span></p>\n</td>\n<td>\n<p><span>$4,380</span></p>\n</td>\n<td>\n<p><span>$5,260</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Good Food Inst.</span></p>\n</td>\n<td>\n<p><span>$25,681</span></p>\n</td>\n<td>\n<p><span>$33,444</span></p>\n</td>\n<td>\n<p><span>$1,100</span></p>\n</td>\n<td>\n<p><span>$3,100</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>ACE Rec. Fund</span></p>\n</td>\n<td>\n<p><span>$24,292</span></p>\n</td>\n<td>\n<p><span>$33,460</span></p>\n</td>\n<td>\n<p><span>$12,220</span></p>\n</td>\n<td>\n<p><span>$13,220</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>CIWF USA</span></p>\n</td>\n<td>\n<p><span>$23,076</span></p>\n</td>\n<td>\n<p><span>$23,081</span></p>\n</td>\n<td>\n<p><span>$1,500</span></p>\n</td>\n<td>\n<p><span>$1,500</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>GiveDirectly</span></p>\n</td>\n<td>\n<p><span>$22,719</span></p>\n</td>\n<td>\n<p><span>$44,786</span></p>\n</td>\n<td>\n<p><span>$100</span></p>\n</td>\n<td>\n<p><span>$4,600</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Sentience Inst.</span></p>\n</td>\n<td>\n<p><span>$15,358</span></p>\n</td>\n<td>\n<p><span>$14,177</span></p>\n</td>\n<td>\n<p><span>$1,249</span></p>\n</td>\n<td>\n<p><span>$2,249</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>New Harvest</span></p>\n</td>\n<td>\n<p><span>$7,475</span></p>\n</td>\n<td>\n<p><span>$7,491</span></p>\n</td>\n<td>\n<p><span>$0</span></p>\n</td>\n<td>\n<p><span>$0</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Innov Poverty</span></p>\n</td>\n<td>\n<p><span>$6,000</span></p>\n</td>\n<td>\n<p><span>$6,000</span></p>\n</td>\n<td>\n<p><span>$0</span></p>\n</td>\n<td>\n<p><span>$0</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>BERI</span></p>\n</td>\n<td>\n<p><span>$4,400</span></p>\n</td>\n<td>\n<p><span>$5,400</span></p>\n</td>\n<td>\n<p><span>$1,000</span></p>\n</td>\n<td>\n<p><span>$1,000</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>One Step</span></p>\n</td>\n<td>\n<p><span>$3,779</span></p>\n</td>\n<td>\n<p><span>$4,179</span></p>\n</td>\n<td>\n<p><span>$2,000</span></p>\n</td>\n<td>\n<p><span>$2,000</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Faunalytics</span></p>\n</td>\n<td>\n<p><span>$2,599</span></p>\n</td>\n<td>\n<p><span>$2,599</span></p>\n</td>\n<td>\n<p><span>$500</span></p>\n</td>\n<td>\n<p><span>$500</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Cool Earth</span></p>\n</td>\n<td>\n<p><span>$1,674</span></p>\n</td>\n<td>\n<p><span>$3,900</span></p>\n</td>\n<td>\n<p><span>$0</span></p>\n</td>\n<td>\n<p><span>$0</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Reducetarian</span></p>\n</td>\n<td>\n<p><span>$150</span></p>\n</td>\n<td>\n<p><span>$150</span></p>\n</td>\n<td>\n<p><span>$0</span></p>\n</td>\n<td>\n<p><span>$0</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Animal Ethics</span></p>\n</td>\n<td>\n<p><span>$150</span></p>\n</td>\n<td>\n<p><span>$104</span></p>\n</td>\n<td>\n<p><span>$0</span></p>\n</td>\n<td>\n<p><span>$0</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Total</span></p>\n</td>\n<td>\n<p><span>$306,145</span></p>\n</td>\n<td>\n<p><span>$383,621</span></p>\n</td>\n<td>\n<p><span>$41,300</span></p>\n</td>\n<td>\n<p><span>$54,041</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><br>The following graph shows the donated amount we had estimated via fundraisers compared to the donated amount reported to us by nonprofits. (We\u2019ve omitted Reducetarian Foundation and Animal Ethics from the graph because the donated amounts are too low to be visible.)</p>\n<p><span><img title=\"Chart\" src=\"http://res.cloudinary.com/cea/image/upload/v1667996120/mirroredImages/vuQobY5yQvWdtPiab/kunwudjblfsyju0o3iak.png\" alt=\"\"></span></p>\n<p>And the following graph shows the match amount we had estimated via fundraisers compared to the match amount reported to us by nonprofits.</p>\n<p><span><img title=\"Chart\" src=\"http://res.cloudinary.com/cea/image/upload/v1667996120/mirroredImages/vuQobY5yQvWdtPiab/kohv0ssl9t5hb2jz7ciz.png\" alt=\"\"></span></p>\n<p>Although in general the total donation and match amounts were higher than our estimates, we don\u2019t know how much of difference was caused by our initiative.</p>\n<h1>Potential effects</h1>\n<p><span>The EA #GivingTuesday Fundraiser Matching project may have had mixed effects, some positive and some negative. In this section, I\u2019ll provide an overview of the various effects the project may have had. The views in this section (some of which is very speculative) should be seen as my own, and may not necessarily reflect the views of other people involved.</span></p>\n<h2>Positive</h2>\n<h3>~$48k in counterfactually valid donation matches</h3>\n<p><span>Though our estimated match percent of 13% is lower than I had expected, I consider the absolute estimated match amount of $48k to be fairly impressive. We think these matches are counterfactually valid and mostly attributable to this initiative, for the following reasons:</span></p>\n<ul>\n<li>Facebook <span><a href=\"https://www.facebook.com/nonprofits/photos/a.85612830917.95996.41130665917/10154974265945918/?type=3&amp;permPage=1\">reports</a></span>&nbsp;that they raised $45 million on #GivingTuesday, thereby exhausting the entire pool of matching funds<span>. This provides evidence that any part of the match we had not captured would have gone to nonprofits of more average effectiveness rather than to other potentially effective interventions funded by the Gates Foundation.<br><br></span></li>\n<li>Anecdotally, we didn\u2019t see evidence that many EAs<span>&nbsp;had plans to take advantage of the donation matching before we began working on it. In addition, in our estimates, we\u2019ve only included data from our fundraisers spreadsheet, so donations and matches for other fundraisers aren\u2019t included in our estimates.<br><br></span></li>\n</ul>\n<h3>Some EAs may have been encouraged to donate more</h3>\n<p><span>The matching incentive and the excitement around the matching may have encouraged some EAs to make donations they would have not made otherwise, or donate more than they would have donated otherwise.</span></p>\n<h3><span>Some non-EAs may have been encouraged to donate to EA nonprofits</span></h3>\n<p><span>The hundreds of fundraisers for EA nonprofits flooding Facebook newsfeeds and timelines, combined with Facebook\u2019s advertising about the matching, may have encouraged some non-EAs to donate to some EA fundraisers. </span></p>\n<p>We attempted to estimate the amount of non-EA donations by looking at fundraisers with \u201csurplus\u201d donations, where the total donation amount exceeded total pledge amount.<sup><a href=\"#ftnt10\">[10]</a></sup>&nbsp;As we began looking at the fundraisers with surplus donations more closely, however, we found that a lot of the<span>se surplus donations appeared to be from EA\u2019s. We went through each fundraiser with surplus donations, and based on our subjective judgment, we excluded fundraisers where most of the surplus donations appeared to originate from EA\u2019s. We also reviewed all fundraisers with non-zero donation amounts that undershot their pledges to look for non-EA donations, and made a subjective judgment about the non-EA donation amounts for each of those fundraisers. We estimate that around $11k (or 3% of donations) originated from non-EA\u2019s, but we feel very uncertain about this estimate.</span></p>\n<p>The methodology used to generate the above estimate can be found <span><a href=\"https://docs.google.com/spreadsheets/d/1QLT6qhd7uwnB8ysG3eLckuCqsgORJ1KMvIkqpYIZQXI/edit\">here</a></span><span>.</span></p>\n<h3><span>Increased openness about donating</span></h3>\n<p><span>The donors participating in our initiative were unusually open about their donations, by pledging donations on the publicly shared spreadsheet, and by making (mostly) public donations on Facebook fundraisers.</span></p>\n<p>This openness may help <span><a href=\"/ea/7q/to_inspire_people_to_give_be_public_about_your/\">inspire people to give</a></span><span>, so I\u2019m considering this a positive. On the other hand, it\u2019s possible that donating large amounts publicly may have been perceived negatively by some non-EAs.</span></p>\n<h3><span>Learning experience</span></h3>\n<p><span>The project provided a number of useful learning experiences that will be useful if we try again in 2018, and may also be useful for similar EA community projects.</span></p>\n<h2><span>Mixed</span></h2>\n<h3><span>Facebook newsfeeds were flooded with fundraisers</span></h3>\n<p><span>As an unintended side effect of our project, the newsfeeds of people with EA friends who had created fundraisers were flooded with these fundraisers. Some people (EAs or outsiders) may have found this spammy or annoying, and it\u2019s possible this could have reflected negatively on the image of EA as a community or individual EAs. Other people may have found it inspiring to see so many people trying to raise money for EA nonprofits, and to see one fundraiser get fully funded after the other.</span></p>\n<p><span>It\u2019s also possible that the flood of fundraisers in newsfeeds resulted in some productivity loss, though it\u2019s similarly possible that it encouraged people to get off of Facebook and do something more productive instead.</span></p>\n<h3><span>Potential tax effects</span></h3>\n<p>The US tax law changes beginning in 2018 will reduce the tax benefits of donating for many people by increasing&nbsp;the standard deduction<span>&nbsp;and reducing tax rates. If some people were encouraged to front-load their 2018 donations into 2017 to try to capture the match, they may receive some tax benefits, which could potentially later translate into increased donations in the future.</span></p>\n<p>On the negative side, one or more people in the UK donated and lost the opportunity to claim Gift Aid for their donations. In <span><a href=\"https://www.facebook.com/groups/eagivingtuesday/permalink/2007790702836167/\">one such case</a></span><span>&nbsp;though, Facebook has been willing to refund these donations.</span></p>\n<p>&nbsp;</p>\n<h2><span>Negative</span></h2>\n<div>\n<h3><span>Some donations may have been made suboptimally</span></h3>\n<p><span>There are a number of reasons to believe that some donations may have been made suboptimally:<br></span></p>\n<ul>\n<li>Most of the donations were pledged within the 24 hours before the match began.<sup><a href=\"#ftnt11\">[11]</a></sup><span>&nbsp;If people hadn\u2019t already thoroughly investigated where to donate, they may have rushed their donations anyway based on the judgment that it was preferable to try to capture the match.<br><br></span></li>\n<li><span>It was unclear whether it was possible to donate to some nonprofits, either because they were not 501(c)3s themselves, or because they were smaller programs within larger 501(c)3s.<br><br></span></li>\n<li>To pledge donations to a nonprofit, it was necessary for donors to find the necessary fundraisers open for that nonprofit, ask others to create the fundraisers, or create the fundraisers themselves. Some people may have had difficulty getting enough fundraisers created for their preferred nonprofit, and may have donated to other less effective nonprofits instead<span>.<br><br></span></li>\n<li><span>A few nonprofits appeared at risk of hitting the $50,000 match limit. It\u2019s possible that some people had donated to nonprofits they saw as somewhat less effective to avoid hitting the match limit.</span></li>\n</ul>\n<p>That being said, to try to mitigate this risk, we did add the following line to our instructions: \u201c<span>Be careful about donating to an organization you think is less effective just for the match.</span>&nbsp;Some EA organizations may be 4x or more effective as others so it might be better to forgo a match instead of donating to a less effective organization.\u201d</p>\n<h3><span>Some productivity may have been lost</span></h3>\n<p>If we consider the&nbsp;hours of work on this project by the core people involved<span>, as well as the 183 EAs involved in creating fundraisers and donating to fundraisers, this may have taken up 500 hours or more of people\u2019s time.</span></p>\n<p><span>There may have also been some additional productivity loss caused by the sleep deprivation of people on the west coast waking up before 5am to donate on time.</span></p>\n</div>\n<h2><span>Lessons learned</span></h2>\n<div>\n<p><span>As noted above, the EA #GivingTuesday Fundraiser Matching project provided a number of lessons learned that may be useful if we try again in 2018, and for other potential similar projects that involve the EA community.</span></p>\n<h3>EAs can work impressively well as a community</h3>\n</div>\n<div>\n<p>I estimate that we had 183 EAs creating fundraisers and pledging to donate to them. Despite the large numbers of people involved, EAs worked together impressively well. I was worried that our spreadsheet would be unmanageable with so many people working on it, but this risk did not materialize, and the spreadsheet was well maintained by the participants.</p>\n<h3>We should have focused more on donating fast</h3>\n</div>\n<div>\n<p>I did not anticipate that the matching funds would have run out in less than 2 minutes. If we had anticipated this, we could have emphasized the need to donate fast, and provided more instructions on donating fast, such as:</p>\n<ul>\n<li><span>Opening tabs for all fundraisers in advance so the only step necessary would be one \u201cDonate\u201d click per fundraiser.<br><br></span></li>\n<li>Watch the atomic clock at <span><a href=\"https://www.time.gov/\">time.gov</a></span><span>&nbsp;and start donating immediately after 8:00:00am EST.</span></li>\n</ul>\n</div>\n<div>&nbsp;</div>\n<h3>We should have prepared more for payment problems</h3>\n<div>\n<p><span>A number of people experienced payment problems when trying to donate. We should have provided more instructions on preparation to mitigate these problems, such as:</span></p>\n<ul>\n<li><span>Transferring funds and requesting credit line increases in advance as necessary.<br><br></span></li>\n<li><span>Adding multiple payment methods in advance to Facebook Payments.<br><br></span></li>\n<li><span>Calling all credit card companies in advance to inform them about the planned donations, to reduce the risk of donations being flagged as suspicious.<br><br></span></li>\n<li><span>Keeping one\u2019s phone nearby, unlocked, and with text notifications at the max volume to provide the opportunity to quickly reply to automated credit card fraud text alerts.</span></li>\n</ul>\n<p>&nbsp;</p>\n</div>\n<h3>We should have began working on this earlier</h3>\n<div>\n<p><span>I only became aware of the Facebook donating matching when Arushi Gupta informed me about it on Thanksgiving morning, less than a week before #GivingTuesday. If we had begun working on this earlier, we could have seen a number of benefits:</span></p>\n<ul>\n<li>We could have planned this better, including creating the shared spreadsheet earlier.<br><span><br></span></li>\n<li><span>We could have started increasing awareness earlier, particularly before Thanksgiving when many people were apparently on a hiatus from Facebook.<br><br></span></li>\n<li><span>We could have given EAs more time to create fundraisers and plan their donations.<br><br></span></li>\n<li><span>We could have communicated our plans earlier to nonprofits. This could have helped nonprofits who were not 501(c)3\u2019s organize ways to receive funds through other 501(c)3\u2019s. This could have also encouraged some nonprofits to sign-up for Facebook Payments so they could receive their funds faster and get more information from Facebook.<br><br></span></li>\n</ul>\n</div>\n<h3>We should have suggested that people not donate privately</h3>\n<div>Private donations made it more difficult for us to collect data on whether donations were matched, because it prevented us from seeing the timestamps of the individual donations.</div>\n<h3>EAs outside of the US ran into problems</h3>\n<div>\n<p><span>Facebook had announced they were matching donations for nonprofits in the UK as well. So early on, we created a separate Google Sheet and Facebook event for people in the UK. It turns out, however, that for nonprofits in the UK to be eligible, it was necessary that they were registered with Facebook Payments. Unfortunately, it turned out that no EA nonprofits in the UK were registered with Facebook Payments, and our early efforts may have been confusing to people.</span></p>\n<p><span>We also found that while some EAs outside the US were able to create fundraisers for US nonprofits, some of them did not see banners from Facebook with information about the amount matching. We interpret this to mean that donations to these fundraisers were not eligible for the match.</span></p>\n<p><span>We should take these problems into account if we try this again in 2018.</span></p>\n</div>\n<h3><span>It might be worthwhile for the EA community to mobilize on other similar opportunities</span></h3>\n<div>\n<p>The opportunities to capture counterfactually valid donation matches to EA nonprofits without distorting donor behavior (too much) seem fairly limited, but perhaps there are others that we\u2019re missing? More broadly, perhaps we should be looking for other kinds of opportunities where the EA community may be able to raise counterfactual donations to EA nonprofits. A number of EAs already do effective work each year <span><a href=\"https://www.facebook.com/groups/1666606496928923/\">raising</a></span><span>&nbsp;counterfactually valid donations through Project For Awesome, but here are some other ideas that come to mind:</span></p>\n<ul>\n<li><span><a href=\"https://pineapplefund.org/\">The Pineapple Fund</a></span>: Some EAs have <span><a href=\"https://www.facebook.com/groups/eahangout/permalink/1608140209272897/\">mobilized on this</a></span>, i.e. by posting and upvoting suggestions on Reddit, by suggesting to EA nonprofits that they apply, etc. But perhaps we should have done more (or should still do more). Recently, the Pineapple Fund <span><a href=\"https://twitter.com/Give_Directly/status/946062637078474754\">donated</a></span><span>&nbsp;to $5 million to GiveDirectly, though it\u2019s unclear whether or not this was caused by the efforts of EA community members.<br><br></span></li>\n<li>Trying&nbsp;to influence wealthy people on Twitter who indicate an interest in donating. Previous examples include <span><a href=\"https://twitter.com/JeffBezos/status/875418348598603776\">Jeff Bezos</a></span>&nbsp;and <span><a href=\"https://twitter.com/rickygervais/status/939899380412403714\">Ricky Gervais</a></span>. Perhaps we should be on the lookout for more of these, and put more effort into mobilizing the EA community to reply to these tweets with suggestions.<br><br></li>\n</ul>\n<h1><span>Should we try again in 2018? Polls suggest yes</span></h1>\n<p><span>If Facebook runs a similar donation matching program for #GivingTuesday 2018, should we try again? To get the views of the EA community, I conducted a number of Facebook polls.</span></p>\n<table>\n<tbody>\n<tr>\n<td colspan=\"1\" rowspan=\"1\">&nbsp;</td>\n<td colspan=\"1\" rowspan=\"1\">\n<p><a href=\"https://www.facebook.com/groups/eagivingtuesday/permalink/1996802017268369/\">EA #GivingTuesday Facebook Group</a></p>\n</td>\n<td colspan=\"1\" rowspan=\"1\">\n<p><a href=\"https://www.facebook.com/events/566463213697311/permalink/571263249883974/\">EA #GivingTuesday Facebook Event</a></p>\n</td>\n<td colspan=\"1\" rowspan=\"1\">\n<p><a href=\"https://www.facebook.com/groups/eahangout/permalink/1598159806937604/\">EA Hangout Facebook Group</a></p>\n</td>\n</tr>\n<tr>\n<td colspan=\"1\" rowspan=\"1\">\n<p>% Yes</p>\n</td>\n<td colspan=\"1\" rowspan=\"1\">\n<p>67%</p>\n</td>\n<td colspan=\"1\" rowspan=\"1\">\n<p>60%</p>\n</td>\n<td colspan=\"1\" rowspan=\"1\">\n<p>72%</p>\n</td>\n</tr>\n<tr>\n<td colspan=\"1\" rowspan=\"1\">\n<p>% Maybe</p>\n</td>\n<td colspan=\"1\" rowspan=\"1\">\n<p>33%</p>\n</td>\n<td colspan=\"1\" rowspan=\"1\">\n<p>24%</p>\n</td>\n<td colspan=\"1\" rowspan=\"1\">\n<p>24%</p>\n</td>\n</tr>\n<tr>\n<td colspan=\"1\" rowspan=\"1\">\n<p>% No</p>\n</td>\n<td colspan=\"1\" rowspan=\"1\">\n<p>0%</p>\n</td>\n<td colspan=\"1\" rowspan=\"1\">\n<p>16%</p>\n</td>\n<td colspan=\"1\" rowspan=\"1\">\n<p>3%</p>\n</td>\n</tr>\n<tr>\n<td colspan=\"1\" rowspan=\"1\">\n<p>Total responses</p>\n</td>\n<td colspan=\"1\" rowspan=\"1\">\n<p>43</p>\n</td>\n<td colspan=\"1\" rowspan=\"1\">\n<p>25</p>\n</td>\n<td colspan=\"1\" rowspan=\"1\">\n<p>29</p>\n</td>\n</tr>\n</tbody>\n</table>\n<p><span><br>In general, the polls appear to show that EAs both involved and not involved in the #GivingTuesday donation matching are generally supportive of trying again in 2018.</span></p>\n<h1><span>Questions for #GivingTuesday 2018</span></h1>\n<p><span>Our experiences this year raises some questions that are relevant if we try this again next year.</span></p>\n<h2><span>Should we encourage people to create multiple fundraisers for the same nonprofit for their own donations?</span></h2>\n<p><span>Facebook did not have rules in place to prevent people from creating multiple fundraisers for the same nonprofit, and we\u2019ve seen no evidence that it prevented any donations from getting matched. I had concerns that encouraging people create lots of fundraisers for the same nonprofit could be perceived as an unethical attempt to circumvent the $1000 limit per fundraiser, and may go too far in the direction of violating the spirit of the matching program. Therefore, our instructions did not encourage people to create multiple fundraisers. On the other hand, perhaps it\u2019s fine as long as we\u2019re following the rules.</span></p>\n<h2><span>How much time will we have?</span></h2>\n<p><span>The matching funds ran out at 1 minute and 26 seconds. Is it reasonable to expect we\u2019ll have 1-2 minutes in 2018? Or should we expect a much shorter period of time, i.e. because other people might optimize to make their donations faster as well?</span></p>\n<h2><span>How should we keep track of fundraisers?</span></h2>\n<p>Our use of a Google Sheet to keep track of fundraisers could have led to problems, including accidental or malicious edits that could have made the spreadsheet unusable<span>. Fortunately, none of these problems materialized, but we may have just been lucky. Should we manage the security of the spreadsheet differently in 2018? Or should we use a different tool to keep track of fundraisers?</span></p>\n<h2><span>What other unexpected problems might we face?</span></h2>\n<p><span>Most of us failed to predict the matching funds would run out so fast. What other unexpected problems might we face in 2018?</span></p>\n<p>&nbsp;</p>\n<hr>\n<div>\n<p><a href=\"#ftnt_ref1\">[1]</a><span>&nbsp;A few nonprofits reported to us that, based on the information they received from Network for Good, it appears the match was a 50/50 split between Facebook and the Gates Foundation.</span></p>\n</div>\n<div>\n<p><a href=\"#ftnt_ref2\">[2]</a><span>&nbsp;After #GivingTuesday had ended, we found that the group was attracting people outside the EA community who were confused about the purpose of the group. To mitigate this, we removed references to #GivingTuesday in the name, description, and group image. We\u2019ll restore these references if/when we resurrect the group for #GivingTuesday 2018.</span></p>\n</div>\n<div>\n<p><a href=\"#ftnt_ref3\">[3]</a><span>&nbsp;We later learned that the matching funds had run out much earlier, at 1 minute and 26 seconds into the match.</span></p>\n</div>\n<div>\n<p><a href=\"#ftnt_ref4\">[4]</a><span>&nbsp;The $338k donated included some donations that did not have pledges in the spreadsheet.</span></p>\n</div>\n<div>\n<p><a href=\"#ftnt_ref5\">[5]</a><span>&nbsp;Both our pledges and donation amounts in our spreadsheet had increased as people added and updated fundraisers in the spreadsheet.</span></p>\n</div>\n<div>\n<p><a href=\"#ftnt_ref6\">[6]</a><span>&nbsp;The $379k donated included some donations that did not have pledges in the spreadsheet. We also think this is an underestimate, because a number of nonprofits gave us donation and match amount numbers substantially higher than what we had estimated, and we\u2019d guess that a lot of these additional donations are still a result of our initiative.</span></p>\n</div>\n<div>\n<p><a href=\"#ftnt_ref7\">[7]</a><span>&nbsp;This estimate is very uncertain, and is based on limited and messy data. We think it\u2019s in the right ballpark though. </span></p>\n</div>\n<div>\n<p><a href=\"#ftnt_ref8\">[8]</a><span>&nbsp;The high donation amount to MIRI was largely the result of an effort by MIRI to try to capture matching funds from Facebook.</span></p>\n</div>\n<div>\n<p><a href=\"#ftnt_ref9\">[9]</a><span>&nbsp;GiveWell did report a match amount of $1,254 to us. To remain consistent though, we\u2019re reporting the amount we estimated from our fundraisers spreadsheet, which is $0.</span></p>\n</div>\n<div>\n<p><a href=\"#ftnt_ref10\">[10]</a><span>&nbsp;Special thanks to Angelina Li who spearheaded this work.</span></p>\n</div>\n<div>\n<p><a href=\"#ftnt_ref11\">[11]</a><span>&nbsp;It\u2019s possible though that some of this was caused by people waiting for GiveWell or Animal Charity Evaluators to release their recommendations, which they did within those 24 </span><span>hours</span><span>.</span></p>\n</div>\n</div>", "user": {"username": "AviN"}}, {"_id": "shyCoFzvN3o9JSKBG", "title": "Nudging donors towards high-impact charities (a request for funding for SoGive)", "postedAt": "2018-01-13T10:06:16.605Z", "htmlBody": "<html><body><p>This post describes SoGive&#x2019;s work to make it easier for donors to find high-impact charities. SoGive is also seeking funds at the moment.</p>\n<p>&#xA0;</p>\n<ul>\n<li><span>The opportunity</span></li>\n<li><span>Who plans to do this?</span></li>\n<li><span>Is there a hidden agenda to this post?</span></li>\n<li><span>How would SoGive encourage donors to make high-impact donation decisions?</span></li>\n<li><span>How would SoGive reach its donors?</span></li>\n<li><span>Why would these clients want to use SoGive?</span></li>\n<li><span>What&#x2019;s a realistic sense for the potential scale of SoGive?</span></li>\n<li><span>How will SoGive determine which are the high-impact charities?</span></li>\n<li><span>Will nudging actually work?</span></li>\n<li><span>The team</span></li>\n<li><span>The budget</span></li>\n<li><span>What has SoGive done/learned so far?</span></li>\n<li><span>What are some reasons why SoGive might not be a good thing to fund?</span></li>\n</ul>\n<p>&#xA0;</p>\n<p>&#xA0;&#xA0;</p>\n<p><strong>The opportunity</strong></p>\n<p><!-- [if !supportLists]-->-<span>&#xA0; &#xA0; &#xA0; &#xA0; &#xA0; -&#xA0;</span><!--[endif]-->Many people consider all charities to be essentially about as good as each other</p>\n<p><!-- [if !supportLists]-->-<span>&#xA0; &#xA0; &#xA0; &#xA0; &#xA0; -&#xA0;</span><!--[endif]-->They are often happy to be reactive rather than proactive in charity selection</p>\n<p><!-- [if !supportLists]-->-<span>&#xA0; &#xA0; &#xA0; &#xA0; &#xA0; -&#xA0;</span><!--[endif]-->This suggests there may be an opportunity for someone to create a choice architecture where it is easy for people to choose high-impact charities</p>\n<p>&#xA0;</p>\n<p><strong>Who plans to do this?</strong></p>\n<p>SoGive. SoGive is run by me (Sanjay) and my cofounder Daniel, together with a small team of (mostly) developers who are working part time. SoGive has been running for about a year, during which time we have been exploring some ways to make an impact-oriented donation website have a positive effect on the world. More about the team below.</p>\n<p><strong>Is there a hidden agenda for this post?</strong></p>\n<p>Yes (I guess it&#x2019;s not so hidden now!) We are seeking philanthropic support. Our budget for 2018 is for around &#xA3;140k-&#xA3;220k</p>\n<p><strong>How would SoGive encourage donors to make high-impact donation decisions?</strong></p>\n<p>We would provide donors with software which has recommendations built in. An example prototype can be found here <a href=\"http://sogive.org/\">http://sogive.org/</a> (although the exact software would be tailored for client needs)</p>\n<p><strong>How would SoGive reach its donors?</strong></p>\n<p>Our plan is to pitch SoGive to specific client groups, each of which represents lots of donors; in other words we find places where donors are already donating and we go to them. These are the three main client categories we&#x2019;re interested in:</p>\n<p><!-- [if !supportLists]-->-<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><!--[endif]-->Organisers of sponsored events (e.g. marathons)</p>\n<p><!-- [if !supportLists]-->-<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><!--[endif]-->Corporates (who provide payroll giving for staff)</p>\n<p><!-- [if !supportLists]-->-<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><!--[endif]-->Financial advisers (who look after clients&#x2019; financial affairs)</p>\n<p>Individual users are also welcome, however we think individuals are harder to recruit than clients (i.e. D2C is harder than B2B) so this is not a main focus for our efforts.</p>\n<p><strong>Why would those clients want to use SoGive?</strong></p>\n<p>SoGive distinguishes itself by having a unique database of charities and the amount of impact each charity achieves, together with costings. This database can</p>\n<p><!-- [if !supportLists]-->-<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><!--[endif]-->Tell users how much of charity&#x2019;s impact is attributable to your donation (to help donors feel proud/positive about their donation)</p>\n<p><!-- [if !supportLists]-->-<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><!--[endif]-->Serve as a value for money database</p>\n<p>The ability to improve the user experience by making the selection of charities easier is also a positive.</p>\n<p>In the sponsored event space, we also use a distinctive charging mechanism, which helps organisers avoid reputational risk.</p>\n<p>For financial advisers, we are also developing an exciting inheritance tax product which will enable advisers to continue add value for their clients (and generate fees) beyond the death of the client.</p>\n<p>&#xA0;</p>\n<p><strong>What&#x2019;s a realistic sense for the potential scale of SoGive&#x2019;s influence?</strong></p>\n<p>It seems reasonable to believe that SoGive will influence c &#xA3;1-10 million in 2018 and &gt; &#xA3;10 million in 2019.</p>\n<p>To justify this, here are some of the most promising leads in the pipeline at the moment:</p>\n<p><!-- [if !supportLists]-->-<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><!--[endif]-->A company has spoken with us about supporting them with charity selection: amount influenced &gt;&#xA3;4 million; likelihood not clear, will become more obvious over the course of 2018</p>\n<p><!-- [if !supportLists]-->-<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><!--[endif]-->An event organiser is potentially going to use the SoGive platform: amount moved &gt;&#xA3;2 million; unfortunately it&#x2019;s now clear that recommendations will not be possible for this client</p>\n<p><!-- [if !supportLists]-->-<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><!--[endif]-->A company which provides payroll giving for other companies may ask SoGive to source the recommendations: amount influenced &gt;&#xA3;100 million ultimately, going up gradually; likelihood fairly good</p>\n<p>Given that SoGive costs c &#xA3;100k-&#xA3;250k to run, a rough ballpark estimate seems to suggest a fundraising ratio of 10:1 for 2018. If SoGive becomes self-sustaining soon, which we think will happen in 2019 or 2020, then the ratio is much higher.</p>\n<p><strong>How will SoGive determine which charities are high-impact?</strong></p>\n<p>Currently we are leaning heavily on GiveWell&#x2019;s recommendations. Over time we will take on more of that analysis from other sources than GiveWell, including our own in-house analysis. This is partly because some of our users have interests other than global health, and partly because (depending on our success) we may be at risk of testing the Room For More Funding of GiveWell-recommended charities.</p>\n<p>&#xA0;</p>\n<p><strong>Will nudging actually work?</strong></p>\n<p>Our findings so far indicate that most users are drawn towards individual specific charities that are put in front of them; looking past those default charity choices placed in front of them and seeking out a charity linked to their own interests seems to be a rare occurrence, based on our current experience. This experience is not yet at scale.</p>\n<p>The research we have found seems to indicate that 70% of donations will be from donors happy to influenced, however this research was not intended to answer this question, and I&#x2019;m not very confident that this interpretation of that research is correct.</p>\n<p>Outside of our experience specifically within the SoGive context, there is plenty of evidence to suggest that people can be positively influenced by their choice architecture.</p>\n<p>&#xA0;</p>\n<p><strong>The team</strong></p>\n<p><!-- [if gte vml 1]><v:shapetype\nid=\"_x0000_t75\" coordsize=\"21600,21600\" o:spt=\"75\" o:preferrelative=\"t\"\npath=\"m@4@5l@4@11@9@11@9@5xe\" filled=\"f\" stroked=\"f\">\n<v:stroke joinstyle=\"miter\"/>\n<v:formulas>\n<v:f eqn=\"if lineDrawn pixelLineWidth 0\"/>\n<v:f eqn=\"sum @0 1 0\"/>\n<v:f eqn=\"sum 0 0 @1\"/>\n<v:f eqn=\"prod @2 1 2\"/>\n<v:f eqn=\"prod @3 21600 pixelWidth\"/>\n<v:f eqn=\"prod @3 21600 pixelHeight\"/>\n<v:f eqn=\"sum @0 0 1\"/>\n<v:f eqn=\"prod @6 1 2\"/>\n<v:f eqn=\"prod @7 21600 pixelWidth\"/>\n<v:f eqn=\"sum @8 21600 0\"/>\n<v:f eqn=\"prod @7 21600 pixelHeight\"/>\n<v:f eqn=\"sum @10 21600 0\"/>\n</v:formulas>\n<v:path o:extrusionok=\"f\" gradientshapeok=\"t\" o:connecttype=\"rect\"/>\n<o:lock v:ext=\"edit\" aspectratio=\"t\"/>\n</v:shapetype><v:shape id=\"Picture_x0020_3\" o:spid=\"_x0000_s1026\" type=\"#_x0000_t75\"\nstyle='position:absolute;left:0;text-align:left;margin-left:76.5pt;\nmargin-top:24.55pt;width:117.75pt;height:117.75pt;z-index:251659264;\nvisibility:visible;mso-wrap-style:square;mso-wrap-distance-left:9pt;\nmso-wrap-distance-top:0;mso-wrap-distance-right:10.45pt;\nmso-wrap-distance-bottom:1.45pt;mso-position-horizontal:absolute;\nmso-position-horizontal-relative:text;mso-position-vertical:absolute;\nmso-position-vertical-relative:text'>\n<v:imagedata src=\"file:///C:\\Users\\Sanjay\\AppData\\Local\\Temp\\msohtmlclip1\\01\\clip_image001.png\"\no:title=\"sanjay\"/>\n</v:shape><v:shape id=\"Image1\" o:spid=\"_x0000_s1027\" type=\"#_x0000_t75\"\nstyle='position:absolute;left:0;text-align:left;margin-left:257.95pt;\nmargin-top:23.2pt;width:93.55pt;height:122.25pt;z-index:251660288;\nvisibility:visible;mso-wrap-style:square;mso-width-percent:0;\nmso-height-percent:0;mso-wrap-distance-left:0;mso-wrap-distance-top:0;\nmso-wrap-distance-right:0;mso-wrap-distance-bottom:0;\nmso-position-horizontal:absolute;mso-position-horizontal-relative:text;\nmso-position-vertical:absolute;mso-position-vertical-relative:text;\nmso-width-percent:0;mso-height-percent:0;mso-width-relative:margin;\nmso-height-relative:margin'>\n<v:imagedata src=\"file:///C:\\Users\\Sanjay\\AppData\\Local\\Temp\\msohtmlclip1\\01\\clip_image002.png\"\no:title=\"\"/>\n</v:shape><![endif]--><!-- [if !vml]--></p>\n<table>\n<tbody>\n<tr>\n<td>&#xA0;</td>\n<td>&#xA0;</td>\n<td>&#xA0;</td>\n<td>&#xA0;</td>\n</tr>\n<tr>\n<td>&#xA0;</td>\n<td colspan=\"2\">&#xA0;</td>\n<td rowspan=\"3\"><img src=\"file:///C:/Users/Sanjay/AppData/Local/Temp/msohtmlclip1/01/clip_image003.jpg\"></td>\n</tr>\n<tr>\n<td>&#xA0;</td>\n<td><img src=\"file:///C:/Users/Sanjay/AppData/Local/Temp/msohtmlclip1/01/clip_image004.png\"></td>\n</tr>\n<tr>\n<td>&#xA0;</td>\n</tr>\n</tbody>\n</table>\n<p><!--[endif]--><strong><span>&#xA0;</span></strong></p>\n<p>&#xA0;</p>\n<p><span>&#xA0;</span></p>\n<p><span>&#xA0;</span></p>\n<p><span>&#xA0;</span></p>\n<p><span>&#xA0;</span></p>\n<p><span>&#xA0;</span></p>\n<p>&#xA0;</p>\n<p><span>SoGive is run by Sanjay (CEO) and his cofounder Daniel (CTO)</span></p>\n<p>Prior to starting up SoGive, Sanjay has worked in finance and strategy consulting, whilst simultaneously supporting the management teams of several charities on a voluntary basis, largely as a trustee. In this process, he developed a lot of relevant experience:</p>\n<p><!-- [if !supportLists]--><span>&#xB7;<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span></span><!--[endif]-->When working as a senior credit analyst at Standard &amp; Poors, he used in-depth analysis of companies&#x2019; accounts and complex models to gain deep insight into companies, and then distilled this into highly simplified output; this is when he started thinking about applying this approach to charities</p>\n<p><!-- [if !supportLists]--><span>&#xB7;<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span></span><!--[endif]-->He also has served on the boards of around half a dozen charities, including roles as treasurer, from which he gained insight into how charity accounting works, how charities use their funds and how they make decisions</p>\n<p><!-- [if !supportLists]--><span>&#xB7;<span>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span></span><!--[endif]-->He has also become involved in a large number of groups for philanthropists, building up his network of potential donors/users of SoGive and his understanding of the donor perspective</p>\n<p><span>&#xA0;</span></p>\n<p>Daniel Winterstein has a combination of a strong technical background and startup experience, having created Winterwell, the software company he now manages. He has developed successful products and worked as a consultant for companies such as the BBC, Harrods, and as a university researcher.</p>\n<ul>\n<li>As a consultant CTO/developer, Daniel played a leading role in the launch of several successful Scottish start-ups: Buddhify, QikServe, AdAppTive, and Yavi.</li>\n<li>He is an expert on the development and use of data analysis software to achieve business goals. He has 10 years industry experience &#x2013; working for organisations including the BBC, Home Office, Ministry of Defence, Tesco Personal Finance, Harrods &amp; Motorola.</li>\n<li>A specialist in machine learning, probability theory, mathematical modelling, data science, profiling, text analysis and social networks, he studied Mathematics at Cambridge University and has a Ph.D. in Artificial Intelligence (AI) from Edinburgh University.</li>\n</ul>\n<p><span>Sanjay and Daniel have worked together on SoGive for the last year, and have known each other for around 10 years before that.</span></p>\n<p>&#xA0;</p>\n<p><strong>The budget</strong></p>\n<table>\n<tbody>\n<tr>\n<td>\n<p><span>Item</span></p>\n</td>\n<td>\n<p><span>Plan A</span></p>\n</td>\n<td>\n<p><span>Plan B</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><strong>Charity data analysis+BizDev</strong></p>\n</td>\n<td>\n<p><span>&#xA3;144,292</span></p>\n</td>\n<td>\n<p><span>&#xA3;89,270</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><strong>Tech (developers)</strong></p>\n</td>\n<td>\n<p><span>&#xA3;54,574</span></p>\n</td>\n<td>\n<p><span>&#xA3;32,739</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><strong>Misc/overheads</strong></p>\n</td>\n<td>\n<p><span>&#xA3;20,875</span></p>\n</td>\n<td>\n<p><span>&#xA3;18,375</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><strong>Total</strong></p>\n</td>\n<td>\n<p><strong><span>&#xA3;219,741</span></strong></p>\n</td>\n<td>\n<p><strong><span>&#xA3;140,383</span></strong></p>\n</td>\n</tr>\n</tbody>\n</table>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p>SoGive needs &#xA3;140k-&#xA3;220k to enable us to influence &#xA3;millions of donations pa. Plan A provides us with a greater probability of growing faster.</p>\n<p>&#xA0;</p>\n<p><strong>What has SoGive done/learned so far?</strong></p>\n<p>SoGive considered using the app as a form of EA outreach &#x2013; i.e. achieving greater scale but less depth than traditional EA outreach methods. This is described here: <a href=\"https://www.facebook.com/groups/effective.altruists/permalink/1211844465538575/\">https://www.facebook.com/groups/effective.altruists/permalink/1211844465538575/</a></p>\n<p>Our experience suggests that this is unlikely to work. We have tried presenting data about charities in a way that is strongly suggestive of value for money thinking, and yet found that users often don&#x2019;t interpret the data in a value for money way.</p>\n<p>Hence our move away from trying influence the mindset of the donor and towards making it easy for donors to find high-impact choices.</p>\n<p>&#xA0;</p>\n<p><strong>What are some reasons why SoGive might not be a good thing to fund?</strong></p>\n<p>I will start with the risk that I find to be the most credible concern, and then follow on with others that have been mentioned to me:</p>\n<p>While there are potential clients with donation amounts well into the millions lined up, they are not yet confirmed; it is possible that all of them fall through and SoGive achieves only a minimal amount. &#xA0;SoGive response: While this is a risk, the existing pipeline does give grounds for hope.</p>\n<p>Is it not better to persuade people to fund more impactful things like the far future (or whichever cause area you think is best)? SoGive response: it&#x2019;s true that we are unlikely to lead lots of donors towards the most &#x201C;weird&#x201D;-sounding charities; our aim is to crowd out the most popular/&#x201D;accessible&#x201D; high-impact funding areas so that the community can focus on the more esoteric options.</p>\n<p>The SoGive CTO has a lot of expertise in AI &#x2013; is this project taking away talent that could be used on AI safety? SoGive response: Daniel is sanguine about the risks of AI, and unlikely to be persuaded that working for MIRI is the best use of his time.&#xA0;</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "Sanjay"}}, {"_id": "hYh6jKBsKXH8mWwtc", "title": "Contact people for the EA community", "postedAt": "2018-01-12T17:04:14.404Z", "htmlBody": "<p>[last significant update December 2022]<br><br>We, Julia Wise and Catherine Low, work on the community health team at the Centre for Effective Altruism. Part of our work is being contact people for problems or concerns you encounter in the EA community. This is one part of the team\u2019s <a href=\"https://www.centreforeffectivealtruism.org/community-health\">larger work</a>.</p><p>Ways to contact us:</p><ul><li>the&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLScJooJD0Sm2csCYgd0Is6FkpyQa3ket8IIcFzd_FcTRU7avRg/viewform\"><u>community health contact form</u></a> (can be anonymous)</li><li>Julia\u2019s&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLScWtG3hNhdzSpVSAEHmpEz7mjlvoRhqch8hsFF2lKJX8dzrvw/viewform\"><u>form</u></a> (can be anonymous)</li><li>Catherine\u2019s&nbsp;<a href=\"https://forms.gle/Rky3VTdAJWTZqmDH7\"><u>form</u></a> (can be anonymous)</li><li>julia.wise@centreforeffectivealtruism.org</li><li>catherine@centreforeffectivealtruism.org&nbsp;</li></ul><h2>Why have point people?</h2><p>People who encounter a problem in the community often don\u2019t feel up to handling it on their own. It can be helpful to have help from someone with experience in this area and time to dedicate to the role.</p><p>Having a central point for collecting information allows patterns to be recognized. For example, imagine that three different people experience a problem from the same person. If there\u2019s no one to collect this data, each case appears to be a one-off incident. But if someone knows about all three incidents, the nature of the problem is much clearer.</p><p>What kinds of situations can we help with?</p><p>Some types of work we\u2019ve done:</p><ul><li>Helping friends of a community member who was experiencing a mental health crisis coordinate to better support their friend</li><li>Advising group organizers on handling conflicts and other problems in their groups</li><li>Speaking to people about ways their behavior has made people uncomfortable or caused problems (for example, mild sexual harassment), and asking them to change their behavior</li><li>Restricting people from attending CEA\u2019s events based on past serious problem behavior</li></ul><h3>Our backgrounds</h3><p>Julia:</p><p>I\u2019m a licensed independent clinical social worker, with studies and work that focused on mental health. I got involved in EA around 2010 and joined CEA to work on supporting the EA community in 2015.</p><p>My experience includes:</p><ul><li>Volunteering at a women\u2019s domestic violence shelter, counseling callers to a domestic violence and sexual assault hotline, and accompanying sexual assault survivors during examinations at a hospital</li><li>Working in a psychiatric hospital with people in mental health crisis</li><li>Counseling inmates and detainees in a jail, including both survivors and perpetrators of community violence, domestic violence, and sexual assault</li></ul><p>Social work is focused not on blame or punishment, but on reaching better outcomes by reducing risk of future harm and by connecting people with resources and support. This approach is very much the one I use in my work in the effective altruism community.</p><p>Catherine:</p><p>I was a high school teacher for 11 years before moving full time into EA community building. I ran local and national EA groups and worked on EA outreach projects ,&nbsp; before joining CEA\u2019s Groups Team in early 2020 to support EA groups worldwide. I started working for the Community Health team mid 2021.&nbsp;</p><h3>Confidentiality</h3><p>If you contact us about a problem you\u2019ve experienced or a concern you have, we will do our best to keep it confidential, but there are occasionally some exceptions. We're currently working on a clearer policy \u2014 in the meantime we suggest asking us about any questions you have here before you share sensitive information.<br>[Edited to add: Julia has made two mistakes on confidentiality that she knows of. More info in this&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/hYh6jKBsKXH8mWwtc/a-contact-person-for-the-ea-community?commentId=h6ra3qdAGr5W2zcwE\">comment</a>.]&nbsp;</p><p>Here are some possibilities:</p><ul><li>You just want to vent or discuss your concern, and do not want the information to go any further</li><li>You are ok with us discussing an anonymized version of your concern with certain other people with your permission</li><li>You would like to be put in touch with other people who have experienced a similar problem (with their permission)</li><li>You would like us to let others know about the situation, for example the organizers of a local group where you experienced a problem</li><li>You would like us to speak to the person who caused a problem about their behavior</li><li>You want to report a problem to us, but don\u2019t want us to take action unless other people also raise the same problem&nbsp;</li></ul><p>We will sometimes offer to discuss with you the possible actions we could take with differing levels of confidentiality, so you can make a well informed decision.</p><p>We will sometimes ask if we can share your case with the other community support person so we can get the benefit of their advice.</p><p><i><strong>Exceptions:</strong></i></p><p>If we thought someone was in physical danger, we would act to reduce that danger. That might include breaking confidentiality. For example, if you tell us you're planning to physically hurt someone, we would warn them.</p><p><i>For Catherine only:</i></p><p>In addition, if Catherine thought someone was at risk of serious mental harm, she would act to reduce that harm.&nbsp;&nbsp;</p><p><i>For Julia only:</i></p><p>Because of the rules for social workers where Julia lives, the only time she has a legal obligation to contact the authorities is if you bring her a concern specifically in her role as a social worker (\u201cJulia, I\u2019m telling you this because you\u2019re a social worker\u201d) AND if the concern is about</p><ul><li>A child under 18 who is being abused or neglected</li><li>A person with an intellectual disability (mental retardation) who is being abused, or</li><li>A person 60 or older who is being abused.</li></ul><h3>Other options</h3><p>We\u2019re not the only options for help in the community. For problems that arise in an EA group, it may be helpful to talk to the organizers of that group.</p><p>If you\u2019re a group organizer or online moderator and would like help preparing to handle community problems or handling a problem that has already arisen, feel free to <a href=\"https://docs.google.com/forms/d/e/1FAIpQLScJooJD0Sm2csCYgd0Is6FkpyQa3ket8IIcFzd_FcTRU7avRg/viewform\">contact us</a>.</p><p><br>&nbsp;</p>", "user": {"username": "Julia_Wise"}}, {"_id": "BxMzd4CRYBF8wuwFW", "title": "The Technological Landscape Affecting Artificial General Intelligence and the Importance of Nanoscale Neural Probes", "postedAt": "2018-01-12T01:10:07.056Z", "htmlBody": "<html><body><p>I have a new paper out where I describe&#xA0;the technological landscape leading to AGI and implications for AI safety (largely building on Bostrom&apos;s work in Superintelligence).</p>\n<p>Here&apos;s the abstract:</p>\n<div><span>In this paper, we contrast three major pathways to human level AI, also known as artificial general intelligence (AGI), and we investigate how safety considerations compare between the three. The first pathway is de novo AGI (dnAGI), AGI built from the ground up. The second is Neuromorphic AGI (NAGI), AGI based loosely on the principles of the human brain. And third is Whole Brain Emulation (WBE), AGI built by emulating a particular human brain, in silico. Bostrom has previously argued that NAGI is the least safe form of the three. NAGI would be messier than dnAGI and therefore harder to align to arbitrary values. Additionally, NAGI would not intrinsically possess safeguards found in the human brain </span><span>&#x2013; </span><span>such as compassion </span><span>&#x2013; </span><span>while WBE would. In this paper, we argue that getting WBE first would be preferable to getting dnAGI first. While the introduction of WBE would likely be followed by a later transition to the less-constrained and therefore more-powerful dnAGI, the creation of dnAGI would likely be less dangerous if accomplished by WBEs than if done simply by biological humans, for a variety of reasons. One major reason is that the higher intelligence and quicker speed of thinking in the WBEs compared to biological humans could increase the chances of traversing the path through dnAGI safely. We additionally investigate the major technological trends leading to these three types of AGI, and we find these trends to be: traditional AI research, computational hardware, nanotechnology research, nanoscale neural probes, and neuroscience. In particular, we find that WBE is unlikely to be achieved without nanoscale neural probes, since much of the information processing in the brain occurs on the subcellular level (i.e., the nanoscale). For this reason, we argue that nanoscale neural probes could improve safety by favoring WBE over NAGI.&#xA0;</span>&#xA0;</div>\n<p>&#xA0;</p>\n<p>If you want to read the full paper, here&apos;s a link:</p>\n<p>http://www.informatica.si/index.php/informatica/article/view/1874/1101</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "Daniel_Eth"}}, {"_id": "AREuWgfX7Py79whPc", "title": "How to get a new cause into EA", "postedAt": "2018-01-10T06:41:47.857Z", "htmlBody": "<html><body><p><span>Effective altruism has had three main direct broad causes (global poverty, animal rights, and far future), for quite some time. I have often heard people worry that it&#x2019;s too hard for a new cause to be accepted by the effective altruism movement. Simultaneously, I do not really see people presenting new cause areas in the way I think would be the most likely for many EAs to consider and take seriously. I wanted to make a quick reference post as a better way for people to propose new cause/intervention areas they might see as promising. Much of this advice could also be used to present a new specific charity within a known high impact cause area. </span></p>\n<p>&#xA0;</p>\n<p>1) Be ready to put in some real time</p>\n<p><span>2) Have a specific intervention within the broad cause to compare</span></p>\n<p><span>3) Compare it to the most relevantly comparable top cause.</span></p>\n<p><span>4) Compare it numerically</span></p>\n<p><span>5) Focus on one change at a time </span></p>\n<p><span>6) Use equal rigour</span></p>\n<p><span>7) Have a summary at the top </span></p>\n<p>&#xA0;</p>\n<p><span>1) Be ready to put in some time</span></p>\n<p>&#xA0;</p>\n<p><span>Comparing different cause areas is hard and takes a good amount of research time. In the effective altruism movement there are many people and organizations who work full time comparing interventions within a single cause, and generally it&apos;s much harder to compare interventions across cause areas. Generally it&apos;s going to take some time to effectively articulate a new cause area, particularly if the EA movement has not spent much collective time considering it. It&apos;s not expected, or even possible, that one person does all the research required in a whole cause area, but if you think a cause area is competitive, you likely will have to be the first one to do some of the initial research and start to build a case for why others should consider it. To start to get enough reasoning for EAs to really consider a cause it has to start to stand out among the hundreds of other causes that could be high impact to work on. </span></p>\n<p>&#xA0;</p>\n<p><span>2) Have a specific intervention within the broad cause to compare</span></p>\n<p>&#xA0;</p>\n<p><span>As mentioned above, comparing whole cause areas is hard. In many ways it&apos;s also not the point. If cause area A is more effective than global poverty on average but all the specific interventions in it can not compete with the best global poverty charities (e.g., AMF), it will still not be a great target to put resources towards. Additionally, it&apos;s much harder to get into the details and comparisons of a whole cause area which will often contain numerous different interventions. The best way around these concerns I have seen is to drill down on an example of a highly promising intervention. For example, if you are making the case that mental health in the third world is a high impact cause area, look deeply into a specific example, like CBT cell phone applications. With a more specific intervention it will be easy to fact check as well as numerically compare it to the other top interventions EAs currently support. </span></p>\n<p>&#xA0;</p>\n<p><span>3) Compare it to the most relevantly comparable top cause.</span></p>\n<p>&#xA0;</p>\n<p><span>A huge number of causes that are brought up are not directly compared to the most relevant comparable cause area. If someone is making a case for positive psychology and mental health, the natural comparison is to the GiveWell top charities. If it&apos;s about wild animal suffering, it needs to be compared to farm animal interventions, and if it&apos;s about bio-risk, it could be compared to AI. If someone is sold on far future and pitching a new cause area within it, making generalized arguments about the far future being better than AMF is not going to do much work convincing people. Most EAs will have already heard AMF vs. AI comparison and those sorts of arguments will not be new or persuasive to AMF supporters and do nothing to compare the cause to its real competition, AI. Some cause areas might be amenable to multiple comparisons (bio risk could be made as a far future case compared to AI or a direct DALYs improved compared to AMF), but in any case, try to compare it to the cause that contains the sorts of people who are most likely to find your new proposed cause high impact.</span></p>\n<p>&#xA0;</p>\n<p><span>4) Compare it numerically</span></p>\n<p>&#xA0;</p>\n<p><span>Effective altruists are a quantitative bunch and numerical comparisons are basically necessary for seriously comparing the good done in different cause areas and interventions. There are a lot of different ways to do this, but a safe bet would be a cost-effectiveness analysis in a spreadsheet or </span><a href=\"https://www.getguesstimate.com/\"><span>guesstimate</span></a><span> model. As mentioned above, depending on the most relevant cause you are comparing to, you will want to generally model things in that context. That would generally mean DALYs or cost per life saved for global poverty, animal DALYs for animals, or percent chance of affecting long term society for far future. Cross-comparing metrics is a useful blog post in and of itself, but it&#x2019;s not going to be best presented while simultaneously presenting a new cause area. This leads well into my next point.</span></p>\n<p>&#xA0;</p>\n<p><span>5) Focus on one change at a time </span></p>\n<p>&#xA0;</p>\n<p><span>Often when people present new cause areas they come with a lot of other proposed changes. They could be ethical (e.g. we should have X view on population ethics), epistemic (e.g. we should value historical evidence more) or logistical (e.g. we should use this CEA software even though it&#x2019;s harder to read for beginners). As mentioned above, all of these might be worthwhile changes for the EA movement to make, but if it&#x2019;s conflated with a suggested cause, generally I have seen people dismiss the cause because of the other associated claims with it. For example, &#x201C;Only negative leaning utilitarians think cause X is important, and I am not negative leaning.&#x201D; This often happens with causes that have a very strong case even with fairly traditional EA standards of evidence/ethics etc. If the cause area as a whole relies on an ethical or other assumption to be competitive, I would generally recommend writing about that specifically before pitching a cause or intervention that is reliant on it. </span></p>\n<p>&#xA0;</p>\n<p><span>6) Use equal rigour</span></p>\n<p>&#xA0;</p>\n<p><span>Not only does a new cause need to be compared -- it ideally needs to be compared with equal rigour, at least as much as is possible. It&apos;s easy to point out flaws in one charity or cause area and only highlight the benefits of another, but without comparing them with the same level of rigour, the numbers will be useless next to each other. To use a clear example of this, I have seen bus ads that claim to save a life for $1 and yet I still donate to GW charities which claim to save a life for $3000. This is mainly because the way the calculation was done was completely different, even if they were both put into a dollar per life saved metric at the end. I expect that if the $1 charity was compared using GiveWell&#x2019;s methodology, its cost-effectiveness would rapidly decrease. Likewise, if a cause area is presented with very optimistic estimates, it&#x2019;s hard to take the endline conclusion seriously -- much like the bus ad.</span></p>\n<p>&#xA0;</p>\n<p><span>This is an easy one to say but very hard to do in practice. The best way I have found is to try to think, &#x201C;How would GiveWell (or ACE, etc) model this?&#x201D;, and try to follow those principles. Another great way is to get an EA or two who you respect and is not sold on your cause area to take a look over your numbers and suggest changes. People will comment, suggesting changes on almost any model, but if it&apos;s too far off a realistic number, many people just will not bother with commenting on all the things that need changes. Lastly, another thing to keep in mind is that often logistical costs are easy to forget. X product may only cost $1,000 and save a life of DALYs, but what about shipping costs, staff overhead, government permissions, etc? Underestimating these often significant costs are a common reason why CEAs get worse as people investigate deeper. &#xA0;</span></p>\n<p>&#xA0;</p>\n<p><span>7) Have a summary at the top of a more in depth review</span></p>\n<p>&#xA0;</p>\n<p><span>Particularly for long posts, having a summary at the top with the strongest arguments and endline conclusions will make it a lot easier for people to know if they should commit to reading the whole post or not, as well as allow engagement from people who do not have time to dig into all the details of the full post. &#xA0;</span></p>\n<p>&#xA0;</p>\n<p><span>Why bother pitching a new cause within EA?</span></p>\n<p>&#xA0;</p>\n<p><span>Following all these steps is a lot of work and that energy and time could be being put into furthering the cause directly or earning money and donating to it. Despite this, I think in almost all cases it is worth presenting a new cause area to EA if it&apos;s possible it could be competitive. The EA community directs large portions of money both directly through earning to give and indirectly from influencing large foundations. Historically, very underfunded causes like AI x-risk and farm animal rights have both massively benefited from EA financial support. In addition, the EA movement directs talent towards high impact cause areas, new charities are founded, Ivy League graduates apply for jobs and volunteer research is done in areas that are seen as high impact. Even if a well written cause report takes 20 hours or more to do the benefits can be much larger if even a small percentage of the EA community is convinced the cause area is worthwhile. </span></p></body></html>", "user": {"username": "Joey"}}, {"_id": "6BvGzeLweswSJondR", "title": "[Paper]: Artificial Intelligence in Life Extension: from Deep Learning to Superintelligence", "postedAt": "2018-01-04T14:31:56.824Z", "htmlBody": "<html><body><p>There are two views on the best strategy among transhumanists and rationalists: The first involves the belief that one must invest in life extension technologies, and the latter, that it is necessary to create an aligned AI that will solve all problems, including giving us immortality or even something better. In our article, we showed that these two points of view do not contradict each other, because it is the development of AI that will be the main driver of increased life expectancy in the coming years, and as a result, even currently living people can benefit (and contribute) from the future superintelligence in several ways.</p>\n<p>&#xA0;</p>\n<p>Firstly, because the use of machine learning, narrow AI will allow the study of aging biomarkers and combinations of geroprotectors, and this will produce an increase in life expectancy of several years, which means that tens of millions of people will live long enough to survive until the date of the creation of the superintelligence (whenever it happens) and will be saved from death. In other words, the current application of narrow AI to life extension provides us with a chance to jump on the &#x201C;longevity escape velocity&#x201D;, and the rapid growth of the AI will be the main factor that will, like the wind, help to increase this velocity.&#xA0;</p>\n<p>&#xA0;</p>\n<p>Secondly, we can&#x2014;here in the present&#x2014;utilize some possibilities of the future superintelligence, by collecting data for &#x201C;digital immortality&#x201D;. Based on these data, the future AI can reconstruct the exact model of our personality, and also solve the identity problem. At the same time, the collection of medical data about the body will help both now&#x2014;as it can train machine learning systems in predicting diseases&#x2014;and in the future, when it becomes part of digital immortality. By subscribing to cryonics, we can also tap into the power of the future superintelligence, since without it, a successful reading of information from the frozen brain is impossible.&#xA0;</p>\n<p>&#xA0;</p>\n<p>Thirdly, there are some grounds for assuming that medical AI will be safer. It is clear that fooming can occur with any AI. But the development of medical AI will accelerate the development of BCI interfaces, such as a <em>Neuralink</em>, and this will increase the chance of AI not appearing separately from humans, but as a product of integration with a person. As a result, a human mind will remain part of the AI, and from within, the human will direct its goal function. Actually, this is also Elon Musk&#x2019;s vision, and he wants to commercialize his <em>Neuralink</em> through the treatment of diseases. In addition, if we assume that the <em>principle of orthogonality</em> may have exceptions, then any medical AI aimed at curing humans will be more likely to have benevolence as its terminal goal.</p>\n<p>&#xA0;</p>\n<p>As a result, by developing AI for life extension, we make AI more safe, and increase the number of people who will survive up to the creation of superintelligence. Thus, there is no contradiction between the two main approaches in improving human life via the use of new technologies.</p>\n<p>Moreover, for a radical life extension with the help of AI, it is necessary to take concrete steps right now: to collect data for digital immortality, to join patient organizations in order to combat aging, and to participate in clinical trials involving combinations of geroprotectors, and computer analysis of biomarkers. We see our article as a motivational pitch that will encourage the reader to fight for a personal and global radical life extension.</p>\n<p>&#xA0;</p>\n<p><!-- [if gte mso 9]><xml>\n <o:OfficeDocumentSettings>\n  <o:AllowPNG/>\n </o:OfficeDocumentSettings>\n</xml><![endif]--> <!-- [if gte mso 9]><xml>\n <w:WordDocument>\n  <w:View>Normal</w:View>\n  <w:Zoom>0</w:Zoom>\n  <w:TrackMoves/>\n  <w:TrackFormatting/>\n  <w:PunctuationKerning/>\n  <w:ValidateAgainstSchemas/>\n  <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid>\n  <w:IgnoreMixedContent>false</w:IgnoreMixedContent>\n  <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText>\n  <w:DoNotPromoteQF/>\n  <w:LidThemeOther>EN-GB</w:LidThemeOther>\n  <w:LidThemeAsian>X-NONE</w:LidThemeAsian>\n  <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript>\n  <w:Compatibility>\n   <w:BreakWrappedTables/>\n   <w:SnapToGridInCell/>\n   <w:WrapTextWithPunct/>\n   <w:UseAsianBreakRules/>\n   <w:DontGrowAutofit/>\n   <w:SplitPgBreakAndParaMark/>\n   <w:EnableOpenTypeKerning/>\n   <w:DontFlipMirrorIndents/>\n   <w:OverrideTableStyleHps/>\n  </w:Compatibility>\n  <m:mathPr>\n   <m:mathFont m:val=\"Cambria Math\"/>\n   <m:brkBin m:val=\"before\"/>\n   <m:brkBinSub m:val=\"&#45;-\"/>\n   <m:smallFrac m:val=\"off\"/>\n   <m:dispDef/>\n   <m:lMargin m:val=\"0\"/>\n   <m:rMargin m:val=\"0\"/>\n   <m:defJc m:val=\"centerGroup\"/>\n   <m:wrapIndent m:val=\"1440\"/>\n   <m:intLim m:val=\"subSup\"/>\n   <m:naryLim m:val=\"undOvr\"/>\n  </m:mathPr></w:WordDocument>\n</xml><![endif]--><!-- [if gte mso 9]><xml>\n <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"false\"\n  DefSemiHidden=\"false\" DefQFormat=\"false\" DefPriority=\"99\"\n  LatentStyleCount=\"380\">\n  <w:LsdException Locked=\"false\" Priority=\"0\" QFormat=\"true\" Name=\"Normal\"/>\n  <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 7\"/>\n  <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 8\"/>\n  <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 9\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"index 1\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"index 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"index 3\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"index 4\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"index 5\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"index 6\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"index 7\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"index 8\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"index 9\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" Name=\"toc 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" Name=\"toc 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" Name=\"toc 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" Name=\"toc 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" Name=\"toc 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" Name=\"toc 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" Name=\"toc 7\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" Name=\"toc 8\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" Name=\"toc 9\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Normal Indent\"/>\n  <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" Name=\"footnote text\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   QFormat=\"true\" Name=\"annotation text\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"header\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"footer\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"index heading\"/>\n  <w:LsdException Locked=\"false\" Priority=\"35\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"caption\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"table of figures\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"envelope address\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"envelope return\"/>\n  <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" Name=\"footnote reference\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   QFormat=\"true\" Name=\"annotation reference\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"line number\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"page number\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"endnote reference\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"endnote text\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"table of authorities\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"macro\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"toa heading\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List Bullet\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List Number\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List 3\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List 4\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List 5\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List Bullet 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List Bullet 3\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List Bullet 4\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List Bullet 5\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List Number 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List Number 3\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List Number 4\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List Number 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"10\" QFormat=\"true\" Name=\"Title\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Closing\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Signature\"/>\n  <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" Name=\"Default Paragraph Font\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Body Text\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Body Text Indent\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List Continue\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List Continue 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List Continue 3\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List Continue 4\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"List Continue 5\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Message Header\"/>\n  <w:LsdException Locked=\"false\" Priority=\"11\" QFormat=\"true\" Name=\"Subtitle\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Salutation\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Date\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Body Text First Indent\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Body Text First Indent 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Note Heading\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Body Text 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Body Text 3\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Body Text Indent 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Body Text Indent 3\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Block Text\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Hyperlink\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"FollowedHyperlink\"/>\n  <w:LsdException Locked=\"false\" Priority=\"22\" QFormat=\"true\" Name=\"Strong\"/>\n  <w:LsdException Locked=\"false\" Priority=\"20\" QFormat=\"true\" Name=\"Emphasis\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Document Map\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Plain Text\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"E-mail Signature\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"HTML Top of Form\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"HTML Bottom of Form\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Normal (Web)\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"HTML Acronym\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"HTML Address\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"HTML Cite\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"HTML Code\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"HTML Definition\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"HTML Keyboard\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"HTML Preformatted\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"HTML Sample\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"HTML Typewriter\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"HTML Variable\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Normal Table\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"annotation subject\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"No List\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Outline List 1\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Outline List 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Outline List 3\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Simple 1\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Simple 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Simple 3\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Classic 1\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Classic 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Classic 3\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Classic 4\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Colorful 1\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Colorful 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Colorful 3\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Columns 1\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Columns 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Columns 3\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Columns 4\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Columns 5\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Grid 1\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Grid 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Grid 3\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Grid 4\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Grid 5\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Grid 6\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Grid 7\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Grid 8\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table List 1\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table List 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table List 3\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table List 4\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table List 5\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table List 6\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table List 7\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table List 8\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table 3D effects 1\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table 3D effects 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table 3D effects 3\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Contemporary\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Elegant\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Professional\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Subtle 1\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Subtle 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Web 1\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Web 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Web 3\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Balloon Text\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"Table Grid\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Table Theme\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Note Level 1\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Note Level 2\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Note Level 3\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Note Level 4\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Note Level 5\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Note Level 6\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Note Level 7\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Note Level 8\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\n   Name=\"Note Level 9\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" Name=\"Placeholder Text\"/>\n  <w:LsdException Locked=\"false\" Priority=\"1\" QFormat=\"true\" Name=\"No Spacing\"/>\n  <w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading\"/>\n  <w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List\"/>\n  <w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid\"/>\n  <w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List\"/>\n  <w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading\"/>\n  <w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List\"/>\n  <w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid\"/>\n  <w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 1\"/>\n  <w:LsdException Locked=\"false\" SemiHidden=\"true\" Name=\"Revision\"/>\n  <w:LsdException Locked=\"false\" Priority=\"34\" QFormat=\"true\"\n   Name=\"List Paragraph\"/>\n  <w:LsdException Locked=\"false\" Priority=\"29\" QFormat=\"true\" Name=\"Quote\"/>\n  <w:LsdException Locked=\"false\" Priority=\"30\" QFormat=\"true\"\n   Name=\"Intense Quote\"/>\n  <w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"19\" QFormat=\"true\"\n   Name=\"Subtle Emphasis\"/>\n  <w:LsdException Locked=\"false\" Priority=\"21\" QFormat=\"true\"\n   Name=\"Intense Emphasis\"/>\n  <w:LsdException Locked=\"false\" Priority=\"31\" QFormat=\"true\"\n   Name=\"Subtle Reference\"/>\n  <w:LsdException Locked=\"false\" Priority=\"32\" QFormat=\"true\"\n   Name=\"Intense Reference\"/>\n  <w:LsdException Locked=\"false\" Priority=\"33\" QFormat=\"true\" Name=\"Book Title\"/>\n  <w:LsdException Locked=\"false\" Priority=\"37\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" Name=\"Bibliography\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\n   UnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"TOC Heading\"/>\n  <w:LsdException Locked=\"false\" Priority=\"41\" Name=\"Plain Table 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"42\" Name=\"Plain Table 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"43\" Name=\"Plain Table 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"44\" Name=\"Plain Table 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"45\" Name=\"Plain Table 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"40\" Name=\"Grid Table Light\"/>\n  <w:LsdException Locked=\"false\" Priority=\"46\" Name=\"Grid Table 1 Light\"/>\n  <w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark\"/>\n  <w:LsdException Locked=\"false\" Priority=\"51\" Name=\"Grid Table 6 Colorful\"/>\n  <w:LsdException Locked=\"false\" Priority=\"52\" Name=\"Grid Table 7 Colorful\"/>\n  <w:LsdException Locked=\"false\" Priority=\"46\"\n   Name=\"Grid Table 1 Light Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"51\"\n   Name=\"Grid Table 6 Colorful Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"52\"\n   Name=\"Grid Table 7 Colorful Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"46\"\n   Name=\"Grid Table 1 Light Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"51\"\n   Name=\"Grid Table 6 Colorful Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"52\"\n   Name=\"Grid Table 7 Colorful Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"46\"\n   Name=\"Grid Table 1 Light Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"51\"\n   Name=\"Grid Table 6 Colorful Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"52\"\n   Name=\"Grid Table 7 Colorful Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"46\"\n   Name=\"Grid Table 1 Light Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"51\"\n   Name=\"Grid Table 6 Colorful Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"52\"\n   Name=\"Grid Table 7 Colorful Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"46\"\n   Name=\"Grid Table 1 Light Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"51\"\n   Name=\"Grid Table 6 Colorful Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"52\"\n   Name=\"Grid Table 7 Colorful Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"46\"\n   Name=\"Grid Table 1 Light Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"51\"\n   Name=\"Grid Table 6 Colorful Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"52\"\n   Name=\"Grid Table 7 Colorful Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"46\" Name=\"List Table 1 Light\"/>\n  <w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark\"/>\n  <w:LsdException Locked=\"false\" Priority=\"51\" Name=\"List Table 6 Colorful\"/>\n  <w:LsdException Locked=\"false\" Priority=\"52\" Name=\"List Table 7 Colorful\"/>\n  <w:LsdException Locked=\"false\" Priority=\"46\"\n   Name=\"List Table 1 Light Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"51\"\n   Name=\"List Table 6 Colorful Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"52\"\n   Name=\"List Table 7 Colorful Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"46\"\n   Name=\"List Table 1 Light Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"51\"\n   Name=\"List Table 6 Colorful Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"52\"\n   Name=\"List Table 7 Colorful Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"46\"\n   Name=\"List Table 1 Light Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"51\"\n   Name=\"List Table 6 Colorful Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"52\"\n   Name=\"List Table 7 Colorful Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"46\"\n   Name=\"List Table 1 Light Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"51\"\n   Name=\"List Table 6 Colorful Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"52\"\n   Name=\"List Table 7 Colorful Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"46\"\n   Name=\"List Table 1 Light Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"51\"\n   Name=\"List Table 6 Colorful Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"52\"\n   Name=\"List Table 7 Colorful Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"46\"\n   Name=\"List Table 1 Light Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"51\"\n   Name=\"List Table 6 Colorful Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"52\"\n   Name=\"List Table 7 Colorful Accent 6\"/>\n </w:LatentStyles>\n</xml><![endif]--><!-- [if gte mso 10]>\n<style>\n /* Style Definitions */\ntable.MsoNormalTable\n\t{mso-style-name:\"Table Normal\";\n\tmso-tstyle-rowband-size:0;\n\tmso-tstyle-colband-size:0;\n\tmso-style-noshow:yes;\n\tmso-style-priority:99;\n\tmso-style-parent:\"\";\n\tmso-padding-alt:0cm 5.4pt 0cm 5.4pt;\n\tmso-para-margin:0cm;\n\tmso-para-margin-bottom:.0001pt;\n\tmso-pagination:widow-orphan;\n\tfont-size:12.0pt;\n\tfont-family:Calibri;\n\tmso-ascii-font-family:Calibri;\n\tmso-ascii-theme-font:minor-latin;\n\tmso-hansi-font-family:Calibri;\n\tmso-hansi-theme-font:minor-latin;}\n</style>\n<![endif]--> <!--StartFragment--> <!--EndFragment--></p>\n<p>In order to substantiate all of these conclusions, we conducted an effort that attempted to be comprehensive in analysis of existing start-ups and directions in the field of AI applications for life extension, and we have identified the beginnings of many of these trends, fixed in the specific business plans of companies.</p>\n<p>&#xA0;</p>\n<p>Michael Batin, Alexey Turchin, Markov Sergey, Alice Zhila, David Denkenberger</p>\n<p>&#x201C;Artificial Intelligence in Life Extension: from Deep Learning to Superintelligence&#x201D;</p>\n<p><!-- [if gte mso 9]><xml>\n<o:OfficeDocumentSettings>\n<o:AllowPNG/>\n</o:OfficeDocumentSettings>\n</xml><![endif]--> <!-- [if gte mso 9]><xml>\n<w:WordDocument>\n<w:View>Normal</w:View>\n<w:Zoom>0</w:Zoom>\n<w:TrackMoves/>\n<w:TrackFormatting/>\n<w:PunctuationKerning/>\n<w:ValidateAgainstSchemas/>\n<w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid>\n<w:IgnoreMixedContent>false</w:IgnoreMixedContent>\n<w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText>\n<w:DoNotPromoteQF/>\n<w:LidThemeOther>EN-GB</w:LidThemeOther>\n<w:LidThemeAsian>X-NONE</w:LidThemeAsian>\n<w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript>\n<w:Compatibility>\n<w:BreakWrappedTables/>\n<w:SnapToGridInCell/>\n<w:WrapTextWithPunct/>\n<w:UseAsianBreakRules/>\n<w:DontGrowAutofit/>\n<w:SplitPgBreakAndParaMark/>\n<w:EnableOpenTypeKerning/>\n<w:DontFlipMirrorIndents/>\n<w:OverrideTableStyleHps/>\n</w:Compatibility>\n<m:mathPr>\n<m:mathFont m:val=\"Cambria Math\"/>\n<m:brkBin m:val=\"before\"/>\n<m:brkBinSub m:val=\"&#45;-\"/>\n<m:smallFrac m:val=\"off\"/>\n<m:dispDef/>\n<m:lMargin m:val=\"0\"/>\n<m:rMargin m:val=\"0\"/>\n<m:defJc m:val=\"centerGroup\"/>\n<m:wrapIndent m:val=\"1440\"/>\n<m:intLim m:val=\"subSup\"/>\n<m:naryLim m:val=\"undOvr\"/>\n</m:mathPr></w:WordDocument>\n</xml><![endif]--><!-- [if gte mso 9]><xml>\n<w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"false\"\nDefSemiHidden=\"false\" DefQFormat=\"false\" DefPriority=\"99\"\nLatentStyleCount=\"380\">\n<w:LsdException Locked=\"false\" Priority=\"0\" QFormat=\"true\" Name=\"Normal\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 7\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 8\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 9\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 6\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 7\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 8\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 9\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 7\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 8\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 9\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Normal Indent\"/>\n<w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"footnote text\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nQFormat=\"true\" Name=\"annotation text\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"header\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"footer\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index heading\"/>\n<w:LsdException Locked=\"false\" Priority=\"35\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"caption\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"table of figures\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"envelope address\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"envelope return\"/>\n<w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"footnote reference\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nQFormat=\"true\" Name=\"annotation reference\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"line number\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"page number\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"endnote reference\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"endnote text\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"table of authorities\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"macro\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"toa heading\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Bullet\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Number\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Bullet 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Bullet 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Bullet 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Bullet 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Number 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Number 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Number 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Number 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"10\" QFormat=\"true\" Name=\"Title\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Closing\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Signature\"/>\n<w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"Default Paragraph Font\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text Indent\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Continue\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Continue 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Continue 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Continue 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Continue 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Message Header\"/>\n<w:LsdException Locked=\"false\" Priority=\"11\" QFormat=\"true\" Name=\"Subtitle\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Salutation\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Date\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text First Indent\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text First Indent 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Note Heading\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text Indent 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text Indent 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Block Text\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Hyperlink\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"FollowedHyperlink\"/>\n<w:LsdException Locked=\"false\" Priority=\"22\" QFormat=\"true\" Name=\"Strong\"/>\n<w:LsdException Locked=\"false\" Priority=\"20\" QFormat=\"true\" Name=\"Emphasis\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Document Map\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Plain Text\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"E-mail Signature\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Top of Form\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Bottom of Form\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Normal (Web)\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Acronym\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Address\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Cite\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Code\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Definition\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Keyboard\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Preformatted\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Sample\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Typewriter\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Variable\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Normal Table\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"annotation subject\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"No List\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Outline List 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Outline List 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Outline List 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Simple 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Simple 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Simple 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Classic 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Classic 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Classic 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Classic 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Colorful 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Colorful 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Colorful 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Columns 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Columns 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Columns 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Columns 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Columns 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 6\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 7\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 8\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 6\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 7\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 8\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table 3D effects 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table 3D effects 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table 3D effects 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Contemporary\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Elegant\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Professional\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Subtle 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Subtle 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Web 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Web 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Web 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Balloon Text\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"Table Grid\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Theme\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Note Level 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Note Level 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Note Level 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Note Level 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Note Level 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Note Level 6\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Note Level 7\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Note Level 8\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Note Level 9\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" Name=\"Placeholder Text\"/>\n<w:LsdException Locked=\"false\" Priority=\"1\" QFormat=\"true\" Name=\"No Spacing\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" Name=\"Revision\"/>\n<w:LsdException Locked=\"false\" Priority=\"34\" QFormat=\"true\"\nName=\"List Paragraph\"/>\n<w:LsdException Locked=\"false\" Priority=\"29\" QFormat=\"true\" Name=\"Quote\"/>\n<w:LsdException Locked=\"false\" Priority=\"30\" QFormat=\"true\"\nName=\"Intense Quote\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"19\" QFormat=\"true\"\nName=\"Subtle Emphasis\"/>\n<w:LsdException Locked=\"false\" Priority=\"21\" QFormat=\"true\"\nName=\"Intense Emphasis\"/>\n<w:LsdException Locked=\"false\" Priority=\"31\" QFormat=\"true\"\nName=\"Subtle Reference\"/>\n<w:LsdException Locked=\"false\" Priority=\"32\" QFormat=\"true\"\nName=\"Intense Reference\"/>\n<w:LsdException Locked=\"false\" Priority=\"33\" QFormat=\"true\" Name=\"Book Title\"/>\n<w:LsdException Locked=\"false\" Priority=\"37\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"Bibliography\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"TOC Heading\"/>\n<w:LsdException Locked=\"false\" Priority=\"41\" Name=\"Plain Table 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"42\" Name=\"Plain Table 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"43\" Name=\"Plain Table 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"44\" Name=\"Plain Table 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"45\" Name=\"Plain Table 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"40\" Name=\"Grid Table Light\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\" Name=\"Grid Table 1 Light\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\" Name=\"Grid Table 6 Colorful\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\" Name=\"Grid Table 7 Colorful\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"Grid Table 1 Light Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"Grid Table 6 Colorful Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"Grid Table 7 Colorful Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"Grid Table 1 Light Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"Grid Table 6 Colorful Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"Grid Table 7 Colorful Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"Grid Table 1 Light Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"Grid Table 6 Colorful Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"Grid Table 7 Colorful Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"Grid Table 1 Light Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"Grid Table 6 Colorful Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"Grid Table 7 Colorful Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"Grid Table 1 Light Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"Grid Table 6 Colorful Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"Grid Table 7 Colorful Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"Grid Table 1 Light Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"Grid Table 6 Colorful Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"Grid Table 7 Colorful Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\" Name=\"List Table 1 Light\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\" Name=\"List Table 6 Colorful\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\" Name=\"List Table 7 Colorful\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"List Table 1 Light Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"List Table 6 Colorful Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"List Table 7 Colorful Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"List Table 1 Light Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"List Table 6 Colorful Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"List Table 7 Colorful Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"List Table 1 Light Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"List Table 6 Colorful Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"List Table 7 Colorful Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"List Table 1 Light Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"List Table 6 Colorful Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"List Table 7 Colorful Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"List Table 1 Light Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"List Table 6 Colorful Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"List Table 7 Colorful Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"List Table 1 Light Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"List Table 6 Colorful Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"List Table 7 Colorful Accent 6\"/>\n</w:LatentStyles>\n</xml><![endif]--><!-- [if gte mso 10]>\n<style>\n/* Style Definitions */\ntable.MsoNormalTable\n{mso-style-name:\"Table Normal\";\nmso-tstyle-rowband-size:0;\nmso-tstyle-colband-size:0;\nmso-style-noshow:yes;\nmso-style-priority:99;\nmso-style-parent:\"\";\nmso-padding-alt:0cm 5.4pt 0cm 5.4pt;\nmso-para-margin:0cm;\nmso-para-margin-bottom:.0001pt;\nmso-pagination:widow-orphan;\nfont-size:12.0pt;\nfont-family:Calibri;\nmso-ascii-font-family:Calibri;\nmso-ascii-theme-font:minor-latin;\nmso-hansi-font-family:Calibri;\nmso-hansi-theme-font:minor-latin;}\n</style>\n<![endif]--> <!--StartFragment--> <!--EndFragment--></p>\n<p><span>Informatica 41 (2017) 401&#x2013;417</span>: <a href=\"http://www.informatica.si/index.php/informatica/article/view/1797\">http://www.informatica.si/index.php/informatica/article/view/1797</a></p></body></html>", "user": {"username": "turchin"}}, {"_id": "zS3gmg45ooRiZ8cmy", "title": "Updates from the GiveWell blog", "postedAt": "2018-01-04T12:50:27.271Z", "htmlBody": "<html><body><p>The latest updates from the <a href=\"https://blog.givewell.org/\">GiveWell blog</a>.</p>\n<p><a href=\"https://blog.givewell.org/2017/12/29/revisiting-evidence-malaria-eradication-americas/\">Revisting the evidence on malaria eradication in the Americas</a></p>\n<p><a href=\"https://blog.givewell.org/2017/12/28/key-questions-about-helen-keller-internationals-vitamin-a-supplementation-program/\">Key questions about Helen Keller International&apos;s vitamin A supplementation program</a></p>\n<p><a href=\"https://blog.givewell.org/2017/12/22/uncertain-cost-effectiveness-analysis/\">How uncertain is our cost-effectiveness analysis?</a></p>\n<p>Please comment on the GiveWell blog, rather than here.</p>\n<p>Crossposted by the forum admins.</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "Crosspost"}}, {"_id": "nft9YktXLhrroZHiF", "title": "2017 LEAN Impact Assessment: Qualitative Findings", "postedAt": "2018-01-03T20:42:54.335Z", "htmlBody": "<html><body><p><a href=\"https://imgur.com/y14e43Z\"><img title=\"source: imgur.com\" src=\"https://i.imgur.com/y14e43Z.png?1\"></a>&#xA0;</p>\n<p><span>Quantitative Findings</span></p>\n<p><span>Qualitative Findings</span></p>\n<p><span>Evaluation and Strategic Conclusions</span></p>\n<p><span>Methodology</span></p>\n<p>&#xA0;</p>\n<p><span>This document is the second in the LEAN Impact Assessment Series, briefly summarising relevant data from our qualitative interviews with 31 EA group organisers. Click </span><a href=\"/ea/1ic/\"><span>here</span></a><span> to read the previous report from the series [1]. The third and penultimate report will synthesise data from qualitative and quantitative reports in order to derive general conclusions.</span><span><br></span></p>\n<p>&#xA0;</p>\n<div>\n<table><colgroup><col></colgroup>\n<tbody>\n<tr>\n<td>\n<p><span>Note:</span><span> Our qualitative report is designed to offer more detailed understanding by devoting a high degree of attention to a small, illustrative sample. This approach also allows for more responsiveness to input from respondents (in contrast to the survey), and in doing so, improves accuracy and allows for overlooked factors to come to light. This evidence should be read as a complement to our quantitative report, that sheds light on the weighting of different issues and draws out new factors unanticipated by our survey design. </span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>&#xA0;</p>\n<p><span>The report is split into two sections: </span></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p><span>Organiser Experiences and Wider Issues</span></p>\n<p><span>1.1 - Appetite for Assessment and Evidence-Based Guidance</span></p>\n<p><span>1.2 - Legitimacy and Reputation Concerns</span></p>\n<p><span>1.3 - The Significance of Context</span></p>\n<p><span>1.4 - Group Influence</span></p>\n<p><span>1.5 - Organiser Insecurities</span></p>\n<p><span>1.6 - Personal Connection and Motivation</span></p>\n<p><span>1.7 - Desire for Integration with the Global Community</span></p>\n<p><span>1.8 - Direct Action</span></p>\n<p><span>1.9 - Productivity and Accountability</span></p>\n<p><span><span><span>&#xA0;</span></span></span></p>\n<p><span><span>Specific Services and Resources</span></span></p>\n<p><span>2.1 - Written Guides and Resources</span></p>\n<p><span>2.2- Websites and Tech Support</span></p>\n<p><span>2.3 - Connecting and Introducing</span></p>\n<p><span>2.4 - One-to-one Support</span></p>\n<p><span>2.5 - Funding</span></p>\n<p><span>2.6 - Venues</span></p>\n<p><span>2.7 - Speakers</span></p>\n<p><span>2.8 - Group Calls</span></p>\n<p><span>The first section presents crucial themes from the interviews relating to common experiences, needs, or concerns experienced by group organisers. The second section presents data on existing forms of support offered by LEAN and other EA organisations supporting groups. In both sections, general observations are made followed by demonstrative quotes from the interview transcripts [2].</span></p>\n<h2><span>Organiser Experiences and Wider Issues</span></h2>\n<h3><span>Appetite for Assessment and Evidence-Based Guidance</span></h3>\n<p><strong>&#xA0;</strong></p>\n<p><span>There is a high level of demand among organisers for formal research into outreach and group management strategy. Organisers also want to be able to effectively monitor their own progress and impact, and they look for certainty and empirical backing to inform aspects of group leadership.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I would really appreciate help with planning ahead and deciding on useful metrics to measure so that we can assess how much impact we&#x2019;re having.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Sort of like a roadmap too&#x2026; like based on other groups and what they&#x2019;ve experienced. Like first they have 1-2 years doing x, y, z and then they reach n members, etc&#x2026;&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Measuring impact for groups is so difficult, it takes so much time to plan. It would be helpful to have a vehicle through which to do this. So LEAN tracking metrics across groups would be very valuable.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;What would be good is some plans. Like how should you organise a small group, a large group etc? How big are other groups? What is the average size?&#x201D; </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;The things you are doing, like the survey, it&#x2019;s really something super important. We were thinking of iterating that here on a local scale, but then the person in charge of that concluded that this survey you did was good enough&#x2026; it&#x2019;s just something that we needed, and this is giving us more insight that we can chew upon.&#x201D; </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I would be really interested in what you&#x2019;re looking at, how you ask people&#x2026; how you set up evaluation and surveys after events you&#x2019;re holding, and stuff like that.&#x201D; </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;The most important is research on how to advertise&#x2026;for example Facebook advertisements. What kind of keywords to write, and is it worth it? How many people do you get that way? How well it responds to persistence and all these things&#x2026;&#x201D;</span></p>\n<h3><span>Legitimacy and Reputation Concerns</span></h3>\n<p><span>In some contexts, organisers feel that there is a functional need for public legitimacy for their groups. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;It would be the idea of us drafting guidelines and having the government department of charitable giving sort of rubber stamping them. With something like that I feel&#x2026; if there&#x2019;s access to more prominent people in this field, we could send them Singer&#x2019;s talk. But I don&#x2019;t know if there&#x2019;s any additional way of getting legitimacy.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;We had more funds by using The Life You Can Save funds for giving games, and also more credibility.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I know I needed a lot of tech support setting up the eahub.org email address, but people respond to that better than something that has Gmail at the end of it. We&#x2019;ve noticed we have higher response rates.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;[LEAN] made us a club website which looks very professional. And that went a long way to showing people that we&#x2019;re not some fringe thing that&#x2019;s only locally run.&#x201D;</span></p>\n<h3><span>The Significance of Context</span></h3>\n<p><span>A crucial insight from the interviews is the degree to which group-specific context shapes and determines the success and opportunities available to organisers. There is variety in the demographic that organisers are able to attract, the culture and attitudes prevalent in specific countries and regions, the proximity of groups to other EA organisations and communities, or to otherwise like-minded collectives, to name but a few distinctions.</span></p>\n<p><span>For this category, summarised examples are a more concise means to demonstrate this than verbatim quotes.</span></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p><span>- A group that raised a significantly high sum through group fundraisers benefitted from a proactive student body, generous state support, and an unusually high national culture of altruism and charitable giving.</span></p>\n<p>&#xA0;</p>\n<p><span>- A group in a large spread out city found regular attendance hard to secure due to the travel times required.</span></p>\n<p>&#xA0;</p>\n<p><span>- One group has a strategy of targeting corporations in order to take advantage of a state requirement for corporate charitable giving.</span></p>\n<p><span>The Pledge is less appropriate for groups in poor countries.</span></p>\n<p>&#xA0;</p>\n<p><span>- Several groups are benefitted by residing in cities with unusual concentrations of academics or experts. For example, policy experts in Washington D.C., Geneva, Brussels or London. Researchers in Oxford, Cambridge and Ivy League institutions and so on.</span></p>\n<p>&#xA0;</p>\n<p><span>- A group in the Middle East has to operate in the face of state limitations on charitable giving and is looking for ways to partner with mosques and existing religious traditions surrounding philanthropy.</span></p>\n<p>&#xA0;</p>\n<p><span>- Several Asian groups are partnering on translation and reframing core EA ideas to target local attitudes.</span></p>\n<p>&#xA0;</p>\n<p><span>- Groups in some countries, like Germany, have to be more sensitive about association with utilitarianism or Peter Singer due to public hostility.</span></p>\n<p>&#xA0;</p>\n<p><span>- 80,000 Hours recommended careers are significantly less accessible in some countries than in others.</span></p>\n<p>&#xA0;</p>\n<p><span><span><span>Group Influence</span></span></span></p>\n<p>&#xA0;</p>\n<p><span>While tangibly gauging impact is a widespread challenge for organisers, some respondents mentioned concrete instances where a person either became actively involved in EA as a result of being introduced to EA in a group or cases where getting involved in group management led to individuals engaging more strongly with EA.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I got into EA through my local group at [University], which explains my interest in it. It&#x2019;s changed a lot of my values, and I wanted to create that opportunity for other people.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;It&#x2019;s definitely helped me grow more, just taking those actions. [Member name] taught me as well, as it was because I got him into EA that he then became very involved.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Because I am organising, I do think the possibility is high that [nearby regional EA Group] is where I can do a lot. And so I certainly consider that in my career path right now&#x2026; I plan on going to [regional EA Group] and working there in the future. So you can see, EA has changed my life a lot.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I think that they [group members who took on leadership after the respondent moved on from organising the group] became more engaged, but I don&#x2019;t know if they would have done that some other way instead. Like maybe going up to [EA Capital City], or more conversations online, or skypes with people. Certainly, a couple of people like [Name] and [Name] are much more involved than they were before and they took the Pledge while we were there. And we had a lot of conversations about these things&#x2026;&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;The most useful feedback I received was locally, from the people who actually came to events. Seeing them be happy also made me happy because we&#x2019;d spent time talking about all sorts of brilliant stuff that we couldn&#x2019;t talk about with anybody else. Or actually convincing the first person to actually donate money, or work for an effective cause, or convincing someone in an argument that other people were thinking about or actually acting upon&#x2026; this sort of feedback was still the best thing I received. And I think the best thing about my job right now is that I&#x2019;m seeing this. So every time I organise an event I&#x2019;m getting more and more people into this, and this is making me feel amazing.&#x201D; </span></p>\n<h3><span>Organiser Insecurities</span></h3>\n<p><span>Some of our respondents experienced insecurity in their role as group organisers, especially those who had recently started up. For example, we encountered abstention from interview participation arising from discomfort in answering questions about group progress. A few participants also expressed concern prior to interview over whether or not their groups were large, active or successful enough to &#x2018;count&#x2019;. In conversation, respondents occasionally unfavourably contrasted their situation to an ideal, with the counterfactual typically being a larger, more active or more professional group. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I feel a lot better about undertaking this with a team, but I don&#x2019;t think&#x2026;I don&#x2019;t know that any of us have done like formal outreach or community growth in this sort of way before.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;It&#x2019;s basically me and a very small team, and we are still learning and none of us are really researchers in one of the EA organisations.&#x201D; </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Not only were insecurities indirectly apparent in how organisers spoke about their groups at times, but there were also some direct reports of specific anxieties and doubts, often related to respondents&#x2019; concerns over their own personal EA credentials, or the fidelity of their approaches.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I hope that [University] group gets going, but I also just feel like an old guy trying to make stuff work.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Some EAs are vocally saying we shouldn&#x2019;t be doing much outreach&#x2026; we don&#x2019;t want to dumb down the message. I worry&#x2026; I think we are doing the right thing, but there are mixed messages out there about what we should be doing. I don&#x2019;t know what people think about us and what we should be doing. I feel lacking in confidence. I kind of want a seal of approval that makes me feel like I&#x2019;m doing the right thing.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;There&#x2019;s just so much literature, Jesus! I started reading about, you know, Peter Singer, and now I&#x2019;m into professional finance&#x2026; and I just can&#x2019;t keep up! I had it in my mind that maybe I could get a position in some EA organisation. It&#x2019;s a bit intimidating, though, seeing the positions and the people working in them, like &#x2018;what the fuck? How did they do that? They raised $2 million?&#x201D; Some of these people seem so alien&#x2026;so far out from normal people.&#x201D;</span></p>\n<h3><span>Personal Connection and Motivation</span></h3>\n<p><span>Personal interaction with other EAs is highly significant and sometimes critical for the motivation of group organisers. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;If I&#x2019;d been without personal interaction, I&#x2019;d never have gotten that much involved with EA.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I always seem to be inspired after I touch base. Because it can be kind of challenging when you&#x2019;re not really immersed in a culture where people have these kind of ideas. So I always find it very uplifting to chat to people who are involved in the overall EA community.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I don&#x2019;t even know why, but meeting all those people in there [EAG] and getting all this awesome advice that I really needed and feeling the sense of the international community rather than a local thing that&#x2019;s generally myself and a couple of guys I&#x2019;ll see each Thursday. I think this really helped me go and invest more, basically.&#x201D; </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;When it comes to feedback from the wider community, I still have this fuel from the 2015 conference. I&#x2019;m still fueled by that experience. And I&#x2019;m going back to London - I just bought a ticket today - I decided I&#x2019;ll just go there and do it again. So in terms of that feedback, there would be no initiative from my side if not for that feedback.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;After last year I felt a bit not connected. I felt alone here&#x2026; yeh I told you how this guy just disappeared. But last year when I had the retreat, I met these wonderful EAs&#x2026;&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>While some of the organisers we spoke to were independent &#x2018;self-starters&#x2019;, most success stories referred back to critical connections, such as meeting people at a conference or retreat or finding an equally committed co-organizer. Conversely, isolation from the wider EA community and a lack of equally motivated partners was one of the most frequently named challenges facing organisers.</span></p>\n<h3><span>Desire for Integration with the Global Community</span></h3>\n<p><span>In addition to the personal needs of organisers, respondents widely expressed a desire for better avenues to connect their members to the wider community.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I&#x2019;m kind of the only EA that has met the others outside of [country]. I kind of want members to have the chance to skype with other EAs.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I really want to see the club get more directly involved and engaged with the local [city] EA community. So, if you&#x2019;re like a club member at EA [university] you pretty much automatically know most of the EAs in [city]. I kind of want that to be an automatic, organic thing.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I&#x2019;d just like us to be more connected to the greater EA community.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Things that really help to implement would be to make sure that we can send more people to EA Global&#x2026;that would be something we&#x2019;d give out like candy if possible.&#x201D; </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>In particular, some organisers found themselves at a loss for where to direct their members in order to integrate them more fully into the movement. Several expressed dissatisfaction with existing channels such as EA Facebook groups and the EA Forum with respect to bridging the gap between seasoned or professional EAs and newcomers.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;The EA Forum doesn&#x2019;t seem like something that can be used at all by people who aren&#x2019;t running an EA org or researching full time or whatever. It doesn&#x2019;t seem like a terribly nice platform to recommend people to go look at.&#x201D;</span></p>\n<h3><span>Direct Action</span></h3>\n<p><span>A related but distinct concern from respondents was the need for direct action opportunities in order to improve member retention. In some cases, EA groups appear to thrive on discussion alone. But in many cases, people need something more tangible.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Around the world there is a common problem, that after people get involved in EA there&#x2019;s not many things to do or keep them interested&#x2026;generally I want to let people know that joining EA isn&#x2019;t just about talking, but actually getting involved in things and improving yourself gradually.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;People will say &#x2018;well great, this sounds like a good idea, but what is there to actually do?&#x2019; It&#x2019;s just a small community of people that are interested, and I think what&#x2019;s tricky is that people don&#x2019;t really see what they get from joining. And it&#x2019;s like &#x2018;ok, great. I like this idea. I&#x2019;ll read all the materials, but why do I need to come?&#x2019; Kind of thing&#x2026;&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;We&#x2019;re getting better at spreading EA as information and making people interested in it, but we get many messages from people wanting to be involved in the club and actually </span><span>do </span><span>stuff. But then, you know, the stuff we do is more host meetups, talk about stuff, get speakers, or we talk about donations.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;We got a lot of students and young professionals who liked the ideas, but because we don&#x2019;t have anything concrete for them to engage in this&#x2026;I can&#x2019;t just say &#x2018;Hey! Job opportunity at ACE! Apply for this!&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;We still look for ways of generating appeal. I think the prioritization project and influence over a certain number of dollars was useful for that. But since we&#x2019;re largely focused around meeting weekly and talking about philosophical issues it&#x2019;s not always as prestigious as joining another society that potentially has more to offer.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;There are people that want to talk about weird stuff and ethical paradoxes, and then there are people saying &#x2018;lets do something tangible as a group&#x2019;. And so I&#x2019;ve been trying to figure out how to make everyone happy&#x2026;It would be cool to be involved in something that ties us to the community and helps people stay invested.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Overall, discussion and social interaction are often insufficient to persuade members to regularly engage and commit to an EA group. Organisers on the lookout for group actions that offer tangible impact, as opposed to traditional mainstay group targets like career changes or pledges. Respondents also felt that activities need to offer goals which new EAs could not reach equally well by going it alone if groups are to retain these individuals.</span></p>\n<h3><span>Productivity and Accountability</span></h3>\n<p><strong>&#xA0;</strong></p>\n<p><span>In many cases where organisers succeed in attracting a core group of regular attendees, they still struggle to find effective strategies for motivating their members to adhere to commitments and to take timely action on group goals. This is compounded by the fact that groups are organised voluntarily, which makes organisers reluctant to put pressure on their teams.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Getting organisers who have enough time. I do organising, but I&#x2019;m really busy. I want others to step up, but I don&#x2019;t want to pressurise them. Others don&#x2019;t care as much as me. Sometimes people offer but then bail, or don&#x2019;t deliver.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;There&#x2019;s a challenge of commitment. When we have administrative talks we&#x2019;ll say &#x2018;right let&#x2019;s do x, y and z&#x2019; but then one week, two weeks later, nothing has been done... And something else that&#x2019;s important is to have accountability partners, accountability checks. We have this kind of&#x2026;everybody&#x2019;s a volunteer, so nobody takes a stand, you know? We talk and talk but the action gets blurry. Who does what and when?&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;It&#x2019;s hard to say &#x2018;alright, here&#x2019;s the thing we need you to do. We won&#x2019;t pay you for it, we just want you to do it in a specific way&#x2026;can you do that consistently for a semester and then train someone else to do it?&#x2019;&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;If me and, say, one other person were to go, I don&#x2019;t think there would be a lot of willpower to really push to get things done.&#x201D; </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I think it&#x2019;s not that hard to find people who are interested in Effective Altruism a little bit, but those who want to be really involved, those are hard to find.&#x201D; </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Entropy, as I would call it, was one of the biggest challenges. There was really a significant cost to getting people together and making them into a group. You know, an actual group&#x2026; an organised group, a sort of quasi-organised group. So whenever I put these people together to work on a task they would quickly disassemble, they would lose focus, lose direction. I would lose direction myself, I would have to change my goals. So managing myself and managing the group, you know, without financial incentive, without the traditional ways of making people work together - as in a company or an actual organisation - that would be difficult.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Not only do organisers struggle in finding committed members and soliciting productive output, but many also found it challenging to find time and motivation for their own goals and ambitions.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I only hit a home run once, and it was when the coach yelled at me and I felt it was unjust. So I had a lot of passion. And if you&#x2019;d be willing to periodically say critical things to me, like &#x2018;[name]! You&#x2019;re not doing good enough!&#x2019; that would be really helpful.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Having consistent events at the same time and place would be helpful. I mean this is largely because I&#x2019;m a bit&#x2026; I&#x2019;m not the most conscientious person, so I&#x2019;ve just been doing events every few weeks.&#x201D; </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Across the board, organisers mentioned the challenges of balancing group administration alongside busy course loads and careers. </span></p>\n<h2><span>Specific Services and Resources</span></h2>\n<h3><span>Written Guides and Resources</span></h3>\n<p><span>Respondents widely viewed written guides and resources to be highly valuable. Resources of this kind were often marked out as especially useful or valuable in helping organisers get their groups started, and in reducing labour; particularly reading and discussion plans and presentations. Respondents were enthusiastic about any resources which provided additional ideas for group activities, and for content creation services such as scripts to use for introducing EA to outsiders, and for content to share on social media. Unsurprisingly, the general EA Newsletter is valued by group organisers for this reason. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I&#x2019;ve gotten a lot of help via content from organisations. Any content on EA and GiveWell, and you guys [LEAN] also. The EA Forum is a great source of information. That has given me a lot of inspiration for meetups and stuff&#x2026; It was very helpful.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Nowadays you can get everything, but two years ago a lot of stuff was not there. Now everything is much more organised, I think. Two years ago we had to do everything ourselves. There weren&#x2019;t materials in German. I didn&#x2019;t know all the presentations that were there.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I tried to put as much effort as possible into the meetings, but when I have resources to make that easier, that&#x2019;s really, really helpful. So I think the meetings would have been a lot less polished and more casual without those resources, which is why I&#x2019;m putting a lot of stock into those resources. I think I was able to get the core group really excited because of having like the reading list, for example. Just having that and being able to print that out&#x2026;&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;One of the really important resources was the list of articles and things that Harri sent me that has been used for an EA curriculum in the past. We&#x2019;ve modified that a little bit, but we&#x2019;ve used that essentially to guide our discussions.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>However, this was also the area most frequently mentioned in reference to ways that group support could be improved. In particular, respondents felt that sufficient quality resources were available, but that organisation, coordination and presentation undermined the accessibility and use-value of these resources substantially. Some respondents also expressed concern regarding the confusion caused by the existence of multiple organisations (e.g. TLYCS, LEAN, CEA, EAF, 80,000 Hours) involved in providing content and support.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;It&#x2019;s really bitty and confusing, which people can do what&#x2026; One obvious starting point would be: if someone wants to start a group&#x2026; I don&#x2019;t know how easy that is, but there&#x2019;s a bunch of things to read and people to talk to, and it&#x2019;s really confusing who&apos;s going to be doing what. Clarifying this for new organisers&#x2026; a tried and tested simple pathway&#x2026;&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Too much duplication&#x2026; too much content in different places.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;It&#x2019;s clear from talking to [EA organisers in the country] that they would like a central point of contact that coordinates with all the resources that exist in general.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I really get confused with all the things that have popped up connected to local groups.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I [organiser of a well known University EA Group] get a lot of emails from groups that want to start up but are lost and don&#x2019;t know what to do. And I know there are some resources online for how to start your group and what to do, but there seem to be issues with finding that information, and navigating it, and making it useful. So I think informational resources could be made more accessible...&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;At the moment, what&#x2019;s confusing to people starting a group is that they don&#x2019;t know whether to correspond with CEA, EAF or LEAN. And I think that gives the impression that the community is not organised. Or that it&#x2019;s a lot of fractured groups rather than a coherent&#x2026; sometimes people post saying &#x201C;I want to start a group, what can I do?&#x201D; and they get about ten different comments from different places, and I think it would be good to focus on just one.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>In mid-2017, LEAN introduced a comprehensive spreadsheet, listing all known online guides and resources with support from CEA and EAF. Feedback on this resource map has been enthusiastically supportive, which reinforces the finding that improved presentation and organisation is sought after in this area.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I think the most useful thing has been the resource map. If I could think of something that would be useful&#x2026; it&#x2019;s just there, which is really nice! I posted it on the conference event recently because I was like &#x201C;This is amazing! Everyone needs to see it!&#x201D;.&#x201D; </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Whoah! This is great!&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>It should be noted that, while the consensus stresses that better use of existing resources is a priority, respondents were not against the provision of fresh material in the future. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>In addition, respondents had several recommendations for new resources or improvements to existing ones. There was a lot of interest in tools that would enable them to gauge impact. Additional content for use on social media was a recurring wish, as was the need for better support in finding speakers and locations. Finally, we received a recommendation to create a map that enables organisers to explain to the general public how EA organisations relate to one another and to local groups, and a resource that gives organisers concrete examples of how different kinds of support had been put to use by other groups.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I&#x2019;m also not sure if there is basic text for a group? Like basic text you can copy into a Facebook event and just slightly change. So basic drafts&#x2026;&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;To have, like, things to post on the general Facebook page because inspiration runs out after a while of course. And sometimes you see, like &#x201C;Oh shit, there was this World Malaria Day, and it was yesterday&#x2026; I could have had a post about it.&#x201D; But if someone who is doing it professionally has like a calendar, and warns you about that in the future&#x2026; Maybe you can post about that and suggest people do this, or whatever&#x2026; because I notice that it actually works out really well.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;It would be cool to have some kind of slide or something to show people the overarching organisations, who they&#x2019;re in touch with, how they&#x2019;re funded&#x2026; which are recommended charities&#x2026; which are charity navigators etc. We could make one ourselves, but if one existed already, that would be cool.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Something like a list of all those things and then a link to examples of how other clubs have used it.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Finally, interest was expressed in the conversion of existing materials into different formats, such as video or audio.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;People have different preferences for consuming information. So maybe if you took the stuff that&#x2019;s been&#x2026; like I really love to read things but I know that some people love podcasts and audio. I love video as well. I probably like video better. So maybe you could refocus stuff that people really enjoy into other mediums.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<h3><span>Websites and Tech Support</span></h3>\n<p><span>In reflecting on their experience of tech resources and support, respondents referred to group websites, paid Meetup.com subscriptions and the EA Hub. We did not receive any feedback about the EA groups platform launched by CEA. </span></p>\n<h4><span>Websites</span></h4>\n<p><span>There is some variety in respondents&#x2019; attitudes towards websites. A vocal minority rated website provision highly helpful and impactful. Some respondents had not yet started using a website and were keen to arrange one in the future. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;We did have a separate website, but it was terrible. I was like &#x2018;Oh dear God&#x2026;&#x2019;; we were extremely grateful. Super helpful! We could have done it ourselves but it would have been ugly and awful and gross.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;The website I&#x2019;d probably rate as 9 or 10 (in terms of usefulness). That thing is just such a great go to.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;A few months ago I wanted to create a website and LEAN offered to help. In the end, I didn&#x2019;t have time to do so, and now I&#x2019;m wondering if there&#x2019;s something that is already ready to set up, where I just have to put the text in.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;We asked [LEAN] for a domain and [LEAN] brought up a webpage for us, which was really great. I think that a new group that is growing - maybe not on the first day or first month of activity - at some point, all group should have websites, and if we can do that at a low cost then that&#x2019;s great. We use that, and especially the little stuff that is concerned, like email aliases so that I can send an email as someone who is behind an organisation, not just a Gmail address. And I think that really does make an impression - especially for organisations you might come into cooperation with.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Among respondents who seemed lukewarm about their websites, most framed the problem in terms of a lack of time or technical skill with which to customise their sites and exploit them to the full.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I guess it would be nice to revamp our website, which isn&#x2019;t very high quality. So one thing would be having a web developer who could help us with that - that&#x2019;s great. Emails would be good, and webhosting would be really nice because we don&#x2019;t have anyone who maintains our website actively. So it&#x2019;s very bare bones, and isn&#x2019;t that nice right now.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I&#x2019;ve been meaning to do a website, mailing list, post on social media etc. but I don&#x2019;t have time.&#x201D; </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;A good website would be nice. We have a lot of knowledge I guess, but we&#x2019;re not good at website. So yeh&#x2026; the website is the biggest help.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>However, one respondent was explicitly negative about the value of websites for groups. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I think many Universities operate through mailing lists and the University pages, and though the website&#x2026; a few people have used it, I find we&#x2019;re not making use of it. So they have been useful but I think you&#x2019;d have diminishing returns if you invest in those because I don&#x2019;t find many groups using them.&#x201D; </span></p>\n<p><strong>&#xA0;</strong></p>\n<h4><span>Meetup.com</span></h4>\n<p><span>Meetup.com is a social media platform tailored to event scheduling and group promotion and management. Group members can access the site and join groups for free, but creating and administering a group on Meetup.com requires a paid subscription. Currently, LEAN covers Meetup.com subscription for group organisers on a case-by-case basis.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>A similar pattern was true of Meetup.com to that which emerged from the website feedback. Namely, a minority of organisers find the platform enormously useful, while others find it modestly useful. We did not receive any negative feedback about Meetup.com at interview. The majority of EA groups do not have a Meetup.com group.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Given my goal, which is meeting other Effective Altruists, I think Meetup.com has been pretty successful. Like we&#x2019;ve got a very decent showing every week, and new people I didn&#x2019;t know of before. So that&#x2019;s been good. Good but not amazing.&#x201D; </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;The Meetup group I&#x2019;d say is 4 out of 10 (in terms of usefulness). We&#x2019;ve attracted a couple of people. I like the fact that people don&#x2019;t have to have Facebook. It feels more inclusive even if we don&#x2019;t glean people from it, although we actually did.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;...in Germany a lot of people don&#x2019;t use Facebook, because privacy is something people really value here. So it&#x2019;s sort of tricky to reach people who don&#x2019;t have Facebook.&#x201D; </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>As illustrated in these examples, there are concrete cases of counterfactual membership gains as a result of using Meetup.com, including one group which is solely maintained through Meetup.com. Organisers also value the platform for enabling them to reach individuals who do not use Facebook.</span></p>\n<h4><span>The EA Hub</span></h4>\n<p><a href=\"http://www.eahub.org\"><span>www.eahub.org</span></a><span> is a website created by Rethink Charity (formerly known as .impact) as a community resource. The EA Hub was initially put together using submissions from the first EA Survey. The site offers a donations registry which encourages EAs to publicly list donations in order to motivate others. It also offers a list of personal EA profiles where EAs share basic personal information such as preferred cause areas, career plans, and so forth. LEAN has created public profiles for EA Groups on the Hub since its&#x2019; inception. The site offers a map of EAs and a map of EA groups designed to help people to access a quick visual of nearby like-minded groups or individuals. Finally, LEAN uses the domain @eahub.org to provide group organisers with official email aliases in instances where a group does not have a website. (Where groups do have a website, we provide group-specific domains which are then&#xA0;used for their email aliases).</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>While not many organisers mentioned the EA Hub, one organiser found it to be instrumentally influential in the formation of his initial committee. Another found the map useful, and the third felt that the Hub had once offered potential, but had become outdated and redundant over time.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;My EA Hub profile is how the first two [group members] found me.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I really like the map [of EAs]. Looking at the map and seeing where groups are is really cool.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I really like your efforts. I like the EA Hub!&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;The EA Hub was a great initiative that sort of feels redundant now. It&#x2019;s not really working, to be honest. It doesn&#x2019;t meet people&#x2019;s expectations of how software should work in 2017 - I&#x2019;ve received that feedback on a couple of occasions. And some of the functionality is broken.&#x201D; </span></p>\n<h3><span>Connecting and Introducing</span></h3>\n<p><span>While there is no explicit service for introducing specific organisers to one another, LEAN has historically helped many organisers on a case-by-case basis, introducing them to other EAs with compatible needs or interests, or similar locations [3]. Respondents raised this as an example of external support that was valuable and impactful.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;If it weren&#x2019;t for LEAN and Rethink Charity, I would be even further behind when it comes to actually finding EAs on the ground in [Country]. People who self-identify as EA in [Country] are very, very rare. I can pick out less than ten right now&#x2026; the people who self-identify as Effective Altruists, as far as I know of&#x2026; that I&#x2019;ve engaged with on a one-to-one basis. I would be starting with nothing, basically, if it weren&#x2019;t for you guys. I think that&#x2019;s something, maybe hard to systematise and predict, but&#x2026; those early introductions that help you build critical mass&#x2026; you&#x2019;ve been really helpful with that.&#x201D; </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Making sure that other group and project email addresses are up to date is so helpful.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Rationalist groups are easy to find, but something harder to search for is if there are EA corporations or non-profits in [City]. I was surprised to learn that [EA Organisation] was based here.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;[LEAN] has been a great support for me. Almost from when we met there was a consistent line of communication, and I would get a lot of advice and he would always connect me to the right people, which really worked out.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;You know I hadn&#x2019;t even thought of trying [to contact other organisers], so after this conversation one of the things I&#x2019;m definitely going to do is just reach out to the guys in [a place LEAN recommended the organiser contact] and see what exactly they&#x2019;re doing regarding such a challenging regulatory environment.&#x201D; </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;&#x2018;I had help from [LEAN] in the beginning when she connected me to some people&#x2026; So far [she] has been super helpful, because if I didn&#x2019;t have [her] I wouldn&#x2019;t have two members who are already in EA, and they&#x2019;ve put a lot of work into it. So I guess that&#x2019;s...very precious stuff.&#x201D;</span></p>\n<h3><span>One-to-One Support</span></h3>\n<p><span>This section refers to input and feedback that organisers receive from LEAN and other organisations. In the case of LEAN this involves personal discussion between an organiser and a member of the LEAN staff, but it may also involve participation in the </span><a href=\"https://docs.google.com/document/d/1-oAMaRiTQpZY4xqZ3buIu6yO-YKQOT3zU7uTS9jt8j4/edit\"><span>mentoring programme</span></a><span>.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Respondents who mentioned having drawn on one-to-one support found it useful. Furthermore, although many respondents had not formally drawn on this support from LEAN or any other EA organisation, many mentioned positive interactions with independent EAs or other organisers. Respondents were widely enthusiastic about the value in continuing to make such opportunities available.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I chatted with [LEAN] and, that was actually a really good chat - I still have the notes from that - uh, this was still in the mode when I was the only real organiser, and I just wasn&#x2019;t able to act on a lot of that stuff because I didn&#x2019;t have the motivation, I didn&#x2019;t have the bandwidth to be like, to feel like I was the only person pushing the group along.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;More frequent conversations and more networking would be highly helpful, like keeping in touch and finding out what other groups are doing, and picking their brains&#x2026; picking their information. That would be very helpful, I think. So right now it doesn&#x2019;t seem like I have any of that.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Nevertheless, some respondents hadn&#x2019;t been aware that support was on offer from LEAN, or they had started up before it was available.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;The part that was the most frustrating was when we started we had nothing. People only start offering help after the first year. But if we had people advising us as soon as we started, it would have cleared things a lot. So I really think it&#x2019;s very important that as soon as you hear that an EA club anywhere in the world has started. Like&#x2026; very fast&#x2026; run to them and offer support. That was probably the number one thing. &#x2018;Cos things are so much easier now, and they could have been at the start if we had that support at the start&#x2026;&#x201D;</span><span><br><br></span></p>\n<p><span>Other respondents felt that their needs were too specific or unique for general support to be helpful. For example, one organiser was most in need of input from an Islamic religious expert, while another stressed the significance of cultural and language barriers.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;A lot of these things is like&#x2026;I mentioned before that there is this language barrier and cultural difference. I think these are not an easy thing to get bridged.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>One or two organisers mentioned that they had considered asking for support but had found themselves too busy to get around to it.</span></p>\n<h3><span>Funding</span></h3>\n<p><span>The most frequently mentioned need among organisers was for improved financial support. Funding was also the category of external support that groups found most helpful when asked to reflect on their development.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I would say the hardest part was, uh, not having constant funding. We were pretty limited in what kind of events, or what have you, we&#x2019;d be able to put up. I mean I sunk something like $200 into like the initial advertising of the club that was never made back. I don&#x2019;t expect it to ever be made back. But of course it wasn&#x2019;t just me&#x2026; everyone kind of helped contribute to it.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;...he called the group to give me some money. So we decided to print some posters, some leaflets&#x2026; to invest in Facebook advertising - as I remember. And we got thirty people attending. It was like&#x2026; that was a nice push right there because these guys were really into Effective Altruism. They were really interested, they knew what they came for. And out of those guys, I think two of those guys are still with us and are key people for the organisation. Or even more&#x2026;like three to five out of thirty. And that was three years ago!&#x201D; </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Getting the Meetup group set up was helpful mainly from a monetary perspective, honestly. I personally don&#x2019;t have much money so that was the main thing.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Funding is especially crucial for groups that want to level up their operations, for example, transitioning from a local to a regional group, or transforming into a foundation or charity. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;How to have enough resources to focus on this stuff. We are students, we work, we do all kinds of stuff, so how can we also do Effective Altruism [Country]? And this has been a huge challenge with regards to money, with regards to time, with regards to all kinds of resources.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;If we really want this club to shine, we will need someone to work on it that isn&#x2019;t also a student. I&#x2019;m not sure exactly how this would work, or what bureaucratic limitations that person would have, but we need a more dedicated team than we currently have.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I no longer think of us as a local chapter because we are becoming an organisation. And that makes it so that you need to, you know, grow&#x2026; to meet the challenges that we are now facing.&#x201D; </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;There are all these small NGOs that get money to go and talk in schools about recycling or racism, and all of these small organisations found money for this to happen, so it should be possible for Effective Altruism to do so.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>In most of these cases, the need for financial resources is connected to the need to hire paid, dedicated staff in order to realise the opportunities and goals that groups have. </span></p>\n<h3><span>Venues</span></h3>\n<p><span>Another frequent challenge for organisers lies in securing appropriate locations for regular meetings. In some cities, the challenge lies in finding places that are conveniently accessible to members from different, far-flung locations. Others know of suitable locations but can&#x2019;t afford the booking fees, or have been struggling in crowded and noisy restaurants, cafes or bars.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;What are good venues to do meetups? Here we have a problem because venues are bad somehow. Like we have to pay or something else, you know?&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Currently we have a bit of a problem with a place we&#x2019;re doing the meetups because we began in a restaurant, but it was too small. When it&#x2019;s loud, you can&#x2019;t really talk. Especially for social meetups. Because we don&#x2019;t have student union membership, we can&#x2019;t book a University room. We have another room, but it&#x2019;s quite expensive.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;The organisation I&#x2019;m at right now, they&#x2019;ve helped us by just offering workspace. We can organise events here at cost price and have our meetings handled, and things like that.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;The main challenges have been booking rooms and working out what events we could run.&#x201D;</span></p>\n<h3><span>Speakers</span></h3>\n<p><span>Organisers also frequently mention access to EA speakers as a significant resource. Either groups struggle to find speakers, or they are based in cities or Universities with a good concentration of speakers and reflect on this as having been instrumental in their success.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Another thing I&#x2019;d love to be able to do is find guest speakers from around the world and help pay for them to get here, so they could give guest lectures.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;For the launch event we really didn&#x2019;t want to do it ourselves. We really wanted an external speaker who could attract a few more people because if we just do it, it&#x2019;s going to be our friends there. And so we asked them (a regional EA organisation) if any of them could come and give a talk and they said &#x2018;no we don&#x2019;t have any time in those months&#x2019;.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I guess maybe for prominent EA speakers. People like Peter Singer and whoever. </span></p>\n<p><strong><br><br></strong></p>\n<p><span>&#x201C;Also just knowing which EAs are where. Like when EAs are visiting a city&#x2026; I can imagine a really awesome EA coming to town and you just not knowing about it.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;Is there any information on what speakers are around in what areas? It&#x2019;s hard to keep tabs on certain people, and if they&#x2019;re coming through to [City] it would be good to say &#x2018;Hey! While you&#x2019;re here, come to our group!&#x2019;&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<h3><span>Group Calls</span></h3>\n<p><span>A few organisers mentioned group calls as having been inspiring and helpful. Some organisers had participated in calls organised by LEAN or CEA, while others had arranged these calls independently. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;In general I think that being able to have Skype conversations, and using online tools has helped a lot. I can imagine that it multiplies.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I think it&#x2019;s very valuable to know the broader EA landscape. So I think that like&#x2026; both CEA and LEAN - you&#x2019;ve been cooperating on that. The talks, the phone calls or the Skype calls&#x2026; they&#x2019;re really valuable. Just talking to other people, that&#x2019;s not easy to do just as a stand-alone EA group.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I haven&#x2019;t had much direct contact. I would love to be active in any group calls if you share that with me.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>The reasons for valuing these calls were similar to those for valuing personal connection with the wider EA community. Although feedback for the calls was mostly positive, two organisers mentioned the fact that group calls had limited value for their needs given significant differences between the opportunities and challenges they were facing and those that the majority of call participants were discussing.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;They were kind of useful but there were so many different kinds of groups. How to help your group at the stage it&#x2019;s in? Sometimes I&#x2019;d have a chat and they were like &#x201C;Oh we&#x2019;re doing all these things&#x201D; and I was like &#x201C;We can&#x2019;t do those things&#x2026; we&#x2019;re not a University, we&#x2019;re just little&#x201D;...It was difficult.&#x201D;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>&#x201C;I&#x2019;m also aware that with a lot of groups&#x2026; with student groups and younger people, less experienced&#x2026; I understand that that&#x2019;s a priority. Um so no, I think it&#x2019;s clear for us that &#x2018;No, that&#x2019;s not for us. That&#x2019;s more for student groups, but that&#x2019;s ok&#x2019;.&#x201D;</span></p>\n<h3><span>Conclusion</span></h3>\n<p><span>Having summarised all of the empirical data from our quantitative and qualitative research, the next, the penultimate report will synthesise both sources in order to develop a thorough interpretation of these findings and their significance for the LEAN project, and for EA movement building generally.</span></p>\n<h3><span>Acknowledgements</span></h3>\n<p><span>This report was written by Richenda Herzig with editorial contributions by David Moss and Tee Barnett. Interview recruitment was overseen by David Vatousios. The interview sample was divided evenly between the interviewers (Richenda Herzig and David Vatousios). Interview recordings were transcribed by Richenda Herzig, David Vatousios, Brooke Jackson, Avi Iyer, Kaitlin Alcantara, Sharmin Tuli, Leticia Pena and Ashley Francis.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>We are highly grateful to Greg Lewis for his input as an external advisor.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>We&#x2019;d like to offer our deep gratitude to each EA organiser that agreed to participate in the interviews, both for the trust and generosity in sharing your thoughts and experiences, and the time sacrificed.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Endnotes</span></p>\n<p><span>[1] The impact assessment utilises a mixed method social research strategy, including both quantitative and qualitative components. In this report, we present transcribed interview data with descriptive commentary only, leaving the full strategic analysis for a later article. Report #4 will describe our methodology in full. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>To briefly recap, we selected a large sample of EA groups based on length of time running, and a desire to include a mix of different nationalities, different group types (University and local), and different affiliations (e.g. groups mostly served by CEA, LEAN, EAF or TLYCS). Of this sample, we interviewed each organiser that was willing to participate, which whittled our selection down to 31 organisers (from around 70). </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Our interviews were conducted over Skype with the exception of one which took place face to face in Vancouver. An audio recording of interviews was taken on our mobile phones in the early stages. Eventually, we invested in commercial Skype recording software, which produced full video recordings. The first two interviews were jointly conducted by David Vatousios and Richenda Herzig for training and calibration purposes. Thereafter the interviews were loosely split between each interviewer for convenience (interviews took place across a range of different time zones, which meant that personal scheduling determined which interviewer was available for any given interview) and in order to improve our objectivity. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Recordings were transcribed by Richenda Herzig, David Vatousios and a team of four remote LEAN volunteers (who each signed non-disclosure agreements). </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>In summarising findings from this dataset, we paid attention to the frequency that a theme would be mentioned by respondents, the weighting respondents gave to these themes as expressed by length of time and volume of detail used in discussing the theme, as well as their explicit evaluative comments. Pragmatic and clarity considerations also affected which extracts could be used. For example, there was considerable overlap in terms of the themes that would be covered in data units as small as one or two sentences. This meant that the number of quotes where an issue was clearly isolated was small. For less isolated quotes, much larger extracts would have been necessary to include in order to render the extract clear. Finally, inconsistency in the quality of transcription meant that some data could not be appropriately formatted and included within our time frame.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>[2] Each example included has been anonymised in order to protect the privacy of respondents and their groups. Where personal identifiers could not be removed, pseudonyms were used. Where changes were made to quotes, either for anonymity or for clarification, square brackets were used.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>[3] We assume the same is true of CEA and EAF.</span></p></body></html>", "user": {"username": "Richenda"}}, {"_id": "ocD2KeKRHb53J8ZJF", "title": "Economics, prioritisation, and pro-rich bias \u00a0", "postedAt": "2018-01-02T22:33:36.355Z", "htmlBody": "<html><body><h1>&#xA0;</h1>\n<p><span>tl;dr: Welfare economics is highly relevant to effective altruism, but tends to rely on a flawed conception of social welfare, which holds that the more someone is willing to pay for a good, the more utility or welfare they would get from consuming that good. (I use &#x2018;welfare&#x2019; and &#x2018;utility&#x2019; interchangeably here). This neglects the fact that differences in willingness to pay are often merely due to differences in initial resource endowments. As a consequence, welfare economics is biased towards policies that favour the rich. Effective altruists should be aware of these problems, and economists should adopt a revised conception of social welfare. </span></p>\n<p>&#xA0;</p>\n<p>cross-posted from my <a href=\"http://johnhalstead.org/index.php/2018/01/02/economics-prioritisation-pro-rich-bias/\">blog</a>.&#xA0;</p>\n<p><span>&#xA0;*</span></p>\n<p><span>Effective altruism is the use of reason and evidence to promote the welfare of all as effectively as possible. Welfare economics is highly relevant to effective altruism because it aims to show which policies or actions would best maximise social welfare. The modern discipline of economics was heavily influenced by early utilitarian thought, and economics has influenced effective altruism in numerous ways with tools such as cost-effectiveness-analysis and Disability Adjusted Life Years. Welfare economics is, in my view, the most useful and practically applicable prioritisation tool currently available to governments. However, as I will now argue, mainstream welfare economics relies on a flawed theory of social welfare, which leads to pro-rich bias in policy evaluation. </span></p>\n<p><span>I hope this post will improve understanding of welfare economics among effective altruists. It would also be useful for economists to recognise these problems and take a revised approach. </span></p>\n<p><span>Touting and social welfare</span></p>\n<p><span>I will bring out this issue by discussing the question of ticket &#x2018;touting&#x2019; or &#x2018;scalping&#x2019;. Economists are somewhat unusual in believing that touting is actually a good thing because it corrects for underpriced tickets. Here is <a href=\"https://www.economist.com/news/britain/21705288-curse-market-forces-makes-ticket-reselling-tough-manage-muggleu2019s-game\"><em>The Economist</em></a> on the issue:</span></p>\n<p><span>&#x201C;Flint-hearted economists might note that a secondary market suggests that the seats were underpriced. Cheaper tickets meant to boost equal access lure in touts, for whom low prices mean bigger premiums. And more scalpers means more disappointed fans in the queue.</span></p>\n<p><span>Rather than allowing touts to profit, the play&#x2019;s producers could take a cue from &#x201C;Hamilton&#x201D;, a wildly successful Broadway musical, and raise prices for the premium seats until demand falls in line with supply (even at up to $849 per ticket, some argue that &#x201C;Hamilton&#x201D; is too cheap). But the Potter producers seem to be more worried about impecunious wizarding fans losing out than about the prospect of touts swiping surplus.</span></p>\n<p><em><span>Stamping out the secondary market entirely means preventing people selling their tickets to those who value them more</span></em><span>. This inefficiency is wince-inducing for economists...&#x201D; [emphasis added]</span></p>\n<p><span>According to some economists, ticket touting improves <a href=\"https://www.economicshelp.org/blog/glossary/allocative-efficiency/\"><em>allocative efficiency</em></a>. </span></p>\n<p><em><span>Allocative efficiency </span></em><span>occurs when there is an optimal distribution of goods according to consumer preferences, or, in other words, when social welfare is maximised. </span></p>\n<p><span>The argument goes as follows. By selling tickets at a single price on a first come first served basis, some people who really want to go to the show will be unable to go. When the ticket is underpriced, Pete, who is willing to pay no more than $50 for a <em>Book of Mormon</em> ticket, can get a ticket, but Rich, who is willing to pay up to $1000, doesn&#x2019;t get a ticket. </span></p>\n<p><em><span>Crucial Premise</span></em><span>: Necessarily, the more someone is willing to pay for a good, the more welfare they get from consuming that good.</span></p>\n<p><span><span><span>[UPDATE: I actually mean this more precise version of the premise. &quot;Necessarily, if person A is willing to pay more for a good than person B, then person A gets more welfare from that good than person B. Thanks to rohinmshah]</span></span></span></p>\n<p><span>So, by meeting the market demand of those willing to pay more or, in other words, ensuring that price is closer to marginal utility, touts ensure that social welfare is maximised. </span></p>\n<p><span>The vast majority (&gt;68%) of economists believe touting increases social welfare, as shown by <a href=\"http://www.igmchicago.org/surveys/ticket-resale\">this</a> IGM poll (a good place to find the views of economists on lots of different topics). It&#x2019;s somewhat unclear whether they do so on the basis of the argument from allocative efficiency and the Crucial Premise, but I would bet that a significant portion do endorse that argument. </span></p>\n<p><span>What&#x2019;s wrong with this argument?</span></p>\n<p><span>I&#x2019;m going to argue that the foregoing argument fails because the Crucial Premise is false. (Note that touting might be justified by other arguments).</span></p>\n<p><span>I&#x2019;ll first clarify the assumptions made in the argument.</span></p>\n<p><em><span>Utilitarianism</span></em><span> = Agents ought to perform the act which maximises total social utility or welfare.&#xA0; </span></p>\n<p><span>A large portion of economists accept <em>preference utilitarianism</em>, according to which utility is conceived of as preference satisfaction. When evaluating policy, many economists like to say that they put morality to one side, but this is seldom true. In actual fact, they are appealing to preference utilitarianism. This is a moral theory.</span></p>\n<p><span>Some economists believe that allocatively efficient outcomes might involve large inequalities and therefore be unfair. Consequently, they endorse an equity or fairness constraint on preference utilitarianism. In philosophical terms, this is equivalent to preference utilitarianism with a welfare egalitarian constraint. Proponents of such a theory tend to recommend that governments correct inequality through redistribution. </span></p>\n<p><span>The pro-touting argument combines preference utilitarianism and the Crucial Premise, concluding that touting is justified because it maximises social welfare. </span></p>\n<p><span>With this clarified, we can now explore why the pro-touting argument does not work. The Crucial Premise is false. It is not necessarily true that willingness to pay for a good is an indicator of how much utility one would get from a good. This is obvious. For example, suppose that Pete is very poor and Rich is very rich. As a consequence, Pete willing to pay up to $50 for a <em>Book of Mormon</em> ticket, but Rich is willing to pay up to $1,000. But this does not necessarily mean that Rich would get more utility from watching the <em>Book of Mormon</em> than Pete. All it shows is that Pete doesn&#x2019;t have as much money. It might be the case that Rich would mildly enjoy the show, but Pete would absolutely love it. </span></p>\n<p><span>Indeed, imagine that Pete has no money at all. According to the view that, necessarily, the more one is willing to pay for a good the more utility one derives from it, Pete would not gain utility from the consumption of any good, even food or water. This is absurd. </span></p>\n<p><span>We can avoid this by correcting for inequality in income or resources between individuals when assessing willingness to pay. We could, for example, ask what Pete would be willing to pay for a ticket if he had as much money as Rich. Thus, <em>hypothetical</em>, rather than actual, willingness to pay would determine consumer preference. Consumer preference would not be revealed by actual market demand. If so, then it is not necessarily true that touting tickets at higher prices increases social welfare by allocating tickets to those who would get most utility from them. </span></p>\n<p><span>Not only is it not <em>necessarily </em>true that actual willingness to pay determines consumer preference, it is not even <em>usually</em> true. Differences in willingness to pay are to a significant extent and in a huge range of cases driven by differences in personal wealth rather than by differences in consumer preference. Rich people tend to holiday in exotic and sunny places at much higher rates than poor people. This is <em>entirely</em> a product of the fact that rich people have more money, not that poor people prefer to holiday in Blackpool. I think the same holds for the vast majority of differences in market demand across different income groups. </span></p>\n<p><span>In sum, the argument for touting from preference utilitarianism and the Crucial Premise fails.</span></p>\n<p><span>Implications for welfare economics</span></p>\n<p><span>This is one instance of a serious general problem for contemporary welfare economics. Equating market demand and utility without correcting for inequality in income or resources leads economists to pro-rich bias. It is this same flaw that led the 1995 IPCC report to conclude, on the basis of a willingness to pay approach, that Indian lives were worth less than American lives.<a title=\"\" href=\"file:///C:/Users/Admin/Google%20Drive/EA%20blogs/Economics,%20markets%20and%20the%20rich.docx#_ftn1\"><span><span><!-- [if !supportFootnotes]--><span><span>[1]</span></span><!--[endif]--></span></span></a> </span></p>\n<p><span>It is easy to see how this bias could come into play for pretty much all policies assessed by welfare economics. Economists will neglect inequality and tend to recommend that goods be distributed by market prices. </span></p>\n<p><span>This is <em>not</em> a criticism of preference utilitarianism from <em>equity</em> or <em>fairness</em>. I am not saying that only aiming to maximise social welfare is inegalitarian, and I am not saying that equality is intrinsically valuable. I am saying that preference utilitarianism alone, properly conceived and without an equity constraint, favours more egalitarian outcomes than economists acknowledge. </span></p>\n<p><span>One advantage of holding that actual willingness to pay determines preference is that it is easier to measure than hypothetical willingness to pay. For this reason, in some cases it may be more practicable to approximate preference utilitarianism (properly conceived) with the Crucial Premise + an independent equity constraint. This equity constraint would be justified on utilitarian grounds, rather than on the grounds that equality is intrinsically important. &#xA0;</span></p>\n<p><span>The downside of this is that economists would still be giving an inaccurate account of what constitutes preference satisfaction. The statement &#x201C;touting optimises the distribution of goods according to consumer preference, but is inequitable&#x201D; is false because the first conjunct is false. </span></p>\n<p>&#xA0;</p>\n<p>Many thanks to Stefan Schubert for always helpful comments.</p>\n<p><span>&#xA0;</span></p>\n<p>&#xA0;</p>\n<div><!-- [if !supportFootnotes]--><br><hr><!--[endif]-->\n<div>\n<p><a title=\"\" href=\"file:///C:/Users/Admin/Google%20Drive/EA%20blogs/Economics,%20markets%20and%20the%20rich.docx#_ftnref1\"><span><span><!-- [if !supportFootnotes]--><span><span>[1]</span></span><!--[endif]--></span></span></a> The great John Broome discusses this on p.15 here - <a href=\"http://users.ox.ac.uk/~sfop0060/pdf/Valuing%20policies%20in%20response%20to%20climate%20change,%20some%20ethical%20issues.pdf\">http://users.ox.ac.uk/~sfop0060/pdf/Valuing%20policies%20in%20response%20to%20climate%20change,%20some%20ethical%20issues.pdf</a></p>\n</div>\n</div>\n<h1>&#xA0;</h1>\n<p>&#xA0;</p></body></html>", "user": null}, {"_id": "ga5qCreTmruCkoL4F", "title": "Cosmic EA: How Cost Effective Is Informing ET?", "postedAt": "2017-12-31T08:37:10.422Z", "htmlBody": "<html><body><p>A number of people have raised about intentionally trying to make contact with extraterrestrials. Most famously, Stephen Hawking famously <a href=\"https://www.sciencealert.com/stephen-hawking-warns-that-we-might-not-want-to-reach-out-to-aliens\">warned</a> that based on the history of first-contacts on Earth we should fear enslavement, exploitation or annihilation by more advanced aliens and the METI proposal to beam high powered signals into space has drawn <a href=\"https://www.nytimes.com/2017/06/28/magazine/greetings-et-please-dont-murder-us.html?mtrref=www.google.co.il\">controversy</a> as well as <a href=\"http://www.davidbrin.com/nonfiction/shouldsetitransmit.html\">criticism from David Brin</a> for METI&apos;s failure to engage in consultation with a broad range of experts.&#xA0; However, I&apos;ve noticed a distinct lack of consideration of the potential benefits <em>to alien life</em> as a result of such contact.</p>\n<p>For instance, while the proposal to <a href=\"https://www.forbes.com/2008/02/21/space-seti-aliens-language_sp08-cx_de_1024aliens.html#12c15b0565d0\">send the google servers</a> might limit our ability to trade in the future it also potentially provides the aliens with whatever benefits they might get from our scientific insights or our historical experiences. For instance, if we were to receive a detailed account of alien society&apos;s struggle with climate change on their planet that second piece of data could be invaluable in choosing our own course not to mention the benefit scientific advancements could offer.</p>\n<p>Indeed, if, as many people seem to think, there is some extinction level disaster waiting for civilizations once they reach, or slightly surpass, our current level of technology then such preemptive broadcasts might be the only serious hope of getting at least one sapient species through this <a href=\"https://en.wikipedia.org/wiki/Great_Filter\">Great Filter</a>. While it might be pretty unlikely that our transmission would start the chain of records from doomed civilizations that will eventually push one species past the filter the returns to utility from such an outcome are so massive that such considerations might well outweigh any effect on humanity in the utility calculus.<br><br>Anyway, given the huge potential upside (even if unlikely) of an intervention which might improve life across the entire galaxy (even if at very low probability) I was wondering if anyone has done even back of the envelope calculations to estimate how funding projects trying to transmit useful data to extraterrestrials compares to the cost effectiveness of more earthly projects.<br><br>Yes, I know any calculation will have to make lots of assumptions but if it would be very informative if it turns out that the math only works out to make it cost-effective if we assume our information is incredibly valuable or if it turns out that even a very very small change of helping an alien species avoid a Great Filter and spread across the galaxy makes it cost effective.</p>\n<p>&#xA0;</p>\n<hr>\n<p>Cross <a href=\"https://rejectingrationality.org/blog/cosmic-effective-altruism/\">posted</a> at my blog: <a title=\"Rejecting Rationality\" href=\"https://rejectingrationality.org/\">Rejecting Rationality</a> (doesn&apos;t mean what you think it does)</p></body></html>", "user": {"username": "TruePath"}}, {"_id": "YKTKsZB8j5gXNsNB6", "title": "Where can I donate to support insect welfare?", "postedAt": "2017-12-31T01:40:17.518Z", "htmlBody": "<html><body><p>As the title says.</p>\n<p>&#xA0;</p>\n<p>This feels undervalued to me and I&apos;d like to donate to support it.</p>\n<p>&#xA0;</p>\n<p>Do you know of any good charities/non-profits/etc to donate for insect welfare?</p></body></html>", "user": {"username": "nonzerosum"}}, {"_id": "Q8isNAMsFxny5N37Y", "title": "How tractable is cluelessness?", "postedAt": "2017-12-29T18:52:56.369Z", "htmlBody": "<p>This is the third in a series of posts exploring <a href=\"https://flightfromperfection.com/cluelessness-what-to-do.html\">consequentialist cluelessness</a> and its implications for effective altruism:</p><ul><li>The <a href=\"https://forum.effectivealtruism.org/ea/1hh/what_consequences/\">first post</a> describes cluelessness &amp; its relevance to EA; arguing that for many popular EA interventions we don\u2019t have a clue about the intervention\u2019s overall net impact.</li><li>The <a href=\"https://forum.effectivealtruism.org/ea/1ix/just_take_the_expected_value_a_possible_reply_to/\">second post</a> considers a potential reply to concerns about cluelessness.</li><li><strong>This post</strong> examines how tractable cluelessness is \u2013 to what extent we can grow more clueful about an intervention through intentional effort?</li><li>The <a href=\"https://forum.effectivealtruism.org/ea/1kv/doing_good_while_clueless/\">fourth post</a>\u00a0discusses what being clueless implies about doing good.</li></ul><p></p><p>Consider reading the <a href=\"https://forum.effectivealtruism.org/ea/1hh/what_consequences/\">first</a> and <a href=\"https://forum.effectivealtruism.org/ea/1ix/just_take_the_expected_value_a_possible_reply_to/\">second</a> posts first.</p><hr class=\"dividerBlock\"/><p>Let&#x27;s consider the\u00a0<a href=\"https://concepts.effectivealtruism.org/concepts/importance-neglectedness-tractability/\">tractability</a> of cluelessness in two parts:</p><ol><li>How clueful do we need to be before deciding on a course of action? (i.e. how much effort should we spend contemplating &amp; exploring before committing resources to an intervention?)</li><li>How clueful can we become by contemplation &amp; exploration?</li></ol><p></p><h2>How clueful do we need to be before deciding on a course of action?</h2><p>In his talk <a href=\"http://www.stafforini.com/blog/bostrom/\">Crucial Considerations and Wise Philanthropy</a>, Nick Bostrom defines a crucial consideration as \u201ca consideration such that if it were taken into account it would overturn the conclusions we would otherwise reach about how we should direct our efforts, or an idea or argument that might possibly reveal the need not just for some minor course adjustment in our practical endeavors but a major change of direction or priority.\u201d</p><p>A plausible reply to \u201chow clueful do we need to be before deciding on a course of action?\u201d might be: \u201cas clueful as is needed to uncover all the crucial considerations relevant to the decision.\u201d</p><p>Deciding to act before uncovering all the crucial considerations relevant to the decision is potentially disastrous, as even one unknown crucial consideration could bear on the consequences of the decision in a way that would entirely revise the moral calculus.</p><p>In contrast, deciding to act before uncovering all non-crucial (\u201cnormal\u201d) considerations is by definition not disastrous, as unknown normal considerations might imply a minor course adjustment but not a radically different direction.</p><p></p><h2>How clueful can we become by contemplation &amp; exploration?</h2><p>Under this framing, our second tractability question can be rephrased as \u201cby contemplation and exploration, can we uncover all the crucial considerations relevant to a decision?\u201d</p><p>For cases where the answer is \u201cyes\u201d, we can become clueful enough to make a good decision \u2013 we can uncover and consider everything that would necessitate a radical change of direction.</p><p>Conversely, in cases where the answer is \u201cno\u201d, we can\u2019t become clueful enough to make a good decision \u2013 despite our efforts there will remain unknown considerations that, if known, would radically change our decision-making.</p><p>There is a difference here between long-run consequences and indirect consequences (see definitions in <a href=\"https://forum.effectivealtruism.org/ea/1hh/what_consequences/\">the first post</a>). By careful investigation, we can uncover more &amp; more of the indirect, temporally near consequences of an intervention. It\u2019s plausible that for many interventions, we could uncover all the indirect consequences that relate to the intervention\u2019s crucial considerations.</p><p>But we probably can\u2019t uncover most of the long-run consequences of an intervention by investigation. We can improve our forecasting ability, but because of the complexity of reality, the fidelity of real-world forecasts declines as they extend into the future. It seems unlikely that our forecasting will be able to generate believable predictions of impacts more than 30 years out anytime soon.</p><p>Because many of the consequences of an intervention unfold on a long time horizon (one that\u2019s much longer than our forecasting horizon), it\u2019s implausible to uncover all the long-run consequences that relate to the intervention\u2019s crucial considerations.</p><p></p><h2>Ethical precautionary principle</h2><p>Then, for any decision whose consequences are distributed over a long time horizon (i.e. most decisions), it\u2019s difficult to be sure that we are operating in the \u201cyes we can become clueful enough\u201d category. More precisely, we can only become sufficiently clueful for decisions where there are no unknown crucial considerations that lie past our forecasting horizon.</p><p>Due to\u00a0<a href=\"https://nickbostrom.com/astronomical/waste.html\">the vast size of the future</a>, even a small probability of an unknown, temporally distant crucial consideration should give us pause.</p><p>I think this implies operating under an <em>ethical precautionary principle:</em> acting as if there were always an unknown crucial consideration that would strongly affect our decision-making, if only we knew it (i.e. always acting as if we are in the \u201cno we can\u2019t become clueful enough\u201d category).</p><p>Does always following this precautionary principle imply <a href=\"https://en.wikipedia.org/wiki/Analysis_paralysis\">analysis paralysis</a>, such that we never take any action at all? I don\u2019t think so. We find ourselves in the middle of a process that\u2019s underway, and devoting all of our resources to analysis &amp; contemplation is itself a decision (\u201c<a href=\"https://genius.com/Rush-freewill-lyrics\">If you choose not to decide, you still have made a choice</a>\u201d).</p><p>Instead of paralyzing us, I think the ethical precautionary principle implies that we should focus our efforts in some areas and avoid others. I\u2019ll explore this further in the <a href=\"https://forum.effectivealtruism.org/ea/1kv/doing_good_while_clueless/\">next post</a>.</p>", "user": {"username": "Milan_Griffes"}}, {"_id": "63XdGhoCDpJpXsadZ", "title": "Viewing Effective Altruism as a System", "postedAt": "2017-12-28T10:09:43.004Z", "htmlBody": "<html><body><p>Meta-EA is most often characterised in terms of discrete units such as dollars and individuals. How many people can we recruit, how much will they donate, how many people can we train to be AI researchers? This approach carries a lot of value, particularly when we wish to craft metrics to evaluate our work. At the same time, sometimes it is better to view Effective Altruism as a system, to look at it holistically.</p>\n<p>I believe that the primary goal of meta-EA should be achieving impact through improving EA as a system. Our starting place should be different theories of how we could do this and metrics should come second, as a way of differentiating between different plans of action and testing hypotheses. I&#x2019;m not suggesting that quantitive facts should be ignored during the hypothesis generation stage, just that we need to understand the hypothesis space before we can choose appropriate metrics, otherwise we may artificially limit the set of theories that we consider.</p>\n<p>In particular, we need to recognise that sometimes a system is more than the sum of its parts. Effective Altruism is one such system, since the various&#xA0;parts of the movement tend to make the other parts work more effective.<span>&#xA0; </span>This article will give a brief summary of how effective altruism works as a system. Please note that this discussion will not just include official EA orgs, but some EA aligned orgs as well.&#xA0;</p>\n<p><strong>The Effective Altruism Eco-system</strong>:</p>\n<p>This section divides up the various parts of the EA eco-system by function. You may want to skim over this section if you already have a good understanding of the eco-system, as otherwise you&#x2019;ll just be reading things that you already know.</p>\n<p>Center for Effective Altruism (CEA)/Local Effective Altruism Network (LEAN): Focuses on movement building and guiding the EA movement generally, including writing articles and sending out the newsletter.</p>\n<p>Open Philanthropy/Giving What We Can Pledge/Founder&apos;s Pledge/Effective Altruism Funds/Raising for Effective Giving: Provides funding for the causes we support, as well as for the various other orgs here as well. CEA: Funds local groups. Effective Altruism Funds: Provides funding for smaller projects. Givewell Incubation Grants: Supports potential new top charities.&#xA0;</p>\n<p>Effective Altruism Global/EAGx: Spreads ideas within the EA movement and provides networking opportunities.</p>\n<p>Less Wrong/Center for Applied Rationality/Broader rationalsphere: Provides tools for thinking more clearly (epistemic rationality) and for being more effective (applied rationality).</p>\n<p>Local EA groups/SHIC: Recruits people into the movement who donate or who join orgs, as well as developing them as EAs and often providing a social group. In particular, local groups are present at many of the world&apos;s most prestigious universities, including Oxford, Cambridge, Stanford, Yale, Harvard, Princeton, MIT.</p>\n<p>EA Bay Area Hub: Big enough to deserve it&apos;s own point. Connects us with/helps us recruit from the tech scene. Brings enough EAs together in the one place that it is likely that people can find other EAs also interested in the same thing.</p>\n<p>80,000 hours: Provides career advice, as well as helping effective orgs fill vacancies.</p>\n<p>EA Forum/various Facebook groups: Allow the sharing of ideas globally</p>\n<p>Global Priorities Institute: New&#xA0;research institute at Oxford broadly examining EA. Does not just perform research, also provides academic credibility. There are a whole of research institutes for specific causes such as Future of Humanity Institute, Center for the Study of Existential Risk, Foundational Research Institute, Wild Animal Suffering Research, ect.</p>\n<p>Givewell/Open Philanthropy Project/Animal Charity Evaluators: Charity evaluators for different causes.&#xA0;</p>\n<p>Charity Science: Research into potential new top charities and assists people who want to create them.&#xA0;</p>\n<p><strong>Interaction Effects</strong>:</p>\n<p>We can see several ways in which the existence of a broader eco-system makes certain tasks much more worthwhile. For example, suppose you see an idea for an effective charity on Charity Science. You contact them and they provide you with advice and link you up with potential cofounders. Givewell provides you with an incubation grant, which you use to hire some staff who&#xA0;were referred through 80,000 hours so that you can run a pilot. Givewell evaluates you and you become a top charity. Various Giving What We Can members donate to you and OpenPhil provides you with significant support. Given the inherent difficulties of charity entrepreneurship, it would plausibly only take a single part of this pipeline to be missing in order to derail the whole project and make all the other efforts worthless.</p>\n<p>There are many other interaction effects as well. For example, it is much more valuable for the Global Priorities Institute to do research if there is a global movement that will try to put the ideas in action. The Founder&apos;s pledge is much more valuable with Give Well and Open Philanthropy existing, since they provide it with research which it can pass on to founders to help them give more effectively. Further 80,000 hours is much more effective when there are meet ups at top universities to refer people for coaching.</p>\n<p><strong>Application:</strong>&#xA0;</p>\n<p>The main purpose of this post is to encourage more people to adopt a more holistic way of looking at Effective Altruism that may lead to further ideas of worthwhile projects. Nonetheless, I do want to make a few suggestions about application:&#xA0;</p>\n<ul>\n<li>Once you have a map of the EA ecosystem (as above), you can start thinking of different pipelines: becoming an AI researcher, starting a new charity, obtaining a financial job for earning to give. You can look for gaps in the pipeline and consider whether the gap might be worth filling or whether the cure is worse than the disease.</li>\n<li>One of the greatest difficulties is figuring out how we should handle co-ordination within the movement. If we just examine our marginal impact based on the status quo remaining the same, we will be ignoring any improvements in the efficiencies of other components or the effects of new components being added to the system. In particular, some components may be incredibly valuable if all of them exist, but have minimal value on their own. For example, Givewell can increase donor&apos;s effectiveness by a factor of ten, but in a world where either nobody had heard of them or nobody listened to them, this component would not be valuable by itself. This is not an easy problem and I don&#x2019;t really know how to address this, but it is plausible that all of the highest impacts come from combinations of components which each increase their effectiveness.</li>\n</ul></body></html>", "user": {"username": "casebash"}}, {"_id": "RBTWyi5MW3LyeX23L", "title": "80,000 Hours annual review released", "postedAt": "2017-12-27T20:31:05.395Z", "htmlBody": "<html><body><p>Hi everyone,</p>\n<p>The full review is <a href=\"https://80000hours.org/2017/12/annual-review/\">here</a>.</p>\n<p>Below is the summary:</p>\n<p>&#xA0;----</p>\n<p>This year, we focused on &#x201C;upgrading&#x201D; &#x2013; getting engaged readers into our top priority career paths.</p>\n<p>We do this by writing articles on why and how to enter the&#xA0;<a href=\"https://80000hours.org/coaching/#priority-paths\">priority paths</a>, providing one-on-one advice to help the most engaged readers narrow down, and introductions to help them enter.</p>\n<p>Some of our main successes this year include:</p>\n<ol>\n<li>We developed and refined this upgrading process, having been focused on introductory content last year. We made lots of improvements to coaching, and released 48 pieces of content.&#xA0;<br><br></li>\n<li>We used the process to grow the number of rated-10 plan changes 2.6-fold compared to 2016, from 19 to 50. We primarily placed people in AI technical safety, other AI roles, effective altruism non-profits, earning to give and biorisk.<br><br></li>\n<li>We started tracking rated-100 and rated-1000 plan changes. We recorded 10 rated-100 and one rated-1000 plan change, meaning that with the new metric, total new impact-adjusted significant plan changes (IASPC v2) doubled compared to 2016, from roughly 1200 to 2400. That means we&#x2019;ve grown the annual rate of plan changes 23-fold since 2013. (If we ignore the rated-100+ category, then IASPCv1 grew 31% from 2017 to 2016, and 12-fold since 2013.)<br><br></li>\n<li>This meant that despite rising costs, cost per IASPC was flat. We updated our historical and marginal cost-effectiveness estimates, and think we&#x2019;ve likely been highly cost-effective, though we have a lot of uncertainty.<br><br></li>\n<li>We maintained a good financial position, hired three great full-time core staff (Brenton Mayer as co-head of coaching; Peter Hartree came back as technical lead; and Niel Bowerman started on AI policy), and started training several managers.</li>\n</ol>\n<p>Some challenges include: (i) people misunderstand our views on career capital so are picking options we don&#x2019;t always agree with (ii) we haven&#x2019;t made progress on team diversity since 2014 (iii) we had to abandon our target to triple IASPC (iv) rated-1 plan changes from introductory content didn&#x2019;t grow as we stopped focusing on them.</p>\n<p>Over the next year, we intend to keep improving this upgrading process, with the aim of recording at least another 2200 IASPC. We think we can continue to grow our audience by releasing more content (it has grown 80% p.a. the last two years), getting better at spotting who from our audience to coach, and offering more value to each person we coach (e.g. doing more headhunting, adding a fellowship). By doing all of this, we can likely grow the impact of our upgrading process at least several-fold, and then we could scale it further by hiring more coaches.</p>\n<p>We&#x2019;ll continue to make AI technical safety and EA non-profits a key focus, but we also want to expand more into other AI roles, other policy roles relevant to extinction risk, and biorisk.</p>\n<p>Looking forward, we think 80,000 Hours can become at least another 10-times bigger, and make a major contribution to getting more great people working on the world&#x2019;s most pressing problems.</p>\n<p>We&#x2019;d like to raise $1.02m this year. We expect 33-50% to be covered by the Open Philanthropy Project, and are looking for others to match the remainder. If you&#x2019;re interested in donating, the easiest way is through&#xA0;<a href=\"https://app.effectivealtruism.org/donations/new?utm_source=80000-hours&amp;utm_medium=partner_charity&amp;utm_campaign=partner_charity_donations&amp;allocation[80000-hours]=100\">the EA Funds</a>.</p>\n<p>If you&#x2019;re interested in making a large donation and have questions, please contact&#xA0;<a href=\"mailto:ben@80000hours.org\">ben@80000hours.org</a>.</p>\n<p>If you&#x2019;d like to follow our progress during the year, subscribe to&#xA0;<a href=\"https://groups.google.com/forum/#!forum/80k_updates\">80,000 Hours updates</a>.</p></body></html>", "user": {"username": "Benjamin_Todd"}}, {"_id": "9x7Ya4SJeMYhCfhvs", "title": "Donation Plans for 2017", "postedAt": "2017-12-23T22:25:49.690Z", "htmlBody": "<html><body><p>Each year <a href=\"http://www.givinggladly.com/\">Julia</a> and I need to decide where we&apos;re giving. Here&apos;s what we&apos;ve been thinking about this year:</p>\n<ul>\n<li>\n<p>We want to continue dividing our donations 50-50 between things that directly do good and more speculative options. This is the approach we&apos;ve been following since ~2012, and I think I first discussed this in my <a href=\"https://www.jefftk.com/p/why-global-poverty\">2015 EA Global talk</a>.</p>\n</li>\n<li>\n<p>We&apos;re planning to continue donating 50%. In January, I <a href=\"https://www.jefftk.com/p/leaving-google-joining-wave\">had written</a> that because I was switching from earning to give to direct work we&apos;d be targeting 30%, but when I <a href=\"https://www.jefftk.com/p/rejoining-google\">switched back</a> we decided to switch our target back as well. [1] As in the past few years I worked with an accountant to estimate our <a href=\"https://www.jefftk.com/p/what-should-income-mean-in-pledging\">adjusted gross income</a> so we would know how much 50% translates to.</p>\n</li>\n<li>\n<p>For things that directly do good we&apos;re planning to continue to follow <a href=\"https://blog.givewell.org/2017/11/27/our-top-charities-for-giving-season-2017/\">GiveWell&apos;s recommendations</a>. This year their preference is for people to donate to GiveWell for allocation at their discretion, but I wanted to participate in <a href=\"https://www.jefftk.com/p/participating-in-donation-matching\">donation matching at work</a> and for that we needed to pick a charity to fundraise for. For people who want to donate directly to charities GiveWell is recommending 70% to the <a href=\"https://www.givewell.org/charities/amf\">Against Malaria Foundation</a> and 30% to the <a href=\"https://www.givewell.org/charities/Schistosomiasis-Control-Initiative\">Schistosomiasis Control Initiative</a>. Because we had already donated $6k to the AMF in January [2], however, and because we weren&apos;t sure what our donation total would be at the time I signed up for matching, we ended up splitting 79% AMF and 21% SCI. [3]</p>\n<p>For the Against Malaria Foundation we donated <a href=\"https://www.jefftk.com/p/paypal-giving-fund\">through the PayPal Giving Fund</a>. When you donate this way PayPal covers credit card transaction fees and also matches <a href=\"https://projects.promotw.com/rules/paypaldonationmatch.html\">an additional 1%</a>. This let us use a 2% cash-back credit card, saving $1k on $50k of donations, and gave the AMF an extra $0.5k. None of the other places we wanted to give to this year are covered, and the reasons pushing us toward the other opportunities were more important than the extra 3%, so everything else was by check.</p>\n</li>\n<li>\n<p>For more speculative things, we want to put part of the money towards a project that a friend we know through the Effective Altruism movement is starting. In general I think this is a good way for people to get funding for early stage projects, presenting their case to people who know them and have a good sense of how to evaluate their plans. [4]</p>\n<p>This project isn&apos;t set up as a 501(c)3, so donations to it aren&apos;t tax deductible. We&apos;re currently talking to CEA to see if we can donate to this project through them, but they may decide it&apos;s not something they can do. Because Julia works there, we want to be careful not to unduly influence the decision, so we&apos;re trying to approach this clearly as community members rather than as staff. If it&apos;s not possible to do this through a grant by a nonprofit, we would fund the project directly but we&apos;re currently talking to <a href=\"https://www.centreforeffectivealtruism.org/\">CEA</a> to see if we can donate through them. There&apos;s a good chance this will end up being an early 2018 donation instead of a 2017 one.</p>\n<p>For the remainder of the speculative portion we&apos;re planning to donate to the new <a href=\"https://app.effectivealtruism.org/funds/ea-community\">EA Community Fund</a>. Last year we donated to Nick Beckstead&apos;s <a href=\"https://blog.givewell.org/2016/12/09/staff-members-personal-donations-giving-season-2016/\">EA Giving Group</a> donor-advised fund, and the EA Community Fund is a more formal continuation of that.</p>\n</li>\n</ul>\n<p>I&apos;ve also enjoyed reading the posts by <a href=\"https://blog.givewell.org/2017/12/11/staff-members-personal-donations-for-giving-season-2017/\">GiveWell</a>, <a href=\"https://www.openphilanthropy.org/blog/staff-members-personal-donations-giving-season-2017\">Open Phil</a> staff about where they&apos;re giving and why.</p>\n<p><br>[1] This does mean borrowing some money. Modeling our cash flow I think we&apos;ll have it paid back in March or April. Much of my current compensation is in stock, and I won&apos;t start getting that until I&apos;ll have been back at Google for a year, which will be this September.</p>\n<p>[2] Before I left Google, in January 2017, I wanted to max out their donation match for the year.</p>\n<p>[3] That our division favored AMF a bit more than GiveWell&apos;s shouldn&apos;t be taken as disagreement, and if anything we (especially Julia) lean a bit more towards towards SCI.</p>\n<p>[4] Another option for this sort of funding would be <a href=\"https://www.effectivealtruism.org/grants/\">EA Grants</a> if it used a rolling process.</p></body></html>", "user": {"username": "Jeff_Kaufman"}}, {"_id": "yRt583tWpA2cSdEkN", "title": "Updates from the Open Philanthropy Blog", "postedAt": "2017-12-22T01:05:32.368Z", "htmlBody": "<html><body><p>The Open Philanthropy Project has published two blog posts in the last days:<br><br><a href=\"https://www.openphilanthropy.org/blog/our-second-chance-program-nih-transformative-research-applicants\">Our &#x2018;Second Chance&#x2019; Program for NIH Transformative Research Applicants</a></p>\n<p><a href=\"https://www.openphilanthropy.org/blog/staff-members-personal-donations-giving-season-2017\">Staff Members&apos; Personal Donations for Giving Season 2017</a></p>\n<p>Crossposted by the forum admins.</p></body></html>", "user": {"username": "Crosspost"}}, {"_id": "MWquqEMMZ4WXCrsug", "title": "\u201cJust take the expected value\u201d \u2013 a possible reply to concerns about cluelessness", "postedAt": "2017-12-21T19:37:07.709Z", "htmlBody": "<p>This is the second in a series of posts exploring <a href=\"https://flightfromperfection.com/cluelessness-what-to-do.html\">consequentialist cluelessness</a> and its implications for effective altruism:</p><ul><li>The <a href=\"https://forum.effectivealtruism.org/ea/1hh/what_consequences/\">first post</a> describes cluelessness &amp; its relevance to EA; arguing that for many popular EA interventions we don\u2019t have a clue about the intervention\u2019s overall net impact.</li><li><strong>This post</strong> considers a potential reply to concerns about cluelessness \u2013 maybe when we are uncertain about a decision, we should just choose the option with the highest expected value.</li><li>Following posts discuss <a href=\"https://forum.effectivealtruism.org/ea/1j4/how_tractable_is_cluelessness/\">how tractable cluelessness is</a>, and what <a href=\"https://forum.effectivealtruism.org/ea/1kv/doing_good_while_clueless/\">being clueless implies about doing good</a>.</li></ul><p>Consider reading the <a href=\"https://forum.effectivealtruism.org/ea/1hh/what_consequences/\">first post</a> first.</p><hr class=\"dividerBlock\"/><p>A rationalist\u2019s reply to concerns about cluelessness could be as follows:</p><ul><li>Cluelessness is just a special case of empirical uncertainty.[1]</li><li>We have a framework for dealing with empirical uncertainty \u2013 <a href=\"https://en.wikipedia.org/wiki/Expected_value\">expected value</a>.</li><li>So for decisions where we are uncertain, we can determine the best course of action by multiplying our best-guess probability against our best-guess utility for each option, then choosing the option with the highest expected value.</li></ul><p>While this approach makes sense in the abstract, it doesn\u2019t work well in real-world cases. The difficulty is that it\u2019s unclear what \u201cbest-guess\u201d probabilities &amp; utilities we should assign, as well as unclear to what extent we should believe our best guesses.\u00a0\u00a0</p><p>Consider this passage from <a href=\"https://flightfromperfection.com/files/post_attachments/cluelessness_greaves_2016.pdf\">Greaves 2016</a> (\u201ccredence function\u201d can be read roughly as \u201cprobability\u201d):</p><blockquote>The alternative line I will explore here begins from the suggestion that in the situations we are considering, instead of having some single and completely precise (real-valued) credence function, agents are rationally required to have imprecise credences: that is, to be in a credal state that is represented by a many-membered set of probability functions (call this set the agent\u2019s \u2018representor\u2019). Intuitively, the idea here is that when the evidence fails conclusively to recommend any particular credence function above certain others, agents are rationally required to remain neutral between the credence functions in question: to include all such equally-recommended credence functions in their representor.</blockquote><p></p><p>To translate a little, Greaves is saying that real-world agents don\u2019t assign precise probabilities to outcomes, they instead consider multiple possible probabilities for each outcome (taken together, these probabilities sum to the agent\u2019s \u201crepresentor\u201d). Because an agent holds multiple probabilities for each outcome, and has no way by which to arbitrate between its multiple probabilities, it cannot use a straightforward expected value calculation to determine the best outcome.</p><p>Intuitively, this makes sense. Probabilities can only be formally assigned when the <a href=\"https://en.wikipedia.org/wiki/Sample_space\">sample space</a> is fully mapped out, and for most real-world decisions we can\u2019t map the full sample space (in part because the world is very complicated, and in part because we can\u2019t predict the long-run consequences of an action).[2] We can make subjective probability estimates, but if a probability estimate does not flow out of a clearly articulated model of the world, its believability is suspect.[3]</p><p>Furthermore, because multiple probability estimates can seem sensible, agents can hold multiple estimates simultaneously (i.e. their representor). For decisions where the full sample space isn\u2019t mapped out (i.e. most real-world decisions), the method by which human decision-makers convert their multi-value representor into a single-value, \u201cbest-guess\u201d estimate is opaque.</p><p>The next time you encounter someone making a subjective probability estimate, ask \u201chow did you arrive at that number?\u201d The answer will frequently be along the lines of \u201cit seems about right\u201d or \u201cI would be surprised if it were higher.\u201d Answers like this indicate that the estimator doesn\u2019t have visibility into the process by which they\u2019re arriving at their estimate.</p><p>So we have believability problems on two levels:</p><ol><li>Whenever we make a probability estimate that doesn\u2019t flow from a clear world-model, the believability of that estimate is questionable.</li><li>And if we attempt to reconcile multiple probability estimates into a single best-guess, the believability of that best-guess is questionable because our method of reconciling multiple estimates into a single value is opaque.[4]</li></ol><p></p><p>By now it should be clear that simply following the expected value is not a sufficient response to concerns of cluelessness. However, it\u2019s possible that cluelessness can be addressed by other routes \u2013 perhaps by diligent investigation, we can grow clueful enough to make believable decisions about how to do good. </p><p>The <a href=\"https://forum.effectivealtruism.org/ea/1j4/how_tractable_is_cluelessness/\">next post</a> will consider this further.</p><p></p><p><em>Thanks to Jesse Clifton and an anonymous collaborator for thoughtful feedback on drafts of this post. Views expressed above\u00a0are my own. Cross-posted to <a href=\"https://flightfromperfection.com/just-take-the-expected-value.html\">my personal blog</a>.</em></p><hr class=\"dividerBlock\"/><h2>Footnotes</h2><p>[1]: This is separate from normative uncertainty \u2013 uncertainty about what criterion of moral betterness to use when comparing options. Empirical uncertainty is uncertainty about the overall impact of an action, given a criterion of betterness. In general, cluelessness is a subset of empirical uncertainty.\u00a0</p><p>[2]: Leonard Savage, who worked out much of the foundations of Bayesian statistics, considered Bayesian decision theory to only apply in &quot;small world&quot; settings. See p. 16 &amp; p. 82 of the second edition of his <a href=\"https://books.google.com/books/about/The_Foundations_of_Statistics.html?id=zSv6dBWneMEC\">Foundations of Statistics</a>\u00a0for further discussion of this point.</p><p>[3]: Thanks to Jesse Clifton to making this point.</p><p>[4]: This\u00a0problem persists even if each input estimate flows from a clear world-model.</p>", "user": {"username": "Milan_Griffes"}}, {"_id": "XKwiEpWRdfWo7jy7f", "title": "2017 AI Safety Literature Review and Charity Comparison", "postedAt": "2017-12-20T21:54:07.419Z", "htmlBody": "<p>Summary: I review a significant amount of 2017 research related to AI Safety and offer some comments about where I am going to donate this year.</p><p></p><h2>Contents</h2><p>Contents</p><p>Introduction</p><p>The Machine Intelligence Research Institute (MIRI)</p><p>The Future of Humanity Institute (FHI)</p><p>Global Catastrophic Risks Institute (GCRI)</p><p>The Center for the Study of Existential Risk (CSER)</p><p>AI Impacts</p><p>Center for Human-Compatible AI (CFHCA)</p><p>Other related organisations</p><p>Related Work by other parties</p><p>Other major developments this year</p><p>Conclusion</p><p>Disclosures</p><p>Bibliography</p><p></p><h2>Introduction</h2><p><a href=\"https://forum.effectivealtruism.org/ea/14w/2017_ai_risk_literature_review_and_charity/\">Like last year</a>, I\u2019ve attempted to review the research that has been produced by various organisations working on AI safety, to help potential donors gain a better understanding of the landscape. This is a similar role to that which GiveWell performs for global health charities, and somewhat similar to an securities analyst with regards to possible investments. \u00a0It appears that once again no-one else has attempted to do this, to my knowledge, so I&#x27;ve once again undertaken the task. While I&#x27;ve been able to work significantly more efficiently on this than last year, I have been unfortunately very busy with my day job, which has dramatically reduced the amount of time I\u2019ve been able to dedicate.</p><p></p><p>My aim is basically to judge the output of each organisation in 2017 and compare it to their budget. This should give a sense for the organisations&#x27; average cost-effectiveness. Then we can consider factors that might increase or decrease the marginal cost-effectiveness going forward. We focus on organisations, not researchers. </p><p></p><p>Judging organisations on their historical output is naturally going to favour more mature organisations. A new startup, whose value all lies in the future, will be disadvantaged. However, I think that this is correct. The newer the organisation, the more funding should come from people with close knowledge. As organisations mature, and have more easily verifiable signals of quality, their funding sources can transition to larger pools of less expert money. This is how it works for startups turning into public companies and I think the same model applies here.</p><p></p><p>This judgement involves analysing a large number papers relating to Xrisk that were produced during 2017. Hopefully the year-to-year volatility of output is sufficiently low that this is a reasonable metric. I also attempted to include papers during December 2016, to take into account the fact that I&#x27;m missing the last month&#x27;s worth of output from 2017, but I can&#x27;t be sure I did this successfully.</p><p></p><p>This article focuses on AI risk work. If you think other causes are important too, your priorities might differ. This particularly affects GCRI and CSER, who both do a lot of work on other issues.</p><p></p><p>We focus virtually exclusively on papers, rather than outreach or other activities. This is party because they are much easier to measure; while there has been a large increase in interest in AI safety over the last year, it\u2019s hard to work out who to credit for this, and partly because I think progress has to come by persuading AI researchers, which I think comes through technical outreach and publishing good work, not popular/political work.</p><p></p><p>My impression is that policy on technical subjects (as opposed to issues that attract strong views from the general population) is generally made by the government and civil servants in consultation with, and being lobbied by, outside experts and interests. Without expert (e.g. top ML researchers at Google, CMU &amp; Baidu) consensus, no useful policy will be enacted. Pushing directly for policy seems if anything likely to hinder expert consensus. Attempts to directly influence the government to regulate AI research seem very adversarial, and risk being pattern-matched to ignorant opposition to GM foods or nuclear power. We don&#x27;t want the &#x27;us-vs-them&#x27; situation, that has occurred with climate change, to happen here. AI researchers who are dismissive of safety law, regarding it as an imposition and encumbrance to be endured or evaded, will probably be harder to convince of the need to voluntarily be extra-safe - especially as the regulations may actually be totally ineffective. The only case I can think of where scientists are relatively happy about punitive safety regulations, nuclear power, is one where many of those initially concerned were scientists themselves. \u00a0Given this, I actually think policy outreach to the general population is probably negative in expectation.</p><p></p><p>The good news on outreach this year is we haven\u2019t had any truly terrible publicity that I can remember, though I urge organisations to remember that the personal activities of their employees, especially senior ones, reflect on the organisations themselves, so they should take care not to act/speak in ways that are offensive to those outside their bubble, and to avoid hiring crazy people. </p><p></p><p>Part of my motivation for writing this is to help more people become informed about the AI safety landscape so they can contribute better with both direct work and donations. With regard donations, at present Nick Beckstead, in his role as both Fund Manager of the <a href=\"https://app.effectivealtruism.org/funds/far-future\">Long-Term Future Fund</a> and officer with the Open Philanthropy Project, is probably the most important financer of this work. He is also probably significantly more informed on the subject than me, but I think it&#x27;s important that the vitality of the field doesn&#x27;t depend on a single person, even if that person is awesome. </p><p></p><h2>The Machine Intelligence Research Institute (MIRI)</h2><p><a href=\"https://intelligence.org/\">MIRI</a> is the largest pure-play AI existential risk group. Based in Berkeley, it focuses on mathematics research that is unlikely to be produced by academics, trying to build the foundations for the development of safe AIs. </p><p></p><p>Their agent foundations work is basically trying to develop the correct way of thinking about agents and learning/decision making by spotting areas where our current models fail and seeking to improve them. Much of their work this year seems to involve trying to address self-reference in some way - how can we design, or even just model, agents that are smart enough to think about themselves? This work is technical, abstract, and requires a considerable belief in their long-term vision, as it is rarely locally applicable, so hard to independently judge the quality.</p><p></p><p>In 2016 they announced they were somewhat pivoting towards work that tied in closer to the ML literature, a move I thought was a mistake. However, looking at <a href=\"https://intelligence.org/all-publications/\">their published research</a> or their <a href=\"https://intelligence.org/2017/12/01/miris-2017-fundraiser/\">2017 review page</a>, in practice this seems to have been less of a change of direction than I had thought, as most of their work appears to remain on highly differentiated and unreplaceable agent foundations type work - it seems unlikely that anyone not motivated by AI safety would produce this work. Even within those concerned about friendly AI, few not at MIRI would produce this work. </p><p></p><p>Critch&#x27;s <a href=\"https://arxiv.org/abs/1701.01302\">Toward Negotiable Reinforcement Learning: Shifting Priorities in Pareto Optimal Sequential Decision-Making</a> (elsewhere titled &#x27;Servant of Many Masters&#x27;) is a neat paper. Basically it identifies the pareto-efficient outcome if you have two agents with different beliefs who want to agree on a utility function for an AI, in a generalisation of Harsanyi&#x27;s <a href=\"http://www.springer.com/us/book/9789027711861\">Cardinal welfare, individualistic ethics, and interpersonal comparisons of utility</a>. The key assumption is both want to use their current beliefs when they calculate the expected value of the deal to themselves, and the (surprising to me) conclusion is that over time the AI will have to weigh more and more heavily the values of the negotiator whose beliefs were more accurate. While I don&#x27;t think this is necessarily Critch&#x27;s interpretation, I take this as something of a reductio of the assumption. Surely if I was negotiating over a utility function, I would want the agent to learn about the world and use that knowledge to better promote my values ... not to learn about the world, decide I was a moron with a bad world model, and ignore me thereafter? If I think the AI is/will be smarter than me, I should be happy for it to do things I&#x27;m unaware will benefit me, and avoid doing things I falsely believe will help me. On the other hand, if the parties are well-informed nation states rather than individuals, the prospect of \u2018getting one over\u2019 the other might be helpful for avoiding arms races?</p><p></p><p></p><p>Kosoy&#x27;s <a href=\"https://arxiv.org/abs/1608.04112\">Optimal polynomial-time estimators</a> addresses a similar topic to the Logical Induction work - assigning &#x27;probabilities&#x27; to logical/mathematical/deductive statements under computational limitations - but with a quite different approach to solving it. The work seems impressive but I didn&#x27;t really understand it. Inside his framework he can prove that various results from probability theory also apply to logical statements, which seems like what we&#x27;d want. (Note that technically this paper came out in December 2016, and so is included in this year rather than last year\u2019s.)</p><p></p><p>Carey&#x27;s article, <a href=\"https://arxiv.org/abs/1709.06275\">Incorrigibility in the CIRL Framework</a>, is a response to Milli et al.\u2019s <a href=\"https://arxiv.org/pdf/1705.09990.pdf\">Should Robots be Obedient</a> and Hadfield-Menel&#x27;s <a href=\"https://arxiv.org/pdf/1611.08219.pdf\">The Off-Switch Game</a>. Carey basically argues it\u2019s not necessarily the case that the CIRLs will be \u2018automatically\u2019 corigible if the AI&#x27;s beliefs about value are very wrong, for example due to incorrect parameterisation or assigning a zero prior to something that turns out to be the case. The discussion section has some interesting arguments, for example pointing out that an algorithm designed to shut itself off unless it had a track record of perfectly predicting what humans would want might still fail if its ontology was insufficient, so it couldn&#x27;t even tell that it was disagreeing with the humans during training. I agree that value complexity and fragility might mean it\u2019s very likely that any AI\u2019s value model will be partially (and hence, for an AGI, catastrophically) mis-parameterised. However, I\u2019m not sure how much the examples that take up much of the paper add to this argument. Milli\u2019s argument only holds when the AI can learn the parameters, and given that this paper assumes the humans choose the wrong action by accident less than 1% of the time, it seems that the AI should assign a very large amount of evidence to a shutdown command... instead the AI seems to simply ignore it?</p><p></p><p>Some of MIRI&#x27;s publications this year seem to mainly be better explanations of previous work. For example, Garrabrant et al&#x27;s <a href=\"https://arxiv.org/abs/1707.08747\">A Formal Approach to the Problem of Logical Non-Omniscience</a> seems to be basically an easier to understand version of last year&#x27;s <a href=\"http://arxiv.org/abs/1609.03543\">Logical Induction</a>. Likewise Yudkowsky and Soares&#x27;s <a href=\"https://arxiv.org/abs/1710.05060\">Functional Decision Theory: A New Theory of Instrumental Rationality</a> seems to be basically new exposition of classic MIRI/LW decision theory work - see for example Soares et al&#x27;s <a href=\"https://arxiv.org/pdf/1507.01986.pdf\">Toward Idealized Decision Theory</a>. Similarly, I didn&#x27;t feel like there was much new in Soares et al&#x27;s <a href=\"https://intelligence.org/files/DeathInDamascus.pdf\">Cheating Death in Damascus</a>. Making things easier to understand is useful - and last year&#x27;s Logical Induction paper was a little dense - but it&#x27;s clearly not as impressive as inventing new things.</p><p></p><p>When I asked for top achievements for 2017, MIRI pointed me towards a lot of work they&#x27;d posted on <a href=\"https://agentfoundations.org/\">agentfoundations.org</a> as being one of their major achievements for the year, especially <a href=\"https://agentfoundations.org/item?id=1468\">this</a>, <a href=\"https://agentfoundations.org/item?id=1356\">this</a> and <a href=\"https://agentfoundations.org/item?id=1712\">this</a>, which pose and then solve a problem about how to find game-theoretic agents that can stably model each other, formulated it as a topological fixed point problem. There is also a lot of other work on agentfoundations that seems interesting, I&#x27;m not entirely sure how to think about giving credit for these. These seem more like &#x27;work in progress&#x27; than finished work - for most organisations I am only giving credit for the latter. MIRI could with some justification respond that the standard academic process is very inefficient, and part of their reason for existence is to do things that universities cannot. However, even if you de-prioritise peer review, I still think it is important to write things up into papers. Otherwise it is extremely hard for outsiders to evaluate - bad both for potential funders and for people wishing to enter the field. Unfortunately it is possible that, if they continue on this route, MIRI might produce a lot of valuable work that is increasingly illegible from the outside. So overall I think I consider these as evidence that MIRI is continuing to actually do research, but will wait until they\u2019re ArXived to actually review them. If you disagree with this approach, MIRI is going to look much more productive, and their research possibility accelerating in 2017 vs 2016. If you instead only look at published papers, 2017 appears to be something of a \u2018down year\u2019 after 2016.</p><p></p><p>Last year I was not keen to see that Eliezer was spending a lot of time producing content on Arbital as part of his job at MIRI, as there was a clear conflict of interest - he was a significant shareholder in Arbital, and additionally I expected Arbital to fail. Now that <a href=\"http://lesswrong.com/r/discussion/lw/otq/whats_up_with_arbital/\">Arbital does seem to have indeed failed</a>, I&#x27;m pleased he seems to be spending less time on it, but confused why he is spending any time at all on it - though <a href=\"https://arbital.com/p/yudkowsky_chollet_reply/\">some of this</a> seems to be <a href=\"https://intelligence.org/2017/12/06/chollet/\">cross-posted from elsewhere</a>. </p><p></p><p>Eliezer&#x27;s book <a href=\"https://www.amazon.com/dp/B076Z64CPG\">Inadequate Equilibria</a>, however, does seem to be high quality - basically another sequence - though only relevant inasmuch as AI safety might be one of many applications of the subject of the book. I also encourage readers to also read this <a href=\"https://forum.effectivealtruism.org/ea/1g7/in_defence_of_epistemic_modesty/\">excellent article</a> by Greg Lewis (FHI) on the other side.</p><p></p><p>I also enjoyed <a href=\"https://intelligence.org/2017/10/13/fire-alarm/\">There&#x27;s No Fire Alarm for Artificial General Intelligence</a>, which although accessible to the layman I think provided a convincing case that, even when AGI is imminent, there would (/might be) no signal that this was the case, and his <a href=\"https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/\">socratic security dialogs</a> on the mindset required to develop a secure AI.</p><p></p><p>I was sorry to hear Jessica Taylor left MIRI, as I thought she did good work.</p><p></p><p>MIRI spent roughly $1.9m in 2017, and aim to rapidly increase this to $3.5m in 2019, to fund new researchers and their new engineering team. </p><p></p><p>The Open Philanthropy Project <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support-2017\">awarded MIRI a $3.75m grant</a> (over 3 years) earlier this year, largely because one reviewer was impressed with their work on Logical Induction. You may recall this was a significant part of why I <a href=\"https://forum.effectivealtruism.org/ea/14w/2017_ai_risk_literature_review_and_charity/\">endorsed MIRI last year</a>. \u00a0However, as this review is focused on work in the last twelve months, they don&#x27;t get credit for the same work two years running! OPP have said they plan to fund roughly half of MIRI&#x27;s budget. On the positive side, one might argue this was essentially a 1:1 match on donations to MIRI - but there are clearly game-theoretic problems here. Additionally, if you had faith in OpenPhil\u2019s process, you might consider this a positive signal of MIRI quality. On the other hand, if you think MIRI&#x27;s marginal cost-effectiveness is diminishing over the multi-million dollar range, this might reduce your estimate of the cost-effectiveness of the marginal dollar. </p><p></p><p>There is also $1m of somewhat plausibly counterfactually valid donation matching <a href=\"https://2017charitydrive.com/\">available for MIRI </a>(but not other AI Xrisk organisations). </p><p></p><p>Finally, I will note that MIRI are have been very generous with their time in helping me understand what they are doing.</p><p></p><h2>The Future of Humanity Institute (FHI)</h2><p>Oxford\u2019s <a href=\"https://www.fhi.ox.ac.uk/\">FHI</a> requested not to be included in this analysis, so I won&#x27;t be making any comment on whether or not they are a good place to fund. Had they not declined (and depending on their funding situation) they would have been a strong candidate. This was disappointing to me, because they seem to have produced <a href=\"https://www.fhi.ox.ac.uk/publications/\">an impressive list of publications</a> this year, including a lot of collaborations. I\u2019ll briefly note two some pieces of research they published this year, but regret not being able to give them better coverage. </p><p></p><p>Saunders et al. published <a href=\"https://arxiv.org/abs/1707.05173\">Trial without Error: Towards Safe Reinforcement Learning via Human Intervention</a>, a nice paper where they attempt to make a Reinforcement Learner that can &#x27;safely&#x27; learn by training a catastrophe-recognition algorithm to oversee the training. It&#x27;s a cute idea, and a nice use of the OpenAI Atari suite, though I was most impressed with the fact that they concluded that their approach would not scale (i.e. would not work). It&#x27;s not often researchers publish negative results!</p><p></p><p>Honourable mention also goes to the very cool (but aren&#x27;t all his papers?) Sandberg et al. <a href=\"https://arxiv.org/pdf/1705.03394.pdf\">That is not dead which can eternal lie: the aestivation hypothesis for resolving Fermi\u2019s paradox</a>, which is relevant inasmuch as it suggests that the Fermi Paradox is not actually evidence against AI as an existential risk.</p><p></p><p>FHI\u2019s <a href=\"https://twitter.com/BrundageBot\">Brundage Bot</a> apparently reads every ML paper ever written.</p><p></p><h2>Global Catastrophic Risks Institute (GCRI)</h2><p>The <a href=\"http://gcrinstitute.org/\">Global Catastrophic Risks Institute</a> is run by Seth Baum and Tony Barrett. They have produced work on a variety of existential risks, including non-AI risks. Some of this work seems quite valuable, especially Denkenberger&#x27;s <a href=\"https://www.amazon.com/Feeding-Everyone-Matter-What-Catastrophe/dp/0128044470\">Feeding Everyone No Matter What</a> on ensuring food supply in the event of disaster, and is probably probably of interest to the sort of person who would read this document. However, they are off-topic for us here. Within AI they do a lot of work on the strategic landscape, and are very prolific. </p><p></p><p>Baum\u2019s <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3070741\">Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy</a> attempts to analyse all existing AGI research projects. This is a huge project and I laud him for it. I don\u2019t know how much here is news to people who are very plugged in, but to me at least it was very informative. The one criticism I would have is it could do more to try to differentiate on capacity/credibility - e.g. my impression is Deepmind is dramatically more capable than many of the smaller organisations listed - but that is clearly a very difficult ask. It\u2019s hard for me to judge the accuracy, but I didn\u2019t notice any mistakes (beyond being surprised that AIXI has an \u2018unspecified\u2019 for safety engagement, given the amount of AI safety papers coming out of ANU.)</p><p></p><p>Baum\u2019s <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3046725\">Social Choice Ethics in Artificial Intelligence</a> argues that value-learning type approaches to AI ethics (like <a href=\"https://intelligence.org/files/CEV.pdf\">CEV</a> ) contain many degrees of freedom for the programmers to finesse it to pick their values, making them no better than the programmers simply choosing an ethical system directly. The programmers can choose whose values are used for learning, how they are measured, and how they are aggregated. Overall I\u2019m not fully convinced - for example, pace the argument on page 3, a Law of Large Numbers argument could support averaging many views to get at the true ethics even if we had no way of independently verifying the true ethics. And there is some irony that, for all the paper\u2019s concern with bias risk, the left-wing views of the author come through strongly. But despite these I liked the paper, especially for the discussion of who has standing - something that seems like it will need a philosophical solution, rather than a ML one. </p><p></p><p>Barrett&#x27;s <a href=\"https://www.dropbox.com/s/7a7eh2law7tbvk0/2017-barrett.pdf?dl=0\">Value of Global Catastrophic Risk (GCR) Information: Cost-Effectiveness-Based Approach for GCR Reduction</a> covers a lot of familiar ground, and then attempts to do some monte carlo cost-benefit analysis on the a small number of interventions to help address nuclear war and comet impact. After putting a lot of thought into setting up the machinery, it would have been good to see analysis of a wider range of risks!</p><p></p><p>Baum &amp; Barrett published <a href=\"http://sethbaum.com/ac/2018_Extreme.pdf\">Global Catastrophes: The Most Extreme Risks</a>, which seems to be essentially a reasonably well argued general introduction to the subject of existential risks. Hopefully people who bought the book for other reasons will read it and become convinced.</p><p></p><p>Baum &amp; Barrett&#x27;s <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3046816\">Towards an Integrated Assessment of Global Catastrophic Risk</a> is a similar introductory piece on catastrophic risks, but the venue - a colloquium on catastrophic risks - seems less useful, as people reading it are more likely to already be concerned about the subject, and I don&#x27;t think it spends enough time on AI risk per se to convince those who were already worried about Xrisk but not AI Xrisk. </p><p></p><p>Last year I was (and still am) impressed by their paper <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2816323\">On the Promotion of Safe and Socially Beneficial Artificial Intelligence</a>, which made insightful, convincing and actionable criticisms of &#x27;AI arms race&#x27; language. I was less convinced by this year&#x27;s <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2976444\">Reconciliation Between Factions Focused on Near-Term  and Long-Term Artificial Intelligence</a>, which argues for a re-alignment away from near-term AI worries vs long-term AI worries towards AI worriers vs non-worriers. However, I&#x27;m not sure why anyone would agree to this - long-term worriers don&#x27;t currently spend much time arguing against short-term worries (even if you thought that AI discrimination arguments were orwellian, why bother arguing about it?), and convincing short-term worriers to stop criticise long-term worries seems approximately as hard as simply convincing them to become long-term worriers.</p><p></p><p>GCRI spent approximately $117k in 2017, which is shockingly low considering their productivity. This was lower than 2016; apparently their grants from the US Dept. of Homeland Security came to an end.</p><p></p><h2>The Center for the Study of Existential Risk (CSER)</h2><p><a href=\"http://cser.ac.uk/\">CSER </a>is an existential risk focused group located in Cambridge. Like GCRI they do work on a variety of issues, notably including Rees\u2019 work on <a href=\"https://www.cser.ac.uk/media/uploads/files/Black-Sky-Workshop-at-the-Royal-Society-Jan.-20171.pdf\">infrastructure resilience</a>.</p><p></p><p>Last year I criticised them for not having produced any online research over several years; they now have a <a href=\"https://www.cser.ac.uk/resources/filter/publication/risks-from-artificial-intelligence/all/all/\">separate page</a> that does list some but maybe not all of their research.</p><p></p><p>Liu, a CSER researcher, wrote <a href=\"http://www.academia.edu/33992500/The_Sure-thing_Principle_and_P2\">The Sure-Thing principle and P2</a> and was second author on Gaifman &amp; Liu&#x27;s <a href=\"https://link.springer.com/article/10.1007%2Fs11229-017-1594-6\">A simpler and more realistic subjective decision theory</a>, both on the mathematical foundations of bayesian decision theory, which is a valuable topic for AI safety in general. Strangely neither paper mentioned CSER as a financial supporter of the paper or affiliation. </p><p></p><p>Liu and Price\u2019s <a href=\"http://yliu.net/wp-content/uploads/darcness.pdf\">Heart of DARCness</a> argues that agents do not have credences for what they will do while deciding whether to do it - their confidence is temporarily undefined. I was not convinced - even someone is deciding whether she\u2019s 75% confident or 50% confident, presumably there are some odds that determine which side in a bet she\u2019d take if forced to choose? I\u2019m also not sure of the direct link to AI safety.</p><p></p><p>They\u2019ve also convened and attended workshops on AI and decision theory, notably the <a href=\"https://www.cser.ac.uk/news/ai-society-symposium/\">AI &amp; Society Symposium in Japan</a>, but in general I am wary of giving organisations credit for these, as they are too hard for the outside observer to judge, and ideally workshops lead to produce papers - in which case we can judge those.</p><p></p><p>CSER also did a significant amount of outreach, including <a href=\"https://www.cser.ac.uk/resources/written-evidence-lords-select-committee-artificial-intelligence/\">presenting to the House of Lords</a>, and apparently have expertise in Chinese outreach (multiple native mandarin speakers), which could be important, given China\u2019s AI research but cultural separation from the west. </p><p></p><p>They are undertaking a novel publicity effort that I won\u2019t name as I\u2019m not sure it\u2019s public yet. In general I think most paths to success involve consensus-building among mainstream ML researchers, and \u2018popular\u2019 efforts risk harming our credibility, so I am not optimistic here. </p><p></p><p>Their annual budget is around \u00a3750,000, with I estimate a bit less than half going on AI risk. Apparently they need to raise funds to continue existing once their current grants run out in 2019.</p><p></p><h2>AI Impacts</h2><p>AI Impacts is a small group that does high-level strategy work, especially on AI timelines, somewhat associated with MIRI.</p><p></p><p>They seem to have produced significantly more this year than last year. The main achievement is the <a href=\"https://arxiv.org/abs/1705.08807\">When will AI exceed Human Performance? Evidence from AI Experts</a>, which asked gathered the opinions of hundreds of AI researchers on AI timelines questions. There were some pretty relevant takeaways, like that most researchers find the AI Catastrophic Risk argument somewhat plausible, but doubt there is anything that can usefully be done in the short term, or that asian researchers think human-level AI is significantly closer than americans do. I think the value-prop here is twofold: firstly, providing a source of timeline estimates for when we make decisions that hinge on how long we have, and secondly, to prove that concern about AI risk is a respectable, mainstream position. It was apparently <a href=\"https://www.altmetric.com/top100/2017/#list\">one of the most discussed papers of 2017</a>.</p><p></p><p>On a similar note they also have data on improvements in a number of AI-related benchmarks, like <a href=\"https://aiimpacts.org/recent-trend-in-the-cost-of-computing/\">computing costs</a> or <a href=\"https://aiimpacts.org/trends-in-algorithmic-progress/\">algorithmic progress</a>.</p><p></p><p>John Salvatier (member of AI Impacts at the time) was also second author on <a href=\"https://arxiv.org/abs/1701.04079\">Agent-Agnostic Human-in-the-Loop Reinforcement Learning</a>, along with Evans (FHI, 4th author), which attempts to design an interface for reinforcement learning that abstracts away from the agent, so you could easily change the underlying agent.</p><p></p><p>AI Impacts\u2019 budget is tiny compared to most of the other organisations listed here; around $60k at present. Incremental funds would apparently be spent on hiring more part-time researchers.</p><p></p><p></p><p></p><h2>Center for Human-Compatible AI (CFHCA)</h2><p>The Center for Human-Compatible AI, founded by Stuart Russell in Berkeley, launched in August 2016. As they are not looking for more funding at the moment I will only briefly survey some of they work on cooperative inverse reinforcement learning.</p><p></p><p>Hadfield-Menel et al&#x27;s <a href=\"https://arxiv.org/pdf/1611.08219.pdf\">The Off-Switch Game</a> is a nice paper that produces and formalises the (at least now I&#x27;ve read it) very intuitive result that a value-learning AI might be corrigible (at least in some instances) because it takes the fact that a human pressed the off-switch as evidence that this is the best thing to do.</p><p></p><p>Milli et al&#x27;s <a href=\"https://arxiv.org/pdf/1705.09990.pdf\">Should Robots be Obedient</a> is in the same vein as Hadfield-Menel et al\u2019s <a href=\"https://arxiv.org/abs/1606.03137\">Cooperative Inverse Reinforcement Learning</a> (last year) on learning values from humans, specifically touching on whether such agents would be willing to obey a command to &#x27;turn off&#x27;, as per Soares&#x27;s paper on <a href=\"https://intelligence.org/files/Corrigibility.pdf\">Corrigibility</a>. She does some interesting analysis about the trade-off between obedience and results in cases where humans are fallible. </p><p></p><p>In both cases I thought the papers were thoughtful and had good analysis. However, I don\u2019t think either is convincing in showing that corrigibility comes \u2018naturally\u2019 - at least not the strength of corrigibility we need.</p><p></p><p>I encourage them to keep their website more up-to-date.</p><p></p><p>Overall I think their research is good and their team promising. However, apparently they have enough funding for now, so I won&#x27;t be donating this year. If this changed and they requested incremental capital I could certainly imagine funding them in future years.</p><p></p><h2>Other related organisations</h2><p><a href=\"http://rationality.org/resources/updates/2017/cfar-2017-fundraiser\">The Center for Applied Rationality</a> (CFAR) works on trying to improve human rationality, especially with the aim of helping with AI Xrisk efforts.</p><p></p><p><a href=\"https://futureoflife.org/2017/11/27/help-support-fli-giving-tuesday/\">The Future of Life Institute</a> (FLI) ran a huge grant-making program to try to seed the field of AI safety research. There definitely seem to be a lot more academics working on the problem now, but it\u2019s hard to tell how much to attribute to FLI.</p><p></p><p><a href=\"https://80000hours.org/articles/extinction-risk/\">Eighty Thousand Hours</a> (80K) provide career advice, with AI safety being one of their key cause areas. </p><p></p><h2>Related Work by other parties</h2><p><a href=\"https://arxiv.org/abs/1706.03741\">Deep Reinforcement Learning from Human Preferences</a>, was possibly my favourite paper of the year, which possibly shouldn\u2019t come as a surprise, given that two of the authors (Christiano and Amodei from OpenAI ) were authors on last year\u2019s <a href=\"https://arxiv.org/abs/1606.06565\">Concrete Problems in AI Safety</a>. It applies ideas on bootstrapping that Christiano has been discussing for a while - getting humans to train an AI which then trains another AI etc. The model performs significantly better than I would have expected, and as ever I\u2019m pleased to see OpenAI - Deepmind collaboration.</p><p></p><p>Christiano continues to produce very interesting content on his blog, like <a href=\"https://ai-alignment.com/corrigibility-3039e668638\">this</a> on Corrigibility. When I first read his articles about how to bootstrap safety through iterative training procedures, my reactions was that, while this seemed an interesting idea, it didn&#x27;t seem to have much in common with mainstream ML. However, there do seem to be a bunch of practical papers about imitation learning now. I&#x27;m not sure if this was always the case, and I was just ignorant, or if they have become more prominent in the last year. Either way, I have updated towards considering this approach to be a promising one for integrating safety into mainstream ML work. He has also written <a href=\"https://ai-alignment.com/alphago-zero-and-capability-amplification-ede767bb8446\">a nice blog post</a> explaining how AlphaZero works, and arguing that this supports his enhancement ideas.</p><p></p><p>It was also nice to see <a href=\"https://scholar.google.com/scholar?hl=en&as_sdt=0,31&sciodt=0,31&cites=6186600309471256628&scipsc=\">~95 papers </a>that were addressing Amodei et al&#x27;s call in last year\u2019s <a href=\"https://arxiv.org/abs/1606.06565\">Concrete Problems</a>.</p><p></p><p>Menda et al&#x27;s <a href=\"https://arxiv.org/abs/1709.06166\">DropoutDAgger</a> paper on safe exploration seems to fit in this category. Basically they come up with a form of imitation learning where the AI being trained can explore a bit, but isn&#x27;t allowed to stray too far from the expert policy - though I&#x27;m not sure why they always have the learner explore in the direction it thinks is best, rather than assigning some weight to its uncertainty of outcome, explore-exploit-style. I&#x27;m not sure how much credit Amodei et al can get for inspiring this though, as it seems to be (to a significant degree) an extension of Zhang and Cho\u2019s <a href=\"https://arxiv.org/abs/1605.06450\">Query-Efficient Imitation Learning for End-to-End Autonomous Driving</a>.</p><p></p><p>However, I don&#x27;t want to give too much credit for work that improves &#x27;local&#x27; safety that doesn&#x27;t also address the big problems in AI safety, because this work probably accelerates unsafe human-level AI. There are many papers in this category, but for obvious reasons I won&#x27;t call them out.</p><p></p><p>Gan&#x27;s <a href=\"https://arxiv.org/pdf/1711.04309.pdf\">Self-Regulating Artificial General Intelligence</a> contains some nice economic formalism around AIs seizing power from humans, and raises the interesting argument that if you need specialist AIs to achieve things, the first human-level AIs might not exhibit takeoff behaviour because they would be unable to sufficiently trust the power-seizing agents they would need to create. I&#x27;m sceptical that this assumption about the need for specialised AIs holds - surely even if you need to make separate AI agents for different tasks, rather than integrating them, it would suffice to give them specialised capabilities and but the same goals. Regardless, the paper does suggest the interesting possibility that humanity might make an AI which is intelligent enough to realise it cannot solve the alignment problem to safely self-improve... and hence progress stops there - though of course this would not be something to rely on.</p><p></p><p>MacFie&#x27;s <a href=\"https://arxiv.org/pdf/1708.09032.pdf\">Plausibility and Probability in Deductive Reasoning</a> also addresses the issue of how to assign probabilities to logical statements, in a similar vein to much MIRI research.</p><p></p><p>Vamplew et al\u2019s <a href=\"https://link.springer.com/article/10.1007/s10676-017-9440-6\">Human-aligned artificial intelligence is a multiobjective problem</a> argues that we should consider a broader class of functions than linear sums when combining utility functions.</p><p></p><p>Google Deepmind continue to churn out impressive research, some of which seems relevant to the problem, like Sunehag et al\u2019s <a href=\"https://arxiv.org/pdf/1706.05296.pdf\">Value-Decomposition Networks For Cooperative Multi-Agent Learning</a> and Danihelka, et al\u2019s <a href=\"https://arxiv.org/pdf/1705.05263.pdf\">Comparison of Maximum Likelihood and GAN-based training of Real NVPs</a> on avoiding overfitting.</p><p></p><p>In terms of predicting AI timelines, another piece I found interesting was Gupta et al.\u2019s <a href=\"https://arxiv.org/pdf/1707.02968.pdf\">Revisiting the Unreasonable Effectiveness of Data</a>, which argued that, for vision tasks at least, performance improved logarithmically in sample size.</p><p></p><p>The Foresight Institute published a <a href=\"https://foresight.org/publications/AGI-Timeframes&PolicyWhitePaper.pdf\">white paper</a> on the general subject of AI policy and risk.</p><p></p><p>Stanford&#x27;s <a href=\"https://ai100.stanford.edu/\">One Hundred Year Study on Artificial Intelligence</a> produced an <a href=\"https://aiindex.org/\">AI Index</a> report, which is basically a report on progress in the field up to 2016. Interestingly various metrics they tracked, summarised in their &#x27;Vibrancy&#x27; metric, suggest that the field actually regressed in 2016, through my experience with similar data in the financial world leaves me rather sceptical of such methodology. Unfortunately the report dedicated only a single word to the subject of AI safety.</p><p></p><p>On a lighter note, the esteemed G.K. Chesterton returned from beyond the grave to <a href=\"http://slatestarcodex.com/2017/04/01/g-k-chesterton-on-ai-risk/\">eviscerate an AI risk doubter</a>, and a group of researchers (some FHI) <a href=\"https://arxiv.org/pdf/1703.10987.pdf\">proved</a> that it is impossible to create a machine larger than a human, so that\u2019s a relief.</p><p></p><h2>Other major developments this year</h2><p>Google&#x27;s Deepmind produced AlphaZero, which learnt how to beat the best AIs (and hence also the best humans) at Go, Chess and Shogi with just a few hours of self-play. </p><p></p><p>Creation of the EA funds, including the <a href=\"https://app.effectivealtruism.org/funds/far-future\">Long-Term Future Fund</a>, run by Nick Beckstead, which has made one smallish grant related to AI Safety, conserved the other 96%.</p><p></p><p>The Open Philanthropy Project funded both MIRI and OpenAI (acquiring a board seat in the process with the latter).</p><p></p><p>Nvidia (who make GPUs used for ML) saw their \u00a0share price approximately doubl, after quadrupling last year.</p><p></p><p>Hillary Clinton was possibly <a href=\"http://lukemuehlhauser.com/hillary-clinton-on-ai-risk/\">concerned about AI risk</a>? But unfortunately Putin seems to have less helpful concerns about an AI Arms race... namely ensuring that <a href=\"https://www.rt.com/news/401731-ai-rule-world-putin/\">he wins it</a>. And China announced a <a href=\"https://www.reuters.com/article/us-china-ai/china-aims-to-become-world-leader-in-ai-challenges-u-s-dominance-idUSKBN1A5103\">national plan </a>for AI with chinese characteristics - but bear in mind they have failed at these before, like their push into Semiconductors, though companies like Baidu do seem to be doing impressive research.</p><p></p><p>There were <a href=\"https://arxiv.org/abs/1711.10337\">some</a> <a href=\"https://arxiv.org/abs/1709.06560\">papers</a> suggesting the replication crisis may be coming to ML? </p><p></p><h2>Conclusion</h2><p>In some ways this has been a great year. My impression is that the cause of AI safety has become increasingly mainstream, with a lot of researchers \u00a0unaffiliated with the above organisations working at least tangentially on it. </p><p></p><p>However, it\u2019s tough from the point of view of an external donor. Some of the organisations doing the best work are well funded. Others (MIRI) seem to be doing a lot of good work but (perhaps necessarily) it is significantly harder for outsiders to judge than last year, as there doesn\u2019t seem to be a really heavy-hitting paper like there was last year. I see MIRI\u2019s work as being a long-shot bet that their specific view of the strategic landscape is correct, but given this they\u2019re basically irreplaceable. GCRI and CSER\u2019s work is more mainstream in this regard, but GCRI\u2019s productivity is especially noteworthy, given the order of magnitude of difference in budget size.</p><p></p><p>As I have once again failed to reduce charity selection to a science, I\u2019ve instead attempted to subjectively weigh the productivity of the different organisations against the resources they used to generate that output, and donate accordingly.</p><p></p><p>My constant wish is to promote a lively intellect and independent decision-making among my readers; hopefully my laying out the facts as I see them above will prove helpful to some readers. Here is my eventual decision,<a href=\"http://www.rot13.com/\"> rot13&#x27;d</a> so you can do come to your own conclusions first if you wish:</p><p></p><p>Fvtavsvpnag qbangvbaf gb gur Znpuvar Vagryyvtrapr Erfrnepu Vafgvghgr naq gur Tybony Pngnfgebcuvp Evfxf Vafgvghgr. N zhpu fznyyre bar gb NV Vzcnpgf.</p><p></p><p>However I wish to emphasis that all the above organisations seem to be doing good work on the most important issue facing mankind. It is the nature of making decisions under scarcity that we must prioritize some over others, and I hope that all organisations will understand that this necessarily involves negative comparisons at times.</p><p></p><p>Thanks for reading this far; hopefully you found it useful. Someone suggested that, instead of doing this annually, I should instead make a blog where I provide some analysis of AI-risk related events as they occur. Presumably there would still be an annual giving-season writeup like this one. If you&#x27;d find this useful, please let me know.</p><p></p><h2>Disclosures</h2><p>I was a Summer Fellow at MIRI back when it was SIAI, volunteered very briefly at GWWC (part of CEA) and once applied for a job at FHI. I am personal friends with people at MIRI, FHI, CSER, CFHCA and AI Impacts but not GCRI (so if you\u2019re worried about bias you should overweight them\u2026 though it also means I have less direct knowledge). However I have no financial ties beyond being a donor and have never been romantically involved with anyone who has ever been at any of the organisations.</p><p></p><p>I shared a draft of the relevant sections of this document with representatives of MIRI, CSER and GCRI and AI Impacts. I&#x27;m very grateful for Alex Flint and Jess Riedel for helping review a draft of this document. Any remaining inadequacies and mistakes are my own. </p><p></p><p><em><strong>Edited 2017-12-21: Spelling mistakes, corrected Amodei&#x27;s affiliation.</strong></em></p><p><em><strong>Edited 2017-12-24: Minor correction to CSER numbers.</strong></em></p><h2>Bibliography</h2><p>Adam D. Cobb, Andrew Markham, Stephen J. Roberts; Learning from lions: inferring the utility of agents from their trajectories; https://arxiv.org/abs/1709.02357</p><p>Alexei Andreev; What&#x27;s up with Arbital; http://lesswrong.com/r/discussion/lw/otq/whats_up_with_arbital/</p><p>Allison Duettmann; Artificial General Intelligence: Timeframes &amp; Policy White Paper; https://foresight.org/publications/AGI-Timeframes&amp;PolicyWhitePaper.pdf</p><p>Anders Sandberg, Stuart Armstrong, Milan Cirkovic; That is not dead which can eternal lie: the aestivation hypothesis for resolving Fermi\u2019s paradox; https://arxiv.org/pdf/1705.03394.pdf</p><p>Andrew Critch, Stuart Russell; Servant of Many Masters: Shifting priorities in Pareto-optimal sequential decision-making; https://arxiv.org/abs/1711.00363</p><p>Andrew Critch; Toward Negotible Reinforcement Learning: Shifting Priorities in Pareto Optimal Sequential Decision-Making; https://arxiv.org/abs/1701.01302</p><p>Andrew MacFie; Plausibility and Probability in Deductive Reasoning; https://arxiv.org/pdf/1708.09032.pdf</p><p>Assaf Arbelle, Tammy Riklin Raviv; Microscopy Cell Segmentation via Adversarial Neural Networks; https://arxiv.org/abs/1709.05860</p><p>Ben Garfinkel, Miles Brundage, Daniel Filan, Carrick Flynn, Jelena Luketina, Michael Page, Anders Sandberg, Andrew Snyder-Beattie, and Max Tegmark; On the Impossibility of Supersized Machines; https://arxiv.org/pdf/1703.10987.pdf</p><p>Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, Sergey Levine; One-Shot Visual Imitation Learning via Meta-Learning; https://arxiv.org/abs/1709.04905</p><p>Chen Sun, Abhinav Shrivastava Saurabh Singh, Abhinav Gupta; Revisiting Unreasonable Effectiveness of Data in Deep Learning Era; https://arxiv.org/pdf/1707.02968.pdf</p><p>Chih-Hong Cheng, Frederik Diehl, Yassine Hamza, Gereon Hinz, Georg Nuhrenberg, \u00a0Markus Rickert, Harald Ruess, Michael Troung-Le; Neural Networks for Safety-Critical Applications - Challenges, Experiments and Perspectives; https://arxiv.org/pdf/1709.00911.pdf</p><p>Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Man\u00e9; Concrete Problems in AI Safety; https://arxiv.org/abs/1606.06565</p><p>David Abel, John Salvatier, Andreas Stuhlm\u00fcller, Owain Evans; Agent-Agnostic Human-in-the-Loop Reinforcement Learning; https://arxiv.org/abs/1701.04079</p><p>Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, Stuart Russell; The Off-Switch Game; https://arxiv.org/pdf/1611.08219.pdf</p><p>Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, Stuart Russell; Cooperative Inverse Reinforcement Learning; https://arxiv.org/abs/1606.03137</p><p>Eliezer Yudkowsky and Nate Soares; Functional Decision Theory: A New Theory of Instrumental Rationality; https://arxiv.org/abs/1710.05060</p><p>Eliezer Yudkowsky; A reply to Francois Chollet on intelligence exposion; https://intelligence.org/2017/12/06/chollet/</p><p>Eliezer Yudkowsky; Coherant Extrapolated Volition; https://intelligence.org/files/CEV.pdf</p><p>Eliezer Yudkowsky; Inadequate Equilibria; https://www.amazon.com/dp/B076Z64CPG</p><p>Eliezer Yudkowsky; There&#x27;s No Fire Alarm for Artificial General Intelligence; https://intelligence.org/2017/10/13/fire-alarm/</p><p>Filipe Rodrigues, Francisco Pereira; Deep learning from crowds; https://arxiv.org/abs/1709.01779</p><p>Greg Lewis; In Defense of Epistemic Modesty; http://effective-altruism.com/ea/1g7/in_defence_of_epistemic_modesty/</p><p>Haim Gaifman and Yang Liu; A simpler and more realistic subjective decision theory; https://link.springer.com/article/10.1007%2Fs11229-017-1594-6</p><p>Harsanyi; Cardinal welfare, individualistic ethics, and interpersonal comparisons of utility; http://www.springer.com/us/book/9789027711861</p><p>Ivo Danihelka, Balaji Lakshminarayanan, Benigno Uria, \u00a0Daan Wierstra, Peter Dayan; Comparison of Maximum Likelihood and GAN-based training of Real NVPs; https://arxiv.org/pdf/1705.05263.pdf</p><p>Jiakai Zhang, Kyunghyun Cho; Query-Efficient Imitation Learning for End-to-End Autonomous Driving; https://arxiv.org/abs/1605.06450</p><p>Joshua Gans; Self-Regulating Artificial General Intelligence; https://arxiv.org/pdf/1711.04309.pdf</p><p>Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, Owain Evans; When will AI exceed Human Performance? Evidence from AI Experts; https://arxiv.org/abs/1705.08807</p><p>Kavosh Asadi, Cameron Allen, Melrose Roderick, Abdel-rahman Mohamed, George Konidaris, Michael Littman; Mean Actor Critic; https://arxiv.org/abs/1709.00503</p><p>Kunal Menda, Katherine Driggs-Campbell, Mykel J. Kochenderfer; DropoutDAgger: A Bayesian Approach to Safe Imitation Learning; https://arxiv.org/abs/1709.06166</p><p>Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, Olivier Bousquet; Are GANs Created Equal? A Large-Scale Study; https://arxiv.org/abs/1711.10337</p><p>Martin Rees; &quot;Black Sky&quot; Infrastructure and Societal Resilience Workshop; https://www.cser.ac.uk/media/uploads/files/Black-Sky-Workshop-at-the-Royal-Society-Jan.-20171.pdf</p><p>Mile Brundage; Brundage Bot; https://twitter.com/BrundageBot</p><p>Minghai Qin, Chao Sun, Dejan Vucinic; Robustness of Neural Networks against Storage Media Errors; https://arxiv.org/abs/1709.06173</p><p>Myself; 2017 AI Risk Literature Review and Charity Evaluation; http://effective-altruism.com/ea/14w/2017_ai_risk_literature_review_and_charity/</p><p>Nate Soares and Benja Fallenstein; Towards Idealized Decision Theory; https://arxiv.org/pdf/1507.01986.pdf</p><p>Nate Soares and Benjamin Levinstein; Cheating Death in Damascus; https://intelligence.org/files/DeathInDamascus.pdf</p><p>Nates Soares, Benja Fallenstein, Eliezer Yudkowsky, Stuart Armstrong; Corrigibility; https://intelligence.org/files/Corrigibility.pdf</p><p>Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei; Deep Reinforcement Learning from Human Preferences; https://arxiv.org/abs/1706.03741</p><p>Paul Christiano; AlphaGo Zero and capability amplification; https://ai-alignment.com/alphago-zero-and-capability-amplification-ede767bb8446</p><p>Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, David Meger; Deep Reinforcement Learning that Matters; https://arxiv.org/abs/1709.06560</p><p>Peter Stone, Rodney Brooks, Erik Brynjolfsson, Ryan Calo, Oren Etzioni, Greg Hager, Julia Hirschberg, Shivaram Kalyanakrishnan, Ece Kamar, Sarit Kraus, Kevin Leyton-Brown, David Parkes, William Press, AnnaLee Saxenian, Julie Shah, Milind Tambe, \u00a0Astro Teller.; One Hundred Year Study on Artificial Intelligence; https://ai100.stanford.edu/</p><p>Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, Thore Graepel; Value-Decomposition Networks For Cooperative Multi-Agent Learning; https://arxiv.org/pdf/1706.05296.pdf</p><p>Peter Vamplew, Richard Dazeley, Cameron Foale, Sally Firmin, Jane Mummery; Human-aligned artificial intelligence is a multiobjective problem; https://link.springer.com/article/10.1007/s10676-017-9440-6</p><p>Ryan Carey; Incorrigibility in the CIRL Framework; https://arxiv.org/abs/1709.06275</p><p>Samuel Yeom, Matt Fredrikson, Somesh Jha; The Unintended Consequences of Overfitting: Training Data Inference Attacks; https://arxiv.org/abs/1709.01604</p><p>Scott Alexander; G.K. Chesterton on AI Risk; http://slatestarcodex.com/2017/04/01/g-k-chesterton-on-ai-risk/</p><p>Scott Garrabrant, Tsvi Benson-Tilsen, Andrew Critch, Nate Soares, Jessica Taylor; A Formal Approach to the Problem of Logical Non-Omniscience; https://arxiv.org/abs/1707.08747</p><p>Scott Garrabrant, Tsvi Benson-Tilsen, Andrew Critch, Nate Soares, Jessica Taylor; Logical Induction; http://arxiv.org/abs/1609.03543</p><p>Seth Baum and Tony Barrett; Global Catastrophes: The Most Extreme Risks; http://sethbaum.com/ac/2018_Extreme.pdf</p><p>Seth Baum and Tony Barrett; Towards an Integrated Assessment of Global Catastrophic Risk ; https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3046816</p><p>Seth Baum; On the Promotion of Safe and Socially Beneficial Artificial Intelligence; https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2816323</p><p>Seth Baum; Reconciliation Between Factions Focused on Near-Term and Long-Term Artificial Intelligence; https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2976444</p><p>Seth Baum; Social Choice Ethics in Artificial Intelligence; https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3046725</p><p>Seth Baum; Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy; https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3070741</p><p>Smitha Milli, Dylan Hadfield-Menell, Anca Dragan, Stuart Russell; Should Robots be Obedient; https://arxiv.org/pdf/1705.09990.pdf</p><p>Tony Barrett; Value of Global Catastrophic Risk (GCR) Information: Cost-Effectiveness-Based Approach for GCR Reduction; https://www.dropbox.com/s/7a7eh2law7tbvk0/2017-barrett.pdf?dl=0</p><p>Vadim Kosoy; Optimal Polynomial-Time Estimators: A Bayesian Notion of Approximation Algorithm; https://arxiv.org/abs/1608.04112</p><p>Victor Shih, David C Jangraw, Paul Sajda, Sameer Saproo; Towards personalized human AI interaction - adapting the behavior of AI agents using neural signatures of subjective interest; https://arxiv.org/abs/1709.04574</p><p>William Saunders, Girish Sastry, Andreas Stuhlmueller, Owain Evans; Trial without Error: Towards Safe Reinforcement Learning via Human Intervention; https://arxiv.org/abs/1707.05173</p><p>Xiongzhao Wang, Varuna De Silva, Ahmet Kondoz; Agent-based Learning for Driving Policy Learning in Connected and Autonomous Vehicles; https://arxiv.org/abs/1709.04622</p><p>Yang Liu and Huw Price; Heart of DARCness; http://yliu.net/wp-content/uploads/darcness.pdf</p><p>Yang Liu; The Sure-Thing principle and P2; http://www.academia.edu/33992500/The_Sure-thing_Principle_and_P2</p><p>Yunpeng Pan, Ching-An Cheng, Kamil Saigol, Keuntaek Lee, Xinyan Yan, Evangelos Theodorou, Byron Boots; Agile Off-Road Autonomous Driving Using End-to-End Deep Imitation Learning; https://arxiv.org/abs/1709.07174</p><p></p><p></p><p></p>", "user": {"username": "Larks"}}, {"_id": "CPjJw24azSxaySEzX", "title": "CFAR's end-of-year Impact Report and Fundraiser", "postedAt": "2017-12-20T18:58:11.507Z", "htmlBody": "<html><body><p>End-of-year updates for those interested:</p>\n<ul>\n<li>CFAR made a&#xA0;larger&#xA0;effort to track our programs&apos; impact on existential risk over the last year; you can find <a href=\"http://rationality.org/resources/updates/2017/cfar-2017-impact-report\">a partial account of our findings</a> on our blog.&#xA0; (Also, while&#xA0;some of the details of our tracking aren&apos;t currently published due to privacy concerns,&#xA0;let me know if there&apos;s some particular thing you want to know and maybe we can share it.)<br><br></li>\n<li>We&apos;re on the cusp of being able to maybe buy a permanent venue, which would dramatically reduce our per-workshop costs and would thereby substantially increase our ability to run free programs (which have historically been the cause of a substantial majority of our apparent impact on existential risk, despite being a smallish minority of our programs).&#xA0; There&apos;re some details in our <a href=\"http://rationality.org/resources/updates/2017/cfar-2017-fundraiser\">fundraiser post</a>, and some details on what we&apos;ve been up to for the last year in our <a href=\"http://rationality.org/resources/updates/2017/cfar-2017-fundraiser\">2017 Retrospective</a>.</li>\n</ul>\n<p>I&apos;d be glad to discuss anything CFAR-related with anyone interested. I continue to suspect that donations to CFAR are among the best ways to turn marginal donations into reducing the talent bottleneck within AI risk efforts (basically because our good done seems almost linear in the number of free-to-participants programs we can run (because those can target high-impact AI stuff), and because the number of free-to-participants programs we can run is more or less linear in donations within the range in which donations might plausibly take us, plus or minus a rather substantial blip depending on whether we can purchase a venue).&#xA0; I don&apos;t know a good way to measure or establish that as such, and I imagine many would disagree -- but I&apos;d still welcome discussion, either here or at anna at rationality dot org.</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "AnnaSalamon"}}]