[{"_id": "h6eYHRQP5fZ3pDtwh", "postedAt": "2014-10-15T13:42:36.239Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>I largely agree with your argument. The most useful way I've heard this explained is that affecting the <strong>direction</strong> of progress is greatly more important than affecting the <strong>speed</strong> of progress. However, I think there are some situations where affecting speed is the most effective way to affect direction, like:</p>\n<ul>\n<li><p>When the rate of progress in one area affects the directional outcome of another area (e.g. increasing AI safety technology more quickly improves the expected directional outcome of AI)</p>\n</li>\n<li><p>When our current situation is very risky (e.g. if you think while humans remain solely on earth, we're really likely to destroy ourselves, but you don't want that to happen, so you try to get us to colonize other planets as soon as possible, decreasing our likelihood of extinction)</p>\n</li>\n</ul>\n", "parentCommentId": null, "user": null}, {"_id": "86hRwFE3QWqpyanGC", "postedAt": "2014-10-15T16:14:52.308Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>Do you mean 'affecting the speed of a subfield of tech is the most effective way to affect the direction of movement of the centre of gravity of our tech capabilities'? If so, I agree.</p>\n<p>Speeding up a particular tech counts as differential tech development.</p>\n", "parentCommentId": "h6eYHRQP5fZ3pDtwh", "user": {"username": "RyanCarey"}}, {"_id": "HNuKjhGDHvoum3Aoq", "postedAt": "2014-10-15T16:41:21.367Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>(epistemic status - A first draft, probably needs more thought and reflection in the future)</p>\n<p>I don't necessarily disagree with your - but I am much more likely to endorse short term interventions that address poverty and short term technological improvements which reduce suffering for a few reasons</p>\n<p>I consider current suffering as extremely negative and (comparatively) easy to fix.  I find serious suffering - of the kind a lot of EA interventions try to prevent - absolutely intolerable disgust and fairness foundations (as Haidt would term them) on a deeply primal level to see needless suffering. Improving the speed of progress in attacking this suffering seems to be a big deal to me, because with some exceptions (defeat of death and ageing), I consider the life of those of us lucky enough to be born WIERD nations to already be pretty damn good - and that the welfare difference between me and a child growing up in Mogadishu may even be incommensurably greater than the difference between me and long run future saturated people. So I am much more concerned with current suffering, because current suffering is big and future suffering is likely - in the very long run, to be either catastrophic or very small. </p>\n<p>The logical followup to that point is that I am interested in directional changes (as you are) that help ensure that we end up at &quot; very small&quot; rather than catastrophic - but I am very sceptical about our ability to measure and show what current research is effective at making such changes. </p>\n<p>I may be wrong about that scepticism, but even if I am I think that globally suffering has a big negative effect on the likleyhood of people investing in decision making or directional investments -  it is hard to plan your mortgage payments when you have a broken leg. Given how comparatively simple it seems to me to be is be too fix our collective broken legs I think even those people who want us to plan our mortgage payments should consider it a high priority to get a splint now so that we can actually plan our mortgage payments without constantly worrying or facing searing pain from our broken leg.  On a similar point,  I disagree with you that &quot;faster progress leading to niceness&quot; is very speculative. Stephen Pinker's excellent &quot;The Better Angels of our Nature&quot; - seems a good reference for this - I think it establishes fairly clearly that moral and technological progress has made us less warlike and less violent as a species.</p>\n", "parentCommentId": null, "user": {"username": "AlasdairGives"}}, {"_id": "e8ggdpCfAFALf39yv", "postedAt": "2014-10-15T17:22:45.742Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>&quot;2.  Why should progress continue indefinitely? Maybe there will be progress until 2100, and the level of sophistication in that year will determine the entire future?</p>\n<p>This scenario just seems to strain plausibility. Again, almost all ways that progress could plausibly stop don't depend on the calendar year, but are driven by human activities (and presumably some intermediating technological progress).&quot;</p>\n<p>I think this is where I disagree with you most: I don't think this strains plausibility at all, and in fact I think the statement as given is basically true up to the year. Examples of events that seem determined by 'calendar year' rather than by the level of progress, but where likelihood of survival seems dependent on the level of progress:</p>\n<ol>\n<li><p>First, second, or later contact with intelligent alien life (especially in a 'they find us' scenario, rather than vice-versa).</p>\n</li>\n<li><p>Asteroid impacts, super-volcanoes, and another 'natural' disasters.</p>\n</li>\n</ol>\n<p>I'm sure people more imaginative than me can think of others.</p>\n<p>You mentioned the latter category explicitly and noted that they are very rare. I agree. But 'very rare' still means 'mathematically guaranteed eventually', so eventually there will be a year X where the level of sophistication does in fact determine the entire future. 1 concerns me more than 2 though, since it seems more of a 'make-or-break' event, and more guaranteed (Aside: I'm always a bit surprised that contact with alien life doesn't seem to turn up much in discussions of the very-far-future. It's normally my primary consideration when thinking that far ahead, since I view it as incredibly important to what the long-run looks like and incredibly likely given sufficient time).</p>\n<p>This is obviously related to a broader disagreement, which is whether the threats and constraints on humanity are primarily external (disease, cosmic issues, aliens, maybe even resource issues) or internal (AI, nukes, engineered microbes). I lean strongly towards external. </p>\n<p>Against an unknown external threat, speeding up broad-based progress is a sensible response. It has so far been the case the primary things hurting humanity have been external, so up to now broad-based progress has been very powerful. You don't appear to disagree with this. Your actual disagreement is, I think, contained in the sentence &quot;For better or worse, almost all problems with the potential to permanently change human affairs are of our own making.&quot;</p>\n<p>Arguing against that would take me somewhat out of the scope of this thread and make this comment even longer than it already is. But it seems clear to me that that assertion is not one that many of those you are arguing against would agree with, and without it, I don't think the rest of your argument holds.</p>\n", "parentCommentId": null, "user": {"username": "AGB"}}, {"_id": "yPu53TmJhK9EpbFDJ", "postedAt": "2014-10-15T17:24:28.180Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>That sounds like what I mean, although I'm not quite sure what you mean by 'centre of gravity' in this context. But yes, this is &quot;differential tech development&quot; through &quot;speeding up a particular tech.&quot; So <strong>direction</strong> is still the the goal (just reached less directly).</p>\n", "parentCommentId": "86hRwFE3QWqpyanGC", "user": null}, {"_id": "joNx7E9J6BFaQJBQR", "postedAt": "2014-10-15T21:13:47.471Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>To bring up population ethics again, how does taking a long-term view affect the desirability/cost-effectiveness of population-changing interventions? Because there's little negative feedback between population size and population growth in today's world (it's not Malthusian), small changes in population will last a long time and result in a lot of extra lives. For example, if there's slight negative feedback and population changes decay by about 10% a generation, an extra person today leads eventually to ~10 extra people.</p>\n<p>I find this a bit counterintuitive (but probably not wrong), since it means that considering a long time horizon dramatically increases the benefit of saving lives, compared to poverty reduction or treatment of non-fatal diseases. In the extreme case of a total utilitarian who values saving and creating lives symmetrically, they would value saving a life in the tens of millions of dollars instead of a few million dollars (again assuming a ~10% &quot;decay rate&quot;).</p>\n", "parentCommentId": null, "user": null}, {"_id": "zDZiRS6cLmvH4rY3a", "postedAt": "2014-10-16T01:55:04.141Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>Can't get it how the content of your essay relates to the notion of this extreme altruism, which is by definition is practical concept having its goal to do more now rather than in 10 years, let alone 100 years. In last 100 years millions of pages have been written by academics and lay people, yet 0.01% of that &quot;mental sweat&quot; had constructive utility to the humankind.</p>\n", "parentCommentId": null, "user": {"username": "Ilya"}}, {"_id": "JDD7Yhx2GQprTYRwz", "postedAt": "2014-10-16T01:56:27.677Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>Some current things that are trying to push on &quot;differential progress&quot;, if I understand you right:</p>\n<ul>\n<li>The <a href=\"http://www.npr.org/blogs/parallels/2014/04/02/297839429/-so-you-think-youre-smarter-than-a-cia-agent\">Good Judgement Project</a></li>\n<li>Political lobbying to promote regulation of biotech &amp; nanotech research, a la CEA's/FHI's <a href=\"http://www.fhi.ox.ac.uk/research/global-priorities-project/\">Global Priorities Project</a></li>\n<li>AI safety research</li>\n<li>Animal rights activism (where the emphasis is on values shift/<a href=\"http://press.princeton.edu/titles/9434.html\">expanding circles</a>)</li>\n</ul>\n<p>Does that look right? What else would you add? </p>\n<p>(Paul, I think I've heard you talk before about trying to improve institutional quality - do you know of anyone you think is doing this well?)</p>\n", "parentCommentId": null, "user": {"username": "HelenToner"}}, {"_id": "hpXuZujPEZyGy6EYj", "postedAt": "2014-10-16T05:25:45.050Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>My estimates for [1] and [2] together are less than 0.01% per year (for [1], they are very much lower!) So the quantitative effect of speeding up progress, via these channels, is quite small. You would have to be very pessimistic about the plausible trajectory-changing impacts of available interventions before such a small effect was relevant.</p>\n<p>I can believe that most of the problems faced by people today are external, and indeed this is related to why I find this disagreement such a painful one. But why do you think that the long-term constraints are external? I've never seen a really plausible quantitative argument for this. Resource limitations are the closest, though (1) I'm quite skeptical as a very long-term factor, but this comes down mostly to views on the likely rate of technological progress over the next centuries, and (2) describing those as &quot;external&quot; rather than &quot;a consequence of progress&quot; is already pushing it.</p>\n<p>I think that the &quot;man-made&quot; troubles have already easily surpassed the natural worries, via the risk of nuclear annihilation and anthropogenic climate change. Do you disagree with this? What natural problems do you think might be competitive? The unknown unknown?</p>\n", "parentCommentId": "e8ggdpCfAFALf39yv", "user": {"username": "Paul_Christiano"}}, {"_id": "kHo53Ad2dRvvR9Ryt", "postedAt": "2014-10-16T10:29:12.446Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>Thanks for a great post. I agree with many of the points.</p>\n<p>I do think that we still need to better understand how short-run changes interact with long-run expectations. In cases where we can effect surprisingly large changes to the world today (and poverty interventions have at least a plausible claim to this), the long-run effects may also be large. While I agree that differential intellectual progress is likely an order of magnitude more important than absolute intellectual progress, the opportunities available to us may in some cases be enough to recover the difference the other way. We should be prepared to look for such opportunities (although we should also be a bit sceptical if we haven't also looked for similarly good opportunities in differential progress).</p>\n", "parentCommentId": null, "user": {"username": "Owen_Cotton-Barratt"}}, {"_id": "pvkLu6ReMp3DGSCYK", "postedAt": "2014-10-16T11:22:05.567Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>Edited for TLDR: </p>\n<p>TLDR; We can robustly increase the speed of progress. We can also establish that, contrary to the argument above, increasing the speed of progress has non-trivial very-long-run value. I don't see how we can robustly change the direction of progress. I also don't see how we could robustly know that the direction we move progress in is valuable, even if we could robustly change the direction of progress.</p>\n<p>At the risk of stating something we both know, 0.01% per year is not at all 'small' in this context. My estimate would actually be lower, and I still think my argument holds. That's because I think progress will continue for many thousands of years. I agree with you that eventually it has to stop or dwindle into insignificance. But all I need to establish is that there is a non-trivial chance that it does not stop <em>before</em> one of the make-or-break events. If progress is currently set to continue for at least 1000 more years and there is indeed a 0.01% chance per year, there is at least a 10% chance of that scenario (the one that you said 'strains plausibility'). 1000 years just isn't a very long time. And once I have that, speeding up progress becomes very valuable again.</p>\n<p>&quot;You would have to be very pessimistic about the plausible trajectory-changing impacts of available interventions before such a small effect was relevant.&quot;</p>\n<p>I am actually very pessimistic about these, because it's not clear to me that we have any examples of trying to do more of this on the margin working. We do have examples of attempts to speed up progress on the margin working.</p>\n<p>Assuming that by 'anthropogenic climate change' you essentially mean rising CO2 levels, I don't actually rate climate change as an x-risk, so it's not obvious to me that it belongs in this discussion. I rate it as something which could cause a great deal of suffering and, as a side-effect of that, slow down progress, but not as something that brings about the 'end of the world as we know it'. If it is an x-risk, then in a sense my response is 'well, that's too bad', since I also view it as inevitable; all the evidence I'm aware of suggests that we collectively missed the boat on this one some time ago. In fact, because we've missed the boat <em>already</em>, the best thing to do right now to combat it is very likely to be to speed up progress so as to be better placed to deal with the problems when they come.</p>\n", "parentCommentId": "hpXuZujPEZyGy6EYj", "user": {"username": "AGB"}}, {"_id": "LE6twEZE5Rag4wGmZ", "postedAt": "2014-10-16T15:58:08.877Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>In the worst case, you personally saving money and spending it later (in the worst case, to hasten progress at a time when the annual risk of doom has increased!) seems very likely to beat 0.01%, unless you have pretty confident views about the future. </p>\n<p>I think research on improving institutional quality, human cognition, and human decision-making, also quite easily cross this bar, have had successes in the past, and have opportunities for more work on the margin. I've <a href=\"http://rationalaltruist.com/2013/02/18/improving-decision-making/\">written about</a> why I think these things would constitute positive changes. But it's also worth pointing out that if you think there is a 0.01% / year chance of doom now, then improvements in decision-making can probably be justified just by short-term impacts on probability of handling a catastrophe soon</p>\n<p>How long progress goes on for before stopping seems irrelevant to its value, according to the model you described. The value of an extra year of progress is always the per annum doom risk. Suppose that after some number of years T the per annum doom probability drops to 0. Then speeding up progress by 1 year reduces that number to T-1, reducing the cumulative probability of doom by the per annum doom probability. And this conclusion is unchanged if the doom probability continuously decreases rather than dropping to 0 all at once, or if there are many different kinds of doom, or whatever. It seems to be an extremely robust conclusion. Another way of seeing this is that speeding up progress by 1 year is equivalent to just pausing the natural doom-generating processes for a year, so naturally the goodness of a year of progress is equal to the badness of the doom-generating processes.</p>\n<p>If you believe a 0.01% / year chance of doom by natural catastrophes, addressing response capabilities to those catastrophes in particular generally seems like it is going to easily dominate faster progress. On your model reducing our probability of doom from natural disasters by 1% over the next year is going to be comparable to increasing the overall rate of progress by 1% over the next year, and given the relative spending on those problems it would be surprising if the latter was cost-effective. I can imagine such a surprising outcome for some issues like encountering aliens (where it's really not clear what we would do), but not for more plausible problems like asteroids or volcanos (since those act by climate disruptions, and there are general interventions to improve society's robustness to massive climate disruptions). </p>\n<p>Whether we've missed the boat on climate change or not, it would undermine your claim that historically the main problems have been nature-made and not man-made (which I find implausible), which you were invoking to justify the prediction that the same will hold true in the future. I'm also willing to believe the risk of extremely severe outcomes is not very large, but you don't have to be very large to beat 0.01% / year.</p>\n<p>One reason I'm happy ignoring aliens is that the timing would have to work out very precisely for aliens to first visit us during any particular 10k year period given the overall timescales involved of at least hundreds of millions of years and probably billions. There are further reasons I discount this scenario, but the timing thing alone seems sufficient (modulo simulation-like hypotheses where the aliens periodically check up on Earth to see if it has advanced life which they should exterminate, or some other weird thing).</p>\n", "parentCommentId": "pvkLu6ReMp3DGSCYK", "user": {"username": "Paul_Christiano"}}, {"_id": "5NYEroMj5pGmCbkD6", "postedAt": "2014-10-16T16:28:55.654Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>The argument for differential progress has been made before by Bostrom a few times and Beckstead as well.</p>\n<p><a href=\"http://www.existential-risk.org/figure5.png\">http://www.existential-risk.org/figure5.png</a> </p>\n<p>I can see six strategies which may prove fruitful to nourish differential technological progress: </p>\n<p>1) Increasing safety savvy insight - e.g. helping with the AI control problem, perfecting our understanding of moral psychology, and of cultural evolution.  </p>\n<p>2) Decreasing rates of progress in dangerous areas - e.g. decelerating the brain emulation project, passing legislation anti-AI progress etc...</p>\n<p>3) Diminishing the incentives which stimulate progress in undesirable areas - e.g. if aging is cured, chronologically old individuals no longer would have an incentive to accelerate the intelligence explosion. </p>\n<p>4) Using a Scorched Earth strategy - e.g. if surpassing a threshold of average wealth would be risky, perhaps because some individuals would have enough power start a world-wrecking cascade, then burning resources now would guarantee that the future will be safer by being poorer. </p>\n<p>5) Using a differentiated Scorched Earth strategy - e.g. try to predict which assets/valuables will be in the hands of those individuals who can start world destroying cascades in the future, and destroy non-liquid resources from that pool now. Whether non-liquid because rare, as enriched uranium, or non-liquid because hard to exchange, as private islands or old churches, the crucial consideration is whether destroying these resources now will counterfactually impede similar resources of being used at the time of their maximum destructive potential by these agents. </p>\n<p>6) Finding whether there are more strategies not considered among the previous 5. </p>\n<p>Which of these, if any, do you see as the lower hanging fruit for EAs? and why?</p>\n", "parentCommentId": null, "user": {"username": "Diego_Caleiro"}}, {"_id": "YyC2ovHNrkLHCqayR", "postedAt": "2014-10-16T17:18:40.000Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>&quot;In the worst case, you personally saving money and spending it later (in the worst case, to hasten progress at a time when the annual risk of doom has increased!) seems very likely to beat 0.01%, unless you have pretty confident views about the future.&quot;</p>\n<p>I don't disagree on this point, except that I think there are better ways to maximise 'resources available to avert doom in year 20xx' than simply saving money and gaining interest.</p>\n<p>&quot;How long progress goes on for before stopping seems irrelevant to its value, according to the model you described. The value of an extra year of progress is always the per annum doom risk.&quot;</p>\n<p>I basically agree, and I should have made that explicit myself. I only invoked specific numbers to highlight that 0.01% annual doom risk is actually pretty significant once we're working on the relevant timescales, and therefore why I think it is plausible/likely that there will indeed be a year, one day, where the level of sophistication determines the entire future.</p>\n<p>&quot;Whether we've missed the boat on climate change or not, it would undermine your claim that historically the main problems have been nature-made and not man-made (which I find implausible), which you were invoking to justify the prediction that the same will hold true in the future.&quot;</p>\n<p>That wasn't the prediction I was trying to make at all, though on re-reading my post I can see why you might have thought I was. But the converse of 'almost all problems...are of our own making' is not 'most problems are not of our own making'. There's a large gap in the middle there, and indeed it's not clear to me which will dominate in the future. I think external has dominated historically and marginally think it still dominates now, but what I'm much more convinced of is that we have good methods to attack it.</p>\n<p>In other words, external doesn't need to be much bigger than internal in the future to be the better thing to work on, all it needs to be is (a) non-trivial and (b) more tractable.</p>\n<p>The rest of your post is suggesting specific alternative interventions. I'm open to the possibility that there is some specific intervention that is both more targeted and that is currently being overlooked. I think that conclusion is best reached by considering interventions or classes of interventions one at a time. But as a prior, it's not obvious to me that this would be the case.</p>\n<p>And on that note, do you think that the above was the case in 1900? 1800? Has it always been the case that focusing on mitigating a particular category of risk is better than focusing on general progress? Or have we passed some kind of tipping point which now makes it so?</p>\n", "parentCommentId": "LE6twEZE5Rag4wGmZ", "user": {"username": "AGB"}}, {"_id": "PjDy6vPMmJSBRdDWh", "postedAt": "2014-10-16T17:18:58.036Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>Hi Ilya,\nI think the reason that Paul is discussing this is because he values everyone equally, regardless of when they exist. And thus he is trying to figure out what actions people should take now in order to maximise the impact he can have on everyone in the world at all times.  I agree with your sentiment that much academic work has had little to no utility to humankind (the median published paper is cited once apparently), however there are some questions such as &quot;how can I do as much good as possible&quot; that are significantly understudied, and so I think Paul is contributing there.  Additionally I know many people who are pursuing technology entrepreneurship and so articles like this one will help them choose which areas they should be working in.  </p>\n", "parentCommentId": "zDZiRS6cLmvH4rY3a", "user": {"username": "Niel_Bowerman"}}, {"_id": "AdZjSNrtoJingupbJ", "postedAt": "2014-10-16T17:38:08.028Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<blockquote>\n<p>And on that note, do you think that the above was the case in 1900? 1800? Has it always been the case that focusing on mitigating a particular category of risk is better than focusing on general progress? Or have we passed some kind of tipping point which now makes it so?</p>\n</blockquote>\n<p>At those dates I think focusing on improving institutional decision-making would almost certainly beat trying to mitigate specific risks, and might well also beat focusing on general progress.</p>\n", "parentCommentId": "YyC2ovHNrkLHCqayR", "user": {"username": "Owen_Cotton-Barratt"}}, {"_id": "K5hix2rgCYinqSmev", "postedAt": "2014-10-16T17:44:09.316Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>What would be an example? It's quite possible I don't disagree on this, because it's very opaque to me what 'improving institutional decision making' would mean in practice.</p>\n", "parentCommentId": "AdZjSNrtoJingupbJ", "user": {"username": "AGB"}}, {"_id": "Gk5YaaFrx7Ek6GcaK", "postedAt": "2014-10-16T18:03:42.130Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>You talk about progress today being of macroscopic relevance if we get an exogenous make-or-break event in the period where progress is continuing. I think it should really be if we get such an event in the period where progress is continuing exponentially. If we've moved into a phase of polynomial growth (plausible for instance if our growth is coming from spreading out spatially) then it seems less valuable. I'm relying here on a view that our (subjective) chances of dealing with such events scale with the logarithm of our resources. I don't think that this changes your qualitative point.</p>\n<p>I do think that endogenous risk over the next couple of centuries is of at least comparable size as exogenous risk over the period before exponential growth. I think that increasing the resources devoted to dealing with endogenous risk by 1% will reduce these risks by a similar amount that increases prosperity by 1% will reduce long-term exogenous risks. And I think it's probably easier to get that 1% increase in the first case than the second one, in large part because there are a lot fewer people already trying to do that.</p>\n", "parentCommentId": "pvkLu6ReMp3DGSCYK", "user": {"username": "Owen_Cotton-Barratt"}}, {"_id": "2FcFHmSW2v7pgHSRQ", "postedAt": "2014-10-16T18:07:17.685Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>Paul gives some examples of things he thinks are in this category today.</p>\n<p>If we go back to these earlier dates, I guess I'd think of working to secure international cooperation, and perhaps to establishing better practices in things like governments and legal systems.</p>\n<p>I think it's hard to find easy concrete things in this category, as they tend to just get done, but with a bit of work progress is possible.</p>\n", "parentCommentId": "K5hix2rgCYinqSmev", "user": {"username": "Owen_Cotton-Barratt"}}, {"_id": "RcCFcuNHHaBj8ohpZ", "postedAt": "2014-10-16T18:33:21.357Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>Research on differential technological development seems much less crowded than the sorts of interventions advocated by proponents of the &quot;progress and prosperity&quot; argument that Paul criticizes.  So unless one regards that area of research as singularly intractable, it seems its scoring much more highly on both the importance and crowdedness dimensions should make it a more promising cause overall.</p>\n", "parentCommentId": "kHo53Ad2dRvvR9Ryt", "user": {"username": "Pablo_Stafforini"}}, {"_id": "NT2un32eJGch66dMn", "postedAt": "2014-10-16T20:03:48.891Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>I agree, but there's an argument in favour of progress that you don't mention. If we magically replace say 2015 with 2016, then we get one fewer year of 21st century conditions, and one more year of saturated-world conditions. If we think that an incredibly valuable saturated world is likely, then an extra year of it instead of the 21st century is well worth it.</p>\n", "parentCommentId": null, "user": {"username": "ciphergoth"}}, {"_id": "72ZnkuLFbzwfxnK32", "postedAt": "2014-10-16T23:56:24.450Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>Firstly, I think this is the best post I've read in a while.</p>\n<p>However, I notice I am confused. It seems that much of your argument is basically &quot;the future is very big; as such, we should invest now rather than consuming, because</p>\n<ul>\n<li>ROI &gt; 0</li>\n<li>Discount Rate = 0</li>\n</ul>\n<p>...where in this case, investing in basically anything other than Xrisk counts as consumption.</p>\n<p>However, this argument pulls roughly equally against many things. (Uniform) economic growth is not very much worse than deworming, or animal welfare, or civil liberties - they're all basically irrelevant compared to the astronomic waste. So I'm not sure of the reason for your emphasis in this article.</p>\n<p>Of course, this has clear implications for the virtues of non-uniform economic growth/</p>\n", "parentCommentId": null, "user": {"username": "Dale"}}, {"_id": "AXHih8ZXGEJgbaCWW", "postedAt": "2014-10-16T23:56:29.315Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>Or a light-cone that started a year earlier, and thus permanently one extra light year in radius.</p>\n", "parentCommentId": "NT2un32eJGch66dMn", "user": {"username": "Dale"}}, {"_id": "GLfs3R4SrQC9Rzj3b", "postedAt": "2014-10-17T01:54:55.487Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>Another response to this is Nick Bostrom's <a href=\"http://www.nickbostrom.com/astronomical/waste.html\">astronomical waste argument</a>.</p>\n<p>tl;dr: The resources in our light cone will decrease even if we don't make use of them. It's quite plausible that even a few months of a massive, highly advanced civilization could have more moral worth than the total moral worth of the next 500 years of human civilization. So  accelerating development by even a small amount, allowing an eventual advanced civilization to be slightly larger and last slightly longer, is still massively important relative to other non x-risk causes.</p>\n", "parentCommentId": null, "user": {"username": "AlexRichard"}}, {"_id": "4wrPDd64Rry42X3xq", "postedAt": "2014-10-17T10:16:51.493Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>Yes, this comes down to empirics: the fraction of extra resources we can reach by starting earlier is very small, so most of our long-term impact comes from nudges to the probability of having a long future at all.</p>\n<p>It seems like the empirical question could have come out the other way, and then progress would be more important.</p>\n", "parentCommentId": "AXHih8ZXGEJgbaCWW", "user": {"username": "Owen_Cotton-Barratt"}}, {"_id": "CAyGZdFecfvQrL5Wc", "postedAt": "2014-10-18T16:02:25.579Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>I discussed some quantitative estimates of this <a href=\"http://rationalaltruist.com/2013/04/30/astronomical-waste/\">here</a>, with a general argument for why it would be small in light of model uncertainty. Overall it seems at least a few orders of magnitude smaller than other issues that favor faster progress.</p>\n", "parentCommentId": "GLfs3R4SrQC9Rzj3b", "user": {"username": "Paul_Christiano"}}, {"_id": "Jg5xtGeu8PWcF7rtS", "postedAt": "2014-10-18T16:08:02.600Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>I think that most good things people do push on differential progress in one way or another, it's just a different standard for evaluation. Those do stand out as things that contribute particularly to differential progress.</p>\n<p>I would guess that on average progress in the social sciences is a net benefit in terms of differential progress, while progress in the hard sciences is a net cost in terms of differential progress. I think the bulk of improvement in institutions comes from people working within organizations trying to help them run better (with the positive developments then propagated through society), and then economists and the social sciences in a distant second.</p>\n", "parentCommentId": "JDD7Yhx2GQprTYRwz", "user": {"username": "Paul_Christiano"}}, {"_id": "bb33Shvuf9Zd6RCSK", "postedAt": "2014-10-20T17:38:37.159Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>I'm relatively new to the area of existential risk, and come more from the development economics/ global health side of things.  I find this argument really interesting, and have a few questions and comments.  </p>\n<p>(1) Economic development in emerging economies generally leads to a demographic transition where birth rates eventually stabilize at or below the replacement rate.  It seems to me that if this transition were hastened, it could lead to a smaller steady state population in the long term and lower long term resource consumption (even if consumption is accelerated in the near term).  I'm not sure about the moral implications of eliminating people from a theoretical future, but it seems to me it would be preferable to have a smaller population with a better quality of life, than a large population with a poor quality of life.  </p>\n<p>(2)  You write: \n&quot;With the possible exceptions of anthropogenic climate change and a particularly bad nuclear war, we barely even have the ability to really mess things up today: it appears that almost all of the risk of things going terribly and irrevocably awry lies in our future. Hastening technological progress improves our ability to cope with problems, but it also hastens the arrival of the problems at almost the same rate.&quot;\n-Do know of any research that has tried to quantify the relative rates/magnitudes of problems generated vs. problems solved?\n-Large quality of life increases in developing economies could come from application of existing technologies where the risks, as you note, are relatively small/pretty well understood.  Would you support the use of existing technology for development?  </p>\n<p>(3) Has anyone made an argument for existential risk mitigation based on recent history?  For example, trying to look forward with the limited perspective of someone in 1900.  Obviously this would be very speculative, but I think it could help make the concepts you are discussing more concrete to outsiders.  Also, doing historical research into warnings about, say, the dangers of nuclear physics could help us understand the challenges of practically implementing policies that are theoretically correct.  </p>\n<p>I generally agree that more resources need to be devoted to existential risk mitigation, but I don't have a very good idea of where the bottlenecks are -- philosophy, theory, or practical implementation?  </p>\n", "parentCommentId": null, "user": {"username": "Phil_Thomas"}}, {"_id": "vBfCoezNzvEsdz2MN", "postedAt": "2015-01-05T21:17:43.415Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>Paul hello. I think I'm missing something crucial in your argument. You are saying that investing in technological progress is worse than... what? I understand that there are black swan technologies which create X-risk therefore we want to invest into safety research rather than into speeding up the technology itself. Are you saying anything besides that? In a universe without risk, would you still be against prioritizing progress?</p>\n<p>Why is the long term asymptotics crucial to the question? Yes, by accelerating progress you are &quot;merely&quot; translating everything in time. So what? The result of skipping from 1800 to 1900 is immense utility gain whatever is the long term scenario.</p>\n", "parentCommentId": null, "user": {"username": "Squark"}}, {"_id": "4z28wD2FLQ9zGJ3y3", "postedAt": "2015-02-07T11:58:32.075Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>1) Seems like a good start, as its likely to draw together people with a common concern, but its v unlikely to stop things. Research capabilities are dispersed, and if you're saying that the stepping on each other's toes effect is going to outweigh the standing on the shoulders of giants effect, then researchers in different parts of world society with different agendas will want to get going with it. It would need enough of the right people to subscribe to this for log enough to prevent the technology. Unlikely. But might buy time.</p>\n<p>Differentiated scorched earth isn't different in consequence than 2) - both are kinds of regulation. One official and centralised, one with the possibility of being unofficial and dispersed. The drawback of the scorched earth strategy is that it's irreversable. The drawback of the regulatory strategy is that its game-able.</p>\n<p>curing aging might be one of the threats to humanity as we know it? Mortality = vulnerability = dependence on others = good society? Also not dependable strategies to reduce incentives if they're reliant on new tech fixes, as tech fixes are very hard to predict or encourage and possibly carry their own unknown risks even if we could do those things (so there's a level at which you could be introducing more risk than risk control and its hard to figure out which is which)</p>\n<p>I personally think a harm control strategy / centralised regulatory + intelligence function is our best bet for differentiated progress. This also comes with the side-benefit of forcing debate and norms in scientific research to allign or not and the public being brought in on it, and they're usually in favour of regulating against scary risks even when they're too small (unless they're framed as protecting us from other human beings).</p>\n", "parentCommentId": "5NYEroMj5pGmCbkD6", "user": {"username": "tomstocker"}}, {"_id": "Ld6nceD5ak6iE2HKh", "postedAt": "2017-02-07T14:01:57.159Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>I think the point is: improving general (intellectual) progress is significantly less effective than improving differential (=specific) intellectual progress. Am I right?</p>\n", "parentCommentId": "vBfCoezNzvEsdz2MN", "user": {"username": "JeroenWauters"}}, {"_id": "9JAXuTxjfG8Edi6kp", "postedAt": "2021-12-02T01:32:26.907Z", "postId": "L9tpuR6ZZ3CGHackY", "htmlBody": "<p>I think this post contributes something novel, nontrivial, and important, in how EA should relate to economic growth, \"Progress Studies,\" and the like. Especially interesting/cool is how this post entirely predates the field of progress studies.<br><br>I think this post has stood the test of time well.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Linch"}}]