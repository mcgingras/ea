[{"_id": "bJkKgwKjzg6HBzqA9", "postedAt": "2024-02-25T18:09:43.637Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<p>Nuclear risk philanthropy is about 30M/y, it seems you are comparing overall nuclear risk effort to philanthropic effort for AI?</p>\n<p>In terms of philanthropic effort AI risk strongly dominates nuclear risk reduction.</p>\n", "parentCommentId": null, "user": {"username": "jackva"}}, {"_id": "oXdAyk9jDbbWrbHeF", "postedAt": "2024-02-25T19:28:47.239Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<p>Hi Johannes,</p><p>My intention is comparing quality-adjusted spending on decreasing nuclear and AI extinction risk, accounting for all sources (not just philanthropic ones).</p><blockquote><ul><li>I consider the annual spending on decreasing extinction risk from AI is 50.6 (= 4.04*10^9/(79.8*10^6)) times that on decreasing extinction risk from nuclear war. I determined this from the ratio between:<ul><li>4.04 <a href=\"https://www.nist.gov/pml/owm/metric-si-prefixes\"><u>G</u></a>$ (4.04 billion USD) on nuclear risk in 2020, which I <a href=\"https://forum.effectivealtruism.org/posts/tvTqRtMLnJiiuAep5/how-to-determine-distribution-parameters-from-quantiles\"><u>got</u></a>&nbsp;from the mean of a lognormal distribution with 5th and 95th percentile equal to 1 and 10 G$, corresponding to the lower and upper bound guessed in 80,000 Hours\u2019 <a href=\"https://80000hours.org/problem-profiles/nuclear-security/\"><u>profile</u></a>&nbsp;on nuclear war. \u201cThis issue is not as neglected as most other issues we prioritise. Current spending is between $1 billion and $10 billion per year (quality-adjusted)\u201d&nbsp;(see <a href=\"https://80000hours.org/problem-profiles/nuclear-security/%23fn-1\"><u>details</u></a>).</li><li><a href=\"https://docs.google.com/spreadsheets/d/1C_QDlzZynG00u7XVHy91Tii9qOl-dk8KtxiYcrd_ZYc/edit#gid=0&amp;range=J11\"><u>79.8 M$</u></a> <a href=\"https://forum.effectivealtruism.org/posts/XdhwXppfqrpPL2YDX/an-overview-of-the-ai-safety-funding-situation\"><u>on</u></a>&nbsp;\u201cAI safety research that is focused on reducing risks from advanced AI\u201d in 2023.</li></ul></li></ul></blockquote>", "parentCommentId": "bJkKgwKjzg6HBzqA9", "user": {"username": "vascoamaralgrilo"}}, {"_id": "kt4C7GjukC2jCBP9B", "postedAt": "2024-02-25T19:50:01.107Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<p>I can't open the GDoc on AI safety research.</p><p>But, in any case, I do not think this works, because philanthropic, private, and government dollars are not fungible, as all groups have different advantages and things they can and cannot do.<br><br>If looking at all resources, then 80M for AI safety research also seems an underestimate as this presumably does not include the safety and alignment work at companies?<br>&nbsp;</p>", "parentCommentId": "oXdAyk9jDbbWrbHeF", "user": {"username": "jackva"}}, {"_id": "ZwetXwgyyvxHEpudP", "postedAt": "2024-02-25T22:32:37.186Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<blockquote><p>But, in any case, I do not think this works, because philanthropic, private, and government dollars are not fungible, as all groups have different advantages and things they can and cannot do.</p></blockquote><p>I think I should be considering all sources of funding. Everything else equal, I expect a problem A which receives little philanthropic funding, but lots of funding from other sources, to be less pressing than a problem B which receives little funding from both philanthropic and non-philanthropic sources. The difference between A and B will not be as large as naively expected because philanthropic and non-philanthropic spending are not fungible. However, if one wants to define neglectedness as referring to just the spending from one source, then the scale should also depend on the source, and sources with less spending will be associated with a smaller fraction of the problem.</p><p>In general, I feel like the case for using the importance, tractability and neglectedness <a href=\"https://forum.effectivealtruism.org/topics/itn-framework\">framework</a> is stronger at the level of problems. Once one starts thinking about considerations within the cause area and increasingly narrow sets of interventions, I would say it is better to move towards cost-effectiveness analyses.</p><blockquote><ul><li>So the nearterm annual extinction risk per annual spending for AI risk is 59.8 M (= 1.69*10^6*35.4) times that for nuclear risk.</li></ul></blockquote><p>Yet, given the above, I would say one should a priori expect efforts to decrease AI extinction risk to be more cost-effective at the current margin than ones to decrease nuclear extinction risk. Note: the sentence just above already includes the correction I will mention below.</p><blockquote><p>I can't open the GDoc on AI safety research.</p></blockquote><p>Sorry! I have fixed the link now.</p><blockquote><p>If looking at all resources, then 80M for AI safety research also seems an underestimate as this presumably does not include the safety and alignment work at companies?</p></blockquote><p>It actually did not include spending from for-profit companies. I thought it included because I had seen they <a href=\"https://forum.effectivealtruism.org/posts/XdhwXppfqrpPL2YDX/an-overview-of-the-ai-safety-funding-situation#For_profit_companies\">estimated</a> just a few tens of millions of dollars coming from them:</p><figure class=\"table\" style=\"height:244.768px;width:656px\"><table style=\"background-color:rgb(255, 255, 255);border:1px double rgb(179, 179, 179)\"><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Company name</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Number of employees&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/XdhwXppfqrpPL2YDX/an-overview-of-the-ai-safety-funding-situation#fnfjilm93blz\"><sup>[1]</sup></a></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">AI safety team size (estimated)</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Median gross salary (estimated)</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Total cost per employee (estimated)</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Total funding contribution (estimated)</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">DeepMind</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">1722</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">5-20</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">$200k</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">$400k</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">$1.6-15m</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">OpenAI</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">1268</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">5-20</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">$290k</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">$600k</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">$2.9-20m</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Anthropic</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">164</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">10-40</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">$360k</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">$600k</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">$6.2-32m</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Conjecture</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">21</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">5-15</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">$150k</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">$300k</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">$1.2-5.5m</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Total</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">$32m</td></tr></tbody></table></figure><p>I have now modified the relevant bullet in my analysis to the following:</p><blockquote><ul><li>114 M$ (= (79.8 + 32 + 2*1)*10^6)&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/XdhwXppfqrpPL2YDX/an-overview-of-the-ai-safety-funding-situation\"><u>on</u></a> \u201cAI safety research that is focused on reducing risks from advanced AI\u201d in 2023:<ul><li><a href=\"https://docs.google.com/spreadsheets/d/1C_QDlzZynG00u7XVHy91Tii9qOl-dk8KtxiYcrd_ZYc/edit#gid=0&amp;range=J11\"><u>79.8 M$</u></a> from the National Science Foundation (<a href=\"https://www.nsf.gov/\"><u>NSF</u></a>), LTFF, Open Philanthropy, SFF and \u201cother\u201d.</li><li>\u201c~$32m per year\u201d&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/XdhwXppfqrpPL2YDX/an-overview-of-the-ai-safety-funding-situation#Summary_of_other_sources_of_funding\"><u>from</u></a> \u201cfor-profit companies\u201d (Anthropic, Conjecture, Google Deepmind and OpenAI).</li><li>2 times \u201cprobably at least $1m per year\u201d&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/XdhwXppfqrpPL2YDX/an-overview-of-the-ai-safety-funding-situation#Summary_of_other_sources_of_funding\"><u>from</u></a> \u201cindividual donors\u201d.</li></ul></li></ul></blockquote><p>My point remains qualitatively the same, as the spending on decreasing AI extinction risk only increased by 42.9 % (= 114/79.8 - 1).</p>", "parentCommentId": "kt4C7GjukC2jCBP9B", "user": {"username": "vascoamaralgrilo"}}, {"_id": "skdRBWbcQxDH9wwBP", "postedAt": "2024-02-25T23:26:37.287Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<blockquote><p>I think the reviewer may be concluding from the above that, given no international food trade, calorie consumption would be much lower, and therefore increasing food production via new food sectors would become much more important relative to distribution. I agree with the former, but not the latter. Loss of international food trade is more of a problem of food distribution than production. If this increased thanks to new food sectors, but could not be distributed to low-income food-deficit countries (<a href=\"https://www.fao.org/countryprofiles/lifdc/en/\"><u>LIFDCs</u></a>) due to loss of trade, there would still be many famine deaths there. Many LIFDCs are in tropical regions too, where there is a smaller decrease in crop yields during a nuclear winter (see Fig. 4 of <a href=\"https://www.nature.com/articles/s43016-022-00573-0\"><u>Xia 2022</u></a>).<br>&nbsp;</p></blockquote><p>Another factor is that if countries are aware of the potential of scaling up resilient foods, they would be less likely to restrict trade. Therefore, I'm thinking the outcomes might be fairly bimodal, with one scenario of resilient food production and continued trade, and another scenario of not having resilient food production and loss of trade, potentially more than just food trade, perhaps with loss of industrial civilization or worse.<br>&nbsp;</p><blockquote><p>Yet, at least ignoring <a href=\"https://forum.effectivealtruism.org/topics/anthropics\"><u>anthropics</u></a>, I believe there would be a probability of full recovery of 100 % (= 1 - e^(-10^9/(66*10^6))) even then, assuming:</p><ul><li>An <a href=\"https://en.wikipedia.org/wiki/Exponential_distribution\"><u>exponential distribution</u></a>&nbsp;for the time to go from i) human extinction due to such an asteroid to ii) evolving a species as capable as humans at steering the future, with mean equal to the aforementioned 66 M years.</li><li>The above evolution could take place in the next 1 billion years during which the Earth will <a href=\"https://www.newscientist.com/article/2269567-most-life-on-earth-will-be-killed-by-lack-of-oxygen-in-a-billion-years/\"><u>remain</u></a>&nbsp;habitable.</li></ul></blockquote><p>I think this assumes a scenario where, after the asteroid that causes human extinction, the next billion years are large asteroid/comet free, which is not a good assumption.</p>", "parentCommentId": null, "user": {"username": "Denkenberger"}}, {"_id": "GbjfWLRhqrPrBRxGt", "postedAt": "2024-02-26T05:58:43.756Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<p>Thanks for the comments, David.</p><blockquote><p>Another factor is that if countries are aware of the potential of scaling up resilient foods, they would be less likely to restrict trade. Therefore, I'm thinking the outcomes might be fairly bimodal, with one scenario of resilient food production and continued trade, and another scenario of not having resilient food production and loss of trade, potentially more than just food trade, perhaps with loss of industrial civilization or worse.</p></blockquote><p>I agree that is a factor, but I guess the distribution of the severity of catastrophes caused by nuclear war is not bimodal, because the following are not binary:</p><ul><li>Awareness of mitigation measures:<ul><li>More or less countries can be aware.</li><li>Any given country can be more or less aware.</li></ul></li><li>Ability to put in practice the mitigation measures.</li><li>Export bans:<ul><li>More or less countries can enforce them.</li><li>Any given country can enforce them more or less.</li></ul></li></ul><p>In addition, I have the sense historical more local catastrophes are not bimodal, following distributions which more closely resemble a <a href=\"https://en.wikipedia.org/wiki/Power_law\">power law</a>, where more extreme outcomes are increasingly less likely.</p><blockquote><p>I think this assumes a scenario where, after the asteroid that causes human extinction, the next billion years are large asteroid/comet free, which is not a good assumption.</p></blockquote><p>Good point! I have updated the relevant bullet in the post:</p><blockquote><ul><li>So Toby would expect an asteroid impact similar to that of the last mass extinction to be an existential catastrophe. Yet, at least ignoring&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/anthropics\"><u>anthropics</u></a>, I believe the probability of not fully recovering would only be 0.0513 % (= e^(-10^9/(132*10^6))), assuming:<ul><li>An&nbsp;<a href=\"https://en.wikipedia.org/wiki/Exponential_distribution\"><u>exponential distribution</u></a> with a mean of 132 M years (= 66*10^6*2) represents the time to go from i) human extinction due to such an asteroid to ii) evolving a species as capable as humans at steering the future. I supposed this on the basis that:<ul><li>An&nbsp;<a href=\"https://en.wikipedia.org/wiki/Exponential_distribution\"><u>exponential distribution</u></a> with a mean of 66 M years describes the time between extinction threats as well as that to go from i) to ii) conditional on no extinction threats.</li><li>Given the above, extinction and full recovery are equally likely. So there is a 50 % chance of full recovery, and one should expect the time until full recovery to be 2 times (= 1/0.50) as long as that conditional on no extinction threats.</li></ul></li><li>The above evolution could take place in the next 1 billion years during which the Earth will&nbsp;<a href=\"https://www.newscientist.com/article/2269567-most-life-on-earth-will-be-killed-by-lack-of-oxygen-in-a-billion-years/\"><u>remain</u></a> habitable.</li></ul></li></ul></blockquote><p>Now the probability of not fully recovering is 0.0513 %, i.e. 1.95 k (= 5.13*10^-4/(2.63*10^-7)) times as high as before. Yet, the updated unconditional existential risk (extinction caused by the asteroid and no full recovery afterwards) is still astronomically low, 3.04*10^-15 (= 5.93*10^-12*5.13*10^-4). So my point remains qualitatively the same.</p><p>I have also added the 2nd sentence in the following bullet:</p><blockquote><ul><li>Even if nuclear war causes a global civilisational collapse which eventually leads to extinction, I&nbsp;<a href=\"https://docs.google.com/document/d/1rr68EOgP6t3XOaAwPlwlAl4zPK2_uEz644IGEEihGbQ/edit#heading=h.pl6fwvf4ycp6\"><u>guess</u></a> full recovery would be extremely likely. In contrast, an extinction caused by advanced AI would arguably not allow for a full recovery.</li></ul></blockquote>", "parentCommentId": "skdRBWbcQxDH9wwBP", "user": {"username": "vascoamaralgrilo"}}, {"_id": "nP8zTGvmSKuGPFavd", "postedAt": "2024-02-26T09:03:01.380Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<p>(Last comment from me on this for time reasons)</p><ul><li>I think if you look at philanthropic neglectedness, the total sums across types of capital are not a good proxy. E.g., as far as I understand the nuclear risk landscape, it is both true that government spending is quite large but also that there is almost no civil society spending. This means that additional philanthropic funding should be expected to be quite effective on neglectedness grounds. Many obvious things are not done.</li><li>The numbers on nuclear risk spending by 80k are entirely made up and not described otherwise (e.g. they do not cite a source and make no effort justifying the estimate, this is clearly a wild guess).</li><li>If one constructed a similar number for AI risk, it could also be in the billions given it would presumably include stuff like the costs of government bureaucracies involved in tech regulation, emerging legislation etc.</li></ul><p>I am fairly convinced your basic point will stand, but it seems important to not overplay the degree to which nuclear risk is not neglected, &nbsp;and to not underplay the degree to which government actors and others are now paying attention to AI risk (obviously, this also needs to be quality discounted, but this discounting does not reduce the value much for nuclear in your estimate).<br>&nbsp;</p>", "parentCommentId": "ZwetXwgyyvxHEpudP", "user": {"username": "jackva"}}, {"_id": "8CEqphdyF2BnTog5s", "postedAt": "2024-02-26T16:03:55.324Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<p>I agree that people should not focus on nuclear risk as a direct extinction risk (and have long argued this), see Toby's nuke extinction estimates as too high, and would assess measures to reduce damage from nuclear winter to developing neutral countries mainly in GiveWell-style or ordinary CBA terms, while considerations about future generations would favor focus on AI, and to a lesser extent bio.&nbsp;<br><br>However, I do think this wrongly downplays the effects on our civilization beyond casualties and local damage of a nuclear war that wrecks the current nuclear powers, e.g. on disrupting international cooperation, rerolling contingent nice aspects of modern liberal democracy, or leading to release of additional WMD arsenals (such as bioweapons, while disrupting defense against those weapons). So the 'can nuclear war with current arsenals cause extinction' question misses most of the existential risk from nuclear weapons, which is indirect in contributing to other risks that could cause extinction or lock-in of permanent awful regimes. I think marginal philanthropic dollars can save more current lives and help the overall trajectory of civilization more on other risks, but I think your direct extinction numbers above do greatly underestimate how much worse the future should be expected to be given a nuclear war that laid waste to, e.g. NATO+allies and the Russian Federation.<br><br>You dismiss that here:<br><br>&gt; Then discussions move to more poorly understood aspects of the risk (e.g. how the distribution of values after a nuclear war affects the longterm values of <a href=\"https://forum.effectivealtruism.org/topics/transformative-artificial-intelligence\"><u>transformative AI</u></a>).<br><br>But I don't think it's a huge stretch to say that a war with Russia largely destroying the NATO economies (and their semiconductor supply chains), leaving the PRC to dominate the world system and the onrushing creation of powerful AGI, makes a big difference to the chance of <a href=\"https://forum.effectivealtruism.org/posts/KqCybin8rtfP3qztq/agi-and-lock-in\">locked-in</a> permanent totalitarianism and the values of one dictator running roughshod over the low-hanging fruit of many others' values. That's very large compared to these extinction effects. It also doesn't require bets on extreme and plausibly exaggerated nuclear winter magnitude.<br><br>Similarly, the chance of a huge hidden state bioweapons program having its full arsenal released simultaneously (including doomsday pandemic weapons) skyrockets in an all-out WMD war in obvious ways.<br><br>So if one were to find super-leveraged ways reduce the chance of nuclear war (this applied less to measures to reduce damage to nonbelligerent states) then in addition to beating GiveWell at saving current lives, they could have big impacts on future generations. Such opportunities are extremely scarce, but the bar for looking good in future generation impacts is less than I think this post suggests.</p>", "parentCommentId": null, "user": {"username": "CarlShulman"}}, {"_id": "3rjCYY2BPzAnfYDie", "postedAt": "2024-02-26T16:41:17.660Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<p>Thanks for elaborating.</p><blockquote><p>I think if you look at philanthropic neglectedness, the total sums across types of capital are not a good proxy. E.g., as far as I understand the nuclear risk landscape, it is both true that government spending is quite large but also that there is almost no civil society spending. This means that additional philanthropic funding should be expected to be quite effective on neglectedness grounds.</p></blockquote><p>I got this was your point, but I am not convinced it holds. I would be curious to understand which empirical evidence informs your views. Feel free to link to relevant pieces, but no worries if you do not want to engage further.</p><blockquote><p>Many obvious things are not done.</p></blockquote><p>I do not think this necessarily qualifies as satisfy empirical evidence that philanthropic neglectedness means high marginal returns. There may be non-obvious reasons for the ovious interventions not having been picked. In general, I am thinking that for any problem it is always possible to pick a neglected set of interventions, but that a priori we should assume diminishing returns in the overall spending, otherwise the government would fund the philanthropic interventions.</p><blockquote><p>The numbers on nuclear risk spending by 80k are entirely made up and not described otherwise (e.g. they do not cite a source and make no effort justifying the estimate, this is clearly a wild guess).</p></blockquote><p>For reference, <a href=\"https://80000hours.org/problem-profiles/nuclear-security/#who-is-working-on-this-problem\">here</a> is some more context on 80,000 Hours' profile:</p><blockquote><h3><strong>Who is working on this problem?</strong></h3><p>The area is a significant focus for governments, security agencies, and intergovernmental organisations.</p><p>Within the nuclear powers, some fraction of all work dedicated to foreign policy, diplomacy, military, and intelligence is directed at ensuring nuclear war does not occur. While it is hard to know exactly how much, it is likely to be in the billions of dollars or more in each country.</p><p>The US budget for nuclear weapons is comfortably in the tens of billions.<a href=\"https://80000hours.org/problem-profiles/nuclear-security/#fn-8\"><sup>8</sup></a> Some significant fraction of this is presumably dedicated to control, safety, and accurate detection of attacks on the US.</p><p>In addition to this, some intergovernmental organisations devote substantial funding to nuclear security issues. For example, in 2016, the International Atomic Energy Agency had a budget of \u20ac361 million.<a href=\"https://80000hours.org/problem-profiles/nuclear-security/#fn-9\"><sup>9</sup></a> Total philanthropic nuclear risk spending in 2021 was approximately <a href=\"https://web.archive.org/web/20221016200309/https://www.getguesstimate.com/models/19174\">$57\u2013190 million</a>.</p></blockquote><p>The spending of 4.04 G$ I mentioned is just 4.87 % (= 4.04/82.9) on the cost of maintaining and modernising nuclear weapons in 2022 <a href=\"https://www.icanw.org/the_cost_of_nuclear_weapons\">of</a> 82.9 G$.</p><blockquote><p>If one constructed a similar number for AI risk, it could also be in the billions given it would presumably include stuff like the costs of government bureaucracies involved in tech regulation, emerging legislation etc.</p></blockquote><p>Good point. I guess the quality-adjusted contribution from those sources is currently small, but that it will become very significant in the next few years or decades.</p><blockquote><p>I am fairly convinced your basic point will stand</p></blockquote><p>Agreed. I estimated a difference of 8 OOMs (factor of 59.8 M) in the nearterm annual extinction risk per funding.</p><blockquote><p>it seems important to not overplay the degree to which nuclear risk is not neglected, &nbsp;and to not underplay the degree to which government actors and others are now paying attention to AI risk (obviously, this also needs to be quality discounted, but this discounting does not reduce the value much for nuclear in your estimate).</p></blockquote><p>Agreed. On the other hand, I would rather see discussions move from neglectedness towards cost-effectiveness analyses.</p>", "parentCommentId": "nP8zTGvmSKuGPFavd", "user": {"username": "vascoamaralgrilo"}}, {"_id": "97CnFC8maNc94djCF", "postedAt": "2024-02-26T17:49:29.432Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<p>Thanks for sharing your thoughts, Carl!</p><blockquote><p>I agree that people should not focus on nuclear risk as a direct extinction risk (and have long argued this), see Toby's nuke extinction estimates as too high, and would assess measures to reduce damage from nuclear winter to developing neutral countries mainly in GiveWell-style or ordinary CBA terms, while considerations about future generations would favor focus on AI, and to a lesser extent bio.</p></blockquote><p>Thanks for mentioning these points. Would you also rely on ordinary CBAs to assess interventions to decrease the direct damage of nuclear war? I think this would still make sense.</p><blockquote><p>So the 'can nuclear war with current arsenals cause extinction' question misses most of the existential risk from nuclear weapons, which is indirect in contributing to other risks that could cause extinction or lock-in of permanent awful regimes.</p></blockquote><p>At the same time, the nearterm extinction risk from AI also misses most of the existential risk from AI? I guess you are implying that the ratio between nearterm extinction risk and total existential risk is lower for nuclear war than for AI.</p><p>Related to your point above, I say that:</p><blockquote><ul><li>Interventions to decrease nuclear risk have <a href=\"https://forum.effectivealtruism.org/topics/indirect-long-term-effects\"><u>indirect effects</u></a>&nbsp;which <a href=\"https://forum.effectivealtruism.org/posts/jYT6c8ByLfDpYtwE9/why-charities-usually-don-t-differ-astronomically-in\"><u>will</u></a>&nbsp;tend to make their cost-effectiveness more similar to that of the best interventions to decrease AI risk. I guess the best marginal grants to decrease AI risk are much less than 59.8 M times as cost-effective as those to decrease nuclear risk.&nbsp;At the same time:<ul><li>I believe it would be a <a href=\"https://forum.effectivealtruism.org/posts/omoZDu8ScNbot6kXS/beware-surprising-and-suspicious-convergence\"><u>surprising and suspicious convergence</u></a>&nbsp;if the best interventions to decrease nuclear risk based on the more direct effects of nuclear war also happened to be the best with respect to the more indirect effects. I would argue directly optimising the indirect effects tends to be better.</li><li>For example, I agree competition between the United States and China is a relevant risk factor for AI risk, and that avoiding nuclear war contributes towards a better relationship between these countries, thus also decreasing AI risk. Yet, in this case, I would expect it would be better to explicitly focus on interventions in <a href=\"https://80000hours.org/career-reviews/ai-policy-and-strategy/\"><u>AI governance and coordination</u></a>, <a href=\"https://80000hours.org/career-reviews/china-related-ai-safety-and-governance-paths/\"><u>China-related AI safety and governance paths</u></a>, <a href=\"https://80000hours.org/career-reviews/emerging-global-power-specialist/\"><u>understanding India and Russia better</u></a>, and <a href=\"https://80000hours.org/career-reviews/china-specialist/\"><u>improving China-Western coordination on global catastrophic risks</u></a>.</li></ul></li></ul></blockquote><p>Regarding:</p><blockquote><p>You dismiss that [\"effects on our civilization beyond casualties and local damage of a nuclear war\"] here:<br><br>&gt; Then discussions move to more poorly understood aspects of the risk (e.g. how the distribution of values after a nuclear war affects the longterm values of <a href=\"https://forum.effectivealtruism.org/topics/transformative-artificial-intelligence\"><u>transformative AI</u></a>).</p></blockquote><p>Note I mention right after this that:</p><blockquote><p>In any case, I recognise it is a <a href=\"https://forum.effectivealtruism.org/topics/crucial-consideration\"><u>crucial consideration</u></a>&nbsp;whether nearterm annual risk of human extinction from nuclear war is a good proxy for the <a href=\"https://forum.effectivealtruism.org/topics/importance\"><u>importance</u></a>&nbsp;of decreasing nuclear risk from a <a href=\"https://forum.effectivealtruism.org/topics/longtermism\"><u>longtermist</u></a>&nbsp;perspective. I would agree further research on this is really valuable.</p></blockquote><p>You say that:</p><blockquote><p>I don't think it's a huge stretch to say that a war with Russia largely destroying the NATO economies (and their semiconductor supply chains), leaving the PRC to dominate the world system and the onrushing creation of powerful AGI, makes a big difference to the chance of <a href=\"https://forum.effectivealtruism.org/posts/KqCybin8rtfP3qztq/agi-and-lock-in\">locked-in</a> permanent totalitarianism and the values of one dictator running roughshod over the low-hanging fruit of many others' values, one very large compared to these extinction effects. It also doesn't require bets on extreme and plausibly exaggerated nuclear winter magnitude.</p></blockquote><p>I agree these are relevant considerations. On the other hand:</p><ul><li>The US may want to attack China in order not to relenquish its position as global hegemon.</li><li>I feel like there has been little research on questions like:<ul><li>How much it would matter if powerful AI was developped in the West instead of China (or, more broadly, in a democracy instead of autocracy).</li><li>The likelihood of lock-in.</li></ul></li></ul><p>On the last point, your <a href=\"https://forum.effectivealtruism.org/posts/KqCybin8rtfP3qztq/agi-and-lock-in\">piece</a> is a great contribution, but you say:</p><blockquote><p>Note that we\u2019re mostly making claims about&nbsp;<i>feasibility</i> as opposed to&nbsp;<i>likelihood</i>.</p></blockquote><p>However, the likelihood of lock-in is crucial to assess the strength of your points. I would not be surprised if the chance of an AI lock-in due to a nuclear war was less than 10^-8 this century.</p><p>In terms of nuclear war indirectly causing extinction:</p><blockquote><p>at least ignoring&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/anthropics\"><u>anthropics</u></a>, I believe the probability of not fully recovering would only be 0.0513 % (= e^(-10^9/(132*10^6))), assuming:</p><ul><li>An&nbsp;<a href=\"https://en.wikipedia.org/wiki/Exponential_distribution\"><u>exponential distribution</u></a> with a mean of 132 M years (= 66*10^6*2) represents the time to go from i) human extinction due to such an asteroid to ii) evolving a species as capable as humans at steering the future. I supposed this on the basis that:<ul><li>An&nbsp;<a href=\"https://en.wikipedia.org/wiki/Exponential_distribution\"><u>exponential distribution</u></a> with a mean of 66 M years describes the time between extinction threats as well as that to go from i) to ii) conditional on no extinction threats.</li><li>Given the above, extinction and full recovery are equally likely. So there is a 50 % chance of full recovery, and one should expect the time until full recovery to be 2 times (= 1/0.50) as long as that conditional on no extinction threats.</li></ul></li><li>The above evolution could take place in the next 1 billion years during which the Earth will&nbsp;<a href=\"https://www.newscientist.com/article/2269567-most-life-on-earth-will-be-killed-by-lack-of-oxygen-in-a-billion-years/\"><u>remain</u></a> habitable.</li></ul></blockquote><p>In contrast, if powerful AI caused extinction, control over the future would arguably permanently be lost.</p><blockquote><p>plausibly exaggerated nuclear winter magnitude.</p></blockquote><p><a href=\"https://forum.effectivealtruism.org/posts/gktZ8zuzyh7HEgjfc/famine-deaths-due-to-the-climatic-effects-of-nuclear-war\">Agreed</a>.</p><blockquote><p>Similarly, the chance of a huge hidden state bioweapons program having its full arsenal released simultaneously (including doomsday pandemic weapons) skyrockets in an all-out WMD war in obvious ways.</p></blockquote><p>Is there any evidence for this?</p><blockquote><p>this applied less to measures to reduce damage to nonbelligerent states</p></blockquote><p>Makes sense. If GiveWell's top charities are not a cost-effective way of improving the longterm future, then decreasing starvation in low income countries in a nuclear winter may be cost-effective in terms of saving lives, but has semingly negligible impact on the longterm future too. Such countries just have too little influence on transformative technologies.</p>", "parentCommentId": "8CEqphdyF2BnTog5s", "user": {"username": "vascoamaralgrilo"}}, {"_id": "3vAzG2dRNsJCinzgW", "postedAt": "2024-02-26T19:32:30.051Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<blockquote><p>but that a priori we should assume diminishing returns in the overall spending, otherwise the government would fund the philanthropic interventions.</p></blockquote><p>I think this is fundamentally the crux -- many of the most valuable philanthropic actions in domains with large government spending will likely be about challenging / advising / informationally lobbying the government in a way that governments cannot self-fund.<br><br>Indeed, when additional government funding does not reduce risk (does not reduce the importance of the problem) but is affectable, there can probably be cases where you should get more excited about philanthropic funding to leverage as public funding increases.</p>", "parentCommentId": "3rjCYY2BPzAnfYDie", "user": {"username": "jackva"}}, {"_id": "CuqYcRLYQTtwNBZZR", "postedAt": "2024-02-27T05:17:40.362Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<p>Rapid fire:<br>&nbsp;</p><ul><li>Nearterm extinction risk from AI is wildly closer to total AI x-risk than the nuclear analog</li><li>My guess is that nuclear war interventions powerful enough to be world-beating for future generations would look tremendous in averting current human deaths, and most of the WTP should come from that if one has a lot of WTP related to each of those worldviews</li><li>Re suspicious convergence, what do you want to argue with here? I've favored allocation on VOI and low-hanging fruit on nuclear risk not leveraging AI related things in the past less than 1% of my marginal AI allocation (because of larger more likely near term risks from AI with more tractability and neglectedness); recent AI developments tend to push that down, but might surface something in the future that is really leveraged on avoiding nuclear war</li><li>I agree not much has been published in journals on the impact of AI being developed in dictatorships</li><li>Re lock-in I do not think it's remote (my views are different from what that paper limited itself to) for a CCP-led AGI future,&nbsp;<br><br>&nbsp;</li></ul>", "parentCommentId": "97CnFC8maNc94djCF", "user": {"username": "CarlShulman"}}, {"_id": "AkpHs2Tdyk7HJGcPd", "postedAt": "2024-02-27T07:00:55.754Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<p>Thanks for following up!</p><blockquote><p>Re suspicious convergence, what do you want to argue with here?</p></blockquote><p>Sorry for the lack of clarity. Some thoughts:</p><ul><li>The <a href=\"https://forum.effectivealtruism.org/posts/6KNSCxsTAh7wCoHko/nuclear-war-tail-risk-has-been-exaggerated#Grantmakers_and_donors_interested_in_decreasing_extinction_risk_had_better_focus_on_artificial_intelligence_instead_of_nuclear_war\">15.3 M$</a> grantmakers aligned with effective altruism have influenced aiming to decrease nuclear risk seem mostly optimised to decrease the nearterm damage caused by nuclear war (especially the spending on nuclear winter), not the more longterm existential risk linked to permanent global totalitarianism.</li><li>As far as I know, there has been little research on how a minor AI catastrophe would influence AI existential risk (although wars over Taiwan <a href=\"https://www.csis.org/analysis/first-battle-next-war-wargaming-chinese-invasion-taiwan\">have</a> been wargamed). Looking into this seems more relevant than investigating how a non-AI catastrophe would influence AI risk.</li><li>The risk from permanent global totalitarianism is still poorly understood, so research on this and how to mitigate it seems more valuable than efforts focussing explicitly on nuclear war. There might well be interventions to increase democracy levels in China which are more effective to decrease that risk than interventions aimed at ensuring that China does not become the sole global hegemon after a nuclear war.</li><li>I guess most of the risk from permanent global totalitarianism does not involve any major catastrophes. As a data point, the Metaculus' community predicts an AI dystopia is 5 (= 0.19/0.037) times as likely as a paperclipalypse by 2050.</li></ul><blockquote><p>I agree not much has been published in journals on the impact of AI being developed in dictatorships</p></blockquote><p>More broadly, which pieces would you recommend reading on this topic? I am not aware of substantial blogposts, although I have seen the concern raised many times.</p>", "parentCommentId": "CuqYcRLYQTtwNBZZR", "user": {"username": "vascoamaralgrilo"}}, {"_id": "sLjGwsJbcDXSBXEkh", "postedAt": "2024-02-27T09:25:01.917Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<blockquote><p>guesses it would cost hundreds of billions of dollars to design and test <a href=\"https://forum.effectivealtruism.org/topics/refuges\"><u>shelters</u></a></p></blockquote><p><u>I looked at the referenced article and could not find a mention of this sum of money - could you please point me to where exactly Salotti makes this guess?</u></p>", "parentCommentId": null, "user": {"username": "Ulrik Horn"}}, {"_id": "7FvsHaZ8CEYhLzikD", "postedAt": "2024-02-27T09:37:45.259Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<p>Hi Ulrik,</p><p>It is not in the article. I have added the following footnote:</p><blockquote><p>Information provided via email.</p></blockquote>", "parentCommentId": "sLjGwsJbcDXSBXEkh", "user": {"username": "vascoamaralgrilo"}}, {"_id": "j49eQaP8nPmTQ73af", "postedAt": "2024-03-17T19:45:26.071Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<p>Any probability as low as 5.93*10^-12 about something as difficult to model as the effects of nuclear war on human society seems extremely overconfident to me. Can you really make 1/5.93*10^-12 (170 billion) predictions about independent topics and expect to be wrong only once? Are you 99.99% [edit: fixed this number] sure that there is no unmodeled set of conditions under which civilizational collapse occurs quickly, which a nuclear war is at least 0.001% likely to cause? I think the minimum probabilities that one should have given these considerations is not much lower than the superforecasters' numbers.</p>", "parentCommentId": null, "user": {"username": "tkwa"}}, {"_id": "smj4RQRutLgZzNjGk", "postedAt": "2024-03-17T22:35:24.929Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<p>Thanks for the comment, Thomas!</p><blockquote><p>Any probability as low as 5.93*10^-12 about something as difficult to model as the effects of nuclear war on human society seems extremely overconfident to me.</p></blockquote><p>I feel like this argument is too general. The human body is quite complex too, but the probability of a biological human naturally growing to have 10 m is still astronomically low. Likewise for the risk of <a href=\"https://forum.effectivealtruism.org/posts/6KNSCxsTAh7wCoHko/nuclear-war-tail-risk-has-been-exaggerated#Cost_effectiveness_of_decreasing_extinction_risk_from_asteroids_and_comets\">asteroids and comets</a>, and <a href=\"https://forum.effectivealtruism.org/posts/iDwxwmKvFPk2Di4ge/supervolcanoes-tail-risk-has-been-exaggerated\">supervolcanoes</a>.</p><p>Nuclear war being difficult to model means more uncertainty, but not necessarily higher risk. There are infinitely many orders of magnitude between 0 and 5.93*10^-12, so I think I can at least in theory be quite uncertain while having a low best guess. I understand greater uncertainty (e.g. higher ratio between the 95th and 5th percentile) holding the median constant tends to increase the mean of heavy-tailed distributions (like lognormals), but it is unclear to which extent this applies. I have also accounted for this by using heavy-tailed distributions whenever I thought appropriate (e.g. I modelled the soot injected into the stratosphere per equivalent yield as a lognormal).</p><blockquote><p>Can you really make 1/5.93*10^-12 (170 billion) predictions about independent topics and expect to be wrong only once? Are you 99.9999% sure that there is no unmodeled set of conditions under which civilizational collapse occurs quickly, which a nuclear war is at least 0.001% likely to cause?</p></blockquote><p>Nitpick. I think you have to remove 2 9s in the 2nd sentence, because the annual chance of nuclear war is around 1 %.</p><p>I do not think I have calibrated intuitions about the probability of rare events. To illustrate, I suppose it is easy for someone (not aware of the relevant dynamics) to guess the probability of a ticket winning the lottery is 0.1 %, whereas it could in fact be 10^-8. Relatedly:</p><blockquote><p>Additionally, I appreciate one should be sceptical whenever a model outputs a risk as low as the ones I mentioned at the start of this section. For example, a model predicting a 1 in a trillion chance of the global real gross domestic product (<a href=\"https://en.wikipedia.org/wiki/Real_gross_domestic_product\"><u>real GDP</u></a>) decreasing from 2008 to 2009 would certainly not be capturing most of the actual risk of recession then, which would come from that model being (massively) wrong. On the other hand, one should be careful not to overgeneralise this type of reasoning, and conclude that any model outputting a small probability must be wrong by many orders of magnitude (<a href=\"https://en.wikipedia.org/wiki/Order_of_magnitude\"><u>OOMs</u></a>). The global real GDP <a href=\"https://data.worldbank.org/indicator/NY.GDP.MKTP.PP.KD\"><u>decreased</u></a>&nbsp;0.743 % (= 1 - 92.21/92.9) from 2008 to 2009, largely owing to the <a href=\"https://en.wikipedia.org/wiki/2007%25E2%2580%25932008_financial_crisis\"><u>2007\u20132008 financial crisis</u></a>, but such a tiny drop is a much less extreme event than human extinction. Basic analysis of past economic trends would have revealed global recessions are unlikely, but perfectly plausible. In contrast, I <a href=\"https://forum.effectivealtruism.org/posts/TwpoedzMpmy7k7NKH/can-a-war-cause-human-extinction-once-again-not-on-priors\"><u>see</u></a>&nbsp;historical data suggesting a war causing human extinction is astronomically unlikely.</p></blockquote><hr><blockquote><p>I think the minimum probabilities that one should have given these considerations is not much lower than the superforecasters' numbers.</p></blockquote><p>I would be curious to see a model based as much as possible on empirical evidence suggesting a much higher risk.</p>", "parentCommentId": "j49eQaP8nPmTQ73af", "user": {"username": "vascoamaralgrilo"}}, {"_id": "FSvAy257qWfDZbZ2u", "postedAt": "2024-03-18T23:24:05.245Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<p>Don't have time to reply in depth, but here are some thoughts:</p><ul><li>If a risk estimate is used for EA cause prio, it should be our betting odds / subjectie probabilities, that is, average over our epistemic uncertainty. If from our point of view a risk is 10% likely to be &gt;0.001%, and 90% likely to be ~0%, this lower bounds our betting odds at 0.0001%. It doesn't matter that it's more likely to be 0%.</li><li>Statistics of human height are much better understood than nuclear war because we have billions of humans but no nuclear wars. The situation is more analogous to finding the probability of a 10 meter tall adult human having only ever observed a few thousand monkeys (conventional wars), plus one human infant (WWII) and also knowing that every few individuals humans mutate into an entirely new species (technological progress).</li><li>It would be difficult to create a model suggesting a much higher risk because most of the risk comes from black swan events. Maybe one could upper bound the probability by considering huge numbers of possible mechanisms for extinction and ruling them out, but I don't see how you could get anywhere near 10^-12.</li></ul>", "parentCommentId": "smj4RQRutLgZzNjGk", "user": {"username": "tkwa"}}, {"_id": "b7QuGuHr3WAmBs9ma", "postedAt": "2024-03-19T06:37:56.290Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<blockquote><p>If a risk estimate is used for EA cause prio, it should be our betting odds / subjectie probabilities, that is, average over our epistemic uncertainty. If from our point of view a risk is 10% likely to be &gt;0.001%, and 90% likely to be ~0%, this lower bounds our betting odds at 0.0001%. It doesn't matter that it's more likely to be 0%.</p></blockquote><p><a href=\"https://forum.effectivealtruism.org/posts/3KAuAS2shyDwnjzNa/predictable-updating-about-ai-risk#6__Constraints_on_future_worrying\">Agreed</a>. I expect my estimate for the nearterm extinction risk from nuclear war to remain astronomically low.</p><blockquote><p>Statistics of human height are much better understood than nuclear war because we have billions of humans but no nuclear wars. The situation is more analogous to finding the probability of a 10 meter tall adult human having only ever observed a few thousand monkeys (conventional wars), plus one human infant (WWII) and also knowing that every few individuals humans mutate into an entirely new species (technological progress).</p></blockquote><p>My study of the monkeys and infants, i.e. my analysis of past wars, <a href=\"https://forum.effectivealtruism.org/posts/TwpoedzMpmy7k7NKH/can-a-war-cause-human-extinction-once-again-not-on-priors\">suggested</a> an annual extinction risk from wars of 6.36*10^-14, which is still 1.07 % (= 5.93*10^-12/(5.53*10^-10)) of my best guess.</p><blockquote><p>It would be difficult to create a model suggesting a much higher risk because most of the risk comes from black swan events. Maybe one could upper bound the probability by considering huge numbers of possible mechanisms for extinction and ruling them out, but I don't see how you could get anywhere near 10^-12.</p></blockquote><p>For the superforecasters' annual extinction risk from nuclear war until 2050 of 3.57*10^-6 to be correct, my model would need to miss 99.9998 % (= 1 - 5.93*10^-12/(3.57*10^-6)) of the total risk. You say most (i.e. more than 50 %) of the risk comes from black swan events, but I think it would be really surprising if 99.9998 % did? The black swan events would also have to absent in some sense from XPT's report, because my estimate accounts for the information I found there.</p><p>I should also clarify my 10^-6 probability of human extinction given insufficient calorie production is supposed to account for unknown unknowns. Otherwise, my extinction risk from nuclear war would be orders of magnitude lower.</p>", "parentCommentId": "FSvAy257qWfDZbZ2u", "user": {"username": "vascoamaralgrilo"}}, {"_id": "CKAkjuLH9tkBkBDWh", "postedAt": "2024-03-20T03:57:07.683Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<blockquote><p>My study of the monkeys and infants, i.e. my analysis of past wars, <a href=\"https://forum.effectivealtruism.org/posts/TwpoedzMpmy7k7NKH/can-a-war-cause-human-extinction-once-again-not-on-priors\">suggested</a> an annual extinction risk from wars of 6.36*10^-14, which is still 1.07 % (= 5.93*10^-12/(5.53*10^-10)) of my best guess.</p></blockquote><p>The fact that one model of one process gives a low number doesn't mean the true number is within a couple orders of magnitude of that. Modeling mortgage-backed security risk in 2007 using a Gaussian copula gives an astronomically low estimate of something like 10^-200, even though they did in fact default and cause the financial crisis. If the bankers adjusted their estimate upward to 10^-198 it would still be wrong.</p><p>IMO it is not really surprising for very near 100% of the risk of something to come from unmodeled risks, if the modeled risk is extremely low. Like say I write some code to generate random digits, and the first 200 outputs are zeros. One might estimate this at 10^-200 probability or adjust upwards to 10^-198, but the probability of this happening is way more than 10^-200 due to bugs.</p>", "parentCommentId": "b7QuGuHr3WAmBs9ma", "user": {"username": "tkwa"}}, {"_id": "9ZaL75z8kbLTsvjPw", "postedAt": "2024-03-20T06:21:01.762Z", "postId": "6KNSCxsTAh7wCoHko", "htmlBody": "<blockquote><p>The fact that one model of one process gives a low number doesn't mean the true number is within a couple orders of magnitude of that.</p></blockquote><p>Agreed. One should not put all weight in a single model. Likewise, one's best guess for the annual extinction risk from wars should not update to (Stephen Clare's) 0.01 % just because one model (Pareto distribution) outputs that. So the question of how one aggregates the outputs of various models is quite important. In my analysis of past wars, I considered 111 models, and got an annual extinction risk of 6.36*10^-14 for what I think is a reasonable aggregation method. You may think my aggregation method is super wrong, but this is different from suggesting I am putting all weight into a single method. Past analyses of war extinction risk did this, but not mine.</p><blockquote><p>IMO it is not really surprising for very near 100% of the risk of something to come from unmodeled risks, if the modeled risk is extremely low. Like say I write some code to generate random digits, and the first 200 outputs are zeros. One might estimate this at 10^-200 probability or adjust upwards to 10^-198, but the probability of this happening is way more than 10^-200 due to bugs.</p></blockquote><p>If it was not for considerations like the above, my best guess for the nearterm extinction risk from nuclear war would be many orders of magnitude below my estimate of 10^-11. I would very much agree that a risk of e.g. 10^-20 would be super overconfident, and not pay sufficient attention to unknown unknowns.</p>", "parentCommentId": "CKAkjuLH9tkBkBDWh", "user": {"username": "vascoamaralgrilo"}}]