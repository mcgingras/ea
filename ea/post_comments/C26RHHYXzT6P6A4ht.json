[{"_id": "qeAF2vZ3zMuKkKatK", "postedAt": "2022-12-14T14:41:24.062Z", "postId": "C26RHHYXzT6P6A4ht", "htmlBody": "<p>Thanks for writing this! I find it a really great resource and am glad to see the stuff you/RP are working on. I'd make a minor suggestion &nbsp;to make the topics of all the speedruns public even if you don't end up releasing them (or default making the topics public and then give a reason for the ones you don't). I think it's of general interest (or like at least I'm curious) and also I'd imagine you could run into a sort of sampling problem if the subject of the unpromising or negative ones were just never revealed.</p>", "parentCommentId": null, "user": {"username": "joshcmorrison"}}, {"_id": "TMxLRmJcmrHAtXL3j", "postedAt": "2022-12-14T15:02:21.802Z", "postId": "C26RHHYXzT6P6A4ht", "htmlBody": "<p>I attended an AI Safety Field Building conference where we spent like an hour mapping out the &nbsp;space. It would have been so much quicker and saved a lot of people's time if a map had already existed. Of course the problem is that it's very hard to keep such maps up to date. I would be enthusiastic if Rethink Priorities were to take responsibility for this and other high-level overview tasks including:</p><ul><li>Maintaining an up-to-date list of the various alignment research organisations, their research directions and key assumptions about the problem.</li><li>Maintaining a list of demand for various kinds of role: research leads, researchers, research engineers, ops, ect.</li><li>Mapping out the pipeline such as how many opportunities for programs at different levels</li><li>Aggregating people's opinions about the current state of the field and/or major debates</li></ul>", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "vQWPeCameEGGDhCk7", "postedAt": "2022-12-14T19:40:36.659Z", "postId": "C26RHHYXzT6P6A4ht", "htmlBody": "<p>Really grateful for your transparency here, Linch!</p>\n", "parentCommentId": null, "user": {"username": "joel_bkr"}}, {"_id": "3uBq5EuBE8nEKPjdD", "postedAt": "2022-12-15T00:01:49.934Z", "postId": "C26RHHYXzT6P6A4ht", "htmlBody": "<p>This seems helpful, though I'd guess another team that's in more frequent contact with AI safety orgs could do this for significantly lower cost, since they'll be starting off with more of the needed info and contacts.</p>\n", "parentCommentId": "TMxLRmJcmrHAtXL3j", "user": {"username": "Mauricio"}}, {"_id": "uGdDw5aarjafbDzEj", "postedAt": "2022-12-15T01:27:24.360Z", "postId": "C26RHHYXzT6P6A4ht", "htmlBody": "<p>Agreed! Other groups will be better placed. But I'm not categorically ruling this out: if <a href=\"https://twitter.com/ASmallFiction/status/901252178588778498\">nobody else appears to be on track</a> for doing this when we're next in prioritization mode, we might revisit this issue and see whether it makes sense to prioritize it anyway.</p>", "parentCommentId": "3uBq5EuBE8nEKPjdD", "user": {"username": "Linch"}}, {"_id": "hw3WJBJRuGAMRzWvD", "postedAt": "2022-12-15T04:54:15.459Z", "postId": "C26RHHYXzT6P6A4ht", "htmlBody": "<p>There's this project that a few people have been working on towards making an overview in Obsidian that seems to be in this vein as well (see the comment, too): <a href=\"https://aisafetyideas.com/?idea=75\">aisi.ai/?idea=75</a>&nbsp;</p>", "parentCommentId": "TMxLRmJcmrHAtXL3j", "user": {"username": "esben-kran"}}, {"_id": "eFSHFZTiJEgzngh36", "postedAt": "2022-12-15T09:41:59.374Z", "postId": "C26RHHYXzT6P6A4ht", "htmlBody": "<p>I think this is not exactly what you had in mind, but seemed tangentially relevant so I am sharing in case people have not come across it: <a href=\"https://futureoflife.org/landscape/\">https://futureoflife.org/landscape/</a></p><p>This is from my understanding mostly about research and the nodes link to various papers and not organizations working on the various topics.<br>&nbsp;</p>", "parentCommentId": "TMxLRmJcmrHAtXL3j", "user": {"username": "Ulrik Horn"}}, {"_id": "Gv9cQg3Fp3kPTKkD9", "postedAt": "2022-12-15T18:52:21.415Z", "postId": "C26RHHYXzT6P6A4ht", "htmlBody": "<p>Thanks for your kind words! Re: the last point, &nbsp;this makes sense to me. I'll talk to the team but I think we'll default to sharing the topics even if we don't release them, barring very good reasons not to, like you mentioned.</p>", "parentCommentId": "qeAF2vZ3zMuKkKatK", "user": {"username": "Linch"}}, {"_id": "eday8BvQ8kRNKDMkG", "postedAt": "2023-01-21T00:34:03.571Z", "postId": "C26RHHYXzT6P6A4ht", "htmlBody": "<p>FWIW, some resources I find useful in mapping just bullet (1):</p><ul><li><a href=\"https://aisafety.world/\">AIS.world</a> (along with the other AIS Support infrastructure like <a href=\"https://aisafety.training/\">AIS.training</a> and <a href=\"https://www.aisafetysupport.org/resources/lots-of-links\">this thing</a>)</li><li><a href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is\">What is everyone doing</a> (mid-2022, but maybe the most ambitious in scope that I've seen)</li><li>I've found <a href=\"https://www.lesswrong.com/posts/C4tR3BEpuWviT7Sje/2021-ai-alignment-literature-review-and-charity-comparison\">this</a> a useful meta-list of orgs at some points, but it's more than a year stale</li><li>I haven't looked at <a href=\"https://aiwatch.issarice.com/\">AI Watch</a> as much</li></ul><p>I agree it's still a messy space. Although I worry about this <a href=\"https://xkcd.com/927/\">failure mode</a> for anyone thinking about adding new standards.</p>", "parentCommentId": "TMxLRmJcmrHAtXL3j", "user": {"username": "Angelina Li"}}, {"_id": "WNbPj5cEd5ihtKKvt", "postedAt": "2023-01-10T06:33:27.987Z", "postId": "C26RHHYXzT6P6A4ht", "htmlBody": null, "parentCommentId": null, "user": null}]