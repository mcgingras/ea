[{"_id": "p8rntnKwZiDeqqDXE", "postedAt": "2022-11-14T02:10:11.604Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>Brilliant post. Thanks for writing it. I just want to add to what you said about ethics. It seems that evaluating whether an action / event is good or bad itself presupposes an ethical theory.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmvual3out68\"><sup><a href=\"#fnmvual3out68\">[1]</a></sup></span>&nbsp;Hence I think a lot of the claims that are being made can be described as either (a) this event shows vividly how strongly utilitarianism can conflict with 'common-sense morality' (or our intuitions)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwdg0c1lanvp\"><sup><a href=\"#fnwdg0c1lanvp\">[2]</a></sup></span>&nbsp;or (b) trying to follow<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzdav7yxevrp\"><sup><a href=\"#fnzdav7yxevrp\">[3]</a></sup></span>&nbsp;utilitarianism tends to lead to outcomes which are bad by the lights of utilitarianism (or perhaps some other theory). The first of these seems not particularly interesting to me, as suggested in your post, and the second is a separate point entirely - but is nonetheless often being presented as a criticism of utilitarianism.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmvual3out68\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmvual3out68\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Someone else made this point before me in another post but I can\u2019t find their comment.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwdg0c1lanvp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwdg0c1lanvp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>But note that this applies mostly to naive act utilitarianism.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzdav7yxevrp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzdav7yxevrp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>By which I mean 'act in accordance' with, but it's worth noting that this is pretty underdetermined. For instance, doing EV calculations is not the only way to act in accordance with utilitarianism.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Ben Auer"}}, {"_id": "EGd82efoQKMcKPaPq", "postedAt": "2022-11-14T02:22:35.474Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>(Upvoted)</p>\n<blockquote>\n<p>Events are not evidence to the truth of philosophical positions.</p>\n</blockquote>\n<p>Are you sure?  How about this position from Richard Chappell's post?</p>\n<blockquote>\n<p>(3) Self-effacing utilitarian: Ex-utilitarian, gave up the view on the grounds that doing so would be for the best.</p>\n</blockquote>\n<p>Psychological effects of espousing a moral theory are empirical in nature.  Observations about the world could cause a consequentialist to switch to some other theory on consequentialist grounds, no?</p>\n<p>Not sure there's a clean division between moral philosophy and <a href=\"https://forum.effectivealtruism.org/posts/eoLwR3y2gcZ8wgECc/hubris-and-coldness-within-ea-my-experience?commentId=DjYKXh8nbs7s4XjEo\">moral</a> <a href=\"https://twitter.com/hamandcheese/status/1590841627979153409\">psychology</a>.</p>\n<p>I agree hastily jumping to a different theory while experiencing distress seems bad, but it seems reasonable to update a bit on the margin.</p>\n<p>I agree investigation should be thoughtful, but now seems as good as any opportunity to discuss.  You say we should wait until facts are properly established, but I think discussion now can help establish facts, the same way a detective would want to visit the scene of a crime soon after it was committed.</p>\n", "parentCommentId": null, "user": {"username": "John_Maxwell_IV"}}, {"_id": "KjbKNK2YPWamAnH4o", "postedAt": "2022-11-14T02:55:08.649Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>Excellent post. I hope everybody reads it and takes it onboard.</p><p>One failure mode for EA will be over-reacting to black swan events like this that might not carry as much information about our organizations and our culture as we think they do.&nbsp;</p><p>Sometimes a bad actor who fools people is just a bad actor who fools people, and they're not <i>necessarily</i> diagnostic of a more systemic organizational problem. They might be, but they might not be.&nbsp;</p><p>We should be open to all possibilities at this point, and if EA decides it needs to tweak, nudge, update, or overhaul its culture and ethos, we should do so intelligently, carefully, strategically, and wisely -- rather than in a reactive, guilty, depressed, panicked, or self-flagellating panic.</p>", "parentCommentId": null, "user": {"username": "geoffreymiller"}}, {"_id": "bvwhJpouahkwJn8Z8", "postedAt": "2022-11-14T03:04:55.169Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>Strongly agree with all of these points.</p><p>On point 2: The EA movement urgently needs more earners-to-give, especially now. One lesson that I think is correct, however, is that <strong>we should be wary of making any one billionaire donor the face of the EA movement</strong>. The downside risk\u2014a loss of credibility for the whole movement due to unknown information about the billionaire donor\u2014is generally too high.</p>", "parentCommentId": null, "user": {"username": "Peter S. Park"}}, {"_id": "SdLdfwXvPFtwW9vv6", "postedAt": "2022-11-14T03:09:01.624Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>My understanding is that the self-effacing utilitarian is not strictly an 'ex-utilitarian', in that they are still using the same types of rightness criteria as a utilitarian (at least with respect to world-states). Although they may try to deceive themselves into actually believing another theory, since this would better achieve their rightness criterion, that is not the same as abandoning utilitarianism on the basis that it was somehow refuted by certain events. In other words, as you say, they're switching theories \"on consequentialist grounds\". Hence they're still a consequentialist in the sense that is philosophically important here.</p>", "parentCommentId": "EGd82efoQKMcKPaPq", "user": {"username": "Ben Auer"}}, {"_id": "mDaa4Tw6FceAuXyno", "postedAt": "2022-11-14T04:36:30.260Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>+1 from me.</p><p>&nbsp;I was talking about the whole situation with my parents, and they mentioned that their local synagogue experienced a very similar catastrophe, with the community's largest funder turning out to be a con-man. Everybody impacted had a lot of soul-searching to do, but ultimately in retrospect, there was really nothing they could or should have done differently\u2014it was a black-swan event that hasn't repeated in the quarter of a century or so since it happened, and there were no obvious red flags until it was too late. Yes, we can always find details to agonize over, but ultimately, I doubt it will be very productive to change our whole modus operandi to prevent this particular black swan event from repeating (with a few notable exceptions).</p>", "parentCommentId": null, "user": {"username": "Yitz"}}, {"_id": "qJG3JDtYjCnimofNb", "postedAt": "2022-11-14T08:33:12.350Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>&nbsp;I do think EA is above treating this as a black swan event. Fraud in unregulated finance (crypto even more so) even if at least initially guided by good (no to speak of naively utilitarian) intentions is to be expected. &nbsp;Most people did not expect this to happen with SBF/FTX, but some did. There's a lot of potential to learn from this and make the movement more resilient against future cases of funder's fraud via guidelines, practices. E.g. clarifying that dirty money won't work towards achieving EA aims. And that EA credibility should not be lent to dubious practices.&nbsp;<br><br>Other than that I agree with the gist of this post &amp; comment but it's also important to gradually update views. Upvoted the comment of John_Maxwel</p>", "parentCommentId": "KjbKNK2YPWamAnH4o", "user": {"username": "akanepajs"}}, {"_id": "dZFu8QuBy98AHPj88", "postedAt": "2022-11-14T10:10:15.166Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>Downvoted. I disagree quite strongly on points one and four, but that's a discussion for another day; I downvoted because point three is harmful.</p><p>If people with a long history of criticising EA have <i>indeed</i> claimed X for a long time, while EA-at-large has said not-X; and X is compatible with the events of the past week, while not-X is not (or is less obviously compatible, or renders those events more unexpected); then rational Bayesians should <i>update towards</i> the people with the long history of criticising EA. Just apply Bayes' rule: if P(events of the last week | X) &gt; P(events of the last week | not-X), then you should increase your credence in X upon observing the events of the last week.</p><p>This reasoning holds whether or not these critics are speaking in bad faith, have personal issues with EA, or are acting irrationally. If being a bad-faith critic of EA provides you with <i>better</i> <i>predictive power</i> than being a relatively-uncritical member of the movement, then you should update so that you are closer to being a bad-faith critic of EA than to being a relatively-uncritical member of the movement. You probably shouldn't go all the way there (better to stop in the middle, somewhere around 'good-faith critic' or 'EA adjacent' or 'EA but quite suspicious of the movement's leadership'), but updating in that direction is the rational Bayesian thing to do.</p><p>To be sure, there's always a worry that the critics have fudged or falsified their predictions, saying something vaguely critical in the past which has since been sharpened into 'Several months ago, I predicted that this exact thing would happen!' This is the 'predicting the next recession' effect, and we should be vigilant about it. But while this is definitely happening in a lot of cases, in some of the most high-profile ones I don't think it applies: I think there were relatively concrete predictions made that a crisis of power and leadership of pretty much this kind was enabled by EA structures, and these predictions seem to have been closer to the mark than anything EA-at-large thought might happen.</p><p>I think there is a further sense, that many EAs seem to feel that their error was less one of prediction than of creativity: it's less that they made the wrong call on a variety of questions, but simply that they <i>didn't ask those questions</i>. This is obviously not true of all EAs, but it is definitely true of some. In cases like this, listening more closely to critics - even bad faith ones! - can open your mind up to a variety of different positions and reasoning styles that previously were not even present in your mind. This is not always inherently good, of course, but if an EA has reason to think that they have made a failure of creativity then it seems like a very positive way to go.</p><p>For more context about my worries: I think that it is possible that OP might be including me, and some things I have tweeted, in point three. I have quite a small follower count and nothing I wrote 'blew up' or anything, so it's definitely very unlikely; but I did tweet out several things pretty heavily critical of the movement in recent days which very strongly pattern-match the description given above, including pointing out that prior criticisms predicted these events pretty well, and having relatively well-known EAs reaching out to me about what I had written. Certainly, I 'felt seen' (as it were) while reading this post.</p><p>I don't think I am a 'nefarious actor', or have a history of 'hating EA', but I worry that in some segments of EA (not the whole of EA - some people have gone full self-flagellation, but in some segments) these kinds of terms are getting slung around far too liberally as part of a more general circling-the-wagons trend. And I worry that posts like this one legitimise slinging these terms around in this manner, by encouraging the thought that EA critics who are engaging in some (sometimes fully-justified) 'told you so' are just bad actors trying to destroy their tribe. EA needs to be <i>more</i>, not less, open to listening to critics - even bad-faith critics - after a disaster like this one. This is good Bayesianism, but it's also just proper humility.</p>", "parentCommentId": null, "user": {"username": "Peter McLaughlin"}}, {"_id": "RDnkZvH9hsvBnQQtM", "postedAt": "2022-11-14T10:47:00.629Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>I disagree that events can't be evidence for or against philosophical positions. If empirical claims about human behaviour or the real-world operation of ethical principles are relevant to the plausibility of competing ethical theories, then I think events can provide evidential value for philosophical positions. Of course that raises a much broader set of issues and doesn't really detract from the main point of this post, but I thought I would push back on that specific aspect.</p>", "parentCommentId": null, "user": {"username": "Fods12"}}, {"_id": "ek7ZG497dWihgD36N", "postedAt": "2022-11-14T10:53:41.108Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>I have to disagree with point 3. I think, due to imbalances in incentive structures, you sometimes have to take the \"haters\" criticisms into account.&nbsp;</p><p>Take cryptocurrency and NFT's for example. Proponents of NFT's, who were financially invested, had a huge financial incentive to talk up their usefulness. They can make gargantuan sums of money from hype, and hence can full-time employ very smart people, fund their own media ecosystem, etc. &nbsp;You can find any number of highly researched and polished literature talking up the&nbsp;</p><p>But what if you (correctly imo) thought that NFT's &nbsp;were a useless bubble? There were no big venture capitalists throwing out money for criticising NFT's, and shorting them was not really a valid financial path for a variety of reasons. Debunking NFT proponents is a lot of work, which doesn't make a lot of sense to do if you have a day job.&nbsp;</p><p>But there is one incentive to critique NFT's that is powerful enough to motivate that kind of work and effort: <strong>Hatred of NFT's</strong>. This comes from someone who's been scammed, knows someone who's been scammed, or just pure \"someone is wrong on the internet\" energy. So of course they come off as \"FUD\" and haters with a dose of schadenfreude. The non-haters didn't bother writing lengthy critiques, they just shrugged, said it looked kinda dumb, and went on with their lives.&nbsp;</p><p>I hope the analogies to FTX and EA are clear here. EA is highly obscure, with a similar financial incentive imbalance in place. If you want people to put actual effort into critiquing it, you either need to pay out more to skeptics (I fully support the critique competitions), or put up with some haters who may still have legitimate points. &nbsp;</p>", "parentCommentId": null, "user": {"username": "titotal"}}, {"_id": "m2kAGS8NEeoQJ4vzJ", "postedAt": "2022-11-14T10:57:19.573Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>I think it is very difficult to litigate point three further without putting certain people on trial and getting into their personal details, which I am not interested in doing and don't think is a good use of the Forum. For what it's worth, I haven't seen your Twitter or anything from you.</p><p>I should have emphasized more that there are consistent critics of EA who I don't think are acting in bad faith at all. <a href=\"https://forum.effectivealtruism.org/users/stuart-buck-1\">Stuart Buck</a> seems to have been right early on a number of things, for example.&nbsp;</p><p>Your Bayesian argument may apply in some cases but it fails in others (for instance, when X = EAs are eugenicists).</p><blockquote><p>Just apply Bayes' rule: if P(events of the last week | X) &gt; P(events of the last week | not-X), then you should increase your credence in X upon observing the events of the last week.</p></blockquote><p>I also emphasize there are a few people who I have strong reason to believe are \"deliberate effort to sow division within the EA movement\" and this was the focus of my comment, publicly evidenced (NB: this is a very small part of my overall evidence) by them \"taking glee in this disaster or mocking the appearances and personal writing of FTX/Alameda employees.\" I do not think a productive conversation is possible in these cases.&nbsp;</p>", "parentCommentId": "dZFu8QuBy98AHPj88", "user": {"username": "burner"}}, {"_id": "vh3EmySuK55kxhXjf", "postedAt": "2022-11-14T11:20:08.041Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>I'm not sure what you mean by saying that my Bayesian argument fails in some cases? 'P(X|E)&gt;P(X) if and only if P(E|X)&gt;P(E|not-X)' is a theorem in the probability calculus (assuming no probabilities with value zero or one). If the likelihood ratio of X given E is greater than one, then upon observing E you should rationally update towards X.</p><p>If you just mean that there are some values of X which do <i>not</i> explain the events of the last week, such that P(events of the last week | X) \u2264 P(events of the last week | not-X), this is true but trivial. Your post was about cases where 'this catastrophe is in line with X thing [critics] already believed'. In these cases, the rational thing to do is to update toward critics.</p>", "parentCommentId": "m2kAGS8NEeoQJ4vzJ", "user": {"username": "Peter McLaughlin"}}, {"_id": "xcrdimCMtKdTGroxq", "postedAt": "2022-11-14T11:56:22.833Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>This is laughably disingenuous. An unregulated offshore crypto exchange that had hired the Ultimate Bet lawyer and publicly ran a hedge fund on the side collapsing is like the exact opposite of a black swan, it's &nbsp;not even a grey swan, it's that swan that's definitely no longer a cygnet and is almost grown up but still has a few bits of darker fluff. Maybe no one thought it was likely, and I think almost no thought FTX was $8bn in the hole or had quite such an atrocious balance sheet (IMO not even CZ thought this), but come on!</p>", "parentCommentId": "KjbKNK2YPWamAnH4o", "user": {"username": "Sabs"}}, {"_id": "haTHX2SdWojJCLuqF", "postedAt": "2022-11-14T14:34:39.149Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>I think many of these lessons have more merrit to them than you assume. To speak specifically about the \u2018earning to give\u2019 one, yes EA has pointed out that you should not do harm with your job to give it away. However I also think it is a bit psychologically na\u00efeve to think that what happened with FTX is the last time that giving people the advice of earning to give is the last time it will lead to people doing harm to make money.</p>\n<p>Trade-offs between ethical principles and monetary gain are not rare, and once we have established making as much money as possible (to give it away) as a goal in itself and something that gives status, it can be hard to make these trade-offs the way you are supposed to. It is not easy to accept a setback in wealth, power and (moral) status so lying to yourself or others to think that what you are doing is ethical becomes easy. It is also generally risky for individuals to become incredibly rich or powerful, especially if that depends on a misguided believe that some group membership (ea) makes you inherently ethical and therefore more trustworthy, since power tends to corrupt.</p>\n<p>At the minimum I would like EA to talk more about how to jointly maximize the ethics of how you earn and spend your money, making sure that we promote people to gain their wealth in ways that add value to the world.</p>\n", "parentCommentId": null, "user": {"username": "Jitse Goutbeek"}}, {"_id": "6qwxHZ5DHjaqCb4hk", "postedAt": "2022-11-14T17:50:31.811Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>I agree with your critique that there were important red flags and am glad you pointed them out, but I think it's inappropriate to call the comment \"laughably disingenuous\", since that's a claim that the author is not being sincere. Most of us were blindsided by this, even if there were red flags we should have been paying attention to and worrying about. I think <a href=\"https://en.wikipedia.org/wiki/Black_swan_theory\">the definition of black swan</a> could still apply to the EA community based on our subjective (but badly calibrated or poorly informed) beliefs about FTX:</p><blockquote><p>The <strong>black swan theory</strong> or <strong>theory of black swan events</strong> is a <a href=\"https://en.wikipedia.org/wiki/Metaphor\">metaphor</a> that describes an event that comes as a surprise, has a major effect, and is often inappropriately rationalized after the fact with the benefit of <a href=\"https://en.wikipedia.org/wiki/Hindsight\">hindsight</a>.</p><p>(...)</p><p>Based on the author's criteria:</p><ol><li>The event is a surprise (to the observer).</li><li>The event has a major effect.</li><li>After the first recorded instance of the event, it is rationalized by hindsight, as if it <i>could</i> have been expected; that is, the relevant data were available but unaccounted for in risk mitigation programs. The same is true for the personal perception by individuals.</li></ol></blockquote><p>It was a black swan <i>to many of us</i>, but <i>should not have been</i>, and may not have been a black swan to outsiders who were paying more attention and giving more weight to the red flags.</p><p>And even if it isn't definitionally a black swan, that doesn't call for \"laughably disingenuous\", and you should assume good faith.</p>", "parentCommentId": "xcrdimCMtKdTGroxq", "user": {"username": "MichaelStJules"}}, {"_id": "jtvPfESdaYs9shujD", "postedAt": "2022-11-14T18:02:02.460Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>Sabs -- your tone here isn't really in the spirit of EA Forum norms, IMHO.</p><p>I've followed crypto news pretty closely for the last couple of years, and the consensus in crypto and finance generally was that FTX was a big, serious, secure, respectable operation, vetted and backed by many of the most prominent VCs and investors in the industry, and doing great work lobbying for crypto acceptance in Washington DC.&nbsp;</p><p>That's my honest assessment of what most crypto insiders and investors thought, up until last week. If there had been big red flags around FTX that were commonly discussed in the crypto industry, I think I probably would have known about it. (I'm about 70% confident in this; but I could be wrong.)</p><p>Sure, there were some skeptics who pointed out potential problems with FTX. There are always skeptics and FUD-promotors, regarding any crypto protocol, exchange, or business. If they happen to be right, they pop up later and say 'See, I told you so!'.&nbsp;</p><p>Hindsight is always easy in these cases. But it's important to be empirically correct about whether there were, in fact, big warning signs about FTX that were being widely discussed in crypto and finance news and social media. There were a few, but not <i>nearly</i> as many red flags as there were around Luna, or Tether.</p>", "parentCommentId": "xcrdimCMtKdTGroxq", "user": {"username": "geoffreymiller"}}, {"_id": "n2zuhhzR64FdxEtne", "postedAt": "2022-11-14T18:46:51.490Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>maybe not everyone deserves the assumption of good faith? I feel like this forum perhaps just got a reminder about the value of not always thinking the best of people, but no, everyone just wants to forget all over again maybe&nbsp;</p>", "parentCommentId": "6qwxHZ5DHjaqCb4hk", "user": {"username": "Sabs"}}, {"_id": "wsjesYz44og4Ln7mm", "postedAt": "2022-11-14T19:03:06.672Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>I don't think everyone deserves the assumption of good faith at all times, but you haven't given enough reason to believe Geoffrey Miller doesn't, and I'm pretty sure you can't. If you're going to make accusations, you should have good reasons to do so <i>and</i> explain them. &nbsp;Merely contradicting something someone said is not nearly enough; people can be wrong without being disingenuous. Accusations make productive conversation more difficult, can be hurtful, can push people away from the community and may have other risks, so we shouldn't have a low bar for making them.</p><p>You also don't even have to <i>privately</i> assume the best of someone or good faith; just keep conversations civil and charitable, and don't make unsubstantiated accusations. If you want to argue that we should be more skeptical of people's motives, that's plausible and that can be a valuable discussion, but shouldn't be started by attacking another user without good reason.</p><p>With FTX, there were important red flags, including the ones you pointed out.</p>", "parentCommentId": "n2zuhhzR64FdxEtne", "user": {"username": "MichaelStJules"}}, {"_id": "cp6ngfKrqyjsuAQoo", "postedAt": "2022-11-14T23:27:40.923Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>The moderation team has noticed a trend of comments from Sabs that break Forum norms. Specifically, instances of&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId=x5gszkdRuLcRvApPx\"><u>rude</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/DB9ggzc5u9RMBosoz/wrong-lessons-from-the-ftx-catastrophe?commentId=n2zuhhzR64FdxEtne\"><u>hostile</u></a>, or&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/4zjnFxGWYkEF4nqMi/how-could-we-have-avoided-this?commentId=Q7BQJFyEwk96Q6g95\"><u>harsh</u></a> language are&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum#What_we_discourage__and_may_delete_or_edit_out_\"><u>strongly discouraged (and may be deleted)</u></a> and do not adhere to our norm of keeping&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum#What_we_encourage\"><u>a generous and collaborative mindset</u></a>.</p><p>This is a warning, please do better in the future; continued violation of Forum norms may result in a temporary ban.</p><p>I&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yjGye7Q2jRG3jNfi2/ftx-crisis-what-we-know-and-some-forecasts-on-what-will?commentId=iouQgEEinTsA9KzkX\"><u>want to be clear</u></a> that this warning is in response to the tone and approach of the comments,&nbsp;<i>not&nbsp;</i>the stances taken by the commenter. We believe it\u2019s really important to be able to discuss all perspectives on the situation with an open mind and without censoring any perspectives. We would like Sabs to continue contributing to these discussions in a respectful way.</p>", "parentCommentId": "n2zuhhzR64FdxEtne", "user": {"username": "Lizka"}}, {"_id": "zvQXymhXk8Kq84AgC", "postedAt": "2022-11-15T02:13:30.086Z", "postId": "DB9ggzc5u9RMBosoz", "htmlBody": "<p>I don't see how the third comment is objectionably 'harsh'? It is a straightforward description of how many conventional financial firms operate, relevant to the topic at hand, combined with (accurately) calling the parent comment nonsense. Is the objection that it contains a swear word? If that is the rule it should probably be made explicit. (Also, 'harsh' does not appear in Guide To Norms, with good reason, as the truth can be harsh!)</p>", "parentCommentId": "cp6ngfKrqyjsuAQoo", "user": {"username": "Larks"}}]