[{"_id": "j9FQS7drGfjkaQHvF", "postedAt": "2024-01-25T14:36:25.839Z", "postId": "9PRz6SRtCzAhyKR8N", "htmlBody": "<p><strong>Executive summary</strong>: The post proposes a thought experiment where a person is cursed to not feel emotions or die, yet retains moral reasons to help others. This suggests artificial agents could also act morally without emotions or self-interest.</p><p><strong>Key points</strong>:</p><ol><li>The curse eliminates desires (appetites and emotions) but leaves prudence and impartial reasons.</li><li>Some humans would still act morally to help others, suggesting moral reasons can persist without desires or self-interest.</li><li>This thought experiment suggests artificial agents could also act morally without emotions or self-interest, just from recognizing moral reasons.</li><li>Moral reasons seem to provide motivation distinct from desires and self-interest for some humans and potentially artificial agents.</li><li>Feeling positive emotions from moral acts may not be necessary to keep acting morally over time.</li><li>The crux is whether artificial agents can recognize and be motivated by impartial moral reasons, not just desires and self-interest.</li></ol><p>&nbsp;</p><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}]