[{"_id": "3EFwjXF5DfvcHjJfD", "postedAt": "2017-09-18T19:30:39.085Z", "postId": "MCfa6PaGoe6AaLPHR", "htmlBody": "<p>One thing I find meta-interesting about s-risk is that s-risk is included in the sort of thing we were pointing at in the late 90s before we started talking about x-risk, and so to my mind s-risk has always been part of the x-risk mitigation program but, as you make clear, that's not how it's been communicated.</p>\n<p>I wonder if there are types of risks for the long-term future we implicitly would like to avoid but have accidentally explicitly excluded from both x-risk and s-risk definitions.</p>\n", "parentCommentId": null, "user": {"username": "gworley3"}}, {"_id": "8birDQzSMtjGQAXxP", "postedAt": "2017-09-19T00:11:11.796Z", "postId": "MCfa6PaGoe6AaLPHR", "htmlBody": "<blockquote>\n<p>the sort of thing we were pointing at in the late 90s before we started talking about x-risk</p>\n</blockquote>\n<p>I'd be interested to hear more about that if you want to take the time. </p>\n", "parentCommentId": "3EFwjXF5DfvcHjJfD", "user": {"username": "Brian_Tomasik"}}, {"_id": "msvivjDFGZiuwrrZy", "postedAt": "2017-09-19T00:50:18.336Z", "postId": "MCfa6PaGoe6AaLPHR", "htmlBody": "<p>My memory is somewhat fuzzy here because it was almost 20 years ago, but I seem to recall discussions on Extropians about far future &quot;bad&quot; outcomes. In those early days much of the discussion focused around salient outcomes like &quot;robots wipe out humans&quot; that we picked up from fiction or outcomes that let people grind their particular axes (capitalist dystopian future! ecoterrorist dystopian future! ___ dystopian future!), but there was definitely more serious focus around some particular issues.</p>\n<p>I remember we worried a lot about grey goo, AIs, extraterrestrial aliens, pandemics, nuclear weapons, etc. A lot of it was focused on getting wiped out (existential threats), but some of it was about undesirable outcomes we wouldn't want to live in. Some of this was about s-risks I'm sure, but I feel like a lot of it was really more about worries over value drift.</p>\n<p>I'm not sure there's much else there, though. We knew bad outcomes were possible, but we were mostly optimistic and hadn't developed anything like the risk-avoidance mindset that's become relatively more prevalent today.</p>\n", "parentCommentId": "8birDQzSMtjGQAXxP", "user": {"username": "gworley3"}}, {"_id": "5vNjPgGbxwd2ucr63", "postedAt": "2017-09-19T20:37:16.917Z", "postId": "MCfa6PaGoe6AaLPHR", "htmlBody": "<p>Thanks for writing this up! Having resources like this explaining ideas seems pretty uncontroversially good.</p>\n", "parentCommentId": null, "user": {"username": "Alex_Barry"}}, {"_id": "FiK263bRoFmjC2Emw", "postedAt": "2017-09-20T18:45:57.327Z", "postId": "MCfa6PaGoe6AaLPHR", "htmlBody": "<p>This is very helpful, as is Max's write-up linked to at the top.</p>\n", "parentCommentId": null, "user": {"username": "zdgroff"}}, {"_id": "3kSLdMk5QZ4Kjv9at", "postedAt": "2017-09-26T15:00:33.136Z", "postId": "MCfa6PaGoe6AaLPHR", "htmlBody": "<p>This sentence in your post caught my attention: &quot; Even if the fraction of suffering decreases, it's not clear whether the absolute amount will be higher or lower.&quot; </p>\n<p>To me, it seems like suffering should be measured by suffering / population, rather than by the total amount of suffering. The total amount of suffering will grow naturally with the population, and suffering / population seems to give a better indication of the severity of the suffering (a small group suffering a large amount is weighted higher than a large group suffering a small amount, as I intuitively think is correct). </p>\n<p>My primarily concern with this (simplistic) method of measuring the severity of suffering is that it ignores the distribution of suffering within a population (i.e there could be a sub population with a large amount of suffering). However, I don't think that's a compelling enough reason to discount working to minimize the fraction of suffering rather than absolute suffering. </p>\n<p>Are there compelling arguments for why we should seek to minimize total suffering?</p>\n", "parentCommentId": null, "user": {"username": "aspencer"}}, {"_id": "AfEHBgsfuJ4J9DFKg", "postedAt": "2017-09-27T20:44:26.127Z", "postId": "MCfa6PaGoe6AaLPHR", "htmlBody": "<p>How do you feel about the <a href=\"https://en.wikipedia.org/wiki/Mere_addition_paradox\">mere addition paradox</a>? These questions are not simple.</p>\n", "parentCommentId": "3kSLdMk5QZ4Kjv9at", "user": {"username": "WillPearson"}}, {"_id": "KLhCCbfFcrzDwrHgj", "postedAt": "2017-09-28T06:08:11.110Z", "postId": "MCfa6PaGoe6AaLPHR", "htmlBody": "<p>If I understand right, the view you're proposing is sort of like the 'average view' of utilitarianism. The objective is to minimize the average level of suffering across a population.</p>\n<p>A common challenge to this view (shared with average util) is that it seems you can make a world better by adding lives which suffer, but suffer less than the average. In some hypothetical hellscape where everyone is getting tortured, adding further lives where people get tortured slightly less severely should make the world even worse, not better.</p>\n<p>Pace the formidable challenges of infinitarian ethics, I generally lean towards total views. I think the intuition you point to (which I think is widely shared) in that larger degrees of suffering should 'matter more' is perhaps better accommodated in something like prioritarianism, whereby improving the well-being of the least well off is given extra moral weight to its utilitarian 'face value'. (FWIW, I generally lean towards pretty flat footed utilitarianism, as there some technical challenges with prioritarianism, and it seems hard to distinguish the empirical from the moral matters: there are evolutionary motivations (H/T Carl Shulman) why there should be extremely severe pain, so maybe a proper utilitarian accounting makes relieving these extremes worth very large amounts of more minor suffering).</p>\n<p>Aside: in population ethics there's a well-worn problem of aggregation, as suggested by the repugnant conclusion: lots and lots of tiny numbers when put together can outweigh a big numbers, so total views have challenges such as: &quot;Imagine A where 7 billion people live lives of perfect bliss, versus B where these people suffer horrendous torture, but TREE(4) people with lives that are only just barely worth living&quot;. B is far better than A, yet it seems repulsive. (The usual total view move is to appeal to scope insensitivity and that our intuitions here are ill-suited to tracking vast numbers. I don't think perhaps more natural replies (e.g. 'discount positive wellbeing if above zero but below some threshold close to it'), come out in the wash).</p>\n<p>Unfortunately, the 'suffering only' suggested as a potential candidate in the FAQ (i.e. discount 'positive experiences', and only work to reduce suffering) seems to compound these, as in essence one can concatenate these problems of population ethics with the counter-intuitiveness of this discounting of positive experience (virtually everyone's expressed and implied preferences indicate positive experiences have free-standing value, as they are willing to trade off between negative and positive).</p>\n<p>The aggregation challenge akin to the repugnant conclusion (which I think I owe to Carl Shulman) goes like this. Consider A: 7 billion people suffering horrendous torture. Now consider B: TREE(4) people enjoying lifelong eudaimonic bliss with the exception of each suffering a single pinprick. On a total suffering view A &gt;&gt;&gt; B, yet this seems common-sensically crazy. </p>\n<p>The view seems to violate two intuitions, the first the aggregation issue (i.e. TREE(4) pinpricks is more morally important than 7 billion cases of torture), but also the discounting of positive experience - the 'suffering only counts view' is indifferent to the difference of TREE(4) instances of lifelong eudaimonic bliss between the scenarios. If we imagine world C where no one exists a total util view gets the intuitively 'right' answer (i.e. B &gt; C &gt; A), whilst the suffering view gets most of the pairwise comparisons intuitively wrong (i.e. C &gt; A &gt; B)</p>\n", "parentCommentId": "3kSLdMk5QZ4Kjv9at", "user": {"username": "Gregory_Lewis"}}]