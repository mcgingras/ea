[{"_id": "a68umLaa2wjzPjejX", "postedAt": "2023-09-25T18:01:24.779Z", "postId": "EjGowxHhRifb2r8tE", "htmlBody": "<p>Fantastic news. Note: don\u2019t forget to share it on LessWrong too.</p>\n", "parentCommentId": null, "user": {"username": "jaythibs"}}, {"_id": "ApbDBsBzWGFLASv8j", "postedAt": "2023-09-25T21:19:16.914Z", "postId": "EjGowxHhRifb2r8tE", "htmlBody": "<p><strong>Executive summary</strong>: The Future of Life Institute is offering PhD and postdoc fellowships in AI Existential Safety for 2024, aiming to foster a cohort of researchers in this field, with no geographic limitations on applicants or host universities.</p><p><strong>Key points</strong>:</p><ol><li>PhD fellowships provide an $80,000 stipend and $10,000 research fund, open until November 16, 2023.</li><li>Postdoc fellowships cover up to $40,000 tuition and fees plus a $10,000 research fund, open until January 2, 2024.</li><li>Fellows participate in workshops and activities to network with others in AI existential safety.</li><li>Applications are inclusive - all backgrounds encouraged, especially women and minorities.</li><li>Requirement is working on AI existential safety with an advisor's support.</li><li>Application is through the FLI grants website - spread the word to potential applicants.</li></ol><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}, {"_id": "mjKPk63YiAXpCYXSG", "postedAt": "2023-09-25T21:32:13.434Z", "postId": "EjGowxHhRifb2r8tE", "htmlBody": "<p>The post seems to confuse the postdoctoral fellowship and the PhD fellowship (assuming the text on the grant interface is correct). It's the postdoc fellowship that has an $80,000 stipend, whereas the PhD fellowship stipend is $40,000.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/mjKPk63YiAXpCYXSG/hvn90my7q629dlddzscu\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/mjKPk63YiAXpCYXSG/rbg5xooeulqbvvk03vyd 290w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/mjKPk63YiAXpCYXSG/cg3dvdqfllta9f9wxjtm 580w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/mjKPk63YiAXpCYXSG/ez0joppgoht1hmtmm1a2 870w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/mjKPk63YiAXpCYXSG/fztqlvv0etv70r8qztk9 1160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/mjKPk63YiAXpCYXSG/jvodcndn1nr0yclhkrcw 1450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/mjKPk63YiAXpCYXSG/bruyjlhoimsrpouvvwoq 1740w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/mjKPk63YiAXpCYXSG/bahwceognroaqprbm1tw 2030w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/mjKPk63YiAXpCYXSG/bza6epkhqfkjpdpxopvb 2320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/mjKPk63YiAXpCYXSG/dr03wltsvsccwopch2bc 2610w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/mjKPk63YiAXpCYXSG/llx0f5vlymutbdqynded 2810w\"></figure>", "parentCommentId": null, "user": {"username": "Stefan_Schubert"}}, {"_id": "WAwhnx4wHCtwcHiCK", "postedAt": "2023-09-27T16:09:07.513Z", "postId": "EjGowxHhRifb2r8tE", "htmlBody": "<p>Thank you for spotting it! I just did the fix :).</p>", "parentCommentId": "mjKPk63YiAXpCYXSG", "user": {"username": "Zhijing Jin"}}, {"_id": "ydPEgfetdAy9wfzkh", "postedAt": "2023-09-27T16:10:26.477Z", "postId": "EjGowxHhRifb2r8tE", "htmlBody": "<p>Good idea! Just made the<a href=\"https://www.lesswrong.com/posts/4Zk53JRaot2NKaafs/\"> other post</a> to reach more audience!</p>", "parentCommentId": "a68umLaa2wjzPjejX", "user": {"username": "Zhijing Jin"}}]