[{"_id": "sch7niutE4Nm7nGs5", "postedAt": "2023-12-20T14:56:33.826Z", "postId": "FAJ5DCmucKnabkBNL", "htmlBody": "<p>Hey Ben, thanks for this great post. Really interesting to read about your experience and decision not to continue in this space.</p><p>I'm wondering if you have any sense of how quickly returns to new projects in this space might diminish? Founding an AI policy research and advocacy org seems like a slam dunk, but I'm wondering how many more ideas nearly that promising are out there.</p>", "parentCommentId": null, "user": {"username": "Stephen Clare"}}, {"_id": "Mj8fFrzB6auzjnAbv", "postedAt": "2023-12-20T18:53:30.094Z", "postId": "FAJ5DCmucKnabkBNL", "htmlBody": "<blockquote><p>We often seemed to be considering projects that didn\u2019t really need \u201cincubating\u201d, as opposed to convincing an existing team or org to work on them (e.g. a new research agenda), and this felt like a feature of the AI x-risk space.</p></blockquote><p>This in particular resonated with me and largely reflects how I updated my views on AI x/gc-risk incubation.</p><p>My (also low resilience take) is that the AI safety ecosystem can probably more effectively get a project or idea off the ground where there's already pooled infrastructure and other resources (e.g. larger existing think tanks). And that leveraging the incubation experience &amp; skills you mentioned above (and bringing them in a coordinated way to) to these existing infrastructures could more effectively accelerate new projects, rather than having to pool many different resources (networks, talent, funds, expertise, reputation, track-record, etc.) together by oneself (with a smaller team).</p><p>I said this elsewhere but I'll repeat myself: I think it's fantastic that you took the time to write this up (and erred on the side of posting) and in doing this adding transparency to the LT/x-risk entrepreneurship community which seems at times impenetrable.</p>", "parentCommentId": null, "user": {"username": "CristinaSchmidtIb\u00e1\u00f1ez"}}, {"_id": "2EjZEyZPjjn86yQwx", "postedAt": "2023-12-22T08:05:13.789Z", "postId": "FAJ5DCmucKnabkBNL", "htmlBody": "<blockquote><p>Outreach to more traditional entrepreneurs, attempting to bridge the EA vs traditional entrepreneurship cultural divide.</p></blockquote><p>Cf. \"<a href=\"https://www.lesswrong.com/posts/PcTLHamp236afJxxT/some-for-profit-ai-alignment-org-ideas\">Some for-profit AI alignment org ideas</a>\"</p>", "parentCommentId": null, "user": {"username": "Roman Leventov"}}, {"_id": "wbo3EzTcHKbTJWjkm", "postedAt": "2023-12-22T08:10:59.104Z", "postId": "FAJ5DCmucKnabkBNL", "htmlBody": "<p><a href=\"https://www.lesswrong.com/posts/FnwqLB7A9PenRdg4Z/for-alignment-we-should-simultaneously-use-multiple-theories#Creating_as_many_new_conceptual_approaches_to_alignment_as_possible__No\">I've earlier argued</a> against the sentiment that \"we need as many technical approaches ('stabs') to solve the AI alignment problem\", and therefore, probably, new organisations to pursue these technical agendas, too.</p>", "parentCommentId": "sch7niutE4Nm7nGs5", "user": {"username": "Roman Leventov"}}, {"_id": "NyxytdAZ7kYMDQwLi", "postedAt": "2023-12-22T12:27:53.962Z", "postId": "FAJ5DCmucKnabkBNL", "htmlBody": "<p>\"It now feels to me like the systematic, weighted-factor-model approach we used for project research wasn't the best choice. I think that something more focused on getting and really understanding the views of central AI x-risk people would have been better.\"</p>\n<p>I'd be interested in a bit more detail about this if you don't mind sharing? Why did you conclude that it wasn't a great approach, and why would better understanding the views of central AI x-risk people help?</p>\n", "parentCommentId": null, "user": {"username": "Jamie_Harris"}}, {"_id": "CHhqCSLjCvCBGdgEC", "postedAt": "2023-12-23T11:31:30.628Z", "postId": "FAJ5DCmucKnabkBNL", "htmlBody": "<p>Thanks for writing this up. We're running an incubation pilot at Impact Academy and found this post very helpful as a reference class (for comparison in terms of success) as well as providing strategic clarity.<br>I'm curious, what were the best initiatives (inside and outside of EA) you came across in your search (e.g., y-combinator, charity entrepreneurship, etc.)?<br><br>&nbsp;</p>", "parentCommentId": null, "user": {"username": "SebastianSchmidt"}}, {"_id": "3YHrPvumB8pQMgPXx", "postedAt": "2024-01-22T12:41:23.910Z", "postId": "FAJ5DCmucKnabkBNL", "htmlBody": "<p>Hi Stephen, thanks for the kind words!</p><blockquote><p>I'm wondering if you have any sense of how quickly returns to new projects in this space might diminish? Founding an AI policy research and advocacy org seems like a slam dunk, but I'm wondering how many more ideas nearly that promising are out there.</p></blockquote><p>&nbsp;</p><p>I guess my rough impression is that there's lots of possible great new projects if there's a combination of a well-suited founding team and support for that team. But \"well-suited founding team\" might be quite a high bar.</p>", "parentCommentId": "sch7niutE4Nm7nGs5", "user": {"username": "Ben_Snodin"}}, {"_id": "GivYZExdaRZhBGRhm", "postedAt": "2024-01-22T12:51:13.738Z", "postId": "FAJ5DCmucKnabkBNL", "htmlBody": "<p>Like a lot of this post, this is a bit of an intuition-based 'hot take'. But some quick things that come to mind: i) iirc it didn't seem like our initial intuitions were very different to the WFM results, ii) when we filled in the weighted factor model I think we had a pretty limited understanding of what each project involved (so you might not expect super useful results), iii) I got a bit more of a belief that it just matters a lot that central-AI-x-risk people have a lot of context (and that this more than offsets the a risk of bias and groupthink) so understanding their view is very helpful, iv) having a deep understanding of the project and the space just seems very important for figuring out what if anything should be done and what kinds of profiles might be best for the potential founders</p>", "parentCommentId": "NyxytdAZ7kYMDQwLi", "user": {"username": "Ben_Snodin"}}, {"_id": "fstjyZu8WMYCcn2FY", "postedAt": "2024-01-22T12:59:45.388Z", "postId": "FAJ5DCmucKnabkBNL", "htmlBody": "<p>I don't necessarily have a great sense for how good each one is, but here are some names. Though I expect you're already familiar with all of them :).</p><p>EA / x-risk -related</p><ul><li>Future of Life Foundation</li><li>Active grantmaking, which might happen e.g. at Open Phil or Longview or Effective Giving, is a bit like incubation</li><li>(Charity Entrepreneurship of course, as you mentioned)</li></ul><p>Outside EA</p><ul><li>Entrepreneur First seems impressive, though I'm not that well placed to judge</li><li>Maybe this is nitpicking: As far as I know Y-Combinator is an accelerator rather than an incubator (ie it's focused on helping out existing startups rather than helping people get something started)</li></ul><p>PS: good luck with your incubation work at Impact Academy! :)</p>", "parentCommentId": "CHhqCSLjCvCBGdgEC", "user": {"username": "Ben_Snodin"}}, {"_id": "ikHzeuo9fmQa9mmh6", "postedAt": "2024-01-25T11:13:02.291Z", "postId": "FAJ5DCmucKnabkBNL", "htmlBody": "<p>Thanks for your response Ben. All of these were on my radar but thanks for sharing.&nbsp;<br><br>Good luck with what you'll be working on too!</p>", "parentCommentId": "fstjyZu8WMYCcn2FY", "user": {"username": "SebastianSchmidt"}}]