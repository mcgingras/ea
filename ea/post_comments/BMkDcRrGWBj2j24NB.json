[{"_id": "Tc5DfwDeFBMKLQBoF", "postedAt": "2022-10-01T15:50:45.158Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<blockquote><p>Current theories are missing important criteria that might be relevant to artificial consciousness because they're defined primarily with the goal of distinguishing conscious from unconscious states of human brains (or possibly conscious human brains from unconscious animals, or household objects). They aren't built to distinguish humans from crude representations of human architectures. It is open to most theorists to expand their theories to exclude digital minds.</p></blockquote><p>&nbsp;</p><p>I think this is a key point that counts strongly against the possibility of building conscious AI so soon, as in the title. Some theories of consciousness are basically theories of human (and sometimes animal) consciousness, really just explaining which neural correlates predict subjective report, but do not claim those minimal neural correlates generate consciousness in any system, so building a computer or writing software that just meets their minimally stated requirements should not be taken as generating consciousness. I think Global Workspace Theory and Attention Schema Theory are theories like this, although Graziano, the inventor of AST, also describes how consciousness is generated in the general case, i.e. illusionism, and how AST relates to this, i.e. the attention schema is where the \"illusion\" is generated in practice in animals. (I'm not as familiar with other theories, so won't comment on them.)</p><p>If you overextend such theories of consciousness, you can get panpsychism, and we already have artificial or otherwise non-biological consciousness:</p><ol><li><a href=\"https://www.unil.ch/files/live/sites/philo/files/shared/DocsPerso/EsfeldMichael/2007/herzog.esfeld.gerstner.07.pdf\">https://www.unil.ch/files/live/sites/philo/files/shared/DocsPerso/EsfeldMichael/2007/herzog.esfeld.gerstner.07.pdf</a></li><li><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6074090/\">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6074090/</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/YE9d4yab9tnkK5yfd/physical-theories-of-consciousness-reduce-to-panpsychism\">https://forum.effectivealtruism.org/posts/YE9d4yab9tnkK5yfd/physical-theories-of-consciousness-reduce-to-panpsychism</a> (I just updated this post to say that I think I was overextending)</li></ol>", "parentCommentId": null, "user": {"username": "MichaelStJules"}}, {"_id": "JEpTDsp29D4dyKnhb", "postedAt": "2022-10-01T17:11:56.684Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<p>I would find this more persuasive if it had thorough references to the existing science of consciousness. I was under the impression that we still don\u2019t know what the necessary conditions for consciousness are, and there are many competing theories on the topic. Stating that one theory is correct doesn\u2019t answer that key question for me.</p>\n", "parentCommentId": null, "user": {"username": "Aidan O'Gara"}}, {"_id": "QuKRjuMKZtcmA4pdt", "postedAt": "2022-10-01T17:26:20.546Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<blockquote>\n<p>Some theories of consciousness are basically theories of human (and sometimes animal) consciousness, really just explaining which neural correlates predict subjective report, but do not claim those minimal neural correlates generate consciousness in any system, so building a computer or writing software that just meets their minimally stated requirements should not be taken as generating consciousness.</p>\n</blockquote>\n<p>From the 50s to the 90s, there was a  lot of debate about the basic nature of consciousness and its relation to the brain. The theory that emerged from that debate as the most plausible physicalist contender suggested that it was something about the functional structure of the brain that matters for consciousness. Materials aren't relevant, just abstract patterns. These debates were very high level and the actual functional structures responsible for consciousness weren't much discussed, but they suggested that we could fill in the details and use those details to tell whether AI systems were conscious.</p>\n<p>From the 90s to the present, there have been a number of theories developed that look like they aim to fill in the details of the relevant functional structures that are responsible for consciousness. I think these theories are most plausibly read as specific versions of functionalism. But you're right, the people who have developed them often haven't committed fully either to functionalism or to the completeness of the functional roles they describe. They would probably resist applying them in  crude ways to AIs.</p>\n<p>The theorists who might resist the application of modern functionalist theories to digital minds are generally pretty silent on what might be missing in the functional story they tell (or what else might matter apart from functional organization). I think this would undercut any authority they might have in denying consciousness to such systems, or even raising doubts about it.</p>\n<p>Suppose that Replika produced a system that had  perceptual faculties and a global workspace,  that tracked its attention and utilized higher-order representations of its own internal states in deciding what to do. Suppose they announced to the media that they had created a digital person, and charged users $5 an hour to talk to it.  Suppose that Replika told journalists that they had worked hard to implement Graziano's theory in their system, and yes, it was built out of circuits, but everything special about human consciousness was coded in there. What would people's reactions be? What would Graziano say about it? I doubt he could come up with many compelling reasons to think it wasn't conscious, even if he could say that his theory wasn't technically intended to apply to such systems. This leaves curious public in the <em>who can really say</em> camp reminiscent of solipsism or doubts about the consciousness of dogs.  I think they'd fall back on problematic heuristics.</p>\n", "parentCommentId": "Tc5DfwDeFBMKLQBoF", "user": {"username": "Derek Shiller"}}, {"_id": "GfAZfnuztiPKquudq", "postedAt": "2022-10-01T17:39:03.038Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<blockquote>\n<p>I was under the impression that we still don\u2019t know what the necessary conditions for consciousness are</p>\n</blockquote>\n<p>We definitely don't, and I hope I haven't committed myself to any one theory. The point is that the most developed views provide few obstacles. Those views tend to highlight different facets of human cognitive architecture. For instance, it may be some form of self-representation that matters, or the accessibility of representations to various cognitive modules. I didn't stress this enough: of the many views, we may not know which is right, but <strong>it wouldn't be technically hard to satisfy them all</strong>. After all, human cognitive architecture satisfies every plausible criterion of consciousness.</p>\n<p>On the other hand, it is controversial whether any of the developed views are remotely right. There are some people who think we've gone in the wrong direction. However, these people generally don't have specific alternative proposals that clearly come down one way or another on AI systems.</p>\n", "parentCommentId": "JEpTDsp29D4dyKnhb", "user": {"username": "Derek Shiller"}}, {"_id": "FAErrHKZ42HdmHbN9", "postedAt": "2022-10-01T18:37:41.096Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<blockquote><p>The theorists who might resist the application of modern functionalist theories to digital minds are generally pretty silent on what might be missing in the functional story they tell (or what else might matter apart from functional organization).</p></blockquote><p>If they weren't silent, they would be proposing further elaborated theories.</p><blockquote><p>I think this would undercut any authority they might have in denying consciousness to such systems, or even raising doubts about it.</p></blockquote><p>Maybe, but I don't think we would be justified in giving much weight to the extension of their theory of consciousness to artificial entities. Taking GWT, for example, many things could qualify as a global workspace.</p><blockquote><p>Suppose that Replika produced a system that had perceptual faculties and a global workspace, that tracked its attention and utilized higher-order representations of its own internal states in deciding what to do.</p></blockquote><p>So far, I think this would be possible with &lt;1000 neurons, and very probably 100 or fewer, if you interpret all of those things minimally. For example, a single neuron to represent an internal state and another another neuron for a higher-order representation of that internal state. Of course, maybe that's still conscious, but the bar seems very low.</p><blockquote><p>Suppose that Replika told journalists that they had worked hard to implement Graziano's theory in their system, and yes, it was built out of circuits, but everything special about human consciousness was coded in there. What would people's reactions be? What would Graziano say about it? I doubt he could come up with many compelling reasons to think it wasn't conscious, even if he could say that his theory wasn't technically intended to apply to such systems. This leaves curious public in the <i>who can really say</i> camp reminiscent of solipsism or doubts about the consciousness of dogs. I think they'd fall back on problematic heuristics.</p></blockquote><p>\"everything special about human consciousness was coded in there\" sounds like whole brain emulation, and I think Graziano and most functionalists should and would endorse its consciousness. Generally, though, I think Graziano and other illusionists would want to test whether it treats its own internal states or information processing as having or seeming to have properties consciousness seems to have, like being<i> </i>mysterious/unphysical/ineffable. Maybe some more specific tests, too. <a href=\"https://www.pnas.org/doi/10.1073/pnas.2102421118\">Wilterson and Graziano have already come up with artificial agents with attention schemas</a>, which should be conscious if you overextended the neural correlate claims of AST too literally and forgot the rest, but would not be conscious according to Graziano (or AST), since it wouldn't pass the illusionist tests. <a href=\"https://www.tandfonline.com/doi/abs/10.1080/17588928.2020.1838468\"><u>Graziano wrote</u></a>:&nbsp;</p><blockquote><p>Suppose the machine has a much richer model of attention. Somehow, attention is depicted by the model as a Moray eel darting around the world. Maybe the machine already had need for a depiction of Moray eels, and it coapted that model for monitoring its own attention. Now we plug in the speech engine. Does the machine claim to have consciousness? No. It claims to have an external Moray eel.</p><p>Suppose the machine has no attention, and no attention schema either. But it does have a self-model, and the self-model richly depicts a subtle, powerful, nonphysical essence, with all the properties we humans attribute to consciousness. Now we plug in the speech engine. Does the machine claim to have consciousness? Yes. The machine knows only what it knows. It is constrained by its own internal information.</p><p>AST does not posit that having an attention schema makes one conscious. Instead, first, having an automatic self-model that depicts you as containing consciousness makes you intuitively believe that you have consciousness. Second, the reason why such a self-model evolved in the brains of complex animals, is that it serves the useful role of modeling attention.</p></blockquote><p>I suppose my claim about AST in my first comment is inaccurate. AST is in fact a general theory of consciousness, but&nbsp;</p><ol><li>the general theory is basically just illusionism (AFAIK), and</li><li>having an attention schema is neither necessary nor sufficient for consciousness in general, but it is both in animals <i>in practice</i>.</li></ol>", "parentCommentId": "QuKRjuMKZtcmA4pdt", "user": {"username": "MichaelStJules"}}, {"_id": "W6cGoMzs3gPLHp4Kc", "postedAt": "2022-10-01T19:06:37.927Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<blockquote>\n<p>For example, a single neuron to represent an internal state and another another neuron for a higher-order representation of that internal state.</p>\n</blockquote>\n<p>This requires an extremely simplistic theory of representation, but yeah, if you allow any degree of crudeness you might get consciousness in very simple systems.</p>\n<p>I suppose you could put my overall point this way:  current theories present very few technical obstacles, so there it would take little effort to build a system which would be difficult to rule out.  Even if you think we need more criteria to avoid getting stuck with panpsychism, we don't have those criteria and so can't wield them to do any work in the near future.</p>\n<blockquote>\n<p>\"everything special about human consciousness was coded in there\" sounds like whole brain emulation</p>\n</blockquote>\n<p>I mean everything that is plausibly relevant according to current theories, which is a relatively short list. There is a big gulf between everything people have suggested is necessary for consciousness and a whole brain emulation.</p>\n<blockquote>\n<p>Generally, though, I think Graziano and other illusionists would want to test whether it treats its own internal states or information processing as having or seeming to have properties consciousness seems to have, like being mysterious/unphysical/ineffable.</p>\n</blockquote>\n<p>It has been awhile since I've read Graziano -- but if I recall correctly (and as your quote illustrates) he likes both illusionism and an attention schema theory. Since illusionism denies consciousness, he can't take AST as a theory of what consciousness is; he treats it instead as a theory of the phenomena that leads us to puzzle mistakenly about consciousness. If that is right, he should think that any artificial mind might be led by an AST architecture, even a pretty crude one, to make mistakes about mind-brain relations and that isn't indicative of any further interesting phenomenon. The question of the consciousness of artificial systems is settled decisively in the negative by illusionism.</p>\n", "parentCommentId": "FAErrHKZ42HdmHbN9", "user": {"username": "Derek Shiller"}}, {"_id": "ucjxRBnN4qtdz6x8Q", "postedAt": "2022-10-01T19:34:59.504Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<blockquote><p>I suppose you could put my overall point this way: current theories present very few technical obstacles, so there it would take little effort to build a system which would be difficult to rule out. Even if you think we need more criteria to avoid getting stuck with panpsychism, we don't have those criteria and so can't wield them to do any work in the near future.</p></blockquote><p>This is also my impression of the theories with which I'm familiar, except illusionist ones. I think only illusionist theories actually give plausible accounts of consciousness in general, as far as I'm aware, and I think they probably rule out panpsychism, but I'm not sure (if small enough animal brains are conscious, and counterfactual robustness is not necessary, then you might get panspychism again).</p><p>&nbsp;</p><blockquote><p>I mean everything that is plausibly relevant according to current theories, which is a relatively short list. There is a big gulf between everything people have suggested is necessary for consciousness and a whole brain emulation.</p></blockquote><p>Fair. That's my impression, too.</p><p>&nbsp;</p><blockquote><p>It has been awhile since I've read Graziano -- but if I recall correctly (and as your quote illustrates) he likes both illusionism and an attention schema theory. Since illusionism denies consciousness, he can't take AST as a theory of what consciousness is; he treats it instead as a theory of the phenomena that leads us to puzzle mistakenly about consciousness. If that is right, he should think that any artificial mind might be led by an AST architecture, even a pretty crude one, to make mistakes about mind-brain relations and that isn't indicative of any further interesting phenomenon. The question of the consciousness of artificial systems is settled decisively in the negative by illusionism.</p></blockquote><p>I guess this is a matter of definitions. I wouldn't personally take illusionism as denying consciousness outright, and instead illusionism says that consciousness does not actually have the apparently inaccessible, ineffable, unphysical or mysterious properties people often attribute to it, and it's just the appearance/depiction/illusion of such properties that makes a system conscious. At any (typo) rate, whether consciousness is a real phenomenon or not, however we define it, I would count systems that have illusions of consciousness, or specifically illusions of conscious evaluations (pleasure, suffering, \"conscious\" preferences) as moral patients and consider their interests in the usual ways. (Maybe with some exceptions that don't count, like giant lookup tables and some other systems that don't have causal structures at all resembling our own.) This is also Luke Muehlhauser's approach in <a href=\"https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood\">2017 Report on Consciousness and Moral Patienthood</a>.</p>", "parentCommentId": "W6cGoMzs3gPLHp4Kc", "user": {"username": "MichaelStJules"}}, {"_id": "86pEs9uHEffz9uFFD", "postedAt": "2022-10-01T22:41:19.738Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<p>Also see section 6.2 in <a href=\"https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood\">https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood</a> for discussion of some specific theories and that they don't answer the hard problem.</p>", "parentCommentId": "Tc5DfwDeFBMKLQBoF", "user": {"username": "MichaelStJules"}}, {"_id": "GeSybxmsadrzEHnoS", "postedAt": "2022-10-02T00:02:24.258Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<blockquote>\n<p>I guess this is a matter of definitions.</p>\n</blockquote>\n<p>I agree that this sounds semantic. I  think of illusionism as a type of error theory, but people in this camp have always been somewhat cagey what they're denying and there is a range of interesting theories.</p>\n<blockquote>\n<p>At an rate, whether consciousness is a real phenomenon or not, however we define it, I would count systems that have illusions of consciousness, or specifically illusions of conscious evaluations (pleasure, suffering, \"conscious\" preferences) as moral patients and consider their interests in the usual ways.</p>\n</blockquote>\n<p>Interesting. Do you go the other way too? E.g. if a creature doesn't have illusions of consciousness, then it isn't a moral patient?</p>\n", "parentCommentId": "ucjxRBnN4qtdz6x8Q", "user": {"username": "Derek Shiller"}}, {"_id": "6gJiywnDHLxxWeu7C", "postedAt": "2022-10-02T02:30:23.116Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<p>Wow, that is a strong claim!</p><p>Could these conscious AI also have affective experience?</p>", "parentCommentId": null, "user": {"username": "Noah Scales"}}, {"_id": "7opmAnA24uEXNJqgs", "postedAt": "2022-10-02T02:51:08.975Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<p>Assuming illusionism is true, then yes, I think only those with illusions of consciousness are moral patients.</p>\n", "parentCommentId": "GeSybxmsadrzEHnoS", "user": {"username": "MichaelStJules"}}, {"_id": "g68ForWvJTsd7mhBf", "postedAt": "2022-10-02T21:18:36.963Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<p>Perhaps I oversold the provocative title. But I do think that affective experiences are much harder, so even if there is a conscious AI it is unlikely to have the sorts of morally significant states we care about. While I think that it is plausible that current theories of consciousness might be relatively close to complete, I'm less sympathetic that current theories of valence are plausible as relatively complete accounts. There has been much less work in this direction.</p>\n", "parentCommentId": "6gJiywnDHLxxWeu7C", "user": {"username": "Derek Shiller"}}, {"_id": "bLJPKgEtYMiBpq5XN", "postedAt": "2022-10-02T22:24:09.281Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<p>Which makes me wonder how anyone expects to identify whether software entities have affective experience.</p><p>Is there any work in this direction that you like and can recommend?</p>", "parentCommentId": "g68ForWvJTsd7mhBf", "user": {"username": "Noah Scales"}}, {"_id": "BwZbyvT7hKbsjiS4o", "postedAt": "2022-10-03T10:05:12.924Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<p>Weak downvoted because I think the title is somewhat misleading given the content of the post (and I think you know this, since you replied below: 'Perhaps I oversold the provocative title.') I think we should discourage clickbait of this nature on the Forum.</p>\n", "parentCommentId": null, "user": {"username": "Bella_Forristal"}}, {"_id": "iPxRjSmjDq8cCWaQa", "postedAt": "2022-10-03T10:53:25.969Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<p>Fair enough</p>\n", "parentCommentId": "BwZbyvT7hKbsjiS4o", "user": {"username": "Derek Shiller"}}, {"_id": "oxTvkYDXAyZrYjuHo", "postedAt": "2022-10-03T13:56:42.179Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<p>There is a growing amount of work in philosophy investigating the basic nature of pain that seems relevant to identifying important valenced experiences in software entities. <em><a href=\"https://www.amazon.com/What-Body-Commands-Imperative-Theory/dp/0262029707/ref=sr_1_1\">What the body commands</a></em> by Colin Klein is a representative and reasonably accessible book-length introduction that pitches one of the current major theories of pain. Applying it to conscious software entities wouldn't be too hard. Otherwise, my impression is that most of the work is too recent and too niche to have accessible surveys yet.</p>\n<p>Overall, I should say that not particularly sympathetic to the theories that people have come up with here, but you might disagree and I don't think you have much reason to take my word for it. In any case they are trying to answer the right questions.</p>\n", "parentCommentId": "bLJPKgEtYMiBpq5XN", "user": {"username": "Derek Shiller"}}, {"_id": "t5wxQQbDH8Bp6HxPo", "postedAt": "2022-12-30T13:09:08.920Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<p>I am wondering if the fact that theories of consciousness relate more to the overall architecture of a system rather than to the low-level workings of that system is a limitation that should be strongly considered, particularly if there are other theories that are more focused on low-level cellular functioning. For example, I've seen a theory from renowned theoretical physicist Roger Penrose (video below) stating that consciousness is a quantum process. If this is the case, then current computers wouldn't be conscious because they aren't quantum devices at the level of individual transistors or circuits. Therefore, no matter what the overall neural architecture is, even in a complicated AI, the system wouldn't be conscious.</p><p>Another interesting point is that the way we incorporate AI into society may be affected by whether the AIs we build are generally sentient. If they are, and we have to respect their rights when developing, implementing, or shutting down such systems; that may create an incentive to do these things more slowly. I think slowing down may be good for the world at large given that plunging into the AI revolution while we still only have this black-box method of designing and training AIs - as in digital neuroscience hasn't progressed enough for us to decode the neural circuits of AIs so that we know what they're actually thinking when they make decisions (https://forum.effectivealtruism.org/posts/rJRw78oihoT5paFGd/high-level-hopes-for-ai-alignment)- seems like a very dangerous result.</p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=Qi9ys2j1ncg\"><div><iframe src=\"https://www.youtube.com/embed/Qi9ys2j1ncg\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure>", "parentCommentId": null, "user": {"username": "andrew_goldbaum"}}, {"_id": "mMhBsBkxghgtsoTkC", "postedAt": "2023-08-11T14:36:05.325Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<blockquote><p>I don't find IIT plausible, despite it's popularity, and am not sure what effect it's inclusion would have on the present arguments.</p></blockquote><p>It sounds like you're giving IIT approximately zero weight in your <a href=\"https://forum.effectivealtruism.org/posts/2WS3i7eY4CdLH99eg/independent-impressions\">all-things-considered view</a>. I find this surprising, given IIT's popularity amongst people who've thought hard about consciousness, and given that you seem aware of this.</p><p>Additionally, I'd be interested to hear how your view may have updated in light of the recent empirical results from the <a href=\"https://en.wikipedia.org/wiki/Integrated_information_theory#Adversarial_Collaboration_to_test_GNW_and_IIT\">IIT-GNWT adversarial collaboration</a>:</p><blockquote><p>In 2019, the <a href=\"https://en.wikipedia.org/wiki/John_Templeton_Foundation\">Templeton Foundation</a> announced funding in excess of $6,000,000 to test opposing empirical predictions of IIT and a rival theory (<a href=\"https://en.wikipedia.org/wiki/Dehaene%E2%80%93Changeux_model\">Global Neuronal Workspace Theory</a> GNWT). The originators of both theories signed off on experimental protocols and data analyses as well as the exact conditions that satisfy if their championed theory correctly predicted the outcome or not. Initial results were revealed in June 2023. None of GNWT's predictions passed what was agreed upon pre-registration while two out of three of IIT's predictions passed that threshold.</p></blockquote>", "parentCommentId": null, "user": {"username": "Will Aldred"}}, {"_id": "HfcWrBpFWzfegzJ9X", "postedAt": "2023-08-12T19:09:00.721Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<blockquote>\n<p>It sounds like you're giving IIT approximately zero weight in your all-things-considered view. I find this surprising, given IIT's popularity amongst people who've thought hard about consciousness, and given that you seem aware of this.</p>\n</blockquote>\n<p>From my experience, there is a significant difference in the popularity of IIT by field. In philosophy, where I got my training, it isn't a view that is widely held. Partly because of this bias, I haven't spent a whole lot of time thinking about it. I have read the seminal papers that introduce the formal model and given the philosophical justifications, but I haven't looked much into the empirical literature. The philosophical justifications seem very weak to me -- the formal model seems very loosely connected to the axioms of consciousness that supposedly motivate it. And without much philosophical justification, I'm wary of the empirical evidence. The human brain is messy enough that I expect you could find evidence to confirm IIT whether or not it is true, if you look long enough and frame your assumptions correctly. That said, it is possible that existing empirical work does provide a lot of support for IIT that I haven't taken the time to appreciate.</p>\n<blockquote>\n<p>Additionally, I'd be interested to hear how your view may have updated in light of the recent empirical results from the IIT-GNWT adversarial collaboration:</p>\n</blockquote>\n<p>If you don't buy the philosophical assumptions, as I do not, then I don't think you should update much on the IIT-GNWT adversarial collaboration results. IIT may have done better, but if the two views being compared were essentially picked out of a hat as representatives of different philosophical kinds, then the fact that one view does better doesn't say much about the kinds. It seems weird to me to compare the precise predictions of theories that are so drastically different in their overall view of things. I don't really like the idea of adversarial approaches across different frameworks. I would think it makes more sense to compare nearby theories to one another.</p>\n", "parentCommentId": "mMhBsBkxghgtsoTkC", "user": {"username": "Derek Shiller"}}, {"_id": "nvjEnFhR7u5wCEtQW", "postedAt": "2023-08-12T22:44:43.504Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<p>Oh, apparently Wilterson and Graziano did admit that artificial agent in their paper is kind of conscious, in the section \"Is the Artificial Agent Conscious? Yes and No.\"</p>\n", "parentCommentId": "FAErrHKZ42HdmHbN9", "user": {"username": "MichaelStJules"}}, {"_id": "aY84SA5kxmxbPmhn9", "postedAt": "2023-08-12T23:07:50.692Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<p>It seems like this may be a non-standard interpretation of illusionism. Being under illusions of consciousness isn't necessary for consciousness according to Frankish, and what is necessary is that if a sufficiently sophisticated introspective/monitoring system were connected in to the system in the right way, then that would generate illusions of consciousness. See, e.g. his talks:</p>\n<ol>\n<li><a href=\"https://youtu.be/xZxcair9oNk?t=3590\">https://youtu.be/xZxcair9oNk?t=3590</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=txiYTLGtCuM\">https://www.youtube.com/watch?v=txiYTLGtCuM</a></li>\n<li><a href=\"https://youtu.be/me9WXTx6Z-Q\">https://youtu.be/me9WXTx6Z-Q</a></li>\n</ol>\n<p>I suspect now that this is also how AST is supposed to be understood, based on the artficial agents paper.</p>\n<p>I do wonder if this is setting the bar too low, though. Humphrey seems to set a higher bar, where some kind of illusion is in fact required, but also mammals and birds probably have them.</p>\n<p>I think we get into a definitional problem. What exactly do mean by \"illusion\" or \"belief\"? If an animal has a \"spooky\" attention schema, and cognitive access to it, then plausibly the animal has beliefs about it of some kind. If an animal or system believes something is good or bad or whatever, is that not an illusion, too, and is that not enough?</p>\n", "parentCommentId": "7opmAnA24uEXNJqgs", "user": {"username": "MichaelStJules"}}, {"_id": "R2EjbihosucLYbaWP", "postedAt": "2023-08-13T19:59:30.878Z", "postId": "BMkDcRrGWBj2j24NB", "htmlBody": "<p>This is very informative to me, thanks for taking the time to reply. For what it\u2019s worth, my exposure to theories of consciousness is from the neuroscience + cognitive science angle. (I very nearly started a PhD in IIT in Anil Seth\u2019s lab back in 2020.) The overview of the field I had in my head could be crudely expressed as: higher-order theories and global workspace theories are ~dead (though, on the latter, Baars and co. have yet to give up); the exciting frontier research is in IIT and predictive processing and re-entry theories.</p><p>I\u2019ve been puzzled by the mentions of GWT in EA circles\u2014the noteworthy example here is how philosopher Rob Long gave GWT a fair amount of air time in&nbsp;<a href=\"https://80000hours.org/podcast/episodes/robert-long-artificial-sentience/\"><u>his 80k episode</u></a>. But given EA\u2019s skew toward philosopher-types, this now makes a lot more sense.</p>", "parentCommentId": "HfcWrBpFWzfegzJ9X", "user": {"username": "Will Aldred"}}]