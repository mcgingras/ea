[{"_id": "oDCcL2nojme3JipEu", "postedAt": "2023-03-15T09:09:22.944Z", "postId": "D5qknQxxqprfFjXCD", "htmlBody": "<p>Thank you for sharing this, I particularly enjoyed the bee comparisons, which I hadn't seen before.</p><p>I didn't quite follow the logic behind \"working on cool AI projects now seems positive to me\".&nbsp;</p><p>It's perhaps because I don't know quite what you mean by \"working on cool AI projects\".</p><p>Are you saying that capabilities research on a \"cool AI project\" is safer than capabilities research at OpenAI or Anthropic? If so I'm not clear on why?</p><p>Or does a cool AI project mean <i>applying</i> AI rather than developing new capabilities?</p>", "parentCommentId": null, "user": {"username": "Sanjay"}}, {"_id": "s9Gmoc3242J4LDDvm", "postedAt": "2023-03-15T18:37:41.471Z", "postId": "D5qknQxxqprfFjXCD", "htmlBody": "<p>i don't think that's how <a href=\"https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy\">dignity points</a> works.</p><p>for me, p(alignment hard) is still big enough that when weighing</p><ul><li>p(alignment hard) \u00d7 value of my work if alignment hard</li><li>p(alignment easy) \u00d7 value of my work if alignment easy</li></ul><p>it's still better to keep working on hard alignment (see <a href=\"https://www.lesswrong.com/posts/88MCcHb77BvawfGZ4/question-answer-counterfactual-intervals\">my plan</a>). that's where the dignity points are.</p><p>\"shut up and multiply\", one might say.</p>", "parentCommentId": null, "user": {"username": "carado"}}, {"_id": "u3FrJC63Sj3RAzGoA", "postedAt": "2023-03-16T00:04:05.653Z", "postId": "D5qknQxxqprfFjXCD", "htmlBody": "<p>Im not trying to get dignity points. Im just trying to have a positive impact. At this point if AI is hard to align we all die (or worse!). I spent years trying to avoid contributing to the problem and helping when I could. But at this point its better to just hope alignment isn't that hard (lost cause timelines) and try to steer the trajectory positively.</p>", "parentCommentId": "s9Gmoc3242J4LDDvm", "user": {"username": "deluks917"}}, {"_id": "rJmBZqTAfPKqDYQsr", "postedAt": "2023-03-16T10:42:55.495Z", "postId": "D5qknQxxqprfFjXCD", "htmlBody": "<p>\"dignity points\" <i><strong>means</strong></i> \"having a positive impact\".</p><p>if alignment is hard we need my plan. and it's still very likely alignment is hard.</p><p>and \"alignment is hard\" is a <a href=\"https://www.lesswrong.com/posts/SFLCB5BgjzruJv9sp/logical-and-indexical-uncertainty\">logical fact not indexical location</a>, we don't get to save \"those timelines\".</p>", "parentCommentId": "u3FrJC63Sj3RAzGoA", "user": {"username": "carado"}}]