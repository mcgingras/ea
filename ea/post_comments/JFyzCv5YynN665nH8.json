[{"_id": "c88fMwPyLAHBXrGao", "postedAt": "2022-12-08T09:40:03.666Z", "postId": "JFyzCv5YynN665nH8", "htmlBody": "<p><i>[weirdness-filter: ur weird if you read m commnt n agree w me lol]</i></p><p>Doing private capabilities research seems not obviously net-bad, for some subcategories of capabilities research. It constrains your expectations about how AGI will unfold, meaning you have a narrower target for your alignment ideas (incl. strategies, politics, etc.) to hit. The basic case: If an alignment researcher doesn't understand how gradient descent works, I think they're going to be less effective at alignment. I expect this to generalise for most advances they could make in their theoretical understanding of how to build intelligences. And there's no fundamental difference between learning the basics and doing novel research, as it all amounts to increased understanding in the end.</p><p>That said, it would in most cases be very silly to publish about that increased understanding, and people should be disincentivised from doing so.&nbsp;</p><p>(I'll delete this comment if you've read it and you want it gone. I think the above can be very bad advice to give some poorly aligned selfish researchers, but I want reasonable people to hear it.)</p>", "parentCommentId": null, "user": {"username": "Emrik"}}, {"_id": "vRt3M9rmnCtyWF5Pr", "postedAt": "2022-12-08T13:57:00.528Z", "postId": "JFyzCv5YynN665nH8", "htmlBody": "<p>I agree that publishing results of the form \"it turns out that X can be done, though we won't say how we did it\" is clearly better than publishing your full results, but I think it's much more harmful than publishing nothing in a world where other people are still doing capabilities research.&nbsp;</p><p>This is because it seems to me that knowing something is possible is often a first step to understanding how. This is especially true if you have any understanding of where this researcher or organisation were looking before publishing this result.&nbsp;</p><p>&nbsp;</p><p>I also think there are worlds where it's importantly harmful to too openly critique capabilities research, but I lean towards not thinking we are in this world, and think the tone of this post is a pretty good model for how this should look going forwards. +1!</p>", "parentCommentId": null, "user": {"username": "brook"}}, {"_id": "HyCfGCZz7noJaMtfi", "postedAt": "2022-12-08T16:29:32.460Z", "postId": "JFyzCv5YynN665nH8", "htmlBody": "<p>Thanks, Brook! I agree that \"it turns out that X can be done\" can accelerate others a lot.</p>", "parentCommentId": "vRt3M9rmnCtyWF5Pr", "user": {"username": "RobBensinger"}}, {"_id": "2krdM7ZyeBkCLv4kQ", "postedAt": "2022-12-09T06:14:01.646Z", "postId": "JFyzCv5YynN665nH8", "htmlBody": "<p><strong>Post summary</strong> (feel free to suggest edits!):<br>Rob paraphrases Nate\u2019s thoughts on capabilities work and the landscape of AGI organisations. Nate thinks:&nbsp;</p><ol><li>Capabilities work is a bad idea, because it isn\u2019t needed for alignment to progress and it could speed up timelines. We already have many ML systems to study, which our understanding lags behind. Publishing that work is even worse.</li><li>He appreciates OpenAI\u2019s&nbsp;<a href=\"https://openai.com/charter/\"><u>charter</u></a>, openness to talk to EAs / rationalists, clearer alignment effort than FAIR or Google Brain, and transparency about their plans. He considers DeepMind and Anthropic on par and slightly ahead respectively on taking alignment seriously.</li><li>OpenAI, Anthropic, and DeepMind are unusually safety-conscious AI capabilities orgs (e.g., much better than FAIR or Google Brain). But reality doesn't grade on a curve, there's still a lot to improve, and they should still call a halt to mainstream SotA-advancing potentially-AGI-relevant ML work, since the timeline-shortening harms currently outweigh the benefits.</li></ol><p>(If you'd like to see more summaries of top EA and LW forum posts, check out the <a href=\"https://forum.effectivealtruism.org/s/W4fhpuN26naxGCBbN\">Weekly Summaries</a> series.)</p>", "parentCommentId": null, "user": {"username": "GreyArea"}}, {"_id": "i5hshLBiBpmBKvMp4", "postedAt": "2022-12-09T06:59:39.534Z", "postId": "JFyzCv5YynN665nH8", "htmlBody": "<p>Thanks, Zoe! This is great. :) Points 1 and 2 of your summary are spot-on.</p><p>Point 3 is a bit too compressed:&nbsp;</p><blockquote><p>Even if an organisation does well for the reference class \u201cAI capabilities org\u201d, it\u2019s better for it to stop, and others not to join that class, because positive effects are outweighed by any shortening of AGI timelines. This applies to all of OpenAI, DeepMind, FAIR, Google Brain etc.</p></blockquote><p>\"This applies to all of OpenAI, DeepMind, FAIR, Google Brain\" makes it sound like \"this organization does well for the reference class 'AI capabilities org'\" applies to all four orgs; whereas actually we think OpenAI, DeepMind, and Anthropic are doing well for that class, and FAIR and Google Brain are not.</p><p>\"Even if an organisation does well for the reference class 'AI capabilities org', it\u2019s better for it to stop\" also makes it sound like Nate endorses this as true for all possible capabilities orgs in all contexts. Rather, Nate thinks it could be good to do capabilities work in some contexts; it just isn't good right now. The intended point is more like:</p><p>OpenAI, Anthropic, and DeepMind are unusually safety-conscious AI capabilities orgs (e.g., much better than FAIR or Google Brain). But reality doesn't grade on a curve, there's still a lot to improve, and they should still call a halt to mainstream SotA-advancing potentially-AGI-relevant ML work, since the timeline-shortening harms currently outweigh the benefits.</p>", "parentCommentId": "2krdM7ZyeBkCLv4kQ", "user": {"username": "RobBensinger"}}, {"_id": "t5rqx36nns8FW43k9", "postedAt": "2022-12-09T08:50:08.868Z", "postId": "JFyzCv5YynN665nH8", "htmlBody": "<p>Thanks, and that makes sense, edited to reflect your suggestion</p>", "parentCommentId": "i5hshLBiBpmBKvMp4", "user": {"username": "GreyArea"}}, {"_id": "YNiY4LxHCxdqZXQgw", "postedAt": "2022-12-11T08:09:34.900Z", "postId": "JFyzCv5YynN665nH8", "htmlBody": "<blockquote><p>It strikes him as helping with parallelizable research goals, whereas our bottleneck is serial research goals.</p></blockquote><p>Can someone please give &nbsp;a precise definition of \"serial research goals\"? See also my <a href=\"https://www.lesswrong.com/posts/vQNJrJqebXEWjJfnz/a-note-about-differential-technological-development?commentId=BiKAjhriAayqfbQLY\">comment on the differential development post</a> for more ways the serial research framing confuses me.</p>", "parentCommentId": null, "user": {"username": "jakubkraus07@gmail.com"}}]