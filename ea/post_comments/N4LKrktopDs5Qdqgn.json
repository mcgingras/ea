[{"_id": "67LobnmsR5quMiZ8c", "postedAt": "2023-07-20T10:20:55.352Z", "postId": "N4LKrktopDs5Qdqgn", "htmlBody": "<blockquote><p>[...] we are impressed by [...] \u2018<a href=\"https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge\"><u>Eliciting Latent Knowledge</u></a>' [that] provided conceptual clarity to a previously confused concept</p></blockquote><p>To me, it seems that ELK is (was) attention-captivating (among the AI safety community) but doesn't assume a solid basis: logic and theories of cognition and language, and therefore is actually <i>confusing</i>, which prompted at least several clarification and interpretation atttempts (<a href=\"https://www.lesswrong.com/posts/rxoBY9CMkqDsHt25t/eliciting-latent-knowledge-elk-distillation-summary\">1</a>, <a href=\"https://www.lesswrong.com/posts/NxApPkbjt9hXraSts/for-elk-truth-is-mostly-a-distraction\">2</a>, <a href=\"https://www.lesswrong.com/posts/8xCtJHAbzyA2oA6J4/clarifying-what-elk-is-trying-to-achieve\">3</a>). I'd argue that most people leave original ELK writings more confused than they were before. So, I'd classify ELK as a mind-teaser and maybe problem-statement (maybe useful than distracting, or maybe more distracting than useful; it's hard to judge as of now), but definitely not as great \"conceptual clarification\" work.</p>", "parentCommentId": null, "user": {"username": "Roman Leventov"}}, {"_id": "PwAnKGAy8y9TA36Gm", "postedAt": "2023-07-20T14:02:34.165Z", "postId": "N4LKrktopDs5Qdqgn", "htmlBody": "<p>I agree with your conclusion but disagree about your reasoning. I think its perfectly fine and should be encouraged to make advances in conceptual clarification which confuse people. Clarifying concepts can often result in people being confused about stuff they weren\u2019t previously, and this often indicates progress.</p>\n", "parentCommentId": "67LobnmsR5quMiZ8c", "user": {"username": "D0TheMath"}}]