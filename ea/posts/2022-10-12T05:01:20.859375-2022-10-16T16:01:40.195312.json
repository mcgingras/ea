[{"_id": "xRZ4P7QC7mnXktkqL", "title": "Halifax, NS \u2013 Monthly Rationalist, EA, and ACX Meetup Kick-Off", "postedAt": "2022-10-16T13:19:09.225Z", "htmlBody": "<p>In the interest of growing the LW/EA/ACX community in Halifax, we will be hosting meetups on the last Saturday of each month. The format is friendly, unstructured discussion (future iterations could emphasise particular topics). Old hands and newcomers are welcome!</p><p>Location: Seven Bays Cafe (2017 Gottingen Street) \u2013 <a href=\"https://plus.codes/87PRMC29+9C\">87PRMC29+9C</a></p><p>Join us at Seven Bayes</p><p>Contact: <a href=\"mailto:conorbarnes93@gmail.com\">conorbarnes93@gmail.com</a></p>", "user": {"username": "Ideopunk"}}, {"_id": "waSHb3QLyjGvoJN7M", "title": "GWWC Pledge Celebration (Europe/Asia)", "postedAt": "2022-10-16T11:54:32.145Z", "htmlBody": "<p>This month we are celebrating the GWWC Pledge, and have invited some members who have been pledged for 10years or more to join us! Please some along to share your pledge stories, learn more from others, ask any questions you might have or just get to know some of your fellow members a bit better :)</p><p>See you there!</p><p>&nbsp;</p><p>This is your chance to meet people in the Giving What We Can community from around the world, hear what's going on, discuss effective giving, share ideas &amp; tips, and more!<br><br>This event is held simultaneously online at the following times:<br>- London: Sunday 9:30 am<br>- Berlin: Sunday 10:30 am<br>- Mumbai: Sunday 3:00 pm<br>- Singapore: Sunday 5:30 pm<br>- Sydney: Sunday 8:30 pm</p>", "user": {"username": "Jmd"}}, {"_id": "zfPYbswjFgdEYpFwo", "title": "GWWC Pledge Celebration (Americas/Oceania)", "postedAt": "2022-10-16T11:50:45.979Z", "htmlBody": "<p>This month we are celebrating the GWWC Pledge, and have invited some members who have been pledged for 10years or more to join us! Please some along to share your pledge stories, learn more from others, ask any questions you might have or just get to know some of your fellow members a bit better :)</p><p>See you there!</p><p>&nbsp;</p><p>This is your chance to meet people in the Giving What We Can community from around the world, hear what's going on, discuss effective giving, share ideas &amp; tips, and more!<br><br>This event is held simultaneously online at the following times:<br>- New York: Saturday 5:00 pm<br>- Los Angeles: Saturday 2:00 pm<br>- Sydney: Sunday 9:00 am<br>- Auckland: Sunday 11:00 am</p>", "user": {"username": "Jmd"}}, {"_id": "dQLKkApAtkrxXj5Zk", "title": "GWWC End of Year Celebration (Europe/Asia)", "postedAt": "2022-10-16T11:48:05.355Z", "htmlBody": "<p>Come join us for a social end of year event! Get updates on GWWC, Givewell, EA in general, learn about the things we are celebrating from this year, and share with other community members your own celebrations, whether its something EA related or not :)<br>Its also a great time of the year to think about how to share and talk about Effective Giving so feel free to invite any friends, or to come along and share experiences on talking about GWWC or EA or donating!</p><p>Look forward to seeing you there!<br><br>This event is held simultaneously online at the following times:<br>- London: Sunday 9:30 am<br>- Berlin: Sunday 10:30 am<br>- Mumbai: Sunday 3:00 pm<br>- Singapore: Sunday 5:30 pm<br>- Sydney: Sunday 8:30 pm</p>", "user": {"username": "Jmd"}}, {"_id": "nMefNMdZSjP4JSRKq", "title": "GWWC End of Year Celebration (Americas/Oceania)", "postedAt": "2022-10-16T11:46:02.908Z", "htmlBody": "<p>Come join us for a social end of year event! Get updates on GWWC, Givewell, EA in general, learn about the things we are celebrating from this year, and share with other community members your own celebrations, whether its something EA related or not :)<br>Its also a great time of the year to think about how to share and talk about Effective Giving so feel free to invite any friends, or to come along and share experiences on talking about GWWC or EA or donating!</p><p>Look forward to seeing you there!<br><br>This event is held simultaneously online at the following times:<br>- New York: Saturday 5:00 pm<br>- Los Angeles: Saturday 2:00 pm<br>- Sydney: Sunday 9:00 am<br>- Auckland: Sunday 11:00 am</p>", "user": {"username": "Jmd"}}, {"_id": "rMpeFK8yiFL7bqDpH", "title": "GWWC Meetup (Europe/Asia)", "postedAt": "2022-10-16T11:41:33.718Z", "htmlBody": "<p>This is your chance to meet people in the Giving What We Can community from around the world, hear what's going on, discuss effective giving, share ideas &amp; tips, and more!<br><br>This event is held simultaneously online at the following times:<br>- London: Sunday 9:30 am<br>- Berlin: Sunday 10:30 am<br>- Mumbai: Sunday 3:00 pm<br>- Singapore: Sunday 5:30 pm<br>- Sydney: Sunday 8:30 pm</p>", "user": {"username": "Jmd"}}, {"_id": "AJCDEv8xv937Hg2mX", "title": "GWWC Meetup (Americas/Oceania)", "postedAt": "2022-10-16T11:37:56.550Z", "htmlBody": "<p>This is your chance to meet people in the Giving What We Can community from around the world, hear what's going on, discuss effective giving, share ideas &amp; tips, and more!<br><br>This event is held simultaneously online at the following times:<br>- New York: Saturday 5:00 pm<br>- Los Angeles: Saturday 2:00 pm<br>- Sydney: Sunday 9:00 am<br>- Auckland: Sunday 11:00 am</p>", "user": {"username": "Jmd"}}, {"_id": "R4nbXRipSzFECwkaE", "title": "Is interest in alignment worth mentioning for grad school applications?", "postedAt": "2022-10-16T04:50:46.547Z", "htmlBody": "<p>I am right now in the process of applying for graduate degrees at some US unis e.g. at Stanford. The major motivation to do this is to set up for technical alignment work and have closer access to the EA bay area community (than I'd have here in Europe). Especially in the light of EA quite rapidly expanding in many countries this year this seems however like a pretty basic motivation: After extensive outreach of EA in general/alignment in particular it seems like a reasonable lower bound that there are at least dozens if not potentially hundreds of students applying with precisely this motivation to stanford grad programmes. Obviously I am writing this in a more refined, specific way but ultimately its not like I'd have an extensive track record in breakthroughs of technical alignment research yet.</p><p>Now I'm wondering is interest in alignment/from an EA angle something even worth mentioning in the motivation letter, or is it to be expected that it has become such a basic thing, that it is just useless (or even negatively) impacting my application?</p>", "user": {"username": "Franziska Fischer"}}, {"_id": "oYBooqQErjgiEGLKk", "title": "Effective Refugee Support + Response?", "postedAt": "2022-10-16T05:49:22.051Z", "htmlBody": "<p>The question is whether there are effective means to support refugees, as well as people in those countries either situationally or systemically prone to exodus/diaspora (but lacking means). I've seen similar questions on this forum -- (e.g., around Ukraine, most recently), and a degree of dismissal for the question itself. I appreciate why that is, but want to stress: there are potential donors who would like to do the most with their means within a specific context (e.g., many of my family and friends, and I doubt that's just me). I get why that is counter to EA principles in the broad sense, but still appreciate whatever guidance/suggestions this forum can offer so we can leverage the best of what EA is without turning-away potential donors in failed attempts to fully convert them.&nbsp;</p><p>Related -- I'm looking for anyone in the Venezuelan chapter, if one exists. Alternatively (and more broadly), looking for EA causes operating in LATAM/Venezuela with emphasis on effective refugee + in-country support.&nbsp;<br>&nbsp;</p>", "user": {"username": "Nick G"}}, {"_id": "rY53wpJ8fMSSg2RNT", "title": "A vision of the future (fictional short-story) ", "postedAt": "2022-10-15T12:38:14.865Z", "htmlBody": "<p>Robert is 800 years old, he came from the first generation of humans who figured out how to extend the human lifespan, he is in the top 5% of oldest humans alive. Robert is about to have his 3rd kid. He was never a huge kid guy, all the way up to his 600\u2019s he never had a single child. He explored the entire earth, traveled to every hotspot there was to see, and eventually had his first child. This changed Roberts's viewpoint on the world entirely. While Robert knew there were so many more things to do, once he finished traveling the world he thought nothing would be the same as coming across as a new place. But Roberts's first son blew open his perspective. He never knew he could feel such strong emotions or love towards something, or someone as his kid, Richie showed him. And so for the first 40 years of his son's life he watched and waited, oftentimes pissed off that he was not able to interact directly with his son yet. There were of course the few times, Robert was able to make an impact on Riche\u2019s life.</p>\n<p>The creators of the upbringing process knew even their most advanced AI counterparts could still not fully grasp the true experience of being a conscious human being and so in in his son\u2019s deepest times of need. Robert was able to show support, in the smallest of forms. Whether it was when Richie's biological mother passed away, and as Richie wept, a faint blue butterfly, his moms favorite color, came and landed on his finger, waiting, watching, as if to show everything was going to be ok. You see, Richard is not supposed to be able to interact with the upbringing process of his Child\u2019s life at all, but to garner powerful connections, and feelings of hope. Small acts at just the right time are able to be chosen by the true genetic parent. So when the curtain is drawn, they'll know and understand that they were watching, they wanted to help, their love is there, and the reason for the upbringing process will be revealed to the new conscious beings so they understand why they had to go through it.</p>\n<p>Now what is this process, and why did humanity need to take such measures to bring their new borns through it in the first place? Lets go back to when Robert he had just turned 150.</p>\n<p>At this time, the world was looking up, artificial intelligence had made most human jobs entirely pointless. And with so much extra value being produced and created, all humans were given a chance at living with real freedom, no one had financial stress anymore. Anything an average person wanted to do, they were able to with their daily stimulus check. The extra funds some people acquired became for show, bonus points if you will. Status points you could choose to care about or not like the show \u201cWhose line is it anyway?\u201d</p>\n<p>There just wasn't that much extra you could do, everyone's free daily limit gave them access to more food they they could possibly eat, travel capabilities to anywhere in the world. And when it came to things, people didnt really own all that much anymore not.  The freedom of knowing that you can buy an indispensable amount of goods daily took away peoples need to hoard. Now, there were people that tried, but there was a department that handled those cases effectively.&nbsp; People were really grateful for the way things turned out.</p>\n<p>(If I get positive reinforcement, I\u2019ll continue this piece. It\u2019s going to be a narrative representation of the Galactic Preschool Theory, like the plot of a White Mirror episode.)</p>\n", "user": {"username": "EffAlt"}}, {"_id": "qxG9CyzQnognatQPo", "title": "The most effective question to ask yourself. ", "postedAt": "2022-10-15T12:28:42.392Z", "htmlBody": "<p>From this present moment, a person without any extraordinary superpowers sets off to change the world positively as much as they can. This person changes the world more than anyone else.</p>\n<p>What do they do?</p>\n<p>Your writing should talk about a person who changes the world more than any other person who is also writing about this prompt. (It\u2019s a competition of sorts)</p>\n<p>Challenge yourself to imagine what actions that person takes and to give a realistic but idealistic narrative of the roadmap they take)</p>\n<p>How much impact could they have?</p>\n<p>How many lives could they affect?</p>\n<p>How many lives could they save?</p>\n<p>I\u2019m super excited to read the responses.</p>\n<h2>STOP READING THIS POST HERE IF YOU PLAN TO DO THE PROMPT (spoiler below)</h2>\n<ul>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n<li></li>\n</ul>\n<p>The reason I did not want you to see this question before you wrote the writing is in the hope to remove the bias from the writing.</p>\n<p>The question is: What is stopping you from living that path that you see as most optimal?</p>\n<p>(Be honest and serious, I\u2019m looking to help remove the blockage for the stories I resonate with and believe in)</p>\n", "user": {"username": "EffAlt"}}, {"_id": "stTTDj6cGDNTgbreC", "title": "Berlin EA Shenanigans (unaffiliated) - please RSVP", "postedAt": "2022-10-15T11:14:14.466Z", "htmlBody": "<p>The first meetup was a lot of fun, so here is round #2.&nbsp;</p><p>I expect a mix of newbies and (more) experienced members of the Berlin EA community and look forward to meet you - whether you've been there the last time or not.</p><p>You're very welcome even if you\u2019ve never been to a meetup or you feel like you don't fit.</p><p>We don't have a fixed agenda. Since last time was our first get together we did a lot of getting to know each other - free conversation and some games.</p><p>The planned location is the <a href=\"https://berlin.ccc.de/\"><u>Chaos Computer Club Berlin</u></a> which can comfortably fit about 20 people. Please RSVP so we know how many to expect.</p><p>Please contact <a href=\"https://forum.effectivealtruism.org/users/new_user_928725294\">__nobody</a> if you have questions about the location.</p><p>We'd like everyone to be there at 6:30 PM CEST but feel free to arrive by 6 PM and stick around as long as you want<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefru83op3xo1\"><sup><a href=\"#fnru83op3xo1\">[1]</a></sup></span>.</p><p>Route (German): <a href=\"https://berlin.ccc.de/page/anfahrt\">https://berlin.ccc.de/page/anfahrt</a></p><p>PS: The \"Berlin EA shenanigans\" Signal group has grown to 90 members, comment here or PM me if you want to be invited.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnru83op3xo1\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefru83op3xo1\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Rooms will close 1 AM.</p></div></li></ol>", "user": {"username": "Milli"}}, {"_id": "qkwghvRD88oGC4zmp", "title": "James Norris from Upgradable on \u201cWhat is Beyond Living a Principled Life\u201d - OpenPrinciples Speaker Session", "postedAt": "2022-10-15T03:22:06.053Z", "htmlBody": "<p>Hi Everyone,&nbsp;<br>&nbsp;</p><p>OpenPrinciples would like to invite you to join our 1 hour Speaker &amp; Q&amp;A Session with James Norris, the CEO of Upgradable, to hear him talk about <strong>how Principled Living fits into a broader definition of the self-growth process</strong>. The first 30 min will be his talk, and the second 30 min will be Q&amp;A.&nbsp;&nbsp;</p><p>James Norris is the Founder and CEO of&nbsp;<a href=\"http://upgradable.org/\"><u>Upgradadable</u></a>, he and his team developed a program&nbsp;that uses emerging technology, behavioral science, and applied rationality&nbsp;to help people plausibly achieve their life goals.</p><p>&nbsp;</p><p>OpenPrinciples is an EA Aligned organization led by current and previous organizers of EA IBM, EA UWaterloo, EA Taft, and EA Kathmandu University, and is a community of like-minded people who like to open-source our own principles, build on each others' principles, and use those principles to make great decisions and make fewer repeated mistakes so we can help the world more effectively.&nbsp;<br>&nbsp;</p><p>Examples of OpenPrinciples' projects include&nbsp;<a href=\"http://openprinciples.org\"><u>OpenPrinciples.org</u></a>, a crowd-sourced database of open-sourced life principles of our community members,<a href=\"https://openprinciples.notion.site/V1-4-OpenPrinciples-Principled-Individual-Template-28175992d4c841c291009f5f6a2c984f\">&nbsp;<u>Principled Individual Notion Template</u></a>, and&nbsp;<a href=\"http://ultrabrain.openprinciples.org/\"><u>Ultrabrain</u></a> Principles AI Assistant that helps people reflect, remind, and act on their life principles.</p><p><br>&nbsp;</p>", "user": {"username": "gty3310"}}, {"_id": "r6hqNPFGg4C6e8AJX", "title": "Testing Impact: Longtermist TV Show", "postedAt": "2022-10-14T23:30:19.620Z", "htmlBody": "<p>I think I may have a unique opportunity for a high-impact intervention.</p><p>My brother is a graduate from UNC School of the Arts, with experience in television. He has even worked on getting a show pitched to networks (\"Systems,\" a medical drama about systemic racism in medicine). If we could get some funding from Open Philanthropy, we could pitch a show about humanity's long-term future (check out <a href=\"https://forum.effectivealtruism.org/posts/jsqTCrixFHEQsSTTj/13-ideas-for-new-existential-risk-movies-and-tv-shows-what\">HaydnBelfield's post</a> on ideas for such a project). The basic theory of change is that if such a show becomes sufficiently popular, it could bring about a culture shift where people place more emphasis on the long term, which would then influence politicians to address things like x-risks.</p><p>My question: how can I test this idea for impact? Are there some low-cost ways for me to test out my theory of change and decide if this idea is worth pursuing?</p><p>(Also, feel free to throw out any critiques you might have of my idea. All feedback is welcome!)</p>", "user": {"username": "Anthony Fleming"}}, {"_id": "7hCCdw8LhWFiMjCff", "title": "A common failure for foxes", "postedAt": "2022-10-14T22:51:00.062Z", "htmlBody": "", "user": {"username": "RobBensinger"}}, {"_id": "LojHRcgnzKX7GTg3J", "title": "[Job]: AI Standards Development Research Assistant", "postedAt": "2022-10-14T20:18:16.746Z", "htmlBody": "<p><a href=\"https://existence.org/apply\">Apply here.</a></p><p><strong>Title:</strong> Research Assistant for AI Standards Development<br><strong>Ideal start date:</strong> December 2022<br><strong>Hours:</strong> 20-40 hours/week<br><strong>Compensation:</strong> $30/hour to $50/hour, depending on experience and qualifications<br><strong>Work location:</strong> Remote<br><strong>Reports to:</strong> Tony Barrett, BERI Senior Policy Analyst</p><p><strong>For best consideration, please apply by Monday November 7th, 2022, 5pm Eastern Time.</strong> Applications received after that date may also be considered, but only after applications that met the deadline.</p><h3><strong>Responsibilities</strong></h3><p>Supporting work planned by <a href=\"https://www.linkedin.com/in/anthony-m-barrett-607891169/\">Tony Barrett</a> and UC Berkeley colleagues to develop an AI-standards \u201cprofile\u201d with best practices for developers of cutting-edge, increasingly general purpose AI, building on the ideas in Section 4 of the paper by Barrett and colleagues, <a href=\"https://arxiv.org/abs/2206.08966\">\u201cActionable Guidance for High-Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks\u201d</a>. The profile guidance will be primarily for use by developers of such AI systems, in conjunction with the NIST AI Risk Management Framework (AI RMF) and/or the AI risk management standard ISO/IEC 23894. Our goal is to help set norms for safety-related practices across regulatory regimes, reducing chances that developers of highly advanced AI systems (including proto-AGI) would have to compromise on safety, security, ethics or related qualities of AI systems in order to be competitive.</p><p>Tasks will include research and analysis of technical or policy issues in AI safety standards or related topics. The goal is to help our team to address key AI technical issues with actionable guidance for AI developers, in ways that improve the overall quality of our profile guidance documents.</p><p>Technical research assistance tasks may include:</p><ul><li>Literature searches on technical methods for safety or security of machine learning models</li><li>Gap analysis to check that our draft guidance would address key technical issues in AI safety, security or other areas</li></ul><p>Policy research assistance tasks may include:</p><ul><li>Identifying and analyzing related standards or regulations</li><li>Mapping specific sections of our draft guidance to specific parts of related standards or regulations</li><li>Checking that our draft guidance would meet the intent and requirements of related standards or regulations</li></ul><p>We currently have funding for approximately one year of work, but we have potential to obtain additional funding to renew or expand this work.</p><h3><strong>Qualification Criteria</strong></h3><p>The most competitive candidates will meet the below criteria.</p><ul><li>Education or experience in one or more of the following:<ul><li>AI development techniques and procedures used at leading AI labs developing increasingly general-purpose AI;</li><li>Technical AI safety concepts, techniques and literature;</li><li>Industry standards and best practices for AI or other software, and compliance with standards language;</li><li>Public policy or regulations (especially in the United States) for AI or other software</li></ul></li><li>Ability to research and analyze technical or policy issues in AI safety standards or related topics</li><li>Ability to track and complete multiple tasks to meet deadlines with little or no supervision</li><li>Good English communication skills, both written and verbal, including editing text to improve understandability</li><li>Availability for video calls (e.g. via Zoom) for 30 minutes three times a week, at some point between 9am and 5pm Eastern Time (it\u2019s not necessary to be available that whole time, and otherwise you can choose your own working hours)</li></ul><p>We will likely hire two people, each on a part-time basis, one with a technical background and one with a policy background. However, we are open to having one person fill both of those roles.</p><h3><strong>Application Process</strong></h3><p><a href=\"https://existence.org/apply\">Apply here.</a></p><p><strong>For best consideration, please apply by Monday November 7th, 2022, 5pm Eastern Time.</strong></p><p>Candidates invited to interview will also be asked to perform a written work test, which we expect to take one to two hours.</p><p><a href=\"https://existence.org/jobs/ai-standards-ra\">More information on BERI's website.</a></p>", "user": {"username": "Tony Barrett"}}, {"_id": "c6RnqjBd3BAkqsknB", "title": "The US expands restrictions on AI exports to China. What are the x-risk effects?", "postedAt": "2022-10-14T18:17:53.583Z", "htmlBody": "<p>Late last week, the Biden administration announced a new set of regulations that make it illegal for US companies to export a range of AI-related products and services to China (Financial Times coverage&nbsp;<a href=\"https://www.ft.com/content/6825bee4-52a7-4c86-b1aa-31c100708c3e\"><u>here</u></a> (<u>paywalled</u>); Reuters&nbsp;<a href=\"https://www.reuters.com/technology/us-aims-hobble-chinas-chip-industry-with-sweeping-new-export-rules-2022-10-07/\"><u>here</u></a>). By all accounts this new export controls policy is a big deal for US-China competition and Chinese AI progress. Its wide-reaching effects touch on a number of issues important to effective altruists, AI safety strategists, and folks interested in reducing global catastrophic risks. This is a quickly-written post in which I summarize my understanding of the new regulations and their probable effects, then list some questions the announcement prompted regarding the effect on various global catastrophic risks.</p><h1>The new export controls policy</h1><p>If you pick a random article commenting on the new export controls policy, you\u2019ll probably see some dramatic language.&nbsp;<a href=\"https://www.reuters.com/technology/us-aims-hobble-chinas-chip-industry-with-sweeping-new-export-rules-2022-10-07/\"><u>Reuters</u></a> describes the new rules as an attempt to \u201chobble\u201d China\u2019s chip industry. CSIS\u2019s Greg Allen&nbsp;<a href=\"https://www.csis.org/analysis/choking-chinas-access-future-ai\"><u>writes</u></a> that they seek to \u201c[choke] off China\u2019s access to the future of AI\u201d. ChinaTalk\u2019s Jordan Schneider&nbsp;<a href=\"https://threadreaderapp.com/thread/1580683596301766656.html\"><u>thinks</u></a> they \u201cwill reshape the global semiconductor industry and the future of the US-China relationship\u201d.</p><p>The export controls seek to slow or stop Chinese companies from developing cutting-edge AI capabilities. They do this by making it illegal for US companies, US citizens, and foreign companies that use US products to sell AI-related hardware, software, and services to Chinese entities. This kind of policy is not new in itself. For years, the US has prohibited companies from selling AI tech to China for military use. However, the new regulations are notable for being much broader than previous restrictions and broad-based, no longer restricted to&nbsp;<i>military&nbsp;</i>uses in particular.</p><p>Allen writes that the regulations target four \u201cchokepoints\u201d in the AI development supply chain. Restrictions have been placed on the export of high-end computer chips, chip design software, chip manufacturing equipment, and manufacturing equipment components. I am going to summarize Allen\u2019s explanations very briefly. If this is relevant to your work I strongly recommend you go read his&nbsp;<a href=\"https://www.csis.org/analysis/choking-chinas-access-future-ai\"><u>article</u></a>, which is packed with interesting and important details. Schneider also has a digestible tweet thread explanation you can read <a href=\"https://threadreaderapp.com/thread/1580683596301766656.html\">here</a>.</p><h2><strong>1. The best computer chips</strong></h2><p>The new policies will functionally end the sale of the \u201cbest\u201d computer chips, i.e. any chip above a certain performance threshold, to China. The US had already blocked the sale of such chips to China\u2019s military. However, Allen writes that this policy was ineffective due to military-civil fusion in China; the boundary between military and non-military organizations in China is purposefully and officially&nbsp;<a href=\"https://www.cnas.org/publications/reports/myths-and-realities-of-chinas-military-civil-fusion-strategy\"><u>blurry</u></a>. Unable to distinguish military from non-military uses, the US government has decided to just block the sale of high-end chips to China entirely.</p><h2><strong>2. The software used to design chips</strong></h2><p>Advanced chips are so complex that specialized software is required to design them. The new regulations also ban companies from providing this software to Chinese companies. This will make it harder for Chinese companies to design new chips that can compete with the high-end chips produced by, e.g., Nvidia and AMC, to which they have just lost access.</p><h2><strong>3. The equipment used to make semiconductors</strong></h2><p>Allen writes that Chinese companies could yet circumvent chokepoints (1) and (2) by designing and manufacturing chips using older programs and equipment. But chokepoint (3) seeks to stop this by banning the sale of chip manufacturing equipment that exceeds a certain performance threshold. US citizens are also no longer allowed to help repair or maintain such equipment in China. Allen writes that this is a \u201cdevastating blow\u201d for Chinese chip manufacturers. Schneider&nbsp;<a href=\"https://threadreaderapp.com/thread/1580889341265469440.html\"><u>says</u></a> these restrictions are already \u201cwreaking havoc\u201d as US citizens in the semiconductor industry in China have stopped working for fear of violating them.&nbsp;</p><h2><strong>4. The components used to make new semiconductor manufacturing equipment</strong></h2><p>Finally, to slow potential efforts by China to nurture domestic production of semiconductor manufacturing equipment, a range of components critical for building these machines are now under export controls. Without them, Allen writes, China will be \u201cstarting from scratch\u201d in building up this industry, with \u201cseven decades\u201d of achievements and experience to replicate; \u201can extremely tall mountain to climb.\u201d</p><h1>Questions I\u2019m left with</h1><p>One of the takeaways that Allen leaves his readers with is that \u201cthis policy signals that the Biden administration believes the hype about the transformative potential of AI and its national security implications is real.\u201d That sentiment probably feels familiar to many readers of this forum. Given the obvious links to EA, I\u2019m left with three kinds of questions. I think more work in this vein, drawing out the implications for both policymakers and EA researchers, could be valuable.</p><p>First, I have questions about <strong>AI capabilities, progress, and timelines</strong>. When, if ever, will China\u2019s AI capacity catch up with the US\u2019s? How does this line up with AI timelines? My impression is that the future trajectory of the AI capabilities of the leading organizations in China remains uncertain, despite the reach of the export controls. Key uncertainties here are the extent to which the US is able to bring allies on board to make this a truly global blockade, and how effective a response the Chinese government and companies can muster. Perhaps in the long-term, the effect of these controls will be to spur the development of a powerful and entirely decoupled AI industry within China. While the technical challenges are considerable, it\u2019s unclear to me whether the drag on Chinese AI progress should be measured in years or decades.</p><p>Second, I have questions about the <strong>likelihood of conflict</strong>. The evidence on how strongly economic interdependence promotes peace is surprisingly mixed. But it\u2019s certainly plausible to me that decoupling in critical sectors makes conflict more likely (at least at the margin) by lowering the costs of supply chain disruption. On the other hand, there are also conditions under which this policy would lower the chance of conflict. This may be the case if (1) transition points when one country is overtaken by another are particularly dangerous, and (2) this policy delays that transition point for the US and China. It\u2019s unclear to me how these drivers net out. And if export controls do increase the probability of conflict, the policy\u2019s net effect on total existential risk is also unclear.</p><p>Finally, I have questions about <strong>cooperation on other existential risks</strong>. To what extent do tension-raising actions in national security hamper efforts to cooperate in other important domains, such as on&nbsp;<a href=\"https://www.reuters.com/business/environment/un-warns-no-way-tackle-climate-change-without-us-china-cooperation-2022-08-05/\"><u>climate change</u></a>, pandemic prevention, or&nbsp;<a href=\"https://en.wikipedia.org/wiki/Wolf_Amendment\"><u>space governance</u></a>? I am not one to naively suggest \u201ccooperation\u201d as a panacea for risk reduction. In many cases, competition and policy diversification can make us more&nbsp;<i>robust&nbsp;</i>to risks at a civilizational scale. But there are other domains where cooperation will clearly be needed to protect global commons and solve coordination problems at the planetary level. Who is thinking about the cross-domain linkages here? What is the long-term outlook?</p><p>I'm sure the EAs whose work is affected by these new policies are already aware of them. But I was surprised by the strength of the new policies, their direct relevance to AI progress, and the lack of discussion of linkages to other domains.</p>", "user": {"username": "Stephen Clare"}}, {"_id": "5xqCrqD3zkNi5ctEx", "title": "Metaculus Launches the 'Forecasting Our World In Data' Project to Probe the Long-Term Future", "postedAt": "2022-10-14T17:00:31.130Z", "htmlBody": "<h3><i>With support from the FTX Future Fund, the project features a public forecasting tournament and a Pro Forecaster initiative delivering 1, 3, 10, 30, and 100-year predictions</i></h3><p><img src=\"https://metaculus-media.s3.amazonaws.com/user_uploaded/owid-final-poster.jpeg\"></p><p><a href=\"https://www.metaculus.com/tournament/forecasting-Our-World-in-Data/\">Metaculus</a> has launched <a href=\"https://www.metaculus.com/tournament/forecasting-Our-World-in-Data/\"><i>Forecasting Our World In Data</i></a>, a tournament that will deliver predictions on technological advancement, global development, and social progress on time horizons ranging from one to 100 years, with close collaboration from the <a href=\"https://ourworldindata.org/\">Our World In Data</a> (OWID) team.</p><p>OWID is one of the world's largest open online repositories of data and is a valuable source of information about both humanity's progress and challenges. Metaculus will collect forecasts and analyses on thirty OWID metrics\u200a\u2014\u200aincluding CO2 emissions, supercomputer performance, nuclear stockpiles, <a href=\"https://www.metaculus.com/tournament/forecasting-Our-World-in-Data/\">and more</a>\u200a\u2014\u200ato generate likelihoods of specific outcomes in humanity's future. Aggregating crowd forecasts and weighting them by forecasters' past performance leads to <a href=\"https://pubmed.ncbi.nlm.nih.gov/24659192/\">greater accuracy</a> than relying on a single model or subject matter expert alone.</p><p><img src=\"https://metaculus-media.s3.amazonaws.com/user_uploaded/prize-structure_bLNrjPM.jpeg\"></p><p>The <i>Forecasting Our World In Data</i> tournament will award a prize pool of $20,000 to incentivize accurate forecasting and insightful written comments by forecasters. For each OWID metric, participants are invited to predict outcomes at 1, 3, 10, 30, and 100 years. Prizes will be awarded for accurate performance on 1 and 3-year outcomes and cogent analysis on 10, 30, and 100-year outcomes. Questions are staggered, with 10 questions opening today and 10 additional questions opening October 19th and 26th. <a href=\"https://www.metaculus.com/help/faq/#what-are-pros\">Metaculus Pro Forecasters</a> will predict each metric alongside the crowd.</p><p>A grant from the <a href=\"https://ftxfuturefund.org/\">Future Fund</a> makes this project possible, and is greatly appreciated. Our thanks also to the forecasting community for contributing to a greater understanding of humanity's near- and long-term future. The first set of questions is now open for public forecasting <a href=\"https://www.metaculus.com/tournament/forecasting-Our-World-in-Data/\">here</a>.</p>", "user": {"username": "christianM"}}, {"_id": "4wdZAozqJqEYazBnt", "title": "The property rights approach to moral uncertainty", "postedAt": "2022-10-14T16:49:27.825Z", "htmlBody": "<p><i>The summary and introduction can be read below. The full paper is available </i><a href=\"https://www.happierlivesinstitute.org/report/property-rights/\"><i>here</i></a><i>.</i></p><p><i>This working paper was produced as part of the Happier Lives Institute\u2019s&nbsp;</i><a href=\"https://www.happierlivesinstitute.org/2022/07/01/2022-summer-research-fellows/\"><i>2022 Summer Research Fellowship</i></a></p><h1>Summary</h1><p>Given the current state of our moral knowledge, it is entirely reasonable to be uncertain about a wide range of moral issues. Hence, it is surprising how little attention contemporary philosophers have paid (until the past decade) to moral uncertainty. In this paper, I have considered the <i>prima facie</i> plausible suggestion that appropriateness under moral uncertainty is a matter of dividing one\u2019s resources between the moral theories in which one has credence, allowing each theory to use its resources as it sees fit. I have gone on to develop this approach into a fully-fledged Property Rights Theory, sensitive to many of the complications that we face in making moral decisions over time. This Property Rights Theory deserves to takes its place as a leading theory of appropriateness under conditions of moral uncertainty.</p><h1>Introduction</h1><blockquote><p><strong>Distribution:</strong> Imagine that some agent J is devoting her life to \u2018earning to give\u2019: J is pursuing a lucrative career in investment banking and plans to donate most of her lifetime earnings to charity. According to the moral theory T<sub>health</sub>&nbsp;in which J has 60% credence, by far and away the best thing for her to do with her earnings is to donate them to global health charities, and the next best thing is to donate them to charities that benefit future generations by fighting climate change. On the other hand, according to the moral theory T<sub>future</sub> in which J has 40% credence, by far and away the best thing for her to do with her earnings is to donate them to benefitting future generations, and the next best thing is to donate them to global health charities.<a><strong><sup>2</sup></strong></a> On all other issues, T<sub>health</sub> and T<sub>future</sub> are in total agreement: for instance, they agree on where J should work, what she should eat, and what kind of friend she should be. They only disagree about which charity J should donate to. Finally, neither T<sub>health</sub> nor T<sub>future</sub>&nbsp;is <i>risk loving</i>: each theory implies that an $<i>x</i> donation to a charity that the theory approves of is at least no worse than a risky lottery over donations to that charity whose expected donation is $<i>x</i>. In light of her moral uncertainty, what is it appropriate for J to do with her earnings?<a><strong><sup>3</sup></strong></a>&nbsp;</p></blockquote><p>According to one <i>prima facie</i> plausible proposal, it is appropriate for J to donate 60% of her earnings to global health charities and 40% of them to benefitting future generations \u2013 call this response <i>Proportionality</i>. Despite <i>Proportionality\u2019s</i> considerable intuitive appeal, none of the theories of appropriateness under moral uncertainty thus far proposed in the literature support this simple response to <strong>Distribution</strong>.</p><p>In this paper, I propose and defend a <i>Property Rights Theory</i> (henceforth: PRT) of appropriateness under moral uncertainty, which supports <i>Proportionality</i> in <strong>Distribution</strong>.<a><strong><sup>4</sup></strong></a> In \u00a72.1, I introduce the notion of appropriateness. In \u00a72.2, I introduce several of the theories of appropriateness that have been proposed thus far in the literature. In \u00a72.3, I show that these theories fail to support <i>Proportionality</i>. In \u00a7\u00a73.1-3.3, I introduce PRT and I demonstrate that it supports <i>Proportionality</i> in <strong>Distribution</strong>. In \u00a7\u00a73.4-3.9, I discuss the details. In \u00a73.10, I extend my characterisation of PRT to cover cases where an agent faces a choice between discrete options, as opposed to resource distribution cases like <strong>Distribution</strong>.<a><strong><sup>5</sup></strong></a>&nbsp;In \u00a74, I argue that PRT compares favourably to the alternatives introduced in \u00a72.2. In \u00a75, I conclude.</p><p><a href=\"https://www.happierlivesinstitute.org/report/property-rights/\"><i>Read the full paper...</i></a></p><p>&nbsp;</p><p><i><strong>Acknowledgements:</strong></i> <i>For helpful comments and conversations, I wish to thank Conor Downey, Paul Forrester, Hilary Greaves, Daniel Greco, Shelly Kagan, Marcus Pivato, Michael Plant, Stefan Riedener, John Roemer, Christian Tarsney, and Martin Vaeth. I also wish to thank the Forethought Foundation and the Happier Lives Institute for their financial support.&nbsp;</i></p>", "user": {"username": "Harry R. Lloyd"}}, {"_id": "KLctBFtEzBtugGup7", "title": "What Peter Singer Got Wrong (And Where Give Well Could Improve)", "postedAt": "2022-10-14T16:15:14.404Z", "htmlBody": "<p><strong>The Failure</strong></p><p>What Peter Singer got wrong, was he failed to imagine a world which does not need philanthropy. His landmark essay, <a href=\"https://personal.lse.ac.uk/robert49/teaching/mm/articles/Singer_1972Famine.pdf\">Famine, Affluence, and Morality</a> does not go far enough in imagining what the affluent ought to do for the impoverished and marginalized people of the world. He makes a compelling moral argument for charitable giving, but I contend the morally superior effort is in aiming to progress the human condition to a place where poverty and marginalization do not exist, where basic rights and freedoms are respected. I will argue it can only begin to happen with equity, not charity.</p><p>Regarding Give Well\u2019s <a href=\"https://www.givewell.org/research/change-our-mind-contest\">Change Our Mind Contest</a>, I am not sure whether this post qualifies, because the goal of the contest is to red team the cost effectiveness of each charity, meanwhile I aim to argue cost effectiveness might not be the most altruistic goal. While I personally value and donate to Give Well\u2019s top charities, every single one of them is a band-aid solution to health problems. Instead of malaria medication and bed nets, we should be seeking the global public good of herd immunity through a vaccine for malaria (I acknowledge this is <a href=\"https://www.sciencedirect.com/science/article/pii/S0140673621009430\">underway elsewhere</a>); instead of vitamin A supplementation, people deserve an adequate diet which provides sufficient vitamin A; instead of cash transfers for childhood vaccines, we should be supporting implementation of universal health care; instead of deworming, we should seek to understand and prevent the underlying causes of worm infestation.</p><p>What if Peter Singer\u2019s drowning child thought experiment reimagined the child in the story to have a sibling who warns him of danger, or a fence preventing his access to the pond? Improving social norms in equity could be that sibling, and preventive health care, social, and economic policies could be the fence. Endorsing a human rights approach to improving health and wellbeing advances these goals. And Peter\u2019s suit would stay clean and dry.</p><p><strong>Aiming to Extend Altruism Where Charity Falls Short - The Veil of Ignorance</strong></p><p>The state of the human condition is unquestionably better now than it has ever been, but why stop? I have seen EAs indicate that child labour, inequities in vaccine and medication distribution, and <a href=\"https://ourworldindata.org/global-economic-inequality\">growing disparities in income</a> are unfortunate necessities of progress. Why must they be? Why can progress not happen while safeguarding the wellbeing of the least among us?</p><p>The heuristic shift is in considering others as agents, rather than patients of our altruistic efforts (to be clear, this post relates to global health and wellbeing causes for humans; meanwhile, non-human animals, the environment, and future people remain patients, deserving of charity. As much as I understand AI, I presume the current situation is to consider AI as patient, but that AI might be agentic one day).</p><p>John Rawls \u201cveil of ignorance\u201d is a moral approach to reducing inequity and improving agency. Rawls\u2019 thought experiment requires people to choose their justice principles under a veil of ignorance, meaning they would know nothing about their positions in society. Under the veil of ignorance people would not know their own age, sex, race, social class, religion, abilities, preferences, life goals, or anything else about themselves. They would also be ignorant of the society from which they came. They would, however, have general knowledge about how such institutions as economic systems and governments worked. Rawls argued that only under a veil of ignorance could human beings reach a fair and impartial agreement (contract) as true unbiased equals. They would need to rely on reason to choose principles of social justice for their society. Under such veil, how would you choose a health care system if you might be an Afghan woman, or an HIV orphan in sub-Saharan Africa?</p><p>Rawls argued that two principles would emerge from the fair agreement of people behind the veil of ignorance:</p><p>1. His First Principle argues that each person has an equal claim to a fully adequate scheme of equal basic liberties, which scheme is compatible with the same scheme of liberties for all.</p><p>2. The Second Principle argues that social and economic inequalities are to satisfy two conditions:<br>a. first, they are to be attached to offices and positions open to all under conditions of fair equality of opportunity;&nbsp;<br>b. and second, they are to be to the greatest benefit of the least-advantaged members of society (the Difference Principle).</p><p>Incidentally, while John Rawls was a critic of utilitarianism, a common moral framework in EA, <a href=\"https://www.pnas.org/doi/10.1073/pnas.1910125116\">research has found</a> that his veil of ignorance in fact supports utilitarian decision-making, including in decisions about donating to the most effective charity. &nbsp;</p><p><strong>Equity vs Efficiency</strong></p><p>Yes, promoting equity is where economic efficiency \u2013 a key goal of Give Well - and morality intersect. Give Well identifies <a href=\"https://www.givewell.org/research/change-our-mind-contest\">cost-effectiveness</a> as \u201cthe single most important input in our decisions about what programs to recommend\u201d. There are many examples, however, of where efficiency fails to reduce inequality or even improve health. Dr. Paul Farmer highlighted the problem with efficiency in medicine in <a href=\"https://www.ucpress.edu/book/9780520243262/pathologies-of-power\">Pathologies of Power</a>, noting, \u201cthe commodification of medicine invariably punishes the vulnerable\u201d, while providing multiple examples in his book. Basically, using market strategies and efficiency to govern health care leaves behind those who have no money to pay for it. Moreover, commodifying medicine reduces patients to a patient, \u201cthe diabetic\u201d, rather than a person with agency, who has a job and a family. The veil of ignorance goal would be to pursue an economic arrangement which provides a social system for health care \u2013 to the greatest benefit of the least advantaged members of society, per Rawls\u2019 Difference Principle.</p><p>Within global health policy, approaches such as malaria bed nets, deworming and vaccine programs are considered <a href=\"https://assets.researchsquare.com/files/rs-1242181/v1/68dc4055-5b97-41d9-b80a-52f2d95e99dd.pdf?c=1643310402\">vertical health programs</a>, specifically targeting one disease, or a narrow health program, whereas a horizontal health program aims to provide universal basic health care. Most of the arguments against horizontal programs have been lack of efficiency, as opposed to lack of fairness or equity. In 1978 the <a href=\"https://www.who.int/teams/social-determinants-of-health/declaration-of-alma-ata\">Alma Ata Declaration</a> of the World Health Organization aimed to implement horizontal health programs and primary health care, but failed to implement due to opposition and lack of funding.&nbsp;</p><p>While universal health care might not be the most efficient system, I would suggest that because EA/Give Well has become quite successful at fundraising, the increase in available funds could mitigate the inefficiencies of supporting universal health care systems.</p><p><strong>Which Economic System Improves Equity?</strong></p><p>I recently saw a conversation on Twitter between EAs on the economic/social divide between capitalism vs socialism, arguing which endorses greater altruism. The capitalist argued that state-owned means of production does not naturally confer altruism, and an example of this failure of collectivism was the \u201cgroup project\u201d. The argument for socialism was that capitalism provides no inherent protection of the rights and wellbeing of the oppressed and impoverished. I like these conversations, because they seem to be working toward the same goal of figuring out how to improve the lot of everyone, particularly by considering beneficiaries as agents. I would suggest, however, that market strategies cannot be equitably used for provision of health care, and other social services like education, no matter how well they work for commodities. Amartya Sen, in <a href=\"https://en.wikipedia.org/wiki/Development_as_Freedom\"><i>Development as Freedom</i></a>, highlights the shortcomings of markets and growth on liberties and freedoms (both essential for people to be agents, rather than patients), using the capabilities approach, \u201cGrowth of GNP or of industrial incomes can, of course, be very important as means to expanding the freedoms enjoyed by the members of the society, but freedoms depend on other determinants such as social and economic arrangements, for example, facilities for education and health care, as well as political and civil rights, for example, the liberty to participate in public discussion and scrutiny\u201d. He argues that using mortality as an indicator of economic success and failure, \u201creminds us of the need to move beyond the cold and often inarticulate statistics \u2026. to look at the various ways in which agency, the capabilities of each person, is constrained\u201d.</p><p>Michael Plant, of the Happier Lives Institute, also questions whether growth is the best avenue to improving the human lot in <a href=\"https://forum.effectivealtruism.org/posts/gCDsAj3K5gcZvGgbg/will-faster-economic-growth-make-us-happier-the-relevance-of\">this post</a>. Citing Richard Easterlin, he seeks alternative possibilities to improve collective wellbeing, \u201cMore concretely, in his 2021 book, An Economist\u2019s Lessons on Happiness, Easterlin suggests that job security, a comprehensive welfare state, getting citizens to be healthy, and encouraging long-term relationships would increase average wellbeing. All of those seem fairly plausible to me.\u201d Health is inextricably tied to wellbeing and longevity, and I would suggest that envisioning persons as agents by improving equity and social health systems using the veil of ignorance is arguably the most moral approach to improvements in health.</p><p>The United State of America <a href=\"https://ourworldindata.org/us-life-expectancy-low#life-expectancy-and-health-expenditure-over-time-the-us-is-an-outlier\">provides a stark example</a> of how the market system fails in the area of health care, and how the lack of universal health care impacts health and life expectancy; despite spending far more on health than any other country in the world, the life expectancy of the American population is shorter than in other rich countries that provide varying levels of social health care, but spend far less.</p><p><strong>To Summarize</strong></p><p>I am suggesting the EA community/Give Well must consider being leaders in the effort of setting new global norms of equity and health as a human right. I am arguing that EA/Give Well move from a cost-effective, vertically-designed charity system for global health, to applying the veil of ignorance to the provision health care. Not the least reason because there is increasing criticism of billionaire philanthropy, suggesting it to be a means of maintaining the status quo of the wealthy and powerful to continue to make decisions for the masses, a position which directly opposes Rawls\u2019 veil of ignorance, and the understanding of persons as agents. Two recent books, <a href=\"https://www.penguinrandomhouse.com/books/539747/winners-take-all-by-anand-giridharadas/\">Winners Take All</a>, by Anand Giridharadas, and <a href=\"https://drodrik.scholar.harvard.edu/publications/globalization-paradox-democracy-and-future-world-economy\">The Globalization Paradox</a>, by Dani Rodrik, have raised this concern.</p><p><strong>Action Points</strong></p><p>Would Give Well consider causes supporting improved equity and social health care?&nbsp;<br>\u2022 By promoting horizontal health care strategies which provide universal basic health care<br>\u2022 Through understanding the person as agent, by endorsing health as a human right. In Pathologies of Power Dr. Paul Farmer argues, \u201chuman rights are respected when everyone has food, shelter, education, and health care\u201d. The <a href=\"https://ajph.aphapublications.org/doi/full/10.2105/AJPH.2004.044313\">movement to access medication for HIV/AIDS</a> has been instrumental in promoting health as a human right for people living with HIV.&nbsp;<br>\u2022 A human rights-based approach to improvement of health care might include <a href=\"https://forum.effectivealtruism.org/posts/uMgPLPpovQbYupPnR/new-cause-area-baby-longtermism-child-rights-and-future\">lobbying the US to ratify the UN Convention on the Rights of the Child</a>.&nbsp;<br>\u2022 Working with <a href=\"https://www.ohchr.org/en/special-procedures/sr-health\">UN Special Rapporteur for Human Rights in Health</a>, Dr. Tlaleng Mofokeng, to determine the wants and needs of people as agents.&nbsp;<br>\u2022 Supporting activists like Priti Krishtel, a health justice lawyer, MacArthur Fellow, and founder of <a href=\"https://www.i-mak.org/people/priti-krishtel/\">Initiatives for Medicine, Access and Knowledge</a>, which advocates to reduce patent protection, and thus costs of medication.&nbsp;</p><p>Notably, Dr. Akhil Bansal's prize-winning new EA cause area, addressing <a href=\"https://forum.effectivealtruism.org/posts/majcwf7i8pW8eMJ3v/new-cause-area-violence-against-women-and-girls\">Violence Against Women and Girls</a> is an excellent example of a health-related human rights cause.</p><p><strong>One Amendment</strong></p><p>As I am about to publish this to the forum, across my Twitter feed I see Peter Singer <a href=\"https://twitter.com/PeterSinger/status/1572285391859752963?s=20&amp;t=0Ercs26t4QuncJ7F59b1ig\">petitioning for the rights of the women of Iran</a>. I stand corrected in suggesting Peter lacks imagination of a world in which everyone enjoys basic rights and freedoms, and will amend to indicate that it was only his 1971 essay which did not imagine it.</p><p>&nbsp;</p><p>ETA: Investing in health systems is a <a href=\"https://www.bmj.com/content/379/bmj.o2475\">global common good</a>.</p>", "user": {"username": "LiaH"}}, {"_id": "CjkkfSFLsZzeTxwmT", "title": "Answering some questions about water quality programs", "postedAt": "2022-10-14T20:36:02.820Z", "htmlBody": "<p><em>Author: Miranda Kaplan, GiveWell Communications Associate</em></p>\n<p>On June 22, we held a virtual event on research into <a href=\"https://www.givewell.org/international/technical/programs/water-quality-interventions\">water quality interventions</a>, featuring presentations from Michael Kremer of the University of Chicago\u2019s Development Innovation Lab; Brett Sedgewick from Evidence Action, the parent organization of Dispensers for Safe Water; and Stephan Guyenet, Elie Hassenfeld, and Catherine Hollander of GiveWell. (If you weren\u2019t able to attend, we\u2019ve published a video recording, audio recording, and transcript <a href=\"https://www.givewell.org/research/research-discussions\">here</a>.)</p>\n<p>We hosted the event to provide some additional background for our recommendation of up to $64.7 million to <a href=\"https://www.evidenceaction.org/dispensersforsafewater/\">Dispensers for Safe Water</a>, which installs chlorine dispensers to treat water at rural collection sites in Kenya, Malawi, and Uganda. This grant was the result of a lengthy investigation and a significant update in our views on the cost-effectiveness of water treatment, which we\u2019ve written about <a href=\"https://blog.givewell.org/2022/04/06/water-quality-overview/\">here</a>.</p>\n<p>Several attendees wrote in with a range of thoughtful questions\u2014about our analysis of the effects of chlorination interventions, about the particulars of Dispensers for Safe Water\u2019s program, or more generally about our work. We covered as many as we could during the event and followed up on others by email. Below, we\u2019re sharing a selection of the questions we responded to in writing, along with other questions we\u2019ve gotten about this work outside of the event, in the hope that they\u2019ll be of interest to a broader audience. Questions and answers have been anonymized; some have been edited slightly for brevity, or to fill in important context that was missing.</p>\n<p>We always appreciate getting your questions\u2014beyond giving us a chance to clarify our work, it provides valuable insight into what we\u2019re not communicating as well as we could. Feel free to email us with your own questions, about water quality or anything else, at <a href=\"mailto:info@givewell.org\">info@givewell.org</a>. You can also comment directly on this blog post or on our <a href=\"https://blog.givewell.org/2022/09/12/september-2022-open-thread/\">most recent open thread</a>. We aim to respond to all questions, though it may take us a few days to get back to you.</p>\n<h3>On Dispensers for Safe Water and our grant recommendation</h3>\n<h4>Q: This grant represents a big investment in the <a href=\"https://www.evidenceaction.org/dispensersforsafewater/\">Dispensers for Safe Water approach</a> of installing chlorine dispensers next to water collection points. A worry for me is that the dispensers won\u2019t be kept in good working order, making the program less effective\u2014I\u2019ve heard that maintenance of facilities can be a challenge in water provision and sanitation programs. Has anyone checked the effectiveness of the maintenance that Dispensers for Safe Water does?</h4>\n<p>A: Dispensers for Safe Water performs a number of checks to make sure that the dispensers are still working as they should be, and also conducts surveys to make sure that the maintenance is working. A summary of Dispensers for Safe Water\u2019s monitoring and evaluation activities can be found in our 2018 report on the program <a href=\"https://www.givewell.org/charities/dispensers-for-safe-water-December-2018-version#WhatmonitoringdoestheDispensersforsafeWaterprogramcollect\">here</a>, although the January 2022 grant covers additional data collection activities.</p>\n<p>Program staff conduct spot checks of dispensers when delivering chlorine and when conducting surveys.<sup class=\"footnote-ref\"><a href=\"#fn-MJeDSvAv5vtCESRSJ-1\" id=\"fnref-MJeDSvAv5vtCESRSJ-1\">[1]</a></sup> Promoters (community volunteers who refill the chlorine dispensers and promote use of the dispensers) are also asked to report any faults with the dispensers and are given a number to call. When promoters in Kenya call to report maintenance issues, the issues are logged and tracked using a cloud-based app; Evidence Action hopes to eventually launch the issue tracker app in Uganda and Malawi as well. Promoters and staff delivering chlorine are trained to address basic functionality problems, and specialized staff engineers are brought in for more serious faults.<sup class=\"footnote-ref\"><a href=\"#fn-MJeDSvAv5vtCESRSJ-2\" id=\"fnref-MJeDSvAv5vtCESRSJ-2\">[2]</a></sup></p>\n<p>To confirm that the dispensers remain functional, Dispensers for Safe Water conducts surveys six times a year to (a) determine how many dispensers remain in use and (b) measure the percentage of households served by dispensers who have chlorine in their water. For the latter, Dispensers for Safe Water randomly selects households to visit, asks them to pour a glass of water, and measures the chlorine content. In 2019, Dispensers for Safe Water\u2019s data showed that approximately 50% of targeted households poured a glass of water that contained chlorine, suggesting that dispensers are in use in those locations. (We\u2019re citing 2019 data here because we think conditions during COVID-19 may not be representative of the future.) See <a href=\"https://docs.google.com/spreadsheets/d/1s9DJcFwFHiyjkvR2UMmrYDH5ZPOfMoDFtHYeUFwpaVY/edit#gid=1671398215&amp;range=C43:I46\">here</a> in <a href=\"https://docs.google.com/spreadsheets/d/1s9DJcFwFHiyjkvR2UMmrYDH5ZPOfMoDFtHYeUFwpaVY/edit#gid=1671398215\">Dispensers for Safe Water 2019 monitoring data analysis</a>.</p>\n<h4>Q: Evidence Action says Dispensers for Safe Water is currently reaching four million people in Kenya, Malawi, and Uganda, and this grant will allow them to expand that to 9.5 million. But Michael Kremer mentioned that more than two billion people are consuming contaminated water each year. Is there a good estimate for how much it would cost to scale up this kind of program so that all of the approximately two billion affected people have safe water?</h4>\n<p>A: We haven\u2019t come up with an estimate for what it would cost to scale up Dispensers for Safe Water (or a similar program) to all people who could benefit from it, and most likely some locations with unsafe water wouldn\u2019t meet our current cost-effectiveness bar for funding. Our cost-effectiveness analyses are location-specific, and we would expect the cost-effectiveness of scaling up this program globally to vary quite a bit in each country, or even from region to region within a country, depending on overall mortality rates there and how much of that mortality is due to enteric infections.</p>\n<p>But generally, our estimates suggest that the locations where simple chlorination interventions, such as Dispensers for Safe Water, tend to be very cost-effective are mostly African countries with a low <a href=\"https://www.healthdata.org/taxonomy/glossary/socio-demographic-index-sdi\">socio-demographic index (SDI)</a>. African countries with a slightly higher SDI (low-middle as opposed to low) are less well represented among the most cost-effective locations.<sup class=\"footnote-ref\"><a href=\"#fn-MJeDSvAv5vtCESRSJ-3\" id=\"fnref-MJeDSvAv5vtCESRSJ-3\">[3]</a></sup> These locations might not meet our cost-effectiveness bar, but they could still be covered by other funders.</p>\n<p>Though we can\u2019t confidently say how much it would cost to bring this chlorination intervention to two billion people annually, we have come up with a very rough, speculative estimated cost of <a href=\"https://docs.google.com/spreadsheets/d/1c64hM7rt8PyJ6YnPnCryBMmWBs49sv1P_iqUF6mGB9c/edit#gid=1812471139&amp;range=B4\">$170 million/year</a> to cover all locations where we think the program might be at least eight times as cost-effective as cash transfers. This figure relies on a number of uncertain assumptions about costs, how much of each country\u2019s population Dispensers for Safe Water could reach, etc., but it may help give a sense of the huge (and cost-effective) global funding gap for water treatment.</p>\n<h3>On our meta-analysis of water treatment\u2019s effects</h3>\n<h4>Q: Michael Kremer and his team found a very large effect of water treatments like chlorination on child mortality, and GiveWell found a smaller but still significant mortality effect. What about effects beyond mortality? For instance, I\u2019m curious about the possibility of increasing the child\u2019s adult income because of improved overall health, or the impact of reducing the family\u2019s expenditure on medical treatment, etc. How confident can we be in these results?</h4>\n<p>A: The single largest benefit we <a href=\"https://docs.google.com/spreadsheets/d/1dxRfhPnTBJ_UYAeoAyVd8u6Orxh4pR9hB3wvdfZcXFs/edit#gid=1988241019\">model</a> from chlorination interventions like Dispensers for Safe Water is a reduction in mortality among children under five, but we do incorporate <a href=\"https://www.givewell.org/international/technical/programs/water-quality-interventions#Other_benefits_of_water_quality_interventions\">other benefits</a> as well:</p>\n<ul>\n<li><strong>Development effects.</strong> We believe reducing illness in children probably improves their development, leading to slightly higher income in adulthood. We refer to this as \u201cdevelopment effects\u201d and <a href=\"https://docs.google.com/spreadsheets/d/1dxRfhPnTBJ_UYAeoAyVd8u6Orxh4pR9hB3wvdfZcXFs/edit#gid=1988241019&amp;range=A38:D49\">include it</a> in our model. We don\u2019t have direct evidence that water treatment causes this, but we do have direct evidence that malaria prevention and deworming cause it, and we think it probably applies here as well.</li>\n<li><strong>Medical costs averted.</strong> If a child becomes ill less often because their water is chlorinated, we do expect the family will save money on medical treatment, and we <a href=\"https://docs.google.com/spreadsheets/d/1dxRfhPnTBJ_UYAeoAyVd8u6Orxh4pR9hB3wvdfZcXFs/edit#gid=1988241019&amp;range=A51:D70\">factor in this savings</a>. The estimate we arrived at in our cost-effectiveness analysis for water treatment was higher than we\u2019d initially expected. We\u2019re working on refining this and investigating how generalizable it might be to other types of programs.</li>\n<li><strong>Over-five mortality.</strong> We estimate that water treatment reduces mortality in people older than five by <a href=\"https://docs.google.com/spreadsheets/d/1dxRfhPnTBJ_UYAeoAyVd8u6Orxh4pR9hB3wvdfZcXFs/edit#gid=1988241019&amp;range=A24:D24\">1 to 4%</a>. That\u2019s less than the <a href=\"https://docs.google.com/spreadsheets/d/1dxRfhPnTBJ_UYAeoAyVd8u6Orxh4pR9hB3wvdfZcXFs/edit#gid=1988241019&amp;range=A10:D10\">6 to 11%</a> reduction in all-cause mortality we estimate in children under five, but not negligible, given that there are more people over five than under five. Much of this benefit is expected to occur in children just over five.<sup class=\"footnote-ref\"><a href=\"#fn-MJeDSvAv5vtCESRSJ-4\" id=\"fnref-MJeDSvAv5vtCESRSJ-4\">[4]</a></sup></li>\n</ul>\n<h4>Q: In describing the meta-analysis on which this grant recommendation was based, you mentioned that you took steps to limit publication bias, and Michael Kremer also mentioned doing checks for publication bias in his own analysis. Can you explain what that is and why it mattered to this investigation?</h4>\n<p>A: \u201cPublication bias\u201d is when a body of scientific literature is biased because certain types of results are more likely to be published than others. There are different types of publication bias, but a common example is that studies that produce statistically significant results are more likely to be published than studies that do not produce such results. This biases the available literature on the subject in favor of research that finds an effect, which causes the literature as a whole to exaggerate effect sizes. The <a href=\"https://en.wikipedia.org/wiki/Publication_bias\">Wikipedia page on publication bias</a> has some good further explanation and examples.</p>\n<p>When reviewing the evidence behind an intervention, we consider whether publication bias is a factor and make adjustments in our cost-effectiveness models accordingly. In this case, we decided to do our own meta-analysis to arrive at our own pooled estimate of water chlorination\u2019s impact on deaths. We think our <a href=\"https://docs.google.com/spreadsheets/d/1dxRfhPnTBJ_UYAeoAyVd8u6Orxh4pR9hB3wvdfZcXFs/edit#gid=123265793&amp;range=A10:B14\">pooled estimate</a>, which guided our decision to recommend the grant to Dispensers for Safe Water, is resistant to publication bias because the vast majority of the weight comes from three large, recent water quality trials. We think this because all of the recent large trials of water quality have reported mortality outcomes, and results from large trials usually don\u2019t go unpublished, regardless of outcome (so it is unlikely that only statistically significant results showing a benefit would be chosen for publication). We also excluded smaller trials with shorter follow-up periods (that is, the period of time in which study participants are observed after the treatment), which tend to be more susceptible to publication bias.<sup class=\"footnote-ref\"><a href=\"#fn-MJeDSvAv5vtCESRSJ-5\" id=\"fnref-MJeDSvAv5vtCESRSJ-5\">[5]</a></sup></p>\n<p>We ultimately arrived at an estimated <a href=\"https://docs.google.com/spreadsheets/d/1dxRfhPnTBJ_UYAeoAyVd8u6Orxh4pR9hB3wvdfZcXFs/edit#gid=123265793&amp;range=A11:B11\">14% reduction</a> in all-cause mortality in children under five for chlorination interventions generally, with <a href=\"https://docs.google.com/spreadsheets/d/1dxRfhPnTBJ_UYAeoAyVd8u6Orxh4pR9hB3wvdfZcXFs/edit#gid=1988241019&amp;range=A10:D10\">lower estimates</a> for the effect of Dispensers for Safe Water specifically. This led us to update our cost-effectiveness estimate for Dispensers for Safe Water to between <a href=\"https://docs.google.com/spreadsheets/d/1dxRfhPnTBJ_UYAeoAyVd8u6Orxh4pR9hB3wvdfZcXFs/edit#gid=1988241019&amp;range=A145:D145\">four and nine times as cost-effective as unconditional cash transfers</a> (depending on location). You can read more about Michael Kremer et al.\u2019s meta-analysis and our own meta-analysis <a href=\"https://www.givewell.org/international/technical/programs/water-quality-interventions#Randomized_controlled_trials\">here</a>, and more about how we arrived at our cost-effectiveness estimates <a href=\"https://www.givewell.org/international/technical/programs/water-quality-interventions#How_we_incorporate_the_evidence_into_our_cost-effectiveness_analyses\">here</a>.</p>\n<h4>Q: Michael Kremer mentioned that his team\u2019s analysis included <a href=\"https://statswithr.github.io/book/the-basics-of-bayesian-statistics.html\">Bayesian estimates</a>. I know just a little about Bayesian analysis in RCTs, but I recall the idea of enthusiastic and skeptical priors. Were those used in this Bayesian analysis? And, are there \u201centhusiasts\u201d and \u201cskeptics\u201d on this issue?</h4>\n<p>Kremer et al. used a prior centered around zero effect, with a wide standard deviation. As they explain in the <a href=\"https://cpb-us-w2.wpmucdn.com/voices.uchicago.edu/dist/0/2830/files/2022/02/Water-meta-analysis-supplement-2022.02.22.docx-1.pdf\">supplementary materials</a> for their <a href=\"https://bfi.uchicago.edu/working-paper/2022-26/\">paper</a>: \u201cFor \u03c4, we set a normal distribution with mean 0 and standard deviation of 10. This prior encodes the belief that causal effects should not be thought of as large unless data contains evidence to the contrary\u201d (p. 3). Kremer et al. conducted a number of sensitivity analyses, but we are not aware that they tried pessimistic and optimistic priors in their Bayesian meta-analysis.<sup class=\"footnote-ref\"><a href=\"#fn-MJeDSvAv5vtCESRSJ-6\" id=\"fnref-MJeDSvAv5vtCESRSJ-6\">[6]</a></sup></p>\n<p>More broadly, GiveWell did not consider optimistic and pessimistic scenarios. We sometimes consider these for decision-making purposes if a program is below our funding bar and we want to see how plausible it is that further work would raise it above the bar. However, if a program is over our funding bar, we typically make funding decisions based on our best guess, assuming that the best guess of cost-effectiveness represents the expected value of the entire probability distribution of possible outcomes. In other words, we assume that the probability that the program is better than we think balances out the probability that it is worse than we think. We are aware that this assumption may not always be satisfied, and we may do further work on this issue.</p>\n<p>Michael Kremer\u2019s meta-analysis is recent and not yet published in a peer-reviewed journal, and we aren\u2019t aware that people have publicly aligned themselves as advocates or skeptics yet. However, the external expert reviewers who we contracted to review an earlier draft of the Kremer et al. meta-analysis were fairly skeptical of the findings.<sup class=\"footnote-ref\"><a href=\"#fn-MJeDSvAv5vtCESRSJ-7\" id=\"fnref-MJeDSvAv5vtCESRSJ-7\">[7]</a></sup> Our analysis of the data reflects the reviewers\u2019 skepticism, and our estimated effect size is quite a bit smaller than the one in the Kremer et al. meta-analysis (although confidence intervals overlap),<sup class=\"footnote-ref\"><a href=\"#fn-MJeDSvAv5vtCESRSJ-8\" id=\"fnref-MJeDSvAv5vtCESRSJ-8\">[8]</a></sup> so it would be fair to say we are also on the more skeptical side.</p>\n<h3>On unintended consequences of charity-run programs</h3>\n<h4>Q: What do you make of the criticism that charities reduce the pressure put on local governments to deliver better services?</h4>\n<p>A: This is a reasonable critique. We believe that there could be knock-on effects from funding charities to implement water chlorination (or other) programs, such as making governments less likely to provide these services themselves, and right now we are not very confident in our ability to measure the likelihood of such effects. However, we believe there is a role for private funding to play in getting vital services to people who need them. Water treatment is a service that is clearly needed and not currently provided by the governments where Dispensers for Safe Water operates.<sup class=\"footnote-ref\"><a href=\"#fn-MJeDSvAv5vtCESRSJ-9\" id=\"fnref-MJeDSvAv5vtCESRSJ-9\">[9]</a></sup> Based on our conversations with Evidence Action and others, it also appears to be highly neglected by private funders, and therefore well suited for impact-motivated philanthropy to step in.</p>\n<p>Additionally, many programs are set up to be partnerships between government and non-governmental organizations, in which the NGO provides training, monitoring and evaluation activities, or other forms of \u201ctechnical assistance\u201d to government staff, who actually deliver the services. We\u2019ve found that these partnerships can result in more impact than NGOs acting alone.</p>\n<p>For example, Deworm the World Initiative supports the governments of India, Kenya, Nigeria, and Pakistan in their implementation of deworming campaigns (more <a href=\"https://www.givewell.org/charities/deworm-world-initiative/August-2022-version#Role\">here</a>). Against Malaria Foundation works with countries\u2019 national malaria programs to determine funding needs and allocation decisions for insecticide treated nets campaigns (more <a href=\"https://www.givewell.org/research/grants/AMF-LLIN-campaigns-October-2021\">here</a>).</p>\n<p>Beyond our top charities, we\u2019ve recommended grants to organizations that provide technical assistance to governments to make their programs more effective. For example, we recommended a grant to another program from Evidence Action that is working to support the Liberian government in switching from HIV rapid tests during routine antenatal care to more effective dual HIV/syphilis rapid tests and syphilis treatment (more <a href=\"https://www.givewell.org/research/incubation-grants/Evidence-Action-syphilis-aug-2020\">here</a>).</p>\n<h3>On GiveWell\u2019s research process and sources of potential new funding opportunities</h3>\n<h4>Q: Could you please share the process GiveWell uses to make funding decisions\u2014how you narrow down potential projects for investigation and then grantmaking?</h4>\n<p>A: Our process for investigating a grant consists of three stages, though not all of them apply to all grants:</p>\n<ul>\n<li><strong>Research review.</strong> We look at the evidence behind an intervention, talk to experts and possible implementing organizations, and build a rough cost-effectiveness model. This stage applies more to new interventions than top charities, since we already are very familiar with top-charity programs and have directed funds to them before.</li>\n<li><strong>Strong interest.</strong> We\u2019ve determined that the program likely is cost-effective enough for us to consider funding, and we want to move on to a deeper investigation of a specific grant opportunity.</li>\n<li><strong>Grant recommendation</strong> (called \u201cconditional approval\u201d internally, as the grant approval is conditional on the funder\u2019s confirmation). We\u2019ve done a thorough analysis of cost-effectiveness and spent a lot of time talking to the potential grantee about the funding opportunity. We\u2019ve decided it meets our criteria for funding, and we want to recommend the grant (or make it ourselves).</li>\n</ul>\n<p>When evaluating a program for funding, we look for evidence of its effectiveness, its impact per dollar spent, room for more funding (i.e., how much money it can productively use), and whether the organization seems like a strong partner that will share transparently with us and allow us to share our views about it publicly. We also consider what effect our funding in this situation will have on other funders\u2019 decisions\u2014i.e., whether it will cause them to allocate more or less money to this intervention (more <a href=\"https://blog.givewell.org/2018/02/13/revisiting-leverage/\">here</a>).</p>\n<p>Right now we have 350 programs in our pipeline that we\u2019d potentially like to investigate, and we\u2019re actively investigating about 60 of them. But we don\u2019t expect to make grants to all of them\u2014we think probably about 15 of these active investigations will result in grants.</p>\n<p>You can read more about our research process and see a partial list of prioritized programs <a href=\"https://www.givewell.org/research/research-on-programs\">here</a>.</p>\n<h2>Notes</h2>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-MJeDSvAv5vtCESRSJ-1\" class=\"footnote-item\"><p>\"Dispensers can break for a variety of reasons, such as vandalism, disruption caused by animals, or wear and tear. DSW staff conduct functionality spot checks when delivering the chlorine and when conducting M&amp;E surveys.\" <a href=\"https://docs.google.com/document/d/17ghPMgm0q5gTqp23wSOu9ZPHWq8Nd1MSG0ylTqDpxUQ/edit\">GiveWell, Dispensers for Safe Water Organization basics, based on field notes, 2022</a>, p. 2. <a href=\"#fnref-MJeDSvAv5vtCESRSJ-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-MJeDSvAv5vtCESRSJ-2\" class=\"footnote-item\"><ul>\n<li>\"Promoters are volunteers who refill the chlorine dispensers and promote the use of the dispensers\u2026\" <a href=\"https://docs.google.com/document/d/17ghPMgm0q5gTqp23wSOu9ZPHWq8Nd1MSG0ylTqDpxUQ/edit\">GiveWell, Dispensers for Safe Water Organization basics, based on field notes, 2022</a>, p. 2.</li>\n<li>\"The two promoters are trained by DSW program staff on their roles. Their key roles are to educate those who might not have attended the meeting, do dispenser promotional activities, receive chlorine from program staff, refill the dispenser, and inform DSW program staff/call center when dispenser repair, maintenance or additional chlorine is needed.\" <a href=\"https://docs.google.com/document/d/17ghPMgm0q5gTqp23wSOu9ZPHWq8Nd1MSG0ylTqDpxUQ/edit\">GiveWell, Dispensers for Safe Water Organization basics, based on field notes, 2022</a>, p. 4.</li>\n<li>\"Promoters are also asked to report faults (they are provided with a phone number they can use to report problems). Promoters and staff delivering chlorine are trained to address basic functionality problems; engineers are mobilised for more serious faults. When promoters call to report maintenance issues, the issues are logged and tracked using a cloud based application (issue tracker) in Kenya. The issue tracker app just launched for testing in Uganda in Sept. 2021 and we hope to begin testing in Malawi in 2022.\" <a href=\"https://docs.google.com/document/d/17ghPMgm0q5gTqp23wSOu9ZPHWq8Nd1MSG0ylTqDpxUQ/edit\">GiveWell, Dispensers for Safe Water Organization basics, based on field notes, 2022</a>, p. 2.</li>\n</ul>\n <a href=\"#fnref-MJeDSvAv5vtCESRSJ-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></li>\n<li id=\"fn-MJeDSvAv5vtCESRSJ-3\" class=\"footnote-item\"><ul>\n<li>SDI, a metric developed by the Institute for Health Metrics and Evaluation (IHME), is expressed on a scale between 0 and 1, as explained on <a href=\"https://www.healthdata.org/taxonomy/glossary/socio-demographic-index-sdi\">this page</a>. A \"low-SDI\" country is one that measures below 0.454. See the range of scores in each quintile <a href=\"https://ghdx.healthdata.org/sites/default/files/record-attached-files/IHME_GBD_2019_SDI_1950_2019_QUINTILES_Y2021M03D21.XLSX\">here</a>.</li>\n<li>See columns <a href=\"https://docs.google.com/spreadsheets/d/1c64hM7rt8PyJ6YnPnCryBMmWBs49sv1P_iqUF6mGB9c/edit#gid=1812471139&amp;range=I8:I38\">\"2019 SDI\"</a> and <a href=\"https://docs.google.com/spreadsheets/d/1c64hM7rt8PyJ6YnPnCryBMmWBs49sv1P_iqUF6mGB9c/edit#gid=1812471139&amp;range=B8:B38\">\"Cost-effectiveness\"</a> in the \"Long term RFMF\" tab of <a href=\"https://docs.google.com/spreadsheets/d/1c64hM7rt8PyJ6YnPnCryBMmWBs49sv1P_iqUF6mGB9c/edit#gid=1812471139\">this spreadsheet</a> for SDI and estimates of cost-effectiveness by potential expansion country. Of the 18 countries in the list that are at least 8x as cost-effective as cash transfers, 16 are low-SDI countries.</li>\n<li>At the time of our decision to recommend the grant to Dispensers for Safe Water, we were looking primarily for funding opportunities that we estimated to be at least eight times as cost-effective as unconditional cash transfers (\"8x cash\"). We've since raised our bar for funding to 10x cash (more <a href=\"https://blog.givewell.org/2022/07/05/update-on-givewells-funding-projections/\">here</a>).</li>\n</ul>\n <a href=\"#fnref-MJeDSvAv5vtCESRSJ-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></li>\n<li id=\"fn-MJeDSvAv5vtCESRSJ-4\" class=\"footnote-item\"><p>See <a href=\"http://ihmeuw.org/5v7o\">Global Burden of Disease, GBD Compare, Age distribution of enteric infection mortality in sub-Saharan Africa, 2019</a>. Deaths from enteric infection drop sharply after age five, but the age group with the next highest number of deaths in 2019 was children between five and nine. <a href=\"#fnref-MJeDSvAv5vtCESRSJ-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-MJeDSvAv5vtCESRSJ-5\" class=\"footnote-item\"><p>For more information about how we generated our pooled estimate, see the \"Randomized controlled trials\" section of our <a href=\"https://www.givewell.org/international/technical/programs/water-quality-interventions#Randomized_controlled_trials\">water quality intervention report</a> (beginning with \"To limit these concerns and generate an estimate\u2026\"). <a href=\"#fnref-MJeDSvAv5vtCESRSJ-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-MJeDSvAv5vtCESRSJ-6\" class=\"footnote-item\"><p>See <a href=\"https://cpb-us-w2.wpmucdn.com/voices.uchicago.edu/dist/0/2830/files/2022/02/Water-meta-analysis-supplement-2022.02.22.docx-1.pdf\">Kremer et al. 2022 supplementary materials</a>, p. 18, table S6, for a list of sensitivity analyses the authors conducted. <a href=\"#fnref-MJeDSvAv5vtCESRSJ-6\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-MJeDSvAv5vtCESRSJ-7\" class=\"footnote-item\"><p>More in the \"Randomized controlled trials\" section of our <a href=\"https://www.givewell.org/international/technical/programs/water-quality-interventions#Randomized_controlled_trials\">water quality intervention report</a> (beginning with \"We have reviewed Kremer et al. 2022 (working paper) internally\u2026\"). <a href=\"#fnref-MJeDSvAv5vtCESRSJ-7\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-MJeDSvAv5vtCESRSJ-8\" class=\"footnote-item\"><ul>\n<li>\"Depending on the method used to pool results, the [Kremer et al.] analysis reports that water quality interventions reduce the odds of all-cause mortality in children under five by 28% (95% confidence interval, 8% to 45%) or 30% (95% credible interval, 8% to 51%).\" <a href=\"https://www.givewell.org/international/technical/programs/water-quality-interventions#Randomized_controlled_trials\">GiveWell, \"Water quality interventions,\" 2022, \"Randomized controlled trials\" section</a></li>\n<li>\"The resulting estimate [from GiveWell's analysis] suggests that chlorination interventions reduce all-cause mortality in children under five by approximately 14% in low-income settings (95% confidence interval, 32% reduction to 10% increase).\" <a href=\"https://www.givewell.org/international/technical/programs/water-quality-interventions#Randomized_controlled_trials\">GiveWell, \"Water quality interventions,\" 2022, \"Randomized controlled trials\" section</a></li>\n</ul>\n <a href=\"#fnref-MJeDSvAv5vtCESRSJ-8\" class=\"footnote-backref\">\u21a9\ufe0e</a></li>\n<li id=\"fn-MJeDSvAv5vtCESRSJ-9\" class=\"footnote-item\"><p>\"Across rural Kenya, Uganda, and Malawi, we consistently provide water treatment in areas that aren\u2019t reached by municipal systems \u2013 and at no cost to users or their communities.\" <a href=\"https://www.evidenceaction.org/dispensersforsafewater/\">Evidence Action, \"Delivering safe water to millions\"</a> <a href=\"#fnref-MJeDSvAv5vtCESRSJ-9\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "GiveWell"}}, {"_id": "zoWypGfXLmYsDFivk", "title": "Counterarguments to the basic AI risk case", "postedAt": "2022-10-14T20:30:49.825Z", "htmlBody": "<p><i>This is cross-posted from the </i><a href=\"https://aiimpacts.org/counterarguments-to-the-basic-ai-x-risk-case/\"><i>AI Impacts blog</i></a></p><p>This is going to be a list of holes I see in the basic argument for existential risk from superhuman AI systems<a href=\"https://aiimpacts.org/counterarguments-to-the-basic-ai-x-risk-case/#easy-footnote-bottom-1-3345\"><sup>1</sup></a>.&nbsp;</p><p>To start, here\u2019s an outline of what I take to be the basic case<a href=\"https://aiimpacts.org/counterarguments-to-the-basic-ai-x-risk-case/#easy-footnote-bottom-2-3345\"><sup>2</sup></a>:</p><p><strong>I. If superhuman AI systems are built, any given system is likely to be \u2018goal-directed\u2019</strong></p><p>Reasons to expect this:</p><ol><li>Goal-directed behavior is likely to be valuable, e.g. economically.&nbsp;</li><li>Goal-directed entities may tend to arise from machine learning training processes not intending to create them (at least via the methods that are likely to be used).</li><li>\u2018Coherence arguments\u2019 may imply that systems with some goal-directedness will become more strongly goal-directed over time.</li></ol><p><strong>II. If goal-directed superhuman AI systems are built, their desired outcomes will probably be about as bad as an empty universe by human lights&nbsp;</strong></p><p>Reasons to expect this:</p><ol><li>Finding useful goals that aren\u2019t extinction-level bad appears to be hard: we don\u2019t have a way to usefully point at human goals, and divergences from human goals seem likely to produce goals that are in intense conflict with human goals, due to a) most goals producing <a href=\"https://en.wikipedia.org/wiki/Instrumental_convergence\">convergent incentives</a> for controlling everything, and b) value <a href=\"https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile\">being</a> \u2018fragile\u2019, such that an entity with \u2018similar\u2019 values will generally create a future of virtually no value.</li><li>Finding goals that are extinction-level bad and temporarily useful appears to be easy: for example, advanced AI with the sole objective \u2018maximize company revenue\u2019 might profit said company for a time before gathering the influence and wherewithal to pursue the goal in ways that blatantly harm society.</li><li>Even if humanity found acceptable goals, giving a powerful AI system any specific goals appears to be hard. We don\u2019t know of any procedure to do it, and we have theoretical reasons to expect that AI systems produced through machine learning training will generally end up with goals other than those they were trained according to. Randomly aberrant goals resulting are probably extinction-level bad for reasons described in II.1 above.<br>&nbsp;</li></ol><p><strong>III. If most goal-directed superhuman AI systems have bad goals, the future will very likely be bad</strong></p><p>That is, a set of ill-motivated goal-directed superhuman AI systems, of a scale likely to occur, would be capable of taking control over the future from humans. This is supported by at least one of the following being true:</p><ol><li><strong>Superhuman AI would destroy humanity rapidly. </strong>This may be via ultra-powerful capabilities at e.g. technology design and strategic scheming, or through gaining such powers in an \u2018intelligence explosion\u2018 (self-improvement cycle). Either of those things may happen either through exceptional heights of intelligence being reached or through highly destructive ideas being available to minds only mildly beyond our own.</li><li><strong>Superhuman AI would gradually come to control the future via accruing power and resources.</strong> Power and resources would be more available to the AI system(s) than to humans on average, because of the AI having far greater intelligence.</li></ol><p>Below is a list of gaps in the above, as I see it, and counterarguments. A \u2018gap\u2019 is not necessarily unfillable, and may have been filled in any of the countless writings on this topic that I haven\u2019t read. I might even think that a given one can probably be filled. I just don\u2019t know what goes in it.&nbsp;&nbsp;</p><p>This blog post is an attempt to run various arguments by you all on the way to making pages on <a href=\"http://aiimpacts.org/\">AI Impacts</a> about arguments for AI risk and corresponding counterarguments. At some point in that process I hope to also read others\u2019 arguments, but this is not that day. So what you have here is a bunch of arguments that occur to me, not an exhaustive literature review.&nbsp;</p><h1><strong>Counterarguments</strong></h1><h2><i><strong>A. Contra \u201csuperhuman AI systems will be \u2018goal-directed\u2019\u201d</strong></i></h2><h3>Different calls to \u2018goal-directedness\u2019 don\u2019t necessarily mean the same concept</h3><p>\u2018Goal-directedness\u2019 is a vague concept. It is unclear that the \u2018goal-directednesses\u2019 that are favored by economic pressure, training dynamics or coherence arguments (the component arguments in part I of the argument above) are the same \u2018goal-directedness\u2019 that implies a zealous drive to control the universe (i.e. that makes most possible goals very bad, fulfilling II above).&nbsp;</p><p>One well-defined concept of goal-directedness is \u2018utility maximization\u2019: always doing what maximizes a particular utility function, given a particular set of beliefs about the world.&nbsp;</p><p>Utility maximization does seem to quickly engender an interest in controlling literally everything, at least for many utility functions one might have<a href=\"https://aiimpacts.org/counterarguments-to-the-basic-ai-x-risk-case/#easy-footnote-bottom-3-3345\"><sup>3</sup></a>. If you want things to go a certain way, then you have reason to control anything which gives you any leverage over that, i.e. potentially all resources in the universe (i.e. agents have \u2018<a href=\"https://en.wikipedia.org/wiki/Instrumental_convergence\">convergent instrumental goals</a>\u2019). This is in serious conflict with anyone else with resource-sensitive goals, even if <i>prima facie</i> those goals didn\u2019t look particularly opposed. For instance, a person who wants all things to be red and another person who wants all things to be cubes may not seem to be at odds, given that all things could be red cubes. However if these projects might each fail for lack of energy, then they are probably at odds.&nbsp;</p><p>Thus utility maximization is a notion of goal-directedness that allows Part II of the argument to work, by making a large class of goals deadly.</p><p>You might think that any other concept of \u2018goal-directedness\u2019 would also lead to this zealotry. If one is inclined toward outcome O in any plausible sense, then does one not have an interest in anything that might help procure O? No: if a system is not a \u2018coherent\u2019 agent, then it can have a tendency to bring about O in a range of circumstances, without this implying that it will take any given effective opportunity to pursue O. This assumption of consistent adherence to a particular evaluation of everything is part of utility maximization, not a law of physical systems. Call machines that push toward particular goals but are not utility maximizers pseudo-agents.&nbsp;</p><p>Can pseudo-agents exist? Yes\u2014utility maximization is computationally intractable, so any physically existent \u2018goal-directed\u2019 entity is going to be a pseudo-agent. We are all pseudo-agents, at best. But it seems something like a spectrum. At one end is a thermostat, then maybe a thermostat with a better algorithm for adjusting the heat. Then maybe a thermostat which intelligently controls the windows. After a lot of honing, you might have a system much more like a utility-maximizer: a system that deftly seeks out and seizes well-priced opportunities to make your room 68 degrees\u2014upgrading your house, buying R&amp;D, influencing your culture, building a vast mining empire. Humans might not be very far on this spectrum, but they seem enough like utility-maximizers already to be alarming. (And it might not be well-considered as a one-dimensional spectrum\u2014for instance, perhaps \u2018tendency to modify oneself to become more coherent\u2019 is a fairly different axis from \u2018consistency of evaluations of options and outcomes\u2019, and calling both \u2018more agentic\u2019 is obscuring.)</p><p>Nonetheless, it seems plausible that there is a large space of systems which strongly increase the chance of some desirable objective O occurring without even acting as much like maximizers of an identifiable utility function as humans would. For instance, without searching out novel ways of making O occur, or modifying themselves to be more consistently O-maximizing. Call these \u2018weak pseudo-agents\u2019.&nbsp;</p><p>For example, I can imagine a system constructed out of a huge number of \u2018IF X THEN Y\u2019 statements (reflexive responses), like \u2018if body is in hallway, move North\u2019, \u2018if hands are by legs and body is in kitchen, raise hands to waist\u2019.., equivalent to a kind of vector field of motions, such that for every particular state, there are directions that all the parts of you should be moving. I could imagine this being designed to fairly consistently&nbsp;cause O to happen within some context. However since such behavior would not be produced by a process optimizing O, you shouldn\u2019t expect it to find new and strange routes to O, or to seek O reliably in novel circumstances. There appears to be zero pressure for this thing to become more coherent, unless its design already involves reflexes to move its thoughts in certain ways that lead it to change itself. I expect you could build a system like this that reliably runs around and tidies your house say, or runs your social media presence, without it containing any impetus to become a more coherent agent (because it doesn\u2019t have any reflexes that lead to pondering self-improvement in this way).</p><p>It is not clear that economic incentives generally favor the far end of this spectrum over weak pseudo-agency. There are incentives toward systems being more like utility maximizers, but also incentives against.&nbsp;</p><p>The reason any kind of \u2018goal-directedness\u2019 is incentivised in AI systems is that then the system can be given an objective by someone hoping to use their cognitive labor, and the system will make that objective happen. Whereas a similar non-agentic AI system might still do almost the same cognitive labor, but require an agent (such as a person) to look at the objective and decide what should be done to achieve it, then ask the system for that. Goal-directedness means automating this high-level strategizing.&nbsp;</p><p>Weak pseudo-agency fulfills this purpose to some extent, but not as well as utility maximization. However if we think that utility maximization is difficult to wield without great destruction, then that suggests a disincentive to creating systems with behavior closer to utility-maximization. Not just from the world being destroyed, but from the same dynamic causing more minor divergences from expectations, if the user can\u2019t specify their own utility function well.&nbsp;</p><p>That is, if it is true that utility maximization tends to lead to very bad outcomes relative to any slightly different goals (in the absence of great advances in the field of AI alignment), then the most economically favored level of goal-directedness seems unlikely to be as far as possible toward utility maximization. More likely it is a level of pseudo-agency that achieves a lot of the users\u2019 desires without bringing about sufficiently detrimental side effects to make it not worthwhile. (This is likely more agency than is socially optimal, since some of the side-effects will be harms to others, but there seems no reason to think that it is a very high degree of agency.)</p><p>Some minor but perhaps illustrative evidence: anecdotally, people prefer interacting with others who predictably carry out their roles or adhere to deontological constraints, rather than consequentialists in pursuit of broadly good but somewhat unknown goals. For instance, employers would often prefer employees who predictably follow rules than ones who try to forward company success in unforeseen ways.</p><p>The other arguments to expect goal-directed systems mentioned above seem more likely to suggest approximate utility-maximization rather than some other form of goal-directedness, but it isn\u2019t that clear to me. I don\u2019t know what kind of entity is most naturally produced by contemporary ML training. Perhaps someone else does. I would guess that it\u2019s more like the reflex-based agent described above, at least at present. But present systems aren\u2019t the concern.</p><p>Coherence arguments are arguments for being coherent a.k.a. maximizing a utility function, so one might think that they imply a force for utility maximization in particular. That seems broadly right. Though note that these are arguments that there is some pressure for the system to modify itself to become more coherent. What actually results from specific systems modifying themselves seems like it might have details not foreseen in an abstract argument merely suggesting that the status quo is suboptimal whenever it is not coherent. Starting from a state of arbitrary incoherence and moving iteratively in one of many pro-coherence directions produced by whatever whacky mind you currently have isn\u2019t obviously guaranteed to increasingly approximate maximization of some sensical utility function. For instance, take an entity with a cycle of preferences, apples &gt; bananas = oranges &gt; pears &gt; apples. The entity notices that it sometimes treats oranges as better than pears and sometimes worse. It tries to correct by adjusting the value of oranges to be the same as pears. The new utility function is exactly as incoherent as the old one. Probably moves like this are rarer than ones that make you more coherent in this situation, but I don\u2019t know, and I also don\u2019t know if this is a great model of the situation for incoherent systems that could become more coherent.</p><p><i><strong>What it might look like if this gap matters: </strong>AI systems proliferate, and have various goals. Some AI systems try to make money in the stock market. Some make movies. Some try to direct traffic optimally. Some try to make the Democratic party win an election. Some try to make Walmart maximally profitable. These systems have no perceptible desire to optimize the universe for forwarding these goals because they aren\u2019t maximizing a general utility function, they are more \u2018behaving like someone who is trying to make Walmart profitable\u2019. They make strategic plans and think about their comparative advantage and forecast business dynamics, but they don\u2019t build nanotechnology to manipulate everybody\u2019s brains, because that\u2019s not the kind of behavior pattern they were designed to follow. The world looks kind of like the current world, in that it is fairly non-obvious what any entity\u2019s \u2018utility function\u2019 is. It often looks like AI systems are \u2018trying\u2019 to do things, but there\u2019s no reason to think that they are enacting a rational and consistent plan, and they rarely do anything shocking or galaxy-brained.</i></p><h3>Ambiguously strong forces for goal-directedness need to meet an ambiguously high bar to cause a risk</h3><p>The forces for goal-directedness mentioned in I are presumably of finite strength. For instance, if coherence arguments correspond to pressure for machines to become more like utility maximizers, there is an empirical answer to how fast that would happen with a given system. There is also an empirical answer to how \u2018much\u2019 goal directedness is needed to bring about disaster, supposing that utility maximization would bring about disaster and, say, being a rock wouldn\u2019t. Without investigating these empirical details, it is unclear whether a particular qualitatively identified force for goal-directedness will cause disaster within a particular time.</p><p><i><strong>What it might look like if this gap matters: </strong>There are not that many systems doing something like utility maximization in the new AI economy. Demand is mostly for systems more like GPT or DALL-E, which transform inputs in some known way without reference to the world, rather than \u2018trying\u2019 to bring about an outcome. Maybe the world was headed for more of the latter, but ethical and safety concerns reduced desire for it, and it wasn\u2019t that hard to do something else. Companies setting out to make non-agentic AI systems have no trouble doing so. Incoherent AIs are never observed making themselves more coherent, and training has never produced an agent unexpectedly. There are lots of vaguely agentic things, but they don\u2019t pose much of a problem. There are a few things at least as agentic as humans, but they are a small part of the economy.</i></p><h2><i><strong>B. Contra \u201cgoal-directed AI systems\u2019 goals will be bad\u201d</strong></i></h2><h3>Small differences in utility functions may not be catastrophic</h3><p>Arguably, humans are likely to have somewhat different values to one another even after arbitrary reflection. If so, there is some extended region of the space of possible values that the values of different humans fall within. That is, \u2018human values\u2019 is not a single point.</p><p>If the values of misaligned AI systems fall within that region, this would not appear to be worse in expectation than the situation where the long-run future was determined by the values of humans other than you. (This may still be a huge loss of value relative to the alternative, if a future determined by your own values is vastly better than that chosen by a different human, and if you also expected to get some small fraction of the future, and will now get much less. These conditions seem non-obvious however, and if they obtain you should worry about more general problems than AI.)</p><p>Plausibly even a single human, after reflecting, could on their own come to different places in a whole region of specific values, depending on somewhat arbitrary features of how the reflecting period went. In that case, even the values-on-reflection of a single human is an extended region of values space, and an AI which is only slightly misaligned could be the same as some version of you after reflecting.</p><p>There is a further larger region, \u2018that which can be reliably enough aligned with typical human values via incentives in the environment\u2019, which is arguably larger than the circle containing most human values. Human society makes use of this a lot: for instance, most of the time particularly evil humans don\u2019t do anything too objectionable because it isn\u2019t in their interests. This region is probably smaller for more capable creatures such as advanced AIs, but still it is some size.</p><p>Thus it seems that some amount of AI divergence from your own values is probably broadly fine, i.e. not worse than what you should otherwise expect without AI.&nbsp;</p><p>Thus in order to arrive at a conclusion of doom, it is not enough to argue that we cannot align AI perfectly. The question is a quantitative one of whether we can get it close enough. And how close is \u2018close enough\u2019 is not known.&nbsp;</p><p><i><strong>What it might look like if this gap matters: </strong>there are many superintelligent goal-directed AI systems around. They are trained to have human-like goals, but we know that their training is imperfect and none of them has goals exactly like those presented in training. However if you just heard about a particular system\u2019s intentions, you wouldn\u2019t be able to guess if it was an AI or a human. Things happen much faster than they were, because superintelligent AI is superintelligent, but not obviously in a direction less broadly in line with human goals than when humans were in charge.</i></p><h3>Differences between AI and human values may be small&nbsp;</h3><p>AI trained to have human-like goals will have something close to human-like goals. How close? Call it <i>d</i>, for a particular occasion of training AI.&nbsp;</p><p>If <i>d</i> doesn\u2019t have to be 0 for safety (from above), then there is a question of whether it is an acceptable size.&nbsp;</p><p>I know of two issues here, pushing <i>d</i> upward. One is that with a finite number of training examples, the fit between the true function and the learned function will be wrong. <a href=\"https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB/p/zthDPAjh9w6Ytbeks\">The other</a> is that you might accidentally create a monster (\u2018misaligned <a href=\"https://www.lesswrong.com/tag/mesa-optimization\">mesaoptimizer</a>\u2019) who understands its situation and pretends to have the utility function you are aiming for so that it can be freed and go out and manifest its own utility function, which could be just about anything. If this problem is real, then the values of an AI system might be arbitrarily different from the training values, rather than \u2018nearby\u2019 in some sense, so <i>d</i> is probably unacceptably large. But if you avoid creating such mesaoptimizers, then it seems plausible to me that <i>d</i> is very small.&nbsp;</p><p>If humans also substantially learn their values via observing examples, then the variation in human values is arising from a similar process, so might be expected to be of a similar scale. If we care to make the ML training process more accurate than the human learning one, it seems likely that we could. For instance, <i>d</i> gets smaller with more data.</p><p>Another line of evidence is that for things that I have seen AI learn so far, the distance from the real thing is intuitively small. If AI learns my values as well as it learns what faces look like, it seems plausible that it carries them out better than I do.</p><p>As minor additional evidence here, I don\u2019t know how to describe any slight differences in utility functions that are catastrophic. Talking concretely, what does a utility function look like that is so close to a human utility function that an AI system has it after a bunch of training, but which is an absolute disaster? Are we talking about the scenario where the AI values a slightly different concept of justice, or values satisfaction a smidgen more relative to joy than it should? And then that\u2019s a moral disaster because it is wrought across the cosmos? Or is it that it looks at all of our inaction and thinks we want stuff to be maintained very similar to how it is now, so crushes any efforts to improve things?&nbsp;</p><p><i><strong>What it might look like if this gap matters:</strong> when we try to train AI systems to care about what specific humans care about, they usually pretty much do, as far as we can tell. We basically get what we trained for. For instance, it is hard to distinguish them from the human in question. (It is still important to actually do this training, rather than making AI systems not trained to have human values.)</i></p><h3>Maybe value isn\u2019t fragile</h3><p>Eliezer argued that <a href=\"https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile\">value is fragile</a>, via examples of \u2018just one thing\u2019 that you can leave out of a utility function, and end up with something very far away from what humans want. For instance, if you leave out \u2018boredom\u2019 then he thinks the preferred future might look like repeating the same otherwise perfect moment again and again. (His argument is perhaps longer\u2014that post says there is a lot of important background, though the bits mentioned don\u2019t sound relevant to my disagreement.) This sounds to me like \u2018value is not resilient to having components of it moved to zero\u2019, which is a weird usage of \u2018fragile\u2019, and in particular, doesn\u2019t seem to imply much about smaller perturbations. And smaller perturbations seem like the relevant thing with AI systems trained on a bunch of data to mimic something.&nbsp;</p><p>You could very analogously say \u2018human faces are fragile\u2019 because if you just leave out the nose it suddenly doesn\u2019t look like a typical human face at all. Sure, but is that the kind of error you get when you try to train ML systems to mimic human faces? Almost none of the faces on <a href=\"http://thispersondoesnotexist.com\">thispersondoesnotexist.com</a> are blatantly morphologically unusual in any way, let alone noseless. Admittedly one time I saw someone whose face was neon green goo, but I\u2019m guessing you can get the rate of that down pretty low if you care about it.</p><p>Eight examples, no cherry-picking:</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1669763256/mirroredImages/zoWypGfXLmYsDFivk/dyvxq9dtoou3b8sd5plt.png\"></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1669763256/mirroredImages/zoWypGfXLmYsDFivk/hzrreyrycwg2dyy0d60o.png\"></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1669763254/mirroredImages/zoWypGfXLmYsDFivk/t5qzyno9c7p75fu1ky9f.png\"></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1669763256/mirroredImages/zoWypGfXLmYsDFivk/hw7czelsszuccuabnqxq.png\"></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1669763254/mirroredImages/zoWypGfXLmYsDFivk/qvutaiixkedsvdzd1rjl.png\"></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1669763255/mirroredImages/zoWypGfXLmYsDFivk/asm3be3ucdob6s0nmc4y.png\"></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1669763254/mirroredImages/zoWypGfXLmYsDFivk/qlou9hetvhg0hce0wzob.png\"></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1669763256/mirroredImages/zoWypGfXLmYsDFivk/hevagou9f9t132qnh9wz.png\"></p><p>Skipping the nose is the kind of mistake you make if you are a child drawing a face from memory. Skipping \u2018boredom\u2019 is the kind of mistake you make if you are a person trying to write down human values from memory. My guess is that this seemed closer to the plan in 2009 when that post was written, and that people cached the takeaway and haven\u2019t updated it for deep learning which can learn what faces look like better than you can.</p><p><i><strong>What it might look like if this gap matters:</strong> there is a large region \u2018around\u2019 my values in value space that is also pretty good according to me. AI easily lands within that space, and eventually creates some world that is about as good as the best possible utopia, according to me. There aren\u2019t a lot of really crazy and terrible value systems adjacent to my values.</i></p><h3>Short-term goals</h3><p>Utility maximization really only incentivises drastically altering the universe if one\u2019s utility function places a high enough value on very temporally distant outcomes relative to near ones. That is, long term goals are needed for danger. A person who cares most about winning the timed chess game in front of them should not spend time accruing resources to invest in better chess-playing.</p><p>AI systems could have long-term goals via people intentionally training them to do so, or via long-term goals naturally arising from systems not trained so.&nbsp;</p><p>Humans seem to discount the future a lot in their usual decision-making (they have goals years in advance but rarely a hundred years) so the economic incentive to train AI to have very long term goals might be limited.</p><p>It\u2019s not clear that training for relatively short term goals naturally produces creatures with very long term goals, though it might.</p><p>Thus if AI systems fail to have value systems relatively similar to human values, it is not clear that many will have the long time horizons needed to motivate taking over the universe.</p><p><i><strong>What it might look like if this gap matters:</strong> the world is full of agents who care about relatively near-term issues, and are helpful to that end, and have no incentive to make long-term large scale schemes. Reminiscent of the current world, but with cleverer short-termism.</i></p><h2><i><strong>C. Contra \u201csuperhuman AI would be sufficiently superior to humans to overpower humanity\u201d</strong></i></h2><h3>Human success isn\u2019t from individual intelligence</h3><p>The argument claims (or assumes) that surpassing \u2018human-level\u2019 intelligence (i.e. the mental capacities of an individual human) is the relevant bar for matching the power-gaining capacity of humans, such that passing this bar in individual intellect means outcompeting humans in general in terms of power (argument III.2), if not being able to immediately destroy them all outright (argument III.1.). In a similar vein, introductions to AI risk often start by saying that humanity has triumphed over the other species because it is more intelligent, as a lead in to saying that if we make something more intelligent still, it will inexorably triumph over humanity.</p><p>This hypothesis about the provenance of human triumph seems wrong. Intellect surely helps, but humans look to be powerful largely because they share their meager intellectual discoveries with one another and consequently save them up over time<a href=\"https://aiimpacts.org/counterarguments-to-the-basic-ai-x-risk-case/#easy-footnote-bottom-4-3345\"><sup>4</sup></a>. You can see this starkly by comparing the material situation of Alice, a genius living in the stone age, and Bob, an average person living in 21st Century America. Alice might struggle all day to get a pot of water, while Bob might be able to summon all manner of delicious drinks from across the oceans, along with furniture, electronics, information, etc. Much of Bob\u2019s power probably did flow from the application of intelligence, but not Bob\u2019s individual intelligence. Alice\u2019s intelligence, and that of those who came between them.</p><p>Bob\u2019s greater power isn\u2019t directly just from the knowledge and artifacts Bob inherits from other humans. He also seems to be helped for instance by much better coordination: both from a larger number people coordinating together, and from better infrastructure for that coordination (e.g. for Alice the height of coordination might be an occasional big multi-tribe meeting with trade, and for Bob it includes global instant messaging and banking systems and the Internet). One might attribute all of this ultimately to innovation, and thus to intelligence and communication, or not. I think it\u2019s not important to sort out here, as long as it\u2019s clear that individual intelligence isn\u2019t the source of power.</p><p>It could still be that with a given bounty of shared knowledge (e.g. within a given society), intelligence grants huge advantages. But even that doesn\u2019t look true here: 21st Century geniuses live basically like 21st Century people of average intelligence, give or take.</p><p>Why does this matter? Well for one thing, if you make AI which is merely as smart as a human, you shouldn\u2019t then expect it to do that much better than a genius living in the stone age. That\u2019s what human-level intelligence gets you: nearly nothing. <a href=\"https://meteuphoric.com/2017/12/28/why-did-everything-take-so-long/\">A piece of rope</a> after millions of lifetimes. Humans without their culture are much like other animals.&nbsp;</p><p>To wield the control-over-the-world of a genius living in the 21st Century, the human-level AI would seem to need something like the other benefits that the 21st century genius gets from their situation in connection with a society.&nbsp;</p><p>One such thing is access to humanity\u2019s shared stock of hard-won information. AI systems plausibly do have this, if they can get most of what is relevant by reading the internet. This isn\u2019t obvious: people also inherit information from society through copying habits and customs, learning directly from other people, and receiving artifacts with implicit information (for instance, a factory allows whoever owns the factory to make use of intellectual work that was done by the people who built the factory, but that information may not available explicitly even for the owner of the factory, let alone to readers on the internet). These sources of information seem likely to also be available to AI systems though, at least if they are afforded the same options as humans.</p><p>My best guess is that AI systems easily do better than humans on extracting information from humanity\u2019s stockpile, and on coordinating, and so on this account are probably in an even better position to compete with humans than one might think on the individual intelligence model, but that is a guess. In that case perhaps this misunderstanding makes little difference to the outcomes of the argument. However it seems at least a bit more complicated.&nbsp;</p><p>Suppose that AI systems can have access to all information humans can have access to. The power the 21st century person gains from their society is modulated by their role in society, and relationships, and rights, and the affordances society allows them as a result. Their power will vary enormously depending on whether they are employed, or listened to, or paid, or a citizen, or the president. If AI systems\u2019 power stems substantially from interacting with society, then their power will also depend on affordances granted, and humans may choose not to grant them many affordances (see section \u2018Intelligence may not be an overwhelming advantage\u2019 for more discussion).</p><p>However suppose that your new genius AI system is also treated with all privilege. The next way that this alternate model matters is that if most of what is good in a person\u2019s life is determined by the society they are part of, and their own labor is just buying them a tiny piece of that inheritance, then if they are for instance twice as smart as any other human, they don\u2019t get to use technology that it twice as good. They just get a larger piece of that same shared technological bounty purchasable by anyone. Because each individual person is adding essentially nothing in terms of technology, so twice that is still basically nothing.&nbsp;</p><p>In contrast, I think people are often imagining that a single entity somewhat smarter than a human will be able to quickly use technologies that are somewhat better than current human technologies. This seems to be mistaking the actions of a human and the actions of a human society. If <a href=\"https://en.wikipedia.org/wiki/Manhattan_Project#Personnel\">a hundred thousand people sometimes get together for a few years and make fantastic new weapons</a>, you should not expect an entity somewhat smarter than a person to make even better weapons. That\u2019s off by a factor of about a hundred thousand.&nbsp;</p><p>There might be places you can get far ahead of humanity by being better than a single human\u2014it depends how much accomplishments depend on the few most capable humans in the field, and how few people are working on the problem. But for instance the Manhattan Project <a href=\"https://en.wikipedia.org/wiki/Manhattan_Project#Personnel\">took</a> a hundred thousand people several years, and von Neumann (a mythically smart scientist) joining the project did not reduce it to an afternoon. Plausibly to me, some specific people being on the project caused it to not take twice as many person-years, though the plausible candidates here seem to be more in the business of running things than doing science directly (though that also presumably involves intelligence). But even if you are an ambitious somewhat superhuman intelligence, the influence available to you seems to plausibly be limited to making a large dent in the effort required for some particular research endeavor, not single-handedly outmoding humans across many research endeavors.</p><p>This is all reason to doubt that a small number of superhuman intelligences will rapidly take over or destroy the world (as in III.i.). This doesn\u2019t preclude a set of AI systems that are together more capable than a large number of people from making great progress. However some related issues seem to make that less likely.</p><p>Another implication of this model is that if most human power comes from buying access to society\u2019s shared power, i.e. interacting with the economy, you should expect intellectual labor by AI systems to usually be sold, rather than for instance put toward a private stock of knowledge. This means the intellectual outputs are mostly going to society, and the main source of potential power to an AI system is the wages received (which may allow it to gain power in the long run). However it seems quite plausible that AI systems at this stage will generally not receive wages, since they presumably do not need them to be motivated to do the work they were trained for. It also seems plausible that they would be owned and run by humans. This would seem to not involve any transfer of power to that AI system, except insofar as its intellectual outputs benefit it (e.g. if it is writing advertising material, maybe it doesn\u2019t get paid for that, but if it can write material that slightly furthers its own goals in the world while also fulfilling the advertising requirements, then it sneaked in some influence.)&nbsp;</p><p>If there is AI which is moderately more competent than humans, but not sufficiently more competent to take over the world, then it is likely to contribute to this stock of knowledge and affordances shared with humans. There is no reason to expect it to build a separate competing stock, any more than there is reason for a current human household to try to build a separate competing stock rather than sell their labor to others in the economy.&nbsp;</p><p>In summary:</p><ol><li>Functional connection with a large community of other intelligences in the past and present is probably a much bigger factor in the success of humans as a species or individual humans than is individual intelligence.&nbsp;</li><li>Thus this also seems more likely to be important for AI success than individual intelligence. This is contrary to a usual argument for AI superiority, but probably leaves AI systems at least as likely to outperform humans, since superhuman AI is probably superhumanly good at taking in information and coordinating.</li><li>However it is not obvious that AI systems will have the same access to society\u2019s accumulated information e.g. if there is information which humans learn from living in society, rather than from reading the internet.&nbsp;</li><li>And it seems an open question whether AI systems are given the same affordances in society as humans, which also seem important to making use of the accrued bounty of power over the world that humans have. For instance, if they are not granted the same legal rights as humans, they may be at a disadvantage in doing trade or engaging in politics or accruing power.</li><li>The fruits of greater intelligence for an entity will probably not look like society-level accomplishments unless it is a society-scale entity</li><li>The route to influence with smaller fruits probably by default looks like participating in the economy rather than trying to build a private stock of knowledge.</li><li>If the resources from participating in the economy accrue to the owners of AI systems, not to the systems themselves, then there is less reason to expect the systems to accrue power incrementally, and they are at a severe disadvantage relative to humans.&nbsp;</li></ol><p>Overall these are reasons to expect AI systems with around human-level cognitive performance to not destroy the world immediately, and to not amass power as easily as one might imagine.&nbsp;</p><p><i><strong>What it might look like if this gap matters:</strong> If AI systems are somewhat superhuman, then they do impressive cognitive work, and each contributes to technology more than the best human geniuses, but not more than the whole of society, and not enough to materially improve their own affordances. They don\u2019t gain power rapidly because they are disadvantaged in other ways, e.g. by lack of information, lack of rights, lack of access to positions of power. Their work is sold and used by many actors, and the proceeds go to their human owners. AI systems do not generally end up with access to masses of technology that others do not have access to, and nor do they have private fortunes. In the long run, as they become more powerful, they might take power if other aspects of the situation don\u2019t change.&nbsp;</i></p><h3>AI agents may not be radically superior to combinations of humans and non-agentic machines</h3><p>\u2018Human level capability\u2019 is a moving target. For comparing the competence of advanced AI systems to humans, the relevant comparison is with humans who have state-of-the-art AI and other tools. For instance, the human capacity to make art quickly has recently been improved by a variety of AI art systems. If there were now an agentic AI system that made art, it would make art much faster than a human of 2015, but perhaps hardly faster than a human of late 2022. If humans continually have access to tool versions of AI capabilities, it is not clear that agentic AI systems must ever have an overwhelmingly large capability advantage for important tasks (though they might).&nbsp;</p><p>(This is not an argument that humans might be better than AI systems, but rather: if the gap in capability is smaller, then the pressure for AI systems to accrue power is less and thus loss of human control is slower and easier to mitigate entirely through other forces, such as subsidizing human involvement or disadvantaging AI systems in the economy.)</p><p>Some advantages of being an agentic AI system vs. a human with a tool AI system seem to be:</p><ol><li>There might just not be an equivalent tool system, for instance if it is impossible to train systems without producing emergent agents.</li><li>When every part of a process takes into account the final goal, this should make the choices within the task more apt for the final goal (and agents know their final goal, whereas tools carrying out parts of a larger problem do not).</li><li>For humans, the interface for using a capability of one\u2019s mind tends to be smoother than the interface for using a tool. For instance a person who can do fast mental multiplication can do this more smoothly and use it more often than a person who needs to get out a calculator. This seems likely to persist.</li></ol><p>1 and 2 may or may not matter much. 3 matters more for brief, fast, unimportant tasks. For instance, consider again people who can do mental calculations better than others. My guess is that this advantages them at using Fermi estimates in their lives and buying cheaper groceries, but does not make them materially better at making large financial choices well. For a one-off large financial choice, the effort of getting out a calculator is worth it and the delay is very short compared to the length of the activity. The same seems likely true of humans with tools vs. agentic AI with the same capacities integrated into their minds. Conceivably the gap between humans with tools and goal-directed AI is small for large, important tasks.</p><p><i><strong>What it might look like if this gap matters:</strong> agentic AI systems have substantial advantages over humans with tools at some tasks like rapid interaction with humans, and responding to rapidly evolving strategic situations.&nbsp; One-off large important tasks such as advanced science are mostly done by tool AI.&nbsp;</i></p><h3>Trust</h3><p>If goal-directed AI systems are only mildly more competent than some combination of tool systems and humans (as suggested by considerations in the last two sections), we still might expect AI systems to out-compete humans, just more slowly. However AI systems have one serious disadvantage as employees of humans: they are intrinsically untrustworthy, while we don\u2019t understand them well enough to be clear on what their values are or how they will behave in any given case. Even if they did perform as well as humans at some task, if humans can\u2019t be certain of that, then there is reason to disprefer using them. This can be thought of as two problems: firstly, slightly misaligned systems are less valuable because they genuinely do the thing you want less well, and secondly, even if they were not misaligned, if humans can\u2019t know that (because we have no good way to verify the alignment of AI systems) then it is costly in expectation to use them. (This is only a further force acting against the supremacy of AI systems\u2014they might still be powerful enough that using them is enough of an advantage that it is worth taking the hit on trustworthiness.)</p><p><i><strong>What it might look like if this gap matters: </strong>in places where goal-directed AI systems are not typically hugely better than some combination of less goal-directed systems and humans, the job is often given to the latter if trustworthiness matters.&nbsp;</i></p><h3>Headroom</h3><p>For AI to vastly surpass human performance at a task, there needs to be ample room for improvement above human level. For some tasks, there is not\u2014tic-tac-toe is a classic example. It is not clear how close humans (or technologically aided humans) are from the limits to competence in the particular domains that will matter. It is to my knowledge an open question how much \u2018headroom\u2019 there is. My guess is a lot, but it isn\u2019t obvious.</p><p>How much headroom there is varies by task. Categories of task for which there appears to be little headroom:&nbsp;</p><ol><li>Tasks where we know what the best performance looks like, and humans can get close to it. For instance, machines cannot win more often than the best humans at Tic-tac-toe (playing within the rules) or solve Rubik\u2019s cubes much more reliably, or extracting calories from fuel</li><li>Tasks where humans are already be reaping most of the value\u2014for instance, perhaps most of the value of forks is in having a handle with prongs attached to the end, and while humans continue to design slightly better ones, and machines might be able to add marginal value to that project more than twice as fast as the human designers, they cannot perform twice as well in terms of the value of each fork, because forks are already 95% as good as they can be.&nbsp;</li><li>Better performance is quickly intractable. For instance, we know that for tasks in particular complexity classes, there are computational limits to how well one can perform across the board. Or for chaotic systems, there can be limits to predictability. (That is, tasks might lack headroom not because they are simple, but because they are complex. E.g. AI probably can\u2019t predict the weather much further out than humans.)</li></ol><p>Categories of task where a lot of headroom seems likely:</p><ol><li>Competitive tasks where the value of a certain level of performance depends on whether one is better or worse than one\u2019s opponent, so that the marginal value of more performance doesn\u2019t hit diminishing returns, as long as your opponent keeps competing and taking back what you just won. Though in one way this is like having little headroom: there\u2019s no more value to be had\u2014the game is zero sum. And while there might often be a lot of value to be gained by doing a bit better on the margin, still if all sides can invest, then nobody will end up better off than they were. So whether this seems more like high or low headroom depends on what we are asking exactly. Here we are asking if AI systems can do much better than humans: in a zero sum contest like this, they likely can in the sense that they can beat humans, but not in the sense of reaping anything more from the situation than the humans ever got.</li><li>Tasks where it is twice as good to do the same task twice as fast, and where speed is bottlenecked on thinking time.</li><li>Tasks where there is reason to think that optimal performance is radically better than we have seen. For instance, perhaps we can estimate how high Chess Elo rankings must go before reaching perfection by reasoning theoretically about the game, and perhaps it is very high (I don\u2019t know).</li><li>Tasks where humans appear to use very inefficient methods. For instance, it was perhaps predictable before calculators that they would be able to do mathematics much faster than humans, because humans can only keep a small number of digits in their heads, which doesn\u2019t seem like an intrinsically hard problem. Similarly, I hear humans often use mental machinery designed for one mental activity for fairly different ones, through analogy. For instance, when I think about macroeconomics, I seem to be basically using my intuitions for dealing with water. When I do mathematics in general, I think I\u2019m probably using my mental capacities for imagining physical objects.</li></ol><p><i><strong>What it might look like if this gap matters: </strong>many challenges in today\u2019s world remain challenging for AI. Human behavior is not readily predictable or manipulable very far beyond what we have explored, only slightly more complicated schemes are feasible before the world\u2019s uncertainties overwhelm planning; much better ads are soon met by much better immune responses; much better commercial decision-making ekes out some additional value across the board but most products were already fulfilling a lot of their potential; incredible virtual prosecutors meet incredible virtual defense attorneys and everything is as it was; there are a few rounds of attack-and-defense in various corporate strategies before a new equilibrium with broad recognition of those possibilities; conflicts and \u2018social issues\u2019 remain mostly intractable. There is a brief golden age of science before the newly low-hanging fruit are again plucked and it is only lightning fast in areas where thinking was the main bottleneck, e.g. not in medicine.</i></p><h3>Intelligence may not be an overwhelming advantage</h3><p>Intelligence is helpful for accruing power and resources, all things equal, but many other things are helpful too. For instance money, social standing, allies, evident trustworthiness, not being discriminated against (this was slightly discussed in section \u2018Human success isn\u2019t from individual intelligence\u2019). AI systems are not guaranteed to have those in abundance. The argument assumes that any difference in intelligence in particular will eventually win out over any differences in other initial resources. I don\u2019t know of reason to think that.&nbsp;</p><p>Empirical evidence does not seem to support the idea that cognitive ability is a large factor in success. Situations where one entity is much smarter or more broadly mentally competent than other entities regularly occur without the smarter one taking control over the other:</p><ol><li>Species exist with all levels of intelligence. Elephants have not in any sense won over gnats; they do not rule gnats; they do not have obviously more control than gnats over the environment.&nbsp;</li><li>Competence does not seem to aggressively overwhelm other advantages in humans:&nbsp;<ol><li>Looking at the world, intuitively the big discrepancies in power are not seemingly about intelligence.</li><li>IQ 130 humans <a href=\"https://www.newscientist.com/article/dn11711-smarter-people-are-no-better-off/\">apparently</a> earn very roughly $6000-$18,500 per year more than average IQ humans.</li><li>Elected representatives are apparently smarter on average, but it is a slightly shifted curve, <a href=\"http://perseus.iies.su.se/~tpers/papers/Draft170103.pdf\">not a radically difference</a>.</li><li>MENSA isn\u2019t a major force in the world.</li><li>Many places where people see huge success through being cognitively able are ones where they show off their intelligence to impress people, rather than actually using it for decision-making. For instance, writers, actors, song-writers, comedians, all sometimes become very successful through cognitive skills. Whereas scientists, engineers and authors of software use cognitive skills to make choices about the world, and less often become extremely rich and famous, say. If intelligence were that useful for strategic action, it seems like using it for that would be at least as powerful as showing it off. But maybe this is just an accident of which fields have winner-takes-all type dynamics.</li><li>If we look at people who evidently have good cognitive abilities given their intellectual output, their personal lives are not obviously drastically more successful, anecdotally.</li><li>One might counter-counter-argue that humans are very similar to one another in capability, so even if intelligence matters much more than other traits, you won\u2019t see that by looking at&nbsp; the near-identical humans. This does not seem to be true. Often at least, the difference in performance between mediocre human performance and top level human performance is <a href=\"https://aiimpacts.org/category/speed-of-ai-transition/range-of-human-performance/\">large</a>, relative to the space below, iirc. For instance, in chess, the Elo difference between the best and worst players is about 2000, whereas the difference between the amateur play and random play is <a href=\"https://chess.stackexchange.com/questions/6508/what-would-be-the-elo-of-a-computer-program-that-plays-at-random\">maybe 400-2800 (if you accept Chess StackExchange guesses as a reasonable proxy for the truth here)</a>. And <a href=\"https://aiimpacts.org/time-for-ai-to-cross-the-human-performance-range-in-chess/\">in terms of AI progress</a>, amateur human play was reached in the 50s, roughly when research began, and world champion level play was reached in 1997.&nbsp;</li></ol></li></ol><p>And theoretically I don\u2019t know why one would expect greater intelligence to win out over other advantages over time.&nbsp; There are actually two questionable theories here: 1) Charlotte having more overall control than David at time 0 means that Charlotte will tend to have an even greater share of control at time 1. And, 2) Charlotte having more <i>intelligence</i> than David at time 0 means that Charlotte will have a greater share of control at time 1 even if Bob has more overall control (i.e. more of other resources) at time 1.</p><p><i><strong>What it might look like if this gap matters:</strong> there are many AI systems around, and they strive for various things. They don\u2019t hold property, or vote, or get a weight in almost anyone\u2019s decisions, or get paid, and are generally treated with suspicion. These things on net keep them from gaining very much power. They are very persuasive speakers however and we can\u2019t stop them from communicating, so there is a constant risk of people willingly handing them power, in response to their moving claims that they are an oppressed minority who suffer. The main thing stopping them from winning is that their position as psychopaths bent on taking power for incredibly pointless ends is widely understood.</i></p><h3>Unclear that many goals realistically incentivise taking over the universe</h3><p>I have some goals. For instance, I want some good romance. My guess is that trying to take over the universe isn\u2019t the best way to achieve this goal. The same goes for a lot of my goals, it seems to me. Possibly I\u2019m in error, but I spend a lot of time pursuing goals, and very little of it trying to take over the universe. Whether a particular goal is best forwarded by trying to take over the universe as a substep seems like a quantitative empirical question, to which the answer is virtually always \u2018not remotely\u2019. Don\u2019t get me wrong: all of these goals involve some interest in taking over the universe. All things equal, if I could take over the universe for free, I do think it would help in my romantic pursuits. But taking over the universe is not free. It\u2019s actually super duper duper expensive and hard. So for most goals arising, it doesn\u2019t bear considering. The idea of taking over the universe as a substep is entirely laughable for almost any human goal.</p><p>So why do we think that AI goals are different? I think the thought is that it\u2019s radically easier for AI systems to take over the world, because all they have to do is to annihilate humanity, and they are way better positioned to do that than I am, and also better positioned to survive the death of human civilization than I am. I agree that it is likely easier, but how much easier? So much easier to take it from \u2018laughably unhelpful\u2019 to \u2018obviously always the best move\u2019? This is another quantitative empirical question.</p><p><i><strong>What it might look like if this gap matters: </strong>Superintelligent AI systems pursue their goals. Often they achieve them fairly well. This is somewhat contrary to ideal human thriving, but not lethal. For instance, some AI systems are trying to maximize Amazon\u2019s market share, within broad legality. Everyone buys truly incredible amounts of stuff from Amazon, and people often wonder if it is too much stuff. At no point does attempting to murder all humans seem like the best strategy for this.&nbsp;</i></p><h3>Quantity of new cognitive labor is an empirical question, not addressed</h3><p>Whether some set of AI systems can take over the world with their new intelligence probably depends how much total cognitive labor they represent. For instance, if they are in total slightly more capable than von Neumann, they probably can\u2019t take over the world. If they are together as capable (in some sense) as a million 21st Century human civilizations, then they probably can (at least in the 21st Century).</p><p>It also matters how much of that is goal-directed at all, and highly intelligent, and how much of that is directed at achieving the AI systems\u2019 own goals rather than those we intended them for, and how much of that is directed at taking over the world.&nbsp;</p><p>If we continued to build hardware, presumably at some point AI systems would account for most of the cognitive labor in the world. But if there is first an extended period of more minimal advanced AI presence, that would probably prevent an immediate death outcome, and improve humanity\u2019s prospects for controlling a slow-moving AI power grab.&nbsp;</p><p><i><strong>What it might look like if this gap matters: </strong>when advanced AI is developed, there is a lot of new cognitive labor in the world, but it is a minuscule fraction of all of the cognitive labor in the world. A large part of it is not goal-directed at all, and of that, most of the new AI thought is applied to tasks it was intended for. Thus what part of it is spent on scheming to grab power for AI systems is too small to grab much power quickly. The amount of AI cognitive labor grows fast over time, and in several decades it is most of the cognitive labor, but humanity has had extensive experience dealing with its power grabbing.</i></p><h3>Speed of intelligence growth is ambiguous</h3><p>The idea that a superhuman AI would be able to rapidly destroy the world seems <i>prima facie</i> unlikely, since no other entity has ever done that. Two common broad arguments for it:</p><ol><li>There will be a feedback loop in which intelligent AI makes more intelligent AI repeatedly until AI is very intelligent.</li><li>Very small differences in brains seem to correspond to very large differences in performance, based on observing humans and other apes. Thus any movement past human-level will take us to unimaginably superhuman level.</li></ol><p>These both seem questionable.</p><ol><li>Feedback loops can happen at very different rates. Identifying a feedback loop empirically does not signify an explosion of whatever you are looking at. For instance, technology is already helping improve technology. To get to a confident conclusion of doom, you need evidence that the feedback loop is fast.</li><li>It does not seem clear that small improvements in brains lead to large changes in intelligence in general, or will do on the relevant margin. Small differences between humans and other primates might include those helpful for communication (see Section \u2018Human success isn\u2019t from individual intelligence\u2019), which do not seem relevant here. If there were a particularly powerful cognitive development between chimps and humans, it is unclear that AI researchers find that same insight at the same point in the process (rather than at some other time).&nbsp;</li></ol><p>A large number of other arguments have been posed for expecting very fast growth in intelligence at around human level. I previously made <a href=\"https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/\">a list of them with counterarguments</a>, though none seemed very compelling. Overall, I don\u2019t know of strong reason to expect very fast growth in AI capabilities at around human-level AI performance, though I hear such arguments might exist.&nbsp;</p><p><i><strong>What it would look like if this gap mattered: </strong>AI systems would at some point perform at around human level at various tasks, and would contribute to AI research, along with everything else. This would contribute to progress to an extent familiar from other technological progress feedback, and would not e.g. lead to a superintelligent AI system in minutes.</i></p><h3>Key concepts are vague</h3><p>Concepts such as \u2018control\u2019, \u2018power\u2019, and \u2018alignment with human values\u2019 all seem vague. \u2018Control\u2019 is not zero sum (as seemingly assumed) and is somewhat hard to pin down, I claim. What an \u2018aligned\u2019 entity is exactly seems to be contentious in the AI safety community, but I don\u2019t know the details. My guess is that upon further probing, these conceptual issues are resolvable in a way that doesn\u2019t endanger the argument, but I don\u2019t know. I\u2019m not going to go into this here.</p><p><i><strong>What it might look like if this gap matters: </strong>upon thinking more, we realize that our concerns were confused. Things go fine with AI in ways that seem obvious in retrospect. This might look like it did for people concerned about the \u2018population bomb\u2019 or as it did for me in some of my youthful concerns about sustainability: there was a compelling abstract argument for a problem, and the reality didn\u2019t fit the abstractions well enough to play out as predicted.</i></p><h2><strong>D. Contra the whole argument</strong></h2><h3>The argument overall proves too much about corporations</h3><p>Here is the argument again, but modified to be about corporations. A couple of pieces don\u2019t carry over, but they don\u2019t seem integral.</p><p><strong>I. Any given corporation is likely to be \u2018goal-directed\u2019</strong></p><p>Reasons to expect this:</p><ol><li>Goal-directed behavior is likely to be valuable in corporations, e.g. economically</li><li><s>Goal-directed entities may tend to arise from machine learning training processes not intending to create them (at least via the methods that are likely to be used).</s></li><li>\u2018Coherence arguments\u2019 may imply that systems with some goal-directedness will become more strongly goal-directed over time.</li></ol><p><strong>II. If goal-directed superhuman corporations are built, their desired outcomes will probably be about as bad as an empty universe by human lights</strong></p><p>Reasons to expect this:</p><ol><li>Finding useful goals that aren\u2019t extinction-level bad appears to be hard: we don\u2019t have a way to usefully point at human goals, and divergences from human goals seem likely to produce goals that are in intense conflict with human goals, due to a) most goals producing convergent incentives for controlling everything, and b) value being \u2018fragile\u2019, such that an entity with \u2018similar\u2019 values will generally create a future of virtually no value.&nbsp;</li><li>Finding goals that are extinction-level bad and temporarily useful appears to be easy: for example, corporations with the sole objective \u2018maximize company revenue\u2019 might profit for a time before gathering the influence and wherewithal to pursue the goal in ways that blatantly harm society.</li><li>Even if humanity found acceptable goals, giving a corporation any specific goals appears to be hard. We don\u2019t know of any procedure to do it<s>, and we have theoretical reasons to expect that AI systems produced through machine learning training will generally end up with goals other than those that they were trained according to</s>. Randomly aberrant goals resulting are probably extinction-level bad, for reasons described in II.1 above.<br>&nbsp;</li></ol><p><strong>III. If most goal-directed corporations have bad goals, the future will very likely be bad</strong></p><p>That is, a set of ill-motivated goal-directed corporations, of a scale likely to occur, would be capable of taking control of the future from humans. This is supported by at least one of the following being true:</p><ol><li><strong>A corporation would destroy humanity rapidly</strong>. This may be via ultra-powerful capabilities at e.g. technology design and strategic scheming, or through gaining such powers in an \u2018intelligence explosion\u2018 (self-improvement cycle). Either of those things may happen either through exceptional heights of intelligence being reached or through highly destructive ideas being available to minds only mildly beyond our own.</li><li><strong>Superhuman AI would gradually come to control the future via accruing power and resources.</strong> Power and resources would be more available to the corporation than to humans on average, because of the corporation having far greater intelligence.</li></ol><p>This argument does point at real issues with corporations, but we do not generally consider such issues existentially deadly.&nbsp;</p><p>One might argue that there are defeating reasons that corporations do not destroy the world: they are made of humans so can be somewhat reined in; they are not smart enough; they are not coherent enough. But in that case, the original argument needs to make reference to these things, so that they apply to one and not the other.</p><p><i><strong>What it might look like if this counterargument matters: </strong>something like the current world. There are large and powerful systems doing things vastly beyond the ability of individual humans, and acting in a definitively goal-directed way. We have a vague understanding of their goals, and do not assume that they are coherent. Their goals are clearly not aligned with human goals, but they have enough overlap that many people are broadly in favor of their existence. They seek power. This all causes some problems, but problems within the power of humans and other organized human groups to keep under control, for some definition of \u2018under control\u2019.</i></p><h1><strong>Conclusion</strong></h1><p>I think there are quite a few gaps in the argument, as I understand it. My current guess (prior to reviewing other arguments and integrating things carefully) is that enough uncertainties might resolve in the dangerous directions that existential risk from AI is a reasonable concern. I don\u2019t at present though see how one would come to think it was overwhelmingly likely.</p>", "user": {"username": "Katja_Grace"}}, {"_id": "q5kazebaj5Gp96c5k", "title": "Is there a UK charitable investment vehicle that I could invest into and then later use to invest in a startup I make in the future?", "postedAt": "2022-10-14T14:53:15.589Z", "htmlBody": "<p>Hi,</p><p>I'm a student at Cambridge, UK and am thinking of making a startup at some point in the future, however before then I'd like to work in consultancy. A startup would require considerable capital, however I aren't sure if I would definitely do it. If I did do a startup and it worked I'd give the vast majority of the earnings to charity.&nbsp;</p><p>However, once I start my job I'd like to donate half my money, potentially into a donor advised fund, so I can invest it and work out exactly where to donate the money. However, I would like to then have the option of using the money in the donor advised fund to help finance a future startup, by issuing the donor advised fund shares or a loan. Is there any way of doing this, or would I have to start a charitable foundation? Or will there be a charity who could do it on behalf of me? Since I'm in the UK I imagine the only way of doing this would be in some kind of UK vehicle, though I wouldn't be adverse to using a US vehicle if it were the best option.</p><p>I appreciate that this will look very dodgy to any regulators, as it would seem like I'm taking charitable donations and using it for personal benefit in my own company. However, the idea would be solely to save on tax as its better to donate the money immediately to the donor advised fund than keep it just in case I happen to make a startup. Any profits from the startup would be donated and I'd take home probably only 20-40k (more if I had family) a year regardless of how well it does.</p><p>I'd love some advice on this from someone who knows about theses things.</p><p>Many thanks</p><p>Olly</p>", "user": {"username": "Olly P"}}, {"_id": "Wij8RgxBtsWHeaubw", "title": "If you could 2x the number of future humans by reducing the QALYs per person by half, would you choose to do it? Why or why not?", "postedAt": "2022-10-14T14:06:49.622Z", "htmlBody": "", "user": {"username": "Parmest Roy"}}, {"_id": "GsNF8D24csTwRZumq", "title": "EA Organization Updates: October 2022", "postedAt": "2022-10-14T13:36:06.610Z", "htmlBody": "<p><i>These monthly posts originated as the \"Updates\" section of the&nbsp;</i><a href=\"https://forum.effectivealtruism.org/topics/effective-altruism-newsletter\"><i>EA Newsletter</i></a><i>. Organizations submit their own updates, which we edit for clarity.</i></p><p><strong>Job listings</strong> that these organizations highlighted (as well as a couple of other impactful jobs) are at the top of this post. Some of the jobs have extremely pressing deadlines.&nbsp;</p><p>You can see previous updates on the&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/ea-organization-updates-monthly-series\">\"EA Organization Updates (monthly series)\"</a> topic page, or in our<a href=\"https://www.effectivealtruism.org/ea-newsletter-archives/\"> repository of past newsletters</a>. Notice that there\u2019s also an&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/org-update\">\u201corg update\u201d tag</a>, where you can find more news and updates that are not part of this consolidated series.</p><p>The organizations are in alphabetical order, starting with R this month.</p><h1>Job listings</h1><p><i>Consider also exploring jobs listed on \u201c</i><a href=\"https://forum.effectivealtruism.org/topics/job-listing-open\"><i><u>Job listing (open)</u></i></a><i>.\u201d</i></p><p><strong>Centre for the Study of Existential Risk</strong></p><ul><li><a href=\"https://www.cser.ac.uk/about-us/careers/director-fixed-term/\"><u>Director</u></a> (Cambridge, apply by&nbsp;<strong>16 October</strong>)</li><li><a href=\"https://www.cser.ac.uk/about-us/careers/research-associate-risks-global-systems-fixed-term/\"><u>Research Associate in Risks from Global Systems</u></a> (Cambridge, apply by&nbsp;<strong>16 October</strong>)</li></ul><p><strong>Effective Ventures Operations</strong></p><ul><li><a href=\"https://ev.org/ops/position/immigration-specialist/\"><u>Immigration Specialist</u></a> (Remote / Oxford)</li><li><a href=\"https://ev.org/ops/position/operations-associate-salesforce-admin/\"><u>Operations Associate - Salesforce Admin</u></a> (Remote / Oxford)</li><li><a href=\"https://ev.org/ops/position/1522/\"><u>Operations Associate - Intercom Contractor</u></a> (Remote / Oxford)</li><li><a href=\"https://ev.org/ops/position/senior-bookkeeper-accountant/\"><u>Senior Bookkeeper / Accountant</u></a> (Remote / Oxford)</li><li><a href=\"https://ev.org/ops/position/finance-associate/\"><u>Finance Associate</u></a> (Remote / Oxford)</li></ul><p><strong>GiveDirectly&nbsp;</strong></p><ul><li><a href=\"https://boards.greenhouse.io/givedirectly/jobs/4096973005\"><u>Executive Director</u></a> (Remote, Africa-based)</li><li><a href=\"https://boards.greenhouse.io/givedirectly/jobs/4085024005\"><u>CTO</u></a> (Remote)</li><li><a href=\"https://boards.greenhouse.io/givedirectly/jobs/4084082005\"><u>Director of Major Giving</u></a> (Remote)</li></ul><p><strong>GiveWell</strong></p><ul><li><a href=\"https://www.givewell.org/about/jobs/senior-researcher\"><u>Senior Researchers</u></a> and&nbsp;<a href=\"https://www.givewell.org/about/jobs/senior-research-associate\"><u>Senior Research Associates</u></a> (Remote / Oakland, CA)</li><li><a href=\"https://www.givewell.org/about/jobs/content-editor\"><u>Content Editors</u></a> (Remote / Oakland, CA)</li></ul><p><strong>Nuclear Acid Observatory at MIT Media Lab</strong> is hiring for a <a href=\"https://www.linkedin.com/jobs/view/3262014292/\">research scientist </a>(Boston, MA)</p><p><strong>Nuclear Threat Initiative (NTI) | bio</strong> is hiring for a <a href=\"https://www.nti.org/job-listing/senior-program-officer-senior-director-global-biological-policy-and-programs-nti-bio/\">Senior Program Officer / Senior Director, Global Biological Policy and Programs</a> (Washington, D.C.)</p><p><strong>Open Philanthropy</strong></p><ul><li><a href=\"https://jobs.ashbyhq.com/openphilanthropy/dfdb0ea9-23c6-462e-9d5b-d141ce2ef33a\"><u>Executive Assistant, Biosecurity &amp; Pandemic Preparedness</u></a> (Remote, US working hours)</li><li><a href=\"https://jobs.ashbyhq.com/openphilanthropy/b15460ec-5554-4c01-9855-0651864b0965\"><u>Program Operations Assistant, Global Health &amp; Wellbeing</u></a> (Remote, US working hours)</li><li><a href=\"https://jobs.ashbyhq.com/openphilanthropy/243da982-26bb-4412-8d48-71d6da45ea92\"><u>Grants Associate, Longtermist Grantmaking</u></a> (Remote)&nbsp;</li></ul><h1>Organizational updates</h1><p><i>These are in alphabetical order, starting with R-Z and then continuing from the top.&nbsp;</i></p><h2>Rethink Priorities (RP)</h2><ul><li>Rethink Priorities is co-sponsoring the&nbsp;<a href=\"https://avasummit.com/speakers\"><u>Animal &amp; Vegan Advocacy Summit</u></a> in Washington, DC from October 20 to 23. Researcher&nbsp;<a href=\"https://www.linkedin.com/in/holly-elmore-aa6301122/\"><u>Holly Elmore</u></a> will give a presentation about wild animal birth control at the Summit.&nbsp;</li><li>The Survey Team is launching&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/tWawcXaNnLAihA2Fv/announcing-ea-pulse-large-monthly-us-surveys-on-ea\"><u>EA Pulse</u></a>\u2014a large, monthly sur\u00advey of the US pop\u00adu\u00adla\u00adtion aimed at mea\u00adsur\u00ading and un\u00adder\u00adstand\u00ading pub\u00adlic per\u00adcep\u00adtions of effec\u00adtive altru\u00adism and EA-al\u00adigned cause ar\u00adeas. The Survey Team intends to 1) track changes in responses to key questions relevant to EA and longtermism over time and 2) run ad hoc questions requested by EA organizations. Please e-mail&nbsp;<a href=\"mailto:david@rethinkpriorities.org\"><u>david@rethinkpriorities.org</u></a> (ideally by October 20) if you would like to request either type of these questions.</li></ul><p>The organization\u2019s new&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/AFgvA9imsT6bww8E3/announcing-the-rethink-priorities-special-projects-program\"><u>Spe\u00adcial Pro\u00adjects Pro\u00adgram</u></a> helps to start promis\u00ading EA ini\u00adti\u00ada\u00adtives by pro\u00advid\u00ading fis\u00adcal spon\u00adsor\u00adship and full-ser\u00advice op\u00ader\u00ada\u00adtional sup\u00adport\u2014in\u00adclud\u00ading, but not limited to hiring, fi\u00adnance, event plan\u00adning, and com\u00admu\u00adni\u00adca\u00adtions. There are various ways to get involved: join the Special Projects team, share lessons learned if you have experience with project incubation, or apply to have your project sponsored. Please reach out by submitting an&nbsp;<a href=\"https://forms.gle/TS5VZ21GhMNsgiSM8\"><u>Expression of Interest</u></a> form.&nbsp;</p><h2>80,000 Hours</h2><p>This month, 80,000 Hours made some changes to its career reviews:</p><ul><li>New career review:&nbsp;<a href=\"https://80000hours.org/career-reviews/should-you-go-to-law-school/\"><u>Should you go to law school in the US to have a high-impact career?</u></a>&nbsp;</li><li>Updated career review:&nbsp;<a href=\"https://80000hours.org/career-reviews/journalism/\"><u>Journalism</u></a></li><li>New audio version:&nbsp;<a href=\"https://80000hours.org/career-reviews/founder-impactful-organisations/#audio-player\"><u>Founder of new projects tackling top problems</u></a></li></ul><p>In podcast news, Rob Wiblin interviewed&nbsp;<a href=\"https://80000hours.org/podcast/episodes/sharon-hewitt-rawlette-hedonistic-utilitarianism/\"><u>Sharon Hewitt Rawlette on why pleasure and pain are the only things that intrinsically matter</u></a> on&nbsp;<i>The 80,000 Hours Podcast</i>, and&nbsp;<a href=\"https://80000hours.org/after-hours-podcast/episodes/kuhan-jeyapragasan-effective-altruism-university-groups/\"><u>Kuhan Jeyapragasan on effective altruism university groups</u></a> on&nbsp;<i>80k After Hours</i>.</p><h2>ALLFED - Alliance to Feed the Earth in Disasters</h2><p>ALLFED, LEEP, GiveWell, and various other EA orgs feature on this recent profile of Effective Altruism in the Dutch newspaper&nbsp;<a href=\"https://www-groene-nl.translate.goog/artikel/red-de-wereld-met-tien-procent-van-je-salaris?_x_tr_sl=auto&amp;_x_tr_tl=en&amp;_x_tr_hl=en-US&amp;_x_tr_pto=wapp\"><u>De Groene Amsterdammer</u></a>:</p><p>ALLFED wants humanity to be spared the fate of the dinosaurs. And this is a more pressing issue than you think. Take, for example, volcanic eruptions: as late as 1815, the eruption of the Tambora in the then Dutch East Indies darkened large parts of the earth, resulting in crop failures and starvation in Europe. Calculations also show that after a major nuclear war between, for example, the US and Russia, the earth could cool down dramatically. In either case, soot or sulfate particles will rise so high that sunlight is blocked for a long time, after which crops fail. And because we don't yet know how to prevent volcanic eruptions \u2013 and the threat of nuclear war is not going away anytime soon \u2013 it makes a lot of sense to think about feeding the world in emergencies.</p><p>The problem is, no government in the world dares to tackle this issue. That's why ALLFED has started to map out all the methods by which humanity can adjust food production in weeks to months to avoid a famine.</p><h2>Animal Charity Evaluators</h2><p>ACE is excited to announce&nbsp;<a href=\"https://animalcharityevaluators.org/blog/announcing-our-new-executive-director/\"><u>Stien van der Ploeg</u></a> as its new Executive Director. Stien brings over 15 years of experience with nonprofit organizations, advocating for various causes in multiple countries. ACE is confident in Stien\u2019s strategic vision and her knowledge of the intersecting areas of effective altruism and animal advocacy. They look forward to seeing ACE\u2019s influence and effectiveness grow under her leadership.</p><h2>Centre for Effective Altruism</h2><p><a href=\"https://www.centreforeffectivealtruism.org/team/rob-gledhill\"><u>Rob Gledhill has taken a role as the new head of CEA\u2019s Groups Team</u></a>. Rob was previously the Community Building Grants Program Manager, where he supports and funds city and national groups in key locations. He will continue this role alongside the Head of Groups Team role.</p><p>We\u2019ve also recently made three new hires:</p><ul><li>Jake McKinnon - University Group Coordinator</li><li>Joris Pijpers - University Groups Support Associate</li><li>Amarins Veringa - incoming Groups Associate, supporting city and national groups (starting in mid-October)</li></ul><p>Jake previously ran Stanford EA, and worked on community building projects in California and Massachusetts. Joris previously ran Positive Impact Society Erasmus and is co-organizing EAGxRotterdam, and Amarins is currently wrapping up her work as co-director of EA Netherlands.</p><p><a href=\"https://www.centreforeffectivealtruism.org/team/yi-yang-chua\"><u>Yi-Yang Chua</u></a>, the EA Virtual Programs Associate, has moved to the Groups team from the Online team, and&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/team/catherine-low\"><u>Catherine Low</u></a> will be transitioning full-time to the Community Health team in October.</p><p>Group organizers can&nbsp;<a href=\"https://resources.eagroups.org/support-and-funding-for-groups/connecting-with-cea-and-other-organisers#h.ejyqr492n59m\"><u>go to this page</u></a> to learn who your main CEA contact person is based on your type of group.</p><p><strong>Conclusions from the EA Criticism and Redteaming Contest (Online Team)</strong></p><p>The&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest\"><u>EA Criticism and Red Teaming Contest has wrapped up</u></a>, awarding $120,000 in prizes to 31 (out of 341) submissions. The criticisms highlighted a number of flaws in existing work and suggested many improvements.</p><h2>Centre for the Study of Existential Risk (CSER)</h2><p>Martin Rees published a new book,&nbsp;<a href=\"https://www.cser.ac.uk/news/new-book-martin-rees-if-science-save-us/\"><u>If Science is to Save Us</u></a>, arguing that science needs to be governed by international and longtermist norms.</p><p>Paul Ingram talked about the threat of nuclear war and hopes of disarmament on&nbsp;<a href=\"https://www.cser.ac.uk/news/paul-ingram-threat-nuclear-war-and-hopes-disarmame/\"><u>the Voices of War</u></a> podcast.</p><p>Lara Mani discussed the threat from large volcanic eruptions on the BBC World Services&nbsp;<a href=\"https://www.cser.ac.uk/news/lara-mani-bbc-newsday/\"><u>Newsday</u></a> programme.</p><p>Shahar Avin talked about the governance of AI on the&nbsp;<a href=\"https://www.cser.ac.uk/news/podcast-shahar-avin-ai-governance/\"><u>Inside View</u></a> podcast.</p><p>Alex McLaughlin published a review of&nbsp;<i>Moralizing Hope</i> by Daniel Moellendorf in the journal&nbsp;<a href=\"https://www.cser.ac.uk/resources/book-review-moralizing-hope-daniel-moellendorf/\"><u>Ethics, Policy &amp; Environment</u></a>.</p><h2>Charity Elections</h2><p><strong>Sponsored charity elections for high schools&nbsp;</strong></p><p>The Charity Elections program has reached a new milestone and opened applications for the 2022-23 school year. Giving What We Can (GWWC) is offering funding for 20+ high schools to run charity elections this year in the expanded program.</p><p>In a&nbsp;<a href=\"https://www.givingwhatwecan.org/events/guides/charity-elections\"><u>charity election</u></a>, a high school\u2019s students vote on which of three charities they\u2019d like to receive up to $2,000 in sponsored funds. The program is meant to promote youth voice and civic engagement while furthering Giving What We Can\u2019s mission of creating a culture where people are inspired to&nbsp;<a href=\"https://www.givingwhatwecan.org/about-us\"><u>give more, and give more effectively</u></a>. It starts conversations and inspires reflection and critical thinking, as seen in&nbsp;<a href=\"https://www.givingwhatwecan.org/events/guides/charity-elections\"><u>participants\u2019 testimonials</u></a>.&nbsp;</p><p><strong>You also have several opportunities to make an impact:&nbsp;</strong></p><ul><li>If you refer a school that successfully runs an election, we will give $100 to any charity of your choosing&nbsp;<a href=\"https://www.givingwhatwecan.org/donate/organizations\"><u>among GWWC\u2019s recommendations</u></a>. Just ask them to list your name and email on the second page of the application.</li><li>You can also help us make calls to schools as a Charity Elections ambassador; please contact&nbsp;<a href=\"mailto:charityelections@givingwhatwecan.org\"><u>charityelections@givingwhatwecan.org</u></a> if you are interested.</li></ul><p>To learn more about applying, visit the&nbsp;<a href=\"https://www.givingwhatwecan.org/events/guides/charity-elections\"><u>Charity Elections website</u></a>.&nbsp;</p><h2>Effective Ventures (EV)</h2><p><a href=\"https://ev.org/\"><u>EV</u></a> published&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/9gHTYC5qbSH9E37vx/cea-ops-is-now-ev-ops\"><u>a post about its recent rebrand</u></a>, explaining its structure and relationship with&nbsp;<a href=\"https://ev.org/ops/\"><u>EV Operations</u></a>. (EV is the federation of projects which includes CEA, 80,000 Hours, and Giving What We Can; EV Operations provides the operational support for these projects.)</p><h2>Faunalytics&nbsp;</h2><p>In honor of Back To School season, Faunalytics has produced three new resources:</p><ul><li>A blog that gives ideas on&nbsp;<a href=\"https://faunalytics.org/knowledge-gaps-animal-focused-research-ideas-for-grad-students/\"><u>high-impact animal-focused research projects</u></a> for graduate students.</li><li>A brand new factsheet on&nbsp;<a href=\"https://faunalytics.org/how-to-become-a-more-effective-vegan-advocate/\"><u>how to be a more effective vegan advocate</u></a>, created in partnership with the Center for Effective Vegan Advocacy.&nbsp;</li><li>A roundtable discussion about&nbsp;<a href=\"https://faunalytics.org/higher-education-for-animal-advocacy/\"><u>studying animal issues</u></a> in a postsecondary context.</li></ul><p>Faunalytics hosted a remote symposium, Fauna Connections, for animal advocates on September 8th and all sessions are available on their YouTube channel in a&nbsp;<a href=\"https://www.youtube.com/playlist?list=PL24QgdIOV-lEsbvGCBSTAWeMvDHfvwNl3\"><u>playlist</u></a>.&nbsp;&nbsp;</p><p>The organization has also updated their research library with articles on topics including&nbsp;<a href=\"https://faunalytics.org/bringing-animals-into-ai-ethics/\"><u>bringing animals into AI ethics</u></a> and how emergency&nbsp;<a href=\"https://faunalytics.org/emergency-covid-19-vaccines-deliver-without-animal-testing/\"><u>approval for COVID-19 vaccines used alternatives to animal testing&nbsp;</u></a>and upheld efficacy.&nbsp;</p><h2>Fish Welfare Initiative</h2><p>Fish Welfare Initiative recently published two new posts about their work:</p><ul><li><a href=\"https://www.fishwelfareinitiative.org/post/ara-fish-welfare\"><u>Updates on their farmer work</u></a>, in which 74 fish farms in Andhra Pradesh India are implementing higher welfare practices.</li><li><a href=\"https://www.fishwelfareinitiative.org/post/fipola\"><u>Updates on their corporate work</u></a>, in which&nbsp;<a href=\"https://www.fipola.in/\"><u>Fipola</u></a>, a large and growing online meat seller, committed to improving fish welfare. You can learn about all of FWI\u2019s partners, corporate and otherwise, on their updated&nbsp;<a href=\"https://www.fishwelfareinitiative.org/partners\"><u>Partners Page</u></a>.</li></ul><p>FWI also continues investing heavily in improving&nbsp;<a href=\"http://fwi.fish/commitment\"><u>its welfare standard</u></a>. They are expecting to publish Version 2 by the end of the year.</p><p>They were also honored to be the recipient of a $250,000 grant from Open Philanthropy.</p><h2>Founders Pledge</h2><p>Founders Pledge has launched its&nbsp;<a href=\"https://founderspledge.com/stories/introducing-the-global-catastrophic-risks-gcr-fund\"><u>Global Catastrophic Risks Fund</u></a> (or \u201cGCR Fund\u201d), a philanthropic co-funding vehicle that will identify and fund interventions to reduce the largest known risks to humanity today. Seed funding for the Global Catastrophic Risks Fund has been provided via the FTX Future Fund Regranting Program.</p><p>FP also published a cause area report on&nbsp;<a href=\"https://docs.google.com/document/d/1T9DLe1bX9V_66sj82RygPpPTCRaZbPUBK6OftR0_r5k/edit#\"><u>air pollution</u></a> in which researcher Tom Barnes lays out the problem and identifies several relevant \u201cimpact multipliers\u201d \u2013 heuristics that can be used to evaluate organizations working on the problem. FP researcher Rosie Bettle published a&nbsp;<a href=\"https://founderspledge.com/stories/measuring-health\"><u>blog post</u></a> describing FP\u2019s cautious use of disability-adjusted life years (DALYs).</p><p>Finally, FP researcher Vadim Albinsky published a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/coryFCkmcMKdJb7Pz/does-economic-growth-meaningfully-improve-well-being-an\"><u>well-received post</u></a> on the EA Forum discussing his research on the Easterlin Paradox and the relationship between economic growth and subjective well-being.</p><h2>GiveDirectly</h2><p>GiveDirectly expanded operations to&nbsp;<a href=\"http://givedirectly.org/yemen\"><u>Yemen</u></a>, their first site in Asia. They also appointed the former UK Aid secretary Rory Stewart as their new president. Vox\u2019s Dylan Matthews&nbsp;<a href=\"https://www.vox.com/future-perfect/2022/8/31/23329242/givedirectly-cash-transfers-rory-stewart\"><u>wrote</u></a>: \u201ccapped by Stewart\u2019s hiring, [GiveDirectly\u2019s growth] tells us a lot about how the field of global development has changed, and what the future not just of global poverty charity but of foreign aid and social policy could look like.\u201d</p><p>Last month in this newsletter,&nbsp;<a href=\"https://blog.givewell.org/2022/08/17/changes-to-top-charity-criteria\"><u>GiveWell announced</u></a> that they\u2019re updating their \u201ctop charities\u201d cutoff: going forward, an organization must have unfunded programs they\u2019ve \u201cestimated at 10 times as cost-effective as unconditional cash transfers\u201d to be included as a top charity. As a result, they\u2019ve now reduced their list of top charities to 4, of which GiveDirectly is not included. GiveWell\u2019s update does not reflect a change in their view of either cash or GiveDirectly: \u201cThis does not reflect an update to our view of GiveDirectly\u2026GiveDirectly is one of the strongest programs that we\u2019ve found in years of research and we continue to have a very high view of their work.\u201d (<a href=\"https://www.givewell.org/charities/give-directly\"><u>GiveWell summary</u></a>) Nor does it reflect a change in their recommended giving allocation, as GiveWell has not directed marginal funding to GiveDirectly since 2015. It\u2019s possible that GiveWell could recommend GiveDirectly again in the future \u2013 either because they increase their estimate of cash\u2019s effectiveness (e.g., by incorporating&nbsp;<a href=\"https://www.givedirectly.org/how-do-cash-transfers-impact-neighbors/\"><u>recent research</u></a> on the multiplier effect of cash) or because they expect to direct more funds and lower their cost-effectiveness threshold as a result (currently 10x, previously 6x and 8x this year and last). GiveDirectly shares more on their thoughts in&nbsp;<a href=\"https://www.givedirectly.org/giving-directly-still-means-giving-well/\"><u>this blog</u></a>.</p><h2>GiveWell</h2><p>GiveWell is running the&nbsp;<a href=\"https://blog.givewell.org/2022/09/06/announcing-change-our-mind-contest/\"><u>Change Our Mind Contest</u></a> to encourage critiques of its existing cost-effectiveness analyses that could improve its allocations. If you haven't yet submitted an entry, the deadline is October 31. Please consider participating and helping to improve GiveWell's work! Detailed contest guidelines can be found&nbsp;<a href=\"https://www.givewell.org/research/change-our-mind-contest\"><u>here</u></a>.</p><p>GiveWell has published pages about the following grants that it recently recommended or made:</p><ul><li><a href=\"https://www.givewell.org/research/grants/malaria-consortium-SMC-july-and-august-2022\"><u>$13.5 million</u></a> to Malaria Consortium to fill part of a $100 million funding gap for its seasonal malaria chemoprevention (SMC) campaigns through 2025.&nbsp;</li><li><a href=\"https://www.givewell.org/research/grants/bridges-to-prosperity-trailbridge-building-rwanda-may-2022\"><u>$3.4 million</u></a> to Bridges for Prosperity (B2P) to fund the construction of additional trailbridges as part of an RCT of B2P's program. B2P builds trailbridges in rural communities, allowing residents easier access to schools and markets, which GiveWell models as primarily benefiting household income.</li></ul><p>GiveWell has published&nbsp;<a href=\"https://docs.google.com/document/d/1Tx7ZyzrtwyqMEI4PDXLHkEa-noPGGrVB6aViUNIW3Cg/edit\"><u>notes</u></a> from a conversation with One Acre Fund about its seedlings program, which helps farmers grow trees that they can sell for timber.</p><h2>Giving What We Can</h2><p>Giving What We Can would love to talk at your workplace or community group!</p><p>Many people start thinking about how to give back towards the end of the year. If you\u2019d like Giving What We Can to host a talk or workshop at your workplace or with your community group about effective giving over the coming months please contact them at&nbsp;<a href=\"mailto:community@givingwhatwecan.org\"><u>community@givingwhatwecan.org</u></a>.&nbsp;</p><p><strong>New content from Giving What We Can</strong></p><p>Blog</p><ul><li><a href=\"https://www.givingwhatwecan.org/blog/should-charity-begin-at-home\"><u>Should charity begin at home?</u></a> \u2013 Alana Horowitz Friedman, contributing writer&nbsp;</li><li>Member profile:&nbsp;<a href=\"https://www.givingwhatwecan.org/blog/member-profile-fernando-martin-gullans\"><u>Fernando Martin-Gullans</u></a>&nbsp;</li><li><a href=\"https://www.givingwhatwecan.org/blog/announcement-renaming-of-legal-entity-to-effective-ventures-foundation\"><u>Announcement: Renaming of legal entity to Effective Ventures Foundation</u></a> \u2013 Giving What We Can team&nbsp;</li><li><a href=\"https://www.givingwhatwecan.org/blog/rutger-bregman-on-effective-giving\"><u>Rutger Bregman on Effective Giving: Highlights from an Interview with Effektiv Spenden</u></a> \u2013 Grace Adams, Head of Marketing</li><li><a href=\"https://www.givingwhatwecan.org/blog/what-is-counterfactual-thinking-and-why-should-you-care-about-it\"><u>What is counterfactual thinking and why should you care about it?</u></a> \u2013 Alana Horowitz Friedman, contributing writer&nbsp;</li></ul><p>YouTube</p><ul><li><a href=\"https://youtu.be/hYm443i3lb8\"><u>You can prevent animal suffering. Here\u2019s how.</u>&nbsp;</a>\u2013&nbsp;<a href=\"https://www.youtube.com/playlist?list=PLT88QiptgOaJBj0E-XrJFsjnQcYRhpkeY\"><u>Giving Effectively</u></a> series</li><li><a href=\"https://youtu.be/tb4KlQHC3tU\"><u>Zachary Brown</u></a> shares his giving story \u2013&nbsp;<a href=\"https://www.youtube.com/playlist?list=PLT88QiptgOaLYdVhB7OnmjZbJnp1Gm3YI\"><u>People Who Give Effectively</u></a> series</li><li><a href=\"https://www.youtube.com/watch?v=efVh51hbRHY&amp;feature=youtu.be\"><u>Don\u2019t make these 10 mistakes when trying to improve the world</u></a>: Dr. Michael Noetel, High Performance Psychologist \u2013&nbsp;<a href=\"https://www.youtube.com/watch?v=WyprXhvGVYk&amp;list=PLT88QiptgOaLGaq5J_1w7dZcwWL6F9P4N\"><u>Effective Altruism</u></a> series</li><li><a href=\"https://www.youtube.com/watch?v=TS28DFakkik&amp;feature=youtu.be\"><u>Using research &amp; strategic thinking to help animals effectively</u></a>: Interview with Neil Dullaghan, Senior Researcher at Rethink Priorities \u2013&nbsp;<a href=\"https://www.youtube.com/watch?v=pK5b2fGKrU8&amp;list=PLT88QiptgOaK2tU5Cxaw9O8iXU30qSOih\"><u>Podcast</u></a> series</li></ul><h2>Global Catastrophic Risk Institute</h2><p>Research Associate Andrea Owe and Executive Director Seth Baum recently published a new article with Vice Dean of University of Vienna, Mark Coeckelbergh called&nbsp;<a href=\"https://gcrinstitute.org/nonhuman-value/\"><u>Nonhuman Value: A Survey of the Intrinsic Valuation of Natural and Artificial Nonhuman Entities</u></a>.</p><p>Research Associate Andrea Owe gave a talk to EA Nordics about Deep Green Ethics and Catastrophic Risk in August. Her talk is now online and can be found&nbsp;<a href=\"https://www.youtube.com/watch?v=oewBx77xAM4&amp;ab_channel=EffektivAltruismeNorge\"><u>here</u></a>. Owe was also recently featured in All Tech Is Human\u2019s&nbsp;<i>Responsible Tech Guide</i> found&nbsp;<a href=\"https://alltechishuman.org/responsible-tech-guide\"><u>here</u></a>.</p><h2>Happier Lives Institute</h2><p>The Happier Lives Institute welcomed three new team members:&nbsp;<strong>Dr Lily Yu</strong> (Grants Strategist),&nbsp;<strong>Dr Ryan Dwyer</strong> (Senior Researcher), and&nbsp;<strong>Rachel Strate</strong> (Operations Manager).&nbsp;<a href=\"https://www.happierlivesinstitute.org/2022/09/01/new-team-members/\"><u>Read more</u></a><u>.</u></p><p>Barry Grimes gave a talk about HLI\u2019s work at EAGxSingapore and Michael Plant gave two talks at EAGxBerlin (one on HLI\u2019s work and another critiquing&nbsp;<i>What We Owe The Future</i>).</p><p>New research on the cost-effectiveness of deworming and anti-malaria bednets will be published in Oct/Nov. Meanwhile, many of HLI\u2019s recent publications have been awarded prizes:</p><p><strong>GiveWell\u2019s Change Our Mind Contest</strong> (<a href=\"https://forum.effectivealtruism.org/posts/MKiqGvijAXfcBHCYJ/deworming-and-decay-replicating-givewell-s-cost?commentId=Qt26uR9ZT6ru8xDqi#comments\"><u>forthcoming</u></a>)</p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/MKiqGvijAXfcBHCYJ/deworming-and-decay-replicating-givewell-s-cost\"><u>Deworming and decay: replicating GiveWell\u2019s cost-effectiveness analysis</u></a> (Joel McGuire, Samuel Dupret, and Michael Plant)</li></ul><p><strong>Open Philanthropy\u2019s Cause Exploration Prizes</strong></p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/dk48Sn6hpbMWeJo4G/to-wellby-or-not-to-wellby-measuring-non-health-non\"><u>WELLBY or not to WELLBY? Measuring non-health, non-pecuniary benefits using subjective wellbeing?</u></a> (Joel McGuire, Samuel Dupret, and Michael Plant)</li></ul><p><strong>EA Criticism and Red Teaming Contest</strong></p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/kxEAkcEvyiwmjirjN/wheeling-and-dealing-an-internal-bargaining-approach-to\"><u>Wheeling and dealing: An internal bargaining approach to moral uncertainty</u></a> (Michael Plant)</li><li><a href=\"https://forum.effectivealtruism.org/posts/bdiDW83SFAsoA4EeB/a-philosophical-review-of-open-philanthropy-s-cause\"><u>A philosophical review of Open Philanthropy\u2019s Cause Prioritisation Framework</u></a> (Michael Plant)</li></ul><p><a href=\"https://forum.effectivealtruism.org/posts/MKiqGvijAXfcBHCYJ/deworming-and-decay-replicating-givewell-s-cost\"><u>Deworming and decay: replicating GiveWell\u2019s cost-effectiveness analysis</u></a> (Joel McGuire, Samuel Dupret, and Michael Plant)</p><h2>Legal Priorities Project</h2><p>LPP published a few&nbsp;<a href=\"https://www.legalpriorities.org/research.html#all-publications\"><u>new research pieces</u></a> from the first half of 2022, including:</p><ul><li>A new upcoming chapter of their&nbsp;<a href=\"https://www.legalpriorities.org/research/research-agenda.html\"><u>research agenda</u></a> on \u201c<a href=\"https://www.legalpriorities.org/research/extreme-climate-change.html\"><u>Extreme Climate Change</u></a>\u201d by&nbsp;<a href=\"https://www.eui.eu/people?id=daniel-bertram\"><u>Daniel Bertram</u></a> (PhD Researcher, European University Institute)</li><li>A paper on \u201c<a href=\"https://www.legalpriorities.org/research/existential-advocacy.html\"><u>Existential Advocacy</u></a>\u201d by&nbsp;<a href=\"https://www.legalpriorities.org/team/john-bliss.html\"><u>Prof. John Bliss</u></a> (LPP, University of Denver, Harvard University), a year-long qualitative study of the emerging legal longtermist community.</li><li>The three winning entries of their&nbsp;<a href=\"https://www.legalpriorities.org/competition\"><u>writing competition</u></a>:<ul><li>\u201c<a href=\"https://www.legalpriorities.org/research/agency-analysis.html\"><u>Catastrophic Risk, Uncertainty, and Agency Analysis</u></a>\u201d by Alasdair Phillips-Robins (Judicial Law Clerk at the US Court of Appeals for the Second Circuit)</li><li>\u201c<a href=\"https://www.legalpriorities.org/research/catastrophic-uncertainty.html\"><u>Catastrophic Uncertainty and Regulatory Impact Analysis</u></a>\u201d by&nbsp;<a href=\"https://www.law.berkeley.edu/our-faculty/faculty-profiles/daniel-farber/\"><u>Prof. Danel Farber</u></a> (UC Berkeley, Faculty Director, Center for Law, Energy, &amp; the Environment)</li><li>\u201c<a href=\"https://www.legalpriorities.org/research/catastrophic-risk-review.html\"><u>Catastrophic Risk Review</u></a>\u201d by&nbsp;<a href=\"https://www.law.virginia.edu/faculty/profile/mal5un/2457619\"><u>Prof. Michael Livermore</u></a> (University of Virginia, Director, Program in Law, Communities and the Environment)</li></ul></li></ul><p>The team plans to publish further updates to their research agenda later this year and is soliciting feedback. Feedback suggestions can be sent to&nbsp;<a href=\"mailto:hello@legalpriorities.org\"><u>hello@legalpriorities.org</u></a>.</p><p>LPP is actively receiving&nbsp;<a href=\"https://legalpriorities.typeform.com/interest\"><u>expressions of interest</u></a> particularly from people looking to contribute to their community-building and outreach projects, as well as people interested in operations roles.</p><h2>One for the World&nbsp;</h2><p>One for the World has successfully launched another academic year of their Chapter Program on university campuses across the United States, Canada, and the UK. This week, Chapter Leaders will celebrate Pledge Week, one of the most intense times of the year for recruiting 1% Pledgers. Some chapters will host daily in-person events to educate their peers about effective giving and the incredible work done by our Nonprofit Partners.&nbsp;</p><p>Following our successful in-person Chapter Leaders Meeting this August, we are proud to share our&nbsp;<a href=\"https://youtu.be/s3g9-VLKBgI\"><u>event recap video</u></a> which demonstrates the power of One for the World and our growing movement of young philanthropists that are revolutionizing charitable giving to end extreme poverty.&nbsp;</p><h2>Open Philanthropy</h2><p>Open Philanthropy announced the winners of its&nbsp;<a href=\"https://www.openphilanthropy.org/research/cause-exploration-prizes-announcing-our-prizes/\"><u>Cause Exploration Prizes</u></a>, after receiving over 150 submissions. The top prize of $25,000 went to&nbsp;<a href=\"https://forum.effectivealtruism.org/users/ben-stewart\"><u>Ben Stewart</u></a> for&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/LSDZ22GFryC3dhWvd/cause-exploration-prize-organophosphate-pesticides-and-other\"><u>Organophosphate pesticides and other neurotoxicants</u></a>; all other good-faith submissions received awards of at least $200.</p><p>Open Philanthropy is seeking&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/CqYHPLCJaFdLn8gp6/open-phil-is-seeking-bilingual-people-to-help-translate-ea\"><u>bilingual people</u></a> to help translate EA-related content into non-English languages.</p><h2>Ought</h2><p>Ought advances&nbsp;<a href=\"https://ought.org/updates/2022-04-06-process\"><u>process-based supervision for language models</u></a>. To make it easier for others to contribute to that goal, Ought released code for writing compositional language model programs and a tutorial that explains how to get started:</p><ul><li>The&nbsp;<a href=\"https://github.com/oughtinc/ice\"><u>Interactive Composition Explorer (ICE)</u></a> is a library for writing and debugging compositional language model programs.</li><li>The&nbsp;<a href=\"https://primer.ought.org/\"><u>Factored Cognition Primer</u></a> is a tutorial using examples to explain how to write such programs.</li></ul><p>Ought has been using ICE as part of its work on&nbsp;<a href=\"https://elicit.org/\"><u>Elicit</u></a> and found it useful in practice. More information on ICE is&nbsp;<a href=\"https://ought.org/updates/2022-10-06-ice-primer\"><u>here.</u></a></p><p>Ought also presented findings from its process supervision in a lab meeting recorded&nbsp;<a href=\"https://www.youtube.com/watch?v=cZqq4muY5_w&amp;\"><u>here.</u></a> The contents of the lab meeting should mostly be accessible to people with limited machine-learning context.&nbsp;</p>", "user": {"username": "Lizka"}}, {"_id": "8whqn2GrJfvTjhov6", "title": "Measuring Good Better", "postedAt": "2022-10-14T13:36:11.657Z", "htmlBody": "<p>At EA Global: San Francisco 2022, the following organisations held a joint session to discuss their different approaches to measuring \u2018good\u2019:&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1669823021/mirroredImages/8whqn2GrJfvTjhov6/ebqcpyjimdofkrlyimnm.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c80e2f753bcaf04a88e849e80f7ac981bc32bce9eb228c4.png/w_130 130w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c80e2f753bcaf04a88e849e80f7ac981bc32bce9eb228c4.png/w_260 260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c80e2f753bcaf04a88e849e80f7ac981bc32bce9eb228c4.png/w_390 390w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c80e2f753bcaf04a88e849e80f7ac981bc32bce9eb228c4.png/w_520 520w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c80e2f753bcaf04a88e849e80f7ac981bc32bce9eb228c4.png/w_650 650w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c80e2f753bcaf04a88e849e80f7ac981bc32bce9eb228c4.png/w_780 780w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c80e2f753bcaf04a88e849e80f7ac981bc32bce9eb228c4.png/w_910 910w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c80e2f753bcaf04a88e849e80f7ac981bc32bce9eb228c4.png/w_1040 1040w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c80e2f753bcaf04a88e849e80f7ac981bc32bce9eb228c4.png/w_1170 1170w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9c80e2f753bcaf04a88e849e80f7ac981bc32bce9eb228c4.png/w_1208 1208w\"></figure><p>A representative from each organisation gave a five-minute lightning talk summarising their approach before the audience broke out into table discussions. You can read the transcripts of the lightning talks below (lightly edited for clarity) or listen to the&nbsp;<a href=\"https://www.youtube.com/watch?v=eLCHaB_BCv0\"><u>audio recording</u> </a>(30 mins).</p><h1><strong>GiveWell (Olivia Larsen)</strong></h1><h2>Why do we need moral weights?</h2><p>GiveWell thinks about measuring good outcomes using a process called \u2018moral weights\u2019. That's because GiveWell evaluates charities that do different things. Some charities increase consumption and income, other charities save the lives of children under five, and other charities save the lives of people over the age of five. In order to do what GiveWell wants to do - create a prioritised list of giving opportunities in order from most to least cost-effective and then use the funds we have available to fill it, starting with most cost-effective until we run out of money - we need an exchange rate between different types of good outcomes. That's why we use moral weights as an input into our cost-effectiveness analysis.&nbsp;</p><h2>What are GiveWell\u2019s moral weights?</h2><p>So here are some of our moral weights. These are all in units of \u2018the moral value of doubling consumption for one person for one year.\u2019 The specific numbers don't mean that we feel totally confident about each number or that we have the right answer. We need a specific number to put into our cost-effectiveness analysis but that definitely doesn't mean that we have a high level of precision or confidence in these.&nbsp;</p><p><i><strong>Table 1:&nbsp;</strong>GiveWell\u2019s moral weights</i></p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>Value of doubling consumption for one person for one year</p></td><td style=\"border:1pt solid #000000;padding:5pt;text-align:right\"><p>1.0</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>Value of averting one year of life lived with disease/disability (YLD)</p></td><td style=\"border:1pt solid #000000;padding:5pt;text-align:right\"><p>2.3</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>Value of averting one stillbirth (1 month before birth)</p></td><td style=\"border:1pt solid #000000;padding:5pt;text-align:right\"><p>33.4</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>Value of preventing one 5-and-over death from malaria</p></td><td style=\"border:1pt solid #000000;padding:5pt;text-align:right\"><p>83.1</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>Value of averting one neonatal death from syphilis</p></td><td style=\"border:1pt solid #000000;padding:5pt;text-align:right\"><p>84.0</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>Value of preventing one under-5 death from malaria</p></td><td style=\"border:1pt solid #000000;padding:5pt;text-align:right\"><p>116.9</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>Value of preventing one under-5 death from vitamin A deficiency</p></td><td style=\"border:1pt solid #000000;padding:5pt;text-align:right\"><p>118.4</p></td></tr></tbody></table></figure><p>So when we say that the value of preventing an under-five death from malaria is about 117, that means we think it's ~117 times more valuable to save this life than it would be to double someone's consumption for a year. To put it another way, if we were given the choice between doubling the consumption of 118 people or preventing one under-five death from malaria we would choose the 118 people, but if it was 116 people we would choose to save the life of the infant.</p><p>One question that sometimes comes up is why we have different values for an under-five death from malaria and an under-five death from vitamin A supplementation. This is because the average age of someone dying from vitamin A deficiency versus malaria is different and our moral weights reflect that difference in age. Here is our curve for the differences in our moral values of death at different ages. It starts one month before birth and ends at over 80 years old. I'm in the \u201825 to 29\u2019 bucket so I'm coming to terms with the fact that I'm past my \u2018moral weightiest\u2019 according to GiveWell.</p><p><i><strong>Figure 1:&nbsp;</strong>GiveWell\u2019s moral values of deaths at different ages (in units of doubling consumption)</i>&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994840/mirroredImages/8whqn2GrJfvTjhov6/nr0pjyyf6tqdf3wwxl8x.png\"></p><h2>What goes into GiveWell\u2019s moral weights?&nbsp;</h2><p>How do we come up with these really specific numbers that we think are valuable, but as I mentioned before, we don't think are as precise as the numbers might suggest? Our moral weights consist of a few different inputs:</p><p><i><strong>Figure 2:</strong> Components of GiveWell\u2019s moral weights</i></p><p>&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994840/mirroredImages/8whqn2GrJfvTjhov6/qymmbifigvd1xefssccv.png\"></p><p>60% of the moral weights come from a donor survey. In 2020, we surveyed about 80 GiveWell donors to ask them how they would make these same types of tradeoffs. The benefits of this survey were that we were able to ask pretty granular questions that let us get to a curve like the one I showed before and that we think GiveWell donors are engaged in this question. They are the users of GiveWell's research, so we want to take into account what they think. But some of the downsides of this are that it's not the most diverse sample and they might not necessarily have complete knowledge or much context-specific knowledge about the places that we\u2019re trying to help.&nbsp;</p><p>30% of our moral weights comes from a&nbsp;<a href=\"https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/2019-moral-weights-research\"><u>survey</u></a> that IDinsight conducted in 2019, which was funded by GiveWell. This survey asked about 2,000 people living in Kenya and Ghana, who are extremely poor, how they would make these same types of tradeoffs. It's an important indicator into our moral weights, but it's not as big as your intuition might think it should be. This is because there were a few issues with the survey. One is that we think that the questions may have been a little bit complicated and challenging, so they might not have been fully understood. Also, some of the results that we got suggested very high values for saving lives, over $10 million, and $10 million is where we stopped asking. So this suggests that some people might not be willing to make&nbsp;<i>any</i> tradeoffs between the value of increasing income versus saving lives and that's something that we're not really able to put into a cost-effectiveness analysis. But this did change our moral weights a lot and moved it toward the direction of valuing the lives of children under five.&nbsp;</p><p>The final portion of our moral weights is a proxy for GiveWell staff opinions, which is something that we used to use more heavily but we've since down-weighted to about 10%. The benefit of this is that GiveWell staff think about these questions a lot, but there aren't that many of us, we don't have a tonne of context-specific knowledge, and the results were very variable based on changes in staff composition.</p><p><strong>Further reading:</strong></p><ul><li><a href=\"https://docs.google.com/document/d/1hOQf6Ug1WpoicMyFDGoqH7tmf3Njjc15Z1DGERaTbnI/edit#\"><u>2020 update on GiveWell\u2019s moral weights</u></a></li><li><a href=\"https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/2019-moral-weights-research\"><u>Research on moral weights - 2019</u></a></li></ul><h1><strong>Open Philanthropy (Jason Schukraft)</strong></h1><p>I'm going to be talking about the global health and wellbeing side of Open Philanthropy. This is not applicable to the longtermist side of the organisation which has a different framework and it\u2019s only somewhat applicable to what happens on the farmed animal welfare team.&nbsp;</p><h2>Why do we need assumptions?</h2><p>The problem is that we want to compare a really wide array of different types of interventions that we might pursue. There's a huge diversity of grantmaking opportunities out there. Everything from improving global health R&amp;D in order to accelerate vaccines to reducing air pollution in South Asia.&nbsp;</p><p>So what do we do? Well, we try to reduce everything to common units and by doing that we can more effectively compare across these different types of opportunities. But this is really, really hard! I can't emphasise enough how difficult this is and we definitely don't endorse all of the assumptions that we make. They're a simplifying tool, they're a model. All models are wrong, but some are useful, and there is constant room for improvement.</p><h2>Valuing economic outcomes</h2><p>Currently, Open Philanthropy is using a log utility model of wellbeing. This graph (below) has a really nice feature which is that increasing someone's income by 1% has the same value in our terms, no matter whether their income is $500,000 or $500.&nbsp;</p><p><i><strong>Figure 3:&nbsp;</strong>The relationship between income and wellbeing</i></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994840/mirroredImages/8whqn2GrJfvTjhov6/lxwlv1kwl3skgygt5cqz.png\"></p><p>So we can say that one unit of philanthropic impact (what we sometimes call \u2018Open Phil dollars\u2019) is giving a single dollar to someone who makes $50,000 a year. When we set this as the arbitrary unit - and let me emphasise, it's just arbitrary - the per capita income in the US was something like $50,000 when we established this unit, so you can think of this as, \u2018what's the simplest, easiest thing that Open Philanthropy could do\u2019? Well, we could take all of our money and just stand on the street corner and hand out dollar bills to Americans. So when we say that $1 to someone who's making $50,000 counts as $1 of philanthropic impact that defines our units. We can then talk about multipliers of that unit. So if someone gets $1, but they\u2019re only making $500 a year rather than $50,000, that's worth 100x our units. And then ultimately, we set a bar for our grantmaking that we want new opportunities to clear.&nbsp;</p><h2>Valuing health outcomes</h2><p>When we measure good at Open Philanthropy, we try to cash out improvements in people's lives on the basis of changes in health and income. We recognise that these are imperfect proxies and that there are lots of ways in which someone's life can go better or worse that aren't going to be fully captured by changes in health and income. But, at least currently, we think that these proxies work pretty well.&nbsp;</p><p>We can now value health outcomes using some of the terminology we've defined. Many of you will be familiar with disability-adjusted life years (DALYs) which are this nice unit combining \u2018years of life lost\u2019 (YLL) and \u2018years lived with disability\u2019 (YLD). In our current valuation, we say that averting a single DALY is worth $100,000 Open Phil dollars, or $100,000 in philanthropic impact. Now, if you're an Open Philanthropy watcher, you might have noticed that we recently doubled this value. We used to value it at $50,000, but now we value health outcomes even more.&nbsp;</p><p>In combination, this gives us a unified framework that lets us tradeoff health and income. There's so much more to say about this topic. I only had five minutes, but I'm really looking forward to talking more at the table.</p><p><strong>Further reading:</strong></p><ul><li><a href=\"https://www.youtube.com/watch?v=eWK1tl1lfNQ\"><u>Open Philanthropy\u2019s Cause Prioritization Framework</u></a> (90 min webinar)</li><li><a href=\"https://www.openphilanthropy.org/research/technical-updates-to-our-global-health-and-wellbeing-cause-prioritization-framework/\"><u>Technical Updates to Our Global Health and Wellbeing Cause Prioritization Framework</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/bdiDW83SFAsoA4EeB/a-philosophical-review-of-open-philanthropy-s-cause\"><u>A philosophical review of Open Philanthropy\u2019s Cause Prioritisation Framework</u></a></li></ul><h1><strong>Happier Lives Institute (Michael Plant)</strong></h1><p>If we're trying to compare different sorts of outcomes, we need some kind of common metric for doing this. How should we think about comparing these three interventions in a sensible way?</p><p><i><strong>Figure 4:&nbsp;</strong>Comparing different outcomes</i></p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1669823021/mirroredImages/8whqn2GrJfvTjhov6/qga2jw1ywz453xgbwjuy.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0661ddd8eb6f36dec923879e77d0d85c02809751d6410713.png/w_130 130w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0661ddd8eb6f36dec923879e77d0d85c02809751d6410713.png/w_260 260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0661ddd8eb6f36dec923879e77d0d85c02809751d6410713.png/w_390 390w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0661ddd8eb6f36dec923879e77d0d85c02809751d6410713.png/w_520 520w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0661ddd8eb6f36dec923879e77d0d85c02809751d6410713.png/w_650 650w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0661ddd8eb6f36dec923879e77d0d85c02809751d6410713.png/w_780 780w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0661ddd8eb6f36dec923879e77d0d85c02809751d6410713.png/w_910 910w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0661ddd8eb6f36dec923879e77d0d85c02809751d6410713.png/w_1040 1040w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0661ddd8eb6f36dec923879e77d0d85c02809751d6410713.png/w_1170 1170w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0661ddd8eb6f36dec923879e77d0d85c02809751d6410713.png/w_1234 1234w\"></figure><h2>Using a subjective wellbeing approach</h2><p>Jason already mentioned the DALY approach as a way to tradeoff quantity and quality of health. At the Happier Lives Institute, we think a better approach is to focus on wellbeing.&nbsp;</p><p>We're interested in doing good, but what do we mean by good? We probably mean wellbeing, but what do we mean by wellbeing? Probably how happy and satisfied people are with their life. That seems like, in the end, probably the thing we really care about, so we should focus on doing that. Rather than using health and income as proxies, let's just focus on the thing which, in the end, matters. Let\u2019s go straight to the source and trade-off health and income in terms of their impact on people's wellbeing.&nbsp;</p><p>So how do you do this? You can measure it with surveys. You can ask people questions like, \u201cHow satisfied are you with your life (0-10)?\u201d If you ask this in the UK, most people say they are seven out of ten and there are other kinds of versions of this question (see below).</p><p><i><strong>Figure 5:&nbsp;</strong>Subjective wellbeing in the UK</i></p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1669823021/mirroredImages/8whqn2GrJfvTjhov6/xk8ort5asrlkyinluly2.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2663ee38ba78a7df38e33ad4d746ec98b3393979743e33aa.png/w_153 153w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2663ee38ba78a7df38e33ad4d746ec98b3393979743e33aa.png/w_233 233w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2663ee38ba78a7df38e33ad4d746ec98b3393979743e33aa.png/w_313 313w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2663ee38ba78a7df38e33ad4d746ec98b3393979743e33aa.png/w_393 393w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2663ee38ba78a7df38e33ad4d746ec98b3393979743e33aa.png/w_473 473w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2663ee38ba78a7df38e33ad4d746ec98b3393979743e33aa.png/w_553 553w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2663ee38ba78a7df38e33ad4d746ec98b3393979743e33aa.png/w_633 633w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2663ee38ba78a7df38e33ad4d746ec98b3393979743e33aa.png/w_713 713w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2663ee38ba78a7df38e33ad4d746ec98b3393979743e33aa.png/w_793 793w\"></figure><p>Instead of DALYs, we think in terms of WELLBYs (wellbeing-adjusted life years). If you think WELLBYs sounds silly, they were nearly called WALYs! So what is one WELLBY? It\u2019s a one-point increase on a 0-10 life satisfaction scale for one year.&nbsp;</p><p>The point is that all these different things happen in your life, you say how you feel, and then we can work out what actually affects your life as you live it. The problem with relying on donor preferences or people's hypothetical preferences is that these are people\u2019s guesses about what they think might matter rather than relying on people's actual experiences as they live their lives.&nbsp;</p><h2>Can we rely on subjective measures?</h2><p>It turns out that if you want to know how happy people are, asking them how happy they are is a good way of finding out how happy they are. These are well-validated measures and it turns out this approach is pretty sensible. You get the correlations and the directions you expect with health, income, what your friends say, and so on.&nbsp;</p><p><i><strong>Figure 6:&nbsp;</strong>Factors that are correlated with subjective wellbeing measures</i></p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1669823021/mirroredImages/8whqn2GrJfvTjhov6/mwrxb1vosmzrqdddwgcb.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/df61c9c4fff7a3445d53164aa042f9d1122a1cef5950d8db.png/w_120 120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/df61c9c4fff7a3445d53164aa042f9d1122a1cef5950d8db.png/w_240 240w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/df61c9c4fff7a3445d53164aa042f9d1122a1cef5950d8db.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/df61c9c4fff7a3445d53164aa042f9d1122a1cef5950d8db.png/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/df61c9c4fff7a3445d53164aa042f9d1122a1cef5950d8db.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/df61c9c4fff7a3445d53164aa042f9d1122a1cef5950d8db.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/df61c9c4fff7a3445d53164aa042f9d1122a1cef5950d8db.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/df61c9c4fff7a3445d53164aa042f9d1122a1cef5950d8db.png/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/df61c9c4fff7a3445d53164aa042f9d1122a1cef5950d8db.png/w_1080 1080w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/df61c9c4fff7a3445d53164aa042f9d1122a1cef5950d8db.png/w_1162 1162w\"></figure><p>This is a map of life satisfaction across the world. It has this familiar picture of countries that are more or less developed so it seems there's some sort of approximately unified scale going on.&nbsp;</p><p><i><strong>Figure 7:&nbsp;</strong>Self-reported life satisfaction around the world</i></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994840/mirroredImages/8whqn2GrJfvTjhov6/wnjkyzckjlkhuso4wuhd.png\"></p><p>Happiness research really only started after the Second World War, but it's been really picking up and now people know, for instance, that the Scandinavian countries are the happiest in the world. But the World Happiness Report has only been going for ten years, so this is new. Our approach is to say, \u201cwe think this is important, but let's not just measure wellbeing at a national level, let's actually work out the most cost-effective ways to improve global wellbeing\u201d. So we are pioneering WELLBY cost-effectiveness.&nbsp;</p><h2>What difference does it make?</h2><p>Does this matter or are we just arguing about metrics? Well, one thing you could do is compare providing cash transfers in low-income countries to treating depression with group psychotherapy. It wouldn't make sense to compare them in terms of income, that's not the value of having your depression alleviated. You could measure it in terms of health, but that's not the value of having your poverty alleviated. If you measure their effects in terms of wellbeing, you can compare them directly in terms of the units that matter.</p><p>So, does it make a difference? We did a couple of&nbsp;<a href=\"https://www.happierlivesinstitute.org/report/donating-money-buying-happiness/\"><u>meta-analyses</u></a>, and here is a picture to indicate that this is how meta-analyses work. We looked at various effects from various studies, this is just trying to show you that we actually did some real research here!&nbsp;</p><p><i><strong>Figure 8:</strong> Forest plot of 37 cash transfer studies</i></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994840/mirroredImages/8whqn2GrJfvTjhov6/vy7i6kcoy6p3vumyhpcf.png\"></p><p>This is the effect over time (see below). What we found is that the therapy has a big effect initially and then it trails off faster. Cash transfers have a smaller initial effect but it lasts longer. This is a $1,000 lump sum cash transfer via GiveDirectly, which is more than a year's household income. So group psychotherapy for the depressed or cash transfers to people who are very poor have sort of the same size effects, the therapy is slightly bigger.</p><p><i><strong>Figure 9:&nbsp;</strong>The total effect of lump sum cash transfers and group psychotherapy</i></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994840/mirroredImages/8whqn2GrJfvTjhov6/zrwf8qfvjkapzhb9isdm.png\"></p><p>But what really drives the difference is that giving people money is expensive. A $1,000 cash transfer costs a bit more than $1,000 to deliver. The group psychotherapy provided by StrongMinds costs about $130 per person. So what we have here are some dots (see below). To account for uncertainty we ran some Monte Carlo simulations, so these aren't just dots, these are fancy dots! On the&nbsp;<i>x-axis</i> is the cost of the treatment, and then that's the wellbeing effect on the&nbsp;<i>y-axis</i>.</p><p><i><strong>Figure 10:</strong> Comparison of cash transfers and psychotherapy</i></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994840/mirroredImages/8whqn2GrJfvTjhov6/ljemnbz2djxkyfqcgc6s.png\"></p><p>And we find that providing psychotherapy is&nbsp;<strong>nine times&nbsp;</strong>more cost-effective than cash transfers.&nbsp;</p><p><i><strong>Table 2:&nbsp;</strong>Comparison of GiveDirectly and StrongMinds</i></p><figure class=\"table\"><table><tbody><tr><td style=\"border-style:solid;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"background-color:#1c5fb8;border:1pt solid #000000;padding:5pt;text-align:center\"><p>WELLBYs</p><p>(per treatment)</p></td><td style=\"background-color:#1c5fb8;border:1pt solid #000000;padding:5pt;text-align:center\"><p>COST</p><p>(US dollars)</p></td><td style=\"background-color:#1c5fb8;border:1pt solid #000000;padding:5pt;text-align:center\"><p>WELLBYs&nbsp;</p><p>(per $1,000)</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>GiveDirectly</p><p>(lump sum cash transfers)</p></td><td style=\"border:1pt solid #000000;padding:5pt;text-align:center\"><p>9</p></td><td style=\"border:1pt solid #000000;padding:5pt;text-align:center\"><p>$1,220</p></td><td style=\"border:1pt solid #000000;padding:5pt;text-align:center\"><p>7.3</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>StrongMinds</p><p>(group psychotherapy)</p></td><td style=\"border:1pt solid #000000;padding:5pt;text-align:center\"><p>12</p></td><td style=\"border:1pt solid #000000;padding:5pt;text-align:center\"><p>$170</p></td><td style=\"border:1pt solid #000000;padding:5pt;text-align:center\"><p>70</p></td></tr><tr><td style=\"background-color:#efefef;border:1pt solid #000000;padding:5pt\">Ratio (SM v GD)</td><td style=\"background-color:#efefef;border:1pt solid #000000;padding:5pt;text-align:center\"><p>1.3 x&nbsp;</p><p>more effective</p></td><td style=\"background-color:#efefef;border:1pt solid #000000;padding:5pt;text-align:center\"><p>14%&nbsp;</p><p>of GD cost</p></td><td style=\"background-color:#efefef;border:1pt solid #000000;padding:5pt;text-align:center\"><p>9 x&nbsp;</p><p>more cost-effective</p></td></tr></tbody></table></figure><p><br>&nbsp;When I speak to people about this, some people tell me I'm mad. Mostly those people are economists that just think this is absolute nonsense. Some people say, \u201cwell of course if you want to alleviate misery, you\u2019ve got to focus on what's going on inside people's heads.\u201d&nbsp;</p><p>So we think this shows that it matters, that we should be using a wellbeing lens, and that this really does give us a new approach.&nbsp;</p><h2>Plans for further research</h2><p>We plan to look at more interventions. We\u2019re starting at the micro-scale, before moving to bigger scales. It turns out that when you're trying to do something new, you run into problems, so we're pioneering this WELLBY approach.</p><ul><li><strong>Micro</strong>: deworming, bednets, cataract surgery, mental health apps, cement flooring</li><li><strong>Meso</strong>: lead paint regulation, access to pain relief, immigration reform</li><li><strong>Macro</strong>: wellbeing policy \u2018blueprints\u2019 for national governments</li></ul><p>There are also various methodological questions to get stuck into:</p><ul><li>How to compare improving lives to saving lives</li><li>Assessing the cardinal comparability of happiness scales</li><li>How to convert between and prioritise different measures of wellbeing</li><li>Understanding and implementing \u2018worldview diversification\u2019</li><li>Plausibility and implications of longtermism</li></ul><p><strong>Further reading:</strong></p><ul><li><a href=\"https://www.happierlivesinstitute.org/2022/08/16/wellby/\"><u>To WELLBY or not to WELLBY? Measuring non-health, non-pecuniary benefits using subjective wellbeing</u></a></li><li><a href=\"https://www.happierlivesinstitute.org/report/estimating-moral-weights/\"><u>Estimating moral weights</u></a></li><li><a href=\"https://www.happierlivesinstitute.org/report/happiness-for-the-whole-family/\"><u>Happiness for the whole family</u></a></li></ul><h1><strong>Founders Pledge (Matt Lerner)</strong></h1><p>Our benchmark, like everybody else, is cash. Things need to be at least as good as cash and historically, we value cash at $199/WELLBY. I say 'historically' because this presentation is mostly about our new approach, which we're working on right now.&nbsp;</p><h2>Our historical (deprecated) approach</h2><p>We used to put everything in WELLBYs, convert to DALYs, rely on moral weights derived from team deliberation, and then apply subjective discounts post hoc based on charity-specific considerations. If that sounds weird, don't worry, because we're moving to a new approach.</p><h2>Goals and constraints for our new approach</h2><p>Founders Pledge advises entrepreneurs and founders on how to spend their money. We also spend some of our own money so we have a bunch of goals we need to evaluate.</p><ol><li>We want to evaluate as many different kinds of interventions as possible.</li><li>We need the metrics to work for both in-depth cost-effectiveness analyses (which we use to justify recommendations to members) and for back-of-the-envelope calculations (for our own rapid grantmaking).&nbsp;</li><li>We need to be flexible enough to deal with interventions that improve quality of life (we don\u2019t want to just slice up disability weights and use \u2018mini-depressions\u2019).&nbsp;</li><li>We need to make the most of existing sources of data.</li><li>We want to make as few subjective decisions as possible.&nbsp;</li><li>We want our weightings and conversion factors to reflect the weight of evidence.</li><li>We want to appropriately account for moral uncertainty.&nbsp;</li></ol><h2>The general idea behind the new approach</h2><p>We have DALYs, we have WELLBYs, and we have income doublings. Those are all noisy measurements of some underlying quantity of interest, the \u2018goodness\u2019 of an intervention, and ultimately we want to be able to measure the impact of an intervention using any of these metrics.&nbsp;</p><p>So moving forward, our approach is WELLBYs, to income, to death, to DALYs, to WELLBYs. And that dotted line indicates that we're only going to figure out conversion factors for these three and then \u2018back out\u2019 a DALY/WELLBY conversion.</p><p><i><strong>Figure 11:</strong> The new approach at Founders Pledge</i></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994840/mirroredImages/8whqn2GrJfvTjhov6/ta7hlpvxtavmbi66otkg.png\"></p><p>The way that we are going to do that, starting at the top right and going clockwise, is we're going to rely very heavily on HLI\u2019s work. HLI did an excellent&nbsp;<a href=\"https://www.happierlivesinstitute.org/report/cash-transfers-systematic-review-and-meta-analysis/\"><u>meta-analysis</u></a>, which was very convincing to us, showing there is a stable relationship between wellbeing and income doublings, that's the first leg.&nbsp;</p><p>The second leg is income doubling to death and we have three approaches that we use. We have&nbsp;<a href=\"https://www.youtube.com/watch?v=eWK1tl1lfNQ\"><u>Open Philanthropy\u2019s approach</u></a>, we have the&nbsp;<a href=\"https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/2019-moral-weights-research\"><u>IDinsight survey</u></a> that GiveWell sponsored in 2019, and we have GiveWell from&nbsp;<a href=\"https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/comparing-moral-weights\"><u>pre-2019</u></a>. The rationale there is we think that the IDinsight survey represents a sort of preference utilitarian approach. We think that Open Philanthropy\u2019s method represents a hedonic utilitarian approach as it relies on some subjective wellbeing data. Finally, we have GiveWell pre-2019 which represents something like an \u2018EA community consensus\u2019. So before the preference utilitarian data arrived, we look at what GiveWell thought and then we weight those three approaches equally and try not to insert our own subjective judgments.&nbsp;</p><p>From death to DALYs is sort of a simple step. We want to convert death to DALYs at different ages so we just need different conversions for different age groups.&nbsp;</p><p>The final leg is DALYs to WELLBYs and this is where we're currently working things out and I'll go into a little bit of detail.&nbsp;</p><h2>How this is looking so far</h2><p>Right now, for a subset of conditions for which there are both WELLBY and DALY data, the correlation is decent (0.65). However, if you restrict this to physical conditions, you get a really strong correlation (0.9). The reason is that mental conditions (anxiety and depression) are big outliers and we think this is actually totally reasonable. There's lots of research on affective forecasting failures that suggest that people just don't have a really good idea of how they're going to feel when something bad happens to them or when they imagine having depression. Some justification for this is that the disability weight for dental caries is very low (0.01) but if you look at pain data, it's about as bad as lower back pain (0.3) That\u2019s 30 times as high for roughly the same amount of pain. So, for certain types of conditions, we think that disability weights really underrate the subjective experience and that's why we want to be able to use all of these different metrics.&nbsp;</p><h2>Going forward</h2><p>We are going to set our benchmark at the $/WELLBY - $/DALY - $/income doubling - $/death figure. For effect sizes, we will use whatever the most applicable unit is and then translate it to our benchmark (income doublings for income-enhancing interventions, DALYs for physical conditions, and WELLBYs for others). We will still probably have to litigate major disagreements between DALYs and WELLBYs on an ad hoc basis when things look really weird, which they undoubtedly will.</p><p><strong>Further reading:</strong></p><ul><li><a href=\"https://founderspledge.com/stories/measuring-health\">Measuring Health: How We Use (And Sometimes Don\u2019t Use) DALY Estimates</a></li><li><a href=\"https://founderspledge.com/stories/our-approach-to-charity\"><u>Our approach to charity</u></a></li><li><a href=\"https://founderspledge.com/stories/oral-healthcare\"><u>Oral healthcare</u></a></li></ul><h1><strong>Innovations for Poverty Action (Katrina Sill)</strong></h1><p>We're going to take a little bit of a \u2018zoom out\u2019 approach here and take this question quite literally, how do you measure good better? Does that mean you need to be looking at these outcomes that we've been looking at today (WELLBYs, DALYs etc.)? Does measuring good always mean that you need to measure that outcome?&nbsp;</p><h2>Impact = solution quality x implementation quality</h2><p>To answer that question, let's first think about what \u2018good\u2019 means. For simplification purposes, let's think of impact as a function of two things: the quality of the solution and the quality of the implementation of that solution. Today, we've been talking primarily about the quality of the solution. One of the major drivers of whether a solution actually has an effect is if that solution is a) the right solution for that context, and b) if it's going to be implemented well and feasibly. So what we want to add to this conversation is that those things are equally important as looking at the final effect.&nbsp;</p><h2>The right way to measure \u2018good\u2019 depends on the question</h2><p>When you're thinking about \u2018measuring good better\u2019 as a goal, you need to think about what the primary question is for a specific intervention. You might start by asking, how much does this specific intervention (e.g. group therapy, bed nets) typically increase something like DALYs and/or WELLBYs? In order to answer that question, you have to look at a lot of other questions too:&nbsp;</p><ul><li>Can our program be implemented well in this new context?</li><li>Can this program be replicated at high quality at scale?</li><li>Is this program&nbsp;<i>really</i> addressing a primary challenge in this context?</li><li>Do people access the program?</li><li>Do people adopt the key behaviour changes needed?</li></ul><p>For example, you might want to ask this question first: is this programme&nbsp;<i>really</i> addressing a challenge in that particular context? If you're looking at rolling this out in East Africa, what is the prevalence of malaria in East Africa, and what is the current use of bednets in East Africa? That's what you would want to look at first, rather than just looking at the overall potential for DALYs or WELLBYs. That doesn\u2019t necessarily mean that you want to run a randomised controlled trial, which is what Innovations for Poverty Action (IPA) is more famous for. For that kind of question, you'd first want to look at the basic underlying data for that context.</p><h2>What to measure and when</h2><p>So what does IPA think that \u2018measuring good\u2019 means? We frame it as this path to scale.&nbsp;</p><p><i><strong>Figure 12:</strong> IPA\u2019s path to scale</i></p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1669823021/mirroredImages/8whqn2GrJfvTjhov6/lausibse6olyqfziyhe3.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6b06523b19d2b003d3cc3424da8fe3f4b8491cad172df348.png/w_130 130w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6b06523b19d2b003d3cc3424da8fe3f4b8491cad172df348.png/w_260 260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6b06523b19d2b003d3cc3424da8fe3f4b8491cad172df348.png/w_390 390w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6b06523b19d2b003d3cc3424da8fe3f4b8491cad172df348.png/w_520 520w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6b06523b19d2b003d3cc3424da8fe3f4b8491cad172df348.png/w_650 650w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6b06523b19d2b003d3cc3424da8fe3f4b8491cad172df348.png/w_780 780w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6b06523b19d2b003d3cc3424da8fe3f4b8491cad172df348.png/w_910 910w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6b06523b19d2b003d3cc3424da8fe3f4b8491cad172df348.png/w_1040 1040w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6b06523b19d2b003d3cc3424da8fe3f4b8491cad172df348.png/w_1170 1170w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6b06523b19d2b003d3cc3424da8fe3f4b8491cad172df348.png/w_1225 1225w\"></figure><p>So you would look at a particular intervention and try to identify - for the goal that you have and the context that you're looking at effecting - where is this intervention on this path? What we've focused on today is the \u2018prove\u2019 step in the learning cycle; measuring the final outcomes and what evidence exists there. If it's a new intervention, what's our best guess of what that measurement might be? Maybe using some assumptions like Open Philanthropy does.&nbsp;</p><p>In addition to doing that, we recommend looking at these other stages. There are a lot of other ways to measure \u2018good\u2019 depending on what stage an intervention is in and these are typically underrepresented in a lot of effective altruism organisations and discussions. These stages include: identifying the problems in a particular context, prototyping with users, figuring out if that's the right solution, testing it out on a small scale, and monitoring if that works. For example, when you distribute a bed net, does it reach the target audience? If it reaches the target user, let's say children and families, are they actually using that bed net? These questions have to work before you can ask, \u2018does it actually improve the number of lives saved\u2019?&nbsp;</p><p>These are important questions for the EA community to think about. Where might the quality solutions be breaking down in implementation? What can we do as a community to make sure that the highest quality interventions are implemented well and that they're the right solution for a particular context?&nbsp;</p><p>The same applies to the end of the path too; adaptation and scale. If you have something that works already (e.g. bed nets or group therapy) and you're moving into a new context, what would change in that context based on what you've looked at previously and your theory of change? How might you need to adapt this in that context?</p><p>IPA is a big proponent of using credible evidence and we generate a lot of that ourselves. We just want to make the case that that's not always the only thing you should be looking at because it can reduce your flexibility in the programme adapting if you're looking too rigidly at those final outcomes too early. It's sometimes not the right time to be looking at that. Instead, we want to be looking at innovation and monitoring in the specific context.</p><p><strong>Further reading:</strong></p><ul><li><a href=\"https://www.poverty-action.org/right-fit-evidence\"><u>Right-fit evidence</u></a></li><li><a href=\"https://www.poverty-action.org/right-fit-evidence/resources/toolkit\"><u>Goldilocks toolkit</u></a></li><li><a href=\"https://www.poverty-action.org/right-fit-evidence/book/the-goldilocks-challenge\"><u>The Goldilocks Challenge: Right-Fit Evidence for the Social Sector</u></a></li></ul>", "user": {"username": "MichaelPlant"}}, {"_id": "jJzSCjPZXFKRqExWa", "title": "Contra shard theory, in the context of the diamond maximizer problem", "postedAt": "2022-10-13T23:51:29.853Z", "htmlBody": "", "user": {"username": "So8res"}}, {"_id": "8gyGtdAAuhT3AkXus", "title": "Changes to EA Giving Tuesday for 2022", "postedAt": "2022-10-13T23:37:02.306Z", "htmlBody": "<p>Recently the organising team led by Megan Jamer posted on the EA Forum&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/d26SGGkxRSHyKbqam/ea-giving-tuesday-will-likely-hibernate-in-2022-unless\"><u>that EA Giving Tuesday might hibernate unless someone picked up the project</u></a>. We are pleased to tell you that Giving What We Can and One For The World have volunteered to take on the management of EA Giving Tuesday for 2022, albeit in a limited capacity.</p><p>(For more context on EA Giving Tuesday there is&nbsp; in-depth information on the&nbsp;<a href=\"https://www.eagivingtuesday.org/\"><u>website</u></a>, and in the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/KmBbJJxrj3Mkum36K/ea-giving-tuesday-donation-matching-initiative-2020\"><u>EA Giving Tuesday 2020 retrospective</u></a>.)</p><p>Both GWWC and OFTW felt that it would be a shame for the community and for effective nonprofits if we did not run EA Giving Tuesday in 2022, so we were determined to find a way to pick up the project. Both organisations have small teams, and with the limited time we have left before the end of November, we unfortunately aren't able to support the project at its previous scale.</p><p>We have decided to cut down on activities that had required a large amount of time from an operational perspective, and this has two significant consequences:</p><ol><li>We will only be directly supporting roughly 25% of the charities/funds that were available last year</li><li>We will not be undertaking significant testing and revision of the donation strategy</li></ol><p>We know that this means that far fewer EA aligned charities will be supported through EA Giving Tuesday 2022, and we thought hard about what was the best path forward, but ultimately felt that it was better to support a limited number of charities than not have the project run at all.</p><p>We also understand that there is a risk that the strategy may not be as effective as in other years, but from some early testing it looks like last year's strategy may hold up for 2022.</p><p>GWWC and OFTW think that there is a reasonable chance that even with a scaled down project, we could still move a similar amount of money to last year (assuming Meta\u2019s available funds and rules remain the same).</p><p>Over the coming weeks we will be updating the EA Giving Tuesday website and, once Meta announces the match, we\u2019ll be providing further instructions.</p><p>If you\u2019d like to participate in EA Giving Tuesday this year, sign up for email updates:</p><h1><a href=\"https://forms.gle/zfZ898pFrBG95LPo6\">Sign up here!</a>&nbsp;</h1><p>&nbsp;</p><p>If we are successful again in 2022, both GWWC and OFTW look forward to figuring out how we can efficiently scale the project back up for 2023.</p><p><br>&nbsp;</p>", "user": {"username": "Giving What We Can"}}, {"_id": "wFC3axfuwABHmoQ9H", "title": "The Vitalik Buterin Fellowship in AI Existential Safety is open for applications!", "postedAt": "2022-10-14T03:23:10.837Z", "htmlBody": "<p>This is a linkpost for<a href=\"https://forum.effectivealtruism.org/out?url=https%3A%2F%2Fgrants.futureoflife.org%2F\">&nbsp;<u>https://grants.futureoflife.org/</u></a></p><p><i>Epistemic status: Describing the fellowship that we are a part of and sharing some suggestions and experiences.</i></p><p>The Future of Life Institute is launching its 2023 cohort of&nbsp;<a href=\"https://grants.futureoflife.org/prog/phd_fellowship/\"><u>PhD and postdoctoral fellowships</u></a> to study<a href=\"https://www.dropbox.com/s/74j0mf4mw5c0yoy/AI%20Existential%20Safety%20Research%20Definition.pdf?dl=0\"><u> AI existential safety</u></a>: that is, research that analyzes the most probable ways in which AI technology could cause an existential catastrophe, and which types of research could minimize existential risk; and technical research which could, if successful, assist humanity in reducing the existential risk posed by highly impactful AI technology to extremely low levels. More information about the 2022 cohort can be found&nbsp;<a href=\"https://futureoflife.org/team/fellowship-winners-2022/\"><u>here</u></a>.</p><p>The<a href=\"https://grants.futureoflife.org/res/p/phd-fellowship-ai/\">&nbsp;<u>Vitalik Buterin PhD Fellowship in AI Existential Safety</u></a> is targeted at students applying to start their PhD in 2023, or existing PhD students who would not otherwise have funding to work on AI existential safety research. Quoting from the page:</p><p>At universities in the US, UK, or Canada, annual funding will cover tuition, fees, and the stipend of the student's PhD program up to $40,000, as well as a fund of $10,000 that can be used for research-related expenses such as travel and computing. At universities not in the US, UK or Canada, the stipend amount will be adjusted to match local conditions. Fellows will also be invited to workshops where they will be able to interact with other researchers in the field.</p><p>In addition, applicants who are short-listed for the Fellowship will be reimbursed for application fees for up to 5 PhD programs, and will be invited to an information session about research groups that can serve as good homes for AI existential safety research.</p><p>Applications for the PhD fellowship close on&nbsp;<strong>Nov 15, 2022</strong>.</p><p>The<a href=\"https://grants.futureoflife.org/res/p/postdoc-fellowship-ai/\">&nbsp;<u>Vitalik Buterin Postdoctoral Fellowship in AI Existential Safety</u></a> is for postdoctoral appointments starting in fall 2023. Quoting from the page:</p><p>For host institutions in the US, UK, or Canada, the Fellowship includes an annual $80,000 stipend and a fund of up to $10,000 that can be used for research-related expenses such as travel and computing. At universities not in the US, UK or Canada, the fellowship amount will be adjusted to match local conditions.</p><p>Applications for the postdoctoral fellowship close on&nbsp;<strong>Jan 2nd, 2023</strong>.</p><p>We (Cynthia Chen and Zhijing Jin) are two of the fellows from the 2022 class, and we strongly recommend whoever sees fit to apply! We especially appreciate these aspects of the fellowship:</p><ol><li>Having access to the broad and vibrant AI existential safety network at FLI.</li><li>Participating in seminars and communicating insights about AI safety with peers and professors.</li><li>Having the freedom to work on the most important AI safety problems during our PhD, without constraints from the supervisors.</li><li>If you\u2019re applying to PhD this year, having obtained a fellowship that can fully fund your research can make you especially advantageous in your application.</li></ol><p>You can apply at<a href=\"https://grants.futureoflife.org/\">&nbsp;<u>grants.futureoflife.org</u></a>, and if you know people who may be good fits, please help spread the word!</p>", "user": {"username": "Cynthia Chen"}}, {"_id": "adpLHqNri6z7zr4ab", "title": "What is the expected value of me giving blood?", "postedAt": "2022-10-13T17:09:44.095Z", "htmlBody": "<h2>About me:</h2><ul><li>27 year old healthy, cis,&nbsp;<a href=\"https://www.blood.co.uk/why-give-blood/demand-for-different-blood-types/why-more-black-blood-donors-are-needed/\"><u>white</u></a>,&nbsp;<a href=\"https://www.blood.co.uk/who-can-give-blood/men-who-have-sex-with-men/\"><u>straight</u></a>&nbsp;<a href=\"https://www.blood.co.uk/who-can-give-blood/why-men-should-donate-blood/\"><u>male</u></a><u>.</u></li><li>I am&nbsp;<a href=\"https://www.blood.co.uk/why-give-blood/blood-types/o-negative-blood-type/\"><u>O- blood type</u></a>, or \u2018universal donor\u2019. O- can be accepted by any recipient, and there are certain situations when&nbsp;<i>only&nbsp;</i>O- blood can be accepted, such as when the donee has O- blood type, or in situations where the blood type of the donee cannot be quickly found. Around 13% of people are O-.</li><li>I used to give regularly, and have given 10 times without issue. I haven\u2019t given for 4 + years, mainly due to me being uncertain how useful it is on the margin, and the costs it incurs me. For me, these costs are:<ul><li>Time. Generally a little under an hour at the donor center, plus 1 hour\u2019s transport time if I get a lift by car or taxi, or 2 hours if I get public transport (I don\u2019t own a car).</li><li>Feeling lethargic and tired afterwards. I find this persists for around 3 days after giving blood, and makes doing exercise harder for up to a week (doing regular exercise is important for me).</li><li>Discomfort of the needle when actually giving blood.</li></ul></li></ul><h2>Situational factors:&nbsp;</h2><ul><li>The UK (where I live) has just declared its f<a href=\"https://news.sky.com/story/nhs-declares-first-amber-alert-as-blood-supplies-drop-low-12718647\"><u>irst ever \u2018amber alert\u2019 for blood shortages</u></a>. Stocks of O- are said to be below 2 days, with the NHS aiming for 6 days at all time. More importantly, it seems stocks are so low that non-urgent hospital operations are being postponed. This is the first time I\u2019ve read of a blood shortages having direct consequences on hospital operations.</li></ul><h2>General evidence around giving blood</h2><p>Much how EA complains that charities often don\u2019t provide transparent evidence of their outcomes, I\u2019m frustrated at the lack of evidence available to donors on the efficacy of donating blood. There seems to be very little attempt to measure the expected value of giving blood, and how it varies depending on blood stocks, or a donor\u2019s blood type. There\u2019s a few posts in the EA-verse, such as&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jqCCM3NvrtCYK3uaB/blood-donation-generally-not-that-effective-on-the-margin\"><u>this one from 2017</u></a>, which estimates \u2018a unit of red blood cells costs about 120 pounds\u2019 (frustratingly, no source is given). There\u2019s&nbsp;<a href=\"https://acesounderglass.com/2015/04/07/is-blood-donation-effective-yes/\"><u>this article from 2015</u></a>, which estimates the value of giving blood to be equivalent to donating between $50 and $1667 to a GiveWell top charity.&nbsp;</p><p>Following some sort of common-sense morality factoring in the current situational factors in the UK, and my comparative advantage with O- blood, I\u2019ll probably donate in the coming two or three weeks. But if someone can provide links to more in-depth analysis on the usefulness of giving blood it\u2019ll make me happy, and probably influence whether I take up donating regularly again.&nbsp;</p>", "user": {"username": "Matt g"}}, {"_id": "ePCMcxtpcLqrgaTJy", "title": "Will Evidence-Based Management Practices Increase Your Impact?", "postedAt": "2022-10-14T03:22:39.400Z", "htmlBody": "<p>The<a href=\"https://forum.effectivealtruism.org/posts/zQ5apJGAJb6otXdvh/high-impact-psychology-hipsy-piloting-a-global-network\"> High Impact Psychology </a>initiative is considering providing Evidence Summaries on effective management practices. We will explain how you can use behavioral science and psychology at work to make your organization more effective and increase your impact.</p><p>To ensure we focus on the most pressing topics, we aim to provide content for the options you upvote the most.</p><p><strong>Which of these might improve the impact of your work the most?</strong>&nbsp;</p><p><strong>Update</strong>: <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdFbUZUyK4jrwRiii7QwEXogRFzm50PENxIyLwtByKQn5nuPw/viewform?usp=sf_link\">Here you can find the updated full form. It takes you 3 min to fill it out.</a></p><p>Former version: Please upvote the comments with your preferred topics. You can select more than one.</p><ol><li>Building effective virtual teams</li><li>Improve employee engagement&nbsp;</li><li>What works in performance management</li><li>Improve employee's mental health and resilience</li><li>Building an effective organizational culture</li><li>The behavioral science of reward, incentives, and recognition</li><li>Effective cross-functional collaboration&nbsp;</li><li>Building diverse and inclusive workplaces</li><li>Building effective learning cultures&nbsp;</li><li>Effective leadership</li><li>Evidence-based recruiting and selection</li></ol>", "user": {"username": "Lorenzo Gall\u00ed"}}, {"_id": "btJWbzqrD8K9cz58X", "title": "How Is EA Rational?", "postedAt": "2022-10-13T16:21:59.134Z", "htmlBody": "<p>I <a href=\"https://forum.effectivealtruism.org/posts/37byNFAMEfbLNPXdm/debate-with-rational-methodology\" title=\"Debate with Rational Methodology?\">asked</a> if EA has a rational debate methodology in writing that people sometimes use. The answer seems to be \u201cno\u201d.</p>\n<p>I <a href=\"https://forum.effectivealtruism.org/posts/rd4WSaJWKzrs7i27P/does-ea-have-an-alternative-to-rational-written-debate\" title=\"Does EA have an alternative to rational, written debate methodology?\">asked</a> if EA has any alternative to rationally resolve disagreements. The answer seems to be \u201cno\u201d.</p>\n<p>If the correct answer to either question is actually \u201cyes\u201d, please let me know by responding to that question.</p>\n<p>My questions were intended to form a complete pair. Do you use X for rationality, and if not do you use anything other than X?</p>\n<p><strong>Does EA have some other way of being rational which wasn\u2019t covered by either question?</strong> Or is something else going on?</p>\n<p>My understanding is that rationality is crucial to EA\u2019s mission (of basically applying rationality, math, evidence, etc., to charity \u2013 which sounds great to me) so I think the issue I\u2019m raising is important and relevant.</p>\n", "user": {"username": "Elliot Temple"}}, {"_id": "Md9hpuefGbiKfCRjG", "title": "The Significance, Persistence, Contingency Framework (William MacAskill, Teruji Thomas and Aron Vallinder)", "postedAt": "2022-10-14T09:24:24.833Z", "htmlBody": "<p>This is a new Global Priorities Institute technical report by William MacAskill, Teruji Thomas and Aron Vallinder.</p><h2>Introduction</h2><p>The world, considered from beginning to end, combines many different features, or states of affairs, that contribute to its value.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3r545d12ntb\"><sup><a href=\"#fn3r545d12ntb\">[1]</a></sup></span>&nbsp;The value of each feature can be factored into its <i>significance</i>\u2014its average value per unit time\u2014and its <i>persistence</i>\u2014how long it lasts. Sometimes, though, we want to ask a further question: how much of the feature\u2019s value can be attributed to a particular agent\u2019s decision at a particular point in time (or to some other originating event)? In other words, to what extent is the feature\u2019s value <i>contingent</i> on the agent\u2019s choice? For this, we must also look at the counterfactual: how would things have turned out otherwise?</p><p>In this note, we give a way to formalise the ideas of significance, persistence, and contingency. We call this the SPC framework. It must be emphasised that the main goal is to help estimate the instrumental value (typically interpreted as the <i>expected</i> value) of an event, compared to the relevant counterfactual. There may be different ways to do this, suitable for different situations. It seems clear to us that thinking in terms of significance, persistence, and contingency can be a useful heuristic when thinking about the importance of historical events and the consequences of our choices for the long-run future; see MacAskill\u2019s <i>What We Owe the Future</i> for many applications. This note shows how that heuristic can be related to a more formal theory of evaluation. Doing so helps to clarify when the heuristic can be useful; gives some discipline to its application; and opens the door to its use in more formal analyses.&nbsp;</p><p>Still, different variations on the SPC framework might be more useful or perspicacious in some situations; in others, the SPC framework may only be useful for evaluating some aspects of an event; and sometimes one may want to use a different framework altogether. So while we hope that this note will be useful to decision-makers, we (of course) make no claim to have the final word.&nbsp;</p><p>In section 2, we explain the framework as it appears in <i>What We Owe the Future</i>. We\u2019ll sketch an alternative approach in section 3. Section 4 discusses how the SPC framework can contribute to the overall evaluation of our options. It explains the \u2018ITN framework\u2019, which evaluates problems in terms of their <i>importance, tractability,</i> and <i>neglectedness</i>, and indicates how the two frameworks can be combined.</p><h3><a href=\"https://globalprioritiesinstitute.org/the-significance-persistence-contingency-framework-william-macaskill-teruji-thomas-global-priorities-institute-university-of-oxford-and-aron-vallinder-forethought-foundation-for-global-prioriti/\">Read the rest of the paper</a></h3><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3r545d12ntb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3r545d12ntb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For these purposes, a \u2018feature\u2019 can be any source of value (or disvalue). For example, one could consider a particular headache, the future existence of human civilisation, or the prevalence of QWERTY keyboards. A feature may contribute by having intrinsic value, or by entailing or promoting other things with intrinsic value, such as human welfare.</p></div></li></ol>", "user": {"username": "Global Priorities Institute"}}, {"_id": "aYQS5nKuFLmcyRPd6", "title": "A stubborn unbeliever finally gets the depth of the AI alignment problem", "postedAt": "2022-10-13T15:16:28.679Z", "htmlBody": "", "user": {"username": "aelwood"}}, {"_id": "ngFdWkELusg5gAKdp", "title": "When should you defer to expertise? A useful heuristic", "postedAt": "2022-10-13T13:59:37.898Z", "htmlBody": "<p>One important aspect of our lives as we search  for knowledge is knowing what and who to defer to, as we usually must take a lot of our knowledge on faith in expertise. However, how should you defer on issues, given that certain areas could be vastly wrong?</p>\n<p>Well, I'll introduce some heuristics from Chris Hallquist that might help you to defer better.</p>\n<p>They can be used in the following ways:</p>\n<ol>\n<li>\n<p>When an EA defers to a non-EA expert, or the movement as a whole defers to non-EA expertise.</p>\n</li>\n<li>\n<p>When a less-knowledgeable EA defers to a more knowledgeable EA on something.</p>\n</li>\n<li>\n<p>When someone outside a field defers to an insider expert.</p>\n</li>\n</ol>\n<h1>Now, before I begin, I want to list caveats here:</h1>\n<ol>\n<li>\n<p>The heuristic only applies to non-moral fields.</p>\n</li>\n<li>\n<p>The heuristic assumes the field is sound. In an upcoming post, I'll talk about signs a field may have unsound bases, and what to expect there.</p>\n</li>\n<li>\n<p>It's not a replacement for EV calculations.</p>\n</li>\n<li>\n<p>If you're in a field or plan to work in a cause area, it's best to replace this heuristic with this post by Emrik: The underappreciated value of original thinking below the frontier.</p>\n</li>\n</ol>\n<p><a href=\"https://www.lesswrong.com/posts/KmkZriGwkn2vDx8gB/the-underappreciated-value-of-original-thinking-below-the\">https://www.lesswrong.com/posts/KmkZriGwkn2vDx8gB/the-underappreciated-value-of-original-thinking-below-the</a></p>\n<p>But let's begin.</p>\n<h1>Conclusion</h1>\n<blockquote>\n<p>When the data show an overwhelming consensus in favor of one view (say, if the number of dissenters is less than the Lizardman's Constant), this almost always ought to swamp any other evidence a non-expert might think they have regarding the issue.</p>\n</blockquote>\n<blockquote>\n<p>When a strong but not overwhelming majority of experts favor one view, non-experts should take this as strong evidence in favor of that view, but there's a greater chance that evidence could be overcome by other evidence (even from a non-expert's point of view).</p>\n</blockquote>\n<blockquote>\n<p>When there is only barely a majority view among experts, or no agreement at all, this is much less informative than the previous two conditions. It may indicate agnosticism is the appropriate attitude, but in many cases non-experts needn't hesitate before having their own opinion.</p>\n</blockquote>\n<blockquote>\n<p>Expert opinion should be discounted when their opinions could be predicted solely from information not relevant to the truth of the claims. This may be the only reliable, easy heuristic a non-expert can use to figure out a particular group of experts should not be trusted.</p>\n</blockquote>\n<h2>What about selection bias?</h2>\n<p>Emrik raised a concern about deferring to experts in that the most informed people are also selection biased to believe that their field is sound, from his post here: The Paradox of Expert Opinion, link below.</p>\n<p><a href=\"https://www.lesswrong.com/posts/S6Qcf5EgX5zAozTAa/the-paradox-of-expert-opinion\">https://www.lesswrong.com/posts/S6Qcf5EgX5zAozTAa/the-paradox-of-expert-opinion</a></p>\n<p>This is why it's so rare for the 1st, strongest condition to hold in practice. Not always, but unless selection effects are controlled for, it's going to produce wrong results.</p>\n<h1>So what's next?</h1>\n<p>This is hopefully a useful resource so that you can defer quite a bit better and with better reasons than before this post.</p>\n", "user": {"username": "Sharmake"}}, {"_id": "FeBfELZgQsKbhMPAS", "title": "EAGxVirtual: A virtual venue, timings, and other updates  ", "postedAt": "2022-10-13T13:22:39.234Z", "htmlBody": "<p><a href=\"https://www.eaglobal.org/events/eagxvirtual-2022/\"><u>EAGxVirtual</u></a> is fast approaching. This post covers updates from the team, including demographics data, dates and times, content, venue, and unique features.</p><h3>Transcending Boundaries</h3><p>We have already received more than 600 applications from people representing over 60 countries, making our conference one of the most geographically diverse EA events ever. For many of them, it would be their first conference. If you are a highly-engaged EA, you can make a difference by being responsive to requests from first-time attendees.&nbsp;</p><p>The map below shows the geographical distribution of the participants:</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995062/mirroredImages/FeBfELZgQsKbhMPAS/h5shyqxirq1cqqheek9i.png\"><br>Still, we would love to see more applications. If you know someone who you think should attend the conference, please encourage them to apply by sending them&nbsp;<a href=\"https://creatorapp.zohopublic.com/centreforeffectivealtruism/ea/form-perma/Referral_form/OgbvyQ3X4KgE7WfgDCjNp5v4srT2qaCPY5Y8DGdt9HzVBg90RFTBtpYwZDReYHGm8spD7EbQrD8yQfmK3fwWuXBanTQ8KtmT4Mkm\"><u>this link</u></a>!&nbsp;</p><p><strong>The deadline for applications is 8:00 am UTC on Wednesday, 19 October.</strong></p><p><a href=\"https://www.eaglobal.org/events/eagxvirtual-2022/\"><strong><u>Apply here</u></strong></a><strong> if you haven\u2019t already.</strong></p><h3>Dates and times</h3><p>The conference will be taking place from 5 pm UTC on Friday, October 21st, until 11:59 pm UTC on Sunday, October 23rd.</p><ul><li>&nbsp;Friday will feature group meetups and an opening session.</li><li>On Saturday and Sunday, the sessions will start at 8 am UTC. We try to make the keynote sessions accessible to people from different time zones but the recordings will be available if you cannot make it.</li><li>There will be a break in the program on Sunday between 3 am and 8 am UTC.</li></ul><h3>Content: what to expect</h3><p>We are working hard on the program. Here are the types of content you might expect, beyond the usual talks and workshops:</p><ul><li>Career stories sessions</li><li>Office Hours hosted by EA orgs</li><li>Q&amp;As and fireside chats</li><li>Group meetups and icebreakers</li><li>Lightning talks from the attendees</li><li>Participant-driven meetups on Gather.Town</li></ul><p>We have confirmed speakers from Charity Entrepreneurship, GFI Asia, Manifold Markets, Spark Wave, CEA, GovAI, HLI, and other organizations. Some exciting confirmed speakers: <a href=\"https://forum.effectivealtruism.org/users/spencerg\">Spencer Greenberg</a>, <a href=\"https://forum.effectivealtruism.org/users/sethbaum\">Seth Baum</a>, <a href=\"https://forum.effectivealtruism.org/users/varun_deshpande\">Varun Deshpande</a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/users/ben-garfinkel\"><u>Ben Garfinkel</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/users/davidmanheim\"><u>David Manheim</u></a>, and others!</p><p>The tentative schedule will be available on the Swapcard app at the end of the week, but it is subject to slight changes in the leadup to the conference.&nbsp;</p><h3>Virtual venue</h3><p>Our main content and networking platform for the conference is the Swapcard. We will share access to the app with all the attendees a week before the conference and provide guidance on how to use it and get the most out of the conference.</p><p>We also collaborate with&nbsp;<a href=\"https://forum.effectivealtruism.org/groups/MCtKD7oex9jhsAWvD\"><u>EA Gather.Town</u></a> to make an always-available virtual space for the attendees to spark more connections and unstructured discussions throughout the conference. There will be spots for private meetings and rooms you can book for group meetups: just like the real conference venue!&nbsp;</p><p>There will be sessions led by&nbsp;<a href=\"https://forum.effectivealtruism.org/groups/e4ijnQ45gevf8Qcck\"><u>EA Virtual Reality</u></a> as well! Gather.Town and EA VR are optional but are exciting opportunities for those who want to experiment with formats beyond usual live streams and calls.&nbsp;</p><h3>Call for volunteers</h3><p>We think volunteering for such events can be a very fulfilling experience, and organizers depend on motivated people like you to support us and make the best out of this event. We are currently looking for volunteers to help in a wide range of positions, including chat management, moderators, emcees, and more.&nbsp;<strong>If you attending the conference, please consider&nbsp;</strong><a href=\"https://forms.gle/77auFZ5GntnHcjaKA\"><strong><u>becoming a volunteer</u></strong></a><strong>.</strong></p><p>We are very excited about the event and hope to see you there!</p><p>EAGxVirtual Team:&nbsp;<a href=\"https://forum.effectivealtruism.org/users/alex_berezhnoy\"><u>Alex</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/users/jordan-pieters\"><u>Jordan</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/users/dion-1\"><u>Dion</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/users/amine\"><u>Amine</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/users/marka-ellertson\"><u>Marka</u></a>, and&nbsp;<a href=\"https://forum.effectivealtruism.org/users/olliebase\"><u>Ollie</u></a></p>", "user": {"username": "Alex_Berezhnoy"}}, {"_id": "mChQDZRsmGMKrSeAB", "title": "How should we build methods and tools to prepare for future pandemics?", "postedAt": "2022-10-13T12:47:35.967Z", "htmlBody": "<blockquote><p>There are several models that we could take to develop methods and tools in order to prepare for future pandemics. We should talk about them.</p></blockquote><p>There are lots of different communities of people interested in \"pandemic preparedness\". Often it seems like we are all talking past each other and that is a missed opportunity. Personally, I think the questions we all want to answer are very similar but even if they are not there should be a lot to learn/share.</p><blockquote><p>I recently came across <a href=\"https://pol.is/\">pol.is</a> which is a platform designed for these sort of conversations. From a first pass it seems great. Below is a conversation I\u2019ve started aiming to find out what people think. Currently the questions are a pretty eclectic mix that I put together but you can and should submit your own.</p></blockquote><p>The linked post provides some context (and perhaps makes my stance clear) for the conversation so feel free to jump right in instead (and add your own viewpoints). The general audience so far has been academic and practising epidemiologists (which is my community of practice).</p><p>Go straight to the conversation: <a href=\"https://pol.is/6ukkcvfbre\">https://pol.is/6ukkcvfbre</a>&nbsp;</p><p>Go straight to the results: <a href=\"https://pol.is/report/r64ajcsmp9butzxhzj44c\">https://pol.is/report/r64ajcsmp9butzxhzj44c</a>&nbsp;</p>", "user": {"username": "Sam Abbott"}}, {"_id": "e3kLF5qPE8cRqsF8v", "title": "Sixty years after the Cuban Missile Crisis, a new era of global catastrophic risks", "postedAt": "2022-10-13T11:25:26.658Z", "htmlBody": "<p>Linkpost for a <a href=\"https://thebulletin.org/2022/10/sixty-years-after-the-cuban-missile-crisis-how-to-face-a-new-era-of-global-catastrophic-risks/\">short op-ed</a> I wrote in the <i>Bulletin of the Atomic Scientists </i>in light of the upcoming 60th anniversary of the Cuban Missile Crisis and President Biden's <a href=\"https://www.reuters.com/world/biden-cites-cuban-missile-crisis-describing-putins-nuclear-threat-2022-10-07/\">recent comments</a> that \"For the first time since the Cuban Missile Crisis, we have a direct threat to the use of nuclear weapons, if in fact things continue down the path they\u2019d been going.\"</p><p>Was asked to keep it mostly nuclear (i.e., only some narrow AI and no bio, which was in my first draft), but managed to keep in some broader points about technology development and deployment, like \"artificial intelligence and other new technologies, if thoughtlessly deployed, could increase the risk of accidents and miscalculation even further.\"</p><p>First couple of paragraphs:</p><blockquote><p>This month marks the 60th anniversary of the Cuban Missile Crisis. For two tense weeks from October 16 to October 29, 1962, the United States and the Soviet Union <a href=\"https://thebulletin.org/2012/10/remembering-the-cuban-missile-crisis/\">teetered on the brink of nuclear war</a>. Sixty years later, tensions between the world\u2019s major militaries are uncomfortably high once again.</p><p>In recent weeks, Russian President Vladimir Putin\u2019s nuclear-charged threats to use \u201c<a href=\"https://www.nytimes.com/2022/10/01/world/europe/washington-putin-nuclear-threats.html\">all available means</a>\u201d in the Russo-Ukrainian war have again raised the prospect of nuclear war. And on October 6, US President Joe Biden <a href=\"https://www.reuters.com/world/biden-cites-cuban-missile-crisis-describing-putins-nuclear-threat-2022-10-07/\">reportedly told a group of Democratic donors</a>: \u201cFor the first time since the Cuban Missile Crisis, we have a direct threat to the use of nuclear weapons, if in fact things continue down the path they\u2019d been going.\u201d</p><p>Any uncontrolled escalation of these existing conflicts could end in global catastrophe, and the history of the Cuban Missile Crisis suggests that such escalation may be more likely to happen through miscalculation and accidents. Lists of <a href=\"https://futureoflife.org/background/nuclear-close-calls-a-timeline/\">nuclear close calls</a> show the variety of pathways that could have led to disaster during the Cuban crisis. Famously, Soviet naval officer <a href=\"https://thebulletin.org/2018/09/a-posthumous-honor-for-the-man-who-saved-the-world/#post-heading\">Vasili Arkhipov vetoed the captain of a nuclear submarine</a> who wanted to launch a nuclear-armed torpedo in response to what turned out to be non-lethal depth charges fired by US forces; had Arkhipov not been on this particular vessel, the captain might have had the two other votes he needed to order a launch.</p><p>Today, artificial intelligence and other new technologies, if thoughtlessly deployed, could <a href=\"https://www.sipri.org/publications/2020/other-publications/artificial-intelligence-strategic-stability-and-nuclear-risk\">increase the risk</a> of accidents and miscalculation even further [...]&nbsp;</p></blockquote>", "user": {"username": "christian.r"}}, {"_id": "MBmFuoHgnow59zGfy", "title": "CNAS report: 'Artificial Intelligence and Arms Control'", "postedAt": "2022-10-13T08:35:27.937Z", "htmlBody": "<p>By Paul Scharre &amp; Megan Lamberth, Center for a New American Security.</p><p>From the introduction:</p><blockquote><p>Advances in artificial intelligence (AI) pose immense opportunity for militaries around the world. With this rising potential for AI-enabled military systems, some activists are sounding the alarm, calling for restrictions or outright bans on some AI-enabled weapon systems.1 Conversely, skeptics of AI arms control argue that as a general-purpose technology developed in the civilian context, AI will be exceptionally hard to control.2 AI is an enabling technology with countless nonmilitary applications; this factor differentiates it from many other military technologies, such as landmines or missiles.3 Because of its widespread availability, an absolute ban on all military applications of AI is likely infeasible. There is, however, a potential for prohibiting or regulating specific use cases.&nbsp;<br><br>The international community has, at times, banned or regulated weapons with varying degrees of success. In some cases, such as the ban on permanently blinding lasers, arms control has worked remarkably well to date. In other cases, however, such as attempted limits on unrestricted submarine warfare or aerial bombardment of cities, states failed to achieve lasting restraint in war. States\u2019 motivations for controlling or regulating weapons vary. States may seek to limit the diffusion of a weapon that is particularly disruptive to political or social stability, contributes to excessive civilian casualties, or causes inhumane injury to combatants.&nbsp;<br><br>This paper examines the potential for arms control for military applications of AI by exploring historical cases of attempted arms control, analyzing both successes and failures. The first part of the paper explores existing academic literature related to why some arms control measures succeed while others fail. The paper then proposes several criteria that influence the success of arms control.4 Finally, it analyzes the potential for AI arms control and suggests next steps for policymakers. Detailed historical cases of attempted arms control\u2014from ancient prohibitions to modern agreements\u2014can be found in appendix A.&nbsp;<br><br><strong>History teaches us that policymakers, scholars, and members of civil society can take concrete steps today to improve the chances of successful AI arms control in the future</strong>. These include taking policy actions to shape the way the technology evolves and increasing dialogue at all levels to better understand how AI applications may be used in warfare. Any AI arms control will be challenging. There may be cases, however, where arms control is possible under the right conditions, and small steps today could help lay the groundwork for future successes.</p></blockquote><p>See also summary <a href=\"https://twitter.com/paul_scharre/status/1580198294587723776\">Twitter thread</a> by Paul Scharre.</p>", "user": {"username": "MatthijsMaas"}}, {"_id": "MYuoB8eyvnMPmmfA2", "title": "Pineapple Operations is expanding to include all operations talent (Oct '22 Update)", "postedAt": "2022-10-13T03:59:45.121Z", "htmlBody": "<p><a href=\"http://pineappleoperations.org\">Pineapple Operations</a> aims to streamline the operations talent search and save both employers\u2019 and candidates\u2019 time. We provide a <a href=\"http://pineappleoperations.org\">public directory</a> of people interested in operations work in the EA ecosystem.</p><h2><strong>Changes to our \u201coperations\u201d&nbsp;</strong></h2><h3>Our database now includes all operations talent</h3><p>We\u2019re excited to announce that we\u2019ve expanded our database to include all operations talent, with <strong>100+ candidates </strong>listed so far! Our goal is for the database to be a central, easy-to-use and up-to-date database. Let us know in the comments or here how we could do that better! We are trying out a minimum viable product, so anyone can list themselves on the database (we remove spam).</p><p>We previously only listed PA/ExA talent.</p><p><a href=\"https://airtable.com/shr2uaKcpsMNx3fc6\">List yourself</a> | <a href=\"https://pineappleoperations.org\">Search the database</a></p><h3>We are not offering any other services as of now</h3><p>Due to capacity constraints we are not offering any personalised services or candidate vetting at this time. We think they are likely to be valuable, but unfortunately cannot do them ourselves.</p><p>We think these services are potentially really valuable, especially for organisations with limited capacity and would experiment more in this space if we had capacity.</p><ul><li>If you\u2019re interested and have prior experience in recruiting and/or operations please <a href=\"mailto:info@pineappleoperations.org\">reach out</a>. We have several ideas we\u2019d like to test (and several potential clients who would like them!).</li><li>Others experimenting on their own or through other projects with outsourcing operations recruiting should also <a href=\"mailto:info@pineappleoperations.org\">get in touch</a> to see if we can work together</li></ul><p>Note: We will be posting two short guides on hiring / getting hired for PA/ExAs &amp; operations staff, which we hope will help in the meantime.</p><h3>New Team</h3><p><a href=\"mailto:info@pineappleoperations.org\">Vaidehi Agarwalla</a> and <a href=\"mailto:info@pineappleoperations.org\">Alexandra Malikova</a> run Pineapple Operations. We are grateful to Holly Morgan and previous team members for founding and working on earlier versions of the project.</p><h2>How are we doing so far? What can we do better?&nbsp;</h2><p>If you\u2019ve found a position or hired someone you found on our database, <a href=\"https://airtable.com/shrUAAWsLHrAPfa58\">please let us know</a>!</p><p>Since we are in the beta testing phase, we would also love to know how we can improve. You can contact us at <a href=\"mailto:info@pineappleoperations.org\">info@pineappleoperations.org</a>, share in the comments below or leave feedback anonymously via our <a href=\"https://airtable.com/shrdqwp66eRYKEERa\">feedback form</a>.</p><hr><figure class=\"image image_resized\" style=\"width:27.38%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1eaa6f4bc8ae5a97c33eae5ec1f9264cd0a2b4e552e0969.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1eaa6f4bc8ae5a97c33eae5ec1f9264cd0a2b4e552e0969.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1eaa6f4bc8ae5a97c33eae5ec1f9264cd0a2b4e552e0969.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1eaa6f4bc8ae5a97c33eae5ec1f9264cd0a2b4e552e0969.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1eaa6f4bc8ae5a97c33eae5ec1f9264cd0a2b4e552e0969.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1eaa6f4bc8ae5a97c33eae5ec1f9264cd0a2b4e552e0969.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1eaa6f4bc8ae5a97c33eae5ec1f9264cd0a2b4e552e0969.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1eaa6f4bc8ae5a97c33eae5ec1f9264cd0a2b4e552e0969.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1eaa6f4bc8ae5a97c33eae5ec1f9264cd0a2b4e552e0969.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1eaa6f4bc8ae5a97c33eae5ec1f9264cd0a2b4e552e0969.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1eaa6f4bc8ae5a97c33eae5ec1f9264cd0a2b4e552e0969.png/w_1094 1094w\"></figure>", "user": {"username": "vaidehi_agarwalla"}}, {"_id": "b8TgXCi9RRZz4Jtnp", "title": "You are better at math (and alignment) than you think", "postedAt": "2022-10-13T03:11:41.697Z", "htmlBody": "", "user": {"username": "trevorw96"}}, {"_id": "eQgLREsGCo252HRmo", "title": "Scout Mindset Poster", "postedAt": "2022-10-13T01:53:57.690Z", "htmlBody": "<p>(Edits: shortened the text on the poster according to AllAmericanBreakfast's recommendations, and added an editable link.)</p><p>I made a thing!</p><p>I recently finished reading the Scout Mindset by Julia Galef, and I particularly loved the section on thought experiments that can help wrestle you out of the soldier mindset. I remember thinking, \"man, I wish I had a poster with all these thought experiments, so I could put it above my desk and really hone those skills.\" So, I decided to go on Canva and whip something up (keep in mind, I have absolutely no graphic design background, so if someone wants to remake this and make it look better, that would be great!)<img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8ba25cd96ae327d498b8383e70ad92e49e1febfe92a7213a.jpg\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8ba25cd96ae327d498b8383e70ad92e49e1febfe92a7213a.jpg/w_240 240w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8ba25cd96ae327d498b8383e70ad92e49e1febfe92a7213a.jpg/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8ba25cd96ae327d498b8383e70ad92e49e1febfe92a7213a.jpg/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8ba25cd96ae327d498b8383e70ad92e49e1febfe92a7213a.jpg/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8ba25cd96ae327d498b8383e70ad92e49e1febfe92a7213a.jpg/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8ba25cd96ae327d498b8383e70ad92e49e1febfe92a7213a.jpg/w_1440 1440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8ba25cd96ae327d498b8383e70ad92e49e1febfe92a7213a.jpg/w_1680 1680w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8ba25cd96ae327d498b8383e70ad92e49e1febfe92a7213a.jpg/w_1920 1920w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8ba25cd96ae327d498b8383e70ad92e49e1febfe92a7213a.jpg/w_2160 2160w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8ba25cd96ae327d498b8383e70ad92e49e1febfe92a7213a.jpg/w_2304 2304w\"></p><p>If anyone has any constructive criticism, feel free to let me know. Otherwise, you can order a print using this Canva link: <a href=\"https://www.canva.com/design/DAFO1-Z1LEE/KdyOWmWGZw14QN8h6Xiujg/edit?utm_content=DAFO1-Z1LEE&amp;utm_campaign=designshare&amp;utm_medium=link2&amp;utm_source=sharebutton\">https://www.canva.com/design/DAFO1-Z1LEE/KdyOWmWGZw14QN8h6Xiujg/edit?utm_content=DAFO1-Z1LEE&amp;utm_campaign=designshare&amp;utm_medium=link2&amp;utm_source=sharebutton</a>&nbsp;</p>", "user": {"username": "Anthony Fleming"}}, {"_id": "fRgN5qeSFALwDDD7u", "title": "Niceness is unnatural", "postedAt": "2022-10-13T01:30:03.656Z", "htmlBody": "", "user": {"username": "So8res"}}, {"_id": "iL24ikM4QjGf4Nz8A", "title": "In extremely high-stakes scenarios, it's ok not to maximise expected utility", "postedAt": "2022-10-13T01:13:24.583Z", "htmlBody": "<p>EA is associated with maximising <i>expected</i> utility. Which leads to counterintuitive recommendations --- such &nbsp;as (repeatedly) pressing a button that destroys the world with 99.99% chance but has a 1% chance of creating a (10 000+<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\epsilon\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u03f5</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>)-times better world. I claim that:</p><ul><li><i>the assumptions underlying expected utility maximisation do not apply to these scenarios</i>,</li><li>it's ok if you don't want to press that button,</li><li>it's ok if EA is not about pressing that button,</li><li>[and aaaaaah, for f...s sake, stop associating EA with pressing that button].<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdjllpac8b2j\"><sup><a href=\"#fndjllpac8b2j\">[1]</a></sup></span></li></ul><hr><h1>More detailed explanation:</h1><p>An argument for maximising expected utility goes something like this:</p><p>Suppose you were risk averse --- for example, suppose you prefferred (A) a &nbsp;&gt;90% chance of $1 to (B) a 1% chance of $1000. Well, then I expose you to a series of 100 bets like this, and [apply probability theory] suddenly we are comparing (A') a &gt;90% chance of $90 with (B') a &gt;90% chance of $100K. Presumably you don't feel so happy about choosing (A) now, eh? Maybe you want to go back on this risk aversion thing, and start maximising expected utility with every decision, like a proper Utilitarian?</p><p>However, this argument critically relies on <i>repeating</i> the bets. If there is only ever one bet, or a small number of them, Central Limit Theorem <i>does not apply!</i></p><p>In particular:</p><ul><li>In your personal life, there is nothing incoherent about being risk averse with respect to your overall happiness, taken over your whole life.</li><li>In terms of near-term social impact, there is nothing incoherent about being risk averse with respect to the planetary scale.</li><li>In longtermism, there is nothing incoherent about being risk averse with respect to the scale of the future trajectories of the universe.</li></ul><p>[Also: uuuhm, maaaybe don't rely on things that you don't understand, particularly if they start telling you to jump off a cliff?]</p><hr><p><i>Disclaimer:</i> There is a hypothetical version of this post that includes references, is polished, isn't a rant, has a better example of risk aversion, makes all the disclaimers such as \"yeah yeah, I know most people's opinions are more nuanced than what I make them\", and that I ran through <a href=\"https://www.facebook.com/groups/OMfCT/permalink/3087097268271837/\">Granterly </a>a few times. Sadly, that version is less fun, more work, and doesn't exist. So apologies to all offended readers.</p><p><i>Edit:</i> changed the title to better reflect the main claim.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndjllpac8b2j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdjllpac8b2j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Insert countless links here. Including by philosophers much smarter than me, who really should know better.</p></div></li></ol>", "user": {"username": "Pivocajs"}}, {"_id": "F9AJpjg5D9qyYA9DM", "title": "Let me blind myself to Forum post authors", "postedAt": "2022-10-13T13:00:56.403Z", "htmlBody": "<p><strong>TLDR:</strong> I'd like to have the ability to hide the author(s) of all the posts on my news feed so that I see can read the content without knowing about the source. I have some intuitions for why this might be good but mainly I think adding the feature is easy and it lets us check whether or not it's good.</p><p><strong>Is this the right place to post this? </strong>I can't remember if there's an obvious place to suggest forum features (I think there might have been a survey a while back but I might have imagined it). Since the bulk of this post is closer to \"Why you might want this\" than \"Why you should add this to the forum you maintain\" I think it's better posted publically. Also it seems likely this can be quickly made with a chrome extension<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6ismvwb7r2k\"><sup><a href=\"#fn6ismvwb7r2k\">[1]</a></sup></span>.</p><h1>The feature</h1><p>I'd like to be able to blur out author names wherever they appear like this...</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/72f8bdecc2f15ca907a450a956076243530fa4aadc09db2c.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/72f8bdecc2f15ca907a450a956076243530fa4aadc09db2c.png/w_160 160w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/72f8bdecc2f15ca907a450a956076243530fa4aadc09db2c.png/w_320 320w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/72f8bdecc2f15ca907a450a956076243530fa4aadc09db2c.png/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/72f8bdecc2f15ca907a450a956076243530fa4aadc09db2c.png/w_640 640w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/72f8bdecc2f15ca907a450a956076243530fa4aadc09db2c.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/72f8bdecc2f15ca907a450a956076243530fa4aadc09db2c.png/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/72f8bdecc2f15ca907a450a956076243530fa4aadc09db2c.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/72f8bdecc2f15ca907a450a956076243530fa4aadc09db2c.png/w_1280 1280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/72f8bdecc2f15ca907a450a956076243530fa4aadc09db2c.png/w_1440 1440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/72f8bdecc2f15ca907a450a956076243530fa4aadc09db2c.png/w_1594 1594w\"><figcaption>One example of how this could look</figcaption></figure><p>(It could also be black boxes of regular size or random strings of nonsense characters like this \u23c1\u2291\u27df\u2307 \u27df\u2307 \u22cf\u235c\u22cf\u2307\u27d2\u22cf\u2307\u27d2<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefb8ncux2am6u\"><sup><a href=\"#fnb8ncux2am6u\">[2]</a></sup></span>)</p><p>I'd like to be able to click on names to reveal them but would be ok with an easy toggle for the feature.</p><p>A larger project would be for folks at CEA to run an AB test where post authors are hidden for one group of users and not hidden for another and publish click through rates and karma from different groups. I think there will be a difference I'm not sure which setting I'd advocate for as a norm though (I'll go into why below).</p><p>EDIT: Since a lot of people are suggesting extensions / greaterwrong style solutions. One benefit of an integrated into the forum solution is the ability to separate blinded Karma from unblinded Karma (even if this is only on the back end). I'm mostly interested in what the frontpage looks like when karma is driven only by post content and not by authorship.&nbsp;</p><h1>Why I might (or might not) want this</h1><p><strong>I don't think I say anything super surprising in this section, you're welcome to skip it.</strong></p><p>It seems pretty obvious that the authorship of a post affects my click through rate. There are good reasons for this. If I recognise a name as someone who I've read content from before and found that content useful I think it's more likely I'll find their new content useful too. This is the same logic that led me to watch the new Game of Thrones spin off, buy a second pair of Levi jeans, and listen to the latest Dodie album.</p><p>However this policy makes me less great at exploring new sources of insight. Historically I'm significantly more likely to re-read Harry Potter and the Methods of Rationality than pick up any specific book on my to-read list<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefuzkng7s9e4r\"><sup><a href=\"#fnuzkng7s9e4r\">[3]</a></sup></span>.&nbsp;</p><p>This actually doesn't just affect me but also, through the karma system, everyone else.<i> FYI this is the factor which made me decide to post this</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefla52q2m5hof\"><sup><a href=\"#fnla52q2m5hof\">[4]</a></sup></span>. I can think of a few examples off hand where I opened a post I might not have otherwise because I know the author personally and want to know what they're up to, I then end up upvoting the post. If I extrapolate my policy to everyone else on the forum I'd expect posts from authors who have a lot of friends in the community to do better than the exact same post posted anonymously<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefn8dcy8r3wm\"><sup><a href=\"#fnn8dcy8r3wm\">[5]</a></sup></span>. I could train myself to stop doing this (and suggest others do so) but it would be a lot easier to just anonymise posts.</p><p>Separately once I open a post I'd guess there are a bunch of associations happening at a level of my thinking I'm not aware of when I read a name I recognise at the top of a post (ala <a href=\"https://en.wikipedia.org/wiki/Horn_effect\">Halo/Horn</a> effect). I mostly guessing about what these would be but I expect people I like / have agreed with before get more of the benefit of the doubt and are engaged with less critically. This would mean I'd be more open to novel or unintuitive seeming proposals from people I know who are already established in the community. I think this effect still exists when I do the obvious thing and approach posts with an open mind, you typically can't fix a bias by knowing about it.</p><p>Here are some other reasons I might or might not want this feature:</p><ul><li>Anonymising the forum lets EA community builders see it a bit more like newcomers to the community would (although we can't remove your jargon dictionary)</li><li>Relying on just titles might incentivise better titles from authors</li><li>Some posts might be upvoted purely because it's valuable for the community to know what influential person/org in EA is doing at the moment. If the posts are anonymised it might make these posts less visible which might be bad for coordination in the community</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6ismvwb7r2k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6ismvwb7r2k\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Here's some code which when run in the chrome console will blur authors on the front page, it might not work forever.</p><pre><code>arr = Array.prototype.slice.call(document.getElementsByClassName(\"PostsUserAndCoauthors-lengthLimited\"))\narr.forEach(v =&gt; {\n       v.style.color = \"transparent\"  \n       v.style.textShadow = \"0 0 10px rgba(0,0,0,0.5)\"\n})</code></pre></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnb8ncux2am6u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefb8ncux2am6u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I actually spent an embarrassingly long time looking for nice looking alien character translators and these aren't up to my standard but sadly I couldn't find any that were aesthetic enough so maybe we should stick with blur.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnuzkng7s9e4r\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefuzkng7s9e4r\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Probably the most embarrassing real world example in this post&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnla52q2m5hof\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefla52q2m5hof\">^</a></strong></sup></span><div class=\"footnote-content\"><p>E.g I consider this factor to be sufficient to suggest this feature and experiment but I don't know if it's necessary (in other words I don't know if none of the other factors would have been sufficient on their own).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnn8dcy8r3wm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefn8dcy8r3wm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Actually I tend to click on anonymous posts because I'm curious about why they're anonymous. I'd expect a post would do better with a popular author than it would under a pseudonym.</p></div></li></ol>", "user": {"username": "Will Payne"}}, {"_id": "CjifvmM3Kjn3beMyB", "title": "A strange twist on the road to AGI", "postedAt": "2022-10-12T23:27:08.721Z", "htmlBody": "<p>By now, many people will have seen my (possibly infamous) posts written for the Future Fund worldview prize, where I argue that AGI won't happen any time soon.</p><p>To prove that life is stranger than fiction, this morning I saw an article which could change everything. Researchers in Melbourne, Australia have built a small network of real neurons which can learn the game of Pong through simple reinforcement learning. This configuration might some day be able to overcome the limitations I identify in artificial neural networks. For example, networks of real neurons might be able to learn real symbolic processes.&nbsp;</p><p>Of course it is way too early to make predictions about what these networks will be capable of in the future. I'm just very excited to see where this goes.</p><p><a href=\"https://www.smh.com.au/national/scientists-teach-brain-cells-in-a-dish-to-play-pong-opening-potential-path-to-powerful-ai-20221012-p5bp32.html\">https://www.smh.com.au/national/scientists-teach-brain-cells-in-a-dish-to-play-pong-opening-potential-path-to-powerful-ai-20221012-p5bp32.html</a></p><p><a href=\"https://www.cell.com/neuron/fulltext/S0896-6273(22)00806-6?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0896627322008066%3Fshowall%3Dtrue\">https://www.cell.com/neuron/fulltext/S0896-6273(22)00806-6?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0896627322008066%3Fshowall%3Dtrue</a></p>", "user": {"username": "cveres"}}, {"_id": "hJXX3Ky9SNYFRqdes", "title": "[MLSN #6]: Transparency survey, provable robustness, ML models that predict the future", "postedAt": "2022-10-12T20:51:58.477Z", "htmlBody": "<p><i>You can </i><a href=\"https://newsletter.mlsafety.org/\"><i>subscribe to the newsletter here</i></a><i>, follow the newsletter on </i><a href=\"https://twitter.com/ml_safety\"><i>twitter</i></a><i> here, or join the subreddit </i><a href=\"https://www.reddit.com/r/mlsafety/\"><i>here</i></a><i>.</i></p><hr><p>Welcome to the 6th issue of the ML Safety Newsletter. In this edition, we cover:</p><ul><li>A <strong>review of</strong> <strong>transparency research</strong> and future research directions</li><li>A <strong>large</strong> <strong>improvement to certified robustness</strong></li><li><strong>\u201cGoal misgeneralization\u201d</strong> examples and discussion</li><li>A benchmark for assessing how well <strong>neural networks predict world events</strong> (geopolitical, industrial, epidemiological, etc.)</li><li>Surveys that track <strong>what the ML community thinks about AI risks</strong></li><li><strong>$500,000</strong> in prizes for new benchmarks</li><li>And much more\u2026</li></ul><hr><h1><strong>Monitoring</strong></h1><h3><strong>Transparency Survey</strong></h3><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F758adc0c-a2ea-49fc-8ab0-413171db9634_2462x1598.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F758adc0c-a2ea-49fc-8ab0-413171db9634_2462x1598.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F758adc0c-a2ea-49fc-8ab0-413171db9634_2462x1598.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F758adc0c-a2ea-49fc-8ab0-413171db9634_2462x1598.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F758adc0c-a2ea-49fc-8ab0-413171db9634_2462x1598.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F758adc0c-a2ea-49fc-8ab0-413171db9634_2462x1598.png 1456w\"></a></p><p><i>A taxonomy of transparency methods. Methods are organized according to what part of the model they help to explain (weights, neurons, subnetworks, or latent representations). They can be intrinsic (implemented during training), post hoc (implemented after training), or can rely on a mix of intrinsic and post hoc techniques. \u2018Hazards\u2019 (in orange) are phenomena that make any of these techniques more difficult.</i></p><p>This survey provides an overview of transparency methods: what\u2019s going on inside of ML models? It also discusses future directions, including:</p><p><strong>Detecting deception and eliciting latent knowledge. </strong>Language models are dishonest when they babble common misconceptions like \u201cbats are blind\u201d despite knowing that this is false. Transparency methods could potentially indicate what the model \u2018knows to be true\u2019 and provide a cheaper and more reliable method for detecting dishonest outputs.</p><p><strong>Developing rigorous benchmarks.</strong> These benchmarks should ideally measure the extent to which transparency methods provide actionable insights. For example, if a human implants a flaw in a model, can interpretability methods reliably identify it?</p><p><strong>Discovering novel behaviors.</strong>&nbsp;An ambitious goal of transparency tools is to uncover why<i> </i>a model behaves the way it does on a set of inputs. More feasibly, transparency tools could help researchers identify failures that would be difficult to otherwise anticipate.</p><p><a href=\"https://arxiv.org/abs/2207.13243\"><strong>[Link]</strong></a></p><p>&nbsp;</p><p><strong>Other Monitoring News</strong></p><p><strong>[</strong><a href=\"https://arxiv.org/abs/2206.07682\"><strong>Link</strong></a><strong>]</strong> This paper discusses the sudden emergence of capabilities in large language models. This unpredictability is naturally a safety concern, especially when many of these capabilities could be hazardous or discovered after deployment. It will be difficult to make models safe if we do not know what they are capable of.<br>&nbsp;</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8727294d-397a-42c9-aaf5-3511e1aaa45b_1456x1014.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8727294d-397a-42c9-aaf5-3511e1aaa45b_1456x1014.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8727294d-397a-42c9-aaf5-3511e1aaa45b_1456x1014.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8727294d-397a-42c9-aaf5-3511e1aaa45b_1456x1014.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8727294d-397a-42c9-aaf5-3511e1aaa45b_1456x1014.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8727294d-397a-42c9-aaf5-3511e1aaa45b_1456x1014.png 1456w\"></a></p><p><strong>[</strong><a href=\"https://arxiv.org/abs/2207.08799\"><strong>Link</strong></a><strong>] </strong>This work attributes emergent capabilities to \u201chidden progress\u201d rather than random discovery.</p><p><strong>[</strong><a href=\"https://arxiv.org/abs/2206.13498\"><strong>Link</strong></a><strong>]</strong> Current transparency techniques (e.g., feature visualization) generally fail to distinguish the inputs that induce anomalous behavior</p><hr><h1><strong>Robustness</strong></h1><h3><strong>Mathematical Guarantees of Model Performance</strong></h3><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa846511b-f7d8-45bb-bd29-c9135181f0c1_1600x365.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa846511b-f7d8-45bb-bd29-c9135181f0c1_1600x365.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa846511b-f7d8-45bb-bd29-c9135181f0c1_1600x365.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa846511b-f7d8-45bb-bd29-c9135181f0c1_1600x365.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa846511b-f7d8-45bb-bd29-c9135181f0c1_1600x365.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa846511b-f7d8-45bb-bd29-c9135181f0c1_1600x365.png 1456w\"></a></p><p><i>The current state-of-the-art method for certified robustness (denoised smoothing) combines randomized smoothing with a diffusion model for denoising. In randomized smoothing, an input is perturbed many times and the most commonly assigned label is selected as the final answer, which guarantees a level robust accuracy within a certain perturbation radius. To improve this method, the perturbed inputs are denoised with a diffusion model after the perturbation step so that they can be more easily classified (from </i><a href=\"https://proceedings.neurips.cc/paper/2020/file/f9fd2624beefbc7808e4e405d73f57ab-Paper.pdf\"><i>Salman et al</i></a><i>.)</i></p><p>A central concern in the robustness literature is that empirical evaluations may not give performance guarantees. Sometimes the test set will not find important faults in a model, and some think empirical evidence is insufficient for having high confidence. However, robustness certificates enable definitive claims for how a model will behave in some classes of situations.</p><p>In this paper, Carlini et al. recently improved ImageNet certified robustness by 14 percentage points by simply using a higher-performing diffusion model and classifier for denoised smoothing (the method described in the image above).</p><p><strong>[</strong><a href=\"https://arxiv.org/abs/2206.10550.pdf\"><strong>Link</strong></a><strong>]</strong></p><p>&nbsp;</p><p><strong>Other Robustness News</strong></p><p><strong>[</strong><a href=\"https://arxiv.org/abs/2206.04137\"><strong>Link</strong></a><strong>]</strong> An adversarial defense method for text based on automatic input cleaning.</p><p><strong>[</strong><a href=\"https://arxiv.org/abs/2201.08555.pdf\"><strong>Link</strong></a><strong>]</strong> A text classification attack benchmark that includes 12 different types of attacks.</p><hr><h1><strong>Alignment</strong></h1><h3><strong>Goal Misgeneralization: </strong>Why Correct Specifications Aren\u2019t Enough For Correct Goals</h3><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F73369657-b35b-4fd5-a2b9-36722fe43a1e_1294x1291.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F73369657-b35b-4fd5-a2b9-36722fe43a1e_1294x1291.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F73369657-b35b-4fd5-a2b9-36722fe43a1e_1294x1291.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F73369657-b35b-4fd5-a2b9-36722fe43a1e_1294x1291.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F73369657-b35b-4fd5-a2b9-36722fe43a1e_1294x1291.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F73369657-b35b-4fd5-a2b9-36722fe43a1e_1294x1291.png 1456w\"></a></p><p>Alignment research is often concerned with <i>specification: </i>how can we design objectives that capture human values (e.g., intrinsic goods such as wellbeing)? For example, if a chatbot is trained to maximize engagement (a proxy for entertaining experiences), it might find that the best way to do this is to show users clickbait. Ideally, the training signal should incorporate everything the designers care about, such as moral concerns (give users the right impression/honesty, give users content that helps them develop their skills, etc.).</p><p>The authors of this paper argue that even correctly specifying the goal is not sufficient for beneficial behavior. They provide several examples of environments where AI agents learn the wrong goals even when the desired behavior is correctly specified. These goals result in reasonable performance in the training environment, but poor or even harmful results under distribution shift.</p><p>The authors draw a distinction between \u201cgoal misgeneralization\u201d errors and \u201ccapability misgeneralization,\u201d where an agent \u2018breaks\u2019 or otherwise doesn\u2019t behave competently. They argue that goal misgeneralization is a greater source of concern because more capable agents can have a greater impact on their environment: actively executing the wrong goal matters while passive failures do not. A way to separate between goal misgeneralization and capabilities misgeneralization would be exciting if work on goal misgeneralization could improve alignment without capabilities externalities.</p><p>However, this distinction might be eroded in the future. Right now, agents \u2018break\u2019 and \u2018get stuck\u2019 and \u2018behave randomly.\u2019 These are very clearly capability failures. But agents in the future aren\u2019t likely to make these mistakes. A robot might fall over, but it will get back up and continue on course towards its objective. Basic agents in the future would have basic \u201crecovery routines\u201d that help them stay on course. Such routines could erode the distinction between \u201cpassive\u201d deep learning misgeneralizations and \u201cactive\u201d goal misgeneralizations. One can construct a broad class of goal misgeneralizations simply from deep learning capabilities misgeneralizations. Imagine a sequential decision making agent using a deep network to understand its goal. The misgeneralization in the deep network would cause the model to pursue the wrong goal, exhibiting goal misgeneralization (assuming it has some recovery routines that help it not get stuck). Consequently deep learning representation faults could be given momentum, amplified, and made impactful by recovery routines in sequential decision making agents.</p><p>How should \u201cgoal misgeneralization\u201d be distinguished from \u201ccapabilities misgeneralization?\u201d for these more capable systems? It might be helpful to check whether they can be distinguished for humans. Albert Einstein described his involvement in the Manhattan project as the greatest mistake of his life. One way to interpret Einstein\u2019s error is to say that he had the wrong goal (to build nuclear weapons). Another interpretation is that he actually had a correct goal (to benefit humanity) but failed to predict the suffering his work would cause, which was an error in capability. If an AI system made a similar mistake, would it be interpreted as \u201cgoal misgeneralization\u201d or \u201ccapabilities misgeneralization\u201d?</p><p>The distinction may not be very meaningful for more capable systems, or the distinction needs to be clearer. If this distinction is not made more clear, work that aims to prevent goal misgeneralization errors may serve to further capabilities. Instead, the safety community may want to focus on addressing specific forms of misgeneralization so as not to increase general capabilities. For instance, one might work on improving the robustness of human value models to unforeseen circumstances or optimizers. Alternatively, one might look for effective ways to incorporate uncertainty into sequential decision-making systems, so that they act more conservatively and do not aggressively pursue the wrong goal. As currently instantiated, the current formulation of goal misgeneralization may be too broad.</p><p><strong>[</strong><a href=\"https://arxiv.org/pdf/2210.01790.pdf\"><strong>Link</strong></a><strong>]</strong></p><p>&nbsp;</p><p><strong>Other Alignment News</strong></p><p><strong>[</strong><a href=\"https://arxiv.org/abs/2209.00731.pdf\"><strong>Link</strong></a><a href=\"https://arxiv.org/abs/2203.09911\"><strong>]</strong></a> A philosophical discussion about what it means for conversational agents to be \u2018aligned.\u2019</p><p><strong>[</strong><a href=\"https://arxiv.org/abs/2206.13353\"><strong>Link</strong></a><strong>]</strong> This report examines a commonplace argument underlying concern about existential risks from misaligned advanced AI.</p><hr><h1><strong>Systemic Safety</strong></h1><h3><strong>Forecasting Future World Events with Neural Networks</strong></h3><p>We care about improving decision-making among political leaders to reduce the chance of rash or possibly catastrophic decisions. These decision-making systems could be used in high-stakes situations where decision makers do not have much foresight, where passions are inflamed, and decisions must be made extremely quickly, perhaps based on gut reactions. Under these conditions, humans are liable to make egregious errors. Historically, the closest we have come to a global catastrophe has been in these situations, including the Cuban Missile Crisis. Work on epistemic improvement technologies could reduce the prevalence of perilous situations. Separately, they could reduce the risks from highly persuasive AI. Moreover, it helps leaders more prudently wield the immense power that future technology will provide. As Carl Sagan reminds us, \u201cIf we continue to accumulate only power and not wisdom, we will surely destroy ourselves.\u201d (motivation taken from <a href=\"https://arxiv.org/abs/2206.05862\">this paper</a>)</p><p>This paper creates the Autocast benchmark to see how well ML systems can forecast events. An example question in the dataset is as follows: \u201cWill at least one nuclear weapon be detonated in Ukraine by 2023?\u201d Autocast also includes a corpus of news articles organized by date to faithfully simulate the conditions under which humans make forecasts.</p><p>The paper establishes a baseline that is far below the performance of human experts: 65% accuracy vs. 92% for human experts on binary questions (random would be 50%). This indicates that there is much room for improvement on this task.</p><p>Separately, there is a <a href=\"https://forecasting.mlsafety.org\">$625,000 prize pool</a> for researchers working on this problem.</p><p><strong>[</strong><a href=\"https://arxiv.org/abs/2206.15474\"><strong>Link</strong></a><strong>]</strong></p><p>&nbsp;</p><p><strong>Other Systemic Safety News</strong></p><p><strong>[</strong><a href=\"https://arxiv.org/abs/2208.07049\"><strong>Link</strong></a><strong>]</strong> A self-supervised method for detecting malware using a ViT achieves state-of-the-art 97% binary accuracy.</p><hr><h1><strong>Other News</strong></h1><p><strong>A survey finds that 36% of the NLP community at least weakly agree with the statement \u201cAI decisions could cause a nuclear-level catastrophe\u201d</strong></p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F54cf7c42-872f-4d30-ad86-ceb6d4bcbfe8_1600x709.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F54cf7c42-872f-4d30-ad86-ceb6d4bcbfe8_1600x709.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F54cf7c42-872f-4d30-ad86-ceb6d4bcbfe8_1600x709.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F54cf7c42-872f-4d30-ad86-ceb6d4bcbfe8_1600x709.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F54cf7c42-872f-4d30-ad86-ceb6d4bcbfe8_1600x709.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F54cf7c42-872f-4d30-ad86-ceb6d4bcbfe8_1600x709.png 1456w\"></a></p><p><i>The percentages in green indicate researcher opinions and the black bolded numbers indicate their perception of the community\u2019s view.</i></p><p><strong>[</strong><a href=\"https://arxiv.org/abs/2208.12852\"><strong>Link</strong></a><strong>]</strong></p><p>&nbsp;</p><p><strong>Another survey: AI researchers give a 5% median probability of \u2018extremely bad\u2019 outcomes (e.g., extinction) from advanced AI</strong></p><p>Many respondents were substantially more concerned: 48% of respondents gave at least 10% chance of an extremely bad outcome. Yet some are much less concerned: 25% said such outcomes are virtually impossible (0%).</p><p><strong>[</strong><a href=\"https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/#General_safety\"><strong>Link</strong></a><strong>]</strong></p><p>&nbsp;</p><p><strong>The Center for AI Safety is hiring research engineers.</strong></p><p>The Center for AI Safety is a nonprofit devoted to technical research and the promotion of safety in the broader machine learning community. Research engineers at the Center pursue a variety of research projects in areas such as Trojans, Adversarial Robustness, Power Aversion, and so on.</p><p><a href=\"https://jobs.lever.co/aisafety/116247a4-2940-4dce-b7d5-a6190328fd4e\"><strong>[Link]</strong></a></p><p>&nbsp;</p><p><strong>Call for benchmarks! Prizes for promising ML Safety benchmark ideas</strong></p><p>As David Patterson observed, \u201cFor better or for worse, benchmarks shape a field.\u201d We are looking for benchmark ideas that have the potential to push ML Safety research in new and impactful directions in the coming years. We will award $50,000 for good ideas and $100,000 for outstanding ideas, with a total prize pool of <strong>$500,000</strong>. For especially promising proposals, we may offer funding for data collection and engineering expertise so that teams can build their benchmark. The competition is open until August 31st, 2023.</p><p><strong>[</strong><a href=\"https://benchmarking.mlsafety.org/\"><strong>Link</strong></a><strong>]</strong></p>", "user": {"username": "Dan Hendrycks"}}, {"_id": "GCfnaf5msCKiCngKs", "title": "Alignment 201 curriculum", "postedAt": "2022-10-12T19:17:07.305Z", "htmlBody": "", "user": {"username": "richard_ngo"}}, {"_id": "SnSAJpPZjLj2JWC56", "title": "Responding to recent critiques of iron fortification in India", "postedAt": "2022-10-12T18:01:13.388Z", "htmlBody": "<p>Two recent EA Forum posts offered critiques on iron fortification in India and particularly the work of Fortify Health, given its relationship with EA:&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/SgqBAeoCbQeLxmMoj/targeted-treatment-of-anemia-in-adolescents-in-india-as-a\"><u>Targeted Treatment of Anemia in Adolescents in India as a Cause Area</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/2gG7eeDD5uqud4Rfm/cost-effectiveness-of-iron-fortification-in-india-is-lower\"><u>Cost-effectiveness of iron fortification in India is lower than GiveWell's estimates</u></a>, both authored by Akash Kulgod.</p><p>We heartily welcome the inquiry and critique, and Fortify Health will remain open minded in pursuit of understanding the true impact of its work and in adjusting course accordingly. I am one of the co-founders of Fortify Health, and although I have since stepped back from day-to-day operations, I offered to address the concerns raised so that the active core team can continue to focus on their implementation and partnership work.</p><p>In this post, I intend to provide initial responses to key points made in Kulgod\u2019s articles:</p><ul><li>Is iron fortification expected to increase diabetes prevalence in India?<ul><li>Short answer: This should not be inferred from available evidence.</li></ul></li><li>Is the proportion of anemia in India attributable to iron deficiency lower than we thought?<ul><li>Short answer: At first glance, more recent estimates of iron deficiency among children cited are on average roughly similar to those incorporated in GiveWell\u2019s 2019 CEA, and state and age specific prevalence could be incorporated into future models. Givewell\u2019s 2019 model largely relies on iron deficiency prevalence rather than iron deficiency anemia prevalence, but I\u2019m uncertain as to whether or for what outcomes use of one or both parameters would lead to the most accurate model.</li></ul></li><li>Should hemoglobin cutoff values be changed, and how does this affect the expected impact of fortification efforts?<ul><li>Short answer: The distribution of hemoglobin levels among the Indian sample cited does not support the inference that outcomes associated with anemia are equivalent between populations at different threshold hemoglobin levels. The new information presented by the cited study does not weaken the expected impact of fortification.</li></ul></li><li>Is a targeted intervention preferable to widespread fortification?<ul><li>Short answer: Targeted screening and treatment should be available as part of comprehensive primary care and possibly school-based programs, but does not preclude rapidly scaling up fortification efforts. Further exploration of this intervention would be worthwhile.</li></ul></li><li>Is micronutrient fortification ethical?<ul><li>Short answer: Extending the benefits of fortification is one part of an ethical imperative for health equity, and absence of fortification does not provide greater autonomy.</li></ul></li></ul><p>I recognize that this post only scratches the surface of complex issues, and it does not provide a comprehensive review of all available arguments and evidence that may be relevant. Its scope is substantially more limited, merely contributing to an evolving conversation started by Kulgod\u2019s posts. I\u2019m eager to get feedback from Kulgod and other readers, and hope we can collaboratively advance our own understanding and the EA community\u2019s understanding of fortification efforts.</p><p><strong>I. Is 10mg/day per capita iron fortification expected to raise prevalence of diabetes by 2-14% as apparently suggested by&nbsp;</strong><a href=\"https://assets.researchsquare.com/files/rs-1136688/v1_covered.pdf?c=1638568122\"><strong><u>Ghosh et al. (preprint 2021)</u></strong></a><strong>?</strong></p><p>Initial reaction: the cited study\u2019s conclusion does not seem to be supported by its results. Claims of correlation between high fasting blood sugar with discrete elevation in serum ferritin are supported by only a small effect size in a subgroup analysis of the highest wealth quintile, which is sharply discontinuous from the next wealth quintile. Furthermore, their model of marginal increase in fasting blood sugar by serum ferritin levels demonstrates minimal effect size, even in this subgroup, so I\u2019m skeptical that the odds ratio presented for the binary outcome of high fasting blood sugar is even internally consistent. Inference of causation from any true correlations is limited by compelling confounders not explored in the study. It\u2019s important to clarify that this \u201craise\u201d is relative, not absolute difference in prevalence. My interpretation of this study does not suggest a causal link between iron fortification and hyperglycemia / prediabetes.</p><ol><li><strong>The conclusion that the risk of of high fasting blood sugar (FBS) / prediabetes (OR of 1.05) is misleading. That is the OR found only in highest wealth quintile and is inconsistent with the remainder of the wealth quintiles.</strong></li><li><strong>There are several reasons to be skeptical of non-null effects, detailed below.</strong></li><li>Table 1 demonstrates the odds ratio for elevated fasting blood sugar (FBS) for each 10\u00b5g/L increase in SF is only barely significant in the richest quintile (CI 1.01-1.08, whereas overall CI 0.99-1.04). It\u2019s unclear if these CI are adjusted for multiple comparisons (i.e. you would expect more false positive findings by chance the more analyses you run, so various approaches to correction are typically used).</li><li><strong>It is suspicious that the next wealth quintile had an effect essentially as strong but in the opposite direction (0.92-1.00), which could lead you to believe (by the paper\u2019s logic) for this quintile, SF is protective against diabetes. In comparing the two findings, CI bound crossing 1.01 vs. 1.0 is fairly arbitrary and isn\u2019t a good reason to hold the richest quintile data in higher regard than the next quintile data. These are contradictory.</strong></li><li><strong>If there is a true correlation in this subgroup, elevated serum ferritin (SF) may be the result of common factors that also independently contribute to risk of hyperglycemia, hypertension, and hypercholesterolemia (confounding).</strong></li><li>Furthermore, the \u201cincreased prevalence of diabetes\u201d in table 2 is potentially easy to misunderstand, as these seem to reflect relative risks or odds ratios, not absolute risk differences&nbsp;<strong>(in other words, they are arguing that the risk or odds are 1.02-1.14 times as large as it would be, not that 2-14% more of the population will develop diabetes.</strong></li><li><strong>However, the clinical effect is negligible.</strong> The linear additive model (adjusted) on page 10 (and aggregate on figure on page 30) reads 0.14mg/dL increase in FBS per 10\u00b5g/L increase in SF. It isn\u2019t immediately clear how that corresponds to the odds ratios of table 1 or the percent change in prediabetes by state in table 2. Nevertheless, on page 8, it seems that fortification was assumed to lead to an SF increase of 7.5-11.5 \u00b5g/L (roughly 10\u00b5g/L). Taken literally, their model suggests a clinically insignificant increase in FBS (e.g.&nbsp;<strong>average 90.8mg/dL \u2192 ~90.94mg/dL overall. </strong>There is no available baseline for richest quintile, but if assumed same, 90.8mg/dL\u2192 91.18 mg/dL).&nbsp;<strong>The conclusions of the study hinge on the proportion of people who cross the threshold into prediabetes e.g. from 99.87-&gt;100.01 or 100.00-&gt;100.14, but health outcomes from this shift would be undetectable. This is illustrated below.</strong></li></ol><p>I tried to roughly estimate how shifting a normal distribution of FBS from mean 90.8 to mean 90.94 or&nbsp; affects the proportion above the 100mg/dL threshold they set for prediabetes (they mention overall sample had 13% prevalence, so I used that to derive \u03c3=8.17) using&nbsp;<a href=\"https://homepage.divms.uiowa.edu/~mbognar/applets/normal.html\"><u>Matt Bognar\u2019s tool</u></a>.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8659e8b5b544f1a4a564658dc14108b4f16b7a73b54c49fe.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8659e8b5b544f1a4a564658dc14108b4f16b7a73b54c49fe.png/w_130 130w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8659e8b5b544f1a4a564658dc14108b4f16b7a73b54c49fe.png/w_260 260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8659e8b5b544f1a4a564658dc14108b4f16b7a73b54c49fe.png/w_390 390w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8659e8b5b544f1a4a564658dc14108b4f16b7a73b54c49fe.png/w_520 520w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8659e8b5b544f1a4a564658dc14108b4f16b7a73b54c49fe.png/w_650 650w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8659e8b5b544f1a4a564658dc14108b4f16b7a73b54c49fe.png/w_780 780w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8659e8b5b544f1a4a564658dc14108b4f16b7a73b54c49fe.png/w_910 910w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8659e8b5b544f1a4a564658dc14108b4f16b7a73b54c49fe.png/w_1040 1040w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8659e8b5b544f1a4a564658dc14108b4f16b7a73b54c49fe.png/w_1170 1170w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8659e8b5b544f1a4a564658dc14108b4f16b7a73b54c49fe.png/w_1260 1260w\"></figure><p>This demonstrated an increase of prediabetes defined by study as FBS&gt;100mg/dL to 13.373% (absolute difference 0.373%, odds ratio 1.03, relative risk 1.03) overall and 14.017% (absolute difference 1.017, odds ratio 1.09, relative risk 1.08 in richest quintile). My estimates here may be different from theirs given adjustments in their model and aberrations from normal distribution.&nbsp;<strong>It should be stressed that Table 2 must be reporting relative rather than absolute differences in prevalence. An \u201celevation\u201d of 3% means something like 1.03 x baseline prevalence.</strong></p><p><strong>It\u2019s not clear why their stated OR among \u201cricher\u201d people is 0.96 in table 1, but all changes in prevalence listed for states in table 2 among \u201cricher\u201d people are positive when they would be expected to be negative, so I\u2019m highly suspect of the methods used to generate table 2. </strong>I suspect peer-review prior to publication would require addressing these points and restating conclusions accordingly.<br>&nbsp;</p><p><strong>II. What proportion of anemia in catchment areas is attributable to iron deficiency or benefits from increased dietary iron?</strong></p><p>Initial reaction: Better estimates of &nbsp;iron deficiency anemia (IDA) and iron deficiency (ID) could have a significant effect on models for Fortify Health's effectiveness. If the prevalences of IDA or ID are much lower than initially modeled, our benefits may be overestimated. Givewell\u2019s 2019 model seems to incorporate estimates of dietary iron deficiency roughly similar to what would be inferred from more recent data cited by Kulgod, but new estimates should be incorporated into future models. Further questions arise about when iron deficiency versus iron deficiency anemia parameters should be used in models. Regardless, addressing the root causes of anemia including poverty remains essential.</p><ol><li>The&nbsp;<a href=\"https://academic.oup.com/jn/article/151/8/2422/6287924?login=false\"><u>Kulkarni et al., (2021)</u></a> estimates of iron deficiency among children seem to add substantial data to this discussion, but this doesn\u2019t directly point us to the proportion of anemia that could be corrected by iron repletion in children (or adults). The state-specific breakdown in&nbsp;<a href=\"https://academic.oup.com/view-large/figure/284063311/nxab145fig2.jpg\"><u>figure 2</u></a> could be useful in evaluating programmatic effects.</li><li><strong>The most recent GiveWell cost effectiveness analysis (CEA) incorporates&nbsp;</strong><a href=\"https://docs.google.com/spreadsheets/d/1QZNP8J_TSCjLN1iIuOdupR1l_v4r-789jXm2kBFfyNg/edit#gid=154585324&amp;range=B19:B20\"><strong><u>estimates of dietary iron deficiency prevalence of 22.5% under 5y and 20.4% 5-14y</u></strong></a><strong>, which is roughly comparable to Kulkarni et al. (2021) estimates of iron deficiency using serum ferritin adjusted with modified BRINDA (31.5% 1-4y, 15.5% 5-9y, 20.9% 10-19y). The latter's estimates are even higher in some of the states where Fortify Health\u2019s work is concentrated.</strong>&nbsp;Direct comparison is difficult due to distinct binning, but an updated CEA could reasonably incorporate Kulkarni et al. (2021) age brackets for modeling effects among children. It is not immediately clear if differences in methods would require additional adjustment to infer impact on modeled outcomes or how it could improve iron deficiency estimates among adults.</li><li>It isn't immediately clear when models of Fortify Health's effectiveness should use iron deficiency or iron deficiency anemia prevalence. Given that data surrounding outcomes of interest may include one or both parameters, it may be prudent to employ one parameter or the other or have a reasonable conversion factor relevant to the areas in which we work (i.e. the proportion of anemia that is due to iron deficiency).</li><li><a href=\"https://academic.oup.com/ajcn/article/106/suppl_1/402S/4668592?login=false\"><u>Engle-Stone et al. (2017)</u></a> / BRINDA article unfortunately doesn\u2019t include data from India but does illustrate that even where infection burden is high, iron deficiency still substantially contributes to burden of anemia (pasting their Figure 1 below).<img src=\"http://res.cloudinary.com/cea/image/upload/v1667995376/mirroredImages/SnSAJpPZjLj2JWC56/mtcxuvjwj3cjbltvj2qh.gif\"></li><li>The Onyeneho et al. (2019) OR for iron deficiency among Indian children with and without anemia in table 3 is hard for me to interpret (I don\u2019t understand the asterisk comment) but it seems essentially close to 1, which would imply in this sample iron deficiency and anemia are not well correlated at all. Perhaps this is in part due to the simplification into binary variables in which the prevalence of iron deficiency is very high. In contrast, table 4 shows significant contribution of iron deficiency in logistic regression, which I don\u2019t know how to reconcile with table 3. It is not clear to me what metric is used to define iron deficiency in this study and whether that truly reflects a deficit in iron intake and absorption or some other process leading to iron sequestration (e.g. inflammation/infection).</li><li><strong>I\u2019m not sure whether there is good evidence about the impact of fortification with highly bioavailable fortificants (like NaFeEDTA) among people with chronic parasitic infections of the gut.&nbsp;</strong>While the etiology of their anemia may be infection that causes occult blood loss in the intestines, this may lead to iron deficiency due to losses if there is not adequate nutritional replenishment. Obviously these people need clean water and deworming treatment, but they may also benefit from iron.</li><li>I agree with the conclusion that addressing other constraints related to poverty is essential. I don\u2019t know how to interpret the findings that lower wealth quintiles had less iron deficiency, but it does open the question of whether there is overcorrecting for covariates in the adjusted models.<br>&nbsp;</li></ol><p><strong>III. Should an alternative to WHO anemia cutoffs be used as suggested by&nbsp;</strong><a href=\"https://www.sciencedirect.com/science/article/pii/S2214109X21000772\"><strong><u>Sachdev et al. (2021)</u></strong></a><strong>?</strong></p><p>Initial reaction: It is likely that the \u201chealthy\u201d sample used is still at increased risk for sequelae of anemia (whether chronically such as cognitive development and physical endurance or with regard to risk profile to future illness) even if they are not demonstrating illnesses that meet the exclusion criteria. Revisiting cutoffs more broadly could have various advantages, but we should use comparable criteria to relate the health impact observed previously and in other places to the areas where Fortify Health works. Other than altitude and known genetic variants (e.g. in sickle cell and thalassemia), there is no compelling genetic reason to believe that lower hemoglobin levels would present lesser risk to an Indian as it would to North Americans or Europeans. Whether lower hemoglobin could provide certain advantages is a separate question not addressed here.</p><ol><li><strong>While I\u2019m sympathetic to concerns about imposing inappropriate standards of \u201cnormal,\u201d I am also wary of redefining the threshold for acceptable health on regional norms that could inappropriately accept a lower standard under false claims of biological difference.</strong></li><li>For example, in the United States, the standard measure of estimated kidney function (eGFR) was for years adjusted by race under the faulty logic that black people have more muscle mass on average, so higher levels of creatinine (an expected product of muscle breakdown) should fall within the normal range. There are significant consequences of this given that eGFR is used to determine eligibility for transplantation and directs medical therapy. <a href=\"https://www.nejm.org/doi/full/10.1056/NEJMms2004740\">Only recently have US doctors acknowledged that such a system was not accurately capturing poor kidney function and the risks and treatment indications associated with it, systematically introducing a lower standard of care for black patients.</a> Examples like these should make us wary of insisting biological difference between populations, rather than other social structures, defines a \u201cnormal parameter.\u201d</li><li>Stunting provides another example. Let\u2019s suppose that in India, the average adult male height is 165cm whereas in Finland it is 180.7cm (<a href=\"https://en.wikipedia.org/wiki/Average_human_height_by_country\"><u>wikipedia</u></a>). We might want to conclude there is only a genetic difference responsible. But there is also a significant effect of stunting due to nutritional differences. As it turns out, childhood stunting in India has improved dramatically (62.7%-&gt;34.7%) over the last 20 years (<a href=\"https://data.worldbank.org/indicator/SH.STA.STNT.ZS?locations=IN\"><u>World Bank</u></a>) not because genes changed but because society changed. Even a \u201chealthy\u201d subgroup defined to establish a normal height for an Indian may show a distribution that is shifted towards lower values because of stunting, yet the distribution would be shifted towards higher values with nutritional improvement that leads to increased height and decreased risks of illness that may not lead to exclusion from a \u201chealthy\u201d sample.</li><li><strong>My main concern with this study is with the definition of \u201chealthy.\u201d Although the study excluded participants with vitamin deficiencies and other conditions, it\u2019s not clear to me that those who remain would not be healthier if they had higher hemoglobin levels. I\u2019d be most interested in health outcomes at various hemoglobin levels</strong>, e.g. demonstrating that the risks associated with a hemoglobin of 6g/dL in the US are similar to the risks associated with a hemoglobin of 4g/dL in India. This may be harder to measure around the threshold for the lower limit of normal as the risks are lower. Certainly, people do adapt to chronic anemia to a great extent, so it may be hard for a given person to \u201cfeel\u201d much different living at a stable level of 10 vs 11 g/dL, though on average population health outcomes may reflect meaningful health consequences.</li><li><strong>The effects on various health outcomes have often been studied using these cutoffs (whether they are ideal or not), so we would expect them to correlate with the impacts of people with hemoglobins up to the higher threshold used historically rather than a revised threshold.</strong></li><li>There is a lot still to learn about the health consequences of chronic anemia. For certain outcomes like&nbsp;<a href=\"https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(18)30078-0/fulltext#tables\"><u>maternal mortality</u></a>, it might make sense to think about higher Hgb as working to provide people a buffer so if they do bleed a lot during childbirth, they have enough reserve not to die - for most people, you will live if regardless of whether you have a Hgb of 7 vs 8 g/dL, but on the margin, more people with a Hgb of 7 g/dL are likely to die. Maybe there\u2019s a moderate difference between 10 and 11, even if both people would be \u201chealthy\u201d by Sachdev\u2019s cutoffs since the negative outcome is only revealed during a later childbirth. Alternatively, it\u2019s possible that further research will reveal there really isn\u2019t a meaningful difference between a Hgb of 11 and 12 g/dL, but this would not be expected to uniquely apply to Indians.</li><li>For other outcomes such as exertion tolerance, the linear relationship may continue even above threshold (e.g. why professional athletes train at altitude or \u201cdope\u201d with epo), and could plausibly be true of worker productivity in people whose livelihoods depend on their efficiency and endurance doing manual labor. You could look at whether people at WHO vs. Sachdev cutoffs have any differences in these kinds of health outcomes to conclude whether it\u2019s an appropriate threshold for \u201chealthy\u201d or if the proposed healthy threshold is suboptimal.</li><li>In understanding why there could be population differences in hemoglobin levels among healthy people, I suspect differences in meat consumption plays a significant role. We should be careful not to pathologize not eating meat - that is not \u201cthe problem,\u201d but this difference could explain differences in distributions of hemoglobin between populations. It\u2019s also possible that people who don\u2019t eat meat have more to gain from fortification programs.</li><li>I suspect for the ultimate health outcomes sought by fortification programs (not just intermediate outcomes of the hemoglobin level itself), the benefits of fortification have a nonlinear effect on people with different baseline hemoglobin levels with diminishing returns as people are more iron replete (both because they will absorb less iron and because they don\u2019t benefit greatly from having additional iron). You could argue that inflection point happens at a lower hemoglobin level. More likely, it happens at a similar hemoglobin level and that that level could be debated in both populations.</li></ol><p><strong>IV. Targeted treatment</strong></p><p>Initial reaction: Targeted treatment is likely worth doing and can occur in parallel to population-level interventions such as fortification, which is likely more rapidly scalable.</p><ol><li>Aside from implementation difficulties and limited available data, targeted treatment is conceptually very appealing. It's great to know that this model is being implemented and studied.</li><li>Individualized treatment would be strongest as part of a comprehensive primary care system that would not be limited to single vertical interventions, as many people targeted may have other primary care needs, and investment in the primary care system could have many positive effects (some more or less easily measured). Fortification similarly could be critiqued and therefore also should not preclude strengthening of healthcare systems.</li><li>Depending on the treatment approach, there may be additional challenges in completing treatment (e.g. if short term GI side effects or limited buy-in make adequate treatment uncommon even when testing and talking phases of \"T3\" approach are undertaken). I haven't reviewed available data on how effective T3 or similar interventions are.</li><li>Costs of this intervention will require further examination, but I appreciate Kulgod's estimates. I haven't looked into relevant evidence of duration of iron repletion, but I imagine most of the people who receive targeted treatment would require repeated interventions unless a fundamental cause of iron deficiency is addressed.</li><li>Targeted treatment may be significantly harder to scale up than fortification, but that doesn\u2019t mean it is not worth doing. A portfolio approach in which these interventions are scaled up simultaneously seems ideal, so long as fortification is deemed safe and effective.</li></ol><p><strong>V. Is food fortification ethical</strong></p><p>Initial reaction</p><ol><li>I think it\u2019s good to be humble about this. Having considered it carefully myself, with a working understanding that the totality of the evidence points towards meaningful benefit and minimal harms (an empirical factor challenged by Kligod's posts), I feel confident that wheat flour fortification with iron, folic acid, and vitamin B12 is an ethical practice.</li><li>A detailed examination of relevant ethical considerations is worth further conversation and I don\u2019t claim my reaction here will suffice.</li><li>The most compelling arguments against fortification being ethical might include<ol><li>Causes significant harm (consequentialist/deontological)<ol><li>But the totality of evidence points towards significant benefits and limited to no harms.</li></ol></li><li>Lacks consent (deontological/autonomy)<ol><li>But status quo lack of fortification and limits to educational opportunity (that might empower people to understand benefits of fortification and choose it), does not reflect a state of greater autonomy.</li><li>Labeling and lack of market saturation technically allows for choosing alternative among those who actively object to fortification.</li><li>A higher standard should not apply to fortification than applies to other food processes or additives (e.g. lack of informed choice about which species of wheat is grown, what fertilizer is employed, or what preservatives are used in processed foods).</li></ol></li></ol></li><li>On the other hand, there are strong arguments in favor of fortification<ol><li>Significant benefit (consequentialist/deontological)</li><li>Extends standard of care sought for wealthy to all (equity)</li></ol></li></ol><p>I'm grateful for the time and attention that Akash Kulgod has dedicated to reviewing these issues, and hope to continue an open yet serious conversation on the nature of the problem of anemia and iron deficiency in India, the intervention we can employ, and the concerns that arise. &nbsp;An improved understanding of each of these components will surely guide Fortify Health's future work and the work of other organizations striving to improve health.</p>", "user": {"username": "e19brendan"}}, {"_id": "fxHsQXr59hkFrptDh", "title": "Is \"EA aligned\" a useful phrase? (Yes) Polis survey and results", "postedAt": "2022-10-12T17:14:37.991Z", "htmlBody": "<h1>Tl;dr;</h1><ul><li>Someone tweeted about this so I ran a small poll&nbsp;<ul><li>Most people think \"EA aligned\" is a useful shorthand for agreement across a range of useful norms/behaviours</li><li>&nbsp;Those who are less engaged in social activities or who care less about x-risk are much more likely to feel they aren't \"ingroup\" enough</li><li>Most people thought the phrase \"aligned\" was worse than \"EA aligned\"</li><li>People were uncertain if \"mission-aligned\" was a normal phrase among other subcultures</li></ul></li><li>Here is the Pol.is (featured in 80k podcast) poll. 63 people have already voted. Vote on it <a href=\"https://pol.is/6ktynyaffe\">https://pol.is/6ktynyaffe</a>&nbsp;<ul><li>Try to write comments that create useful cruxes</li></ul></li><li>There are two currently two main clusters:<ul><li>Welfare-maximising utilitarian EAs who think the phrase is fine</li><li>Other kinds of EAs, and non-EAs who don't like it, who are characterised as being less part of the social scene and a worry that they aren't ingroup enough</li></ul></li><li>Then I give some context to this discussion</li></ul><h1>Current survey results (n=104, not random or representative)</h1><p>Full results here: <a href=\"https://pol.is/report/r7ksdnnjzbsmnewa4yevm\">https://pol.is/report/r7ksdnnjzbsmnewa4yevm</a> &nbsp;</p><p>A nice graph showing the 4 big groups</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7ee4b4fb6036ef4b7b501e190bd82c80e9e6345cc0eb3e.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7ee4b4fb6036ef4b7b501e190bd82c80e9e6345cc0eb3e.png/w_180 180w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7ee4b4fb6036ef4b7b501e190bd82c80e9e6345cc0eb3e.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7ee4b4fb6036ef4b7b501e190bd82c80e9e6345cc0eb3e.png/w_540 540w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7ee4b4fb6036ef4b7b501e190bd82c80e9e6345cc0eb3e.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7ee4b4fb6036ef4b7b501e190bd82c80e9e6345cc0eb3e.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7ee4b4fb6036ef4b7b501e190bd82c80e9e6345cc0eb3e.png/w_1080 1080w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7ee4b4fb6036ef4b7b501e190bd82c80e9e6345cc0eb3e.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7ee4b4fb6036ef4b7b501e190bd82c80e9e6345cc0eb3e.png/w_1440 1440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7ee4b4fb6036ef4b7b501e190bd82c80e9e6345cc0eb3e.png/w_1620 1620w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ea7ee4b4fb6036ef4b7b501e190bd82c80e9e6345cc0eb3e.png/w_1738 1738w\"></figure><p>First, we'll look at the things everyone thought, then look at the two subgroups A and B. There are some people in neither subgroup.</p><h2>Majority</h2><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b10fb3cc0c752447a96af1c531d175b9e9e73114e5e7e914.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b10fb3cc0c752447a96af1c531d175b9e9e73114e5e7e914.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b10fb3cc0c752447a96af1c531d175b9e9e73114e5e7e914.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b10fb3cc0c752447a96af1c531d175b9e9e73114e5e7e914.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b10fb3cc0c752447a96af1c531d175b9e9e73114e5e7e914.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b10fb3cc0c752447a96af1c531d175b9e9e73114e5e7e914.png/w_1000 1000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b10fb3cc0c752447a96af1c531d175b9e9e73114e5e7e914.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b10fb3cc0c752447a96af1c531d175b9e9e73114e5e7e914.png/w_1400 1400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b10fb3cc0c752447a96af1c531d175b9e9e73114e5e7e914.png/w_1600 1600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b10fb3cc0c752447a96af1c531d175b9e9e73114e5e7e914.png/w_1800 1800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b10fb3cc0c752447a96af1c531d175b9e9e73114e5e7e914.png/w_1910 1910w\"></figure><p>As you can see, most people think \"EA aligned is a pretty useful phrase\". They mostly think that EA should be welcoming to people with different \"vibes\" (energies, modes of being, social presentations, non-central beliefs). Most people didn't feel they had been dismissed for \"not being EA enough\".</p><h2>Group A</h2><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/545435b70ee4a5271b77709130689dc100ca8b00ca5db0a3.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/545435b70ee4a5271b77709130689dc100ca8b00ca5db0a3.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/545435b70ee4a5271b77709130689dc100ca8b00ca5db0a3.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/545435b70ee4a5271b77709130689dc100ca8b00ca5db0a3.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/545435b70ee4a5271b77709130689dc100ca8b00ca5db0a3.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/545435b70ee4a5271b77709130689dc100ca8b00ca5db0a3.png/w_1000 1000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/545435b70ee4a5271b77709130689dc100ca8b00ca5db0a3.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/545435b70ee4a5271b77709130689dc100ca8b00ca5db0a3.png/w_1400 1400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/545435b70ee4a5271b77709130689dc100ca8b00ca5db0a3.png/w_1600 1600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/545435b70ee4a5271b77709130689dc100ca8b00ca5db0a3.png/w_1800 1800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/545435b70ee4a5271b77709130689dc100ca8b00ca5db0a3.png/w_1922 1922w\"></figure><p>Group A is characterised by calling themselves EAs. They care about x-risk and they don't worry about not being ingroup enough.</p><h2>&nbsp;Group B</h2><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4680800c8ad2ce9e17846b48567035a255bc7ad746f08e3d.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4680800c8ad2ce9e17846b48567035a255bc7ad746f08e3d.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4680800c8ad2ce9e17846b48567035a255bc7ad746f08e3d.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4680800c8ad2ce9e17846b48567035a255bc7ad746f08e3d.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4680800c8ad2ce9e17846b48567035a255bc7ad746f08e3d.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4680800c8ad2ce9e17846b48567035a255bc7ad746f08e3d.png/w_1000 1000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4680800c8ad2ce9e17846b48567035a255bc7ad746f08e3d.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4680800c8ad2ce9e17846b48567035a255bc7ad746f08e3d.png/w_1400 1400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4680800c8ad2ce9e17846b48567035a255bc7ad746f08e3d.png/w_1600 1600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4680800c8ad2ce9e17846b48567035a255bc7ad746f08e3d.png/w_1800 1800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4680800c8ad2ce9e17846b48567035a255bc7ad746f08e3d.png/w_1910 1910w\"></figure><p>&nbsp;</p><p>Group B don't work in EA fields and don't like \"EA aligned as a phrase\"</p><h1>Group C</h1><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c71acb7cfc4ae2607025a6ed8213ba2a951c443bd28e49dd.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c71acb7cfc4ae2607025a6ed8213ba2a951c443bd28e49dd.png/w_190 190w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c71acb7cfc4ae2607025a6ed8213ba2a951c443bd28e49dd.png/w_380 380w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c71acb7cfc4ae2607025a6ed8213ba2a951c443bd28e49dd.png/w_570 570w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c71acb7cfc4ae2607025a6ed8213ba2a951c443bd28e49dd.png/w_760 760w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c71acb7cfc4ae2607025a6ed8213ba2a951c443bd28e49dd.png/w_950 950w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c71acb7cfc4ae2607025a6ed8213ba2a951c443bd28e49dd.png/w_1140 1140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c71acb7cfc4ae2607025a6ed8213ba2a951c443bd28e49dd.png/w_1330 1330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c71acb7cfc4ae2607025a6ed8213ba2a951c443bd28e49dd.png/w_1520 1520w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c71acb7cfc4ae2607025a6ed8213ba2a951c443bd28e49dd.png/w_1710 1710w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c71acb7cfc4ae2607025a6ed8213ba2a951c443bd28e49dd.png/w_1896 1896w\"></figure><p>Group C is characterised by not like \"EA aligned\", working in an EA field and having heard people describing others as \"not aligned\"</p><h1>Group D</h1><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4471c568a18700eae5aec22d712527d86e107dcc79f6dcce.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4471c568a18700eae5aec22d712527d86e107dcc79f6dcce.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4471c568a18700eae5aec22d712527d86e107dcc79f6dcce.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4471c568a18700eae5aec22d712527d86e107dcc79f6dcce.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4471c568a18700eae5aec22d712527d86e107dcc79f6dcce.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4471c568a18700eae5aec22d712527d86e107dcc79f6dcce.png/w_1000 1000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4471c568a18700eae5aec22d712527d86e107dcc79f6dcce.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4471c568a18700eae5aec22d712527d86e107dcc79f6dcce.png/w_1400 1400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4471c568a18700eae5aec22d712527d86e107dcc79f6dcce.png/w_1600 1600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4471c568a18700eae5aec22d712527d86e107dcc79f6dcce.png/w_1800 1800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4471c568a18700eae5aec22d712527d86e107dcc79f6dcce.png/w_1922 1922w\"></figure><p>Group D is characterised by worrying they are not ingroup enough and thinking EA is a bit culty. They are often involved in EA social life.</p><h1>Tentative Conclusions</h1><ul><li>Do these current clusters hold? I guess they will (70%), but I don't know, please answer and we can see <a href=\"https://pol.is/6ktynyaffe\">https://pol.is/6ktynyaffe</a>&nbsp;</li><li>A big chunk of those who wouldn't identify as EAs work in EA fields. It seems potentially valuable that they don't think they aren't axious about their place due to \"not being EA enough\". And I mean potentially valuable in the $5mn - $50mn range&nbsp;</li></ul><h1>Context</h1><p>&nbsp;Someone tweeted that they didn't like when EAs used the phrase <a href=\"https://twitter.com/jesswhittles/status/1579913870625640448\">EA alinged</a>. It got 21 retweets and quote tweets and a number of comments so seemingly people thought it was notable.&nbsp;</p><p>As we can see above, it's not clear that the phrase itself is that relevant but it does speak to a deeper sense among some EAs that they aren't \"EA enough\". Can you think of a way to fix this?</p><p>&nbsp;</p><p>If you thought this was interesting please do the poll: <a href=\"https://pol.is/6ktynyaffe\">https://pol.is/6ktynyaffe</a>&nbsp;</p>", "user": {"username": "nathan"}}, {"_id": "uxnpir7zLgwf3whg7", "title": "Cultural EA considerations for Nordic folks", "postedAt": "2022-10-12T17:11:00.212Z", "htmlBody": "<p>I wrote this to list some cultural EA considerations from a Finnish/Nordic perspective. I guess some other people might find it useful too. And some others might feel that all of this is very obvious to them.</p><p>If you spot any mistakes or disagree on something please let me know in the comments!</p><h3>US/UK cultural stuff</h3><ul><li>Many EA folks are located in the UK, especially London and Oxford</li><li>Even more EA folks are located in the US, especially in the Bay Area, which is an area in California and has places like San Francisco and Berkeley in it. This area is also known for having many IT companies (if you have heard of Silicon Valley this is the same place). This place is so common for EAs to live in that if you see posts like \u201cis anyone up for going to the park\u201d in an EA group they probably live there. (Kind of like people from Helsinki forget there are other places in Finland.)</li><li>And some US EA folks are in Boston which is on the other side of the country. Of course there are EAs also in other US places but these are some of the biggest ones.</li><li>It is more common to move to countries within the anglophone world (for example from Australia to the US) for work or study. People in EA might assume you\u2019d be willing to do that (or that what is stopping you from doing that are external conditions and not for example \u201cwanting to live in Finland\u201d).</li><li>Many cities many EAs live in have very high living costs. Thus, even full-time working adults often do flat sharing. This is why some people have set up EA (or rationalist) group houses: they\u2019d prefer to live with like-minded folks. Edit: People have pointed out in the comments that it is common to do flat sharing even if you could afford living alone, and that this might be more common for EAs than non-EAs.</li><li>Since the state is not that great at taking care of people in the US, many people feel a social expectation to give money to their struggling relatives or acquaintances, for example if they don\u2019t have enough to pay for a medical operation.&nbsp;<ul><li>Because of this, US non-EA folks might think they are already being altruistic in their daily lives \u2013 they are, but it is not the impartial and cost-effective type.</li><li>In Finland it might be more common for non-EAs to feel like they should not be asked to donate, because helping all people should be outsourced to the state and helping should not be the responsibility of an individual. (I mostly agree with the sentiment but we don\u2019t live in an ideal world, so help from individuals is still needed.)</li></ul></li></ul><h3>Other geographical stuff</h3><ul><li>There are quite some EA/rationalist folks in Central Europe as well, in particular in Germany, Switzerland and Austria.</li><li>Also, Prague has a lot of EA/rationalist folks and established locations that are used for rationalist workshops and EA coworking.</li><li>Many other Western places have EA groups, some of them bigger than others.</li><li>There are also EAs in non-Western places but many non-Western groups are still small. But not all, for example EA Philippines is quite active.</li><li>Out of Nordic countries, the probably most well-established EA activity happens in Norway, but all Nordic countries have active EA national organizations and are catching up.</li><li>In the EA context Estonia is sometimes counted as a Nordic country. It has more EA activity than other Baltics and participates in some Nordic collaboration.</li><li>If you are interested in the cross-cultural aspect of EA, you are in luck<ul><li>A lot of EAs like meeting (or even housing) other EAs when traveling, so if you are going abroad anyway, a good option is to check if there is a local group where you could meet new interesting people.</li><li>You can also connect to other EAs across the world through online EA stuff such as&nbsp;<a href=\"https://www.effectivealtruismanywhere.org/\"><u>EA Anywhere</u></a> or the&nbsp;<a href=\"https://forum.effectivealtruism.org/s/agBJmNzGpPetYxXoy/p/nxfhxwQg4HJ7KQz4A\"><u>EA Gathertown Coworking Space</u></a> (you don\u2019t need to be working on anything directly EA related to spend time on the latter!)</li></ul></li></ul><h3>Donations</h3><ul><li>In the US, it is quite common to donate large sums of money. This is because there are significant tax benefits in doing so, effectively meaning that in some cases you can somewhat choose to either pay taxes or to donate to an organization of your choice. Edit: It appears I had an exaggerated idea on the amount of taxes you can convert to donations, see <a href=\"https://forum.effectivealtruism.org/posts/uxnpir7zLgwf3whg7/cultural-ea-considerations-for-nordic-folks?commentId=5muCsHjRGPrRzmbvy\">discussion on comments</a>.</li><li>Many other countries also have tax benefits for donations. (In Finland you can only get tax benefits only if you donate to one of a few selected education and art related beneficiaries, and only and only for specific types of tax \u2013 practically the average person never needs to worry about this.)</li><li>Anyway, most people in Finland still donate some money to charity. According to surveys, around 80% of Finns have donated within the last year and have 20% set up a regular monthly donation to some non-profit.&nbsp;</li><li>Some religious (Jewish, Christian and Muslim) people around the world practice tithing, which often means giving 10% of your income to the local religious organization. Thus, the Giving What We Can pledge with its 10% of income to effective charity might sound familiar to people who know about tithing. In Finland tithing is maybe not that well-known as churches usually get their income from a church membership tax. While GWWC has not chosen this percentage for religious purposes, the already established similar practice is said to help people consider taking the GWWC pledge as well.<ul><li>For some Finns,&nbsp;<a href=\"https://www.thelifeyoucansave.org/take-the-pledge/\"><u>the pledge from The Life You Can Save</u></a> might seem more intuitively appealing as the website gives you an income-based donation percentage recommendation.</li></ul></li></ul><h3>Careers</h3><ul><li>Many people living in Finland would strongly prefer to continue living in Finland (including me). This can sometimes pose constraints in applying for \u201cEA jobs\u201d.<ul><li>Many EA organizations and other places recommended on the 80 000 hours job board only hire for location specific roles. None of these locations are in Finland. (Currently there are two registered EA organizations in Finland: EA Finland and Aalto EA.) Most EA jobs are located in the US or UK.</li><li>Some organizations allow remote work too (or are completely remote).&nbsp;<ul><li>Unfortunately, this does not necessarily mean you can live in Finland and work for these roles. For example, the organization might not want to adopt a policy that is compatible with the Finnish employment legislation. It could also be that they want all employees to be within a certain time zone range.&nbsp;</li><li>So if you are applying for a remote role, remember to check that the organization knows where you are located so that you can make sure if they can even hire you.</li></ul></li><li>Legally, working as a contractor or as a grantee is usually much less location specific.<ul><li>If you need to calculate cost estimates for contracting or being a grantee yourself, remember to check what taxes and other side costs apply. This is very nation-specific.</li></ul></li></ul></li><li>For the specific case of Finns who want to have children and want to work in remote roles at EA organizations: the organization\u2019s idea of \u201cgenerous family leave\u201d might not match your expectation of a (minimal) family leave, because Nordic countries have unusually strong state support for people taking care of their infants and then returning to work when the child is a little older. My recommendation here is to be open about your wishes for family leave during the hiring process, but I don\u2019t know anyone in this situation who has actually gotten that far in an EA organization hiring process so this is somewhat hypothetical.</li></ul><h3>Education</h3><ul><li>In many countries, people start their university education at a younger age than Finland. (In many places the typical age of finishing upper secondary education is younger than in Finland and the university admission process might not give you chances to try out next year again until you get to the program of your choice. Gap years before starting university are not often encouraged / well-perceived like in Finland.)</li><li>Moreover, Finland is uncommonly flexible about the speed in which you need to complete your studies. In many other places, students are just expected to complete the program they attend in a given time, and delays or gap years might not be permitted. Studying at a slower speed in order to work aside your study might not be possible.</li><li>As a result, many people in EA might have obtained their university degree at an age that feels weirdly young to Finns, but recent graduates might not have that much working experience and might not have tried out different study programs to the extent that would be common in Finland.<ul><li>Many other countries also put less emphasis on self-sufficiency and \u201cadult skills\u201d on students than Finland</li></ul></li><li>In the US/UK, people care a lot about attending a good university. This might matter even more than your field of study.&nbsp;<ul><li>People might introduce themselves as \u201cI am X from Y\u201d \u2013 if you don\u2019t know what Y is, it is probably a university US/UK folks consider well-known. (In Finland it would be more common for students to say \u201cI\u2019m X and I study Z\u201d because there are not that many differences between universities.)</li><li>Unfortunately none of the Finnish universities are considered \u201cgood universities\u201d in this sense: not particularly bad either, people just don\u2019t know how to rate them. (It does not matter much that the University of Helsinki is well-placed in international rankings, because this is about the general reputation of the university.)</li><li>I actually have no idea what the real difficulty level of for example an \u201cOxford PhD\u201d is compared to a \"University of Helsinki PhD\". Is it significantly more difficult to actually get the PhD done or is it just difficult to get in a PhD program? Do people from elite universities have way better skills or do they mostly just get a reputational advantage? I don\u2019t know.</li></ul></li><li>Finnish employers also place more value on hiring someone with a directly related degree. This means if you want to apply for jobs in the global EA space, you can place less value on having \u201cthe right education\u201d to the position than you would normally in Finland.</li></ul><h3>Community building</h3><ul><li>So, Finnish students tend to be older and have more relevant work experience than a median EA student. In EA, a lot of community building work is done by students, especially if the target group is students. As a result, if you do community building, people might assume you are younger and have less experience than you actually have.</li><li>Even if you are not a student, people might assume you are a student based on the fact that you are doing community building work. (This is not that false given that many people working in EA Finland are in fact students or have a student status despite having no intention to actually graduate because they\u2019ve been in working life for such a long time already. But EA Finland is still a national organization and it does not target students only.)</li></ul><h3>Language</h3><ul><li>Your level of English is fine! I promise.</li><li>If you don\u2019t understand something you read on the EA forum and think it is a language issue, it might actually be about jargon or domain specific vocabulary; or the content just might require some context you don\u2019t have. It is of course still annoying not to understand things, but in this case, native speakers are affected too.</li></ul><h3>Tone (especially hype)</h3><ul><li>Sometimes EA folks or materials can use \u201chypey\u201d language that might seem off-putting to some Finns.&nbsp;<ul><li>In Finland, hype might be interpreted as a sign of incompetence, fluffiness or even dishonesty, so it can take some time to get used to the tone.</li><li>But on the other hand I feel like this happens way less in EA than in some other technology contexts, probably because EAs need to reserve words like \u201cthe best [solution/method/way]\u201d to something that is actually the best, not just \u201cgood\u201d</li></ul></li><li>It can be especially intimidating to see some program advertised to \u201chigh-achieving\u201d or \u201cextraordinary\u201d or \u201cextremely talented\u201d folks because in Finland this is not a typical way to advertise anything<ul><li>Most Finns have no idea how to even find out if someone is \u201cextraordinary\u201d or \u201chigh-achieving\u201d. Many other countries have things like university class ratings, so students can for example know if they are the 8. best student in their year. Finnish education system is not compatible with giving anyone this information, so many students estimate their performance is \u201caverage\u201d even if it is unlikely for everyone to be average at the same time.</li><li>Even if people know they are \u201cbest of Finland\u201d in something they might think it is not a very good achievement \u201cbecause Finland is so small\u201d. (If you think this about yourself, please stop: most likely you have still achieved something quite important.)</li><li>And even if someone actually does something clearly extraordinary, there is a cultural expectation to emphasize that it was not because they were special or anything. For example, when Olli from EA Turku finished his 5 year degree in Mathematics in 2 years and entered a PhD program as a 19 year old,&nbsp;<a href=\"https://www.hs.fi/kaupunki/helsinki/art-2000008097845.html\"><u>they interviewed him for the newspaper</u></a> but emphasized that he is actually a \u201cbalanced young man\u201d who just likes math and enjoys hanging out with friends.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjmqh57rm4o\"><sup><a href=\"#fnjmqh57rm4o\">[1]</a></sup></span></li></ul></li><li>People from some other cultural contexts actually like things with a hypey tone. While a Finn might think \u201cthis program is for extraordinary people, so not for me\u201d, some others might be motivated to be able to join something that is for extraordinary folks because this means joining is worth their time, and maybe getting accepted can feel nice, \u201cit is a proof of my extraordinariness\u201d</li><li>But again on the other hand non-Finnish EAs are often also intimidated by this kind of language, and it is often repeated that <a href=\"https://forum.effectivealtruism.org/posts/gp94EeYgbh5qjfu65/my-experience-with-imposter-syndrome-and-how-to-partly\">imposter syndrome</a> is common in EA. So if you are from Finland and an EA, you are very likely to underestimate how talented etc. you actually are.</li></ul><h3>Interacting with EA folks</h3><ul><li>Sometimes when talking to other EAs I feel like they perceive me as more shy and inexperienced than I actually am. I\u2019m not sure if this is a cultural thing (it might also be that people assume I\u2019m younger than I am) but it could also be because Finns are less assertive even if they are feeling confident. So maybe I come across as insecure/lost when I just don\u2019t feel like saying anything (for example because I already know everything the other person is telling me and don\u2019t need to ask questions).</li><li>I have the feeling people sometimes just disappear even if we already agreed to have a call or to meet up (but for example did not agree on the time yet). This of course happens occasionally in Finland too but I feel like it is more common in EA than what I am used to. I suspect this is a cultural thing (what I think of as \u201cpromise\u201d the other person might have thought of as \u201csuggestion\u201d). Some Finns told me this could also be the result of some complex rationalist calculation that ultimately leads to having more meetups but I don\u2019t think this is likely.</li><li>I think many Finns will enjoy the fact that EAs are more likely than average non-Finns to get straight to the point, skip small talk and communicate with high integrity. EAs like effective communication and this can make you feel at home.</li></ul><h3>Bonus: On \u201cefektiivinen altruismi\u201d as a term</h3><ul><li>For the history of the term \u201ceffective altruism\u201d,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/9a7xMXoSiQs3EYPA2/the-history-of-the-term-effective-altruism\"><u>see here</u></a></li><li>Some of the folks of original Effective Altruism Finland started using the most straight-forward translation in 2013 and it is now what the movement is called in Finland<ul><li>Although on the Helsingin Sanomat newspaper they also used&nbsp;<a href=\"https://www.hs.fi/sunnuntai/art-2000007784618.html\"><u>\u201ctehokas altruismi\u201d</u></a> (\u201cefficient altruism\u201d)</li></ul></li><li>There are downsides to this term, most notably that nobody ever understands it at the first go<ul><li>\u201cefektiivinen\u201d is technical term that can refer to&nbsp;<a href=\"https://www.kielitoimistonsanakirja.fi/#/efektiivinen\"><u>efficacy and impact</u></a>, but also to something being&nbsp;<a href=\"https://fi.wikipedia.org/wiki/Efektiivisyys\"><u>approximated</u></a>&nbsp;</li><li>outside of these technical uses and in \u201cefektiivinen altruismi\u201d, \"efektiivinen\" is not really used in Finnish</li><li>people might not exactly remember what \"altruismi\" means either (for example I've heard people translate it as \"self-sacrifice\")</li><li>\u201cEA\u201d in Finnish commonly refers to \u201censiapu\u201d (\u201cfirst aid\u201d) which can sometimes lead to funny misunderstandings</li></ul></li><li>There are also upsides to this term<ul><li>since nobody understands it on the first go you\u2019ll at least get their attention to explain what you are even talking about</li><li>it is easy to automatically detect if someone is talking/writing about EA because they would not use the words in any other context</li></ul></li><li>Even if I consider the term very uncatchy and too technical, I haven\u2019t been able to come up with a better translation either \u2013 especially nothing that would justify changing the terminology now.&nbsp;</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjmqh57rm4o\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjmqh57rm4o\">^</a></strong></sup></span><div class=\"footnote-content\"><p>One example of this cultural phenomenon is that I asked Olli if it is ok to post this example here, because I was worried he'd be uncomfortable with me talking about his achievements. He was ok with it, and said one of the reasons this was emphasized on the article was that the journalist had a model of his life being really weird, and he wanted to correct them by explaining that his life is pretty normal.</p></div></li></ol>", "user": {"username": "Ada-Maaria Hyv\u00e4rinen"}}, {"_id": "Y5Qcywuz6AtdJ2b9z", "title": "Forecasting Newsletter: September 2022.", "postedAt": "2022-10-12T16:37:40.419Z", "htmlBody": "<h2><strong>Highlights</strong></h2><ul><li>PredictIt vs Kalshi vs CFTC saga&nbsp;<a href=\"https://comments.cftc.gov/Handlers/PdfHandler.ashx?id=34691#?w=sapqmnxoxn\"><u>continues</u></a></li><li>Future Fund announces&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/W7C5hwq7sjdpTdrQF/announcing-the-future-fund-s-ai-worldview-prize#comments\"><u>$1M+ prize</u></a> for arguments which shift their probabilities about AI timelines and dangers</li><li>Dan Luu&nbsp;<a href=\"https://danluu.com/futurist-predictions/\"><u>looks at the track record of futurists</u></a></li></ul><h2><strong>Index</strong></h2><ul><li>Prediction Markets, Forecasting Platforms &amp;co<ul><li>PredictIt, Kalshi &amp; the CFTC</li><li>Metaculus</li><li>Manifold Markets</li><li>Squiggle</li><li>Odds and Ends</li></ul></li><li>Research<ul><li>Shortform</li><li>Longform</li></ul></li></ul><p>Browse past newsletters&nbsp;<a href=\"https://forecasting.substack.com/\"><u>here</u></a>, or view this newsletter on substack&nbsp;<a href=\"https://forecasting.substack.com/p/forecasting-newsletter-september-57b\"><u>here</u>.</a> If you have a content suggestion or want to reach out, you can leave a comment or find me on&nbsp;<a href=\"https://twitter.com/NunoSempere\"><u>Twitter</u></a>.</p><h2><strong>Prediction Markets and Forecasting Platforms</strong></h2><h3><strong>PredictIt, Kalshi &amp; the CFTC</strong></h3><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995459/mirroredImages/Y5Qcywuz6AtdJ2b9z/h5dwmbmcwl1yaulpplyi.jpg\"><figcaption>America, Land of the Free</figcaption></figure><p>Previously:</p><ul><li>Kalshi hired a former&nbsp;<a href=\"https://kalshi.com/blog/former-cftc-commissioner-brian-quintenz-joins-our-board\"><u>CFTC commissioner</u></a> (<a href=\"https://web.archive.org/web/20220201175613/https://kalshi.com/blog/former-cftc-commissioner-brian-quintenz-joins-our-board\"><u>a</u></a>).</li><li>The CFTC&nbsp;<a href=\"https://www.cftc.gov/PressRoom/PressReleases/8567-22\"><u>withdrew its no-action letter</u></a> (<a href=\"https://web.archive.org/web/20220805010244/https://www.cftc.gov/PressRoom/PressReleases/8567-22\"><u>a</u></a>) from PredictIt</li><li>Kalshi applied to the CFTC for permission to host a market on which party will control the US Congress after the 2022 mid-term elections. The CFTC&nbsp;<a href=\"https://comments.cftc.gov/PublicComments/CommentList.aspx?id=7311\"><u>asked the public for comments</u></a> (<a href=\"https://web.archive.org/web/20220828210656/https://comments.cftc.gov/PublicComments/CommentList.aspx?id=7311\"><u>a</u></a>) (<a href=\"https://www.politico.com/news/2022/09/05/voters-betting-elections-trading-00054723\"><u>secondary source</u></a>, (<a href=\"https://web.archive.org/web/20220924141931/https://www.politico.com/news/2022/09/05/voters-betting-elections-trading-00054723\"><u>a</u></a>)).&nbsp;</li></ul><p>Since then, on September the 9th&nbsp;<a href=\"https://www.jdsupra.com/legalnews/unpredictable-future-of-political-1333136/\"><u>PredictIt sued the CFTC</u></a> (<a href=\"http://web.archive.org/web/20220925015149/https://www.jdsupra.com/legalnews/unpredictable-future-of-political-1333136/\"><u>a</u></a>). Richard Hanania comments&nbsp;<a href=\"https://richardhanania.substack.com/p/why-im-suing-the-federal-government\"><u>why he is joining the lawsuit</u></a> (<a href=\"http://web.archive.org/web/20221001194707/https://richardhanania.substack.com/p/why-im-suing-the-federal-government\"><u>a</u></a>).&nbsp;</p><p>Solomon Sia and Pratik Chougule\u2014in collaboration with others like myself\u2014wrote&nbsp;<a href=\"https://comments.cftc.gov/Handlers/PdfHandler.ashx?id=34691#?w=sapqmnxoxn\"><u>this extremely thorough letter to the CFTC</u></a> (<a href=\"https://web.archive.org/web/20221012143802/https://comments.cftc.gov/Handlers/PdfHandler.ashx?id=34691#w=sapqmnxoxn\"><u>a</u></a>), examining many aspects of the decision.&nbsp;</p><p>There has been&nbsp;<a href=\"https://news.google.com/search?q=PredictIt%20CFTC&amp;hl=en-GB&amp;gl=GB&amp;ceid=GB%3Aen\"><u>a range of newspaper articles</u></a> (<a href=\"https://archive.ph/uQEvL\"><u>a</u></a>) commenting on the PredictIt spat (e.g.,&nbsp;<a href=\"https://www.wsj.com/articles/why-wont-the-cftc-let-you-take-a-position-on-the-election-11582933734\"><u>1</u></a>,&nbsp;<a href=\"https://slate.com/business/2022/08/predictit-cftc-shut-down-politics-forecasting-gambling.html\"><u>2</u></a>,&nbsp;<a href=\"https://www.coindesk.com/policy/2021/10/28/the-cftc-vs-the-truth/\"><u>3</u></a>,&nbsp;<a href=\"https://www.chicagotribune.com/opinion/commentary/ct-opinion-political-prediction-markets-public-discourse-20220906-lfuvziy3fnfkfgw33lzhsno4h4-story.html\"><u>4</u></a>, etc.), and on Kalshi\u2019s. I particularly liked&nbsp;<a href=\"https://www.chicagotribune.com/opinion/commentary/ct-opinion-political-prediction-markets-public-discourse-20220906-lfuvziy3fnfkfgw33lzhsno4h4-story.html\"><u>this article</u></a> (<a href=\"http://web.archive.org/web/20220907164742/https://www.chicagotribune.com/opinion/commentary/ct-opinion-political-prediction-markets-public-discourse-20220906-lfuvziy3fnfkfgw33lzhsno4h4-story.html\"><u>a</u></a>) on the Chicago Tribune on how prediction markets are an antidote to degraded public discourse.&nbsp;</p><p>Kalshi has an&nbsp;<a href=\"https://www.kalshikit.co/p/obamas-cabinet-used-prediction-markets\"><u>interesting newsletter issue</u></a> (<a href=\"https://web.archive.org/web/20221012110423/https://www.kalshikit.co/p/obamas-cabinet-used-prediction-markets\"><u>a</u></a>) in which they briefly report on how the Obama administration used prediction markets for their decision-making. Note that these would have probably been PredictIt's markets.</p><h3><strong>Metaculus</strong></h3><p>Per their newsletter, Metaculus reached 1 million predictions. They have also&nbsp;<a href=\"https://nitter.privacy.com.de/fianxu/status/1569537636917825536\"><u>reorganized</u></a> as a&nbsp;<a href=\"https://en.wikipedia.org/wiki/Benefit_corporation\"><u>public benefit corporation</u></a> (<a href=\"http://web.archive.org/web/20221001234507/https://en.wikipedia.org/wiki/Benefit_corporation\"><u>a</u></a>), i.e., a for-profit entity that aims to pursue some positive impact, as distinct from shareholder value. I think this leaves Metaculus in a better position, and decreases the (already pretty small) chance that Metaculus starts doing some damaging gatekeeping, etc.</p><p>Metaculus is also building an AI Forecasting team, and hiring for&nbsp;<a href=\"https://apply.workable.com/metaculus/\"><u>a number of positions</u></a> (<a href=\"http://web.archive.org/web/20220913093930/https://apply.workable.com/metaculus/\"><u>a</u></a>), growing its 12-person<a href=\"https://www.metaculus.com/about/\"><u> strong team</u></a> (<a href=\"http://web.archive.org/web/20220925082358/https://www.metaculus.com/about/\"><u>a</u></a>), presumably using its&nbsp;<a href=\"https://www.openphilanthropy.org/grants/metaculus-platform-development/\"><u>2022 Open Philanthropy Grant</u></a> (<a href=\"http://web.archive.org/web/20220929072721/https://www.openphilanthropy.org/grants/metaculus-platform-development/\"><u>a</u></a>).</p><h3><strong>Manifold Markets</strong></h3><p>Manifold continued having a high development speed, e.g., they added a&nbsp;<a href=\"https://manifold.markets/twitch\"><u>Twitch bot</u></a> (<a href=\"http://web.archive.org/web/20221005181649/https://manifold.markets/twitch\"><u>a</u></a>) and ran their&nbsp;<a href=\"https://manifold.markets/tournaments\"><u>first tournaments</u></a> (<a href=\"https://web.archive.org/web/20221012144555/https://manifold.markets/tournaments\"><u>a</u></a>), which I was really glad to see. They have an experimental projects page at&nbsp;<a href=\"https://manifold.markets/labs\"><u>manifold.markets/labs</u></a> (<a href=\"http://web.archive.org/web/20221005182149/https://manifold.markets/labs\"><u>a</u></a>) And they have added a few reputational features:</p><blockquote><p>If a resolved market receives enough reports relative to the number of traders, it will be considered a \u201cbad\u201d market. Creators with enough bad markets will have a warning next to their name on any of their markets. This is just a first step towards reputational features which is a highly requested feature.</p></blockquote><p>Manifold Markets removed and deprioritized their&nbsp;<a href=\"https://news.manifold.markets/p/above-the-fold-updates-and-join-our\"><u>numeric markets</u></a> (<a href=\"http://web.archive.org/web/20220908215157/https://news.manifold.markets/p/above-the-fold-updates-and-join-our\"><u>a</u></a>), citing difficulties in user usage. But from the post, the decision to do so seems like it was evaluated on the wrong grounds: It's not that numeric markets will immediately prove popular and intuitive, it's that experimenting with them is a public good that could unlock value in the medium term.</p><p>More generally, as I\u2019ve been seeing in these past few years, I think that there is a huge attractor of sports and wall-street-type bets. And new prediction-market startups tend to flirt with these a bit. I think this is a mistake, because it\u2019s hard to differentiate oneself from competitors on the basis of better sports betting: traditional sports betting houses like Betfair in Europe or DraftKings in the US are already catering to a similar user base. Instead, my recommendation would be to target virgin communities, to which already existing betting houses don\u2019t already cater.&nbsp;</p><p>You can also see their job board&nbsp;<a href=\"https://www.notion.so/Manifold-Markets-Job-Board-e1b932b3bb2c4ec2b5a95865ec8f0f61\"><u>here</u></a> (<a href=\"https://web.archive.org/web/20221012093824/https://www.notion.so/Manifold-Markets-Job-Board-e1b932b3bb2c4ec2b5a95865ec8f0f61\"><u>a</u></a>).</p><h3><strong>Squiggle</strong></h3><p><a href=\"https://www.squiggle-language.com/#code=eNqrVirOyC8PLs3NTSyqVLIqKSpN1QELuaZkluQXQURqARlkDng%3D\"><u>Squiggle</u></a> is a web-capable language for manipulating probabilities and probability distributions that we at the&nbsp;<a href=\"https://quantifieduncertainty.org/\"><u>Quantified Uncertainty Research Institute</u></a> have been working on. In August, we announced a $1k&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ZrWuy2oAxa6Yh3eAw/usd1-000-squiggle-experimentation-challenge\"><u>Squiggle experimentation prize</u></a>, which has now been resolved. Winners are:</p><ul><li>1st prize of $600 to&nbsp;<a href=\"https://twitter.com/tanaerao?lang=en-GB\"><u>Tanae Rao</u></a> for&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/4Qdjkf8PatGBsBExK/adding-quantified-uncertainty-to-givewell-s-cost\"><u>Adding Quantified Uncertainty to GiveWell's Cost Effectiveness Analysis of the Against Malaria Foundation</u></a></li><li>2nd prize of $300 to&nbsp;<a href=\"https://danwahl.net/\"><u>Dan Wahl</u></a> for&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/BK7ze3FWYu38YbHwo/squiggle-experimentation-challenge-cea-leep-malawi\"><u>CEA LEEP Malawi</u></a></li><li>3rd prize of $100 to&nbsp;<a href=\"https://www.erichgrunewald.com/posts/how-many-effective-altruist-billionaires-five-years-from-now/\"><u>Erich Grunewald</u></a> for&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Ze2Je5GCLBDj3nDzK/how-many-ea-billionaires-five-years-from-now\"><u>How many EA billionaires five years from now?</u></a></li></ul><p>Congrats!&nbsp;</p><p>We also announced a larger&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/noDYmqoDxYk5TXoNm/usd5k-challenge-to-quantify-the-impact-of-80-000-hours-top\"><u>$5k challenge to quantify the impact of 80,000 hours' top career paths</u></a>. I think that participation in this contest has a fairly high value, but also a fairly high expected monetary value: I invite readers to do a quick estimation, e.g.: the contest will have 3 to 15 participants, implying each participant will get between ~$300 and $1.6k.</p><p>I also wrote two posts introducing Squiggle:&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/vh3YvCKnCBp6jDDFd/simple-estimation-examples-in-squiggle\"><u>Simple estimation examples in Squiggle</u></a> and a follow-up at&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/BDXnNdBm6jwj6o5nc/five-slightly-more-hardcore-squiggle-models\"><u>Five slightly more hardcore Squiggle models.</u></a></p><h3><strong>Odds and Ends</strong></h3><p>The FTX Future Fund announces a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/W7C5hwq7sjdpTdrQF/announcing-the-future-fund-s-ai-worldview-prize\"><u>$1M+ prize</u></a> (<a href=\"https://web.archive.org/web/20221002051012/https://forum.effectivealtruism.org/posts/W7C5hwq7sjdpTdrQF/announcing-the-future-fund-s-ai-worldview-prize\"><u>a</u></a>) for arguments that shift their probabilities around AGI timelines and dangers.</p><p>Friend of the newsletter Walter Frick has started a&nbsp;<a href=\"https://nonrival.pub/\"><u>newsletter</u></a> (<a href=\"https://web.archive.org/web/20221005181423/https://nonrival.pub/\"><u>a</u></a>)&nbsp; that combines analysis of a newsworthy topic with an invitation and a prompt for readers to forecast on a related event. The newsletter then reports readers\u2019 forecasts and resolves them when time comes due. Readers might remember Walter from&nbsp;<a href=\"https://qz.com/2069284/facebook-is-shutting-down-its-experimental-app-forecast/\"><u>his excellent coverage of the shutdown of Facebook\u2019s Forecast platform</u></a> (<a href=\"https://web.archive.org/web/20220730061335/https://qz.com/2069284/facebook-is-shutting-down-its-experimental-app-forecast/\"><u>a</u></a>) at Quartz.</p><p>The&nbsp;<a href=\"https://forecasting.mlsafety.org/\"><u>Autocast competition</u></a> (<a href=\"http://web.archive.org/web/20221011173753/https://forecasting.mlsafety.org/\"><u>a</u></a>) offers $625k in prizes for improving the forecasting abilities of machine learning models. This builds on the&nbsp;<a href=\"https://arxiv.org/abs/2206.15474\"><u>Autocast</u></a> (<a href=\"http://web.archive.org/web/20220914001702/https://arxiv.org/abs/2206.15474\"><u>a</u></a>) paper. It might be that the contest has a connection to AI safety, but I'm not really seeing it. The deadline to submit results for the warmup round is February 10th.</p><p>Adam Sherman reports on his frustrations with the&nbsp;<a href=\"https://twitter.com/Squee451/status/1579647834957451264\"><u>UMA project</u></a> (<a href=\"https://archive.org/details/uma-unreliable-market-assumption-protocol\"><u>a</u></a>). These rhyme somewhat with previous complaints about&nbsp;<a href=\"https://deepfivalue.substack.com/p/the-kleros-experiment-has-failed\"><u>Kleros</u></a> (<a href=\"https://web.archive.org/web/20220701003955/https://deepfivalue.substack.com/p/the-kleros-experiment-has-failed\"><u>a</u></a>). Abstracting away from the specifics, the UMA oracle is a&nbsp;<a href=\"https://en.wikipedia.org/wiki/Keynesian_beauty_contest\"><u>Keynesian Beauty Contest</u></a>, meaning that consensus is valued over truth. In this case, a powerful but not dictatorial participant announced that he was going to vote one way, and because the protocol rewards people who vote with the consensus, he convinced others to vote with him. My sense is that a Keynesian Beauty Contest might still be a worthy tradeoff for some crypto protocols because of the added decentralization. But if too many of these events happen, the tradeoff might stop being worth it.</p><p><a href=\"https://www.quantifiedintuitions.org/\">Quantified Intuitions</a> is an <a href=\"https://forum.effectivealtruism.org/posts/W6gGKCm6yEXRW5nJu/quantified-intuitions-an-epistemics-training-website\">epistemics training website</a>. Readers might be familiar with the <a href=\"https://www.pastcasting.com/\">pastcasting</a> app, by the same group.</p><p>The Social Science prediction platform has&nbsp;<a href=\"https://socialscienceprediction.org/forecaster_incentives\"><u>added some large-for-graduate-students forecaster incentives</u></a> (<a href=\"http://web.archive.org/web/20220916011552/https://socialscienceprediction.org/forecaster_incentives\"><u>a</u></a>). They are offering $100 per 10 surveys completed\u2014a survey is usually just a set of predictions that will be used in a future paper. I welcome this development. I used to view it as annoying that participation was restricted to graduate students and faculty. But the thought came to mind that restriction to academics is just a socially acceptable\u2014if coarse\u2014way of selecting for intelligence without saying as much.</p><p>Reddit has&nbsp;<a href=\"https://www.reddit.com/r/polls/predictions/\"><u>r/polls/predictions</u></a> (<a href=\"http://web.archive.org/web/20220709055805/https://www.reddit.com/r/polls/predictions/\"><u>a</u></a>), an embryonic implementation of a prediction market tournament inside Reddit. This builds on Reddit's past prediction functionality, as reported&nbsp;<a href=\"https://forecasting.substack.com/p/forecasting-newsletter-july-2021\"><u>previously</u></a> (<a href=\"http://web.archive.org/web/20211229170227/https://forecasting.substack.com/p/forecasting-newsletter-july-2021\"><u>a</u></a>) in&nbsp;<a href=\"https://forecasting.substack.com/p/forecasting-newsletter-october-2021\"><u>this newsletter</u></a> (<a href=\"http://web.archive.org/web/20220217162710/https://forecasting.substack.com/p/forecasting-newsletter-october-2021\"><u>a</u></a>). It would be useful to talk to whoever is building this functionality at Reddit. They probably have some different goals, more geared towards being a social media site. But some cross-pollination might still be interesting.</p><p>The Swift Centre has an analysis of&nbsp;<a href=\"https://www.swiftcentre.org/can-biden-win-in-2024/\"><u>Biden's chances in the 2024 election</u></a> (<a href=\"http://web.archive.org/web/20220916112924/https://www.swiftcentre.org/can-biden-win-in-2024/\"><u>a</u></a>). See also some other forecasts on&nbsp;<a href=\"https://metaforecast.org/?query=US+president\"><u>Metaforecast</u></a> (<a href=\"https://archive.ph/4n30X#from=https://metaforecast.org/?query=US+president\"><u>a</u></a>), e.g., on&nbsp;<a href=\"https://polymarket.com/market/will-joe-biden-win-the-us-2024-democratic-presidential-nomination\"><u>Polymarket</u></a> (<a href=\"http://web.archive.org/web/20220128214008/https://polymarket.com/market/will-joe-biden-win-the-us-2024-democratic-presidential-nomination\"><u>a</u></a>) or on&nbsp;<a href=\"https://www.betfair.com/exchange/plus/politics/market/1.178176964\"><u>Betfair</u></a> (<a href=\"http://web.archive.org/web/20210831231714/https://www.betfair.com/exchange/plus/politics/market/1.178176964\"><u>a</u></a>).</p><p><a href=\"https://www.ycombinator.com/companies/craze\"><u>Craze</u></a> (<a href=\"https://web.archive.org/web/20221012093558/https://www.ycombinator.com/companies/craze\"><u>a</u></a>) is a Y-Combinator-funded company which brings predictions markets to India.</p><p>I was surprised to see that famous rapper Nicki Minaj has&nbsp;<a href=\"https://maximbet.com/nicki-minaj\"><u>partnered</u></a> (<a href=\"http://web.archive.org/web/20220531210904/https://maximbet.com/nicki-minaj\"><u>a</u></a>) with a&nbsp;<a href=\"https://nitter.privacy.com.de/nickiminaj/status/1531670747399065600\"><u>sports</u></a> (<a href=\"https://web.archive.org/web/20221012110813/https://nitter.privacy.com.de/nickiminaj/status/1531670747399065600\"><u>a</u></a>) betting&nbsp;<a href=\"https://nitter.privacy.com.de/nickiminaj/status/1531670747399065600\"><u>site</u></a> (<a href=\"https://web.archive.org/web/20221012110813/https://nitter.privacy.com.de/nickiminaj/status/1531670747399065600\"><u>a</u></a>). Curious</p><p>INFER continues to have small-money incentives for forecasters, and sending me&nbsp;<a href=\"https://i.imgur.com/j0Ar3BH.png\"><u>mildly cringy emails</u></a> (<a href=\"http://web.archive.org/web/20221012093838/https://i.imgur.com/j0Ar3BH.png\"><u>a</u></a>), and talking about a&nbsp;<a href=\"https://mailchi.mp/cultivatelabs/cset-foretell-launch-9372521\"><u>\"Global AI Race\"</u></a> (<a href=\"http://web.archive.org/web/20221012112754/https://mailchi.mp/cultivatelabs/cset-foretell-launch-9372521\"><u>a</u></a>). I'd continue to recommend it for university students, because it's one of the few sites that have a team functionality, though.</p><p>On Good Judgment Open,&nbsp;<a href=\"https://www.gjopen.com/questions/2090-will-amazon-com-begin-to-accept-any-cryptocurrency-for-purchases-on-the-us-site-before-1-october-2022\"><u>Will Amazon.com begin to accept any cryptocurrency for purchases on the US site before 1 October 2022?</u></a> (<a href=\"http://web.archive.org/web/20220529175114/https://www.gjopen.com/questions/2090-will-amazon-com-begin-to-accept-any-cryptocurrency-for-purchases-on-the-us-site-before-1-october-2022\"><u>a</u></a>) just resolved negatively. I remember it being at 30% a year ago. Crazy times.</p><h2><strong>Research</strong></h2><h3><strong>Shortform</strong></h3><p>Nostalgebraist looks at&nbsp;<a href=\"https://nostalgebraist.tumblr.com/post/695521414035406848/on-ai-forecasting-one-year-in\"><u>AI forecasting one year in</u></a> (<a href=\"http://web.archive.org/web/20220917144833/https://nostalgebraist.tumblr.com/post/695521414035406848/on-ai-forecasting-one-year-in\"><u>a</u></a>) and warns against taking it as a&nbsp;<a href=\"https://en.wikipedia.org/wiki/Stylized_fact\"><u>stylized fact</u></a> (<a href=\"http://web.archive.org/web/20220927235855/https://en.wikipedia.org/wiki/Stylized_fact\"><u>a</u></a>) that AI progress is going faster than forecasters expected.</p><p><a href=\"https://samotsvety.org/\">Samotsvety Forecasting</a>, my forecasting group, looks at the probability of <a href=\"https://forum.effectivealtruism.org/posts/EG9xDM8YRz4JN4wMN/samotsvety-s-ai-risk-forecasts\">various AI catastrophes</a> in the future, and at the <a href=\"https://forum.effectivealtruism.org/posts/2nDTrDPZJBEerZGrk/samotsvety-nuclear-risk-update-october-2022\">risk of a nuclear bomb being used</a> (<a href=\"https://web.archive.org/web/20221012124008/https://forum.effectivealtruism.org/posts/2nDTrDPZJBEerZGrk/samotsvety-nuclear-risk-update-october-2022\">a</a>) in the coming months (see also a <a href=\"https://forum.effectivealtruism.org/posts/8k9iebTHjdRCmzR5i/overreacting-to-current-events-can-be-very-costly\">follow-up</a> by Kelsey Piper).</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/051056eeab5aec3f3f3573afd6135d8010bf99bae7ca9233.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/051056eeab5aec3f3f3573afd6135d8010bf99bae7ca9233.png/w_90 90w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/051056eeab5aec3f3f3573afd6135d8010bf99bae7ca9233.png/w_180 180w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/051056eeab5aec3f3f3573afd6135d8010bf99bae7ca9233.png/w_270 270w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/051056eeab5aec3f3f3573afd6135d8010bf99bae7ca9233.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/051056eeab5aec3f3f3573afd6135d8010bf99bae7ca9233.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/051056eeab5aec3f3f3573afd6135d8010bf99bae7ca9233.png/w_540 540w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/051056eeab5aec3f3f3573afd6135d8010bf99bae7ca9233.png/w_630 630w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/051056eeab5aec3f3f3573afd6135d8010bf99bae7ca9233.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/051056eeab5aec3f3f3573afd6135d8010bf99bae7ca9233.png/w_810 810w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/051056eeab5aec3f3f3573afd6135d8010bf99bae7ca9233.png/w_834 834w\"><figcaption>Taken from <a href=\"https://polymarket.com/market/will-russia-use-a-nuclear-weapon-before-2023\">Polymarket</a>. Note that money is worth less in the event of a nuclear war.</figcaption></figure><p>Some researchers at the University of Pennsylvania are&nbsp;<a href=\"https://nitter.privacy.com.de/rajtmajer_sarah/status/1573465138300059649\">looking for forecasters to predict replication outcomes</a> (<a href=\"https://web.archive.org/web/20221012114519/https://nitter.privacy.com.de/rajtmajer_sarah/status/1573465138300059649\">a</a>). They are paying a $20 base incentive and $25 per market. This is low in absolute terms, but high if you enjoy doing this kind of thing anyways. h/t Ago Lajko.</p><p>Richard Hanania argues that&nbsp;<a href=\"https://richardhanania.substack.com/p/the-problem-with-polling-might-be/\"><u>the problem with polling might be unfixable</u></a>, i.e,. that Republican nonresponse bias might be very hard to estimate. I left a comment with some suggestions, but I agree that the situation&nbsp;<a href=\"https://richardhanania.substack.com/p/the-problem-with-polling-might-be/comment/9327296\"><u>looks grim</u></a> (<a href=\"http://web.archive.org/web/20220927183755/https://richardhanania.substack.com/p/the-problem-with-polling-might-be/comment/9327296\"><u>a</u></a>).</p><p><a href=\"https://www.lesswrong.com/posts/YQ8H4e7z3q8ngev7J/raising-the-forecasting-waterline-part-1\"><u>Two</u></a> (<a href=\"http://web.archive.org/web/20220710073545/https://www.lesswrong.com/posts/YQ8H4e7z3q8ngev7J/raising-the-forecasting-waterline-part-1\"><u>a</u></a>) old&nbsp;<a href=\"https://www.lesswrong.com/posts/YEKHh5nyqhpE3E4Bm/raising-the-forecasting-waterline-part-2\"><u>posts</u></a> (<a href=\"http://web.archive.org/web/20220927155721/https://www.lesswrong.com/posts/YEKHh5nyqhpE3E4Bm/raising-the-forecasting-waterline-part-2\"><u>a</u></a>) from ten years ago look at the lessons learnt by someone who was participating in the IARPA forecasting tournament which led to the Superforecasting book.</p><h3><strong>Longform</strong></h3><p>Dan Luu looks at the track record of futurists, and finds that their track record is generally poor. Readers of this newsletter should&nbsp;<a href=\"https://danluu.com/futurist-predictions/\"><u>read the post</u></a> (<a href=\"https://archive.ph/WJEBd#from=https://danluu.com/futurist-predictions/\"><u>a</u></a>).</p><p>For some background points:</p><ul><li>The AI safety community has been advocating that future artificial intelligence systems (AI) might be so intelligent as to be world-ending dangers.</li><li>Open Philanthropy, a large foundation, is giving some weight to AI safety, and has been donating large amounts of money to that cause.</li><li>As part of their decision-making, Open Philanthropy commissioned research by&nbsp;<a href=\"https://arbresearch.com/\"><u>friends of the newsletter Arb research</u></a> (<a href=\"http://web.archive.org/web/20221011153414/https://arbresearch.com/\"><u>a</u></a>) on the&nbsp;<a href=\"https://arbresearch.com/files/big_three.pdf\"><u>track record of the three biggest science-fiction authors of the 20th century</u></a> (<a href=\"http://web.archive.org/web/20220711161231/https://arbresearch.com/files/big_three.pdf\"><u>a</u></a>) (Asimov, Heinlein and Clarke)</li><li>The CEO of Open Philanthropy later&nbsp;<a href=\"https://www.cold-takes.com/the-track-record-of-futurists-seems-fine/\"><u>used that analysis</u></a> (<a href=\"http://web.archive.org/web/20220914130350/https://www.cold-takes.com/the-track-record-of-futurists-seems-fine/\"><u>a</u></a>), as well as other research Open Philanthropy had been doing, to justify and explain Open Philanthropy's investments in AI Safety.</li></ul><p>In his own analysis of futurists' track record, Dan Luu seems to point out that this process has some characteristics of a shit show. Here is a long extract from Luu's post, minimally edited for readability:</p><blockquote><p>We've seen, when evaluating futurists with an eye towards evaluating longtermists, Karnofsky heavily rounds up in the same way Kurzweil and other futurists do, to paint the picture they want to create.&nbsp;</p><p>There's also the matter of his summary of a report on Kurzweil's predictions being incorrect because he didn't notice the author of that report used a methodology that produced nonsense numbers that were favorable to the conclusion that Karnofsky favors.&nbsp;</p><p>It's true that Karnofsky and the reports he cites do the superficial things that the forecasting literature notes is associated with more accurate predictions, like stating probabilities. But for this to work, the probabilities need to come from understanding the data.&nbsp;</p><p>If you take a pile of data, incorrectly interpret it and then round up the interpretation further to support a particular conclusion, throwing a probability on it at the end is not likely to make it accurate.&nbsp;</p><p>Although he doesn't use these words, a key thing Tetlock notes in his work is that people who round things up or down to conform to a particular agenda produce low accuracy predictions. Since Karnofsky's errors and rounding heavily lean in one direction, that seems to be happening here.</p></blockquote><blockquote><p>We can see this in other analyses as well. Although digging into material other than futurist predictions is outside of the scope of this post, nostalgebraist has done this and he said (in a private communication that he gave me permission to mention) that Karnofsky's summary of <a href=\"https://www.openphilanthropy.org/research/could-advanced-ai-drive-explosive-economic-growth/\">Could Advanced AI Drive Explosive Economic Growth?</a> is substantially more optimistic about AI timelines than the underlying report in that there's at least one major concern raised in the report that's not brought up as a \"con\" in Karnofsky's summary.</p><p>And nostalgebraist later wrote&nbsp;<a href=\"https://nostalgebraist.tumblr.com/post/693718279721730048/on-bio-anchors\"><u>this post</u></a> (<a href=\"http://web.archive.org/web/20221004024842/https://nostalgebraist.tumblr.com/post/693718279721730048/on-bio-anchors\"><u>a</u></a>), where he (implicitly) notes that the methodology used in a report he examined in detail is fundamentally not so different than what the futurists we discussed used. There are quite a few things that may make the report appear credible (it's hundreds of pages of research, there's a complex model, etc.), but when it comes down to it, the model boils down to a few simple variables.&nbsp;</p><p>In particular, a huge fraction of the variance of whether or not TAI is likely or not likely comes down to the amount of improvement will occur in terms of hardware cost, particularly FLOPS/$. The output of the model can range from 34% to 88% depending how much improvement we get in FLOPS/$ after 2025. Putting in arbitrarily large FLOPS/$ amounts into the model, i.e., the scenario where infinite computational power is free (since other dimensions, like storage and network aren't in the model, let's assume that FLOPS/$ is a proxy for those as well), only pushes the probability of TAI up to 88%, which I would rate as too pessimistic, although it's hard to have a good intuition about what would actually happen if infinite computational power were on tap for free.&nbsp;</p><p>Conversely, with no performance improvement in computers, the probability of TAI is 34%, which I would rate as overly optimistic without a strong case for it. But I'm just some random person who doesn't work in AI risk and hasn't thought about too much, so your guess on this is as good as mine (and likely better if you're the equivalent of Yegge or Gates and work in the area).</p></blockquote><p>I'm sympathetic to both sides of this.&nbsp;</p><p>On the one hand, I worry that the side concerned about AI safety acts like a machine that predictably surfaces and amplifies arguments in favor of its side, and predictably discounts arguments for the other side.&nbsp;</p><p>On the other hand, I also see Luu's analysis as perhaps too harsh, e.g.:</p><ul><li>not giving partial credit for predictions that are missed by a few years or that only happen in rich countries rather than worldwide,</li><li>considering predictions that have a \"may\" as unfalsifiable (instead of e.g., assigning a probability of 50% and looking at the resulting Brier or log score),&nbsp;</li><li>evaluating two propositions connected by an \"and\" as one failed prediction instead of one correct and one incorrect prediction.</li><li>evaluating predictions about the \"twenty-first century\" as having already failed</li><li>generally being on the harsh side of things</li></ul><p>Overall, it seems like there is a garden of forking paths with regards to the more specific question of how accurate past futurists were, but also with regards to the more general question about the degree to which it is possible to make predictions about future events, particularly about transformative technologies.&nbsp;</p><p>One way to navigate that garden of forking paths would be an&nbsp;<a href=\"https://en.wikipedia.org/wiki/Adversarial_collaboration\"><u>adversarial collaboration</u></a> (<a href=\"http://web.archive.org/web/20220725190412/https://en.wikipedia.org/wiki/Adversarial_collaboration\"><u>a</u></a>). Funding for this would probably be available, if not from Open Philanthropy itself then from&nbsp;<a href=\"https://ftxfuturefund.org/\"><u>the FTX Future Fund</u></a> (<a href=\"http://web.archive.org/web/20221011034322/https://ftxfuturefund.org/\"><u>a</u></a>), from&nbsp;<a href=\"https://www.super-linear.org/#list2\"><u>Nonlinear</u></a> (<a href=\"https://web.archive.org/web/20221012112602/https://www.super-linear.org/#list2\"><u>a</u></a>), or even from&nbsp;<a href=\"https://nitter.privacy.com.de/NunoSempere\"><u>myself</u></a>. I mention funding because I personally view cold hard cash as an honest signal that some work is truly perceived to be valuable. But one could also choose to carry out an adversarial collaboration pro bono, for the sake of curiosity, etc.</p><p><a href=\"https://arxiv.org/abs/2209.08778\"><u>Price Formation in Field Prediction Markets</u></a> is an arxiv preprint which discusses where the accuracy of prediction markets comes from. The two hypotheses it considers are:</p><ol><li>from averaging the different pieces of information that each participant has</li><li>from traders which are able to individually do more research than everyone else, and profit from this.</li></ol><p>They have a method I'm not completely convinced by in order to identify \"price sensitive\" traders, whom they identify with informed traders, and they use their dataset to conclude that hypothesis 2 is mostly what\u2019s going on. They use data from&nbsp;<a href=\"https://www.almanisprivate.com/\"><u>Almanis</u></a> (<a href=\"http://web.archive.org/web/20220202051215/https://www.almanisprivate.com/\"><u>a</u></a>), one of the smaller prediction market sites that still have some liquidity.</p><p>The paper has some interesting elements. And for all I know, it's better than 99% of the papers in its field. But I'm left with the impression that the topic of research is a bit of a bad fit for academic investigation, because one could get a better idea of the dynamics of prediction markets by listening to the&nbsp;<a href=\"https://starspangledgamblers.com/\"><u>Star Spangled Gamblers</u></a> (<a href=\"http://web.archive.org/web/20221001143818/https://starspangledgamblers.com/\"><u>a</u></a>) guys.</p><hr><p>Note to the future: All links are added automatically to the Internet Archive, using this <a href=\"https://github.com/NunoSempere/longNowForMd\">tool</a> (<a href=\"http://web.archive.org/web/20220711161908/https://github.com/NunoSempere/longNowForMd\">a</a>). \"(a)\" for archived links was inspired by <a href=\"https://www.flightfromperfection.com/\">Milan Griffes</a> (<a href=\"http://web.archive.org/web/20220814131834/https://www.flightfromperfection.com/\">a</a>), <a href=\"https://www.andzuck.com/\">Andrew Zuckerman</a> (<a href=\"http://web.archive.org/web/20220316214638/https://www.andzuck.com/\">a</a>), and <a href=\"https://guzey.com/\">Alexey Guzey</a> (<a href=\"http://web.archive.org/web/20220901135024/https://guzey.com/\">a</a>).</p><hr><blockquote><p>\u2014 What are you waiting for?<br>\u2014 I don't know... Something amazing, I guess.<br>\u2014 Me too, kid</p></blockquote><p><a href=\"https://en.wikipedia.org/wiki/The_Incredibles\">The Incredibles</a>, 30'50''</p>", "user": {"username": "NunoSempere"}}, {"_id": "xxcFicsuNzyn2JbaR", "title": "BBC: 'Sentient' (?) mini-brains...", "postedAt": "2022-10-12T16:29:21.142Z", "htmlBody": "<p>I found this article interesting, and thought it worth sharing:</p><p><a href=\"https://www.bbc.co.uk/news/science-environment-63195653\">https://www.bbc.co.uk/news/science-environment-63195653</a>&nbsp;</p>", "user": {"username": "Forumite"}}, {"_id": "wqmY98m3yNs6TiKeL", "title": "Parfit + Singer + Aliens = ?", "postedAt": "2022-10-12T15:50:32.474Z", "htmlBody": "<h2>Summary:</h2><p>If you</p><ol><li>Have a wide moral circle that includes non human animals and&nbsp;</li><li>Have a low or zero moral discount rate&nbsp;</li></ol><p>Then the discovery of alien life should radically change your views on existential risk.</p><hr><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F578d0da7-1886-47b9-a519-30a81455bcd8_1200x675.jpeg\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F578d0da7-1886-47b9-a519-30a81455bcd8_1200x675.jpeg\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F578d0da7-1886-47b9-a519-30a81455bcd8_1200x675.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F578d0da7-1886-47b9-a519-30a81455bcd8_1200x675.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F578d0da7-1886-47b9-a519-30a81455bcd8_1200x675.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F578d0da7-1886-47b9-a519-30a81455bcd8_1200x675.jpeg 1456w\"></a></p><p>A grad student plops down in front of their computer where their code has been running for the past few hours. After several months of waiting, she had finally secured time on the James Webb Space Telescope to observe a <a href=\"https://en.wikipedia.org/wiki/Proxima_Centauri_d\"><u>freshly discovered exoplanet: Proxima Centauri D</u></a>.</p><p>\u201cThat\u2019s strange.\u201d Her eyes dart over the results. Proxima D\u2019s short 5 day orbit meant that she could get observations of both sides of the tidally locked planet. But <a href=\"https://arxiv.org/pdf/2105.08081.pdf\"><u>the brightness of each side doesn\u2019t vary nearly as much as it should</u></a>. <strong>The dark side is gleaming with light.</strong></p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1a7439a9-8407-4ff4-a440-a0a5f25dc9e8_1318x679.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1a7439a9-8407-4ff4-a440-a0a5f25dc9e8_1318x679.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1a7439a9-8407-4ff4-a440-a0a5f25dc9e8_1318x679.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1a7439a9-8407-4ff4-a440-a0a5f25dc9e8_1318x679.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1a7439a9-8407-4ff4-a440-a0a5f25dc9e8_1318x679.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1a7439a9-8407-4ff4-a440-a0a5f25dc9e8_1318x679.png 1456w\"></a></p><p>Northern Italy at night</p><hr><h2>The Argument</h2><p>This argument requires a few assumptions.</p><ol><li>Strong evidence of intelligent alien life on a nearby planet</li><li>Future moral value is not inherently less important than present moral value</li><li>Many types of beings contain moral value <a href=\"https://nautil.us/do-aliens-have-inalienable-rights-236557/\"><u>including nonhuman animals and aliens</u></a></li></ol><p>I will call people who have a Singer-style wide moral circle and a Parfit-style concern for the long term future \u201cLongtermist EAs.\u201d Given these assumptions, lets examine the basic argument given by Longtermist EAs for why existential risks should be a primary concern.</p><ul><li>&nbsp;Start with assumption 2. The lives of future human beings are not inherently less important than the lives of present ones. <a href=\"https://drive.google.com/file/d/16xd6X53FSnwwepdReuBzdxQOU1wHc1Hz/view\"><u>Should Cleopatra eat an ice cream that causes a million deaths today</u></a>?</li><li>&nbsp;Then consider that <a href=\"https://existential-risk.org/concept\"><u>humanity may last for a very long time, or may be able to greatly increase the amount of moral value it sustains, or both</u></a>.</li><li>&nbsp;Therefore, the vast majority of moral value in the universe lies along these possible future paths where humanity does manage to last for a long time and support a lot of moral value.</li><li>&nbsp;Existential risks make it less likely or impossible to end up on these paths so they are extremely costly and important to avoid.</li></ul><p>But now introduce assumptions 1 and 3 and the argument falls apart. The link between the second and third points is broken when we discovery another morally valuable species which also has a chance to settle the galaxy.</p><p>Discovering aliens nearby means that there are likely billions of planetary civilizations in our galaxy. If, like Singer, you believe that alien life is morally valuable, then humanity\u2019s future is unimportant to the sum total of moral value in the universe. If we are destroyed by an existential catastrophe, another civilization will fill the vacuum. If humanity did manage to preserve itself and expand, most of the gains would be zero-sum; won at the expense of another civilization that might have taken our place. Most of the arguments for caring about human existential risk implicitly assume a morally empty universe if we do not survive. <strong>But if we discover alien life nearby, this assumption is probably wrong and humanity\u2019s value-over-replacement goes way down.</strong></p><p>Holding future and alien life to be morally valuable means that, on the discovery of alien life, humanity\u2019s future becomes a vanishingly small part of the morally valuable universe. In this situation, Longtermism ceases to be action relevant. It might be true that certain paths into far future contain the vast majority of moral value but if there are lots of morally valuable aliens out there, the universe is just as likely to end up one of these paths whether humans are around or not so Longtermism doesn\u2019t help us decide what to do. We must either impartially hope that humans get to be the ones tiling the universe or go back to considering the nearer term effects of our actions as more important.</p><p>Consider Parfit\u2019s classic thought experiment:</p><p>Option A: Peace</p><p>Option B: Nuclear war that kills 99% of human beings</p><p>Option C: Nuclear war that kills 100% of humanity</p><p>He claims that the difference between C and B is greater than between B and A. The idea being that option C represents a destruction of 100% of present day humanity <i>and</i> <i>all future value</i>. <strong>But if we\u2019re confident that future value will be fulfilled by aliens whether we destroy ourselves or not then there isn\u2019t much of a jump between B and C.</strong> When there\u2019s something else to take our place, there\u2019s little long-run difference between any of the three options so the upfront 99 &gt; 1 wins out.</p><p>Many of the important insights of Longtermism remain the same even after this change of perspective. We still underinvest in cultivating long term benefits and avoiding long term costs, even if these costs and benefits won\u2019t compound for a billion years. There are other important differences, however.</p><p>The most important practical difference that the discovery of alien life would make to EA Longermist prescriptions is volatility preference. When the universe is morally empty except for humans, the cost of human extinction is much higher than the benefit of human flourishing, so it\u2019s often worth giving up some expected value for lower volatility. <a href=\"https://nickbostrom.com/existential/risks\"><u>Nick Bostrom encapsulates this idea in the Maxipok rule</u></a>.</p><blockquote><p><i>Maximize the probability of an okay outcome</i>, where an \u201cokay outcome\u201d is any outcome that avoids existential disaster.</p></blockquote><p>Since morally valuable aliens flatten out the non-linearity between catastrophe and extinction, EA Longermists must be much more open to high volatility strategies after the discovery of alien life. So they don\u2019t want to Maxipok, they want to maximize good ol\u2019 expected value. This makes things like AI and biotechnology which seem capable of both destroying us or bringing techno-utopia a lot better. In comparison, something like climate change which, in the no-aliens world, is bad but not nearly as bad as AI or bio-risk because it has low risk of complete extinction, looks worse than it used to.</p><p>The discovery of alien life would therefore bring EA Longtermism closer to the progress studies/Stubborn Attachments view. Avoiding collapse is almost as important as avoiding extinction, compounding benefits like economic growth and technological progress are the highest leverage ways to improve the future, not decreasing existential risks at all costs, and there is room for \u2018moral side constraints\u2019 because <a href=\"https://forum.effectivealtruism.org/posts/8wWYmHsnqPvQEnapu/getting-on-a-different-train-can-effective-altruism-avoid\"><u>existential risks no longer impose arbitrarily large utilitarian costs</u></a>.</p><h2>Examining and Relaxing Assumptions</h2><h3>Singer</h3><p>Probably the easiest assumption to drop is the third one which claims that alien life is morally valuable. Humans find it pretty easy to treat even other members of their species as morally worthless, let alone other animals. It would be difficult to convince most people that alien life is morally valuable, <a href=\"https://nautil.us/do-aliens-have-inalienable-rights-236557/\"><u>although E.T managed it</u></a>. Many find it intuitive to favor outcomes that benefit <i>homo sapiens </i>even if they come at the expense of other animals and aliens. This bias would make preserving humanity\u2019s long and large future important even if the universe would be filled with other types of moral value without us.</p><p>If you support including cows, chickens, octopi, and shrimp in our wide and growing moral circle, then it seems difficult to exclude advanced alien life without resorting to \u2018planetism.\u2019 It might be that humans somehow produce more moral value per unit energy than other forms of life which would be a less arbitrary reason to favor our success over aliens. Even after discovering the existence of alien life, however, we would not have nearly enough data to expect anything other than humans being close to average in this respect.</p><h3>Aliens</h3><p>The first assumption of observing alien civilization is sufficient but not entirely necessary for the same result. <strong>Observation of any alien life, even simple single cellular organisms, on a nearby planet greatly increases our best guess at the rate of formation of intelligent and morally valuable life in our galaxy</strong>, thus decreasing humanity\u2019s importance in the overall moral value of the universe.</p><p>Longtermism and wide moral circles may dilute existential risk worries even on earth alone. If humans go extinct, what are the chances that some other animal, perhaps another primate or the octopus, will fill our empty niche? Even if it takes 10 million years, it all rounds out in the grand scheme which Longtermism says is most important.</p><p><a href=\"https://grabbyaliens.com/\"><u>Robin Hanson\u2019s Grabby Aliens theory</u></a> implies that the very absence of aliens from our observation combined with our early appearance in the history of the universe is evidence that many alien civilizations will soon fill the universe. The argument goes like this:</p><p>Human earliness in the universe needs explaining. Earth is 13.8 billion years old, but the average start lives for 5 trillion years. Advanced life is the result of several unlikely events in a row so it\u2019s much more likely for it to arise later on the timeline than earlier.</p><p>One way to explain this is if some force prevents advanced life from arising later on the timeline. If alien civilizations settle the universe, they would prevent other advanced civilizations from arising. So the only time we could arise is strangely early on before the universe is filled with other life.</p><p>The fact that we do not yet see any of these galaxy settling civilizations implies that they must expand very quickly so the time between seeing their light and being conquered is always low.</p><p>The argument goes much deeper, but if you buy these conclusions along with Longtermism and a wide moral circle then humanity\u2019s future barely matters even if we don\u2019t find cities on Proxima D.</p><h2>Big If True</h2><p><strong>The base rate of formation of intelligent or morally valuable life on earth and in the universe is an essential but unknown parameter for EA Longtermist philosophy.</strong> Longtermism currently assumes that this rate is very low which is fair given the lack of evidence. If we find evidence that this rate is higher, then wide moral circle Longtermists should shift their efforts from shielding humanity from as much existential risk as possible, to maximizing expected value by taking higher volatility paths into the future.</p>", "user": {"username": "Maxwell Tabarrok"}}, {"_id": "PKKhGPHnyiaWtxACX", "title": "Apply for EAGxBerkeley ", "postedAt": "2022-10-12T20:45:35.876Z", "htmlBody": "<h1><a href=\"https://creatorapp.zohopublic.com/centreforeffectivealtruism/ea/form-perma/EAG_EAGx_Application_form/MkJjYp0dqYgRCOk8PZk6vQhRmxACvDnuOP7x1DPZqjyfr3QCuGyZBDHdOOMkhjuQ1HnBqTG9aObWyUWR5mWJH48kaeR7vQrbBsvv\"><strong>Applications are open! Apply here&nbsp;</strong></a></h1><h3>Applications close Sunday, Nov. 20th. <strong>But please apply early.</strong></h3><p><strong>Why?</strong></p><ul><li>We have a limited number of hotel rooms booked for attendees</li><li>Flights will likely be less expensive</li><li>You can invest more time prepping for the conference</li><li>It will let us better plan to meet your needs</li><li>You'll be helping us (the organisers!) out</li></ul><h3><strong>When:</strong> <strong>The conference is Dec 2nd (Friday) - Dec 4th (Sunday).&nbsp;</strong></h3><p><strong>Friday is a full program starting around 1 pm. </strong>There will be bonus content happening in Berkeley / San Francisco Monday the 5th and the week leading up to the conference! <strong>We recommend planning to be there Friday morning through Monday.</strong></p><h3><strong>The intended audience is new-er EA's in North America.&nbsp;</strong></h3><p>It's important to note that this is not an EA Global and the intended audience is not EAs from around the world. It is primarily for students and young professionals in North America (and more experienced EAs who are also nearby).</p><h3><strong>For more information on the conference, check out our</strong><a href=\"https://www.eaglobal.org/events/eagxberkeley2022/\"><strong> website</strong></a><strong> :)</strong></h3><h3><strong>Questions or concerns: </strong><a href=\"mailto:berkeley@eaglobalx.org\"><strong>email us</strong></a><strong> or message me</strong></h3>", "user": {"username": "ElikaSomani"}}, {"_id": "pHLjjqwBMT3h5W9KE", "title": "Growth Theory Reading List", "postedAt": "2022-10-12T14:26:03.665Z", "htmlBody": "<p>The reading list below is based on a reading list originally used for an internal GPI reading group in spring 2021, organized by&nbsp;<a href=\"https://forum.effectivealtruism.org/users/trammell\"><u>Phil Trammell</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/users/leopold\"><u>Leopold Aschenbrenner</u></a>, and on the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/L3WPuztkSMohBTWqZ/etgp-2022-materials-feedback-and-lessons-for-2023\"><u>ETGP 2022</u></a> syllabus, also organized by Phil. I also added a few resources I deemed relevant on some of the topics below, but the credit for almost all of these resources goes to Phil and Leopold.</p><p>My suggested usage for this reading list is in conjunction with, and as a complement to, the slides for the first week of ETGP 2022 (available&nbsp;<a href=\"https://docs.google.com/document/d/1dfPCyBhJ6_NnGRHVvaZDY3pT-1M5PVbsNIlT6fXwiJo/edit\"><u>here</u></a>), as the course was specifically designed to teach economic theory in a global prioritization context. As such, the structure of the reading list matches that of the course.</p><p><i>Disclaimer: The views presented in the readings suggested below do not necessarily represent views held by the reading group organizers, me, GPI, or any other GPI staff member.</i></p><h2>Motivation</h2><p>There are two main reasons why growth theory is particularly relevant for EAs. First, growth models allow for a lot of flexibility regarding which factors are included in the model, and which timescale one uses to evaluate the effects of changes in these factors. These characteristics make growth models useful even if one is ultimately only interested in factors other than economic growth, like AI or climate change. Second, economic growth plays a central role in the contemporary world, in a way that understanding its dynamics is likely key for global prioritization.</p><h1>1. Introduction and basic models</h1><ul><li>Jones, Charles I., and Dietrich Vollrath. 2013.&nbsp;<a href=\"https://books.google.co.uk/books/about/Introduction_to_Economic_Growth.html?id=cQPQLwEACAAJ&amp;redir_esc=y\"><i><u>Introduction to Economic Growth</u></i></a>.<ul><li>Chapter 2, up to section 2.1.4</li></ul></li><li>Trammell, Philip. 2020. \u201c<a href=\"https://globalprioritiesinstitute.org/philip-trammell-and-anton-korinek-economic-growth-under-transformative-ai/\"><u>Economic Growth under Transformative AI</u></a>.\u201d&nbsp;<ul><li>Section 2</li></ul></li></ul><h1>2. Accumulation-based models and growth versus level effects</h1><ul><li>Jones, Charles I., and Dietrich Vollrath. 2013.&nbsp;<a href=\"https://books.google.co.uk/books/about/Introduction_to_Economic_Growth.html?id=cQPQLwEACAAJ&amp;redir_esc=y\"><u>Introduction to Economic Growth</u></a>.<ul><li>Chapter 9</li></ul></li><li>Piketty, Thomas. 2014.&nbsp;<a href=\"https://books.google.co.uk/books/about/Capital_in_the_Twenty_First_Century.html?id=J222AgAAQBAJ&amp;redir_esc=y\"><u>Capital in the Twenty-First Century</u></a>.&nbsp;<ul><li>Section \u201cToo Much Capital Kills the Return on Capital\u201d up to and including \u201cCapital-Labor Substitution in the Twenty-First Century: An Elasticity Greater Than One\u201d</li></ul></li><li>Jones, Charles I. 2003. \u201c<a href=\"https://are.berkeley.edu/~zilber11/ARE241/fall2005/jones_alpha100.pdf\"><u>Growth, Capital Shares, and a New Perspective on Production Functions</u></a>.\u201d</li></ul><h1>3. Scale effects in researcher-based models</h1><ul><li>Jones, Charles I. 2005. \u201c<a href=\"https://doi.org/10.1016/S1574-0684(05)01016-6\"><u>Growth and Ideas</u></a>.\u201d<ul><li>Section 1</li><li>Section 3</li><li>Section 5 (skip 5.4)</li><li>Sections 6.1 and 6.2</li></ul></li><li>Bloom, Nicholas, Charles I. Jones, John Van Reenen, and Michael Webb. 2020. \u201c<a href=\"https://doi.org/10.1257/aer.20180338\"><u>Are Ideas Getting Harder to Find?</u></a>\u201d<ul><li>Introduction</li><li>See figures in the&nbsp;<a href=\"https://conference.nber.org/confer/2022/IRBCs22/ChadJones3.pdf\"><u>slides</u></a></li></ul></li><li>Bond-Smith, Steven. 2019. \u201c<a href=\"https://doi.org/10.1111/joes.12329\"><u>The Decades-Long Dispute Over Scale Effects in the Theory of Economic Growth</u></a>.\u201d</li></ul><h1>4. Existential risk and growth</h1><ul><li>Aschenbrenner, Leopold. 2020. \u201c<a href=\"https://globalprioritiesinstitute.org/leopold-aschenbrenner-existential-risk-and-growth/\"><u>Existential Risk and Growth</u></a>.\u201d</li><li>Trammell, Philip. 2021. \u201c<a href=\"https://philiptrammell.com/static/ExistentialRiskAndExogenousGrowth.pdf\"><u>Existential Risk and Exogenous Growth</u></a>.\u201d</li><li>Hilton, Benjamin. 2021. \u201c<a href=\"https://drive.google.com/file/d/1OEAlS-Bsak5kRbfFepsDY1SXn3YEhSvi/view\"><u>Existential Risk Mitigation as a Public Good</u></a>.\u201d</li><li>Hilary Greaves\u2019s&nbsp;<a href=\"https://www.youtube.com/watch?v=WctJDYUuWuc\"><u>lecture</u></a> on longtermism and economic growth</li></ul><h1>5. Long-run historical growth</h1><ul><li>Kremer, Michael. 1993. \u201c<a href=\"https://doi.org/10.2307/2118405\"><u>Population Growth and Technological Change: One Million B.C. to 1990</u></a>.\u201d</li><li>Jones, Charles I. \u201c<a href=\"http://web.stanford.edu/~chadj/VeryLongRun.pdf\"><u>Economic Growth over the Very Long Run</u></a>.\u201d</li><li>Roodman, David. 2021. \u201c<a href=\"https://www.openphilanthropy.org/sites/default/files/Modeling-the-human-trajectory.pdf\"><u>On the Probability Distribution of Long-Term Changes in the Growth Rate of the Global Economy: An Outside View</u></a>.\u201d</li></ul><h1>6. The mechanics of the industrial revolution \u201cphase change\u201d</h1><ul><li>Galor, Oded, and David N. Weil. 2000. \u201c<a href=\"https://doi.org/10.1257/aer.90.4.806\"><u>Population, Technology, and Growth: From Malthusian Stagnation to the Demographic Transition and Beyond</u></a>.\u201d</li><li>Crafts, Nicholas. 2011. \u201c<a href=\"https://doi.org/10.1017/S1361491610000201\"><u>Explaining the First Industrial Revolution: Two Views</u></a>.\u201d</li></ul><h1>7. AI and growth</h1><ul><li>Trammell, Philip. 2020. \u201c<a href=\"https://globalprioritiesinstitute.org/philip-trammell-and-anton-korinek-economic-growth-under-transformative-ai/\"><u>Economic Growth under Transformative AI</u></a>.\u201d&nbsp;<ul><li>Section 1</li><li>Sections 3.1-3.3</li><li>Section 4.1-4.2</li><li>Section 5.2</li><li>Section 6</li><li>Section 7</li></ul></li><li>Aghion, Philippe, Benjamin F Jones, and Charles I Jones. 2019. \u201c<a href=\"https://web.stanford.edu/~chadj/AJJ-AIandGrowth.pdf\"><u>Arti\ufb01cial Intelligence and Economic Growth</u></a>.\u201d<ul><li>GovAI&nbsp;<a href=\"https://www.fhi.ox.ac.uk/jonesjones/\"><u>Presentation</u></a></li></ul></li><li>Davidson, Tom. 2021. \u201c<a href=\"https://www.openphilanthropy.org/research/could-advanced-ai-drive-explosive-economic-growth/\"><u>Could Advanced AI Drive Explosive Economic Growth?</u></a>\u201d</li></ul><h1>8. Stagnation</h1><ul><li>Jones, Charles. 2022. \u201c<a href=\"https://web.stanford.edu/~chadj/emptyplanet.pdf\"><u>The End of Economic Growth? Unintended Consequences of a Declining Population</u></a>.\u201d<ul><li><a href=\"https://www.fhi.ox.ac.uk/jonesjones/\"><u>Video</u></a> (starting around minute 25)</li></ul></li><li>Jones, Charles I. 2016. \u201c<a href=\"https://web.stanford.edu/~chadj/LifeandGrowthJPE2016.pdf\"><u>Life and Growth</u></a>.\u201d<ul><li>Section I</li><li>Section II</li></ul></li><li>Rachel, \u0141ukasz. 2020. \u201c<a href=\"https://economics.princeton.edu/wp-content/uploads/2021/01/RachelLukasz-JMP.pdf\"><u>Leisure-Enhancing Technological Change</u></a>.\u201d<ul><li>Section 1&nbsp;</li><li>Section 2, until \u201cBalanced growth preferences with growing M.\u201d</li></ul></li></ul><h1>9. Growth and human happiness</h1><ul><li>Easterlin, Richard A., and Kelsey O\u2019Connor. 2020. \u201c<a href=\"https://doi.org/10.2139/ssrn.3743147\"><u>The Easterlin Paradox</u></a>.\u201d</li><li>Clark, Andrew E., Paul Frijters, and Michael A. Shields. 2008. \u201c<a href=\"https://doi.org/10.1257/jel.46.1.95\"><u>Relative Income, Happiness, and Utility: An Explanation for the Easterlin Paradox and Other Puzzles</u></a>.\u201d<ul><li>Section 2</li><li>Section 3.4</li><li>Section 5.1</li></ul></li><li>Jones, Charles I., and Peter J. Klenow. 2016. \u201c<a href=\"https://doi.org/10.1257/aer.20110236\"><u>Beyond GDP? Welfare across Countries and Time</u></a>.\u201d<ul><li>Introduction</li></ul></li><li>Klenow, Peter, Charles I. Jones, Mark Bils, and Mohamad Adhami. 2022. \u201c<a href=\"https://web.stanford.edu/~chadj/slides-popwelfare.pdf\"><u>Population and Welfare: The Greatest Good for the Greatest Number</u></a>.\u201d</li></ul><h1>10. Inequality and growth</h1><ul><li>B\u00e9nabou, Roland. 1996. \u201c<a href=\"https://doi.org/10.1086/654291\"><u>Inequality and Growth</u></a>.\u201d</li><li>Jones, Charles I. 2022. \u201c<a href=\"https://doi.org/10.1086/720394\"><u>Taxing Top Incomes in a World of Ideas</u></a>.\u201d</li><li>Stiglitz, Joseph E. 2016. \u201c<a href=\"https://doi.org/10.7916/d8-gjpw-1v31\"><u>Inequality and Economic Growth</u></a>.\u201d</li></ul>", "user": {"username": "LuisMota"}}, {"_id": "BL8Rny4hBgDvLsjci", "title": "Hackathon on Mon, 12/5 to follow EAGxBerkeley", "postedAt": "2022-10-15T00:06:16.306Z", "htmlBody": "<p>We're going to do a Hackathon on Mon, 12/5 at <a href=\"https://shop.sportsbasement.com/blogs/stores/berkeley\">Sports Basement Berkeley</a> from 10am to 5pm following <a href=\"https://eaglobal.org/events/eagxberkeley2022/\">EAGxBerkeley</a>.</p><ul><li><i><strong>Who</strong></i>:&nbsp;anyone!&nbsp;software engineers will be primary contributors of course, but we will offer optional introductory sessions for the curious / aspiring developer.<strong>&nbsp;You do not have to have attended </strong><a href=\"https://eaglobal.org/events/eagxberkeley2022/\"><strong>EAGxBerkeley</strong></a><strong> to attend the Hackathon.</strong></li><li><i><strong>Where</strong></i>:&nbsp; <a href=\"https://shop.sportsbasement.com/blogs/stores/berkeley\">Sports Basement Berkeley</a> at&nbsp;2727 Milvia St.&nbsp;<ul><li>Note this is 15 - 20 minutes from the conference by public transit or car.&nbsp; We recommend taking <a href=\"https://docs.google.com/document/d/19HiXtzIXqrV0v6ZA_pXC4vgcbiIh2pf8V-WUS8XC8rY/edit\">BART</a>.</li></ul></li><li><i><strong>When</strong></i>:&nbsp; Mon, 12/5 from 10am - 5pm</li><li><i><strong>What</strong></i>:&nbsp; Work independently or with collaborators on EA-aligned project of your choosing&nbsp;&nbsp;</li></ul><p>If you would like to <strong>submit a potential Hackathon project idea</strong>, please leave a <strong>comment</strong>!</p><p>As a Hackathon participant, be on the lookout for related events each day of the conference:</p><ul><li>On Friday evening, we'll meet for a social around dinner time</li><li>On Saturday, we'll have a speed networking event for Software Engineers</li><li>On Sunday, we'll have a Hackathon Q&amp;A and planning session</li></ul><p>The scheduled events for the Hackathon will begin Monday at 10am when Sports Basement Berkeley opens to participants.&nbsp;</p><ul><li>10am \u2014 venue opens</li><li>10:15 \u2014 opening talk</li><li>10:30 \u2014 project pitches \u2014 people with ideas can share those to the group</li><li>10:45 \u2014 start work</li><li>12pm \u2014 lunch</li><li>4:55 \u2014 wrap up</li></ul><p>We will also be offering optional 45-minute learning sessions:</p><ul><li>10:45 \u2014 basics of git (for newcomers to coding)</li><li>1pm \u2014 setting up your development environment (for newcomers)</li><li>2pm \u2014 intro to frontend development (for everyone)</li><li>3pm \u2014 intro to backend development (for everyone)</li></ul><p>&nbsp;These talks will be at a separate table so that we are minimally disruptive to people who are still working.</p><p>In terms of projects, we hope people will work on things that they find to be interesting and potentially impactful. &nbsp;Keep in mind, there may be the option to continue collaborating virtually, so don't limit the scope of your ambitions to what can be accomplished in a single day. &nbsp;</p><p>We hope you'll join us for the EA Hackathon!<i> &nbsp;</i>Please complete this <a href=\"https://forms.gle/e4orN6MbxzycUokv9\"><strong>Expression of Interest form</strong></a> and consider signing up for the <a href=\"https://subscribepage.io/eaSoftwareEngineers\"><strong>EA Software Engineering mailing list</strong></a>. &nbsp;And don't forget to <a href=\"https://creatorapp.zohopublic.com/centreforeffectivealtruism/ea/form-perma/EAG_EAGx_Application_form/MkJjYp0dqYgRCOk8PZk6vQhRmxACvDnuOP7x1DPZqjyfr3QCuGyZBDHdOOMkhjuQ1HnBqTG9aObWyUWR5mWJH48kaeR7vQrbBsvv\"><strong>apply to EAGxBerkeley</strong></a> if you want to go to that too.</p>", "user": {"username": "NicoleJaneway"}}, {"_id": "A2tcxJEd8DZkLtujR", "title": "What makes a good 1-1? How important are they as a CB strategy?", "postedAt": "2022-10-12T14:13:49.877Z", "htmlBody": "<p>It's a super open ended question and I know there's lots already on this, but would love any and all opinions around the topic. Some specific sub-questions I have:</p><ul><li>How valuable are 1-1s for new-EAs? In terms of building EA connections, feeling apart of the community, career value/measured impact, etc?</li><li>Are intro 1-1s better than career specific 1-1s for early stage EAs?&nbsp;</li><li>What do good 1-1s look like virtually (over Zoom, etc)?</li><li>Should they be structured (ex. going through a document) or unstructured?</li><li>Should there be follow up? Does it make a difference in terms of the participants further engagement in EA?&nbsp;</li><li>What are ways to improve the quality / outcomes of 1-1s?</li><li><strong>Are 1-1's 'worth it'?&nbsp;</strong><ul><li><strong>Should we just have a sink-or-swim &nbsp;mindset where people have to get further involved in EA only by their own effort?</strong></li></ul></li></ul><p>I'd also love just general reflections on how you do 1-1s, what makes them 'worth it', and general advice.</p>", "user": {"username": "ElikaSomani"}}, {"_id": "YCobjyMhaKHArjwQt", "title": "What resources should job seekers know about?", "postedAt": "2022-10-12T11:31:35.271Z", "htmlBody": "<p>Alternate title: EA Job Boards Board</p><p><a href=\"https://xkcd.com/927/\">Relevant XKCD</a></p><p><a href=\"https://forum.effectivealtruism.org/posts/bbtvDJtb6YwwWtJm7/epistemic-status-an-explainer-and-some-thoughts\">Epistemic status</a>: this is a quickly drafted and incomplete list. This is not wisdom.</p><p>&nbsp;</p><p>What am I missing from this list of organizations and options relevant to job-seekers?</p><p>&nbsp;</p><p>Career/Jobs Organizations<br><a href=\"https://80000hours.org/\">80,000 Hours</a><br><a href=\"https://www.animaladvocacycareers.org/\">Animal Advocacy Careers</a><br><a href=\"https://www.highimpactprofessionals.org/\">High Impact Professionals</a></p><p><a href=\"https://forum.effectivealtruism.org/posts/vGiyvfaGGFEzQsETR/what-s-happening-in-australia#High_Impact_Recruitment\">High Impact Recruitment</a><br><a href=\"https://ea-internships.pory.app/\">EA Opportunities</a><br><a href=\"https://www.eapathfinder.org/\">EA Pathfinder</a><br><a href=\"https://www.openphilanthropy.org/career-development-and-transition-funding/\">Open Philanthropy's career development and transition funding</a> (they may have other services I have failed to include)<br><a href=\"https://pineappleoperations.org/\">Pineapple Operations</a><br><a href=\"https://www.probablygood.org/\">Probably Good</a></p><p><a href=\"https://www.successif.org/\">Successif</a><br><a href=\"https://www.trainingforgood.com/\">Training for Good</a></p><p><a href=\"https://www.nonlinear.org/blog/thinking-about-starting-an-org-in-ai-safety-get-career-advice-from-nonlinear \">Nonlinear advises for AI safety careers in technical, governance, or meta</a></p><p>(don\u2019t forget to actually talk to the orgs who offer advising or other services)<br>&nbsp;</p><p>Facebook groups (such as <a href=\"https://www.facebook.com/groups/1062957250383195\">effective altruism job postings</a>) - there are a number of industry or location oriented groups as well. See this <a href=\"https://www.facebook.com/EffectiveGroups\">effective altruism groups directory</a> - or this<a href=\"https://forum.effectivealtruism.org/community#online\"> list of online groups</a>, some of which are profession oriented</p><p>The opportunities channel &nbsp;in the <a href=\"https://www.effectivealtruismanywhere.org/\">EA Anywhere</a> Slack.</p><p>&nbsp;</p><p><a href=\"https://www.eaglobal.org/\">EA Global</a> events are very much networking events and, I suspect, partially job fairs.<br>&nbsp;</p><p>Don\u2019t forget about high-impact jobs outside of explicitly-EA orgs.</p><p>80,000 Hours may have a big list of unexplored, potentially high-impact paths but I couldn't find it in a quick search.<br>&nbsp;</p><p>I\u2019ve probably got a huge blindspot around options for students and academics so those are largely missing from this list. Scholarships for students?<br>&nbsp;</p><p>Make your own job:</p><ul><li>apply for a grant (<a href=\"https://forum.effectivealtruism.org/topics/funding-opportunities?utm_source=The+EA+Newsletter&amp;utm_campaign=30a1d41c6e-EMAIL_CAMPAIGN_2022_10_17_09_48&amp;utm_medium=email&amp;utm_term=0_51c1df13ac-30a1d41c6e-309681869\">big list here</a>, or <a href=\"https://manifund.org\">manifund.</a>, &nbsp;<a href=\"https://ftxfuturefund.org/\"><s>FTX Future Fund</s></a><s> </s>or a regranter, community building grants from the Centre for Effective Altruism, and likely more)</li><li>start your own charity/organization/enterprise (e.g. with Charity Entrepreneurship, or on your own)</li><li>start a startup and earn to give</li><li>start an impact-focused for-profit company</li></ul><p>(Considerations apply when starting your own project, such as: downside risk, but also the risk of being ambitious enough)</p><p>&nbsp;</p><p>Make someone else\u2019s job:</p><ul><li>earn to give some other way</li><li>or save for your future self to put time towards impact</li></ul><p>&nbsp;</p><p>Win your job retrospectively:</p><ul><li><a href=\"https://www.super-linear.org/\">prizes</a></li><li>impact markets / impact certificates</li></ul>", "user": {"username": "KevinO"}}, {"_id": "oWa4MGtqC4pRpdufT", "title": "Singapore - Small casual dinner in Chinatown #5", "postedAt": "2022-10-12T08:59:10.238Z", "htmlBody": "", "user": {"username": "Joe Rocca"}}, {"_id": "kSw7xp83o4nQ3sQgM", "title": "What are the best examples of \"how to work with me\" documents?", "postedAt": "2022-10-12T07:08:54.356Z", "htmlBody": "<p>Have any stood out in your mind as very well put-together? I'm looking for ideas and inspiration for creating one.</p>", "user": {"username": "jlemien"}}, {"_id": "ZXTqMektxs2LNyMim", "title": "My argument against AGI", "postedAt": "2022-10-12T06:32:25.809Z", "htmlBody": "<p>This is the third post about my argument to try and convince the Future Fund Worldview Prize judges that \"all of this AI stuff is a misguided sideshow\". My first post was an extensive argument that unfortunately confused many people.&nbsp;</p><p>(<a href=\"https://forum.effectivealtruism.org/posts/FdAfhdsSGKxP6axZY/the-probability-that-artificial-general-intelligence-will-be\">The probability that Artificial General Intelligence will be develop</a>)</p><p>My second post was much more straightforward but ended up focusing mostly on revealing the reaction that some \"AI luminaries\" have shown to my argument<br><a href=\"https://forum.effectivealtruism.org/posts/aBR589xDLLPb8cg48/don-t-expect-agi-anytime-soon\">(Don't expect AGI anytime soon)</a></p><p>Now, as a result of answering many excellent questions that exposed the confusions caused by my argument, I believe I am in a position to make a very clear and brief summary of the argument in point form.</p><p>To set the scene, the Future Fund is interested in predicting when we will have AI systems that can match human level cognition: \"This includes entirely AI-run companies, with AI managers and AI workers and everything being done by AIs.\" This is a pretty tall order. It means systems with advanced planning and decision making capabilities. But this is not the first time people predicted that we will have such machines. In my first article I reference a 1960 paper which states that the US Air Force predicted such a machine by 1980. The prediction was based on the same \"look how much progress we have made, so AGI can't be too far away\" argument we see today. There must be a new argument/belief if today's AGI predictions are to bear more fruit than they did in 1960. My argument identifies this new belief. Then it shows why the belief is wrong.</p><p>Part 1</p><ol><li>Most of the prevailing cognitive theories involve <i>classical</i> symbol processing systems (with a combinatorial syntax and semantics, like formal logic). For example, theories of reasoning and planning involve logic like processes and natural language is thought by many to involve phrase structure grammars, like for example Python does.</li><li>Good old-fashioned AI was (largely) based on the same assumption, that classical symbol systems are necessary for AI.</li><li>Good old-fashioned AI failed, showing the limitations of classical symbol systems.</li><li>Deep Learning (DL) is an alternative form of computation that does not involve classical symbol systems, and its amazing success shows that human intelligence is not based on classical symbolic systems. In fact, Geoff Hinton in his Turing Award Speech proclaimed that \"the success of machine translation is the last nail in the coffin of symbolic AI\".</li><li>DL will be much more successful than symbolic AI because it is based on a better model of cognition: the brain. That is, the brain is a neural network, so clearly neural networks are going to be better models.</li><li>But hang on. DL is now very good at producing syntactically correct Python programs. But argument 4. should make us conclude that Python does not involve classical symbolic systems because a non-symbolic DL model can write Python. Which is patently false. The argument becomes a <i>reductio ad absurdum</i>. One of the steps in the argument must be wrong, and the obvious choice is 4, which gives us 7.</li><li>The success of DL in performing some human task tells us nothing about the underlying human competence needed for the task. For example, natural language might well be the production of a generative grammar in spite of the fact that statistical methods are currently better than methods based on parsing.&nbsp;</li><li>Point 7. defeats point 5. There is no scientific reason to believe DL will be much more successful than symbolic AI was in attaining some kind of general intelligence.</li></ol><p>Part 2</p><ol><li>In fact, some of my work is already done for me as many of the top experts concede that DL alone is not enough for \"AGI\". They propose a need for a symbolic system to supplement DL, in order to be able to do planning, high level reasoning, abductive reasoning, and so on.&nbsp;</li><li>The symbolic system should be <i>non-classical</i> because of Part 1 point 2 and 3. That is, we need something better than classical systems because good old-fashioned AI failed as a result of its assumptions about symbol systems.</li><li><i>DL-symbol systems</i> (whatever those are) will be much better because DL has already shown that classical symbol systems are not the right way to model cognitive abilities.</li><li>But Part 1 point 7 defeats Part 2 point 3. We don't know that DL-symbol systems (whatever those are) will be much better than classical AI because DL has not shown anything about the nature of human cognition.</li><li>We have no good reason, only faith and marketing, to believe that we will accomplish AGI by pursuing the DL based AI route. The fact that DL can do Python shows that it is good at mimicking symbolic systems when lots of example productions are available, like language and Python. But it struggles in tasks like planning where such examples aren't there.</li><li>We should instead focus our attention of human-machine <i>symbiosis</i>, which explicitly designs systems that supplement rather than replace human intelligence.</li></ol>", "user": {"username": "cveres"}}, {"_id": "GS3DHjJeWD6o5m9sK", "title": "A call for EA Hubs to post public guides", "postedAt": "2022-10-13T18:34:11.861Z", "htmlBody": "<p>Write a guide to your city if you\u2019re hosting a conference, consider yourself a major EA hub, are having a seasonal push, or expect a constant influx of EAs.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefn75mkzcvy4l\"><sup><a href=\"#fnn75mkzcvy4l\">[1]</a></sup></span>&nbsp;</p><p>You can use <a href=\"https://forum.effectivealtruism.org/posts/W2w7xA9AtDnjcK6DP/an-ea-s-guide-to-berkeley-and-the-bay\">our template</a> and time-cap yourself at 2 hours.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9ug4o6y1byu\"><sup><a href=\"#fn9ug4o6y1byu\">[2]</a></sup></span></p><p>Cities<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefve0jfmihs4\"><sup><a href=\"#fnve0jfmihs4\">[3]</a></sup></span>&nbsp;we'd love to see guides from (in no particular order):</p><ul><li>New York</li><li>London</li><li>Prague</li><li>Berlin</li><li>DC</li><li>Boston</li><li>Oxford</li><li>Cambridge</li><li>Singapore</li><li>Mexico</li><li>Israel</li><li>Manila</li><li>Sydney</li></ul><p>Bonus: consider doing some outcomes research on how this helps build EA connections and is impactful<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref075qx5mv7xgq\"><sup><a href=\"#fn075qx5mv7xgq\">[4]</a></sup></span></p><p><a href=\"https://forum.effectivealtruism.org/posts/6whiBq7czKJk4Bx29/a-forum-post-can-be-short\">Posts can be short</a>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8d8963vonat\"><sup><a href=\"#fn8d8963vonat\">[5]</a></sup></span></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnn75mkzcvy4l\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefn75mkzcvy4l\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Why is this useful? Building EA connections, building a local and welcoming EA community, it's helpful for new EAs moving to your city, etc. We think it's helpful to make hubs accessible to the community, especially for newer, less well-connected folks.&nbsp;</p><p>You can also save yourself some time by adding in FAQs and forwarding people to the guide!&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9ug4o6y1byu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9ug4o6y1byu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Got more time? Share it with 5 friends to add their takes, and share it with a few non-locals to make sure it's useful!&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnve0jfmihs4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefve0jfmihs4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;and countries? Singapore is a city-state. Mexico is mostly Mexico City but maybe a country one works?</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn075qx5mv7xgq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref075qx5mv7xgq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We're currently researching the 'Visiting the Bay Summer 2022' push. If you're interested in helping out - comment below or reach out to <a href=\"https://forum.effectivealtruism.org/users/elika\">Elika</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8d8963vonat\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8d8963vonat\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We wrote this in 10 minutes</p></div></li></ol>", "user": {"username": "ElikaSomani"}}, {"_id": "W2w7xA9AtDnjcK6DP", "title": "An EA's Guide to Berkeley and the Bay Area", "postedAt": "2022-10-13T18:34:04.000Z", "htmlBody": "<p><img style=\"width:100%\" src=\"https://res.cloudinary.com/cea/image/upload/v1675224728/mirroredImages/W2w7xA9AtDnjcK6DP/rvwvy1zuxe0nbyfmxejs.jpg\"></p><p>If you are visiting or new to the Bay (and Berkeley specifically) and are looking for an EA community, this guide is a useful place to start. It can be a difficult place to navigate for newcomers, and in this guide we (Elika and Vaidehi, communtiy members of East Bay EA) hope to make it a bit easier. <i>This guide is not a representation of the views of everyone in the community or any organisation.</i></p><p>This guide is most helpful if you\u2019re already planning or seriously considering coming to Berkeley and want to get more context on the community and culture, rather than trying to convince you to come or be fully comprehensive. We mostly focus on the Berkeley community in this guide, but a lot of what we talk about is relevant for the San Francisco community as well.&nbsp;</p><p>We try to be pretty frank on the challenges and the negatives because we think it\u2019s important to give an accurate picture of the community. That being said, Berkeley (and the Bay) is a great place with a really vibrant EA community!&nbsp;<strong>We hope you enjoy it and are welcomed warmly :) To encourage that, feel free to reach out to any local community organisers listed in the&nbsp;</strong><a href=\"https://forum.effectivealtruism.org/posts/W2w7xA9AtDnjcK6DP/an-ea-s-guide-to-berkeley-and-the-bay#People\"><strong><u>People</u></strong></a><strong> section! We also recommend joining the </strong><a href=\"https://forum.effectivealtruism.org/posts/W2w7xA9AtDnjcK6DP/an-ea-s-guide-to-berkeley-and-the-bay#Useful_Links\"><strong>group chats listed</strong></a><strong>.&nbsp;</strong></p><h1><strong>Overview</strong></h1><figure class=\"image image_resized\" style=\"width:59.29%\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1675224729/mirroredImages/W2w7xA9AtDnjcK6DP/vlrnzi5a4kpuigl4yfgj.png\"></figure><p>Here\u2019s a map. EAs usually mean San Francisco or Berkeley in the East Bay (a uni town for UC Berkeley 45 minutes from San Francisco) when they say \u201cthe bay\u201d.&nbsp;</p><p>Berkeley is particularly focused on technical AI alignment, housing organisations like&nbsp;<a href=\"https://www.redwoodresearch.org/\"><u>Redwood Research</u></a>,&nbsp;<a href=\"http://www.rationality.org/\"><u>CFAR</u></a>,&nbsp;<a href=\"https://intelligence.org/\"><u>MIRI</u></a>,&nbsp;<a href=\"https://humancompatible.ai/\"><u>CHAI&nbsp;</u></a>(at UC Berkeley) and&nbsp;<a href=\"https://www.lightconeinfrastructure.com/\"><u>Lightcone Infrastructure</u></a>. As a result, there is significant overlap between the EA and rationalist communities in Berkeley, much more so than any other EA community (<a href=\"https://docs.google.com/document/d/11s9Us2bDTsAaCEF9xMW6IjNmgX06IjA-6sM4eVwIfLQ/edit#heading=h.9bivqsgutvb7\"><u>see more</u></a>). Generally, most senior or engaged community members in Berkeley (and many in SF) are longtermists and work on AI safety.&nbsp;</p><p>Many longtermist EA programs in summer of 2022 are being hosted in Berkeley and bring in people from all over, such as the SERI MATS program and the EA Communications Fellowship.&nbsp;</p><p>The San Francisco community is overlapping but distinct, with relatively less rationalists and AI focused people. Organisations who have offices in SF include Open Phil, GiveWell, Founders\u2019 Pledge, and some EA startups like Mission Barns (alternative meats) and Momentum. OpenAI and Anthropic also have offices in SF.</p><p>The Bay is a major hub because of founders' effects (and <a href=\"https://forum.effectivealtruism.org/posts/W2w7xA9AtDnjcK6DP/an-ea-s-guide-to-berkeley-and-the-bay-area?commentId=ewvdcCE6pH6pqLvBN#comments\">'funder's effects'</a>) - several historically important founding communities and organisations of the EA movement were founded &nbsp;here / or are based here for many years. This includes rationalist organisations like LessWrong, CFAR and MIRI in Berkeley, and separately GiveWell in San Francisco. Find links to&nbsp;<a href=\"https://docs.google.com/document/d/11s9Us2bDTsAaCEF9xMW6IjNmgX06IjA-6sM4eVwIfLQ/edit#heading=h.76uzx6govn5k\"><u>community pages here</u></a><strong>,&nbsp;</strong><a href=\"https://forum.effectivealtruism.org/posts/ZRZHJ3qSitXQ6NGez/about-going-to-a-hub-1\"><u>advice about moving to an EA hub here</u></a><strong>.&nbsp;</strong></p><h1><strong>Culture</strong></h1><p>The EA culture in Berkeley is influenced by the communities it is a part of - the rationality, silicon valley / tech communities. Most people are very passionate and dedicated longtermists who tend to take existential risks very seriously and/or have short AGI timelines. There can be a \u201cdoom-y\u201d vibe (sometimes in social settings as well, although you can find events that are more fun). Below is an incomplete list of some aspects of the community that might be important to contextualise your experiences:&nbsp;</p><ul><li>The community in Berkeley is overwhelmingly longtermist. You may feel alienated if you don\u2019t hold these beliefs, or find it more difficult to find lots of in-depth conversations on other cause areas because people are primarily interested in a few topics.&nbsp;</li><li>There can be a more intense work culture in the Bay, with high variance. For some people it\u2019s normal to work long hours and come in on the weekends. This can be really motivating and inspiring for some people. But there can also be pressure to work a lot, and to some it\u2019s overall good. However, you don\u2019t need to do this if you don\u2019t think it\u2019s something that would benefit your work or health. It\u2019s okay to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zu28unKfTHoxRWpGn/you-have-more-than-one-goal-and-that-s-fine\"><u>have more than one goal</u></a>.&nbsp;</li><li>Many EAs are part of the rationality community. One norm is that they have more of an&nbsp;<a href=\"https://www.lesswrong.com/tag/guess-ask-tell-culture\"><u>ask or tell culture</u></a>: they can be very direct and open which may come off as rude if you\u2019re not used to it. If you have time, familiarising yourself with rationality might be a helpful (and useful) thing to do.</li><li>The community is not diverse (even by EA standards). The Bay community is very white / male, in part because of the focus on technical AI safety work. This can be a bit disconcerting at first, but people are generally pretty friendly.&nbsp;</li><li>Many people value their time very highly in the Bay (and are quite busy). As a result, they often choose to funge time with money, and the wealth culture might be a bit of a culture shock at first. If you visit the&nbsp;<a href=\"https://docs.google.com/document/d/11s9Us2bDTsAaCEF9xMW6IjNmgX06IjA-6sM4eVwIfLQ/edit#heading=h.z927lfwkgt4n\"><u>longtermist coworking spaces</u></a> you\u2019ll notice they are well-stocked, have meals fully provided, nap rooms, etc. Many EAs will commonly refer to the value of their time and how it impacts their actions (ex. Ubering instead of walking).&nbsp;</li><li>Many EA or rationality community members in the Bay Area community are&nbsp;<a href=\"https://psychcentral.com/health/polyamorous-relationship\"><u>polyamorous</u></a> (or poly). If you aren\u2019t sure and you might be romantically interested in someone, just ask! People are very transparent about this. (See also: the section on&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/fovDwkBQgTqRMoHZM/power-dynamics-between-people-in-ea#Romantic_relationships\"><u>romantic relationships</u></a> in this post on power dynamics). Many community members use&nbsp;<a href=\"https://www.reciprocity.io/\"><u>Reciprocity</u></a> for dating. Reciprocity is a website where you can indicate if you want to be friends and/or date your Facebook friends, and if the other person checks the same box you\u2019ll see if you\u2019ve matched.&nbsp;</li><li>The Bay has a high concentration of funders and EA organisations, so there can be some complicated social dynamics which are not clear to newcomers. Julia Wise\u2019s (excellent)&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/fovDwkBQgTqRMoHZM/power-dynamics-between-people-in-ea\"><u>post</u></a> on power dynamics in EA outlines some of these concerns, and are worth keeping in mind when you interact with people.</li><li>The community is pretty large and fairly decentralised as far as event organising goes, and there are many social events (such as open-invite house parties or gatherings) organised by community members (check out the&nbsp;links below).&nbsp;</li><li>One flipside of having many community members are already familiar with EA, a lot of EA / rationality context is assumed, and people often use jargon that may not always be familiar. Don\u2019t hesitate to ask if you\u2019re not sure what people are talking about - they\u2019re often happy to explain! If you don\u2019t have a lot of experience with STEM-heavy or technical conversations, some conversations may feel alienating or boring.&nbsp;</li><li>We strongly recommend reading <a href=\"https://forum.effectivealtruism.org/posts/ZRZHJ3qSitXQ6NGez/about-going-to-a-hub-1\"><u>Advice about Going to an EA Hub</u></a>&nbsp;</li></ul><figure class=\"image image_resized\" style=\"width:80.86%\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1675224728/mirroredImages/W2w7xA9AtDnjcK6DP/g4vp3bm5ylf44frv4kbo.jpg\"><figcaption>We do like to have fun too. This was a painting picnic party :)</figcaption></figure><p>There is a slight distinction in the kinds of people in the Berkeley EA community and the San Francisco (SF) EA community, because Berkeley is a hub for technical safety work and San Francisco is more broad. The SF EA community is slightly less associated with the rationalist community and less longtermist / technical AI safety focused and has more professionals. That being said, there\u2019s lots of intermixing in community members and culture between both cities.&nbsp;</p><p>Importantly, EA culture is not necessarily reflective of the overall culture in the Bay Area. For broader social context, the Bay Area is very politically progressive and wokeness is highly valued. There is a long history of advocacy spanning environmental, disability rights, US Anti-apartheid, Black Power, and slow food movements, as well as countless others. Even before its statehood, California has been famous for attracting immigrants, which is reflected today by its diverse demographics and large Latino and Asian populations (moreso in Oakland and SF than in Berkeley). Although the region is wealthy, it also has enormous and highly visible wealth disparities. The rise of tech is relatively recent, and is considered as somewhat \u201cother\u201d from the standard cultural identity of many long-time residents.</p><h1><strong>Meeting people</strong></h1><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1675224728/mirroredImages/W2w7xA9AtDnjcK6DP/dylfyylt4ok1wa6nalv8.jpg\"></p><p>It can sometimes be difficult to get started in the Berkeley community if you don\u2019t already know people, so you may need to be a bit more outgoing and take initiative to meet people if you don\u2019t already know anyone.&nbsp;<strong>You can reach out to&nbsp;</strong><a href=\"https://docs.google.com/document/d/11s9Us2bDTsAaCEF9xMW6IjNmgX06IjA-6sM4eVwIfLQ/edit#heading=h.5uhc0jk2lxko\"><strong><u>community members&nbsp;</u></strong></a><strong>who\u2019ve volunteered to be points of contact if you\u2019d like some support!</strong></p><ul><li>You can meet people at East Bay EA community events in Berkeley (see the&nbsp;<a href=\"https://docs.google.com/document/d/11s9Us2bDTsAaCEF9xMW6IjNmgX06IjA-6sM4eVwIfLQ/edit#heading=h.7qsy1gggw1fg\"><u>links below</u></a>). This includes monthly dinners at both the Lodge &amp; Burrow (two EA group houses), and picnics and hikes every couple of months.&nbsp;</li><li>There are also a lot of informal hangouts, parties and dinners at group houses too, which people might invite you to. Some of them are posted to the&nbsp;<a href=\"https://www.facebook.com/groups/1100350234030076\"><u>East Bay EA Hangout</u></a> Facebook page.&nbsp;</li><li>Many EAs (and Bay Area people in general) like rock climbing, there\u2019s a group chat (<a href=\"https://docs.google.com/document/d/11s9Us2bDTsAaCEF9xMW6IjNmgX06IjA-6sM4eVwIfLQ/edit#heading=h.7qsy1gggw1fg\"><u>see below</u></a>) you can join if you want to find climbing buddies. You can also post on the&nbsp;<a href=\"https://docs.google.com/document/d/11s9Us2bDTsAaCEF9xMW6IjNmgX06IjA-6sM4eVwIfLQ/edit#heading=h.iyzhuq92tx5k\"><u>visiting the bay chats</u></a> to find people to do&nbsp;<a href=\"https://docs.google.com/document/d/11s9Us2bDTsAaCEF9xMW6IjNmgX06IjA-6sM4eVwIfLQ/edit#heading=h.5wftwxo0d59f\"><u>activities</u></a> with.&nbsp;</li><li>Coworking spaces are often good places to meet people over meals if you work from there or are invited as a guest.&nbsp;</li><li><i>Note: Right now (summer 2022) there are lots of EAs visiting Berkeley, and there are lots of events, meetups and more. We are not sure how much this will continue after the summer programs end, but expect that there will be some public / open events that will continue beyond this period of time.&nbsp;</i></li></ul><h1><strong>(Longtermist) Coworking Spaces &amp; Offices</strong></h1><p>There are 2 main offices that longtermist EAs based in Berkeley work from: Lightcone and Constellation. Both offices are centrally located in Downtown Berkeley, very close to the BART station (metro) and the UC Berkeley campus. <i>We won't share the public locations of either office for privacy reasons.&nbsp;</i></p><p>The offices are membership-based. You can apply to be a regular member, but are unlikely to get access as there is limited space and they are selective with whom they give access to.&nbsp;<i>We are not sharing the application link for either office here.&nbsp;</i>The acceptance criteria is not always legible, so if you don\u2019t get accepted you may not know why. Both offices have guest policies for their members to invite guests to come visit, with varying rules. If you\u2019re invited by someone, you can ask them for more details.&nbsp;</p><p><strong>Lightcone&nbsp;</strong>is operated by&nbsp;<a href=\"https://www.lightconeinfrastructure.com/\"><u>Lightcone Infrastructure</u></a>, the people who also run LessWrong (so, rationalists). They are focused on AI alignment and projects that they believe will contribute to alignment. Lightcone members include alignment researchers, longtermist community builders, and some rationalists. About ~40 people work from Lightcone in any given week and about ~140 people have access. If you\u2019re not working on longtermist projects that the Lightcone team is excited by, you will likely not get access to Lightcone.&nbsp;</p><p><strong>Constellation&nbsp;</strong>is run by&nbsp;<a href=\"https://www.redwoodresearch.org/\"><u>Redwood Research</u></a>. Aside from the Redwood team, which is about 30 people, they have a coworking space that is more targeted at organisations rather than independent researchers or smaller projects. Thus, Constellation hosts staff from Open Phil, ARC, the FTX Future Fund, CEA, AI Impacts, Atlas Fellowship, MIRI, Lightcone, Alvea, and GCP. Access to Constellation is typically more limited than Lightcone. C<i>urrently (as of July 2022) there is no application form, and they are mostly focused on supporting members from the organisations in the space.</i></p><figure class=\"image image_resized\" style=\"width:50.57%\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1675224728/mirroredImages/W2w7xA9AtDnjcK6DP/pxp3s9ih6vfp1wh65v20.jpg\"><figcaption>No idea who took this, sorry!</figcaption></figure><h1><strong>What to do (that\u2019s not work)</strong></h1><h3><strong>Attractions</strong>:&nbsp;&nbsp;</h3><ul><li><a href=\"https://docs.google.com/spreadsheets/d/1Bl_0kXwD4rJ6DlwdIiZKfG3Y1yCDyR2aolWXY51OaCg/edit#gid=0\"><u>Bay Area Attractions</u></a>&nbsp;</li><li><a href=\"https://www.atlasobscura.com/\"><u>Atlas Obscura</u></a></li></ul><h3><strong>Nature:&nbsp;</strong></h3><p>There are tons of amazing nature things to do in the Bay - hiking, climbing, surfing and skiing. I\u2019ll let you Google them (or use&nbsp;<a href=\"https://www.alltrails.com/\"><u>AllTrails</u></a> or Berkeley Path Wanderers Association <a href=\"https://www.berkeleypaths.org/self-guidedwalks\">Resources for self-guided walks</a>) but here\u2019s a short list :)</p><ul><li><a href=\"https://www.ebparks.org/maps\"><u>East Bay Regional Park District</u></a>&nbsp; - Berkeley amenities include lots of hiking, a small farm, and Lake Anza</li><li>Fire Trails (up in the hills near Berkeley, accessible by foot)</li><li>UC Berkeley Botanical Garden and the Tilden Botanical Garden</li><li>Coastal Access around&nbsp;<a href=\"https://www.ebparks.org/sites/default/files/mclaughlin_eastshore_map.pdf\"><u>Eastshore State Park</u></a>, including walking and bike access as well as weird art at Albany Bulb</li><li>Crescent Lawn (right at Downtown Berkeley) for some close-to-city park-ing</li></ul><figure class=\"image image_resized\" style=\"width:68.22%\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1675224728/mirroredImages/W2w7xA9AtDnjcK6DP/iu23g8cp2royehwdw4cj.jpg\"></figure><h3><strong>Best Sunset Points:</strong></h3><ul><li>Lawrence Hall of Science for a mountain top view</li><li>Albany Bulb for an ocean view</li><li>Indian Rock Park for a bouldering spot with a beautiful sunset</li></ul><h3><strong>Food</strong>:&nbsp;</h3><p>Check out <a href=\"https://docs.google.com/document/d/19qfanP09oXIw-ZmTsT-p8ufFADGHaw_KoZ63ATL1VsE/edit?usp=drive_web&amp;ouid=105393907938073020054\"><u>Eating in Berkeley</u></a> by Aris Richardson<strong>&nbsp;</strong></p><h3><strong>Museums:&nbsp;</strong></h3><ul><li>Art and culture:&nbsp;SFMOMA, Oakland Museum, BAMPFA, DeYoung &amp; Legion of Honor</li><li>California Academy of Sciences</li></ul><h3><strong>Coffee/Tea Shops&nbsp;</strong>(good for working):</h3><ul><li>Caffe Strada</li><li>Asha</li><li>1951 Coffee Company</li><li>Caffeinated</li></ul><h1><strong>Logistics</strong></h1><h2><strong>Housing</strong></h2><ul><li>Start planning your trip in advance (&gt;3 weeks) if you want to find good housing options, even for a short trip. The Bay Area is expensive, so make sure you have your finances in order before coming.</li><li>Rent is usually $1600 - $1800 for a single and $900 - $1200 for a double (aka sharing a room). (These are prices for places within a 15-minute walking distance from Downtown Berkeley, where the 2 main&nbsp;<a href=\"https://docs.google.com/document/d/11s9Us2bDTsAaCEF9xMW6IjNmgX06IjA-6sM4eVwIfLQ/edit#heading=h.z927lfwkgt4n\"><u>longtermist coworking spaces</u></a>.)</li><li>People generally live in group houses or flats of 3-9 people, which often have open rooms for short- and long-term stays.</li><li>Sharing a bedroom is somewhat typical for young people (e.g. students) here due to high housing costs, this may surprise non-Americans</li><li>There are&nbsp;<a href=\"https://www.facebook.com/groups/664817953593844/\"><u>these</u></a>&nbsp;<a href=\"https://www.facebook.com/groups/2266502166822026/\"><u>Facebook</u></a>&nbsp;<a href=\"https://www.facebook.com/groups/1021488637885621/\"><u>groups</u></a> where housing and (sometimes free) couch-surfing openings at group houses are sometimes posted and you can post a \u201clooking for\u201d-post.&nbsp;</li><li><strong>Note as of Aug 2nd: the Lodge in Berkeley is booked for several weeks and at capacity for guests, please do not reach out to us regarding housing.&nbsp;</strong></li></ul><h2><strong>Transportation</strong></h2><ul><li>Trains:&nbsp;<a href=\"https://www.bart.gov/system-map\"><u>BART</u></a> is the Bay Area equivalent of a metro/commuter rail system,&nbsp;<a href=\"https://www.sfmta.com/maps/muni-service-map\"><u>MUNI&nbsp;</u></a>is the SF metro system,&nbsp;<a href=\"https://www.caltrain.com/?active_tab=route_map_tab&amp;center=-122.22590%7E37.61888\"><u>Caltrain&nbsp;</u></a>connects to the South Bay</li><li>Buses:&nbsp;<a href=\"https://www.actransit.org/\"><u>AC transit</u></a> offers an extensive bus network. Buses can be a helpful transit option in the US - train infrastructure is less well developed.&nbsp;</li><li>A&nbsp;<a href=\"https://www.clippercard.com/ClipperWeb/\"><u>Clipper Card</u></a> will let you pay for access to all local transport, and even the cable cars in SF!</li><li>While bike infrastructure is improving and there is a nascent cycling culture in the Bay,&nbsp; bike accidents and theft (especially back wheels) can also occur. Wear a helmet, use lights, consider a safety vest, and&nbsp;<i>always</i> lock your bike.&nbsp;</li><li>Berkeley is quite a small and walkable city, SF is also walkable but can get very hilly and steep.</li></ul><h2><strong>Visitor Visas (for casual visits / attending EAG(x)'s)</strong></h2><p><i>This is not official legal / immigration advice, if you\u2019re uncertain on any of these points, contact a lawyer. If you\u2019re participating in a retreat or program, talk to the organisers about the relevant visa for that program.</i></p><ul><li>Lots of countries participate in&nbsp;<a href=\"https://esta.cbp.dhs.gov/\"><u>ESTA</u></a> - a visa waiver program for tourism or business travel which takes up to 72 hours to process (we recommend applying farther in advance). If you can\u2019t get an ESTA,&nbsp;<strong>it could take several months to get a visa.&nbsp;</strong>Apply as soon as possible, even if your exact travel plans are not set.&nbsp;</li><li>Just a note: non-US citizens are not allowed to work in the US without a visa that permits work authorisation.&nbsp;</li><li>At the border:<ul><li>Border officials will ask you about the purpose of your trip. You can say that you\u2019re visiting friends in the Bay, and (if they ask) that you met them through a nonprofit conference (a simpler way of explaining EA events) or a community that is trying to improve the charity landscape.&nbsp;</li><li>You\u2019ll need to show you booked a return flight within the correct time frame. Book a fully refundable / changeable flight if your dates are uncertain.&nbsp;</li><li>In general, never do or say anything that could be construed as an intention to stay in the US permanently.&nbsp;</li></ul></li></ul><h2><strong>Climate and Geography</strong></h2><h3><strong>Weather</strong></h3><p>The Bay Area has a mild, Mediterranean climate with little precipitation and moderate temperatures year round. Low humidity and dry heat means the shade is often much cooler than the sun.&nbsp; Also, it\u2019s usually sunny, so the weather in Berkeley is epic.</p><p>Evenings and mornings can be surprisingly cool, breezy, and damp because of the role sea breezes have in transporting fog in and out of the Bay. Carry warm layers even when it\u2019s warm and sunny to avoid being cold in the evening. Common layers include thin merino/cashmere sweaters, down puffers, fleeces, and cotton sweatshirts. Heavy coats are usually not needed.&nbsp;</p><p>California is further south than most of Europe, and typically has much higher UV index. Protect yourself from UV exposure and sunburns.</p><h3><strong>Earthquakes&nbsp;</strong></h3><p>Keep in mind this is very unlikely, but if you experience an Earthquake:</p><ol><li>Don\u2019t panic</li><li>Get underneath a heavy object like a desk, bed, or table. Protect the back of your neck with your hands and wait until the shaking stops.&nbsp;</li></ol><p>Most Earthquakes are small and end quickly, if everyone is ok and you don\u2019t see any damage (e.g. cracks in walls, broken glass, etc.) then it\u2019s probably safe to continue with your day, but be conscious this&nbsp;<i>could</i> be a foreshock. If you haven\u2019t already, you may want to locate sturdy shoes, water, warm layers, or other easily accessible&nbsp;<a href=\"https://www.usgs.gov/faqs/what-emergency-supplies-do-i-need-earthquake\"><u>items on this list</u></a><strong>.</strong></p><ol><li>If things are not obviously ok (you experience \u201cThe Big One\u201d), go outside quickly and safely and look for an open area clear of things that can fall (e.g. buildings and power-lines) such as a park. Seek medical attention if needed.&nbsp;</li><li>Be prepared to not return for several hours or potentially days. If you\u2019re able to, bring emergency supplies and turn off your home\u2019s gas and electricity if you can. Wait for guidance as to when it is safe to return and expect that cell coverage could go down</li></ol><h3><strong>Wildfires</strong></h3><p>California has a natural fire season that is part of the state\u2019s ecology, but have become more frequent and intense due to fire suppression and climate change. Do your part by following local restrictions on when and where campfires are allowed, and always extinguishing them properly. If visiting California between May and November, keep in mind the possibilities of wildfire air quality impacts even in urban areas.</p><p><strong>Check local fire activity</strong> before you plan trips to rural or wilderness areas. See the&nbsp;<a href=\"https://www.fire.ca.gov/incidents/2022/\"><u>CalFire incident archive</u></a>. There is rarely an active reason for concern related to large scale smoke transport until there are incidents around or above 50,000 acres with low containment.</p><p><strong>Monitor air quality&nbsp;</strong>using&nbsp;<a href=\"https://airquality.weather.gov/probe_aq_data.php?latitude=37.87&amp;longitude=-122.29\"><u>24 hour air quality forecasts</u></a> from NOAA, and&nbsp;<a href=\"https://map.purpleair.com/1/mAQI/a10/p604800/cC0#11.24/37.7774/-122.3733\"><u>PurpleAir</u></a> or&nbsp;<a href=\"https://www.airnow.gov/\"><u>AirNow</u></a> for real time air quality. Lower AQIs values are better, anything below 100 is alright. Isolated high numbers usually happen if in a local geographic low or downwind of a large intersection or highway. When air quality or smoke is bad, invest in air purifiers and find ways to seal windows.</p><p><strong>Be prepared for evacuations&nbsp;</strong>by having a go-bag with essentials you can grab if you do need to evacuate on short notice.</p><h1><strong>Useful Links</strong></h1><h2>Online Groups &amp; Chats</h2><ul><li><a href=\"https://join.slack.com/t/bayareaea/shared_invite/zt-1hoh9mqeb-WVIRfQY0yQmSZ5_9U9rs8A\"><u>Bay Area EA Slack</u> </a>- general slack for disucssions, with channels for visitors, social activities and housing&nbsp;</li><li>Messenger Chats <i>(join via mobile,&nbsp;or anyone in the group can add you!)</i><ul><li>Climbing (<a href=\"https://m.me/j/Abayg0Jf-g_yH5ba/\">Invite link</a>)</li></ul></li><li><a href=\"https://www.facebook.com/groups/2266502166822026\"><u>EA / Rationality Housing Board</u></a> - for long- &amp; short-term housing</li><li><a href=\"https://join.slack.com/t/ea-uc-berkeley/shared_invite/zt-1c1j7vp91-4PGZC1emLzJngcT9iBA98g\"><u>UC Berkeley EA Slack</u></a> (+ <a href=\"https://eaberkeley.com/\"><u>Website</u></a> + <a href=\"http://facebook.com/eastbayeffectivealtruism\"><u>Facebook Page</u></a> for events)</li><li>(Mostly) Events<ul><li><a href=\"https://www.facebook.com/groups/bayea\"><u>Bay Area Facebook group</u></a> - a general group for Bay-area based EAs</li><li><a href=\"https://www.facebook.com/groups/1100350234030076\"><u>East Bay EA Hangout Facebook Group</u></a> - an informal group for EA &amp; social events in the East Bay</li><li><a href=\"http://facebook.com/eastbayeffectivealtruism\"><u>East Bay EA Facebook Pag</u>e</a> (<a href=\"https://forum.effectivealtruism.org/groups/vf9G2MNcrhrbqtoWx\">East Bay EA</a> Forum page)</li><li><a href=\"http://facebook.com/effectivealtruismsf\"><u>San Francisco EA Facebook Page</u></a> (<a href=\"https://forum.effectivealtruism.org/groups/yCFPwoJ4yLmNGDPGu\"><u>San Francisco EA</u></a> Forum Page<u>)</u></li><li><a href=\"http://facebook.com/South.Bay.Effective.Altruism\"><u>South Bay EA Facebook Group</u></a></li></ul></li></ul><h2>Useful Apps&nbsp;</h2><ul><li>BayWheels: renting bicycles</li><li>Clipper Card (or add to apple wallet): for BART and buses&nbsp;</li><li>ZipCar: for car rentals&nbsp;</li><li>Superpedestrian or Veo: for renting e-scooters</li></ul><hr><p><i>Acknowledegements: Thanks to Luise Woehlke for the original version of this guide, and countless others who gave feedback and added in useful information on drafts. If you have suggestions for this guide or other feedback, please comment or message us.&nbsp;</i></p>", "user": {"username": "ElikaSomani"}}, {"_id": "aFu9ejXKHxKQEpzGF", "title": " Open Thread: October \u2014 December 2022", "postedAt": "2022-10-12T10:41:00.424Z", "htmlBody": "<p>Welcome!</p><h2>If you're <a href=\"https://forum.effectivealtruism.org/posts/BsnGqnLzrLdmsYTGt/new-start-here-useful-links-1\">new</a> to the EA Forum:</h2><ul><li>Consider using this thread to introduce yourself!</li><li>You could talk about how you found effective altruism, what causes you work on and care about, or personal details that aren't EA-related at all.</li><li>(You can also put this info into your <a href=\"https://forum.effectivealtruism.org/posts/2j8ERGPu68L5Bd95y/you-should-write-a-forum-bio\">Forum bio</a>.)</li></ul><h2>Everyone:&nbsp;</h2><ul><li>If you have something to share that doesn't feel like a full post, add it here! (You can also create a <a href=\"https://forum.effectivealtruism.org/shortform\">Shortform post</a>.)</li><li>You might also share good news, big or small (See <a href=\"https://forum.effectivealtruism.org/posts/F8FaFPaNYNZKmdNA5/progress-open-thread-october-student-summit-2020\">this post</a> for ideas.)</li><li>You can also <strong>ask questions about anything that confuses you</strong> (and you can answer them, or discuss the answers).</li></ul><p>For inspiration, you can see <a href=\"https://forum.effectivealtruism.org/posts/LpCewmJgosEaz7ZkW/open-thread-june-september-2022\">the last open thread here</a>.&nbsp;</p><hr><h2>Other Forum resources</h2><ol><li><a href=\"https://forum.effectivealtruism.org/posts/4WxHNBf5LeM9gQneT/you-should-write-on-the-ea-forum\"><u>\ud83d\udd8b\ufe0f&nbsp; Write on the EA Fo</u>rum</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum\">\ud83e\udd8b&nbsp; Guide to norms on the Forum</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/Y8gkABpa9R6ktkhYt/forum-user-manual\">\ud83d\udee0\ufe0f&nbsp; Forum User Manual</a></li></ol><p>&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673309943/mirroredImages/aFu9ejXKHxKQEpzGF/gbffstkxukifkcn14wpy.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673309943/mirroredImages/aFu9ejXKHxKQEpzGF/mqiluuhl0d81qm2fwxc8.png 170w, https://res.cloudinary.com/cea/image/upload/v1673309943/mirroredImages/aFu9ejXKHxKQEpzGF/pmlxq4snbwunbljqnvw4.png 340w, https://res.cloudinary.com/cea/image/upload/v1673309943/mirroredImages/aFu9ejXKHxKQEpzGF/jsz7812zhn148iq9fnof.png 510w, https://res.cloudinary.com/cea/image/upload/v1673309943/mirroredImages/aFu9ejXKHxKQEpzGF/l7zquofsfev8hiitd8wt.png 680w, https://res.cloudinary.com/cea/image/upload/v1673309944/mirroredImages/aFu9ejXKHxKQEpzGF/dx46k1sadr1snm6iwkgi.png 850w, https://res.cloudinary.com/cea/image/upload/v1673309943/mirroredImages/aFu9ejXKHxKQEpzGF/ku2d8xcw1slswebdmrmx.png 1020w, https://res.cloudinary.com/cea/image/upload/v1673309944/mirroredImages/aFu9ejXKHxKQEpzGF/dqysspry2uwb5quj7itu.png 1190w, https://res.cloudinary.com/cea/image/upload/v1673309944/mirroredImages/aFu9ejXKHxKQEpzGF/iicnzgwc6ut380pcuqcb.png 1360w, https://res.cloudinary.com/cea/image/upload/v1673309944/mirroredImages/aFu9ejXKHxKQEpzGF/njv0njh7ubmkspo331fq.png 1530w, https://res.cloudinary.com/cea/image/upload/v1673309943/mirroredImages/aFu9ejXKHxKQEpzGF/umrb4lnnpdxjhqutcbca.png 1699w\"><figcaption>I personally like adding pretty images to my Forum posts. Credit to DALL-E.</figcaption></figure>", "user": {"username": "Lizka"}}, {"_id": "AdyAr3mtyajSirqp9", "title": "Against the normative realist's wager", "postedAt": "2022-10-13T16:36:46.018Z", "htmlBody": "<p>(Cross-posted from <a href=\"https://joecarlsmith.com/2022/10/09/against-the-normative-realists-wager\">my website</a>. Audio version <a href=\"https://joecarlsmithaudio.buzzsprout.com/2034731/11464432-against-the-normative-realist-s-wager\">here</a>, or search \"Joe Carlsmith Audio\" on your podcast app.)</p><h2><strong>Summary</strong></h2><p>Non-naturalist normative realism is the view that there are mind-independent normative facts that aren\u2019t reducible to facts about the natural world (more <a href=\"https://jc.gatspress.com/2021/01/03/the-despair-of-normative-realism-bot/\">here</a>). I don\u2019t find this view very plausible, for various reasons.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefybnjh3cbgwf\"><sup><a href=\"#fnybnjh3cbgwf\">[1]</a></sup></span>&nbsp;Interestingly, though, some fans of the view <i>don\u2019t find it very plausible either</i>. But they think that in expectation, all (or almost all) of the \u201cmattering\u201d happens in worlds where non-naturalist normative realism is true. So for practical purposes, they argue, we should basically condition on it, even if our overall probability on it is low. Call this view \u201cthe realist\u2019s wager.\u201d</p><p>I disagree with this view. This essay describes a key reason why: namely, the realist\u2019s wager says horrible things about cases where you get modest benefits if non-naturalist normative realism is true, but cause terrible harm if it\u2019s false.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefehijttyy1w8\"><sup><a href=\"#fnehijttyy1w8\">[2]</a></sup></span>&nbsp;I describe this sort of case in the first section. The rest of the essay teases out various subtleties and complications. In particular, I discuss:</p><ul><li>The difference between \u201cextreme wagers,\u201d which treat nihilism as the <i>only</i> viable alternative non-naturalist normative realism, and more moderate ones.</li><li>Whether it makes sense to think that things matter <i>more</i> conditional on non-naturalist normative realism than they do conditional on some non-nihilist alternative, because the former has more \u201cnormative oomph.\u201d &nbsp;</li><li>Whether it makes sense to at least condition on the falsehood of <i>nihilism</i>. (I\u2019m skeptical here, too.)</li></ul><p>My aim overall is to encourage advocates of the realist\u2019s wager (and related wagers) to think quantitatively about the meta-ethical bets they\u2019d actually accept, rather than focusing on more sweeping assumptions like \u201cif blah is false, then nothing matters.\u201d</p><p><i>Thanks to Ketan Ramakrishnan, Katja Grace, Jacob Trefethen, Leopold Aschenbrenner, Will MacAskill and Ben Chang for discussion.</i></p><h2><strong>I. Martha the meta-ethical angel</strong></h2><p>The type of case I have in mind works like this:</p><blockquote><p><i>Martha\u2019s deal</i>: Martha the meta-ethical angel appeals before you. She knows the truth about meta-ethics, and she offers you a deal. If non-naturalist normative realism is true, she\u2019ll give you a hundred dollars. If it\u2019s false, she\u2019ll burn you, your family, and a hundred innocent children alive.</p></blockquote><p>Claim: don\u2019t take the deal. This is a bad deal. Or at least, I personally am a \u201chell no\u201d on this deal, especially if my probability on non-naturalist normative realism is low \u2013 say, one percent.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1umm8303emdh\"><sup><a href=\"#fn1umm8303emdh\">[3]</a></sup></span>&nbsp;Taking the deal, in that case, amounts to burning more than a hundred people alive (including yourself and your family) with 99% probability, for a dollar in expectation. No good.</p><p>But realist wagerers take deals like this. That is, they say \u201cworlds where the burning happens are worlds that basically don\u2019t matter, at least relative to worlds where you get the hundred dollars. So even if your probability of getting the hundred dollars is low, the deal is positive in expectation.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkjt5kvhbm5\"><sup><a href=\"#fnkjt5kvhbm5\">[4]</a></sup></span></p><p>Or at least, that\u2019s the sort of thing their position implies (I\u2019m hoping that they\u2019ll think more about this sort of case, and reconsider). Let\u2019s look at some of the issues here in more detail.</p><h2><strong>II. Extreme wagers</strong></h2><p>I\u2019m assuming, here, an <a href=\"https://jc.gatspress.com/2022/03/16/on-expected-utility-part-1-skyscrapers-and-madmen/\">expected-utility-ish</a> approach to meta-ethical uncertainty.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7cgmodx1hqk\"><sup><a href=\"#fn7cgmodx1hqk\">[5]</a></sup></span>&nbsp;That is, I\u2019m assuming that we have a probability distribution over meta-ethical views, that these views imply that different actions have different amounts of choiceworthiness, and that we pick the action with the maximum expected choiceworthiness.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefo7orbnkrmp\"><sup><a href=\"#fno7orbnkrmp\">[6]</a></sup></span></p><p>This isn\u2019t the only approach out there, and it has various problems (notably, with comparing the amounts of choiceworthiness at stake on different theories \u2013 this will be relevant later). But my paradigm realist wagerer is using something like this approach, it\u2019s got some strong arguments in its favor,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2knasmaj3qw\"><sup><a href=\"#fn2knasmaj3qw\">[7]</a></sup></span>&nbsp;and it\u2019s helpfully easy to work with. Also, I expect other approaches to imply similar issues.</p><p>I\u2019m also focusing, in particular, on <i>non-naturalist </i>normative realism, as opposed to other variants \u2013 for example, variants that posit mind-independent normative facts that <i>are </i>reducible to facts about the natural world. I\u2019m doing this partly to simplify the discussion, partly because my paradigm realist wagerer is focused on non-naturalist forms of realism as well, and partly because in my opinion, the question of \u201care there normative facts that can\u2019t be reduced to facts about the natural world\u201d is the most important one in this vicinity (once you\u2019ve settled on naturalism, it\u2019s not clear \u2013 at least to me \u2013 how substantive or interesting additional debate in meta-ethics becomes).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyv4zkmo3ir\"><sup><a href=\"#fnyv4zkmo3ir\">[8]</a></sup></span></p><p>With this in mind, let\u2019s suppose that we are splitting our credence between the following three views only:</p><ol><li><i>Non-naturalist realism: </i>There are mind-independent normative facts that can\u2019t be reduced to facts about the natural world.</li><li><i>Naturalism-but-things-matter</i>: There normative facts of some sort (maybe mind-independent, maybe mind-dependent), but they can be reduced to facts about the natural world.</li><li><i>Nihilism</i>: There are no normative facts.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefiztfocve3b\"><sup><a href=\"#fniztfocve3b\">[9]</a></sup></span></li></ol><p>The simplest and most extreme form of the realist wager runs as follows:</p><blockquote><p>Naturalism-but-things-matter is false: it\u2019s either non-naturalist realism, or nihilism. Probably, it\u2019s nihilism (the <a href=\"https://jc.gatspress.com/2022/01/17/the-ignorance-of-normative-realism-bot/\">objections to non-naturalist realism</a> are indeed serious). But if it\u2019s nihilism, nothing matters. Thus, all the mattering happens conditional on non-naturalist realism. Thus, for practical purposes, I will condition on non-naturalist realism, even though probably, nothing matters.</p></blockquote><p>Thus, for concreteness, let\u2019s say this wagerer is 99% on nihilism, 1% on realism, and 0% on naturalism-but-things-matter.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffqdv00iy52m\"><sup><a href=\"#fnfqdv00iy52m\">[10]</a></sup></span>&nbsp;So the deal above looks like:</p><p>So, what\u2019s the expected mattering of the deal overall? A dollar\u2019s worth. Not amazing, but not nothing: go for it.</p><p>Now maybe you\u2019re thinking: wait, really, <i>zero </i>percent on anything mattering conditional on the natural world being all there is? That sounds overconfident. And indeed: yes. Even beyond \u201ccan Bayesians ever be certain about anything,\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffziubrlgr2u\"><sup><a href=\"#fnfziubrlgr2u\">[11]</a></sup></span>&nbsp;this sounds like the wrong place to spend your certainty points. Meta-ethics is tricky, people.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefaip921ghkzv\"><sup><a href=\"#fnaip921ghkzv\">[12]</a></sup></span>&nbsp;Are you so sure you even know what this debate is about \u2013 what it means for something to matter, or to be \u201cnatural,\u201d or to be \u201cmind-dependent\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefopi5dg2t1c\"><sup><a href=\"#fnopi5dg2t1c\">[13]</a></sup></span>\u2014 let alone what the answer is? So sure that if it\u2019s only raw nature \u2013 only joy, love, friendship, pain, grief, and so on, with <a href=\"https://www.lesswrong.com/posts/NpwPi5HkKpcAvexJz/the-ignorance-of-normative-realism-bot#II__Can_you_touch_the_ghostly_frosting_with_your_mind_\"><i>no extra non-natural frosting on top</i></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefptcrriae82e\"><sup><a href=\"#fnptcrriae82e\">[14]</a></sup></span>&nbsp;-- then there\u2019s nothing to fight for, or against? I, for one, am not. And perhaps, if you find yourself hesitating to take a deal like this, you aren\u2019t, actually, either.</p><p>So \u201cyou should have higher credence on naturalism-but-things-matter\u201d is the immediate objection here. Indeed, I think this objection cautions wariness about the un-Bayesian-ness of much philosophical discourse. Some meta-ethicist might well declare confidently \u201cif naturalism is true, then nothing matters!\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefaszbeh4yd2u\"><sup><a href=\"#fnaszbeh4yd2u\">[15]</a></sup></span>&nbsp;But they are rarely thinking in terms of quantitative credences on \u201cok but actually maybe if naturalism is true some things matter after all,\u201d or about the odds at which they\u2019re willing to bet. I\u2019m hoping that reflection on deals like Martha\u2019s can prompt, at least, a bit more precision.</p><p>Some may also notice a more conceptually tricky objection: namely, that this deal seems bad <i>even if you have this pattern of credences</i> \u2013 i.e., even if non-naturalist realism and nihilism are genuinely the only live options. That is, you may notice that you don\u2019t want to be burned alive, <i>even if nihilism is true, and it doesn\u2019t matter that you\u2019re being burned alive</i>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2obbhgcdvxm\"><sup><a href=\"#fn2obbhgcdvxm\">[16]</a></sup></span>&nbsp;And the same for your mother, the children, and so on.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefd9lllxyyu1\"><sup><a href=\"#fnd9lllxyyu1\">[17]</a></sup></span>&nbsp;That is, you may feel like, somehow, the whole \u201cexpected mattering\u201d ontology here is leaving your interest in not being burned alive too much up-for-grabs.</p><p>I think this is a good objection, too. In fact, I think it may get closer to the heart of the issue than \u201cyou should have higher credence on naturalism-but-things-matter\u201d (though the two objections are closely linked). However, it\u2019s also a more complicated objection, so for now I want to set it aside (I discuss it more below), and assume that we accept that expected mattering is what we\u2019re after, and that nihilism gives us none.</p><h2><strong>III. Ways to wager</strong></h2><p>The credences I gave above (in particular, the 0% on naturalism-but-things-matter) were extreme. What happens if we moderate them?</p><p>Suppose, for example, that your credences are: 10% on non-naturalist realism, 1% on naturalism-but-things-matter, and 89% on nihilism. That is, you give <i>some</i> credence to things mattering in a purely natural world, but you think this is 10x less likely than the existence of non-natural normative facts \u2013 and that both of these are much less likely than nothing mattering at all.</p><p>Now we start getting into some trickier issues. In particular, now we need to start distinguishing between a number of factors that enter into your expected mattering calculations, conditional on anything mattering (e.g., on non-nihilism): namely,</p><blockquote><p>(a) How likely you think the various non-nihilist meta-ethical theories are.</p></blockquote><p>For example, the credences just discussed.</p><blockquote><p>(b) Whether your<i> non-normative beliefs </i>alter in important ways, conditional on different non-nihilist meta-ethical theories.&nbsp;</p></blockquote><p>For example, you might think (indeed, I\u2019ve argued, <a href=\"https://www.lesswrong.com/posts/NpwPi5HkKpcAvexJz/the-ignorance-of-normative-realism-bot#VII__Will_the_aliens_agree_\">you should think</a>) that epistemically non-hopeless forms of non-natural normative realism make <i>empirical predictions</i> (uh oh: careful, philosophers...) about the degree of normative consensus to expect amongst intelligent aliens, AI systems, and so on with sufficient opportunity to reflect \u2013 predictions that other views (importantly) <a href=\"https://nickbostrom.com/superintelligentwill.pdf\">don\u2019t share</a>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1xqeep9e5p2\"><sup><a href=\"#fn1xqeep9e5p2\">[18]</a></sup></span>&nbsp;Or: if non-naturalist realism is true, maybe you should expect your realist friends to be right about other stuff, too (and vice versa).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpaqbbdxm788\"><sup><a href=\"#fnpaqbbdxm788\">[19]</a></sup></span></p><blockquote><p>(c) Whether <i>different things</i> matter, conditional on different non-nihilist meta-ethical theories, because your object-level normative views are correlated with your meta-ethical views.</p></blockquote><p>For example, conditional on non-naturalist realism you might be a total utilitarian (so you love <a href=\"https://jc.gatspress.com/2021/03/14/against-neutrality-about-creating-happy-lives/\">creating new happy lives</a>), but conditional on naturalism-but-things-matter you\u2019re a person-affector (so you shrug at new happy lives), such that even if you had equal credence in these two meta-ethical theories, you\u2019d pay more to create a new happy life conditional on non-naturalist realism than you would conditional on naturalism-but-things-matter.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2gk3jpxjws7\"><sup><a href=\"#fn2gk3jpxjws7\">[20]</a></sup></span></p><blockquote><p>(d) Whether <i>the same thing matters different amounts</i>, conditional on different non-nihilist meta-ethical theories, because some meta-ethical theories are just intrinsically \u201chigher stakes.\u201d</p></blockquote><p>For example, you might be certain of total utilitarianism conditional on both non-naturalist realism <i>and</i> naturalism-but-things-matter, and you might have equal credence on both these meta-ethical views, but you\u2019d <i>still</i> pay more to create a happy life conditional on non-naturalist realism than on naturalism-but-things-matter, because on non-naturalist realism, the mattering somehow has more <i>oomph</i> \u2013 i.e., it\u2019s some objective feature of the fabric of reality, rather than something grounded in e.g. facts about what we care about (realist: \u201cbleh, who cares what we care about\u201d), and <i>objective </i>mattering is extra special.</p><p>To isolate some of these factors, let\u2019s focus for a moment on (a) alone. That is, let\u2019s assume that you have the same normative views (for simplicity, let\u2019s say you\u2019re a total utilitarian), with the same level of <i>oomph</i>, regardless of which non-nihilist meta-ethics is true; and that your non-normative views are independent of meta-ethics as well.</p><p>Granting for the moment that we\u2019re after expected mattering, then, and that nihilism gives us none, we can consider cases like the following:</p><blockquote><p><i>Martha\u2019s buttons</i>: Martha the meta-ethical angel offers you one of two buttons. The red button saves one life if non-naturalist realism is true, and does nothing otherwise. The blue button saves <i>N</i> lives if non-naturalist realism is <i>false</i>, and does nothing otherwise.</p></blockquote><p>Here, the point at which your expected mattering calc becomes indifferent to the buttons is the point where naturalism-but-things-matter is <i>N</i> times less likely than non-naturalist realism. Thus, using the credences above, and setting <i>N</i> to 10, we get:</p><p><i>Blue button</i></p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/20b978e66aa91dde14b35ef4dfb1eb788863adce79f42a38.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/20b978e66aa91dde14b35ef4dfb1eb788863adce79f42a38.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/20b978e66aa91dde14b35ef4dfb1eb788863adce79f42a38.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/20b978e66aa91dde14b35ef4dfb1eb788863adce79f42a38.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/20b978e66aa91dde14b35ef4dfb1eb788863adce79f42a38.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/20b978e66aa91dde14b35ef4dfb1eb788863adce79f42a38.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/20b978e66aa91dde14b35ef4dfb1eb788863adce79f42a38.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/20b978e66aa91dde14b35ef4dfb1eb788863adce79f42a38.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/20b978e66aa91dde14b35ef4dfb1eb788863adce79f42a38.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/20b978e66aa91dde14b35ef4dfb1eb788863adce79f42a38.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/20b978e66aa91dde14b35ef4dfb1eb788863adce79f42a38.png/w_1392 1392w\"></figure><p><i>Red button</i></p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2537a8cf80c5d175204124336e6ec2754938a1d6ed3be900.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2537a8cf80c5d175204124336e6ec2754938a1d6ed3be900.png/w_150 150w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2537a8cf80c5d175204124336e6ec2754938a1d6ed3be900.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2537a8cf80c5d175204124336e6ec2754938a1d6ed3be900.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2537a8cf80c5d175204124336e6ec2754938a1d6ed3be900.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2537a8cf80c5d175204124336e6ec2754938a1d6ed3be900.png/w_750 750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2537a8cf80c5d175204124336e6ec2754938a1d6ed3be900.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2537a8cf80c5d175204124336e6ec2754938a1d6ed3be900.png/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2537a8cf80c5d175204124336e6ec2754938a1d6ed3be900.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2537a8cf80c5d175204124336e6ec2754938a1d6ed3be900.png/w_1350 1350w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2537a8cf80c5d175204124336e6ec2754938a1d6ed3be900.png/w_1452 1452w\"></figure><p>I.e., both of these buttons save a tenth of a life-that-matters in expectation.</p><p>(Now maybe you\u2019re thinking: wait a second, granted some pattern of credences about the metaphysics of normativity and the semantics of normative discourse, you want me to take a 10% chance of saving one life over a 90% chance of saving 9 lives? Red flag. And again: I\u2019m sympathetic \u2013 more below. But I think this red flag is about the idea that expected mattering is what we\u2019re after and that nihilism give us none, which we\u2019re assuming for now. Or perhaps, it\u2019s about having this sort of credence on nihilism \u2013 a closely related worry, but one that rejects the set-up of the case.)</p><p>Now consider your own credences on these different meta-ethical theories. What sort of <i>N</i> do they imply? That is, if Martha showed up and offered you these buttons, for what <i>N</i> would your stated credences imply that you should be indifferent? And (more importantly), for what<i> N</i> would you <i>actually</i> be indifferent? If the two differ substantially, I suggest not taking for granted that you\u2019ve worked everything out, realism-wager-wise.</p><p>My own <i>N</i>, here, is quite a bit <i>less</i> than 1 \u2013 e.g., I choose the blue button even if it only causes Martha, conditional on the falsehood of non-naturalist realism, to flip a coin about whether to save a life or not (thereby saving .5 lives in expectation). This is for various reasons, but a simple one is that I think that non-naturalist realism (to the extent it\u2019s a candidate for truth vs. falsehood) is more likely than not to be false \u2013 and even if it\u2019s false, I care about the people whose lives I could save.</p><p>The broader point, though, is that keeping in mind what sort of <i>N </i>your stated views imply is a good consistency check on whether you actually believe them.</p><h2><strong>IV. Should realist mattering get extra oomph?</strong></h2><p>That said, understanding the relationship between your <i>N</i> and your credences gets more complicated once we start to bring in the other factors \u2013 (b), (c), and (d) \u2013 discussed above. In particular, let\u2019s focus for a second on (d) \u2013 the extra normative \u201coomph\u201d you might give to a certain meta-ethical theory, independent of your credence on it.</p><p>Thus, suppose you think that conditional on non-naturalist realism, everything matters ten times more than it does conditional on naturalism-but-things-matter, because non-naturalist mattering is extra special.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxbdsw6dyst\"><sup><a href=\"#fnxbdsw6dyst\">[21]</a></sup></span>&nbsp;Granted 10% on non-naturalist realism, 1% on naturalism-but-things-matter, and 89% on nihilism, this bumps your <i>N</i> up to 100 rather than 10, while holding your credences fixed. Thus, let\u2019s call one-life-saved <i>conditional on non-naturalist realism</i> one \u201coomph-adjusted life-that-matters.\u201d The red button, then, is worth a tenth of an oomph-adjusted life-that-matters. But with <i>N</i> = 10, the blue button is only worth a <i>hundredth</i> of an oomph-adjusted life-that-matters: e.g., it has a 1% chance of saving ten lives-that-matter-at-all (so a tenth of a life-that-matters-at-all in expectation), but each of these lives matters 10x less than it would conditional on non-naturalist realism, so adjusting for oomph, you\u2019re down to a hundredth of an oomph-adjusted life-that-matters overall. To compensate for this reduction, then, you have to bump <i>N</i> up to 100 to be indifferent to the buttons.</p><p>(<i>Skeptic</i>: \u201cSo, with this sort of oomph adjustment, you take a 1% chance of saving one life over a 90% chance of saving 99 lives?\u201d</p><p><i>Wagerer</i>: \u201cYep.\u201d</p><p><i>Skeptic:</i> \u201cAnd you want to call this altruism?\u201d)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzbsz7sqfiv8\"><sup><a href=\"#fnzbsz7sqfiv8\">[22]</a></sup></span></p><p>Should we make an oomph adjustment? One reason not to is that \u2013 unlike in many normative-uncertainty cases, where we need to find some way to compare units of mattering across very <i>different</i> first order, normative-ethical theories (e.g., average and total utilitarianism) \u2013 in this case, we\u2019re conditioning on total utilitarianism either way, so there is a natural and seemingly-privileged way of normalizing the units of mattering at stake conditional on non-naturalist realism vs. naturalism-but-things-matter: namely, say that the <i>same stuff</i> matters <i>the same amount</i>. I.e., say that one life matters the same amount conditional on either of these meta-ethical views \u2013 and the same for two lives, or ten lives, and so on. When you\u2019re grappling with different first-order ethical views, you don\u2019t have the luxury of this option.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3ol1wtbrn1u\"><sup><a href=\"#fn3ol1wtbrn1u\">[23]</a></sup></span>&nbsp;But when you have this luxury, it\u2019s tempting to take advantage of it.</p><p>What\u2019s more, a number of the standard ways of normalizing across first-order normative-ethical theories <i>won\u2019t allow you</i> to give totalism-conditional-on-non-naturalist-realism more oomph than totalism-conditional-on-naturalism-but-things-matter. That\u2019s because these ways appeal only to the <i>structure</i> of a theory\u2019s ranking over options, rather than to any notion of the absolute amount of value that it posits.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrddkuiaaln\"><sup><a href=\"#fnrddkuiaaln\">[24]</a></sup></span>&nbsp;That is, on many ways of approaching normative uncertainty, it doesn\u2019t make sense to differentiate between \u201csmall-deal total utilitarianism\u201d and \u201cbig-deal total utilitarianism\u201d \u2013 where both imply the same behavior,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8do6sf7esrn\"><sup><a href=\"#fn8do6sf7esrn\">[25]</a></sup></span>&nbsp;but one of them implies that all the stakes are blah times higher.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6o9segiuhow\"><sup><a href=\"#fn6o9segiuhow\">[26]</a></sup></span>&nbsp;But this is what normative <i>oomph</i> adjustments are all about.</p><p>Once we start going in for oomph-adjustments, we might also wonder whether non-naturalist realism \u2013 at least of the standard secular sort -- is really going to be the most oomph-y meta-ethics out there, even weighted by probability. Thus, consider divine command theory, on which theism is true, and the normative facts holds in virtue of God <i>commanding you</i> to do certain things. Do the <i>divine commands</i> of the <i>ultimately source of all reality</i> \u2013 \u201cGod Himself, alive, pulling at the other end of the cord, perhaps approaching at an infinite speed, the hunter, king, husband\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefims0bw30sno\"><sup><a href=\"#fnims0bw30sno\">[27]</a></sup></span>&nbsp;\u2013 significantly out-oomph those thin, desiccated, \u201cexisting but not in an ontological sense\u201d normative facts the Parfitians hope to spread like invisible frosting over science? And if so, should you, perhaps, be wagering on theism instead,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefobcpjm8yvo\"><sup><a href=\"#fnobcpjm8yvo\">[28]</a></sup></span>&nbsp;rather than Oxford\u2019s favorite secular meta-ethics? Is <i>God</i> where roughly all the expected mattering <i>really</i> happens?</p><p>(But wait: have we looked hard enough for ways we can pascal\u2019s mug ourselves, here? Consider the divine commands of the especially-big-deal-meta-ethics spaghetti monster \u2013 that unholy titan at the foundation of all being, who claims that his commands have a graham\u2019s number times more normative oomph than would the commands of any more standard-issue God/frosting, if that God/frosting existed. Shall we ignore all the conventional meta-ethical views, in favor of views like this, that claim for themselves some fanatical amount of oomph? What about the infinitely oomph-y meta-ethical views? Non-zero credence on those, surely...)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefngvcbifjc6\"><sup><a href=\"#fnngvcbifjc6\">[29]</a></sup></span></p><p>I\u2019m not, here, going to delve deeply into how to think about oomph adjustments. And despite their issues, I do actually feel some resonance with some adjustments in this vein.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjm2vjmum8xd\"><sup><a href=\"#fnjm2vjmum8xd\">[30]</a></sup></span>&nbsp;As with credences, though, I at least want to urge attention to the implications of one\u2019s stated views about this topic with respect to cases like <i>Martha\u2019s buttons</i>. It can feel easy, in the abstract, to say \u201ceverything matters much less if non-naturalist realism is false\u201d; harder, perhaps, to say \u201cI choose a 1% probability of saving one life over a 90% probability of saving 99 lives, because of how much less those lives matter if non-naturalist realism is false.\u201d But it is in the latter sort of choice that the rubber meets the road.</p><h2><strong>V. Speaking the nihilist\u2019s language</strong></h2><p>We can imagine yet further complications by introducing (b) and (c) into the picture as well (that is, correlations between meta-ethical views, normative-ethical views, and non-normative views), but for now I want to set those aside.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefroxz5f462e\"><sup><a href=\"#fnroxz5f462e\">[31]</a></sup></span>&nbsp;Instead, I want to return to the assumption we granted, temporarily, above: namely, that expected mattering is what we\u2019re after, and that nihilism gives us none.</p><p>Is this right? Consider the following variant of Martha\u2019s deal:</p><blockquote><p><i>Martha\u2019s nihilism-focused deal</i>: Martha the meta-ethical angel offers you the following deal. If nihilism is false, she\u2019ll give you a hundred dollars. If nihilism is true, she\u2019ll burn you, your family, and a hundred innocent children alive.</p></blockquote><p>Let\u2019s say that you have at least some substantive credence on nihilism, here \u2013 say, 10% (the realist wagerer\u2019s credence is generally much higher). Should you take the deal?</p><p>Here, we no longer have to deal with questions about how much expected mattering is at stake conditional on some alternative to both non-naturalist realism <i>and</i> nihilism. Rather, we know that the burning is only going to happen in worlds where no expected mattering is at stake. So: easy call, right? It\u2019s basically just free money-that-matters.</p><p>Hmm, though. Is this an easy call? Doesn\u2019t seem like it to me. In particular, as I mentioned above: are you sure you don\u2019t care about getting burned alive, conditional on nihilism? Does nihilism really leave you utterly indifferent to the horrific suffering of yourself and others?</p><p>Remember, nihilism \u2013 the view that there are no normative facts<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefddk0var3v2q\"><sup><a href=\"#fnddk0var3v2q\">[32]</a></sup></span>&nbsp;\u2013 is distinct from what we might call \u201cindifference-ism\u201d \u2013 that is, the view that there are normative facts, and they actively say that you <i>should be</i> indifferent to everything.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcmvdiy1xmug\"><sup><a href=\"#fncmvdiy1xmug\">[33]</a></sup></span>&nbsp;On nihilism, indifference is no more normatively required, as a response to the possibility of innocent children being burned alive, than is intense concern (or embarrassment, or desire to do a prime number of jumping jacks). Conditional on nihilism, nothing is telling you not to care about yourself, or your family, or those children: you\u2019re absolutely free to do so. And plausibly \u2013 at least, if your psychology is similar to many people who claim to accept something like nihilism \u2013 you still <i>will </i>care. Or at least, let\u2019s assume as much for the moment.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefij7bh8hkc2\"><sup><a href=\"#fnij7bh8hkc2\">[34]</a></sup></span></p><p>Response: \u201cGranted, I still care; granted, I\u2019m not indifferent. But conditional on nihilism, this care has no <i>normative weight</i>, no <i>justification</i>. I just <i>do</i> care, but it\u2019s not the case that I <i>should</i>. I don\u2019t <i>want</i> the children to be burned alive, and they don\u2019t either, but their lives don\u2019t <i>matter</i>; what we want doesn\u2019t <i>matter</i>. Nothing matters. So, even if its <i>psychologically</i> difficult to take the deal, because my brute, unjustified care gets in the way even conditional on nihilism, when we ask whether I <i>should</i> take the deal, it\u2019s an easy call: yes.\u201d</p><p>We can dramatize this sort of thought by imagining three versions of yourself: one who lives in a nihilist world, one who lives in a non-naturalist realist world, and one who lives in a naturalism-but-things-matter world. Suppose that each of these selves sends a representative to the High Court, which is going to decide whether to take Martha\u2019s nihilism-focused deal. The judge at the high court asks the representatives in turn:</p><blockquote><p><strong>Judge</strong>: \u201cTo the representative of non-naturalist realism: in your world, should we take this deal?\u201d</p><p><strong>Non-naturalism realist</strong>: \u201cYes, your honor. It gives us a hundred dollars that matter.\u201d</p><p><strong>Judge</strong>: \u201cTo the representative of naturalism-but-things-matter: in your world, should we take this deal?\u201d</p><p><strong>Naturalist-but-things-matter</strong>: \u201cYes, your honor. It gives us a hundred dollars that matter.\u201d</p><p><strong>Judge</strong>: \u201cTo the representative of nihilism: in your world, is it the case that we <i>shouldn\u2019t</i> take this deal?\u201d</p><p><strong>Nihilist</strong>: \u201cNo, your honor. In my world, shoulds aren\u2019t a thing.\u201d</p><p><strong>Judge</strong>: \u201cWell then, it seems we have an easy call. Multiple representatives say that we should take the deal, and no representative says that we shouldn\u2019t. So, let\u2019s take it.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdyeydedr5i\"><sup><a href=\"#fndyeydedr5i\">[35]</a></sup></span></p></blockquote><p>But I think this sort of court is refusing to speak to the nihilist in her own language; refusing to accept the currency of a nihilist world. After all (we\u2019ve assumed above), it\u2019s not the case that, conditional on nihilism, you would be totally non-plussed as to how to decide between e.g. being burned alive, or taking a walk in a beautiful forest; between saving a deer from horrible pain, or letting it suffer; between building a <a href=\"https://jc.gatspress.com/2021/01/18/actually-possible-thoughts-on-utopia/\">utopia</a>, or a prison camp.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmkvj0t8wb9i\"><sup><a href=\"#fnmkvj0t8wb9i\">[36]</a></sup></span>&nbsp;Granted, you wouldn\u2019t have any shoulds (non-natural <i>or </i>natural) to tell you what to do. But do you, actually, need them? After all, there is still other deliberative currency available \u2013 \u201cwants,\u201d \u201ccares,\u201d \u201cprefers,\u201d \u201cwould want,\u201d \u201cwould care,\u201d \u201cwould prefer,\u201d and so on. Especially with such currency in the mix, is it so hard to decide for yourself?</p><p>But if we admit this sort of currency into the courtroom \u2013 without requiring that we later cash it out in terms of \u201cshoulds\u201d -- then the judge no longer has such an easy time. Thus, if the judge had instead asked the representatives whether they <i>want</i> the deal to get taken, she would have received an importantly different answer from the nihilist: namely, \u201cnihilism world really <i>really </i>doesn\u2019t want you to take this deal.\u201d And she would not have been able to say \u201cmultiple representatives want us to take this deal, and no one wants us not to.\u201d</p><p>That is, by choosing specifically via the should-focused procedure described above, the judge is declaring up front that \u201cThis is the court of should-y-ness (choiceworthiness, expected mattering, etc) in particular. All representatives from worlds that can pay in should-y-ness for influence over the decision shall be admitted to the table. The voices of those who cannot pay in should-y-ness, however, shall not be listened to.\u201d And when the representative of nihilism world approaches the judge and says \u201cJudge, I care deeply about your not taking this deal; I am willing to pay what would in my world be extreme costs to prevent you from taking this deal; I ask, please, for some sort of influence,\u201d the judge says only: \u201cDoes all this talk of \u2018caring deeply\u2019 and \u2018willing to pay extreme costs\u2019 translate into your having any should-y-ness to offer? No? Then begone!\u201d</p><p>Now, you might say: \u201cBut: the judge isn\u2019t picking some arbitrary currency, here. Decision-making <i>really is</i> the court of should-y-ness.\u201d And indeed, we do often think of it that way. But I think the topic at least gets tricky. Can nihilists make decisions, without self-deception? As mentioned above, I think they can. Perhaps it\u2019s a slightly different kind of decision, framed in slightly different terms -- e.g., \u201cIf I untangle the deer from the barbed wire, then it can go free; I want this deer to be able to go free; OK, I will untangle the deer from the barbed wire\u201d -- but it\u2019s at least <i>similar</i> to the non-nihilist version, and I think it\u2019s a substantive choice to assume, at the outset, that reasoning of this kind has no place in the high court.</p><p>And it\u2019s not as though all the other, non-nihilists theories have the exact same currency, either. In the land of divine command theory, the currency is mattering made out of divine commands. In the land of non-naturalist realism, the currency is mattering made out of non-natural frosting. In the land of naturalism-but-things-matter, the currency is mattering made of out of natural stuff. And in the land of nihilism, the currency is just: natural stuff. Do all the realms that call their currency \u201cmattering\u201d get to gang up on the one that doesn\u2019t? Who set up this court? We would presumably object if the court only accepted shoulds that were made out of e.g. divine commands, or non-natural frosting. So why not accept the currency of <i>every</i> representative? Letting <i>everyone</i> have a say seems the fairest default.</p><h2><strong>VI. The analogy of the Galumphians</strong></h2><p>Here's an analogy that might help illustrate the nihilist representative\u2019s perspective on the situation. Suppose that there are three tribes of Galumphians, all of whom descend from an ancient tribe that placed the highest value on protecting the spirit god Galumph, who was amorphously associated with a certain style of ancient temple. All three tribes continue to build and protect this style of temple, but they understand it in different ways:</p><ul><li>In the first tribe, they say: \u201cGalumph floats on top of (\u2018supervenes on\u2019) our temples, in a separate realm that no one can detect or interact with, and we protect our temples for that reason.\u201d</li><li>In the second tribe, they say: \u201cGalumph <i>just is</i> the temples; in protecting our temples, we protect Galumph.\u201d</li><li>In the third tribe, they say: \u201cWe no longer believe in the god Galumph: our temples are only themselves. But we love them dearly, and we will fight tooth and nail to protect them.\u201d</li></ul><p>Let\u2019s say, further, that none of these tribes cares at all about what happens to the other tribes (the first two tribes each think that Galumph only lives in their realm in particular; and the third tribe is just fully focused on protecting its own temples in particular).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefuy46z7k6iba\"><sup><a href=\"#fnuy46z7k6iba\">[37]</a></sup></span></p><p>Now suppose that a powerful sorcerer visits the tribes and offers them a deal: \u201cI\u2019ll add a few bricks of protection to a temple in the first tribe and in the second tribe. However, I will burn all the thousand temples of the third tribe.\u201d</p><p>The three tribes hold a grand meeting to decide whether to take this deal.</p><blockquote><p><strong>Leader of the meeting</strong>: \u201cFriends, we are all here from the grand lineage of the Galumphians. True, we each only care about what happens in our own realm; but spiritually, we are united in our desire to protect the great god Galumph, wherever he may live. So, let\u2019s start by going around and saying how we each believe this deal affects Galumph protection in our own realm.\u201d</p><p><strong>First tribe representative</strong>: \u201cIn my realm, we believe that this deal protects Galumph; we support it.\u201d</p><p><strong>Second tribe representative</strong>: \u201cIn my realm, we believe that this deal protects Galumph; we support it.\u201d</p><p><strong>Third tribe representative</strong>: \u201cSorry, in my realm we don\u2019t believe in Galumph, but this deal will result in the complete destruction of all of our beloved temples, and we are seriously against it.\u201d</p><p><strong>Leader of the meeting</strong>: \u201cAh, so two tribes believe that this deal protects Galumph; and no one believes that this deal will hurt Galumph. It appears, then, that the decision should be easy: clearly, the Galumph-optimal decision, by everyone\u2019s lights, should be to take the deal.\u201d</p><p><strong>Third tribe representative: </strong>\u201cSorry, who decided that this decision was going to get made entirely in terms of Galumph-protection? Who picked this leader? The decision-process here is clearly biased against my realm: just because we don\u2019t believe in Galumph anymore doesn\u2019t mean that we\u2019re OK with our temples getting destroyed.\u201d</p><p><strong>First tribe representative</strong>: \u201cIt was us; we picked the leader. But it\u2019s totally fair to make this decision in Galumph-protection terms: after all, to decide what to do <i>just is </i>to decide what protects Galumph the most. Sure, we have our disagreements about the nature of Galumph, but we can all agree that Galumph is what it\u2019s all about, right?\u201d</p><p><strong>Third tribe representative</strong>: \u201cNo -- we don\u2019t think that Galumph is what it\u2019s all about at all, actually.\u201d</p><p><strong>First tribe representative</strong>: \u201cBut how do you decide what to do?\u201d</p><p><strong>Third tribe representative: </strong>\u201cWe try to protect our temples. We love them.\u201d</p><p><strong>First tribe representative: </strong>\u201cBut\u2026 you don\u2019t think they have any Galumph.\u201d</p><p><strong>Third tribe representative: </strong>\u201cNo Galumph at all.\u201d</p><p><strong>First tribe representative: </strong>\u201cBut why aren\u2019t you in total despair? Why aren\u2019t you indifferent to everything, in the absence of Galumph to guide you? Indeed, I expected you to show up at this meeting and be like: \u2018Go ahead and burn our temples, we checked for Galumph but there\u2019s none in our realm, so we\u2019ve got no skin in the game.\u2019\u201d</p><p><strong>Third tribe representative: </strong>\u201cLong ago, when we first started having doubts about Galumph, we thought maybe that\u2019s how we\u2019d feel if we stopped believing in him. Indeed, for a while, even as the evidence against Galumph came in, we said \u2018well, if there\u2019s no Galumph, then nothing protects Galumph, so for practical purposes we can basically condition on Galumph existing even if it\u2019s low probability.\u2019 Eventually, though, as the evidence came in, we realized that actually, we loved our temples whether Galumph lives in them or not.\u201d</p><p><strong>First tribe representative: </strong>\u201cOk but you admit that your love for your temples does not protect Galumph at all \u2013 it\u2019s just some attitude that you have. So while I recognize that it may be <i>hard, </i>psychologically,to let go of these temples (I think you haven\u2019t fully internalized that <i>they have no Galumph</i>), you agree, surely, that the Galumph-protecting thing to do here is to submit your temples to the fire, and support the deal.\u201d</p><p><strong>Third tribe representative</strong>: \u201cI am not here to protect Galumph. I am here to protect the temples.\u201d</p><p><strong>First tribe representative</strong>: \u201cI cannot talk to this person. It is not Galumph-protecting for them to be admitted into this process. We are trying to make a <i>decision</i> here, people, and <i>decisions are about Galumph protection</i>.\u201d</p><p><strong>Third tribe representative</strong>: \u201cC\u2019mon, it\u2019s not even very clear what the difference is between us and the second tribe: we have the same fundamental metaphysics, after all. Plausibly, the disagreement is mostly semantic \u2013 it\u2019s about whether to call the temples Galumph or not.\u201d</p><p><strong>First tribe representative</strong>: \u201cWe agree: we think that the second tribe is a bunch of heathens, who barely understand the concept of Galumph. Galumph cannot <i>be</i> a temple; for with every temple, it is an <a href=\"https://en.wikipedia.org/wiki/Open-question_argument\">open question</a> whether Galumph lives in it! Thus, Galumph must live in a separate realm that no one can touch or interact with.\u201d</p><p><strong>Second tribe representative</strong>: \u201cYeah, we\u2019re not sure that there\u2019s a deep difference between us and the third tribe, either. But we\u2019re definitely not into the separate realm thing, and we find talking about Galumph pretty natural and useful, so our bet is that Galumph talk is in good order and fine to continue with.\u201d</p><p><strong>Third tribe representative: </strong>\u201cWhatever; it\u2019s about the temples at the end of the day.\u201d</p><p><strong>Second tribe representative</strong>: \u201cYeah, true, Galumph protection is really all about the temples. It\u2019s not like Galumph is some extra thing, anyway.\u201d</p><p><strong>Representative from the first tribe</strong>: \u201cYou must talk about Galumph. Temples are nothing if they have no Galumph. Burn them for any shred of Galumph protection\u2026\u201d</p></blockquote><p>This is far from a perfect analogy, but hopefully it can illustrate the sense in which a focus on \u201cexpected mattering\u201d biases the decision against nihilist worlds in a way that the inhabitants of those worlds wouldn\u2019t necessarily endorse.</p><h2><strong>VII. How much is nihilistic despair about meta-ethics anyway?</strong></h2><p>I said above that the \u201cyou should have higher credence on naturalism-but-things-matter\u201d objection and the \u201cwait but I still care about things even if nihilism is true\u201d objection to the realist\u2019s wager are closely related. That is, in both cases one rejects the idea of indifference (or: effective indifference) to worlds without the non-natural normative frosting. We can articulate this rejection as \u201cthings still matter even without the frosting!\u201d, or as \u201cI\u2019m not indifferent even if things don\u2019t matter!\u201d. But it\u2019s not always clear what of substance is at stake in the difference \u2013 just as it\u2019s not always clear what\u2019s at stake in the disagreement between the second and the third tribes, other than how to use the <i>word</i> \u201cGalumph.\u201d</p><p>I\u2019m not, here, going to dig in on exactly how deep the differences between nihilism and naturalism-but-things-matter go. To the extent there are substantive differences, though, my own take is that naturalism-but-things-matter is probably the superior view \u2013 I expect normative talk (perhaps with some amount of adjustment/re-interpretation) to make sense even in a purely natural world (and I\u2019m skeptical, more generally, of philosophers who argue that \u201cX widely-used-folk-theoretical-term \u2013 \u2018value,\u2019 \u2018consciousness,\u2019 \u2018pain,\u2019 \u2018rationality,\u2019 etc -- <i>must</i> be used to mean Y-very-specific-and-suspicious-metaphysical-thing, and thus, either some very-specific-and-suspicious metaphysics is true, or X does not exist!). And even in the absence of substantive differences, I feel far more resonance with the aesthetic and psychological connotations of naturalism-but-things-matter than with the aesthetic and psychological connotations of nihilism \u2013 e.g., despair, apathy, indifference, and so on. I have defended nihilism\u2019s right to have a say in your decision-making, but I have not defended <i>those</i> things \u2013 to the contrary, one of my main points is that nihilism, as a purely meta-ethical thesis, does not imply them.</p><p>Indeed, my own sense is that most familiar, gloomy connotations of nihilism often aren\u2019t centrally about meta-ethics at all. Rather, they are associated more closely with a cluster of <i>psychological</i> and <i>motivational </i>issues related to depression, hopelessness, and loss of connection with a sense of care and purpose. Sometimes, these issues are bound up with someone\u2019s views about the metaphysics of normative properties and the semantics of normative discourse (and sometimes, we grope for this sort of abstract language in order to frame some harder-to-articulate disorientation). But often, when such issues crop up, meta-ethics isn\u2019t actually the core explanation. After all, the most meta-ethically inflationary realists, theists, and so on can see their worlds drain of color and their motivations go flat; and conversely, the most metaphysically reductionist subjectivists, anti-realists, nihilists and so on can fight just as hard as others to save their friends and families from a fire; to build flourishing lives and communities; to love their neighbors as themselves. Indeed, often (and even setting aside basic stuff about mental health, getting enough sleep/exercise, etc), stuff like despair, depression, and so on is often prompted most directly by the world not being a way <i>you want it to be</i> -- e.g., not finding the sort of love, joy, status, accomplishment and so forth that you\u2019re looking for; feeling stuck and bored by your life; feeling overwhelmed by the suffering in the world and/or unable to make a difference; etc -- rather than by actually not wanting anything, or by a concern that something you want deeply isn\u2019t <i>worth</i> wanting at the end of the day. Meta-ethics can matter in all this, yes, but we should be careful not to mistake psychological issues for philosophical ones \u2013 even when the lines get blurry.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyib385nnjkd\"><sup><a href=\"#fnyib385nnjkd\">[38]</a></sup></span></p><p>That said, I acknowledge that there are some people for whom the idea that there might be no non-natural normative facts prompts the sort of despair, hopelessness, and indifference traditionally associated with nihilism (whether they are implied by it or no). And to those people, the realist\u2019s wager can appear as a kind of lifeline \u2013 a way to stay in connection with some source of meaning and purpose, even if only via a slim thread of probability; a reason to keep getting up in the morning; a thin light amidst a wash of grey.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefx17s2an5u1l\"><sup><a href=\"#fnx17s2an5u1l\">[39]</a></sup></span>&nbsp;If such people have been left unconvinced by my arguments above (both for \u201cyou should have higher credence on naturalism-but-things-matter\u201d and for \u201cyou can still care about things, and give weight to that care, even if nihilism is true\u201d), then I grant that it\u2019s better to accept the realist\u2019s wager than to collapse into full-scale indifference and despair; better to at least demand a dollar, in some worlds, before acquiescing to being burned alive in others; better to fight for the realist worlds rather than for no worlds at all. But I want to urge the possibility of fighting for more worlds, too.</p><h2><strong>VIII. Do questions about the realist\u2019s wager make a difference in practice?</strong></h2><p>There\u2019s more to say about lots of issues here, and I don\u2019t claim to have pinned them all down. The main thing I want to urge people to do is to think carefully about Martha-style cases before blithely accepting claims like \u201cI should just basically condition on non-naturalist realism, because that\u2019s where all the expected mattering is,\u201d or \u201cit\u2019s fine to just ignore the nihilism worlds.\u201d It can be easy to say such things when it\u2019s mostly an excuse to stop thinking about some objection to non-naturalist realism, or some worry about nihilism, and to get back to whatever you were doing anyway. But it can be harder when lives are on the line.</p><p>Perhaps one wonders, though:<i> are</i> any lives on the line? Does any of this matter in the real world, where meta-ethical angels do not appear and ask us to make bets about the true theory of meta-ethics? I think it does matter, partly because of the (b) and (c) stuff above \u2013 that is, because of the possible correlations between meta-ethical, normative-ethical, and empirical views.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxkx1rh7mbei\"><sup><a href=\"#fnxkx1rh7mbei\">[40]</a></sup></span>&nbsp;Thus, if you have <i>different normative-ethical</i> <i>credences</i> (for example, on totalism vs. something messier) conditional on non-naturalist realism vs. its falsehood, then \u201cI should just condition on non-naturalist realism, because that\u2019s where all the expected mattering is\u201d will alter your first order normative credences as well. And similarly, if you make <i>different empirical predictions </i>conditional on non-naturalist realism vs. not (for example, about the degree of normative consensus to expect amongst aliens and AIs), then it\u2019ll alter the empirical worlds you\u2019re acting like you live in, too. I won\u2019t, here, try to map out or argue for any particular correlations, here \u2013 but I think they can make a decision-relevant difference.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefczmmhnxscii\"><sup><a href=\"#fnczmmhnxscii\">[41]</a></sup></span></p><p>That said, I don\u2019t want to overplay that difference, either, or to encourage getting bogged down in meta-ethical debates when they aren\u2019t necessary. Indeed, as I\u2019ve discussed in the past, I think that high-level goals like \u201cmake it to a wise and empowered future, where we can figure out all this stuff much better\u201d look reasonably robust across meta-ethical (and normative ethical) views, as do many more of the \u201cethical basics\u201d (suffering = bad, flourishing = good, etc). And often, getting the basics right is what counts.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnybnjh3cbgwf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefybnjh3cbgwf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In particular, I think it leaves us without the right type of epistemic access to the normative facts it posits. More <a href=\"https://handsandcities.com/2022/01/17/the-ignorance-of-normative-realism-bot/\">here</a>.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnehijttyy1w8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefehijttyy1w8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This objection is closely related to the one I discuss in \u201c<a href=\"https://jc.gatspress.com/2021/01/03/the-despair-of-normative-realism-bot/\">The despair of normative realism bot</a>\u201d \u2013 but it focuses more directly on how someone with a \u201crealism-or-bust\u201d attitude reasons in expectation about meta-ethical bets.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1umm8303emdh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1umm8303emdh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>One is tempted to say stronger things, which I won\u2019t focus on here. For example (though this gets complicated): it\u2019s&nbsp;<i>wrong</i>&nbsp;to take this deal. Also (though this is a separate point): it\u2019s&nbsp;<i>scary</i>&nbsp;to take this deal. Most other people \u2013 for example, your mother, those children, etc \u2013 would still very much prefer to not be burned alive, even if the natural world is all there is, and your favorite meta-ethics is false (what\u2019s meta-ethics again?). So realist wagering puts you and them at odds. (Perhaps you say: \u201cthat\u2019s scary\u201d isn\u2019t a philosophical objection. And strictly: fair enough \u2013 but it might be a clue to one.)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkjt5kvhbm5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkjt5kvhbm5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Alternatively, if the realist wagerer is&nbsp;<i>actually</i>&nbsp;conditioning on normative realism, they won\u2019t even say things like this. Rather, they\u2019ll just predict that in fact, they\u2019ll win the hundred dollars (even if e.g. they just saw their friend get burned alive taking a deal like this \u2013 Martha must\u2019ve made a mistake!). But this is an especially silly way to wager \u2013 one that compromises not just your morals, but your epistemics, too. Here I\u2019m reminded of a friend sympathetic to the realist\u2019s wager, who wondered whether he should lower his credence on the theory of evolution, because normative realism struggles with the evolutionary origins of our moral beliefs. Bad move, I say. But also, not necessary: realist wagerers can stay clearer-eyed.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7cgmodx1hqk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7cgmodx1hqk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>It can sometimes feel strange to assign credences to meta-ethical views \u2013 and in particular, views that you disagree with \u2013 because it can be hard to say exactly what the views are claiming, and one suspects that if they are false, they are also incoherent and/or deeply confused. I\u2019m going to set this sort of hesitation aside, though.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fno7orbnkrmp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefo7orbnkrmp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See&nbsp;<a href=\"https://static1.squarespace.com/static/5506078de4b02d88372eee4e/t/5f5a3ddd466873260486fb06/1599749604332/Moral+Uncertainty.pdf\">MacAskill, Bykvist, and Ord (2020)</a>&nbsp;for more. I\u2019ll mostly talk about expected \u201cmattering\u201d in what follows, rather than choiceworthiness, but the difference isn\u2019t important.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2knasmaj3qw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2knasmaj3qw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See in particular&nbsp;<a href=\"https://philpapers.org/archive/RIEUV.pdf\">Riedener (2021)</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyv4zkmo3ir\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyv4zkmo3ir\">^</a></strong></sup></span><div class=\"footnote-content\"><p>That said, I think that some of the discussion below \u2013 for example, the question of whether objective normative facts have more \u201coomph\u201d than subjective ones \u2013 applies naturally to categories that cut across the distinction between naturalist and non-naturalist views (for example, because some naturalist views treat normative facts as objective). Most of the points I make, though, could be reformulated using a different and/or more fine-grained carving up of the options.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fniztfocve3b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefiztfocve3b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Nihilism is sometimes accompanied by some further claims about the semantics of normative discourse \u2013 i.e., the claim that normative claims are candidates for truth or falsity, as opposed to e.g. expressions of emotions, intentions, etc \u2014 but I\u2019m going to focus on the simpler version.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfqdv00iy52m\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffqdv00iy52m\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Yes, 0% is the wrong credence to have here \u2013 more on that in a moment.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfziubrlgr2u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffziubrlgr2u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Traditional answer: no? Wait, what about the thing you updated on? Hmm\u2026</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnaip921ghkzv\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefaip921ghkzv\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I say this as someone with fairly strong views about it.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnopi5dg2t1c\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefopi5dg2t1c\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Let alone: is-ought gaps, moral twin earth, the semantics of normative discourse, the implicit commitments of deliberation, and so on\u2026</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnptcrriae82e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefptcrriae82e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201c<i>Merely&nbsp;</i>joy<i>\u201d</i>&nbsp;\u201c<i>just</i>&nbsp;love<i>\u201d</i>&nbsp;\u2013 say it with a bit of disgust.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnaszbeh4yd2u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefaszbeh4yd2u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See, famously,&nbsp;<a href=\"https://www.amazon.com/What-Matters-Berkeley-Tanner-Lectures/dp/019968104X\">Parfit (2013)</a>: \u201cNaturalists believe both that all facts are natural facts, and that normative claims are intended to state facts. We should expect that, on this view, we don\u2019t need to make irreducibly normative claims. If Naturalism were true, there would be no facts that only such claims could state. If there were no such facts, and we didn\u2019t need to make such claims, Sidgwick, Ross, I, and many others [i.e. normative theorists] would have wasted much of our lives. We have asked what matters, which acts are right or wrong, and what we have reasons to want, and to do. If Naturalism were true, there would be no point in trying to answer such questions\u201d (vol 2, p. 367).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2obbhgcdvxm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2obbhgcdvxm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Remind me what it means for something to matter again?</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnd9lllxyyu1\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefd9lllxyyu1\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Possible realist wagerer response: yes but when you\u2019re deciding in the face of meta-ethical uncertainty, giving weight to your a-rational desire to not be burned alive, even in nihilist worlds, is wrong and bad and selfish. Or maybe: that\u2019s because you secretly have high credence on normative realism after all, even when you condition on nihilism! More on this below.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1xqeep9e5p2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1xqeep9e5p2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Intuition pump: realism wants to make morality like math. But we expect the aliens, the paperclippers, etc to agree about math.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpaqbbdxm788\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpaqbbdxm788\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Though note that the realist <i>wagerers</i>, at least, never said realism was&nbsp;<i>true</i>. So careful about giving them Bayes points. I.e., if Bob says \u201c99% percent that I\u2019m about to die of poison, but if that\u2019s true then all my actions are low-stakes, so I\u2019m going to act like it\u2019ll be fine,\u201d and then it&nbsp;<i>is&nbsp;</i>fine, you might give Bob points for his expected-utility reasoning, but you shouldn\u2019t treat him like someone who&nbsp;<i>predicted</i>&nbsp;that it will be fine.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2gk3jpxjws7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2gk3jpxjws7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Thanks to Will MacAskill for discussion of this sort of factor.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxbdsw6dyst\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxbdsw6dyst\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Are we going to get&nbsp;<a href=\"https://reducing-suffering.org/two-envelopes-problem-for-brain-size-and-moral-uncertainty/\">two envelope problems</a>, here? Maybe \u2014 I\u2019m hoping to double click on this issue more at some point. But for now I mostly want to gesture at some of the basic dynamics (and I think two envelope problems are going to bite hardest if we have&nbsp;<i>uncertainty</i>&nbsp;about what sort of normative oomph factor to use, and try to take the expectation \u2013 but my examples here won\u2019t involve that).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzbsz7sqfiv8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzbsz7sqfiv8\">^</a></strong></sup></span><div class=\"footnote-content\"><p><i>Wagerer</i>: \u201cIt\u2019s altruism towards the people-that-matter, at least? Weighted by their mattering?\u201d</p><p><i>Skeptic</i>: \u201cWhat about towards the people, period?\u201d</p><p><i>Wagerer</i>: \u201cI don\u2019t care about people, period. I only care about people-that-matter.\u201d</p><p><i>Skeptic</i>: \u201cSounds a bit scary. Would you burn me alive for a dollar-that-matters, I didn\u2019t matter?\u201d</p><p><i>Wagerer</i>: \u201cYes. That\u2019s like asking: would you burn me alive, if it were the case that you&nbsp;<i>should</i>&nbsp;burn me alive?\u201d</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3ol1wtbrn1u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3ol1wtbrn1u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>E.g., if you say that X \u2013 say, the difference between a world with one person at 10 welfare and a world with one person at 20 welfare \u2014 matters the same amount conditional on total vs. average utilitarianism, you won\u2019t always be able to say the same thing about Y \u2013 e.g. the difference between a world with&nbsp;<i>two</i>&nbsp;people, both at 10, and that world, but with one of them at 20 instead. Comparison 1: &lt;10&gt; vs. &lt;20&gt; Comparison 2: &lt;10, 10&gt; vs. &lt;10, 20&gt; That is, in both cases you add ten welfare to the total, but in the first you double the average, whereas in the second you multiple it by 1.5. So what sort of change to the average is equivalent in mattering, on average-ism, to adding ten units of value, on totalism? Is it 1.5x, or 2x, or something else? The available answers seem worryingly arbitrary.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrddkuiaaln\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrddkuiaaln\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See MacAskill, Bykvist, and Ord (2020), Chapter 5, for discussion of and objections to structuralist views.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8do6sf7esrn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8do6sf7esrn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Or at least, they imply the same behavior if you\u2019re certain about them. If you have different amounts of credence on them (and we have some way of separating your credences from your normative oomph factors), then this difference can matter in Martha-like cases \u2013 a difference that I think helps diffuse the objection that the difference between small-deal and big-deal total utilitarianism is behaviorally meaningless.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6o9segiuhow\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6o9segiuhow\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is also true of the standard way of understanding utility functions as unique up to positive affine transformation.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnims0bw30sno\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefims0bw30sno\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is from&nbsp;<a href=\"https://www.amazon.com/Miracles-C-S-Lewis/dp/0060653019\">Lewis (1947)</a>. The full passage is one of my favorites from Lewis: \u201cMen are reluctant to pass over from the notion of an abstract and negative deity to the living God. I do not wonder. Here lies the deepest tap-root of Pantheism and of the objection to traditional imagery. It was hated not, at bottom, because it pictured Him as man but because it pictured Him as king, or even as warrior. The Pantheist\u2019s God does nothing, demands nothing. He is there if you wish for Him, like a book on a shelf. He will not pursue you. There is no danger that at any time heaven and earth should flee away at His glance. If He were the truth, then we could really say that all the Christian images of kingship were a historical accident of which our religion ought to be cleansed. It is with a shock that we discover them to be indispensable. You have had a shock like that before, in connection with smaller matters\u2014when the line pulls at your hand, when something breathes beside you in the darkness. So here; the shock comes at the precise moment when the thrill of&nbsp;<i>life&nbsp;</i>is communicated to us along the clue we have been following. It is always shocking to meet life where we thought we were alone. \u2018Look out!\u2019 we cry, \u2018it\u2019s&nbsp;<i>alive</i>.\u2019 And therefore this is the very point at which so many draw back\u2014I would have done so myself if I could\u2014and proceed no further with Christianity. An \u2018impersonal God\u2019\u2014well and good. A subjective God of beauty, truth and goodness, inside our own heads\u2014better still. A formless life-force surging through us, a vast power which we can tap\u2014best of all. But God Himself, alive, pulling at the other end of the cord, perhaps approaching at an infinite speed, the hunter, king, husband\u2014that is quite another matter. There comes a moment when the children who have been playing at burglars hush suddenly: was that a&nbsp;<i>real&nbsp;</i>footstep in the hall? There comes a moment when people who have been dabbling in religion (\u2018Man\u2019s search for God!\u2019) suddenly draw back. Supposing we really found Him? We never meant it to come to&nbsp;<i>that!&nbsp;</i>Worse still, supposing He had found us?\u201d</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnobcpjm8yvo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefobcpjm8yvo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Yet&nbsp;<a href=\"https://plato.stanford.edu/entries/pascal-wager/\">another reason</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnngvcbifjc6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefngvcbifjc6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Of course, these aren\u2019t new issues. We\u2019ve just added another factor to our EV calcs \u2013 and thus, another place for the mugger to strike. And we can block the mugger elsewhere, perhaps we can block it here as well. But realist wagerers also tend to be usually open to being mugged.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjm2vjmum8xd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjm2vjmum8xd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>That is, I do think that&nbsp;<i>objective&nbsp;</i>normative facts have a different intuitive character than subjective ones \u2013 one that makes intrinsic differences in oomphy-ness intuitive as well.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnroxz5f462e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefroxz5f462e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is partly because I think that we just do have to incorporate those considerations into our decision-making \u2013 and I\u2019m not sure that there are any distinctive issues for realist-wagerers in doing so. That said, I do think that (b) and (c) are crucial to why realist wagers matter in practice, even in the absence of Martha-like cases.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnddk0var3v2q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefddk0var3v2q\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Reminder that I\u2019m not, here, including any further claims about the semantics of normative discourse.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fncmvdiy1xmug\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefcmvdiy1xmug\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Ok, but what about the following version of the case?</p><blockquote><p><i>Martha\u2019s indifference-ism-focused deal</i>: Martha the meta-ethical angel offers you the following deal. If indifference-ism is false, she\u2019ll give you a hundred dollars. If indifference-ism is true, she\u2019ll burn you, your family, and a hundred innocent children alive.</p></blockquote><p>One issue here is that indifference-ism is very implausible \u2013 and if the probability that the burning happens is low&nbsp;<i>enough</i>, you should take the deal regardless of whether you care about what happens in that case. That said, there are some normative-ethical views that have implications in the broad vicinity of indifference-ism \u2013 i.e., versions of totalism that become&nbsp;<a href=\"https://www.lesswrong.com/posts/5iZTwGHv2tNfFmeDa/on-infinite-ethics#III__Problems_for_totalism\">indifferent to all finite actions in an infinite world</a>, because they can\u2019t affect the total (infinite) utility; or perhaps, views on which we are sufficiently \u201c<a href=\"https://users.ox.ac.uk/~mert2255/papers/cluelessness.pdf\">clueless</a>\u201d about the consequences of our actions that they all have the same EV. So you can\u2019t just say that \u201cthis is deal is OK, but indifference-ism is totally out of the question.\u201d</p><p>And indeed, interestingly, I think there\u2019s some hesitation about taking this deal, too \u2013 hesitation that seems harder to justify than hesitations about the nihilism-focused version (or the original), and which might therefore prompt suspicion of&nbsp;<i>all</i>&nbsp;the hesitations about Martha-like deal. In particular, there is, perhaps, some temptation to say \u201ceven if I&nbsp;<i>should</i>&nbsp;be indifferent to these people burned alive, I\u2019m not! Screw indifference-ism world! Sounds like a shitty objective normative order anyway \u2013 let\u2019s rebel against it.\u201d That is, it feels like indifference-ism worlds have told me what the normative facts are, but they haven\u2019t told me about my&nbsp;<i>loyalty</i>&nbsp;to the normative facts, and the shittyness of these normative facts puts that loyalty even more in question. &nbsp;</p><p>And perhaps, as well, there\u2019s some temptation to think that \u201cWell, indifference-ism world is morally required to be indifferent to my overall decision-procedure as well \u2013 so I\u2019ll use a decision-procedure that&nbsp;<i>isn\u2019t</i> indifferent to what happens in indifference-ism world. Indifference-ism world isn't allowed to care!\u201d &nbsp;</p><p>These responses might seem dicey, though. If they (or others) don't end up working, ultimately I think that biting the bullet and taking this sort of deal is in fact less bad than doing so in the nihilism-focused version or the original. So it\u2019s an option if necessary \u2013 and one I\u2019d substantially prefer to biting the bullet in all of them.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnij7bh8hkc2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefij7bh8hkc2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Perhaps, as a psychological matter, some genuinely would be indifferent to being burned alive, to the suffering of others, and so on, conditional on nihilism \u2013 or at least, substantially less invested in whether or not these things happen. I\u2019ll discuss this relationship with nihilism, and meta-ethics more broadly, more below.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndyeydedr5i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdyeydedr5i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See&nbsp;<a href=\"https://www.jstor.org/stable/10.1086/505234\">Ross (2006)</a>&nbsp;for more on this sort of reasoning; and&nbsp;<a href=\"https://www.journals.uchicago.edu/doi/abs/10.1086/669564\">MacAskill (2013)</a>&nbsp;for some problems.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmkvj0t8wb9i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmkvj0t8wb9i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Some realists seem to think that making decisions like this requires some kind of self-deception about whether nihilism is true, but I\u2019m skeptical.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnuy46z7k6iba\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefuy46z7k6iba\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is necessary to capture the analogy with different possible worlds, where the representatives of each world are treated as knowing that their world is the one that exists.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyib385nnjkd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyib385nnjkd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Here I\u2019m reminded of a diagnosis of solipsism/other-minds-skepticism that I think I once heard associated with Stanley Cavell, on which one mistakes a psychological difficulty relating to other people with an epistemic or metaphysical one about whether they or their minds exist at all. I\u2019m not going to evaluate that diagnosis here, though.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnx17s2an5u1l\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefx17s2an5u1l\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Though here I think again of the analogy with theism. When I used to talk to a lot of Christian apologists, I would often hear the argument that without God, life and ethics are meaningless \u2013 an argument that, looking back, seems to me to function in a quasi-coercive way. That is, if you can convince someone that all meaning and purpose depends on your ideology being true, you can scare them into believing it \u2013 or at least, trying to believe it, and betting on it in the meantime, while assuming (I expect, wrongly) that they\u2019ll fall into un-ending despair and hopelessness if they ever stop. And I worry that the non-naturalist realists are somehow pulling a similar move on themselves, and on their innocent undergrads (see \u201c<a href=\"https://jc.gatspress.com/2021/01/03/the-despair-of-normative-realism-bot/\">The despair of normative realism bot</a>\u201d for some more on this). But I wish I had asked the Christian apologists I spoke to: if life and ethics are meaningless without God, does that mean you\u2019d take a deal like \u201cI\u2019ll give you a dollar if theism is true, but I\u2019ll torture you and a hundred others if its false?\u201d (Would even Jesus take that deal? What about Jesus on the cross?&nbsp;<i>Eli</i>,&nbsp;<i>Eli</i>\u2026) And if not, what\u2019s your&nbsp;<i>N</i>, such that you\u2019re indifferent between saving one people conditional on theism, vs.&nbsp;<i>N</i>&nbsp;people conditional on non-theism? And I want to ask the same of the non-naturalist realists, now.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxkx1rh7mbei\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxkx1rh7mbei\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See Chappell&nbsp;<a href=\"https://rychappell.substack.com/p/when-metaethics-matters\">here</a>&nbsp;for more discussion of ways meta-ethics can matter.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnczmmhnxscii\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefczmmhnxscii\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See e.g.&nbsp;<a href=\"https://www.lesswrong.com/s/SXurf2mWFw8LX2mkG/p/Yicjw6wSSaPdb83w9\">Eliezer Yudkowsky\u2019s history</a>&nbsp;with the realist\u2019s wager, which initially led him to want to push for the singularity as fast as possible \u2013 a view he later came to see as extremely misguided.</p></div></li></ol>", "user": {"username": "Joe_Carlsmith"}}, {"_id": "i5kRzbyDmZtaYmcbG", "title": "Sign of quality of life in GiveWell\u2019s analyses", "postedAt": "2022-10-16T14:54:53.186Z", "htmlBody": "<h3><strong>Content information: This post discusses that lives can be valued negatively by intended beneficiaries.</strong></h3><p><i><strong>TLDR: GiveWell can include the sign of life quality variable in its analyses.</strong></i></p><h3>Centre for Pesticide Suicide Prevention (CPSP)</h3><p>In January 2021, GiveWell&nbsp;<a href=\"https://www.givewell.org/research/incubation-grants/CPSP-general-support-jan-2021\"><u>recommended</u></a> an approximately $7 million general support grant for the Centre for Pesticide Suicide Prevention (CPSP). The Centre supports the deregistration of pesticides commonly used in suicide.&nbsp;</p><p>GiveWell&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1eNBp1Yd9HcOM1-MHQU6HPFz9204GZqcRXGCJrPp5aNE/edit#gid=278459571&amp;range=C89:E89\"><u>assumes</u></a> that the Centre accelerates the pesticide deregistration by 9 years in each of the countries of operation. The number of lives saved is calculated as the difference between the numbers of lives saved in different policy scenarios weighted by these scenarios' probabilities and the counterfactual improvement in pesticide suicide rates, as extrapolated from past trends. 25,938 lives are expected to be saved.</p><p>The assumption that the intended beneficiaries would otherwise suicide can suggest that they value their lives negatively.</p><p>GiveWell&nbsp;<a href=\"https://www.givewell.org/research/incubation-grants/CPSP-general-support-jan-2021\"><u>assumes</u></a> that as a result of the pesticide deregistration [edit based on a <a href=\"https://forum.effectivealtruism.org/posts/i5kRzbyDmZtaYmcbG/sign-of-quality-of-life-in-givewell-s-analyses?commentId=XcyynuyA7eAAovbvE\">comment</a>: and agricultural productivity decrease, the expected value of the program falls by 30%. The productivity decrease can be much lower than 30%, around low units of percent.] The effects of this estimated productivity decrease on the quality of the intended beneficiaries\u2019 lives are not discussed.</p><p>CPSP-promoted pesticide bans would affect entire nations with large farming populations, who live in extreme and national poverty. A <s>30%</s> decrease in yield can result in increased hunger levels and decreased income levels, which can negatively affect the health and quality of life of millions of people.</p><p>If the decreases in health and life quality cause a large percentage of farmers to value their lives negatively, support of CPSP can cause millions of people to live dissatisfied.</p><p>Thus, while CPSP is expected to save thousands of lives, it can also cause [edit: <s>millions</s> an unknown number of people] to live dissatisfied.</p><h3>Malaria vaccination in Kenya</h3><p>(Edit based on a <a href=\"https://forum.effectivealtruism.org/posts/i5kRzbyDmZtaYmcbG/sign-of-quality-of-life-in-givewell-s-analyses?commentId=2tbB8HKJ6MPmbvoSo\">comment</a>: The <a href=\"https://happiness-report.s3.amazonaws.com/2022/WHR+22.pdf\">2022 World Happiness Report </a>(WHR) (p. 19) estimates 'happiness'<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzrq9cfpuxeh\"><sup><a href=\"#fnzrq9cfpuxeh\">[1]</a></sup></span>&nbsp;in Kenya as 4.5/10.) The&nbsp;<a href=\"https://www.idinsight.org/publication/measuring-peoples-preferences/\"><u>Measuring people\u2019s preferences</u></a> IDinsight survey (edit: which shows values lower than similar reports) cites 2.3/10 (p. 42). If the&nbsp;<a href=\"https://www.happierlivesinstitute.org/key-ideas/\"><u>neutral point</u></a>, \u201cthe point where someone is neither satisfied nor dissatisfied,\u201d is above (edit: 2.3/10\u20134.5/10), reducing mortality in Kenya by&nbsp;<a href=\"https://www.givewell.org/research/grants/PATH-malaria-vaccines-January-2022\"><u>malaria vaccination</u></a> can save lives of dissatisfaction.</p><p>A small-scale (n=30)&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/13YHZWHvpi-aH6Mk2airk4raIlB5ijTT4_wZBP39wIVo/edit?usp=sharing\"><u>survey</u></a> that I conducted in early 2021 in a Kenyan slum suggests that many respondents can value their lives negatively. Respondents were asked to label death and their quality of life on a visual analog scale (<a href=\"https://drive.google.com/file/d/1NZC3eSc4nCO5mvP4jlcLNzng3FZU73s7/view?usp=sharing\"><u>arrow</u></a>) from the worst and the best imaginable situation. If worst corresponds to -1, death to 0, and best to 1, the average quality of life was -0.18, with a standard deviation of 0.40 (column AC). On average, respondents wanted to live 13 additional years (SD=26), while the median was 2 additional years.<img src=\"http://res.cloudinary.com/cea/image/upload/v1667995751/mirroredImages/i5kRzbyDmZtaYmcbG/uktncll0stzuwcpklhfc.png\"></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995751/mirroredImages/i5kRzbyDmZtaYmcbG/lj61sqjhe2vxyhqoexth.png\"></p><p>The wording of the quality of life questions and their survey context can be assessed as leading, which can suggest low quality of data.</p><blockquote><p>If you mai[n]tain your lifestyle and the future is what you expect it to be, how many more years starting from now do you want to live? You can say anything from 0 to forever.&nbsp;</p></blockquote><p>&nbsp;</p><blockquote><p>Where do you place death on this arrow? Please put a line there and write \"0.\" \u2026 Where do you place yourself on this arrow? Please put a line there and write \"myself.\"&nbsp;</p></blockquote><p>&nbsp;</p><p>A local enumerator familiar with the area collected the data, while knowing that it is gathered for informational purposes only, without a possible benefit of answering in any specific way. While some (globally poor) respondents accepted a small stipend for their time (which is a custom in the area), many were happy to share their responses without a stipend. This can suggest that respondents answered relatively honestly, with little experimenter bias.&nbsp;</p><p>This survey shows that the quality of life in a Kenyan slum can be valued negatively by the respondents. The quality of life in areas of Kenya that are prioritized for malaria vaccination can be different in sign, for example due to urban-rural disparities. Further research on the expected sign of life quality of intended malaria vaccination beneficiaries (considering possible philanthropic and market co-interventions) can inform whether, when, and to whom GiveWell should recommend the scale up of this pilot.</p><h3>Conclusion</h3><p>GiveWell can consider incorporating a variable on the sign of the quality of life from the perspective of intended beneficiaries in their Centre for Pesticide Suicide Prevention and malaria vaccination analyses. The inclusion of this variable in GiveWell's analyses can inform whether programs that \"<a href=\"https://www.givewell.org/\">save or improve</a>\" lives should be prioritized.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzrq9cfpuxeh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzrq9cfpuxeh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>WHR \"happiness\" is a function of GDP per capita, social support, healthy life expectancy, freedom to make life choices, generosity, perceptions of corruption, and a constant (<a href=\"https://happiness-report.s3.amazonaws.com/2022/WHR+22.pdf\">p. 19</a>). Other possible aspects of happiness, such as family relationships and perceptions of fairness can be covered to a limited extent by this metric. I have not extensively reviewed the World Happiness Report methodology. &nbsp;</p></div></li></ol>", "user": {"username": "brb243"}}, {"_id": "EckMY2g2FsozZDbQn", "title": "Is there more to life than survival and income?", "postedAt": "2022-10-13T21:56:40.194Z", "htmlBody": "<p><i><strong>TLDR: GiveWell\u2019s moral weights can include additional categories, based on intended beneficiaries\u2019 and prospective funders\u2019 preferences.</strong></i></p><h2>Introduction</h2><p>GiveWell\u2019s cost-effectiveness analyses can comprise a greater scope of moral weight categories. Currently, preventing death at different ages and increased income/consumption are the only considered criteria (<a href=\"https://docs.google.com/document/d/1hOQf6Ug1WpoicMyFDGoqH7tmf3Njjc15Z1DGERaTbnI/edit#\"><u>pp. 5\u20136</u></a>). Intended beneficiaries may have a complex set of priorities, which can resonate with prospective funders.&nbsp;</p><p>Covering a broader range of non-profit goals can increase international cooperation and safeguard peace. A caveat is that current power dynamics among nations may be strengthened, although individuals, globally, presented with a greater variety of lifestyle choices.&nbsp;</p><p>Sufficiently disaggregated national statistics can be used to estimate the impact of programs on different moral weight categories. World Bank data aggregates these statistics.</p><p>With additional moral weights, the impact of individual programs will likely be nominally smaller. This could discourage funders looking to save a large number of lives and to increase others' income relatively highly from donating. However, a well-developed set of weights can better showcase the strategic importance of specific funding focus as well as the need for philanthropic coordination.</p><p>My specific recommendations rely on a biased interpretation and selection of evidence. Research that mitigates human biases can be conducted.</p><p>GiveWell should research intended beneficiaries\u2019 and prospective funders\u2019 preferences robustly, if it can thus maintain its competitive advantage over larger charity assessors. Alternatively, GiveWell can seek to cooperate with other assessment organizations that define development more broadly.</p><h2>Intended beneficiaries\u2019 preferences</h2><p>Based on in-person interactions with globally poor individuals in Asia, Africa, and Latin America, as well as international development undergraduate and graduate coursework, IPA and J-PAL resources, and other studies and expert insights, I conclude that many intended beneficiaries have, at least some of, the following priorities:</p><ul><li>Prospects for themselves and family</li><li>Ability to afford emergency healthcare</li><li>Mutual respect within community</li><li>Adequate rest and physical health</li><li>Enjoyable living environment</li><li>Healthy family relationships</li><li>Safety from conflict and financial risks</li></ul><p>This perspective suggests that moral weights should include some of the following categories:</p><ul><li>Employment and underemployment</li><li>Education rates and quality</li><li>Preventive healthcare and insurance</li><li>Emergency healthcare availability and affordability</li><li>Community epistemics and relationships</li><li>Healthy and strenuous exercise</li><li>Rest, sleep, noise levels</li><li>Environmental sustainability</li><li>Spousal cooperation and relationships</li><li>Treatment of children</li><li>Civil and international conflict</li><li>Local crime rates</li></ul><p>These categories can be understood as examples of what intended beneficiaries may value and what could be measured to resolve and prevent underlying issues. An extensive study should be conducted to understand these preferences better. Alternatively, existing studies can be synthesized by an impartial team or a software. Enumerator bias should be controlled for, including by non-leading survey design and impartial and trusted enumerator engagement.</p><h2>Funders\u2019 and prospective funders\u2019 values</h2><p>The values commonly presented in the Giving Pledge letters and by the top 100 Forbes billionaires, in my perspective, include:</p><ul><li>Family and business success</li><li>Healthcare innovation and longevity</li><li>Technological innovation and competitive advantage</li><li>Meaningful AI advancement and safety</li><li>Education and upskilling</li><li>Market growth and brand recognition</li><li>Environmental sustainability</li><li>Economic inclusion (mainly within the US)</li><li>Art, culture, heritage</li></ul><p>This would suggest that GiveWell moral weights should include:</p><ul><li>Family cooperation and relationships<ul><li>Assuming that prospective funders\u2019 would be willing to support also other families\u2019 happiness</li></ul></li><li>Global value chain (GVC) participation</li><li>Healthcare systems advancement<ul><li>Assuming the willingness to share innovations abroad</li></ul></li><li>Life expectancy at different ages</li><li>Supplier efficiency increase</li><li>Education rates and economic relevance, job training</li><li>Total and brand product consumption</li><li>Climate change mitigation, adaptation, and preparedness</li><li>Impact of industrialization on nature and animals<ul><li>For example, industrialization can prevent the suffering of wild animals</li></ul></li><li>Income equality<ul><li>Assuming preference for income equality also abroad</li></ul></li><li>Artistic expression, cultural celebration, heritage preservation<ul><li>Assuming preferences for such also in other nations</li></ul></li></ul><p>My perspective may be biased by the set of resources that I reviewed and their order as well as the frameworks that I used to filter and synthesize prospective funders\u2019 preferences. GiveWell should dedicate significant effort to understand its customers better, in order to present a competitive product.</p><h2>International cooperation, security, power distribution&nbsp;</h2><p>Programs with a broader set of objectives can have wider impacts on international cooperation, security, and power distribution.</p><h3>Job training and education</h3><p>More inclusive job training and education can prevent the rise of elites abroad, which can disempower these nations compared to those where elites have already emerged. However, greater economic inclusion and GVC integration can safeguard global peace: countries that trade together have more to lose in a conflict.</p><h3>Family and community relationships</h3><p>Positive family and community relationships can influence constituents\u2019 and leaders' preference for a peaceful conflict resolution. Cooperative norms among nations with weaker institutions are increasingly more important as destructive technologies become more affordable.</p><h3>Health</h3><p>Health can have little direct impact on international security, although indirectly, increased economic integration due to healthier workforce can prevent the use of force in conflict resolution.</p><h3>Environment</h3><p>The profitability of environmental commitments and green technology adaptation should advantage nations with the greatest potential to implement relevant changes and develop technologies. Climate change mitigation, adaptation, and preparedness can prevent disputes related to environmental migration. Considering nature and non-human animals can increase societal empathy, which can motivate peaceful conflict resolution.</p><h3>Art, culture, heritage</h3><p>The impact of the advancement of art, culture, and heritage can have a range of impact, depending on the sentiments that it inspires. For example, a gallery that presents family farm pictures can have a widely different impact from a place that glorifies medieval torture instruments.</p><h2>World Bank data</h2><p>The&nbsp;<a href=\"https://data.worldbank.org/\"><u>World Bank</u></a> periodically collects thousands of indicators relevant to various moral weight categories and collates additional thousands of datasets. GiveWell should peruse these datasets to find the most relevant and complete and least biased statistics.</p><p>While the&nbsp;<a href=\"https://databank.worldbank.org/home.aspx\"><u>DataBank</u></a> databases display national-level data, national statistics may exist at the individual, household, and other levels that present sufficient disaggregation and sample sizes for statistically robust studies.</p><p>Examples of metrics relevant to moral weight categories preferred by both beneficiaries\u2019 and prospective funders include (<a href=\"https://databank.worldbank.org/source/world-development-indicators\"><u>World Development Indicators</u></a>):</p><ul><li>\u201cLabor force with basic education (% of total working-age population with basic education)\u201d</li><li>\u201cPrimary education, pupils (% female)\u201d</li><li>\u201cFirms offering formal training (% of firms)\u201d</li><li>\u201cChildren in employment, unpaid family workers (% of children in employment, ages 7-14)\u201d</li><li>\u201cWomen participating in the three decisions (own health care, major household purchases, and visiting family) (% of women age 15-49)\u201d</li><li>\u201cBirths attended by skilled health staff (% of total)\u201d</li><li>\u201cNumber of people spending more than 25% of household consumption or income on out-of-pocket health care expenditure\u201d</li><li>\u201cCPIA policy and institutions for environmental sustainability rating (1=low to 6=high)\u201d</li><li>\u201cTotal fisheries production (metric tons)\u201d</li></ul><h2>Nominal values, strategic importance, philanthropic cooperation</h2><p>A greater variety of moral weight categories will make the contribution of individual programs nominally smaller. For example, a chlorine dispenser program can greatly improve an aspect of the WASH metric, which falls under the health moral weight category as well as community relationships and culture (chatting during water collection). However, the program can have no impact on education, environment, global value chain integration, other aspects of health, and art. Thus, the impact of this globally top program can look relatively small. This small value could discourage a prospective donor from selecting the program.</p><p>Businesses, however, can see the strategic importance of supporting specific aspects of global development. For example, a company that supplies clean water in rural areas can be interested in early brand recognition in an emerging market by donating chlorine dispensers, in addition to its philanthropic contribution.</p><p>With multiple moral weight categories, it will become apparent that an individual, or a single foundation, cannot resolve all global issues by itself. This can motivate philanthropic coordination.</p><p>While GiveWell can facilitate this philanthropic coordination by implementing additional categories in its own analyses, it can also cooperate with other assessors that evaluate charities based on a broader range of impact criteria.</p><h2>Note on personal biases</h2><p>I am biased by the selection of resources that I engaged with, their order, and the frameworks that I reviewed them with. It is possible that intended beneficiaries and prospective funders have different priorities. Research that mitigates human biases can be conducted.</p><h2>Conclusion</h2><p>I suggested that GiveWell should include additional moral weight categories in its cost-effectiveness analyses or cooperate with charity assessors that use broader impact criteria. I hypothesized intended beneficiary and prospective funders\u2019 preferences. Further, I briefly discussed the effects of a broader set of philanthropic objectives on international cooperation, security, and power distribution. Relevant World Bank data series and metric examples were overviewed. The impact of nominal changes in GiveWell\u2019s analyses on the global philanthropic landscape was debated. I concluded with a note on personal biases.</p>", "user": {"username": "brb243"}}]