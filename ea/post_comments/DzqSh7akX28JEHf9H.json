[{"_id": "hXoeNzMJ4EFqxicHi", "postedAt": "2023-10-09T20:39:03.734Z", "postId": "DzqSh7akX28JEHf9H", "htmlBody": "<p>Great work and great writing, thank you. I wonder if there's anything better powered than t-tests in this setting though?</p>\n<p>ETA: is \"which forecaster is best?\" actually the right question to be answering? If the forecasts are close enough that we can't tell the difference after 100 questions, maybe we don't care about the difference?</p>\n", "parentCommentId": null, "user": {"username": "jooke"}}, {"_id": "WTYMayK7pectwem38", "postedAt": "2023-10-11T12:11:37.365Z", "postId": "DzqSh7akX28JEHf9H", "htmlBody": "<p>Can't think of anything better than a t-test, but open for suggestions.&nbsp;</p><p>If a forecaster is consistently off by like 10 percentage points - I think that is a difference that matters. But even in that extreme scenario where the (simulated) difference between two forecasters is in fact quite large, we have a hard time picking that up using standard significance tests.</p>", "parentCommentId": "hXoeNzMJ4EFqxicHi", "user": {"username": "nikos"}}, {"_id": "TPgphrENaqkiyrJAn", "postedAt": "2023-10-12T19:37:02.145Z", "postId": "DzqSh7akX28JEHf9H", "htmlBody": "<p>Afraid I don't have good ideas here.</p>\n<p>Intuitively, I think there should be a way to take advantage of the fact that the outcomes are heavily structured. You have predictions on the same questions and they have a binary outcome.</p>\n<p>OTOH, if in 20% of cases the worse forecaster is better on average, that suggests that there is just a hard bound on how much we can get.</p>\n", "parentCommentId": "WTYMayK7pectwem38", "user": {"username": "jooke"}}, {"_id": "yXcB4nozzPEtMZG9w", "postedAt": "2023-10-19T23:01:00.123Z", "postId": "DzqSh7akX28JEHf9H", "htmlBody": "<p>You can get a sense for these sorts of numbers just by looking at a binomial distribution.</p><p>e.g., Suppose that there are n events which each independently have a 45% chance of happening, and a noisy/biased/inaccurate forecaster assigns 55% to each of them.</p><p>Then the noisy forecaster will look more accurate than an accurate forecaster (who always says 45%) if &gt;50% of the events happen, and you can use the binomial distribution to see how likely that is to happen for different values of n. For example, according to <a href=\"https://stattrek.com/online-calculator/binomial\">this binomial calculator</a>, with n=51 there is a 24% chance that at least 26/51 of the p=.45 events will resolve as True, and with n=201 there is a 8% chance (I'm picking odd numbers for n so that there aren't ties).</p><p>With slightly more complicated math you can look at statistical significance, and you can repeat for values other than trueprob=45% &amp; forecast=55%.</p>", "parentCommentId": null, "user": {"username": "Dan_Keys"}}, {"_id": "NGFjaB2k33Rvs6bpX", "postedAt": "2023-10-20T13:42:44.717Z", "postId": "DzqSh7akX28JEHf9H", "htmlBody": "<p>Good comment, thank you!</p>", "parentCommentId": "yXcB4nozzPEtMZG9w", "user": {"username": "nikos"}}, {"_id": "ckXJZBj8tT5huhqj8", "postedAt": "2023-10-24T17:26:50.618Z", "postId": "DzqSh7akX28JEHf9H", "htmlBody": "<p>If I can modus tolens this modus polens, it feels to me that</p><blockquote><p>Indeed, even for 100 questions [...] this would come up as significant less than 50% of the time</p></blockquote><p>is evidence that the noise level is low, and the skill difference is small.</p><p>E.g., taking the top 20 forecasters in Metaculus' last <a href=\"https://www.metaculus.com/tournament/quarterly-cup-2023q3/\"><u>Quarterly Cup</u></a>, we see average score differences of ~0.05 (equivalent to your highest noise level), and that's among the very top forecasters we had on that tournament!</p>", "parentCommentId": null, "user": {"username": "Sylvain"}}]