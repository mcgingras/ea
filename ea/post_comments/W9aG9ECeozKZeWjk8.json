[{"_id": "brJqgNtCnrTDjPxNH", "postedAt": "2022-08-24T15:00:04.870Z", "postId": "W9aG9ECeozKZeWjk8", "htmlBody": "<p>It's a very important question.</p>\n<blockquote>\n<p>However, it probably doesn't make sense to keep this information to oneself since other people can begin to work on research and mitigation if they are aware of the risk.</p>\n</blockquote>\n<p>I don't think this is always the case. In anthropogenic x-risk domains, it can be very hard to decrease the chance of an existential catastrophe from a certain technology, and very easy to inadvertently increase it (by drawing attention to an info hazard). Even if the researchers (within EA) are very successful, their work can easily be ignored by the relevant actors in the name of competitiveness (\"our for-profit public-benefit company takes the risk much more seriously than the competitors, so it's better if we race full speed ahead\", \"regulating companies in this field would make China get that technology first\", etc.).</p>\n<p>(See also: <a href=\"https://nickbostrom.com/papers/vulnerable.pdf\">The Vulnerable World Hypothesis</a>.)</p>\n", "parentCommentId": null, "user": {"username": "ofer"}}]