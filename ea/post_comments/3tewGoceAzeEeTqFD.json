[{"_id": "BJhxwxSvjnqDiP3kW", "postedAt": "2022-12-27T16:37:53.520Z", "postId": "3tewGoceAzeEeTqFD", "htmlBody": "<p>You're <a href=\"https://www.lesswrong.com/posts/dKTh9Td3KaJ8QW6gw/why-assume-agis-will-optimize-for-fixed-goals\">not the first person</a> to notice this issue, and they didn't get a satisfying answer either, in my opinion. It seems like a holdover from early days of AI theorizing before we understood the power of machine learning/ evolutionary algorithm techniques. I personally find it highly unlikely that we'll end up with single minded consequentialist goal function maximisers, it seems like a difficult thing to program with machine learning techniques and one that would be unstable even if you could build it.&nbsp;</p>", "parentCommentId": null, "user": {"username": "titotal"}}]