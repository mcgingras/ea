[{"_id": "pQCmaYacgv45Py2CP", "postedAt": "2017-03-28T08:31:18.237Z", "postId": "N95ZziqxBbEbZJWmJ", "htmlBody": "<p>Optimizing for a narrower set of criteria allows more optimization power to be put behind each member of the set. I think it is plausible that those who wish to do the most good should put their optimization power behind a single criteria, as that gives it some chance to actually succeed. The best candidate afaik is right to exit, as it eliminates the largest possible number of failure modes in the minimum complexity memetic payload. Interested in arguments why this might be wrong.</p>\n", "parentCommentId": null, "user": {"username": "RomeoStevens"}}, {"_id": "dcNk3MuZEkPCtyxc6", "postedAt": "2017-03-28T16:34:18.160Z", "postId": "N95ZziqxBbEbZJWmJ", "htmlBody": "<p>It's great to see people thinking about these topics and I agree with many of the sentiments in this post. Now I'm going to write a long comment focusing on those aspects I disagree with. (I think I probably agree with more of this sentiment than most of the people working on alignment, and so I may be unusually happy to shrug off these criticisms.)</p>\n<p>Contrasting &quot;multi-agent outcomes&quot; and &quot;superintelligence&quot; seems extremely strange. I think the default expectation is a world full of many superintelligent systems. I'm going to read your use of &quot;superintelligence&quot; as &quot;the emergence of a singleton concurrently with the development of superintelligence.&quot;</p>\n<p>I don't consider the &quot;single superintelligence&quot; scenario likely, but I don't think that has much effect on the importance of AI alignment research or on the validity of the standard arguments. I do think that the world will gradually move towards being increasingly well-coordinated (and so talking about the world as a single entity will become increasingly reasonable), but I think that we will probably build superintelligent systems long before that process runs its course.</p>\n<blockquote>\n<p>The future looks broadly good in this scenario given approximately utilitarian values and the assumption that ems are conscious, with a large growing population of minds which are optimized for satisfaction and productivity, free of disease and sickness.</p>\n</blockquote>\n<p>On total utilitarian values, the actual experiences of brain emulations (including whether they have any experiences) don't seem very important. What matters are the preferences according to which emulations shape future generations (which will be many orders of magnitude larger).</p>\n<blockquote>\n<p>&quot;freewheeling evolutionary developments, while continuing to produce complex and intelligent forms of organization, lead to the gradual elimination of all forms of being that we care about&quot;</p>\n</blockquote>\n<p>Evolution doesn't really select against what we value, it just selects for agents that want to acquire resources and are patient. This may cut away some of our selfish values, but mostly leaves unchanged our preferences about distant generations.</p>\n<p>(Evolution might select for particular values, e.g. if it's impossible to reliably delegate or if it's very expensive to build systems with stable values. But (a) I'd bet against this, and (b) understanding this phenomenon is precisely the alignment problem!)</p>\n<p>(I discuss several of these issues <a href=\"https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/\">here</a>, Carl discusses evolution <a href=\"http://reflectivedisequilibrium.blogspot.com/2012/09/spreading-happiness-to-stars-seems.html\">here</a>.)</p>\n<blockquote>\n<p>Whatever the type of agent, arms races in future technologies would lead to opportunity costs in military expenditures and would interfere with the project of improving welfare. It seems likely that agents designed for security purposes would have preferences and characteristics which fail to optimize for the welfare of themselves and their neighbors. It\u2019s also possible that an arms race would destabilize international systems and act as a catalyst for warfare.</p>\n</blockquote>\n<p>It seems like you are paraphrasing a standard argument for working on AI alignment rather than arguing against it. If there weren't competitive pressure / selection pressure to adopt future AI systems, then alignment would be much less urgent since we could just take our time.</p>\n<p>There may be other interventions that improve coordination/peace more broadly, or which improve coordination/peace in particular possible worlds etc., and those should be considered on their merits. It seems totally plausible that some of those projects will be more effective than work on alignment. I'm especially sympathetic to your first suggestion of addressing key questions about what will/could/should happen.</p>\n<blockquote>\n<p>Not only is this a problem on its own, but I see no reason to think that the conditions described above wouldn\u2019t apply for scenarios where AI agents turned out to be the primary actors and decisionmakers rather than transhumans or posthumans.</p>\n</blockquote>\n<p>Over time it seems likely that society will improve our ability to make and enforce deals, to arrive at consensus about the likely consequences of conflict, to understand each others' situations, or to understand what we would believe if we viewed others' private information.</p>\n<p>More generally, we would like to avoid destructive conflict and are continuously developing new tools for getting what we want / becoming smarter and better-informed / etc. </p>\n<p>And on top of all that, the historical trend seems to basically point to lower and lower levels of violent conflict, though this is in a race with greater and greater technological capacity to destroy stuff.</p>\n<p>I would be more than happy to bet that the intensity of conflict declines over the long run. I think the question is just how much we should prioritize pushing it down in the short run.</p>\n<blockquote>\n<p>\u201cthe only way to avoid having all human values gradually ground down by optimization-competition is to install a Gardener over the entire universe who optimizes for human values.\u201d</p>\n</blockquote>\n<p>I disagree with this. See my earlier claim that evolution only favors patience.</p>\n<p>I do agree that some kinds of coordination problems need to be solved, for example we must avoid blowing up the world. These are similar in kind to the coordination problems we confront today though they will continue to get harder and we will have to be able to solve them better over time---we can't have a cold war each century with increasingly powerful technology.</p>\n<blockquote>\n<p>There is still value in AI safety work... but there are other parts of the picture which need to be explored</p>\n</blockquote>\n<p>This conclusion seems safe, but it would be safe even if you thought that early AI systems will precipitate a singleton (since one still cares a great deal about the dynamics of that transition).</p>\n<blockquote>\n<p>Better systems of machine ethics which don\u2019t require superintelligence to be implemented (as coherent extrapolated volition does)</p>\n</blockquote>\n<p>By &quot;don't require superintelligence to be implemented,&quot; do you mean systems of machine ethics that will work even while machines are broadly human level? That will work even if we need to solve alignment prior long before the emergence of a singleton? I'd endorse both of those desiderata.</p>\n<p>I think the main difference in alignment work for unipolar vs. multipolar scenarios is how high we draw the bar for &quot;aligned AI,&quot; and in particular how closely competitive it must be with unaligned AI. I probably agree with your implicit claim, that they either must be closely competitive or we need new institutional arrangements to avoid trouble.</p>\n<blockquote>\n<p>Rather than having a singleminded focus on averting a particular failure mode</p>\n</blockquote>\n<p>I think the mandate of AI alignment easily covers the failure modes you have in mind here. I think most of the disagreement is about what kinds of considerations will shape the values of future civilizations.</p>\n<blockquote>\n<p>both working on arguments that agents will be linked via a teleological thread where they accurately represent the value functions of their ancestors</p>\n</blockquote>\n<p>At this level of abstraction I don't see how this differs from alignment. I suspect the details differ a lot, in that the alignment community is very focused on the engineering problem of actually building systems that faithfully pursue particular values (and in general I've found that terms like &quot;teleological thread&quot; tend to be linked with persistently low levels of precision).</p>\n", "parentCommentId": null, "user": {"username": "Paul_Christiano"}}, {"_id": "qCJX4rCFSyKpyFcQ6", "postedAt": "2017-03-28T17:26:15.126Z", "postId": "N95ZziqxBbEbZJWmJ", "htmlBody": "<blockquote>\n<p>Optimizing for a narrower set of criteria allows more optimization power to be put behind each member of the set. I think it is plausible that those who wish to do the most good should put their optimization power behind a single criteria, as that gives it some chance to actually succeed. </p>\n</blockquote>\n<p>Only if you assume that there are high thresholds for achievements.</p>\n<blockquote>\n<p>The best candidate afaik is right to exit, as it eliminates the largest possible number of failure modes in the minimum complexity memetic payload. </p>\n</blockquote>\n<p>I do not understand what you are saying.</p>\n<p>Edit: do you mean, the option to get rid of technological developments and start from scratch? I don't think there's any likelihood of that, it runs directly counter to all the pressures described in my post.</p>\n", "parentCommentId": "pQCmaYacgv45Py2CP", "user": {"username": "Zeke_Sherman"}}, {"_id": "JZqtsbX4YXL7yGard", "postedAt": "2017-03-28T17:53:25.428Z", "postId": "N95ZziqxBbEbZJWmJ", "htmlBody": "<p>Thanks for the comments.</p>\n<blockquote>\n<p>Evolution doesn't really select against what we value, it just selects for agents that want to acquire resources and are patient. This may cut away some of our selfish values, but mostly leaves unchanged our preferences about distant generations.</p>\n</blockquote>\n<p>Evolution favors replication. But patience and resource acquisition aren't obviously correlated with any sort of value; if anything, better resource-acquirers are destructive and competitive. The claim isn't that evolution is intrinsically &quot;against&quot; any particular value, it's that it's extremely unlikely to optimize for any particular value, and the failure to do so nearly perfectly is catastrophic. Furthermore, competitive dynamics lead to systematic failures. See the citation.</p>\n<p>Shulman's post assumes that once somewhere is settled, it's permanently inhabited by the same tribe. But I don't buy that. Agents can still spread through violence or through mimicry (remember the quote on fifth-generation warfare).</p>\n<blockquote>\n<p>It seems like you are paraphrasing a standard argument for working on AI alignment rather than arguing against it.</p>\n</blockquote>\n<p>All I am saying is that the argument applies to this issue as well.</p>\n<blockquote>\n<p>Over time it seems likely that society will improve our ability to make and enforce deals, to arrive at consensus about the likely consequences of conflict, to understand each others' situations, or to understand what we would believe if we viewed others' private information.</p>\n</blockquote>\n<p>The point you are quoting is not about just any conflict, but the security dilemma and arms races. These do not significantly change with complete information about the consequences of conflict. Better technology yields better monitoring, but also better hiding - which is easier, monitoring ICBMs in the 1970's or monitoring cyberweapons today?</p>\n<p>One of the most critical pieces of information in these cases is intentions, which are easy to keep secret and will probably remain so for a long time.</p>\n<blockquote>\n<p>By &quot;don't require superintelligence to be implemented,&quot; do you mean systems of machine ethics that will work even while machines are broadly human level? </p>\n</blockquote>\n<p>Yes, or even implementable in current systems.</p>\n<blockquote>\n<p>I think the mandate of AI alignment easily covers the failure modes you have in mind here.</p>\n</blockquote>\n<p>The failure modes here are a different context where the existing research is often less relevant or not relevant at all. Whatever you put under the umbrella of alignment, there is a difference between looking at a particular system with the assumption that it will rebuild the universe in accordance with its value function, and looking at how systems interact in varying numbers. If you drop the assumption that the agent will be all-powerful and far beyond human intelligence then a lot of AI safety work isn't very applicable anymore, while it increasingly needs to pay attention to multi-agent dynamics. Figuring out how to optimize large systems of agents is absolutely not a simple matter of figuring out how to build one good agent and then replicating it as much as possible.</p>\n", "parentCommentId": "dcNk3MuZEkPCtyxc6", "user": {"username": "Zeke_Sherman"}}, {"_id": "Sw2QopCQTSuEq2X6d", "postedAt": "2017-03-28T19:07:18.052Z", "postId": "N95ZziqxBbEbZJWmJ", "htmlBody": "<p>right to exit means right to suicide, right to exit geographically, right to not participate in a process politically etc.</p>\n", "parentCommentId": "qCJX4rCFSyKpyFcQ6", "user": {"username": "RomeoStevens"}}, {"_id": "DG5XkK6e46xJvaRdq", "postedAt": "2017-03-28T22:53:56.236Z", "postId": "N95ZziqxBbEbZJWmJ", "htmlBody": "<blockquote>\n<p> If you drop the assumption that the agent will be all-powerful and far beyond human intelligence then a lot of AI safety work isn't very applicable anymore, while it increasingly needs to pay attention to multi-agent dynamics</p>\n</blockquote>\n<p>I don't think this is true in very many interesting cases. Do you have examples of what you have in mind? (I might be pulling a no-true-scotsman here, and I could imagine responding to your examples with &quot;well that research was silly anyway.&quot;)</p>\n<p>Whether or not your system is rebuilding the universe, you want it to be doing what you want it to be doing.  Which &quot;multi-agent dynamics&quot; do you think change the technical situation?</p>\n<blockquote>\n<p>the claim isn't that evolution is intrinsically &quot;against&quot; any particular value, it's that it's extremely unlikely to optimize for any particular value, and the failure to do so nearly perfectly is catastrophic</p>\n</blockquote>\n<p>If evolution isn't optimizing for anything, then you are left with the agents' optimization, which is precisely what we wanted. I though you were telling a story about why a community of agents would fail to get what they collectively want. (For example, a failure to solve AI alignment is such a story, as is a situation where &quot;anyone who wants to destroy the world has the option,&quot; as is the security dilemma, and so forth.)</p>\n<blockquote>\n<p>Yes, or even implementable in current systems.</p>\n</blockquote>\n<p>We are probably on the same page here. We should figure out how to build AI systems so that they do what we want, and we should start implementing those ideas ASAP (and they should be the kind of ideas for which that makes sense). When trying to figure out whether a system will &quot;do what we want&quot; we should imagine it operating in a world filled with massive numbers of interacting AI systems all built by people with different interests (much like the world is today, but more).</p>\n<blockquote>\n<p>The point you are quoting is not about just any conflict, but the security dilemma and arms races. These do not significantly change with complete information about the consequences of conflict. </p>\n</blockquote>\n<p>You're right.</p>\n<p>Unsurprisingly, I have a similar view about the security dilemma (e.g. think about automated arms inspections and treaty enforcement, I don't think the effects of technological progress are at all symmetrical in general). But if someone has a proposed intervention to improve international relations, I'm all for evaluating it on its merits. So maybe we are in agreement here.</p>\n", "parentCommentId": "JZqtsbX4YXL7yGard", "user": {"username": "Paul_Christiano"}}, {"_id": "MG5khTNmiHscsqwvd", "postedAt": "2017-03-29T10:11:59.636Z", "postId": "N95ZziqxBbEbZJWmJ", "htmlBody": "<p>Great to see a nuanced different perspective \nI'd be interested in how work on existing multi-agent problems can be translated into improving the value-alignment of a potential singleton (reducing the risk of theoretical abstraction uncoupling from reality with).</p>\n<p>Amateur question: would it help to also include back-of-the-envelop calculations to make your arguments more concrete?</p>\n", "parentCommentId": null, "user": {"username": "remmelt"}}, {"_id": "d34MuerKm4wfJBBsz", "postedAt": "2017-03-30T19:05:49.888Z", "postId": "N95ZziqxBbEbZJWmJ", "htmlBody": "<p>Isn't Elon Musk's OpenAI basically operating under this assumption? His main thing seems to be to make sure AGI is distributed broadly so no one group with evil intentions controls it. Bostrom responded that might be a bad idea, since AGI could be quite dangerous, and we similarly don't want to give nukes to everyone so that they're &quot;democratized.&quot; </p>\n<p>Multi-agent outcomes seem like a possibility to me, but I think the alignment problem is still quite important. If none of the AGI have human values, I'd assume we're very likely screwed, while we might not be if some do have human values.</p>\n<p>For WBE I'd assume the most important things for its &quot;friendliness&quot; is that we upload people who are virtuous and our ability and willingness to find &quot;brain tweaks&quot; that increase things like compassion.\nIf you're interested, here's a paper I published where I argued that we will <em>probably</em> create WBE by around 2060 if we don't get AGI through other means first:\n<a href=\"https://www.degruyter.com/view/j/jagi.2013.4.issue-3/jagi-2013-0008/jagi-2013-0008.xml\">https://www.degruyter.com/view/j/jagi.2013.4.issue-3/jagi-2013-0008/jagi-2013-0008.xml</a></p>\n<p>&quot;Industry and academia seem to be placing much more effort into even the very speculative strains of AI research than into emulation.&quot;\nActually, I'm gonna somewhat disagree with that statement. Very little research is done on advancing AI towards AGI, while a large portion of neuroscience research and also a decent amount of nanotechnology research (billions of dollars per year between the two) are clearly pushing us towards the ability to do WBE, even if that's not the reason that research is conducting right now.</p>\n", "parentCommentId": null, "user": {"username": "Daniel_Eth"}}, {"_id": "kjMdgHgdrNFgRtKzW", "postedAt": "2017-03-31T12:25:03.595Z", "postId": "N95ZziqxBbEbZJWmJ", "htmlBody": "<p>Just a comment on growth functions: I think a common prior here is that once we switch to computer consciousnesses, progress will go with Moore's law, which is exponential with doubling time of roughly 18 months (Ray Kurzweil says it is actually slow exponential growth in the exponent). Hanson sees the transition to a much shorter doubling time, something around one month. Others have noted that if the computer consciousnesses are making the progress and they are getting faster with Moore's law, you actually get a hyperbolic shape which goes to infinity in a finite time (around three years). Then you get to recursive self-improvement of AI, which could have a doubling time of days or weeks, and I think this is roughly the Yudkowsky position (though he does recognize that progress could get harder). I think this is the most difficult to manage. Then going the other direction from the Moore's law prior would be many economists who see continued exponential growth with the doubling time of decades. Then we have historical economists who think economic growth rate will go back to zero. Next you have the resource (or climate) doomsters who think there will be slow negative economic growth. Further down, you have faster catastrophes, which we might recover from. Finally, you have sudden catastrophes with no recovery. Quite the diversity in opinion: It would be an interesting project (or paper?) to try to plot this out.</p>\n", "parentCommentId": null, "user": {"username": "Denkenberger"}}, {"_id": "ZTmYqd6z28Cek9P3Y", "postedAt": "2017-03-31T21:13:14.048Z", "postId": "N95ZziqxBbEbZJWmJ", "htmlBody": "<p>There's a lot of value in having an AI safety orthodoxy for coordination purposes; there's also a lot of value in this sort of heterodox criticism of the orthodoxy. Thanks for posting.</p>\n<p>One additional area of orthodoxy that I think could use more critique is the community's views on consciousness. A few thoughts here (+comments):\n<a href=\"http://effective-altruism.com/ea/14t/principia_qualia_blueprint_for_a_new_cause_area/\">http://effective-altruism.com/ea/14t/principia_qualia_blueprint_for_a_new_cause_area/</a></p>\n<p>Also: nobody seems to be really looking into the state of AI safety &amp; x-risk memes inside of China. Whether they're developing a different 'availability cascade' seems hugely important and under-studied.</p>\n", "parentCommentId": null, "user": {"username": "MikeJohnson"}}, {"_id": "dmTeKPyGqYJ4F8vED", "postedAt": "2017-04-07T22:31:09.870Z", "postId": "N95ZziqxBbEbZJWmJ", "htmlBody": "<p>One implication of a multi-agent scenario is that there would likely be enormous variety in the types of minds that exist, as each mind design could be optimised for a different niche. So in such a scenario, it seems quite plausible that each feature of our minds would turn out to be a good solution in at least a few situations, and so would be reimplemented in minds designed for those particular niches.</p>\n", "parentCommentId": null, "user": {"username": "JoeW"}}, {"_id": "PjQ4JFpNTccQ7sva2", "postedAt": "2017-06-04T08:16:44.519Z", "postId": "N95ZziqxBbEbZJWmJ", "htmlBody": "<blockquote>\n<p>Very little research is done on advancing AI towards AGI, while a large portion of neuroscience research and also a decent amount of nanotechnology research (billions of dollars per year between the two) are clearly pushing us towards the ability to do WBE, even if that's not the reason that research is conducting right now.</p>\n</blockquote>\n<p>Yes, but I mean they're not trying to figure out how to do it safely and ethically. The ethics/safety worries are 90% focused around what we have today, and 10% focused on superintelligence.</p>\n", "parentCommentId": "d34MuerKm4wfJBBsz", "user": {"username": "Zeke_Sherman"}}, {"_id": "kKdofbB2ugFwJzpPg", "postedAt": "2017-06-04T08:19:46.505Z", "postId": "N95ZziqxBbEbZJWmJ", "htmlBody": "<blockquote>\n<p>Amateur question: would it help to also include back-of-the-envelop calculations to make your arguments more concrete?</p>\n</blockquote>\n<p>Don't think so. It's too broad and speculative with ill-defined values. It just boils down to (a) whether my scenarios are more likely than the AI-Foom scenario, and (b) whether my scenarios are more neglected. There's not many other factors that a complicated calculation could add.</p>\n", "parentCommentId": "MG5khTNmiHscsqwvd", "user": {"username": "Zeke_Sherman"}}, {"_id": "CRHnP6QuXdykuwqpr", "postedAt": "2017-06-04T08:28:33.037Z", "postId": "N95ZziqxBbEbZJWmJ", "htmlBody": "<blockquote>\n<p>I don't think this is true in very many interesting cases. Do you have examples of what you have in mind? (I might be pulling a no-true-scotsman here, and I could imagine responding to your examples with &quot;well that research was silly anyway.&quot;)</p>\n</blockquote>\n<p>Parenthesis is probably true, e.g. most of MIRI's traditional agenda. If agents don't quickly gain decisive strategic advantages then you don't have to get AI design right the first time; you can make many agents and weed out the bad ones. So the basic design desiderata are probably important, but it's just not very useful to do research on them now. Not familiar enough with your line of work to comment on it, but just think about the degree to which a problem would no longer be a problem if you can build, test and interact with many prototype human-level and smarter-than-human agents.</p>\n<blockquote>\n<p>Whether or not your system is rebuilding the universe, you want it to be doing what you want it to be doing. Which &quot;multi-agent dynamics&quot; do you think change the technical situation?</p>\n</blockquote>\n<p>Aside from the ability to prototype as described above, there are the same dynamics which plague human society when multiple factions with good intentions end up fighting due to security concerns or tragedies of the commons, or when multiple agents with different priors interpret every new piece of evidence they see differently and so go down intractably separate paths of disagreement. FAI can solve all the problems of class, politics, economics, etc by telling everyone what to do, for better or for worse. But multiagent systems will only be stable with strong institutions, unless they have some other kind of cooperative architecture (such as universal agreement in value functions, in which case you now have the problem of controlling everybody's AIs but without the benefit of having an FAI to rule the world). Building these institutions and cooperative structures may have to be done right the first time, since they are effectively singletons, and they may be less corrigible or require different kinds of mechanisms to ensure corrigibility. And the dynamics of multiagent systems means you cannot accurately predict the long term future merely based on value alignment, which you would (at least naively) be able to do with a single FAI.</p>\n<blockquote>\n<p>If evolution isn't optimizing for anything, then you are left with the agents' optimization, which is precisely what we wanted.</p>\n</blockquote>\n<p>Well it leads to agents which are optimal replicators in their given environments. That's not (necessarily) what we want.</p>\n<blockquote>\n<p>I though you were telling a story about why a community of agents would fail to get what they collectively want. (For example, a failure to solve AI alignment is such a story, as is a situation where &quot;anyone who wants to destroy the world has the option,&quot; as is the security dilemma, and so forth.)</p>\n</blockquote>\n<p>That too!</p>\n", "parentCommentId": "DG5XkK6e46xJvaRdq", "user": {"username": "Zeke_Sherman"}}]