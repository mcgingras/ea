[{"_id": "uKaD7Q6gzKoy99ufw", "postedAt": "2023-05-08T00:29:30.541Z", "postId": "7p6CFnd6fYYqsH42r", "htmlBody": "<p>Prime work. Super quick read and I gained some value out of it. Thanks!</p>", "parentCommentId": null, "user": {"username": "Elua"}}, {"_id": "4nYbq87dzrp7HEjGL", "postedAt": "2023-05-08T04:11:33.125Z", "postId": "7p6CFnd6fYYqsH42r", "htmlBody": "<blockquote><p>Do you disagree with any?&nbsp;</p></blockquote><p>Treating \"good future\" and \"irreversibly messed up future\" as exhaustive seems clearly incorrect to me.&nbsp;</p><p>Consider for instance the risk of an AI-stabilized personalist dictatorship, in which <i>literally </i>all political power is concentrated in a single immortal human being.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzfak21cucb\"><sup><a href=\"#fnzfak21cucb\">[1]</a></sup></span>Clearly things are not going great at this point. But whether they're irreversibly bad hinges on a lot of questions about human psychology - about the psychology of one particular human, in fact - that we don't have answers to.&nbsp;</p><ul><li>There's some evidence Big-5 Agreeableness <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2562318/\">increases slowly over time</a>. Would the trend hold out to thousands of years?</li><li>How long-term are long-term memories (augmented to whatever degree human mental architecture permits)?&nbsp;</li><li>Are value differences between humans really insurmountable or merely very very very hard to resolve? Maybe spending ten thousand years with the classics really would cultivate virtue.&nbsp;</li><li>Are normal human minds even stable in the very long run? Maybe we all wirehead ourselves eventually, given the chance.&nbsp;</li></ul><p>So it seems to me that if we're not ruling out permanent dystopia we shouldn't rule out \"merely\" very long lived dystopia either.&nbsp;</p><p>This is clearly not a \"good future\", in the sense that the right response to \"100% chance of a good future\" is to rush towards it as fast as possible, and the right response to \"10% chance of utopia 'till the stars go cold, 90% chance of spending a thousand years beneath Cyber-Caligula's sandals followed by rolling the dice again\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsz91jnx3u4j\"><sup><a href=\"#fnsz91jnx3u4j\">[2]</a></sup></span>&nbsp;is to slow down and see if you can improve the odds a bit. But it doesn't belong in the \"irreversibly messed up\" bin either: even after Cyber-Caligula takes over, the long-run future is still <i>almost certainly</i> utopian.&nbsp;</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzfak21cucb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzfak21cucb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Personally I think this is far less likely than AI-stabilized oligarchy (which, if not exactly a good future, is at least much less likely to go off into rotating-golden-statue-land) but my impression is that it's the prototypical \"irreversible dystopia\" for most people.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsz91jnx3u4j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsz91jnx3u4j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Obviously our situation is much worse than this</p></div></li></ol>", "parentCommentId": null, "user": {"username": "yefreitor"}}]