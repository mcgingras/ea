[{"_id": "2mkrtTWwxJZtrzDAc", "postedAt": "2023-03-06T02:01:11.986Z", "postId": "B9JrWqkzdmsPkpuzY", "htmlBody": "<p>The second video seems really interesting to me, as someone who's into moral philosophy. The first video personally falls into \"it's bad on purpose to make you click\" territory, though.</p>", "parentCommentId": null, "user": {"username": "Peter Berggren"}}, {"_id": "aMggReY6FNybn3awZ", "postedAt": "2023-03-06T02:14:49.384Z", "postId": "B9JrWqkzdmsPkpuzY", "htmlBody": "<p>If you watch from when I suggest in the link, I think it's less bad than you make out</p>\n", "parentCommentId": "2mkrtTWwxJZtrzDAc", "user": {"username": "Gideon Futerman"}}, {"_id": "SZhWNHotEEFnSr5Qg", "postedAt": "2023-03-06T04:41:11.538Z", "postId": "B9JrWqkzdmsPkpuzY", "htmlBody": "<p>I skimmed from 37:00 to the end. It wasn't anything groundbreaking. There was <s>one incorrect claim (\"AI safteyists encourage work at AGI companies\")</s>, I think her apparent moral framework that puts disproportionate weight on negative impacts on marginalised groups is not good, and overall she comes across as someone who has just begun thinking about AGI x-risk and so seems a bit naive on some issues. However, \"bad on purpose to make you click\" is very unfair.<br><br>But also: she says that hyping AGI encourages races to build AGI. I think this is true! Large language models at today's level of capability - or even somewhat higher than this - are clearly not &nbsp;a \"winner takes all\" game; it's easy to switch to a different model that suits your needs better and I expect the most widely used systems to be the ones that work the best for what people want them to do. While it makes sense that companies will compete to bring better products to market faster, it would be unusual to call this activity an \"arms race\". Talking about arms races makes more sense if you expect that AI systems of the future will offer advantages much more decisive than typical \"first mover\" advantages, and this expectation is driven by somewhat speculative AGI discourse.<br><br>She also questions whether AI safetyists should be trusted to improve the circumstances of everyone vs their own (perhaps idiosyncratic) priorities. I think this is also a legitimate concern! MIRI were at some point apparently aiming to 1) build an AGI and 2) use this AGI to stop anyone else building an AGI (<a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\">Section A, point 6</a>). If they were successful, that would put them in a position of extraordinary power. Are they well qualified to do that? I'm doubtful (though I don't worry about it too much because I don't think they'll succeed)</p>", "parentCommentId": "aMggReY6FNybn3awZ", "user": {"username": "David Johnston"}}, {"_id": "j9ZnhgjcdeqCKChsY", "postedAt": "2023-03-06T06:14:04.453Z", "postId": "B9JrWqkzdmsPkpuzY", "htmlBody": "<blockquote><p>There was one incorrect claim (\"AI safteyists encourage work at AGI companies\")</p></blockquote><p>\"AI safetyists\" absolutely do encourage work at AGI companies. To take one of many examples, <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\">80,000 Hours are \"AI safetyists\"</a>, and their <a href=\"https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy\">job board</a> currently encourages work at OpenAI, Deepmind, and Anthropic, which are AGI companies.</p><p>(I haven't watched the video.)</p>", "parentCommentId": "SZhWNHotEEFnSr5Qg", "user": {"username": "Sarah Levin"}}, {"_id": "mcGvMbuRiJmPBddbr", "postedAt": "2023-03-06T06:22:15.084Z", "postId": "B9JrWqkzdmsPkpuzY", "htmlBody": "<p>Fair enough, she mentioned Yudkowsky before making this claim and I had him in mind when evaluating it (incidentally, I wouldn't mind picking a better name for the group of people who do a lot of advocacy about AI X-risk if you have any suggestions)</p>", "parentCommentId": "j9ZnhgjcdeqCKChsY", "user": {"username": "David Johnston"}}]