[{"_id": "8M6obZbgMaHELra94", "postedAt": "2023-09-16T10:40:40.416Z", "postId": "3hSEQnEN2D3SSzHWn", "htmlBody": "<p>I appreciate the way this post adds a lot of clarity and detail to pause proposals. Thanks for writing it and thanks also to the debate organisers.</p>\n<p>However, I think you\u2019re equivocating kind of unhelpfully between LLM development - which would be presumably affected by a pause - and special-purpose model development (e.g. the linked MegaSyn example) which would probably not be. This matters because the claim that AI is currently an emergency and harms are currently occurring. For a pause to prevent these harms, they would have to be from cutting-edge LLMs, but I\u2019m not aware of any compelling examples of such.</p>\n", "parentCommentId": null, "user": {"username": "tommcgrath"}}, {"_id": "6nRJWtTBzFZame2Ni", "postedAt": "2023-09-16T16:01:25.182Z", "postId": "3hSEQnEN2D3SSzHWn", "htmlBody": "<p>Hmm, when my friends talk about a government-enforced pause, they most often mean a limit on training compute for LLMs. (Maybe you don't think that's \"compelling\"? Seems at least as compelling as other versions of \"pause\" to me.)</p>", "parentCommentId": "8M6obZbgMaHELra94", "user": {"username": "zsp"}}, {"_id": "Ti5DpTZZQf4y2xxPP", "postedAt": "2023-09-16T19:37:11.006Z", "postId": "3hSEQnEN2D3SSzHWn", "htmlBody": "<p>Ah, seems like my original comment was unclear. I was objecting to the conjunction between (a) AI systems already have the potential to cause harm (as evidenced by the Weapons of Math Destruction, Nature, and MegaSyn links) and (b) a pause in frontier AI would reduce harms. The potential harms due to (a) wouldn't be at all mitigated by (b) so I think it's either a bit confused or misleading to link them in this article. Does that clarify my objection?</p><p>In general I'm quite uncomfortable by the equivocation I see in a lot of places between \"current models are actually causing concrete harm X\" and \"future models could have the potential to cause harm X\" (as well as points in between, and interchanging special-purpose and general AI). I think these equivocations particularly harms the debate on open-sourcing, which I value and feels especially under threat right now.</p>", "parentCommentId": "6nRJWtTBzFZame2Ni", "user": {"username": "tommcgrath"}}, {"_id": "igW7DFWXPrxL2SqZB", "postedAt": "2023-09-17T17:18:54.338Z", "postId": "3hSEQnEN2D3SSzHWn", "htmlBody": "<p>Tracking compute is required for both. These models provide sufficient reason to track compute, and to ensure that other abuses are not occurring, which was the reason I think it's relevant.</p>", "parentCommentId": "8M6obZbgMaHELra94", "user": {"username": "Davidmanheim"}}, {"_id": "7fsRreFDM7qAxtrvY", "postedAt": "2023-09-18T17:24:21.935Z", "postId": "3hSEQnEN2D3SSzHWn", "htmlBody": "<p>Thanks - this is clarifying. I think my confusion was down to not understanding the remit of the pause you're proposing. How about we carry on the discussion in the other comment on this?</p>", "parentCommentId": "igW7DFWXPrxL2SqZB", "user": {"username": "tommcgrath"}}, {"_id": "K2ydPcyC5nhyEdahX", "postedAt": "2023-09-19T09:02:07.220Z", "postId": "3hSEQnEN2D3SSzHWn", "htmlBody": "<p>Glad you also mentioned preventing harmful uses of AI with existing laws.</p>\n<p>Law list:</p>\n<ul>\n<li>\n<p>copyright infringement of copying creatives works into datasets for training models used to compete in the original authors\u2019 markets.</p>\n</li>\n<li>\n<p>violating the EU Digital Single Market Directive\u2019s stipulations on Text and Data Mining since it \u201cprejudices the legitimate interests of the rightholder or which conflicts with the normal exploitation of his work or other subject-matter.\u201d</p>\n</li>\n<li>\n<p>violating GDPR/CCDA by not complying with citizens\u2019 requests for access to personal data being processed, and to erase that data.</p>\n</li>\n<li>\n<p>CSAM collected in image and text datasets (as well as synthetic CSAM generated in outputs).</p>\n</li>\n<li>\n<p>Biometrics collected (eg. faces in images).</p>\n</li>\n</ul>\n<p>That\u2019s just on data laundering. Didn\u2019t get into employment law, product liability, or environmental regulations.</p>\n", "parentCommentId": null, "user": {"username": "remmelt"}}, {"_id": "aixEy3s8kNFTAWh48", "postedAt": "2023-09-19T09:09:48.959Z", "postId": "3hSEQnEN2D3SSzHWn", "htmlBody": "<p>It\u2019s the other way around \u2013 comprehensive enforcement of laws to prevent current harms also prevent \u201cfrontier models\u201d from getting developed and deployed. See my comment.</p>\n<p>It\u2019s unethical to ignore the harms of uses of open-source models (see laundering of authors\u2019 works, or training on and generation of CSAM).</p>\n<p>Harms there need to be prevented too. Both from the perspective of not hurting people in society now, and from the perspective of preventing the build up of risk.</p>\n<p>Also, this raises the question whether  \u201copen-source\u201d models are even \u201copen-source\u201d in the way software is:\n<a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807\">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807</a></p>\n", "parentCommentId": "Ti5DpTZZQf4y2xxPP", "user": {"username": "remmelt"}}, {"_id": "bRyXgbJwsuPmGDnR8", "postedAt": "2023-09-19T12:16:55.961Z", "postId": "3hSEQnEN2D3SSzHWn", "htmlBody": "<blockquote><p>Moving beyond current needs, as both a way to ensure that domestic policy doesn\u2019t get stuck dealing with immediate economic, equity, and political issues, I think we should push for an ambitious intermediate goal to promote the adoption of international standards regarding high-risk future models. To that end, I would call for every country to pass laws today that will trigger a full ban on deploying or training AI systems larger than GPT-4 which have not been reviewed by an international regulatory body with authority to reject applications, starting in 2025, pending international governance regimes with mandatory review provisions for potentially dangerous applications and models. This isn\u2019t helpful for the most obvious immediate risks and economic impacts of AI - and for exactly that reason, it\u2019s critical as a way to ensure the tremendous future risks aren\u2019t ignored.</p></blockquote><p>I strongly agree with that.<br><br>You don't talk much about compute caps as a lever elsewhere in the text, so I'm going to paste some passages I wrote on why I'm excited about compute-related interventions to slow down AI. (My summary on slowing AI is available on the database for AI governance researchers \u2013 if anyone is planning to work on this topic but doesn't have access to that database, feel free to email me and I can give you access to a copy.)</p><blockquote><p>Compute seems particularly suited for governance measures: it\u2019s quantifiable, can\u2019t be used by multiple actors at once, and we can restrict access to it. None of these three factors apply to software (so it\u2019s unfortunate that software progress plays a more significant role for AI timelines than compute increases). Monitoring compute access is currently difficult because compute is easy to transport, and we don\u2019t know where much of it is. Still, we could help set up a database, demand reporting from sellers, and shift compute use from physical access to cloud computing or data center access (centralizing access helps with monitoring). The ideal target state for compute governance might be some kind of \u201cmoving bright line\u201d of maximum compute allowances for training runs. (A static cap might be too difficult to enforce because compute costs to circumvent the cap will fall over time.) The regulation could be flexible so labs with a proven safety mindset can receive authorization to go beyond the cap. More ambitiously, there\u2019s the idea of&nbsp;</p><ul><li><strong>hardware-enabled governance mechanisms</strong> (previous terminology: \u201con-chip measures\u201d). These are tamper-proof mechanisms on chips (or on the larger hardware components of compute clusters) that would allow for actions like communicating information about a chip\u2019s location or its past activity, remote shutdown, or restricting the chip\u2019s communication with other chips (limiting the size of a training run it could be involved in). Hardware-enabled mechanisms don\u2019t yet exist in a tamper-proof way, but NVIDIA has chips that illustrate the concept. I\u2019m particularly excited about hardware-enabled governance mechanisms because they\u2019re the only idea related to slowing AI progress that could (combined with an ambitious regulatory framework) address the problem as a whole, instead of just giving us a small degree of temporary slowdown. (Hardware-enabled mechanisms would also continue to be helpful after the first aligned TAI is developed&nbsp; \u2013 it\u2019s not like coordination challenges will automatically go away at the point when an aligned AI is first developed.) Widespread implementation of such mechanisms is several years away even in a best-case scenario, so it seems crucial to get started.<ul><li>Onni Arne and Lennart Heim have been looking into hardware-enabled governance mechanisms.&nbsp;(My sense from talking to them is that when it comes to monitoring and auditing of compute, they see the most promise in measures that show a chip's past activity, \"proof of non-training.\") &nbsp;<a href=\"https://arxiv.org/abs/2303.11341\"><u>Yonadav Shavit</u></a> also works on compute governance and seems like a great person to talk to about this.</li></ul></li></ul></blockquote><p>And here's an unfortunate caveat about how compute governance may not be sufficient to avoid an AI catastrophe:</p><blockquote><p><strong>Software progress vs. compute:</strong> I\u2019m mostly writing my piece based on the assumption that software progress and compute growth are both important levers (with software progress being the stronger one). However, there\u2019s a view on which algorithmic improvements are a lot jumpier than Ajeya Cotra assumes in her \u201c2020 compute training requirements\u201d framework. If so, and if we\u2019re already in a compute overhang (in the sense that it\u2019s realistic to assume that new discoveries could get us to TAI with current levels of compute), it could be tremendously important to prevent algorithmic exploration by creative ML researchers, even at lower-than-cutting-edge levels of compute. (Also, the scaling hypothesis would likely be false or at least incomplete in that particular world, and compute restrictions would matter less since building TAI would mainly require software breakthroughs.) In short, if the road to TAI is mostly through algorithmic breakthroughs, we might be in a pretty bad situation in terms of not having available promising interventions to slow down progress.</p></blockquote><p>But there might still be some things to do to slow progress a little bit, such as improving information security to prevent leakage of insights from leading labs, and export controls on model weights.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Lukas_Gloor"}}, {"_id": "kREepkyy8rf4evg8J", "postedAt": "2023-09-19T19:01:06.137Z", "postId": "3hSEQnEN2D3SSzHWn", "htmlBody": "<p>I think that these are good points for technical discussions of how to implement rules, and thanks for brining them up - but I don't really makes sense to focus on this if the question is whether or not to regulate AI or have a moratorium.</p>", "parentCommentId": "bRyXgbJwsuPmGDnR8", "user": {"username": "Davidmanheim"}}, {"_id": "eiR7rC9RMeP6HfZFM", "postedAt": "2023-09-22T01:00:40.904Z", "postId": "3hSEQnEN2D3SSzHWn", "htmlBody": "<p>Thank you for your carefully thought-through essay on AI governance. Given your success as a forecaster of geopolitical events, could you sketch out for us how we might implement AI governance on, for example, Iran, North Korea, and Russia? You mention sensors on chips to report problematic behavior, etc. However, badly behaving nations might develop their own fabs. We could follow the examples of attacks on Iran's nuclear weapons technologies. But would overt/covert military actions risk missing the creation of a \"black ball\" on the one hand, or escalation into global nuclear/chemical/biological conflict?&nbsp;</p>", "parentCommentId": null, "user": {"username": "cmeinel"}}, {"_id": "moBFFME7FmuKQp9Fh", "postedAt": "2023-09-22T07:30:44.517Z", "postId": "3hSEQnEN2D3SSzHWn", "htmlBody": "<p>These are difficult problems, but thankfully not the ones we need to deal with immediately. None of Iran, Russia and North Korea are chip producers nor are they particularly close to SOTA in ML - if there is on-chip monitoring for manufacturers, and cloud compute has restrictions, there is little chance they accelerate. And we stopped the first two from getting nukes for decades, so export controls are definitely a useful mechanism. In addition, the incentive for nuclear states or otherwise dangerous rogue actors to develop AGI as a strategic asset is lessened if they aren't needed for balance of power - so a global moratorium makes these states less likely to feel a need to keep up in order to stay in power.&nbsp;</p><p>That said, a moratorium isn't a permanent solution to proliferation of dangerous tech, even if the regime were to end up being permanent. Like with nuclear weapons, we expect to raise costs of violating norms to be prohibitively high, and we can delay things for quite a long time, but if we don't have further progress on safety, and we remain / become convinced that unaligned ASI is an existential threat, we would need to continually reassess how strong sanctions and enforcement needs to be to prevent existential catastrophe. But if we get a moratorium in non-rogue states, thankfully, we don't need to answer these questions this decade, or maybe even next.</p>", "parentCommentId": "eiR7rC9RMeP6HfZFM", "user": {"username": "Davidmanheim"}}]