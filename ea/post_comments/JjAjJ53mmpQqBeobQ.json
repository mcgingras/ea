[{"_id": "aTyYiyZxbDSMpiAJE", "postedAt": "2023-05-19T15:25:10.769Z", "postId": "JjAjJ53mmpQqBeobQ", "htmlBody": "<p>psyched about this post, want to jot down a quick nit-pick</p><blockquote><ul><li>S-Risk can occur any time from now until the end of the race, and represents \u2013 for example - a totalitarian government seizing control of the world to such an extent that human flourishing is permanently curtailed, but the development of AI is not (so S-Risk can occur before AI is Invented).</li></ul></blockquote><p>I don't think curtailing human flourishing constitutes s-risk, I don't think the suffering-focused community likes to draw equivalences between opportunity cost and more immediate or obvious disvalue. When the s-risk community talks about malevolent actors (see CLR), they're talking more about associations between totalitarianism and willingness/ability to literally-torture at scale, whereas other theorists (not in the suffering-focused community) may worry about a flavor of totalitarianism where everyone has reasonable quality of life they just can't steer or exit.&nbsp;</p><p>One citation for the idea that opportunity costs (say all progress <i>but</i> spacefaring continues) and literally everyone literally dying is morally similar is the precipice. We can (polarizingly!) talk about \"existential risk\" not equalling \"extinction risk\" but equaling under some value function. This is one way of thinking about totalitarianism in the longtermist community.&nbsp;</p><p>Political freedoms and the valence of day to day experience aren't necessarily the exact same thing.&nbsp;</p>", "parentCommentId": null, "user": {"username": "quinn"}}, {"_id": "DxPE3KaWSoouiLjDN", "postedAt": "2023-05-20T08:16:43.364Z", "postId": "JjAjJ53mmpQqBeobQ", "htmlBody": "<p>Great analysis, Froolow!</p><blockquote><p>Analysts discussing AI Risk should describe the structure of their model much more explicitly. I observe there is a bit of a tendency on the forums to be cagey about one\u2019s \u2018actual\u2019 model of AI Risk when presenting estimates of Catastrophe, and imply that the \u2018actual\u2019 model of AI Risk one has is significantly more complicated than could possibly be explained in the space of a single post (phrases like, \u201cThis is <u>roughly</u> my model\u201d are a signifier of this).</p></blockquote><p>Agreed!</p>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "E9LxcYPBsKfPa8ywL", "postedAt": "2023-05-20T08:51:18.366Z", "postId": "JjAjJ53mmpQqBeobQ", "htmlBody": "<p>Thank you, really interesting comment which clarifies a confusion I had when writing the essay!</p>\n", "parentCommentId": "aTyYiyZxbDSMpiAJE", "user": {"username": "Froolow"}}, {"_id": "vBjPxNFh3swW6jQPy", "postedAt": "2023-07-01T13:22:13.746Z", "postId": "JjAjJ53mmpQqBeobQ", "htmlBody": "<p>I really liked this!</p>", "parentCommentId": null, "user": {"username": "BenStewart"}}]