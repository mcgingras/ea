[{"_id": "2quvunxkJwkpXEafz", "postedAt": "2023-06-07T15:33:47.794Z", "postId": "KRSthwicCTRw9Ayzg", "htmlBody": "<p>Really great post, and I'm waiting to see what others would think of it. My personal answer to most questions is that EA isn't as smart as we want to think it is, and we should indeed \"be more normal\".</p>\n<p>One note is that I'd like to challenge the assumption that EA is the \u201clargest and smartest\u201d expert group on \u201cMight AI lead to extinction?\u201d. I don't think this is true? This question involves a ton of different disciplines and many big guesses, and people in EA and Rationality who work on it aren't relatively better at them (and certainly not at all of them at once) than others. EAs might have deliberated more on this question, but the motives for that make it a biased sample.</p>\n", "parentCommentId": null, "user": {"username": "Guy Raveh"}}, {"_id": "ZEePDkw5r8nki6P9M", "postedAt": "2023-06-08T07:13:51.625Z", "postId": "KRSthwicCTRw9Ayzg", "htmlBody": "<blockquote><p>For example, can I do better than just deferring to the \u201clargest and smartest\u201d expert group on \u201cMight AI lead to extinction?\u201d (which seems to be EA). Can I instead look at the arguments and epistemics of EAs versus, say, opposing academics and reach a&nbsp;<i>better&nbsp;</i>conclusion? (Better in the sense of \u201cmore likely to be correct\u201d.) If so, how much and how should I do that in the details?</p></blockquote><p>&nbsp;</p><p>Deference is a major topic in EA. I am currently working on a research project simulating various models of deference.</p><p>So far, my findings indicate that deference is a double-edged sword:</p><ul><li>You will tend to have more accurate beliefs if you defer to the wisdom of the crowd (or perhaps to a subject-matter expert - I haven't specifically modeled this yet).</li><li>However, remember that others are also likely to defer to you. If they fail to track the difference between your all-things-considered, deferent best guess and the independent observations and evidence you bring to the table, this can inhibit the community's ability to converge on the truth.</li><li>If the community is extremely deferent <i>and</i> if there is about as much uncertainty about what the community's collective judgment actually is as there is about the object-level question at hand, then it tentatively appears that it's better even for individual accuracy to be non-deferent. It may be that there are even greater gains to be made just by being less deferent than the group.</li><li>Many of these problems can be resolved if the community has a way of aggregating people's independent (non-deferent) judgments, and only <i>then</i> deferring to that aggregate judgment when making decisions. It seems to me progress can be made in this direction, though I'm skeptical we can come very close to this ideal.</li></ul><p>So if your goal is to improve the community's collective accuracy, it tentatively seems best to focus on articulating your own independent perspective. It is also good to seek this out from others, asking them to not defer and to give their own personal, private perspective.</p><p>But when it comes time to make your own decision, then you will want to defer to a large, even extreme extent to the community's aggregate judgments.</p><p>Again, I haven't included experts (or non-truth-oriented activists) into my model. I am also basing my model on specific assumptions about uncertainty, so there is plenty of generalization from a relatively narrow result going on here.</p>", "parentCommentId": null, "user": {"username": "AllAmericanBreakfast"}}, {"_id": "EwL5LAsNfzR6hT8Km", "postedAt": "2023-06-08T09:47:36.762Z", "postId": "KRSthwicCTRw9Ayzg", "htmlBody": "<p>My impression is that others have thought so much less about AI x-risk than EAs and rationalists, and for generally bad reasons, that EAs/rats are the \"largest and smartest\" expert group basically 'by default'. Unfortunately with all the biases that come with that. I could be misunderstanding the situation tho.</p>", "parentCommentId": "2quvunxkJwkpXEafz", "user": {"username": "Luise"}}, {"_id": "HBiKZwSxzBzHfNyZN", "postedAt": "2023-06-08T14:04:50.521Z", "postId": "KRSthwicCTRw9Ayzg", "htmlBody": "<p>I definitely don't have the answers, but want to acknowledge that a significant degree of deference to someone or something is simply unavoidable in an increasingly complex world. I feel there's sometimes this undercurrent (not in your post specifically, just in general) that if you're a really smart person, you <u>should </u>be able to have well-thought-out, not-too-deferential views on everything of importance. It's just not possible; no matter how smart someone is, they lack the ability to warp the fabric of time to slow it down enough to reach that end. That's not meant to discourage anyone from seeking to improve their thinking about the world -- it's to reassure anyone who feels bad about the fact that they have to defer so much.</p><p>Each of us have to decide what questions are most important for us to dig deeply into, and which we should rely mainly on deference to answer. This is often affected by the importance of the question, but there are other variables -- like how much time we'd have to invest to get a more reliable answer, and the extent to which a change from the answer we are assuming would change our actions.</p><p>I know that doesn't help decide who or what to defer to . . . .</p>", "parentCommentId": null, "user": {"username": "Jason"}}, {"_id": "F2X7jXJHkS4NYf97E", "postedAt": "2023-06-08T14:17:50.845Z", "postId": "KRSthwicCTRw9Ayzg", "htmlBody": "<p>Yeah, there's almost certainly some self-selection bias there. If someone thinks that talk of AI x-risk is merely bad science fiction, they will either choose not to become an EA or one chooses to go into a different cause area (and are unlikely to spend significant time thinking any more about AI x-risk or discussing their heterodox view).</p><p>For example, people in crypto have thought so much more about crypto than people like me . . . but I would <i>not </i>defer to the viewpoints of people in crypto about crypto. I would want to defer to a group of smart, ethical people who I had bribed so heavily that they were all willing to think deeply about crypto whether they thought it was snake oil or more powerful than AGI. People who chose to go into crypto without my massive bribery are much more likely to be pro-crypto than an unbiased sample of people would be.</p>", "parentCommentId": "EwL5LAsNfzR6hT8Km", "user": {"username": "Jason"}}, {"_id": "gaypeTGQ4ZWKv5fMA", "postedAt": "2023-06-08T18:44:11.174Z", "postId": "KRSthwicCTRw9Ayzg", "htmlBody": "<p>The idea of deferring to common wisdom while continuing to formulate your own model reminds me EY's post on <a href=\"https://Even if subjects think they\u2019ve come up with a hypothesis, they don\u2019t have to actually bet on that prediction in order to test their hypothesis. They can say, \u201cNow if this hypothesis is correct, the next card will be red\u201d\u2014and then just bet on blue. They can pick blue each time, accumulating as many nickels as they can, while mentally noting their private guesses for any patterns they thought they spotted. If their predictions come out right, then they can switch to the newly discovered sequence.\">Lawful Uncertainty</a>. The focus was an experiment from the 60s where subjects guessed card colors from a deck of 70% blue cards. People keep on trying to guess red based on their own predictions even though the optimal strategy was to always pick blue. EY's insight which this reminded me of was:</p><blockquote><p>Even if subjects <i>think</i> they\u2019ve come up with a hypothesis, they don\u2019t have to <i>actually bet</i> on that prediction in order to test their hypothesis. They can say, \u201cNow if <i>this</i> hypothesis is correct, the next card will be red\u201d\u2014and then just bet on blue. They can pick blue each time, accumulating as many nickels as they can, while mentally noting their private guesses for any patterns they thought they spotted. If their predictions come out right, <i>then</i> they can switch to the newly discovered sequence.</p></blockquote>", "parentCommentId": "ZEePDkw5r8nki6P9M", "user": {"username": "FinalFormal"}}, {"_id": "ZdsyJpb8S3QbansK6", "postedAt": "2023-06-09T00:30:15.434Z", "postId": "KRSthwicCTRw9Ayzg", "htmlBody": "<p>Yeah I don't have non-deference based arguments of really basic and important things like:</p><ul><li>whether stars exist</li><li>how the money system works</li><li>gravity</li></ul><p>And it was only in the last few years that I considered inside view arguments for why the Earth isn't flat.&nbsp;</p>", "parentCommentId": "HBiKZwSxzBzHfNyZN", "user": {"username": "Linch"}}, {"_id": "BkDEvWusxESkWZEZZ", "postedAt": "2023-06-09T15:07:26.545Z", "postId": "KRSthwicCTRw9Ayzg", "htmlBody": "<p>We've thought about it a lot, but that doesn't mean we got anything worthwhile? It's like saying that literal doom prophets are the best group to defer to about when the world would end, because they've spent the most time thinking about it.</p>\n<p>I think maybe about 1% of publicly available EA thought about AI isn't just science fiction. Maybe less. I'm much more worried about catastrophic AI risk than 'normal people' are, but I don't think we've made convincing arguments about how those will happen, why, and how to tackle them.</p>\n", "parentCommentId": "EwL5LAsNfzR6hT8Km", "user": {"username": "Guy Raveh"}}, {"_id": "zsopBhJNhB6u4CtBf", "postedAt": "2023-06-09T22:58:47.552Z", "postId": "KRSthwicCTRw9Ayzg", "htmlBody": "<blockquote>\n<p>I'd like to challenge the assumption that EA is the \u201clargest and smartest\u201d expert group on \u201cMight AI lead to extinction?\u201d. I don't think this is true?</p>\n</blockquote>\n<p>You seem to imply that there is another expert group which discusses the question of extinction from AI deeply (and you consider the possibility that the other group is in some sense \"better\" at answering the question)</p>\n<p>Who are these people?</p>\n", "parentCommentId": "2quvunxkJwkpXEafz", "user": {"username": "harfe"}}, {"_id": "nfzpnN2nHPYp3zT3C", "postedAt": "2023-06-09T23:44:50.295Z", "postId": "KRSthwicCTRw9Ayzg", "htmlBody": "<p>I'm not necessarily implying that. EA is <em>not</em> an expert group on AI. There are some experts among us (many of which work at big AI labs, doing valuable research), but most people here discussing it aren't experts. Furthermore, discussing a question 'deeply' does not guarantee that your answer is more accurate (especially if there's more than one 'deep' way to discuss it).</p>\n<p>I would defer to AI experts, or to the world at large, more than I would to just EA alone. But either of those groups carries uncertainty and internal disagreement - and indeed, the best conclusion might just be that the answer is currently uncertain. And that we therefore need (as many experts outside EA have now come to support) to engage many more people and institutions in a collaborative effort to mitigate the <em>possible</em> danger.</p>\n", "parentCommentId": "zsopBhJNhB6u4CtBf", "user": {"username": "Guy Raveh"}}, {"_id": "QKzrzFPrkWRRGSwjp", "postedAt": "2023-06-10T01:02:14.461Z", "postId": "KRSthwicCTRw9Ayzg", "htmlBody": "<p>Your link didn't get pasted properly. Here it is: <a href=\"https://www.lesswrong.com/posts/msJA6B9ZjiiZxT6EZ/lawful-uncertainty\">Lawful Uncertainty</a>.</p>", "parentCommentId": "gaypeTGQ4ZWKv5fMA", "user": {"username": "AllAmericanBreakfast"}}, {"_id": "j9agbjfRgG2aefuF5", "postedAt": "2023-06-10T19:39:08.192Z", "postId": "KRSthwicCTRw9Ayzg", "htmlBody": "<p>I like this post.</p><blockquote><p><i>\"4. Can I assume 'EA-flavored' takes on moral philosophy, such as utilitarianism-flavored stuff, or should I be more 'morally centrist'?\"</i></p></blockquote><p>I think being more \"morally centrist\" should mean caring about what others care about in proportion to how much they care about it. It seems self-centered to be partial to the human view on this. The notion of arriving at your moral view by averaging over other people's moral views strikes me as relying on the wrong reference class.</p><p>Secondly, what do you think moral views have been optimised for in the first place? Do you doubt the social signalling paradigm? You might reasonably realise that your sensors are very noisy, but this seems like a bad reason to throw them out and replace them with something you <i>know</i> wasn't optimised for what you care about. If you wish to a priori judge the plausibility that some moral view is truly altruistic, you could reason about what it likely evolved for.</p><blockquote><p><i>\"I now no longer endorse the epistemics ... that led me to alignment field-building in the first place.\"</i></p></blockquote><p>I get this feeling. But I think the reasons for believing that EA is a fruitfwl library of tools, and for believing that \"AI alignment\" (broadly speaking) is one of the most important topics, are obvious enough that even relatively weak epistemologies can detect the signal. My epistemology has grown a lot since I learned that 1+1=2, yet I don't feel an urgent need to revisit the question. And if I <i>did </i>feel that need, I'd be suspicious it came from a social desire or a private need to either look or be more modest, rather than from impartially reflecting on my options.</p><blockquote><p><i>\"3. Are we deluding ourselves in thinking we are better than most other ideologies that have been mostly wrong throughout history?\"</i></p></blockquote><p>I feel like this is the wrong question. I could think my worldview was the best in the world, or the worst in the world, and it wouldn't necessarily change my overarching policy. The policy in either case is just to <i>improve </i>my worldview, no matter what it is. I could be crazy or insane, but I'll try my best either way.</p>", "parentCommentId": null, "user": {"username": "Elua"}}, {"_id": "5AAvBzSt9Zx3Dps3T", "postedAt": "2023-06-15T15:12:57.567Z", "postId": "KRSthwicCTRw9Ayzg", "htmlBody": "<p>I think a common pitfall from being part of groups which appear to have better epistemics than a lot of others (i.e. EA, LW) is that being part of these groups implicitly gives a feeling of being able to let your [epistemic] guard down (e.g. to defer).&nbsp;</p><p>I've noticed this in myself recently; identifying (whether consciously or not) as being more intelligent/rational than average Joe is actually is a surefire way for me to end up not thinking as clearly as I would have otherwise. (this is obvious in retrospect, but I think it's pretty important to keep in mind)</p><p>I agree with a lot of what you said, and have had similar concerns. &nbsp;I appreciate you writing this and making it public!</p>", "parentCommentId": null, "user": {"username": "harrygietz@gmail.com"}}, {"_id": "TH86tHdTvTRbF4v3x", "postedAt": "2023-06-15T22:22:44.291Z", "postId": "KRSthwicCTRw9Ayzg", "htmlBody": "<p>I think you absolutely should take these questions to heart and not feel compelled to follow the EA consensus on any of them. Especially with alignment, it\u2019s hard to do independent thinking without feeling like a fool, but I say we should all be braver and volunteer to be the fool sometimes to make sure we aren\u2019t in the Emperor\u2019s New Clothes.</p>\n", "parentCommentId": null, "user": {"username": "Holly_Elmore"}}, {"_id": "rRWz9JFJNhrP57Nsk", "postedAt": "2023-06-15T22:27:53.859Z", "postId": "KRSthwicCTRw9Ayzg", "htmlBody": "<p>I think this is true, and I only discovered in the last two months how attached a lot of EA/rat AI Safety people are to going ahead with creating superintelligence\u2014 even though they think the chances of extinction are high\u2014 because they want to reach the Singularity (ever or in their lifetime). I\u2019m not particularly transhumanist and this shocked me, since averting extinction and s-risk is obviously the overwhelming goal in my mind (not to mention the main thing these Singularitarians would talk about to others). It made me wonder of we could have sought regulatory solutions earlier and we didn\u2019t because everyone was so focused on alignment or bust\u2026</p>\n", "parentCommentId": "EwL5LAsNfzR6hT8Km", "user": {"username": "Holly_Elmore"}}, {"_id": "Bx6uF3jgRe8kCxYbG", "postedAt": "2023-06-22T00:22:57.858Z", "postId": "KRSthwicCTRw9Ayzg", "htmlBody": "<blockquote>\n<p>Maybe there\u2019s a huge illusion in EA of \u201csomeone else has probably worked out these big assumptions we are making\u201d. This goes all the way up to the person at Open Phil thinking \u201cHolden has probably worked these out\u201d but actually no one has.</p>\n</blockquote>\n<p>I just wanted to highlight this in particular; I have heard people at Open Phil say things along the lines of \"... but we could be completely wrong about this!\" about large strategic questions. A few examples related to my work:</p>\n<ul>\n<li>Is it net positive to have a dedicated community of EAs working on reducing GCBRs, or would it be better for people to be more fully integrated into the broader biosecurity field?</li>\n<li>If we want to have this community, should we try to increase its size? How quickly?</li>\n<li>Is it good to emphasize concerns about dual-use and information hazards when people are getting started in biosecurity, or does that end up either stymieing them (or worse, inspiring them to produce more harmful ideas)?</li>\n</ul>\n<p>These are big questions, and I have spent dozens (though not hundreds) of hours thinking about them... which has led to me feeling like I have \"working hypotheses\" in response to each. A working hypothesis is not a robust, confident answer based on well-worked-out assumptions. I could be wrong, but I suspect this is also true in many other areas of community building and cause prioritisation, even \"all the way up\".</p>\n", "parentCommentId": null, "user": {"username": "tessa"}}]