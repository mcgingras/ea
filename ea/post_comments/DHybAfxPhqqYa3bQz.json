[{"_id": "estAR8G7fgcjWNtT7", "postedAt": "2023-12-16T01:42:52.945Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<p>I second this request, for similar reasons.&nbsp;</p><p>In particular, I'm interested in accounts of the \"how\" of AI extinction, beyond \"oh, it'll be super duper smart so it'll automatically beat us\". I think this is a pretty bad excuse not to at least give a vague outline of a realistic scenario, and you aren't going to be changing many minds with it.&nbsp;</p>", "parentCommentId": null, "user": {"username": "titotal"}}, {"_id": "WeufwE4XrmacTbmnG", "postedAt": "2023-12-16T13:28:41.810Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<p>Again, this is just one salient example, but: Do you find it unrealistic that a top human level persuasion skills (think interchangeably Mao, Sam Altman and FDR depending on the audience) together with 1 million times ordinary communication bandwidth (i.e. entertaining this amount of conversations) would enable you to take over the world?\nOr would you argue that AI is never going to get to that level?</p>\n", "parentCommentId": "estAR8G7fgcjWNtT7", "user": {"username": "Nick K."}}, {"_id": "hBobxziTcbav8JPQj", "postedAt": "2023-12-16T18:16:46.417Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<p>I think he's partly asking for \"takeover the world\" to be operationalized a bit.&nbsp;</p>", "parentCommentId": "WeufwE4XrmacTbmnG", "user": {"username": "Dr. David Mathers"}}, {"_id": "5p2JTe9kYPZWpd7Zw", "postedAt": "2023-12-16T18:19:04.668Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<p><a href=\"https://www.lesswrong.com/posts/tyE4orCtR8H9eTiEr/stuxnet-not-skynet-humanity-s-disempowerment-by-ai\">https://www.lesswrong.com/posts/tyE4orCtR8H9eTiEr/stuxnet-not-skynet-humanity-s-disempowerment-by-ai</a></p>\n", "parentCommentId": "estAR8G7fgcjWNtT7", "user": {"username": "Wei_Dai"}}, {"_id": "wMFZyYPFhMNpprTHJ", "postedAt": "2023-12-17T06:51:11.482Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<p>Suppose we brought back Neanderthals, genetically engineered them to be smarter and stronger than us, and integrated them into our society. As their numbers grew, it became clear that, if they teamed up against all of humanity, they could beat us in a one-on-one fight.</p><p>In this scenario \u2014 taking the stated facts as a given \u2014 I'd still be pretty skeptical of the suggestion that there is a substantial chance that humanity will go extinct at the hands of the Neanderthals (at least in the near-to-medium term). Yes, the Neanderthals could kill all of us they wanted to, but they likely won't want to for a number of reasons. And my skepticism here goes beyond a belief that they'd be \"aligned\" with us. They may in fact have substantially different values from homo sapiens, on average, and yet I still don't think we'd likely go extinct merely because of that.</p><p>From this perspective, within the context of the scenario described, I think it would be quite reasonable and natural to ask for a specific plausible account that illustrates why humanity would go extinct if they continued on their current course with the Neanderthals. It's reasonable to ask the same thing about AI.</p>", "parentCommentId": "WeufwE4XrmacTbmnG", "user": {"username": "Matthew_Barnett"}}, {"_id": "dJmi6WKqwyRwWW8Ab", "postedAt": "2023-12-18T23:33:25.981Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<p>It's unclear whether I'll end up writing this critique, but if I do, then based on the feedback to the post so far, I'd likely focus on the arguments made in the following posts (which were <a href=\"https://forum.effectivealtruism.org/posts/DHybAfxPhqqYa3bQz/what-is-the-current-most-representative-ea-ai-x-risk?commentId=xsGYvhSx9xJMrC5us\">suggested by Ryan Greenblatt</a>):</p><ul><li><a href=\"https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to\">Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/jd7QinmkdzegerRCm/new-report-scheming-ais-will-ais-fake-alignment-during\">Scheming AIs: Will AIs fake alignment during training in order to get power?</a></li></ul><p>The reason is that these two posts seem closest to presenting a detailed and coherent case for expecting a substantial risk of a catastrophe that researchers still broadly feel comfortable endorsing. Additionally, the DeepMind AGI safety seem <a href=\"https://www.lesswrong.com/posts/GctJD5oCDRxCspEaZ/clarifying-ai-x-risk\">appears to endorse</a> the first post as being the \"closest existing threat model\" to their view.</p><p>I'd prefer not to focus on <a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\">List of Lethalities</a>, even though I disagree with the views expressed within even more strongly than the views in the other listed posts. My guess is that criticism of MIRI threat models, while warranted, is already relatively saturated compared to threat models from more \"mainstream\" researchers, although I'd still prefer more detailed critiques of both.</p><p>If I were to write this critique, I would likely try to cleanly separate my empirical arguments from the normative ones, probably by writing separate posts about them and focusing first on the empirical arguments. That said, I still think both topics are important, since I think many EAs seem to have a faulty background chain of reasoning that flows from their views about human disempowerment risk, concluding that such risks override most other concerns.&nbsp;</p><p>For example, I suspect either a majority or a substantial minority of EAs would agree with the claim that it is OK to let 90% of humans die (e.g. of aging), if that reduced the risk of an AI catastrophe by 1 percentage point. By contrast, I think that type of view seems to naively prioritize a concept of \"the human species\" far above actual human lives in a way that is inconsistent with careful utilitarian reasoning, empirical evidence, or both. And I do not think this logic merely comes down to whether you have person-affecting views or not.</p>", "parentCommentId": null, "user": {"username": "Matthew_Barnett"}}, {"_id": "75qbgDkrKkYDgMiG2", "postedAt": "2023-12-19T15:02:52.304Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<p>I\u2019ve decided to curate this question post because:</p><ul><li>It exemplifies a truth seeking approach to critique. The author,&nbsp;<a href=\"https://forum.effectivealtruism.org/users/matthew_barnett\"><u>Matthew_Barnett</u></a>, is starting off with a feeling that they disagree with EAs about AI risk, but wants to respond to specific arguments. This is obviously a more time intensive approach to critique than simply writing up your own impressions, but it is likely to lead to more precise arguments, which are easier to learn from.&nbsp;&nbsp;</li><li>The comments are particularly helpful. I especially think that&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/DHybAfxPhqqYa3bQz/what-is-the-current-most-representative-ea-ai-x-risk?commentId=qbL6yDXofhG9BoSRC\"><u>this comment</u></a> from&nbsp;<a href=\"https://forum.effectivealtruism.org/users/tom-barnes\"><u>Tom Barnes</u></a> is likely to help a reader who is also asking \u201c<a href=\"https://forum.effectivealtruism.org/posts/DHybAfxPhqqYa3bQz/what-is-the-current-most-representative-ea-ai-x-risk\">What is the current most representative EA AI x-risk argument?</a>\u201d</li><li>I hope curating this post will encourage even more helpful responses from Forum users. AI risk is heterogeneous, and discussions around it are constantly changing, so both newcomers and long interested readers can benefit from answers to this post\u2019s question.&nbsp;</li></ul>", "parentCommentId": null, "user": {"username": "tobytrem"}}, {"_id": "CZ7fze2DnguL2LBCn", "postedAt": "2023-12-20T04:46:52.465Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<blockquote><p>That said, I still think both topics are important, since I think many EAs seem to have a faulty background chain of reasoning that flows from their views about human disempowerment risk, concluding that such risks override most other concerns.&nbsp;</p><p><br>For example, I suspect either a majority or a substantial minority of EAs would agree with the claim that it is OK to let 90% of humans die (e.g. of aging), if that reduced the risk of an AI catastrophe by 1 percentage point. By contrast, I think that type of view seems to naively prioritize a concept of \"the human species\" far above actual human lives in a way that is inconsistent with careful utilitarian reasoning, empirical evidence, or both. And I do not think this logic merely comes down to whether you have person-affecting views or not.</p></blockquote><p>I currently don't think these normative arguments make much of a difference in prioritization or decision making in practice. So, I think this probably isn't that important to argue about.</p><p>Perhaps the most important case in which they would lead to very different decision making is the case of pausing AI (or trying to speed it up). Strong longtermists likely want to pause AI (at the optimal time) until the reduction in p(doom) per year is around the same as the exogenous doom. (This includes the chance of societal disruption which makes the situation worse and then results in doom. For instance, nuclear war induced societal collapse which results in building AI far less safely. Gradual changes in power over time also seem relevant, e.g. china.) I think the go-ahead-point for longtermists probably looks like 0.1% to 0.01% reduction in p(doom) per year of delay for longtermists, but this might depend on how optimistic you are about other aspects of society. Of course, if we could coordinate sufficiently to also eliminate other sources of risk, the go-ahead-point might lower considerably.</p><p>ETA: note that waiting until the reduction in p(doom) per year of delay is 0.1% does <i>not</i> imply that the final p(doom) is 0.1%. It's probably notably higher, maybe over an order of magnitude higher.</p><p>[Low confidence] If we apply the preferences of typical people (but gloomier empirical views about AI), then it seems very relevant that people broadly don't seem care that much about saving the elderly, life extension, or getting strong versions of utopia for themselves before they die. But, they do care a lot about avoiding societal collapse and ruin. And they care some about the continuity of human civilization. So, the go-ahead-point in reduction in doom per year if we use the preferences of normal people might look pretty similar to longtermists (though it's a bit confusing to apply somewhat incoherant preferences). I think it's probably less than a factor of 10 higher, maybe a factor of 3 higher. Also, normal people care about the absolute level of risk: if we couldn't reduce risk below 20%, then it's plausible that the typical preferences of normal people <i>never</i> want to build AI because they care more about not dying in a catastrophe than not dying of old age etc.</p><p>If we instead assume something like utilitarian person-affecting views (let's say only caring about humans for simplicity), but with strongly diminishing returns (e.g. logarithmic) above the quality of live of current americans and with similarly diminishing returns after 500 years of life, then I think you end up do 1% reduction in P(doom) per year of delay as the go-ahead point. This probably leads to pretty similar decisions in most cases.</p><p>(Separately, pure person-affecting views seem super implausible to me. Indifference to the torture of arbitrary number of new future people seems strong from my perspective. If you have asymmetric person-affecting views, then you plausible get dominated by the potential for reducing suffering in the long run.)</p><p>The only views which seem to lead to a pretty different conclusions are views with radically higher discount rates, e.g. pure person-affecting views where you care mostly about the lives of short lived animals or perhaps some views where you care about fulfilling the preferences of current humans (who might have high discount rates on their preferences?). But it's worth noting that these views seem indifferent to the torture of an arbitrary number of future people in a way that feels pretty implausible to me.</p><blockquote><p>By contrast, I think that type of view seems to naively prioritize a concept of \"the human species\" far above actual human lives in a way that is inconsistent with careful utilitarian reasoning, empirical evidence, or both.</p></blockquote><p>I don't think this depends on the concept of \"the human species\". Personally, I care about the overall utilization of resources in the far future (and I imagine many people with a similar perspective agree with me here). For instance, I think literal extinction in the event of AI takeover is unlikely and also not very importantly worse relative to full misaligned AI takeover without extinction. Similarly, I would potentially be happier to turn over the universe to aliens instead of AIs.</p><p>Separately, I think scope-sensitive/linear-returns person-affecting views are likely dominated by the potential for using a high fraction of future resources to simulate huge number of copies of existing people living happy lives. In practice, no one goes here because the actual thing people mean when they say \"person-affecting views\" is more like caring about the <i>preferences</i> of currently existing humans in a diminishing returns way. I think the underlying crux isn't well described as person-affecting vs non-person-affecting and is better described as diminishing returns.</p>", "parentCommentId": "dJmi6WKqwyRwWW8Ab", "user": {"username": "Ryan Greenblatt"}}, {"_id": "xGedHSr6hJatBLbiW", "postedAt": "2023-12-20T19:44:11.027Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<p>I think it's clear from your search results and the answers below that there isn't one representative position.&nbsp;<br><br>But even if there were, I think it's more useful for you to just make your arguments in the way you've outlined, without focusing on one general article to disagree with.&nbsp;<br><br>There's a very specific reason for this: Think about the target audience. These questions are now vital topics being discussed at government level, which impact national and international policies, and corporate strategies at major international companies.&nbsp;<br><br>If your argument is strong, surely you'd want it to be accessible to the people working on these policies, rather than just to a small group of EA people who will recognise all the arguments you're addressing.&nbsp;<br><br>If you want to write in a way that is very accessible, it's better not to just say \"I disagree with Person X on this\" but rather, \"My opinion is Y. There are those, such as person X, who disagree, because they believe that ... (explanation of point of disagreement).\"<br><br>There is the saying that the best way to get a correct answer to any question these days is to post an incorrect answer and let people correct you. In the same spirit, if you outline <i>your</i> positions, people will come back with objections, whether original or citing other work, and eventually you can modify your document to address these. So it becomes a living document representing your latest thinking.&nbsp;<br><br>This is also valuable because AI itself is evolving, and we're learning more about it every day. So even if your argument is accurate based on what we know today, you might want to change something tomorrow.&nbsp;<br><br>(Yes, I realise what I've proposed is a lot more work! But maybe the first version, outlining what you think, is already valuable in and of itself).&nbsp;</p>", "parentCommentId": null, "user": {"username": "Denis "}}, {"_id": "MqbagHiZevhY5FR2W", "postedAt": "2023-12-23T08:46:49.431Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<blockquote>\n<p>I think the go-ahead-point for longtermists probably looks like 0.1% to 0.01% reduction in p(doom) per year for longtermists, but this might depend on how optimistic you are about other aspects of society.</p>\n</blockquote>\n<p>To be clear, my argument would be that the go-ahead-point for longtermists likely looks much higher, like a 10% total risk of catastrophe. Actually that's not exactly how I'd frame it, since what matters more is how much we can reduce the risk of catastrophe by delaying, not just the total risk of a catastrophe. But I'd likely consider a world where we delay AI until the total risk falls below 0.1% to be intolerable from several perspectives.</p>\n<p>I guess one way of putting my point here is that you probably think of \"human disempowerment\" as a terminal state that is astronomically bad, and probably far worse than \"all currently existing humans die\". But I don't really agree with this. Human disempowerment just means that the species <em>homo sapiens</em> is disempowered, and I don't see why we should draw the relevant moral boundary around our <em>species</em>. We can imagine other boundaries like \"our current cultural and moral values\", which I think would drift dramatically over time even if the human species remained.</p>\n<p>I'm just not really attached to the general frame here. I don't identify much with \"human values\" in the abstract as opposed to other salient characteristics of intelligent beings. I think standard EA framing around \"humans\" is simply bad in an important way relevant to these arguments (and this includes most attempts I've seen to broaden the standard arguments to remove references to humans). Even when an EA insists their concern isn't about the human species <em>per se</em> I typically end up disagreeing on some other fundamental point here that seems like roughly the same thing I'm pointing at. Unfortunately, I consistently have trouble conveying this point to people, so I'm not likely to be understood here unless I give a very thorough argument.</p>\n<p>I suspect it's a bit like the arguments vegans have with non-vegans about whether animals are OK to eat because they're \"not human\". There's a conceptual leap from \"I care a lot about humans\" to \"I don't necessarily care a lot about the human species boundary\" that people don't reliably find intuitive except perhaps after a lot of reflection. Most ordinary instances of arguments between vegans and non-vegans are not going to lead to people successfully crossing this conceptual gap. It's just a counterintuitive concept for most people.</p>\n<p>Perhaps as a brief example to help illustrate my point, it seems very plausible to me that I would identify more strongly with a smart behavioral LLM clone of me trained on my personal data compared to how much I'd identify with the human species. This includes imperfections in the behavioral clone arising from failures to perfectly generalize from my data (though excluding extreme cases like the entity not generalizing any significant behavioral properties <em>at all</em>). Even if this clone were not aligned with humanity in the strong sense often meant by EAs, I would not obviously consider it bad to give this behavioral clone power, even at the expense of empowering \"real humans\".</p>\n<p>On top of all of this, I think I disagree with your argument about discount rates, since I think you're ignoring the case for high discount rates based on epistemic uncertainty, rather than pure time preferences.</p>\n", "parentCommentId": "CZ7fze2DnguL2LBCn", "user": {"username": "Matthew_Barnett"}}, {"_id": "tdjZDwCRAdYZ3tKuX", "postedAt": "2023-12-23T17:28:45.306Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<p>I think you misunderstood the points I was making. Sorry for writing an insufficently clear comment.</p><blockquote><p>Actually that's not exactly how I'd frame it, since what matters more is how much we can reduce the risk of catastrophe by delaying, not just the total risk of a catastrophe.</p></blockquote><p>Agreed that's why I wrote \"0.1% to 0.01% <strong>reduction</strong> in p(doom) <strong>per year</strong>\". I wasn't talking about the absolute level of doom here. I edited my comment to say \"0.1% to 0.01% <strong>reduction</strong> in p(doom) <strong>per year of delay</strong>\" which is hopefully more clear. The expected absolute level of doom is probably notably higher than 0.1% to 0.01%.</p><blockquote><p>Human disempowerment just means that the species <i>homo sapiens</i> is disempowered, and I don't see why we should draw the relevant moral boundary around our <i>species</i>.</p></blockquote><p>I don't. That's why I said \"Similarly, I would potentially be happier to turn over the universe to aliens instead of AIs.\"</p><p>Also, note that I think AI take over is unlikely to lead to extinction.</p><p>ETA: I'm pretty low confidence about a bunch of these tricky moral questions.</p><p>I would be reasonably happy (e.g. 50-90% of the value relative to human control) to turn the universe over to aliens. The main reduction in value is due to complicated questions about the likely distribution of values of aliens. (E.g., how likely is that aliens are very sadistic or lack empathy. This is probably still not the exact right question.) I'd also be pretty happy with (e.g.) uplifted dogs (dogs which are made to be as intelligent as humans while keeping the core of \"dog\" whatever that means) so long as the uplifting process was reasonable.</p><p>I think the exact same questions apply to AIs, I just have empirical beliefs that AIs which end up taking over are likely to do predictably worse things with the cosmic endowment (e.g. 10-30% of the value). This doesn't have to be true, I can imagine learning facts about AIs which would make me feel a lot better about AI takeover. Note that conditioning on the AI taking over is important here. I expect to feel systematically better about smart AIs with long horizon goals which are either not quite smart enough to take over or don't take over (for various complicated reasons).&nbsp;</p><p>More generally, I think I basically endorse the views <a href=\"https://www.lesswrong.com/posts/3kN79EuT27trGexsq/when-is-unaligned-ai-morally-valuable\">here</a> (which discusses the questions of when you should cede power etc.).</p><p>Note that in my ideal future it seems really unlikely that we end up spending a non-trivial fraction of future resources running literal humans instead of finding out better stuff to spend computational resources on (e.g. like beings with experiences that a wildly better than our experiences or beings which are vastly cheaper to run).</p><p>(That said, we can and should let all humans live for as long as they want and dedicate some fraction of resources to basic continuity of human civilization insofar as people want this. 1/10^12 of the resources would easily suffice from my perspective, but I'm sympathic to making this more like 1/10^3 or 1/10^6.)</p><blockquote><p>Perhaps as a brief example to help illustrate my point, it seems very plausible to me that I would identify more strongly with a smart behavioral LLM clone of me trained on my personal data compared to how much I'd identify with the human species.</p></blockquote><p>I think \"identify\" is the wrong word from my perspective. The key question is \"what would the smart behavioral clone do with the vast amount of future resources\". That said, I'm somewhat sympathetic to the claim that this behavioral clone would do basically reasonable things with future resources. I also feel reasonably optimistic about pure imitation LLM alignment for somewhat similar reasons.</p><blockquote><p>On top of all of this, I think I disagree with your argument about discount rates, since I think you're ignoring the case for high discount rates based on epistemic uncertainty, rather than pure time preferences.</p></blockquote><p>Am I ignoring this case? I just think we should treat \"what do I terminally value\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhva9wbxqdnm\"><sup><a href=\"#fnhva9wbxqdnm\">[1]</a></sup></span>&nbsp;and \"what is the best route to achieving that\" as most separate questions. So, we should talk about whether \"high discount rates due to epistemic uncertainty\" is a good reasoning heuristic for achieving my terminal values separately from what my terminal values are.</p><p>Separately, I think a high per year discount rate due to epistemic uncertainty seems pretty clearly wrong. I'm pretty confiden that I can influence, to at least a small degree (e.g. I can affect the probability by &gt;10^-10, probably much greater), whether or not the moral equivalent of 10^30 people are tortured in 10^6 years. It seems like a very bad idea from my perspective to put literally zero weight on this due to 1% annual discount rates.</p><p>For less specific things like \"does a civilization descended from and basically endorsed by humans exist in 10^6 years\", I think I have considerable influence. E.g., I can affect the probability by &gt;10^-6 (in expectation). (This influence is distinct from the question of how valuable this is to influence, but we were talking about epistemic uncertainty here.)</p><p>My guess is that we end up with basically a moderate fixed discount over very long run future influence due to uncertainty over how the future will go, but this is more like 10% or 1% than 10^-30. And, because the long run future still dominates in my views, this just multiplies though all calculations and ends up not mattering much for decision making. (I think acausal trade considerations implicitly mean that I would be willing to tradeoff long run considerations in favor of things which look good as weighted by current power structures (e.g. helping homeless children in the US) if I had a 1,000x-10,000x opportunity to do this. E.g., if I could stop 10,000 US children from being homeless with a day of work and couldn't do direct trade, I would still do this.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhva9wbxqdnm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhva9wbxqdnm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>More precisely, what would my CEV (Coherant Extrapolated Volition) want and how do I handle uncertainty about what my CEV would want?</p></div></li></ol>", "parentCommentId": "MqbagHiZevhY5FR2W", "user": {"username": "Ryan Greenblatt"}}, {"_id": "2vqWgECmTdwcDnjov", "postedAt": "2023-12-23T17:31:40.469Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<p>Note that the comment you're replying to says \"take over the world\" not extinction.</p><p>I think extinction is unlikely conditional on takeover (and takeover seems reasonably likely).</p><p>Neanderthal take over doesn't seem very bad from my perspective, so probably I'm basically fine with that. (Particularly if we ensure that some basic ideas are floating around in Neanderthal culture like \"maybe you should be really thoughtful and careful with what you do with the cosmic endowment\".)</p>", "parentCommentId": "wMFZyYPFhMNpprTHJ", "user": {"username": "Ryan Greenblatt"}}, {"_id": "roGsY9heKK2vdGEk9", "postedAt": "2023-12-23T17:42:02.175Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<p>Also, another important clarification is that my views are probably quite different from that of the median EA who identifies as longtermist. So I'd be careful not to pattern match me.<br><br>(And I prefer not to identify with movements, so I'd say that I'm not an EA.)</p>", "parentCommentId": "MqbagHiZevhY5FR2W", "user": {"username": "Ryan Greenblatt"}}, {"_id": "RebxxWvtW2LqqSg9C", "postedAt": "2023-12-23T19:57:06.919Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<blockquote>\n<p>Note that the comment you're replying to says \"take over the world\" not extinction.</p>\n</blockquote>\n<p>I agree, but the original comment said \"In particular, I'm interested in accounts of the \"how\" of AI extinction\".</p>\n", "parentCommentId": "2vqWgECmTdwcDnjov", "user": {"username": "Matthew_Barnett"}}, {"_id": "APARdDp2bXDHdNzgB", "postedAt": "2023-12-23T20:13:17.350Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<blockquote><p>Agreed that's why I wrote \"0.1% to 0.01% <strong>reduction</strong> in p(doom) <strong>per year</strong>\". I wasn't talking about the absolute level of doom here. I edited my comment to say \"0.1% to 0.01% <strong>reduction</strong> in p(doom) <strong>per year of delay</strong>\" which is hopefully more clear</p></blockquote><p>Ah, sorry. I indeed interpreted you as saying that we would reduce p(doom) to 0.01-0.1% per year, rather than saying that each year of delay reduces p(doom) by that amount. I think that view is more reasonable, but I'd still likely put the go-ahead-number higher.</p><blockquote><p>That's why I said \"Similarly, I would potentially be happier to turn over the universe to aliens instead of AIs.\"</p></blockquote><p>Apologies again for misinterpreting. I didn't know how much weight to put on the word \"potentially\" in your comment. Although note that I said, \"Even when an EA insists their concern isn't about the human species <i>per se</i> I typically end up disagreeing on some other fundamental point here that seems like roughly the same thing I'm pointing at.\" I don't think the problem is literally that EAs are anthropocentric, but I think they often have anthropocentric intuitions that influence these estimates.&nbsp;</p><p>Maybe a more accurate summary is that people have a bias towards \"evolved\" or \"biological\" beings, which I think might explain why you'd be a little happier to hand over the universe to aliens, or dogs, but not AIs.</p><blockquote><p>I would be reasonably happy (e.g. 50-90% of the value relative to human control) to turn the universe over to aliens. [...]</p><p>I think the exact same questions apply to AIs, I just have empirical beliefs that AIs which end up taking over are likely to do predictably worse things with the cosmic endowment (e.g. 10-30% of the value).</p></blockquote><p>I guess I mostly think that's a pretty bizarre view, with some obvious reasons for doubt, and I don't know what would be driving it. The process through which aliens would get values like ours seems much less robust than the process through which AIs gets our values. AIs are trained on our data, and humans will presumably care a lot about aligning them (at least at first).&nbsp;</p><p>From my perspective this is a bit like saying you'd prefer aliens to take over the universe rather than handing control over to our genetically engineered human descendants. I'd be very skeptical of that view too for some basic reasons.</p><p>Overall, upon learning your view here, I don't think I'd necessarily diagnose you as having the intuitions I alluded to in my original comment, but I think there's likely something underneath your views that I would strongly disagree with, if I understood your views further. I find it highly unlikely that AGIs will be even more \"alien\" from the perspective of our values than literal aliens (especially if we're talking about aliens who themselves build their own AIs, genetically engineer themselves, and so on).</p>", "parentCommentId": "tdjZDwCRAdYZ3tKuX", "user": {"username": "Matthew_Barnett"}}, {"_id": "8tEc25CsJiC6pGDd7", "postedAt": "2023-12-23T21:19:14.785Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<blockquote><p>The process through which aliens would get values like ours seems much less robust than the process through which AIs gets our values. AIs are trained on our data, and humans will presumably care a lot about aligning them (at least at first).&nbsp;</p></blockquote><p>Note that I'm conditioning on AIs successfully taking over which is strong evidence against human success at creating desirable (edit: from the perspective of the creators) AIs.</p><blockquote><p>if I understood your views further. I find it highly unlikely that AGIs will be even more \"alien\" from the perspective of our values than literal aliens</p></blockquote><p>For an intuition pump, consider future AIs which are trained for the equivalent of 100 million years of next-token-prediction<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref84f5ruclxm2\"><sup><a href=\"#fn84f5ruclxm2\">[1]</a></sup></span>&nbsp;on low quality web text and generated data and then aggressively selected with outcomes based feedback. This outcomes based feedback results in selecting the AIs for carefully tricking their human overseers in a variety of cases and generally ruthlessly pursuing reward.</p><p>This scenario is somewhat worse than what I expect in the median world. But in practice I expect that it's at least systematically possible to change the training setup to achieve in predictably better AI motivation and values. Beyond trying to influence AI motivations with crude tools, it seems even better to have humans retain control, use AIs to do a huge amount of R&amp;D (or philosophy work), and then decide what should actually happen with access to more options.</p><p>Another way to put this is that I feel notably better about the decisions making of current power structures in the western world and in AIs labs than I feel about going with AI motivations which likely result from training.</p><p>More generally, if you are the sole person in control, it seems strictly better from your perspective to carefully reflect on who/what you want to defer to rather than doing this somewhat arbitrarily (this still leaves open the question of how bad arbitrarily defering is).</p><blockquote><p>From my perspective this is a bit like saying you'd prefer aliens to take over the universe rather than handing control over to our genetically engineered human descendants. I'd be very skeptical of that view too for some basic reasons.</p></blockquote><p>I'm pretty happy with slow and steady genetic engineering as a handover process, but I would prefer even slower and more deliberate than this. E.g., existing humans thinking carefully for as long as seems to yield returns about what beings we should defer to and then defer to those slightly smart beings which think for a long time and defer to other beings, etc, etc.</p><blockquote><p>I guess I mostly think that's a pretty bizarre view, with some obvious reasons for doubt, and I don't know what would be driving it.</p></blockquote><p>Part of my view on aliens or dogs is driven from the principle of \"aliens/dogs are in a somewhat similar position to us, so we should be fine with swapping\" (roughly speaking) and \"the part of my values which seem most dependent on random emprical contingencies about evolved life I put less weight on\". These intuitions transfer somewhat less to the AI case.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn84f5ruclxm2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref84f5ruclxm2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Current AIs are trained on perhaps 10-100 trillion tokens and if we think 1 token the equivalent of 1 second then (100*10^12)/(60*60*24*365) = 3 milion years.</p></div></li></ol>", "parentCommentId": "APARdDp2bXDHdNzgB", "user": {"username": "Ryan Greenblatt"}}, {"_id": "KfEpCEj5M2uguBcLf", "postedAt": "2023-12-23T21:28:25.019Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<p>TBC, it's plausible that in the future I'll think that \"marginally influencing AIs to have more sensible values\" is more leveraged than \"avoiding AI take over and hoping that humans (and our chosen successors) do something sensible\". I'm partially defering to others on the view that AI takeover is the best angle of attack, perhaps I should examine further.<br><br>(Of course, it could be that from a longtermist perspective other stuff is even better than avoiding AI takeover or altering AI values. E.g. maybe one of conflict avoidance, better decision theory, or better human institutions for post singularity is even better.)<br><br>I certainly wish the question of how much worse/better AI takeover is relative to human control was &nbsp;investigated more effectively. It seems notable to me how important this question is from a longtermist perspective and how little investigation it has received.</p><p>(I've spent maybe 1 person day thinking about it and I think probably less than 3 FTE years have been put into this by people who I'd be interested in defering to.)</p>", "parentCommentId": "APARdDp2bXDHdNzgB", "user": {"username": "Ryan Greenblatt"}}, {"_id": "DtyPxKuvhLYXi2MHN", "postedAt": "2023-12-23T21:50:26.332Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<blockquote><p>Note that I'm conditioning on AIs successfully taking over which is strong evidence against human success at creating desirable AIs.</p></blockquote><p>I don't think it's strong evidence, for what it's worth. I'm also not sure what \"AI takeover\" means, and I think existing definitions are very ambiguous (would we say Europe took over the world during the age of imperialism? Are smart people currently in control of the world? Have politicians, as a class, taken over the world?). Depending on the definition, I tend to think that AI takeover is either ~inevitable and not inherently bad, or bad but not particularly likely.</p><blockquote><p>This outcomes based feedback results in selecting the AIs for carefully tricking their human overseers in a variety of cases and generally ruthlessly pursuing reward.</p></blockquote><p>Would aliens not also be incentivized to trick us or others? What about other humans? In my opinion, basically all the arguments about AI deception from gradient descent apply in some form to other methods of selecting minds, including evolution by natural selection, cultural learning, and in-lifetime learning. Humans frequently lie to or mislead each other about our motives. For example, if you ask a human what they'd do if they became world dictator, I suspect you'd often get a different answer than the one they'd actually chose if given that power. I think this is essentially the same epistemic position we might occupy with AI.</p><p>Also, for a bunch of reasons that I don't currently feel like elaborating on, I expect humans to anticipate, test for, and circumvent the most egregious forms of AI deception in practice. The most important point here is that I'm not convinced that incentives for deception are much worse for AIs than for other actors in different training regimes (including humans, uplifted dogs, and aliens).</p>", "parentCommentId": "8tEc25CsJiC6pGDd7", "user": {"username": "Matthew_Barnett"}}, {"_id": "juhCjJRgA5gADM5mf", "postedAt": "2023-12-24T00:33:10.118Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<blockquote><p>Also, for a bunch of reasons that I don't currently feel like elaborating on, I expect humans to anticipate, test for, and circumvent the most egregious forms of AI deception in practice. The most important point here is that I'm not convinced that incentives for deception are much worse for AIs than for other actors in different training regimes (including humans, uplifted dogs, and aliens).</p></blockquote><p>I don't strongly disagree with either of these claims, but this isn't exactly where my crux lies.</p><p>The key thing is \"generally ruthlessly pursuing reward\".</p><p>I'm checking out of this conversation though.</p>", "parentCommentId": "DtyPxKuvhLYXi2MHN", "user": {"username": "Ryan Greenblatt"}}, {"_id": "ceQbwS5atxWrEyGne", "postedAt": "2023-12-24T00:35:41.842Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<blockquote><p>I don't think it's strong evidence, for what it's worth. I'm also not sure what \"AI takeover\" means, and I think existing definitions are very ambiguous (would we say Europe took over the world during the age of imperialism? Are smart people currently in control of the world? Have politicians, as a class, taken over the world?). Depending on the definition, I tend to think that AI takeover is either ~inevitable and not inherently bad, or bad but not particularly likely.</p></blockquote><p>By \"AI takeover\", I mean <i>autonomous</i> AI coup/revolution. E.g., violating the law and/or subverting the normal mechanisms of power transfer. (Somewhat unclear exactly what should count tbc, but there are some central examples.) By this definition, it basically always involves subverting the intentions of the creators of the AI, though may not involve violent conflict.</p><p>I don't think this is super likely, perhaps 25% chance.</p>", "parentCommentId": "DtyPxKuvhLYXi2MHN", "user": {"username": "Ryan Greenblatt"}}, {"_id": "HJuNGrDLtgfwsv7fg", "postedAt": "2023-12-24T03:03:27.038Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<p>If you're interested in diving into \"how bad/good is it to cede the universe to AIs\", I strongly think it's worth reading and responding to \"<a href=\"https://www.lesswrong.com/posts/3kN79EuT27trGexsq/when-is-unaligned-ai-morally-valuable\">When is unaligned AI morally valuable?</a>\" which is the current state of the art on the topic (same thing I linked above). I now regret rehashing a bunch of these arguments which I think are mostly made better here. In particular, I think the case for \"AIs created in the default way might have low moral value is reasonably well argued for <a href=\"https://www.lesswrong.com/posts/3kN79EuT27trGexsq/when-is-unaligned-ai-morally-valuable#Intuitions_and_an_analogy\">here</a>:</p><blockquote><p>Many people have a strong intuition that we should be happy for our AI descendants, whatever they choose to do. They grant the <i>possibility</i> of pathological preferences like paperclip-maximization, and agree that turning over the universe to a paperclip-maximizer would be a problem, but don\u2019t believe it\u2019s realistic for an AI to have such uninteresting preferences.</p><p>I disagree. I think this intuition comes from analogizing AI to the children we raise, but that it would be just as accurate to compare AI to the corporations we create. Optimists imagine our automated children spreading throughout the universe and doing their weird-AI-analog of art; but it\u2019s just as realistic to imagine automated PepsiCo spreading throughout the universe and doing its weird-AI-analog of maximizing profit.</p><p>It might be the case that PepsiCo maximizing profit (or some inscrutable lost-purpose analog of profit) is intrinsically morally valuable. But it\u2019s certainly not obvious.</p><p>Or it might be the case that we would never produce an AI like a corporation in order to do useful work. But looking at the world around us today that\u2019s <i>certainly</i> not obvious.</p><p>Neither of those analogies is remotely accurate. Whether we should be happy about AI \u201cflourishing\u201d is a really complicated question about AI and about morality, and we can\u2019t resolve it with a one-line political slogan or crude analogy.</p></blockquote><p>(And the same recommendation for onlookers.)</p>", "parentCommentId": "APARdDp2bXDHdNzgB", "user": {"username": "Ryan Greenblatt"}}, {"_id": "fLfPgiFKDEYevkTbK", "postedAt": "2023-12-24T03:52:09.266Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<blockquote><p>I now regret rehashing a bunch of these arguments which I think are mostly made better here.</p></blockquote><p>It's fine if you don't want to continue this discussion. I can sympathize if you find it tedious. That said, I don't really see why you'd appeal to that post in this context (FWIW, I read the post at the time it came out, and just re-read it). I interpret Paul Christiano to mainly be making arguments in the direction of \"unaligned AIs might be morally valuable, even if we'd prefer aligned AI\" which is what<i> I</i> thought I was broadly arguing for, in contradistinction to your position. I thought you were saying something closer to the opposite of what Paul was arguing for (although you also made several separate points, and I don't mean to oversimplify your position).</p><p>(But I agree with the quoted part of his post that we shouldn't be happy with AIs doing \"whatever they choose to do\". I don't think I'm perfectly happy with unaligned AI. I'd prefer we try to align AIs, just as Paul Christiano says too.)</p>", "parentCommentId": "HJuNGrDLtgfwsv7fg", "user": {"username": "Matthew_Barnett"}}, {"_id": "CGvcxarvX8gRAyq97", "postedAt": "2023-12-24T04:10:52.788Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<blockquote><p>The key thing is \"generally ruthlessly pursuing reward\".</p></blockquote><p>It depends heavily on what you mean by this, but I'm kinda skeptical of the strong version of ruthless reward seekers, for similar reasons given in <a href=\"https://www.lesswrong.com/posts/TWorNr22hhYegE4RT/models-don-t-get-reward\">this post</a>. I think AIs by default might be ruthless in some other senses -- since we'll be applying a lot of selection pressure to them to get good behavior -- but I'm not really sure how how much weight to put on the fact that AIs will be \"ruthless\" when evaluating how good they are at being our successors. It's not clear how that affects my evaluation of how much I'd be OK handing the universe over to them, and my guess is the answer is \"not much\" (absent more details).</p><p>Humans seem pretty ruthless in certain respects too, e.g. about survival, or increasing their social status. I'd expect aliens, and potentially uplifted dogs to be ruthless too along some axes depending on how we uplifted them.</p><blockquote><p>I'm checking out of this conversation though.</p></blockquote><p>Alright, that's fine.</p>", "parentCommentId": "juhCjJRgA5gADM5mf", "user": {"username": "Matthew_Barnett"}}, {"_id": "MAyTtiHhdCjisgsDD", "postedAt": "2023-12-24T04:34:06.123Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<p>Huh, no I almost entirely agree with this post as I noted in my prior comment. I cited this much earlier: \"More generally, I think I basically endorse the views here (which discusses the questions of when you should cede power etc.).\"</p>\n<p>I do think unaligned ai would be morally valuable (I said in an earlier comment unaligned ai which take over might capture 10-30% of the value. That's a lot of value.)</p>\n<blockquote>\n<p>I don't think I'm perfectly happy with unaligned AI. I'd prefer we try to align AIs, just as Paul Christiano says too.</p>\n</blockquote>\n<p>I think we've probably been talking past each other. I thought the whole argument here was \"how much value do we lose if (presumably misaligned) AI takes over\" and you were arguing for \"not much, caring about this seems like overly fixating on humanity\" and I was arguing \"(presumably misaligned) ais which take over probably results in substantially less value\". This now seems incorrect and we perhaps only have minor quantitative disagreements?</p>\n<p>I think it probably would have helped if you were more quantitative here. Exactly how much of the value?</p>\n", "parentCommentId": "fLfPgiFKDEYevkTbK", "user": {"username": "Ryan Greenblatt"}}, {"_id": "Cm4BwFN8CwhYgEzFi", "postedAt": "2023-12-24T06:35:10.062Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<blockquote>\n<p>I thought the whole argument here was \"how much value do we lose if (presumably misaligned) AI takes over\"</p>\n</blockquote>\n<p>I think the key question here is: compared to what? My position is that we lose a lot of potential value both from delaying AI and from having unaligned AI, but it's not a crazy high reduction in either case. In other words they're pretty comparable in terms of lost value.</p>\n<p>Ranking the options in rough order (taking up your offer to be quantitative):</p>\n<ul>\n<li>\n<p>Aligned AIs built tomorrow: 100% of the value from my perspective</p>\n</li>\n<li>\n<p>Aligned AIs built in 100 years: 50% of the value</p>\n</li>\n<li>\n<p>Unaligned AIs built tomorrow: 15% of the value</p>\n</li>\n<li>\n<p>Unaligned AIs built in 100 years: 25% of the value</p>\n</li>\n</ul>\n<p>Note that I haven't thought about these exact numbers much.</p>\n", "parentCommentId": "MAyTtiHhdCjisgsDD", "user": {"username": "Matthew_Barnett"}}, {"_id": "7yhaomPbTbuyscGyv", "postedAt": "2023-12-24T07:15:04.399Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<blockquote>\n<p>Aligned AIs built in 100 years: 50% of the value</p>\n</blockquote>\n<p>What drives this huge drop? Naive utility would be very close to 100%. (Do you mean \"aligned ais built in 100y if humanity still exists by that point\", which includes extinction risk before 2123?)</p>\n", "parentCommentId": "Cm4BwFN8CwhYgEzFi", "user": {"username": "frib"}}, {"_id": "Tfe3LWEvXhLjfDCoY", "postedAt": "2023-12-24T09:57:04.524Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": "<p>I attempted to explain the basic intuitions behind my judgement in this thread. Unfortunately it seems I did a poor job. For the full explanation you'll have to wait until I write a post, if I ever get around to doing that.</p>\n<p>The simple, short, and imprecise explanation is: I don't really value humanity as a species as much as I value the people who currently exist, (something like) our current communities and relationships, our present values, and the existence of sentient and sapient life living positive experiences. Much of this will go away after 100 years.</p>\n", "parentCommentId": "7yhaomPbTbuyscGyv", "user": {"username": "Matthew_Barnett"}}, {"_id": "djaujJ9mJQqJfHs6G", "postedAt": "2023-12-20T20:08:33.943Z", "postId": "DHybAfxPhqqYa3bQz", "htmlBody": null, "parentCommentId": "xGedHSr6hJatBLbiW", "user": null}]