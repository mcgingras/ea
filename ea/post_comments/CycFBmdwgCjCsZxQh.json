[{"_id": "2ZkeJGKjLc8w8EwMj", "postedAt": "2022-12-24T16:25:12.090Z", "postId": "CycFBmdwgCjCsZxQh", "htmlBody": "<p>Thank you for this. I would only caution that we should be vigilant when it comes to poor reasoning disguised as \u201cintuition\u201d. Most EAs would reject out of hand someone\u2019s intuition that people of a certain nationality or ethnic group or gender matter less, even if they claim that it\u2019s about suffering intensity as opposed to xenophobia, racism or sexism. Yet intuitions about the moral weight of nonhuman animals that often seem to have been plucked out of thin air, without any justification provided, seem to get a pass, especially when they\u2019re from \u201chigh-status\u201d individuals.</p>\n", "parentCommentId": null, "user": {"username": "JBentham"}}, {"_id": "gpQvdxsXpSxAknGDT", "postedAt": "2022-12-24T16:32:26.703Z", "postId": "CycFBmdwgCjCsZxQh", "htmlBody": "<p>It's a pleasure to see you here, Sir.</p>", "parentCommentId": "2ZkeJGKjLc8w8EwMj", "user": {"username": "Pablo_Stafforini"}}, {"_id": "CkwevBpxuHAcZTZ3G", "postedAt": "2022-12-24T18:05:14.586Z", "postId": "CycFBmdwgCjCsZxQh", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/topics/sequence-vs-cluster-thinking\">Cluster thinking</a> could provide value. Not quite the same as moral uncertainty, in that cluster thinking has broader applicability, but the same type of \"weighted\" judgement. I disagree with moral uncertainty as a personal philosophy,given the role I suspect that self-servingness plays in personal moral judgements. However, cluster thinking applied in limited decision-making contexts appeals to me.</p>\n<p>A neglected areas of exploration in EA is selfishness, and self-servingness along with that. Both influence worldview, sometimes on the fly, and are not necessarily vulnerable to introspection. I suppose a controversy that could start early is whether all altruistic behavior has intended selfish benefits in addition to altruistic benefits. Solving the riddle of self-servingness would be a win-win-win-win-win-win .</p>\n<p>Self-servingness has signs that include :</p>\n<ul>\n<li>soldier mindset</li>\n<li>missing (or <a href=\"https://forum.effectivealtruism.org/posts/EK3LoxzHXywZprRnu/two-guts#comments\">misrepresented</a>) premises</li>\n<li>bad argumentation (analogical, inductive, deductive).</li>\n</ul>\n<p>but without prior knowledge useful to identify those signs, I have not gotten any further than <a href=\"https://forum.effectivealtruism.org/posts/pPvvF2hdbCAXSzGjc/noah-scales-s-shortform?commentId=cyLMpegP328n95nMj\">detecting self-servingness with simple heuristics</a> (for example, as present when defending one's vices).</p>\n", "parentCommentId": null, "user": {"username": "Noah Scales"}}, {"_id": "6spTybDc3m7Lj5aCn", "postedAt": "2022-12-24T18:35:06.549Z", "postId": "CycFBmdwgCjCsZxQh", "htmlBody": "<p>EDIT: Maybe what you mean is that differences in empirical beliefs are not as important, perhaps because we don't disagree so much on empirical beliefs in a way that significantly influences prioritization? That seems plausible to me.</p><p>&nbsp;</p><p>Thanks for writing this! I agree that moral (or normative) intuitions are pretty decisive, although I'd say empirical features of the world and our beliefs about them are similarly important, so I'm not sure I agree with \"mostly moral intuition\". For example, if chickens weren't farmed so much (in absolute and relative numbers), we wouldn't be prioritizing chicken welfare, and, more generally, if people didn't farm animals in large numbers, we wouldn't prioritize farm animal welfare. If AGI seemed impossible or much farther off, or other extinction risks seemed more likely, we would give less weight to AI risks relative to other extinction risks. Between the very broad \"direct\" EA causes (say global health and poverty, animal welfare, x-risks (or extinction risks and s-risks, separately)), what an individual EA prioritizes seems to be mostly based on moral intuition, but that we're prioritizing these specific causes at all (rather than say homelessness or climate change), and the prioritization of specific interventions or sub-causes depends a lot on empirical beliefs.</p><p>&nbsp;</p><p>Also, a nitpick on person-affecting views:</p><blockquote><p>Rejecting person-affecting views, and the principle of neutrality, is required for (strong) <a href=\"https://longtermism.com/\">longtermism</a>.</p></blockquote><p>The principle of neutrality is compatible with concern for future people and longtermism (including strong longtermism) if you reject the independence of irrelevant alternatives or transitivity, which most person-affecting views do (although not all are concerned with future people). You can hold <i>wide</i> person-affecting views, so that it's better for a better off person to be born than a worse off person, and we should ensure better off people come to exist than people who would be worse off, even if we should be indifferent to whether any (or how many) such additional people exist at all. There's also the possibility that people alive today could live for millions of years.</p><p><i>Asymmetric</i> person-affecting views, like Meacham's to which you link, reject neutrality, because bad lives should be prevented, and are also compatible with (strong) longtermism. They might recommend ensuring future moral patients are as well off as possible or reducing s-risks. See also <a href=\"https://globalprioritiesinstitute.org/teruji-thomas-the-asymmetry-uncertainty-and-the-long-term/\">Thomas, 2019</a> for asymmetric views that allow offsetting bad lives with good lives, but not outweighing bad lives with good lives, and section 6 for practical implications.</p><p>&nbsp;</p><p>Finally, with respect to the procreation asymmetry in particular, I think Meacham's approach offers some useful insights into how to build person-affecting views, but I think he doesn't really offer much defense of the asymmetry (or harm-minimization) itself and instead basically takes it for granted, if I recall correctly. I would recommend actualist accounts and <a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/phpe.12139\">Frick's account</a>. Some links and discussion <a href=\"https://forum.effectivealtruism.org/posts/vZ4kB8gpvkfHLfz8d/critique-of-macaskill-s-is-it-good-to-make-happy-people?commentId=ygrcxjBPKSmttnRXR\">in my comment here</a>.</p>", "parentCommentId": null, "user": {"username": "MichaelStJules"}}, {"_id": "phnLxWsuPeu5Hvp9E", "postedAt": "2022-12-24T20:03:59.770Z", "postId": "CycFBmdwgCjCsZxQh", "htmlBody": "<p>Thanks for the thoughtful reply Michael! &nbsp;I think I was thinking more along what you said in your edit: empirical beliefs are very important, but we (or EAs at least) don't really disagree on them e.g. objectively there are billions of chickens killed for food each year. Furthermore, we can actually resolve empirical disagreements with research etc such that if we do hold differing empirical views, we can find out who is actually right (or closer to the truth). On the other hand, with moral questions, it feels like you can't actually resolve a lot of these in any meaningful way to find one correct answer. As a result, roughly holding empirical beliefs constant, moral beliefs seem to be a crux that decides what you prioritise.&nbsp;</p><p>I also agree with your point that differing moral intuitions probably lead to different views on worldview prioritisation (e.g. animal welfare vs global poverty) rather than intervention prioritisation (although this is also true for things like StrongMinds vs AMF).&nbsp;</p><p>Also appreciate the correction on person-affecting views (I feel like I tried to read a bunch of stuff on the Forum about this, including a lot from you, but still get a bit muddled up!). Will read some of the links you sent and amend the main post.&nbsp;</p>", "parentCommentId": "6spTybDc3m7Lj5aCn", "user": {"username": "JamesOz"}}, {"_id": "Hi2zgDbopkchRwSbd", "postedAt": "2022-12-26T05:35:35.283Z", "postId": "CycFBmdwgCjCsZxQh", "htmlBody": "<p>Over time, I've come to see the top questions as:</p>\n<ol>\n<li>Is there such a thing as moral/philosophical progress? If yes, is there anything we can feasibly do to ensure continued moral/philosophical progress and maximize the chances that human(-descended) civilization can eventually reach moral/philosophical maturity where all of the major problems that currently confuse us are correctly solved?</li>\n<li>Is there anything we might do prior to reaching moral/philosophical maturity that would constitute a non-negligible amount of irreparable harm? (For example, perhaps creating an astronomical amount of digital/simulated suffering would qualify.) How can we minimize the chances of this?</li>\n</ol>\n<p>In one of your charts you jokingly ask, \"What even is philosophy?\" but I'm genuinely confused why this line of thinking doesn't lead a lot more people to view metaphilosophy as a top priority, either in the technical sense of solving the problems of <a href=\"https://www.greaterwrong.com/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy\">what philosophy is</a> and what constitutes philosophical progress, or in the sociopolitical sense of how best to structure society for making philosophical progress. (I can't seem to find anyone else who often talks about this, even among the many philosophers in EA.)</p>\n", "parentCommentId": null, "user": {"username": "Wei_Dai"}}, {"_id": "nAFGnDojnNrbqkvnb", "postedAt": "2022-12-26T13:54:10.768Z", "postId": "CycFBmdwgCjCsZxQh", "htmlBody": "<p>Thank you for this thoughtful post! I am just about to start a PhD in philosophy on the psychology and metaethics of well-being, so I am fairly familiar with the research literature on this topic in particular. I totally agree with you that foundational issues should be investigated more deeply in EA circles. To me, it is baffling that there is so little discussion of meta-ethics and the grounds for central propositions to EA.&nbsp;</p><p>You are right that many philosophers, including some who write about method, think that ethics is about weighing intuitions in reflective equilibrium. However, I think that it is seriously misleading to state this as if it is an undisputed truth (like you do with the quote from Michael Plant). In the 2020 <a href=\"https://philarchive.org/archive/BOUPOP-3\">Philpapers survey</a>, I think about half of respondents thought &nbsp;intuitions-based philosophy was the most important. However, the most cited authors that do contemporary work in philosophical methodology eg. <a href=\"https://global.oup.com/academic/product/philosophical-method-a-very-short-introduction-9780198810001?cc=no&amp;lang=en&amp;\">Timothy Williamson</a> and <a href=\"https://academic.oup.com/book/8898\">Herman Cappelen</a>, dont think intuitions plays important roles at all, at least not if intuitions are thought to be distinct from beliefs.&nbsp;</p><p>I think that all the ideas you mention concerning how to move forward look very promising. I would just &nbsp;add \"explore meta-ethics\", and in particular \"non-intuitionism in ethics\". I think that there are several active research programs that might help us determine what matters, without relying on intuitions in a non-critical way. I would especially recommend <a href=\"https://www.philosophy.ox.ac.uk/john-locke-lectures#collapse476186\">Peter Railton's project</a>. I have also written about this and I am going to write about it in my PhD project. I would be happy to talk about it anytime!&nbsp;</p>", "parentCommentId": null, "user": {"username": "paal-fredrik-skjorten-kvarberg"}}, {"_id": "XsGCK2XgMtxYRc9cA", "postedAt": "2022-12-29T03:52:54.664Z", "postId": "CycFBmdwgCjCsZxQh", "htmlBody": "<p>&nbsp;Thanks for the post!</p><p>Adding on to <a href=\"https://forum.effectivealtruism.org/posts/CycFBmdwgCjCsZxQh/what-you-prioritise-is-mostly-moral-intuition?commentId=6spTybDc3m7Lj5aCn\">what Michael said</a> (and disagreeing a bit): when it comes to what people prioritize, I think this post might implicitly underrate the importance of disagreeing beliefs about the reality of the world relative to the importance of moral disagreements. (I think Carl Shulman makes a related point <a href=\"https://80000hours.org/podcast/episodes/carl-shulman-common-sense-case-existential-risks/#suspicious-convergence-around-x-risk-reduction-024931\">in this podcast</a> and possibly elsewhere.) In particular, I think a good number of people see empirical beliefs as some of their key <a href=\"https://forum.effectivealtruism.org/topics/crux\">cruxes</a> for working on what they do (<a href=\"https://forum.effectivealtruism.org/posts/Ayu5im98u8FeMWoBZ/my-personal-cruxes-for-working-on-ai-safety\">e.g.</a>), and others might disagree with those beliefs. &nbsp;</p><p>Potentially a more minor point: I also think people use some terminology in this space in different ways, which might cause additional confusion. For instance, you define \"worldview research\" as \"Research into what or who matters most in the world,\" and write \"Disagreements here are mostly based on moral intuitions, and quite hard to resolve\" (in the chart). This usage is fairly common (<a href=\"https://forum.effectivealtruism.org/search?query=worldview%20diversification\">e.g.</a>). But in other places, \"worldview\" might be used to describe empirical reality (<a href=\"https://forum.effectivealtruism.org/posts/vcjLwqLDqNEmvewHY/tips-for-conducting-worldview-investigations\">e.g.</a>).&nbsp;</p>", "parentCommentId": null, "user": {"username": "Lizka"}}]