[{"_id": "uLBcuFwwz8MAQxcuE", "postedAt": "2022-12-16T22:36:47.362Z", "postId": "cGM86RhxMdfDYbQnn", "htmlBody": "<p>Congrats on posting your draft!</p><p>Ultimately I agree with: \"x-risk is high\", \"the long-term is overwhelmingly important\", and \"we should use reason and evidence to decide what is most important and then do it\", so what I choose to emphasize to people in my messaging is a strategic consideration about what I think will have the best effects, including convincingness. (I think you agree.)</p><p>One reason why the EA community seems to spend so much energy on the EA-principle thing is that we've figured out that it's a good meme. It's well-exercised. Whereas the \"x-risk is high\" message is less-well validated. I would also share you concern that it would turn people off. But maybe it would be good? I think we should probably try more on the margin!</p><p>I do think \"the long-term is overwhelmingly important\" is probably over-emphasized in messaging. Maybe it's important for more academic discussions of cause prioritization, but I'd be surprised if it deserved to be as front-and-center as it is.</p>", "parentCommentId": null, "user": {"username": "jpaddison"}}, {"_id": "x7ijL9gYWHHawMhmy", "postedAt": "2022-12-16T23:06:51.783Z", "postId": "cGM86RhxMdfDYbQnn", "htmlBody": "<p>I agree pretty strongly with this. I think it especially matters since, in my cause prio, the case for working on AI x-risk is much higher than other causes of x-risk even if level of x-risk they posed were the same because I'm not convinced that the expected value of the future conditional on avoiding bio and nuclear x-risk is positive. More generally I think the things that it's worth focusing on from a longtermist perspective compared to just a \"dying is bad\" perspective can look different within cause areas especially AI. For instance, I think it makes governance stuff and avoiding multi-agent failures look much more important. &nbsp;</p>", "parentCommentId": null, "user": {"username": "Nathan_Barnard"}}, {"_id": "GoECZRgCrSiFbLjkk", "postedAt": "2022-12-17T04:05:44.685Z", "postId": "cGM86RhxMdfDYbQnn", "htmlBody": "<p>I appreciate this post and think you make broadly reasonable arguments!</p>\n<p>As the cited example of \"screw longtermism\", I feel I should say that my crux here is mostly that I think AI x risk is just actually really important, and that given this, that longtermism is bad marketing and unnecessarily exclusionary.</p>\n<p>It's exclusionary because it's a niche and specific philosophical position that has some pretty unsavoury conclusions, and is IMO incredibly paralysing and impractical if you AREN'T trying to minimise x risk. I think that if framed right, \"make sure AI does what we want especially as it gets far more capable\" is just an obviously correct thing to want, and I think the movement already has a major PR problem among natural allies (see, eg Timnit Gebru's Twitter) that this kind of thing exacerbates</p>\n<p>It's bad marketing because it's easily conflated with neglecting people alive today, Pascal's Mugging, naive utilitarianism, strong longtermism, etc. And I often see people mocking EA or AI Safety who point to the same obvious weakness of \"if there's just a. 0001% chance of it being really bad we should drop everything else to fix it\". I think this is actually a pretty valid argument to mock!</p>\n<p>And even for people who avoid that trap, it seems pretty patronising to me to frame \"caring about future people\" as an important and cruxy moral insight - in practice the distinguishing thing about EA is our empirical beliefs about killer robots!</p>\n<p>I am admittedly also biased because I find most moral philosophy debates irritating, and think that EAs as a whole spend far too much time on them rather than actually doing things!</p>\n", "parentCommentId": null, "user": {"username": "Neel Nanda"}}, {"_id": "bPQg426wpWBesKzzk", "postedAt": "2022-12-17T16:47:51.846Z", "postId": "cGM86RhxMdfDYbQnn", "htmlBody": "<p>Broadly agree; nitpick follows.</p><blockquote><p>I think that outlining the case for longtermism (and EA principles, more broadly) is better for building a community of people who will reliably choose the highest-priority actions and paths to do the most good and that this is better for the world and keeping x-risk low in the long-run.</p></blockquote><p>I'm persuaded of all this, except for the \u201cbetter for the world\u201d part, which I'm not sure about and which I think you didn't argue for. That is, you've persuasively argued that emphasising the process over the conclusions has benefits for community epistemics and long-term community health; but this does trade off against other metrics one might have, like the growth/capacity of individual x-risk-related fields, and you don't comment about the current margin.</p><p>For example, if you adopt David Nash's lens of <a href=\"https://forum.effectivealtruism.org/posts/Zm6iaaJhoZsoZ2uMD/effective-altruism-as-coordination-and-field-incubation\">EA as an incubator and coordinator of other communities</a>, &nbsp;\u201cit's possible that by focusing on EA as a whole rather than specific causes, we are holding back the growth of these fields.\u201d</p><p>The low-fidelity message \u201c<a href=\"https://forum.effectivealtruism.org/posts/rFpfW2ndHSX7ERWLH/simplify-ea-pitches-to-holy-shit-x-risk\">holy shit, x-risk</a>\u201d may be an appropriate pitch for some situations, given that people have limited attention, and 'getting people into EA per se' is not what we directly care about. For example, among mid-career people with relevant skills, or other people who we expect to be more collaborators with EA than participants.</p><p>The high-fidelity message-sequence \u201cEA \u2192 Longtermism \u2192 x-risk\u201d, as a more complicated idea, is more suited to building the cause prioritisation community, the meta-community that co-ordinates other communities. For example, when fishing for future highly-engaged EAs in universities.</p><p>This still leaves open the question of which one of these should be the visible outer layer of EA that people encounter first in the media etc., and on that I think the current margin (which emphasises longtermism over x-risk) is OK. But my takeaway from David Nash's post is that we should make sure to maintain pathways within EA \u2014 even 'deep within', e.g. at conferences \u2014 that provide value and action-relevance for people who aren't going to consider themselves EA, but who will go on to be informed and affected by it for a long time (that's as opposed to having the implicit endpoint be \"do direct work for an EA org\"). If these people <a href=\"https://forum.effectivealtruism.org/posts/g8aBf2oLwDvgd4ovf/much-ea-value-comes-from-being-a-schelling-point\">know they can find each other</a> here in EA, that's also good for the community's breadth of knowledge.</p>", "parentCommentId": null, "user": {"username": "David Mears"}}, {"_id": "zA3joEYhQn3uCBDQD", "postedAt": "2022-12-18T01:15:48.306Z", "postId": "cGM86RhxMdfDYbQnn", "htmlBody": "<blockquote>\n<p>I am admittedly also biased because I find most moral philosophy debates irritating, and think that EAs as a whole spend far too much time on them rather than actually doing things!</p>\n</blockquote>\n<p>I'd say the biggest red flag for moral philosophy is that it still uses intuition both as a hypothesis generator and reliable evidence, when it's basically worthless for conclusions to accept. Yet that's the standard moral philosophy is in. It's akin to the pre-science era of knowledge.</p>\n<p>That's why it's so irritating.</p>\n<p>So I can draw 2 conclusions from that:</p>\n<ol>\n<li>\n<p>Mind independent facts about morality are not real, in the same vein as identity is not real (controversially, consciousness probably is this.)</p>\n</li>\n<li>\n<p>There is a reality, but moral philosophy needs to be improved.</p>\n</li>\n</ol>\n<p>And I do think it's valuable for EA to do this, if only to see whether there is a reality at the end of it all.</p>\n", "parentCommentId": "GoECZRgCrSiFbLjkk", "user": {"username": "Sharmake"}}, {"_id": "qyiWXzCJkDp2QhYek", "postedAt": "2022-12-19T14:48:04.679Z", "postId": "cGM86RhxMdfDYbQnn", "htmlBody": "<p>I'm curious if anyone has tried experimentally evaluating what messaging works. It seems like this new lab in NYC will be doing just this sort of work, so I'll be following along with them: https://www.eapsychology.org/</p>", "parentCommentId": null, "user": {"username": "Sharang Phadke"}}, {"_id": "siL3JwbHNeLxT4fb9", "postedAt": "2022-12-24T10:58:24.857Z", "postId": "cGM86RhxMdfDYbQnn", "htmlBody": "<p>Thanks :) And thanks for your original piece!</p><p>There seems to be tension in your comment here. You're claiming both that longtermism is a niche and specific philosophical position but also that it's patronising to point out to people.&nbsp;</p><p>Perhaps you're pointing to some hard trade-off? Like, if you make the full argument, it's paralysing and impractical, but if you just state the headline, it's obvious? That strikes me as a bit of a double-strawman - you can explain the idea in varying levels of depth depending on the context.</p><p>I don't think longtermism need be understood as a niche and specific philosophical position and discussion about longtermism doesn't need to engage in complex moral philosophy, but I agree that it's often framed this way (in the wrong contexts) and that this is bad for the reasons you point to. I think the first chapter of What We Owe the Future gets this balance right, and it's probably my favourite explanation of longtermism.</p><p>I disagree that most people already buy its core claim, which I think is more like \"protecting the long-term future of humanity is extremely important and we're not doing it\" and not just \"we should care about future people\". I think many people do \"care\" in the latter way but aren't <a href=\"https://www.lesswrong.com/posts/WDwGqLd2Puedouogq/on-sincerity\">sincerely</a> engaging with the implications of that.</p><blockquote><p>think that EAs as a whole spend far too much time on them rather than actually doing things!</p></blockquote><p>I agree with this!</p>", "parentCommentId": "GoECZRgCrSiFbLjkk", "user": {"username": "OllieBase"}}, {"_id": "AtzpFwbHmjuHmktiz", "postedAt": "2022-12-24T11:05:55.237Z", "postId": "cGM86RhxMdfDYbQnn", "htmlBody": "<p>Thanks!</p><p>Yep, agree with the first paragraph. I do think a good counterargument to this post is \"but let's try it out! If it is effective, that might make community-building much more straightforward\".</p><p>I'm unsure about the prevalence of \"the long-term is overwhelmingly important\". On the one hand, it might be unnecessary but, on the other, this feels like one of the most important ideas I've come across in my life!&nbsp;</p>", "parentCommentId": "uLBcuFwwz8MAQxcuE", "user": {"username": "OllieBase"}}, {"_id": "uA7g4zcobX6CFaSGq", "postedAt": "2022-12-24T11:08:25.423Z", "postId": "cGM86RhxMdfDYbQnn", "htmlBody": "<p>Thanks! Yes, I'm sympathetic to the idea that I'm anchoring too hard on EA growth being strongly correlated with more good being done in the world, which might be wrong. Also agree that we should test out and welcome people who are convinced by some messages but not others.</p>", "parentCommentId": "bPQg426wpWBesKzzk", "user": {"username": "OllieBase"}}, {"_id": "hhDXsWCxkCeCgwuyo", "postedAt": "2022-12-24T11:09:12.770Z", "postId": "cGM86RhxMdfDYbQnn", "htmlBody": "<p>Huh, this is an interesting angle! Thanks :)&nbsp;</p>", "parentCommentId": "x7ijL9gYWHHawMhmy", "user": {"username": "OllieBase"}}, {"_id": "Re4vFpeM73bYq8cPq", "postedAt": "2022-12-24T20:34:13.236Z", "postId": "cGM86RhxMdfDYbQnn", "htmlBody": "<p>Yeah, fair point, I'm conflating two things here. Firstly, strong longtermism/total utilitarianism, or the slightly weaker form of \"the longterm future is overwhelmingly important, and mostly dominates short term considerations\", is what I'm calling the niche position. And \"future people matter and we should not only care about people alive today\" is the common sense patronising position. These are obviously very different things!</p>\n<p>In practice, my perception of EA outreach is that it mostly falls into one of those buckets? But this may be me being uncharitable. WWOTF is definitely more nuanced than this, but I mostly just disagree with its message because I think it significantly underrates AI.</p>\n<p>I do think that the position of \"the longterm future matters a lot, but not overwhelmingly, but is significantly underrated/under invested in today\" is reasonable and correct and falls in neither of those extremes. And I would be pro most of society agreeing with it! I just think that the main way that seems likely to robustly affect the longterm future is x risk reduction, and that the risk is high enough that this straightforwardly makes sense from common sense morality.</p>\n", "parentCommentId": "siL3JwbHNeLxT4fb9", "user": {"username": "Neel Nanda"}}, {"_id": "GsmEvzAmd2DiqYGTs", "postedAt": "2022-12-27T11:23:51.619Z", "postId": "cGM86RhxMdfDYbQnn", "htmlBody": "<p>All makes sense, I agree it's usually one of those two things and that the wrong one is sometimes used.</p><p>Yeah, I think that last sentence is where we disagree. I think it's a reasonable view that I'd respond to with something like my \"our situation could change\" or \"our priorities could change\". But I'm glad not everyone is taking the same approach and think we should make both of these (complimentary) cases :)&nbsp;</p><p>Thanks for engaging!&nbsp;</p>", "parentCommentId": "Re4vFpeM73bYq8cPq", "user": {"username": "OllieBase"}}]