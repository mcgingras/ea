[{"_id": "dmEAvffrweJiDpNg2", "postedAt": "2023-07-17T22:14:14.816Z", "postId": "PChvstTKnf6iAP36F", "htmlBody": "<p>FYI this paper seems to have a really good list of EA Organisations in it. This may well come in handy!</p>\n", "parentCommentId": null, "user": {"username": "Gideon Futerman"}}, {"_id": "oDQicsXHCQe8Z5tJb", "postedAt": "2023-07-17T22:53:15.533Z", "postId": "PChvstTKnf6iAP36F", "htmlBody": "<p>That\u2019s a pretty impressive and thorough piece of research, regardless of whether you agree with the conclusions. I think one of its central points \u2014 that x-risk/longtermism has always been a core part of the movement \u2014 is correct. Some recent critiques have overemphasised the degree to which EA has shifted toward these areas in the last few years. It was always, if not front and centre, \u2018hiding in plain sight\u2019. And there was criticism of EA for focusing on x-risk from very early on (though it was mostly drowned out by criticisms of EA\u2019s global health work, which now seems less controversial along with some of the farmed animal welfare work being done).</p>\n<p>If someone disagrees empirically with estimates of existential risk, or holds a person-affecting view of population ethics, the idea that it is a front for longtermism is a legitimate criticism to make of EA. Even more resources could be directed toward global health if it wasn\u2019t for these other cause areas. A bit less reasonably, people who hold non-utilitarian beliefs might even suspect that EA was just a way of rebranding \u2018total utilitarianism\u2019 (with the \u2018total\u2019 part becoming slowly more prominent over time).</p>\n<p>At the same time, EAs still do a lot in the global health space (where a majority of EA funding is still directed), so the movement is in a sense being condemned because it has actually noticed these problems (see the Copenhagen Interpretation of Ethics).</p>\n<p>This isn\u2019t to say that the paper itself is criticising EA (it seems to be more of a qualitative study of the movement).</p>\n", "parentCommentId": null, "user": {"username": "JBentham"}}, {"_id": "LKLysK5peS2xXa4fE", "postedAt": "2023-07-18T06:35:53.764Z", "postId": "PChvstTKnf6iAP36F", "htmlBody": "<p>I don't know, but this critique feels like 5 years too late. There was a time when the focus of many within EA on longtermist issues wasn't as upfront, but there's been a sustained effort to be more upfront on this and anyone who's done the intro course will know that it is a big focus of EA.</p><p>I'd love to know if anyone thinks that there are parts of this critique that hold up today. There very well might be as I've only read the summary above and not the original post.</p>", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "smzSxZx36ujinha7r", "postedAt": "2023-07-18T08:13:15.319Z", "postId": "PChvstTKnf6iAP36F", "htmlBody": "<p>As far as I understand, the paper doesn't disagree with this and an explanation for it is given in the conclusion:</p><blockquote><p>Communication strategies such as the \u2018funnel model\u2019 have facilitated the enduring perception amongst the broader public, academics and journalists that \u2018EA\u2019 is synonymous with \u2018public-facing EA\u2019. As a result, many people are confused by EA\u2019s seemingly sudden shift toward \u2018longtermism\u2019, particularly AI/x-risk; however, this \u2018shift\u2019 merely represents a shift in EA\u2019s communication strategy to more openly present the movement\u2019s core aims.</p></blockquote>", "parentCommentId": "LKLysK5peS2xXa4fE", "user": {"username": "TobiasH"}}, {"_id": "yoJ8TavFhFcd9PMjv", "postedAt": "2023-07-18T09:13:46.889Z", "postId": "PChvstTKnf6iAP36F", "htmlBody": "<p>I think it holds up. I wrote a highly upvoted post on <a href=\"https://forum.effectivealtruism.org/posts/mzzPMrBjGpra2JSDw/ea-organizations-should-have-a-transparent-scope\">organisations being transparent</a> about their scope one month ago due to similar concerns.</p>", "parentCommentId": "LKLysK5peS2xXa4fE", "user": {"username": "Joey"}}, {"_id": "tsqvm7zLkY2XbWcuE", "postedAt": "2023-07-18T17:33:54.167Z", "postId": "PChvstTKnf6iAP36F", "htmlBody": "<p>Interesting. Seems from my perspective to be a shift towards AI, followed by a delayed update on EA\u2019s new position, followed by further shifts towards AI.</p>\n", "parentCommentId": "smzSxZx36ujinha7r", "user": {"username": "casebash"}}, {"_id": "eu46q4e48hX2SvoGJ", "postedAt": "2023-07-18T22:23:38.152Z", "postId": "PChvstTKnf6iAP36F", "htmlBody": "<p>I think the strategic ambiguity that the paper identifies is inherent to EA. The central concept of EA is so broad - \"maximize the good using your limited resources\" - that it can be combined with different assumptions to reach vastly different conclusions. For example, if you add assumptions like \"influencing the long-term future is intractable and/or not valuable\", you might reach the conclusion that the best thing to do with your limited resources is to mitigate global poverty through GiveWell-recommended charities or promoting economic growth. But if you tack on assumptions like \"influencing the long-term future is tractable and paramount\" and \"the best way to improve the future is to reduce x-risk\", then you get the x-risk and AI safety agenda.</p><p>This makes it challenging and often awkward to talk about what EA focuses on and why. But it's important to avoid describing EA in a way that implies it only supports either GHWB or the longtermist agenda. The paper cites <a href=\"https://resources.eagroups.org/running-a-group/communicating-about-ea/tips-to-help-your-conversation-go-well#h.f9cvjivt514r\">this section</a> of the EA Hub guide for EA groups which addresses this pitfall.</p>", "parentCommentId": null, "user": {"username": "evelynciara"}}, {"_id": "me6Rbo8iLa7hquWZY", "postedAt": "2023-07-20T00:16:51.741Z", "postId": "PChvstTKnf6iAP36F", "htmlBody": "<p>I skimmed through the article; thanks for sharing!</p><p>Some quick thoughts:</p><blockquote><p>community-members are fully aware that EA is not actually an open-ended question but a set of conclusions and specific cause areas</p></blockquote><ul><li>The cited evidence here is one user claiming this is the case; I think they are wrong. For example, if there were a dental hygiene intervention that could help, let's say, a hundred million individuals and government / other philanthropic aid were not addressing this, I would expect a CE-incubated charity to jump on it immediately.<ul><li>There are other places where the author makes what I would consider sweeping generalizations or erroneous inferences. For instance:<ul><li>\"...given the high level of control leading organizations like the Centre for Effective Altruism (CEA) exercise over how EA is presented to outsiders\" \u2014 The evidence cited here is mostly all the guides that CEA has made, but I don't see how this translates to \"high level of control.\" EAs and EA organizations don't have to adhere to what CEA suggests.&nbsp;</li><li>\"The general consensus seems to be that re-emphasizing a norm of donating to global poverty and animal welfare charities provides reputational benefits...\" \u2014 upvotes to a comment \u2260 general consensus.&nbsp;</li></ul></li></ul></li><li>Table 1, especially the <i>Cause neutrality</i> section, seems to wedge a line where one doesn't exist.</li><li>The author acknowledges in the Methodology section that they didn't participate in EA events or groups and mainly used internet forums to guide their qualitative study. I think this is the critical drawback of this study. Some of the most exciting things happen in EA groups and conferences, and I think the conclusion presented would be vastly different if the qualitative study included this data point.</li><li>I don't know what convinces the article's author to imply that there is some highly coordinated approach to funnel people into the \"real parts of EA.\" If this is true (and my tongue-in-cheek remark here), I would suggest these core people<a href=\"https://forum.effectivealtruism.org/posts/ZbaDmowkXbTBsxvHn/historical-ea-funding-data\"> not spend&gt;50% of the money on global health</a> as there could be cheaper ways of maintaining this supposed illusion.<br><br>Overall, I like the background research done by the author, but I think the author's takeaways are inaccurate and seem too forced. At least to me, the conclusion is reminiscent of the discourse around conspiracies such as the deep state or the \"plandemic,\" where there is always a secret group, a \"they,\" advancing their agenda while puppeteering tens of thousands of others.&nbsp;<br><br>Much more straightforward explanations exist, which aren't entertained in this study.<br><br>EA is more centralized than most other movements, and it would be ideal to have several big donors with different priorities and worldviews. However, EA is also more <a href=\"https://forum.effectivealtruism.org/posts/zAYai4TAKYkByxeme/ea-is-too-vague-let-s-be-more-specific\">functionally diverse</a> and consists of some ten thousand folks (and growing), each of whom is a stakeholder in this endeavor and will collectively define the movement's future.</li></ul>", "parentCommentId": null, "user": {"username": "satpathyakash"}}, {"_id": "bGod9rhzx5MTfJ9bG", "postedAt": "2023-08-03T22:42:45.114Z", "postId": "PChvstTKnf6iAP36F", "htmlBody": "<p>I also through that it was a fairly good (and concise) history of EA. I have been reading EA material for a few years now, but I haven't before seen such a clear tracing of the history of it.</p>", "parentCommentId": null, "user": {"username": "jlemien"}}, {"_id": "mhyH7TEYsrZ8LfZeL", "postedAt": "2023-08-04T13:35:25.398Z", "postId": "PChvstTKnf6iAP36F", "htmlBody": "<p>I put the whole list in a spreadsheet for ease of use, in case anyone wants to access it in a way that is a bit more editable than a PDF: https://docs.google.com/spreadsheets/d/1KDcDVpTKylk3qP3CqLFSscWmH01AkW4LLNwjOOcWpF8/edit?usp=sharing</p>", "parentCommentId": "dmEAvffrweJiDpNg2", "user": {"username": "jlemien"}}]