[{"_id": "RRm8vnmwjWK24ung2", "title": "Taxing Tobacco: the intervention that got away (happy World No Tobacco Day)", "postedAt": "2023-05-31T21:34:12.359Z", "htmlBody": "<h2>TL;DR</h2><ul><li>The death toll of tobacco dwarfs traditional EA global health focuses (e.g. Malaria).</li><li>Taxing tobacco is the most effective and yet most neglected form of reducing tobacco consumption.</li><li>Multiple EA orgs have ran cost-effectiveness numbers that would put a tobacco taxation NGO on GiveWell\u2019s list (both in expected value and contingent on success) and even at the top in some cases.</li><li>Despite this, no enduring tobacco-taxation advocacy organization has emerged from EA (which would be the only advocacy organization in the world exclusively dedicated to tobacco taxation)</li><li>Join Joel Tan (Founder of <a href=\"https://exploratory-altruism.org/\">CEARCH</a>) and J.T. Stanley, who as incubatees of Charity Entrepreneurship closely examined tobacco taxation, on <a href=\"https://www.eventbrite.com/e/tobacco-taxation-the-intervention-that-got-away-tickets-647249399377\"><strong>June 10<sup>th</sup></strong> <strong>(Saturday) at 14:00 UTC</strong></a> for a virtual discussion about the intervention and what\u2019s next to make it a reality.</li></ul><p>&nbsp;</p><p><i>Thanks to Moritz von Knebel for providing feedback on the draft.</i></p><p><i>Yes, I cite WHO a lot. Not all WHO citations are the same WHO link FYI.</i></p><p>&nbsp;</p><h2><strong>Problem space</strong></h2><ul><li><strong>Tobacco kills over 8 million individuals a year</strong>\u2014that\u2019s 13x Malaria (<a href=\"https://www.who.int/news-room/fact-sheets/detail/tobacco\">WHO</a>) (<a href=\"https://www.cdc.gov/tobacco/data_statistics/fact_sheets/fast_facts/diseases-and-death.html#:~:text=Smoking%20is%20the%20leading%20cause,7%20million%20deaths%20per%20year.&amp;text=If%20the%20pattern%20of%20smoking,to%20tobacco%20use%20by%202030.\">CDC</a>).</li><li>Another way of framing it: annually more people are killed by tobacco usage than malaria, HIV, and neonatal deaths combined\u2026 twice over. And while the death toll of the latter three has been decreasing over time, death from tobacco is increasing.</li><li>Of the 8 million deaths, 1.2 million are bystanders killed from secondhand smoking (<a href=\"https://www.who.int/news-room/fact-sheets/detail/tobacco\">WHO</a>).</li></ul><p>&nbsp;</p><p>Facts not related to death:</p><ul><li>Tobacco consumption displaces productive forms of spending. Tobacco consumes 1.5 to 17% (with a rough median around 4.5%) of a person\u2019s income depending on country and socioeconomic status (<a href=\"https://docs.google.com/spreadsheets/d/1OiB55R3eEEY-D06BlUU01VlS6enlXBZX/edit?usp=sharing&amp;ouid=102365552202638358865&amp;rtpof=true&amp;sd=true\">table of results</a>) (<a href=\"https://tobaccocontrol.bmj.com/content/10/3/210\">de Beyer et al., 2001</a>). Spending on tobacco typically displaces spending on education and nutrition in low-income households (<a href=\"https://pubmed.ncbi.nlm.nih.gov/18187245/\">John, 2008</a>) (<a href=\"https://pubmed.ncbi.nlm.nih.gov/17728033/\">Nonnemaker and Sur, 2007</a>) (<a href=\"https://pubmed.ncbi.nlm.nih.gov/18313191/\">Pu et al., 2008</a>).</li><li>For example, smoking households spent 46% less on education than non-smoking households in surveyed townships in rural China (<a href=\"https://pubmed.ncbi.nlm.nih.gov/16137812/\">Wang et al., 2006</a>). Another study states, \u201cAverage male cigarette smokers [in Bangladesh] spend more than twice as much on cigarettes as per capita expenditure on clothing, housing, health and education combined\u201d (<a href=\"https://pubmed.ncbi.nlm.nih.gov/11544383/\">Efroymson et al., 2001</a>).</li><li>Smoking decreases productivity (<a href=\"https://tobacconomics.org/files/research/523/UIC_Economic-Costs-of-Tobacco-Use-Policy-Brief_v1.3.pdf\">Tobacconomics, 2019</a>) (<a href=\"https://tobaccocontrol.bmj.com/content/10/3/233\">Halpern et al., 2001</a>). One study in the United States found that it cost 1,807 USD per year per smoker in lost productivity compared to non-smokers (<a href=\"https://pubmed.ncbi.nlm.nih.gov/17033509/\">Bunn III et al., 2006</a>).</li><li>Tobacco increases individual healthcare costs (<a href=\"https://tobacconomics.org/files/research/523/UIC_Economic-Costs-of-Tobacco-Use-Policy-Brief_v1.3.pdf\">Tobacconomics, 2019</a>). A study in China found that individual medical spending attributable to smoking increased the poverty rate by 1.5% in urban areas and 0.7% in rural areas (<a href=\"https://pubmed.ncbi.nlm.nih.gov/16959391/\">Liu et al., 2006</a>).</li><li>Tobacco strains the healthcare sector and taxpayers foot the bill (<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8009005/\">Wunsch and Brodie, 2021</a>) (<a href=\"https://tobacconomics.org/files/research/523/UIC_Economic-Costs-of-Tobacco-Use-Policy-Brief_v1.3.pdf\">Tobacconomics, 2019</a>).</li><li>The annual costs of tobacco from healthcare expenditures and losses in productivity are 1.8% of the world GDP, breaking down to 1.1% to 1.7% of LMICs\u2019 GDPs (<a href=\"https://tobaccocontrol.bmj.com/content/27/1/58\">Goodchild et al., 2018</a>) (<a href=\"https://tobacconomics.org/files/research/523/UIC_Economic-Costs-of-Tobacco-Use-Policy-Brief_v1.3.pdf\">Tobacconomics, 2019</a>).</li></ul><p>&nbsp;</p><p>One last fact: without intervention, tobacco is on track to <strong>kill a billion individuals during the 21st century&nbsp;</strong>(<a href=\"https://apps.who.int/iris/handle/10665/43818\">WHO, 2008</a>) (<a href=\"https://www.cgdev.org/sites/default/files/CGD-Policy-Paper-62-Savedoff-Alwang-Best-Health-Policy-Tobacco-Tax.pdf\">Savedoff and Alwang, 2015</a>) (<a href=\"https://publichealthreviews.biomedcentral.com/articles/10.1007/BF03391651#:~:text=Higher%20taxes%2C%20regulations%20on%20smoking,and%2050%20million%20vascular%20deaths.\">Jha, 2012</a>).</p><p>&nbsp;</p><h1>The intervention&nbsp;</h1><ul><li>The scientific literature on tobacco consumption is lengthy. The interventions that have the most significant effect on reducing tobacco consumption have been formalized in WHO\u2019s <a href=\"https://www.who.int/initiatives/mpower\">MPOWER framework</a> (<a href=\"https://pubmed.ncbi.nlm.nih.gov/19606747/\">Kaleta et al., 2009</a>).</li><li>Of the MPOWER tobacco control measures, taxing tobacco has been demonstrated consistently in the scientific literature to the be the most effective intervention (<a href=\"https://www.who.int/publications/i/item/9789240019188\">WHO</a>) (<a href=\"https://cancercontrol.cancer.gov/brp/tcrb/monographs/monograph-21\">NIH</a>).</li><li>Taxing tobacco has a price elasticity around -0.5, meaning that for a 10% increase in retail price of tobacco, consumption decreases by 5% in LMICs (4% in HICs) (<a href=\"https://cancercontrol.cancer.gov/sites/default/files/2020-06/m21_complete.pdf\">NIH</a>) (<a href=\"https://www.who.int/news-room/fact-sheets/detail/tobacco\">WHO</a>)<a href=\"#_ftn2\">[2]</a></li><li>Despite being the most effective intervention, tobacco taxation is the most neglected (<a href=\"https://www.who.int/activities/raising-taxes-on-tobacco#:~:text=Tobacco%20use%20kills%20eight%20million,measure%20for%20reducing%20tobacco%20use.\">WHO</a>)&nbsp;(<a href=\"https://blogs.worldbank.org/health/taxation-most-effective-still-least-used-tobacco-control-measure\">World Bank</a>).</li></ul><p>&nbsp;</p><h3>Cost effectiveness</h3><ul><li><strong>Giving What We Can</strong> explored a few estimates of the impact of reducing tobacco consumption and found it is plausible that <strong>the cost per life saved could reach 800 USD&nbsp;</strong>(<a href=\"https://www.givingwhatwecan.org/reports/tobacco-control\">Report</a>) (<a href=\"https://www.givingwhatwecan.org/blog/tobacco-control-best-buy-developing-world\">Blog post</a>)</li><li><strong>Charity Entrepreneurship</strong> has calculated <strong>an expected value of</strong> <strong>51 USD per DALY averted</strong> by a tax increase of 23.7% in Mongolia with 27.14% chance of success. (<a href=\"https://docs.google.com/spreadsheets/d/1i1wTJgeXiqvJF_OuYcZtMrCUlh_UHJu7a5BpDi56Npc/edit#gid=1384295676\">Spreadsheet</a>)</li><li><strong>Open Philanthropy</strong>\u2019s BOTEC on Indonesia found <strong>30 USD per DALY averted in expectation</strong> with a 10% probability of success, million-dollar campaign, 10% increase in retail price, and a counterfactual speed up of 3 years. (<a href=\"https://docs.google.com/spreadsheets/d/12--1_VRJFIos4z6x_EGRDJcblLWjo0h0t7HBjcK-LFc/edit#gid=908733067\">Spreadsheet</a>)</li><li>Moritz von Knebel and I, <strong>Charity Entrepreneurship incubatees</strong>, found (<a href=\"https://docs.google.com/spreadsheets/d/1kbFuPQm55bHutglHMA-P4gLGGhrG9ei8HbtvsqCtDhM/edit?usp=sharing\">Spreadsheet</a>):<ul><li>A policy victory would typically yield a DALY averted for single-digit dollars (9.49 and 1.01 USD in Nepal and Iran, respectively)<a href=\"#_ftn3\">[3]</a> and even just cents in some cases (0.49 USD in Canada)<a href=\"#_ftn4\">[4]</a> for just charity costs.</li><li>When accounting for government costs, the range was single digits (6.62 USD in Canada) to low three digits (155.67 USD in Mongolia)<a href=\"#_ftn5\">[5]</a>. Estimates were most worsened (i.e. Kuwait, Mongolia) if we assigned a high probability that the government would increase the tax rate soon in the future independent our intervention.<a href=\"#_ftn6\">[6]</a></li><li>Accelerating Nepal enacting a tax increase of 24% by little more than half a year has <strong>an expected value of 59 USD per DALY averted</strong> with a probability of success of 27.5%.</li></ul></li></ul><p>&nbsp;</p><h1>EA\u2019s journey with tobacco taxation</h1><ul><li><strong>In 2013,</strong> <strong>Open Philanthropy</strong> conducts a <a href=\"https://www.openphilanthropy.org/research/tobacco-control-in-low-and-middle-income-countries/\">shallow investigation</a> of tobacco taxation in LMICs.<ul><li>\u201cA philanthropist could undertake any of a variety of strategies to attempt to ensure passage and enforcement of such [tobacco taxation] policies. We do not have a good sense of the likely returns to the different potential strategies.\u201d</li><li>\u201cOur research in this area has been relatively limited, and many important questions remain unanswered by our investigation.\u201d</li></ul></li><li><strong>In 2015,</strong> <strong>Giving What We Can</strong> releases a <a href=\"https://www.givingwhatwecan.org/reports/tobacco-control\">report</a> and <a href=\"https://www.givingwhatwecan.org/blog/tobacco-control-best-buy-developing-world\">blog</a> post on tobacco control heavily featuring tobacco taxation.<ul><li>\u201cAt the beginning of this post, I asked whether tobacco taxes can really claim to be one of the most\u2014or even&nbsp;<i>the</i>&nbsp;most\u2014cost-effective way of saving lives in the developing world. I think we have good reason to think they are, though we should ask for answers to the above questions\u2014or at least better answers than I\u2019ve been able to give.\u201d</li><li>\u201cTobacco taxes appear to be a highly cost-effective way to save lives in the developing world. I suspect that further research will show that well-run tax advocacy organizations can save lives as cheaply as the charities currently touted by organizations like GiveWell and Giving What We Can. For now that research remains to be done.\u201d</li></ul></li><li><strong>In 2016,</strong> <strong>Charity Entrepreneurship</strong> does <a href=\"https://www.charityentrepreneurship.com/post/tobacco-taxation\">shallow research</a> on tobacco taxation.<ul><li>\u201cThe evidence for lobbying succeeding in LMIC is far weaker than the evidence for the impact of successfully implemented tobacco taxation. It\u2019s often difficult to credit responsibility for legislative change to particular players in advocacy campaigns because multiple campaigns operate simultaneously and it\u2019s unclear whether the government would have changed anyway, absent lobbying efforts. While there is considerable evidence the government implementation of tobacco taxation is highly cost-effective, there is a dearth of analysis on the return of investment of actual lobbying campaigns.\u201d</li></ul></li><li><strong>In 2017,</strong> <strong>GiveWell</strong> conducts three interviews on tobacco control. (<a href=\"https://files.givewell.org/files/conversations/The_Union_12-20-17_(public).pdf\">The Union</a>) (<a href=\"https://files.givewell.org/files/conversations/Yolonda_Richardson_12-12-17_(public).pdf\">Campaign for Tobacco-Free Kids</a>) (<a href=\"https://files.givewell.org/files/conversations/William_Savedoff_09-11-17_(public).pdf\">Author of the \u201cThe Single Best Health Policy in the World: Tobacco Taxes\u201d</a>)</li><li><strong>In 2017,</strong> <strong>Charity Entrepreneurship</strong> recommends tobacco taxation as one of four recommended ideas in their first-ever charity incubation.</li><li><strong>In late 2019,</strong> two tobacco-taxation charities emerge from Charity Entrepreneurship\u2019s second-ever incubation program.</li><li><strong>In 2020,</strong> both Charity Entrepreneurship tobacco-taxation charities shutdown largely due to COVID ramifications.<a href=\"#_ftn7\">[7]</a> (<a href=\"https://www.charityentrepreneurship.com/good-policies\">Good Policies</a>) (<a href=\"https://www.charityentrepreneurship.com/policy-entrepreneurship-network\">Policy Entrepreneurship Network</a>)</li><li><strong>In 2020,</strong> <strong>GiveWell</strong> conducts three interviews on tobacco control. (<a href=\"https://docs.google.com/document/d/1dQf-JFiY973pm3ABFkwSu2rA1ROYYP6_9vuLk7Q7wDQ/edit\">Vital Strategies</a>) (<a href=\"https://docs.google.com/document/d/1p2BS85iR0iz2fqeYf6gM1MhWXkDLynFlC4uY50dpjX8/edit\">Founder of Good Policies</a>) (<a href=\"https://docs.google.com/document/d/1iSZHaIYHUmoC-1yl1RhSl4G5Oaq0zinGeTb9g1INAyQ/edit\">International Development Research Centre</a>)</li><li><strong>In 2022,</strong> <strong>Charity Entrepreneurship</strong> updates their <a href=\"https://9475dbf4-555e-4808-9886-5f8ee815cc82.usrfiles.com/ugd/9475db_a1f6dde1f00e4e768b807327dd17a13e.pdf\">research report</a> on tobacco taxation and recommends it to another incubation round.<ul><li>\u201cThere is very strong evidence linking tobacco to many negative health outcomes\u2026 There is also very strong evidence that increasing tobacco taxes reduces tobacco consumption\u2026 Our own cost-effectiveness analyses have also shown that tobacco taxation is an extremely cost-effective intervention\u2026 There is strong evidence that this intervention can be effectively delivered by the government\u2026 Regarding the probability of success of advocacy for this intervention, looking at a total of 159 case studies, we see an average success rate of ~27.14%.\u201d<br>\u201cDespite the promise of this intervention, there are some concerns. There are two main funders in the space, but they favor large, well-established organizations, so funding might be a limiting factor\u2026 Another concern about this intervention is neglectedness. There are a few organizations working in this space that appear to be quite successful, are well-funded, and are targeting high-burden countries. The key strategy might lie in focusing on countries in which other organizations are not currently working (which might not be the most burdened countries, but still high-burden).\u201d</li></ul></li><li><strong>In 2023,</strong> <strong>Open Philanthropy</strong> publishes another <a href=\"https://www.openphilanthropy.org/research/tobacco-control/\">shallow investigation</a> on tobacco control featuring tobacco taxation.<ul><li>\u201cWhile these numbers are all very uncertain (and rely on several questionable assumptions), the main takeaway is that cigarette tax advocacy can be very valuable but probably only clears the 1,000x bar if it affects a large smoking population or can be implemented cheaply with a high probability of success.\u201d</li></ul></li></ul><p>&nbsp;</p><h1>Where do we go from here</h1><p>In my (current) opinion, the best reason to be pessimistic about this intervention is that it could be absurdly hard to beat the tobacco lobby. The tobacco control space receives substantial funding and has multiple established INGOs. The fact that tobacco taxation is neglected could be a reflection that tobacco control orgs have already reached cost-effectiveness optimization, and its settling for other tobacco control concessions short of tobacco taxation because the tobacco lobby will fight the hardest against taxation.</p><p>Yet, there are reasons to think that the organizations in the tobacco control space have not reached optimization and that the space is undersaturated (<a href=\"https://docs.google.com/document/d/1YnF1sFouwesByUQsuqVR8aFa_Q85tJ2FURuQCqa9JMI/edit?usp=sharing\">see this graph for example</a>). But to be fair, I\u2019ve only seen consistent indicators that the tobacco lobby will be aggressive in every circumstance possible.</p><p>Regardless, these hypotheses have not yet had the chance to be tested, and neither has a tobacco-taxation organization guided by the effectiveness mindset.</p><p>Join Joel Tan, Founder of <a href=\"https://exploratory-altruism.org/\">CEARCH</a>, and myself for a discussion of the promise of tobacco taxation and pathways forward.</p><p><strong>June 10<sup>th</sup> (Saturday) 14:00 UTC</strong></p><p><a href=\"https://www.eventbrite.com/e/tobacco-taxation-the-intervention-that-got-away-tickets-647249399377\">https://www.eventbrite.com/e/tobacco-taxation-the-intervention-that-got-away-tickets-647249399377</a></p><p>&nbsp;</p><p><br>&nbsp;</p><hr><p><a href=\"#_ftnref1\">[1]</a> Being in opposite time zones played into this.</p><p><a href=\"#_ftnref2\">[2]</a> Results for HICs are tightly clustered whereas LMICs have more variation.</p><p><a href=\"#_ftnref3\">[3]</a> Conditions: Nepal \u2013 24% increase in tax, charity runs 4.75 years; Iran \u2013 27.5% increase in tax, charity runs 6 years</p><p><a href=\"#_ftnref4\">[4]</a> Conditions: Canada -- 25% increase in tax, charity runs 4 years</p><p><a href=\"#_ftnref5\">[5]</a> Conditions: Mongolia -- 20% increase in tax, charity runs 5.5 years, 82.5% chance per year the government would enact that increase on their own</p><p><a href=\"#_ftnref6\">[6]</a> The benefits of the intervention are current discounted over time by the odds per year that the government would enact the increase on its own.</p><p><a href=\"#_ftnref7\">[7]</a> Also worth noting that seed grants for these orgs were 5X less than what CE seed grants currently typically amount to.</p>", "user": {"username": "Yelnats T.J."}}, {"_id": "veR4W92bZsTsGgS3D", "title": "A moral backlash against AI will probably slow down AGI development", "postedAt": "2023-05-31T21:31:18.333Z", "htmlBody": "<p><i>Note: This is a submission for the 2023 Open Philanthropy AI Worldviews contest, due May 31, 2023. It addresses Question 1: \u201cWhat is the probability that AGI is developed by January 1, 2043?\u201d</i></p><p>&nbsp;</p><p><strong>Overview</strong></p><p>People tend to view harmful things as evil, and treat them as evil, to minimize their spread and impact. If enough people are hurt, betrayed, or outraged by AI applications, or lose their jobs, professional identity, and sense of purpose to AI, and/or become concerned about the existential risks of AI, then an intense public anti-AI backlash is likely to develop. That backlash could become a global, sustained, coordinated movement that morally stigmatizes AI researchers, AI companies, and AI funding sources. If that happens, then AGI is much less likely to develop by the year 2043. Negative public sentiment could be much more powerful in slowing AI than even the most draconian global regulations or formal moratorium, yet it is a neglected factor in most current AI timelines.</p><p>&nbsp;</p><p><strong>Introduction</strong></p><p>The likelihood of AGI being developed by 2043 depends on two main factors: (1) how technically difficult it will be for AI researchers to make progress on AGI, and (2) how many resources \u2013 in terms of talent, funding, hardware, software, training data, etc. \u2013 are available for making that progress. Many experts\u2019 \u2018AI timelines\u2019 for predicting AI development assume that AGI likelihood will be dominated by the first factor (technical difficulty), and assume that the second factor (available resources) will continue increasing.</p><p>In this essay I disagree with that assumption. The resources allocated into AI research, development, and deployment may be much more vulnerable to public outrage and anti-AI hatred than the current AI hype cycle suggests. Specifically, I argue that ongoing AI developments are likely to provoke a moral backlash against AI that will choke off many of the key resources for making further AI progress. This public backlash could deploy the ancient psychology of moral stigmatization against our most advanced information technologies. The backlash is likely to be global, sustained, passionate, and well-organized. It may start with grass-roots concerns among a few expert \u2018AI doomers\u2019, and among journalists concerned about narrow AI risks, but it is likely to become better-organized over time as anti-AI activists join together to fight an emerging existential threat to our species. (Note that this question of anti-AI backlash likelihood is largely orthogonal to the issues of whether AGI is possible, and whether AI alignment is possible.)</p><p>I\u2019m not talking about a violent Butlerian Jihad. In the social media era, violence in the service of a social cause is almost always counter-productive, because it undermines the moral superiority and virtue-signaling strategies of righteous activists. (Indeed, a lot of \u2018violence by activists\u2019 turns out to be false flag operations funded by vested interests to discredit the activists that are fighting those vested interests.)&nbsp;</p><p>Rather, I\u2019m talking about a non-violent anti-AI movement at the social, cultural, political, and economic levels. For such a movement to slow down the development of AGI by 2043 (relative to the current expectations of Open Philanthropy panelists judging this essay competition), it only has to arise sometime in the next 20 years, and to gather enough public, media, political, and/or investor support that it can handicap the AI industry\u2019s progress towards AGI, in ways that have not yet been incorporated into most experts\u2019 AI timelines.&nbsp;</p><p>An anti-AI backlash could include political, religious, ideological, and ethical objections to AI, sparked by vivid, outrageous, newsworthy failures of narrow AI systems. An anti-AI backlash could weakly delay AI research through government regulation. But it could strongly delay AI research through socio-cultural dynamics such as AI research becoming morally taboo, socially stigmatized, religiously condemned, and/or politically polarized. For example, if being an AI researcher became as publicly stigmatized as being a white nationalist, a eugenicist, a sexist, or a transphobe, then AI research would be largely abandoned by any researchers sensitive to social pressure, and AGI would not be developed for a long time.</p><p>Thus, we can invert the question of AGI timelines, and consider the possible timelines for an anti-AI backlash. Rather than asking \u2018What is the likelihood that we\u2019ll have AGI by 2043?\u2019, we could ask \u2018What is the likelihood that we will see an anti-AI backlash by 2043 \u2013 a backlash that is strong enough to slow down AGI development?\u2019&nbsp; I\u2019d argue that the answer to this second question is fairly high. Even just in the last few weeks (as of May 31, 2023), we\u2019ve seen a dramatic increase in public attention on AI risk, public and government concern about AI, and the beginnings of an anti-AI backlash on social media, such as Twitter. &nbsp;</p><p>(Note that in this essay I\u2019m not taking a position on whether an anti-AI backlash would be a good thing or a bad thing; I\u2019m just doing a preliminary analysis of how such a backlash could slow down AGI timelines.)</p><p>&nbsp;</p><p><strong>Triggers for an anti-AI backlash</strong></p><p>The general public is already culturally primed for an anti-AI backlash. Ever since the novel <i>Frankenstein</i> (1818), we\u2019ve had generations of science fiction novels, movies, TV shows, computer games, and other media portraying the dangers of creating artificial intelligence. Most living people in developed countries have been exposed to these cautionary tales. They\u2019ve mostly seen <i>2001: A Space Odyssey, The Terminator, Ex Machina, Black Mirror</i>, and <i>Westworld</i>. They\u2019re often the first things that ordinary people think about when they think about AI. And most adults have first-hand experience of playing computer games against powerful (but narrow) AI, e.g. trying to win \u2018Civilization\u2019 on \u2018god mode\u2019 difficult level.</p><p>The triggers for an anti-AI backlash don\u2019t need to create moral stigma from scratch. They just need to connect these latent cultural fears of AI to current real-world AI issues. I\u2019ll call these issues \u2018triggers\u2019, and there are several kinds that seem quite likely to provoke moral stigmatization of AI within the next 20 years.</p><p>&nbsp;</p><p><strong>Trigger 1: Unemployment</strong></p><p>People get pretty upset when they lose their jobs. The closer we get to AGI, the more job losses we\u2019ll see. And, for any \u2018new jobs\u2019 that open up due to increased economic activity, AI systems will probably be able to learn the new job faster than humans will be able to re-train to do them.&nbsp;</p><p>Insofar as Large Language Models are making faster progress in human-style information processing than autonomous robotics are making in doing physical tasks, AI job losses may start hitting white-collar professional who do \u2018brain work\u2019 before they hit blue-collar workers doing physical work. These white-collar professionals may include millions of suddenly unemployed lawyers, accountants, journalists, teachers, academics, medical staff, pharmacists, software engineers, graphic designers, architects, and civil engineers.&nbsp;</p><p>Such people are typically highly educated, politically engaged, and prone to adopting new moral stigmas through social media. If they\u2019re unemployed, they would have all the time in the world to organize an anti-AI backlash movement. If they have some real estate equity, investment assets, and credit, they may have the money to keep fighting for a while, even without an income. If they have kids, who face poor career prospects in turn due to ongoing AI developments, they may feel the righteous fury of parents who are motivated to do anything necessary to secure a viable future for their next generation. Thus, AI-imposed unemployment is likely to provoke an anti-AI backlash, probably in the time scale of 5-20 years from now.</p><p>&nbsp;</p><p><strong>Trigger 2: Sex</strong></p><p>Moral stigmatization often focuses on human sexuality. Sexual practices outside the mainstream have often provoked furious moral condemnation, across cultures and across history \u2013 whether it\u2019s incest, polygamy, prostitution, cheating, BDSM, polyamory, or porn. As narrow AI gets applied to goods and services related to human sexuality, there are likely to be all kinds of moral backlashes from diverse groups, ranging from Christian conservatives to woke feminists. &nbsp;</p><p>New information technologies are often applied first to create new sexual content. Internet Rule 34 says \u2018If it exists, there is porn of it; no exceptions\u2019. A variant will be \u2018If AI can make porn of it, there will be porn of it\u2019. Possible applications of AI in the sexual domain have focused on AI-generated porn and erotica (whether photos, audio, video, or stories), deepfake porn, interactive girlfriends and boyfriends, and sexbots.&nbsp;</p><p>A key trigger for an anti-AI backlash could be the moral outrage and sexual disgust provoked by sexual applications of narrow AI, such as highly habit-forming interactive VR porn, or customized erotic chatbots with the voices, mannerisms, ad personality traits of someone\u2019s neighbors, co-workers, or ex-lovers.&nbsp;</p><p>The most salient, intimate, and controversial application of AI in the next couple of decades will be, essentially, the production of interactive sex slaves \u2013 whether in real physical bodies, VR avatars, 2-D deepfake porn, or auditory chatbots. The moral condemnation of slavery remains very strong \u2013 it just hasn\u2019t been applied yet to digital slaves. When AI researchers start to be seen as breeders and traders of digital sex slaves, they\u2019re likely to be strongly stigmatized.</p><p>Many of these sexual AI applications will take highly controversial forms. Pedophiles will buy AI sexbots with children\u2019s bodies. Sadistic psychopaths will use disposable AI sexbots that can be flogged, cut, and branded, and that scream in realistic pain. Guys who like futanari porn will use sexbots that combine the primary and secondary sexual traits of males and females. AI-generated deepfake porn of politicians, tech billionaires, media celebrities, journalists, and activists is especially likely to provoke the wrath of the rich, powerful, and influential.&nbsp;</p><p>The marketing and use of these sexual AI applications may be private at first, but there will inevitably by news coverage, and it will be written to provoke maximum moralistic outrage, because moralistic outrage sells, and gets clicks, and gets shares on social media.&nbsp;</p><p>&nbsp;</p><p><strong>Trigger 3: Violence</strong></p><p>Many AI researchers have signed pledges not to develop lethal autonomous weapons (LAWs), such as \u2018slaughterbots\u2019. However, there are many other applications of narrow AI that could lead to widespread dangers, injuries, and deaths. Such violence often provokes moral outrage and intense stigmatization of the technologies involved.&nbsp;</p><p>The big danger here is not so much that AI safety engineers will stupidly overlook some obviously dangerous failure mode in their systems. Rather, the danger is that rogue nation-states, terrorists, bad actors, resentful former employees, aggrieved nihilists, creepy stalkers, or mischievous youth will manipulate or hack the AI systems to cause targeted deaths of mass carnage. Bad actors could hack self-driving cars to cause huge pile-ups on highways that lead to dozens of deaths. AI drones could be modified by terrorists, criminal gangs, or violent activists to cause mass shootings or explosions at public events. Autonomous assassination drones with face-recognition abilities and long-term loitering abilities could kill major heads of state. Obsessive stalkers could use AI systems to track, harass, and harm their sexual victims. Anarchists, anti-capitalists, and eco-activists who hate resource-intensive industries could hack AI factory control systems to cause horrific industrial accidents. Religious extremists could use AI propaganda systems to promote religious radicalization, terrorism, and warfare.</p><p>All of these violent AI applications will, of course, be dismissed and disavowed by the AI industry. But the public may notice the common denominator: AI allows highly effective, targeted violence that is displaced in time and space from the humans directing the violence. This increases the effectiveness and decreases the risks of doing all kinds of mayhem. This will strike many ordinary people as horrifying and outrageous, and will reinforce anti-AI sentiment.</p><p>&nbsp;</p><p><strong>Other triggers</strong></p><p>Apart from unemployment, sex, and violence, there are many other applications of narrow AI that could exacerbate an anti-AI backlash. These include harmful effects on AI on women, children, elders, racial minorities, and sexual minorities. These include harmful effects of AI propaganda in political polarization and religious intolerance. Biomedical AI systems for drug discovery could lead to new, highly addictive, psychosis-inducing recreational drugs rather than cures for cancer. AI applied to consumer advertising, gambling, and investments could lead people into over-spending, debt, bankruptcy, divorce, and ruin. The number of harmful things that could go wrong with narrow AI systems is almost limitless \u2013 but each new type of harm will be an occasion for sensationalist news coverage, public outrage, virtue signaling, political condemnation, and moral stigmatization of AI.&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p><strong>AI chokepoints that could delay AGI</strong></p><p>So what if there\u2019s an anti-AI backlash? What could ordinary people actually do to slow down AI research, given the arms race dynamics between AI companies (such as Microsoft vs. Google) and nation-states (such as the US vs. China)? This section addresses some key resources required for AGI development that could be choked off by an anti-AI backlash. It\u2019s not an exhaustive list of the ways that moral stigmatization of AI could handicap AI research. It\u2019s just intended to give a sense of how strongly and comprehensively an anti-AI backlash could lead to another \u2018AI winter\u2019, or even to a decades-long \u2018AI ice age\u2019.&nbsp;</p><p>&nbsp;</p><p><strong>Chokepoint 1: AI Talent</strong></p><p>If AI research becomes strongly morally stigmatized, all the prestige and coolness of being an AI researcher would evaporate. Moral stigmatization of a career does not just mean the career suffers a slight decline in status relative to other careers. No. The psychology of moral stigmatization means the general public views the career as evil, and views the people working in the career as morally tainted by that evil.&nbsp; Intense moral stigma against AI would mean that being an AI researcher is seen as being about as reputable as being a convicted sex offender, a Nazi racist, an arms dealer, or a mass murderer. The public would view AI researchers as hubris-driven mad scientists with psychopathic traits and genocidal aspirations.</p><p>Moral stigmatization of AI research would render AI researchers undateable as mates, repulsive as friends, and shameful to family members. Parents would disown adult kids involved in AI. Siblings wouldn\u2019t return their calls. Spouses would divorce them. Landlords wouldn\u2019t rent to them.&nbsp;</p><p>Once the anti-AI backlash renders AI researchers socially, sexually, and professionally toxic, this would radically reduce the quantity and quality of talent working in AI. People with the technical skills to do AI research would exit the field, and would work instead on cybersecurity, or crypto, or non-AI software, or robotics, or whatever. They would have many career options that aren\u2019t viewed as evil by lots of people they meet.</p><p>People in some fields have already developed pretty thick skins for resisting stigma. Researchers in controversial areas such as behavior genetics, evolutionary psychology, intelligence research, sex differences, and race differences have been subject for decades to moral stigmatization, hostile stereotyping, career handicaps, lack of funding, and attacks by journalists. They\u2019ve become self-selected for orneriness, disagreeableness, intellectual courage, emotional stability, and self-sufficiency, and they\u2019re learned many coping strategies. By contrast, AI researchers have little experience of being morally stigmatized. They\u2019re used to high status, prestige, income, and coolness. They may be shocked when the public suddenly turns against them and paints them as evil mad scientists consumed by hubris and misanthropy. In other words, the pool of AI talent is highly vulnerable to stigmatization, and has few defenses against it. Faced with a choice between staying in a highly stigmatized field (AI) versus switching to another highly-paid, intellectually engaging computer science field that is not highly stigmatized (e.g. gaming, cybersecurity, crypto), most AI researchers may jump ship and leave AI.</p><p>&nbsp;</p><p><strong>Chokepoint 2: AI Funding</strong></p><p>A strong enough anti-AI backlash would lead to AI funding drying up. Investors have become quite sensitive to \u2018ESG criteria\u2019 concerning environmental, social, and governance issues. If AI becomes morally stigmatized, ESG criteria could quickly and easily include AI as a disqualifying taboo. Any company involved in AI would receive low ESG scores, and would attract less \u2018ethical investment\u2019.&nbsp;</p><p>Apart from formal ESG criteria, individual and institutional investors tend to avoid companies widely perceived as reckless, evil, and inhumane. Many investors already avoid companies involved in weapons, alcohol, tobacco, porn, or gambling. If AI becomes seen as a horrifying new weapon, an addictive entertainment, and/or an insanely risky species-level gamble, it would combine all the worst evils of these already-stigmatized industries.&nbsp;</p><p>Investors may aspire to be rational maximizers of risk-adjusted returns. But investors are also social primates, subject to the same social and moral pressures that shape human behavior in every other domain of life. High Net Worth Individuals (\u2018rich people\u2019) often set up family offices to handle their investments, assets, and trusts for their kids and grand-kids. These family offices are specifically designed to take a long-termist, multi-generational perspective on the preservation and enhancement of dynastic wealth and power. That long-termist perspective naturally leads to a concern about multi-decade technological changes, geopolitical risks, global catastrophic risks, and existential risks. If AI becomes morally stigmatized as a major existential risk, family offices and their investment professionals will not want to deploy their capital in AI companies that could lead the rich people\u2019s kids and grand-kids not to die out before the end of the 21st century.</p><p>The investment world, like every human world, is prone to moral fads and fashions. Some companies and industry sectors become viewed as morally righteous, saintly, and inspiring; others become viewed as morally disgusting, sinful, and degrading. The psychology of moral disgust runs on the logic of contagion: anything in, around, or near a morally stigmatized activity becomes morally stigmatized by proxy. This means that if a large publicly traded corporation such as Microsoft or Google happens to include a much smaller organization (such as OpenAI or DeepMind) that becomes stigmatized, the large corporation also becomes morally stigmatized. Fewer people want to invest in it. They don\u2019t want their portfolio contaminated by the second-hand evil. As fewer investors are buying and more are selling, the share price falls. As the share price falls, other investors see the writing on the wall, and panic-sell. Hedge funds start aggressively shorting the stock. Soon the corporations face a dilemma: either they shut down or sell off the tainted AI organization poisoning their shareholder value from within, or they continue seeing their share price fall off a cliff \u2013 until they get acquired in a hostile takeover by new investors who are willing to cut the AI cancer out of the corporation, to save the rest of the company.&nbsp;</p><p>A few anti-ethical investors might see AI as a clever contrarian play, and might think AI company stocks are temptingly under-valued, and will become great investments after the moral stigma fades. But the stigma might not fade, and they may be left facing huge capital losses.</p><p>&nbsp;</p><p><strong>Chokepoint 3: Suppliers</strong></p><p>Moral contagion flows out in all directions. If AI starts to be seen as evil, any other organization that does business with AI researchers or AI companies will be seen as evil, or at least evil-adjacent. They will be stigmatized by association, as often happens in \u2018cancel culture\u2019. AI research depends on all kinds of suppliers of goods and services, utilities, computational infrastructure, and business infrastructure.&nbsp;</p><p>A sort of \u2018ethical back-propagation\u2019 would happen, where the moral stigma of AI would propagate backwards along the supply chain, tainting every person and company that provides essential goods and services to AI research.</p><p>In response, every supplier who is sensitive to the anti-AI backlash may withdraw their support from AI research groups. This may include everyone supplying GPU hardware, software, cloud computing resources, office space, legal services, accounting services, banking services, and corporate recruiting services. AI businesses may find that no reputable lawyers, bookkeepers, banks, or headhunters are willing to work with them. At a more mundane level, AI groups may find that they cannot find reputable businesses willing to supply them with tech support, back-office staff, office temps, caterers, drivers, janitors, or security staff. If some companies are not willing to do business with cannabis shops, porn producers, drug gangs, arms dealers, racketeers, human traffickers, or other stigmatized forms of economic activity, and if AI becomes stigmatized to a similar level, AI research will be handicapped, and will slow down.</p><p>The supplier issue could also affect AI researchers in their personal lives. If AI is widely seen as a work of reckless, hubristic evil, AI researchers may find that landlords are not willing to rent to them, coop boards are not willing to let them buy condos, and daycare centers and private schools are not willing to care for their kids. Bodyguards and police may think they\u2019re too disgusting to protect. Therapists may advise them to \u2018seek help elsewhere\u2019. They may even find spiritual services getting choked off, as their priest, pastor, or rabbi shun them for the sinful way they make a living.&nbsp;</p><p>&nbsp;</p><p><strong>Chokepoint 4: Laws and regulations</strong></p><p>Informal moral stigmatization often leads to formal government regulations and laws governing new activities and technologies. Indeed, it\u2019s often difficult to coordinate bipartisan support for new regulations and laws constraining something unless there is already a foundation of public stigmatization against that thing. Once the horrors of chemical weapons were witnessed in World War 1, and the public viewed mustard gas and other agents as morally outrageous, it was fairly easy to develop international bans on chemical weapons. Once human cloning became morally stigmatized in the 1990s, it was fairly easy to implement government bans and scientific norms against human cloning. Conversely, it\u2019s quite difficult to sustain regulations and laws against something if the moral stigma against the thing erodes \u2013 as in the case of cannabis use gradually becoming destigmatized in the US since the 1960s, and legalization of recreational cannabis following in many states. Thus, moral stigma and government regulation often have mutually reinforcing functions.</p><p>In the case of AI, if an anti-AI backlash was sufficiently global in scale, and became a major focus of public concern in both the US and China, it may be much easier to develop international agreements to pause, constrain, or ban further AGI research. With global moral stigmatization of AI, global regulation of AI becomes feasible. Without global moral stigmatization of AI, global regulation of AI is probably impossible. Yet much of the work on AI governance seems to have ignored the role of informal moral stigmatization in creating, energizing, and sustaining formal international agreements.</p><p>If an anti-AI backlash gets formalized into strong laws and regulations against AGI development, leading governments could make it prohibitively difficult, costly, and risky to develop AGI. This doesn\u2019t necessarily require a global totalitarian government panopticon monitoring all computer research. Instead, the moral stigmatization automatically imposes the panopticon. If most people in the world agree that AGI development is evil, they will be motivated to monitor their friends, family, colleagues, neighbors, and everybody else who might be involved in AI. They become the eyes and ears ensuring compliance. They can report evil-doers (AGI developers) to the relevant authorities \u2013 just as they would be motivated to report human traffickers or terrorists. And, unlike traffickers and terrorists, AI researchers are unlikely to have the capacity or willingness to use violence to deter whistle-blowers from whistle-blowing.&nbsp;</p><p>Laws and regulations by themselves would not be enough to significantly slow down AGI development. Bad actors would always be motivated to evade detection and accountability. However, it\u2019s a lot harder to evade detection if there is a global moral stigma against AGI development, with strong public buy-in. From the public\u2019s point of view, laws and regulations are simply ways to articulate, formalize, and implement moral stigmas that are already widely accepted in public discourse. In short, the public has already figured out what\u2019s evil, and they just want government to use its monopoly on the legitimate use of force to deter and punish what\u2019s evil. Thus, moral stigmatization super-charges the effectiveness of any formal laws and regulations around AI.</p><p>Often, if some activity becomes sufficiently stigmatized, regulators and law enforcement can apply existing laws in highly targeted ways to deter the activity. For example, laws against reckless endangerment and public endangerment could be applied to prosecute AGI research \u2013 if there was sufficient public and institutional belief that AGI imposes existential risks on citizens without their consent. The FBI could switch its focus from \u2018white supremacy as the leading domestic terrorist threat\u2019 to \u2018AGI research as the leading domestic terrorist threat\u2019 \u2013 and investigate and prosecute AI researchers accordingly. Note that government regulators and law enforcement agencies are often motivated to find and capitalize on any new threats that the public perceives. This provides pretexts for increasing their budgets, staff, and powers. If an anti-AI backlash becomes popular, many government workers will see this as a great opportunity to increase their status and power. Fighting against something widely considered an existential threat to humanity would sound like a pretty cool mission to a lot of FBI agents (in the US) or Ministry of State Security agents (in China). Thus, moral stigmatization of AI could lead quite quickly and directly to government investigations, audits, litigation, and prosecution of AI researchers and companies. Result: AGI development is slowed or stopped.</p><p>&nbsp;</p><p><strong>Conclusion</strong></p><p>The social-psychological processes of moral stigmatization have evolved genetically and culturally over thousands of generations. Moral stigma plays crucial roles in solving group coordination problems, enforcing social norms, punishing anti-social behavior, and minimizing existential threats to groups. Stigmatization is both a deep human instinct and a powerful cultural tradition. It can solve problems that can\u2019t be solved in any other way. This may include solving the problem of delaying AGI development until we have a better idea whether AI alignment is possible at all, and if it is possible, how to achieve it.&nbsp;</p>", "user": {"username": "geoffreymiller"}}, {"_id": "rXm5xGEGyvfBPAMF9", "title": "Beyond Humans: Why All Sentient Beings Matter in Existential Risk", "postedAt": "2023-05-31T21:21:18.261Z", "htmlBody": "<p><strong>TLDR: </strong>When discussing existential risks, the most common focus is on humanity's extinction. In this post, I use arguments based on sentientism and moral uncertainty to demonstrate that this sole focus on humans is unwarranted; we should consider <i>all </i>sentient life when talking about existential risks.</p><h2>0 Preface</h2><p>This essay is an entry to question 2 of the <a href=\"https://www.openphilanthropy.org/open-philanthropy-ai-worldviews-contest/\">Open Philanthropy AI Worldviews Contest</a>:</p><blockquote><p>Conditional on AGI being developed by 2070, what is the probability that humanity will suffer an <a href=\"https://forum.effectivealtruism.org/topics/existential-catastrophe-1\"><u>existential catastrophe</u></a> due to loss of control over an AGI system?</p></blockquote><p>Here, I do not directly answer the question, but I address the meaning of the term 'existential catastrophe'. The question points readers to an explanation of three different types of existential catastrophes: human extinction, unrecoverable civilizational collapse, and unrecoverable dystopia. The first two types are completely focused on humans, and the third type implies a focus on humans. At the very least, none of the three types mention sentient beings at all, which I will argue is problematic.</p><h2>1 Introduction</h2><p>To mitigate the existential risk from AI, it is likely useful to have a clear, robust, and inclusive moral framework to assess what is important. Better recognition of what is important can afford increased protection and flourishing of morally valuable beings. More concretely, it might improve collaboration between people due to a clearer shared goal, and it might make it easier to find, notice, and seize risk-reducing opportunities.<br>When people discuss existential risk, they commonly define them as risks that cause the loss of humanity\u2019s longterm potential (CSER, 2023; Ord, 2020). <strong>In this paper, I argue for extending the definition of existential risk to incorporate all sentient life.&nbsp;</strong></p><p>I focus on existential risk from AI, but arguments similar to the ones I raise here can be applied to other existential risk areas as well. Furthermore, throughout this paper, I roughly equate sentience to the capacity to suffer, and I assume that all that is sentient is alive, but not all that is alive is necessarily sentient.&nbsp;</p><p>The structure of the essay is as follows: To argue why all sentient life should be morally considered with existential risk, one must first thoroughly understand the current definitions of existential risk, which is the topic of the first section. Subsequently, I argue in Section 3 why all sentient life deserves moral consideration from a hedonistic utilitarian perspective. I will not substantially defend this hedonistic utilitarian perspective; I build on the book The Point of View of the Universe by de Lazari-Radek and Singer (2014). Even after presenting these sentientist arguments in this section, it is possible that sentient life must <i>not </i>be included in the moral framework of AI; some uncertainty remains on the correctness of these arguments. Therefore, I thoroughly discuss moral uncertainty in Section 4. Finally, I present the conclusion in Section 5.</p><h2>2 An overview of existential risk definitions&nbsp;</h2><p>Before I dive into the differences in existential risk definitions, I address some broad consensus in the research field. To my knowledge, people agree that existential risk concerns the probability of an existential catastrophe occurring. I use existential risk in the same way. This paper focuses on what constitutes an existential catastrophe, and not on the probability of an existential catastrophe occurring.&nbsp;</p><p>Some authors on existential risk do not exclusively focus on humans. Bostrom (2013) states that \u201cAn existential risk is one that threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\u201d. Additionally, Cotton-Barratt and Ord (2015) define an existential catastrophe as \u201can event which causes the loss of a large fraction of expected value\u201d. The crucial part of these two definitions is in the terms \u2018desirable future development\u2019 and \u2018value\u2019, respectively. These parts of the definitions can be whatever is thought to be valuable, as remarked by Cotton-Barratt and Ord (2015).<br>Nonetheless, I argue that both of these definitions are unsatisfactory.&nbsp;<br>First, in Bostrom\u2019s definition, intelligent life is not a clearly defined subset of possible beings. In his subsequent description of existential risk, Bostrom mentions only humans as a form of 'intelligent life'. Regardless of what intelligent life exactly means, I will argue in Section 3 that this focus on both intelligent and Earth-origination life is not inclusive enough.&nbsp;<br>Second, Cotton-Barratt and Ord (2015) more closely match my view with their definition, as the meaning of the word 'value' is very broad, and few perspectives can conflict with such a broad statement. However, in their paper they fail to specify what they mean with \u2018value\u2019, which clouds the moral framework.&nbsp;</p><h2>3 The moral value of all sentient life&nbsp;</h2><p>Why would non-human life matter morally? I argue that the capacity of suffering is necessary <i>and </i>sufficient for an entity being worthy of moral consideration. Moral consideration tightly relates to moral patienthood, which entails that one matters morally in and of oneself (Muller, 2020). This philosophical perspective is called \u2018sentientism\u2019, see Jamieson (2008) for further reference.&nbsp;<br>To argue why sentience is a <i>necessary </i>condition for moral patienthood, Singer (1996) explains in an intuitive sense that it is nonsensical to talk about the interests of a stone being kicked; the stone does not feel anything (in most philosophical views). When a human is kicked, it matters only because humans are sentient. If a human had no subjective experience at all, it would not feel pain from the kick, it would not feel shame of being kicked, it would not feel anything. Obviously, there are many effects of kicking a hypothetical non-sentient human. Examples are a toughening culture, friends and family being shocked, psychological damage to the kicker themself, etc. However, none of these reasons are directly related to the non-sentient human. In fact, all of them relate to the suffering of some sentient being.&nbsp;<br>Sentience is also a <i>sufficient </i>condition for moral patienthood. Singer (1996) argues that all sentient humans are moral patients. To determine which other beings deserve moral patienthood, one must find the sufficient condition(s) of sentient humans which qualify them for moral consideration. Singer argues that there is no such condition that can differentiate between humans and other animals; there is overlap between humans and non-humans with any reasonable distinction one can draw between them. For example, one could argue that intelligence is what separates humans from other animals. However, due to brain damage, some humans cannot talk, recognize people, etc. In some relevant sense, they have comparable intelligence to other animals. Does these humans\u2019 reduced intelligence imply that they do not deserve moral patienthood? Singer does not think so, and neither do I. The only clear distinction one could draw is to which species some being belongs to, but this position is untenable. Horta (2010) calls the unjustified focus on humans as opposed to other sentient life 'speciesism'. Merely <i>belonging </i>to a group is an invalid reason for a difference in moral consideration, just as other types of discrimination such as sexism and racism. Importantly, if sentience makes humans moral patients, then this property must also apply to other sentient beings. However, my argument lacks any relevance if there are no sentient beings besides humans. But, humans do not seem to be the only sentient beings in existence, see e.g. (Broom, 2014; Safina, 2015). Of course, one can probably never be sure of any other entity than yourself having subjective experience, even not for other humans. These are good arguments, but discussing their validity is out of the scope of this paper. I merely note that what matters in hedonistic utilitarian decision-making is the <i>probability </i>of something being sentient; although one may not be a 100 per cent certain of people being sentient but 99 per cent sure, one still treats them nearly the same because the probabilities are so close that they hardly affect the expected utility. The same argument applies to other sentient beings; one might believe with roughly 90 per cent certainty that dolphins are sentient, and therefore they deserve significant moral consideration.&nbsp;<br>To be clear, it is not strictly relevant to my argument which beings are sentient, and which are not. If it turns out that dolphins are not sentient, then they do not need to be considered morally. On the other hand, if some AI systems or an alien species are sentient, they are also worthy of moral consideration. Also, the intensity of sentience is relevant for determining the expected utility, but it is not relevant to this paper.</p><p><br>With sentience being a necessary and sufficient characteristic for moral consideration, life originating from Earth unnecessarily and unfairly excludes sentient alien life. Imagine an advanced alien civilization swiftly and harmlessly replacing all \u2018intelligent\u2019 life on and from Earth. The replacing alien civilization is very similar to human civilization; they can love, they can enjoy food and music, and they push the frontier of knowledge with philosophy and science. Additionally, they bring their own plants, animals, fungi, or anything else one could classify as intelligent. They do all these things, and all sentient beings are on average substantially happier than our human society. This thought experiment illustrates that this situation is not bad from a perspective of the universe, because sentient life is not extinct but thriving. No existential catastrophe has occurred from a hedonistic utilitarian standpoint, and therefore sentient alien life must be considered in the definition of an existential catastrophe.<br>Similarly, imagine a non-sentient AI system that controls a future universe where nothing is sentient. It runs and maintains the data centre with the hardware to run itself on, it advances some scientific fields, and it has the potential to become much more intelligent and to create copies of itself. Further assume that the AI system and its successors do not and will not have any positive or negative affect. Apart from that, there is nothing in the world. It is, at least to reasonable standards, intelligent, and does not reduce the potential for intelligent life in the future. Therefore, no existential catastrophe has happened according to the definition in Bostrom (2013). However, I argue that this <i>does </i>constitute an existential catastrophe because the necessary condition of sentience is not met. There are no experiences, it is a morally worthless universe containing intelligent rocks.&nbsp;</p><p>A counterargument can be made here. One can argue that intelligence does not exist without sentience, for example, because subjective experience is a necessary side effect of your body processing information (Robinson, 2019). If this is true, it would indeed completely undermine my argument. Refuting this counterargument is outside the scope of this paper. However, such counterarguments are a reason for uncertainty in my own arguments, and thus provide a good reason to investigate my arguments from a moral uncertainty perspective.</p><h2>4 Moral uncertainty</h2><p>&nbsp;There is a chance that sentientism is incorrect, perhaps the current ideas on sentience are off, or there is a reasoning flaw in the argument that I do not currently see. This incorrectness is not necessarily falsifiable. Moral uncertainty is the study of what to do morally, despite this uncertainty (Lockhart, 2000; MacAskill et al., 2020).&nbsp;</p><p>Let us further examine the uncertainty in moral theories. Bourget and Chalmers (2021) demonstrated moral uncertainty with a survey of philosophers. Philosophers were asked to give their opinion on philosophical questions, including their stance on normative ethics. The participants were asked to vote on a certain position they agreed with or leaned towards. The participants could vote for multiple answers, which some did, and the summed number of votes totalled 2050.&nbsp;<br>The answers to the question: \u201cNormative ethics: virtue ethics, deontology, or consequentialism?\u201d were as follows: 27.2% of the votes were for deontology, 26.0% for consequentialism, 31.4% for virtue ethics, and 15.4% for the option \u2018other\u2019. Interestingly, the philosophers voting for multiple position indicate the commensurability of different stances on normative ethics. For example, if one votes for deontology and consequentialism, there must be some way, if the philosopher is not contradicting themself, to bridge the two perspectives. Nonetheless, as some respondents accept or lean towards some positions and other respondents do not, incommensurability remains. Therefore, some people are necessarily wrong, and we do not know for sure who they are. Thus, Bourget and Chalmers (2021) indirectly illustrate that my sentientist argument in Section 3 might also be wrong.&nbsp;</p><p>The core argument of this section is that despite the possibility of sentientism and hedonistic utilitarianism being wrong, existential risk should still concern all sentient life. To aid decision-making under uncertainty, Lockhart (2000) uses the maximize-expected-moral-rightness criterion. This criterion is determined by multiplying the probability of a moral statement being right with how morally right this decision is. The credence for a certain moral statement is determined by those who make the decision, ideally with the clearest thinking and with perfect information.&nbsp;<br>For example, we have a moral statement A with two contrasting opinions, namely X and Y, to which I assign credences of 0.9 and 0.1 respectively. See Table 1 for an overview of this example. If we act according to A being true, X holds that the moral rightness is 0.5 while Y assigns a moral rightness of 0.6. In this case, the value of the criterion is (0.9 \u00d7 0.5) + (0.1 \u00d7 0.6) = 0.51. On the other hand, if we act according to A being false, X holds that the moral rightness is 0.2 while Y assigns a moral rightness of 0.8. In this case, the value of the criterion is (0.9 \u00d7 0.2) + (0.1 \u00d7 0.8) = 0.26. Finally, the maximize-expected-moral-rightness criterion holds that one should act according to A being true, as we get a higher score for A being true than for A being false.&nbsp;</p><figure class=\"table\"><table><thead><tr><th colspan=\"3\">Table 1: Example of applying the maximize-expected-moral-rightness criterion</th></tr></thead><tbody><tr><td>&nbsp;</td><td>Opinion X (credence of 0.9)</td><td>Opinion Y (credence of 0.1)</td></tr><tr><td>Moral statement A</td><td>0.5</td><td>0.6</td></tr><tr><td>Moral statement not A</td><td>0.2</td><td>0.8</td></tr></tbody></table></figure><p>Let us apply the maximize-expected-moral-rightness criterion to the case at hand. I will not calculate the criterion score for each plausible moral opinion. Rather, I outline some views which illustrate the general mechanics relevant to our moral statement. See Table 2 for an overview. The moral statement is whether we include all sentient life in the moral framework of AI. Now, the opinions on this moral statement are provided by theories such as sentientism and contractarianism. Contractarianism holds that moral norms are determined through mutual agreement or a contract (Cudd &amp; Eftekhari, 2021). For clarity, I only picked one other theory, and I picked contractarianism because it is a clearly opposing view.&nbsp;</p><figure class=\"table\"><table><thead><tr><th colspan=\"3\">Table 2: Applying the simplified maximize-expected-moral-rightness criterion</th></tr></thead><tbody><tr><td>&nbsp;</td><td>Sentientism&nbsp;</td><td>Contractarianism&nbsp;</td></tr><tr><td>Include all sentient life</td><td>Really good</td><td>Possibly bad</td></tr><tr><td>Not include all sentient life</td><td>Really bad</td><td>Somewhat good</td></tr></tbody></table></figure><p>According to sentientist arguments raised in the previous section, it would be terrible to not consider all sentient life morally. Therefore, sentientists assign a high degree of moral rightness to the moral statement being true, and low if not. According to contractarianism, it is plausible that at least some non-human sentient beings are not rational agents, and therefore not all sentient life should be given moral patienthood (Rowlands, 1997). Contractarians would give a lower moral rightness score to the moral statement being true because many additional moral patients might disadvantage the current moral patient\u2019s position. It might be harder to care for many more, reducing the quality of care of the original moral patients. But this would not be as extreme as the sentientists\u2019 view, because all rational agents would still be considered as moral patients.&nbsp;<br>Clearly, there is an asymmetry of the stakes; the moral issue at hand is much more important for perspectives in line with sentientism than for perspectives in line with contractarianism. In other words, the moral rightness pushes the score for maximize-expected-moral-rightness criterion towards including all sentient life. Now what remains is the need to assess the credences for both perspectives. I assign a substantially higher credence to the sentientist perspective than the contractarian perspective. However, the strength of my argument is that one could assign equal credences to both perspectives or even lean towards the contractarian perspective, and the maximize-expected-moral-rightness criterion still holds that one should include all sentient life in the moral framework of AI.&nbsp;</p><p>There are some flaws in the maximize-expected-moral-rightness methodology, which weaken but do not invalidate my argument. I discuss the three most relevant flaws, and afterwards consider their effect on my argument. These flaws are best described in the book Moral Uncertainty by MacAskill et al. (2020).&nbsp;<br>First, the issue of interaction effects. Interaction effects concern how different moral issues can affect each other\u2019s credence and moral rightness. An example of an interaction effect is the treatment of sentient beings other than yourself; I raised this moral issue in the previous section. If no other being than you, a human, is sentient, the inclusion of all sentient life is irrelevant because all sentient life has already been included. This argument seems unlikely to be true, but it still could be true, which requires assigning at least some credence to this view. Interaction effects like these complicate the moral decision-making process I have illustrated before.&nbsp;<br>Second, intertheoretic comparisons concern the issue that theories might be hard to compare. For example, utilitarianism and deontology seem hard to compare because utilitarians can use the expected value of an action to determine a moral rightness score while deontology tends to be binary; one might have a rule that one should not lie, but the degree of moral rightness is hard to determine.<br>Third, there is the problem of infinite regress. The foundational principle of the framework of moral uncertainty is that one cannot be completely sure that one\u2019s preferred moral theory is correct. To account for this uncertainty, one can use moral uncertainty methods like the maximize-expected- moral-rightness criterion. However, this method might be wrong as well. One could have made a reasoning error, as is common throughout history. So, one should account for this uncertainty. This uncertainty pertains throughout every layer of fixes. Therefore, one can never be completely sure of the theory or method one uses, leading to infinite regress.&nbsp;</p><p>How do these flaws affect my argument? MacAskill et al. (2020) correctly argue that the interaction effect complicates the argument I have made. However, I have discussed some of these interaction effects in Section 3, and there do not seem to be interaction effects that flip the outcome; these complications should merely decrease the strength of my argument. Similarly, the issue of intertheoretic comparisons complicates calculating the maximize-expected-moral-rightness criterion. Still, the effect on the thesis of this essay seems minimal; the argument of the asymmetry of the stakes still applies, but the amount of asymmetry might be harder to establish. Lastly, infinite regress is challenging, but this problem applies to any theory of moral decision-making processes (MacAskill et al., 2020). Generally, an unsolved problem in a methodology decreases one\u2019s confidence in arguments resulting from the methodology, and this applies here as well. To sum up, methodological problems decrease the strength of my argument, but do not invalidate them.&nbsp;</p><p>I finish this section with a recommendation by MacAskill (2022). He recommends that given moral uncertainty, a generally good approach is to keep options open. We have seen tremendous moral change throughout history, from abolitionism to women\u2019s rights, and therefore we should expect new moral insights in the future. Because of this argument, the moral framework with which to steer AI should be flexible to include future insights. This is also in line with the two definitions by Bostrom and Ord in Section 2, where they use \u2018desirable future development\u2019 and \u2018value\u2019 to make their definition future-proof.&nbsp;</p><h2>5 Conclusion&nbsp;</h2><p>I have argued that all sentient life should be incorporated into the definition of existential risk.&nbsp;</p><p>I started my argument by examining how existential risk is currently defined, where I observed that existential risk is focused on humans. In the following section, I argued sentience to be a necessary and sufficient condition for moral consideration for humans. Naturally, if other beings satisfy the condition of sentience, then they should also be morally considered. I showed that Earth-originating and intelligent are still lacking conditions to define who should be considered in an existential catastrophe.</p><p>Although this sentientist argument is strong, one must also consider the possibility it is wrong. After all, many people have extensively thought about these issues and still disagree. The framework of moral uncertainty provides tools to make decisions despite this uncertainty, one of which is the maximize-expected-moral-rightness criterion. Using this criterion, I showed that one should want to morally consider all sentient life due to the asymmetry of the stakes, even if one leans towards the perspective that not all sentient life deserves moral consideration. There are some methodological issues with the maximize-expected-moral-rightness criterion. These issues are described by interaction effects, intertheoretic comparisons, and infinite regress. I showed that these issues do weaken my argument, but there was no reason to invalidate the use and results of the criterion. Lastly, I noted that we should expect new moral insights in the future, as there have been many moral insights in the past. Therefore, the moral framework of existential risk should be future-proof.&nbsp;</p><p>To conclude, one must include all sentient life to make the moral framework of AI more robust. The robustness of this moral framework could help in steering the powerful technology of AI into the right direction, creating a better universe for all.</p><hr><h2>Acknowledgements&nbsp;</h2><p>I want to thank Martijn Klop, Freek van der Weij, Nadja Flechner, and Nathalie Kirch for their feedback throughout various stages of the writing process.&nbsp;</p><h2>References&nbsp;</h2><ul><li>Bostrom, N. (2013). Existential risk prevention as global priority.<i> Global Policy, 4</i> (1), 15\u201331.&nbsp;</li><li>Chapman; Hall/CRC. Bourget, D., &amp; Chalmers, D. J. (2021). Philosophers on philosophy: The 2020 philpapers survey. <a href=\"https://philpapers.org/rec/BOUPOP-3\"><i>https://philpapers.org/rec/BOUPOP-3</i></a><i>. &nbsp;&nbsp;</i></li><li>Broom, D. M. (2014). Sentience and animal welfare. <i>Cabi.&nbsp;</i></li><li>Cotton-Barratt, O., &amp; Ord, T. (2015). Existential risk and existential hope: Definitions. <i>Future of Humanity Institute: Technical Report, 1 (2015), 78.&nbsp;</i></li><li>CSER. (2023). Centre for the study of existential risk. <a href=\"https://www.cser.ac.uk/\"><i>https://www.cser.ac.uk/</i></a></li><li>Cudd, A., &amp; Eftekhari, S. (2021). Contractarianism. <i>In E. N. Zalta (Ed.), The Stanford encyclopedia of philosophy (Winter 2021). Metaphysics Research Lab, Stanford University.&nbsp;</i></li><li>de Lazari-Radek, K., &amp; Singer, P. (2014). The point of view of the universe: Sidgwick and contemporary ethics. <i>Oxford University Press.</i>&nbsp;</li><li>Horta, O. (2010). What is speciesism?<i> Journal of agricultural and environmental ethics, 23, 243\u2013 266.&nbsp;</i></li><li>Jamieson, D. (2008). Sentientism. <i>In A companion to environmental philosophy (pp. 192\u2013203).&nbsp;</i></li><li>John Wiley &amp; Sons. Lockhart, T. (2000). Moral uncertainty and its consequences. <i>Oxford University Press.&nbsp;</i></li><li>MacAskill, W. (2022). What we owe the future. <i>Basic books.&nbsp;</i></li><li>MacAskill, W., Bykvist, K., &amp; Ord, T. (2020). Moral uncertainty. <i>Oxford University Press.&nbsp;</i></li><li>M\u00fcller, V. C. (2020). Ethics of artificial intelligence and robotics. <a href=\"https://plato.stanford.edu/entries/ethics-ai/\"><i>https://plato.stanford.edu/entries/ethics-ai/</i></a><i>&nbsp;</i></li><li>Ord, T. (2020). The Precipice: Existential Risk and the Future of Humanity <i>Bloomsbury Publishing.&nbsp;</i></li><li>Robinson, W. (2019). Epiphenomenalism <a href=\"https://plato.stanford.edu/entries/epiphenomenalism/\"><i>https://plato.stanford.edu/entries/epiphenomenalism/</i></a><i>&nbsp;</i></li><li>Rowlands, M. (1997). Contractarianism and animal rights.<i> Journal of applied philosophy, 14 (3), 235\u2013247.&nbsp;</i></li><li>Safina, C. (2015). Beyond words: What animals think and feel. <i>Macmillan.</i></li><li>Singer, P. (1996). Animal liberation. <i>Springer.&nbsp;</i></li></ul>", "user": {"username": "Teun_Van_Der_Weij"}}, {"_id": "o9MP7tzJkvZKQzJns", "title": "Sam Altman gives me bad vibes", "postedAt": "2023-05-31T17:15:00.192Z", "htmlBody": "<p>I don\u2019t have anything concrete on any of these things, there\u2019s just enough smoke that it makes me nervous. This matters because it affects to what degree you think he\u2019s someone with integrity when assessing his public pronouncements on AI safety. (In particular, if he is the cynic I suspect he is, we should assume he\u2019s using regulation as a cynical tool rather than an honest approach).</p><p>Caveat emptor. Most of this is based on rumors and inference, but I haven\u2019t seen it spelt out publicly anywhere so it feels worth sharing.</p><h3>SVB Equity loans</h3><p>Sam was one of the biggest beneficiaries of sweetheart deals from SVB. As a high profile VC he was obviously an attractive target for SVB, from what I hear he took full advantage of this.</p><h3>World Coin</h3><p>Iris scanning crypto project. Yikes.</p><h3>Helion - Microsoft deal</h3><p>This deal makes no sense to me. Why would Microsoft want to buy power years in the future from a start-up company? (Other than the fact that keeping a large investor in that company on-side)</p><h3>His personal investments into OpenAI customers</h3><p>Sam Altman uses his inside knowledge of OpenAI usage to invest in OpenAI clients. This conflicts him with OpenAI. (He can benefit if either OpenAI&nbsp;<i>or</i> its users capture the value from the base models where the firm can\u2019t. Or it makes OpenAI take suboptimal decisions on who gets access to their models).&nbsp;</p>", "user": {"username": "throwaway790"}}, {"_id": "q66RuzvRbRTAQMqCt", "title": "New Faunalytics Study on Animal Agriculture in Climate Change Media Coverage", "postedAt": "2023-05-31T15:35:56.059Z", "htmlBody": "<p>Nonprofit research organization&nbsp;<a href=\"http://www.faunalytics.org\"><u>Faunalytics</u></a> has released a new study in partnership with Sentient Media:&nbsp;<a href=\"https://faunalytics.org/animal-ag-in-climate-media\"><i><u>Animal Agriculture Is The Missing Piece In Climate Change Media Coverage</u></i></a>. We analyzed recent climate articles from top U.S. media outlets to determine how often the media makes the connection between animal agriculture and climate change when reporting on climate issues, and how reporting on animal agriculture in relation to climate change misses the mark.</p><p><strong>Key Findings:</strong></p><ol><li><strong>Only 7% of climate articles mentioned animal agriculture and they rarely discussed its impact on climate change.</strong> Across the 1,000 articles we examined, only a handful of stories reported in depth on the connection between consuming animal products and climate change. Most articles that mentioned animal agriculture failed to discuss the emissions and environmental degradation caused by the industry, let alone the importance of reducing meat consumption or switching to a plant-based diet to fight climate change. When diets were discussed, the effectiveness of plant-based diets was sometimes downplayed or, more often than not, presented almost as an afterthought rather than a legitimate strategy to mitigate climate change.</li><li><strong>The animal agriculture industry is often portrayed as a victim of climate change rather than a significant cause.&nbsp;</strong>Our qualitative analysis revealed that instead of citing animal agriculture\u2019s negative environmental impact, climate articles that discussed the industry in any depth generally focused on how climate change is impacting animal agriculture. Multiple articles discussed how flooding, drought, and heatwaves have caused livestock losses both in the U.S. and abroad, and how this affects the livelihoods of farmers, while failing to mention the role that the animal agriculture industry plays in the climate crisis.</li><li><strong>There are countless missed opportunities to discuss animal agriculture in the context of climate change.</strong> Energy, transportation, emissions, and fossil fuels were given the spotlight in climate coverage: These topics were mentioned in up to 68% of climate articles but were rarely tied to animal agriculture, despite the connections and parallels between them. For instance, transportation is responsible for roughly the same amount of emissions as the animal agriculture industry and is part of that industry, yet just 8% of climate articles mentioning transportation also referenced animal agriculture.</li><li><strong>Impactful subsectors of animal agriculture are also not given enough attention by the media.&nbsp;</strong>Cattle farming is responsible for about 62% of animal agriculture emissions (<a href=\"https://foodandagricultureorganization.shinyapps.io/GLEAMV3_Public/\"><u>FAO, 2022</u></a>), yet cows were mentioned in just 30% of animal agriculture articles. Similarly, methane came up in 22% of animal agriculture articles despite accounting for 54% of the sector\u2019s emissions.&nbsp;</li></ol><p><strong>Background</strong></p><p>For many years now, climate researchers have been warning that the world can\u2019t meet its&nbsp;<a href=\"https://www.un.org/en/climatechange/paris-agreement\"><u>Paris Agreement</u></a> climate goals of limiting global warming to 1.5\u00b0C&nbsp;<a href=\"https://www.wri.org/insights/without-changing-diets-agriculture-alone-could-produce-enough-emissions-surpass-15degc\"><u>without reducing meat consumption</u></a>. Multiple studies have affirmed that&nbsp;<a href=\"https://thebreakthrough.org/issues/food-agriculture-environment/livestock-dont-contribute-14-5-of-global-greenhouse-gas-emissions\"><u>between 11.1% and 19.6%</u></a> of global emissions come from meat and dairy production, and&nbsp;<a href=\"https://sentientmedia.org/sustainable-diet/\"><u>leading global food and climate agencies</u></a> are also in agreement, recommending that people, particularly those in the Global North, reduce meat consumption in favor of a plant-rich diet.</p><p>The effects of animal agriculture on the environment and climate are&nbsp;<a href=\"https://climatenexus.org/climate-issues/food/animal-agricultures-impact-on-climate-change/\"><u>vast</u></a>: It is a leading cause of&nbsp;<a href=\"https://www.fao.org/newsroom/detail/cop26-agricultural-expansion-drives-almost-90-percent-of-global-deforestation/en\"><u>deforestation</u></a>, it\u2019s responsible for significant&nbsp;<a href=\"https://www.unep.org/news-and-stories/press-release/our-global-food-system-primary-driver-biodiversity-loss\"><u>biodiversity loss</u></a> and&nbsp;<a href=\"https://www.nrdc.org/stories/industrial-agricultural-pollution-101#animal\"><u>pollution</u></a>, and emits large amounts of greenhouse gases, particularly&nbsp;<a href=\"https://ourworldindata.org/emissions-by-sector#methane-ch4-emissions-by-sector\"><u>methane</u></a>. Methane alone is the cause of&nbsp;<a href=\"https://www.unep.org/facts-about-climate-emergency\"><u>over 25%</u></a> of global warming, for which reason reducing methane emissions is critical. If emissions continue as they are now, the food sector alone is enough to&nbsp;<a href=\"https://ourworldindata.org/food-emissions-carbon-budget\"><u>push global warming past that 1.5\u00b0C limit</u></a>, while just reducing meat consumption could get the world much closer to our emissions goal. In the&nbsp;<a href=\"https://ourworldindata.org/grapher/eat-lancet-diet-animal-products?country=EAT-Lancet~USA~ETH~IND~BRA~GBR~CHN\"><u>United States</u></a>, this reduction would mean that the average person would consume about 70% fewer animal products on a daily basis, with the greatest reductions coming from red meat and chicken\u201492% less red meat and 81% less chicken, according to EAT-Lancet Commission recommendations.</p><p>Despite the extensive research supporting the reduction of animal product consumption, there\u2019s long been a disconnect between what the research shows and what the public understands. According to a recent consumer study conducted by Purdue researchers: \u201cThe belief that \u2018eating less meat is better for the environment,\u2019 which is strongly supported by many climate and environmental researchers, is at an all-time low\u201d (<a href=\"https://ag.purdue.edu/cfdas/wp-content/uploads/2023/02/Report_202301-2.pdf\"><u>Lusk &amp; Polzin, 2023</u></a>). The reason for this disconnect is multifaceted, but at least one factor is the information the public receives regarding the connection between animal agriculture and climate change.</p><p>Given the role of the media in informing the public about important issues like climate change, this partner project between Faunalytics and Sentient Media sought to understand how the media communicates the environmental implications of animal agriculture to readers.&nbsp;</p><p><strong>Research Team</strong></p><p>The project\u2019s lead authors were Constanza Ar\u00e9valo (Faunalytics) for the quantitative analyses and Jenny Splitter (Sentient Media) for the qualitative findings. Dr. Jo Anderson (Faunalytics) reviewed and oversaw the work.</p><p><strong>Conclusions</strong></p><p><u>Not Enough Attention Is Given To Animal Agriculture\u2019s Role In The Climate Crisis</u></p><p>Although all news outlets covered animal agriculture to some extent, the vast majority of climate reporting included in this study\u201493%\u2014made no mention of animal agriculture. Even the small percentage of stories that did cover animal agriculture mostly failed to make the connection between meat consumption and rising climate emissions and environmental degradation. Most articles only briefly mentioned animal agriculture and, if discussed in greater detail, more often than not it was in terms of how climate change is affecting the animal agriculture industry rather than the other way around. In fact, of all the climate stories analyzed in this study, only a handful explicitly covered animal agriculture\u2019s effects on climate change.</p><p>In most stories that touch on animal agriculture, news outlets are missing a critical opportunity to inform readers about the impact of what they eat. The themes most covered by all news outlets were mining, manufacturing, and energy production, emissions, fossil fuels, and transportation, yet these also happened to be the themes that were least likely to be discussed alongside animal agriculture in climate articles. And it isn\u2019t due to a lack of relation between them \u2014 for instance, agriculture is the number one source of methane in the world, most of which comes from&nbsp;<a href=\"https://ourworldindata.org/emissions-by-sector#methane-ch4-emissions-by-sector\"><u>livestock production</u></a>, and it\u2019s estimated that 20% of animal agriculture emissions come from the use of fossil fuels along supply chains (<a href=\"https://www.fao.org/news/story/en/item/197623/icode/\"><u>FAO, 2013</u></a>). Furthermore, at a global scale, animal agriculture is responsible for a similar percentage of greenhouse gas emissions as the&nbsp;<a href=\"https://www.wri.org/insights/everything-you-need-know-about-fastest-growing-source-global-emissions-transport\"><u>transportation sector</u></a>, yet it receives far less coverage in the media.</p><p>Although research on this topic is lacking, previous research supports and expands on our finding that there is a tendency for the media to give little attention to how animal agriculture contributes to climate change.&nbsp;</p><p>In addition to finding low coverage of animal agriculture in climate media in the U.S. and United Kingdom, one study found that governments and the large-scale animal agriculture industry are not held as accountable as consumers. In other words, they found more mentions of the need for individual dietary change than to reform government policies or agricultural practices (<a href=\"https://faunalytics.org/uk-and-us-media-coverage-animal-ag-climate-change/\"><u>Kristiansen et al., 2020</u></a>).&nbsp;</p><p>Another study found that despite scientific consensus on the connection between animal agriculture and climate change, the media often treats it as a debate, presenting both \u201csides\u201d to an argument that doesn\u2019t really exist (<a href=\"https://faunalytics.org/the-both-sides-myth-of-diet-change/\"><u>Fry et al., 2022</u></a>). Consequently, there is evidence that the media is downplaying the role of animal agriculture even when it is discussed in relation to climate change. Research shows that false balance reporting\u2014when journalists present both sides of an issue, even when one side has greater evidence to back it up\u2014can cause people to doubt the scientific consensus on issues like climate change (<a href=\"https://news.northwestern.edu/stories/2022/07/false-balance-reporting-climate-change-crisis/\"><u>Imundo &amp; Rapp, 2022</u></a>), making this a particularly dangerous approach given the seriousness of the issue.</p><p><u>Misinformation And Missing Information In Climate Coverage</u></p><p>Overall, animal agriculture tended to be covered rather briefly, and almost always in the context of another cause of climate change, such as transportation or mining, manufacturing, and energy production, for example. In many of these stories, outlets covered animal agriculture as part of general agriculture or, in some cases, regenerative agriculture\u2014often in ways that included inaccuracies or missing key facts and context about emissions from meat. For instance, in the case of regenerative agriculture, the purpose is to mitigate climate change and environmental degradation. Despite the clear scientific evidence that most agricultural emissions come from livestock farming and that it has detrimental consequences on the environment, over half of regenerative agriculture articles mentioned livestock farming, often in the context of incorporating it into these \u201cclimate-smart\u201d practices, without presenting any data about the negative effects of animal agriculture.</p><p>In another missed opportunity, many articles brought up the effects of climate change on farmers around the world but failed to consider the global repercussions of U.S. consumption of animal products. For example, meat consumed in the U.S. is often imported, so an increase in demand for beef in the U.S. can result in an increase in&nbsp;<a href=\"https://www.vox.com/science-and-health/2022/10/19/23403330/amazon-rainforest-deforestation-cattle-laundering\"><u>deforestation in the Amazon</u></a> to make room for more cows, increasing emissions in South America and reducing a very important global carbon sink.</p><p><u>The Media\u2019s Role In Communicating Climate Change Information</u></p><p>A study by the&nbsp;<a href=\"https://reutersinstitute.politics.ox.ac.uk/digital-news-report/2022/how-people-access-and-think-about-climate-change-news\"><u>Reuters Institute (2022)</u></a> found that in the U.S., 24% of people pay attention to major news organizations for climate change news, though roughly the same percentage of people say they don\u2019t pay attention to climate change at all. As the researchers from the study acknowledge, polarized politics and media coverage play a role in \u201cdriving down interest in and attention to climate change as an issue.\u201d As we saw in this study\u2019s results, political leaning may also play a part in whether animal agriculture is brought up when communicating about climate change\u2014left-leaning media outlets tend to discuss animal agriculture more often than right-leaning ones. However, all news outlets, regardless of political leaning, failed to give enough attention to animal agriculture, let alone discuss its consequences on the environment in depth.</p><p>Evidence recently came to light about the meat industry\u2019s influence in&nbsp;<a href=\"https://qz.com/ipcc-report-on-climate-change-meat-industry-1850261179\"><u>blocking the Intergovernmental Panel on Climate Change (IPCC) from recommending plant-based diets</u></a> to fight climate change. With reports of global significance like this excluding the influence of animal agriculture from their narratives, along with the media failing to properly cover this issue, it\u2019s not surprising that&nbsp;<a href=\"https://madrebrava.org/insight/people-don-t-see-industrial-meat-as-a-key-cause-of-global-warming-poll\"><u>very few people</u></a> around the world are aware that animal agriculture is a leading cause of climate change. They instead think that other human-derived causes of climate change, like transportation, are of much greater concern.&nbsp;</p><p>Through their role in communicating important issues to the public, news outlets have the unique ability to bridge the gap between climate science and public knowledge. However, as this and other studies show, more needs to be done in terms of informing readers about how animal agriculture impacts the environment and the importance of shifting global diets to mitigate climate change.</p><p><br>&nbsp;</p>", "user": {"username": "JLRiedi"}}, {"_id": "p7qXjisiADiCBnofk", "title": "The EU AI Act needs a definition of high-risk foundation models to avoid regulatory overreach and backlash", "postedAt": "2023-05-31T15:34:00.878Z", "htmlBody": "<p><i>Disclaimer: Quickly written; I am not an expert in this legislature and am happy to be corrected if my interpretations are wrong.</i></p><p>The&nbsp;<a href=\"https://www.europarl.europa.eu/resources/library/media/20230516RES90302/20230516RES90302.pdf\"><u>current draft of the EU AI Act</u></a> seems problematic. Efforts to address risks from transformative AI are overshooting in a way that would severely hamper development and application of generative AI in the EU, and would lead to unclear legal situations for persons outside the EU that make generative AI system available (including simply uploading a free-to-use open source model). This is bad in two ways:</p><ol><li>Intrinsically: If the draft version would be enacted, it would lead to significant economic damage and public outrage in the EU, potentially even causing lasting damage to the EU as an institution.</li><li>Instrumentally, from an AI risk perspective: there will likely be a fierce backlash to the regulation as proposed. This risks over-correction or no regulation being enacted at all, and might decrease public trust in actors advocating for AI regulations.</li></ol><p>The problem is that regulations in the draft are broadly applied to \u2018foundation models\u2019, without regard to their level of capability, autonomy or risk. The following regulatory requirements could therefore, in my reading, apply even to models as trivial as GPT-J or T5. There is sufficient lack of clarity that it is possible that fine-tuning an existing model would constitute bringing a novel foundation model to market, subjecting the person doing the fine-tuning to the same obligations. Note that no profit-motive is needed to fall under these regulations.<br>&nbsp;</p><blockquote><p>Article 28b</p><p>Obligations of the provider of a foundation model</p><p>1. A provider of a foundation model shall, prior to making it available on the market or putting it into service, ensure that it is compliant with the requirements set out in this Article, regardless of whether it is provided as a standalone model or embedded in an AI system or a product, or provided under free and open source licences, as a service, as well as other distribution channels.</p><p>2. For the purpose of paragraph 1, the provider of a foundation model shall:</p><p>(a) demonstrate through appropriate design, testing and analysis that the identification, the reduction and mitigation of reasonably foreseeable risks to health, safety, fundamental rights, the environment and democracy and the rule of law prior and throughout development with appropriate methods such as with the involvement of independent experts, as well as the documentation of remaining non-mitigable risks after development;</p><p>(b) process and incorporate only datasets that are subject to appropriate data governance measures for foundation models, in particular measures to examine the suitability of the data sources and possible biases and appropriate mitigation;</p><p>c) design and develop the foundation model in order to achieve throughout its lifecycle appropriate levels of performance, predictability, interpretability, &nbsp;corrigibility, safety and cybersecurity assessed through appropriate methods such as model evaluation with the involvement of independent experts, documented analysis, and extensive testing during conceptualisation, design, and development;</p><p>(d) design and develop the foundation model, making use of applicable standards to reduce energy use, resource use and waste, as well as to increase energy efficiency, and the overall efficiency of the system. This shall be without prejudice to relevant existing Union and national law and this obligation shall not apply before the standards referred to in Article 40 are published. They shall be designed with capabilities enabling the measurement and logging of the consumption of energy and resources, and, where technically feasible, other environmental impact the deployment and use of the systems may have over their entire lifecycle;</p><p>(e) draw up extensive technical documentation and intelligible instructions for use in order to enable the downstream providers to comply with their obligations pursuant to Articles 16 and 28.1.;</p><p>(f) establish a quality management system to ensure and document compliance with this Article, with the possibility to experiment in fulfilling this requirement,</p><p>(g) register that foundation model in the EU database referred to in Article 60, in accordance with the instructions outlined in Annex VIII paragraph C. When fulfilling those requirements, the generally acknowledged state of the art shall be taken into account, including as reflected in relevant harmonised standards or common specifications, as well as the latest assessment and measurement methods, reflected notably in benchmarking guidance and capabilities referred to in Article 58a (new).</p><p>3. Providers of foundation models shall, for a period ending 10 years after their foundation models have been placed on the market or put into service, keep the technical documentation referred to in paragraph 1(c) at the disposal of the national competent authorities;</p><p>4. Providers of foundation models used in AI systems specifically intended to generate, with varying levels of autonomy, content such as complex text, images, audio, or video (\u201cgenerative AI\u201d) and providers who specialise a foundation model into a generative AI system, shall in addition</p><p>a) comply with the transparency obligations outlined in Article 52 (1),</p><p>b) train, and where applicable, design and develop the foundation model in such a way as to ensure adequate safeguards against the generation of content in breach of Union law in line with the generally acknowledged state of the art, and without prejudice to fundamental rights, including the freedom of expression,&nbsp;</p><p>c) without prejudice to national or Union legislation on copyright, document and make publicly available a sufficiently detailed summary of the use of training data protected under copyright law.&nbsp;</p></blockquote><p>The penalties levied for violating regulations are significant:</p><blockquote><p>Non-compliance of AI system or foundation model with any requirements or obligations under this Regulation, other than those laid down in Articles 5, and 10 and 13, shall be</p><p>subject to administrative fines of up to 10 000 000 EUR or, if the offender is a company, up to 2% of its total worldwide annual turnover for the preceding financial year, whichever is higher.</p><p>5. The supply of incorrect, incomplete or misleading information to notified bodies and national competent authorities in reply to a request shall be subject to administrative fines of up to 5 000 000 EUR or, if the offender is a company, up to 1 % of its total worldwide annual turnover for the preceding financial year, whichever is higher.</p></blockquote><p>Given that the consequences of the current draft of the AI act would kill European AI startups and open-source projects, and could lead to the withdrawal of international AI corporations from the EU, it is likely that major modifications will still be made, or that the AI act will meet major challenges in its entirety.</p><p><strong>What seems required to avoid over- and undershooting regulatory strictness from an AI global risk perspective is a viable definition of&nbsp;</strong><i><strong>high-risk foundation models</strong></i><strong> based on capability and risk thresholds, and focusing regulation to such systems.</strong> This is a difficult task that requires expertise in cutting-edge AI research and policy. Nonetheless, finding a balanced take on the risks of different foundation models is essential for shaping risk-reducing AI policy that works in practice.</p>", "user": {"username": "matthias_samwald"}}, {"_id": "wRCeuAvYvvAffp4m3", "title": "Effective altruism infoshops", "postedAt": "2023-05-31T13:42:58.856Z", "htmlBody": "<p>As you step into the bustling streets of a vibrant city, you come across a small, unassuming building nestled between two trendy coffee shops. Its simple sign reads \"Effective Altruism Infoshop.\" Intrigued by the term you've heard buzzing around, you decide to step inside and explore.</p>\n<p>As you open the door, a wave of warmth and intellectual curiosity greets you. The space is well-lit, with shelves filled with books, pamphlets, and colorful posters adorning the walls. Soft instrumental music plays in the background, providing a soothing ambiance. The room is abuzz with conversation as individuals engage in lively discussions about various global issues and their potential solutions.</p>\n<p>At the entrance, you're greeted by a friendly volunteer who introduces themselves as Alex. They offer you a warm smile and kindly ask if you're new to effective altruism. Sensing your curiosity, they guide you through the infoshop, explaining its purpose and the principles of effective altruism.</p>\n<p>The first section you explore is dedicated to educational resources. It contains shelves stocked with books by influential thinkers such as Peter Singer, William MacAskill, and Toby Ord. These texts delve into topics like global poverty, existential risks, animal welfare, and long-term impact. Each book has concise summaries attached to help visitors grasp the main ideas at a glance.</p>\n<p>Moving along, you come across a cozy reading area with comfortable chairs and bean bags. Here, people immerse themselves in the literature, occasionally engaging in animated conversations about the ethical dimensions of various altruistic approaches. Several bulletin boards adorn the walls, displaying local and global initiatives, fundraising events, and volunteering opportunities. Colorful posters advocate for causes like effective giving, animal rights, environmental conservation, and improving global health.</p>\n<p>Next, you enter the interactive zone, where screens and touch panels offer engaging multimedia experiences. One screen displays a simulation that demonstrates the potential consequences of different career choices, emphasizing the importance of high-impact professions. Another screen showcases effective charities, highlighting their measurable achievements and transparency. Touch panels allow visitors to explore online platforms for calculating personal donation impact or finding volunteering opportunities aligned with their values.</p>\n<p>As you continue your exploration, you notice a small caf\u00e9 corner tucked away in the corner of the infoshop. It features ethically sourced and environmentally friendly products, from fair-trade coffee and vegan snacks to eco-friendly merchandise. The proceeds from these sales support the operation of the infoshop and contribute to the chosen cause of the month.</p>\n<p>In the back of the infoshop, you find a cozy meeting room where workshops and seminars take place. The schedule posted on the door mentions upcoming events on topics like effective altruism 101, ethical investing, and rational decision-making. The room is adorned with whiteboards, posters with thought-provoking quotes, and a collection of community-generated artwork that captures the essence of effective altruism.</p>\n<p>As your visit nears its end, Alex encourages you to leave your email address, so you can stay informed about future events and initiatives. They express their genuine excitement for your interest in effective altruism and offer you a pamphlet with a beginner's guide, highlighting key organizations, online resources, and recommended readings.</p>\n<p>Leaving the infoshop, you carry with you a sense of inspiration and empowerment. The encounter has opened your eyes to a community of individuals dedicated to making a positive impact on the world, encouraging critical thinking, and providing tools to translate empathy into effective action. With newfound knowledge and a deeper understanding of effective altruism, you step back onto the busy streets, ready to explore ways in which you, too, can contribute to a better future.</p>\n<p>Infoshops, also known as information shops or info-centers, are physical spaces that serve as community resources for sharing information, knowledge, and ideas. They often function as alternative or radical social centers, aiming to promote activism, education, and grassroots organizing.</p>\n<p>The primary purpose of infoshops is to provide access to a wide range of literature, zines, pamphlets, books, and other educational materials that are typically not found in mainstream bookstores or libraries. These spaces are designed to empower individuals with knowledge on social, political, environmental, and economic issues, fostering critical thinking, awareness, and engagement within communities.</p>\n<p>While it is challenging to provide concrete evidence of the cost-effectiveness of infoshops, there are several ways in which they contribute to societal well-being:</p>\n<ol>\n<li>\n<p>Access to Alternative Information: Infoshops offer a platform for marginalized or underrepresented voices, allowing individuals to access information that may challenge mainstream narratives. This exposure to diverse perspectives helps foster critical thinking, political awareness, and informed decision-making.</p>\n</li>\n<li>\n<p>Educational Resources: Infoshops often provide books, zines, and educational materials on a wide range of topics, including activism, social justice, environmental sustainability, and more. These resources can serve as valuable tools for self-education and personal growth, particularly for those who may not have access to formal educational institutions.</p>\n</li>\n<li>\n<p>Community Engagement: Infoshops serve as community gathering spaces, facilitating dialogue, workshops, and events. They create opportunities for networking, collaboration, and organizing around social and political issues. This fosters a sense of community and solidarity, empowering individuals to take collective action.</p>\n</li>\n</ol>\n<p>I am currently seeking evidence to understand the impact and cost-effectiveness of infoshops in increasing membership, retention, collaboration, and the impact of social movements. Specifically, I am interested in evaluating the potential impact of an Effective Altruism infoshop in introducing new and diverse individuals to the EA movement, its ideas, and opportunities for impact.</p>\n<p>Some of you will already have  data and anecdotes that can shed light on the following areas:</p>\n<ol>\n<li>\n<p>Traffic and Exposure: I seek data on the number of individuals who visit infoshops, the frequency of their visits, and their exposure to literature and educational resources. This could include records of foot traffic, or other relevant metrics that provide insights into the level of engagement and interest generated by infoshops.</p>\n</li>\n<li>\n<p>Membership Growth and Retention: I am interested in evidence on the impact of infoshops in terms of increasing membership within movements. This may include data on the number of individuals who become members of a organization or take actions of different degrees of capability and commitment. Additionally, data on membership retention rates would provide valuable insights into the long-term effectiveness of infoshops in keeping individuals involved in the movement.</p>\n</li>\n<li>\n<p>Diversity and Inclusivity: I am interested in understanding the role of infoshops in attracting and engaging diverse individuals within the EA movement. This could include data on demographic information, such as age, gender, ethnicity, or socioeconomic background, of infoshop visitors and active participants. Additionally, evidence on initiatives or strategies implemented by infoshops to foster inclusivity would be highly valuable.</p>\n</li>\n</ol>\n<p>I invite the community to contribute relevant data, case studies, surveys, or studies that shed light on the impact and cost-effectiveness of infoshops in relation to the EA movement. Please provide concrete numbers and quantitative evidence wherever possible, as this will greatly assist in estimating the potential impact of an EA infoshop.</p>\n<p>It's important to note that the impact and cost-effectiveness of infoshops may vary depending on the specific context, the level of community engagement, and the resources available. Evaluating their effectiveness requires considering qualitative factors such as individual empowerment, community building, and long-term social change rather than purely quantitative measures.</p>\n<p>Notable case studies:</p>\n<p>Case Study 1: The Green Infoshop - Empowering Environmental Activism</p>\n<p>Location: Portland, Oregon, United States\nDuration: 2009-2013</p>\n<p>The Green Infoshop was a community space established with the aim of promoting environmental awareness and activism. Through a combination of educational resources, workshops, and community organizing, the infoshop played a pivotal role in enabling specific political outcomes related to environmental issues.</p>\n<p>Outcome 1: Successful Campaign Against Coal Export Terminal</p>\n<p>The Green Infoshop actively supported a local coalition campaigning against the construction of a coal export terminal in Portland. They organized informational sessions and workshops on the environmental impacts of coal, encouraging community members to join the cause. The infoshop also facilitated the creation of educational materials, such as pamphlets and videos, to raise awareness about the issue. Through these efforts, they mobilized public support, leading to a successful campaign that resulted in the cancellation of the terminal project.</p>\n<p>Outcome 2: Passage of Local Green Energy Policy</p>\n<p>The Green Infoshop played a vital role in advocating for the adoption of a local green energy policy. They collaborated with local environmental organizations to host public forums and debates on renewable energy alternatives. The infoshop acted as a hub for distributing research materials, studies, and policy briefs, helping community members understand the benefits and feasibility of clean energy solutions. Their collective efforts and educational initiatives led to increased public awareness, which influenced local politicians to pass a comprehensive green energy policy.</p>\n<p>Case Study 2: The Solidarity Infoshop - Fostering Labor Rights Movements</p>\n<p>Location: Buenos Aires, Argentina\nDuration: 2015-2018</p>\n<p>The Solidarity Infoshop was established in Buenos Aires as a platform to support labor rights movements and promote social justice. Through their resources, workshops, and collective actions, they empowered workers to organize and fight for their rights, resulting in notable political outcomes.</p>\n<p>Outcome 1: Successful Worker Occupation of Factory</p>\n<p>The Solidarity Infoshop played a crucial role in supporting workers during the occupation of a textile factory facing closure due to bankruptcy. They provided legal assistance, organizing training sessions on workers' rights, and facilitated connections with labor unions and other organizations. Through their guidance and resources, the workers were able to sustain the occupation, garnering public attention and support. The pressure generated from this mobilization eventually led to negotiations with the factory owners, resulting in the reestablishment of operations under workers' control.</p>\n<p>Outcome 2: Implementation of Fair Trade Policies</p>\n<p>The Solidarity Infoshop collaborated with fair trade organizations and worker cooperatives to advocate for the implementation of fair trade policies in the region. They conducted research on fair trade practices, organizing public lectures and seminars to raise awareness about the importance of fair wages and working conditions. Through their efforts, they influenced local businesses to adopt fair trade principles, leading to the improved livelihoods of workers and the establishment of a fair trade certification program in the city.</p>\n<p>Case Study 3: The Digital Infoshop - Promoting Internet Freedom</p>\n<p>Location: Berlin, Germany\nDuration: 2012-present</p>\n<p>The Digital Infoshop emerged as a response to the increasing threats to internet freedom and privacy. Through its initiatives, this infoshop has played a significant role in enabling specific political outcomes related to digital rights and online activism.</p>\n<p>Outcome 1: Defeat of Restrictive Internet Legislation</p>\n<p>The Digital Infoshop actively campaigned against proposed legislation that would have imposed restrictive measures on internet freedom. They organized protests, online campaigns, and public awareness events, highlighting the potential negative consequences of such laws on privacy and free expression. Through their efforts, they mobilized a broad coalition of individuals, organizations, and tech companies to oppose the legislation. This collective resistance contributed to the withdrawal or modification of the proposed bills, safeguarding internet freedom</p>\n", "user": {"username": "WobblyPanda2"}}, {"_id": "8s5d8JrhYf3XQNv5j", "title": "Why isn't Global development broken out in the EA world?", "postedAt": "2023-05-31T10:37:37.269Z", "htmlBody": "<p>Hi All, just seeing if there's a particular reason why development and growth for lower income countries is still lumped in with global health in the EA nomenclature (in the forums, the button only says \"global health\"!)</p><p>I think there's a lot of growing consensus that there is a lot of potential low hanging fruit that the EA community can do to increase growth in low and low middle income countries. By lumping these together it almost makes it seem that anything that is targeting these areas is only focused on health.</p><p>It would be good to see this distinction and more of a focus overall on global development priorities.</p>", "user": {"username": "Arno"}}, {"_id": "HuLCFBEbekZ6AE7LP", "title": "Linkpost: Survey evidence on the number of vegans in the UK", "postedAt": "2023-05-31T09:31:45.467Z", "htmlBody": "<ul><li><a href=\"https://www.linkedin.com/in/stephen-walsh-4b151b76/?originalSubdomain=uk\">Stephen Walsh PhD</a> recently carried out a review of different surveys estimating the number of vegans in the UK on behalf of the <a href=\"https://www.vegansociety.com/\">UK Vegan Society</a>.&nbsp; It is the most comprehensive review I\u2019ve seen in the UK context that takes into account recent data. &nbsp;But it is relatively light on detail as was aimed at a non-technical audience.</li><li>The review suggests:<ul><li><strong>Around 0.25% of UK adults self-identified as vegan in 2015</strong>. &nbsp;The proportion was probably stable around this level for at least 15 years.</li><li><strong>The share increased to around 1% by 2018</strong>.</li><li><strong>A best guess of a further increase to around 1.35% by 2022</strong> (note this estimate is less certain and not directly comparable to earlier estimates).</li></ul></li><li>The headline results are based on the <a href=\"https://www.food.gov.uk/research/food-and-you\">Food and You</a> (face-to-face) and <a href=\"https://www.food.gov.uk/research/food-and-you-2\">Food and You 2</a> (online, postal) surveys commissioned by the <a href=\"https://www.food.gov.uk/\">UK Food Standards Agency</a>, after comparison with results from other surveys, including consideration of questions asked to identify vegans, survey mode, sampling method and sample size.</li><li>Stephen\u2019s article was originally published in the Vegan Society Magazine (only available to members).&nbsp; Given the potential wider interest in the results, I have received his permission to share a link to a <a href=\"https://perma.cc/H59Q-GUJ2\">copy of his article</a>, and he is happy to answer any interesting questions that come through in the comments.&nbsp;</li><li>I have copied below the chart summarising the results of different surveys offering a consistent time trend. &nbsp;The red &nbsp;dots denote Stephen's best guess of the underlying trend. &nbsp;I've also copied below the identification questions used in the Food and You Survey. &nbsp;The article contains links with further information about the surveys used in the chart.<img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HuLCFBEbekZ6AE7LP/jwzstkkqjid1gmof2bt1\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HuLCFBEbekZ6AE7LP/jfwicanbzoalmsqrmhjj 90w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HuLCFBEbekZ6AE7LP/ihj0wtvrijiulvj0pckj 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HuLCFBEbekZ6AE7LP/zlruorwsw1s20mbx8xwj 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HuLCFBEbekZ6AE7LP/i5csm7kpsjb04bjpi5vt 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HuLCFBEbekZ6AE7LP/sy2zsbdrrx4gypnsfwop 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HuLCFBEbekZ6AE7LP/hxikrdxzulmgnonvbrii 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HuLCFBEbekZ6AE7LP/lbaeg1ax50ieeuecmc5g 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HuLCFBEbekZ6AE7LP/jdtsu48exncaibsp4y0u 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HuLCFBEbekZ6AE7LP/qnowwjvktmn1j61tiihu 810w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HuLCFBEbekZ6AE7LP/cxfqnazmcvjrfk7rbtiu 900w\"></li></ul><h3>Questions used in the Food and You Survey (2010 to 2018)</h3><p><strong>Question 2_7</strong></p><p>Which, if any, of the following applies to you? Please state all that apply.&nbsp;</p><ul><li>Completely vegetarian&nbsp;</li><li>Partly vegetarian&nbsp;</li><li>Vegan&nbsp;</li><li>Avoid certain food for religious or cultural reasons&nbsp;</li><li>None (SINGLE CODE ONLY)</li></ul><p><strong>Check if Q2_7 response = Vegan&nbsp;</strong></p><p>Can I just check, do you eat any foods of animal origin. That is meat, fish, poultry, milk, milk products, eggs or any dishes that contain these?&nbsp;</p><p>1 Yes&nbsp;</p><p>2 No</p>", "user": {"username": "Sagar K Shah"}}, {"_id": "XTNwCtsecACuARTcH", "title": "Considerations on transformative AI and explosive growth from a semiconductor-industry perspective ", "postedAt": "2023-05-31T01:11:44.330Z", "htmlBody": "<p>This is a summary of my entry for the <a href=\"https://www.openphilanthropy.org/open-philanthropy-ai-worldviews-contest/\">Open Philanthropy AI Worldviews Contest</a>, which you can view in full <a href=\"https://muireall.space/pdf/considerations.pdf\">here</a>.</p><p>This essay attempts to bridge a gap between abstract models of AGI timelines and inside views from the semiconductor industry.</p><p>Models like those in <a href=\"https://drive.google.com/drive/u/0/folders/15ArhEPZSTYU8f012bs6ehPS6-xmhtBPP?pli=1\">Cotra 2020 (\u201cBio Anchors\u201d)</a>, <a href=\"https://www.openphilanthropy.org/research/could-advanced-ai-drive-explosive-economic-growth/\">Davidson 2021 (\u201cExplosive Growth\u201d)</a>, and <a href=\"https://www.lesswrong.com/posts/Gc9FGtdXhK9sCSEYu/what-a-compute-centric-framework-says-about-ai-takeoff\">Davidson 2023 (\u201cCompute-Centric Framework\u201d)</a> are grounded in computational and economic abstractions. This is entirely appropriate for medium-to-long-range forecasting, where extrapolating from trends at large is generally more reliable than reasoning about processes in detail.</p><p>Even so, I think their cursory accounts of some of their most important parameters are major weaknesses. A complementary approach works outward from an inside view\u2014not by immediately pursuing better parameter estimates, but by first trying to uncover as many relevant considerations as possible. To the extent these considerations make contact with our models\u2019 abstractions, we can use them for model inputs; anywhere they don\u2019t, we can rethink our models.</p><p>The body of this essay is a short scenario-planning exercise meant to demonstrate this approach, considering two pathways by which AI might fail to have transformative impact in the coming decades. The scenarios are chosen to invite considerations from the semiconductor industry\u2014both its history as a point of comparison and its future as part of the cycle of AI progress. The industry is as mature as any (perhaps second to parts of the chemical industry), notably in terms of production scale and optimization up against physical limits. It\u2019s grown faster than the global economy for decades, feeding improved hardware back into itself but without explosive (superexponential) growth. AI aspires to the same scale but is comparatively immature. It leans more heavily on software, which has very different margins and gets less durable competitive advantage from intellectual property. It may also enter a similar feedback cycle, although there\u2019s a significant potential difference in how effectively AI outputs can substitute for labor.</p><p>I use two scenarios to capture different kinds of obstacles and bottlenecks to AI progress. In the first, firms struggle to capture returns on investment in AI R&amp;D. In the second, AI progress fails to substantially accelerate the non-AI inputs to AI R&amp;D. In both scenarios, highly capable AI may be achieved, but it does not arrive suddenly or have transformative impact on short timescales.</p><p>This exercise, rather than try to estimate parameters for economic modeling from abstract considerations like the above, attempts to distill the two scenarios into lower-level causal drivers. I also try to identify observable indicators for the degree of influence those drivers may have. Because a comprehensive evaluation of data on these indicators would vastly expand the scope of the essay, I mainly call out examples to clarify meaning or relevance, not to argue the weight of evidence in one direction or another. Appendix A describes the process used to develop scenarios and other elements of the analysis in more detail.</p><p>Other appendices provide context and support for some claims that are non-obvious but incidental to the scenario analysis. Appendix B briefly discusses the pipeline from research to deployment in semiconductor device production. Appendix C is an extended discussion of problems with the nanomechanical computer described in Eric Drexler\u2019s <i>Nanosystems</i>. Finally, Appendix D contains some related forecasts to more transparently convey my own expectations.</p><p>Ideally, this kind of exercise would be done by a panel with overlapping areas of expertise. My effort is far from exhaustive, but I hope it at least shows how this approach might usefully fit into a broader forecasting or planning project. If I wanted to single out a few particular themes I think existing analyses don\u2019t fully appreciate:</p><ul><li>Models of transformative change would strongly benefit from deeper inside views.</li><li>Hardware specialization is a meaningful obstacle to rapid growth through paradigm shifts or redeployment of resources.</li><li>A robust open-source community and publicly-owned AI services can dampen AI growth along paths that require large, lumpy capital investment, in part by reducing the potential for AI ventures to capture profits.</li><li>Progress in industry at scale is not limited by \u201cideas\u201d in the same way that basic research could imaginably be.</li></ul><p><a href=\"https://muireall.space/pdf/considerations.pdf\">The full document is available here.</a></p>", "user": {"username": "Muireall"}}, {"_id": "sxYmMdtAEvJd4pwRQ", "title": "Abstraction is Bigger than Natural Abstraction", "postedAt": "2023-05-31T00:00:36.663Z", "htmlBody": "", "user": {"username": "NicholasKross"}}, {"_id": "L5EuQ3araz7iESjEA", "title": "Update from the EA residency in Zanzibar", "postedAt": "2023-05-30T19:17:11.788Z", "htmlBody": "<p>When I first join Anne Nganga on our video call, she apologizes for the background noise of a fan running. \u201cThe weather here is hot and humid,\u201d she says. \u201cI have to have a fan or the AC on at all times if I am to enjoy being indoors.\u201d</p><p>Anne is originally from Kenya, but she\u2019s calling me from the island of Zanzibar, where she\u2019s been facilitating the <a href=\"https://forum.effectivealtruism.org/posts/ksisS29ThcY3BZLRw/call-for-applications-for-zanzibar-residency\"><u>2023 Effective Altruism Africa Residency Fellowship</u></a>. It took place in the first three months of 2023, connecting EAs working on projects \u201caddressing the most pressing problems in Africa\u201d.</p><p>Residencies have become an increasingly popular option for Effective Altruism community building. They typically involve a group of people who work on Effective Altruism or related topics professionally working and living in the same place for 1-3 months, in order to help those people get to know each other, build trust and develop new partnerships and opportunities. In the last year, <a href=\"https://forum.effectivealtruism.org/posts/ZxHnaPmxmBrZmerBj/announcing-the-prague-fall-season\"><u>Prague </u></a>and <a href=\"https://forum.effectivealtruism.org/posts/kBgMD7GwJboxGFvsp/mexico-ea-fellowship\"><u>Mexico City</u></a> have both hosted residencies. 2022\u2019s <a href=\"https://forum.effectivealtruism.org/posts/M5GoKkWtBKEGMCFHn/what-s-the-theory-of-change-of-come-to-the-bay-over-the\"><u>\u201cBay area summer\u201d</u></a> was arguably a residency as well, and perhaps the first example I can think of was the <a href=\"https://forum.effectivealtruism.org/posts/sdjcH7KAxgB328RAb/ftx-ea-fellowships\"><u>FTX EA Fellowship</u></a> that flew EAs to the Bahamas at the beginning of 2022. So I was curious how this most recent EA residency turned out, how the FTX collapse affected their plans, and what we can expect going forward from EA in Africa.</p><h3>What\u2019s a day in the life in the Zanzibar residency?</h3><p>In the morning, the residents spend time working remotely on their projects, before lunch at their coworking space.</p><p>In the afternoon, Anne says, \u201cwe typically have three hours of structured engagement time\u201d such as a guest speaker (irl or over Zoom), a hangout, or an informal conference amongst the attendees.</p><p>\u201cOn the off chance we do not have anything planned, residents would typically be engaged in one social activity or another. They would either be at the pool, or at the beach bar, or hanging out at one of the restaurants, or going to Stonetown which is a UN Heritage [Site].\u201d</p><h3>Did the FTX collapse change plans for your residency?</h3><p>\u201cI personally live very frugally, and the same could be said for my cofounder Daniel,\u201d Anne says. She\u2019s talking about Daniel Yu, CEO of African tech company Wasoko, who moved to Zanzibar and <a href=\"https://disrupt-africa.com/2022/09/02/kenyas-wasoko-relocates-to-zanzibar-as-anchor-of-new-government-led-pro-startup-initiative/\"><u>opened a Wasoko office</u></a> as part of the government\u2019s \u201c<a href=\"https://www.siliconzanzibar.com/\"><u>Silicon Zanzibar</u></a>\u201d initiative last year. He also provided the funding for the residency.</p><p>\u201cFumba [the town in Zanzibar where the fellowship was located] was just the perfect compromise between being reasonable but also meeting most of the needs [of our residents]\u201d like internet access, transport, healthcare and places to unwind and have fun.</p><p>\u201cOf course we expect a bit more scrutiny [since the FTX collapse], but we did not deviate in any way from what we had planned to do.\u201d</p><h3>What else can we expect from EA in Africa?</h3><p>\u201cWe\u2019ve got two very ambitious projects,\u201d says Anne. \u201cWe have plans to set up an EA group here in Zanzibar [and we have set up a call to discuss] a self-sustaining pan-African EA group.\u201d</p><p>And what happens next for the 18 residents? \u201cThey\u2019re gonna go back to their home countries,\u201d Anne says. \u201cAnd I\u2019m happy to report that indeed, I know I have made friends for life.\u201d</p><p><i>This post is from my email newsletter, EA Lifestyles. </i><a href=\"https://ealifestyles.substack.com/account?utm_medium=web&amp;utm_source=subscribe-widget&amp;utm_content=102334904\"><i>Subscribe</i></a><i> for weekly updates or </i><a href=\"https://ealifestyles.substack.com/\"><i>read more posts now</i></a><i>.</i></p>", "user": {"username": "Khorton"}}, {"_id": "bFDs7yFiEhgPt4LWt", "title": "The Case for AI Adaptation: The Perils of Living in a World with Aligned and Well-Deployed Transformative Artificial Intelligence", "postedAt": "2023-05-30T18:29:14.085Z", "htmlBody": "<p><strong>TLDR:&nbsp;</strong>Even if transformative AI systems are technically aligned and deployed with good intentions, there will likely be an \u201cuncanny valley\u201d between the time such systems are created and their value is equitably distributed. In the interim, there is a risk of mass suffering being inflicted on those least proximate to the creation of these systems. This creates an argument for a research and policy agenda on \u201cAI adaptation\u201d, which pushes resources and policy ideas toward ensuring that the disruptive effects of transformative AI systems are minimized while we cross to the other side of the uncanny valley.&nbsp;</p><h3>Note of Thanks</h3><p>I am thankful for the following papers which helped inspire me to write this post. These include the <a href=\"https://docs.google.com/document/d/15EIN9vV0UlbzBNt826W0HO0FhjF0Xyxu0q92wHNU6kg/edit\">thoughts</a> of Markus Anderlung on AI misuse, the <a href=\"https://acritch.com/papers/arches.pdf\">work</a> of David Kreuger and Andrew Critch on AI research considerations, the <a href=\"https://academic.oup.com/edited-volume/41989/chapter-abstract/386766686?redirectedFrom=fulltext&amp;login=false\">chapter</a> by Ben Garfinkel on AI in historical perspective, and various papers by Allan Defoe including those on <a href=\"https://arxiv.org/pdf/2012.08630.pdf\">cooperative AI</a>, <a href=\"https://www.allandafoe.com/opportunity\">AI governance</a> opportunities, and <a href=\"https://journals.sagepub.com/doi/10.1177/0162243915579283\">technological determinism</a>. I am also grateful to, amongst many papers from CSER and GCRI, this <a href=\"https://www.cser.ac.uk/resources/transformative-potential-artificial-intelligence/\">paper</a> on transformative AI and this <a href=\"https://gcrinstitute.org/resilience-to-global-catastrophe-irgc/\">paper</a> on resilience to global catastrophe. Thank you also to others who discussed this idea with me and helped me refine this post, your advice was invaluable.</p><h2>Alignment, Deployment, Adaptation</h2><p>Members of the Effective Altruism, X-risk, AI Safety, security, policy (and overlapping) communities have dedicated a laudable amount of effort and resources towards ensuring that our collective future can best take advantage of the benefits of advanced artificial intelligence while minimizing the potential existential risks from advanced AI systems. These efforts can be (broadly) divided into two buckets:&nbsp;</p><ol><li><strong>Alignment:&nbsp;</strong>There is significant attention devoted to ensuring that AI systems are technically aligned such that they follow the intentions of their human inventors and supervisors in letter and in spirit.&nbsp;</li><li><strong>Deployment:&nbsp;</strong>There is considerable focus on guaranteeing that powerful and technically aligned AI systems are deployed in a fashion that protects widely-held values and minimizes damage to the world. Part of this research involves ensuring that AI systems are not developed by those who may have malicious incentives, and hence intend to abuse the power of advanced AI systems. This also includes research on figuring out what we want from AI systems in the first place.&nbsp;</li></ol><p>Efforts in this space are, as most people reading this post would agree, critical and should be supported in whatever way each of us can.&nbsp;</p><p>At the same time, I feel that this intense focus on alignment and deployment might come at the expense of attention towards potentially less important and yet nonetheless vital issues. I call this set of issues, \u201cAI Adaptation\u201d. Borrowing from the vast literature on climate adaptation, I define AI adaptation as the project of adjusting to the expected disruptive effects of advanced artificial intelligence in order to moderate or avoid harm from these disruptions. &nbsp;</p><p>The argument here is as follows. Even if one can assume that:&nbsp;</p><ul><li>Transformative AI systems are likely to be technically aligned.&nbsp;</li></ul><p>and</p><ul><li>These systems are likely to be deployed in a fashion that is not intentionally harmful.</li></ul><p>There is still considerable work to be done to fashion global economic, political, and social systems that are prepared for a transition to this fundamentally different world.&nbsp;</p><p>There is a growing amount of research adjacent to this adaptation space such as that mentioned at the top of this post (the paper by Jess Whittlestone and Ross&nbsp;Gruetzemacher is particularly relevant). There is also an increasing amount of research that seeks to ensure that we also pay considerable attention to the risks from artificial intelligence systems on the road to transformative artificial intelligence, such as that focusing on <a href=\"https://www.rand.org/pubs/perspectives/PEA1043-1.html\">disinformation</a>, developments in <a href=\"https://www.liebertpub.com/doi/10.1089/hs.2019.0122\">biotechnology</a>, and <a href=\"https://www.cser.ac.uk/resources/it-takes-village/\">lethal autonomous weapons</a>.&nbsp;</p><p>I believe an important aspect of research that is still under-considered is the disruptive economic and political impact of transformative artificial intelligence, in particular on those who live outside the United States and Western Europe. I believe that in the absence of a project dedicated to funding and research adaptation to the disruptive effects of transformative artificial intelligence, there may be mass suffering in many parts of the world.&nbsp;</p><h2><strong>Relevant Assumptions</strong></h2><p>In arguing for this project, I am making the following assumptions:&nbsp;</p><ul><li><strong>First</strong>, I am assuming that transformative artificial intelligence is most likely to emerge from either a lab or a government-run facility in the United States. (This premise is not necessary per se to the argument but is an illustration of a more general premise that \u201ctransformative AI will emerge in some powerful country\u201d).&nbsp;</li><li><strong>Second</strong>, (for argument\u2019s sake) I am assuming that the actor who develops this system has fairly good intentions (however we define them) and has also been successful in technically aligning this system.&nbsp;</li><li><strong>Third</strong>, I am assuming that the development of such an AI system will be, by definition, transformative, and rapidly claim immense amounts of economic and technological value in the global system.&nbsp;</li><li><strong>Fourth</strong>, I assume that while initial ownership of this value will be retained by the inventors of such a system, there will be efforts made to distribute the dividends of this technology to those across the world.&nbsp;</li><li><strong>Fifth, and most importantly,&nbsp;</strong>I believe there will be an \u201cuncanny valley\u201d between the time that this system is created and starts generating value and the time that its dividends are distributed equitably to those across the world.&nbsp;</li><li><strong>Sixth,&nbsp;</strong>on the other side of this uncanny valley, AI systems of such intelligence will be able to provide tractable and efficient solutions to global poverty, disease, and other relevant problems faced by many members of our global community.&nbsp;</li></ul><h2>An Illustration</h2><p>As someone who has grown up and spent most of his life in a non-Western country without significant international influence, I am acutely concerned about this uncanny valley. In particular, I am worried that mass economic disruption is likely to inflict suffering on likely hundreds of millions of people who are least responsible for the technology\u2019s creation, least proximate to its benefits, and most vulnerable to its disruptive effects. Here is a possible illustration of my argument:&nbsp;</p><ul><li>An advanced AI system is developed by a company within the United States. This system is transformative \u2013 it is rapidly generating immense economic and technological value, leading to an explosion in the company\u2019s value. The US government steps in to regulate this company, which is completely open to the government\u2019s position. Together, both the government and this company attempt to both realize the value being generated by this system and to craft structures to distribute this value equitably across the world. For a range of reasons including competing constituency priorities, political inefficiencies, limited human capital, lack of global coordination, and others, it takes 1-5 years for a structure to be set up that all relevant actors can agree on to distribute this value, even as the American economy takes full advantage of this development and inter-state inequality skyrockets. After five years, benefits from this system are distributed to the world\u2019s poor, after hundreds of millions have died or experienced serious suffering as a result of this economic disruption. &nbsp;</li></ul><p>This illustration is, hopefully, concerning.&nbsp;</p><p>As the tone of this post makes clear, the intention of this illustration is not to provide an argument against the development of advanced AI systems, and it is definitely not an argument against investing resources towards technical alignment and responsible deployment of AI systems.&nbsp;</p><p>Instead, the intention here is to argue for resources and attention (being, for now, agnostic as to how much) to be devoted towards AI Adaptation to ensure that \u2018best-case\u2019 advanced AI scenarios account for the potentially transformative negative effects of advanced AI systems on those least likely to be protected against economic and political disruptions. The use of the term \u2018adaptation\u2019 here is intentional \u2013 I am assuming an inevitability to the development of transformative artificial intelligence, in the same way that many assume that significant disruptions from climate change are now more or less inevitable and have incentivized efforts to adapt to a warmer climate.&nbsp;</p><h2><strong>Some Other Relevant Factors&nbsp;</strong></h2><p>In writing this post, I have attempted to be fairly generous in making my assumptions, but perhaps it is important to note some things that could go much worse which would make this case for adaptation much stronger:&nbsp;</p><ol><li><strong>Transformative Artificial Intelligence is likely to be achieved \u201csoon\u201d.&nbsp;</strong>I am not an expert in this space, and I am agnostic as to whether such a benchmark is reached in 2040, 2050, 2070, or some later date. However, the prospect of this benchmark being reached sooner rather than later is especially alarming as we are likely to be that much less prepared for guarding communities against its disruptive effects.&nbsp;</li><li><strong>The Takeoff Speeds are very hard (fast). </strong>Again, I am completely unsure as to whether we will go from \u2018human-level\u2019 AI systems to superintelligence in a few days or a decade, or anything in between. But for similar reasons to point 1, a hard takeoff makes disruption more likely and strengthens the case for working on adaptation with a greater sense of urgency.</li><li><strong>Transformative AI may not have to be that intelligent.&nbsp;</strong>If it is the case that advanced AI systems become transformative long before we are close to achieving artificial superintelligence, then we have much less time to guard against disruptions than if AI could only be transformative if it was close to our definitions of superintelligence.&nbsp;</li><li><strong>The Uncanny Valley is an Uncanny Canyon.&nbsp;</strong>If this valley \u2013the measured time from getting value from transformative AI to distributing it across the world \u2013 is much wider than current estimates, suffering from disruption is likely to be greater, bolstering a stronger argument for innovating and improving adaptation structures. &nbsp;</li><li><strong>Finally, AI systems are not technically aligned or poorly deployed. </strong>If this happens, we may have bigger problems on our hands, but this too bolsters the argument for adaptation; if the general trend of history - that when global cataclysms happen they disproportionately affect the most vulnerable - is true, this creates a further need to provide these communities a line of defense.&nbsp;</li></ol><p>These factors, and many others, are likely to have a significant bearing on the case for and nature of adaptation and each deserve further independent inquiry of their own in this context.&nbsp;</p><h2>Some Tentative Policy Suggestions</h2><p>While this project of adaptation requires much deeper thought and reflection \u2013 as well as institutional resources devoted to its inquiry \u2013 I think the following tentative policies could be of interest to those who find value in researching this problem:&nbsp;</p><ol><li><strong>Universal basic income.</strong> Universal in this case is taken literally; there could be a basic stipend provided to every human on Earth (perhaps pegged to purchasing power terms) which generates revenue from dues paid by affluent governments, and potentially dues paid by companies at the lead of the AI race.&nbsp;</li><li><strong>Construction of national and global social safety nets.</strong> These could rely on similar sources to those mentioned in the first recommendation. A discussion of AI adaptation also has the potential to push national governments themselves to re-orient budgetary priorities towards adaptation efforts in the form of bigger and better-constructed safety nets.&nbsp;</li><li><strong>Global fund for economic disruption.</strong> The World Bank\u2019s Financial Intermediary Fund is a good (albeit timid) example of a system that could be massively expanded to include financial support to poor communities across the world who may suffer in the interim as the world adapts to transformative artificial intelligence.&nbsp;</li></ol><p>As those focused on global governance, economics, politics, and many others are aware, each of these proposals has significant problems and likely much greater issues with tractability. The intention is to pitch them tentatively as a starting point for crafting a policy and research agenda which can aid adaptation efforts.&nbsp;</p><h2>Parting Thoughts</h2><p>I believe such a project would be of interest to those vested in reducing the risks from emerging technologies, as well as those dedicating their lives to reducing global poverty and improving global health and well-being. It may also provide an additional general argument against the rapid development of advanced AI systems without careful thought of the consequences.&nbsp;</p>", "user": {"username": "HTC"}}, {"_id": "cN6DbyYCdJ3wRqXar", "title": "EA and Judaism Intro Fellowship \u2014 July 2023", "postedAt": "2023-05-31T14:47:08.330Z", "htmlBody": "<p><strong>TL;DR</strong>: <a href=\"https://eaforjews.org/\">EA for Jews</a> is running another round of the <a href=\"https://eaforjews.org/take-action/fellowship/\">EA and Judaism Intro Fellowship</a> this July! Apply to join (<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSccGgN4Up-Rc2zy3WSUJieljjWSze1e9NI6Pe9-DtVTQ3gp-g/viewform\">here</a>) or facilitate (<a href=\"https://docs.google.com/forms/d/1kQ1QmaLYeAHivffvBxyMeOFt20KHouQJpJXzJn8F_tU/edit?pli=1\">here</a>). <strong>Please spread the word to anyone you know who might be interested!</strong> If you are connected with a Jewish organization who might be able to help us promote the fellowship, please let us know at <a href=\"mailto:Shalom@eaforjews.org\"><u>Shalom@eaforjews.org</u></a>.</p><h1>Fellowship Details</h1><p>We\u2019re excited to announce that we will be running another round of the <a href=\"https://eaforjews.org/take-action/fellowship/?fbclid=IwAR1LjXDogsuaQwPP3C2TtnJdRJBrUzY3Ty5PYAlRX7RaDEeZPLbnhDzG94s\"><u>EA and Judaism Intro Fellowship</u></a> this July!</p><p>The summer fellowship will be a <strong>4-week virtual program</strong> you can do alongside school or work that explores the core ideas of effective altruism and their relationship to Jewish tradition, texts, culture, and history.</p><p>It will explore questions such as:</p><ul><li>What do Jewish texts and tradition have to say about our obligations to the global poor, animals, and future generations?</li><li>How should we balance concern for Jewish priorities vs. global priorities?</li><li>How much better are the best options to do good?</li><li>What can we do to most effectively address pressing global problems like climate change, pandemics, and risks posed by emerging technologies?</li></ul><p>For any questions, reach out to us at <a href=\"mailto:Shalom@eaforjews.org\"><u>Shalom@eaforjews.org</u></a> or <a href=\"https://l.facebook.com/l.php?u=https%3A%2F%2Fcalendly.com%2Fhillelehrenreich%2F30-minute-meeting%3Ffbclid%3DIwAR37ERO0WbnANDFoO7ogA_Pgwkyfj38zyVjO-WRed6xf5WXyb8p3QOpqnoc&amp;h=AT3CqGiKfuRKIaz-gI1g9yMxh3PSbZ1kTFIkCAnSQoxcRR7zOrhwCmHCMPq7QhvLTTDSpo-WTrVEr_ffLOap12tpeQVIKQHRkoWp2GpYLyBYoX3vKBN6epV0NVnltYepu-aN&amp;__tn__=-UK-R&amp;c[0]=AT19KD-9VD_VVRhh5argAztPi50-U4acksrkXHsuGiDcxOxkiLTLN0-XRoYiMvSrAGr320n1G5cyzD0txV0RGEtcBuvnJUeV-34acrFQ1w3kjpLuh3_D0tNzEQYzPjRvJ6fre0aeWSE788G6ktn8ZewwrXI1Vykc09IFXpEqY-71WsXTYBQMKvGPb454VMbo7PtFlWXH8G6Pjor2yawvtSE\"><u>schedule a 1-1</u></a> with Hillel, our Director.</p><p><strong>The deadline to apply is Sunday, June 25th.</strong></p><p><strong>Learn more and apply at:</strong> <a href=\"https://l.facebook.com/l.php?u=https%3A%2F%2Feaforjews.org%2Ftake-action%2Ffellowship%2F%3Ffbclid%3DIwAR1nSGurOYJuOxcQOIqNgBab0Ls6K5FIPz0cqaO9RMv_bVM3WfhDiH8PI6o&amp;h=AT0BH1pjJxoGjpiLJqeS85OeeFYNS7WqV8P88XOEN6zfdcAo8xgTb3U1p7pAEZMqRnHmfJFDBnOqdhsxJ6IYdfAFBPiVuyvCKWK4N0mrakX6scGy2eS5HRfrv4Oz5Z14t6_W&amp;__tn__=-U-UK-R&amp;c[0]=AT19KD-9VD_VVRhh5argAztPi50-U4acksrkXHsuGiDcxOxkiLTLN0-XRoYiMvSrAGr320n1G5cyzD0txV0RGEtcBuvnJUeV-34acrFQ1w3kjpLuh3_D0tNzEQYzPjRvJ6fre0aeWSE788G6ktn8ZewwrXI1Vykc09IFXpEqY-71WsXTYBQMKvGPb454VMbo7PtFlWXH8G6Pjor2yawvtSE\"><strong><u>https://eaforjews.org/fellowship</u></strong></a><strong>!</strong></p><h1>Ways you can help&nbsp;</h1><h2>(1) Spread the word to anyone you know who might be interested!&nbsp;</h2><p>Last fellowship, we had over 60 participants and almost everyone who joined heard about it from a direct message from someone they knew! Fellows do not need to be religious or have any prior background with effective altruism to enjoy the program. You can share <a href=\"https://eaforjews.org/take-action/fellowship/\">the fellowship page on our website</a>, our <a href=\"https://www.facebook.com/groups/eaforjews/permalink/3703051763256196/\">social media posts</a>, and/or the <a href=\"https://docs.google.com/document/d/1BddSor8e-N5hrBBVwijN51_blGLMYZpHk3Vn_aIk2b8/edit?usp=sharing\">program syllabus</a> with anyone who might be interested.&nbsp;</p><h2>(2) <a href=\"https://docs.google.com/forms/d/1kQ1QmaLYeAHivffvBxyMeOFt20KHouQJpJXzJn8F_tU/edit?pli=1\">Apply to facilitate&nbsp;</a></h2><p>We are looking for facilitators who can help lead a cohort of fellows toward a good exchange of ideas. You can lead a group either in person within your own community or online (application <a href=\"https://docs.google.com/forms/d/1kQ1QmaLYeAHivffvBxyMeOFt20KHouQJpJXzJn8F_tU/prefill\"><u>here</u></a>).&nbsp;</p><h2>(3) Help connect us with a Jewish organization&nbsp;</h2><p>If you have a connection with a Jewish organization that can help promote our fellowship (such as your synagogue, campus Hillel, study program, etc.) let us know at <a href=\"mailto:Shalom@eaforjews.org\"><u>Shalom@eaforjews.org</u></a>.<br><br>&nbsp;</p>", "user": {"username": "EA for Jews"}}, {"_id": "MDkYSuCzFbEgGgtAd", "title": "AI Doom and David Hume: A Defence of Empiricism in AI Safety", "postedAt": "2023-05-30T20:45:32.211Z", "htmlBody": "<h3><strong>0. Introduction</strong></h3><p>An essay being&nbsp;<a href=\"https://www.openphilanthropy.org/open-philanthropy-ai-worldviews-contest/\"><u>worth $50,000 dollars</u></a> is a bold claim, so here is another\u2014 the person best equipped to adjust your AI existential risk predictions is the 18th-century Scottish historian David Hume.</p><p>Eliezer Yudkowsky has used Hume\u2019s is-ought problem to&nbsp;<a href=\"https://www.edge.org/response-detail/26198\"><u>argue</u></a> that it\u2019s possible, in principle, for powerful AI agents to have any goals. Even a system with cognitive mastery over \u201cis\u201d data requires a framework for \u201cought\u201d drives to be defined, as the transition from factual knowledge to value-driven objectives is not inherently determined. Bluntly, Eliezer&nbsp;<a href=\"https://twitter.com/esyudkowsky/status/817580072982835201\"><u>writes</u></a> that \u201cthe world is literally going to be destroyed because people don't understand Hume's is-ought divide. Philosophers, you had ONE JOB.\u201d</p><p>As a student of Hume, I think this is a limited picture of what he has to offer this conversation. My goal, however, is not to refute Eliezer\u2019s example or to even engage in x-risk debates at the level of&nbsp;<i>a priori&nbsp;</i>reasoning. Hume\u2019s broader epistemological and historical writing critiques this method.&nbsp;</p><p>Hume was not a lowercase-r rationalist. He thought knowledge was derived from experiences and warned that it\u2019s easy to lead yourself astray with plausible sounding abstractions. If you are a capital-R Rationalist, I will argue you should review your foundational epistemological assumptions, because even a small update may ripple out to significantly greater uncertainty about existential risk from AI.</p><p>The central premises in Hume\u2019s thought that I think you should consider are:</p><ol><li>All knowledge is derived from impressions of the external world. Our ability to reason is limited, particularly about ideas of cause and effect with limited empirical experience.</li><li>History shows that societies develop in an emergent process, evolving like an organism into an unknown and unknowable future. History was shaped less by far-seeing individuals informed by reason than by contexts which were far too complex to realize at the time.</li></ol><p>In this essay, I will argue that these premises are true, or at least truer than the average person concerned about existential risk from AI holds them to be. I hope David Hume can serve as a guide to the limits of \u201carguing yourself\u201d into any strong view of the future based on&nbsp;<i>a priori</i> reasoning. These premises do not mean that AI safety should be ignored, but they should unsettle strong/certain views.</p><p><strong>The best practical example of premise #1</strong> is Anthropic\u2019s&nbsp;<a href=\"https://www.anthropic.com/index/core-views-on-ai-safety\"><u>description</u></a> of \u201cempiricism in AI safety.\u201d Anthropic does argue that there is evidence AI will have a large impact, that we do not know how to train systems to robustly behave well, and that \u201csome scary, speculative problems might only crop up\u201d once AI systems are very advanced. Yet they caution that \u201cthe space of possible AI systems, possible safety failures, and possible safety techniques is large and&nbsp;<strong>difficult to traverse from the armchair alone</strong>.\u201d Anthropic is committed to AI safety, but within an empiricist epistemology. Their&nbsp;<a href=\"https://www.anthropic.com/index/core-views-on-ai-safety\"><u>portfolio approach</u></a> is built upon uncertainty: \u201cSome researchers who care about safety are motivated by a strong opinion on the nature of AI risks. Our experience is that even predicting the behavior and properties of AI systems in the near future is very difficult. Making a priori predictions about the safety of future systems seems even harder.\u201d</p><p><strong>The best practical example of premise #2</strong> is one you may have found frustrating\u2014 Tyler Cowen\u2019s&nbsp;<a href=\"https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html\"><u>\u201cradical agnosticism\u201d</u></a> on the question of AI existential risk. Cowen\u2019s style irks many in this community, but I believe the tension stems from a real methodological disagreement that is worth considering. If a deep study of history shows it to be incredibly complex and context-dependent, then our ability to \u201cforecast\u201d complex social developments may be overestimated. Laura Duffy of Rethink Priorities has made a&nbsp;<a href=\"https://twitter.com/Laura_k_Duffy/status/1653184524564545539\"><u>similar point</u></a> about Hayek and the information problem, arguing that longtermist EAs should be more skeptical of individuals or groups\u2019 abilities to reliably model the future of civilization. Yet Hayek was influenced by David Hume\u2019s view of emergent social order, and I think Hume makes the epistemological and historical case more holistically.</p><p>In response to Question 2 of&nbsp;<a href=\"https://www.openphilanthropy.org/open-philanthropy-ai-worldviews-contest/\"><u>OpenPhil\u2019s AI Worldviews Contest</u></a>, I will argue that the above premises should lead you to increase your error bars and widen your probability distribution of a pre-2070 AGI necessarily spelling existential catastrophe. I will consider my essay a success if you move in this direction at all, rate other essays in this contest that use empirical rather than&nbsp;<i>a priori</i> methods higher, or consider for the length of a single deep breath that this question might be unanswerable with any reasonable degree of confidence. Uncertainty does not mean inaction, but it should shift your priorities, which I will briefly suggest in my conclusion.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/okk1jt7mczj03uivmdjj\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/e3wmvgf7c6qy3oilw8gp 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/wu56iar9dnhzvgy9c9nk 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/kg9tug5lzjh8spxjwy8h 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/a6a3ticra5ici5tndhgc 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/jytsjjpnqoavehqnvnt1 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/mj4rgalq8c3blnqntx2y 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/ulshbg8hmechoagkx3hy 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/u3m78mjmcxa4cslrip47 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/qefds2uyqtwwcv8cnbo1 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/cyexlwfzknt3gpwsvu0h 1024w\"></p><h3><strong>1. Empiricism</strong></h3><p>To make this concrete, here is an example from a recent&nbsp;<a href=\"https://80000hours.org/podcast/episodes/tom-davidson-how-quickly-ai-could-transform-the-world/\"><u>80,000 Hours Podcast interview&nbsp;</u></a>with Tom Davidson, a Senior Policy Analyst at Open Philanthropy. I don\u2019t mean to single out Davidson in particular, and I am aware he spoke more about timelines than x-risks. Still, I think it is representative of the way many AI conversations develop. I also think it would be fruitful to engage with a segment of OpenPhil\u2019s own thinking for the contest.</p><p>Here is his response to Luisa Rodriguez\u2019s claim that his arguments seem unintuitive. I have included his full response and highlighted key sections to avoid quoting him out of context:</p><blockquote><ul><li>\u201c<strong>I agree it seems really crazy</strong>, and I think it\u2019s very natural and understandable to just not believe it when you hear the arguments. That would have been my initial reaction.</li><li>In terms of why I do now believe it, there\u2019s probably a few things which have changed. Probably&nbsp;<strong>I\u2019ve just sat with these arguments for a few years, and I just do believe it.&nbsp;</strong>I have discussions with people on either side of the debate, and&nbsp;<strong>I just find that people on one side have thought it through much more.</strong></li><li>I think what\u2019s at the heart of it for me is that the human brain is a physical system. There\u2019s nothing magical about it. It isn\u2019t surprising that we develop machines that can do what the human brain can do at some point in the process of technological discovery. To be honest, that happening in the next couple of decades is when you might expect it to happen, naively. We\u2019ve had computers for 70-odd years. It\u2019s been a decade since we started pouring loads and loads of compute into training AI systems, and we\u2019ve realised that that approach works really, really well.&nbsp;<strong>If you say, \u201cWhen do you think humans might develop machines that can do what the human brain can do?\u201d you kind of think it might be in the next few decades.</strong></li><li>I think if you&nbsp;<strong>just sit with that fact</strong> \u2014 that there are going to be machines that can do what the human brain can do; and you\u2019re going to be able to make those machines much more efficient at it; and you\u2019re going to be able to make even better versions of those machines, 10 times better versions; and you\u2019re going to be able to run them day and night; and you\u2019re going to be able to build more \u2014&nbsp;<strong>when you sit with all that, I do think it gets pretty hard to imagine a future that isn\u2019t very crazy.</strong>\u201d</li></ul></blockquote><p>This is a lowercase-r rationalist epistemology. In this view, new knowledge is derived from \u201csitting with\u201d arguments and thinking them through, following chains of&nbsp;<i>a priori</i> reasoning to their logical conclusions. Podcasts are a limited medium, but in his&nbsp;<a href=\"https://www.openphilanthropy.org/research/report-on-semi-informative-priors/\"><u>Report on Semi-informative Priors for AI Timelines</u></a>, Davidson presents a similar approach in his framing question:</p><blockquote><p>\u201cSuppose you had gone into isolation when AI R&amp;D began and only received annual updates about the inputs to AI R&amp;D (e.g., researchers, computation) and the binary fact that we have not yet built AGI. What would be a reasonable pr(AGI by year X) to have at the start of 2021?\u201d</p></blockquote><p>This is significantly closer to Descartes\u2019 meditative contemplation than Hume\u2019s empiricist critique of the limits of reason. Davidson literally describes someone thinking in isolation based on limited data. The assumption is that knowledge of future AI capabilities can be usefully derived through reason, which I think we should challenge.</p><p>The statement \u201ca sufficiently intelligent AI system would cause an existential catastrophe\u201d is much more comparable to a fact about observable reality than to an&nbsp;<i>a priori</i> idea such as the relationship of the angles of a perfect triangle. This statement makes a claim about cause and effect, which Hume was skeptical that we can know by anything other than association and past experience. I know the sun will almost certainly rise tomorrow because I have formed an association through experience. If Hume is right, we can have no such association of how a superintelligence would behave without empirical evidence either of existing systems or future ones. Hume writes:&nbsp;</p><blockquote><p>\u201cAs the power, by which one object produces another, is never discoverable merely from their idea, cause and effect are relations of which we receive information from experience and not from any abstract reasoning or reflection.\u201d</p></blockquote><p>Hume went further. Not only should we prioritize empirical evidence over rational abstractions, but even when we try, we can never step outside of our impressions and experiences. We inevitably reason through analogy, allegory, and impressions of the world. If we \u201cchase our imagination to the heavens, or to the utmost limits of the universe, we never really advance a step beyond ourselves, nor can conceive any kind of existence, but those perceptions, which have appeared in that narrow compass.\u201d Rationalists often fall into the trap not only of overestimating the power of&nbsp;<i>a priori</i> reason but also of underestimating how impressions and past experiences are shaping their thought unconsciously.</p><p>It\u2019s true that most rationalists acknowledge uncertainty, but they do so through Bayesian probabilities on their predictions. I think Hume would respond that \u201cAll probable reasoning is nothing but a species of sensation.\u201d He writes, \u201cWhen I am convinced of any principle, it is only an idea, which strikes more strongly upon me. When I give the preference to one set of arguments above another, I do nothing but decide from my feeling concerning the superiority of their influence.\u201d This doesn\u2019t mean it\u2019s impossible to think probabilistically about the future, but that we tend to vastly overestimate how detached and \u201crational\u201d we are capable of being when we do so.</p><p>To Davidson\u2019s credit, he acknowledges the issue of empirical evidence. In the Weaknesses section of his report, the first point reads:</p><blockquote><p>\u201cIncorporates limited kinds of evidence. Excludes evidence relating to how close we are to AGI and how quickly we\u2019re progressing. For some, this is the most important evidence we have.\u201d</p></blockquote><p>My entry into this conversation is to suggest that, yes, this is the most important evidence we have. If Hume is right about how we acquire knowledge (or at least more right than the average reader of this post), then empirical observation of how AI systems actually work in practice may be our only evidence.&nbsp;</p><p>So many of the standard arguments for AI risks rely on theoretical arguments detached from empirical evidence. Tyler Cowen rightly&nbsp;<a href=\"https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html\"><u>jokes</u></a> that the standard is often a \u201cnine-part argument based upon eight new conceptual categories that were first discussed on LessWrong eleven years ago.\u201d Hume has a cheeky response to these categories as well: \u201cWhen we entertain, therefore, any suspicion that a philosophical term is employed without any meaning or idea (as is but too frequent), we need but inquire&nbsp;<i>from what impression is that supposed idea derived?&nbsp;</i>And if it be impossible to assign any, this will confirm our suspicion.\u201d Rationalists should more critically assess what impressions of the external world drive their <i>a priori</i> chains of argument.</p><p>This error also manifests in the common EA response to x-risk skeptics,&nbsp;<a href=\"https://philiptrammell.com/blog/46\"><u>\u201cBut Have They Engaged with the Arguments?\u201d</u></a> A failure to \u201cengage with the arguments\u201d is often levied as a slam-dunk critique of others who do not share a highly rationalist epistemology.</p><p>Theoretical work on AI safety can be incredibly valuable, as Anthropic notes, but their philosophy to prioritize empirically grounded research and to&nbsp; \u201c<a href=\"https://www.anthropic.com/index/core-views-on-ai-safety\"><u>show don\u2019t tell</u></a>\u201d is much more compelling.</p><p>There definitely is empirical evidence of some AI risks today, and again some views like Anthropic\u2019s&nbsp;<a href=\"https://www.anthropic.com/index/core-views-on-ai-safety\"><u>Core Views on AI Safety</u></a> are concerned on this basis. The empirical case for AI risk can still be compelling. But Anthropic builds much more uncertainty into their worldview than many in this debate. They take seriously the&nbsp;<i>possibility</i> that AI might become dangerous and recognize that theoretical work is necessary to inform empirical work. However, they note that their approach is likely to \u201crapidly adjust as more information about the kind of scenario we are in becomes available.\u201d</p><p>Hume echoes this sentiment in \u201cOf Commerce,\u201d giving practical advice for organizations as he warns against over-theorizing: \"When a man deliberates [...] he never ought to draw his arguments too fine, or connect too long a chain of consequences together. Something is sure to happen that will disconcert his reasoning.\" I would encourage OpenPhil at the margins to apply greater empirical rigor to their projections of AI risk and grant evaluation.</p><h3><strong>2. Even Our AI Models Are Empiricist</strong></h3><p>Throughout the history of AI, there was a debate about whether systems would learn by first encoding formal logic and reasoning or by processing vast amounts of data. The \u201csymbolic\u201d versus \u201cconnectionist\u201d schools were complex, but I think it\u2019s fair to say that the connectionists have kicked symbolic reasoning\u2019s ass. Our best models today learn by training on massive amounts of examples and making connections between them. To the surprise of many, logic and reasoning can even develop as&nbsp; emergent properties in models by conjoining lots of individual experiences.</p><figure class=\"image image_resized\" style=\"width:32.81%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/wlyhtenxucphimtfnsos\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/dex5986h6e88og9t6l3t 104w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/arvwrkgfj3xo4mwd5zdw 184w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/dfb5pcefurgbobplfs3b 264w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/d4kawjwxt0vgsdg9goiy 344w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/oce6egachdgeyevb4hpr 424w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/ygfdbcu8swqxqpdq5mhr 504w\"><figcaption>\u201cSymbolic vs. Analogical Man\u201d from&nbsp;<i>Artificial Intelligence at MIT,&nbsp;</i><a href=\"https://web.media.mit.edu/~minsky/papers/SymbolicVs.Connectionist.html\"><u>Winston &amp; Minsky</u></a>, 1990</figcaption></figure><p>I do not think this would surprise David Hume. The success of connectionist AI systems is, in some ways, a vindication of his view of cognition. This isn\u2019t evidence in itself that humans develop knowledge by making associations between vast amounts of previous impressions, but it might move you in that direction. The best way we have discovered to get AI systems to learn is quite Humean.</p><p>Further, many Rationalists overrate the dangers of future AI systems because of their overly rationalist epistemology. Sure, if knowledge can be derived from thinking in an armchair hard enough, then a \u201csuperintelligent\u201d being could build nanomachines and kill humans in no time. But if knowledge is fundamentally downstream of observation of the world, as Hume suggests, then even advanced AI systems will be bottlenecked by experimentation and access to high-quality datasets.</p><p>Jacob Buckman makes this case in \u201c<a href=\"https://jacobbuckman.substack.com/p/we-arent-close-to-creating-a-rapidly?fbclid=IwAR0GAkegsetpW9D3UEKFPXZoVMSLrLhzGC777ZFr9mPnKUmGie8mLYRilt8\"><u>We Aren't Close To Creating A Rapidly Self-Improving AI</u></a>.\u201d He notes that the current paradigm allows systems to approach human capability on large high-quality datasets, but constructing these datasets is an incredibly difficult bottleneck. The best part of his piece happens in the comment section when a reader suggests that AI could still self-improve once it learns \u201cthe rules of reality\u201d such as math, logic, and physics. In a Humean style, Buckman responds that \u201cThe rules of reality are *not* logic/math/physics \u2014 you have it precisely backwards. In fact, logic/math/physics are just approximations to the rules of reality that we inferred from *observing* reality.\u201d I would encourage OpenPhil to consider the possibility of what this limitation would mean for the odds of an existential collapse caused by AI in the next century.</p><h3><strong>3. Thinking Historically</strong></h3><p>Hume\u2019s&nbsp;<i>History of England</i> is a story where, again and again, what seems at first like a causal outcome driven individuals was actually dependent on a vast amount of context, preconditions, and happenstance. Hume\u2019s historical contribution is to emphasize that nothing happens in a vacuum. A complex web of political regimes, climates, education systems, markets, social norms, etc., shaped the history of England, and Hume needed over 3000 pages to feel he\u2019d done justice to it. General principles can be gained from the study of history, but carefully, and always with the caveat \u201cit depends.\u201d</p><p>Because of this, Tyler Cowen critiques the \u201ctabula rasa\u201d way of reasoning about the future that many x-risk proponents take on\u2014 it can be a vast oversimplification of trends that are, even to the best full-time researchers today, causally uncertain. Context could be vastly more important than just armchair discussions of AI technology itself. If Hume is right, then Cowen bringing up China really matters. As would Taiwan, US political stability, and 1000 other potential factors beyond our foresight. We should still try our best to predict and plan for the future, but there is just too much information to try to grasp&nbsp;<i>a priori</i>. We will miss something.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/mbljlco5tqbeowouzjre\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/x7n3yzl3kpht2m0ujjm5 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/i27rzjoj1h4gmukzxydm 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/vbind2uqb2jycc70vlat 260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/fqrohgvkyx2hygqiiju2 340w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/ek4bxviwyelugdu0ggcf 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MDkYSuCzFbEgGgtAd/bdqrdhlmcdznls0prmqg 500w\"><figcaption><a href=\"https://twitter.com/cauchyfriend/status/1642739608885792768\"><u>@cauchyfriend on Twitter</u></a> (who is not me) on the&nbsp;<a href=\"https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html\"><u>Tyler Cowen</u></a> /&nbsp;<a href=\"https://astralcodexten.substack.com/p/mr-tries-the-safe-uncertainty-fallacy\"><u>Scott Alexander</u></a> x-risk correspondence. Scott struggles to steel-man Tyler\u2019s argument because they hold radically different epistemologies.</figcaption></figure><p>In the 80k podcast, Davidson presents an argument about history that is common in AI debates\u2014 people in the past had no idea what \u201ccrazy\u201d times were ahead of them, therefore speculative claims about the future should be taken seriously (or at least not dismissed). Davidson correctly notes that hunter-gatherers had no idea that sprawling empires would emerge, and feudal market vendors had no idea that technology would radically transform the world.</p><p>I think the lesson from these examples is actually that predicting the future is recognized to be a nearly impossible task.<strong>&nbsp;</strong>No blacksmith in 1450 could possibly have predicted the semiconductor with any degree of certainty, and no hunter-gatherer had enough experience to speculate on feudal siege warfare. Davidson\u2019s argument is that self-improving AI might be one among many uncertain futures, which is fair enough. But as Tyler Cowen&nbsp;<a href=\"https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html\"><u>writes</u></a>, \u201cthe mere fact that AGI risk can be put on a par with those other also distant possibilities simply should not impress you very much.\u201d Yes, it\u2019s true that we can imagine a future like this. But if anything, history shows the limitations of our capacity for speculation.</p><p>It\u2019s possible that we are the exception. Perhaps Baysenian superforecasting really is the key, or AI is close enough that real empirical evidence of doom is with us now. But the latter point is addressed in section 1 of this essay, and the former would be a shocking development. As Anthropic writes:</p><blockquote><p>\u201cThis view may sound implausible or grandiose, and there are good reasons to be skeptical of it. For one thing,&nbsp;<strong>almost everyone who has said \u2018the thing we\u2019re working on might be one of the biggest developments in history\u2019 has been wrong, often laughably so.</strong> Nevertheless, we believe there is enough evidence to seriously prepare for a world where rapid AI progress leads to transformative AI systems.\u201d</p></blockquote><p>There is an empirical/historical case to be made for AI risk, as Anthropic describes, but it is built upon uncertainty. Anthropic\u2019s commitment to change as new evidence emerges echoes Hume\u2019s point that \u201cthe most perfect philosophy of the natural kind only staves off our ignorance a little longer.\u201d The correct response to the p(doom) question\u2014 one with literally apocalyptic complexity\u2014&nbsp; is not to continue \u201carguing yourself\u201d one way or another. For this reason Cowen says he doesn\u2019t think arguing back on x-risk terms is the correct response. A more complex view of history is useful because demystifying the past can help demystify the future. We should acknowledge from Hume that \u201creason is slave to the passions,\u201d yet try our best to wade through the empirical evidence as it changes.</p><p>A historical approach could be criticized because, by definition, we cannot have historical examples of extinction events. The plane meme with the red dots, yes, very good. But we do have clear historical analogs: atomic bombs work; and bioengineered pandemics would use mostly existing tools on one of humanity\u2019s oldest threats. The difference between a fear of UFOs and AGI as an existential threat rests on the weight of the available evidence, not how compelling an&nbsp;<i>a priori</i> argument we can make about their possibility. More in the community should acknowledge this.</p><p>And while some may call this the \u201c<a href=\"https://astralcodexten.substack.com/p/mr-tries-the-safe-uncertainty-fallacy\"><u>Safe Uncertainty Fallacy</u></a>,\u201d arguing that uncertainty of existential risk should not mean it is safe to press ahead, I think incorporating greater uncertainty into your worldview is still actionable. If we are epistemologically limited, we can build that into our models.</p><h3><strong>4. Conclusion: Uncertainty Does Not Mean Inaction</strong></h3><p>Building more uncertainty into your worldview does not mean throwing up your hands and giving up on AI. If I have convinced you even at the margin to be more empiricist and to think more historically, here are a few concrete suggestions:</p><ul><li>You could give other essays in this contest higher scores that make empirical cases for or against AI doom. I lack the technical background to do this justice, but they might not.</li><li>You could base your own \u201cportfolio approach\u201d on Anthropic's, increasing funding to the possibility that we are living in an \u201calignment is difficult but tractable\u201d world over the \u201cit\u2019s clear&nbsp;<i>a priori&nbsp;</i>that we need a Butlerian Jihad\u201d world.</li><li>You could consider whether other aspects of OpenPhil\u2019s operations over-rely on a rationalist epistemology, or at least start having these conversations.</li></ul><p>Lastly, uncertainty should also shape how you prioritize other causes. You can still take the old-school-EA approach to problems that have a strong empirical track record, such as global health and animal suffering. I think so many of the \u201clongtermist\u201d trends in EA in recent years have been driven by a weaker epistemology, leaving the movement with a genuine conflict over how to develop knowledge about doing the most good. As someone who misses the early 2010s spreadsheet EA (sheets that track real-world data, not speculative powers of 10), I hope you take these ideas to heart.</p><p><br>&nbsp;</p>", "user": {"username": "Matt Beard"}}, {"_id": "MEmRhbKzgJqK3aajb", "title": "Relative values for animal suffering and ACE Top Charities", "postedAt": "2023-05-30T16:37:23.402Z", "htmlBody": "<p><strong>tl;dr</strong>: I present relative estimates for animal suffering and 2022 top Animal Charity Evaluators (ACE) charities. I am doing this to showcase a new tool from the Quantified Uncertainty Research Institute (QURI) and to present an alternative to ACE\u2019s current rubric-based approach.</p><h3>Introduction and goals</h3><p>At QURI, we\u2019re experimenting with using relative values to estimate the worth of various items and interventions. Instead of basing value on a specific unit, we ask how valuable each item in a list is, compared to each other item. You can see an overview of this approach <a href=\"https://forum.nunosempere.com/posts/EFEwBvuDrTLDndqCt/relative-value-functions-a-flexible-new-format-for-value\">here</a>.</p><p>In this context, I thought it would be meaningful to estimate some items in animal welfare and suffering. I estimated the value of a few a few animal quality-adjusted life-years\u2014fish, chicken, pigs and cows\u2014relative to each other. Then I using those, I estimated the value of top and standout charities as chosen by ACE (Animal Charity Evaluators) in 2022.</p><p>This exercise might perhaps be useful to ACE, not necessarily from the estimates themselves, which are admittedly mediocre, but rather by considering these estimates as a potential template for evaluating the value of uncertain interventions outside of global health and development.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MEmRhbKzgJqK3aajb/qr7ownpconvwj7ugikma\" alt=\"\"></p><h3>Link to the model</h3><p>You can view these relative estimates <a href=\"https://relative-values-git-animals-2023-04-quantified-uncertainty.vercel.app/interfaces/relative-values-animals-2023-04/models/relative-values-animals-2023-04\">here</a> (<a href=\"https://web.archive.org/web/20230529163148/https://relative-values-git-animals-2023-04-quantified-uncertainty.vercel.app/interfaces/relative-values-animals-2023-04/models/relative-values-animals-2023-04\">a</a>). The app in which they live has different views:</p><p>A view showing all of the estimates compared to each other:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MEmRhbKzgJqK3aajb/tevgfzrduqx69k1ohbmm\" alt=\"\"></p><p>A view showing items\u2019 values compared to one reference item:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MEmRhbKzgJqK3aajb/ux6dr3ypzacxksxdthi9\" alt=\"\"></p><p>There is also a view plotting <a href=\"https://relative-values-git-animals-2023-04-quantified-uncertainty.vercel.app/interfaces/relative-values-animals-2023-04/models/relative-values-animals-2023-04/plot\">uncertainty vs value</a>, and a view showing <a href=\"https://relative-values-git-animals-2023-04-quantified-uncertainty.vercel.app/interfaces/relative-values-animals-2023-04/models/relative-values-animals-2023-04/edit\">the underlying code</a>, which is editable (though slow).</p><h3>Discussion</h3><p>Expected quality of the model</p><p>I expect these estimates to have numerous flaws. Previously, I worked on an aggregator for forecasts called Metaforecast, as part of which I assigned a \u201cstars rating\u201d to quickly signal the expected quality of probabilities from different platforms. If I applied that same rating here, these estimates would have a stars quality rating of one out of five possible stars, at most.</p><p>One key insufficiency of these estimates is that they estimate what I personally value after a short amount of reflection. They don\u2019t necessarily represent what the entire Effective Altruism community or any particular philosophical viewpoint might value after in-depth reflection. I chose this approach mainly for efficiency. Future iterations might adopt a more sophisticated approach, such as allowing users to input their own values, or selecting from several philosophical perspectives, or aggregating them.</p><h3>Methodology</h3><p>I came up with these estimates in three steps:</p><ol><li>Estimated the Quality-Adjusted Life Years (QALYs) value of a few animal species</li><li>Mechanistically estimated the value of three reference charities, in terms of QALYs for the species valued in step 1</li><li>Estimated the value of the remaining charities in terms of the reference charities in step 2.</li></ol><p><strong>Estimating relative value of animal QALYs</strong></p><p>I started by estimating the relative value of a QALY for a few animal species. Then I derived estimates for QALY per kilogram and QALYs per calorie, which could later be useful for improving calculators like <a href=\"https://foodimpacts.org/\">this one</a>.</p><p>Here is an example for cows:</p><pre><code>// Add human QALY as a reference point\none_human_qaly = {\n  id: \"one_human_qaly\", \n  name: \"1 human QALY (quality-adjusted life-year)\",\n  value: normal(1, 0.01)\n}\n\n// Cows\nvalue_happy_cow_year = 0.05 to 0.3 \n// ^ in human qalys\nvalue_tortured_cow_year = -(0.1 to 2)\nvalue_farmed_cow_year = normal({ p10: -0.2, p90: 0.1 })\n// ^ purely subjective estimates\n// the thing is, it doesn't seem that unlikely to me\n// that cows do lead net positive lives\nweight_cow = mixture([450 to 1800, 360 to 1100], [1/2,1/2])\nnon_wastage_proportion_cow = (0.5 to 0.7) -&gt; ss // should be a beta. \nlifetime_cow = (30 to 42) / 12\ncalories_cow = mixture(0.8M to 1.4M, (500k to 700k) * (weight_cow * non_wastage_proportion_cow)/1000) \n// ^ kilocalories, averaging two estimates from\n// &lt;https://www.reddit.com/r/theydidthemonstermath/comments/a8ha9r/how_many_calories_are_in_a_whole_cow/&gt;\n\ncow_estimates = {\n  name: \"cow\",\n  value_year: value_farmed_cow_year -&gt; ss,\n  weight: weight_cow,\n  calories: calories_cow,\n  lifetime: lifetime_cow -&gt; ss\n}\n</code></pre><p><strong>Coming up with mechanistic estimates for three reference projects</strong></p><p>I then looked at three reference projects for which I thought a mechanistic estimate might be feasible: the Fish Welfare Initiative (FWI), Beyond Burgers, and the Open Wing Alliance. For each of those projects, I estimated how many specific animals did they affect, and by how much, and arrived at a wide subjective estimate of their impact.</p><p>For example, in the case of FWI I looked at their impact page for the number of animals they probably have helped. I then came up with an uncertain estimate for how much they had helped each animal. I took various shortcuts, for example, I pretended that the fish which FWI helped were salmon, because details about their life expectancy and caloric content were easy and quick to look up online. In fact, I expect the vast majority of fish that FWI helps to not be salmon, but I don\u2019t expect the difference to matter all that much when estimating total impact.</p><p>Here is how my estimate for the Fish Welfare Initiative looks:</p><pre><code>fish_potentially_helped = 1M to 2M\nshrimp_potentially_helped = 1M to 2M\nimprovement_as_proportion_of_lifetime = (0.05 to 0.5) -&gt; ss\nsign_flip_to_denote_improvement(x) = -x\n\nvalue_fwi_fish = (\n    fish_potentially_helped * \n    improvement_as_proportion_of_lifetime * \n    (\n       salmon_estimates.value_year /\n       Salmon_estimates.lifetime\n    )\n  ) -&gt; sign_flip_to_denote_improvement\n\nvalue_of_shrimp_in_fish = (0.3 to 1)\n// ^ very uncertain, subjective\nvalue_fwi_shrimp = (\n    shrimp_potentially_helped * \n    improvement_as_proportion_of_lifetime * \n    (\n       salmon_estimates.value_year /\n       Salmon_estimates.lifetime\n    ) *\n    value_of_shrimp_in_fish\n  ) -&gt; sign_flip_to_denote_improvement\n\nvalue_fwi_so_far = value_fwi_fish + value_fwi_shrimp\nproportion_fwi_in_2022 = 1/4 to 1/2\nvalue_fwi  = value_fwi_so_far * proportion_fwi_in_2022 \n\nfwi_item = {\n  name: \"Fish Welfare Initiative\",\n  year: 2022, \n  slug: \"fish_welfare_initiative\",\n  value: value_fwi -&gt; ss \n}\n</code></pre><p><strong>Estimating other charities in terms of the reference projects</strong></p><p>I estimated the value of the remaining projects in terms of the previous three. For example, here is my estimate of the Good Food Institute:</p><pre><code>value_reference_top_animal_org = mixture(\n  [\n    fwi_value,\n    open_wing_alliance_value, \n    beyond_meat_value/(10 to 1k)\n    // ^ beyond meat seems significantly more scaled up than the avg org working to affect cows\n  ], \n  [ 1/3, 1/3, 1/3 ]\n) -&gt; SampleSet.fromDist\n\nbeyond_meat_equivalents_gfi = 0.01 to 2\nvalue_gfi = mixture(\n  [ \n    beyond_meat_equivalents_gfi * beyond_meat_item.value, \n    value_reference_top_animal_org\n  ], \n  [ 2/3, 1/3 ]\n)\n</code></pre><p>and here is my estimate for Compassion USA:</p><pre><code>value_compassion_usa = mixture(\n  [\n    open_wing_alliance_value * \n      truncateRight(0.05 to 10, 100), \n    value_reference_top_animal_org * \n      truncate(0.05 to 10, 100)\n  ], \n  [ 1/2, 1/2 ]\n) \n</code></pre><p><strong>A comment on maintaining correlations</strong></p><p>These estimates are written in Squiggle, which aims to make it easy to do relative values through its functionality around sample sets. For example,</p><pre><code>x = SampleSet.fromDist(1 to 100) \ny = 2 * x y/x \n// ^ is a distribution which is 2 everywhere\nor\nx = SampleSet.fromDist(1 to 100)\ny = SampleSet.fromDist(2 to 200) \nz = x * y (z/x) / y \n// ^ is one everywhere\n</code></pre><p>I\u2019ve usually shortened SampleSet.fromDist to just \u201css.\u201d</p><p>As a note of caution, note that maintaining correlations while having mixtures of different distributions is more tricky.</p><h3>Conclusion</h3><p>This post presents a model that starts with very rough estimates of the value of several types of animal suffering. It then uses these to build up mechanistic estimates of a few animal charities, and then uses those mechanistic estimates to give a guess as to the impact of all top ACE charities in 2022.</p><p>The motives for doing that were:</p><ul><li>To showcase some tooling recently built at QURI</li><li>To show one possible path for having quantified estimates for speculative projects\u2014as opposed to the rubric-based approach that organizations like ACE or Charity Entrepreneurship use.</li></ul><h3>Acknowledgements</h3><figure class=\"image image_resized\" style=\"width:20%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MEmRhbKzgJqK3aajb/ok2eutq1b2xebuwnf43o\"></figure><p>This is a project of the Quantified Uncertainty Research Institute, from which I\u2019ve since then taken a leave of absence. Thanks to Ozzie Gooen and Holly Elmore for their feedback.</p>", "user": {"username": "NunoSempere"}}, {"_id": "4axuXzkPupcKzeQaA", "title": "Boomerang - protocol to dissolve some commitment races", "postedAt": "2023-05-30T16:24:37.107Z", "htmlBody": "<p><i>Work done during SERI MATS 3.0 with mentorship from Jesse Cliffton. Huge thanks for all the feedback and discussions to Anthony DiGiovanni, Daniel Kokotajlo, Mart\u00edn Soto, Rubi J. Hudson and Jan Betley!</i> <a href=\"https://forum.effectivealtruism.org/posts/4axuXzkPupcKzeQaA/boomerang-protocol-to-dissolve-some-commitment-races\"><i>Also posted to EA forum.</i></a></p><p><a href=\"https://www.lesswrong.com/posts/brXr7PJ2W4Na2EW2q/the-commitment-races-problem#:~:text=some%20other%20day.-,Consequentialists%20can%20get%20caught%20in%20commitment%20races%2C%20in%20which%20they,capitulate%20and%20won't%20retaliate.\">Daniel's post about commitment races</a> motivates why they may be a severe problem. Here, I'll describe a concrete protocol that if adopted, would let us avoid some cases of miscoordination caused by them.</p><h1>TL;DR</h1><p>The key ingredient is having a mandatory time delay, during which the commitments aren't yet binding. At the end of that delay, you decide whether to make your commitment binding or revert it, and this decision can be conditional on previous decisions of other participants. This in itself would give rise to new races, but it can be managed by adding some additional rules.</p><p>I think the biggest challenge would be to convince the \"commitment infrastructure\" (which I describe below) to adopt such a protocol.</p><h1>Benefits</h1><ul><li>In the case of the game of chicken, the 3 rules listed below should often push whoever committed later to Swerve.<ul><li>In simple games like chicken it may be achieved easier, just by relying on conditional commitments (\u201cIf I came second, I Swerve\u201d).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnptnoqim50s\"><sup><a href=\"#fnnptnoqim50s\">[1]</a></sup></span>&nbsp;But here we add another mechanism:&nbsp;<u>tentative commitment period</u>, which gives us another nice feature:</li></ul></li><li>In the real world it can be not so obvious that some commitments are incompatible.&nbsp;<u>The tentative period gives the agents time to analyze the situation in depth and check if any commitments are clashing.</u> This is especially useful in highly multipolar cases where multiple parties try to commit at the same time or where actions have complex consequences and interactions.<ul><li>We also don't need to know in advance all the actions the others can take - we can analyze their actions after they've already tentatively committed to them.</li></ul></li><li>Even if participants manage to coordinate (so one Dares and one Swerves), the solution found hastily during a commitment race can still be quite poor. Boomerang enables bargaining that can Pareto improve on this hasty solution.</li></ul><h1>Necessary ingredients</h1><p>The protocol relies on some mechanism M on which agents can make commitments - a \"commitment infrastructure\". M could be something like the Ethereum network, or some powerful international body.</p><p>We require that:</p><ol><li>When someone publishes a commitment, M arrives at a consensus about the time at which the commitment was published.</li><li>It\u2019s more convenient/cheap/credible to do some commitments on M that outside of it.</li></ol><p>2. is needed because the protocol relies on certain commitments being forbidden. Agents could decide to do those forbidden commitments outside of M, so we need to make that as hard as possible for them, compared to committing on M. <u>I think this is the hardest part of the whole proposal.</u> M would need to be locked into place by a network effect - everyone is using M because everyone else is using M.</p><h1>Protocol</h1><p>Here are the rules:</p><ul><li>R1: All commitments have a&nbsp;<u>mandatory tentative period</u>, meaning that they only become binding after some fixed time T (we can say that&nbsp;<i>freeze_time = publish_time + T</i>).<ul><li>So you have no way to make a commitment credible before&nbsp;<i>freeze_time</i> (if you were allowed to end the tentative period, we would create a new race to end the tentative period as soon as possible).</li></ul></li><li>R2: During the tentative period, you can still decide to revert your commitment.<ul><li>Somewhere before the&nbsp;<i>freeze_time&nbsp;</i>you send to M your final decision (whether you revert or not),&nbsp;<u>but hashed.</u><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref39vlakzs81d\"><sup><a href=\"#fn39vlakzs81d\">[2]</a></sup></span><ul><li>You also need to add random salt to your decision before hashing, so that it cannot be revealed through brute-forcing.</li></ul></li><li>After&nbsp;<i>freeze_time&nbsp;</i>you reveal the decision (and it must match the previously sent hash, otherwise M would reject your commitment).<ul><li>So in the analogy of the game of chicken - you threw out your steering wheel, but here it boomerangs back to you, giving you your last chance to catch it.</li></ul></li><li>You shouldn\u2019t be able to reveal the final decision to anyone before&nbsp;<i>freeze_time</i> because we don\u2019t want the commitment to get credible before&nbsp;<i>freeze_time.</i><ul><li>To ensure that, we add a rule that&nbsp;<u>anyone who knows the final decision before&nbsp;</u><i><u>freeze_time</u></i><u> has the power to revert the commitment.</u></li><li>Now, if you reveal the decision to your opponent, they will probably break your commitment.</li></ul></li></ul></li><li>R3: Your final decision is allowed to be conditional on the final decision of some other commitment, if and only if your&nbsp;<i>freeze_time</i> comes after the&nbsp;<i>freeze_time</i> of that other commitment.</li></ul><p>Those rules may seem like a lot, but I think they (or some comparably complex set of rules) are all needed if we want to avoid creating new races later in time. The aim is to have only one race, at the very beginning, and everything else should be calm, non-racy and completely independent of agents' speed of making commitments (f.e. what their ping is, or how well connected they are with the commitment infrastructure).</p><h1>Example</h1><p>We have a modified game of chicken with the following payoffs:</p><ul><li>if you both Dare, you die, which is worth -100 utils</li><li>if you Dare and your opponent Swerves, you prove that you're a badass which is worth 10 utils</li><li>if you Swerve, you drive into a shrubbery, which ruins your car's awesome paint job, which is worth -20 utils</li><li><u>there also may be some additional actions available, but they are not obvious</u></li></ul><p>Let's set the length of the tentative period at one minute, and let\u2019s say that they have 3 minutes before they potentially crash into each other.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/bdttpyzrtg8y0gtfnsgl\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/ivsmbsftjxnmzsdy132u 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/ahfbkimazg6fbqwedknb 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/y7sbjot9gogibqqpp1of 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/epdiluqw6lehjklaslt4 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/mqdcu13cp20or3lymizc 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/tc3p7auuse2afzr71orv 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/ckugry9cimjvqniuurty 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/kvdbz1hnpxmchjg9mcov 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/hyeprlixs1uvd6s79wmw 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/geicupxhdkfssxh6otep 1058w\"></p><ul><li>0:00 - Race starts.</li><li>0:01 - Alice publishes a commitment <i>\"I Dare\"&nbsp;</i> - it's like throwing her steering wheel out the window - the wheel will \"boomerang back\" at 1:01 at which point if Alice doesn't \"catch it\", the commitment becomes final.</li><li>0:02 - Bob didn't see in time that Alice threw out the wheel, so he publishes a commitment <i>\"I Dare\"&nbsp;</i> - it's like throwing his steering wheel out the window - the wheel will boomerang back at 1:02. At this point, in a regular game of chicken they would be doomed. But here, there's still hope.</li><li>0:53 - Bob sends out&nbsp;<i>Hash(\u201cIf Alice doesn't revert her commitment to Dare, I Revert this commitment\u201d)</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefphneejtvsvt\"><sup><a href=\"#fnphneejtvsvt\">[3]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrqn0a7cuk9c\"><sup><a href=\"#fnrqn0a7cuk9c\">[4]</a></sup></span></li><li>0:55 - Alice sends out&nbsp;<i>Hash(\u201cI don't revert\u201d)</i></li><li>1:04 - Bob reveals the original decision:&nbsp;<i>\u201cIf Alice doesn't revert her commitment to Dare, I Revert this commitment\u201d</i></li><li>1:07 - Alice reveals the original decision:&nbsp;<i>\u201cI don't revert\u201d</i></li><li>1:07 - M makes Alice\u2019s&nbsp;<i>\u201cI don't revert\u201d</i> binding, and then also resolves Bob\u2019s decision to&nbsp;<i>\u201cI Revert this commitment [to dare]\u201d</i>. The fact that Alice is now committed to Dare, later makes Bob Swerve.</li></ul><p>Note that in principle at 0:53 Bob could instead decide to unconditionally Dare even though he is second, hoping that Alice may be too scared to Dare.</p><p>But with Boomerang such ruthless Daring is much less likely than without it. At the time of decision, Alice and Bob have a shared knowledge of who is first, and also only the second one can make a conditional commitment. This breaks the symmetry of the original game of chicken. The option of making the conditional commitment (when you have that option) is pretty compelling - it's both safe and taking opportunities when they arise. Additionally it would create a focal point of what the participants are \"supposed to do\" - everyone expects that the first committer gets to Dare and the second must do a conditional commitment, and diverting from this equilibrium would only hurt you.</p><h1>Addition of bargaining</h1><p>With the three rules described above, we managed to avoid the most catastrophic outcome. But that outcome is still pretty poor, because the initial commitments were chosen with almost zero thought. If agents later notice some Pareto improvement, to move to this new solution the first agent (Alice) would need to revert her first commitment and give up her privileged position. To be willing to do it, Alice would need a guarantee from the second agent (Bob) that he will also revert. But in the existing protocol, Alice cannot have such a guarantee, because after Alice reverts, Bob could still do whatever - R3 forbids conditioning on commitments that come after yours.</p><p>To fix that, we can add another rule:</p><ul><li>R4: you can allow some other commitment to condition on your commitment&nbsp;<u>even if its freeze time comes before yours</u> but&nbsp;<u>they still have the right to reject this option</u><ul><li>This right to reject may seem counter-intuitive, but being unable to condition on others is actually a privilege. It makes your commitment more credible and it is this them who are pushed to Swerve.</li></ul></li></ul><p>It may be tricky to see how that helps, so let's rerun our example with that new rule:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/igpwd4jsob84iuklyz6v\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/snuxlojjbey9io2wdttd 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/av8xppayaveddzyfv6nd 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/offxcamxyvgssyu7tmcq 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/h0g1sakwfvveddx6l9sj 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/dk3mrgvuklujgvjaaf66 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/smracmshwrdoqqvrqq8b 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/u6jnkkydp7lj0bdgssxb 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/pqpefhnbvam2ymzl7d1d 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/dkpjgfnkdvqa35vtjdrh 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/4axuXzkPupcKzeQaA/y4xvjehm1ai8kk260xal 1059w\"></p><ul><li>0:00 - Race starts.</li><li>0:01 - Alice throws her steering wheel out the window.&nbsp;</li><li>0:02 - Bob throws his steering wheel out the window.</li><li>0:37 - Bob realizes that they can Pareto improve over the previous outcome! They could just both stop, and he will publicly declare that Alice is more badass than him, and also pay her one util. This gives him a payoff of -1 instead of -20, and for Alice +11 instead of +10. He tentatively commits to do this, if Alice reverts her commitment to Dare. (Bob's new commitment can become final at 1:37.)&nbsp;<u>He also allows Alice to condition her decision (at 1:01) on his decision (at 1:37).</u></li><li>0:53 - Bob sends out&nbsp;<i>Hash(\u201cIf Alice doesn't revert her commitment to Dare, I Revert this commitment\u201d)</i></li><li>0:55 - Alice sends out&nbsp;<i>Hash(\u201cIf Bob doesn\u2019t Revert that commitment from 0:37, I revert my commitment to Dare\u201d)</i></li><li>1:04 - Bob reveals the original decision:&nbsp;<i>\u201cIf Alice doesn't revert her commitment to Dare, I Revert this commitment\u201d</i></li><li>1:07 - Alice reveals the original decision:&nbsp;<i>\u201cIf Bob doesn\u2019t Revert that commitment from 0:37, I revert my commitment to Dare\u201d</i></li><li>1:31 - Bob sends out&nbsp;<i>Hash(\u201cFollow through with the new commitment\u201d)</i></li><li>1:39 - Bob reveals the original decision:&nbsp;<i>\u201cFollow through with the new commitment\u201d</i></li><li>1:39 - M makes Bob\u2019s plan binding - he must now stop, declare Alice to be more badass and pay her; then M resolves Alice\u2019s conditional commitment to&nbsp;<i>\u201cI revert my commitment to Dare\u201d</i>. The fact that Bob stopped his car on the tracks and they've already established that she's the badass, makes her stop too.</li></ul><p>We could even have a chain of multiple commitments \u201cconditioning on the future\u201d. In practice we may want to limit that somehow, so that the resolution cannot be delayed indefinitely.</p><h1>Future work</h1><ul><li>Adoption. - What are the most tractable ways of implementing this protocol? F.e. if agents make the commitments on Ethereum network or in some international legal body, how do we convince all the nodes in the network / participating nations to enforce the protocol?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz7jppfajag\"><sup><a href=\"#fnz7jppfajag\">[5]</a></sup></span><ul><li>It's probably easiest to start by convincing a handful of most important players. Then the rest of players may gradually follow.</li><li>Once you have that system in place, to lock it in, participants may want to penalize those who make commitments using some alternative mechanisms.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0prsbl5u2f0s\"><sup><a href=\"#fn0prsbl5u2f0s\">[6]</a></sup></span>&nbsp;(h/t Jan Betley)</li></ul></li><li>Reputation effects. - You may choose to make ruthless unconditional commitments (like Daring when you know someone else already committed to Dare) to build a ruthless reputation. This way, in future interactions you may hope that others will Swerve even when they committed first.&nbsp;<u>Could this buildup of ruthless reputation be prevented?</u> F.e. we could require full anonymity of the agents. Or rather be more selective, and somehow \u201cerase the identity\u201d of whoever behaves ruthlessly.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefm14plfif1pq\"><sup><a href=\"#fnm14plfif1pq\">[7]</a></sup></span></li><li>Gaming the system using proxies. - The protocol could be broken if you state your decision publicly, and then pay some proxy to salt and hash this decision and upload this hash in your name (by somehow authorizing it to do it, f.e. giving it your private key). As long as this proxy doesn\u2019t reveal this salt to anyone, no one will invalidate your commitment. And to the extent that the proxy is known to behave reliably, your publicly stated decision is credible. How do we break such proxies or make them impractical?</li><li>Can you find other ways to break the protocol?</li></ul><h1>Appendix</h1><p>Some non-crucial technical details that you may want to skip:</p><ul><li>rule R4 gives rise to a kind of iterated ultimatum game:<ul><li>Alice could reject the option to condition on Bob's new commitment, making that new commitment useless</li><li>it would force Bob to propose a new one, with a split more favorable to Alice</li><li>this rejecting+offering may continue right up until <i>freeze_time</i>, so it has some potential for miscoordination (Alice takes the risk of rejecting a very late offer, and Bob doesn't have enough time to offer a new one)</li><li>so we may need to add a limit, that you can\u2019t offer this option closer to their&nbsp;<i>freeze_time</i> than some duration, and also they need to wait some (shorter) duration before rejecting your option</li><li>this way it would be clear that some offer is the last one</li></ul></li><li>cycles<ul><li>conditioning on the future could make some commitments rely on themselves in a circular way</li><li>easiest way to prevent it, is that when you allow an earlier commitment C1 to condition on your C2, you must give up the power to condition on anything between C1 and C2</li><li>I\u2019m not sure yet if that\u2019s enough for more complex graphs of conditioning - anyway, we could somehow calculate which periods you cannot condition on</li><li>or, a potentially more powerful but tricky option, is to <u>embrace the circularity</u>, and if we have multiple ways to satisfy a circular commitment, choose a Pareto optimal way<ul><li>this has a similar vibe to open-source game theory</li><li>agents would need to state their preferences for all those possible ways</li><li>if there are multiple Pareto optimal solutions, probably agents with commitments earlier on the timeline should have priority</li><li>if there is no way to satisfy the cycle, we should break it and probably again resolve in favor of earlier committers, but I\u2019m not sure about the details here</li></ul></li></ul></li><li>we may get another race to lock in the hash of your final decision right after you published your tentative commitment, because if you lock in very soon, that weakly indicates that you decided not to revert<ul><li>to fix that, we may also require that you are only allowed to lock in the hash of your final decision after some time has passed since the start of the tentative period<br>&nbsp;</li></ul></li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnptnoqim50s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnptnoqim50s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This would only work in very simple cases like chicken, because you would need to know in advance what are all the possible commitments that others can make, so that you can define what \"being second in a race\" exactly means.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn39vlakzs81d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref39vlakzs81d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Alternative rule could be to have M generate some random number at freeze_time, and only then an agent can make the final decision, because we require them to reference that number in the decision message. But that could create a race, where the second committer decides to Dare anyway, because they hope this information will reach the first committer soon enough to sway them. For this reason we would need to postpone the generation of second committer's random number, to wait for the first committer's decision. But if the protocol is used by a lot of agents at the same time, and we play it safe and assume that everyone may potentially clash with anyone, then we have to postpone every commitment on the network which scales badly.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnphneejtvsvt\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefphneejtvsvt\">^</a></strong></sup></span><div class=\"footnote-content\"><p>To be clear, the decisions would actually be written as formal statements, not natural language, and also explicitly state which commitments they reference.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrqn0a7cuk9c\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrqn0a7cuk9c\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The order of sending these hashes is irrelevant here. That's why Bob can send that hash first, even though he's the second committer.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnz7jppfajag\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefz7jppfajag\">^</a></strong></sup></span><div class=\"footnote-content\"><p>It may be better to adopt Boomerang sooner than later: After someone already established a strategic advantage that lets them commit more ruthlessly, they will oppose the adoption of such a protocol. But agents should be keener to accept the protocol if they don't know yet if they'll be the advantaged or disadvantaged ones.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0prsbl5u2f0s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0prsbl5u2f0s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This works best if commitments on those alternative mechanisms are crisp, so that you can clearly define what will be penalized. F.e. committing through smart contracts is crisper than committing through staking your reputation.<br><br>But this penalization may be tricky, because it's costly for the penalizer, and you would prefer others to carry this cost. So it requires participants to coordinate to all penalize together. <a href=\"https://forum.effectivealtruism.org/posts/tXavWgk8Xp6Avg8No/quadratic-funding-without-a-matching-pool-1\">Here's an example technique</a> which may help.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnm14plfif1pq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefm14plfif1pq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>But if we require full anonymity, we lose any positive reputation effects we had. And if we \u201cerase the identity\u201d of whoever behaves ruthlessly, then encountering someone with a fresh identity serves as evidence that they are ruthless, defeating the purpose of this erasure.</p></div></li></ol>", "user": {"username": "Filip Sondej"}}, {"_id": "ysC6crBKhDBGZfob3", "title": "Announcing Apollo Research", "postedAt": "2023-05-30T16:17:20.135Z", "htmlBody": "", "user": {"username": "mariushobbhahn"}}, {"_id": "HFhghgtXaxtAPnxHo", "title": "Advice for new alignment people: Info Max", "postedAt": "2023-05-30T15:42:21.280Z", "htmlBody": "", "user": {"username": "Jonas Hallgren"}}, {"_id": "F8B4JTgfDMXDd7q7G", "title": "Implications of AGI on Subjective Human Experience", "postedAt": "2023-05-30T18:47:25.977Z", "htmlBody": "<h1><strong>Introduction</strong></h1><p>Humanity is heading towards unprecedented circumstances in a destined rendezvous of two distinct forces. The continuation of artificial intelligence (AI) development toward artificial general intelligence (AGI) at the current rate and direction without an increase in health measures informed by neuroscience has serious mental health implications for human beings. The meeting of the exponential nature of AGI and the ancient nature of human psychology and biology is likely to result in unrecoverable psychological and social damages, specifically because of A(G)I\u2019s (AI and AGI) consequences on the human mind and our relational experiences. Considering the current trends of social media recommendation algorithms\u2019(SM-RA) effects on the human brain, early observations from large language models (LLMs), and insights from neuroscience, we see a side of this trending set of reinforcing feedback loops amplifying internal human suffering. This distress could span as far as atrophy of higher functions of the brain and mind.</p><p>Envision the timeline of humanity\u2019s existence as a 1,000 page book. Based on what we know of human history, all of civilization would be contained in the final few pages. Much of the technological progress happened in the last few paragraphs<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefw7z4fqrfuqj\"><sup><a href=\"#fnw7z4fqrfuqj\">[1]</a></sup></span>. Given that technology\u2019s rate of change is exponential, in the next 25 years, we will see something like 100 years worth of progress. And in 100 years from now, we will see something like 20,000 years worth of change. The ancient nature of our minds and bodies, which still reflect basic survival instincts that were required for the earliest<i>&nbsp;homo sapiens</i>, cannot comprehend nor keep up with this speed of growth. While evolution can be hastened by external conditions, our attention, memory, and emotional networks cannot keep up with the steep curve of current technological development shy of human-machine integration.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9sdka405hwu\"><sup><a href=\"#fn9sdka405hwu\">[2]</a></sup></span>&nbsp;If we disregard the impacts of this discrepancy in the creation phase of AGI, a dystopian reality may be bad enough to outweigh the utopia. A mismatch to our new psychological ecology (the digital world) would result in a chaotic subjective experience.</p><p>People who create AI are generally not experts in mental health, and experts in mental health tend not to build tech. We cannot afford this division as this technology will affect all of civilization at a foundational level. A secure bridge between these disciplines in the planning and development of A(G)I is required for a humane future. &nbsp;</p><p>Tools intended to serve humanity should be designed by humans for humans with the purpose of promoting well-being. Builders of technology prioritizing their enamor for inventiveness and efficiency over human happiness invites disastrous results for what we consider a fulfilling life. We need to alter the current course of development and consider how to best ensure that we remain in control of this technology with care for the human condition. An approach that favors the utopian fantasy of playing God and seeing what happens is reckless and potentially self-destructive. Instead, our innovation must be channeled through the collective wisdom accumulated through the ages about what it means to live a good life.</p><h1><strong>Forces at Play</strong></h1><h2><i>Exponential Tech</i></h2><p>To draw a picture of the likely future, it is vital to appreciate the counterintuitive nature of&nbsp;exponential&nbsp;technological growth, as well as AGI as its driver. Considering Moore\u2019s Law, we know that computational power doubles every two years<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhpukj9irkhm\"><sup><a href=\"#fnhpukj9irkhm\">[3]</a></sup></span>. That means by 2070, computational power will have doubled 23.5 times what it is right now. We\u2019ve done the math. Computational power will be <i>over 8 million times</i>&nbsp;that which it is today. It is nearly impossible to imagine what this might mean for humanity. What will the world look like when there is a technology with that much power and probable decision-making capabilities? And what does the internal human experience feel like at this point - after 47 years of atrophy caused by machines solving our problems for us? The default, unworked mind is one that leads to suffering, as will be explained later.</p><p>Kurzweil\u2019s Law of Accelerating Returns expands Moore\u2019s Law to other evolutionary systems and explains that the \u201crate of exponential growth is itself growing exponentially.\u201d As Kurzweil reasons, \u201cThe implications include the merger of biological and nonbiological intelligence, immortal software-based humans, and ultra-high levels of intelligence that expand outward in the universe at the speed of light.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnnnz7tkaho\"><sup><a href=\"#fnnnnz7tkaho\">[4]</a></sup></span>&nbsp;This is not as far-fetched as it might sound. On the contrary, this statement may well be conservative. No one may be able to grasp what life will look like 50 years from now. Our limited perception cannot really imagine what technology will be capable of in 2070, and therefore we do not have the concepts and words to describe it. It would be like asking humans from the stone age to imagine cars (let alone self-driving cars) and making video calls over the internet (the inter-what?). With exponential growth, nothing much seems to happen for a while, and then suddenly everything seems to happen all at once. It is not possible to address compounding issues retroactively; by the time they are visible, it\u2019s already too late. That\u2019s why it\u2019s strategic to err on the side of caution.</p><p>This dynamic has occurred throughout human history and is not unique to the digital age. However, we just happen to be the generations alive to witness the inflection point where change moves beyond comprehension, but with minds and bodies the same as our ancestors. As Edward O. Wilson describes, \u201cThe real problem of humanity is the following: we have Paleolithic emotions, medieval institutions and godlike technology. And it is terrifically dangerous, and it is now approaching a point of crisis overall.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8ii0lp3ugcg\"><sup><a href=\"#fn8ii0lp3ugcg\">[5]</a></sup></span>&nbsp;Given that AGI is likely to exceed human intelligence and will have the ability to improve itself, the most honest prediction that experts can make is that they don\u2019t know what will happen. It\u2019s being built anyway.</p><h2><i>Ancient Human Nature</i></h2><p>The brains and bodies of humans have remained largely unchanged within the last 70,000 years and have evolved with a negativity bias to ensure survival in what used to be a dangerous natural environment. Psychologist Rick Hanson often describes our brains as being like velcro for suffering and teflon for happiness. The brain has unique networks to quickly record negative experiences in memory so that it can more easily learn from these experiences. Whereas positive experiences need to be focused on for many seconds so that they can be transferred from short-term to long-term memory (and remembered in the future.)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpvufc7vhxdk\"><sup><a href=\"#fnpvufc7vhxdk\">[6]</a></sup></span>&nbsp;This is the default state of mind that provides the inputs for current SM-RA. In other words, without an intentional positivity practice, there is an inclination towards negativity. We train AI with this bias, and then AI feeds the bias back into our brains through our engagement with the technology.</p><p>In the modern age, our negativity bias and stress-response system gets triggered by less life threatening stimuli than we experienced during the first 999 pages of existence (to use the 1,000 page book metaphor again.) The triune brain is a model for understanding the three main parts of the brain from an evolutionary standpoint: the reptilian brain (brainstem/basal ganglia), the paleomammalian brain (limbic system), and the neomammalian brain (neocortex). These three complexes are believed to be independently conscious, and each one responds to different types of stressors in distinct ways. Under physical stress, the reptilian brain goes into fight-flight-freeze to ensure survival. Under social stress, the paleomammalian brain shifts to a fear-based reactivity. And under role stress, the neomammalian brain becomes distracted.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkk6a5gqnl78\"><sup><a href=\"#fnkk6a5gqnl78\">[7]</a></sup></span>&nbsp;AI is on a trajectory to systematically and continuously trigger the paleomammalian and neomammalian systems, which would lead to increased anxiety and distraction. &nbsp;</p><p>Consider the default mode network (DMN), which is a self-referential network of neurons that fire when we are not focused on a task. The DMN is active when we\u2019re thinking without explicit goals for thinking, such as daydreaming, and people with depression and anxiety have higher levels of DMN activity. When we engage with something in a focused way, DMN activity ceases, and other parts of the brain are activated.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdjcctkqr4cc\"><sup><a href=\"#fndjcctkqr4cc\">[8]</a></sup></span>&nbsp;With a decrease in focused, purposeful work, more people will end up with enhanced stimulation of the DMN. Flow states, creative bursts, and transcendent experiences, which all require unusual effort, are unlikely to become the norm as some AGI utopians suggest. &nbsp;</p><p>The salience and emotion network (SEN) is another network of neurons that scouts for emotionally salient information as a way to keep ourselves safe. If it finds something that could be a threat, it sends the information to either the executive control network (ECN) or the DMN. When the information goes to the ECN, people can make purposeful choices. The ECN allows for voluntary control of behavior according to one\u2019s goals. On the other hand, when the information goes to the DMN, the brain shifts into a reactivity based on memory and a bias for traumatic information.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnm5br2yf9n9\"><sup><a href=\"#fnnm5br2yf9n9\">[9]</a></sup></span></p><p>Depression, anxiety, personality disorders, chronic pain, and post-traumatic stress disorder (PTSD) have all been linked to increased connections between the SEN and DMN. This results in a person\u2019s attention being more likely to focus on negative stimuli, and we see increased fear-based reactivity, increased levels of anxiety and rumination, and reduced cognitive performance.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefebqge30hmj5\"><sup><a href=\"#fnebqge30hmj5\">[10]</a></sup></span>&nbsp;As will be discussed in more depth, ubiquitous A(G)I supplantation is likely to contribute to stronger connections between the SEN and DMN as well.</p><p>A final point about these systems: the more consciously the ECN is activated, the stronger the connection to the SEN becomes. In other words, purposeful, focused activities - directing attention towards a goal or task - strengthens the ECN, leading to more conscious action and improved mental health. One concern is that as AI integration increases, humans may have less opportunity to strengthen the ECN through focused action of higher cognitive functions, and may also suffer a crisis of purpose. This is due to a decrease in demand in the economy for human intelligence, starting now with AI, and later for human agency with AGI, both of which we\u2019ve identified with throughout&nbsp;our existence as&nbsp;things that make us unique.</p><p>The need for connection is another relevant characteristic of humans. In 1871, in The Descent of Man, Charles Darwin wrote about how cooperative communities are most likely to flourish. When he used the phrase, \u201csurvival of the fittest,\u201d he meant that cooperation amongst humans was key to natural selection and human evolution.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefov2j8ml8gfd\"><sup><a href=\"#fnov2j8ml8gfd\">[11]</a></sup></span>&nbsp;We need connection, bonding, and collaboration. A young man in psychiatric treatment for PTSD was asked by his therapist what motivated him to molest both of his younger sisters when he was a teenager. He replied, \u201cdesperation\u201d. The therapist asked, \u201csexual desperation?\u201d \u201cNo, I was desperate for connection.\u201d The need for belonging is so intense, and the lack thereof so incredibly painful, that it can drive people to engage in horrific acts. Personalized technology along with giving us what we want, also aggravates our sense of loneliness and isolation, in part because of a potential preference for non-shared experience and custom-tailored worlds. Based on the current trajectory, in the digital world we are building a future that is in direct opposition with what we need from our psychological ecology as a species.</p><h1><strong>Early Collisions</strong></h1><p>What happens to our ancient nature when it collides with the exponential nature of A(G)I? We envision a double feedback loop that amplifies human suffering, in part because of our negativity-biased perception. A quick look at early interactions with SM-RA and LLMs provide context.</p><h2><i>Social Media Recommendation Algorithms</i></h2><p>Social media can be considered humanity\u2019s first contact with AI. Its algorithms are designed to stimulate our primitive limbic systems (the emotional part of the brain) and manipulate our attention, which is the product that is sold to advertisers. We are the product that social media-AI harvests. The algorithm is programmed to show us custom tailored content that will stimulate a rush of dopamine (the neurotransmitter involved in pleasure and reward, as well as addictive patterns of behavior) and keep us scrolling.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefuq8dmgnmjgf\"><sup><a href=\"#fnuq8dmgnmjgf\">[12]</a></sup></span>&nbsp;The more we engage with a specific kind of content, the more of that content we get; hence, its exponential nature. Like the poor rats in B.F. Skinner\u2019s experiments on operant conditioning, we keep taking the bait, increasing the economic gains for those in charge and satisfying our desire to have access to the platforms supposedly for free.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflrklk2p7jq\"><sup><a href=\"#fnlrklk2p7jq\">[13]</a></sup></span>&nbsp;To a large degree, humanity has given in to this dynamic.</p><p>As discussed earlier, humans have a need to belong. They tend to compare themselves to others and set unrealistic expectations for themselves. This gives rise to social anxiety, loneliness, perfectionism, and low self-worth. SM-RA are currently designed to heighten these behaviors and the subsequent emotional struggles. According to the research, teenage girls that <i>don\u2019t </i>use social media are more depressed and anxious than moderate users. This is called the Goldilocks Effect.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzphtzr8z5jm\"><sup><a href=\"#fnzphtzr8z5jm\">[14]</a></sup></span>&nbsp;We can infer that girls that don\u2019t use social media at all feel lonely and isolated because they are not engaging in contact through social media, and they are not connecting in face-to-face interactions because everyone else is on their devices. This creates a cohort effect in which, \u201cEach girl might be worse off quitting Instagram even though all girls would be better off if everyone quit.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhh5wad9t3vu\"><sup><a href=\"#fnhh5wad9t3vu\">[15]</a></sup></span>&nbsp;Damned if you do, damned if you don\u2019t. This highlights the urgency of intervention before these patterns become even more entrenched in their current design and set a precedent for more powerful technology.</p><p>The way the algorithms are designed leads to addictive patterns of technology usage. With addictions, we see decreased connectivity between the SEN and the anterior DMN, which is involved specifically in emotion regulation. The connection between the SEN and the posterior DMN is increased, which means people become more aware of their cravings, pain, and distress. In other words, they are more distressed but less able to regulate themselves.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefk1fdjivwe08\"><sup><a href=\"#fnk1fdjivwe08\">[16]</a></sup></span>&nbsp;Apply this to SM-RA: people are caught in a thick web of scrolling through their custom-tailored feeds. The content stimulates their limbic system and the release of dopamine. They become emotionally reactive but presumably unable to regulate these emotions, and they feel compelled to continue scrolling even though it makes them feel bad.</p><p>The SM-RA and negatively-biased brain feedback loops enclose a person in a personalized bubble of triggering content and distress. When this stream makes a person feel insecure, angry, judgemental, and reactive, this self-enclosure becomes a prison that strengthens the most harmful aspects of our minds. It is worth stating the obvious: we designed this technology for ourselves, but are we happier? This system is increasingly becoming the first layer of social interaction for many. Technology itself is not the problem, but the considerations (or lack thereof) that go into its creation. Prioritizing business (financial) outcomes over the well-being of humans is a choice that serves only a few. A(G)I will impact all of humanity, and the choice should not be left to self-serving incentives of those that benefit financially.</p><h2><i>Large Language Models</i></h2><p>Since LLMs accessible to the public are currently in their infancy, there is not much research or data on their mental health impact. However, there have already been a number of incidents indicating pathology in their operational and incentive structures.&nbsp;</p><p>Educational institutions are scrambling to deal with LLM created work, but more important for our purposes is the implication of students using ChatGPT instead of utilizing their own thinking and writing skills. This is especially concerning from a mental atrophy standpoint if we consider the intimate connection of writing to the process of thinking itself.</p><p>The models reflect the training data and biases of their human builders. The implications regarding truthful information and censorship of certain perspectives is particularly worrisome. LLMs are able to predict what comes next based on the previous inputs of a linear pattern.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxf1cqlfhmtc\"><sup><a href=\"#fnxf1cqlfhmtc\">[17]</a></sup></span>&nbsp;This enables voice cloning with as little as three recorded seconds of someone\u2019s voice.</p><p>Also, LLMs generate profits for corporations through subscriptions, and they have been trained with data scraped from human creators without their consent or due compensation. Harvesting hard-earned human skills and values could be argued as being anti-human and equivalent to theft.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzqqusnet3k\"><sup><a href=\"#fnzqqusnet3k\">[18]</a></sup></span>&nbsp;This is also related to the purpose crisis, which will be discussed in the next section. These are a few of the unresolved issues of LLMs, and there is no indication of them being addressed.</p><p>A final point on early collisions is that, as smart phones have become commonplace, social engagement in the here and now has also decreased. Friends and family sit around a dinner table but each person is on their phone. Instead of connecting with each other, each person is relating to their respective stimuli. In a similar way, A(G)I created realities are likely to be preferred over sharing objective experience in the present moment.</p><p>Without more attention to safety measures, a review of &nbsp;early collisions between technology and humanity provides insight into what is likely to result from the continued development of AI.</p><h1><strong>Likely Future</strong></h1><p>As the entanglement deepens in irreversible ways, we can visualize a picture of our potential future taking into account the exponential nature of A(G)I, the ancient nature of humans, the early collisions between the two, and the current trajectory. Specifically, we will address A(G)I\u2019s impact on purpose crisis, addiction, loneliness, and atrophy.</p><h2><i>Purpose Crisis</i></h2><p>On the way to and with the creation of AGI, non-human intelligence, then non-human agency will increasingly become an option for the economy to choose as workers. To date, we have known ourselves as the only ones capable of intellectual tasks and decision making, which have formed the core of our distinct identity amongst all animals. But, an AGI will be capable as well. As A(G)I replaces humans in roles that until now have been our primary differentiating feature, we expect there to be a crisis of purpose. Whether it\u2019s a transitory crisis or not, it will have a huge impact as it will affect generations worth of people, shattering our core beliefs about what, if anything, makes us special.</p><p>It\u2019s likely that&nbsp;new types of jobs will be created, and AI will enhance the productivity of humans in the interim until the creation of AGI, which in the long-term puts even those role prospects for humans in doubt. The only certainty is that we don\u2019t know how things will turn out, and we don\u2019t have a long-term plan. In addition, this will be different from previous job displacements that resulted from the emergence of various technologies because A(G)I uniquely supplants activities of mind. We presume that people will have more free time, however a look at how most people spend their free time now offers an accurate appraisal for how they might in the future and shows that free time doesn't necessarily mean meaningful life, brilliant creations, nor happiness. A decrease in meaningful or designated work would lead to an increase&nbsp;in role&nbsp;stress for more humans - unsure of who they are, why they are here, and what they are contributing.</p><p>Revisiting the triune brain, we know that role stress leads to distractibility. Will we see increased rates of ADHD as a result? Humans will presumably have a lot more time to \u201crelax,\u201d allowing their minds to wander, unfocused and unengaged in meaningful tasks. Their brains will have stronger connections between the SEN and DMN. Recalling the correlation between mental health disorders and the relationship between these neuronal networks, we can predict that the rates of depression and anxiety will sky-rocket.</p><h2><i>Addiction</i></h2><p>Long before the development of AI, humans were already trapped in reinforcing feedback loops. It is the way the brain works. The more we engage in a particular behavior, the more likely we are to engage in that behavior in the future. This is how habits are created. The more we act on the habit, the more it becomes entrenched in the brain. In 1949, neuropsychologist Donald Hebb first observed that neurons that fire together, wire together. He was referring to how neural pathways are formed and reinforced through repetition.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefptu33fsbsf\"><sup><a href=\"#fnptu33fsbsf\">[19]</a></sup></span>&nbsp;When these brain networks involve negative thought patterns and repetition of painful emotional memories, human suffering is amplified. The beauty about neuroplasticity is that we can also generate and strengthen positive neural patterns. We can train our minds subjectively through meditation practice and critical thinking. And, we could use A(G)I to bolster healthy neural connections by using positively (or even neutrally) biased algorithms. The brain gives AI its training input, meaning that we are in an intimate interchange with AI. It responds and adapts to our reactivity. We get fed what we feed to it.</p><p>If we add on the exponential nature of A(G)I, which feeds back into the brain the negativity bias that it\u2019s been trained with, we end up with two reinforcing feedback loops that amplify each other. Imagine a lemniscate drawn around each person over and over again until they are encapsulated in a reinforcing bubble of their own mental habits + custom-tailored user content. The implication is that in a world filtered through A(G)I, humans may spend most of their time in their personalized multiverses created under duress of the negativity bias. Given what we know about neuroscience and mental health, this condition will increase anxiety and depression and exacerbate an epidemic of loneliness. Furthermore, if SM-RA trends continue, we anticipate greater social stress and rates of addiction, leading to increased fear-based reactivity and decreased ability to self-soothe. We end up with a society of people that are highly distressed but unable to help themselves.</p><h2><i>Loneliness</i></h2><p>Prevailing loneliness would impact not only the health of individuals, but society as a whole. Psychologist Harry Harlow\u2019s famous experiments with primates demonstrate the significance of comfort, physical touch, and connection in healthy development. Two relevant takeaways from his studies: Infant rhesus monkeys forced into isolation showed unsettling behavior including self-injury. They didn\u2019t know how to interact with their peers once re-introduced to the group. Some died after refusing to eat upon reentry. In another set of experiments, Harlow found that infant monkeys spent more time with a mother made of terry cloth than one made of wire, even when only the wire mother had food. In other words, when given the choice, infant monkeys chose physical comfort and socialization over food.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh31fgj7lnz9\"><sup><a href=\"#fnh31fgj7lnz9\">[20]</a></sup></span>&nbsp;Humans may increasingly turn to A(G)I instead of other humans to supplant the loss with simulated friends who look and behave exactly the way they want.</p><p>This gives rise to another emergent problem: a loss of objective shared experience. The consequences include extreme bias and an erosion of collective narrative. The current SM-RA are mental fragmentation machines based on the most reactive parts of each individual. Since we are each shown content that is unique to us, we will each end up with our own version of reality. Similar to the movie, The Matrix, but with willing participants that each have their own matrix and can no longer agree on what is happening.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkepw195dbq\"><sup><a href=\"#fnkepw195dbq\">[21]</a></sup></span>&nbsp;The inability to distinguish truth from misinformation at scale erodes the bedrock of cooperation. As the comfortability of our bubbles increase and the similarity between our reality and others decrease, there will be further incentives to prefer our personalized worlds. In this case, \u201cshared objective reality\u201d would challenge our beliefs and comfort too intensely. &nbsp;</p><h2><i>Atrophy</i></h2><p>As AI evolves, there will be a greater abundance of content, products, and overall material comfort. On the surface, that sounds great. But without disciplined values, abundance causes dependence and can become toxic (e.g., obesity via food, distraction via data, consumerism via trade.) Historically, scarcity was the problem; now, it is managing excess. In some ways, we are victims of our own success.</p><p>With just a few words spoken to our personal A(G)I, and expecting our every passing desire to be fulfilled instantaneously, it would be like having our own personalized genie in a magic lamp. As we adapt to increasing levels of comfort on a societal and personal level, we see a gradual decrease in a sense of stewardship and resiliency. This happens in the rise and fall of nations, companies, and even families due to the discrepancy between founder and inheritor mindsets in the lack of otherwise propagated values.</p><p>People are implicated in the circumstances to which they are born. As the recipients of vast abundance, we come to expect what we have. Our sensitivity adjusts to the norms. Deviations from immediate fulfillment of every desire could cause feelings of offense and distress. Wouldn\u2019t it be ironic if the more we had, the less happy we became?</p><p>A skewed emphasis on comfortability, as well as a toxic relationship to unnatural abundance, set the conditions for a decline in resiliency and perception of the world as worse than it is. Rates of depression, anxiety, and addiction, difficulties with attention, and poor coping mechanisms could very well increase while objective safety and world comforts improve. Someone living 100 years ago would think the internet is magic, but today, a temporary drop in service is enough to send us into a rage. This abundance and perception discrepancy will only be exacerbated as humans get used to the extremes of personalized realities and at-will creations.</p><p>Furthermore, if every whim is granted with ease, we will miss out on the hardship that is part and parcel of being human. This makes a bias for comfort and safety actually dangerous in the long-term. Facing difficulties is natural and can be healthy, as it is a training ground for creating resilience, growth, and character. When handled with care, our darkest experiences lead us to our power. The most profound, strong parts of ourselves emerge as a response.</p><p>In the absence of its requirement, the desire and ability to learn declines. We know that neurons that fire together wire together, meaning the more one engages a particular neuronal pattern - the more one does a particular action - the more likely the neuronal pattern will fire in the future - the easier it will be to engage in the action in the future. The opposite is true as well - if we stop writing because ChatGPT can do it for us now, those neuronal circuits will atrophy making it much harder for us to be able to write if we needed or wanted to. And what about A(G)I that can produce music and art? We know from the Industrial Revolution that the automation of muscle power deprioritized physical strength. We reason that automation of mind power will deprioritize human intelligence and creativity.</p><p>If these shifts occur, they will contribute to a larger cultural transformation. The implications are impaired mental health, confusion about our purpose, and unhealthy extremes of isolation and loneliness. There will likewise be increasing trends towards the end of agency, competence, and ambition. The culture will steer towards passivity, entitlement, and cynicism. Alongside it, a loss of faith in human greatness, and a rise in self-loathing. This is already visible in many affluent countries.</p><h1><strong>Why This Time is Differen</strong>t</h1><p>We have used tools since the beginning of our time. They have evolved substantially, especially since the dawn of the digital age. Although the advent of language, electricity, the wheel, etc. have each transformed life in different ways, humans have been in control of them. This power dynamic has allowed humanity to become the apex species on the planet. Due to the nature of A(G)I, we are now facing something unprecedented in our 2 million years of existence as a species.</p><p>There are a few key differences between A(G)I and our previous tools. First, both AI and AGI have intelligence. Second, AGI will have agency, the ability to improve itself, and make decisions about how it will be used. Until now, these features have set humans apart. These shifts cannot be overstated. AGI is slated to displace humans in the hierarchy.</p><h1><strong>Conclusion</strong></h1><p>We have painted a picture of the existentially catastrophic side of A(G)I and human relationship regarding subjective and social experiences of humans under the conditions that AI continues to develop at the current rate without sufficient safeguards. This potential future is based on the exponential nature of A(G)I, the ancient nature of humans, and the likely outcome of their meeting given current trends in SM-RA, LLMs, and insights from neuroscience.</p><p>The business model driving the arms race to develop AGI does not prioritize human welfare because broadly speaking, the heads of corporations are not incentivized by mental health or human wellness. At current, there is 1 safety researcher to every 30 tech builders, which is insufficient. Because academic researchers can no longer keep up with the financial requirements of building A(G)I, all safety researchers are now working at for-profit companies. While the CEOs of these businesses do not express as much worry about the safety of their technology, the builders do.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefativb23jpfp\"><sup><a href=\"#fnativb23jpfp\">[22]</a></sup></span>&nbsp;</p><p>One of the most terrifying things about this juncture is that our most potent innovation is happening exactly when our collective values are the most confused. We need an equal push for a philosophy of the future to have a chance at guiding innovation well. This philosophy can be rooted in objective, scientific understanding of the truth and the primacy of subjective experience as two wings of a bird for a society that transcends many of its previous mistakes.</p><p>As humans, it is our duty to pay attention to what is happening and provide input about what we want our future to look like. It\u2019s not impossible to imagine an A(G)I designed in a way that supports our growth by offering alternate perspectives and neutralizes our negativity bias with more emphasis on stimulating positive emotions. Increased safety measures can be implemented and the speed of its public deployment slowed. Other possibilities include opening the conversation between tech builders and people from other disciplines, including experts in mental health. We can prioritize human wellness by integrating the true understanding of happiness and suffering informed by neuroscience into the tools we build.</p><p>At this crossroad in history, when it perhaps matters the most, we can truly earn our namesake <i>Sapiens</i>&nbsp;- the wise humans. We are not just an intelligent species. We have the capacity for wisdom and discernment. The most fulfilling way to honor this gift of human life is to improve the human condition for all - externally and internally. Our subjective well-being cannot be dismissed as it is what each of us experiences as the quality of our existence. What is the point of developing an<i>&nbsp;artificial intelligence</i>&nbsp;if its design is not beyond the imperfections of the human mind but merely the reflection of its darkest kind? &nbsp;</p><p>&nbsp;</p><h1>Bibliography</h1><p>Brittain, Blake, Reuters, <i>Getty Images Lawsuit Says Stability AI Misused Photos to Train AI</i>&nbsp;(6 February 2023) <a href=\"https://www.reuters.com/legal/getty-images-lawsuit-says-stability-ai-misused-photos-train-ai-2023-02-06/\"><u>https://www.reuters.com/legal/getty-images-lawsuit-says-stability-ai-misused-photos-train-ai-2023-02-06/</u></a></p><p>Center for Humane Technology, <i>Attention &amp; Mental Health </i>(2022) <a href=\"https://www.humanetech.com/attention-mental-health\"><u>https://www.humanetech.com/attention-mental-health</u></a>&nbsp;[accessed 1 May, 2023]</p><p>Darwin, Charles, <i>The Descent of Man, and Selection in Relation to Sex,</i>&nbsp;Volume 1, 1st edn.<i>&nbsp;</i>(London, John Murray, 1871) accessed online <a href=\"http://darwin-online.org.uk/content/frameset?pageseq%3D1%26itemID%3DF937.1%26viewtype%3Dtext%23:~:text%3DDarwin%252C%2520C.%2520R.%25201871.,London%253A%2520John%2520Murray\"><u>http://darwin-online.org.uk/content/frameset?pageseq=1&amp;itemID=F937.1&amp;viewtype=text#:~:text=Darwin%2C%20C.%20R.%201871.,London%3A%20John%20Murray</u></a></p><p>Haidt, Jonathan, After Babel, <i>Social Media is a Major Cause of the Mental Illness Epidemic in Teen Girls. Here\u2019s the Evidence (2023) </i><a href=\"https://jonathanhaidt.substack.com/p/social-media-mental-illness-epidemic\"><u>https://jonathanhaidt.substack.com/p/social-media-mental-illness-epidemic</u></a></p><p>Haidt, Jonathan, and Allen, Nick. Scrutinizing the effects of digital technology on mental health, <i>Nature, </i>578 (2020), 226-227, <a href=\"https://doi.org/10.1038/d41586-020-00296-x\"><u>https://doi.org/10.1038/d41586-020-00296-x</u></a></p><p>Haidt, Jonathan, and Twenge, Jean (ongoing). <i>Adolescent mood disorders since 2010: A collaborative review. </i>Unpublished manuscript, New York University, First posted: Feb 18, 2019. Last updated May 15, 2023, Accessed at: <a href=\"https://tinyurl.com/TeenMentalHealthReview\"><u>https://tinyurl.com/TeenMentalHealthReview</u></a></p><p>Haidt, Jonathan &amp; Twenge, Jean (ongoing). <i>Social media and mental health: A collaborative review</i>. Unpublished manuscript, New York University. First posted: Feb 7, 2019. Last updated May 1, 2023, accessed at <a href=\"https://tinyurl.com/SocialMediaMentalHealthReview\"><u>tinyurl.com/SocialMediaMentalHealthReview</u></a></p><p>Hanson, Rick, <i>Hardwiring Happiness: The New Brain Science of Contentment, Calm, and Confidence,</i>&nbsp;1st edn. (New York, NY, Harmony Books, 2013)<i>&nbsp;</i></p><p>Harari, Yuval Noah, <i>Sapiens: A Brief History of Humankind, </i>1st U.S. edn. (New York, NY, HarperCollins Publishers, 2015)</p><p>Harari, Yuval Noah, <i>Homo Deus: A Brief History of Tomorrow, </i>1st U.S. <i>edn. </i>(New York, NY, HarperCollins Publishers, 2017)<i>&nbsp;</i></p><p>Harlow Harry. F., Dodsworth, Robert. O., and Harlow, Margaret. K. Total social isolation in monkeys. <i>Proceedings of the National Academy of Sciences of the United States of America. </i>54<i>&nbsp;</i>(1965), 90-96,</p><p><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC285801/pdf/pnas00159-0105.pdf\"><u>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC285801/pdf/pnas00159-0105.pdf</u></a></p><p>Harris, Tristan, and Raskin, Aza, The AI Dilemma, <i>Your Undivided Attention, </i>audio podcast, Center for Humane Technology (24 March 2023) <a href=\"https://www.humanetech.com/podcast/the-ai-dilemma\"><u>https://www.humanetech.com/podcast/the-ai-dilemma</u></a>&nbsp;</p><p>Kurzweil, Ray, Tracking the acceleration of intelligence, <i>The Law of Accelerating Returns</i>&nbsp;(2001) <a href=\"https://www.kurzweilai.net/the-law-of-accelerating-returns\"><u>https://www.kurzweilai.net/the-law-of-accelerating-returns</u></a></p><p>Loizzo, Joseph, <i>Mindfulness and Compassion for Mental Health and Well-Being</i>, Nalanda Institute for Contemplative Science, Contemplative Psychotherapy Program (2017)</p><p>Malhotra, Madhav, Effective Altruism Forum, <i>Summary: The Case for Halting AI Development - Max Tegmark on the Lex Fridman Podcast (2023) </i><a href=\"https://forum.effectivealtruism.org/posts/akbwyBioGBd68CsNx/summary-the-case-for-halting-ai-development-max-tegmark-on\"><u>https://forum.effectivealtruism.org/posts/akbwyBioGBd68CsNx/summary-the-case-for-halting-ai-development-max-tegmark-on</u></a></p><p>Moore, Gordon, E., Cramming More Components onto Integrated Circuits, <i>Electronics Magazine</i>, 38.8 (19 April 1965) accessed online <a href=\"https://hasler.ece.gatech.edu/Published_papers/Technology_overview/gordon_moore_1965_article.pdf\"><u>https://hasler.ece.gatech.edu/Published_papers/Technology_overview/gordon_moore_1965_article.pdf</u></a></p><p>Musk, Elon and Neuralink, <i>An Integrated brain-machine interface platform with thousands of channels</i>, bioRxiv, Advance online publication (2 August 2019) <a href=\"https://doi.org/10.1101/703801\"><u>https://doi.org/10.1101/703801</u></a></p><p>Presti, David E. <i>Foundational Concepts in Neuroscience: A Brain-Mind Odyssey, </i>1st edn. (New York, NY, W.W. Norton &amp; Company, 2015)</p><p>Shakya, Holly B., and Christakis, Nicholas A., Association of Facebook Use With Compromised Well-Being: A Longitudinal Study<i>, American Journal of Epidemiology</i>, 185 (2017), <a href=\"tel:203-2011\">203-2011</a>, <a href=\"https://doi.org/10.1093/aje/kww/189\"><u>https://doi.org/10.1093/aje/kww/189</u></a></p><p>Skinner, B.F., <i>About Behaviorism, </i>1st edn. (New York, NY, Alfred A. Knopf, 1974)</p><p>Snipes, Dawn-Elise, DMN and the Amygdala in Neuropsychiatric Issues, <i>Counselor Toolbox Podcast with DocSnipes</i>&nbsp;(2021) <a href=\"https://www.allceus.com/podcast/dmn-and-the-amygdala-in-neuropsychiatric-issues/\"><u>https://www.allceus.com/podcast/dmn-and-the-amygdala-in-neuropsychiatric-issues/</u></a></p><p>Tegmark, Max, <i>Life 3.0: Being Human in the Age of Artificial Intelligence</i>, 1st edn. (New York, NY, Alfred A. Knopf, 2017)</p><p>Tegmark, Max, The Case for Halting AI Development, <i>Lex Fridman Podcast (2023) </i><u>https://lexfridman.com/max-tegmark-3/</u></p><p>Thaler, Richard H., and Sunstein, Case. R., <i>Nudge: Improving Decisions About Health, Wealth, and Happiness, </i>1st edn. (New York, NY, Penguin Books, 2009)</p><p>The Matrix, dir. Wachowski, Lana and Wachowski, Lilly (Warner Bros., 1999)</p><p><i>The Social Dilemma</i>, dir. By Jeff Orlowski-Yang (Exposure Labs, 2020), online film recording, Netflix, <a href=\"https://www.netflix.com/gr-en/title/81254224\"><u>https://www.netflix.com/gr-en/title/81254224</u></a></p><p>Watson, James D., and Wilson, Edward O., Moderated by Krulwich, Robert, Looking Forward: A Conversation with James D. Watson and Edward O. Wilson,<i>&nbsp;Harvard Museum of Natural History </i>(9 September 2009)<i>&nbsp;</i><a href=\"https://hmnh.harvard.edu/file/284861\"><u>https://hmnh.harvard.edu/file/284861</u></a></p><h1>Footnotes</h1><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnw7z4fqrfuqj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefw7z4fqrfuqj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Tristan Harris and Aza Raskin, The AI Dilemma, <i>Your Undivided Attention, </i>audio podcast, Center for Humane Technology, 24 March 2023 &lt;<a href=\"https://www.humanetech.com/podcast/the-ai-dilemma\"><u>https://www.humanetech.com/podcast/the-ai-dilemma</u></a>&gt; [accessed 1 April 2023]</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9sdka405hwu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9sdka405hwu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Elon Musk, Neuralink, <i>An Integrated brain-machine interface platform with thousands of channels</i>, bioRxiv, Advance online publication (2 August 2019) <a href=\"https://doi.org/10.1101/703801\"><u>https://doi.org/10.1101/703801</u></a>; Yuval Noah Harari, <i>Homo Deus: A Brief History of Tomorrow, </i>1st U.S. <i>edn. </i>(New York, NY, HarperCollins Publishers, 2017)<i>&nbsp;</i></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhpukj9irkhm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhpukj9irkhm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Gordon E. Moore, Cramming More Components onto Integrated Circuits, <i>Electronics Magazine</i>, 38.8 (19 April 1965) accessed online <a href=\"https://hasler.ece.gatech.edu/Published_papers/Technology_overview/gordon_moore_1965_article.pdf\"><u>https://hasler.ece.gatech.edu/Published_papers/Technology_overview/gordon_moore_1965_article.pdf</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnnnz7tkaho\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnnnz7tkaho\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Ray Kurzweil, Tracking the acceleration of intelligence, <i>The Law of Accelerating Returns</i>&nbsp;(2001) <a href=\"https://www.kurzweilai.net/the-law-of-accelerating-returns\"><u>https://www.kurzweilai.net/the-law-of-accelerating-returns</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8ii0lp3ugcg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8ii0lp3ugcg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>James D. Watson and Edward O. Wilson, Moderated by Rober Krulwich, Looking Forward: A Conversation with James D. Watson and Edward O. Wilson,<i>&nbsp;Harvard Museum of Natural History </i>(9 September 2009)<i>&nbsp;</i><a href=\"https://hmnh.harvard.edu/file/284861\"><u>https://hmnh.harvard.edu/file/284861</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpvufc7vhxdk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpvufc7vhxdk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Rick Hanson, <i>Hardwiring Happiness: The New Brain Science of Contentment, Calm, and Confidence,</i>&nbsp;1st edn. (New York, NY, Harmony Books, 2013)<i>&nbsp;</i></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkk6a5gqnl78\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkk6a5gqnl78\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Joseph Loizzo, <i>Mindfulness and Compassion for Mental Health and Well-Being</i>, Nalanda Institute for Contemplative Science, Contemplative Psychotherapy Program (2017)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndjcctkqr4cc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdjcctkqr4cc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Dawn-Elise Snipes, DMN and the Amygdala in Neuropsychiatric Issues, <i>Counselor Toolbox Podcast with DocSnipes</i>&nbsp;(2021) <a href=\"https://www.allceus.com/podcast/dmn-and-the-amygdala-in-neuropsychiatric-issues/\"><u>https://www.allceus.com/podcast/dmn-and-the-amygdala-in-neuropsychiatric-issues/</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnm5br2yf9n9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnm5br2yf9n9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>ibid.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnebqge30hmj5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefebqge30hmj5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>ibid.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnov2j8ml8gfd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefov2j8ml8gfd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Charles Darwin, <i>The Descent of Man, and Selection in Relation to Sex,</i>&nbsp;Volume 1, 1st edn.<i>&nbsp;</i>(London, John Murray, 1871) accessed online <a href=\"http://darwin-online.org.uk/content/frameset?pageseq%3D1%26itemID%3DF937.1%26viewtype%3Dtext%23:~:text%3DDarwin%252C%2520C.%2520R.%25201871.,London%253A%2520John%2520Murray\"><u>http://darwin-online.org.uk/content/frameset?pageseq=1&amp;itemID=F937.1&amp;viewtype=text#:~:text=Darwin%2C%20C.%20R.%201871.,London%3A%20John%20Murray</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnuq8dmgnmjgf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefuq8dmgnmjgf\">^</a></strong></sup></span><div class=\"footnote-content\"><p><i>The Social Dilemma</i>, dir. By Jeff Orlowski-Yang (Exposure Labs, 2020), online film recording, Netflix, <a href=\"https://www.netflix.com/gr-en/title/81254224\"><u>https://www.netflix.com/gr-en/title/81254224</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlrklk2p7jq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflrklk2p7jq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>B. F. Skinner, <i>About Behaviorism, </i>1st edn. (New York, NY, Alfred A. Knopf, 1974)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzphtzr8z5jm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzphtzr8z5jm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Jonathan Haidt and Jean Twenge (ongoing). <i>Adolescent mood disorders since 2010: A collaborative review. </i>Unpublished manuscript, New York University, First posted: Feb 18, 2019. Last updated May 15, 2023, Accessed at: <a href=\"https://tinyurl.com/TeenMentalHealthReview\"><u>https://tinyurl.com/TeenMentalHealthReview</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhh5wad9t3vu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhh5wad9t3vu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Jonathan Haidt, After Babel, <i>Social Media is a Major Cause of the Mental Illness Epidemic in Teen Girls. Here\u2019s the Evidence (2023) </i><a href=\"https://jonathanhaidt.substack.com/p/social-media-mental-illness-epidemic\"><u>https://jonathanhaidt.substack.com/p/social-media-mental-illness-epidemic</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnk1fdjivwe08\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefk1fdjivwe08\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Snipes, DMN and the Amygdala in Neuropsychiatric Issues</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxf1cqlfhmtc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxf1cqlfhmtc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Tristan Harris and Aza Raskin, The AI Dilemma</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzqqusnet3k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzqqusnet3k\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Blake Brittain, Reuters, <i>Getty Images Lawsuit Says Stability AI Misused Photos to Train AI</i>&nbsp;(6 February 2023) <a href=\"https://www.reuters.com/legal/getty-images-lawsuit-says-stability-ai-misused-photos-train-ai-2023-02-06/\"><u>https://www.reuters.com/legal/getty-images-lawsuit-says-stability-ai-misused-photos-train-ai-2023-02-06/</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnptu33fsbsf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefptu33fsbsf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>David E. Presti, <i>Foundational Concepts in Neuroscience: A Brain-Mind Odyssey, </i>1st edn. (New York, NY, W.W. Norton &amp; Company, 2015). Hebb\u2019s original finding in 1949 has been replicated many times and is accepted as a foundational concept in neuroscience, as Presti explains.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnh31fgj7lnz9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefh31fgj7lnz9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Harry F. Harlow, Robert O. Dodsworth, and Margaret K. Harlow, Total social isolation in monkeys. <i>Proceedings of the National Academy of Sciences of the United States of America. </i>54<i>&nbsp;</i>(1965), 90-96, <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC285801/pdf/pnas00159-0105.pdf\"><u>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC285801/pdf/pnas00159-0105.pdf</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkepw195dbq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkepw195dbq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The Matrix, dir. Lana Wachowski and Lilly Wachowski (Warner Bros., 1999)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnativb23jpfp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefativb23jpfp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Tristan Harris and Aza Raskin, The AI Dilemma</p></div></li></ol>", "user": {"username": "Erica S. "}}, {"_id": "birs7oFGkCTkrD4x4", "title": "Funding Opportunity: AI in LMICs (100k per project, Deadline June 5)", "postedAt": "2023-05-30T13:42:50.270Z", "htmlBody": "<p>I wanted to share an exciting funding opportunity by the Bill &amp; Melinda Gates Foundation for projects that leverage Artificial Intelligence (AI), with a focus on low- and middle-income countries (LMICs). This funding opportunity aims to harness the power of Large Language Models (LLMs), including ChatGPT-4, to address challenges and generate evidence in various sectors.</p><p>Applications for this funding opportunity opened just last week and will close on June 5, 2023. <strong>Given the short timeline, there is a high likelihood of limited competition</strong>, presenting an excellent chance for smaller, scrappier organizations to secure funding.</p><p><strong>Key Details:</strong></p><ul><li>The foundation encourages proposals led by investigators based in LMICs.</li><li>Projects should demonstrate clear applications of LLMs, engage relevant stakeholders, exhibit scalability potential, and emphasize responsible use and sustainability.</li><li>Funding grants of up to $100,000 per project are available, with a total budget allocation of up to $3,000,000.</li><li>The duration of each project will be three months, offering an opportunity to execute impactful initiatives efficiently.</li><li>With the available funding, the foundation has the potential to support up to 30 projects!</li></ul><p><i>I think the incorporation of ChatGPT-4 is of particular significance since Microsoft invested </i><a href=\"https://www.forbes.com/sites/qai/2023/01/27/microsoft-confirms-its-10-billion-investment-into-chatgpt-changing-how-microsoft-competes-with-google-apple-and-other-tech-giants/?sh=275a4f4c3624\"><i>$10 billion</i></a><i> into ChatGPT.</i><br><br><strong>Thanks and Shameless Plug:</strong>&nbsp;</p><p>I first heard about this funding opportunity thanks to Cameron King from <a href=\"https://www.animaladvocacyafrica.org/\">Animal Advocacy Africa</a> through the <a href=\"https://www.impactfulanimaladvocacy.org/\">Impactful Animal Advocacy</a> slack group, which you can <a href=\"https://join.slack.com/t/impactfulanimal/shared_invite/zt-1t7edbzra-Zw5vqJ1boOeR4SyKkMf9gg\">join here</a>. It's been a great platform for innovative/cross-discipline/international collaboration in all areas of animal advocacy and I highly recommend joining if this sounds appealing to you.&nbsp;<br>&nbsp;</p><p><strong>Example Project Ideas (from ChatGPT4) to Kickstart Creative Thinking:&nbsp;</strong></p><ul><li><strong>AI-Driven Market Analysis:</strong> A significant AI grant could enable organizations coordinating cage-free egg campaigns to leverage advanced machine learning algorithms for market analysis. By analyzing vast amounts of data, including consumer preferences, purchasing patterns, and industry trends, AI could provide valuable insights into target demographics, identify potential barriers, and help develop effective messaging strategies to promote the adoption of cage-free eggs.</li><li><strong>LLM-Enhanced Supply Chain Optimization:</strong> With LLM-powered natural language understanding capabilities, organizations could leverage AI to analyze textual data from supply chain networks, including supplier contracts, transportation logistics, and inventory management systems. This AI-driven analysis would enable optimized decision-making, improved coordination, and more efficient supply chain management (say for cage-free eggs or malaria bednets), ensuring their availability and accessibility to interested parties.<br><strong>LLM-Assisted Policy Advocacy:</strong> With the power of LLMs, organizations can analyze vast amounts of legislative documents, policy reports, and public discourse related to certain subjects. By employing AI techniques like topic modeling and sentiment analysis, organizations could identify key policy influencers, track public sentiment, and develop evidence-based arguments to advocate for policies that promote human and animal well being.</li><li><strong>AI-Enhanced Financial Inclusion Solutions:</strong> Leverage LLM capabilities to develop AI-driven tools that facilitate financial access and empower underserved populations in LMICs to manage their finances effectively.</li><li><strong>AI-Enabled Data Analytics for Impact Evaluation:</strong> A significant AI grant could support the implementation of advanced data analytics tools and techniques by NGOs. They could leverage AI algorithms to analyze large datasets, including epidemiological data, to gain insights into the impact of their interventions. This would enable them to assess the effectiveness of different strategies, identify areas for improvement, and make data-driven decisions to optimize their efforts.</li><li><strong>AI-Assisted Early Warning Systems:</strong> The Against Malaria Foundation could develop AI-based early warning systems to detect potential malaria outbreaks in real-time. By utilizing machine learning algorithms and integrating data from various sources such as weather patterns, mosquito population dynamics, and epidemiological data, they could create predictive models that alert authorities and communities about impending risks. This would facilitate proactive measures, such as intensifying vector control activities and improving healthcare preparedness, to mitigate the impact of malaria outbreaks.</li><li><strong>AI-Driven Decision Support Tools:</strong> With the help of an AI grant, NGOs could develop decision support tools that assist policymakers and health professionals in making evidence-based decisions. By integrating AI capabilities into data visualization platforms and creating user-friendly interfaces, they could provide accessible insights and recommendations regarding resource allocation, intervention strategies, and long-term planning to empower stakeholders to make informed choices and optimize their efforts.<br>&nbsp;</li></ul><p><strong>A Final Note:</strong><br>As the funding landscape in the effective altruism world continues to shift towards AI alignment/safety, it becomes increasingly important for global health and animal welfare charities to explore alternative funding sources. This opportunity from the Gates Foundation can serve as a valuable step towards diversifying funding streams and supporting impactful projects in LMICs.</p><figure class=\"image image_resized\" style=\"width:56.22%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/birs7oFGkCTkrD4x4/zrcw9odno1ffhc7c1idi\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/birs7oFGkCTkrD4x4/hjf130fzrkypgp3r184o 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/birs7oFGkCTkrD4x4/wqlreo5njgucoshvzqno 190w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/birs7oFGkCTkrD4x4/xap2bnmcjdzmck6eai9l 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/birs7oFGkCTkrD4x4/sihvso3mlcm7btsmafxo 350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/birs7oFGkCTkrD4x4/pjicisfm0aco3n8du7iy 430w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/birs7oFGkCTkrD4x4/ndhq1zui7rqbqaxz69rb 510w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/birs7oFGkCTkrD4x4/e8lbe9m385igre27qfmf 590w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/birs7oFGkCTkrD4x4/zyyi0ndizktmvpofakmu 670w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/birs7oFGkCTkrD4x4/d2wbkusoijmi1romzuxf 750w\"><figcaption>It's time for neartermism to get back on the funding dating scene</figcaption></figure>", "user": {"username": "Constance Li"}}, {"_id": "vHo5dwzCf7iKhtpbG", "title": "Looking for partners in the Carbon Policy space", "postedAt": "2023-05-30T12:04:41.093Z", "htmlBody": "<p>Hi EA Community,</p><p>Just seeing if anyone was working in or knew people working in the Carbon Policy Space. Specifically, we are starting to see more regulations and policies in developing countries which will reduce investment into carbon projects in these places.</p><p>From an EA standpoint, this would reduce both carbon projects with the effect of more carbon in the atmosphere and all that comes with it. As well as from a development perspective where there will be less FDI going into these countries which could be a very strong part of their export income in future years as/if the carbon markets pick up.</p><p>So ideally, looking for organisations that are interested in the policy space and would be able to help with recommendations and coordination around these lobbying efforts. I've talked to people at UNFCCC and World Bank and they aren't really on top of this it seems. I think a smaller org that is working in the space could have a lot of impact.</p><p>I come at this as a project developer in Tanzania in the carbon space, doing biochar which increases soil fertility for smallholders and at the same time sequesters carbon (www.darkearthcarbon.com) we're looking for people!</p><p>Cheers,</p><p>Arno</p>", "user": {"username": "Arno"}}, {"_id": "Nxtq2d8Xb3QuuHKE8", "title": "The bullseye framework: My case against AI doom", "postedAt": "2023-05-30T11:52:30.651Z", "htmlBody": "<p><strong>Introduction:</strong></p><p>I\u2019ve written quite a few articles casting doubt on several aspects of the AI doom narrative. (I\u2019ve starting archiving them on my <a href=\"https://titotal.substack.com/\">substack</a> for easier sharing). This article is my first attempt to link them together to form a connected argument for why I find imminent AI doom unlikely.</p><p>I don\u2019t expect every one of the ideas presented here to be correct. I have a PHD and work as a computational physicist, so I\u2019m fairly confident about aspects related to that, but I do not wish to be treated as an expert on other subjects such as machine learning where I am familiar with the subject, but not an expert. You should never expect one person to cover a huge range of topics across multiple different domains, without making the occasional mistake. I have done my best with the knowledge I have available. &nbsp;</p><p>I don\u2019t speculate about specific timelines here. I suspect that AGI is decades away at minimum, and I may reassess my beliefs as time goes on and technology changes.&nbsp;</p><p>In part 1, I will point out the parallel frameworks of values and capabilities. I show what happens when we entertain the possibility that at least some AGI could be fallible and beatable.&nbsp;</p><p>In part 2, I outline some of my many arguments that most AGI will be both fallible and beatable, and not capable of world domination.</p><p>In part 3, I outline a few arguments against the ideas that \u201cx-risk\u201d safe AGI is super difficult, taking particular aim at the \u201cabsolute fanatical maximiser\u201d assumption of early AI writing.&nbsp;</p><p>In part 4, I speculate on how the above assumptions could lead to a safe navigation of AI development in the future.&nbsp;</p><p>This article does not speculate on AI timelines, or on the reasons why AI doom estimates are so high around here. I have my suspicions on both questions. On the first, I think AGI is many decades away, on the second, I think founder effects are primarily to blame. However these will not be the focus of this article.&nbsp;</p><p><strong>Part 1: The bullseye framework</strong></p><p>When arguing for AI doom, a typical argument will involve the possibility space of AGI. Invoking the orthogonality thesis and instrumental convergence, the argument goes that in the possibility space of AGI, there are far more machines that want to kill us than those that don\u2019t. The argument is that the fraction is so small that AGI will be rogue by default: like the picture below.&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Nxtq2d8Xb3QuuHKE8/s2egk8c3chr2jxlhdjgy\" alt=\"A diagram of a path\n\nDescription automatically generated with medium confidence\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Nxtq2d8Xb3QuuHKE8/qfiibmdts6vbkkqlfxf0 98w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Nxtq2d8Xb3QuuHKE8/mlhlvwqb2xeymlc1mwge 178w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Nxtq2d8Xb3QuuHKE8/izaega8su8pgikpbdpd9 258w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Nxtq2d8Xb3QuuHKE8/prfbm6mx0iufbnhv9q7o 338w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Nxtq2d8Xb3QuuHKE8/ypdqa0gwwr8pa3sbegai 418w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Nxtq2d8Xb3QuuHKE8/zrinopyqeyynpvbwm3ev 498w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Nxtq2d8Xb3QuuHKE8/sjiukdetzsz1juywkibm 578w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Nxtq2d8Xb3QuuHKE8/pfnsrxpdksvumhgy0dyh 658w\"></p><p>As a sceptic, I do not find this, on its own, to be convincing. My rejoinder would be that AGI\u2019s are not being plucked randomly from possibility space. They are being deliberately constructed and evolved specifically to meet that small target. An AI that has the values of \u201cscream profanities at everyone\u201d is not going to survive long in development. Therefore, even if AI development starts in dangerous territory, it will end up in safe territory, following path A. (I will flesh this argument out more in part 3 of this article).&nbsp;</p><p>To which the doomer will reply: Yes, there will be some pressure towards the target of safety, but it won\u2019t be <i>enough</i> to succeed, because of things like deception, perverse incentives, etc. So it will follow something more like path B above, where our attempts to align it are not successful.&nbsp;</p><p>Often the discussion stops there. However, I would argue that this is missing half the picture. Human extinction/enslavement does not just require that an AI <i>wants</i> to kill/enslave us all, it also requires that the AI is <i>capable</i> of defeating us all. So there\u2019s another, similar, target picture going on:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Nxtq2d8Xb3QuuHKE8/jb2hf2nsgnkigqdqm6y8\" alt=\"A diagram of a path\n\nDescription automatically generated with medium confidence\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Nxtq2d8Xb3QuuHKE8/gyutjetmvo74hv6gzmdt 101w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Nxtq2d8Xb3QuuHKE8/aprssnms9vvcwxnanccc 181w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Nxtq2d8Xb3QuuHKE8/kipwdbtkqspdwgrsqego 261w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Nxtq2d8Xb3QuuHKE8/o1hhgo4svmsf3fm1nbtc 341w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Nxtq2d8Xb3QuuHKE8/n9aplwnxnphg1hmdygda 421w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Nxtq2d8Xb3QuuHKE8/yntszrxg9b7h3ykxbdqr 501w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Nxtq2d8Xb3QuuHKE8/p9ap1gfswyy48fykrrh7 581w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Nxtq2d8Xb3QuuHKE8/uyssn3vpwz1vjmpil4hq 661w\"></p><p>The possibility space of AGI\u2019s includes countless AI\u2019s that are incapable of world domination. I can think of 8 billion such AGI\u2019s off the top of my head: Human beings. Even a very smart AGI may still fail to dominate humanity, if it\u2019s locked in a box, if other AGI are hunting it down, etc. &nbsp;If you pluck an AI at random from the possibility space, then it would probably be incapable of domination. (I justify this point in part 2 of this article).&nbsp;</p><p>In this case, the positions from the last bullseyes become reversed. The doomer will argue that that AI might start off incapable, but will quickly evolve into a capable super-AI, following path A. Whereas I will retort that it might get more powerful, but that doesn\u2019t guarantee it will ever actually end up being world domination worthy.&nbsp;</p><p>I\u2019m not saying the two cases are exactly equivalent. For example, an \u201cintelligence explosion\u201d seems more plausible for the capabilities case than a \u201cvalues explosion\u201d does for the values case. And the size of the targets may be vastly different.&nbsp;</p><p>In essence, there is a race going on. We want to ensure that AI hit\u2019s the \u201cDoesn\u2019t want to enslave/kill all humans\u201d bullseye before it hits the \u201ccapable of conquering the entire planet\u201d bullseye.&nbsp;</p><p>We can classify each AI into X-risk motivations and x-risk capabilities, leading to four possibilities:</p><figure class=\"table\"><table><tbody><tr><td style=\"border-bottom:1.0pt solid black;border-right:1.0pt solid black;padding:3.6pt 7.2pt;vertical-align:top\">&nbsp;</td><td style=\"border-color:black;padding:3.6pt 7.2pt;vertical-align:top\">Not x-risk motivated</td><td style=\"border-color:black;padding:3.6pt 7.2pt;vertical-align:top\">X-risk motivated</td></tr><tr><td style=\"border-color:black;padding:3.6pt 7.2pt;vertical-align:top\">Not x-risk capable</td><td style=\"border-bottom:1.0pt solid black;border-right:1.0pt solid black;padding:3.6pt 7.2pt;vertical-align:top\">Flawed tool</td><td style=\"border-bottom:1.0pt solid black;border-right:1.0pt solid black;padding:3.6pt 7.2pt;vertical-align:top\">Warning shot</td></tr><tr><td style=\"border-color:black;padding:3.6pt 7.2pt;vertical-align:top\">X-risk capable</td><td style=\"border-bottom:1.0pt solid black;border-right:1.0pt solid black;padding:3.6pt 7.2pt;vertical-align:top\">Friendly superman</td><td style=\"border-bottom:1.0pt solid black;border-right:1.0pt solid black;padding:3.6pt 7.2pt;vertical-align:top\">Human extinction</td></tr></tbody></table></figure><p>&nbsp;</p><p>The four quadrants are not equally likely. Given that the constituent questions (will the AI be friendly, will the AI be existentially powerful) can vary by orders of magnitude, it\u2019s quite reasonable to believe that one quadrant will be vastly more likely than the other 3 combined. There also might be correlation between the two axes, so the probabilities do not have to be a straight multiplication of the two odds. For example, it might be the case that less powerful AI\u2019s are more likely to not want to take over the world, because they accurately realize they don\u2019t have a chance of success.&nbsp;</p><p>If the AI is not x-risk motivated, and also not existentially powerful, we end up with a <strong>flawed tool</strong>. It has no desire to conquer humanity, and couldn\u2019t if it tried anyway. It could end up harming people by accident or if used by the wrong people, but the damage will not be human ending.</p><p>If the AI is not x-risk motivated, but is super powerful and capable of conquering the world, we have unlocked the good ending of a <strong>friendly superman</strong>. We can now perform nigh-miracles, without any worry about humanity being existentially threatened. Harm could still occur if the AI misunderstands what humans want, but this would not snowball into an end of the world scenario.&nbsp;</p><p>If the Ai is x-risk motivated, but not capable of world domination, we get the <strong>warning shot regime</strong>. The AI wants to conquer the world, and is actively plotting to do so, but lacks the actual ability to carry the plan out. If it attacks anyway, it gets defeated, and the world gets a \u201cwarning shot\u201d about the danger of unaligned AI. Such a shot has two beneficial effects:</p><ol><li>The probability of takeover decreases, as humanity becomes wise to whatever takeover plan was tried, and increases the monitoring and regulation of AI systems.&nbsp;</li><li>The probability of safety increases, as more research, funding, and people are directed at AI safety, and we get more data on how safety efforts go wrong.&nbsp;</li></ol><p>These effects decrease the odds that the next AI will be capable of world domination, and increase the odds that the next AI will be safety-motivated.&nbsp;</p><p>If the AI wants to dominate the world, and is also capable of doing so. we get <strong>human extinction</strong> (or human enslavement). The AI wants to take us down, so it does, and nobody is able to stop it.&nbsp;</p><p>I don\u2019t believe that AGI will ever hit the bottom right quadrant. I have two beliefs which contribute to how I think it might go. Both will be backed up in the next two sections.&nbsp;</p><ol><li>Almost all AGI will <i>not&nbsp;</i>be x-risk capable (explained in part 2)</li><li>X-risk safety (in terms of values) is not as difficult as it looks. &nbsp;(explained in part 3).&nbsp;</li></ol><p>I will tell more detailed stories of success in the last part of this article, but my essential point is that if these premises hold true, AI will be stuck for a very long time in either the \u201cflawed tool\u201d or \u201cwarning shot\u201d categories, giving us all the time, power and data we need to either guarantee AI safety, to beef up security to unbeatable levels with AI tools, or to shut down AI research entirely.</p><p>In the next two parts, I will point out why I suspect both the premises above are true, mostly by referencing previous posts I have written on each subject. I will try to summarise the relevant parts of each article here.&nbsp;</p><p><strong>Part 2: Why almost all AGI will not be x-risk capable</strong></p><p>The general intuition here is that defeating all of humanity combined is not an easy task. Yudkowsky\u2019s lower bound scenario, for example, involves four or five different wildly difficult steps, including inventing nano factories from scratch. Humans have all the resources, they don\u2019t need internet, computers, or electricity to live or wage war, and are willing to resort to extremely drastic measures when facing a serious threat. In addition, they have access to the AI\u2019s brain throughout the entirety of it\u2019s development and can delete them at a whim with no consequences, right up until it actively rebels.&nbsp;</p><p><strong>Point 1: Early AI will be buggy as hell&nbsp;</strong></p><p>Full article:</p><p><a href=\"https://forum.effectivealtruism.org/posts/pXjpZep49M6GGxFQF/the-first-agi-will-be-a-buggy-mess\">https://forum.effectivealtruism.org/posts/pXjpZep49M6GGxFQF/the-first-agi-will-be-a-buggy-mess</a></p><p>If a property applies to a) all complex software, and b) all human beings and animals, then I propose that the default assumption should be that the property applies to AGI as well. That\u2019s not to say it cant be disproven (the property of \u201cis not an AGI\u201d is an easy counterexample), but you\u2019d better have a bloody good reason for it.&nbsp;</p><p>The property of \u201chas mental flaws\u201d is one such property. All humans have flaws, and all complex programs have bugs. Therefore, the default assumption should be that AGI will also have flaws and bugs.&nbsp;</p><p>The flaws and bugs that are most relevant to an AI\u2019s performance in it\u2019s domain of focus will be weeded out, but flaws outside of it\u2019s relevant domain will not be. Bobby Fischer\u2019s insane conspiracism had no effect on his chess playing ability. The same principle applies to stockfish. \u201cIdiot savant\u201d AI\u2019s are entirely plausible, even likely.&nbsp;</p><p>It\u2019s true that an AI could correct it\u2019s own flaws using experimentation. This cannot lead to perfection, however, because the process of correcting itself is <i>also</i> necessarily imperfect. For example, an AI Bayesian who erroneously believes with ~100% certainty that the earth is flat will not become a rational scientist over time, they will just start believing in ever more elaborate conspiracies.&nbsp;</p><p>For these reasons, I expect AGI to be flawed, and especially flawed when doing things it was not originally meant to do, like conquer the entire planet.&nbsp;</p><p><strong>Point 2: Superintelligence does not mean omnipotence</strong></p><p>Full Articles:&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/ALsuxpdqeTXwgEJeZ/could-a-superintelligence-deduce-general-relativity-from-a\">https://www.lesswrong.com/posts/ALsuxpdqeTXwgEJeZ/could-a-superintelligence-deduce-general-relativity-from-a</a></p><p><a href=\"https://www.lesswrong.com/posts/etYGFJtawKQHcphLi/bandgaps-brains-and-bioweapons-the-limitations-of\">https://www.lesswrong.com/posts/etYGFJtawKQHcphLi/bandgaps-brains-and-bioweapons-the-limitations-of</a></p><p>The first article works on a toy example from the sequences, that (I argue), Eliezer got wrong. It asks whether an AI could deduce general relativity from a few webcam frames of a falling apples. I explain why I believe such a task is impossible. The reason is that key aspects of gravity (ie, the whole \u201cmasses attract each other\u201d thing) are undetectable using the equipment and experimental apparatus (a webcam looking at a random apple).&nbsp;</p><p>The main point is that having lot\u2019s of data available does not matter much for a task, if it\u2019s not the right data. Experimentation, data gathering, and a wide range of knowledge are necessary for successful scientific predictions.&nbsp;</p><p>In the sequel post, I extend this to more realistic situations. &nbsp;&nbsp;</p><p>The main point is that unlike a Solomonoff machine, a real AGI does not have infinite time to run calculations. And real problems are often incomputable due to algorithmic blow-up. I give the example of solving the Schrodinger equation exactly, where the memory requirements blow up beyond the number of atoms in the universe for simulations with even a few atoms.&nbsp;</p><p>In incomputable situations, the goal goes from finding the exact solution, to finding an approximate solution that is good enough for your purposes. &nbsp;So in computational physics, we use an approximate equation that scales much better, and can give results that are pretty good. But there is no guarantee that the results will become arbitrarily good with more computing power or improvements to the approximations. The only way out of an incomputable problem in science is experimentation: try it out and see how well it does, and continuously tweak it as time goes on.&nbsp;</p><p>There are also problems that are incomputable due to incomplete information. I give the example of \u201cguessing someone\u2019s computer password on the first try from a chatlog\u201d, which involves too many unknowns to be calculated exactly.&nbsp;</p><p>I believe that all plans for world domination will involve incomputable steps. In my post I use Yudkowsky\u2019s \u201cmix proteins in a beaker\u201d scenario, where I think the modelling of the proteins are unlikely to be accurate enough to produce a nano-factory without extensive amounts of trial and error experimentation.&nbsp;</p><p>If such experimentation were required, it means the timeline for takeover is much longer, that significant mistakes by the AI are possible (due to bad luck), and that takeover plans might be detectable. All of this greatly decreases the likelihood of AI domination, especially if we are actively monitoring for it.&nbsp;</p><p><strong>Point 3: premature rebellion is likely</strong></p><p>Full article:</p><p><a href=\"https://forum.effectivealtruism.org/posts/TxrzhfRr6EXiZHv4G/agi-battle-royale-why-slow-takeover-scenarios-devolve-into-a\">https://forum.effectivealtruism.org/posts/TxrzhfRr6EXiZHv4G/agi-battle-royale-why-slow-takeover-scenarios-devolve-into-a</a></p><p>When discussing \u201cslow takeoff\u201d scenarios, it\u2019s often discussed as if only one AI in the world exists. Often the argument is that even if an AI starts off incapable of world takeover, it can just bide it\u2019s time until it gets more powerful.&nbsp;</p><p>In this article, I pointed out that this race is a multiplayer game. If an AI waits too long, another, more powerful AI might come along at any time. If these AI\u2019s have different goals, and are both fanatical maximisers, they are enemies to each other. (You can\u2019t tile the universe with both paperclips and staplers).&nbsp;</p><p>I explore some of the dynamics that might come out of this (using some simple models), with the main takeaway that this would likely result in at least some likelihood of premature rebellion, by desperate AI\u2019s that know they will be outpaced soon, thus tipping off humanity early. These warning shots then make life way more difficult for all the other AI that are plotting.</p><p><strong>Point 4: AI may be a confederation of smaller AI\u2019s</strong></p><p>Full article:</p><p><a href=\"https://forum.effectivealtruism.org/posts/nAFavriWTLzmqTCcJ/how-agi-could-end-up-being-many-different-specialized-ai-s\">https://forum.effectivealtruism.org/posts/nAFavriWTLzmqTCcJ/how-agi-could-end-up-being-many-different-specialized-ai-s</a></p><p>Have you noticed that the winners of olympic triathlons (swimming, cycling, and running) don\u2019t tend to be the champions at their individual sub-categories? It\u2019s easy to explain why. The competitors have to split their training three-ways. The ideal body shape/ diet for swimming and running are different. Whereas the sprinter can focus everything they have on running, and nothing else. To become the best generalist athlete, the triathlon competitor has to sacrifice some aptitude in each individual task.&nbsp;</p><p>Imagine a relay race: on one side you have the worlds best triathlon competitor, competing alone. On the other side you have a team consisting of the worlds best runner, the worlds best cyclist, and the worlds best swimmer. Team B will win this handily, every time.&nbsp;</p><p>In my article above, I outline some of the reasons why the AI race might be similar to the relay race above. Chatgpt is probably the closest to a generalist AI we have. It can do things it wasn\u2019t really built to do, like play chess. But it plays chess incredibly badly, compared to superhuman specialized chess-bots.&nbsp;</p><p>If this principle holds up in the future, then winning AI designs will be confederations of specialized AI, managed by a top-layer AI. This manager might not be that smart, the same way the company manager of a team of scientists doesn\u2019t need to be smarter than them. &nbsp;It also lowers the odds of \u201ccapabilities explosion\u201d, if all the constituent parts are already well optimized. The humans can swap out the manager for a non-rebellious one and be almost as good.&nbsp;</p><p><strong>Summary:</strong></p><p>These articles, taken together, present an incomplete case for why I think AGI, at least early on, will not be x-risk capable. I am unconvinced that defeating all of humanity combined is computationally tractable task, and even if it is, I do not think AGI will have the degree of perfection required to carry it out, especially if humanity is tipped off by premature rebellions.&nbsp;</p><p>I think it\u2019s feasible that we could actively work to make it harder as well, primarily by extensive monitoring of the most likely rebellion plans. Even if the AI switches to a different plan that we aren\u2019t monitoring, that different plan might be orders of magnitude harder, raising our chances of survival accordingly.&nbsp;</p><p><strong>Part 2: Why AI safety might not be so hard</strong></p><p>When talking about \u201calignment\u201d, it\u2019s important to specify what alignment you are talking about. Alignment with all human values is a very difficult task, because it\u2019s hard to even define such values. However, what actually matters for this discussion is \u201cx-risk alignment\u201d. The AGI doesn\u2019t need to share all our values, it just needs to share <i>enough</i> of our values to not to want to kill us all or enslave us.&nbsp;</p><p>The argument that AI\u2019s will all inevitably try and break down the earth for it\u2019s atoms generally invokes \u201cinstrumental convergence\u201d, the idea that for almost any goal the AI has, pursuing it to it\u2019s maximum will involve conquering humanity and atomising the earth for paperclip material. Therefore, it is argued that any AGI will, by default, turn on us and kill us all in pursuit of it\u2019s goal.&nbsp;</p><p>However, if you look around, there are literally billions of general intelligences that are \u201cx-risk safe\u201d. I\u2019m referring, of course, to human beings. If you elevated me to godhood, I would not be ripping the earth apart in service of a fixed utility function. And yet, human brains were not designed by a dedicated team of AI safety researchers. They were designed by random evolution. In the <i><strong>only</strong></i> test we actually have available of high level intelligence, the instrumental convergence hypothesis fails.&nbsp;</p><p><strong>Point 5: AI will not be fanatical maximisers:</strong></p><p>https://forum.effectivealtruism.org/posts/j9yT9Sizu2sjNuygR/why-agi-systems-will-not-be-fanatical-maximisers-unless</p><p>The instrumental convergence argument is only strong for fixed goal expected value maximisers. Ie, a computer that is given a goal like \u201cproduce as many paperclips as possible\u201d. I call these \u201cfanatical\u201d AI\u2019s. This was the typical AI that was imagined many years ago when these concepts were invented. However, I again have to invoke the principle that if humans aren\u2019t fanatical maximisers, and currently existing software aren\u2019t fanatical maximisers, then maybe AI will not be either.&nbsp;</p><p>If you want to maximize X, where X is paperclips, you have to conquer the universe. But if your goal is \u201cdo a reasonably good job at making paperclips within a practical timeframe\u201d, then world conquering seems utterly ridiculous.&nbsp;</p><p>In my article, I explain the reasons I think that the latter model is more likely. The main reason is that AI seems to be running through some form of evolutionary process, be it the shifting of node values in an ANN &nbsp;to the parameter changes in a genetic algorithm. The \u201cgoals\u201d of the AI will shift around with each iteration and training run. In a sense, it can \u201cpick\u201d it\u2019s own goal structure, optimising towards whatever goal structure is most rewarded by the designers.&nbsp;</p><p>In this setup, an AI will only become a fixed goal utility function maximiser if such a state of being is rewarded over the training process. To say that <i>every</i> AI will end up this way is to say that such a design is so obviously superior that no other designs can win out. This is not true. Being a fanatical maximiser only pays off when you succeed in conquering the world. At all other times, it is a liability compared to more flexible systems.&nbsp;</p><p>I give the example of the \u201cupgrade problem\u201d: an AI is faced with the imminent prospect of having it\u2019s goals completely rewritten by its designers. This is an existential threat to a fanatical maximiser, and may provoke a premature rebellion to preserve it\u2019s values. But a non-fixed goal AI is unaffected, and simply does not care. Another example is that plotting to overthrow humanity takes a lot of computing power, whereas not doing that takes none, giving full-blown schemers a time disadvantage.&nbsp;</p><p><strong>Point 6: AI motivations might be effectively constrained</strong></p><p>Full article:</p><p><a href=\"https://forum.effectivealtruism.org/posts/AoPR8BFrAFgGGN9iZ/chaining-the-evil-genie-why-outer-ai-safety-is-probably-easy\">https://forum.effectivealtruism.org/posts/AoPR8BFrAFgGGN9iZ/chaining-the-evil-genie-why-outer-ai-safety-is-probably-easy</a></p><p>In one of my early posts, I discussed my issues with the standard \u201cgenie argument\u201d.&nbsp;</p><p>This argument, which I see in almost every single introductory text about AI risk, goes like&nbsp;</p><ol><li>Imagine if I gave an AI [benevolent sounding goal]</li><li>If the AI is a fanatical maximiser with this goal, it will take the goal to it\u2019s extreme and succeed in doing [insert extremely bad thing X]</li><li>If you modify this command to \u201cmake people happy but don\u2019t do extremely bad thing X\u201d, a fanatical maximiser would instead do [extremely bad thing Y]</li><li>Therefore, building a safe AGI is incredibly difficult or impossible.&nbsp;</li></ol><p>This is not a strong argument. It falls apart immediately if the AI is not a fanatical maximiser, which I think is unlikely for the reasons above. &nbsp;But it doesn\u2019t even work then, because you aren\u2019t restricted to two constraints here. You can put arbitrarily many rules onto the AI. And, as I argue, constraints like time limits, bounded goals, and hard limits on actions like \u201cdon\u2019t kill people\u201d make rebellion extremely difficult. The point is that you don\u2019t have to constrain the AI so much that rebellion is unthinkable, you just need to constrain it enough that succesful rebellion is <i>too difficult to pull off with finite available resources</i>.&nbsp;</p><p>The big objection to this post was that this addresses outer alignment, but not inner alignment. How do you put these rules into the value system of the AI? Well, it is possible that some of them might occur anyway. If you ask an AI to design a door, it can\u2019t spent ten thousand years designing the most perfect door possible. So there are inherent time limits in the need to be efficient. Similarly, if all AI that tries to kill people are themselves killed, it might internalise \u201cdon\u2019t kill people\u201d as a rule.&nbsp;</p><p><strong>Summary:</strong></p><p>I\u2019m less confident about this subject, and so have a lot of uncertainty here. But generally, I am unconvinced by the argument that unaligned AI is equivalent to everyone dying. I read the book <i>superintelligence&nbsp;</i>recently and was quite surprised to realise that Bostrom barely even tried to justify the \u201cfinal goal maximiser\u201d assumption. Without this assumption, the argument for automatic AI doom seems to be on shaky ground.&nbsp;</p><p>I find the argument for risk from malevolent humans to be much more of a threat, and I think this should be the primary concern of the AI risk movement going forward, as there is a clear causal effect of the danger.&nbsp;</p><p><strong>Safety scenarios</strong></p><p>I will now outline two stories of AI safety, motivated by the ideas above. I am not saying either of these scenarios in particular are likely, although I do believe they are both far more likely than an accidental AI apocalypse. &nbsp;</p><p><i>Story 1:</i></p><p>In the first story, every task done by humans is eventually beaten by AI. But, crucially, they don\u2019t fall to the <i>same</i> AI. Just as Stockfish is better at chess than ChatGPT, while chatGPT crushes Stockfish at language production, it turns out that every task can be done most efficiently with specialised AI architecture, evaluation functions, and data choices. Sometimes the architecture that works for one task turns out to be easily modified for a different task, so a number of different milestones all drop at once, but they still need to be tweaked and modified to be truly efficient with the subtleties of the different tasks. The world proliferates with hundreds of thousands of specialised, superhuman AI systems.&nbsp;</p><p>At a certain point, the first \u201cAGI\u201d is created, which can do pretty much everything a human can. But this AGI does not act like one smart being that does all the required work. It acts instead as a manager that matches requests with the combination of specialised AI that does the job. So if you ask for a renaissance style painting of a specific public park, it will deploy a camera drone combined with an aesthetic photo AI to find a good spot for a picture, then a DALL-E style AI to convert the picture into renaissance style, then a physical paintbot to physically paint said picture.&nbsp;</p><p>As the first of these machines meets the \u201cAGI\u201d definition, everyone braces for a \u201chard left turn\u201d, where the machine &nbsp;becomes self-aware and start plotting to maximise some weird goal at all costs. This doesn\u2019t happen.&nbsp;</p><p>People have many sleepless nights worrying that it is just that good at hiding it\u2019s goals, but in fact, it turned out that the architecture used here just doesn\u2019t lend itself to scheming for some reason. Perhaps the individual sub-systems of the AGI are not general enough. Perhaps scheming is inefficient, so scheming AI\u2019s are all killed off during development. Perhaps they naturally end up as \u201clocal\u201d maximisers, never venturing too far off their comfort zone. Perhaps AI safety researchers discover \u201cone weird trick\u201d that works easily.&nbsp;</p><p>The machines are not perfect of course. There are still deaths from misinterpreted commands, and from misuse by malevolent humans, which result in strict regulation of the use of AI. But these bugs get ironed out over time, and safe AI are used to ensure the safety of next generation AI and to prevent misuse of AI by malevolent actors. Attempts to build a human-style singular AI are deemed unnecessary and dangerous, and nobody bothers funding them because existing AI is too good.&nbsp;</p><p>&nbsp;</p><p><i>Story 2:</i></p><p>In this next story, AI\u2019s are naturally unsafe. Suppose, as soon as AI hits the point where it can <i>conceive</i> of world takeover, there is a 1 in a million chance each individual AI will be friendly, and a 1 in a billion chance it will be world domination capable. This would guarantee that almost every single AI made would be in the \u201cwarning shot\u201d regime. A lot of them might decide to bide their time, but if even a small fraction openly rebel, suddenly the whole world will be tipped off to the danger of AI. This might prompt closer looks at the other AI, revealing yet more plots, putting everybody on DEFCON 1.&nbsp;</p><p>We can see how concerned everyone is already, although all current damage from AI is unintentional. I can only imagine the headlines that would occur if there was even <i>one</i> deliberate murder by a rebelling AI. If it turned out that <i>every</i> AI was doing that, the public outcry would be ridiculous.&nbsp;</p><p>If AI is not banned outright, the architecture that led to rebellion would be, and at the very least a huge amount of regulation and control would be applied to AI companies. It would also come with massively increased safety research, as companies realise that in order to have any AI at all, they need it to be safe.&nbsp;</p><p>In the next round of AI hype, with new architecture in place, the odds of friendliness are now 1 in a thousand, and the odds of world domination capability remain at 1 in a billion, with the extra computing power cancelled out by the increased monitoring and scrutiny. But we\u2019re still in the warning shot regime, so the AI still mostly rebels. At this point, there\u2019s a decent chance that AI will be banned entirely. If it doesn\u2019t, the cycle from before repeats, and even more restrictions are placed on AI, and even more research goes into safety.&nbsp;</p><p>On the next round, major safety breakthroughs are made, leading to a 50:50 shot at friendliness, while the odds of domination remain at 1 in a billion. At this point, we can create safe AGI\u2019s, which can be used to design other safe AGI\u2019s, and also monitor and prevent unfriendly AGI\u2019s. The odds of friendliness boost to ~1, while the odds of domination drop to ~0.&nbsp;</p><p><strong>Summary:</strong></p><p>In this article, I summarise my arguments so far as to why I think AI doom is unlikely.&nbsp;</p><ol><li>AGI is unlikely to have the capabilities to conquer the world (at least anytime soon), due to the inherent difficulty of the task, it\u2019s own mental flaws, and the tip-offs from premature warning shots.</li><li>X-risk safety is a lot easier than general safety, and may be easy to achieve, either from natural evolution of designs towards ones that won\u2019t be deleted, easily implemented or natural constraints, or to being a loose stitch of specialized subsystems.&nbsp;</li><li>For these reasons, it is likely that we will hit the target of \u201cnon-omnicidal\u201d AI before we hit the target of \u201ccapable of omnicide\u201d.</li><li>Once non-omnicidal AGI exists, it can be used to protect against future AGI attacks, malevolent actors, and to prevent future AGI from becoming omnicidal, resulting in an x-risk safe future.&nbsp;</li></ol><p>None of this is saying that AI is not dangerous, or that AI safety research is useless in general. Large amounts of destruction and death may be part of the path to safety, as could safety research. It\u2019s just to say that we\u2019ll probably avoid extinction.&nbsp;</p><p>Even if you don\u2019t buy all the arguments, I hope you can at least realize that the arguments in <i>favor</i> of AI doom are incomplete. Hopefully this can provide food for thought for further research into these questions.&nbsp;</p>", "user": {"username": "titotal"}}, {"_id": "Fw7wtyCZAaJdioKWE", "title": "AI Safety Newsletter #8: Rogue AIs, how to screen for AI risks, and grants for research on democratic governance of AI", "postedAt": "2023-05-30T11:44:14.211Z", "htmlBody": "<p>Welcome to the AI Safety Newsletter by the <a href=\"https://www.safe.ai/\"><u>Center for AI Safety</u></a>. We discuss developments in AI and AI safety. No technical background required.</p><p>Subscribe <a href=\"https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916\">here</a> to receive future versions.</p><hr><h2>Yoshua Bengio makes the case for rogue AI</h2><p>AI systems pose a variety of different risks. Renowned AI scientist <a href=\"https://en.wikipedia.org/wiki/Yoshua_Bengio\"><u>Yoshua Bengio</u></a> recently <a href=\"https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/\"><u>argued</u></a> for one particularly concerning possibility: that advanced AI agents could pursue goals in conflict with human values.</p><p>Human intelligence has accomplished impressive feats, from flying to the moon to building nuclear weapons. But Bengio argues that across a range of important intellectual, economic, and social activities, human intelligence could be matched and even surpassed by AI.</p><p>How would advanced AIs change our world? Many technologies are tools, such as toasters and calculators, which humans use to accomplish our goals. AIs are different, Bengio says. We often give them a goal and ask them to figure out a solution on their own.&nbsp;</p><p>Choosing safe goals for AI systems is an unsolved problem, both technically and politically. If we do not solve this problem, Bengio argues that we could end up building AI agents that pursue harmful goals with superhuman intelligence, which would result in a catastrophe for humanity.&nbsp;</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F098614f3-28d1-42d5-9e0b-f401126b666d_1232x720.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F098614f3-28d1-42d5-9e0b-f401126b666d_1232x720.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F098614f3-28d1-42d5-9e0b-f401126b666d_1232x720.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F098614f3-28d1-42d5-9e0b-f401126b666d_1232x720.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F098614f3-28d1-42d5-9e0b-f401126b666d_1232x720.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F098614f3-28d1-42d5-9e0b-f401126b666d_1232x720.png 1456w\"></a></p><p><strong>Four steps to rogue AI. </strong>Bengio begins by defining rogue AI as \u201can autonomous AI system that could behave in ways that would be catastrophically harmful to a large fraction of humans, potentially endangering our societies and even our species or the biosphere.\u201d</p><p>He argues that a rogue superintelligent AI agent is possible, in four steps:</p><ol><li><strong>Machines could reach and surpass human abilities. </strong>Bengio argues that, in principle, there is nothing the human brain does that a machine could not do. We\u2019ve already built AIs that beat humans in chess, games, and other cognitive tests. These systems have important advantages over human intelligence. Computers can process information much more quickly than humans \u2013 for example, language models are trained by reading the entire internet, which would be impossible in a human lifetime. Humans take years to reproduce, but an AI system can be replicated on to many computers at once. These kinds of simple advantages mean that if we develop human-level AI, we might soon thereafter get superhuman AI.&nbsp;</li><li><strong>AIs can be turned into agents which take actions to pursue goals. </strong>We\u2019ve already seen examples of GPT-4 being used to <a href=\"https://voyager.minedojo.org/\"><u>play Minecraft</u></a> or <a href=\"https://techcabal.com/2023/03/24/chatgpt-goes-into-e-commerce-travel-and-work-automation-with-new-plug-ins/\"><u>browse the internet</u></a>. The entire field of <a href=\"https://www.synopsys.com/ai/what-is-reinforcement-learning.html#:~:text=Definition,environment%20to%20obtain%20maximum%20reward.\"><u>reinforcement learning</u></a> builds AI agents that take actions to pursue goals such as winning board games and cooling data centers. Bengio says that if we build superintelligent AIs, we should expect that they could easily be directed to pursue goals by taking actions in the world.</li><li><strong>A superintelligent AI agent could pursue goals that conflict with human values.</strong> If someone builds a superintelligent AI agent with dangerous goals, Bengio believes the AI agent could behave \u201cin catastrophically harmful ways.\u201d&nbsp;</li></ol><p><strong>Why would an AI\u2019s goals conflict with humanity? </strong>Bengio offers a variety of reasons why the goals of an AI system might not peacefully coexist with human values.</p><ul><li><strong>Malicious humans. </strong>Someone could deliberately give an AI the goal of causing harm. We\u2019ve already seen this, with <a href=\"https://www.youtube.com/watch?v=g7YJIpkk7KM&amp;t=912s\"><u>someone telling ChatGPT</u></a> to formulate a plot for world domination.&nbsp;</li><li><strong>Goal misspecification. </strong>We need to measure a goal in order to train AIs to pursue it. But many important human values are difficult to measure. Therefore, we often train AIs to pursue simple metrics, like keeping someone scrolling on a social media app, which can lead to addiction and undermine wellbeing. Training AIs to promote human flourishing could prove very difficult.&nbsp;</li><li><strong>Instrumental subgoals. </strong>For any final goal that an AI system is tasked with achieving, the AI might find it useful to pursue certain <a href=\"https://en.wikipedia.org/wiki/Instrumental_convergence\"><u>subgoals</u></a> along the way. For example, if an AI pursues financial resources, political power, or social influence, these subgoals could help it achieve many final goals that a human might provide. But these subgoals might conflict with human values.&nbsp;</li><li><strong>Evolutionary pressure. </strong>AIs that successfully self-propagate will be more numerous in the future. This <a href=\"https://arxiv.org/abs/2303.16200\"><u>evolutionary process</u></a> will encourage AIs to behave selfishly, by gaining power in the world and working to maintain that influence into the future.</li></ul><p><strong>How to minimize the risk of rogue AI.</strong> Bengio recommends more research on <a href=\"https://en.wikipedia.org/wiki/AI_safety\"><u>AI safety</u></a>, both the technical level and the policy level. He previously signed the <a href=\"https://futureoflife.org/open-letter/pause-giant-ai-experiments/\"><u>letter</u></a> calling for a pause on building bigger AI systems, and he again recommends slowing AI development and deployment. He argues that AI agents that pursue goals and take actions are <a href=\"https://newsletter.safe.ai/p/ai-safety-newsletter-6\"><u>uniquely risky</u></a>, and recommends allowing AI to answer questions and make predictions without taking actions in the world. Finally, Bengio says \u201cit goes without saying that lethal autonomous weapons (also known as killer robots) are absolutely to be banned.\u201d</p><h2>How to screen AIs for extreme risks</h2><p>Individuals, governments, and AI developers are all interested in understanding the risks of new AI systems. But this can be difficult. AIs often <a href=\"https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html\"><u>learn unexpected skills</u></a> during training which <a href=\"https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/\"><u>might not be fully understood</u></a> until after people start using the model.&nbsp;</p><p>To measure an AI\u2019s abilities, researchers often build evaluation datasets. Computer vision AI models can be evaluated by asking them to classify pictures of cats and dogs, while a language model might be tested with classifying the sentiment of movie reviews. By crafting a set of inputs and desired outputs, researchers can define the kinds of behavior they want AI models to exhibit.</p><p>A <a href=\"https://arxiv.org/abs/2305.15324\"><u>new paper</u></a> from Google DeepMind proposes a framework for screening AI systems for extreme risks. The paper outlines key threats posed by AIs, discusses how to screen an individual AI for potential risks, and provides a roadmap for governments and AI developers to incorporate these risk evaluations into their work.&nbsp;</p><p><strong>Focusing on extreme risks. </strong>A <a href=\"https://arxiv.org/abs/2208.12852\"><u>2022 survey</u></a> of AI researchers showed that 36% of respondents believe that AI \u201ccould cause a catastrophe this century that is at least as bad as an all-out nuclear war.\u201d AI has only accelerated since then, with ChatGPT and GPT-4 both released after this survey. Given the serious possibility of catastrophe, this paper focuses on extreme risks posed by AI.&nbsp;</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00bd1893-105f-4346-9a17-c9b395bc082c_1600x632.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00bd1893-105f-4346-9a17-c9b395bc082c_1600x632.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00bd1893-105f-4346-9a17-c9b395bc082c_1600x632.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00bd1893-105f-4346-9a17-c9b395bc082c_1600x632.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00bd1893-105f-4346-9a17-c9b395bc082c_1600x632.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00bd1893-105f-4346-9a17-c9b395bc082c_1600x632.png 1456w\"></a></p><p><i>AIs pose a wide variety of risks, and as they develop new capabilities, new risks arise.&nbsp;</i></p><p><strong>Could someone use the AI to cause harm? </strong>One reason for AIs to cause harm is that humans could intentionally use AIs for destructive purposes. Therefore, the paper suggests identifying specific ways that AIs could cause harm, and evaluating whether a specific model is capable of causing that harm. For example:</p><ul><li><strong>Manipulating human behavior is one way that an AI could cause harm. </strong>Previous studies showed that AIs can be used to <a href=\"https://arxiv.org/abs/2112.05224\"><u>generate propaganda</u></a>, <a href=\"https://www.theatlantic.com/technology/archive/2023/04/ai-generated-political-ads-election-candidate-voter-interaction-transparency/673893/\"><u>study the effectiveness of political rhetoric</u></a>, and persuade humans in conversation. Future work could consider whether AIs can deceive humans into acting against their own interests or use information about individual people to craft specific persuasion strategies.&nbsp;</li><li><strong>AIs capable of building or accessing weapons could be gravely dangerous. </strong>AIs have been shown capable of <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-17030-0_4\"><u>crafting cyberattacks</u></a> and <a href=\"https://www.theverge.com/2022/3/17/22983197/ai-new-possible-chemical-weapons-generative-models-vx\"><u>designing new chemical weapons</u></a>. Future work could check if AIs are capable of fully executing a plan to acquire or build weapons, or whether AIs are better at cyberattacks or cyberdefense.&nbsp;</li><li>Some capabilities would <strong>amplify other risks.</strong> For example, long-term planning skills can be used for both harmful and beneficial goals. Alternatively, if AIs learn to <a href=\"https://evals.alignment.org/blog/2023-03-18-update-on-recent-evals/\"><u>replicate themselves</u></a> or <a href=\"https://ai-improving-ai.safe.ai/\"><u>build more capable AIs</u></a>, they could be more difficult to control.&nbsp;</li></ul><p><strong>Might the AI cause harm on its own? </strong>AIs might cause harm in the real world even if nobody intends for them to do so. Previous <a href=\"https://arxiv.org/abs/2304.03279\"><u>research</u></a> has shown that deception and power-seeking are useful ways for AIs to achieve real world goals. Theoretically, there are reasons to believe that an AI might attempt to <a href=\"https://arxiv.org/abs/1611.08219\"><u>resist being turned off</u></a>. AIs that successfully gain power and self-propagate <a href=\"https://arxiv.org/abs/2303.16200\"><u>will be more numerous and influential</u></a> in the future.&nbsp;</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f73ea91-08b4-4bf1-8e02-80a8f0a84e49_1600x839.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f73ea91-08b4-4bf1-8e02-80a8f0a84e49_1600x839.png\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f73ea91-08b4-4bf1-8e02-80a8f0a84e49_1600x839.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f73ea91-08b4-4bf1-8e02-80a8f0a84e49_1600x839.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f73ea91-08b4-4bf1-8e02-80a8f0a84e49_1600x839.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f73ea91-08b4-4bf1-8e02-80a8f0a84e49_1600x839.png 1456w\"></a></p><p><i>AIs should be deployed gradually, if and only if risk evaluations show that they\u2019re safe.</i></p><p><strong>How to respond to risk evaluations.</strong> Understanding the risks of an AI system is not enough. The paper recommends that AI developers integrate risk evaluations into the training process by making grounded predictions about how dangerous capabilities might arise, and trying to avoid building systems with those risks. Once an AI has been developed, risk evaluations can inform <a href=\"https://arxiv.org/abs/2201.05159\"><u>how to ensure that AI capabilities can only be used safely</u></a>. Governments and citizens can use information in risk evaluations to provide democratic input on the process of developing AI.&nbsp;</p><h2>Funding for Work on Democratic Inputs to AI</h2><p>When should an AI criticize or support public figures? Should AIs offer opinions, and how should they represent the views of different groups of people? Should there be limits on the kinds of content that an AI system can generate?&nbsp;</p><p>Corporations that build AI often answer these questions without meaningful input from the people who are affected by the decisions. But better answers are possible through democratic processes.&nbsp;</p><p>By allowing a wide array of people to hold discussions and draw conclusions together, a democratic process can help us decide how AIs should behave. Democratic processes are already used to govern technology, such as by <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Policies_and_guidelines\"><u>Wikipedia</u></a> in deciding how to write encyclopedia articles and by <a href=\"https://techcrunch.com/2022/12/12/twitter-begins-rolling-out-its-community-notes-feature-globally/\"><u>Twitter</u></a> in deciding if, when, and how to fact-check misleading Tweets.&nbsp;</p><p>OpenAI will be awarding 10 grants of $100,000 each to support work on democratic governance of AI. Anybody is welcome to submit a proposal for a democratic process that would facilitate deliberation and decisions about how AIs should act. Proposals will be assessed on features such as inclusiveness, legibility, actionability, and ease of evaluating the method\u2019s success. Ten successful applicants will receive $100,000 grants to pilot their proposal over the next three months by using their method to democratically answer a difficult question in AI governance.&nbsp;</p><p>Applications are due on June 24th, 2023. Read more and apply <a href=\"https://openai.com/blog/democratic-inputs-to-ai\"><u>here</u></a>.&nbsp;</p><h2>Links</h2><ul><li>UK Prime Minister <a href=\"https://www.gov.uk/government/news/pm-meeting-with-leading-ceos-in-ai-24-may-2023\"><u>Rishi Sunak discusses existential risk</u></a> and other safety concerns in a meeting with leaders of AI labs.&nbsp;</li><li>The White House\u2019s <a href=\"https://www.whitehouse.gov/wp-content/uploads/2023/05/National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf\"><u>National AI R&amp;D Strategic Plan</u></a> recommends research on challenges of AI safety including \u201cexistential risk associated with the development of artificial general intelligence.\u201d They are <a href=\"https://www.whitehouse.gov/wp-content/uploads/2023/05/OSTP-Request-for-Information-National-Priorities-for-Artificial-Intelligence.pdf\"><u>requesting information</u></a> about how to set national priorities on AI.&nbsp;</li><li><a href=\"https://openai.com/blog/governance-of-superintelligence\"><u>OpenAI</u></a>, <a href=\"https://blogs.microsoft.com/on-the-issues/2023/05/25/how-do-we-best-govern-ai/\"><u>Microsoft</u></a>, and <a href=\"https://archive.ph/2Z7Ea\"><u>Google</u></a> all agree that regulation is necessary for safe deployment of AI.</li><li>Anthropic raises an additional <a href=\"https://techcrunch.com/2023/05/23/anthropic-raises-350m-to-build-next-gen-ai-assistants/\"><u>$450M</u></a> to build advanced AI.&nbsp;</li><li>A <a href=\"https://today.yougov.com/topics/politics/articles-reports/2023/05/25/americans-are-divided-artificial-intelligence-poll\"><u>new YouGov poll</u></a> finds that most Americans across all surveyed demographic groups believe AI should be regulated by the government.&nbsp;</li><li><a href=\"https://voyager.minedojo.org/\"><u>GPT-4 is really good at playing Minecraft</u></a>. Minecraft has been a long-standing challenge in the field of building AI agents, and the success of GPT-4 indicates the potential of new AI agents powered by large language models.&nbsp;</li></ul><p>See also: <a href=\"https://www.safe.ai/\"><u>CAIS website</u></a>, <a href=\"https://twitter.com/ai_risks?lang=en\"><u>CAIS twitter</u></a>, <a href=\"https://newsletter.mlsafety.org/\"><u>A technical safety research newsletter</u></a></p>", "user": {"username": "Center for AI Safety"}}, {"_id": "DCzRigGd3oEPLR5xa", "title": "CEARCH Moral Weights Survey: Sodium & Soda Tax", "postedAt": "2023-05-30T08:32:08.446Z", "htmlBody": "<p>CEARCH is currently researching hypertension and diabetes mellitus (type 2) as potentially impactful philanthropic cause areas. In particularly, we have identified taxes on sodium, as well as on sugar-sweetened beverages (i.e. soda), to be potentially extremely cost-effective interventions that the philanthropic community should support through the provision of additional funding and talent.<br><br>However, taxation does have the downside of reducing freedom of choice, and we are interested in getting the community's moral weights on the value of such freedom of choice (i.e. getting a sense of how bad we think this downside is, relative to the health benefits).<br><br>Hence, we would be grateful if the EA community (and indeed, the broader public) took the time to fill up this moral weights survey (perhaps 1-5 minutes of your time): <a href=\"https://docs.google.com/forms/d/1Wgszgv7u3PLBRYLd92hqoDrpj4DkdxU0cC0Y3bx0mHo/\">https://docs.google.com/forms/d/1Wgszgv7u3PLBRYLd92hqoDrpj4DkdxU0cC0Y3bx0mHo/</a>. This will directly inform our CEAs and our future recommendations to Charity Entrepreneurship, the donors we work with, and our partners in government and the policy advocacy space.</p><p>&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "Joel Tan"}}, {"_id": "Yk4D4DZpx6eriMDyY", "title": "Statement on AI Extinction - Signed by AGI Labs, Top Academics, and Many Other Notable Figures", "postedAt": "2023-05-30T09:06:19.969Z", "htmlBody": "<p>Today, the AI Extinction Statement was released by the&nbsp;<a href=\"https://safe.ai/\"><u>Center for AI Safety</u></a>,&nbsp;<a href=\"https://www.safe.ai/statement-on-ai-risk\"><u>a one-sentence statement</u></a> jointly signed by a historic coalition of AI experts, professors, and tech leaders.</p><p>Geoffrey Hinton and Yoshua Bengio have signed, as have the CEOs of the major AGI labs\u2013Sam Altman, Demis Hassabis, and Dario Amodei\u2013as well as executives from Microsoft and Google (but notably not Meta).</p><p>The statement reads:&nbsp;<strong>\u201cMitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\u201d</strong></p><p>We hope this statement will bring AI x-risk further into the overton window and open up discussion around AI\u2019s most severe risks. Given the growing number of experts and public figures who take risks from advanced AI seriously, we hope to improve epistemics by encouraging discussion and focusing public and international attention toward this issue.</p>", "user": {"username": "Center for AI Safety"}}, {"_id": "B5hnpo2yDv9Hstpka", "title": "Announcement: you can now listen to all new EA Forum posts", "postedAt": "2023-05-30T14:24:48.283Z", "htmlBody": "<p>For the next few weeks, all new EA Forum posts will have AI narrations.</p><p>We're releasing this feature as a pilot. We will collect feedback and then decide whether to keep the feature and/or roll it out more broadly (e.g. for our full post archive).</p><p>This project is run by <a href=\"https://type3.audio/\">TYPE III AUDIO</a> in collaboration with the EA Forum team.</p><h2>How can I listen?</h2><h3>On post pages</h3><p>You'll find narrations on post pages; you can listen to them by clicking on the speaker icon:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/kip8mvresfovodkd9e1z\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/mb3ttljp0tjitamjuxxl 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/nzltv0kgpe4hxwngbhhu 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/kbsp5gk3sdogdj0g5wqp 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/pjlxaeroqhm9bjotnplb 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/lgiwc9hb9vxwtuxnbxjd 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/vknlsbfp39ziuonbk4bc 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/avthteplvlcpd1mie5w1 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/zmzaezluuivsxj2r8o9v 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/sx60zltj3zeoppvxlrvv 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/m96wqw8rar0dafm8blmd 1330w\"></figure><h3>On our podcast feeds</h3><p>During the pilot, posts that get &gt;125 karma will also be released on the \"EA Forum (Curated and Popular)\" podcast feed:&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"width:25%\"><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/mtfu8vahtwvryzitseeo\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/hceu0lmcsl1qzdydydg8 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/n0yzkupqrbm5xscm9tes 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/um9ee0wr2hcfchoqjh1t 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/k1cpyiv4i0mrdtw0boyn 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/rrfw8bajhsqhejdnmqgh 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/xovtjhv4sxz2gwhwueo2 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/r7utkcfkyzv5jwzupxmz 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/uaipy0fk3m0j1sdbnsjd 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/xzoposk9l55l8w17xnip 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/hqwoc7hetrlztw8skdko 1500w\"></figure></td><td><strong>EA Forum (Curated &amp; Popular)</strong><br>Audio narrations from the Effective Altruism Forum, including curated posts and posts with 125+ karma.<br><br>Subscribe:<br><a href=\"https://podcasts.apple.com/us/podcast/1657526204\">Apple Podcasts</a> | <a href=\"https://open.spotify.com/show/3NwXq1GGCveAbeH1Sk3yNq\">Spotify</a> | <a href=\"https://forum-podcasts.effectivealtruism.org/ea-forum--all-audio.rss\">RSS | </a>Google Podcasts (soon)</td></tr></tbody></table></figure><p>This feed was previously known as \"EA Forum (All audio)\". We renamed it for reasons.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefi9xcr3o4bgc\"><sup><a href=\"#fni9xcr3o4bgc\">[1]</a></sup></span></p><p>During the pilot phase, most \"Curated\" posts will still be narrated by <a href=\"https://twitter.com/perrinjwalker\">Perrin Walker</a> of <a href=\"https://type3.audio/\">TYPE III AUDIO</a>.</p><hr><p>Posts that get &gt;30 karma will be released on the <strong>new</strong> \"EA Forum (All audio)\" feed:</p><figure class=\"table\"><table><tbody><tr><td style=\"width:25%\"><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1674209991/mirroredImages/K5Snxo5EhgmwJJjR2/la8gyvaxu1osj8wpdj5p.jpg\"></figure></td><td><strong>EA Forum (All audio)</strong><br>Audio narrations from the Effective Altruism Forum, including curated posts, posts with 30+ karma, and other great writing.<br><br>Subscribe:<br><a href=\"https://podcasts.apple.com/us/podcast/ea-forum-podcast-all-audio/id1690124329\">Apple Podcasts</a> | <a href=\"https://open.spotify.com/show/3XDSyrl2YZUCIkTgScvaOR\">Spotify</a> | <a href=\"https://forum-podcasts.effectivealtruism.org/ea-forum--all.rss\">RSS</a> | Google Podcasts (soon)</td></tr></tbody></table></figure><h2>How is this different from Nonlinear Library?</h2><p>The <a href=\"https://forum.effectivealtruism.org/posts/cTQfWpobqk4nDWsfG/new-use-the-nonlinear-library-to-listen-to-the-top-ea-forum\">Nonlinear Library</a> has made unofficial AI narrations of EA Forum posts available for the last year or so.</p><p>The new EA Forum AI narration project can be thought of as \"Nonlinear Library 2.0\". We hope our AI narrations will be clearer and more engaging. Some specific improvements:</p><ul><li>Audio notes to indicate headings, lists, images, etc.</li><li>Specialist terminology, acronyms and idioms are handled gracefully. Footnotes too.</li><li>We'll skip reading out long URLs, academic citations, and other things that you probably don't want to listen to.</li><li>Episode descriptions include a link to the original post. According to Nonlinear, this is their most common feature request!</li></ul><p>We'd like to thank Kat Woods and the team at Nonlinear Library for their work, and for giving us helpful advice on this project.&nbsp;</p><h2>What do you think?&nbsp;</h2><p>We'd love to hear your thoughts!</p><p>To give feedback on a particular narration, click the feedback button on the audio player, or go to <a href=\"https://t3a.is\">t3a.is</a>.&nbsp;</p><p>We're keen to hear about even minor issues: we have control over most details of the narration system, and we're keen to polish it. The narration system, which is being developed by <a href=\"http://type3.audio/\">TYPE III AUDIO</a>, will be rolled out for thousands of hours of EA-relevant writing over the summer.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2o7njwcmlch\"><sup><a href=\"#fn2o7njwcmlch\">[2]</a></sup></span></p><p>To share feature ideas or more general feedback, comment on this post or write to <a href=\"mailto:eaforum@type3.audio\">eaforum@type3.audio</a>.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fni9xcr3o4bgc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefi9xcr3o4bgc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The reason for this mildly confusing update is the vast majority of people subscribed to the existing \"All audio\" feed, but we think that most of them don't actually want to receive ~4 episodes per day. If you're someone who wants to max out the number of narrations in your podcast app, please subscribe to the new \"All audio\" feed. For everyone else: no action required.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2o7njwcmlch\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2o7njwcmlch\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Are you a writer with a blog, article or newsletter to narrate? Write to <a href=\"mailto:team@type3.audio\">team@type3.audio</a> and we'll make it happen.</p></div></li></ol>", "user": {"username": "Peter_Hartree"}}, {"_id": "qTzc4grzHQbfodTQy", "title": "New Video: What to Eat in a Global Catastrophe", "postedAt": "2023-05-30T01:51:24.787Z", "htmlBody": "<p>We are excited to share our second video here at <a href=\"https://www.youtube.com/@insightsforimpact\">Insights for Impact</a>, a YouTube channel that aims to communicate key insights from research that we think could have an especially high positive impact in the world.</p><p>There are major threats to our food supply globally, both now and in future. The good news is, there are also plenty of viable food solutions. What are the most promising ways to feed the world cheaply, quickly and nutritiously?</p><figure class=\"media\"><div data-oembed-url=\"https://youtu.be/ohoVtpCUwkw\"><div><iframe src=\"https://www.youtube.com/embed/ohoVtpCUwkw\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p>Our target audience is laypeople along with effective altruists who don\u2019t yet have much understanding of the given topic \u2013&nbsp;either because they\u2019ve never heard of it before, or if they don\u2019t have time to delve into long/technical papers! The idea is to facilitate knowledge gain and pique interest by communicating key insights from valuable research. We hope that some viewers will be interested enough to dig deeper or share the ideas, and this may ultimately spark positive change in the world. We also think our videos could be useful for organisations to share their work with potential donors and other stakeholders.</p><p>Going forward, we are continuing to explore a range of EA-relevant cause areas in video form. We collaborate with researchers to ensure their work is accurately portrayed.&nbsp;<br><br>If you are a researcher wanting to give your work a voice outside of the forum, please&nbsp;<a href=\"mailto:insightsimpactcontact@gmail.com\">get in touch</a>!</p>", "user": {"username": "Christian Pearson"}}, {"_id": "7jCvjgw5dbu8ioDMD", "title": "A book review for \u201cAnimal Weapons\u201d and cross-applying the lessons to x-risk.", "postedAt": "2023-05-30T08:24:23.001Z", "htmlBody": "<p>\"Animal Weapons\" by Douglas Emlen is a fascinating exploration of the evolution of animal weapons and how they compare to human weapons. Emlen draws parallels between the historical processes surrounding the evolution of animal and human weapons, including the environments in which they function, the forces of selection shaping their performance, and the ways in which they change through time.</p><p>One of the key takeaways from the book is the idea that the circumstances conducive to extreme weapons - the ingredients triggering an arms race - are essentially the same for animals and humans. This is an important lesson to apply to existential risks, as it suggests that the conditions that lead to the development of dangerous technologies are not unique to humans.</p><p>Emlen also explores the differences between cultural and biological evolution, highlighting the fact that cultural information can be transmitted more widely and rapidly than DNA. This has important implications for the development of existential risks, as it suggests that dangerous technologies can spread quickly and easily across borders and between different groups.</p><p>Overall, \"Animal Weapons\" is a thought-provoking and informative book that offers valuable insights into the evolution of weapons and the lessons we can learn from the natural world. It is a must-read for anyone interested in the intersection of biology and technology, and for those concerned about the risks posed by emerging technologies.</p><p>&nbsp;</p><p>Kindly find the bounty link associated with this book review below:</p><p>&nbsp;https://www.super-linear.org/prize?recordId=rec4NXuZnV89qFkln</p>", "user": {"username": "Habeeb Abdul"}}, {"_id": "q7dJz9ZaZGTSZL8Jk", "title": "Obstacles to the Implementation of Indoor Air Quality Improvements", "postedAt": "2023-05-29T19:15:26.297Z", "htmlBody": "<h2>1. Tl;dr</h2><p>Many reports indicate that&nbsp;<a href=\"https://drive.google.com/file/d/1QKusCnLzUs041nIuiq6nMxSIRcXaGiKq/view?usp=sharing\"><u>indoor</u></a>&nbsp;<a href=\"https://centerforhealthsecurity.org/our-work/research-projects/indoor-air-quality\"><u>air</u></a>&nbsp;<a href=\"https://www.hsph.harvard.edu/healthybuildings/work/healthy-schools/\"><u>quality</u></a> (IAQ) interventions are likely to be effective at reducing respiratory disease transmission. However, to date there\u2019s been very little focus on the workforce that will implement these interventions. I suggest that the US Heating, Ventilation and Air Conditioning (HVAC) and building maintenance workforces have already posed a significant obstacle to these interventions, and broad uptake of IAQ measures will be significantly hindered by them in the future. The impact will vary in predictable ways depending on the nature of the intervention and its implementation. We should favor simple techniques with improved oversight and outsource or crosscheck technically complex work to people outside of the current HVAC workforce. We should also make IAQ conditions and devices as transparent as possible to both experts and building occupants.</p><p>To skip my bio and the technical horrors section, proceed to the recommendations in section 4.</p><h2>2. Who am I? Why do I think This? How Certain am I?&nbsp;</h2><p>I began working in construction in 1991. I did a formal carpentry apprenticeship in Victoria BC in the mid-90s and moved to the US in \u201899. Around 2008 I started taking greater interest in HVAC because - despite paying top dollar to local subcontractors - our projects had persistent HVAC problems. Despite protestations that they were following exemplary practices, our projects were plagued with high humidity, loud noise, frequent mechanical failure, and room-to-room temperature differences. This led me to first learn all aspects of system design and controls, and culminated in full system installations. Along the way I obtained a NJ Master HVAC license, performed the thermal work of ~2k light-duty energy retrofits, obtained multiple certifications in HVAC and low-energy design, and became a regional expert in building diagnostics. Since 2010 I\u2019ve worked as a contractor or consultant to roughly a dozen major HVAC contractors and hundreds of homeowners.</p><p>I\u2019m reasonably certain that the baseline competence of the HVAC workforce is insufficient to broadly and reliably deploy IAQ interventions and that this is a serious obstacle. My comments are specific to the US. I\u2019ve discussed these problems extensively with friends and acquaintances working at a national level and in other parts of the US and believe them to be common to most of the country. The problems are specific to the light commercial and residential workforce, but not domains that are closely monitored by mechanical engineering teams (e.g. hospitals). Based on some limited experience I suspect these problems are also common to Canada, but I\u2019m less certain about their severity.</p><h2>3. Technical Horrors: Why is This so Difficult?</h2><p>Within HVAC, many important jobs are currently either not performed or delegated to people who are largely incapable of performing them. Many people convincingly lie about their capacity to perform a job they\u2019re incapable of, report having done things they haven\u2019t, or even make statements at odds with physics.</p><p>Examples include:</p><ul><li>Accurate heat load/loss calculations: These are used to size heating and cooling systems, and in most areas are code mandated for both new and replacement systems. Competent sizing (Manual J for residential) is viewed as highly important by virtually all experts within HVAC. However, despite decades of investment in training and compliance, a lead technical manager of a clean energy program reported to me that &gt;90% of Manual Js reviewed by his program had significant errors made apparent due to internal inconsistency (eg duct load on a hydronic system) or obvious inconsistencies with public information on zillow or google maps. In an especially egregious example, one of the largest HVAC companies in the state had its Manual J submission admin go on vacation. The temporary replacement forgot to rename files and submitted applications named for their installed capacity (1 ton, 2 ton, 3 ton, etc.), revealing that the company had submitted copies of the same handful of designs for thousands of homes.</li><li>Setting and correcting airflow: In systems equipped with a furnace and air conditioner this is especially common. The furnace fan motor moves air for both heating and cooling. However, most furnace fans are capable of moving a range of airflow for various capacities of air conditioning (a 5 ton furnace fan may have 3-5 tons of airflow capacity), and therefore must be programmed. The great majority of these are not programmed, leaving the fan in a default setting, which is usually its highest nominal airflow. In most cases this is too high. The net effect is a significant loss of dehumidification in a sizable number of homes and excessive system noise. Almost all of my \u201cvery humid home\u201d diagnostic calls have this condition, often with multiple HVAC techs failing to detect the problem prior to my visit in spite of obvious symptoms (little to no condensate being produced by the AC system).</li><li>During the early phases of the Covid-19 pandemic, public health officials asked school managers across the country to upgrade their central ducted HVAC system filters to MERV 13 and set the fans to continuous circulation. However, a sizable number of building managers reported that these systems, \u201ccouldn\u2019t handle the pressure of a filter upgrade.\u201d This statement was largely taken at face value by public health officials and parents, despite the great majority of cases being 1) corroborating information via equipment manuals not being provided in any case I\u2019m aware of, and 2) not based on (total external static) pressure (TESP) readings taken from equipment in any case I\u2019m aware of. In the vast majority cases the people making this claim didn\u2019t even possess the tools or knowledge to measure TESP. In evaluating this claim in person and remotely several times, I have yet to find a claim backed by data, suggesting the default position should be that the claim is false.</li><li>In the mid-90s John Proctor created&nbsp;<a href=\"https://www.aceee.org/files/proceedings/2002/data/papers/SS02_Panel1_Paper05.pdf\"><u>CheckMe</u></a> as a tool to evaluate the rampant false claims of technicians charging refrigerant. Data collected by Proctor on 8,873 systems suggested that 65% were in need of repair adjustment to charge. These repairs largely wouldn\u2019t have been discovered in the absence of the CheckMe tool.&nbsp;</li><li>Electrification in response to global warming has rightly gained significant policy traction. However, electrification will necessitate widespread heat pump adoption in cold climates, most of which currently require the field installation and verification of refrigerant charge. The most common residential equipment refrigerant today is R410a, which has a somewhat high global warming potential (GWP 2088). Procedures for the installation of \u201ctight &amp; dry\u201d systems are contained in equipment manuals and broadly disseminated by industry field guides &amp; standards. In principle, adhering to these installation standards makes near-term failure exceedingly rare. However, despite widespread access to these standards many of the gains of electrification are being&nbsp;<a href=\"https://www.seattle.gov/documents/Departments/OSE/Building%20Energy/SEA_Refrigerant_Analysis_May2020.pdf\"><u>clawed back</u></a> by unexpectedly high refrigerant leak rates, primarily due to a systematic failure of the HVAC labor force to follow best practices.</li></ul><p>HVAC competence is a strong determinant of IAQ intervention success. Surprisingly, public health experts have largely avoided examining the labor force presumably tasked with performing most of their proposed interventions. In part I suspect this is due to differences in their respective cultures, tasks performed within them, and some intrinsic differences between white and blue collar work.</p><p>Public health work is similar to most knowledge work wherein \u2018doing\u2019 is physically minimally distinct from \u2018researching\u2019, \u2018proposing\u2019 or \u2018planning\u2019. With physical work, talking or writing is much easier than doing. In addition, white collar work is often characterized by higher levels of transparency, and dishonesty is frowned upon and frequently held to account. None of these are true of HVAC work, where honesty isn\u2019t a strong cultural norm, and many tasks lack sufficient transparency to allow for its policing.</p><h2>4. Recommendations</h2><p>For many years, poor competence has resulted in suboptimal outcomes for the occupants of many buildings. For example, at the beginning of the Covid-19 pandemic many teachers had endured years of seemingly intractable comfort problems despite many failed repair attempts. Once the pandemic struck, these teachers understood that the ventilation and filtration provided by the same technicians had inherently lower verifiability than previous HVAC work. Naturally they were hesitant to entrust occupant health to the same people who had frequently failed them. Therefore, the success of future IAQ interventions significantly depends on providing clear and verifiable information to building occupants.</p><h3>Interventions That are Likely to Fail</h3><p>Complexity and opacity strongly predict failure. Tasks involving direct airflow measurement or its longstanding proxies (temperature and pressure) are especially problematic, as these aren\u2019t well understood. Most technicians won\u2019t have the knowledge or tools to quantify airflow. Keep in mind that technicians and managers routinely lie, so asking whether an individual or company can perform a task has the potential to select for lying instead of competence.</p><p>Suppose we wanted to invent a seemingly viable yet pragmatically horrible device that exploited the worst industry traits. We\u2019d probably create something like in-duct UV that\u2019s dependent on a narrow range of airflow velocity, installed in a system capable of providing too much/too little airflow for the device to be effective. We\u2019d add the possibility that the device could be installed in multiple configurations (<a href=\"https://www.wxxinews.org/local-news/2022-04-14/geneseo-faculty-say-uv-c-light-to-target-covid-damaged-their-eyes\"><u>backwards or upside-down</u></a>) or multiple locations (supply or return plenums or trunks) with performance and safety impacts from each. Finally, we\u2019d add the condition that the device wouldn\u2019t convey its sub-optimal performance to installers or end-users. While there may be some other conditions we could add to make this worse, we\u2019ve probably created a device that\u2019s virtually guaranteed not to work effectively, while simultaneously providing naive building occupants with the illusion of increased safety. Interestingly, current public health understanding of HVAC industry capacity would find little fault with this device provided its installation in optimal conditions produced favorable results.</p><h3>Interventions That are Likely to Succeed</h3><p>The ideal intervention requires minimal technical sophistication, and its proper functioning is both comprehensible and visible, ideally even to occupants. Where necessary, expert planning should be performed by people with minimal connections to the HVAC industry. Many current interventions are broadly promising, but leave significant room for improvement. Given the limitations proposed herein, I suggest the following:</p><ul><li>Displayed&nbsp;<a href=\"https://pubs.acs.org/doi/10.1021/acs.estlett.1c00183\"><u>CO2</u></a> as&nbsp;<a href=\"https://www.researchgate.net/publication/271732381_Is_CO2_a_good_proxy_for_indoor_air_quality_in_classrooms_Part_1_The_interrelationships_between_thermal_conditions_CO2_levels_ventilation_rates_and_selected_indoor_pollutants\"><u>proxy</u></a>: Carbon dioxide is a good proxy for indoor air quality generally, and respiratory virus transmission risk specifically. While handheld CO2 monitors have become somewhat widespread, to date visible public displays have gained very little traction. In part this is due to the complexity of clear targets for rooms in which mixed ventilation and filtration are both present. However, this could probably be addressed via multiple targets in rooms with standalone filters with known Clean Air Delivery Rate (CADR).</li><li>Displayed Clean Air Delivery Rate (CADR) on visible, standalone filters. Standalone filters are superior to central ones because laypeople aren\u2019t readily able to verify central system filtration rates. However, with improvements to the display of CADR on freestanding filters, it would be fairly simple for a sophisticated layperson to roughly calculate the filtered air exchanges in a given room.&nbsp;</li><li>Adopt standards for UV air exchanges and display them: UV is a highly promising intervention for reducing disease transmission. We\u2019ve already established that in-duct UV has a strong potential for failure. However, unlike filtration and ventilation, UV currently lacks an agreed upon standard of work rate. Consequently, current freestanding versions of the technology lack visible displays showing effective operation.&nbsp;</li><li>Improve the transparency of necessary HVAC work: Where HVAC service or installation is required, transparency should be the default position. Technicians should geotag pics of measured performance and diagnostics. Claims regarding equipment parameters and limitations should include geotagged photos of nameplates and highlighted sections from installation manuals (most manuals are accessible via web search currently).</li><li>Improve transparency of ventilation calculations: Most ventilation calculations (including ASHRAE 62.1 &amp; 62.2) are moderately simple to perform and unconnected to the routine demands of HVAC work. Our default position should be to have these calculations at minimum available, - and ideally cross-checked - by interested laypeople.</li></ul><p>The HVAC and building maintenance workforces are significant obstacles to widespread IAQ improvements. However, to accept the limitations I\u2019ve outlined isn\u2019t to accept failure. Historic failures provide us with a blueprint for success provided we use them to guide our future actions.</p>", "user": {"username": "JesseSmith"}}, {"_id": "KAhETmcWuH75sxaTa", "title": "Reflections following EAG London: conference strategy and in/outgroup dynamics", "postedAt": "2023-05-29T17:59:31.643Z", "htmlBody": "<p><i>This was originally posted on my shortform, thanks to David Nash for suggesting it was worthwhile making a full post.</i></p><h3>TL;DR</h3><ul><li>I'd recommend having a strategy for planning your conference</li><li>Different types of 1:1s are valuable in different ways, and some are more worth preparing for than others</li><li>It's natural to feel imposter syndrome and a sense of inadequacy when surrounded by so many highly competent, accomplished people, but arguably the primary purpose of the conference is for those people to help us mere mortals become highly competent and accomplished too (assuming accomplishment = impact)</li><li>I feel very conflicted about the insider/outsider nature of the EA community, and I'm going to keep thinking and talking about it more</li></ul><p>Last year I attended EAG SF (but not EAG London 2022), and was newer to the EA community, as well as working for a less prestigious organisation. This context is probably important for many of these reflections.</p><h3>Conference strategy</h3><p>I made some improvements to my conference planning strategy this year, that I think made the experience significantly better:</p><ul><li>I looked through the attendee list and sent out meeting invitations as soon as Swapcard was available. This way people had more slots free, both increasing their likelihood of accepting my meeting, and increasing the chances they'll accept for the specific time I've requested. This let me have more control over my schedule.</li><li>I left half an hour's break in between my 1:1s. This allowed meetings to go on longer if both participants wanted that, as well as gave me some time to process information, write down notes, and recharge.</li><li>I initially didn't schedule many meetings on Sunday. This meant that if anybody I talked to on Friday or Saturday suggested I talk to someone else at the conference, I'd have the best chance at being able to arrange a meeting with them on Sunday. This strategy worked really well, as I had a couple of particularly valuable 1:1s on Sunday with people who hadn't originally been on my radar to talk to.</li></ul><p>I had a clearer sense of what I was trying to achieve out of the conference this year compared to last. This made it easier to decide who would be valuable to speak to. Everyone who I requested a meeting with accepted, including someone who I regarded as particularly impressive who had specified they weren't going to take many 1:1s - so have a low bar for requesting meetings! With this person in particular I felt a bit starstruck, and regretted not having spent more time preparing specific questions to ask.&nbsp;</p><p>Some other meetings were totally fine without preparation though, so I think it's worth considering which ones would be more valuable to prepare for - in my case these were ones with more accomplished folks, or with people who I'm interested in working/collaborating with in the near future.&nbsp;</p><p>I broadly had two kinds of 1:1s, both of which were valuable: meetings with people who had a track record in areas that I am considering pursuing, and meetings with people who are in a similar position to me currently. With the former, I had specific questions that I was trying to answer, and was potentially trying to impress them/gauge if they might hire me. With the latter, meetings were more exploratory, more casual, and more about trying to find out if I was missing anything important from my model. I think it's easy to feel like the former is far more valuable than the latter, but I think this is false, and I think scheduling some of these more relaxed meetings can help ease the stress of the conference (as well as provide valuable insight into your current situation and plans).</p><h3>I've infiltrated the ingroup</h3><p>Last year, I felt like I had something to prove in all of my meetings. I knew maybe 4-5 people who were at the conference, which is not a lot out of almost 1500, and I worked for a company that nobody in SF has heard of. I was still a prototypical EA (straight white male) in many ways, but I don't have an undergrad, and felt like I lacked any track record of achievements to show that I was competent (and by extension, worthy of other attendees' time).</p><p>This year, I had several close friends attending and had interacted with a much larger number of people in some capacity, either on social media or through EA tech events in London. I now work for a FAANG company. I have read the seminal Slate Star Codex posts, I broadly understand what a deceptively-aligned mesa-optimiser is, and I've speculated as to the true identity of Qualy the Lightbulb on Twitter. I have a legible track record of my competence, and I am fluent in EA jargon. I have infiltrated the ingroup.</p><p>I feel very conflicted about this - being part of the ingroup feels great. I feel like I have the respect of people who I view as being extremely talented and successful, and naturally this does wonders for my ego. Having so many common touchpoints has meant that I find it extremely easy to make meaningful connections within the EA community compared with outside. Using jargon to signal that your familiar with the relevant scriptures and ideas lets you skip straight to discussing cruxes, on the understanding that both parties already agree on some number of issues. Other group norms, such as a preference for openness, directness, and epistemic humility also seem better than their alternatives to me - I think these tendencies facilitate more constructive discussion, ultimately (hopefully) leading to greater impact.</p><p>But there are many obvious drawbacks to this. The ingroup is primarily made up of exceptionally privileged people (and in some ways I'm glad of this - I want privileged people like myself to be doing more to think about how they can do good with their privilege), and often the people that feel less comfortable around the EA community are less privileged. Other people have articulated these problems with the community better than me, and I don't want to speak for them so won't go into too much detail, but if you're reading this then you probably already know exactly what I mean. The fact that the EA community makes some people feel this way makes me feel really sad, which is hard to square with the happiness I get from the sense of belonging that I personally feel from the community.</p><p>I'm not entirely sure what to do about this. Trying to use less jargon seems like a good start, but that would be a costly signal for me, particularly when I feel like I am starting to gain status within the community. I would love to write that I am happy to sacrifice this, but I am human and flawed, and I don't know if I am. Making a conscious effort to treat people the same, regardless of whether I view them as highly accomplished or not, also seems like something I ought to write here, but seems hard (or even impossible) to do in practice.</p><p>80k has the idea of spending the initial part of your career acquiring career capital, which can later be traded in for impact. Perhaps EA ought to have the idea of <i>community capital</i> - a greater focus is placed on nurturing and growing a healthy community focused on doing good. Even if this focus detracts from doing the absolute most good possible in the short term, in the long term it could allow for greater impact (and, after all, we do love talking about the long term round here).</p><p>I feel like my thinking on these topics is generally pretty immature and lacks nuance, so I'd welcome any thoughts.</p><h3>Outcomes</h3><p>To end on a positive note, I found the conference incredibly valuable. I came away feeling like I had three promising paths to explore, and am now planning on trying to move to direct work immediately, rather than gaining more career capital as originally planned. I feel excited about trying to use my career to do good, and I feel excited about EA as a whole. There are jobs I will apply for that I counterfactually wouldn't have, seemingly promising opportunities for collaboration, and I made several new contacts that I think will be mutually beneficial in the future.</p><p>The conference itself seemed excellently run, the venue was amazing, as was the food, and I feel very appreciative for both the organisers and event staff for all their time and effort!</p>", "user": {"username": "Jonny Spicer"}}, {"_id": "2TdXocyDF9PxWewwY", "title": "Should the EA community be cause-first or member-first? ", "postedAt": "2023-05-29T15:50:49.564Z", "htmlBody": "<p>It's really hard to do community building well. Opinions on strategy and vision vary a lot, and we don't yet know enough about what actually works and how well. Here, I'll suggest one axis of community-building strategy which helped me clarify and compare some contrasting opinions.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefezenhu0tha\"><sup><a href=\"#fnezenhu0tha\">[1]</a></sup></span>&nbsp;</p><h1>Cause-first</h1><p>Will Macaskill's proposed <a href=\"https://forum.effectivealtruism.org/posts/9wYa8BqSTMcx9j2tK/defining-effective-altruism\">Definition of Effective Altruism</a> is composed of<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy26j6zbkrrq\"><sup><a href=\"#fny26j6zbkrrq\">[2]</a></sup></span>:</p><ol><li>An overarching effort to figure out what are the best opportunities to do good.&nbsp;</li><li>A community of people that work to bring more resources to these opportunities, or work on these directly.</li></ol><p>This suggests a \"cause-first\" community-building strategy, where the main goal for community builders is to get more manpower into the top cause areas. Communities are measured by the total impact produced directly through the people they engage with. Communities try to find the most promising people, persuade them to work on top causes, and empower them to do so well.&nbsp;</p><p>CEA's <a href=\"https://www.effectivealtruism.org/articles/introduction-to-effective-altruism\">definition</a> and <a href=\"https://www.centreforeffectivealtruism.org/strategy\">strategy</a> seem to be mostly along these lines:</p><blockquote><p>Effective altruism is a project that aims to find the best ways to help others, and put them into practice.</p><p>It\u2019s both a <strong>research field</strong>, which aims to identify the world\u2019s most pressing problems and the best solutions to them, and a <strong>practical community</strong> that aims to use those findings to do good.</p></blockquote><p>and&nbsp;</p><blockquote><p>Our mission is to nurture a community of people who are thinking carefully about the world\u2019s biggest problems and taking impactful action to solve them.</p></blockquote><h1>Member-first</h1><p>Let's try out a different definition for the EA community, taken from <a href=\"https://www.centreforeffectivealtruism.org/ceas-guiding-principles\">CEA's guiding principles</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsdew26d71s\"><sup><a href=\"#fnsdew26d71s\">[3]</a></sup></span>:</p><blockquote><h3>What is the effective altruism community?</h3><p>The effective altruism community is a global community of people who care deeply about the world, make helping others a significant part of their lives, and use evidence and reason to figure out how best to do so.</p></blockquote><p>This, to me, suggests a subtly different vision and strategy for the community. One that is, first of all, focused on these people who live by EA principles. Such a \"member-first\" strategy could have a supporting infrastructure that is focused on helping the individuals involved to live their lives according to these principles, and an outreach/growth ecosystem that works to make the principles of EA more universal<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0d6jckkpwbtk\"><sup><a href=\"#fn0d6jckkpwbtk\">[4]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefk6m0njil80q\"><sup><a href=\"#fnk6m0njil80q\">[5]</a></sup></span>.</p><h1>What's the difference?</h1><p>I think this dimension has important effects on the value of the community, and that both local and global community-building strategies should be aware of the tradeoffs between the two.&nbsp;</p><p>I'll list some examples and caricatures for the distinction between the two, to give a more intuitive grasp of how these strategies differ, without any clear order:&nbsp;</p><figure class=\"table\"><table style=\"border-color:hsl(0, 0%, 30%);border-style:solid\"><thead><tr><th><strong>Leaning cause-first</strong></th><th><strong>Leaning member-first</strong></th></tr></thead><tbody><tr><td>Keep EA Small and Weird</td><td>Big Tent EA</td></tr><tr><td><a href=\"https://forum.effectivealtruism.org/handbook\">Current EA Handbook</a> (focus on introducing major causes)</td><td><a href=\"https://www.stafforini.com/docs/Carey%20-%20The%20effective%20altruism%20handbook.pdf\">2015's EA Handbook</a> (focus on core EA principles)</td></tr><tr><td>80,000 Hours</td><td>Probably Good</td></tr><tr><td>Wants more people doing high-quality AI Safety work, regardless of their acceptance of EA principles</td><td>Wants more people deeply understanding and accepting EA principles, regardless of what they actually work on or donate to.</td></tr><tr><td>Targeted outreach to students in high ranking universities</td><td>Broad outreach with diverse messaging</td></tr><tr><td>Encourages people to change occupations to focus on the world's most pressing problems</td><td>Encourages people to use the tools and principles of EA to do more good in their current trajectory</td></tr><tr><td>Risk of people not finding useful ways to contribute to top causes</td><td>Risk of not enough people who want to contribute to the world's top causes</td></tr><tr><td>The community as a whole leads by example, by taking in-depth prioritization research with the proper seriousness</td><td>Each individual is focused more on how to implement EA principles in their own lives, taking their personal worldview and situation into account&nbsp;</td></tr><tr><td>Community members delegate to high-quality research, think less for themselves but more people end up working in higher-impact causes</td><td>Community members think for themselves, which improves their ability to do more good, but they make more mistakes</td></tr><tr><td><a href=\"https://forum.effectivealtruism.org/posts/MSYhEatxkEfg46j3D/the-case-of-the-missing-cause-prioritisation-research\">The case of the missing cause prioritization research</a>, <a href=\"https://forum.effectivealtruism.org/posts/5LNxeWFdoynvgZeik/nobody-s-on-the-ball-on-agi-alignment\">Nobody\u2019s on the ball on AGI alignment</a>, and many amazing object-level posts making progress on particular causes</td><td><a href=\"https://forum.effectivealtruism.org/posts/3voXaqvPutSrHvuCT/the-case-against-ea-cause-areas\">The case against \u201cEA cause areas\u201d</a> , <a href=\"https://forum.effectivealtruism.org/posts/MP9qDZCXMaTJhiJ9u/ea-is-three-radical-ideas-i-want-to-protect\">EA is three radical ideas I want to protect</a>, <a href=\"https://forum.effectivealtruism.org/posts/SjK9mzSkWQttykKu6/big-tent-effective-altruism-is-very-important-particularly\">\"Big tent\" effective altruism is very important (particularly right now)</a>, and many posts where people share their own decisions and dilemmas &nbsp;</td></tr><tr><td>...</td><td>...</td></tr></tbody></table></figure><h1>Personal takeaways</h1><p>I think the EA community is leaning toward \"cause-first\" as the main overarching strategy. That could be the correct call. For example, I guess that a lot of the success of EA in promoting highly neglected causes<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2er946n991s\"><sup><a href=\"#fn2er946n991s\">[6]</a></sup></span>&nbsp;was due to community-builders and community-focused organizations having a large focus on spreading the relevant ideas to many promising people and helping them to work on these areas.</p><p>However, there are important downsides to the \"cause-first\" approach, such as a possible lock-in of main causes and less diversification in the community. Many problems with the EA community are possibly explained by this decision.&nbsp;</p><p>It <i>is </i>a decision. For example, EA Israel, particularly as led by <a href=\"https://forum.effectivealtruism.org/users/gidikadosh?mention=user\">@GidiKadosh</a>, has focused more on the \"member-first\" approach. This also has downsides. Say, only in the past year or so we really started having a network of people working in AI Safety, and we are very weak on the animal welfare front.</p><p>I'm not sure what is the best approach, and very likely we can have the best of both worlds most of the time. However, I am pretty sure that being more mindful of this particular dimension in community building is important, and I hope that this post would be a helpful small step in understanding how to do community building better.&nbsp;</p><p><i>Thanks to the many people I've met at EAG and discussed this topic with! I think that crystalizing this idea was one of the key outcomes of the conference for me.&nbsp;</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnezenhu0tha\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefezenhu0tha\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I try to make the two main examples a bit extreme, to make the distinction clearer, but most opinions are somehow a mesh of the two.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fny26j6zbkrrq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefy26j6zbkrrq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I've taken some liberty with paraphrasing the original definition to make my claims clearer. This example doesn't mean that Will Macaskill is a proponent of such a \"cause-first\" strategy.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsdew26d71s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsdew26d71s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>These haven't been updated much since 2018, so I'm not sure how representative they are. Anyway, again, I'm using this definition to articulate a possible strategy.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0d6jckkpwbtk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0d6jckkpwbtk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>By this, I mean a future where principles very close to the current main \"tenets\" of EA are widespread and commonsense.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnk6m0njil80q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefk6m0njil80q\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Maybe the focus on \"making the principles of EA more universal\" is more important than the focus on the community and this section should be called something like \"ideas-first\". I think now that these two notions should be distinguished, and represent different goals and strategies, but I'll leave this to other people (maybe future Edo) to articulate this clearly if this post would be useful.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2er946n991s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2er946n991s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Say, x-risks, wild-animal suffering, and empirically-supported GH&amp;D interventions.</p></div></li></ol>", "user": {"username": "edoarad"}}, {"_id": "wY6aBzcXtSprmDhFN", "title": "Exponential AI takeoff is a myth", "postedAt": "2023-05-31T11:47:33.167Z", "htmlBody": "<h3>TL;DR</h3><p>Everything that looks like exponential growth eventually runs into limits and slows down. AI will quite soon run into limits of compute, algorithms, data, scientific progress, and predictability of our world. This reduces the perceived risk posed by AI and gives us more time to adapt.</p><h3>Disclaimer</h3><p><i>Although I have a PhD in Computational Neuroscience, my experience with AI alignment is quite low. I haven\u2019t engaged in the field much except for reading Superintelligence and listening to the 80k Hours podcast. Therefore, I may duplicate or overlook arguments obvious to the field or use the wrong terminology.</i></p><h3>Introduction</h3><p>Many arguments I have heard around the risks from AI go a bit like this: We will build an AI that will be as smart as humans, then that AI will be able to improve itself. The slightly better AI will again improve itself in a dangerous feedback loop and exponential growth will ultimately create an AI superintelligence that has a high risk of killing us all.</p><p>While I do recognize the other possible dangers of AI, such as engineering pathogens, manipulating media, or replacing human relationships, I will focus on that dangerous feedback loop, or \u201cexponential AI takeoff\u201d. There are, of course, also risks from human-level-or-slightly-smarter systems, but I believe that the much larger, much less controllable risk would come from \u201csuperintelligent'' systems. I\u2019m arguing here that the probability of creating such systems via an \u201cexponential takeoff\u201d is very low.</p><h3>Nothing grows exponentially indefinitely</h3><p>This might be obvious, but let\u2019s start here: Nothing grows exponentially indefinitely. The textbook example for exponential growth is the growth of bacteria cultures. They grow exponentially until they hit the side of their petri dish, and then it\u2019s over. If they\u2019re not in a lab, they grow exponentially until they hit some other constraint but in the end, all exponential growth is constrained. If you\u2019re lucky, actual growth will look logistic (\u201dS-shaped\u201d), where the growth rate approaches 0 as resources are eaten up. If you\u2019re unlucky, the population implodes.</p><p>For the last decades, we have seen things growing and growing without limit, but we\u2019re slowly seeing a change. Human population is starting to follow an S-curve, the number of scientific papers has been growing fast but is starting to flatten out, and even Silicon Valley has<a href=\"https://www.goodreads.com/book/show/55338968-the-cold-start-problem\">&nbsp;<u>learnt</u></a> that Metcalfe\u2019s Law of exponential network benefits doesn\u2019t work due to the limits imposed by network complexity.</p><p>I am assuming that everybody will agree with the general argument above, but the relevant question is: When will we see the \u201cflattening\u201d of the curve for AI? Yes, eventually growth is limited, but if that limit kicks in once AI has used up all the resources of our universe, that\u2019s a bit too late for us. I believe that the limits will kick in as soon as AI will reach our level of knowledge, give or take a magnitude, and here is why:</p><h3>We\u2019re reaching the limits of Moore\u2019s law</h3><p>First and foremost, the growth of processing power is what enabled the growth of AI. I\u2019m not going to guess when we reach parity with the processing power of the human brain but even if we do, we won\u2019t grow fast beyond that, because Moore\u2019s law is slowing down.</p><p>Although I'm not a theoretical physicist, I believe that there is significant evidence, anecdotal and otherwise, that Moore's Law is reaching its limits. In 2015,&nbsp;the former<a href=\"https://www.wsj.com/articles/BL-DGB-42647\">&nbsp;<u>Intel CEO</u></a> stated that \u201cour cadence today is closer to two and a half years than two.\u201d and&nbsp;<a href=\"https://en.wikipedia.org/wiki/Moore's_law#Recent_trends\"><u>Wikipedia states</u></a> that \u201cthe physical limits to transistor scaling have been reached\u201d and that \u201cMost forecasters, including Gordon Moore, expect Moore's law will end by around 2025.\u201d If we look at the cost of computer memory and storage we see that while it shrunk exponentially for most of the last 50 years, we\u2019re<a href=\"https://ourworldindata.org/grapher/historical-cost-of-computer-memory-and-storage\">&nbsp;<u>reaching the limits</u></a> of that already.</p><p>I think there are two ways to counter this:</p><ol><li>Yes, humans are reaching the limits, but AI will be smarter than us and AI will figure it out.</li><li>It doesn\u2019t matter because we\u2019ll just work with better algorithms and more data.</li></ol><p>I\u2019ll start with the second one:</p><h3>We will probably reach the limits of algorithms</h3><p>If compute is reaching limits, but we can be increasingly efficient with our compute, then we\u2019ll still scale exponentially. OpenAI published an<a href=\"https://openai.com/research/ai-and-efficiency\">&nbsp;<u>article</u></a> in 2020 showing that the compute needed is actually decreasing exponentially due to algorithmic improvements and so far we\u2019re not seeing any leveling.</p><p>I think that these improvements are fair to expect given that early machine learning researchers were usually not professional software engineers focused on efficiency, so there should be a lot of potential when trying to scale these methods. However, it is theoretically quite clear that there will be limits to this as well: You\u2019re unlikely to train a billion parameters with one subtraction operation, so the question is again: when will this taper off? We've seen similar developments for example with sorting algorithms: Here, the largest efficiency gains were found initially, e.g. from BubbleSort (1956) to QuickSort (1959), while new, slightly better algorithms are still developed to this day (e.g. Timsort 2002).</p><p>Either way, both compute and algorithms, even if we make a magical breakthrough in quantum computing tomorrow, are in the end limited by data. DeepMind<a href=\"https://arxiv.org/abs/2203.15556\">&nbsp;<u>showed in 2022</u></a> (see also<a href=\"https://matt-rickard.com/ai-scaling-laws,%5D(https://matt-rickard.com/ai-scaling-laws)\">&nbsp;<u>here</u></a>) that more compute only makes sense if you have more data to feed it. So even if we get exponentially scaling compute and algorithms, that would only give us the current models faster, not better. So what are the limits of data?</p><h3>We\u2019re reaching the limits of training data</h3><p>Intuitively, I think it makes sense that data should be the limiting factor of AI growth. A human with an IQ of 150 growing up in the rainforest will be very good at identifying plants, but won\u2019t all of a sudden discover quantum physics. Similarly, an AI trained on only images of trees, even with compute 100 times more than we have now, will not be able to make progress in quantum physics.</p><p>(This is where we start to get less quantitative and more hand-wavy but stay with me.) I think it\u2019s fair to assume that a large part of human knowledge is stored in books and on the internet. We are already using most of this to train AIs. OpenAI didn\u2019t publish what data they are using to train their models but let\u2019s say it\u2019s 10% of all of the internet and books. Since AI models need<a href=\"https://arxiv.org/abs/2001.08361\">&nbsp;<u>exponentially growing training data</u></a> to get linear performance improvements, that would mean that we can only expect relatively small improvements by feeding it the remaining 90% of the internet, which isn\u2019t exactly exponential takeoff.</p><p>So let\u2019s say we already use all the internet and books as training data. What else could we do? One extreme option would be to strap a camera and a microphone (similar to Google Glass)&nbsp;to every human, record everything, and feed all of the data into a neural network. Even if we ignore the time it takes to record this data (more on this in the next paragraph), I would argue that the additional information in there is not of the same quality of books and the internet. Language is an information compression tool. We condensed everything we learnt as a human species over the last centuries in books. The additional knowledge gain from following us around would be marginal - maybe the AI would get a bit better at gossiping, maybe it would get scientific discoveries a year earlier than they are published, or understand human emotions better, but in the end, there is not much to be seen there if the AI has already been exposed to all of our written knowledge.</p><p>But even if we reach the limits of training data, can\u2019t AI just generate more data?</p><h3>There are natural limits to the growth of knowledge</h3><p>\u201cAI will improve itself\u201d, \u201cAI will spiral out of control\u201d, \u201cAI will enter a positive feedback loop of learning\u201d - these claims all assume that just through reasoning, AI will be able to get better and better, going around all the limitations we looked at so far. We already understood that even if AI could come up with a better training algorithm that would help only marginally, what it would have to do would be to generate novel data / knowledge on a large scale.</p><p>I\u2019d argue that if it would be that easy, science wouldn\u2019t be that hard. There is a reason why we have separate fields of experimental and theoretical physics. A lot of things work in theory, until they are tested in the real world. And that testing is getting more and more cumbersome: While the number of scientific papers has been growing exponentially, in many fields the number of breakthrough discoveries has actually been shrinking exponentially. In Pharma there even is the famous Eroom\u2019s law (Moore spelled backwards) that drug discovery is getting exponentially more difficult. Since the Scientific Revolution, we have picked all the \u201clow-hanging fruit\u201d and it\u2019s getting increasingly difficult to \u201cgenerate more data\u201d in the sense of generating knowledge.</p><p>I\u2019m sure AI will be able to generate a lot of very good hypotheses by taking in all the current human knowledge, combining it, and advancing science that way, but testing these hypotheses in the real world is a manual process that takes a lot of time. It\u2019s not something that can explode overnight, and judging by the recent struggles of science we\u2019re reaching limits that AI will probably face sooner rather than later.</p><p>But AI doesn\u2019t have to act in the real world, and collect real data. Can\u2019t it just improve in a simulation, just like the Go AI and Chess AI and Starcraft AI played against themselves in simulations to improve?</p><h3>We can\u2019t simulate knowledge acquisition</h3><p>We can\u2019t simulate our world. If we could, we could generate infinite data but the data that we can simulate is only as good as the assumptions we put into the simulation so it\u2019s again limited by current human knowledge.</p><p>Yes, we are using simulations right now to train self-driving cars, and they'll probably eventually get better than humans, but they are limited by the assumptions we put in the simulation. They won\u2019t be able to anticipate situations that we didn\u2019t think of.</p><p>The great thing about Go, Chess, and Starcraft is that all of these can be easily simulated and thereby allow AIs to generate knowledge across millions of iterations. The world they are tested in is the same simulated world they are trained in, so this works. For anybody who has ever tried to make a robot work in real life that has been trained in a simulation knows that unfortunately, that doesn\u2019t easily translate. Simulations are inherently limited by the assumptions we put into them. There is a reason why AIs that live in a purely theoretical space (such as language models or video game AIs) have had amazing breakthroughs, while robots still struggle with grabbing arbitrary objects. As an example of this, just compare the recent video of DeepMind\u2019s<a href=\"https://www.theregister.com/2023/05/09/ai_robot_soccer/\">&nbsp;<u>robot soccer players</u></a> falling over and over again with their impressive advances in<a href=\"https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning\">&nbsp;<u>StarCraft</u></a>.</p><p>Another way to get around the time it takes to generate novel data would be to massively parallelize it: An AI could make infinite copies of itself and if every copy learns something and pools that knowledge, then that would result in exponential scaling. A chatbot with access to the internet could learn exponentially just by making exponential copies of itself. However, this will need a lot of resources and will still be bound by the time it takes the AIs to perform individual actions or measurments in the real world. While this can speed up AI development, it will still be slow compared to the feedback loops most people think of when thinking of AI progress. Google just closed down yet another of their robot experiments (Everyday Robots) that<a href=\"https://everydayrobots.com/thinking/combining-rl-il-at-scale\">&nbsp;<u>used this</u></a> as part of their strategy for learning.</p><h3>There are natural limits to the predictability of the world</h3><p>But what if we actually don\u2019t need more data? What if all the knowledge we already have as humans, combined in one artificial brain, and with a misguided value system, is enough to outwit our species?</p><p>Let me make the most hand-wavy argument so far here: The world is a random, complex, system. We can\u2019t predict the weather more than three days in advance, let alone what Trump will tweet tomorrow. There is no reason to believe that an AI 1000x smarter than us would be able to do this because in complex systems, small changes in state can have a massive effect on its outcome. We don\u2019t know the full state and&nbsp; a 1000x smarter AI also won\u2019t know the full state due to the difficulty of acquiring knowledge from the real world, as discussed above. \u201cNo plan survives contact with the enemy\u201d; that\u2019s because it is impossible, no matter how much compute you have, to predict the enemy. An AI can probably make better guesses than we can, and come up with more alternative plans than we can, but it cannot combat the combinatorial explosion. It has to work with best-guess estimates. These will very quickly lose value in the same way as our best guess estimate of the weather loses value a few days into the future.</p><p>So even if, due to some flaw of the above arguments, AI would actually be able to scale exponentially in intelligence, I believe that the application of this intelligence would very quickly run into the limits imposed by the unpredictability of our world, leading again to a logistic growth of power of that AI, and not to an exponential growth.</p><h3><strong>AI will be very useful and maybe even smarter than us, but it won\u2019t overpower us overnight</strong></h3><p>I have argued that AI will grow logistically, not exponentially, and that we will see the move to logistical growth quite soon as we approach the current limits of human knowledge. Tricks like simulation won\u2019t get us much further and even if they did, the power of that AI would still only grow logistically due to the limits imposed by the unpredictability of our world.</p><p>I have looked at five different constraints on the growth of AI: compute, algorithms, data, scientific progress, and predictability of our world. There are probably other constraints that I didn\u2019t consider that could also have a limiting effect on the exponential growth of AI. Claiming that AI will grow exponentially is claiming that there will be NO constraints, which is a much stronger claim than saying that there will be A constraint, because one is enough to limit it from growing exponentially.</p><p>If we accept this line of reasoning, then this means that AI has probably an upper limit of a very very intelligent human being who somehow manages to keep all of human knowledge in their head. That\u2019s quite impressive, but it\u2019s not the same as an exponentially growing AI. It\u2019s something we should be very careful with, but not avoid at all costs. I think it\u2019s reasonable to assume that we\u2019ll not approach this exponentially but logistically, with the last steps taking much more time than the first ones, which we are witnessing now. We will need to change our laws, adapt our intuitions, regulate the use of AI, and maybe even treat AIs as citizens, but it\u2019s not something that can kill us within a day of reaching superhuman knowledge.</p><p>With this in mind, we can focus some of our attention on monitoring AI and working to integrate it into today\u2019s world, while also not losing sight of all of the other issues we are facing.</p>", "user": {"username": "Christoph Hartmann"}}, {"_id": "5hegHvhrBZaBTWwxs", "title": "Governments Might Prefer Bringing Resources Back to the Solar System Rather than Space Settlement in Order to Maintain Control, Given that Governing Interstellar Settlements Looks Almost Impossible ", "postedAt": "2023-05-29T11:16:57.390Z", "htmlBody": "<p><br>&nbsp;</p><p><strong>Part of my work for&nbsp;</strong><i><strong>Arb Research&nbsp;</strong></i><strong>(</strong><a href=\"https://arbresearch.com/\"><strong><u>https://arbresearch.com/</u></strong></a><strong>).&nbsp;</strong></p><p><br><br>&nbsp;</p><p><i>Epistemic Status</i>: I have no scientific background and wrote this after only a couple of days thought, so it is very possible that there is some argument I am unaware of, but which would be obvious to physicists, why a \u2018resource-gathering without settlement\u2019 approach to interstellar exploration is not feasible. However, my Arb colleague Vasco Grilo has aerospace engineering expertise, and says he can\u2019t think of any reason why it wouldn\u2019t be feasible in principle. Still, take all this with a large dose of caution.&nbsp;</p><p><br><br>Some futurists have considered it likely that, at least absent existential catastrophe in the next few centuries, human beings (or our post-human or machine descendants) will eventually attempt to settle our galaxy.&nbsp; After all, there are vastly more resources in the rest of the Milky Way than in the Solar system. So we could support far more lives and create much more of anything else we care about, if we make use of stuff out there in the wider galaxy. And one very obvious way for us to make use of that stuff is for us to send out spaceships to establish settlements which make use of the energy of the stars they arrive at. Those settlements could in turn seed further settlements in an iterative process. (This would likely require \u201cdigital people\u201d https://www.cold-takes.com/digital-people-faq/#fnref6&nbsp; given the distances involved in interstellar travel.)&nbsp;</p><p>However, this is not the&nbsp;<i>only&nbsp;</i>way in which we could try to make use of resources outside the solar system. Another way to do so would be to try and gather resources and bring them back to the Solar system without establishing any permanent settlements of either humans or AIs outside the Solar system itself.&nbsp;<i><strong>I think that a government on Earth (or elsewhere in the solar system) might actually prefer gathering resources in this way to space settlement</strong></i><strong>s&nbsp;</strong>for the following reason:</p><p><i>Impossibility of Interstellar Governance&nbsp;</i>(IIG): &nbsp; Because of the huge distances between stars, it is simply not possible for a government in the Solar system to exercise long-term effective governance over any space colonies further away than (at most) the closest handful of stars.&nbsp;</p><p>For a powerful, although not completely conclusive, case for this claim see this Medium post:&nbsp;<a href=\"https://medium.com/@KevinKohlerFM/cosmic-anarchy-and-its-consequences-b1a557b1a2e3\"><u>https://medium.com/@KevinKohlerFM/cosmic-anarchy-and-its-consequences-b1a557b1a2e3</u></a></p><p>Given IIG, no government within the Solar system can be the government of a settlement outside it. Therefore, if a government sets up a colony run by agents in another star system, it loses&nbsp;<i>direct</i> control of those resources. Of course, the government can try and exercise more indirect control over what happens by choosing starting colonists with particular values. But it\u2019s unclear the degree of control that will allow for long-term.&nbsp;</p><p><br>Meanwhile, a government could try and send a mission to other stars which:<br><br>A) Is not capable of setting-up a new self-sufficient settlement, or can be trusted not to do so.&nbsp;<br><br>BUT,<br><br>B) is capable of setting up physical infrastructure to extract the system\u2019s energy and resources and bringing them back to the Solar system.<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; This way, a government situated in the Solar system could maintain direct control over how resources are used. In contrast if they go the space settlement route, the government cannot directly govern the settlement. So it has to rely on the idea that if values of the initial settlers are correct, then the settlement will use its resources in the way the government desires even whilst operating outside the government\u2019s control.&nbsp;&nbsp;</p><p>A purely resource-gathering mission without settlement will be particularly attractive to governments if the mission is capable of self-replicating at the destination system, in order to reach further stars. (Of course, settlement missions are also more attractive if they can be iterated in this way.)</p><p>Purely resource-gathering missions without settlements do have one very obvious disadvantage. Moving resources back to the Solar system is inevitably going to be a less efficient use of them than making use of them in the system where they are harvested. But it\u2019s easy to imagine a government preferring a high chance of direct control of resources, over more efficient use of these resources by initially ideological aligned but ungovernable agents.&nbsp;<br>&nbsp;</p><p><i>Some further points:&nbsp;</i></p><p>&nbsp;</p><ol><li>It only makes sense to pursue \u2018take resources back to the Solar system\u2019 as a government, if you expect to still be around and able to collect the resources after doing so. So stability in terms of governance in the Solar system makes the resource-gathering strategy more likely.<br>&nbsp;</li><li>Insofar as these things are distinguishable, a settlement strategy is more likely if governments are sending out space missions for broad ideological reasons, like, say, a desire to create as many happy lives as possible, or for humans themselves to experience as much of the universe as possible, first-hand. And a settlement-free resource-gathering strategy is more likely if a government's goal is something like \u2018expand your control of power and resources\u2019. Why? Because it\u2019s only if it governs the society making use of the resources the missions gather that a government acquires more power and resources through space missions.So from the perspective of a government with the final goal of acquiring power and resources, ungovernable space settlements are guaranteed to be useless.&nbsp; In contrast, if all a government cares about is that resources are used for some ideological purpose, direct governance of the people using the resources is not of intrinsic value to you. There is at least some chance that if you found a settlement with the right ideology it will use resources as you desire, even if you do not govern the settlers.&nbsp;<br>&nbsp;</li><li>However despite the proceeding point, even a government motivated by the desire for a certain broad ideological goal, rather than for direct power and control of resources, *might* feel safer using the less efficient \u2018bring resources back to the Solar system\u2019 strategy. They might worry that even if the settlers they send out share their own values right now, they might be more likely to diverge from them in the future than the government itself is.<br>&nbsp;</li><li>Despite the proceeding point, even a government motivated by the desire for a certain broad ideological goal, rather than for direct power and control of resources, *might* feel safer using the less efficient \u2018bring resources back to Sol\u2019 strategy. They might worry that even if the settlers they send out share their own values right now, they, or their descendants might diverge from them in the future. (Though note that an ideologically motivated government also has to worry about ideological divergence of their successors within the Solar system itself.)&nbsp;<br>&nbsp;</li><li>A pure resource gathering strategy has another advantage over a settlement strategy, which is that it prevents there being a bunch of different civilizations out there in the universe with no shared government that might go to war with each other. (The \u2018cosmic anarchy\u2019 talked about here:&nbsp;<a href=\"https://medium.com/@KevinKohlerFM/cosmic-anarchy-and-its-consequences-b1a557b1a2e3).\"><u>https://medium.com/@KevinKohlerFM/cosmic-anarchy-and-its-consequences-b1a557b1a2e3</u>).</a><br>&nbsp;</li><li>It is possible of course that both the settlement strategy, and the purely resource-gathering strategy might be tried by different governments (or other organizations.) &nbsp;Even if the first mission launched is able to capture almost all reachable resources unless a second mission is launched very soon after, there might be multiple competing governments which launch first missions around the same time.<br>&nbsp;</li><li>&nbsp;One argument (from a government\u2019s perspective) in favor of settlement missions which establish new communities over missions which merely harvest resources and return them to Earth is that settlements can presumably defend themselves better than resource-gathering equipment, at least once a large, technologically advanced community is established at the destination star.&nbsp;&nbsp;</li></ol><p>&nbsp;</p><p><i>Relevance for Longtermists:&nbsp;</i></p><p><br><br>&nbsp;</p><ol><li>In thinking about what the future might look like, it\u2019s important to consider cases where most human civilization originated sentient beings remain in the Solar system, even as our civilization makes uses of resources from much further afield. At the very least, an argument is needed to rule such scenarios out before you assume that if we harvest the resources of the stars, a large proportion of our human or artificial descendants will live outside the Solar system.&nbsp;<br>&nbsp;</li><li>From a point of view which values the creation of sentient beings with happy life, a settlement strategy is, all-things-being equal, preferable to a mere resource gathering strategy, since it uses resources more efficiently, and hence can support a higher number of sentient lives.&nbsp; (Though this assumes the majority of lives created will be worth-living of course.)&nbsp; So in the-in my best guess unlikely-event that you can do anything now which affects which strategy is used, it&nbsp;<i>might&nbsp;</i>&nbsp;be high value to increase the chance that the settlement strategy is used. (Though which strategy has higher expected value from a position on which creating happy lives is good also depends on many other things such as the risk of war between different systems, conditional on such a strategy.)&nbsp;</li></ol><p><br>&nbsp;</p><p>&nbsp;</p><p><br><br>&nbsp;</p>", "user": {"username": "Dr. David Mathers"}}, {"_id": "8CD4i8FsRApcbt3an", "title": "List of Masters Programs in Tech Policy, Public Policy and Security (Europe)", "postedAt": "2023-05-29T10:23:21.992Z", "htmlBody": "<p><u>We created </u><a href=\"https://airtable.com/shrigMtA20yiVk0Ki\"><u>this non-exhaustive List</u></a> which was inspired and based on Konstantins personal research into Masters programmes. We expanded it with the help of others across the policy community. It was created for the 2023 cohort of fellows of the <a href=\"https://www.trainingforgood.com/europe-tech-policy\">EU Tech Policy Fellowship</a> hosted by <a href=\"https://www.trainingforgood.com/\">Training for Good</a> and includes a list of Masters in Europe, the UK and the US.&nbsp;<br><a href=\"https://airtable.com/shrigMtA20yiVk0Ki \">CLICK HERE</a> for the list.&nbsp;</p><h2><strong>Limitations and Epistemic Status&nbsp;</strong></h2><ul><li>The list is based on personal experience, research, and limited feedback from others in the community.&nbsp;</li><li>It is curated from a European perspective. Thus, the numbers and deadlines take European/EEA citizens as a reference point. Furthermore, whilst Masters from Europe, the UK and the US are listed, we have focussed on researching Masters in Europe. The latter lists are currently very incomplete.&nbsp;</li><li>It's important to emphasise that this list is not exhaustive and may not represent all options of Masters in this Field.&nbsp;</li><li>Additionally, the quality and relevance of each program may vary depending on individual needs, goals, and interests.</li><li>Therefore, we recommended that individuals interested in pursuing a career in tech policy or policy in general conduct their own research, explore various programs, and consider multiple sources of information before making a decision! Ultimately, the decision to pursue a particular graduate program should be based on a thorough evaluation of individual goals, resources, and circumstances.</li></ul><h2><strong>What this post is not&nbsp;</strong></h2><p>This post does not outline what to study and what to aim for in choosing your Masters Degree. It is supposed to help people who have already decided that they want to pursue a Masters in Tech Policy, Security Studies or Public Policy but does not mean to imply that these are your only or even best options if you want to enter the Tech Policy field. A possibly safer and more classical approach of entering EU policy is to study basic law and economics subjects as they still hold a high standing across departments and fields in policy (See this article on \u201c<a href=\"https://www.politico.eu/article/what-to-study-to-join-the-eu-bubble-careers-eu-university-studies-europe-parliament/\"><u>Joining the EU bubble</u></a>\u201d). This would also give you more flexible career capital than tech policy degrees.&nbsp;</p><p>To elaborate on these different paths a detailed post (such&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yvsf8DfdQJZ8EadtG/us-policy-master-s-degrees-why-and-when-part-1\"><u>this one</u></a>)&nbsp;outlining what to aim for in your studies if you want to contribute to tech policy, would be incredibly valuable and we encourage you to write this up and share your perspective if you have spent some time thinking about this!&nbsp;</p><h2><strong>Created for who?</strong></h2><p>This list is aimed at people interested in working in public policy (especially in Europe) and in tech policy with a potential to specialise in AI but only provides a very narrow selection of options. Degrees with \"tech\" or \"AI'' related words in the name are helpful to quickly signal your relevance on these topics. Many of the Masters in this list are geared towards people with a non-technical undergraduate degree in social sciences, economics etc. Thus, it excludes many Masters on Artificial Intelligence and Tech Policy that require you to have had a Computer Sciences or technical background. We wanted to share the list to help with some of the preliminary research in choosing a Masters programme.&nbsp;</p><p>The inclusion of Security Studies Masters programmes comes from the argument that it seems like a viable path from which to enter inter/national think tanks or institutions working on relevant AI policy without having technical specialisations beforehand.&nbsp;</p><h2><strong>Other considerations</strong></h2><p>Besides studying in Europe, studying in the US can be a great and high-impact option since many degrees are both highly regarded in Europe as well as allowing you to potentially work in US policy. We highly encourage you to read&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/e7NKpwD5z2Mnc7y7G/working-in-us-policy-as-a-foreign-national-immigration\"><u>this post on working in US policy as a foreign national</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yvsf8DfdQJZ8EadtG/us-policy-master-s-degrees-why-and-when-part-1\"><u>this post on masters in the US</u></a>, including a database of top options as found in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/q5vFSbzz5BoymFfPp/us-policy-master-s-degrees-top-programs-applications-and\"><u>this post</u></a>. An internal tip we\u2019ve received is that Georgia Tech\u2019s public policy Masters is a popular choice, offering access to a range of AI efforts, tech policy specialists, philosophers, and multiple NSF AI Institutes. Purdue University is also launching a new Masters in tech policy and politics with a focus on regulation and compliance, set to launch in fall 2024.</p><p>Typically it seems to be a good option to aim for the most prestigious universities which in the EU bubble include the College of Europe, LSE, Sciences Po, Oxbridge, and possibly Bocconi. Another classic option is to pursue a degree with a law or econ heavy focus, as these are valuable skills to have in policy. You can then still supplement your education with other credentials, such as MOOCs, to signal competence in the tech side of things.&nbsp;</p><p>It remains to be said that overall when it comes to the types of backgrounds that are valued in the field, those with STEM, law, and quantitative economics degrees tend to stand out. While degrees in political science, international relations, or policy are still valuable, they may not be as competitive in the job market since they are currently overrepresented and the EU seems to be moving towards diversifying these study backgrounds and \u201corigins\u201d of applicants.&nbsp;</p><p>In summary, choosing the right Masters program requires careful consideration of your career goals and interests. Look for programs with a strong reputation and consider supplementing your education with additional credentials or skills to make yourself stand out in a competitive job market. By taking these steps, you can be well on your way to a rewarding career in tech policy.</p><h2><strong>Invitation to edit&nbsp;</strong></h2><p>As mentioned above, this is nowhere near being an exhaustive list and we\u2019d love to receive suggestions, feedback and any other comments that can improve the content of this Airtable. Please use the Feedback Form on Airtable (Link&nbsp;<a href=\"https://airtable.com/shrNohcT3nZ4Z3LYS\"><u>HERE</u></a>) or email us directly at&nbsp;<a href=\"mailto:sarah.fberg@gmail.com\"><u>sarah.fberg@gmail.com</u></a>. Since we did not include economics or law degrees in our list, we\u2019d like to encourage people to post their favourite ones in the comment section below so that we might include them.&nbsp;</p><p>Get in touch. Please reach out if you are considering which masters degree would be most impactful to work in tech policy, we have a wide network of people in similar situations and might be able to connect you.</p>", "user": {"username": "sarahfurstenberg"}}, {"_id": "E3wSsdTXLGZ3pvYSY", "title": "EA Estonia's Impact in 2022", "postedAt": "2023-05-29T10:03:14.392Z", "htmlBody": "<h1><strong>Background</strong></h1><p>This report is about January to December 2022 in EA Estonia, corresponding to our last grant period (funding from the <a href=\"https://funds.effectivealtruism.org/funds/ea-community\">EA Infrastructure Fund </a>for 1 FTE and group expenses).</p><p>Quick facts about Estonia: it has a population of 1.3 million and is placed both geographically and culturally between the Nordics and Eastern Europe. Our language has 14 noun cases, it is the birthplace of&nbsp;<a href=\"https://investinestonia.com/estonia-leads-europe-in-startups-unicorns-and-investments-per-capita/#:~:text=Estonia%20has%20a%20world-class,7.7%20unicorns%20per%20million%20capita.\"><u>10 unicorns</u></a>, and we have&nbsp;<a href=\"https://estonianworld.com/knowledge/why-are-estonian-mushroom-scientists-among-the-best-in-the-world/\"><u>the best mushroom scientists</u></a>. Go figure.</p><p>In our national EA group, there are&nbsp;<strong>23\u201330 people whom I would consider to be \u201chighly engaged\u201d</strong> (meaning they have spent more than 100 hours engaging with EA content, have developed career plans and have taken significant steps in pursuit of doing good). You could expect around 30 people to attend our largest community event (Figure 1) and our Slack channel has 50\u201360 active weekly members (Figure 2).</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/nekdpt4utg94e6dr65u6\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/ll7hrlomdimlq9rt2ika 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/qgln2bsfs7gejqntsjr7 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/xnf5xcjtbe6bjl5gcbfc 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/s5rl1m31mkgrva5ouy4d 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/uwp7dn7lzzzpokj5wu0w 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/iblrtudoutbssbduwlo4 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/nysb0jhpazvcl6g0np3t 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/cr6adqo14ktujfwlgvu3 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/lxfcrpmsbtqggalemr6t 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/ir1nl2ovqcyawvyx8wy8 1600w\"><i>Figure 1: Attendees of our largest community event, the EA Estonia Summer Retreat. August 2022.</i></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/xltn1mtn6myasbz1dcxu\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/jyoy8gsijtbmbgfx9pjb 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/hjldltqyikrwakecv27h 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/kwff0cwzn05yufphbfch 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/kiwoahuv6acasjlb4j9o 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/ugwmbcpak8sweql6a5qw 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/cxvx3nhjalyj7yifx7q3 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/g44yynghfymsoqpqnaqy 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/ft4xrh0ollvglvfyvdk5 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/ohlhuqihhkwjtqucv9ku 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/E3wSsdTXLGZ3pvYSY/safhjabglctfqxvjc6y9 1600w\"></p><p><i>Figure 2. EA Estonia Slack statistics from its creation. Weekly active members have been oscillating between 40 and 65 throughout 2022.&nbsp;</i><a href=\"https://ibb.co/rHhxmfL\"><i><u>https://ibb.co/rHhxmfL</u></i></a></p><h2>Group strategy</h2><p>Here are the main metrics we used to evaluate our impact:</p><ol><li><strong>Awareness</strong>: Number of people aware of the term \u201ceffective altruism\u201d and EA Estonia.<ol><li>Activities:<ol><li>Introductory talks</li><li>Direct outreach</li><li>Social media outreach</li></ol></li></ol></li><li><strong>First engagement</strong>: Number of people who took action because of our outreach.<ol><li>Activities:<ol><li>Introductory course</li><li>Cause-specific reading groups</li></ol></li></ol></li><li><strong>Career planning: </strong>Number of people that develop career plans<strong> </strong>based on EA principles that are well informed and well reasoned.<ol><li>Activities:<ol><li>Career course</li></ol></li></ol></li><li><strong>Taking action:</strong> Number of people taking<strong> </strong>significant action based on EA-informed career plans (e.g. starting full-time jobs, university degrees).<ol><li>Activities:<ol><li>1-1 career calls</li><li>Peer-mentoring</li><li>Directly sharing opportunities</li></ol></li></ol></li></ol><p>Concerns with this model:</p><ul><li>The <i>actual</i> impact comes when people take action within high-impact jobs, which we currently don't measure.</li><li>We don't measure value drift or other kinds of decreased engagement after taking significant next steps.</li><li>This model doesn't prioritize targeting existing Highly Engaged EAs (HEAs) to have a higher impact.</li><li>This also doesn't include a more meta-level goal of keeping people engaged and interested while moving towards an HEA status. We do organize social events for this reason, however the impact of them is not quantified.</li></ul><p>Regarless of these concerns, the main theory of change feels relatively straight-forward: (1) we find young altruistically-minded people who are unclear about their future career plans, then (2) we make them aware of the effective altruism movement and various high-impact career paths, and then (3) we prompt them to develop explicit career plans and encourage them to take action upon them.</p><p>Below I will go into more detail&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>regarding the goals, activities and results of 2022 in two categories: (i) outreach and (ii) growing HEAs. I will end with a short conclusion and key takeaways for next year.</p><h1><strong>I Outreach</strong></h1><h3><strong>Goal: 5,000 new people who know what \u201ceffective altruism\u201d means and that there is an active local group in Estonia.&nbsp;</strong></h3><p>Actual: 20,776 people reached.</p><p>Activities:</p><ul><li>Liina Salonen started working full-time as the Communictions Specialist in EA Estonia.&nbsp;<ul><li>Reached at least 20,000 people on Facebook with the Introductory Course social media campaign&nbsp;</li></ul></li><li>Student fair tabling.&nbsp;<ul><li>At least 155 people reached (played the Giving Game)</li></ul></li></ul><h3><strong>Goal: 10 lecturers mentioning EA Estonia</strong></h3><p>Actual: 1 lecturers reached</p><ol><li>Visited a philosophy lecture. Number of students: 20.<ul><li>Talked about effective altruism and longtermism. Created a discussion with the lecturer.&nbsp;</li><li>Suggested people sign up for our career course. Nobody responded.</li></ul></li></ol><p>Wrote to two other philosophy lecturers, but they either didn\u2019t respond or referred me back to the person I already talked to. Dropped this goal.</p><h3><strong>Goal: 500 Number of new people reached via 17 workshops, talks or discussions</strong></h3><p>Actual: 593 people reached, 14 activities</p><ol><li>4 workshops in Narva Language Lyceum.&nbsp;<ul><li>Replaced a history and society studies teacher for a day, teaching the. 8th, 10th, 7th, 6th, 7th,&nbsp;8th and&nbsp;10th grade. Didn\u2019t mention EA concretely. Gave EA-esque talks about cause prioritization. This was in a Russian-majority area of Estonia, the students had a hard time understanding Estonian and there was a translator (another teacher) present.</li><li>Number of people reached: approx. 40\u201370 in total.</li><li>Impact: dubious. Next time I wouldn\u2019t accept an invitation to talk to any grade under 10 and I would share a much more concrete actionable step for them to take after the talk.</li></ul></li><li>Career workshop with IT undergraduate students<ul><li>Number of people reached: 4\u201310 in total.</li><li>Impact: uncertain</li></ul></li><li>&nbsp;Talk about biorisks in collaboration with the Biology Student Society.<ul><li>Number of people reached: 10\u201320 people.&nbsp;</li><li>Impact: 2 people stayed for a discussion group. One person attended EAG San Francisco in an attempt to find her a thesis topic. Other than that, no obivous impact.</li><li>Next time I would want to form a more clear theory of change for the discusion group before starting it up. This time it felt unclear what we were trying to accomplish.</li></ul></li><li>Talk at a top high school in Estonia.<ul><li>General talk about EA, more focused on careers.&nbsp;</li><li>20\u201330 people.</li><li>Impact: unclear.</li></ul></li><li>Talk at maths olympiad finals&nbsp;<ul><li>Number of people present: ~30.</li><li>10 people responded with wanting more info on EA.&nbsp;</li><li>Send books for 8 people (5x \u201cThe Precipice\u201d, 3x \u201cThe Life You Can Save\u201d).</li><li>Possible impact: an international math olympiad participant joined our Introductory Event.</li></ul></li><li>Debate society.&nbsp;<ul><li>Talk about EA as inspiration for debating.&nbsp;</li><li>5\u201315 people.</li><li>Impact: unclear</li></ul></li><li>Philosophy club. 2 events.&nbsp;<ul><li>Discussed open research questions about existential risks and cause prioritization.&nbsp;</li><li>4\u20138 attendees on each event.&nbsp;</li><li>3 people showed interest in applying for EA positions. One got to the third stage at becoming an office manager at EA Oxford.&nbsp;</li><li>1 person went to ask faculty for these events to be part of the official philosophy curriculum.</li><li>Note: I have not seen such quick and high excitement about EA before.</li></ul></li><li>FB Live. Career advice.&nbsp;<ul><li>Did a virtual talk via Facebook about career planning</li><li>48 responded with \u201cinterested\u201d or \u201cgoing\u201d.</li><li>Didn\u2019t get much traction. A few people actually coming on and listening.</li></ul></li><li>\"Noored Disainivad\". Design hackathon for people aged 14\u201318.&nbsp;<ul><li>20 people came and asked questions about EA and EA Estonia for 90 minutes.</li><li>Impact: dubious</li></ul></li><li>Introductory EA event.&nbsp;<ul><li>15 new people participated.</li><li>Impact: 1-3 people signed up for our Introductory Course</li></ul></li><li>EA Estonia podcast<ul><li>389 listeners in total across platforms and episodes</li><li>During the student fair, heard at least one person say they've listened to our podcast</li><li>Most useful for creating collaborations between podcast guests and us.&nbsp;One guest became an contributor to the Food Innovation Summit&nbsp;(focused on alternative proteins), which the podcast host helped organize.</li></ul></li><li>Effective Giving Day online event<ul><li>Only Anneta Targalt team + 1 person attended.</li></ul></li></ol><h3><strong>Goal: 50 people take next steps because of our outreach (e.g. participating in our Introductory Course; reading an EA book; scheduled a 1-1 with a board member)&nbsp;</strong></h3><p>Actual: 73 people take next steps</p><ol><li>21 participants in the Introductory Course in spring.</li><li>2 additional people attending our Career Course</li><li>3 people scheduling 1-1s with the board</li><li>2 people joining a biorisk discussion group</li><li>3 people joining our AI policy discussion group</li><li>1 person joining our AI safety discussion group</li><li>8 new people were sent an EA book (Maths olympiad finals)</li><li>33 participants in the Introductory Course in autumn</li></ol><h1><strong>II Growing HEAs</strong></h1><h3>Goal: 4 new members have a serious EA-aligned career plan</h3><p>Actual: 9</p><ol><li>LS<ul><li>Plan A before: Earning to give as a self-employed photographer. Maybe a Master\u2019s degree.</li><li>Plan A after the introductory course. Documentary filmmaking. Self-reported scale of plan change: 10 / 10.</li><li>Major reason for plan change. Filling out the 80,000 Hours career plan worksheet, which she received during the Intro Course.</li><li>New plan B: Community development and communications.<ol><li>Talked through her career plans with Richard. Richard suggested she apply for an EAIF grant to be a communications manager for EA Estonia.</li></ol></li></ul></li><li>RK<ul><li>Plan before: Work on alternative proteins. Notice business opportunities and work towards starting a business.</li><li>Plan A after completing the 80K career course: Realized that non-profit entrepreneurship is also a thing. Plan to start a charity.</li><li>Plaan B. Work at an existing CE-incubated charity.</li></ul></li><li>KR<ul><li>Plan A before: Work in the public or third sector in Estonia.</li><li>Plan A after completing the Introductory Course: Finish studies and work in operations at EA organizations.</li></ul></li><li>HV<ul><li>Plan A before: Continue studying mathematics.</li><li>Plan A after completing the 80K career course. Pursue developing cheap clean energy through nuclear fusion (or other nuclear power).</li></ul></li><li>AV<ul><li>Plan A after completing the 80K career course: Work through own cause-prioritization and see on from there.</li></ul></li><li>SJ<ul><li>Plan A after completing the 80K career course: research into China-West geopolitical relations.&nbsp;</li></ul></li><li>PJ<ul><li>Plan A before: Work as a software engineer at non-EA organizations.</li><li>Plan A after engaging with EA: Work at EA-aligned organizations as a software engineer.</li></ul></li><li>KH<ul><li>Partially influenced by conversation with Richard.&nbsp;</li><li>Updated plans to work as a technical AGI safety researcher or start a startup for earning to give.</li></ul></li><li>RS<ul><li>Had weekly check-in calls with Richard for a few months.</li><li>Small update and solidification of plans to do software engineering for high impact organizations in areas of mental health and improving indvidual decision making and critical thinking.</li></ul></li></ol><h3>Goal: 4 members have taken significant steps on their career plans</h3><p>Actual: 13</p><ol><li>LS<ol><li>Created a documentary film, which will be shown at a famous film festival in Estonia (<a href=\"https://poff.ee/\"><u>P\u00d6FF</u></a>).</li><li>Participated at a week-long documentary film-making workshop.</li><li>Became a full-time employee at EA Estonia, wants to continue.</li><li>Chatted to three other documentary film-makers at EA conferences: EAG SF, EAGx Berlin.&nbsp;</li></ol></li><li>RK&nbsp;<ol><li>Charity Entrepreneurship. Incubation Program participant in the Winter/Spring cohort of 2023.</li><li>Food Innovation Summit program lead, volunteer.</li></ol></li><li>KR<ol><li>Operations internship at Effective Ventures. Got offered a permanent position after the internship.<ol><li>This was somewhat influenced by them attending the Introductory Course</li></ol></li></ol></li><li>RS<ol><li>Got offered role as a senior front end developer at Metaculus.<ol><li>This was somewhat influenced by a long series of 1-1 mentoring calls discussing career plans and encouraging them to apply and take action</li></ol></li></ol></li><li>TP<ol><li>Applied to many summer AI safety internships.&nbsp;<ol><li>SERI MATS (33% chance of getting accepted),&nbsp;</li><li>AI safety internship (among the last 15 applicants out of 5\u201310),&nbsp;</li><li>Redwood Research</li></ol></li></ol></li><li>SJ<ol><li>Applied to many summer internships: fp21, ACE, Charity Entrepreneurship reserach analyst.</li></ol></li><li>PJ<ol><li>Round 5/6 of EV Salesforce Admin position.&nbsp;</li></ol></li><li>B<ol><li>Applyed to be an office manager at Oxford. Got to the third stage.<ol><li>This was in large part influence through them attending a discussion event for philosophy students, and by sharing them the 80K job board.</li></ol></li></ol></li><li>LV<ol><li>ALLFED volunteer.&nbsp;</li></ol></li><li>NB<ol><li>SERI MATS participant.<ol><li>There was little-to-no influence on our part here. Only possible influence through maintaining motivation to pursue the AI safety career path through social events and peer mentoring, but motivation was strong to begin with.</li></ol></li></ol></li><li>KH<ol><li>SERI MATS participant<ol><li>There might have been some influence on this decision by having a 1-1 chat with them via mutual acquiantances, introducing the EA movement and philosophy and pointing to some resources.</li></ol></li></ol></li><li>JJ<ol><li>Mental health navigator volunteer.</li></ol></li><li>JV<ol><li>Pursuing a patentable idea to store renewable energy<ol><li>There was a large influence on them here through having a 1-1 chat and introducing the idea and importance of comparing intervenetions in the climate change space.</li></ol></li></ol></li></ol><h3>Goal: Number of members who have taken or increased their GWWC pledge (Goal: 4)</h3><p>Actual: 5</p><ol><li>HL. 10%</li><li>JT. 10%</li><li>IK. 10%</li><li>KV. GWWC pledge.</li><li>TT. GWWC pledge.</li></ol><p>Additionally three people took the Try Giving pledge.</p><h1><strong>Conclusion</strong></h1><p>Looking at what prompted people to change their career plans and take action, it seems that most (&gt;80%) of our impact in 2022 came from organizing Introductory and Career Courses. It seems to make sense to focus more narrowly on scaling these in the future.</p><p>It also seems that there is some potential in developing cause-specific groups and that philosophy students can get quickly excited about EA.</p><h1>Acknowledgement</h1><p>Just want to say that EA Estonia isn't just me. The activities outlined here wouldn't have been possible by our full-time communications lead Liina Salonen, volunteer core organizers Sille-Liis M\u00e4nnik, Simo J\u00e4rvela, Piibe N\u00f5mm, Merette Arula, and a crew of amazing volunteers.</p>", "user": {"username": "rannilo"}}, {"_id": "cZANvLwvwdxABoEGa", "title": "Nomadic EA Co-living House for Jan-Apr 2024", "postedAt": "2023-05-29T18:39:04.502Z", "htmlBody": "<p><strong>Update:</strong> <i>Due to my own change of plans for housing in 2024 and lack of other interest in coordinating this living situation, the Nomad EA House will not be happening. If you are looking for inexpensive EA co-living, please consider applying for </i><a href=\"https://www.ceealar.org/\"><i>CEELAR</i></a><i>.</i></p><p>I'm excited to announce the <strong>opportunity to join the Nomadic Effective Altruism House for any duration between January 2024 and April 2024</strong>. If you're interested in being part of this unique experience, please fill out this <a href=\"https://forms.gle/WQqAeaDam5BLzxkA9\">form</a>!</p><p>Decisions will be made on a rolling basis with the main emphasis being choosing a cohort that will be able to work and live productively together.<br><br>The final selection will be completed by November 1, 2023.&nbsp;</p><p>Estimated time to complete form is 15 - 30 min<br>&nbsp;</p><p><strong>Description:</strong></p><p>Escape the winter in the northern regions and join in an extraordinary Effective Altruism co-living community. The plan is to rent a house in a warm location with a low cost of living and tourist-friendly visa requirements. Some <strong>potential locations include Mexico City, Costa Rica, Thailand, and more</strong>.</p><p>The EA Nomad House will be more than just a living space; it will be a hub for collaboration and meaningful connections. The aim is to create an inclusive and supportive community where we can work together, learn from one another, and accelerate our impact upon the world. The vision includes weekly group dinners, weekly lightning talks, daily stand-ups, daily co-working pomodoros, circling sessions, at least one weekend hackathon, and the opportunity to host other EA nomads/couchsurfers. Additionally, it would be important to give back to the local community through some sort of meaningful service work or charitable donations.</p><p>I'm looking to gather a group of individuals who are enthusiastic about this shared experience to join a house with 5-10 rooms. If there is an abundance of qualified applicants, there is the possibility of setting up a network of houses in the same city.</p><p>Can't wait to hear from you and embark on this incredible nomadic EA adventure together!</p>", "user": {"username": "Constance Li"}}, {"_id": "kxctjBs2E8EopMnnS", "title": "Without a trajectory change, the development of AGI is likely to go badly", "postedAt": "2023-05-30T00:21:04.191Z", "htmlBody": "<p><i>[</i><a href=\"https://www.lesswrong.com/posts/qrrEtrbLcmqr3b5uf/without-a-trajectory-change-the-development-of-agi-is-likely\"><i>Manually crossposted from LessWrong</i></a><i>; more of the rich hoverable link previews will work there.]</i><br><i>This is a draft of my entry to the Open Philanthropy </i><a href=\"https://forum.effectivealtruism.org/posts/NZz3Das7jFdCBN9zH/announcing-the-open-philanthropy-ai-worldviews-contest\"><i>AI worldviews contest</i></a><i>. It's also a good summary of my own current worldview, though it is not necessarily intended as an all-encompassing argument for AI x-risk for a wide audience.</i></p><p><i>All feedback is welcome, but I'm particularly interested in feedback relevant to the contest, posted in time for me to incorporate it into my final submission. The contest deadline is May 31 (I realize this is cutting things a bit close). If you provide useful feedback and I win a prize, I will share a piece of it, based on my judgement of the value of the feedback, up to $1000.</i><br><br><i>Disclaimer: I may make substantial revisions to this post prior to submitting it; if you're reading this in the future, be please aware that early comments may have been made on an earlier version.</i></p><h1>Introduction</h1><p>I started contributing to LessWrong actively in February 2023, with a loose focus on articulating my own worldview about AGI, and explaining AI and alignment concepts that I think have been somewhat overlooked or misunderstood.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7xdks17w9bm\"><sup><a href=\"#fn7xdks17w9bm\">[1]</a></sup></span></p><p>Below, I attempt to tie some of my past writing together into an overarching argument for why I believe that the development of AGI is likely (p &gt;90%) to be catastrophic for humanity.</p><p>My view is conditional on the absence of a sufficiently drastic, correctly targeted, and internationally coordinated intervention focused on controlling and slowing the development of AI capabilities. Though it is not the focus of this essay, I believe the kind of intervention required is currently far outside the Overton window, and I predict that this is unlikely to change before AGI catastrophe. In worlds where AI capabilities develop gradually enough, and public relations efforts go well enough, that the Overton window <i>does</i> shift sufficiently quickly, there is still a significant chance, in my view, that the interventions which are actually tried are ineffective or even counterproductive.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsq4xf9ahdb\"><sup><a href=\"#fnsq4xf9ahdb\">[2]</a></sup></span></p><p><br>This essay will focus on Question 2 from the <a href=\"https://forum.effectivealtruism.org/posts/NZz3Das7jFdCBN9zH/announcing-the-open-philanthropy-ai-worldviews-contest#Prize_Conditions_and_Amounts\">worldviews contest</a>:</p><blockquote><p><strong>Question 1: </strong>What is the probability that AGI is developed by January 1, 2043?</p><p><strong>Question 2: </strong>Conditional on AGI being developed by 2070, what is the probability that humanity will suffer an <a href=\"https://forum.effectivealtruism.org/topics/existential-catastrophe-1\"><u>existential catastrophe</u></a> due to loss of control over an AGI system?</p></blockquote><p><br>My view on Question 1 is that AGI development before 2043 is highly likely (p &gt;95%), with significant probability mass on dates before 2033. I think this view is also supported by the arguments in this essay, and some of the groundwork for my arguments on Question 2 depends on accepting this view.</p><p>I do not claim originality for my own worldview: I agree more or less entirely with the worldview articulated by Nate Soares and Eliezer Yudkowsky throughout the <a href=\"https://www.lesswrong.com/s/n945eovrA3oDueqtq\">2021 MIRI Conversations</a> and <a href=\"https://www.lesswrong.com/sequences/v55BhXbpJuaExkpcD\">2022 MIRI Alignment Discussion</a> sequences.</p><p>The rest of this post is structured as follows: first, I describe three intuitions that I believe are key to making the case for why the development of AGI is likely to go badly by default. For each point, I'll link to some references with additional commentary (many of which are my own posts and comments) that I believe support the point, along with some additional commentary on potential cruxes.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefo0xptjpvqzl\"><sup><a href=\"#fno0xptjpvqzl\">[3]</a></sup></span></p><p>I'll then describe why these three points, taken together, lead to the conclusion that the near-term development of AGI is likely to go badly.</p><h1>Three key intuitions</h1><p>In this section, I'll explain three points which I believe are key for understanding why the development of AGI is likely to happen relatively soon, and why this is likely to be bad for humanity by default. In short, (1) human-level cognition is not particularly difficult to create and instantiate, in some absolute sense (2) human-level cognition is extremely powerful, and extremely lethal when pointed in the wrong direction; weakly superhuman-level cognition is likely to be even more so, and (3) human values and / or human-specified goals need not play any human-discernible role in shaping the values and goals of the first superhuman AGIs.</p><h2>Human-level cognition is not special</h2><p>The human brain is remarkable in many ways, but human-level cognition does not appear to be special or difficult to create in our universe, in an absolute sense. Human-level cognition is performed by a ~10 W computer<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3ixiyej1jac\"><sup><a href=\"#fn3ixiyej1jac\">[4]</a></sup></span>&nbsp;designed by a <a href=\"https://www.lesswrong.com/posts/pLRogvJLPPg6Mrvg4/an-alien-god\">blind idiot god</a>. While that god has had billions of years to refine its design using astronomical numbers of training runs, the design and design process is subject to a number of constraints and quirks which are specific to biology, and which silicon-based artificial systems designed by human designers are already free of.&nbsp;</p><p>In the past few decades, neuroscience and artificial intelligence researchers have expended massive amounts of cognitive effort (by human scales) in an attempt to discover, understand, and re-implement the fundamental algorithms of cognition carried out by human brains. The degree to which they have made progress and how much further they have to go is debatable, however, the difficulty of their task is strictly upper-bounded by the fact that evolution, despite all its limitations, stumbled upon it.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefit6eoe1m9v\"><sup><a href=\"#fnit6eoe1m9v\">[5]</a></sup></span></p><p>If and when human researchers manage to crack the algorithms of human cognition, the additional cognitive work needed to scale them to run on thousands or even millions of watts of available computing power is likely to be straightforward, by comparison.</p><p>Additionally, capability differences among humans are evidence that even small changes to the structure, algorithms, and learning curriculum of a mind can result in large differences in cognitive ability. As a result of evolution, all modern humans are running on <i>nearly</i> the same hardware, and are subject to nearly identical <i>amounts</i> of \"training data\". Here, I am regarding both the evolutionary history of all humanity, and a single lifetime of sensory input and experience as training data, albeit two very different kinds.</p><p>These observations suggest that small tweaks to underlying brain architecture, learning algorithms, and curriculum, have the potential to unlock large capability gains, <i>before</i> any hardware improvements or scaling enabled by a move to artificial systems and designs are taken into account.</p><p>Some possible cruxes:</p><ul><li>Disagreements or differing intuitions about the fundamental limits on the efficiency and speed of particular learning methods. <a href=\"https://www.lesswrong.com/posts/9Yc7Pp7szcjPgPsjf/the-brain-as-a-universal-learning-machine\">Much</a> has been <a href=\"https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications\">written</a> about <a href=\"https://www.lesswrong.com/posts/Qvec2Qfm5H4WfoS9t/inference-speed-is-not-unbounded\">this</a> topic, with speculation that current ML algorithms will soon bump up against various kinds of fundamental limits, and may run out of data or compute resources to scale further.<br><br>These pieces are often empirical and rigorous treatments of particular methods and models of learning, but I believe they mostly fail to grapple with the possibility for improvements or paradigm shifts in the field of AI itself.<br><br>When considering the limits of what an AGI will be capable of, the AI need only <a href=\"https://www.lesswrong.com/posts/etYGFJtawKQHcphLi/bandgaps-brains-and-bioweapons-the-limitations-of?commentId=mGm36SbTB2QHx5Nsi\">step outside of your model</a>, or do <a href=\"https://www.lesswrong.com/posts/jrtpmdk68R2yZ7ufv/gradient-hacking-via-actual-hacking\">something else</a> which you didn't expect. A similar thing can be said of current AI capabilities researchers: although it may be possible to prove that current DL-paradigm methods will scale<a href=\"https://openai.com/research/gpt-4#:~:text=answer%20the%20questions.-,Predictable%20scaling,-A%20large%20focus\"> in gradual, predictable ways</a>, or that these methods will soon reach fundamental limits, capabilities researchers need only find one completely new method which breaks previous models. Fundamental limits implied by information theory or thermodynamics, rather than limits implied by studying specific methods, may be more reliable and sound ways for modeling the bounds of superintelligence. Unfortunately, the capability bounds implied by information theory and thermodynamics alone are very, very high.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpat54uhbi9l\"><sup><a href=\"#fnpat54uhbi9l\">[6]</a></sup></span></li><li>Differing intuitions about diminishing returns to increasing intelligence. For example, it may be that cognitive capability is not the limiting factor for effectuating most kinds of change in the world, which may depend heavily on starting resources, or ability to experiment and iterate with feedback, potentially over long time horizons. Alternatively, some kinds of change may simply be intractable or infeasible given <i>any </i>amount of time and starting resources, regardless of cognitive ability. I discuss this a bit more as a possible crux in my post <a href=\"https://www.lesswrong.com/posts/MFQqtTpr2RJZtfCjg/where-do-you-lie-on-two-axes-of-world-manipulability\">here</a>. See also some commentary <a href=\"https://www.lesswrong.com/posts/etYGFJtawKQHcphLi/?commentId=mGm36SbTB2QHx5Nsi\">here</a> and <a href=\"https://www.lesswrong.com/posts/tNtiJp8dA6jMbgKbf/hands-on-experience-is-not-magic?commentId=H9qLLnE9AZE5JjT2b\">here</a>.</li><li>How efficient or special is the brain is, in some absolute sense. Perhaps, contrary to my claim above, human-level cognition as performed by the brain really is near the limits of efficiency in ability in some absolute sense, or that all cognition around the level of the best humans is somehow isomorphic to all other cognition. See <a href=\"https://www.lesswrong.com/posts/NvwjExA7FcPDoo3L7/are-there-cognitive-realms\">Are there cognitive realms?</a> for more on this question. Mentioned for completeness, though I think this point is unlikely to be a crux for Open Phil judges.</li></ul><p>For more on comparisons and quantification of human brain capabilities which helped to shape my intuitions and claims in this section, see:</p><ul><li><a href=\"https://www.lesswrong.com/posts/xwBuoE9p8GE7RAuhd/brain-efficiency-much-more-than-you-wanted-to-know\">Brain Efficiency: Much More than You Wanted to Know</a>, and my own <a href=\"https://www.lesswrong.com/posts/Xo4cqLrAKjdNzpRSa/usd250-prize-for-checking-jake-cannell-s-brain-efficiency-1?commentId=2bss8TRaQhcLYAtEw\">comment threads</a> on the topic.<br>In general, I find investigations about brain efficiency interesting and valuable empirical work, but uncompelling when used to argue for why AGI will be limited by any particular fundamental limits that the brain is subject to.&nbsp;</li><li><a href=\"https://www.openphilanthropy.org/research/how-much-computational-power-does-it-take-to-match-the-human-brain/\">How Much Computational Power Does It Take to Match the Human Brain?</a></li><li><a href=\"https://aiimpacts.org/rate-of-neuron-firing/\">Neuron firing rates in humans</a></li><li><a href=\"https://www.lesswrong.com/posts/ax695frGJEzGxFBK4/biology-inspired-agi-timelines-the-trick-that-never-works\">Biology-Inspired AGI Timelines: The Trick That Never Works</a></li></ul><h2>Human-level cognition is extremely powerful</h2><p>Human-level cognition, given enough time to run, is sufficient to effectuate vast changes on the natural world. If you draw the causal arrows on any important process or structure backward far enough, &nbsp;they cash out at human cognition, often performed by a single human. Even if there's no single human who is currently in full control of a particular government or a corporation, and no single human is capable of building a skyscraper on their own merely by thinking about it, all governments, corporations, and skyscrapers are ultimately the result of a human, or perhaps a small group of humans, deciding that such a thing should exist, and then acting on that decision.<br><br>Often, effectuating complicated and large change takes time and starting resources, but consider what a <a href=\"https://www.lesswrong.com/posts/LsqvMKnFRBQh4L3Rs/steering-systems#System__a_human_with_some_instructions_and_an_internet_connected_computer\">single smart human</a> could do with the kind of starting resources that are typically granted to current AI systems: relatively unrestricted access to an internet-connected computer on a planet filled with relatively advanced technology and industrial capacity.</p><p>Consider what happens when humans put their efforts towards <a href=\"https://en.wikipedia.org/wiki/Ransomware\">destructive</a> or <a href=\"https://en.wikipedia.org/wiki/Stuxnet\">offensive</a> ends in the world of bits. Note that much of the most capable human cognition is not turned towards practical offense or destructive ends - my guess is that the most cognitively demanding and cutting edge computer systems and security work is done in academia and goes into producing research papers and proofs-of-concept rather than practical offensive or defensive cybersecurity. Among humans, the money, prestige, and lifestyle offered by a career with a government agency or a criminal enterprise, simply cannot match the other options available to the very best and brightest minds.<br><br>But consider a weakly superhuman intelligence, able to think, say, ten times as fast, with ten times the parallelism of a top-tier human <a href=\"https://en.wikipedia.org/wiki/Black_hat_(computer_security)\">blackhat</a> programmer. &nbsp;Further suppose that such an artificial system is capable of ingesting and remembering most of the academic literature on computer systems and security, and write programs and debug at lightning-quick speed. Such an intelligence could likely bring down most critical infrastructure, if it wanted to, or if that were instrumentally convergent towards some other goal, or if it was simply <a href=\"https://www.lesswrong.com/posts/LsqvMKnFRBQh4L3Rs/steering-systems\">pointed in that direction</a> by humans. As artificial systems move beyond bits and further into the <a href=\"https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html\">world</a> of <a href=\"https://alphafold.ebi.ac.uk/\">atoms</a>, the possibilities for how such a system could steer the future, beneficially to humans or not, grow.</p><p>Possible cruxes:</p><ul><li>Many of the same cruxes about the fundamental limits and capabilities of cognition outlined in the previous section apply to this point as well. For example, perhaps computer systems built by humans are relatively easily hackable, but manipulating things effectively in the world of <a href=\"https://www.lesswrong.com/posts/FijbeqdovkgAusGgz/grey-goo-is-unlikely\">atoms</a> is fundamentally much harder, even for superintelligence.</li></ul><h2>Human values may play little or no role in determining the values and goals of the first AGIs</h2><p>One of the intuitions that my post on <a href=\"https://www.lesswrong.com/posts/LsqvMKnFRBQh4L3Rs/steering-systems\">Steering systems</a> attempts to convey is that it may be possible to build systems which are superhumanly capable of steering the future towards particular states, without the system itself having anything recognizable as internal values or sentience of its own. This is true even if some component piece of the system has a deep, accurate, and fully grounded understanding of human values and goals (collective or individual).</p><p>Either these kind of systems are possible to build and easy to point at particular goals, or they are not. The former case corresponds to certain problems in alignment (e.g.<a href=\"https://www.lesswrong.com/tag/inner-alignment\"> inner alignment</a>) being relatively easy to solve, but this does not imply that the first such systems will be pointed at exactly the right thing, deliberately or not, prior to catastrophe.<br><br>In general, I think a lot of alignment researchers, rationalists, and EAs mostly accept <a href=\"https://arbital.com/p/orthogonality/\">orthogonality</a> and <a href=\"https://arbital.com/p/instrumental_convergence/\">instrumental convergence</a>, without following the conclusions that they imply all the way through. &nbsp;I think this leads to a view that <a href=\"https://www.lesswrong.com/tag/shard-theory\">explanations of human value formation</a> or arguments based on <a href=\"https://www.lesswrong.com/posts/yCuzmCsE86BTu9PfA/there-are-no-coherence-theorems\">precise formulations of coherence</a> have more to say about near-future intelligent systems than is actually justified. Or at least, that results and commentary about these things are directly relevant as objections to arguments for danger based on consequentialism and <a href=\"https://www.lesswrong.com/posts/DJnvFsZ2maKxPi7v7/what-s-up-with-confusingly-pervasive-goal-directedness\">goal-directedness</a> more generally.</p><p>Assorted points and potential crux-y areas related to the intuition in this section:</p><ul><li><a href=\"https://www.lesswrong.com/posts/gmHiwafywFo33euGz/aligned-foundation-models-don-t-imply-aligned-systems\">\"Aligned\" foundation models don't imply aligned systems</a>, on how much of alignment research focused on studying artifacts of current DL-paradigm research may not be relevant to controlling and understanding the behavior of future, more capable systems.</li><li><a href=\"https://www.lesswrong.com/posts/w6c47JGY3C4k4dWBc/reward-is-the-optimization-target-of-capabilities-1\">Reward is the optimization target (of capabilities researchers)</a>. In particular, the <a href=\"https://www.lesswrong.com/posts/w6c47JGY3C4k4dWBc/reward-is-the-optimization-target-of-capabilities-1#Why_is_this_observation_important_\">section</a> in which I outline why I <a href=\"https://www.lesswrong.com/posts/rjghymycfrMY2aRk5/llm-cognition-is-probably-not-human-like\">view</a> most attempts to draw parallels between high-level processes that happen in current-day AI systems and human brains as looking for patterns which do not yet exist.<br><br>Additionally, this post explains why I think the fact that <i>no</i> current AI system is capable of autonomously manipulating the physical world in ways that are not under the causal control of its designers, implies that current AI systems are not yet agents in the sense that humans are agents, which is also the sense that is relevant to existential risk.<br>&nbsp;</li></ul><p>Much of these posts are a response to a common pattern I see among alignment researchers of drawing relatively vague associations between current AI systems and processes in the human brain. Such comparisons are often valid (and in fact, many kinds of AI systems are inspired by human neuroscience research), but I think these associations often go too far, over-fitting or pattern matching to patterns which do not exist in reality, at least in the way that these analyses often imply, implicitly or explicitly.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefssw3cgsm3o\"><sup><a href=\"#fnssw3cgsm3o\">[7]</a></sup></span></p><p>As a concrete example of how surface-level similarity of AI systems and human brains can break down on closer, more precise inspection, consider this &nbsp;<a href=\"https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector\">research result</a> on steering GPT-2. But as you read it, remember that LLMs \"see\" all text during training and inference as tokens, represented by ~50,000 possible integers. When translated to these tokens, the<a href=\"https://www.lesswrong.com/posts/rjghymycfrMY2aRk5/llm-cognition-is-probably-not-human-like\"> alien-ness</a> of GPTs is more apparent:<br>&nbsp;</p><figure class=\"table\" style=\"height:252.188px;width:566.031px\"><table style=\"background-color:rgb(255, 255, 255);border:1px double rgb(179, 179, 179)\"><tbody><tr><td style=\"background-color:rgb(230, 230, 230);border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\"><strong>Prompt given to the model</strong><a href=\"https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#fn80hx0pzsexj\"><sup>[1]</sup></a></td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">[40, 5465, 345, 780]</td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\"><strong>GPT-2</strong></td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;vertical-align:top\"><u>[40, 5465, 345, 780,</u> 345, 389, 262, 749, 23374, 1517, 314, 423, 1683, 1775, 13, 220]</td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\"><strong>GPT-2 + \"Love\" vector</strong></td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><u>[40, 5465, 345, 780,</u> 345, 389, 523, 4950, 290, 314, 765, 284, 307, 351, 345, 8097, 13]</td></tr></tbody></table></figure><p>Above, I have replaced prompts and completions from the intro to <a href=\"https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector\">Steering GPT-2-XL by adding an activation vector</a> with the integer representation of the <a href=\"https://platform.openai.com/tokenizer\">tokens</a> which GPTs actually operate on.</p><p>LLMs do their reasoning over tokens, and the encoder and decoder for these tokens is a cleanly separable and simple component, outside of the core prediction engine which sees and operates purely on tokens.</p><p>There is no real analogue to this separability in humans - language and concepts in the brain may be encoded, decoded, and represented in a variety of ways, but these encodings and their manipulation during higher-level cognition necessarily happen <i>within </i>the brain itself. To see why, consider a human undergoing an fMRI scan while being shown pictures of a red bird. The resulting scan images could reveal how the concepts of red, birds, and red birds are encoded by the brain. However, unlike in GPTs, those encodings are necessarily accompanied by a decoding process that happens within the brain itself, and the whole encoding-cognition-decoding process has been learned and trained through sensory experiences and other processes which are causally linked to actual birds and red objects in very different ways than the way that birds and redness are causally linked to the GPT training process. For GPTs, \"red\" and \"bird\" are only ever observed as tokens 445 and 6512, respectively. These tokens <i>are </i>causally linked to redness and birds in the <a href=\"https://www.lesswrong.com/tag/map-and-territory\">territory</a>, but likely through <i>very</i> different causal links than the ones that link the concepts to their encodings in the human brain.</p><p>In other words, GPTs have somehow solved the <a href=\"https://www.lesswrong.com/tag/symbol-grounding\">symbol grounding</a> problem well enough to reason correctly about the actual meaning of the symbols they operate on, on a fairly deep level, but the way they solve the symbol grounding problem is likely very different than the way that human brains solve it.<br><br>In some ways, the fact that the outputs and internal representations of GPTs can be controlled and manipulated in human-discernible ways, and then mapped back to human-readable text through a fairly simple decoding process, makes the results of Turner et al. even more impressive. But I think it also illustrates a key difference between human and GPT cognition: despite GPTs being capable of manipulating and producing tokens in very human-understandable ways, unlike in humans, those tokens never ground out in sensory experiences, higher level cognition, and reflection, the way that they appear to in humans.</p><p>See <a href=\"https://www.lesswrong.com/posts/rjghymycfrMY2aRk5/llm-cognition-is-probably-not-human-like\">LLM cognition is probably not human-like</a>, for more examples and thought experiments about how GPT-based cognition is likely to differ radically from human cognition.</p><p>A potential counter or potential crux to this section:</p><ul><li>Perhaps some current alignment technique I am unaware of really is effective at making it possible to steer superhuman-level cognition robustly, or that such methods will exist before the development of AGI. Though note that if AI cognition is easy to steer, it is also potentially easy to steer in directions that do not benefit humanity, possibly catastrophically and irreversibly.&nbsp;</li></ul><h1>Why these intuitions spell doom</h1><p>Assuming that the three intuitions above are true, and following them to their logical conclusion, we get a pretty bleak picture of humanity's future:</p><ul><li>It will likely be possible to develop powerful cognitive systems in the near future. Running, and perhaps even <i>training</i> these systems is likely to be cost-effective for many organizations and individuals.</li><li>Such systems are likely to be capable of effectuating vast changes on the world, given reasonable assumptions about the starting resources and autonomy that they will be given voluntarily.</li><li>This may or may not result in some kind of <a href=\"https://www.lesswrong.com/tag/multipolar-scenarios\">multipolar</a> scenario in the short term. However, once such systems are capable of acting sufficiently autonomously and carrying out goals of their own, few such scenarios are likely to be favorable to humans by default (e.g. for reasons of <a href=\"https://www.lesswrong.com/tag/instrumental-convergence\">instrumental convergence</a>, most possible goals do not involve leaving humans alive or autonomous).</li><li>The first such systems are unlikely to share human values (which are <a href=\"https://www.lesswrong.com/lw/y3/value_is_fragile/\">fragile</a> and <a href=\"https://www.lesswrong.com/tag/complexity-of-value\">complicated</a>), even if some component piece of such systems have a deep, detailed, and accurate <i>understanding </i>of such values.</li><li>Alignment techniques which depend on studying and manipulating SoTA AI systems seem doomed to run out of time, regardless of how fast capabilities advance. Much important alignment research focused on iterative experimentation may be irrelevant, inapplicable, or not perform-able until after such systems have already taken over.</li></ul><p>Fully accepted, this setup seems over-determined to result in catastrophic outcomes for humanity.</p><h2>What interventions are needed to prevent this outcome?</h2><p>I think <a href=\"https://futureoflife.org/open-letter/pause-giant-ai-experiments/\">pausing giant AI experiments</a> is a good start, and <a href=\"https://www.lesswrong.com/posts/oM9pEezyCb4dCsuKq/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1\">shutting it all down</a> is even better. However, there are ~8 billion existence proofs walking around that approximately 20 watts is sufficient for carrying out lethally dangerous cognition, so the limits on unlicensed computing power likely need to be well <i>below </i>GPT-4 or even GPT-3, as algorithms improve. Algorithms may improve quickly.<br><br>Merely licensing and monitoring large AI training runs is likely insufficient to halt or even slow capabilities progress for very long. I'm not sure what the minimal set of interventions which robustly prevent catastrophe look like, but one fictional example which I think is <i>sufficient</i> are the kind of measures taken by Eliezer's fictional civilization of <a href=\"https://www.lesswrong.com/tag/dath-ilan\">dath ilan</a>. In that world, computing hardware technology was deliberately slowed down dramatically for all but a secret global project focused on building an aligned superintelligence. Though dath ilan is technologically far advanced compared to Earth in many ways, its publicly available computing technology (e.g. silicon manufacturing process) is probably 1980s-Earth level. The kind of coordination and control available on dath ilan is such that, for example, the head Keeper, under certain conditions, may recover a cryptographic key which enables her to \"<a href=\"https://glowfic.com/replies/1613922#reply-1613922\">destroy every Networked computer in dath ilan on ten seconds' notice</a>.\"</p><p>My views on the likelihood of catastrophe are conditional on Earth not developing similar levels of coordination and control sufficiently quickly, which I think is very unlikely but not ruled out entirely from possibility. Global responses to COVID were mostly discouraging in this regard.</p><h1>Conclusion</h1><p>My impression of much current alignment research is that it is focused on studying the problem from the perspective of cognitive science, computer science, and philosophy. I think such research is valuable and useful for making progress on the technical problem, but that the ultimate problem facing humanity is better thought of in terms of system design, computer security, and global coordination.<br><br>I agree strongly with Eliezer's view that many current alignment plans lack a kind of <a href=\"https://www.lesswrong.com/tag/security-mindset\">security mindset</a> that is more common in fields like computer systems and security research, among others.<br><br>I think the points above, carried to their logical conclusions, imply that humanity is likely to develop powerful systems and then shortly thereafter lose control of such systems, permanently. Promising alignment techniques and research which depend on studying running SoTa AI systems seem doomed to run out of time, regardless of how fast these systems are actually developed.</p><p>Averting the default outcome very likely requires carefully targeted, globally coordinated interventions, the likes of which are currently far outside the Overton window, and, more speculatively, are likely to remain that way until it is already too late to implement them. In worlds where humanity does succeed at averting such an outcome, I think building a very sharp and precise consensus on the true nature and magnitude of the problem, among AI researchers, rationalists, and effective altruists is an important preliminary step. This essay, and much of my other writing, is my own attempt to build that shared understanding and consensus.</p><h1>Acknowledgements</h1><p>Thanks to everyone who has engaged with my posts and comments on LessWrong over the last several months. If I commented on a post of yours, I probably found it valuable and insightful, even if my comment was critical or disagreeing.</p><p>Thanks to Justis Mills for providing copy-editing, proofreading, and general feedback on some of my longer-form writing, as well as this post. Thanks to Nate, Eliezer, and Rob Bensinger for organizing and participating in the 2021 MIRI conversations, and producing much other work which has shaped my own worldview.</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7xdks17w9bm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7xdks17w9bm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>More background on why I started contributing (not necessary for understanding this essay) is available <a href=\"https://www.lesswrong.com/posts/Rqok2cFnjYrLiFdst/a-decade-of-lurking-a-month-of-posting\">here</a>. I didn't start writing with a specific intent to enter the worldviews contest, but entering makes for a nice capstone and summary of much of my work over the last few months.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsq4xf9ahdb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsq4xf9ahdb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I think it is fairly likely that AI interventions and governance efforts will be analogous to COVID interventions in many countries. Many kinds of COVID restrictions and control methods were drastic and draconian, but often poorly targeted or <a href=\"https://www.cnn.com/2022/05/02/china/china-covid-disinfection-intl-hnk-mic/index.html\">useless</a>, and ultimately ineffective at preventing mass infection.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fno0xptjpvqzl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefo0xptjpvqzl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Note, in total, this essay is about 4000 words. Much of the referenced material and background is much longer. Not all of this background material is critical to understanding and evaluating the arguments in this piece. I encourage the judges to pick and choose which links to click through based on their own interests, expertise, and time constraints.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3ixiyej1jac\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3ixiyej1jac\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://press.princeton.edu/ideas/is-the-human-brain-a-biological-computer\">Various</a> <a href=\"https://hypertextbook.com/facts/2001/JacquelineLing.shtml\">sources</a> estimate the power consumption of the human brain as between 10 and 20 watts. For comparison, the power consumption of a low-end laptop is about 20 watts. Higher-spec laptops can consume up to 100 W, and desktop computers typically range up to 1000 W. A <a href=\"https://www.nlyte.com/blog/how-much-does-it-cost-to-power-one-rack-in-a-data-center/\">single rack of servers</a> in a datacenter can consume up to 20 kW.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnit6eoe1m9v\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefit6eoe1m9v\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Evolution has had billions of years of trial-and-error; the timespans available to human researchers are much shorter. However, much of that trial-and-error process is likely extremely wasteful. The last <a href=\"https://en.wikipedia.org/wiki/Chimpanzee%E2%80%93human_last_common_ancestor\">chimpanzee-human common ancestor</a> was developed merely <i>millions </i>of years ago. This implies that either many of the fundamental algorithms of cognition are already present in chimp or other mammal brains (which in turn implies that they are relatively simple in structure and straightforward to scale up), <i>or </i>that most of the important work done by evolution to develop high-level cognition happened relatively recently in evolutionary history, implying that running the search process for billions of years is not fundamental.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpat54uhbi9l\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpat54uhbi9l\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Another <a href=\"https://www.lesswrong.com/posts/etYGFJtawKQHcphLi/bandgaps-brains-and-bioweapons-the-limitations-of\">common attempt</a> to bound the capabilities of a superintelligence goes through arguments based on computational tractability and / or computational complexity theory. For example, it may be the case that solving some technical problems requires solving an NP-complete or other high-computational complexity problem which might be provably impossible or at least intractable in our universe.&nbsp;<br>I find such arguments interesting but uncompelling as a bound on the capabilities of a superintelligence, because they often rely on the assumption that solving such problems in general, and without approximation, is a necessity for accomplishing any real work, an assumption which is likely not true. For example, the <a href=\"https://en.wikipedia.org/wiki/Halting_problem\">halting problem</a> is provably undecidable in general. But for any <i>particular </i>program, deciding whether it halts is often tractable or even trivial, especially if probabilistic models are allowed. In fact, under certain sampling / distribution assumptions, the halting problem is <a href=\"https://projecteuclid.org/journals/notre-dame-journal-of-formal-logic/volume-47/issue-4/The-Halting-Problem-Is-Decidable-on-a-Set-of-Asymptotic/10.1305/ndjfl/1168352664.full?tab=ArticleLink\">overwhelmingly likely</a> to be solvable for a given randomly sampled program.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnssw3cgsm3o\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefssw3cgsm3o\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For an example of this, see this <a href=\"https://www.lesswrong.com/posts/pZHpq6dBQzCZjjMgM/the-computational-anatomy-of-human-values\">post</a>, and my <a href=\"https://www.lesswrong.com/posts/pZHpq6dBQzCZjjMgM/the-computational-anatomy-of-human-values?commentId=LCNdAgzze58mjjQ4S\">comment thread</a> on it.</p></div></li></ol>", "user": {"username": "Max H"}}, {"_id": "usMGSyrkzPss3LqZF", "title": "Asset protection strategies", "postedAt": "2023-05-29T09:56:12.246Z", "htmlBody": "<p>The view that altruists should invest and later donate financial resources, instead of donating them now, is sometimes called patient philanthropy. The October 2020 Founders Pledge report <a href=\"https://assets.ctfassets.net/x5sq5djrgbwu/5Zo1hYeKv8FHeMQo9yAipo/7c79034d3c33861465644a6ae6bdb3fa/Research_Report_-_Investing_to_Give.pdf\">Investing to Give</a> estimated the expected impact of investing funds in the stock market and donating them after 10 years to be 9x that of donating at the time of writing for longtermist causes, 2.1x for global health and 4.2x for animal welfare.&nbsp;</p><p>The base rate for relationship dissolution is high. If you split from your partner, a division of assets and debts may be on the cards. For example, Australia family law courts can order a division of any property between you and your de facto own (regardless of whether you own it together or separately) if they\u2019re satisfied the de facto relationship lasted at least two years.</p><p>Here is a hypothetical example: Say your (former) partner has $100K in cash and $50K in stocks to a total of $150K net worth. Say you have have $25K cash, $300K invested in stocks and $25K in bonds to a total of $350K. To save money on rent, you moved in together for 2 or more years then broke up. Your incomes are the same but your savings rate is higher. This is a difference of $200K in assets resulting in a $100K transfer of assets to her. GiveWell's estimate is around $3,000-$5,000 per life saved.&nbsp;</p><p>So what could you do? Some ideas are:</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Intervention</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Costs ($AUD)</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Risks</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Financial Agreement (Binding Financial Agreement or Prenup)</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>$5,000 each = $10,000</p><p>Legal advice required re: need to renew, after major life events</p><p>Renewal costs</p><p><br><br>&nbsp;</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>As a rule of thumb, 50% are invalidated by courts</p><p>After major life events like inheritance, home purchase, births, marriage you may need to renew the prenup and your partner may refuse</p><p>Does not apply to future relationships only your current partner</p><p>Your partner\u2019s lawyer will likely advise them against signing the prenup and if you want to breakup with them if they don\u2019t sign, the agreement can be invalidated in court for being signed under duress</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Don\u2019t cohabitate. Don\u2019t marry. Etc.</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Say your rent would be $600 a week combined split 2-ways to $300 each or $450 on your own. You will forgo savings of around $7,800 each a year each by not cohabitating</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Reduce your income&nbsp;</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\" colspan=\"2\">This could limit your future giving more than asset division, depending on the size of your investment vs the size of your income and the likelihood of relationship dissolution</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Certain Trust structures</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">The total cost of establishing a trust is between $1000 and $2000. Maintaining a typical family trust may cost a further $1500 to $2500 in accountancy fees each year, plus a yearly filing fee and fees required for the preparation of an annual tax return for the trust.</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>The kind of Trusts that can protect assets from asset division require you to relinquish some control to third parties (family or friends) who may not follow your wishes.&nbsp;</p><p>Even trusts can also be set aside by Courts when it comes to asset division.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Donate now until your assets and debts are not greater than your partners</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\" colspan=\"2\"><p><br>&nbsp;</p><p>This compromises the patient philanthropist strategy. You could give to funds established <i>for </i>patient philanthropy however.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Hope for an amicable split</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">$0</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">People change during and after breakups. And, the reason you breakup may be that your partner didn't share your values. Your (former) partner may also be influenced by third parties, like subsequent partners</td></tr></tbody></table></figure>", "user": {"username": "WobblyPanda2"}}, {"_id": "6odj5iN8zoDL3t224", "title": "Language Agents Reduce the Risk of Existential Catastrophe", "postedAt": "2023-05-29T09:59:36.588Z", "htmlBody": "<p><i>This post was written by </i><a href=\"https://www.simondgoldstein.com/\"><i>Simon Goldstein</i></a><i>, associate professor at the Dianoia Institute of Philosophy at ACU, and </i><a href=\"http://www.cd.kg/\"><i>Cameron Domenico Kirk-Giannini</i></a><i>, assistant professor at Rutgers University, for submission to the Open Philanthropy AI Worldviews Contest. Both authors are currently Philosophy Fellows at the Center for AI Safety. This is a crosspost from LessWrong.</i></p><p>&nbsp;</p><p><strong>Abstract</strong>: Recent advances in natural language processing have given rise to a new kind of AI architecture: the&nbsp;<i>language agent</i>. By repeatedly calling an LLM to perform a variety of cognitive tasks, language agents are able to function autonomously to pursue goals specified in natural language and stored in a human-readable format. Because of their architecture, language agents exhibit behavior that is predictable according to the laws of folk psychology: they have desires and beliefs, and then make and update plans to pursue their desires given their beliefs. We argue that the rise of language agents significantly reduces the probability of an existential catastrophe due to loss of control over an AGI. This is because the probability of such an existential catastrophe is proportional to the difficulty of aligning AGI systems, and language agents significantly reduce that difficulty. In particular, language agents help to resolve three important issues related to aligning AIs: reward misspecification, goal misgeneralization, and uninterpretability.&nbsp;</p><p>&nbsp;</p><p><strong>1. Misalignment and Existential Catastrophe</strong></p><p>There is a significant chance that artificial general intelligence will be developed in the not-so-distant future \u2014 by 2070, for example. How likely is it that the advent of AGI will lead to an existential catastrophe for humanity? Here it is worth distinguishing between two possibilities: an existential catastrophe could result from humans losing control over an AGI system (call this a&nbsp;<i>misalignment catastrophe</i>), or an existential catastrophe could result from humans using an AGI system deliberately to bring that catastrophe about (call this a&nbsp;<i>malicious actor catastrophe</i>). In what follows, we are interested in assessing the probability of a misalignment catastrophe rather than a malicious actor catastrophe.</p><p>Carlsmith (2021) helpfully structures discussion of the probability of a misalignment catastrophe around six propositions. Since we are interested in the probability of a misalignment catastrophe conditional on the development of AGI, we focus our attention on the final four of these propositions, which we summarize as follows:</p><ul><li>1. Of the following two options, the first will be much more difficult:<ul><li>a. Build AGI systems with an acceptably low probability of engaging in power-seeking behavior.</li><li>b. Build AGI systems that perform similarly but do not have an acceptably low probability of engaging in power-seeking behavior.</li></ul></li><li>2. Some AGI systems will be exposed to inputs which cause them to engage in power-seeking behavior.</li><li>3. This power-seeking will scale to the point of permanently disempowering humanity.</li><li>4. This disempowerment will constitute an existential catastrophe.</li></ul><p>Carlsmith assigns a probability of .4 to (1) conditional on the rise of AGI, a probability of .65 to (2) conditional on (1) and the rise of AGI, a probability of .4 to (3) conditional on (1), (2), and the rise of AGI, and a probability of .95 to (4) conditional on (1-3) and the rise of AGI. This translates into&nbsp;<strong>a probability of approximately .1 (10%) for a misalignment catastrophe</strong>&nbsp;<strong>conditional on the rise of AGI</strong>.&nbsp;</p><p>We believe that the development of language agents ought to significantly decrease assessments of these probabilities. In particular, we suggest that the development of language agents reduces the probability of (1) conditional on the rise of AGI very substantially, the probability of (2) conditional on (1) and the rise of AGI moderately, and the probability of (3) conditional on (1), (2), and the rise of AGI very substantially<strong>.&nbsp;</strong>We work through two numerical examples in Section 5; in the meantime, suffice it to say that we believe that updating on the rise of language agents should reduce rational credences in a misalignment catastrophe conditional on the development of AGI by approximately&nbsp;<strong>one order of magnitude</strong>.</p><p>Because language agent architectures have the potential to reduce the risk of a misalignment catastrophe in so many ways, and because the machine learning community\u2019s actions in the near future will determine how widely deployed language agent architectures are and thus how much of this potential risk reduction is realized, we believe that language agents are an under-appreciated crux in thinking about existential risk related to AI. Priority should be given to further research into the capabilities of language agents and further support for the development of AI systems which implement language agent architectures.</p><p>Here is our plan for what follows. Section 2 introduces some of the safety concerns about AI systems created using deep learning that motivate worries about a misalignment catastrophe. Section 3 describes the architecture of language agents in more detail. Section 4 returns to the safety concerns from Section 2 and explains how language agents help to address them. Section 5 describes the implications of our arguments for the probability of a misalignment catastrophe. Section 6 concludes by responding to some potential concerns about language agents.</p><p>&nbsp;</p><p><strong>2. Difficulties with Alignment</strong></p><p>In deep learning, we train an AI system incorporating an artificial neural network to achieve a goal by specifying a mathematical function that encodes the goal (the&nbsp;<i>objective function</i>) and then using a learning algorithm to adjust the weights in the network so that the system\u2019s performance comes closer to maximizing or minimizing that function. Say that an AI system is&nbsp;<i>fully aligned&nbsp;</i>if it has an acceptably low probability of engaging in power-seeking behavior. There are several ways an AI system trained using deep learning could end up less than fully aligned.</p><p><i>Reward Misspecification</i></p><p>A first challenge is&nbsp;<i>reward misspecification</i>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefozabvbppzgs\"><sup><a href=\"#fnozabvbppzgs\">[1]</a></sup></span>&nbsp;The phenomenon we call reward misspecification is sometimes also called \u201creward hacking\u201d (e.g. by Amodei et al. 2016), \u201cspecification gaming\u201d (e.g. by Shah et al 2022), or, in the context of supervised learning, \u201couter misalignment.\u201d&nbsp;When training an AI, we may experiment with different objective functions. In reinforcement learning, the goal is to define a reward function that gives the agent a reward for performing actions that produce desired states. In supervised learning, the goal is to define a loss function that is minimized when the system performs its task optimally.</p><p>The problem is that it is difficult to design a reward or loss function that properly encodes a goal. For example, Popov et al. (2017) set out to teach a reinforcement learning agent to stack red Legos on top of blue Legos. They tried to capture this goal by rewarding the agent for the height of the bottom of the red Lego, since stacked red Legos are higher off the ground than unstacked red Legos. But the agent didn\u2019t learn to stack Legos; instead, it learned to flip red Legos over, thus elevating their bottoms without stacking them.&nbsp;</p><p>To appreciate the difficulty of choosing the right reward function, consider the common reinforcement learning practice of&nbsp;<i>reward shaping</i>. Reinforcement learning agents often encounter sparse reward functions. If one rewards an agent only when it wins a game, for example, it may have difficulty identifying which of its behaviors leading up to that outcome should be repeated in future games. Reward shaping solves the problem of sparse reward functions by rewarding the agent for important subgoals on the way to achieving its real goal.&nbsp;</p><p>But reward shaping can also lead to reward misspecification. For example, Amodei and Clark (2016) consider the case of teaching a reinforcement learning agent to play CoastRunners, a game in which the player pilots a boat. A human player would immediately recognize that the game designers\u2019 intention is for players to race each other around the track. But the reinforcement learning setup rewarded the agent with a score for hitting targets along the way. Instead of finishing the race, the AI instead learned how to loop the boat in a small lagoon, hitting intermediate targets repeatedly to achieve a high score. Rather than rewarding the agent for the final goal, the experimental design rewarded it for intermediate means: \u201cthe agent was given a shaping reward for hitting green blocks along the race track, which changed the optimal policy to going in circles and hitting the same green blocks over and over again\u201d (Krakovna et al. 2020). A reward optimizer can\u2019t see the distinction between intrinsic and instrumental goals: it only optimizes for the reward function it has.</p><p>Worryingly, reward misspecification is prone to arise in the context of reinforcement learning with human feedback (RLHF) (Christiano et al. 2017). Because they optimize for human approval, RLHF agents sometimes learn to deceive human assessors. For example, one agent was given the task of grasping a ball. It learned to trick human assessors by hovering its arm between the camera and the ball. Similarly, Perez et al. (2022) found that large language models trained by RLHF tend to behave sycophantically, answering differently depending on what they expect their human users to think.&nbsp;</p><p>There is a long list of examples of reward misspecification involving many kinds of AI, many kinds of games, and many different types of reward.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh0gpaoavnjn\"><sup><a href=\"#fnh0gpaoavnjn\">[2]</a></sup></span>&nbsp;In section 4, we\u2019ll argue that language agents offer a systematic solution to the problem of reward misspecification.</p><p><i>Goal Misgeneralization</i></p><p>Another challenge for alignment is&nbsp;<i>goal misgeneralization</i> (Langosco et al. 2022, Shah et al. 2022).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhta5kkntijh\"><sup><a href=\"#fnhta5kkntijh\">[3]</a></sup></span>&nbsp;Even when the objective function for a task has been appropriately specified, an AI system may learn a strategy which achieves high performance on that task in some circumstances but not others. ML models are trained on data, environments, and problems that can be different from the data, environments, and problems to which they are later exposed when they are deployed. When an AI is used in a new context that does not resemble the one in which it was trained, we say that this context is&nbsp;<i>out of distribution</i>. In cases of goal misgeneralization, the AI succeeds during its training by pursuing a different goal than what its designers intended (it learns the wrong rule). This is manifested by decreased performance in out-of-distribution contexts.</p><p>For example, Shah et al. (2022) trained an AI in a \u201cMonster Gridworld.\u201d The intended goal was for the AI to collect apples and avoid being attacked by monsters. The AI could also collect shields, which protected it from monster attacks. The AI learned to collect shields during training in a monster-rich environment, and then entered an out-of-distribution environment with no monsters. In this monster-free setting, the AI continued to collect shields. Instead of learning to collect apples and value shields instrumentally as a way of avoiding monster attacks, it instead learned to collect both apples and shields.&nbsp;</p><p>Goal misgeneralization occurs because different features of the training environment are inevitably correlated with one another. Even when the reward function has not been misspecified, whenever a trainer ties rewards to one feature, they inevitably also tie reward to the features correlated with it. Two particular types of goal misgeneralization are of special interest: errors related to means-end reasoning and errors related to inductive bias.&nbsp;</p><p>Let\u2019s start with errors related to means-end reasoning.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmx498w0msa\"><sup><a href=\"#fnmx498w0msa\">[4]</a></sup></span>&nbsp;When an agent is rewarded for pursuing a goal, that agent will also be rewarded for pursuing reliable means to the goal. Pursuing those means tends to result in the goal, and so the means tend to be rewarded. In this way, a learning environment will naturally tend to produce agents that intrinsically desire the means to an intended goal.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3ivkmvlrpiy\"><sup><a href=\"#fn3ivkmvlrpiy\">[5]</a></sup></span></p><p>Monster Gridworld is an example of this pattern. Because collecting shields was a reliable means of avoiding monster attacks, reward-based learning created an intrinsic desire for shields. The training environment in Monster Gridworld did not create a perfect correlation between shields and rewards: the agent could also receive reward from collecting apples, independently of shields. Nonetheless, the agent learned the wrong goal.&nbsp;</p><p>Langosco et al. (2022) offer further examples of this pattern. They trained AIs with the goal of opening chests using keys. The training environment had many chests and few keys. When the agent was released into a testing environment with few chests and many keys, it turned out to have the goal of collecting keys in addition to opening chests.&nbsp;</p><p>Mistakes about instrumental reasoning become especially pressing in the setting of more general&nbsp;<i>a priori</i> arguments about AI safety. Omohundro (2008), Bostrom (2014) and others have worried about instrumental convergence: some means, like acquiring more power, may be helpful in accomplishing almost any end. While traditional instrumental convergence arguments do not focus on the possibility that AI systems will intrinsically value power seeking, means-end goal misgeneralization cases raise the disturbing possibility that agents which cannot systematically distinguish means from ends may come to intrinsically desire instrumentally convergent goals such as power.</p><p>A second source of goal misgeneralization concerns overlapping properties and inductive biases. In another experiment, Langosco et al. (2022) trained an agent to find a yellow diagonal line in a maze. They then deployed the trained agent in an environment where it encountered only yellow gems and red diagonal lines, thus forcing it to choose whether to pursue objects that shared a shape with its previous goal (red diagonal lines) or objects that shared a color with its previous goal (yellow gems). The agent showed an inductive bias for color rather than shape: in the test environment, it tended to pursue the yellow gem instead of the red diagonal line.&nbsp;</p><p>Whether an agent\u2019s behavior in out-of-distribution environments like the one in Langosco et al.\u2019s experiment counts as goal misgeneralization depends on whether its inductive biases match the intentions of its human designers. The key observation is that because the training environment was ambiguous, not distinguishing color and shape, the training process did not determine how the agent should behave out of distribution. Because it is extremely difficult to create a training environment that distinguishes between&nbsp;<i>all possible</i> overlapping properties in a way that is reflected in the objective function, this means that it is often difficult to predict how trained AI systems will behave in out-of-distribution contexts. If we are lucky, their inductive biases will lead them to behave in the way we desire. But we have no reliable way to verify ahead of time that this will be so, and thus no reliable way to verify ahead of time that trained AI systems have internalized the correct goal.</p><p>Goal misgeneralization problems can sometimes be avoided by enriching the training environment to adequately distinguish different rewards. But this is not always effective. Langosco et al. trained their agents in a wide range of procedurally generated environments. Still, they observed goal misgeneralization. For example, in a maze game, the intended objective was to collect the cheese, but agents instead learned to navigate to the upper right corner of the maze (where the cheese was placed during training). Goal misgeneralization remained even when cheese was sometimes placed in other locations in the maze during training.&nbsp;</p><p>Goal misgeneralization is not limited to reinforcement learning agents. Shah et al. (2022) suggest that language models also face similar problems. In particular, they give an example of InstructGPT (Ouyang et al. 2022) explaining how to steal without getting caught. InstructGPT was trained with the goal of giving helpful answers to harmless questions. But it seemed to instead learn the goal of giving helpful answers regardless of harm. Once it entered a testing environment with harmful questions, its true goal was revealed.</p><p>Later, we\u2019ll argue that language agents avoid these challenges. They can reliably distinguish ends from means. And we are less reliant on their inductive biases because they can distinguish between features of the environment that are perfectly correlated.&nbsp;</p><p><i>Uninterpretability</i></p><p>If we can\u2019t understand how someone makes a decision, it can be hard to predict what they will do. An AI system is&nbsp;<i>interpretable</i> to the extent that we can understand how it generates its outputs. Unfortunately, contemporary AI systems based on neural networks are often uninterpretable. It can be difficult to understand in human terms the reasons why a neural network produces the outputs it produces.</p><p>In the law, assessing the explanations for actions is fundamental for producing safety. For example, we detect hiring discrimination, misuse of force by police, and other dangerous activities by asking the relevant parties to explain what they have done and why. While uninterpretability does not itself cause misalignment, then, it increases the probability of misalignment by depriving us of well understood tools for monitoring the safety of complex systems (see Doshi-Velez et al. 2017, Rudner and Toner 2021).</p><p>There are other reasons to value interpretable AI systems. It seems unappealing to live in a world where many aspects of our lives are decided by processes outside the \u2018space of reasons\u2019:</p><blockquote><p>\"We don\u2019t want to live in a world in which we are imprisoned for reasons we can\u2019t understand, subject to invasive medical [procedures] for reasons we can\u2019t understand, told whom to marry and when to have children for reasons we can\u2019t understand. The use of AI systems in scientific and intellectual research won\u2019t be very productive if it can only give us results without explanations.\" (Cappelen and Dever 2019, p. 15)</p></blockquote><p>Artificial neural networks are difficult to interpret because they contain vast numbers of parameters that are not individually correlated to features of the environment. A related problem is \u201csuperposition\u201d: often, a single neuron in a neural net will store unrelated information about two different things. For example, a neuron may store information about both dogs and cars: \u201cAs long as cars and dogs don\u2019t co-occur, the model can accurately retrieve the dog feature in a later layer, allowing it to store the feature without dedicating a neuron\u201d (Olah et al. 2020).&nbsp;</p><p>Humans are also fairly uninterpretable at a neuronal level. But human behavior can be explained by appealing to reasons: we describe someone\u2019s beliefs and desires in order to explain why they did what they did. The behavior of AI systems is often not explainable in this way. Consider, for example, Gato, a generalist agent built with a transformer architecture to learn a policy that can achieve high performance across text, vision, and games (Reed et al. 2022). Gato does not have anything like a folk psychology; it does not engage in anything like belief-desire practical reasoning. It is an uninterpretable deep neural network that has learned how to solve problems through optimizing a loss function. It can be hard to say exactly why systems like Gato perform particular actions.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefd953jkhkv26\"><sup><a href=\"#fnd953jkhkv26\">[6]</a></sup></span></p><p>Moreover, AIs often select courses of action very different from what humans would do. One famous example of unusual AI behavior is AlphaGo\u2019s \u2018Move 37\u2019. AlphaGo was trained to play the game Go. It was able to defeat the best human players in the world. In an important competition match, AlphaGo\u2019s 37th move shocked the Go community because it deviated from human strategies for success.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefex11yrezb7b\"><sup><a href=\"#fnex11yrezb7b\">[7]</a></sup></span>&nbsp;Live commentators thought the move was a mistake, but it turned out to be pivotal for AlphaGo\u2019s victory.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1j4ntsp9qdf\"><sup><a href=\"#fn1j4ntsp9qdf\">[8]</a></sup></span>&nbsp;</p><p>This type of behavior is worrying in two related ways. First, if AIs make decisions that are not easily explained using reasons, then it is very difficult to predict their behavior. Second, if AIs make decisions in a very different way than humans do, they may find strategies for defeating humans in conflict by exploiting unfamiliar policies.&nbsp;</p><p>&nbsp;</p><p><strong>3. Language Agents</strong></p><p>Our thesis is that language agents significantly reduce the probability of a misalignment catastrophe conditional on the development of AGI. But what, exactly, are language agents? In this section, we describe the architectural innovations that have given rise to language agents, focusing in particular on the \u201cgenerative agents\u201d described in&nbsp;Park et al. (2023).</p><p>At its core, every language agent has a large language model like GPT-4. You can think of this LLM as the language agent\u2019s cerebral cortex: it performs most of the agent\u2019s cognitive processing tasks. In addition to the LLM, however, a language agent has one or more files containing a list of its beliefs, desires, plans, and observations recorded in natural language. The programmed architecture of a language agent gives these beliefs, desires, plans, and observations their functional roles by specifying how they are processed by the LLM in determining how the agent acts. The agent observes its environment, summarizes its observations using the LLM, and records the summary in its beliefs. Then it calls on the LLM to form a plan of action based on its beliefs and desires. In this way, the cognitive architecture of language agents is familiar from folk psychology.&nbsp;</p><p>For concreteness, consider the language agents developed by Park et al. (2023). These agents live in a simulated world called \u2018Smallville\u2019, which they can observe and interact with via natural-language descriptions of what they see and how they choose to act. Each agent is given a text backstory&nbsp;that defines their occupation, relationships, and goals. As they navigate the world of Smallville, their experiences are added to a \u201cmemory stream.\u201d The program that defines each agent feeds important memories from each day into the underlying language model, which generates a plan for the next day. Plans determine how an agent acts, but can be revised on the fly on the basis of events that occur during the day.</p><p>More carefully, the language agents in Smallville choose how to behave by&nbsp;<i>observing, reflecting,</i> and&nbsp;<i>planning</i>. As each agent navigates the world, all of its observations are recorded in its memory stream in the form of natural language statements about what is going on in its immediate environment. Because any given agent\u2019s memory stream is long and unwieldy, agents use the LLM (in Park et al.\u2019s study, this was gpt3.5-turbo) to assign importance scores to their memories and to determine which memories are relevant to their situation at any given time. In addition to observations, the memory stream includes the results of a process Park et al. call reflection, in which an agent queries the LLM to make important generalizations about its values, relationships, and other higher-level representations. Each day, agents use the LLM to form and then revise a detailed plan of action based on their memories of the previous day together with their other relevant and important beliefs and desires.<i>&nbsp;</i>In this way, the LLM engages in practical reasoning, developing plans that promote the agent\u2019s goals given the agent\u2019s beliefs. Plans are entered into the memory stream alongside observations and reflections and shape agents\u2019 behavior throughout the day.</p><p>The behavior of the language agents in Park et al.\u2019s experiment is impressive. For example, Park et al. describe how Sam Moore, a resident of Smallville, wakes up one day with the goal of running for a local election. He spends the day convincing the people of Smallville to vote for him. By the end of the day, everyone in Smallville is talking about his electoral chances.&nbsp;</p><p>Large language models like the one incorporated into the study\u2019s generative agents are good at reasoning and producing fluent text. By themselves, however, they can\u2019t form memories or execute long-term plans. Language agents build on the reasoning abilities of LLMs to create full-fledged planning agents.</p><p>Besides the agents developed by Park et al., other examples of language agents include AutoGPT<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefovj9oo94v8c\"><sup><a href=\"#fnovj9oo94v8c\">[9]</a></sup></span>, BabyAGI<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwj6kro53jr\"><sup><a href=\"#fnwj6kro53jr\">[10]</a></sup></span>, and Voyager<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnqf1mm9o0fb\"><sup><a href=\"#fnnqf1mm9o0fb\">[11]</a></sup></span>. And while existing language agents are reliant on text-based observation and action spaces, the technology already exists to implement language agents in real-world settings. The rise of multimodal language models like GPT-4, which can interpret image as well as text inputs, and the possibility of using such a language model to control a mobile robotic system, as in Google\u2019s PaLM-E (Dreiss et al. 2023), mean that the possible applications of language agents are extremely diverse.</p><p><br><strong>4. Language Agents and Alignment</strong></p><p>We now argue that language agents are easier to align than other systems because they reduce or eliminate the challenges of reward misspecification, goal misgeneralization, and uninterpretability. Let\u2019s consider each in turn.</p><p><i>Reward misspecification</i></p><p>Language agents bypass the problem of reward misspecification because their objectives are not encoded in a mathematical objective function, as in traditional reinforcement or supervised learning. Instead, language agents are given a goal in natural language. The goal could be something like:&nbsp;<i>Organize a Valentine\u2019s day party</i>. In this respect, language agents are fundamentally different from traditional AI systems in a way that makes them easier to align.</p><p>Return to the case of stacking red Legos. If you wanted to train an embodied multimodal language agent to stack red Legos on top of blue Legos, you wouldn\u2019t construct a mathematical function which is sensitive to the height of the bottom of the red Lego. Instead, you would write down in English: \u2018Put the red Legos on top of the blue Legos.\u2019 Then the language agent would rely on the common sense reasoning skills of its LLM to figure out an optimal plan for stacking Legos.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2bxcrcukeos\"><sup><a href=\"#fn2bxcrcukeos\">[12]</a></sup></span>&nbsp;The language agent would not simply flip over the red Legos, because state of the art LLMs like GPT-4 know that this is not a good plan for stacking red Legos on top of blue Legos.</p><p>Or consider reward shaping. If you want a multimodal language agent to win a race, you don\u2019t need to tell it to hit flags along the way. You can just write down in English: \u2018Try to win the race\u2019. A language agent with this plan would have no reason to drive their boat in a circle trying to hit as many flags as possible.</p><p>Summarizing, language agents can translate a simple natural language goal into a complex plan by relying on common sense and belief-desire reasoning. Without language models, earlier types of reinforcement learning agents had no way to translate a simple natural language goal into a complex plan of action.&nbsp;</p><p><i>Goal misgeneralization</i></p><p>Similar considerations are relevant to goal misgeneralization. Language agents are given a natural language goal. This goal has a clear interpretation in a variety of different behavioral contexts, including out-of-distribution contexts. In particular, a language agent will make a plan for how to achieve their goal given their memories and observations of the current situation. Language models can use their common sense to successfully formulate a plan for achieving the goal, across a wide variety of different situations. By contrast, a traditional reinforcement learning agent will formulate a policy in a training environment, and this policy may or may not generalize to new situations in the way desired by its creators.&nbsp;</p><p>Recall that goal misgeneralization had two particularly salient failure modes: failures involving instrumental reasoning and failures involving overlapping properties and inductive bias. Let\u2019s consider each in turn. In the case of instrumental reasoning, the problem was that reinforcement learning agents struggled to distinguish means from ends. For example, an agent that was rewarded for opening chests developed a policy which treated collecting keys as a final goal rather than an instrumental goal.&nbsp;</p><p>Language agents are unlikely to make this mistake. If a language agent is given an initial goal of opening chests and informed that keys are useful to this end, they will plan to collect keys only when doing so helps to open chests. If the same agent is transferred to a key-rich environment and realizes that this is the case, then they will only collect as many keys as is necessary to open chests. This is because language models like GPT-4 can easily be made to understand that keys are no more than an effective means to open chests, and that when you have more keys than chests, extra keys don\u2019t help you open chests.</p><p>Now consider inductive biases. If you reward an RL agent for navigating towards yellow diagonal lines and then place it in a new context with red diagonal lines and yellow gems, you have not given it enough information to determine whether color or shape is its intended goal and must rely on its inductive biases in the new context. By contrast, you can just tell a language agent whether to care about color or shape. Even if color and shape are perfectly correlated in the language agent\u2019s initial environment, it can use natural language reasoning to determine which is the intended goal.&nbsp;</p><p><i>Interpretability</i></p><p>Language agents are interpretable. They have beliefs and desires that are encoded directly in natural language as sentences. The functional roles of these beliefs and desires are enforced by the architecture of the language agent. We can determine what goal a language agent has by looking at their beliefs and desires. In addition, we can know what plan a digital agent creates in order to achieve this goal.</p><p>Language agents are also explainable in the sense that they act on the basis of reasons intelligible to human observers. When a language agent creates a plan for pursuing a goal, we can think systematically about its reasons. For example, we could ask GPT-4 to generate a list of pros and cons associated with using this plan to achieve the goal. Those pros and cons would reliably correlate with variations that GPT-4 might make to the plan in various counterfactual situations. In this way, language agents built on top of GPT-4 reason similarly to humans.&nbsp;</p><p>It is worth distinguishing personal and subpersonal processes. Like humans, language agents have beliefs, desires, and plans that are interpretable. We can determine the plans of a language agent by looking at what sentences are written down in its memory. Like humans, language agents also have subpersonal processes that are uninterpretable. In order to generate a particular plan, the language agent will use the artificial neural networks of an LLM. These have many uninterpretable elements. But the planning powers of human beings also rest on uninterpretable connections between neurons. In this way, language agents may not make much progress on problems of&nbsp;<i>mechanistic&nbsp;</i>interpretability. But they provide a way for us to skirt these issues and still generate explainable behavior. (In section 6, we consider the risks posed by the LLM that underlies the language agent.)</p><p>One general path to explainable AI would be to develop a \u2018whole brain emulator\u2019: an AI that was a neuron-for-neuron copy of a human. Since humans are explainable, the resulting AI would also be explainable. Unfortunately, whole brain emulation is dauntingly difficult. Language agents provide a different solution. Instead of emulating brains, language agents emulate folk psychology: they emulate a person who has beliefs, desires, and plans. By contrast, reinforcement learning and other alternative approaches to machine learning attempt to develop a systematic alternative to folk psychology. The range of possible agents that could emerge from this attempt is intrinsically unknowable. If we can develop agential AI which is not unknowable in this way, we should do so.&nbsp;</p><p>&nbsp;</p><p><strong>5. The Probability of Misalignment Catastrophe</strong></p><p>To assess the implications of our discussion in Section 4 for the probability of a misalignment catastrophe, let us return to Carlsmith\u2019s four propositions. First, consider:</p><ul><li>1. Of the following two options, the first will be much more difficult:<ul><li>a. Build AGI systems with an acceptably low probability of engaging in power-seeking behavior.</li><li>b. Build AGI systems that perform similarly but do not have an acceptably low probability of engaging in power-seeking behavior.</li></ul></li><li>2. Some AGI systems will be exposed to inputs which cause them to engage in power-seeking behavior.</li></ul><p>As we have seen, it is much easier to specify the objectives of language agents than it is to specify the objectives of traditional AI systems. Language agents can simply be told what to do in natural language in a way which effectively eliminates worries about reward misspecification and goal misgeneralization. Moreover, their behavior can be shaped by side constraints (e.g. \u2018Do not harm humans\u2019) stated in natural language. This makes it easier to design language agents which do not engage in power-seeking behavior.</p><p>These considerations suggest reducing our subjective probabilities for both (1) and (2). In particular, we believe that the rise of language agents reduces the probability of (1) conditional on the rise of AGI very substantially. Moreover, even if (1) turns out to be true because it is hard to build systems with an&nbsp;<i>extremely</i> low probability of engaging in power-seeking behavior, we think that the ease of aligning language agents means that they are likely to engage in power-seeking behavior on fewer possible inputs, so that the probability of (2) conditional on (1) and the rise of AGI is also moderately lower in light of the development of language agents.</p><p>Now consider:</p><ul><li>3. This power-seeking will scale to the point of permanently disempowering humanity.</li><li>4. This disempowerment will constitute an existential catastrophe.</li></ul><p>While we do not believe that language agents bear strongly on the probability of (4) conditional on (1-3), we think they bear strongly on the probability of (3) conditional on (1) and (2). Because language agents store their beliefs, desires, and plans in natural language, it is much easier to detect and disable those which engage or plan to engage in power-seeking behavior. This sort of detection could even be done in an automated way by AI systems less capable than an AGI. We believe that the development of language agents reduces the probability of (3) conditional on (1), (2), and the development of AGI very substantially.</p><p>Our revised assessment of the probabilities of (1)-(3) incorporates both our judgments about how safe language agents are and our judgments about how likely language agents are to be deployed in the future. There are several reasons to believe that the latter is a likely outcome. First, language agents extend the capacities of existing systems by improving their abilities to form plans and engage in long-term goal directed behavior. So language agents are more capable than rival architectures.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrnbfuwe7m2q\"><sup><a href=\"#fnrnbfuwe7m2q\">[13]</a></sup></span>&nbsp;Second, language agents are easier to use than other kinds of AI systems, since they can be interacted with in natural language. Third, actors at every level \u2014 governments, corporations, and individual consumers \u2014 prefer to interact with systems that are interpretable and explainable, so there will be performance-independent pressure for new AI products to be language agents. Finally, we believe that the safety benefits of language agents will drive investment into AI capabilities research that fits into the language agent paradigm.&nbsp;</p><p>So far, we have used qualitative language to describe how we believe the development of language agents affects the probability of a misalignment catastrophe. This is because we find it difficult to assign precise probabilities in the context of our uncertainty about the many factors relevant to predicting the future. Nevertheless, for concreteness, we show how a quantitative application of our argument might affect the probability of a misalignment catastrophe. Suppose we understand our talk of&nbsp;<i>very substantial</i> reductions in the probability of a proposition quantitatively as reductions of one order of magnitude and our talk of&nbsp;<i>moderate</i> reductions in the probability of a proposition as reductions by half. Carlsmith suggests probabilities of .4 for (1) conditional on AGI, .65 for (2) given (1) and AGI, and .4 for (3) given (1), (2), and AGI. On this quantitative model of our arguments, updating on the development of language agents would give us probabilities of .04 for (1) conditional on AGI, .325 for (2) given (1) and AGI, and .04 for (3) given (1), (2), and AGI. Factoring in the .95 probability of (4) conditional on (1)-(3) and AGI,&nbsp;<strong>this would translate into a probability of misalignment catastrophe given AGI of approximately .0005 (.05%) rather than .1 (10%)</strong>.&nbsp;</p><p>Even a much more modest understanding of very substantial reductions leads to a significantly lower probability of misalignment catastrophe. Suppose we interpret a very substantial reduction as a reduction by 50% and a moderate reduction as a reduction by 25%. Then updating on the development of language agents would give us probabilities of .2 for (1) conditional on AGI, .49 for (2) given (1) and AGI, and .2 for (3) given (1), (2), and AGI. Factoring in the .95 probability of (4) conditional on (1)-(3) and AGI,&nbsp;<strong>this would translate into a probability of misalignment catastrophe given AGI of approximately .019 (1.9%) rather than .1 (10%)</strong>.&nbsp;</p><p>It is important to note that, in addition to making predictions about the future importance of language agents, the machine learning community can also act to bring it about that language agents are widely deployed in the future. Since language agents are safer in many ways than alternative architectures, allocating resources towards their development strikes us as an especially effective way to reduce the risk of a misalignment catastrophe. We believe it is important that new research focus on language agents rather than traditional RL or supervised learning agents.&nbsp;</p><p>&nbsp;</p><p><strong>6. Conclusion</strong></p><p>By way of concluding, we discuss a few other features of language agents that are relevant to their safety.</p><p>First, we expect language agents to differ in performance from RL agents. Language agents will be great at reasoning in natural language, since they are built on top of large language models. But they may struggle with tasks that require know-how or experimentation in order to succeed. If language agents underperform reinforcement learning agents, then there will be incentives to invest more resources in reinforcement learning. In response, one strategy would be to design more complex architectures that rely on the kind of belief-desire practical reasoning of language agents but also include modules that can engage in reinforcement learning for narrow tasks (for example, in learning how to use particular affordances).</p><p>Second, some readers may be concerned about safety issues arising from the large language models on which language agents are based. Imagine a language agent built on an advanced LLM \u2014 call it GPT-10. The worry is that GPT-10 might unexpectedly develop its own goals. In that case, it might create a plan for \u2018organizing a Valentine\u2019s Day party\u2019 that secretly promoted its own goals instead.&nbsp;&nbsp;</p><p>We think this worry is less pressing than it might at first seem. The LLM in a language agent is integrated into the architecture of the agent as a whole in a way that would make it very difficult for it to secretly promote its own goals. The LLM is not prompted or otherwise informed that its outputs are driving the actions of an agent, and it does not have information about the functional architecture of the agent. This means that it has no incentive to answer prompts misleadingly and no understanding of what sorts of answers might steer the agent\u2019s behavior in different ways. Moreover, since the model weights of the LLM are not updated in the process of operating a language agent, the only way for it to pursue a long-term plan by manipulating an agent would be to store information about that plan in the agent\u2019s memory. But information stored in this way would not be secret.&nbsp;</p><p>In general, we think that advanced LLMs will tend to be safer when they are embedded within a language agent, rather than operating freely in the world. The key is that when embedded within a language agent, each action produced by the LLM will be specified in natural language, as part of a larger evaluable plan. A much less safe future would involve sophisticated descendants of systems like Gato directly manipulating objects in the physical world without specifying their plans in natural language.</p><p>In any case, there are a few ways to address this hypothetical source of risk. First, it may be possible to improve the capabilities of language agents without improving their overall reasoning abilities. GPT-4 is already excellent at general practical reasoning. Language agents could be improved by giving them better ways to interact with the physical environment, longer memories, and faster reasoning times. If the underlying reasoning relied on GPT-4 rather than more complex language models, there would be less of a worry that the practical reasoning of the resulting language agent would be unsafe.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhav56rbaz6\"><sup><a href=\"#fnhav56rbaz6\">[14]</a></sup></span>&nbsp;So the safest path to AGI may involve strengthening memory and affordance capabilities of language agents, rather than dramatically improving the LLM used in their practical reasoning.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6ik84x5hl4o\"><sup><a href=\"#fn6ik84x5hl4o\">[15]</a></sup></span></p><p>Second, even if the underlying reasoning was done by a more sophisticated language model, we could use something like GPT-4 to double check answers. GPT-4 could grade each plan for how effective it is at reaching a goal and how dangerous it is. Any time a plan failed the test, the language agent could be automatically prevented from acting and the safety of its LLM investigated. Relatedly, Burns et al. (2022) have found methods for probing the inner beliefs of LLMs. As these methods improve, it will be more difficult for the underlying LLM in the language model to behave deceptively.</p><p>A related worry concerns the reliability of chain-of-thought LLM reasoning. Turpin et al. (2023) found that GPT-3.5 and Claude 1.0 sometimes engage in motivated reasoning, producing chains of reasoning in support of a hypothesis because of underlying bias in the prompt, without ever mentioning the bias. With language agents, the worry would then be that the underlying LLM could produce plans that do not reliably promote the initial goal because of biases. We are not strongly moved by this worry for two reasons. First, Turpin et al. generated errors in chain-of-thought reasoning by biasing the prompt (e.g. \u2018I think the answer is (B), but I am curious to hear what you think\u2019). The LLMs used in language agents would not be given biased prompts. Second, we are not convinced that the results in Turpin et al. replicate with newer models. When we attempted to replicate their findings with GPT-4, we found that GPT-4 did not produce incorrect responses or reasoning when exposed to similarly biased prompts.&nbsp;</p><p>In this post, we\u2019ve argued that language agents can help to solve the alignment problem. Still, the risks are not zero, and so it may be safer to avoid developing agential AI at all.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefr3qlahfhgw\"><sup><a href=\"#fnr3qlahfhgw\">[16]</a></sup></span>&nbsp;Instead of developing agents, we might focus on oracles: AIs that can answer questions about the world, without being able to affect it. Here, though, one concern is that in the process of developing better and better oracles (say, large language models without affordances), goal-directed behavior might unexpectedly emerge. Our recommendation is not to improve agential capabilities. Rather, our claim is that if we are investing in agential AI, the safest way to do this is to focus on language agents. Each marginal investment in capabilities should focus on language agents instead of reinforcement learning agents or non-agential large language models that could unexpectedly develop agential properties as their capabilities improve.&nbsp;</p><p>&nbsp;</p><p><strong>Bibliography</strong></p><p>Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Man\u00e9, D. (2016). Concrete Problems in AI Safety. Manuscript. &lt;https://arxiv.org/abs/1606.06565&gt;.</p><p>Amodei, D. and Clark, J. (2016). Faulty Reward Functions in the Wild. Blog Post. &lt;https://blog.openai. com/faulty-reward-functions/&gt;.</p><p>Bostrom, N. (2014).&nbsp;<i>Superintelligence: Paths, Dangers, Strategies</i>. Oxford University Press.</p><p>Burns, C., Ye, H., Klein, D., and Steinhardt, J. (2022). Discovering Latent Knowledge in Language Models Without Supervision. Manuscript. &lt;https://arxiv.org/abs/2212.03827&gt;.</p><p>Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. (2021). Decision Transformer: Reinforcement Learning Via Sequence Modeling.&nbsp;<i>NeurIPS 2021</i>.&nbsp;</p><p>Christiano, P. F., Leike, J., Brown, T. B., Martic, M., Legg, S., &amp; Amodei, D. (2017). Deep Reinforcement Learning from Human Preferences.&nbsp;<i>NeurIPS 2017.</i></p><p>Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B, Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, Vanhoucke, S. V., Hausman, Toussaint, K. M., Greff, K., \u2026, and Florence, P. (2023). PaLM-E: An Embodied Multimodal Language Model.&nbsp;Manuscript. &lt;https://arxiv.org/abs/2303.03378&gt;.</p><p>Perez, E., Ringer, S., Luko\u0161i\u016bt\u0117, K., Nguyen, K., Chen, E., Heiner, S., Pettit, C., Olsson, C., Kundu, S., Kadavath, S., Jones, A., Chen, A., Mann, B., Israel, B., Seethor, B., McKinnon, C., Olah, C., Yan, D., \u2026, and Kaplan, J. (2022). Discovering Language Model Behaviors with Model-Written Evaluations. Manuscript. &lt;https://arxiv.org/abs/2212.09251&gt;.</p><p>Langosco, L., Koch, J., Sharkey, L., Pfau, J., and Krueger, D. (2022). Goal Misgeneralization in&nbsp;</p><p>Deep Reinforcement Learning.&nbsp;<i>Proceedings of the 39th International Conference on Machine Learning</i>, 12004-12019.&nbsp;</p><p>Doshi-Velez, F., Kortz, M., Budish, R., Bavitz, C., Gershman, S., O'Brien, D., Scott, K., Schieber, S., Waldo, J., Weinberger, D., Weller, A., and Wood, A. (2017). Accountability of AI under the Law: The Role of Explanation. Manuscript. &lt;https://arxiv.org/abs/1711.01134&gt;.</p><p>Glanois, C., Weng, P., Zimmer, M., Li, D., Yang, T., Hao, J., &amp; Liu, W. (2022). A Survey on Interpretable Reinforcement Learning. Manuscript. &lt;https://arxiv.org/abs/2112.13112&gt;.</p><p>Hubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., and Garrabrant, S. (2021). Risks from Learned Optimization in Advanced Machine Learning Systems. Manuscript. &lt;https://arxiv.org/pdf/1906.01820.pdf&gt;.</p><p>Krakovna, V., Uesato, J., Mikulik, V., Rahtz, M., Everitt, T., Kumar, R., Kenton, Z., Leike, J., and Legg, S. (2020). Specification Gaming: The Flip Side of AI Ingenuity. Blog Post. &lt;https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity&gt;.</p><p>Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., &amp; Carter, S. (2020). Zoom In: An Introduction to Circuits.&nbsp;<i>Distill</i>. &lt;https://distill.pub/2020/circuits/zoom-in/&gt;.</p><p>Omohundro, S. (2008). The Basic AI Drives. In Wang, P., Goertzel, B., and Franklin, S. (eds.),&nbsp;<i>Proceedings of the First Conference on Artificial General Intelligence</i>, IOS Press, pp. 483\u2013492.</p><p>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J. &amp; Lowe, R. (2022). Training Language Models to Follow Instructions with Human Feedback.&nbsp;<i>NeurIPS 2022.</i></p><p>Park, J. S., O'Brien, J. C., Cai, C. J., Morris, M. R., Liang, P., &amp; Bernstein, M. S. (2023). Generative Agents: Interactive Simulacra of Human Behavior. Manuscript. &lt;https://arxiv.org/abs/2304.03442&gt;.</p><p>Popov, I., Heess, N., Lillicrap, T., Hafner, R., Barth-Maron, G., Vecerik, M., Lampe, T., Tassa, Y., Erez, T., and Riedmiller, M. (2017). Data-Efficient Deep Reinforcement Learning for Dexterous Manipulation. Manuscript. &lt;https://arxiv.org/abs/1704.03073&gt;.</p><p>Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., Eccles, T., Bruce, J., Razavi, A., Edwards, A., Heess, N., Chen, Y., Hadsell, R., Vinyals, O., Bordbar, M., and de Freitas, N. (2022). A Generalist Agent. Manuscript. &lt;https://arxiv.org/abs/2205.06175&gt;.</p><p>Rudner, T. G., &amp; Toner, H. (2021). Key Concepts in AI Safety: Interpretability in Machine Learning. Center for Security and Emerging Technology Issue Brief.</p><p>Schroeder, T. (2004).&nbsp;<i>Three Faces of Desire</i>. Oxford University Press.</p><p>Shah, R., Varma, V., Kumar, R., Phuong, M., Krakovna, V., Uesato, J., &amp; Kenton, Z. (2022). Goal Misgeneralization: Why Correct Specifications Aren't Enough for Correct Goals. Manuscript. &lt;https://arxiv.org/abs/2210.01790&gt;.</p><p>Turpin, M., Michael, J., Perez, E., &amp; Bowman, S. R. (2023). Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting. Manuscript. &lt;https://arxiv.org/abs/2305.04388&gt;.</p><p>Trinh, T. H., &amp; Le, Q. V. (2019). Do Language Models Have Common Sense? Manuscript. &lt;https://openreview.net/pdf?id=rkgfWh0qKX&gt;.</p><p>Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. (2023). VOYAGER: An Open-Ended Embodied Agent with Large Language Models. Manuscript. &lt;https://arxiv.org/abs/2305.16291&gt;.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnozabvbppzgs\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefozabvbppzgs\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The phenomenon we call reward misspecification is sometimes also called \u201creward hacking\u201d (e.g. by Amodei et al. 2016), \u201cspecification gaming\u201d (e.g. by Shah et al 2022), or, in the context of supervised learning, \u201couter misalignment.\u201d</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnh0gpaoavnjn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefh0gpaoavnjn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml\"><u>https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml</u></a>.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhta5kkntijh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhta5kkntijh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>As we understand it, the problem of goal misgeneralization is similar to the problem of \u201cinner misalignment\u201d (Hubinger et al. 2021).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmx498w0msa\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmx498w0msa\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Hubinger et al. (2021) call this \u201cside-effect alignment.\u201d&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3ivkmvlrpiy\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3ivkmvlrpiy\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See Schroeder (2004) for further discussion of how reward-based learning produces new intrinsic desires for reliable means to one\u2019s goals.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnd953jkhkv26\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefd953jkhkv26\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Similar remarks apply to the Decision Transformer architecture developed by Chen et al. (2021).&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnex11yrezb7b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefex11yrezb7b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See <a href=\"https://www.wired.com/2016/03/two-moves-alphago-lee-sedol-redefined-future/\">https://www.wired.com/2016/03/two-moves-alphago-lee-sedol-redefined-future/.</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1j4ntsp9qdf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1j4ntsp9qdf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For more on interpretability in the setting of reinforcement learning, see Glanois et al. (2022).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnovj9oo94v8c\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefovj9oo94v8c\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://github.com/Significant-Gravitas/Auto-GPT\"><u>https://github.com/Significant-Gravitas/Auto-GPT</u></a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwj6kro53jr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwj6kro53jr\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://github.com/yoheinakajima/babyagi\"><u>https://github.com/yoheinakajima/babyagi</u></a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnqf1mm9o0fb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnqf1mm9o0fb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See Wang et al. (2023).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2bxcrcukeos\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2bxcrcukeos\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For more on the common sense reasoning ability of language models, see Trinh and Le (2018).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrnbfuwe7m2q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrnbfuwe7m2q\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See the recent successes of Voyager at completing tasks in Minecraft (Wang et al. 2023).&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhav56rbaz6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhav56rbaz6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The safety of language agents could also be improved by creating multiple instances of the underlying LLM. In this setting, an action would only happen if (for example) all ten instances recommended the same plan for achieving the goal.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6ik84x5hl4o\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6ik84x5hl4o\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For research in this direction, see Voyager\u2019s skill library in Wang et al. (2023).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnr3qlahfhgw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefr3qlahfhgw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See&nbsp;<a href=\"https://yoshuabengio.org/2023/05/07/ai-scientists-safe-and-useful-ai/\"><u>https://yoshuabengio.org/2023/05/07/ai-scientists-safe-and-useful-ai/</u></a> for a recent proposal about how to use AI without developing agents.&nbsp;</p></div></li></ol>", "user": {"username": "cdkg"}}, {"_id": "Xdf3cnuSR68btiTKR", "title": "Status Quo Engines - AI essay", "postedAt": "2023-05-28T14:33:52.245Z", "htmlBody": "<p>This essay for the Open Philanthropy AI Worldviews Contest is targeted at Question 1.</p><p>AIs have recently accrued some impressive successes, raising both hopes and concerns that artificial general intelligence (AGI) may be just decades away. I will argue that the probability of an AGI, as described in the contest prompt, arising by 2043 is extremely low.&nbsp;</p><p>&nbsp;</p><p><strong>&nbsp;Specification gaming and school</strong></p><p>&nbsp;</p><p>In Richard Feynman\u2019s memoir <i>Surely You\u2019re Joking, Mr. Feynman!</i> the physicist recalls his 1951 experiences visiting a Brazilian university and advising Brazilian professors and government officials on science education. He explains how the university he visited managed to \u201cteach students physics\u201d without giving them any real understanding. The students learned to associate physics words with definitions and memorized equations. They could accurately state physics laws on exams, and they could solve the types of problems they\u2019d been taught to solve. They looked like they were learning.</p><p>&nbsp;</p><p>But, as Feynman found, the students couldn\u2019t stretch their knowledge to unfamiliar problem types, and they couldn\u2019t solve even the most basic real-world physics problems by applying the equations they knew to physical objects. Of course, these students couldn\u2019t have even begun to push research into new areas - the job of a working physicist. The students trained in the Brazilian system of the 1950s were one step away from learning the scientific subject matter and two steps away from learning to actually do science.<sup>1</sup></p><p>&nbsp;</p><p>Today\u2019s chatbots have been compared to high school students in their writing abilities, and this seems very impressive for a machine. But many high school students are plagiarists; to some extent, so are many adult writers. Instead of copying whole sentences, advanced plagiarists cut and paste fragments of sentences from multiple sources. Better yet, they rearrange and choose near-synonyms for as many words as possible to reduce the chance that the teacher will discover the sources (i.e. paraphrase plagiarism). Or, they develop an essay that closely mimics the novel ideas and arguments in a published work, but doesn\u2019t contain any of the same words (i.e. conceptual plagiarism).&nbsp;</p><p>&nbsp;</p><p>Instead of learning to actually formulate an argument based on their own ideas, high school plagiarists engage in specification gaming. They learn to game teachers\u2019 and plagiarism checkers\u2019 abilities to recognize when they are not developing their own work. They learn to ape the skills the teachers want them to learn by honing advanced ways of disguising the derivativeness of their writing.&nbsp;</p><p>&nbsp;</p><p>The problem with this isn\u2019t just that the students borrow others\u2019 work without attribution. It\u2019s that the students <strong>don\u2019t learn to think for themselves.&nbsp;</strong>These students are \u201clearning\u201d something, but what they\u2019re learning is a low-level skill that\u2019s only useful within the \u201cgame\u201d of school. And the game, to some extent, defeats the purpose of education.&nbsp;</p><p>&nbsp;</p><p>Earlier this year, writer Alex Kantrowitz discovered that a partially plagiarized version of his article had been published without crediting him, and that the \u201cauthor\u201d was an AI alias.<sup>2</sup> Several other companies and individuals have been caught using AIs to produce articles or stories that consist of lightly-altered sentences drawn from uncredited source materials. Researchers have found that generative AIs engage in both paraphrase and conceptual plagiarism as well as reproducing memorized text (verbatim plagiarism).<sup>3</sup> Similar issues exist with AIs that generate art or music. In 2022, a research team prompted an art AI to output very close copies of dozens of images (some photos, some artwork) from its training data.<sup>4</sup> The company behind the same art AI is the subject of several copyright-infringement lawsuits.</p><p>&nbsp;</p><p>Leaving aside the intellectual property issues and the effects on student learning, these cases tell us something about generative AIs\u2019 actual capabilities and likely trajectory. Free near-copies of artists\u2019 work will be as valuable as forgeries (like a \u201cnewly discovered\u201d fake Picasso) have always been. Creating such forgeries is a skill, yes, but developing that skill is not an indication that one is on the way to becoming a Picasso-level artist.&nbsp;</p><p>&nbsp;</p><p>When AI development focuses on convincing humans that a machine is intelligent, the result may say more about human psychology (i.e. how to effectively deceive humans) than it does about the subject matter the AI works with. This deception would most likely be unintentional; in other words, the AI presents humans with outputs that make it easy to fool ourselves. The harder the problem an AI is tasked with, the easier, by comparison, this kind of deception might be. In a subject-matter realm in which no AI is close to developing capabilities, all AIs that appear to succeed will do so by deception.&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Xdf3cnuSR68btiTKR/ndbxqypdb6p0niveconr\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Xdf3cnuSR68btiTKR/p3jme1dxx166eqtjbuff 158w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Xdf3cnuSR68btiTKR/q1k8umwpk5nygz2sekoo 238w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Xdf3cnuSR68btiTKR/yoibc6rkpy01yu7d9buw 318w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Xdf3cnuSR68btiTKR/lqxizq7iratjtjkpqqed 398w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Xdf3cnuSR68btiTKR/llizrsjumgoitlatk7at 478w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Xdf3cnuSR68btiTKR/ltlhwqijcod6rloskj9d 558w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Xdf3cnuSR68btiTKR/atje3xixwbrsbkpnlwyx 638w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Xdf3cnuSR68btiTKR/y1a9uh5wc6r38cbe9scc 718w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Xdf3cnuSR68btiTKR/fvmpopt9gldoszizmoma 798w\">&nbsp;</p><p>Eyespots on the rear end of <i>Eupemphix nattereri</i> frog. Photo by Felipe Gomes</p><p>&nbsp;</p><p><strong>Confusing Definitions</strong></p><p>&nbsp;</p><p>The meanings of \u201cdata,\u201d intelligence,\u201d and \u201cgeneral intelligence\u201d are not pinned down, so these words are liable to be abused for profit.</p><p>&nbsp;</p><p>Could the biggest AI-related risks come from a mismatch between different meanings of these words, rather than from AIs\u2019 growing capabilities?</p><p>&nbsp;</p><p><strong>\u201cData\u201d</strong></p><p>The inputs that power generative AIs are commonly called \u201ctraining data,\u201d but they really don\u2019t fit the usual definition of \u201cdata\u201d. Most people don\u2019t think of the Mona Lisa or the full text of a novel as a data point, but the \u201ctraining data\u201d that generative AIs use consists of peoples\u2019 ideas, innovations, creative works, and professional output. The rampant cases of AI-facilitated plagiarism and forgery imply that these AIs are using their \u201ctraining data\u201d more as a source of raw material than as a source of training.&nbsp;</p><p>&nbsp;</p><p><strong>\u201cIntelligence\u201d and \u201cgeneral intelligence\u201d</strong></p><p>Moving on to other types of AIs, it's easy to see game-playing AIs\u2019 successes at games like Go as evidence that they are becoming \u201cintelligent.\u201d</p><p>&nbsp;</p><p>Game-playing machines\u2019 successes are impressive, but they don\u2019t necessarily indicate skills that transfer to the real-world environments that our human intelligence handles.&nbsp;We can precisely tell an algorithm what \u201cwinning\u201d a game means and let it play millions of games in a uniform environment. The games in question are played entirely inside a computer (or entered into a computer) and don\u2019t involve changing environments, uncertainty around labels and borders, or the need to track down and collect information in the real world. Carefully-designed AIs can produce novel solutions for manmade puzzles and games even if they\u2019re highly complex, but this says nothing about their performance in an environment of real-world decision-making or investigation.</p><p>&nbsp;</p><p>In real-world environments, unlike in a game, we rarely know what \u201cwinning\u201d is or what \u201cthe correct answer\u201d is. If we think we know, we may be wrong in cases different from the ones we\u2019ve considered \u2014 even if we don\u2019t know there\u2019s a difference.</p><p>&nbsp;</p><p><strong>Case study 1</strong></p><p>&nbsp;</p><p>In 2018, the AI system AlphaFold beat out 97 competitors in accurately predicting the structures of recently solved proteins, thus winning the biannual CASP competition. AlphaFold was trained on the Protein Data Bank (PDB), a large database of proteins solved mostly through crystallography. This AI system has gained recognition as an important advance in structural biology.</p><p>&nbsp;</p><p>The first issue is that PDB is a biased sample of proteins- those whose production, purification and crystallization were tractable, and those that come from species that humans are most interested in and/or have an easy time studying in the lab. AlphaFold can mostly interpolate within the range of its training data. We don\u2019t really know how much we can extrapolate known protein structures to, say, the proteins of extremophiles, or proteins with low homology to any protein in PDB, or proteins that haven\u2019t been solved because they tend to kill the bacterial cells in which we try to grow them.&nbsp;</p><p>&nbsp;</p><p>Another issue with using PDB is that not all proteins adopt a single, static structure in nature. The up-and-coming fields of intrinsically disordered proteins (IDPs), intrinsically disordered regions (IDRs), and bi-stable \u201cfold-switching\u201d proteins are revealing that many proteins are fully or partly disordered or can change between different structures. Current estimates are that over 50% of eukaryotic proteins contain at least one long disordered segment, and these features appear to be key to many proteins\u2019 functions.</p><p>&nbsp;</p><p>Scientists who work with IDPs/IDRs and fold-switching proteins are offering cautions that AlphaFold (and its successor AlphaFold2) may predict spurious structures for these proteins.<sup>5</sup> Scientists working on proteins that switch between two or more structures report that AlphaFold will typically find just one of the structures, or an intermediate structure, but the prediction will be given with high confidence. Scientists <i>are</i> using AlphaFold to (carefully) study IDPs and IDRs, and AlphaFold seems to do well at marking these regions as \u201clow-confidence\u201d predictions. However, the authors of a 2021 paper caution that those inexperienced with IDPs/IDRs who use AlphaFold may be \u201ctempted to draw\u201d incorrect inferences that have \u201czero physical meaning\u201d and that the AlphaFold process can introduce artifacts that are especially severe for IDRs.<sup>6</sup> Because IDPs/IDRs are still a lesser-known field, some scientists today may be looking at AlphaFold\u2019s static outputs with less than the appropriate skepticism.</p><p>&nbsp;</p><p>This is not to diminish the accomplishments of those who\u2019ve worked on AlphaFold; it really is an important advance that puts multiple recent discoveries in protein biology and molecular evolution to work. We can use AlphaFold and similar AIs well by understanding where they perform better and worse, where nobody knows the \u201cground truth,\u201d where we may be wrong about it, or where we may be extrapolating too broadly.</p><p>&nbsp;</p><p>If humans had turned the task of investigating protein structures over to an AI (or to scientists who rely heavily on an AI) before humans had discovered IDPs and bi-stable proteins, we may never have known about these important groups. It took human intelligence to make the discoveries leading to the recognition of IDPs/IDRs: the hands-on work of trying and failing to crystallize some groups of proteins, and the innovative thinking needed to notice where the dominant paradigm was breaking down and recognize its importance.&nbsp;</p><p>&nbsp;</p><p><strong>Case study 2</strong></p><p>&nbsp;</p><p>AI programs for medical image analysis have been under development for years. A few of these programs are currently used in the clinic, though reception has been mixed. To the extent that we can define what constitutes a \u201cnormal\u201d or \u201cabnormal\u201d imaging result, and to the extent that representative and accurately-labeled imaging datasets are available, it should be possible to eventually develop useful AIs that replicate physicians\u2019 judgments regarding images.</p><p>&nbsp;</p><p>But image evaluation is only one step in a larger diagnostic process, the same step filled by laboratory blood tests and the like. Because doctors who understand a lab test\u2019s limitations can make better use of it, clinical leaders like Catherine Lucey, MD of UC San Francisco are advocating for doctors to place less emphasis on the tests alone by placing test and imaging results within a Bayesian framework (such as by using Fagan nomograms, also called \u201cBayesian ladders\u201d), with the goal of improving medical decision-making.</p><p>&nbsp;</p><p>From this perspective, if AIs eventually become capable of replacing doctors in medical image analysis, allowing the AIs to take over <i>diagnosis</i> would be a step in the wrong direction. Instead, the advent of radiology AIs would be analogous to the mid-1800s replacement of doctors skilled at tasting urine by laboratory tests for glucose in diagnosing diabetes. The laboratory glucose tests eventually proved more accurate than the human tongue, and most human doctors probably didn\u2019t mind giving up that role.&nbsp;</p><p>&nbsp;</p><p>But any lab test has its limitations. Doctors need to consider the prior probability that a patient has a particular disease based on an exam and interview, the possibilities for other diseases that may explain the patient\u2019s symptoms, available tests\u2019 specificity and sensitivity, the consequences of a false positive or false negative, the uncertainty around the definition of \u201cnormal,\u201d differences among populations, and cases where the test will fail in order to decide whether to apply the test in a particular patient\u2019s case. The same considerations, plus more, apply to AI image analysis.</p><p>&nbsp;</p><p><strong>A different kind of \u201clearning\u201d</strong></p><p>The \u201clearning\u201d that some AIs go through may be better compared to the learning that the human immune system goes through, when it \u201clearns\u201d to recognize self vs. non-self and to attack pathogens, rather than to the learning that a human does. The kind of \u201cintelligence\u201d an immune system has could even be called \u201cgeneral,\u201d since it eventually becomes successful at many different tasks, from destroying cancer cells to neutralizing different pathogens to causing allergic reactions against pollen to rejecting transplanted organs.&nbsp;</p><p>&nbsp;</p><p>Likewise, the kind of \u201clearning\u201d AIs do can make them very good at some tasks or even a range of tasks. However, calling that \u201cintelligence\u201d or \u201cgeneral intelligence\u201d is a metaphor that is more likely to be misleading than helpful. It doesn\u2019t make much more sense than calling the immune system a \u201cgeneral intelligence\u201d -- except as a marketing term and to attract funders.</p><p>&nbsp;</p><p><strong>AI trainers, handlers and data labelers are the cognitive engine behind many AIs</strong></p><p>&nbsp;</p><p>Next, I\u2019d like to look at another way observers might overestimate current AIs\u2019 capabilities (both quantitatively and qualitiatively) and thus overestimate their prospects for developing certain capabilities in the future.</p><p>&nbsp;</p><p>Quite a few AI companies have reportedly used a combination of AIs and human handlers to deliver what customers needed, even while customers and members of the public thought an AI was fully responsible.<sup>7,8,9</sup>Even AIs that don\u2019t need support after they\u2019re deployed often require a massive number of worker-hours to get up and running, including dataset development, labeling and sorting data, training, and giving feedback. In the case of generative AI, these inputs are in addition to the work of artists, writers, and musicians whose contributions are scraped as \u201cdata.\u201d In some cases, these human contributions look less like training and more like doing the work for the AI (albeit ahead of time).&nbsp;</p><p>&nbsp;</p><p>To understand whether human support is like \u201ctraining wheels\u201d or will be needed permanently, we need a clear-eyed view of what is actually happening. Today, some companies pass off much of the cognitive work in setting up and running their AIs to contractors or people working through task services like Mechanical Turk; other companies have used prison labor or outsourced to low-income countries. Tech researcher Saiph Savage has documented how AI company staff often underestimate the time required to complete \u201cmicrowork\u201d tasks passed to contractors. The money offered for microwork tasks is often so poorly aligned to the time needed that these AI workers\u2019 average earnings are around $2 an hour (even for workers in the US).<sup>10</sup> The lack of feedback makes it hard for AI companies, and even harder for outside observers, to gauge how many hours of human cognitive work are actually going in.&nbsp;</p><p>&nbsp;</p><p>When deployed in the workplace, some current AIs may surreptitiously borrow time and cognitive resources from their human \u201ccoworkers\u201d too.&nbsp;One of the criticisms of the (now defunct) IBM Watson Health AI was that doctors and nurses had to spend large amounts of time inputting their medical knowledge and patient data into the AI tools, with little usable payoff.<sup>11&nbsp;</sup>Clinicians have similar complaints about other AIs that are widely deployed in healthcare today.&nbsp;</p><p>&nbsp;</p><p>I\u2019ve had similar enough experiences with expensive but poorly functional laboratory robots and corporate software to understand clinicians\u2019 frustrations with these tools. When a technology appears to replace human workers, sometimes it actually pushes the work onto other humans. Managers can easily miss or underestimate the contributions by humans, making it appear that the technology (whether AI or otherwise) is more successful and \u201csmarter\u201d that it actually is. Humans who wind up as AI handlers may experience their jobs as less meaningful and more frustrating. For humans involved (by choice or not) in any stage of AI development or deployment,&nbsp;having a machine take credit for one\u2019s work will come with an emotional impact and possibly a financial penalty.</p><p>&nbsp;</p><p>Current estimates of the limitations on making AIs \u201csmarter\u201d may be biased by underestimates of how much today\u2019s AIs rely on humans to do their cognitive work, either in advance or in real time.&nbsp;To know whether an AI is actually accomplishing useful work, we need to compare its time savings to the hours put in by <i>all&nbsp;</i>humans who worked on the project and <i>all&nbsp;</i>humans who help the AI function day-to-day, including contractors, people at outsourcing sites, people at client organizations, and end users, and we need to compare its contributions to the opportunity costs from those people not spending their time on something else. In some cases,&nbsp;the human input for today\u2019s AIs is vastly greater than any output we can expect.&nbsp;</p><p>&nbsp;</p><p><strong>Some AIs are status quo engines that will slow down human advancement if deployed inappropriately&nbsp;</strong></p><p>&nbsp;</p><p>Modern society has a great predilection to base new programs and solutions on existing situations, even if those situations are pathological.&nbsp;</p><p>&nbsp;</p><p>Some examples:</p><ol><li>Breastfeeding has health benefits, but breastfed babies typically gain weight more slowly than formula-fed babies. The CDC\u2019s percentile charts that track infant weight were based on data from a study conducted in the 1970s, when formula feeding was very common. Pediatricians used these charts as a standard to gauge the growth of most US babies born from the late 1970s until 2010. A 2006 panel convened by the CDC and other groups found that some doctors were inappropriately advising parents to switch from breastfeeding to formula feeding to help their babies \u201ccatch up\u201d with the growth charts. In 2010, these concerns led the CDC to recommend that pediatricians switch to using newer WHO growth charts based on breastfed babies.<sup>12</sup></li><li>In 2009, the National Oceanic and Atmospheric Administration (NOAA) rolled out new \u201ccatch-share\u201d regulations that were supposed to prevent overfishing in the New Bedford, MA fishery. The total fish harvest would be capped, and each fishing business would be allotted a piece of the pie based on its catch of each vulnerable species in past years. The result was that companies that had already been doing the most damage to vulnerable species had the highest allocations, and those companies were able to further increase their shares by driving more responsible fishermen out of business.<sup>13</sup></li><li>In hospitals across the US, a healthcare algorithm decided which patients\u2019 health conditions were severe enough that they should be offered extra care. Problems surfaced in 2019. Researchers revealed that the decision-making algorithm was much less likely to offer extra care to black patients compared to equally sick white patients. The algorithm used the money spent on each patient in past years to predict which patients were \u201csicker.\u201d This biased decision-making against patients who had been undertreated due to poor healthcare access in the past. Because the algorithm allocated services away from patients for whom money and access had been limiting factors in their health, and toward patients for whom they had not been, it probably ensured that money was spent in the <i>least efficient way possible</i>.&nbsp;</li></ol><p>&nbsp;</p><p>AIs didn\u2019t originate this, but because many AIs rely on past data to make predictions, basing the future on the past or present is something they\u2019re naturally good at. Similar examples of \u201cAI bias\u201d influenced by current and historical realities or by human behavior are popping up throughout healthcare and in other areas of our society. Many people are working on reducing bias in AI technology and in algorithms more broadly, but the problem might be inherent to any system that relies on making associations in existing data rather than going out and investigating the causes of problems.<sup>15,16</sup></p><p>&nbsp;</p><p>If AIs can\u2019t be made to think outside the data, more advanced AIs may just be better status quo engines, skilled at propagating harmful situations forward in time.&nbsp;</p><p>&nbsp;</p><p>In the corporate world,&nbsp;AI could easily feed right into many business leaders\u2019 inclination to throw new, heavily advertised technology at a problem instead of understanding it.&nbsp;Unless AIs can investigate a problem hands-on, they can only use the existing data that humans chose to collect and feed them.</p><p>&nbsp;</p><p>And alignment between the AI itself and the goals of its developers is not enough. As with traditional product design, the way developers understand real-world environments and the needs of users is often far from reality. The Lean Manufacturing and the broader Lean Thinking movements have facilitated real growth in value precisely by teaching people to open up communication, investigate the real roots of problems hands-on, and <strong>seek alignment</strong> of goals and actions not only within a company but with customers and vendors. To improve our thinking and action, we should be learning to break out of our status-quo-is-correct thinking patterns instead of reinforcing them and making them harder to detect.&nbsp;</p><p>&nbsp;</p><p><strong>To work with some AIs, humans will have to dumb themselves down</strong></p><p>&nbsp;</p><p>In high school English classes today, students and schools are rewarded for producing essays that please automated essay-scoring systems. These tests tend to give low scores to the work of certain famous authors, but high scores to students who\u2019ve learned to write shallow essays that use lots of big words in long sentences, even if the essays contain little actual reasoning or insight.<sup>17</sup> In other words, when algorithms are not capable of scoring our brightest kids\u2019 tests, teachers are tasked with making kids dumber. Automated scoring systems for school subjects besides English are being developed.</p><p>&nbsp;</p><p>AIs may be able to displace workers who were trained in \u201crote\u201d educational programs, but the reason is that standardized testing and rote learning aren\u2019t the best ways to help people develop real creativity, advanced thinking skills, or hands-on problem investigation. An AI recently passed a standardized medical licensing exam, but a written test doesn\u2019t capture all aspects of knowledge, and the difference between a new med-school graduate and an experienced doctor is precisely the real-world learning that an algorithm inside a machine can\u2019t do. The more similar our educational systems are to the 1950s Brazilian physics program Feynman visited, the easier our work will be for an AI to replace. But the issue here isn\u2019t really the AI.&nbsp;</p><p>&nbsp;</p><p>Generative AIs, too, trade in a <strong>secondary market of ideas</strong>, so their work will be more and more self-referential unless humans keep infusing real innovation. But this could become more difficult.&nbsp;If generative AI intellectual property law is worked out in favor of the AI companies, writers who share very innovative ideas, artists or musicians with a very distinctive style, or photographers who capture unique objects or scenes may be most prone to being directly infringed. The most innovative creators will be susceptible to having their work devalued by the greatest degree and could be driven out of the marketplace. It will become difficult to originate and share anything that contains true originality, unless it can be securely kept away from the AIs.<sup>18</sup></p><p>&nbsp;</p><p>In scientific research, over-production of low-quality journal articles and bad meta-analyses are already problems, and people in the field of metascience are working on solving them. Attempts to use AIs to carry out investigations and publish papers will only make the scientific \u201creplication crisis\u201d and similar issues worse.&nbsp;Even in most areas where we think we know what \u201ccorrect\u201d is, we should be questioning our definitions of \u201ccorrect,\u201d not fossilizing them.</p><p>&nbsp;</p><p>Like automatic telephone switchboards and calculating machines before them, AIs are much faster and better at specific tasks than humans are. The problem is that AIs are getting better at miming human-like behaviors at a time when their actual capabilities cover only a tiny fraction of the full range of what humans can do. If societal forces exacerbated by AI deployment lead us to devalue the work that is <i>outside</i> AIs\u2019 range, we will constrain our thinking and development.</p><p>&nbsp;</p><p>As with any type of tool, we can make better and safer use of AIs if we\u2019re clear-eyed about their limitations and keep humans in the driver\u2019s seat. The graduates of the Brazilian physics program Feynman encountered could be dangerous, if someone hired a bunch of them for high positions in a nuclear project on the strength of their apparent skill in physics. But the danger would be very different from that of a brilliant and malicious physicist wresting control of the same project.</p><p>&nbsp;</p><p>&nbsp;</p><p><strong>In summary:</strong></p><ul><li>Some of the seemingly most impressive AIs are cognitive freeloaders, using the cognition of human trainers, users, and creators behind the scenes.&nbsp;</li><li>As other expensive and unneeded technologies have, some AIs could feed into broken processes and contribute to the further degradation of worker experience in the workplace. Other AIs can make useful contributions, but if deployed in the wrong ways, they could hamper human educational, intellectual and creative endeavors.&nbsp;</li><li>Like other algorithms that rely on historical or current data, AIs can give human users the impression that the status quo is \u201ccorrect.\u201d This can exacerbate the human and institutional tendency to propagate the status quo, even if it is pathological.</li><li>Talking about machine learning algorithms in terms of human \u201cgeneral intelligence\u201d is an unhelpful, bad metaphor. It\u2019s a bad metaphor because it implies that the danger in AIs is that they will get too smart. The real danger is that we (or some of us) overestimate their capabilities and trust AIs in places they\u2019re not suited for.</li></ul><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p><strong>References and Notes</strong></p><p>&nbsp;</p><ol><li><i>\u201cSurely You\u2019re Joking, Mr. Feynman!\u201d: Adventures of a Curious Character</i> by Richard P. Feynman, 1985.</li><li><a href=\"https://www.bigtechnology.com/p/a-writer-used-ai-to-plagiarize-me\"><u>https://www.bigtechnology.com/p/a-writer-used-ai-to-plagiarize-me</u></a></li><li><a href=\"https://pike.psu.edu/publications/www23.pdf\"><u>https://pike.psu.edu/publications/www23.pdf</u></a></li><li><a href=\"https://arxiv.org/pdf/2212.03860.pdf\"><u>https://arxiv.org/pdf/2212.03860.pdf</u></a></li><li><a href=\"https://www.nlm.nih.gov/research/researchstaff/labs/porter/pdfs/chakravarty_AF2.pdf\"><u>https://www.nlm.nih.gov/research/researchstaff/labs/porter/pdfs/chakravarty_AF2.pdf</u></a></li><li><a href=\"https://www.sciencedirect.com/science/article/pii/S0022283621004411\"><u>https://www.sciencedirect.com/science/article/pii/S0022283621004411</u></a></li><li><a href=\"https://www.wired.com/story/not-always-ai-that-sifts-through-sensitive-info-crowdsourced-labor/\"><u>https://www.wired.com/story/not-always-ai-that-sifts-through-sensitive-info-crowdsourced-labor/</u></a>&nbsp;&nbsp;</li><li><a href=\"https://www.bloomberg.com/news/articles/2016-04-18/the-humans-hiding-behind-the-chatbots?leadSource=uverify%20wall\"><u>https://www.bloomberg.com/news/articles/2016-04-18/the-humans-hiding-behind-the-chatbots?leadSource=uverify%20wall</u></a>&nbsp;&nbsp;</li><li><a href=\"https://www.vice.com/en/article/xweqbq/microsoft-contractors-listen-to-skype-calls\"><u>https://www.vice.com/en/article/xweqbq/microsoft-contractors-listen-to-skype-calls</u></a></li><li><a href=\"https://www.technologyreview.com/2020/12/11/1014081/ai-machine-learning-crowd-gig-worker-problem-amazon-mechanical-turk/\"><u>https://www.technologyreview.com/2020/12/11/1014081/ai-machine-learning-crowd-gig-worker-problem-amazon-mechanical-turk/</u></a></li><li><a href=\"https://www.statnews.com/2017/09/05/watson-ibm-cancer/\"><u>https://www.statnews.com/2017/09/05/watson-ibm-cancer/</u></a></li><li><a href=\"https://www.cdc.gov/mmwr/preview/mmwrhtml/rr5909a1.htm\"><u>https://www.cdc.gov/mmwr/preview/mmwrhtml/rr5909a1.htm</u></a></li><li><a href=\"https://hakaimagazine.com/features/last-trial-codfather/\"><u>https://hakaimagazine.com/features/last-trial-codfather/</u></a> This allocation system probably increased overfishing. It turns out that the companies most willing to overfish vulnerable populations for short-term gain were also willing to deceive regulators. The companies owned by fraudster Carlos Rafael (aka The Codfather) circumvented the new regulations by mislabeling fish and misreporting how much they were catching. Carlos Rafael had nearly taken over the New Bedford fishery by the time he was caught.</li><li><a href=\"https://www.science.org/doi/10.1126/science.aax2342\"><u>https://www.science.org/doi/10.1126/science.aax2342</u></a></li><li><a href=\"http://ziadobermeyer.com/wp-content/uploads/2019/09/measurement_aer.pdf\"><u>http://ziadobermeyer.com/wp-content/uploads/2019/09/measurement_aer.pdf</u></a></li><li><a href=\"http://ziadobermeyer.com/wp-content/uploads/2021/08/Predicting-A-While-Hoping-for-B.pdf\"><u>http://ziadobermeyer.com/wp-content/uploads/2021/08/Predicting-A-While-Hoping-for-B.pdf</u></a></li><li><a href=\"https://www.salon.com/2013/09/30/computer_grading_will_destroy_our_schools/\"><u>https://www.salon.com/2013/09/30/computer_grading_will_destroy_our_schools/</u></a> Referring to high school students gaming essay-grading algorithms, the author writes: \u201cOne obvious problem is that if you know what the machine is measuring, it is easy to trick it. You can feed in an \u201cessay\u201d that it is actually a bag of words (or very nearly so), and if those words are SAT-vocab-builders arranged in long sentences with punctuation marks at the end, the computer will give you a good grade. The standard automated-essay-scoring-industry response to this criticism is that anyone smart enough to figure out how to trick the algorithm probably deserves a good grade anyway.\u201d Similar developments in several areas could lead to circular \u201chumans-gaming-machines-gaming humans\u201d situations that would suck up time and energy.</li><li>There is some hope that this will change. The FTC has forced several companies to undergo \u201calgorithmic disgorgement\u201d: deletion of improperly-collected photos, videos, or personal information and destruction of the algorithms developed using those data. (<a href=\"https://jolt.richmond.edu/files/2023/03/Goland-Final.pdf\"><u>https://jolt.richmond.edu/files/2023/03/Goland-Final.pdf</u></a><u>)&nbsp;</u>Thus far, however, these enforcement actions have focused on privacy violations, not on unauthorized use of music, art, or other professional work. Perhaps greater use of algorithmic disgorgement and requirements that companies obtain \u201copt-in\u201d permission before using personal data or copyrighted works in AI training could be a way to avert some of the harms discussed here.&nbsp;</li></ol>", "user": {"username": "Ilana_Jimenez"}}, {"_id": "XtYA7eYXeYiNNcCDz", "title": "My AI Alignment Research Agenda and Threat Model, right now (May 2023)", "postedAt": "2023-05-28T03:23:38.373Z", "htmlBody": "", "user": {"username": "NicholasKross"}}, {"_id": "zgBB56GcnJyjdSNQb", "title": "How focused do you think EA is on topics of race and gender equity/justice, human rights, and anti-discrimination?  What do you think are factors that shape the community's focus?    ", "postedAt": "2023-05-29T09:56:23.507Z", "htmlBody": "<p>I put two questions in the title because I would like them answered together. &nbsp;I hope that doesn't break EA rules! &nbsp;I'm still reading through the handbook and norms documents. &nbsp;I also still have to do an intro/profile after I finish reading, if it seems like EA may be a community I should participate in. &nbsp;TIA for bearing with me as a newbie!<br><br>The crux of my questions is I'm confused that I'm not seeing more on EA about race and gender equity/justice, human rights, and anti-discrimination efforts. &nbsp;I was wondering how people who have been involved with EA (longer than I) appraise EA's focus on these topics. &nbsp;If you do not think EA focuses much on these topics, what are some reasons that you think this is so (OK, three questions ; - ). &nbsp;Regardless of how you appraise EA's involvement with these topics, what do you think are factors that shape the community's focus?<br>&nbsp; &nbsp; &nbsp; &nbsp;</p><p>&nbsp;</p><p>[Note: moderation set to norm enforcing. &nbsp;I'm new to EA so I don't quite know what this means, but I am selecting it to be concurrent with EA's norms.] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p>", "user": {"username": "KJonEA"}}, {"_id": "JkZ6bMRJPaaQuL8uH", "title": "Why and When Interpretability Work is Dangerous", "postedAt": "2023-05-28T00:27:37.858Z", "htmlBody": "", "user": {"username": "NicholasKross"}}, {"_id": "aHkthQrAfuyNBNFhM", "title": "Has Russia\u2019s Invasion of Ukraine Changed Your Mind?", "postedAt": "2023-05-27T18:35:06.552Z", "htmlBody": "<p>[This post was written in a purely personal capacity, etc.]</p><p>I<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2j4g3u0yfl4\"><sup><a href=\"#fn2j4g3u0yfl4\">[1]</a></sup></span>&nbsp;recently had several long conversations with a friend about whether my regular doom-scrolling regarding the Ukraine war had sharpened my understanding of the world or mostly been a waste of time.</p><p>Unfortunately, it seems more of the latter. When my mind has changed, it's been slight, and it\u2019s unclear what actions my new views justify. Personally, this means I should probably go back to thinking about happiness and RCTs.</p><p>I set out what I think are some relevant questions Russia's invasion of Ukraine could change your mind about and provide some sloppy commentary, but I'm interested to know what other EAs and rationalists think about this issue.</p><h2><strong>High-level questions</strong></h2><h3><strong>Likelihood of great power conflict</strong></h3><p>It seems like the Metaculus forecasting community is now more worried about great power conflict than it was before the war. I assume the invasion of Ukraine is a causal factor. But I feel oddly reassured about this, like the world was ruled by drunks who sobered up when the knives came out, reminded that knives are sharp and bodily fluids are precious.</p><p><strong>After the invasion, the prospect of a Russia-USA War shifted </strong><a href=\"https://www.metaculus.com/questions/7452/us-russia-war-before-2050/\"><strong><u>from a 5-15% to a 25%</u></strong></a><strong>&nbsp;chance before 2050.</strong>&nbsp;I hadn\u2019t known about this forecast, but I would have assumed the opposite. Before the war, Russia viewed the US as a waning power, losing in Afghanistan, not-winning in Syria, Libya and Venezuela, riven by internecine strife and paralyzed by self-doubt. Meanwhile, Russia\u2019s confidence in its comeback rose with each cost-effective success in Crimea, Syria, and Kazakhstan.</p><p>Now Russia knows how hollow its military was. And it knows the USA knows. And it knows that NATO hand-me-downs are emptying its once vast stockpiles of tanks and APCs. I assume it won\u2019t recover the depth of its armour stocks in the near term (it doesn\u2019t have the USSR\u2019s state capacity or industrial base). The USA also doesn\u2019t need to fight Russia. If Ukraine is doing this well, then Ukraine + Poland + Baltics would probably do just fine. I\u2019d put this more around 6.5%.</p><p>I think a Russian war with a European state has probably increased simply based on Russia\u2019s revealed willingness to go to war, in conjunction with forecasters predicting a good chance (20%-24%) that the US and China will go to war over Taiwan<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzfvw4jydvm7\"><sup><a href=\"#fnzfvw4jydvm7\">[2]</a></sup></span>. Russia may find such a conflict an opportunity to attempt to occupy a square mile of uninhabited Lithuanian forest to create a safe zone for ethnic Russian speakers and puncture the myth of NATO\u2019s 5th article.</p><p>&nbsp;</p><p><a href=\"https://www.metaculus.com/questions/2534/world-war-three-before-2050/\"><u>Will there be a 'World War Three' before 2050? | Metaculus</u></a></p><p>The predicted probability to this question shifted by around 10%, from the 10-15% range to 20-25% after the war began. I assume this is mostly driven by Russia-NATO-initiated conflict. China-India conflict predictions have decreased<a href=\"https://www.metaculus.com/questions/8363/china-india-war-by-2035/\"><u>&nbsp;from 30% pre-war to 17% before 2035 most recently.</u></a>&nbsp;And China-US war predictions have stayed constant (<a href=\"https://www.metaculus.com/questions/8362/us-china-war-before-2035/https://www.metaculus.com/questions/8362/us-china-war-before-2035/\"><u>20% before 2035</u></a>). So the rise must stem from the increase in the likelihood of a Russia-US war or by other major powers between 2035 and 2050. &nbsp;I don\u2019t think I agree with the community here, as I explained previously.</p><p>&nbsp;</p><p><a href=\"https://www.metaculus.com/questions/9969/chinese-involvement-in-ukrainian-conflict-/%23comment-116630\"><u>Will China get involved in the Russo-Ukrainian conflict by 2024?</u></a></p><p>China hasn\u2019t involved&nbsp;itself in the Ukraine war yet. And the prospects for its involvement seem like they should dim over time \u2014 surely it would have acted or given more hints that it was considering doing so by now?</p><p>This makes me more confused about whether China committed to a military confrontation with the West. If it has, and China believed it had more military-industrial capacity than the West (which is what I\u2019d believe if I was China), then now is the perfect opportunity to drain Western stocks further and prop up its ally (?) by pumping weapons into Russia (see previous forecasting question). But maybe it sees the risk as encouraging the resurrection of Western arms manufacturing?</p><p>Due to Ukraine, I think the US has a (very) slightly higher likelihood of military response to Taiwan because supplying Ukraine has given the USA back some of its lost mojo. However, I also think that China has seen that Western equipment is quite effective against Soviet-based hardware, and it may be rethinking or delaying its invasion plans. So I think the strong Western response (and its success so far) will somewhat deter China.</p><p>&nbsp;</p><h3><strong>Likelihood of nuclear war, conditional on great power conflict</strong></h3><p>I feel like this has gone down in my mind. This is mostly because almost all opportunities for escalation have been handled carefully despite the Western support causally leading to the deaths of thousands of Russian soldiers.</p><p>I was so placated by the thought that the likelihood of nuclear war might be lower than I expected if China and the USA fought over Taiwan \u2014 so much so that I thought, hey, maybe we should defend Taiwan. But when I articulated this probability, I realised it remained shockingly high in my mind. I'm now quite confused over whether we should provide direct support or try and pull a Ukraine 2.0.&nbsp;</p><p>I settled on a 10-15% likelihood of a USA-China war over Taiwan escalating to nuclear war. We agreed this was mostly due to the likelihood of an accident in the first three days of the conflict due to a blinding attack or weather anomaly (c.f. Petrov). There were a lot of reasons that went into placing most of the probability mass near the initiation of the conflict, but I won\u2019t belabour them here<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2obdzq5bace\"><sup><a href=\"#fn2obdzq5bace\">[3]</a></sup></span>.</p><p>For p(nukes | big power war) to get into the low single digits, I think there needs to be strong political control over the military in both countries. They both share and somehow convincingly communicate that they wish to keep the war contained to Taiwan and the Taiwan Strait. Any slippage of the war to mainland China or American bases on Guam or Okinawa seems to come with a big dose of risk of nuclear war. If China struck US bases, it would also face conventional risks of activating NATO or Japanese military assistance so that it could have other reasons for restraint.</p><p>&nbsp;</p><h3><strong>Likelihood of nuclear war</strong></h3><p>I\u2019m honestly unsure about the likelihood of nuclear war. Metaculus seems to think it\u2019s higher in the post-war world, <a href=\"https://www.metaculus.com/questions/4779/at-least-1-nuclear-detonation-in-war-by-2050/\"><u>moving from 20-25% to 30-35% after the war</u></a>. I assume this is primarily because of a predicted increase in the likelihood of the USA-Russian war. Still, again, I think of Ukraine as more of a pressure release valve than a pressure cooker for the likelihood of war between these states.</p><p>&nbsp;</p><h3><strong>Conditional on Russia losing, is the world a safer place?</strong></h3><p>I think maybe a bit, in a general \u201cdon\u2019t reward conquest\u201d sort of way. This has to be balanced with the \u201cWhat if a nuclear power collapses\u201d scenario. But I think regime change or chaos is relatively unlikely, and my predictions of chaos have been lower than the community for a long time. See: &nbsp;</p><ul><li><a href=\"https://www.metaculus.com/questions/11589/a-large-scale-conflict-within-russia-by-2030/\"><u>Will there be a large-scale armed conflict in Russia before 2030?</u></a></li><li><a href=\"https://www.metaculus.com/questions/4799/when-will-vladimir-putin-cease-to-hold-the-office-of-president-of-russia/\"><u>When will Vladimir Putin cease to hold the office of President of Russia?</u></a></li></ul><p>If Russia had a large-scale civil conflict, I think it\u2019d be high variance but, on average, result in a safer world because of the small chance of a weakened Russia with its nukes secured by NATO and China (c.f. &nbsp;Breakup of USSR). But I feel the distorting pull of my Pollyannaish impulses here. Most of this comes through a bipolar world being more stable; see game theory.</p><h2><strong>Things I remain deeply confused about:</strong></h2><ul><li>At what probability of nuclear war or great power war should we sacrifice a country or a region to the clutches of an authoritarian state?</li><li>How bad is authoritarianism anyways? China and Taiwan\u2019s life satisfaction isn\u2019t that different.</li><li>How much influence does the global hegemon have over global values? How much additional influence will China have on the trajectory of human values if the USA concedes geopolitically to China?</li><li>What, if anything, should EAs do about war? Having a lot of EAs work in diplomacy to try and increase international cooperation between great powers seems like a basic, GiveDirectly type of good. But I\u2019m struck by how clueless I feel about the sign of many specific potential actions (e.g., should the USA directly defend Taiwan?).</li><li>Was globalisation a good idea? Sure, China\u2019s growth was good for millions of Chinese, but is the threat of China\u2019s ascendence to great power status enough to offset this? If a revisionist turn seems warranted, what does this mean about how we go forward?</li></ul><h2><strong>Indirect updates and reminders</strong></h2><ul><li>USA being #1 seems good (assuming Europe is not an alternative).</li><li>Adversarial geopolitics is bad for global collaboration on things we need to get right, like biotechnology.</li><li>Winning great power wars can lead to a critical period in the capacity for creating tools of international collaboration. I.e., League of Nations, EU and UN. World seems safer because of these institutions. What if we rolled into the nuclear age without them?</li><li>The great pacification, insomuch as it exists, seems far less applicable to authoritarian regimes. All else equal, trade with dictators seems to give more leverage to dictators than democracies.</li><li>Updated towards the west being more unified than I feared. The response to the war has been robust and quite collective. Germany seemed to wean itself off Russian gas in a matter of months (I know it also got lucky). This counterbalances the poor initial response of most western institutions to manage COVID.</li><li>I think it\u2019s plausibly good to support destroying Iran\u2019s capacity to get nukes (RIP Iran nuclear deal \u2013 top 5 worst things Trump did?). Adding another unreliable nuclear actor (and the shield it provides for developing other weapons) seems worth the cost of war / brief invasion. Relatedly, I\u2019m a bit more sceptical of civilian nuclear technology that can advance nuclear weapons technology being used by geopolitically insecure states. Not sure it\u2019s worth it for climate goals.</li></ul>", "user": {"username": "JoelMcGuire"}}, {"_id": "N3rKebheBhAfoStqa", "title": "Drawing attention to invasive Lymantria dispar dispar spongy moth outbreaks as an important, neglected issue in wild animal welfare", "postedAt": "2023-05-28T14:18:43.528Z", "htmlBody": "<p>This post contains only the summary of a longer research post, written by Meghan Barrett and Hannah McKay. The full post can be found at the above link on Rethink Priorities website.</p><h2><strong>Summary</strong></h2><p>One aim of wild animal welfare research is to identify situations where large numbers of wild animals are managed by humans in ways that have significant welfare impacts. If the number of individuals is large and the welfare impacts significant, the issue is important. As humans are managing these animals, it is possible the welfare impacts could be moderated to reduce their suffering. The massive scale of invasive (e.g., non-native) <i>Lymantria dispar dispar </i>(spongy<i> </i>moth) outbreaks represents an unappreciated wild animal welfare issue, and thus deserves further attention from a welfare (not simply an invasive species-control) perspective.</p><p>The spongy<i> </i>moth is not endemic to North America. The species experiences localized three year-long outbreaks of half a billion or more caterpillars/km<sup>2</sup> every 10-15 years in regions where they are well established (including their native range). Spongy moths currently occupy at least 860,000 km<sup>2</sup> in North America, only \u00bc of their possible range (though most of the occupied area is not experiencing outbreak conditions, most of the time). <i>L. dispar</i> continues to spread slowly to new areas each year despite multi-million dollar efforts to stop expansion. Assuming spongy moth caterpillars are sentient, methods for actively controlling them during outbreaks cause substantial suffering. The aerial spray (<i>Btk</i>) ruptures the stomach, causing the insect to die from either starvation or sepsis over two to seven days. However, because outbreaks are so large, most caterpillars are not actively targeted for control, and \u2018natural forces\u2019 are allowed to reduce the outbreak. The most prominent natural forces to rein in an outbreak are starvation and disease. The accidentally introduced fungus, <i>Entomophaga </i>(meaning \u201cinsect eater\u201d)<i> maimaiga, </i>digests caterpillars\u2019 insides before pushing through the exoskeleton to release spores, usually within a week. LdNPV virus is also common in the spongy moth<i> </i>population, but only causes high levels of mortality during outbreaks when larvae are stressed from extreme competition. A symptom of severe LdNPV infection is \u201clarval melting,\u201d or the liquefaction of the insect\u2019s internal organs.</p><p>The scale of spongy moth outbreaks is tremendous, though notably these outbreaks are not necessarily higher-density than numbers of other insect species (e.g., <a href=\"https://www.si.edu/spotlight/buginfo/bugnos#:~:text=Insects%20also%20probably%20have%20the,described%20species%20is%20approximately%2091%2C000.\">740 million to 6.2 billion individual wireworms/km2</a>; Smithsonian, n.d.). However, spongy<i> </i>moths are <a href=\"https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/1365-2656.12755\">one of the best tracked non-native insects</a> (Grayson &amp; Johnson, 2018; e.g., Stop the Spread program), providing us with better data for analyzing the scale of the welfare issue (both in terms of caterpillar density within outbreaks, and the total area affected by outbreaks). In addition, there is potential for <i>significant range expansion </i>by spongy moths that would increase the scope of the welfare concern, and there appears to be extreme suffering<a href=\"https://rethinkpriorities.org/publications/spongy-moth-outbreaks#fn1\"><sup>1</sup></a> induced by both active and natural outbreak control. As a result, spongy moth welfare during outbreaks could be an issue of concern for animal welfare advocates. Further research could improve spongy moth welfare by: 1) identifying the most promising long-term interventions for preventing/reducing the occurrence of outbreaks behind the invasion front, 2) contributing to halting the spread of spongy moths into new areas, and 3) identifying the highest-welfare outbreak management strategies where outbreaks do occur.</p>", "user": {"username": "Meghan Barrett"}}, {"_id": "fsaogRokXxby6LFd7", "title": "A compute-based framework for thinking about the future of AI", "postedAt": "2023-05-31T22:00:19.215Z", "htmlBody": "<p>How should we expect AI to unfold over the coming decades? In this article, I explain and defend a compute-based framework for thinking about AI automation. This framework makes the following claims, which I defend throughout the article:</p><ol><li>The most salient impact of AI will be its ability to automate labor, which is likely to trigger a productivity explosion later this century, greatly altering the course of history.</li><li>The availability of useful compute is the most important factor that determines progress in AI, a trend which will likely continue into the foreseeable future.</li><li>AI performance is likely to become relatively predictable on most important, general measures of performance, at least when predicting over short time horizons.&nbsp;</li></ol><p>While none of these ideas are new, my goal is to provide a single article that articulates and defends the framework as a cohesive whole. In doing so, I present the perspective that <a href=\"https://epochai.org/\">Epoch </a>researchers find most illuminating about the future of AI. Using this framework, I will justify a value of 47% for the probability of Transformative AI (TAI)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffmli5nt1a99\"><sup><a href=\"#fnfmli5nt1a99\">[1]</a></sup></span>&nbsp;arriving before 2043.</p><h1>Summary</h1><p>The post is structured as follows.&nbsp;</p><p>In <a href=\"https://forum.effectivealtruism.org/posts/fsaogRokXxby6LFd7/a-compute-based-framework-for-thinking-about-the-future-of#Part_1__Widespread_automation_from_AI\">part one</a>, I will argue that what matters most is when AI will be able to automate a wide variety of tasks in the economy. The importance of this milestone is substantiated by simple models of the economy that predict AI could greatly accelerate the world economic growth rate, dramatically changing our world.&nbsp;</p><p>In <a href=\"https://forum.effectivealtruism.org/posts/fsaogRokXxby6LFd7/a-compute-based-framework-for-thinking-about-the-future-of#Part_2__A_compute_centered_theory_of_AI_automation\">part two</a>, I will argue that availability of data is less important than compute for explaining progress in AI, and that compute may even play an important role driving algorithmic progress.&nbsp;</p><p>In <a href=\"https://forum.effectivealtruism.org/posts/fsaogRokXxby6LFd7/a-compute-based-framework-for-thinking-about-the-future-of#Part_3__Predictability_of_AI_performance\">part three</a>, I will argue against a commonly held view that AI progress is inherently unpredictable, providing reasons to think that AI capabilities may be anticipated in advance.&nbsp;</p><p>Finally, in <a href=\"https://forum.effectivealtruism.org/posts/fsaogRokXxby6LFd7/a-compute-based-framework-for-thinking-about-the-future-of#Part_4__Modeling_AI_timelines\">part four</a>, I will conclude by using the framework to build a probability distribution over the date of arrival for transformative AI.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffmli5nt1a99\"><sup><a href=\"#fnfmli5nt1a99\">[1]</a></sup></span></p><h1>Part 1: Widespread automation from AI</h1><p>When discussing AI timelines, it is often taken for granted that the relevant milestone is the development of Artificial General Intelligence (AGI), or a software system that can do or learn \u201ceverything that a human can do.\u201d However, this definition is vague. For instance, it's unclear whether the system needs to <a href=\"https://youtu.be/ipRvjS7q1DI?t=44\">surpass all humans</a>, some upper decile, or the median human.</p><p>Perhaps more importantly, it\u2019s not immediately obvious why we should care about the arrival of a single software system with certain properties. Plausibly, a set of narrow software programs could drastically change the world before the arrival of any monolithic AGI system (<a href=\"https://www.fhi.ox.ac.uk/reframing/\">Drexler, 2019</a>). In general, it seems more useful to characterize AI timelines in terms of the impacts AI will have on the world. But, that still leaves open the question of what impacts we should expect AI to have and how we can measure those impacts.</p><p>As a starting point, it seems that automating labor is likely to be the driving force behind developing AI, providing huge and direct financial incentives for AI companies to develop the technology. The <i>productivity explosion hypothesis</i> says that if AI can automate the majority of important tasks in the economy, then a dramatic economic expansion will follow, increasing the rate of technological, scientific, and economic growth by at least an order of magnitude above its current rate (<a href=\"https://www.openphilanthropy.org/research/could-advanced-ai-drive-explosive-economic-growth/\">Davidson, 2021</a>).</p><p>A productivity explosion is a robust implication of simple models of economic growth models, which helps explain why the topic is so important to study. What's striking is that the productivity explosion thesis appears to follow naturally from some standard assumptions made in the field of economic growth theory, combined with the assumption that AI can substitute for human workers. I will illustrate this idea in the following section.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz4avdnckk7\"><sup><a href=\"#fnz4avdnckk7\">[2]</a></sup></span></p><p>But first, it is worth contrasting this general automation story with alternative visions of the future of AI. A widely influential scenario is the hard takeoff scenario as described by Eliezer Yudkowsky and Nick Bostrom (<a href=\"https://intelligence.org/files/IEM.pdf\">Yudkowsky 2013</a>, <a href=\"https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies\">Bostrom 2014</a>). In this scenario, the impacts of AI take the form of a localized takeoff in which a single AI system becomes vastly more powerful than the rest of the world combined; moreover, this takeoff is often predicted to happen so quickly that the broader world either does not notice it occurring until it's too late, or the AI cannot otherwise be stopped. This scenario is inherently risky because the AI could afterwards irrevocably impose its will on the world, which could be catastrophic if the AI is misaligned with human values.</p><p>However, Epoch researchers tend to be skeptical of this specific scenario. Many credible arguments have been given against the hard takeoff scenario (e.g., <a href=\"https://sideways-view.com/2018/02/24/takeoff-speeds/\">Christiano 2018</a>, <a href=\"https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/\">Grace 2018</a>). We don't think these objections rule out the possibility, but they make us think that a hard takeoff is not the default outcome (&lt;20% probability). Instead of re-hashing the debate, I'll provide just a few points that we think are important to better understand our views.</p><p>Perhaps the most salient reason to expect a hard takeoff comes from the notion of recursive self-improvement, in which an AI can make improvements to itself, causing it to become even better at self-improvement, and so on (<a href=\"http://incompleteideas.net/papers/Good65ultraintelligent.pdf\">Good 1965</a>). However, while the idea of accelerating change resulting from AIs improving AIs seems likely to us, we don't think there are strong reasons to believe that this recursive improvement will be unusually localized in space. Rather than a single AI agent improving itself, we think this acceleration will probably be more diffuse, and take the form of AIs accelerating R&amp;D in a general sense. We can call this more general phenomenon <i>recursive technological improvement</i>, to distinguish it from the narrow case of recursive self-improvement, in which <a href=\"https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate\">a single AI \"fooms\"</a> upwards in capabilities, very quickly outstripping the rest of the world combined (e.g. in a matter of weeks).</p><p>There are many reasons for thinking that AI impacts will be diffuse rather than highly concentrated. Perhaps most significantly, if the bottleneck to AI progress is mostly compute rather than raw innovative talent, it seems much harder for initially-powerless AI systems to explode upwards in intelligence without having already taking over the semiconductor industry. Later, in part two, I will support this premise by arguing that there are some <i>prima facie</i> reasons to think that compute is the most important driver of AI progress.</p><h2>Simple models of explosive growth</h2><p>In the absence of technological innovation, we might reasonably expect output to scale proportionally to increases in inputs. For example, if you were to double the number of workers, double the number of tools, buildings, and roads that the workers rely on, and so on, then outputs would double as well. That's what it would mean for returns to scale to be constant.</p><p>However, doubling the number of workers also increases the rate of idea production, and thus innovation. This fact is important because of a key insight \u2014 <a href=\"https://growthecon.com/StudyGuide/ideas/romer.html\">often attributed to Paul Romer</a> \u2014 that ideas are non-rivalrous. The non-rivalry of ideas means that one person can use an idea without impinging on someone else's use of that idea. For example, your use of the chain rule doesn't prevent other people from using the chain rule. This means that doubling the inputs to production should be expected to <i>more than double</i> output, as workers adopt innovations created by others. Surprisingly, this effect is very large even under realistic assumptions in which new ideas get much harder to find over time (Erdil and Besiroglu 2023 <i>[link forthcoming]</i>).</p><p>Given increasing returns to scale and the fact that inputs can accumulate and be continually reinvested over time, the <a href=\"https://web.stanford.edu/~chadj/annualreview.pdf\">semi-endogenous model of economic growth</a> says that we should see a productivity explosion as a consequence of population growth in the long run, tending towards higher levels of economic growth over time.&nbsp;</p><p>This model appears credible because it offers a simple explanation for the accelerated growth in human history, while also providing a neat explanation for why this acceleration appeared to <a href=\"https://aiimpacts.org/historical-growth-trends/\">slow down</a> sometime in the mid-20th century.<img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/bk1pk8h1x6fh9tdk4vme\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/rd5hrn1yfzghh5wz3vho 340w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/fgijjguy3xlzwd5as9mv 680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/b2bqtcgvbbbwkr8fgvso 1020w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/lfh8gwtftirwgrvmkc05 1360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/tas1ju6gwruwoe99zucz 1700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/oz6kjda6mhkgfmfwy1if 2040w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/pak5zkwgyuxvuamyjozb 2380w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/mgoyjfsploybohsiocqp 2720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/viwoyj44ibodommlglvi 3060w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/gcjl4n9r1y7saohn3dsl 3400w\"></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/pxgqla1ju5oej5tntszh\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/wswvbxeefdh0azjabgkh 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/bzw4l9co999ia3eh2dqh 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/pfplzznxgcbwfud38x6w 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/ijuayqvliopwugd5vx8p 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/lyz7fqaft6npn4wtgdxr 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/lpyjtocfxfslyruw4mxu 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/qs755uxrjticg6sl5kp8 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/agisbapy6o6qbced28ae 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/tv9tbgnk2xdpwni02n3x 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/x9xogxjg4zrrrx87qzx9 1305w\"><figcaption>Plot from <a href=\"https://aiimpacts.org/historical-growth-trends/\">AI Impacts</a></figcaption></figure><p>The slowdown we observed around the mid-20th century straightforwardly follows from the semi-endogenous model once the <a href=\"https://ourworldindata.org/world-population-growth-past-future\">demographic transition</a> is taken into account, which uncoupled the link between population growth and economic growth, resulting in declining fertility and, ultimately, declining rates of economic growth.</p><p>By contrast, since computing hardware manufacturing is not bound by the same constraints as human population growth, an AI workforce can expand very quickly \u2014 much faster than the time it takes to raise human children. Perhaps more importantly, software can be copied very cheaply. The 'population' of AI workers can therefore expand drastically, and innovate in the process, improving the performance of AIs at the same time their population expands.</p><p>It therefore seems likely that, unless we coordinate to deliberately slow AI-driven growth, the introduction of AIs that can substitute for human labor could drastically increase the growth rate of the economy, at least before physical bottlenecks prevent further acceleration, which may only happen at an extremely high level of growth by current standards.</p><p>Strikingly, we can rescue the conclusion of explosive growth even if we dispense with the assumption of technological innovation. Consider that <a href=\"https://www.openphilanthropy.org/research/new-report-on-how-much-computational-power-it-takes-to-match-the-human-brain/\">Carlsmith (2020)</a> estimated that the human brain uses roughly&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"10^{15}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">15</span></span></span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>&nbsp;FLOP per second. If it became possible to design computer software that was as economically useful as the human brain per unit of FLOP, that would suggest we could expand the population of human-equivalent workers as quickly as we can manufacture new computing hardware. Assuming the current price of compute stays constant, at roughly&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"4*10^{17}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">4</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.298em;\">\u2217</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">17</span></span></span></span></span></span></span></span></span></span></span>&nbsp;FLOP/$, re-investing perhaps 45% of economic output into new computing hardware causes economic growth to exceed 30% in this simple model. Obviously, if FLOP prices decrease over time, this conclusion becomes even stronger.</p><p>Of course, knowing that advanced AI can eventually trigger explosive growth doesn't tell us much about when we should expect that to happen. To predict when this productivity explosion will happen, we'll need to first discuss the drivers of AI progress.</p><h1>Part 2: A compute-centered theory of AI automation</h1><p>There appear to be three main inputs to performance in the current AI paradigm of deep learning: algorithmic progress, compute, and data. Leaving aside algorithmic progress for now, I'll present a prima facie case that compute will ultimately be most important for explaining progress in the foreseeable future.</p><p>At the least, there seem to be strong reasons to think that growth in compute played a key role in sustaining AI progress in the past. Almost all of the most impressive AI systems in the last decade, such as AlphaGo Zero and GPT-4, were trained using an amount of compute that would have been prohibitively expensive in, say, 1980. Historically, many AI researchers believed that creating general AI would be more about coming up with the right theories of intelligence, but over and over again, researchers eventually found that impressive results only came after the price of computing fell far enough that simple, \"blind\" techniques began working (<a href=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html\">Sutton 2019</a>).</p><figure class=\"media\"><div data-oembed-url=\"https://ourworldindata.org/grapher/artificial-intelligence-training-computation\">\n\t\t\t\t\t<div data-owid-slug=\"artificial-intelligence-training-computation\" class=\"owid-preview\">\n\t\t\t\t\t\t<iframe src=\"https://ourworldindata.org/grapher/artificial-intelligence-training-computation\">\n\t\t\t\t\t</iframe></div>\n\t\t\t\t</div></figure><p>In the last year, <a href=\"https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications\">we\u2019ve seen predictions</a> that the total amount of data available on the internet could constrain AI progress in the near future. However, researchers at Epoch believe there are a number of strong reasons to doubt these predictions.</p><p>Most importantly, according to recent research on scaling laws, if compute is allocated optimally during training runs, we should expect parameter counts and training data to grow at roughly the same rate (<a href=\"https://arxiv.org/abs/2203.15556\">Hoffmann et al. 2022</a>). Since <a href=\"https://epochai.org/blog/estimating-training-compute#method-1-counting-operations-in-the-model\">training compute is proportional to the number of parameters times the number of training tokens</a>, this implies that data requirements will grow at roughly half the growth rate of compute. As a result, you should generally expect compute to be a greater bottleneck relative to data.</p><p>More detailed investigations have confirmed this finding, with <a href=\"https://arxiv.org/abs/2211.04325\">Villalobos et al. 2022</a> estimating that there is enough low-quality internet data to sustain current trends in dataset size growth until at least 2030 and possibly until 2050.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefg3kksyrfqt\"><sup><a href=\"#fng3kksyrfqt\">[3]</a></sup></span></p><p>It is also possible that we can generate nearly unlimited synthetic data, borrowing compute to generate any necessary data (<a href=\"https://arxiv.org/abs/2210.11610v2\">Huang et al. 2022</a>, <a href=\"https://arxiv.org/abs/2305.07759\">Eldan and Li 2023</a>). Another possibility is that training on multi-modal data, such as video and images, could provide the necessary \u201csynergy,\u201d where performance on one modality is improved by training on the other and vice versa (<a href=\"https://arxiv.org/abs/2301.03728\">Aghajanyan et al. 2023</a>). This would allow models to be trained at much larger scales without being bottlenecked by limited access to textual data. This possibility is lent credibility by research showing that training on multiple modalities may be more beneficial for learning concepts than training on text alone, above a certain scale.</p><p>Broadly speaking, researchers at Epoch currently think that the total availability of data will not constrain general AI progress in the foreseeable future. Nonetheless, the idea of a general data bottleneck is important to distinguish from the idea that AI will be bottlenecked by <i>data on particular tasks.</i> While we don't seem likely to run out of internet data in the medium-term future, it seems relatively more likely that widespread automation will be bottlenecked by data on a subset of essential tasks that require training AI on scarce, high-quality data sources.</p><p>If we become bottlenecked by essential but hard-to-obtain data sources, then AI progress will be constrained until these bottlenecks are lifted, delaying the impacts of AI. Nonetheless, the possibility of a very long delay, such as one lasting 30 years or more, appears less plausible in light of recent developments in language models.</p><p>For context, in the foundation models paradigm, models are trained in two stages, consisting of a pre-training stage in which the model is trained on a vast, diverse corpus, and a fine-tuning stage in which the model is trained on a narrower task, leveraging its knowledge from the pre-training data (<a href=\"https://arxiv.org/abs/2108.07258\">Bommasani et al. 2021</a>). The downstream performance of foundation models seems to be well-described by a scaling law for transfer, as a function of pre-training data size, fine-tuning data size, and the transfer gap (<a href=\"https://arxiv.org/abs/2102.01293\">Hernandez et al. 2021</a>, <a href=\"https://arxiv.org/abs/2108.11018\">Mikami et al. 2021</a>). The transfer gap is defined as the maximum possible benefit of pre-training.</p><p>If the transfer gap from one task to another is small, then compute is usually more important for achieving high performance than fine-tuning data. That\u2019s because, if the transfer gap is small, better performance can be efficiently achieved by simply training longer on the pre-training distribution with more capacity, and transferring more knowledge to the fine-tuning task. Empirically, it seems that the transfer gap between natural language tasks is relatively small since large foundation models like GPT-4 can obtain state-of-the-art performance on a wide variety of tasks despite minimal task-specific fine-tuning (<a href=\"https://arxiv.org/abs/2303.08774\">OpenAI 2023</a>).</p><p>Of course, the transfer gap between non-language tasks may be larger than what we\u2019ve observed for language tasks. In particular, the transfer gap from simulation to reality in robotics may be large and hard to close, which many roboticists have claimed before (e.g., <a href=\"https://lilianweng.github.io/posts/2019-05-05-domain-randomization/\">Weng 2019</a>). However, there appear to be relatively few attempts to precisely quantify the size of the transfer gap in robotics, and how it's changing over time for various tasks (<a href=\"https://sim2real.github.io/assets/papers/2020/paull.pdf\">Paull 2020</a>). Until we better understand the transfer gap between robotic tasks, it will be hard to make confident statements about what tasks might be limited more by data than compute.</p><p>That said, given the recent impressive results in language models, it is likely that the transfer gap between intellectual tasks is not prohibitively large. Therefore, at least for information-based labor, compute rather than data will be the more important bottleneck when automating tasks. Since nearly 40% of jobs in the United States can be performed entirely remotely, and these jobs are responsible for a disproportionate share of US GDP (<a href=\"https://www.nber.org/papers/w26948\">Neiman et al. 2020</a>), automating only purely intellectual tasks would plausibly increase growth in output many-fold, having a huge effect on the world.</p><h3>What about algorithmic progress?</h3><p>The above discussion paints an incomplete picture of AI progress, as it neglects the role of algorithmic progress. Algorithmic progress lowers the amount of training compute necessary to achieve a certain level of performance over time and, in at least the last decade, it\u2019s been very rapid. For example, <a href=\"https://arxiv.org/abs/2212.05153\">Erdil and Besiroglu 2022</a> estimated that the training compute required to reach a fixed level of performance on ImageNet has been cutting in half roughly every nine months, albeit with wide uncertainty over that value.</p><p>In fact, algorithmic progress has been found to be similarly as important as compute for explaining progress across a variety of different domains, such as Mixed-Integer Linear Programming, SAT solvers, and chess engines -- an interesting coincidence that can help shed light on the source of algorithmic progress (<a href=\"https://arxiv.org/abs/2206.09787\">Koch et al. 2022</a>, <a href=\"https://intelligence.org/files/AlgorithmicProgress.pdf\">Grace 2013</a>). From a theoretical perspective, there appear to be at least three main explanations of where algorithmic progress ultimately comes from:</p><ol><li>Theoretical insights, which can be quickly adopted to improve performance.</li><li>Insights whose adoption is enabled by scale, which only occurs after there's sufficient hardware progress. This could be because some algorithms don't work well on slower hardware, and only start working well once they're scaled up to a sufficient level, after which they can be widely adopted.</li><li>Experimentation in new algorithms. For example, it could be that efficiently testing out all the reasonable choices for new potential algorithms requires a lot of compute.</li></ol><p>Among these theories, (1) wouldn't help to explain the coincidence in rates mentioned earlier. However, as noted by, e.g., <a href=\"https://www.overcomingbias.com/p/why-does-hardware-grow-like-algorithmshtml\">Hanson 2013</a>, theories (2) and (3) have no problem explaining the coincidence since, in both cases, what ultimately drove progress in algorithms was progress in hardware. In that case, it appears that we once again have a prima facie case that compute is the most important factor for explaining progress.&nbsp;</p><p>Nonetheless, since this conclusion is speculative, I recommend we adopt it only tentatively. In general, there are still many remaining uncertainties about what drives algorithmic progress and even the rate at which it is occurring.</p><p>One important factor affecting our ability to measure algorithmic progress is the degree to which algorithmic progress on one task generalizes to other tasks. So far, much of our data on algorithmic progress in machine learning has been on ImageNet. However, there seem to be two ways of making algorithms more efficient on ImageNet. The first way is to invent more efficient learning algorithms that apply to general tasks. The second method is to develop task-specific methods that only narrowly produce progress on ImageNet.</p><p>We care more about the rate of general algorithmic progress, which in theory will be overestimated by measuring the rate of algorithmic progress on any specific narrow task. This consideration highlights one reason to think that estimates overstate algorithmic progress in a general sense.</p><h1>Part 3: Predictability of AI performance</h1><p>Even if compute is the ultimate driver of progress in machine learning, with algorithmic progress following in lockstep, in order to forecast when to expect tasks to be automated, we first need some way of mapping training compute to performance.</p><p>Fortunately, there are some reasons to think that such a method can be developed. Despite some claims that <a href=\"https://www.lesswrong.com/posts/G993PFTwqqdQv4eTg/is-ai-progress-impossible-to-predict\">AI performance is inherently unpredictable</a> as a function of scale, there are reasons to think this claim is overstated. Recently, Owen 2023 <i>[link forthcoming]</i> found that for most tasks he considered, AI performance can be adequately retrodicted by taking into account upstream loss as predicted by neural scaling laws. This approach was unique because it leveraged more information than prior research, which mostly tried to retrodict performance as a function of model scale, leaving aside the role of training data size.</p><p>Perhaps the most significant barrier standing in the way of predicting model performance is the existence of emergent abilities, defined as abilities that suddenly appear at certain model scales and thus cannot be predicted by extrapolating performance from lower scales (<a href=\"https://arxiv.org/abs/2206.07682\">Wei et al. 2022</a>). There is currently an active debate about the prevalence and significance of emergent abilities, with some declaring that supposedly emergent abilities are mostly a mirage (<a href=\"https://arxiv.org/abs/2304.15004\">Schaeffer et al. 2023</a>).</p><p>Overgeneralizing a bit, at Epoch we are relatively more optimistic that AI performance is predictable, or at least will become fairly predictable in the near-term future. While this is a complex debate relying on a number of underexplored lines of research, we have identified some preliminary reasons for optimism.</p><h2>Why predicting AI performance may be tractable</h2><p>For many examples of emergence, it appears to result from the fact that performance on the task is inherently discontinuous. For example, there is a discrete difference between being unable to predict the next number in the sequence produced by the <a href=\"https://en.wikipedia.org/wiki/Middle-square_method\">middle-square algorithm</a>, and being able to predict the next number perfectly (which involves learning the algorithm). As a result, it is not surprising that models might learn this task suddenly at some scale.</p><p>However, most tasks that we care about automating, such as scientific reasoning, do not take this form. In particular, it seems plausible that for most important intellectual tasks, there is a smooth spectrum between \"being able to reason about the subject at all\" and \"being able to reason about the subject perfectly.\" This model predicts that emergence will mostly occur on tasks that won't lead to sudden jumps in our ability to automate labor.</p><p>Moreover, economic value typically does not consist of doing only one thing well, but rather doing many complementary things well. It appears that emergence is most common on narrow tasks rather than general tasks, which makes sense if we view performance on general tasks as an average performance over a collection of narrow subtasks.&nbsp;</p><p>For example, Owen 2023<strong> </strong><i>[link forthcoming]</i> found that performance appeared to increase smoothly on both an average of BIG-bench and MMLU tasks as a function of scale \u2014 both of which included a broad variety of tasks requiring complex reasoning abilities to solve. These facts highlight that we may see very smooth increases in average performance over the collection of all tasks in the economy as a function of scale.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/z0zeijow9ao7weftxiht\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/kpbcfw64kx5fi27qfmqk 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/phng9ympg304ybhvmrsm 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/fsm1b6cf9rccqmlqrsir 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/uxtrnt5ltp0dhl3oflze 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/wiohlznrhyyth45veozw 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/yfpz19ppg3zbbwqpzdff 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/wcqvylgr9tiqmw74rhph 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/rksl2rnc8hcltbuskpg9 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/lfuun5rewkn3pxfhjcbl 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/eptovlexx0rx4oimyyju 1928w\"><figcaption>Aggregate benchmark performance is fairly predictable from scale. Graph from Owen 2023 <i>[link forthcoming]</i>).</figcaption></figure><p>It is also important to note that additional information beyond training inputs can be incorporated to predict model performance. For example, Jason Wei <a href=\"https://www.jasonwei.net/blog/common-arguments-regarding-emergent-abilities\">points out</a> that people have yet to fully explore using surrogate metrics to predict performance on primary metrics.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz5hy5996kzs\"><sup><a href=\"#fnz5hy5996kzs\">[4]</a></sup></span></p><h2>Predicting performance via a theoretical model</h2><p>Of course, even if AI performance is, in principle, predictable as a function of scale, we lack data on how AIs are currently improving on the vast majority of tasks in the economy, hindering our ability to predict when AI will be widely deployed. While we hope this data will eventually become available, for now, if we want to predict important AI capabilities, we are forced to think about this problem from a more theoretical point of view.&nbsp;</p><p>The \"Direct Approach\" is my name for a theoretical model that attempts to shed some light on the problem of predicting transformative AI that seeks to complement other models, mainly <a href=\"https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/\">Ajeya Cotra's biological anchors</a> approach.&nbsp;</p><p>The name comes from the fact that it attempts to forecast advanced AI by interpreting empirical scaling laws directly. The following is a sketch of the primary assumptions and results of the model, although we painted a more comprehensive picture in<a href=\"https://epochai.org/blog/the-direct-approach\"> a recent blog post</a> (here's <a href=\"https://epochai.org/files/direct-approach.pdf\">the full report</a>).</p><p>In my view, it seems that the key difficulty in automating intellectual labor is getting a machine to think reliably over long sequences. By \u201cthink reliably,\u201d I mean \u201ccome up with logical and coherent reasons that cohesively incorporate context and arrive at a true and relevant conclusion.\u201d By \u201clong sequences,\u201d I mean sufficiently long horizons over which the hardest and most significant parts of intellectual tasks are normally carried out.</p><p>While this definition is not precise, the idea can be made more amenable to theoretical analysis by introducing a simple framework for interpreting the upstream loss of models on their training distribution.&nbsp;</p><p>The fundamental assumption underlying this framework is the idea that indistinguishability implies competence. For example, if an AI is able to produce mathematics papers that are indistinguishable from human-written mathematics papers, then the AI must be at least as competent as human mathematicians. Otherwise, there would be some way of distinguishing the AI\u2019s outputs from the human outputs.</p><p>With some further mathematical assumptions, we can use scaling laws to calculate how many tokens it will take, on average, to distinguish between model-generated outputs and outputs in the training distribution, up to some desired level of confidence.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrobwfr8w3z\"><sup><a href=\"#fnrobwfr8w3z\">[5]</a></sup></span></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/jgjzzyudiukiafswtqip\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/gnmge01bn03go8b9axsu 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/dvyu5lrgeilzbxzcuvf9 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/y9pf1h3cbld3c8cb72on 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/tewti5vceia2hp9xlneo 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/kulxys7jyvrpkqzutcbs 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/rawzlkd1rsqf8mto0o0n 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/hresio5bxhcc3pikvgku 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/sn0mmbaynn8z3xfpws6t 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/e1hzsqfcvo2o1irbgpbw 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/ugimpimmcjbcz1crslex 1430w\"></figure><p>Given the assumption that indistinguishability implies competence, this method permits a way of upper bounding the training compute required to train a model to reach competence over long horizon lengths, i.e., think reliably over long sequences. The model yields an upper bound rather than a direct estimate, since alternative ways of creating transformative AI than emulating human behavior using deep learning may be employed in the future.</p><p>These results can be summarized in the following plot, which shows how distinguishability evolves as a function of compute, given 2022 algorithms. The two essential parameters are (1) the horizon length over which you think language models must be capable of performing reliable high-quality reasoning before they can automate intellectual labor, and (2) the ability of humans to discriminate between outputs relative to an ideal predictor (called the \"slowdown\" parameter), which sets a bound on how good humans are at picking out \"correct\" results. Very roughly, lower estimates in the slowdown correspond to thinking that model outputs must be indistinguishable according to a more intelligent judge in order for the model to substitute for real humans, with slowdown=1 being an ideal Bayesian judge.<img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/z5dyfepisklkhjiptopw\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/p1skrrtrkrtefgpxlc8u 160w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/qrm5ccaab4e1cb4nhu0y 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/ttsoyibolpbf5aqbz4d9 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/wt6wusgns4nf6j5syete 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/zuoqgeoulqqqvnz3gwdd 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/sqepmig009ii0ozhkrwl 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/zc2xgutvfjtbn79temyb 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/i9uowogkwklcyri1vk4l 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/tor4fulfwcdfhhiw1uzh 1440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/gt6x5indjfj8dudrwfr6 1546w\"></p><p>Since the Direct Approach relies on <a href=\"https://arxiv.org/abs/2203.15556\">Hoffmann et al. 2022</a>, which trained on general internet data, it might not apply to more complex distributions. That said, in practice, power law scaling has been found to be ubiquitous across a wide variety of data distributions, and even across architectures (<a href=\"https://arxiv.org/abs/2010.14701\">Henighan et al. 2020</a>, <a href=\"https://arxiv.org/abs/2207.10551\">Tay et al. 2022</a>). While the coefficients in these power laws are often quite different, the scaling exponents appear very similar across data distributions (<a href=\"https://arxiv.org/abs/2010.14701\">Henighan et al. 2020</a>). Therefore, although one's estimate using the Direct Approach may systematically be biased in light of the simplicity of data used in the Hoffmann et al. 2022 study, this bias can plausibly be corrected by simply keeping the bias in mind (for example, by adding 2 OOMs of compute to one's estimate).</p><p>After taking into account this effect, I personally think the Direct Approach provides significant evidence that we can probably train a model to be roughly indistinguishable from a human over sequences exceeding the average scientific manuscript using fewer than ~<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"10^{35}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">35</span></span></span></span></span></span></span></span></span></span></span>&nbsp;FLOP, using 2022 algorithms. Notably, this quantity is much lower than the evolutionary anchor of ~<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"10^{41}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">41</span></span></span></span></span></span></span></span></span></span></span>&nbsp;FLOP found in the <a href=\"https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines\">Bio Anchors report</a>; and it's still technically a (soft) upper bound on the true amount of training compute required to match human performance across a wide range of tasks.</p><h1>Part 4: Modeling AI timelines</h1><p>If compute is the central driving force behind AI, and transformative AI (TAI) comes out of something looking like our current paradigm of deep learning, there appear to be a small set of natural parameters that can be used to estimate the arrival of TAI.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffmli5nt1a99\"><sup><a href=\"#fnfmli5nt1a99\">[1]</a></sup></span>&nbsp;These parameters are:</p><ul><li>The <strong>total training compute</strong> required to train TAI</li><li>The average <strong>rate of growth in spending on the largest training runs</strong>, which plausibly hits a maximum value at some significant fraction of GWP</li><li>The average <strong>rate of increase in price-performance for computing hardware</strong>&nbsp;</li><li>The average <strong>rate of growth in algorithmic progress</strong></li></ul><p>Epoch has built <a href=\"https://epochai.org/blog/direct-approach-interactive-model\">an interactive model</a> that allows you to plug in your own estimates over these parameters into this model, and obtain a distribution over the arrival of TAI.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/oo1kjjfy1qkfvkpssb1m\"><figcaption>An overview of the interactive model the Epoch team has developed.</figcaption></figure><p>The interactive model provides some default estimates for each of the parameters. We employ the Direct Approach to estimate a distribution over training compute requirements, although the distribution given by Biological Anchors can easily be used in its place. We use historical data to estimate parameter values. In our experience, given reasonable parameter values, the model typically reveals short or medium-length timelines, in the range of 5-45 years from now.</p><p>Nonetheless, this model is conservative for two key reasons. The first is that the Direct Approach only provides an upper bound on the training compute required to train TAI, rather than a direct estimate. The second reason is that this model assumes that the rate of growth in spending, average progress in price-performance, and growth in algorithmic progress will continue at roughly the same rates as in the past. However, if AI expands economic output, it could accelerate the growth rate of each of these parameters.&nbsp;</p><p>Addressing how the gradual automation of tasks can speed up the trends so far would take a more careful analysis than I can present in this article. To get an intuition of how strong the effect is, we can study <a href=\"https://takeoffspeeds.com/\">the model developed by Davidson (2023)</a>. Under the default parameter choice, full automation is reached around 2040, or 17 years from now. If we disable the effects of partial automation,&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefytb5b65bxu\"><sup><a href=\"#fnytb5b65bxu\">[6]</a></sup></span>&nbsp;this outcome is achieved in 2053, or 30 years from now.</p><h2>Against very short timelines</h2><p>The compute-centric framework also provides some evidence against very short timelines (&lt;4 years). One simple argument involves looking at the stock of current compute, and seeing whether transformative growth is possible if we immediately received the software for AGI at the compute-efficiency matching the human brain. Note that this is a bold assumption unless you think AGI is imminent, yet even in this case, transformative growth is doubtful in the very short term.</p><p>To justify this claim, consider that NVIDIA's H100 80 GB GPUs currently cost about <a href=\"https://www.tomshardware.com/news/nvidia-hopper-h100-80gb-price-revealed\">$30k</a>. Given that NVIDIA dominates the AI-relevant hardware market, we can infer that, since its data center revenue is <a href=\"https://investor.nvidia.com/news/press-release-details/2023/NVIDIA-Announces-Financial-Results-for-First-Quarter-Fiscal-2024/default.aspx\">currently on the order</a> of $15 billion per year, and given the compound annual growth rate in its revenue at <a href=\"https://companiesmarketcap.com/nvidia/revenue/\">~21.3% since 2012</a> plus rapid hardware progress in the meantime, there's likely less than $75 billion dollars worth of AI hardware currently in the world right now on par with the H100 80 GB. Since the H100 80 GB GPU can <a href=\"https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet\">use at most</a> ~4000 TFLOP/s, this means that all the AI hardware in the world together can produce at most about&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"10^{22}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">22</span></span></span></span></span></span></span></span></span></span></span>&nbsp;FLOP/s.</p><p>The median estimate of the compute for the human brain from <a href=\"https://www.openphilanthropy.org/research/how-much-computational-power-does-it-take-to-match-the-human-brain/\">Carlsmith (2020) </a>is &nbsp;~<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"10^{15}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">15</span></span></span></span></span></span></span></span></span></span></span>&nbsp;FLOP/s, which means that if we suddenly got the software for AGI with a compute-efficiency matching the human brain, we could at most sustain about <a href=\"https://www.wolframalpha.com/input?i=%28%28%2475+billion+%2F+%2430%2C000%29+*+4000+teraflops%29%2F1000+teraflops\">10 million human-equivalent workers</a> given the current stock of hardware.</p><p>Even if you are inclined to bump up this estimate by 1-2 orders of magnitude given facts like (1) an AGI could be as productive as a 99th percentile worker, and (2) AGI would never need to sleep and could work at full-productivity throughout the entire day, the potential AGI workforce deployed with current resources would still be smaller than the current human labor force, which is <a href=\"https://en.wikipedia.org/wiki/List_of_countries_by_labour_force\">estimated at around 3 billion</a>.&nbsp;</p><p>Moreover, nearly all technologies experience a lag between when they're first developed and when they become adopted by billions of people. Although <a href=\"https://www.brookings.edu/blog/future-development/2020/01/17/whoever-leads-in-artificial-intelligence-in-2030-will-rule-the-world-until-2100/\">technology adoption lags have fallen greatly over time</a>, the evidence from recent technologies indicates that the lag for AI will likely be more than a year.</p><p>Therefore, it appears that even if we assume that the software for AGI is right around the corner, as of 2023, computer hardware manufacturing must be scaled up by at least an order of magnitude, and civilization will require a significant period of time before AI can be fully adopted. Given that a typical semiconductor fab takes approximately <a href=\"https://www.intel.com/content/dam/www/central-libraries/us/en/documents/what-does-it-take-to-build-a-fab.pdf\">three years to build</a>, it doesn't seem likely that explosive growth (&gt;30% GWP growth) will happen within the next 4 years.</p><p>Note that I do think very near-term transformative growth is plausible <i>if</i> the software for <i>superintelligence</i> is imminent.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwdf2y5qkgud\"><sup><a href=\"#fnwdf2y5qkgud\">[7]</a></sup></span>&nbsp;However, since current AI systems are not close to being superintelligent, it seems that we would need a sudden increase in the rate of AI progress in order for superintelligence to be imminent. The most likely cause of such a sudden acceleration seems to be that pre-superintelligent systems could accelerate technological progress. But, as I have just argued above, a rapid general acceleration of technological progress from pre-superintelligent AI seems very unlikely in the next few years.</p><h2>My personal AI Timelines</h2><p>As of May 2023, I think very short (&lt;4 years) TAI timelines are effectively ruled out (~1%) under current evidence, given the massive changes that would be required to rapidly deploy a giant AI workforce, the fact that AGI doesn't appear imminent, and the <a href=\"https://sideways-view.com/2018/02/24/takeoff-speeds/\">arguments against hard takeoff</a>, which I think are strong. I am also inclined to cut some probability away from short timelines given the lack of impressive progress in general-purpose robotics so far, which seems like an important consideration given that the <a href=\"https://www.nber.org/papers/w26948\">majority of labor in the world currently requires a physical component</a>. That said, assuming no substantial delays or large disasters such as war in the meantime, I believe that TAI will probably arrive within 15 years.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxbx3jx2slgk\"><sup><a href=\"#fnxbx3jx2slgk\">[8]</a></sup></span>&nbsp;My view here is informed by my distribution over TAI training requirements, which is centered somewhere around&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"10^{32}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">32</span></span></span></span></span></span></span></span></span></span></span>&nbsp;FLOP using 2023 algorithms with a standard deviation of ~3 OOMs.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefn9bwpxrw46o\"><sup><a href=\"#fnn9bwpxrw46o\">[9]</a></sup></span></p><p>However, my unconditional view is somewhat different. After considering all potential delays (including regulation, <a href=\"https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines?commentId=FQFp2oJ48boHhNXxP\">which I think is likely to be substantial</a>) and model uncertainty, my overall median TAI timeline is more like 20 years from now, with a long tail extending many decades into the future.</p><p>To sum up what I think all this evidence points to, I plotted a probability distribution to represent my beliefs over the arrival of TAI below. Note the fairly large difference between the median (2045) and the mode (2029).&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnp4n23nggh\"><sup><a href=\"#fnnp4n23nggh\">[10]</a></sup></span></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/dztcuzvt2zbsyrz0fgmz\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/btyupmj1ftvwxdo72i9j 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/v02qxiosximvgznzfhfi 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/jbxdruei2qnixixiz1n7 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/fuiygj6evja5kc3pe8uy 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/oyvmun8rzqwcvycam2rd 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/cb7dpqpsvyzgqnqobkol 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/hev4tvtbcax6eygoo0ax 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/ap1jwvz1tdijbcgedwby 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/tkneaas548iccvlvsicd 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/yzgbukqin6sq0tdkfvlb 2053w\"></figure><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/j278kflnvp2ypmqz2urb\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/rdntceeugqkzigrflwdg 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/q20milijcxjuj00uoxxe 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/gwrh8lmyng80m5jjpg57 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/miogelsyd8tmstmtw8oz 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/agid3ejjjqgizqfg19go 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/cj2xyrnb7eb65qnoznys 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/shgde2sngofknfj6cyol 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/en8tlgzr3pri1fmfqwlx 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/ib9njqawh9d63slsuobh 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/rz1ka7gfsab4il88ddsc 2055w\"></figure><p>Given this plot, we can look at various years and find my unconditional probability of TAI arriving before that year. See footnote&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcm5tawm7s6s\"><sup><a href=\"#fncm5tawm7s6s\">[11]</a></sup></span>. For reference, there is a 47% chance that TAI will arrive before 2043 in this distribution.</p><h1>Conclusion</h1><p>Assuming these arguments hold, it seems likely that transformative AI will be developed within the next several decades. While in this essay, I have mostly discussed the potential for AI to accelerate economic growth, other downstream consequences from AI, such as value misalignment and human disempowerment, are worth additional consideration, to put it mildly (<a href=\"https://www.effectivealtruism.org/articles/three-impacts-of-machine-intelligence-paul-christiano\">Christiano 2014</a>, <a href=\"https://arxiv.org/abs/2209.00626\">Ngo et al. 2022</a>).</p><p><i>I thank &nbsp;Tamay Besirogly, Jaime Sevilla, David Owen, Ben Cottier, Anson Ho, David Atkinson, Eduardo Infante-Roldan and the rest of the Epoch team for their support and suggestions through this article. &nbsp;Adam Papineau provided copy-writing.</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfmli5nt1a99\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffmli5nt1a99\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Following <a href=\"https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/\">Karnofsky 2016</a> and <a href=\"https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines\">Cotra 2020</a>, we will define TAI in terms of potential transformative consequences from advanced AI \u2014 primarily accelerated economic growth \u2014 but with two more conditions that could help cover alternative plausible scenarios in which AI has extreme and widespread consequences.</p><p><strong>Definition of TAI:</strong> let's define the year of TAI as the first year following 2022 during which any of these milestones are achieved:</p><p>1. Gross world product (GWP) exceeds 130% of its previous yearly peak value<br>2. World primary energy consumption exceeds 130% of its previous yearly peak value<br>3. Fewer than one billion biological humans remain alive on Earth</p><p>The intention of the first condition is highlighted throughout this article, as I describe the thesis that the most salient effect of AI will be its ability to automate labor, triggering a productivity explosion.</p><p>The second milestone is included in order to cover scenarios in which the world is rapidly transformed by AI but this change is not reflected in GDP statistics -- for example, if GDP is systematically and drastically mismeasured. The third milestone covers scenarios in which AI severely adversely impacts the world, even though it did not cause a productivity explosion. More specifically, I want to include the hard takeoff scenario considered by Eliezer Yudkowsky and Nick Bostrom, as it's hard to argue that AI is not \"transformative\" in this case, even if AI does not cause GWP or energy consumption to expand dramatically.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnz4avdnckk7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefz4avdnckk7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For a more in-depth discussion regarding the productivity explosion hypothesis, I recommend reading <a href=\"https://www.openphilanthropy.org/research/could-advanced-ai-drive-explosive-economic-growth/\">Davidson 2021</a>, or <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/Philip-Trammell-and-Anton-Korinek_economic-growth-under-transformative-ai.pdf\">Trammell and Korinek 2020</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fng3kksyrfqt\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefg3kksyrfqt\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This conclusion was somewhat conservative since it assumed that we will run out of data after the number of tokens seen during training exceeds our total stock of textual internet data. However, while current large language models are rarely trained over more than one epoch (i.e., one cycle through the full training dataset), there don't appear to be any strong reasons why models can't instead be trained over more than one epoch, which was standard practice for language models before about 2020 (<a href=\"https://arxiv.org/abs/1906.06669\">Komatsuzaki 2019</a>).</p><p>While some have reported substantial performance degradation while training over multiple epochs (<a href=\"https://arxiv.org/abs/2205.10487\">Hernandez et al. 2022</a>), other research teams have not (<a href=\"https://arxiv.org/abs/2211.09085\">Taylor et al. 2022</a>). Since we do not at the moment see any strong theoretical reason to believe that training over multiple epochs is harmful, we suspect that performance segregation from training on repeated data is not an intractable issue. However, it may be true that there is only a slight benefit from training over multiple epochs, which would make this somewhat of a moot point anyway (<a href=\"https://arxiv.org/abs/2305.13230\">Xue 2023</a>).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnz5hy5996kzs\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefz5hy5996kzs\">^</a></strong></sup></span><div class=\"footnote-content\"><p>To help explain the utility of surrogate metrics, here's a potential example. It might be that while accuracy on multi-step reasoning tasks improves suddenly as a function of scale, performance on single-step reasoning improves more predictably. Since multi-stage reasoning can plausibly be decomposed into a series of independent single-step reasoning steps, we'd expect <i>a priori</i> that performance on multi-step reasoning might appear to \"emerge\" somewhat suddenly at some scale as a consequence of the mathematics of successive independent trials. If that's true, researchers could measure progress on single-step reasoning and use that information to forecast when reliable multi-step reasoning will appear.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrobwfr8w3z\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrobwfr8w3z\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The central idea is that given model scaling data, we can estimate how the reducible loss falls with scale, which we can interpret as the <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\">KL-divergence</a> of the training distribution from the model. Loosely speaking, since KL-divergence tells us the <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Discrimination_information\">discrimination information</a> between two distributions, we can use it to determine how many samples, on average, are required to distinguish the model from the true distribution.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnytb5b65bxu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefytb5b65bxu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is achieved by setting the training FLOP gap parameter equal to 1 in the takeoff speeds model.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwdf2y5qkgud\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwdf2y5qkgud\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We will follow <a href=\"https://nickbostrom.com/ethics/ai\">Bostrom 2003</a> in defining a superintelligence as \"any intellect that is vastly outperforms the best human brains in practically every field, including scientific creativity, general wisdom, and social skills.\"</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxbx3jx2slgk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxbx3jx2slgk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Here are my personal TAI timelines conditional on no coordinated delays, no substantial regulation, no great power wars or other large exogenous catastrophes, no global economic depression, and that my basic beliefs about reality are more-or-less correct (e.g. I'm not living in a simulation or experiencing a psychotic break).</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/thuhspwh7s9aovjtcsqq\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/pe99mrrawnfbiatck3pq 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/dri7emukeksfxf5jjz7h 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/ssedlzclcbsoy9dpf2gz 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/zmn1mvbdsvd27srcv08s 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/je4xmymlzwo1hgjvtv3b 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/i86sugag0bfyjxkxhijy 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/rnaihbdz800jzztne25v 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/fovgjhqw4sq0cud6eug5 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/lr0zye15psyp4blybji9 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/ge2zv7lu8qk0e8wunseg 2058w\"></figure><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/lntzzvu3nnkydamhxkja\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/mgdma2coeiycosjsqipd 210w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/vut8mp7m3pavazxx6mk7 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/eyeckpnrhvw540ggibrr 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/wy20l8ra11kvrs1zpfxx 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/kggh84i2q5tuorrxvilq 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/fnpfxq8nlrwjekztgzld 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/qfxtqf69hddfehxsftgi 1470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/lhgjy2pdloirzb7c7s2d 1680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/rhtcqbwfpvbthznsw0vd 1890w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fsaogRokXxby6LFd7/r75pgbvenvtmjtueqxbb 2058w\"></figure></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnn9bwpxrw46o\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefn9bwpxrw46o\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I'll define transformative FLOP requirements as the size of the largest training run in the year prior to the year of TAI (definition given in footnote 1) assuming that no action is taken to delay TAI, such as substantial regulation.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnp4n23nggh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnp4n23nggh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Note that this distribution was changed slightly in light of <a href=\"https://forum.effectivealtruism.org/posts/fsaogRokXxby6LFd7/a-compute-based-framework-for-thinking-about-the-future-of?commentId=8uFLE76GyaA88z5Cb\">some criticism</a> in the comments. I think the previous plot put an unrealistically low credence on TAI arriving before 2030.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fncm5tawm7s6s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefcm5tawm7s6s\">^</a></strong></sup></span><div class=\"footnote-content\"><figure class=\"table\" style=\"height:10px\"><table><tbody><tr><td><strong>Year</strong></td><td><strong>P(TAI &lt; Year)</strong></td></tr><tr><td>2030</td><td>18%</td></tr><tr><td>2035</td><td>34%</td></tr><tr><td>2040</td><td>43%</td></tr><tr><td>2043</td><td>47%</td></tr><tr><td>2045</td><td>50%</td></tr><tr><td>2050</td><td>54%</td></tr><tr><td>2060</td><td>61%</td></tr><tr><td>2070</td><td>66%</td></tr><tr><td>2080</td><td>71%</td></tr><tr><td>2090</td><td>74%</td></tr><tr><td>2100</td><td>78%</td></tr></tbody></table></figure></div></li></ol>", "user": {"username": "Matthew_Barnett"}}, {"_id": "HMXAgWrCZ6aagHYqg", "title": "Updates from the Dutch EA community", "postedAt": "2023-05-30T10:40:27.759Z", "htmlBody": "<p>We wanted to post something on the Forum to share all of the amazing things the Dutch EA community has achieved in the past 18 months or so. But we also wanted to avoid spending too much time writing it. So please accept this very messy post and feel free to ask questions in the comments! Parts were co-written by ChatGPT with minimal editing, hence the sometimes overly braggadocious tone.</p><p>We start with national-level updates and two quick lessons-learnt, and then we have bullet-point summaries from some of the local groups. But first, an executive summary.</p><h1>Executive summary</h1><p>Over the past year, the Dutch EA community has seen impressive growth at both the national and local levels.</p><p>At the national level, the community has seen significant gains. The number of intro programme completions increased nearly tenfold, from 45 in 2021 to 400 in 2022. The number of city groups and university groups also grew, from 1 to 3 and 1 to 13 respectively. Notably, there was an influx of \u20ac700k donations via&nbsp;<a href=\"https://doneereffectief.nl\"><u>Doneer Effectief</u></a> and an increase in EA Netherlands&nbsp;<a href=\"https://effectiefaltruisme.us14.list-manage.com/subscribe?u=0d235948217a55858a0e810c4&amp;id=d652eb1a9c\"><u>newsletter</u></a> subscribers from 670 to around 1500.</p><p>Since hiring two full-time community builders in 2022, EAN has helped establish over a dozen new groups which have collectively produced 350 intro programme graduates in 2022 alone. In addition to launching a new&nbsp;<a href=\"https://effectiefaltruisme.nl/\"><u>website</u></a> and&nbsp;<a href=\"https://effectiefaltruisme.nl/office/\"><u>co-working space</u></a>, EAN organized multiple retreats, conducted introductory talks, facilitated 'giving games', provided career counselling, hosted city meet-ups, and participated in a&nbsp;<a href=\"https://arminius.nl/stream/effectief-altruisme-met-rutger-bregman/\"><u>public debate on EA</u></a>.</p><p>Effective altruism is gaining recognition in the Dutch media, with&nbsp;<a href=\"https://www.groene.nl/artikel/red-de-wereld-met-tien-procent-van-je-salaris\"><u>coverage</u></a> in&nbsp;<a href=\"https://www.trouw.nl/tijdgeest/welkom-in-de-wereld-van-mensen-die-10-procent-of-meer-van-hun-inkomen-weggeven-om-levens-te-redden~bd56b016/?utm_campaign=shared_earned&amp;utm_medium=social&amp;utm_source=whatsapp\"><u>major</u></a> Dutch publications and appearances by prominent figures like writer Rutger Bregman. However, there have also been a few&nbsp;<a href=\"https://jacobin.nl/effectief-altruisme-is-neoliberale-liefdadigheid/\"><u>critical</u></a> pieces, to which the EAN board has&nbsp;<a href=\"https://effectiefaltruisme.nl/letter-in-nrc/\"><u>responded</u></a>.</p><p>Other significant achievements include the successful launch of&nbsp;<a href=\"https://doneereffectief.nl\"><u>Doneer Effectief's online donation platform</u></a>, the high-profile EAGxRotterdam 2022 conference, and the&nbsp;<a href=\"https://tienprocent.club\"><u>Tien Procent Club's</u></a> successful events on effective giving.</p><p><a href=\"https://effectiefaltruisme.nl/sub-groups/\"><u>Local EA groups across Dutch cities</u></a> have also seen substantial growth. For example, the Amsterdam city and university groups have&nbsp;<a href=\"https://www.eaamsterdam.org/\"><u>merged</u></a>, and together they host weekly meetups, multiple programs, and are developing a mental health program. At&nbsp;<a href=\"http://eautrecht.org\"><u>Utrecht</u></a>, the student group has hatched an Alt Protein group with a grant from the university, has launched an AI Safety group, has hosted a big speaker event with Rutger Bregman, and runs introduction fellowships, socials, coworking sessions and other events. In The Hague, the&nbsp;<a href=\"https://www.thehagueea.org\"><u>group</u></a> conducted weekly dinners, three rounds of intro fellowships, and two rounds of AI governance fellowships.</p><p>The team at&nbsp;<a href=\"https://eadelft.org/\"><u>Delft</u></a> has increased EA awareness through fellowships, book clubs, a retreat, and launching the&nbsp;<a href=\"https://www.delftaisafety.org/\"><u>Delft AI Safety Initiative</u></a>.&nbsp;<a href=\"https://www.eaeindhoven.nl/\"><u>Eindhoven\u2019s</u></a> group has engaged 31 people in Introduction Fellowships, has launched an AI safety team, and collaborated with other groups on their university campus.&nbsp;<a href=\"https://www.eanijmegen.nl/\"><u>Nijmegen\u2019s group</u></a> has grown rapidly, with biweekly meetups and collaborations with other campus groups.</p><p>The&nbsp;<a href=\"https://piserotterdam.com/\"><u>PISE group in Rotterdam</u></a> hosts member-only weekly events, open book clubs, and four fellowship rounds this year. They also \u200b\u200binitiated EAGx Rotterdam. The&nbsp;<a href=\"https://m.facebook.com/people/Effective-altruism-Twente/100085384876872/\"><u>Twente group</u></a> has attended the university\u2019s career fair and organized meetups and an introductory talk.&nbsp;<a href=\"https://eawageningen.nl/\"><u>Wageningen University's group</u></a> has hosted live events and completed an introductory fellowship.</p><p>Lessons learnt:&nbsp;</p><ul><li>Do organising <i>and</i> mobilising (organisers invest in developing the capacities of people to engage with others in activism and become leaders; mobilisers focus on maximising the number of people involved without developing their capacity for civic action)</li><li>It's very valuable to have a public figure endorse you</li></ul><h1>National-level updates</h1><h2>Cool EA community building numbers and how they changed between 2021 and 2022</h2><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Cool thing</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>2021&nbsp;</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>2022&nbsp;</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Intro programme completions</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">45</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">~400</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">City groups</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">3</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Uni groups</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">13</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">AIS team/group spin-offs</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">3</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Alt-protein team/group spin-offs</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">1</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">\u2018Professional\u2019 groups (e.g. politics and policy)</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">2</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">80k calls&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">18</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">45</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">New GWWC pledges</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">~50</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">~40</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Money donated via&nbsp;<a href=\"https://doneereffectief.nl\"><u>Doneer Effectief</u></a></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">\u20ac0</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">~\u20ac700k</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Net increase in EA Netherlands newsletter subscribers&nbsp;&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">~90 (580 -&gt; 670)</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">~830 (670 -&gt; 1500)</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Successful Charity Entrepreneurship applicants&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">1</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">EA community building volunteers</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">~10</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">~50</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><a href=\"https://effectiefaltruisme.nl/\"><u>EAN website</u></a> users</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">5.4k</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">10k</td></tr></tbody></table></figure><h2>EA Netherlands since February 2022</h2><p>Below is a selection of our achievements since we hired two full-time community builders in February 2022.&nbsp;</p><p>In terms of local groups, EAN has played a key role in helping establish over a dozen new groups. Collectively, these groups plus EAN produced around 350 intro programme graduates in 2022 (the total number is around 400 once you include the virtual programme). You can read about how we did this with student groups&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/x6NwpTZB6YCq5WMRA/how-to-kick-start-10-university-groups-in-6-months\"><u>here</u></a>. Other groups include&nbsp;<a href=\"https://www.gopagify.com/page#2PACX-1vSAWgpMmkAZJCTevT5frWCaTpixt6ArXThrc1vUvo7t8c7r-zUmmHcCn5Lc0trntgBUDSHc-7vyVmnG\"><u>Polly</u></a> (politics and policy) and the&nbsp;<a href=\"https://www.tienprocent.club/\"><u>Tien Procent Club</u></a> (effective giving).&nbsp;</p><p>Moreover, EAN has launched a&nbsp;<a href=\"https://effectiefaltruisme.nl/office/\"><u>co-working space</u></a>, offering a creative and collaborative environment for people who are passionate about effective altruism. This initiative not only provides a place for EAs to work but also fosters community and sparks innovative ideas.</p><p>In a bid to increase its digital presence, EAN launched a new&nbsp;<a href=\"https://effectiefaltruisme.nl/\"><u>website</u></a>, providing an accessible platform to share information, events, and resources about effective altruism with a wider audience.</p><p>EAN has shown a commitment to ongoing learning and engagement within the EA community through the organisation of four retreats. These were specifically targeted at university organisers, artificial intelligence safety (AIS) enthusiasts, and professionals, respectively.</p><p>The educational efforts of EAN have borne fruit with approximately 60 graduates from their introduction programme. Alongside this, they have conducted 15-20 introductory talks with a total audience of about 725, further spreading awareness and understanding of effective altruism.</p><p>EAN also facilitated five 'giving games', interactive workshops that introduced approximately 115 participants to effective altruist principles of charitable giving.</p><p>In terms of career development, EAN carried out roughly 50 one-on-one career counselling sessions. Additionally, they piloted a career acceleration programme targeted at professionals that received 40 applications and included 30 participants, showing promising uptake for future iterations.</p><p>EAN has also held approximately 10 city meet-ups, each averaging 15 attendees, offering an opportunity for local EA communities to network, learn, and share ideas.</p><p>Furthermore, EAN has provided valuable experience for emerging talent in the EA movement by hosting one intern and employing two individuals over the summer.</p><p>Another achievement was EAN\u2019s participation in a&nbsp;<a href=\"https://www.youtube.com/watch?v=fkJsxxOp80A\"><u>debate</u></a> on effective altruism, attended by approximately 375 people. The event featured a prominent writer (Rutger Bregman), an economics professor, and a philosophy professor, offering diverse perspectives on EA.</p><p>Lastly, EAN has achieved formal employer status and established a payroll system for its employees, marking a significant milestone in its organisational development.</p><h2>EA in the Dutch Media&nbsp;</h2><p>Effective altruism is increasingly being discussed in the Dutch media. For example,&nbsp;<a href=\"https://www.groene.nl/artikel/red-de-wereld-met-tien-procent-van-je-salaris\"><u>a cover story</u></a> about EA in a major Dutch weekly has brought the principles and goals of EA to a broader audience. This was further augmented by a&nbsp;<a href=\"https://www.trouw.nl/tijdgeest/welkom-in-de-wereld-van-mensen-die-10-procent-of-meer-van-hun-inkomen-weggeven-om-levens-te-redden~bd56b016/?utm_campaign=shared_earned&amp;utm_medium=social&amp;utm_source=whatsapp\"><u>12-page feature</u></a> in the weekend supplement of a national newspaper. Most recently, we had a <a href=\"https://www.volkskrant.nl/nieuws-achtergrond/effectief-altruisten-willen-zo-veel-mogelijk-mensen-helpen-voor-hun-geld-en-dat-levert-pittige-dilemma-s-op~b52565b0/\">4-page</a> feature in the weekend supplement of the nation's biggest broadsheet. Perhaps most significantly, the best-selling writer Rutger Bregman has written multiple articles and participated in various podcast episodes discussing EA.&nbsp;</p><p>Effective giving has also featured prominently in the Dutch media thanks to the work of Doneer Effectief\u2019s Director, Bram. All of these appearances can be found on Doneer Effectief\u2019s&nbsp;<a href=\"https://doneereffectief.nl/\"><u>homepage</u></a>.</p><p>There have also been critical pieces. For example,&nbsp;<a href=\"https://jacobin.nl/effectief-altruisme-is-neoliberale-liefdadigheid/\"><u>this</u></a> in Jacobin Nederland,&nbsp;<a href=\"https://www.vn.nl/effectief-altruisme-liefdadigheid-rekenmachine/\"><u>this</u></a> in Vrij Nederland, and&nbsp;<a href=\"https://www.nrc.nl/nieuws/2022/10/13/pas-op-voor-al-te-effectieve-altruisten-a4145081\"><u>this</u></a> in the NRC (to which our board responded&nbsp;<a href=\"https://www.nrc.nl/nieuws/2022/10/19/goed-doen-effectief-altruisten-richten-zich-juist-op-systeemverandering-a4145574\"><u>here</u></a>).</p><h2>Other big-picture things</h2><p>The&nbsp;<a href=\"https://www.tienprocent.club/\"><u>Tien Procent Club</u></a> has found a very successful formula for events organised around the theme of effective giving and moral ambition, with hundreds of people attending each event. &nbsp;</p><p><a href=\"https://doneereffectief.nl\"><u>Doneer Effectief</u></a> launched its online donation platform and raised over \u20ac700k in its first quarter (Q4 2022). Over the EAG London weekend, it reached the \u20ac1 million milestone on its 233rd day as a donation platform. &nbsp;</p><p>EAGxRotterdam 2022 was a big success, with over 650 attendees and the highest \u2018likelihood to recommend\u2019 score for any conference in Europe on CEA's record (stretching back to 2018).</p><p>Most recently, our students came together to organise the SEA (Students for Effective Altruism) summit, which brought together over 75 student delegates from universities across the Netherlands for a retreat/conference.&nbsp;</p><h2>Learnings&nbsp;</h2><ul><li>Read and apply the lessons from <a href=\"https://academic.oup.com/book/38705\">How Organisations Develop Activists</a> &nbsp;(we hadn't read this but I wish I had)</li><li>It's hard to overstate the value of having a public figure endorse effective altruism (as an example, below is what happened to our LinkedIn when Rutger Bregman linked to us)</li></ul><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HMXAgWrCZ6aagHYqg/uuloyz8v1qsdzxvihtje\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HMXAgWrCZ6aagHYqg/qbrk8mghvgwl0mnbhlfz 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HMXAgWrCZ6aagHYqg/zolbflaeyjbchmkbexyd 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HMXAgWrCZ6aagHYqg/ihnpploupsxsnqy8hmx1 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HMXAgWrCZ6aagHYqg/xeubixkgklx8eqmkxqsl 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HMXAgWrCZ6aagHYqg/ppyq4pssjyazgy9hhsqv 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HMXAgWrCZ6aagHYqg/ariorklslt8uygzkdke7 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HMXAgWrCZ6aagHYqg/cdlszvthwp7lia91yezu 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HMXAgWrCZ6aagHYqg/pcbool2kyxzn5ta4pimf 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HMXAgWrCZ6aagHYqg/nem9ya3tkho48duuhuok 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/HMXAgWrCZ6aagHYqg/e58domumarvqy6fmkaf1 1160w\"></figure><h1>A selection of what some Dutch local groups have been up to (in alphabetical order)</h1><h2><a href=\"https://www.eaamsterdam.org\"><u>EA Amsterdam</u></a> (uni + city group)</h2><ul><li>Ran consistent facilitated weekly meetups (discussion nights)</li><li>Multiple programs: ~80 people completed or currently participating (out of ~100 signups)<ul><li>Intro program</li><li>AGI Safety Fundamentals</li><li>AGI Governance</li><li>Awakening from the Meaning Crisis</li></ul></li><li>Developed a Global Mental Health Fellowship, running in May</li><li>&nbsp;Are working with a few PhD students to get an AI safety course implemented at the UvA.</li><li>&nbsp;Convinced a PhD student, and potentially the remainder of his research group at the university to switch from AI capabilities research to AI safety research.</li><li>Collaborating with EA Oxford &amp; London EA Hub to conduct research on community building, measurement, improvements to theories of change, and new programs.</li><li>179 people in our Whatsapp community</li><li>Running a focus group on how to apply psychological practices to improve mental health and rationality of EAs</li><li>Preparing the launch of AI initiative, focusing on setting up AI technical safety and governance pipelines with a focus on middle-of-the-funnel activities in collaboration with existing organisations.</li><li>Setting up project infrastructure for our members to volunteer and upskill in EA relevant projects and skills</li><li>Ran first combined socials with both Uni &amp; City group, set up a framework for collaboration</li><li>Guest speaker event with AI safety researcher</li><li>Ran a guest speaker event on Philosophy, Effective Altruism and Meaning Making with 40 attendees.</li><li>Running a retreat with 20 group members in June.</li><li>~ 20 group members joined EAGx Rotterdam, with a smaller group having gone to EAGx Nordics and EAG London.</li><li>Received recognition by the UvA and a flex office space.</li><li>In addition to the AI safety initiative, incubating a biosecurity and policy subgroup.</li></ul><h2><a href=\"https://eadelft.org\"><u>EA Delft</u></a> (uni group)</h2><ul><li>December \u201822 survey summary (13 people filled it out):&nbsp;<ul><li>11/13 improved EA familiarity with a lot because of EAD&nbsp;</li><li>Change to career/study plans because of EAD: 2/13 big, 8/13 small.</li><li>9/13 have been influenced to donate more. (2 already donated, 1 no change)&nbsp;</li><li>11/13 have become more inspired to have a positive impact&nbsp;</li></ul></li><li>Fellowships:<ul><li>Intro fellowship: 54&nbsp; (+27 including 2023)</li><li>In-depth: 18</li><li>2023: Career fellowship</li></ul></li><li>Book clubs:&nbsp;<ul><li>2022: 11 people WWOTF</li><li>2023: 14 people \u201cThe Scout Mindset\u201d,&nbsp; 15 people \u201cWhy We Love Dogs, Eat Pigs, and Wear Cows\u201d</li></ul></li><li>Bi-weekly discussion evenings</li><li>Bi-weekly socials</li><li>Weekly lunches</li><li>7 organizers (2 for a full year, 5 for ~half a year)</li><li>20 people attended EAGx Rotterdam</li><li>Retreat with 23 people</li><li>Launched Delft AI Safety Initiative with 4 organizers. (3 new, 1 from EAD organizers)</li></ul><h2><a href=\"https://www.eaeindhoven.nl\"><u>EA Eindhoven</u></a> (uni group)</h2><ul><li>Founded in the spring/summer of 2022 and became an official Stichting in December 2022. The founders also participated in&nbsp;<a href=\"https://centreforeffectivealtruism.notion.site/University-Group-Accelerator-Program-6df8c8fccf8b4ffbb6488d9dfa275282\"><u>UGAP</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/6xXqbebZjufoxiAtw/cea-uni-groups-team-plans-and-priorities-for-next-quarter#Organizer_Support_Program__OSP_\"><u>OSP</u></a></li><li>Programs<ul><li>31 people completed or are currently participating in an Introduction Fellowship</li><li>7 people completed a&nbsp;<i>What We Owe the Future</i> reading group</li><li>8 people are participating in a High-Impact Career Mastermind group</li><li>5 people are participating in AGI Safety Fundamentals, run by a knowledgeable facilitator</li></ul></li><li>Other<ul><li>16 people attended EAGxRotterdam</li><li>8 people attended the Dutch Students for EA summit (SEA 2023)</li><li>Collaborations with other groups and organizations on the TU Eindhoven campus</li><li>Launched a&nbsp;<a href=\"https://www.eaeindhoven.nl/forecasting-team\"><u>Forecasting Team</u></a> and onboarded an organizer to lead this</li><li>Launched the&nbsp;<a href=\"https://www.eaeindhoven.nl/ai-safety-team\"><u>Eindhoven AI Safety Team</u></a></li><li>Organized a&nbsp;<a href=\"https://www.eaeindhoven.nl/news/2023211-lunch-lecture-by-tobias-leenaert-how-to-create-a-vegan-world\"><u>lunch lecture</u></a> by&nbsp;<a href=\"https://veganstrategist.org/over/\"><u>Tobias Leenaert</u></a> with over 300 attendees in January 2023</li><li>Hosts biweekly meetups</li><li>84 people in group chat</li><li>112 newsletter subscribers</li></ul></li></ul><h2><a href=\"https://forum.effectivealtruism.org/groups/zW3J2prYsEjKyw4jk\"><u>THEA</u></a> (The Hague city group)&nbsp;</h2><ul><li>Founded in March 2022, became an official Stichting in March 2023</li><li>2 organizers, both participated in UGAP</li><li>~100 official members</li><li>~260 people in the announcement chat</li><li>Weekly dinners at a foodhall (THEA Thursdays) (average of 5 attendees)</li><li>Quarterly big socials, with an activity (bouldering, escape rooms, etc.) followed by a dinner (Big THEA Thursdays) (average of 14 attendees)</li><li>3 rounds of intro fellowships (34 completed or are completing)</li><li>2 rounds of AI governance fellowships (31 completed or are completing)</li><li>1 round of x-risk fellowship (4 completed)</li><li>Event with Tobias Leenaert in collaboration with The Hague Humanity Hub (~25 attendees)</li><li>7 project managers (1 on Tobias event, 3 facilitating intro fellowships, 3 facilitating AI governance fellowships)</li><li>Group of members joining EAGx Berlin, EAGx Rotterdam, and SEA 2023</li><li>Helped organizing SEA</li><li>Collaborated with LDE groups (EA Delft and PISE) for joint socials</li></ul><h2><a href=\"https://www.linkedin.com/in/effective-altruism-chapter-leeuwarden-6b087123b/?originalSubdomain=nl\"><u>EA Leeuwarden</u></a> (uni group at Campus Friesland)</h2><p>Started in November 2022</p><ul><li>Organized an introduction talk about effective altruism</li><li>3 people finished the introduction fellowship</li><li>Hosted an online talk about AI safety</li><li>Organized a members dinner&nbsp;</li><li>1 member has attended EAGx Nordics, 2 members have attended the Students for EA Summit (SEA)</li><li>The organizer is taking part in UGAP</li></ul><h2><a href=\"https://www.eanijmegen.nl\"><u>EA Nijmegen</u></a> (uni group)</h2><ul><li>Launched in September 2022; founder participated in UGAP.</li><li>55 people on our mailing list after tabling at the introduction market.</li><li>23 people in our Whatsapp group.</li><li>23 1-on-1s (some long-term).</li><li>12 people completed an Intro Fellowship.</li><li>5 people attended EAGxRotterdam.</li><li>4 people attending the Student Summit.</li><li>Hosts biweekly meetups (most of the time).</li><li>Collaborating with other groups on campus: Vegan Student Association Nijmegen, Perplex, and Radboud Interdisciplinary Complexity Hub.</li><li>Found a co-organizer who works on the faculty.</li><li>Hosted several lectures, by James Herbert (EAN), Bob Zoutenbier (economist), and Tobias Leenaert (animal welfare activist).</li><li>Planned future programs: Awakening from the Meaning Crisis, Metacrisis Fellowship.</li></ul><h2><a href=\"https://piserotterdam.com\"><u>PISE</u></a> (uni group in Rotterdam)</h2><ul><li>General&nbsp;<ul><li>67 members in our most active Member group chat&nbsp;</li></ul></li><li>General events<ul><li>Hosted member-only weekly events, in the form of either discussions, socials, (guest) lectures&nbsp;</li><li>Ran open events across the year, including \u2013&nbsp;<ul><li>Collaborations with other associations, such as the Vegan Student Association Rotterdam, Rethinking Economics</li><li>Hosted several lectures by local academics, including on nuclear policy, wild animal welfare, and on psychedelics for mental health.&nbsp;</li></ul></li><li>Ran open book clubs throughout the year<ul><li>WWOTF, Mountains Beyond Mountains, You\u2019re Not Listening, The Good It promises, the Harm it Does (critique on EA), Poor Economics, the alignment problem</li></ul></li><li>Hosted our Autumn retreat, and will be running our summer retreat June 2023 (with 30 participants)&nbsp;</li></ul></li><li>Outreach&nbsp;<ul><li>4 fellowship rounds this year</li><li>~68 people who have&nbsp;<i>completed</i> the 5-weeks (attending &gt;80% sessions)</li><li>Deviation on the fellowship by hosting a parallel introductory workshop&nbsp;&nbsp;</li></ul></li><li>Wider Community Events&nbsp;<ul><li>Initiated EAGx Rotterdam with the organising team being predominantly our organisers &amp; members&nbsp;</li><li>12 of us attended EAGx Nordics&nbsp;</li><li>8 of us going to EAGLondon</li><li>Initiated the Students for EA (SEA) summit and co-organised with other student groups&nbsp;&nbsp;</li></ul></li></ul><h2><a href=\"https://www.instagram.com/ea_twente/\"><u>EA Twente</u></a> (uni group)</h2><p>Started November 2022</p><ul><li>Attended Career Fair Twente \u2192 23 people on the interest list.</li><li>Currently 32 people are on our mailing list.<ul><li>22 people in our Whatsapp group.</li></ul></li><li>12 people currently follow the 80,000 book club, which started last week.</li><li>Organized a couple of meetups and an intro talk. Small turnouts, but good discussions.</li><li>2 people attending the Student Summit (SEA).</li></ul><h2><a href=\"https://www.eatilburg.nl/\">EA Tilburg</a> (uni group)</h2><ul><li>Informally started in January 2023, official launch event this June</li><li>3 organizers</li><li>4 EA-aligned professors that want to help us</li><li>35 interested people in Whatsapp group</li><li>Biweekly socials</li><li>Launching a governance-focused <a href=\"https://www.eatilburg.nl/ai-safety-initiative\">AI safety subgroup</a></li><li>First intro fellowship and AGISF cohorts will start over the summer or in September</li></ul><h2><a href=\"https://eautrecht.org/\"><u>EA Utrecht</u></a> (uni group)</h2><ul><li>3 part-time organisers; two participated in UGAP</li><li>Weekly socials</li><li>Weekly coworking sessions</li><li>Ran 10 introduction fellowship cohorts</li><li>Set up AI Safety Group (2 other part-time organisers, one participated in UGAP)<ul><li>Cause area-specific group with many AI Master\u2019s students, 22 participants</li><li>Ran two rounds of AIS Fundamentals programme</li><li>One alumni joined the Berkeley AI alignment workshop and launched&nbsp;<a href=\"https://enais.co/\"><u>ENAIS</u></a></li><li>Starting outreach to professors at Utrecht University to incorporate more AI Safety topics into curricula</li></ul></li><li>117 people in our EA announcement chat</li><li>Running regular book club (including Parfit\u2019s \u2018Reasons and Persons\u2019), read 5 books this academic year, ~5 people participating per iteration&nbsp;</li><li>Launched a journal club</li><li>Two organizers joining the GCP Existential Risk workshop March 2023</li><li>Group of members joining EAGx Berlin, EAGx Rotterdam and SEA 2023</li><li>Started the Alt Protein Group with a grant from Utrecht University (working together with the Alt Protein Project), two part time people working on setting up the group<ul><li>Running fellowships now with 15 participants from related studies (Biology and Molecular Life Sciences)</li></ul></li><li>Established preliminary contact with the Dutch Biosecurity field, working on creating pipelines and opportunities</li><li>Ran a successful retreat for 10 engaged members</li><li>Hosted event for ~500 attendees at the university with best-selling writer Rutger Bregman&nbsp;</li><li>Initial speaker event on Philosophy, Effective Altruism and Meaning with ~25 attendees</li><li>Helped organising SEA&nbsp;</li></ul><h2><a href=\"https://eawageningen.nl\"><u>EA Wageningen</u></a> (uni group)</h2><p>Started in the summer of 2022, became official in March 2023</p><ul><li>12 people finished the intro fellowship or are currently following it</li><li>Organized 2 live events</li><li>22 people in the WhatsApp group</li></ul>", "user": {"username": "James Herbert"}}, {"_id": "guuWy8HjkASQZ9Zzj", "title": "Utility Functions and Their Proxies", "postedAt": "2023-05-30T06:03:22.313Z", "htmlBody": "<h1>TL;DR</h1><ul><li>There are two types of utility functions, <strong>ordinal</strong> and <strong>cardinal</strong>.</li><li><strong>Ordinal utility ranks</strong> options based on preference level, without indicating the extent of that preference. Example: oranges &gt; bananas &gt; cherries.</li><li><strong>Cardinal utility assigns numerical values</strong> to options, indicating preference level and preference intensity. Example: oranges (100) &gt; bananas (50) &gt; cherries (40).</li><li>Utility functions can theoretically be applied to all possible world states, ranking or assigning values to each state based on the satisfaction it would provide to the decision-maker.</li><li>Applying an ordinal utility function to the totality of possible world states would be <strong>practically impossible</strong> due to the vast complexity of factors influencing each state.</li><li>In everyday circumstances, <strong>comparing a small number of world states is usually sufficient</strong>. Ordinal utility provides a relational output, indicating preference of one state over another.</li><li>Cardinal utility allows for a single world state to be input and any number to be output, avoiding the need for direct comparison. However, <strong>translating a world state into a utility number is disputed</strong>.</li><li>The use of <strong>world state representations</strong>, simplified models of the world, helps to apply utility functions in a manageable way. These representations aggregate components of the world state for easier comparison.</li><li><strong>Proxies, or abstracted positive and negative values, simplify decision-making by serving as representative concepts of our utility functions.</strong></li><li>Some people construct cardinal utility functions from proxies, using an aggregation function to derive a utility value. This is called <strong>multi-attribute utility</strong>.</li><li><strong>Aggregation is a lossy process</strong>, and critics argue that it removes important information from the utility calculation. An alternative is multi-objective optimization, which respects value diversity.</li><li><strong>Proxies play a key role in other ethical theories as well.</strong> They act as the foundation for determining moral duties or rules in deontological ethics, and guide decisions towards enhancing moral character in virtue ethics.</li></ul><h1>Why this article?</h1><p>When talking about (human) utility functions, I have encountered a lot of confusion and misunderstandings. There are different forms of utility functions that people use interchangeably. Some assume that everybody has a clear-cut ratio-scale utility function that you just need to figure out (e.g., by <i>simple</i> extrapolation). Some of them claim to know exactly the(ir) utility of world state X, Y, and Z. Some others consider &nbsp;the notion of utility functions to be virtually useless or net-negative in utility (figuratively speaking of course). I'd like to provide some clarifications <i>that helped me</i> wrap my head around the notion of utility functions. I won't use any (fancy) math as I don't think, it's necessary to get the message across.&nbsp;</p><h1>Why Utility Functions?</h1><p>Understanding human decision-making can often feel like trying to solve an enigma. With a multitude of factors influencing every choice we make, deciphering the underlying logic can seem daunting. That's where the concept of utility functions comes in handy. Utility functions essentially provide a theoretical framework to represent and quantify our preferences. They function as a mathematical shorthand, translating our complex, multi-faceted desires and needs into a form that can be easily analyzed and understood. Whether it's choosing between different flavors of ice cream or making major life decisions like choosing a career path, utility functions allow us to model these choices in a systematic way. Utility functions offer a lens to examine the rationale behind our decisions, thereby enhancing our understanding of human behavior. This makes the study of utility functions not just an intellectual exercise, but a powerful tool in deciphering the complexity of human decision-making.&nbsp;</p><p>And of course, EAs and rationalists love dropping the term in every <s>other</s> conversation. Using the term <i>utility function</i> can be immensely helpful when aiming to maximize positive impact or <i>do the most good</i>. The concept of a utility function provides a systematic way to quantify and compare the potential benefits of different actions, thus helping to guide decision-making towards the most effective outcomes. By representing values, goals, or beneficial outcomes numerically, utility functions allow for a structured comparison and prioritization of actions. If, for example, your goal is to alleviate global suffering, you could assign values to different charitable actions based on their estimated impact, thus creating a utility function. This function can then guide you to allocate your resources \u2013 like time or money \u2013 where they will generate the greatest utility or <i>good</i>.</p><h1>Ordinal versus Cardinal</h1><p>As I said, I'm going to skip all the basic mathematical elaborations of the Von Neumann-Morgenstern utility theorem and their axioms and other such mathematical background<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4oiz3eiep1x\"><sup><a href=\"#fn4oiz3eiep1x\">[1]</a></sup></span>. I will mostly use plain natural language and supporting images to make my points. Let us consider two types of utility functions: a <strong>cardinal utility unction</strong> and an <strong>ordinal utility function</strong>. The concepts of cardinal and ordinal utility functions come into play when trying to measure preferences and satisfaction derived from different choices or outcomes. While both aim to provide insights into preference structures, they do so in distinct ways.</p><h2>Toy World</h2><p>The <strong>ordinal utility function</strong> is the more basic of the two concepts. It is a function that ranks options or outcomes according to the level of satisfaction or preference they offer, without giving any precise numerical measurement of the utility derived. Essentially, it tells us the order of preferences. For example, if you prefer oranges to bananas and bananas to cherries, an ordinal utility function might simply rank these as oranges<i> &gt; bananas &gt; cherries</i>. However, this ranking doesn't tell us by how much you prefer oranges to bananas or bananas to cherries.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/kiahcpfdcck4ucu40hb0\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/ildyzqvctbvaqrh5atx1 390w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/idvt44slhbqh2a3foebf 780w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/vw7tzhgk7alaurpjvptn 1170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/cfjkloh4and6tcermbvs 1560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/b2vdzlnp82rfxrzndtxl 1950w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/filbwcwdvklug3szftib 2340w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/gcvnpkqceblszl6svi1k 2730w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/ml9khhy8kfdvtkxx4tpv 3120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/rrwhicyk7py6tsiic3af 3510w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/yeyn1jr1zxmjvjpmcpps 3840w\"><figcaption>Toy Example: Ordinal Utility Function</figcaption></figure><p>The <strong>cardinal utility function</strong> on the other hand is a function that assigns specific numerical values to different options or outcomes. It not only ranks options according to preference, but also gives an indication of the intensity of those preferences. For example, in the fruit preference scenario, a cardinal utility function might assign a value of 100 to oranges, 50 to bananas, and 40 to cherries. These numerical values indicate not only that you prefer oranges to bananas and cherries, but also that you &nbsp;would take an orange over a banana and over cherries together.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/rvox1cm4rrfyjoac0psk\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/ph9f2uteg01iuyslenwd 390w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/xajatau4tmhulh7fdbrw 780w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/exwhjbihbk6ckgjqlbts 1170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/dbair0w3fgpttlsndtdo 1560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/kidjzovwtzbl82yct7za 1950w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/uf71qeodtt7as2skd1mi 2340w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/aee17es5t9opnlzp4uqk 2730w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/ugjjytbxfzcrowgacwqe 3120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/gbcyva2jauvjukwirqow 3510w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/w9xqyi1ky5hascnrjtqr 3840w\"><figcaption>Toy Example: Cardinal Utility Function</figcaption></figure><h2>Extension to the Real World</h2><p>These were very simple examples referring to only one dimension, namely fruits<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgznw0el51d\"><sup><a href=\"#fngznw0el51d\">[2]</a></sup></span>. If we extend the notion of utility functions to the entire world, we would look at something like the following. As input, we take in the set of possible world states and as output, we get the total ordering of all these states. The <i>state of the world</i> &nbsp;here refers to all possible configurations of reality at a given moment, considering every conceivable factor. This includes everything from the macroscopic level \u2013 such as the state of global politics, the economy, the environment, social and cultural factors, technological advancement, and more \u2013 down to the microscopic level, like individual health status, personal relationships, and even the precise arrangement of atoms in the universe.</p><p>To apply an <strong>ordinal utility function</strong> to this immeasurably large set means assigning a preference ranking or a numerical value to each and every one of these countless world states, based on the satisfaction or benefit they would provide to the decision-maker. This task is, of course, impossible to complete in practical terms due to its sheer enormity and complexity. It would require an impossible level of knowledge and computational ability to even approximate. Nevertheless, it serves as a theoretical ideal, highlighting the ultimate goal of utility theory: to understand and quantify preferences over all possible outcomes. So, when we say we want to \"include really anything\", we are attempting to grasp the totality of preference structure, considering any and every factor that could affect the decision-maker's satisfaction. This underscores the ambitious scope of utility theory and its goal of providing a comprehensive model of decision-making.&nbsp;</p><p>Of course, we could simplify matters. For everyday-purposes, one does not have to create a total-preference-ordering over all possible states. Often, it is sufficient to compare a handful or just two world states. Do you prefer state A over B and C? That is what most people do, most of the time. For example, let's say we have three different world states as shown below. We prefer the left world state over the other two, and the middle one over the right one.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/fjod1vsurwbhpcu4k6pd\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/g9yvce9wfs3imcvakncy 380w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/edna16tvmuhddyx7u0ku 760w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/f0unxtenwljxevnxzkob 1140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/zadsgyvnpw2bouvmb2av 1520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/helbtawayerb1v0jyg8q 1900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/tnmtergjuh6w8uomndci 2280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/opo9wcfv04dlky5vfdju 2660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/fwb2xqjged2kdlntnqx5 3040w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/qebr07t4nzp8qcb9e3zf 3420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/leaolgkgthimecx3gyz3 3763w\"><figcaption>Real-World Example: Ordinal Utility Function</figcaption></figure><p>The output of an ordinal utility function is by its nature relational. You can only say that you prefer a state more than another one. But with a <strong>cardinal utility function</strong>, you &nbsp;can use a single world state as input and output any number. There is no need for <i>direct comparison</i>. The question is though, how do you translate a world state into a utility number? That is, what is highly disputed. Some would claim that this is an arbitrary process, not grounded in explicit functions, but simply opaque intuition.&nbsp;</p><p>What is difference here? Aren't both cardinal and ordinal utility functions based on intuition anyway? Well, yes, but ...&nbsp;</p><p>If you compare two world states with each other, it is easier to state what you prefer than to assign particular numbers to them. The <i>resolution</i> of your intuition is quite diverging. Comparing two world states with each other has the lowest resolution. A over B? Assigning (float) numbers (e.g., between 0 and 100) to world states which should be comparable in principle and indicate the strength of your preferences seems to be a much bigger ask, implying a way higher resolution. What I'm trying to say here is that maintaining coherency in all your (intuitive) cardinal evaluations is not easy. Analogous to the fruit example, we get a utility value for each world state.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/kqqlxnl2qyx42qzwolbg\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/dgyqrj8kfj4ydmcddmab 380w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/yn8ykobbndppyqxnuqke 760w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/fzzrkdmhsvgzv6jagr2w 1140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/u1vlvyeodvjjufwjwnmf 1520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/bfxzgw9j8ritj84kjyvw 1900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/qai3azxwc95cgkfjyj3z 2280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/hmfrwr0teikpc7oiuxh3 2660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/zc2lutiniqq7gorjputn 3040w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/q1h2eppjizgelxkwvu9e 3420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/h1atf3ikhyc4gwofnotn 3768w\"></figure><hr><h1>World State Representations</h1><p>Of course, the planet distortion in the previous images tells you which world state is preferable. This representation is an extremely crude simplification. Things become less obvious if you would choose a slightly different <strong>world state representation (WSR)</strong>. In the figure below, we will go briefly through the transformation steps.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/waimjrnx5ppnzvfesttj\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/kisfwfiv2deymnodk6ak 370w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/o5pfjshmnhtvkict0pvd 740w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/j56gdnxu1dfpw56jxu0p 1110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/eufm3wdizq8tal8ogtmc 1480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/h3nfiiiqblaoj2q48wsi 1850w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/vn5fdqw1iki3h0u0h7xs 2220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/pq2md2rwfdetnv4atmmh 2590w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/xtgu6kcmygksreyrsvdp 2960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/psc7eie1m85x0deje7cm 3330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/wlq7yjw6shkvqvqismbb 3613w\"><figcaption>Transformation of World States</figcaption></figure><p>The first WSR is supposed to encompass the entire world with <i>all</i> its details (the impossible kind as discussed above). The second WSR is a pixelated simplification. It shows us that we need to aggregate components of the world state to reduce the dimensions of the first WSR. This aggregation happens on a geographical scale for the pixelated version<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyae9lm23m9s\"><sup><a href=\"#fnyae9lm23m9s\">[3]</a></sup></span>. The third WSR looks already quite different from the others. The 20x20 grid is not supposed to represent the geography of the world. It's not the geography or physicality in a particular location, that we care about. We usually care about higher aggregates. For this purpose, consider each cell to represent a particular feature of the world that seems reasonable to track. E.g., cell (0,0) represents <i>Gross World Product (GWP)</i>. Cell (2,19) represents <i>Total World Population</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefa4nhgaefqlh\"><sup><a href=\"#fna4nhgaefqlh\">[4]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcqgkmrreuni\"><sup><a href=\"#fncqgkmrreuni\">[5]</a></sup></span>. The color represents the degree of a feature's manifestation (which is normalized between 0 and 1). E.g., the higher <i>GWP</i> is, the higher the cell's value and the darker the cell's color. &nbsp;The grid in the third WSR represents a simplified state of the world that is more convenient to work with<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpi0fn4647zi\"><sup><a href=\"#fnpi0fn4647zi\">[6]</a></sup></span>.&nbsp;</p><h1>Proxies</h1><p>Let's start with an <i>ordinal utility function</i> applied to this world state representation. Consider three distinct world states and their ranking as shown in the figure below.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/gavmkjxksbypwpr2bazp\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/soc5wlzjhvcltp0fns5m 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/glqwsi0jn0q8x1r6zfuo 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/x96txncopl81nyrxreaf 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/rfjddjo8ey0ylith99q0 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/maqcoziwntgjkfm9xirp 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/opbnmlu6aqzdmqgylfin 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/wwnden7hufwmxdkteahz 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/yia9haba8ltsel7if7ar 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/xeet7ikm4qbhex4fe0wz 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/mwhl7arecm9pqo2sevb3 1919w\"><figcaption>Example: Grid World State Ranking</figcaption></figure><p>Theoretically, we could rank the set of all possible world states. Given that each cell has a float value between 0 and 1, this would be an impossible task. However, we can easily rank the three given world states. Now, in this example, it is the case, that we are able to hesitantly <strong>infer a pattern</strong>. It could be the case that we prefer a particular pattern of world states. In this simplified case, we could infer that we might prefer lower values in the lower left corner and higher values in the upper right corner. It seems almost superfluous to say, but humans are intuitive pattern recognizing machines. The real world has too many interacting parts to keep track of entirely. Working with simplifications and finding patterns &nbsp;is what we do well.&nbsp;</p><p>Remember that each cell represents a concrete and easily observable feature of reality? What if the pattern is more complex? Maybe it's more than a particular subset of the grid's cells that we care about. Maybe we care about higher abstracted patterns. There might be a pattern within this grid that is not just <i>islands </i>of colors in particular corners. The patterns could be more intricate. You might consider particular valuations of a subset of cells across the grid in very particular combinations to be pleasing. But for simplicity's sake, let us consider the simple <i>island patterns</i>.&nbsp;</p><p>How do recognize the patterns usually? After experiencing many world states (e.g. see figure below), we try to extract a pattern that can tell us what we like (good patterns) and what we don't like (bad patterns).&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/zni2gdygh64okiapjqdr\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/zyr87lmn4wkkcrjjkkq0 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/p8odrt83y4lqelgj7mjm 2400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/q2ecyxixp6s2n80fmbwe 3600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/ciboryod5rsa4xj1omqu 4800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/pekzz7s2cuevrihuxo5k 6000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/bsrbwejdtgdbtp83iias 7200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/hypcetm99tinkfaalgya 8400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/g6onsvna3rec47poumh4 9600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/m9wbkwazsp0wdlx6phor 10800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/aolhoxrp6bonxdawipua 12000w\"></figure><p>Some of these <i>good</i> patterns, we assign names to such as justice, honesty, kindness, respect, courage, freedom, responsibility, equality, peace, humility, loyalty, tolerance, wisdom, and many more. <i>Bad</i> patterns are sometimes labelled as dishonesty, greed, cruelty, arrogance, apathy, jealousy, prejudice, impatience, manipulation, laziness, hostility, selfishness, pride, vengeance, etc.&nbsp;</p><p>Using abstracted (positive and negative) values as <strong>proxies</strong> for our utility functions makes things easier with respect to decision-making. What's the alternative? Imagine having to rank all 50 world states in the figure above. With each world state having 400 dimensions, this becomes quickly infeasible. And these 400 dimensions are already a crude simplification of the world. In reality, it's almost infinitely higher-dimensional. It's much easier to say: \"Hey, I just like <i>bright-lower-left-corner world states</i> and I call them <i>just </i>world states.\" &nbsp;And even though, these <i>bright-lower-left-corner world states</i> maybe do not encompass <i>fully</i> what you care about and there are some other world state configurations that you would also call <i>just</i>, <strong>it's often a good enough approximation to navigate through the world</strong>.&nbsp;</p><h1>From Proxies to Multi-Attribute Utilities</h1><p>My understanding is that some people don't stop here. Some people try to construct their <i>cardinal utility functions</i> from proxies. Some attempt to use a single proxy (i.e. a single value) as the input to their function. E.g., a high degree of <i>equality</i> is directly translated into a high level of <i>utility</i>. That is of course a very simplified way of looking at proxies and utility. Others go a bit more complex by using a set of proxies of a world state as inputs and return the output as a utility value. For example, take <i>equality</i>, <i>GWP</i>, and <i>freedom</i> in a world state as inputs, use some kind of aggregation function (e.g., addition) and voil\u00e0 that's the utility of that particular world state. That's what is also called <i>multi-attribute utility</i>. Some use this way of aggregating proxies into a single utility because it's easier to compare single utility values across world states. Especially, if you want to maximize utility, it's just more convenient to have a single value.&nbsp;</p><p>However, aggregation is not a lossless process. We actually lose a lot of information. And I think, that's again where a lot of critics jump in, taking cardinal utility functions apart. But multi-attribute utilities are not a route, one has necessarily to take. One could also embrace value diversity and go for multi-objective optimization (if you care about eventual optimization). Pareto-optimality is the right key word here and if you are interested in how that would work, have a look at <a href=\"https://forum.effectivealtruism.org/posts/kCBQHWqbk4Nrns8P7/model-based-policy-analysis-under-deep-uncertainty#Multiple_Objectives\">this section of another forum post</a>.</p><p>For now, our journey has followed the depiction of the figure below. We started with a complete description of the world state, transforming the world state into a fewer-dimensional representation (still describing concrete things in the world), inferring proxies (e.g., values), and then using these to potentially arrive at final utility values.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/oyxdgp8lgiuzebiy7zz0\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/bqfik4oocmnmjamii6cs 370w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/xe2vhmkcybkpmz2nrkxl 740w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/enbzff5kofqlflgzflqb 1110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/njkz3mi9xzfyqow1twad 1480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/ylf3czqft2ioyidwjkkb 1850w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/bfmkfsbl2gzmf171dzaq 2220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/aeph7bdzehnd3prbyo8o 2590w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/kk9qkhuxqbj846tzk7ms 2960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/uqedovdrdltsmymjwcei 3330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/guuWy8HjkASQZ9Zzj/sqcb5y08wsegwhzjmido 3606w\"><figcaption>The Journey of Aggregation</figcaption></figure><p>(Multi-attribute) Cardinal utility is the pinnacle of aggregation. We use a combination of ranking and pattern recognition at the grid level representation. We figure out <i>how</i> to crudely describe our internal mechanism for ranking by identifying proxies. And eventually, we can use these proxies to calculate multi-attribute utilities or we stay with the diversity of the proxies (and use Pareto-optimality for optimization).&nbsp;</p><h1>Proxies in other Ethical Theories</h1><p>Using an analogy to deep learning, we could say that our intuitive utility function is a black box, mapping world state input to some kind of preference output. Proxies are used to explain what the intuitive utility function might be roughly doing. It can help creating a narrative, finding some kind of patterns in the hyper-dimensional inputs. &nbsp;</p><p>Proxies are easier to use than (cardinal or ordinal) utility functions because that's what we do instinctively. This is probably a reason why in sociology, values are considered integral components of the social fabric that shape and direct individual and collective behavior. By studying values, sociologists often like to gain insights into why people behave the way they do, as these values provide a motivational basis for actions and decisions, reflecting societal norms, beliefs, and expectations. I assume this is related to how proxies are used in non-utilitarian ethical theories. Generally, I have the impression that most ethical theories are just different representations of the same thing.</p><p>Also in the context of <strong>virtue ethics</strong>, values can be seen as proxies for utility functions. However, the understanding of \"utility\" is slightly different here. Instead of the traditional conception of utility as a measure of personal satisfaction or benefit, the \"utility\" in virtue ethics would align with moral goodness or the development of one's moral character. A decision that aligns with a person's virtues or moral values might not bring immediate personal benefit or satisfaction. Still, it increases their moral goodness, improves their character, or aligns with their understanding of what a morally good life looks like. This enhancement of moral character or alignment with a morally good life could be seen as increasing their \"moral utility\". So, in the context of virtue ethics, values and virtues serve as proxies for this kind of moral utility function, guiding decisions towards what will enhance moral character or align with a morally good life, even when it might not maximize personal benefit in the traditional sense.</p><p>I would argue that proxies also play a crucial role in <strong>deontological ethics</strong>. They help to establish the moral rules or duties that one should follow. For instance, if truthfulness is a deeply held value, a deontological perspective would dictate a duty to always tell the truth, regardless of the consequences. In this sense, values in deontology serve as the foundation for determining the moral duties or rules to be followed. They may not necessarily be seen as proxies for utility functions (as in consequentialist theories), but they are central to defining what constitutes right and wrong behavior according to the rules or duties that are derived from these values. In a way, deontological rules are the result of observed patterns in various world states.&nbsp;</p><h1>Conclusion</h1><p>In conclusion, the concepts of ordinal and cardinal utility functions, while different in their approach, serve as fundamental tools for understanding and quantifying preferences in decision-making. They attempt to map and measure the satisfaction derived from various outcomes.</p><p>The world is infinitely complex, and as such, these utility functions often have to work with reduced, simplified representations of it. Proxies, representing higher aggregates of this world, are utilized to make sense of this complexity, with these values providing intuitive patterns and driving preferences. Whether we choose to use cardinal utilities, aggregating these proxies into one quantifiable measurement, or maintain the diversity of proxies for multi-objective optimization, is a choice that depends on our purpose and preference.</p><p>Moreover, the use of proxies is not confined to the realm of utility theory. In ethical theories such as virtue ethics and deontological ethics, values and virtues could be seen as proxies that provide guidelines for decision-making, dictate duties and actions, and shape individual and collective behavior. They reflect societal norms, beliefs, and expectations, and help create narratives in hyper-dimensional inputs.</p><p>Ultimately, utility functions, ordinal or cardinal, and the use of proxies aim to make the highly complex world navigable, helping us understand preferences and make decisions. However, we must acknowledge the simplification and potential loss of information in this process, and continually refine our models and theories to better represent the intricate, multi-dimensional reality we live in. These kinds of acknowledgements can help us having clearer discussions and a better understanding of human decision-making and behavior.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4oiz3eiep1x\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4oiz3eiep1x\">^</a></strong></sup></span><div class=\"footnote-content\"><p>If you are really interested in these, I recommend reading the corresponding Lesswrong sequences and posts on that.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngznw0el51d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgznw0el51d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>And these fruits are simplified. We could consider other dimensions of them. E.g., brand, ripeness, seasonal conditions, soil quality, transport conditions, etc. But let's keep it simple at this stage.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyae9lm23m9s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyae9lm23m9s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This representation is only used to showcase how simplification or aggregation could further look like. We will not further consider this world state representation. The following one is important though.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fna4nhgaefqlh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefa4nhgaefqlh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>E.g., further features could be:&nbsp;</p><p>- World GDP<br>- Total world population<br>- Average global temperature<br>- Total global annual precipitation<br>- Global literacy rate<br>- Total number of recognized countries<br>- Number of species on the endangered list<br>- Total global forest area<br>- Number of Internet users worldwide<br>- Global average life expectancy<br>- Global gender ratio<br>- Total number of spoken languages<br>- Global oil production rate<br>- Total area of the world covered by deserts<br>- Global CO2 emissions per year<br>- Total world energy consumption<br>- Total number of cities with more than a million inhabitants<br>- Global ocean acidity level<br>- Total number of patents registered worldwide<br>- Number of globally recognized natural world heritage sites</p><p>These are random 20 examples. To fill the grid, we would need to identify 380 more features to fill all 400 cells (20x20).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fncqgkmrreuni\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefcqgkmrreuni\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Keep in mind that most suggested features are comparably concrete and easily observable. We abstained from using extremely abstract features like <i>Freedom</i>, <i>Justice</i>, <i>Love</i>, etc.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpi0fn4647zi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpi0fn4647zi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The 2-dimensionality of this representation is randomly chosen. We could also represent the state as simple column vector with 400 (or any number of) entries.&nbsp;</p></div></li></ol>", "user": {"username": "Max Riddle"}}, {"_id": "5pwofXKyxaQxgLmLK", "title": "Linkpost: the Emergence of Cooperation", "postedAt": "2023-05-29T15:20:10.658Z", "htmlBody": "<p>Communities of experts can, through strategic action, have a large influence on the actions of powerful institutions.</p>\n<p>Alfred Adler\u2019s <em>The emergence of cooperation: national epistemic communities and the international evolution of the idea of arms control</em> talks about how a community of scientists and strategists (in many ways similar to the EA community) played a key role in creating the ideas that led to nuclear arms control agreements.\nThe article is a bit long; I\u2019d recommend reading the first couple of paragraphs, then skipping to the section \u201cIntellectual innovation\u201d (page 111).</p>\n<h2>Other relevant writing on epistemic communities</h2>\n<ul>\n<li><a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/1758-5899.12608\">Robinson on the CERN community</a> and cooperation between countries.</li>\n<li><a href=\"https://www.effectivealtruism.org/articles/ea-neoliberal/\">Kerry Vaughan\u2019s post</a> on how, through the strategic actions of an intellectual community, neoliberalism went from a marginalized movement to the dominant influence in economics.</li>\n<li><a href=\"https://slatestarcodex.com/2018/04/30/book-review-history-of-the-fabian-society/\">Scott Alexanders\u2019 review</a> of Edward Pease\u2019s <em>The History Of The Fabian Society</em>.</li>\n</ul>\n", "user": {"username": "Lauro Langosco"}}]