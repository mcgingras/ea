[{"_id": "DQ538KChqeQd3shAP", "postedAt": "2022-11-18T09:01:24.046Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>(on phone so quick thoughts) Thank you for writing this.  It  is very brave! I am actually quite sympathetic\nto your arguments. I would like EA to evolve over time from being a movement into being a boring norm of  behaviour and  reasoning etc. Just like how being a suffragette gradually failed to have any purpose. However,  I think that doing so right now is a bit premature. I'd welcome some more debate though.</p>\n", "parentCommentId": null, "user": {"username": "Peterslattery"}}, {"_id": "XZjZS4gtCZJ3g7iDK", "postedAt": "2022-11-18T09:19:02.156Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I agree that it's premature - I don't think we should cancel EAG in 2023, but I do think that we're likely to make minor changes and keep doing poorly in various ways if we aren't explicitly thinking about the question of what the community should be.&nbsp;<br><br>And I'm certainly open to hearing if and why I'm wrong. But I worry that if we aren't thinking about what the community looks like in a decade, we'll keep stumbling forward ineffectually, with unnecessarily and unexpected problems and failures from unplanned growth.</p>", "parentCommentId": "DQ538KChqeQd3shAP", "user": {"username": "Davidmanheim"}}, {"_id": "cdGzk6yCtDfezuQDv", "postedAt": "2022-11-18T09:40:18.121Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<blockquote><p>I don\u2019t think that&nbsp;<a href=\"https://arxiv.org/abs/2201.11214\"><u>what Cremer and Kemp suggest</u></a> is the right approach, nor are&nbsp;<a href=\"https://docs.google.com/document/d/1Y9opezUk9_JNQBYCAmHoERiqlaf2HmashQQ_ydGwyRY/edit?usp=sharing\"><u>Cremer\u2019s suggestions to MacAskill</u></a> sufficient for a large and growing movement, but some are necessary, and if those measures are not taken, I think that the community should be announcing alternative structures sooner rather than later.</p></blockquote><p>Which ones do you think are necessary?&nbsp;</p>", "parentCommentId": null, "user": {"username": "pseudonym"}}, {"_id": "uX7extKnSndy2kwgr", "postedAt": "2022-11-18T10:02:57.431Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I think it\u2019s interesting to explore far out ideas and I suppose it might makes sense from the perspective of someone focused on near-termism.</p>\n<p>However, as someone more focused on AI safety, one of the cause areas that is more talent dependent and less immediately legible, this seems like this would be a mistake.</p>\n<p>If the community is uncertain between the causes, I suggest that it probably wouldn\u2019t be a good idea to dismantle the community now, at least if we think we might obtain more clarity over the next few years.</p>\n", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "u6MB2pHz44k8xrKus", "postedAt": "2022-11-18T10:06:56.666Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I think that AI safety needs to be promoted as a cause, not as a community. If you have personal moral uncertainty about whether to focus on animal suffering or AI risk, it might make sense to be a vegan AI researcher. But if you have moral uncertainty about what the priority is overall, you shouldn't try to mix the two.<br><br>People in Machine learning are increasingly of the opinion that there is a risk, and it would be much better to educate them than to try to bring them in to a community which has goals they don't, and don't need to, care about.</p>", "parentCommentId": "uX7extKnSndy2kwgr", "user": {"username": "Davidmanheim"}}, {"_id": "8DqREBtuBuGmEghSA", "postedAt": "2022-11-18T10:08:43.284Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Ombudsmen and clear rules about and norms of protection for whistleblowing, more funding transparency, and better disclosure about conflicts of interest.<br><br>(None of these relate to having a community, by the way - they are just important things to have if you care about having well run organizations.)</p>", "parentCommentId": "cdGzk6yCtDfezuQDv", "user": {"username": "Davidmanheim"}}, {"_id": "FErGvKyj6tDSBsQNx", "postedAt": "2022-11-18T10:28:04.804Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Thanks. A few more quick thoughts! I actually don't think that the community failed here. I am pretty sure that all communities have bad actors, and EA probably has fewer than many communities that are doing quite well. We hold ourselves to a very high standard.<br><br>With that said, I agree that we should think more about what we want the community to become and how to get it there, and would love to hear some visions. Here is a quick attempt at making some predictions for how things go in the next 10-20 years (if we are still around):</p><ul><li>EA becomes an established academic discipline and more mainstream. Something like what I think happened with psychology, economics and various other new disciplines/intellectual innovations.</li><li>Major conceptual innovations, like the using the ITN framework and expected value in philanthropic settings etc go mainstream and are now widely used in relevant settings.</li><li>The overall EA identity weakens and EA starts to become one of many relatively uninteresting topics that people are interested in rather than a cool new identity to have.&nbsp;</li><li>People start forming identities and groups focused on current and new causes areas rather than EA&nbsp;</li><li>Cause areas/EA related actions (e.g., AI safety or earning to give) become like academic/professional communities with their own conferences and subcultures. We start seeing some quite well managed and formalised community building by specific organisations as they scale up.</li><li>We never properly manage things at the EA community level due to the impossible to match increase in the scale and complexity of all the causes and their sub-communities and our decentralised nature. We continue to have informal groups etc but it never really gets more formalised or much larger scale than now.</li><li>EAGs continue. These function like academic conferences and start being funded by large cause area orgs to find hires and allow people in their areas to meet and collaborate etc.&nbsp;</li><li>The EA forum becomes much less used, having achieved its goal of facilitating and incubating many new ideas, concepts and connection that have now spun off into their own more specific areas of focus or gone mainstream. &nbsp;Again, it's like an psychology forum or similar at this point, and most people are more interested in something more specific so use more specific forums.<br><br>That's all pretty rushed because I need to go to sleep now (actually 30 minute ago), but it was interesting to attempt, and I would like to hear thoughts or other perspectives.</li></ul>", "parentCommentId": "XZjZS4gtCZJ3g7iDK", "user": {"username": "Peterslattery"}}, {"_id": "9h8vnPciRnhRgyH8D", "postedAt": "2022-11-18T10:36:53.880Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>As someone who has been deeply into community building for years (most of it outside EA), I am biting my lip yet upvoting this. &nbsp;I deeply agree that \"Being an EA\" as an identity has problematic implications, to say the least. &nbsp;While I have many thoughts, for now I'll just highlight what you wrote which for me is the most important: \"[we should be] convincing individuals to consider the ideas, not to \u201cjoin EA.\u201d\".&nbsp;</p>", "parentCommentId": null, "user": {"username": "Dvir Caspi"}}, {"_id": "CRH6JeoWJMuhwAoxG", "postedAt": "2022-11-18T11:07:19.262Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Can you expand a bit on what you mean by why these ideas applying better to near \u2013 termism?</p>\n<p>E.g. Out of 'hey it seems like machine learning systems are getting scarily powerful, maybe we should do something to make sure they're aligned with humans' vs 'you might think it's most cost effective to help extremely poor people or animals but actually if you account for the far future it looks like existential risks are more important, and AI is one of the most credible existential risks so maybe you should work on that', the first one seems like a more scalable/legible message or something. Obviously I've strawmaned the second one a bit to make a point but I'm curious what your perspective is!</p>\n", "parentCommentId": "uX7extKnSndy2kwgr", "user": {"username": "Asa Cooper Stickland"}}, {"_id": "KcnYquwjTCcrMGB39", "postedAt": "2022-11-18T11:14:51.136Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Upvoted despite disagreeing, since I think this is an important question to explore. &nbsp;But I'm puzzled by the following claim:</p><blockquote><p>from where I stand, someone who is giving half their salary to the \u201caltruistic cause\u201d of having community events and recruiting more people isn\u2019t effective altruism.</p></blockquote><p>Obviously the motivation for community-building is not that the community is an end in itself, but instrumental: more people \"joining EA\", taking the GWWC pledge and/or going into directly high-impact work, means indirectly causing more good for all the other EA causes that we ultimately care about. &nbsp;Without addressing this head-on, I'm not sure which of the following you mean:</p><p>(1) An empirical disagreement: You deny that EA community-building is instrumentally <i>effective</i> for (indirectly) helping other, first-order EA causes.</p><p>(2) A moral/conceptual disagreement: You deny that indirectly causing good counts as <i>altruism</i>.</p><p>Can you clarify which of these you have in mind?</p>", "parentCommentId": null, "user": {"username": "RYC"}}, {"_id": "CPEjnB3fWKYeHuXgC", "postedAt": "2022-11-18T11:40:32.839Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I think that <a href=\"https://forum.effectivealtruism.org/posts/cvzEpGgmQ8BefiwGt/making-effective-altruism-enormous\">my earlier attempt to discuss this</a> mostly matches what you're saying.&nbsp;<br><br>A key thing that changed is that I no longer think we should <i>try</i> to \"manage things at the EA community level\" - and if we're not attempting that, we should reconceptualize what it means to be good community members and leaders, and what failure modes we should anticipate and address.<br><br>The other thing I want is more ambitious - ideally, in 20+ years I want the idea of prioritizing giving part of your income, viewing the future as at least some level of moral priority, and cause neutrality to all look like women's suffrage does; so obviously correct and uncontroversial that it's not a topic of discussion.</p>", "parentCommentId": "FErGvKyj6tDSBsQNx", "user": {"username": "Davidmanheim"}}, {"_id": "5FBLwDmHqLRW3hHyS", "postedAt": "2022-11-18T11:50:46.442Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I don't mean either (1) or (2), but I'm not sure it's a single argument.&nbsp;<br><br>First, I think it's epistemically and socially healthy for people to separate giving to their community from altruism. To explain a bit more, it's good to view your community as a valid place to invest effort independent of eventual value. Without that, I think people often end up being exploitative, pushing people to do things instead of treating them respectfully, or being dismissive of others, for example, telling people they shouldn't be in EA because they aren't making the right choices. If your community isn't just about the eventual altruistic value they will create, those failure modes are less likely.</p><p>Second, it's easy to lose sight of eventual goals when focused on instrumental ones, and get stuck in a mode where you are goodharting community size, or dollars being donated - both community size and total dollars seem like an &nbsp;unfortunately easy attractor for this failure.<br><br>Third, relatedly, I think that people should be careful not to build models of impact that are too indirect, because they often fail at unexpected places. The simpler your path to impact is, the fewer failure points exist. Community building in many steps removed from the objective, and we should certainly be cautious about doing na\u00efve EV calculations about increasing community size!</p>", "parentCommentId": "KcnYquwjTCcrMGB39", "user": {"username": "Davidmanheim"}}, {"_id": "MLuLatvpj8AiMGzaP", "postedAt": "2022-11-18T12:43:04.249Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Separate but related to community, I think your point about identity, and whether fostering EA as an identity is epistemically healthy, is also relevant to (1).&nbsp;<br><br>Your analogy to church spoke very powerfully to me and to something I have always been a bit uncomfortable with. To me, EA is a philosophy/school of thought, and I struggle to understand how a person can \"be\" a philosophy, or how a philosophy can \"recruit members\".&nbsp;</p><p>I also suspect that a strong self-perception that one is a \"good person\" can just as often provide (internal and external) cover for wrong-doing as it can be a motivator to actually do good, as any number of high-profile non-profit scandals (and anecdotal experience from I'm guessing most young women who have ever been involved in a movement for change) can tell you.&nbsp;</p><p>I have nothing at all against organic communities, or professional conferences etc, but I also wonder whether there is evidence that building EA as an <i>identity</i> &nbsp;(\"join us!\") as opposed to something that people can <i>do</i> is instrumentally effective for first-order causes. Maybe it does, but I think it warrants some interrogation.&nbsp;</p>", "parentCommentId": "5FBLwDmHqLRW3hHyS", "user": {"username": "Chantal"}}, {"_id": "3EmSZZKvHLFZbYNPD", "postedAt": "2022-11-18T12:59:00.545Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Maybe I should have said global health and development, rather than near-termism.</p>\n", "parentCommentId": "CRH6JeoWJMuhwAoxG", "user": {"username": "casebash"}}, {"_id": "JBTynDqtDEXDcAqf9", "postedAt": "2022-11-18T13:00:31.917Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I\u2019d suggest that we need multiple paths for drawing talent and general EA community building has been surprisingly successful so far.</p>\n", "parentCommentId": "u6MB2pHz44k8xrKus", "user": {"username": "casebash"}}, {"_id": "n8iZw87WB6xdZHGCj", "postedAt": "2022-11-18T13:22:38.024Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Honest question: isn't an option for the AI Safety community being just the AI Safety community, independent of there being an EA community?</p>\n<p>I understand the idea of the philosophy of effective altruism and longtermism being a motivation to work in AI Safety, but that could as well be a worry about modern ML systems, or just sheer intellectual interest. I don't know if the current entanglement between both communities is that healthy.</p>\n<p>EDIT: Corrected stupid wording mistakes. I wrote in a hurry.</p>\n", "parentCommentId": "uX7extKnSndy2kwgr", "user": {"username": "HausdorffSpace"}}, {"_id": "97iA8F5QEzHLhL3FD", "postedAt": "2022-11-18T13:36:32.022Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I certainly think that having an academic discipline devoted to AI safety is an option, but I think it's a bad idea for other reasons; if safety is viewed as separate from ML in general, you end up in a situation similar to cybersecurity, where everyone builds dangerous shit, and then the cyber people recoil in horror, and hopefully barely patch the most obvious problems.<br><br>That said, yes, I'm completely fine with having informal networks of people working on a goal - it exists regardless of efforts. But a centralized effort at EA community building in general is a different thing, and as I argued here, I tentatively think this are bad, at least at the margin.</p>", "parentCommentId": "n8iZw87WB6xdZHGCj", "user": {"username": "Davidmanheim"}}, {"_id": "fBSrwBMybhCgmAbug", "postedAt": "2022-11-18T13:51:28.544Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I\u2019m in favour of direct AI safety movement building too, but the point still remains that the EA community is a vital talent pipeline for cause areas that are more talent dependent. And given the increasing prominence of these cause areas, it seems like it would be a mistake to optimise for the other cause, at least when it\u2019s looking highly plausible that the community may shift even more in the longtermist/x-risk over the next few years.</p>\n", "parentCommentId": "n8iZw87WB6xdZHGCj", "user": {"username": "casebash"}}, {"_id": "9bD3WTp5bnTsiKpXD", "postedAt": "2022-11-18T14:24:52.225Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>But if we were to eliminate the EA community, an AI safety community would quickly replace it, as people are often attached to what they do. And this is even more likely  if you add any moral connotation. People working at a charity, for example, are drawn to build an identity around it.</p>\n", "parentCommentId": "u6MB2pHz44k8xrKus", "user": {"username": "PabloAMC"}}, {"_id": "aDnTxfMcC6F6MHJED", "postedAt": "2022-11-18T15:49:05.646Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Haven't finished reading yet, but I feel obliged to flag* (like anywhere else where they come up) that this paragraph:</p>\n<blockquote>\n<p>We were all too trusting of someone who, according to several reports, had a history of breaking rules and cheating others, including an acrimonious split that happened early on at Alameda, and evidently more recently <a href=\"https://twitter.com/geoffanders/status/1590932949461266434\">frontrunning</a>.</p>\n</blockquote>\n<p>is linking to a <a href=\"https://medium.com/@zoecurzi/my-experience-with-leverage-research-17e96a8e540b\">known cult leader</a>. This is deeply ironic.</p>\n<p>*The reason I think this should be stated every time is that there are many new people coming in all the time, and it's important that none of them encounter these people without the corresponding warning.</p>\n", "parentCommentId": null, "user": {"username": "Guy Raveh"}}, {"_id": "YRDEqyFeLA2MHFdyu", "postedAt": "2022-11-18T16:09:37.470Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>The first time I heard about effective altruism was when someone told me \"you should check out your local EA community--it's where the cool people hang out\". Indeed this turned out to be the case; I had met thoughtful, curious people, and gotten quite interested in the ideas of effective altruism.</p><p>When I moved a year later to a different city in a different country, I spent a few weeks lamenting the difficulty of meeting people, until I realized I could go have thoughtful and interesting conversations with wonderful people at the local EA community. In a way, it feels not unlike to churches in towns in the United States--you can move from one state to another, and yet the next Sunday you'll find a tight-knit community of people who approximately share a large subset of your values. And while in some denominations of Christianity, there's a large global hierarchical structure, as far as I am aware, there's no global fund for starting new churches (I may be totally wrong, I don't actually know much about this. Though I have heard of <a href=\"https://en.wikipedia.org/wiki/Church_planting\">church planting</a>, but even there, the new church \"must eventually have a separate life of its own and be able to function without its parent body\").</p><p>One of the major reasons people I've met valued the global EA community was due to the coordination problems of otherwise getting a lot of people to start some new initiative. That is, say you're trying to start an organization centred around giving RL agents numinous experiences. The median number of people who think this is <i>the</i> most worthwhile thing to work on, in each local EA community, is exactly zero, but through the connections formed by the global community (through EAG and forums) it's a lot easier to find people interested in starting rLANE, which would never have been possible on the local scale.</p><p>Fundamentally, at my level of involvement with the EA community, I'm not sure I've witnessed the failure modes that have been taken as the antecedent for this whole post. Small, local communities haven't felt elitist in the same way that nobody calls the local swing dance club elitist, despite it being very obvious whether you're an experienced swing dancer or not. The global community has been very good at solving coordination problems to tackle specific challenges, and I haven't witnessed any downsides personally (but once again, I've never been accepted to an EAG or anything, and I've never lived in any of the \"big\" EA cities). If decentralization is the correct direction to go down, I think the \"who's interested in helping me work on &lt;x&gt;?\" is one of the most valuable things to make sure has a strong foundation of interconnectedness between different communities in different places.</p>", "parentCommentId": null, "user": {"username": "dyusha"}}, {"_id": "tb65hf4LqejnCwTEv", "postedAt": "2022-11-18T16:33:07.850Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I agree with you insofar as separating AI safety from ML is terrible, since the objective of AI safety, in the end, is not to only study safety but to actually implement it in ML systems, and that can only be done in close communication with the general ML community (and I really enjoyed your analogy with cybersecurity).<br><br>I don't know what is the actual current state of this communication, nor who is working on improving it (although I know people are discussing it), but a thing I want to see at least are alignment papers published in NeurIPS, ICML, JMLR, and so on. My two-cent guess is that this would be easier if AI safety would be more dissociated with EA or even longtermism, although I could easily envision myself being wrong.</p><p>EDIT: One point important to clarify is that \"more dissociated\" does not mean \"fully dissociated\" here. It may be as well that EA donors support AI safety research, effective altruism as an idea makes people look into AI safety, and so on. My worry is AI safety being seen by a lot of people as \"that weird idea coming from EA/rationalist folks\". No matter how fair this view actually is, the point is that AI safety should be popular, non-controversial, if safety techniques are to be adopted en masse (which is the end goal).</p>", "parentCommentId": "97iA8F5QEzHLhL3FD", "user": {"username": "HausdorffSpace"}}, {"_id": "Lm4pvNvCsFyh4iC6Z", "postedAt": "2022-11-18T17:01:21.807Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>What is missing to me is an explanation of exactly how your suggestions would prevent a future SBF situation. It's not really clear to me that this is true. The crux of your argument seems to come from this paragraph:</p><blockquote><p>The community was trusting - in this case, much too trusting. And people have said that they trusted the apparent (but illusory) consensus of EAs about FTX. I am one of them. We were all too trusting of someone who, according to several reports, had a history of breaking rules and cheating others, including an acrimonious split that happened early on at Alameda, and evidently more recently&nbsp;<a href=\"https://twitter.com/geoffanders/status/1590932949461266434\"><u>frontrunning</u></a>. But the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId=NbevNWixq3bJMEW7b\"><u>people who raised flags</u></a> were evidently ignored, or in other cases feared being pariahs for speaking out more publicly.</p></blockquote><p>Would this have been any different if EA consisted of an archipelago of affiliated groups? If anything, Whistleblowing is <i>easier</i> in a large group since you have a network of folks you can contain to raise the alarm. Without a global EA group, who exactly do the ex-Alameda folks complain to? I guess they could talk to a journalist or something, but \"trading firm CEO is kind of an amoral dick\" isn't really newsworthy (I'd say that's probably the default assumption).</p><p>I also generally disagree that making EA more low trust is a good idea. It's pretty well established that low trust societies have more crime and corruption than high trust societies. In that sense, making EA more low trust seems counterproductive to prevent SBF v2.0. In a low trust society, trust is typically reserved for your immediate community. This has obvious problems though! Making trust community-based (i.e. only trusting people in my immediate EA community) seems worse than making trust idea-based (i.e. trusting anyone that espouses shared EA values). People are more likely to defend bad actors if they consider them to be part of their in-group.</p><p>To be honest, I'd recommend the exact opposite course of action: make EA even more high trust. High trust societies succeed by binding members to a common consensus on ethics and morality. EAs need to be clearer about our expectations are with regard to ethics. It was apparently not clear to SBF that being a part of the EA community means adherence to a set of norms outside of naive utilitarian calculus. The EA community should emphatically state our norms and expectations. The corollary to that is that members that break the rules must be called-out and potentially even banished from the group.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Matthew Stork"}}, {"_id": "BmmFWjE6c92izqBWe", "postedAt": "2022-11-18T17:20:51.199Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Communities give hugely leveraged power to those who inform and represent them - you can get lots of people to change their minds or say \"the community believes X\"<br><br>While I think these powers are open to abuse, it's not to say that they aren't valuable.&nbsp;</p><p>A cost-benefit analysis seems appropriate here (and there have been a lot of costs recently) rather than suggesting that there are no benefits.</p>", "parentCommentId": null, "user": {"username": "nathan"}}, {"_id": "rDrzygieFXeLWbn22", "postedAt": "2022-11-18T17:34:08.898Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>One small personal experience: I worked a non-EA job for three years. None of my close friends were interested in EA, and my job wasn\u2019t in a highly impactful cause area. I developed some other interests during those years, reading a lot about startups and VC and finance. Despite my enthusiasm when I first read Peter Singer and Doing Good Better, I think my interest in working on EA topics could have slowly faded and been replaced with other interesting ideas.</p>\n<p>The EA community was a big part of what kept me engaged with EA. This forum was a steady stream of information about how to do good in the world, and one that allowed me to voice my own opinions and have lots of interesting conversations. I attended two online EA Globals which mostly made me identify more as an EA. Later I went back to school, where the university EA group leader reached out and encouraged me to join a reading group. We had weekly dinners and great conversations, and only a few months later, I quit my part-time job at a for-profit startup and began working on AI Safety.</p>\n<p>It\u2019s hard to say what the counterfactual is, but I think the odds I\u2019d be working on AI Safety right now would have been much lower without the identity, personal connections, and intellectual engagement from the EA community.  Part of it is nerdsniping \u2014 it\u2019s not always easy to find smart, sensible conversations about the world, but I\u2019ve always found EA to provide plenty of them. There are real downsides \u2014 I used to think that I deferred way too much on my opinions about cause prioritization (I think I\u2019ve improved, but maybe I\u2019ve just lost my independent thinking). Your post is a great analysis of those dynamics and I\u2019m not trying to argue for a bottom line, but just wanted to share one personal benefit of the community.</p>\n", "parentCommentId": null, "user": {"username": "Aidan O'Gara"}}, {"_id": "CH77smyBjfQE7qFJP", "postedAt": "2022-11-18T18:11:36.985Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I appreciate this post, but pretty strongly disagree. The EA I've experienced seems to be at most a loose but mutually supportive coalition motivated by trying to most effectively do good in the world. It seems pretty far from being a monolith or from having unaccountable leaders setting some agenda.&nbsp;</p><p>While there certainly things I don't love such as treating EAGs as mostly opportunities to hang out and some things like MacAskill's seemingly very expensive and opaque book press tour, your recommendations seem like they would mostly hinder efforts to address the causes the community has identified as particularly important to work on.</p><p>For instance, they'd dramatically increase the transaction costs for advocacy efforts (i.e. most college groups) aimed at introducing people to these issues and giving them an opportunity to consider working on solving them. One of the benefits of EA groups is that it allows for a critical mass of people to become involved where there might not be enough interest to sustain clubs for individual causes (and again the costs of people needing to organize multiple groups). In effect, this would mostly just cede ground and attention to things like consulting, finance, and tech firms.&nbsp;</p><p>Similarly, &nbsp;we shouldn't discount the (imo enormous) value of having people (often very senior people) willing to offer substantial help/advice on projects they aren't involved with simply because the other person/group is part of the same community and legibly motivated for similar reasons. I can also see ways in which a loss of community would lead to reduced cooperation between orgs and competition over resources. It seems important to note too that being part of a cause-neutral community makes people more able to change priorities when new evidence/arguments emerge (as the EA community has done several times since I've been involved).&nbsp;</p><p>I think proposals of this kind really ought to be grounded in saying how the arguments the community has endorsed for some particular strategy are flawed, e.g. showing how community building is not in fact impactful. &nbsp;We generally seem to be over updating on a single failure (even allowing that the failure was particularly harmful).&nbsp;</p><p>Note: wrote this fairly quickly, so it's probably not the most organized collection of thoughts.</p>", "parentCommentId": null, "user": {"username": "ZacharyRudolph"}}, {"_id": "6jDe9tPnEp5ZCBNsN", "postedAt": "2022-11-18T18:17:44.712Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>In the most respectful way possible, I strongly disagree with the overarching direction put forth here. A very strong predictor of engaged participation and retention in advocacy, work, education and many other things in life is the establishment of strong, close social ties within that community.<br><br>I think this direction will greatly reduce participation and engagement with EA, and I'm not even sure it will address the valid concerns you mentioned.<br><br>I say this despite the fact that I didn't have super close EA friends in the first 3-4 years, and still managed to motivate myself to work on EA stuff as well as policy successful advocacy in other areas. When it comes to getting new people to partake in self-motivated, voluntary social causes/projects, one of the first things I do is to make sure they find a friend to keep them engaged, and this likelihood is greatly increased if they simply meet more people.<br><br>I am also of the opinion that long-term engagement relying on unpaid, ad-hoc community organising is much more unreliable than paid work. I think other organisers will agree when I say: organising a community around EA for the purpose of deeply engaging EAs is time-consuming, and greatly benefits from external guidance and financial support. If you want to get people engaging deeply with EA ideas and actually taking EA roles, unpaid volunteer organisers are a significant bottleneck. You're expecting one organiser to regularly host events, perform tasks and engage multiple people at a deep level without central support, and that's a very difficult ask.<br><br>I will add also that I am from a non-EA hub, and the only people I know who work full-time with EA orgs directly cite EAGs as a catalyst for their long-term involvement.<br><br>I'm just ... skeptical of the theory of change put forth here.</p>", "parentCommentId": null, "user": {"username": "Minh Nguyen"}}, {"_id": "DXEHmquwuXwJrvTsg", "postedAt": "2022-11-18T19:07:25.009Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>In agreement with the first part of this comment at least. If there were EA causes but not an EA community, it seems like much the same thing would have happened. A bunch of causes SBF thought were good would have gotten offered money, probably would have accepted the money, and then wound up accidentally laundering his reputation for being charitable while facing the prospect that some of the money they got was ill-gotten, and some of the money they had planned on getting wasn't going to come. Maybe SBF wouldn't have made his money to begin with? I find it unlikely, ideas like earning to give and ends-justifies-means naive consequentialism and high-risk strategies for making more money are all ideas that people associate with EA, but which don't appeal to anything like a \"community\". This isn't to say none of these points are important aside from SBF, but well, it's just odd to see them get so much attention because of him. Similar points have been made in Democratizing Risk, and in a somewhat different way in the recent pre-collapse Clearer Thinking interview with Michael Nielson and Ajeya Cotra. Maybe it's still worth framing this in terms of SBF if now is an unusually good chance to make major movement changes, but at the same time I find it a little iffy. It seems misleading to frame this in terms of SBF if SBF didn't actually provide us with good reasons to update in this direction, and it feels a bit perverse to use such a difficult time to promote an unrelated hobbyhorse, as a more <a href=\"https://forum.effectivealtruism.org/posts/7ZjJ9w2xf7Mkdofk8/does-sam-make-me-want-to-renounce-the-actions-of-the-ea\">recent post</a> harped on (I think a bit too much, but I have some sympathy for it).</p>\n", "parentCommentId": "Lm4pvNvCsFyh4iC6Z", "user": {"username": "Devin Kalish"}}, {"_id": "paovdMQghZKcpHvRf", "postedAt": "2022-11-18T19:28:01.837Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I think this comment would be much more helpful if it linked to the relevant posts about Leverage rather than just called Geoff a \"known cult leader\".</p>\n<p>(On phone right now but may come back and add said links later unless Guy / others do)</p>\n", "parentCommentId": "aDnTxfMcC6F6MHJED", "user": {"username": "cilliancrosson@gmail.com"}}, {"_id": "fSp3MW8qvtNf8x3WA", "postedAt": "2022-11-18T19:54:14.560Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Agree with your post and want to add one thing. Ultimately this was a failure of the EA <i>ideas</i> more so than the EA <i>community</i>. SBF used EA ideas as a justification for his actions. Very few EAs would condone his amoral stance w.r.t. business ethics, but business ethics isn't really a central part of EA ideas. Ultimately, I think the main failure was EAs failing to adequately condemn naive utilitarianism.&nbsp;</p><p>I think back to the old Scott Alexander post about the rationalist community: <a href=\"https://slatestarcodex.com/2017/04/07/yes-we-have-noticed-the-skulls/\">Yes, We Have Noticed The Skulls | Slate Star Codex</a>. I think he makes a valid point, that the rationalist community has tried to address the obvious failure modes of rationalism. This is also true of the EA community, in that there has absolutely been some criticism of galaxy brained naive utilitarianism. However, there is a certain defensiveness in Scott's post, an annoyance that people keep bringing up past failure modes even though rationalists try really hard to not fail that way again. I suspect this same defensiveness may have played a role in EA culture. Utilitarianism has always been criticized for the potential that it could be used to justify...well, SBF-style behavior. EAs can argue that we have newer and better formulations of utilitarianism / moral theory that don't run into that problem, and this is true (in theory). However, I do suspect that this topic was undervalued in the EA community, simply because we were super annoyed at critics that keep harping on the risks of naive utilitarianism even though clearly no <i>real</i> EA actually endorses naive utilitarianism.&nbsp;</p>", "parentCommentId": "DXEHmquwuXwJrvTsg", "user": {"username": "Matthew Stork"}}, {"_id": "BfsDAFbErBHJSETgd", "postedAt": "2022-11-18T20:11:13.313Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Edited</p>\n", "parentCommentId": "paovdMQghZKcpHvRf", "user": {"username": "Guy Raveh"}}, {"_id": "KvJ6tEbSCHdpAq7AJ", "postedAt": "2022-11-18T21:45:01.600Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>On the one hand, I have a strong urge to say something like: &nbsp;But David, community building is not only useful for \"trust\" and \"vetting people\"!</p><p>On the other hand - In the last 2.5 years as a community builder, I was fighting desperately trying to make EA groups more practical and educational, instead of social and network-based.</p><p>I'm not the only one. I know many other community builders who tried to argue that our resources should focus on \"tools\", and less on anecdotes about why the maximization mindset is important and anecdotes about the most pressing cause areas.&nbsp;<br>Instead, I think that the value that we provide should be something in the lines of <i>providing them with actual tools</i> for applying the maximization mindset, and for prioritizing their career/donation/research/etc opportunities by social impact.<br><br>Almost everyone I spoke with agreed with this notion, including multiple representatives from CEA - but nothing changed so far regarding groups' resources or incentives.&nbsp;<br>So, if the main value of community building was meant to be for vetting, then I'd say that the community failed. I don't strongly believe this is the case right now, but I think that many perceive this to be the main value we provide. Anyhow - SBF is a great example, but I think that trust failures are not the only reason why this shift is needed.<br><br>My view is that instead of being social-focused, we need a more practical form of community building. &nbsp;EA distinct itself from the traditional NGO world or from other communities using \u201cimpact\u201d as a buzzword, because we care about maximizing impact. But then, groups don't really have the resources to actually help people maximize their impact. After 2.5 years, I still don\u2019t know where to find a good, simple article or video that describes how to create a theory of change (which is needed when submitting grants to EA funders!), or a clear article describing the practical aspects of \u201cThinking at the margin\u201d, if I want to send those to community members. It takes an absurdly long time to find a good article about the basics of cost-effectiveness estimates, compared to how rooted this idea is in the movement. How long does it take to find an <i>advanced </i>handbook on conducting cost-effectiveness research in the context of social impact? I don\u2019t know, we found none and had to write such a guide in EA Israel.</p><p>I strongly agree with the notion that \"community\" isn't binary:</p><blockquote><p>And I have a few ideas what a less community-centered EA might look like. To preface the ideas, however, \u201ccommunity\u201d isn\u2019t binary.&nbsp;</p></blockquote><p>But I think that the suggestion in this post goes too far on the non-community side of the spectrum. I think that communities provide significant value (such as motivation or connections), and thatthis is the bottleneck for impact for many people. And I also wouldn't denounce the concept of \"being an EA\" too quickly, as it still means something like \"I think carefully about helping others, compared to the default scope-insensitive notion of doing good\".&nbsp;<br>But convincing people of this basic concept is really an easy win in my opinion. The more important challenge in my view is getting people to develop their own, independent outlooks on maximizing impact.&nbsp;</p>", "parentCommentId": null, "user": {"username": "GidonKadosh"}}, {"_id": "SAbTCPQQbaGSffimw", "postedAt": "2022-11-18T21:58:50.011Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Interesting to see that people disagree with me.  I am interested to hear why if anyone wants to share.</p>\n", "parentCommentId": "FErGvKyj6tDSBsQNx", "user": {"username": "Peterslattery"}}, {"_id": "dfMcv7KKKXmhfJmBC", "postedAt": "2022-11-18T21:59:57.786Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I think there\u2019s a kernel of truth to this suggestion! I would put it this way: EA should be global, and it should continue to be powered by communities, but those communities should be local and small.</p>\n<p>First, work in specific cause areas should continue to happen globally, but should not operate with an automatic assumption of trust.</p>\n<p>\u201cLow trust\u201d wouldn\u2019t mean we stop doing a lot of good; it would just mean that we need to be more transparent and rigorous, rather than just having major EA figures texting with billionaires and the rest of us just hoping they do it right.</p>\n<p>GiveWell is a great example of an EA institution built in a \u201clow trust\u201d framework. And it\u2019s great! It would be very hard for me to imagine anything nearly this bad happening in the global health and well-being side of EA precisely because of places like GiveWell.</p>\n<p>But small, high-trust community is also great! And we should encourage that \u2014 more local meetup groups, more university groups. I agree that funding for such things, after the \u201cstartup phase,\u201d should also be a bit more locally-sourced, with alumni funding university groups since students usually lack sources of income.</p>\n<p>When I first hosted some EAs for a dinner in my home, someone in the group asked if I wanted funding for it. I had enough context in the EA community that I knew where this sentiment was coming from, but I still found it weird to imagine the tendrils of central EA funds reaching all the way down into my little house dinner. And given that the source was probably, at least counterfactually/fungibly, FTX, I\u2019m glad I didn\u2019t take it.</p>\n", "parentCommentId": null, "user": {"username": "Sam Elder"}}, {"_id": "ibsJidnZFoXLhrfKz", "postedAt": "2022-11-18T22:15:13.185Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<blockquote><p>It's pretty well established that low trust societies have more crime and corruption than high trust societies</p></blockquote><p>FWIW, I've generally assumed that causality goes the other way, or a third factor causes both.&nbsp;</p>", "parentCommentId": "Lm4pvNvCsFyh4iC6Z", "user": {"username": "Linch"}}, {"_id": "QusFhwcoNSZwymZuE", "postedAt": "2022-11-19T00:47:18.657Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Philosophers have perhaps struggled with these impossible questions before,</p><p>https://brill.com/view/journals/rip/26/1/article-p25_2.xml?language=en</p><p>-- being new here, I have the feeling that for all of its love of applied utilitarian philosophy, this community is dissapointing in its lack of openness to other philosophical readings.</p><p>I understand that due to the recency and impact of events there is a work of mourning happening in this forum, and proposing healthy paths for this , if possible, &nbsp;is important to those who have belonged and formed affinities and conversations. The trajectory change has already happened, it just may not be recognizable yet.&nbsp;</p>", "parentCommentId": null, "user": {"username": "cookiecut"}}, {"_id": "CgGkC8t36YuhbH3Qk", "postedAt": "2022-11-19T00:58:12.091Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I don't know if an EA community is a good thing, but as a related point, I think it's worthwhile to share that I think the EA community as it currently exists, and in particular, EA leadership, has done a very poor job in advancing the interests of EA causes.</p>\n<p>At present, EA has an awful reputation and most people view the community with contempt and it's ideas as noxious.</p>\n<p>Candidly, I'm embarrassed to share any affiliation I have with EA to colleagues and non-close peers.</p>\n<p>This didn't have to be this way and frankly, given the virtue of EA, it takes a special type of failure to have steered the community down this path.</p>\n<p>I think EA would be significantly better served if a number of leading EA orgs and thought leaders dramatically reevaluated their role, strategy and involvement with EA.</p>\n", "parentCommentId": null, "user": {"username": "DMMF"}}, {"_id": "CayWmAi25qaSb6gAm", "postedAt": "2022-11-19T01:33:55.643Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Yes, economists chose to use the term 'trust' but I think a better term for what they are really discussing is 'trustworthyness'; I suspect they made the substitution for optics reasons.</p>", "parentCommentId": "ibsJidnZFoXLhrfKz", "user": {"username": "Larks"}}, {"_id": "Wmac4WH79rKxtFLuQ", "postedAt": "2022-11-19T01:43:31.201Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>The shift to longtermism/x-risk to me seems to have been an intentional one, but your comment makes it sound otherwise?</p>", "parentCommentId": "fBSrwBMybhCgmAbug", "user": {"username": "pseudonym"}}, {"_id": "HJaupRbEbf9uu7pEp", "postedAt": "2022-11-19T06:54:30.600Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I don't know what you mean by intentional or not.</p><p>But my guess is that the community will shift more long-termist after more people have had time to digest What We Owe the Future.</p>", "parentCommentId": "Wmac4WH79rKxtFLuQ", "user": {"username": "casebash"}}, {"_id": "cCZbwGHJmwDC4y9pA", "postedAt": "2022-11-19T07:39:32.643Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>The community is shifting more long-termist because of intentional decisions that were made - there's no reason that these shifts have to be locked into place if there happens to be a good reason to shift away from them - not suggesting there is one! If the shift turns out to be a mistake in the future, we should be happy to move away from it, not say \"oh but the community may shift towards it in the future\", especially when that shift is caused by intentional decisions in EA leadership.</p>", "parentCommentId": "HJaupRbEbf9uu7pEp", "user": {"username": "pseudonym"}}, {"_id": "fZ4NXdRvZ3BFd7Frq", "postedAt": "2022-11-19T07:44:34.097Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I guess this is why I asked what you meant.</p><p>Publishing What We Owe the Future was an intentional decision, but there's a sense in which people read whatever people write and make up their own minds.</p><p>\"Oh but the community may shift towards it in the future\" - I guess some of these shifts are pretty predictable in advance, but that's less important than the point I was making about maintaining option value especially for options that are looking increasingly high value.</p>", "parentCommentId": "cCZbwGHJmwDC4y9pA", "user": {"username": "casebash"}}, {"_id": "3wW3G47ZiTnSGAyJA", "postedAt": "2022-11-19T11:25:35.944Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<blockquote><p>Candidly, I'm embarrassed to share any affiliation I have with EA to colleagues and non-close peers.</p></blockquote><p>I only realised this recently, but honestly I think most people are embarrassed to share almost <i>any</i> ethical (political, religious, philosophical, etc) affiliation with their colleagues and non-close peers. So I think that avoiding that is an unreasonable standard to expect or be aiming for.</p><blockquote><p>This didn't have to be this way and frankly, given the virtue of EA, it takes a special type of failure to have steered the community down this path.</p></blockquote><p>I can see why you'd see things this way as an EA adherent. But I'm sure many members of various political parties, movements, religious groups etc feel the same - given that our ideas are so obviously incredibly virtuous, it's exceptionally bad that our leaders have nevertheless managed to make us look bad to most people outside of the community (i.e. the set of people who don't think it looks so good that they've joined).</p><blockquote><p>I think EA would be significantly better served if a number of leading EA orgs and thought leaders dramatically reevaluated their role, strategy and involvement with EA.</p></blockquote><p>PR is hard. &nbsp;Extremely hard. Otherwise you wouldn't have this situation where affiliations with pretty much any group generally lose you a bit of status in the eyes of people outside those groups. I think a better indicator of success is growth, assuming you want growth. And I actually &nbsp;think that when 'EA' has wanted growth, we've generally been very good at it.</p><p>And that's in spite of the limitation of a significant fraction of the community chastising others in the community for ever caring about PR.</p>", "parentCommentId": "CgGkC8t36YuhbH3Qk", "user": null}, {"_id": "4YiYowoAtmLHkffbn", "postedAt": "2022-11-19T11:54:00.060Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<blockquote><p>https://brill.com/view/journals/rip/26/1/article-p25_2.xml?language=en</p><p>--being new here, I have the feeling that for all of its love of applied utilitarian philosophy, this community is dissapointing in its lack of openness to other philosophical readings</p></blockquote><p>I think this is a bit like saying, \"[Link to avocado salad recipe.] For all its love of cookies, this Sugar Appreciation Society is disappointing in its lack of openness to other foods.\"</p><p>We're not <i>only</i> about utilitarianism, sure. A lot of us take <a href=\"https://www.williammacaskill.com/info-moral-uncertainty\">moral uncertainty</a> seriously. We realise that other philosophical traditions might be helpful in guiding us towards good <a href=\"https://forum.effectivealtruism.org/posts/nvus8kuGxyacyfXeg/naive-vs-prudent-utilitarianism\">rule/prudent utilitarian</a> practices and providing us with inspiration in our personal lives outside of EA. Some of us have much <a href=\"https://forum.effectivealtruism.org/posts/wtQ3XCL35uxjXpwjE/ea-survey-2019-series-community-demographics-and#Morality\">stronger beliefs</a> in other ethical theories than we do in utilitarianism.</p><p>But if a 'trajectory change' means that 'EA' on the whole stops resembling utilitarianism significantly more than other ethical theories, I personally wouldn't consider that EA any more.</p>", "parentCommentId": "QusFhwcoNSZwymZuE", "user": null}, {"_id": "yEt2SHjXst2ekghgT", "postedAt": "2022-11-19T12:24:17.641Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Perhaps it is more like saying that the Sugar Appreciation Society tends to have a certain blindness to 1. the effects of sugar (which in extreme cases may cause blindness) 2. the effects of appreciation or fanaticism &nbsp;or sweetness as a fetish or as a means of persuasion into a cause 3. The complex origins and history of sugar and its links to empire and exploitation.&nbsp;</p><p>But it is hard to not like SAS. As a non-SAS I am in awe of what has been built and discussed, and even of the openness that does exist which is greater than in other societies, and I am at fault for pointing towards its closedness when I am new, and prefer not to be as open about my love of sugar but engage in it perhaps less conciously than I would like, and also must acknowledge that sugar is everywhere.&nbsp;</p><p>But if SAS looks at itself in the mirror it wants to see only a sweet image. It that image changes, as it must, into a saltier or spicier one as it may as it engages with other perspectives, SAS may become unrecognizable, and may fear that, as one fears a ghost ?</p>", "parentCommentId": "4YiYowoAtmLHkffbn", "user": {"username": "cookiecut"}}, {"_id": "g8wB2JjRLv9zY6fvQ", "postedAt": "2022-11-19T12:43:21.071Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>You mean something like this ? <a href=\"https://www.carnegie.org/about/governance-and-policies/\">https://www.carnegie.org/about/governance-and-policies/</a></p><p>I feel like Effective Ventures, the parent organization here, lacks this and more. One does not know if it has a startup feeling and they have not yet gotten to the point of giving clarity on these things, or if the lack of clarity is part and parcel.&nbsp;</p>", "parentCommentId": "8DqREBtuBuGmEghSA", "user": {"username": "cookiecut"}}, {"_id": "8NfaqenhYMFJnJMdT", "postedAt": "2022-11-19T14:07:17.572Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<blockquote><p>Obviously the motivation for community-building is not that the community is an end in itself, but instrumental: more people \"joining EA\", taking the GWWC pledge and/or going into directly high-impact work, means indirectly causing more good for all the other EA causes that we ultimately care about.</p></blockquote><p>&nbsp;</p><p>I took OP's point here to be that this logic looks suspiciously like the kind of rationalizations EA got its start criticizing in other areas.&nbsp;</p><p>\"Why do they throw these fancy gala fundraising dinners instead of being more frugal and giving more money to the cause?\" seems like a classic EA critique of conventional philanthropy. But once EA becomes not just an idea but an identity, then it's understood that building the community is per se good, so suddenly sponsoring a fellowship slash vacation in the Bahamas becomes virtuous community building. To anyone outside the bubble, this looks like just recapitulating problems from elsewhere.&nbsp;</p>", "parentCommentId": "KcnYquwjTCcrMGB39", "user": {"username": "Matthew Yglesias"}}, {"_id": "PcoEut8ZmLnZDMaLD", "postedAt": "2022-11-19T14:40:38.200Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>If I had to guess, I'd point at having a long bulleted list of different specific predictions about the future as a risk factor for someone registering disagreement.&nbsp;</p>", "parentCommentId": "SAbTCPQQbaGSffimw", "user": {"username": "Lumpyproletariat"}}, {"_id": "bGhc2n4dzCPTJAggK", "postedAt": "2022-11-19T14:55:17.675Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Hmm, I think of the \"classic EA\" case for GiveWell over Charity Navigator as precisely based on an awareness that bad optics around \"overhead\", CEO pay, fundraising, etc., aren't necessarily bad uses of funds, and we should instead look at what the organization ultimately achieves.</p>", "parentCommentId": "8NfaqenhYMFJnJMdT", "user": {"username": "RYC"}}, {"_id": "HLzJvN4xTWWhuLEoG", "postedAt": "2022-11-19T15:56:45.165Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>\"What is missing to me is an explanation of exactly how your suggestions would prevent a future SBF situation.\"<br><br>1. The community is unhealthy in various ways.&nbsp;<br>2. You're suggesting centralizing around high trust, without a mechanism to build that trust.<br><br>I don't think that the EA community could have stopped SBF, but they absolutely could have been independent of him in ways that mean EA as a community didn't expect a random person most of us had never &nbsp;heard of before this to automatically be a trusted member of the community. Calling people out is far harder when they are a member of your trusted community, and the people who said they had concerns didn't say it loudly because they feared community censure. That's a big problem.</p>", "parentCommentId": "Lm4pvNvCsFyh4iC6Z", "user": {"username": "Davidmanheim"}}, {"_id": "7AcH2GjKcKrwHxWdk", "postedAt": "2022-11-19T15:58:46.898Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Completely fair - I'm not endorsing him, just pointing to a source for the allegation. (And more recent allegations, about a compete lack of control and self dealing, are far more damning.)</p>", "parentCommentId": "aDnTxfMcC6F6MHJED", "user": {"username": "Davidmanheim"}}, {"_id": "fcq6zEccNiyag3emk", "postedAt": "2022-11-19T16:07:00.524Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>This seems very plausibly a better direction. I think we agree there is something wrong, and the direction you're pointing may be a better one - but I'm concerned, because I don't see a way to make an extent and large community shift, and think that we need a more concrete theory of change...<br><br>Speaking of which, \"I still don\u2019t know where to find a good, simple article or video that describes how to create a theory of change\" - you should have asked! I'd recommend <a href=\"https://www.unicef-irc.org/publications/pdf/brief_2_theoryofchange_eng.pdf\">here</a> and <a href=\"https://archive.globalfrp.org/evaluation/the-evaluation-exchange/issue-archive/evaluation-methodology/an-introduction-to-theory-of-change\">here</a>. (I also have a couple more PDFs of relevant articles from classes in grad school, if you want.)</p>", "parentCommentId": "KvJ6tEbSCHdpAq7AJ", "user": {"username": "Davidmanheim"}}, {"_id": "sfDa2asrC6n7cJA2d", "postedAt": "2022-11-19T16:10:03.424Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>It seems like you're ignoring that he said EA has an actively bad reputation, and viewing this as a generic claim about not wanting to share a view others don't embrace.</p>", "parentCommentId": "3wW3G47ZiTnSGAyJA", "user": {"username": "Davidmanheim"}}, {"_id": "oRWcNfkxYNATj7tiB", "postedAt": "2022-11-19T16:13:23.205Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>First, I didn't say there are no benefits.&nbsp;<br><br>Second, I don't have a clear enough vision to lay out the alternative,<br><br>Third, I'm skeptical of doing a cost-benefit analysis without considering options for a specific actor. &nbsp;I can't usefully compare alternatives of specific actions I would take, as someone not controlling any of the decisions. If CEA wants to evaluate 3 different potential strategies, they could do a useful CBA.</p>", "parentCommentId": "BmmFWjE6c92izqBWe", "user": {"username": "Davidmanheim"}}, {"_id": "vEhcLgBh7nKpjKvtm", "postedAt": "2022-11-19T16:17:31.789Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Don't you think that CEA can run a large community shift, by changing the guidance and incentives for local groups?<br><br>(Thank you so much for the material! Seems better than the material I have today, but I think we need much simpler and more communicative material for proper out-facing community building)</p>", "parentCommentId": "fcq6zEccNiyag3emk", "user": {"username": "GidonKadosh"}}, {"_id": "sw9m8t2PETiGLcsKQ", "postedAt": "2022-11-19T17:01:54.539Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I hope they can, but don't know that it's easy to direct groups so easily. My biggest concern is with college EA groups, where well-intentioned 21 year olds with very limited life experience &nbsp;are running groups, often without much external monitoring.<br><br>And regarding material for theories of change, I'm skeptical that it can be taught well without somewhat deep engagement. In grad school, it took thinking, feedback, and practice to get to the point where we could coherently lay out a useful theory of change.</p>", "parentCommentId": "vEhcLgBh7nKpjKvtm", "user": {"username": "Davidmanheim"}}, {"_id": "AeABqPgh4pCzE5JjJ", "postedAt": "2022-11-19T17:24:27.082Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I'm not a member of the EA community, and in fact have been quite sceptical of it, but I do believe in the idea of altruism being effective, so I wanted to engage on a post like this. For context, I work on public health in developing countries, and have worked in a variety of fields in the traditional aid sector from agriculture to women's rights to civil society \u2013 in my observation public health is the most effective, followed by agriculture. While I'm sceptical of EA as a community, I do believe in some of the tenets, and even use services like GiveWell to guide my own donations. I wanted to ask the some questions, if you or community members are willing to answer \u2013 bear with me as I don't know the EA jargon &nbsp;that well.<br><br>One of the main reasons I'm sceptical is sort of a generalised scepticism of cliques, identities, and subcultures in general. Human beings are social animals, and we naturally seek status. So when a community/subculture forms, suddenly people seek status in it, seek to associate with its 'leaders' or popular causes, this short circuits the ostensibly rational analysis people think they're doing. Of course we don't know we're doing this, we think we're being perfectly rational, but a lot of what I hear coming out of the EA community seems to follow this dynamic - longtermism, crypto, FTX and its supporters all feel typical of social dynamics in other communities, and I just don't think there's any way for human beings to get around this. Does this make the idea of an EA 'community' self-defeating?<br><br>Similarly, the other aspect I don't see EA (as it filters to the outside) dealing with well is humility, specifically humility and one's own motivations and humanity. Knowing we're susceptible to all sorts of faults, we should acknowledge these when we make plans. When we observe thousands of people saying 'I'll do good things when I'm rich/powerful', and then getting caught up in their own world, it seems absurd to say 'aha, but I'll be different!' We have to assume that some of our motivations aren't entirely pure or rational. EA seems to think we can put that aside with enough technical terminology and get to pure reason, but I just don't see it happening. Once it becomes a human social structure, how can humility remain a part of an EA community? Human communities just don't tend to work that way - and the individuals who DO see subtleties tend to lose status in closed communities.</p><p>My own view is that things that work in the world are rare, so when you find one you need to do what you can to replicate or widen it. You also need to double-check it constantly \u2013 for my work I insist on constantly interacting at the clinic level to see if what I'm building is actually used and useful, and change it if it isn't. I want to fully acknowledge the massive pathologies of the formal aid sector, but I work to mitigate those in the course of my job. I haven't, to be honest, seen anything from the EA community that would help me with that other than an articulation of fairly obvious general principles. So what is it all for?</p>", "parentCommentId": null, "user": {"username": "Michael M-n"}}, {"_id": "aCQ4Xet3eZH8uP9LF", "postedAt": "2022-11-19T17:39:44.590Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I reeeeeaally don't want to get into harmful stereotypes here obviously so I'll just pick a few real examples in my own case: when I've told old colleagues, family, school-friends etc that I was vegan, an environmentalist, a feminist, I think their reaction was quite a bit worse than simply \"I don't agree.\" But maybe we move in different circles.</p><p>Of course it may be the case that EA really does have \"an awful reputation and most people view the community with contempt and it's ideas as noxious.\" But as any movement grows it's bound to attract more and more bad (and good) press, and you're going to feel embarrassed talking about it, and in times of bad press it may well feel like it's taken over your entire brand and it's doomed and it's taken \"a special type of failure\" to cause this and leadership should \"dramatically reevaluate\" their involvement in EA i.e. I'm generally expecting people to be more pessimistic than they should be right now.</p><p>And even before all this I'd been noticing in myself that I was taking \"I'm embarrassed to tell people I'm an EA\" as a doom-y sign, but then I realised basically all causes have this effect so that in itself is not sufficient for me to conclude that we'd done a really bad job of PR. So I wanted to point this out to anyone I suspected was thinking similarly.</p>", "parentCommentId": "sfDa2asrC6n7cJA2d", "user": null}, {"_id": "GKovzznRAw3nRb6ok", "postedAt": "2022-11-19T19:01:02.658Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Welcome! I'm a bit pushed for time but thought I'd offer up an answer to you question:</p><blockquote><p>So what is it all for?</p></blockquote><p>If I had to pick one USP of EA it's the serious attempt to <a href=\"https://forum.effectivealtruism.org/topics/cause-prioritization\">prioritise between causes</a>.</p>", "parentCommentId": "AeABqPgh4pCzE5JjJ", "user": null}, {"_id": "a6oRFHypgKMBPH3Sh", "postedAt": "2022-11-19T19:34:50.542Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>It's worth considering <a href=\"https://forum.effectivealtruism.org/posts/aHPhh6GjHtTBhe7cX/proposals-for-reform-should-come-with-detailed-stories\">Eric Neyman's questions</a>: (1) are the proposed changes realistic, (2) would the changes actually have avoided the current crisis, and (3) would its benefits exceed its costs generally.</p><p>On <strong>(1)</strong>, I think David's proposals are clearly realistic. Basically, we would be less of an \"undifferentated social club\", and become more like a group of academic fields, and a professional network, with our conferences and groups specialising, in many cases, into particular careers.</p><p>On <strong>(2)</strong>, I think part of our mistake was we used an overly one-dimensional notion of trust. We would ask \"is this person <i>value-aligned</i>?\", as a shorthand for evaluating trustworthiness. The problem is that any self-identified utilitarian who hangs around EA for a while will then seem trustworhty, whereas objectively, an act utilitarian might be anything but. Relatedly, we thought that if our leadership trust someone, we must trust them too, even if they are running an objectively shady business like an offshore crypto firm. This kind of deference is a classic problem for social movements as well.</p><p>Another angle on what happened is that FTX behaved like a splinter group. Being a movement means you can convince people of some things for not-fully-rational reasons - based on them liking your leadership and social scene. But this can also be used against you. Previously, the rationalist community has been burned by the Vassarites, the Zizians. There has been Leverage Research. And now, we could say that FTX had its own charismatic leadership, dogmas (about drugs, and risk-hunger), and social scene. If we were less like a movement, it might've been harder for them to be.</p><p>So I do think being less communal and more professional could make things like FTX less likely.</p><p>On <strong>(3)</strong>, I think this change would come with significant benefits. Fewer splinter groups. Fewer issues with sexual harrassment (since it would be less of a dating scene). Fewer tensions relating to whether written materials are \"representative\" of people's views. Importantly, high-performing outsiders would less likely bounce off due to EA seeming \"cultic\", as did Sam Harris, per his latest pod. Also see Matt Y's <a href=\"https://twitter.com/mattyglesias/status/1593635421854044164\">comment</a>.</p><p>I think the main downside to the change would be if it involved giving up our impact somehow. Maybe movements attract more participants than non-movements? But do they attract the right talent? Maybe members of a movement are prepared to make larger sacrifices for one another? But this doesn't seem a major bottleneck presently.</p><p>So I think the proposal does OK on the Eric-test. There is way more to be said on all this, but FWIW, my current best guess is that David's ideas about <i>professionalising</i> and <i>disaggregating by professional interest</i> should be a big part of EA's future.</p>", "parentCommentId": null, "user": {"username": "RyanCarey"}}, {"_id": "tgHZ7JrnDbJFJftNT", "postedAt": "2022-11-20T07:17:52.984Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<blockquote><p>Does this make the idea of an EA 'community' self-defeating?</p></blockquote><p><br>I think that it's possible to have a community that minimizes the distortion of these social dynamics, or at least is capable of doing significant good despite the distortions - but as I argued in the post, at scale this is far harder, and I think it might be net negative to try to build a single global community the way EA seems to have decided to do.<br>&nbsp;</p><blockquote><p>My own view is that things that work in the world are rare, so when you find one you need to do what you can to replicate or widen it.&nbsp;</p></blockquote><p>Agreed - and that was one of the key things early EA emphasized - and it's been accepted, in small part due to EA, to the point that it is now conventional wisdom.</p><blockquote><p>I want to fully acknowledge the massive pathologies of the formal aid sector, but I work to mitigate those in the course of my job. I haven't, to be honest, seen anything from the EA community that would help me with that other than an articulation of fairly obvious general principles.</p></blockquote><p>I don't think that EA as a movement is well placed to provide ways to reform traditional aid. As you point out, it has many pathologies, and I don't think there is a simple answer to fix a complex system deeply embedded in geopolitics and social reality. I do think that EA-promoted ideas, including giving directly, have the potential to displace some of the broken systems, and we should work towards figuring out where simpler systems can replace &nbsp;current complex but broken ones. I also think that an EA-like focus on measuring outcomes helps push for the parts of traditional aid that do work. That is, it identifies specific programs &nbsp;which are effective and evaluates and scales them. This isn't to say that traditional aid doesn't have related efforts which are also successful, but I think overall it's helpful to have external pushes from EA and people who embrace related approaches for this work.</p>", "parentCommentId": "AeABqPgh4pCzE5JjJ", "user": {"username": "Davidmanheim"}}, {"_id": "hhxSbWJcEAqSoNELc", "postedAt": "2022-11-20T08:25:40.633Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I think that social ties are useful, yet having a sprawling global community is not. I think that you're attacking a bit of a straw man, one which claims that we should have no relationships or community whatsoever.<br><br>I also think that there is an unfair binary you're assuming, where on one side you have \"unpaid, ad-hoc community organising\" and on the other you have the current abundance of funding for community building. Especially in EA hubs like London, the Bay Area, and DC, the local community can certainly afford to pay for events and event managers without needing central funding, and I'd be happy for CEA to continue to do community building - albeit with the expectation that communities do their own thing and pay for events, which would be a very significant change from the current environment. Oh, and I also don't live in an EA hub, and have never attended an EAG - but I do travel occasionally, and have significant social interaction with both EAs and non-EAs working in pandemic preparedness, remotely. &nbsp;The central support might be useful, but it's far from the only way to have EA continue.</p>", "parentCommentId": "6jDe9tPnEp5ZCBNsN", "user": {"username": "Davidmanheim"}}, {"_id": "7yeJHePXavsa3GFxH", "postedAt": "2022-11-20T23:49:58.981Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Thanks for this response! I don't want to go too deep on the traditional aid sector, being still in it and all, but I do think they could do with a lot more thinking about real effectiveness. Or even just to occasionally step back, and think 'what are we trying to do here?' I don't disagree with anything you've written, except to wonder if it's even possible to have non-global communities anymore, and if even small scale communities succumb to the same dynamics.</p><p>Just when an idea becomes popular it becomes a community, and the community imports the social dynamics. I suppose if I had to say what I would envision for something EA-like is in line with a what you said about conventional wisdom \u2013 a kind of invisible force that no one really identifies with, but nudges decisions in a better direction. To some extent I wonder if game theory and microeconomics have maybe achieved this \u2013 people seem to subconsciously think a lot more in terms of cost/benefit than they did 20 years ago. But whenever an online community becomes a 'thing', I really feel like those social communities overwhelm \u2013 and my experience living in Berlin suggests to me that even the tiniest subcultures develop the same dynamics.</p><p>Speaking of geopolitics and social reality, do you think EA grapples with that well? In my experience one of the most crucial elements of effectiveness for aid projects has been good buy-in from the local government and community, which can be a messy, political and extremely tedious process, and I'm lucky enough to have an employer that takes the time. What's the EA opinion on 'do something suboptimal because otherwise one department of a ministry will hate you and your whole project is screwed'?</p>", "parentCommentId": "tgHZ7JrnDbJFJftNT", "user": {"username": "Michael M-n"}}, {"_id": "mzBdKdFHax3Jdhvoe", "postedAt": "2022-11-21T09:04:58.905Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<blockquote><p>Ultimately this was a failure of the EA <i>ideas</i> more so than the EA <i>community</i>. SBF used EA ideas as a justification for his actions. Very few EAs would condone his amoral stance w.r.t. business ethics, but business ethics isn't really a central part of EA ideas. Ultimately, I think the main failure was EAs failing to adequately condemn naive utilitarianism.&nbsp;</p></blockquote><p>So I disagree with this because:</p><ol><li>It's unclear whether it's right to attribute SBF's choices to a failure of EA ideas. Following SBF's interview with Kelsey Piper and based on other things I've been reading, I don't think we can be sure at this point whether SBF was generally more motivated by naive utilitarianism or by seeking to expand his own power and influence. And it's unclear which of those headspaces led him to the decision to defraud FTX customers.</li><li>It's plausible there actually were serious ways that the EA community failed with respect to SBF. According to <a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId=SQZo8AQe53mh45kkc\">a</a> <a href=\"https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId=D5tcqafcnikFLabTh#YNH2Gj35cueyT35tp\">couple</a> &nbsp;<a href=\"https://forum.effectivealtruism.org/posts/SP3Hkas3jo6i2cmqb/who-s-at-fault-for-ftx-s-wrongdoing?commentId=5YXZc7pQHkCMGyBgS\">accounts</a>, at least several people in the community had reason to believe SBF was dishonest and sketchy. Some of them spoke up about it and others didn't. The accounts say that these concerns were shared with more central leaders in EA who didn't take a lot of action based on that information (e.g. they could have stopped promoting Sam as a shining example of an EA after learning of reports that he was dishonest, even if they continued to accept funding from him). [1]<br><br>If this story is true (don't know for sure yet), then that would likely point to community failures in the sense that EA had a fairly centralized network of community/funding that was vulnerable, and it failed to distance itself from a known or suspected bad actor. This is pretty close to the OP's point about the EA community being high-trust and so far not developing sufficient mechanisms to verify that trust as it has scaled.</li></ol><p>--</p><p>[1]: I do want to clarify that in addition to this story still not being unconfirmed, I'm mostly not trying to place a ton of blame or hostility on EA leaders who may have made mistakes. Leadership is hard, the situation sounds hard and I think EA leaders have done a lot of good things outside of this situation. What we find out may reduce how much responsibility I think the EA movement should put with those people, but overall I'm much more interested in looking at systemic problems/solutions than fixating on the blame of individuals.</p>", "parentCommentId": "fSp3MW8qvtNf8x3WA", "user": {"username": "Evan R. Murphy"}}, {"_id": "Wc5C3nJf9no4tKB5Z", "postedAt": "2022-11-21T23:17:59.914Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>A few thoughts, excuse the mess:</p>\n<p>I think that local groups should continue to exist, they are what makes up the community. In my view, you can run a local group well without outside funding. In any case, your local group should not be dependent on outside funding.</p>\n<p>I'd like to see CEA run a topic-specific conference.</p>\n<p>I'd like to see more democratically organized EA structures, like the German <a href=\"https://ealokal.de\">https://ealokal.de</a></p>\n<p>I'd like to see more grassroots community events by and for community members.</p>\n<p>None of this would have prevented SBF from committing fraud, but we would feel fewer ripple effects in the community.</p>\n", "parentCommentId": null, "user": {"username": "ludwigbald"}}, {"_id": "9EnELoJeZuXg2NLEZ", "postedAt": "2022-11-22T01:34:09.816Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I really like this framing Gideon. It seems aligned with CEA's <a href=\"https://www.centreforeffectivealtruism.org/core-principles\">Core EA principles.</a> I'd love EA to be <a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-all-teach-here-s-how-to-do-it-better\">better at helping people learn skills</a>. &nbsp;One of our working drafts for an EA MOOC focuses more on the those core principles and skills. Is something like <a href=\"https://docs.google.com/spreadsheets/d/11-Bv95SWVlBAaITaDZqqV9pLdaoKM8NLCP-TVac2YrQ/edit?usp=sharing\">this work-in-progress closer</a> to what you had in mind?</p>", "parentCommentId": "KvJ6tEbSCHdpAq7AJ", "user": {"username": "mnoetel"}}, {"_id": "9rkGGYkzoEJZC5CyE", "postedAt": "2022-11-22T13:16:50.810Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>This hurts but it checks out.</p>\n", "parentCommentId": null, "user": {"username": "nongiga"}}, {"_id": "L3GgTZqwS5D46PCTn", "postedAt": "2023-09-28T17:57:06.867Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Hi David, I think I follow your thinking, but I'm not hopeful that there is a viable route to \"ending the community\" or \"ending community-building\" or ending people \"identifying as EAs\", even if a slight majority agreed it was desirable, which seems unlikely.<br><br>On the other hand, I vary much agree that a single Oxford or US-based organisation can't \"own\" and control the whole of effective altruism, and aiming not for a \"perfect supertanker\" but a varied \"fleet\" or \"regatta\" of EA entities would be preferable, and much more viable. Then supervision and gatekeeping and checks could be done in a single timezone, and the size of EA entities and groups could be kept at 150 or less. Also different EA regions or countries or groups could develop different strengths.&nbsp;<br><br>We'd end up with a confederation, rather like Oxfam, the Red Cross, Save the Children etc. (It's not an accident that the federated movements often have a head office in the Netherlands or Switzerland, where the laws on what NGOs/charities can and can't do are more flexible than in the UK or USA, which is kinda helpful for an 'unusual' movement like EA.)<br><br>Oxfam themselves also formed INTRAC as a training entity, and one could imagine CEA doing something similar, offering training in (for example)<br>- lessons learned<br>- bringing in MEv &nbsp;trainers for evaluation training<br>- PLA trainers for participatory budgeting etc.</p>", "parentCommentId": null, "user": {"username": "RayTaylor"}}, {"_id": "c6AKTq2uwaivaEY7w", "postedAt": "2023-09-28T18:01:36.809Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>Both of you now seem to be focusing specifically on funding for community building, whereas the original post was much broader:<br><br>... maybe if those broader issues were addressed, the question of which community-building to fund would then be easier to work out?</p>", "parentCommentId": "hhxSbWJcEAqSoNELc", "user": {"username": "RayTaylor"}}, {"_id": "stuawJwpZ2yMBGw4P", "postedAt": "2023-09-28T18:07:59.947Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>It's also hard to call people out when a lot of you are applying to him/them for funding, and are mostly focused on trying to explain how great and deserving your project is.<br><br>One good principle here is \"be picky about your funders\". Smaller amounts from steady, responsible, principled and competent funders, who both do and submit to due diligence, are better than large amounts from others.&nbsp;<br><br>This doesn't mean you HAVE to agree with their politics or everything they say in public - it's more about having proper governance in place, and funders being separate from boards and boards being separate from executive, so that undue influence and conflicts of interest don't arise, and decisions are made objectively, for the good of the project and the stated goals, not to please an individual funder or get kudos from EAs.<br><br>I've written more about donor due diligence in the main thread, with links.</p>", "parentCommentId": "HLzJvN4xTWWhuLEoG", "user": {"username": "RayTaylor"}}, {"_id": "rAWgavsxJuzwkva9E", "postedAt": "2023-09-28T18:21:02.957Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<blockquote><p>&gt; healthy for people to separate giving to their community from altruism.<br><br>Is this realistically achievable, with the community we have now? How?</p><p><br>(I imagine it would take a comms team with a social psychology genius and a huge budget, and still would only work partially, and would require very strong buy in from current power players, and a revision of how EA is presented and introduced? but perhaps you think another, leaner and more viable approach is possible?)<br><br>&gt;The simpler your path to impact is, the fewer failure points exist<br><br>That's not always true.&nbsp;<br><br>Some extreme counter-examples:<br><br>a. Programmes on infant stunting keep failing, partly because an overly simple approach has been adopted (intensive infant feeding, Plumpy Nuts etc, with insufficient attention to maternal nutrition, aflatoxin removal, treating parasites in pregnancy, adolescent nutrition, conditional cash transfers etc)<br><br>b. A critical path plan was used for Apollo, and worked much better than the simpler Soviet approach, despite being much more complicated.&nbsp;<br><br>c. The Brexit Leave campaign SEEMED simple but was actually formed through practice on previous campaigns, and was very sophisticated \"under the hood\", which made it hard to oppose.</p></blockquote>", "parentCommentId": "5FBLwDmHqLRW3hHyS", "user": {"username": "RayTaylor"}}, {"_id": "ezdS5wx3KDCew7Niy", "postedAt": "2023-09-28T19:06:18.602Z", "postId": "56CHyqoZskFejWgae", "htmlBody": "<p>I think it's wise to separate the FTX and due diligence issue from the broader thesis. Here I'm just commenting on due diligence with donors.<br><br>Who was/is responsible for checking the probity or criminality of ...<br><br>&nbsp;(a) FTX and Almeda?<br><br>&nbsp;(b) donors to a given charity like CEA? (I put some links on this below)<br><br>(a) First it's their own board/customers/investors, but presumably supervisory responsibility is or should also be with central bank regulators, FBI, etc. If the CEO of a company is a member of Rotary, donates to Oxfam, invests in a football team, it doesn't suddenly become the primary responsibility of all of those entities (ahead of board, FBI etc) to check out his business and ethics and views, <i>unless</i> (and this is important) he's going to donate big and then have influence on programmes, membership etc.&nbsp;<br><br>(See the links below on how both due diligence and reputational considerations* can matter a lot to the recipient charity. If there is some room for doubt about the donor, but it doesn't reach a threshold, it may be possible to create a layer of distance or supervision e.g. create a trust with it's own board, which does the donating.)<br><br>(b) Plenty of charities accepted donations from Enron, Bernie Madoff and others.&nbsp;<br><br>Traditionally, their job is to do <i>their</i> job, not evaluate the probity of all their donors. However, there has been a change of mood since oil industry disinvestment campaigns and the opioid crisis (with the donations from the Sackler family, here in the UK at least**). Political parties are required to do checks on donors.&nbsp;<br><br>Marshall Rosenberg turned down lots of people who wanted to fund NVC and the cNVC nonprofit, because he felt that taking money put him into relationship with them, and some companies he just didn't want to be in relationship with. This worked well for him, and made sure there was no pressure to shift focus, but it did frustrate his staff team quite often.<br><br>It might be possible as a matter of routine policy to ask large donors if they are happy to have their main income checked, especially if they want to be publicly associated with a particular project, or to go more discreetly to ratings agencies and so on. A central repository of donor checks could be maintained, to minimise costs. This wouldn't be perfect, but a due diligence process, ideally open and transparent, would sometimes be a good defence if problems arise later?&nbsp;<br><br>These are the (more minimal) UK Charity Commission guidelines on checking out your donors, and even this might have helped if it had been done rigorously:<br><br><a href=\"https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/550694/Tool_6.pdf\">https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/550694/Tool_6.pdf</a>&nbsp;<br><br>Here's a plain English version where the overall advice is \"be reasonable\":<br><br><a href=\"https://manchestercommunitycentral.org/sites/manchestercommunitycentral.co.uk/files/Ethical%20fundraising%20-%20how%20to%20conduct%20due%20diligence%20on%20potential%20donors_0.pdf\">https://manchestercommunitycentral.org/sites/manchestercommunitycentral.co.uk/files/Ethical%20fundraising%20-%20how%20to%20conduct%20due%20diligence%20on%20potential%20donors_0.pdf</a>&nbsp;<br><br>**This is for bigger donations:<br><a href=\"https://www.nao.org.uk/wp-content/uploads/2017/08/Due-diligence-processes-for-potential-donations.pdf\">https://www.nao.org.uk/wp-content/uploads/2017/08/Due-diligence-processes-for-potential-donations.pdf</a> &nbsp;<br><br>*This is about how things went wrong for Prince Charles's charities:<br><a href=\"https://www.charitytoday.co.uk/due-diligence-for-charities-ensuring-transparency-and-trustworthiness/\">https://www.charitytoday.co.uk/due-diligence-for-charities-ensuring-transparency-and-trustworthiness/</a>&nbsp;</p>", "parentCommentId": null, "user": {"username": "RayTaylor"}}]