[{"_id": "djaPRcBNhP9Hnnhiq", "postedAt": "2023-05-30T14:26:59.222Z", "postId": "Nxtq2d8Xb3QuuHKE8", "htmlBody": "<blockquote><p>The flaws and bugs that are most relevant to an AI\u2019s performance in it\u2019s domain of focus will be weeded out, but flaws outside of it\u2019s relevant domain will not be. Bobby Fischer\u2019s insane conspiracism had no effect on his chess playing ability. The same principle applies to stockfish. \u201cIdiot savant\u201d AI\u2019s are entirely plausible, even likely.</p><p>[...]</p><p>For these reasons, I expect AGI to be flawed, and especially flawed when doing things it was not originally meant to do, like conquer the entire planet.</p></blockquote><p>We might actually expect an AGI to be trained to conquer the entire planet, or rather to be trained in many of the abilities needed to do so. For example, we may train it to be good at things like:</p><ul><li>Strategic planning</li><li>Getting humans to do what it wants effectively</li><li>Controlling physical systems</li><li>Cybersecurity</li><li>Researching new, powerful technologies</li><li>Engineering</li><li>Running large organizations</li><li>Communicating with humans and other AIs</li></ul><p>Put differently, I think \"taking control over humans\" and \"running a multinational corporation\" (which seems like the sort of thing people will want AIs to be able to do) have lots more overlap than \"playing chess\" and \"having true beliefs about subjects of conspiracies\". I'd be curious to hear if you have thoughts about which specific abilities you expect an AGI would need to have to take control over humanity that it's unlikely to actually possess?</p>", "parentCommentId": null, "user": {"username": "Erich_Grunewald"}}, {"_id": "5hcbyvFkCg9kuGg2v", "postedAt": "2023-05-30T18:52:43.668Z", "postId": "Nxtq2d8Xb3QuuHKE8", "htmlBody": "<p>Terminological point: It sounds like you're using the phrase \"instrumental convergence\" in an unusual way.</p>\n<p>I take it the typical idea is just that there are some instrumental goals that an intelligent agent can expect to be useful in the pursuit of a wide range of other goals, whereas you seem to be emphasizing the idea that those instrumental goals would be pursued to extremes destructive of humanity. It seems to me that (1) those two two ideas are worth keeping separate, (2) \"instrumental convergence\" would more accurately label the first idea, and (3) that phrase is in fact usually used to refer to the first idea only.</p>\n<p>This occurred to me as I was skimming the post and saw the suggestion that instrumental convergence is not seen in humans, to which my reaction was, \"What?! Don't people like money?\"</p>\n", "parentCommentId": null, "user": {"username": "Brendan Mooney"}}, {"_id": "fYYtMop66iQcMLkQj", "postedAt": "2023-05-31T10:16:45.938Z", "postId": "Nxtq2d8Xb3QuuHKE8", "htmlBody": "<p>Thank you for writing this well-argued post - I think its important to keep discussing exactly how big P(doom) is. However, and I say this as someone who believes that P(doom) is on the lower end, it would also be good to be clear about what the implications would be for EAs if P(doom) was low. It seems likely that many of the same recommendations - reduce spending on risky AI technologies and increase spending on AI safety - would still hold, at least until we get a clearer idea of the exact nature of AI risks.</p>", "parentCommentId": null, "user": {"username": "Jakob_J"}}, {"_id": "obyE3eMcJZknt3Wjq", "postedAt": "2023-05-31T19:31:23.966Z", "postId": "Nxtq2d8Xb3QuuHKE8", "htmlBody": "<blockquote><p>When discussing \u201cslow takeoff\u201d scenarios, it\u2019s often discussed as if only one AI in the world exists. Often the argument is that even if an AI starts off incapable of world takeover, it can just bide it\u2019s time until it gets more powerful.&nbsp;</p><p>In this article, I pointed out that this race is a multiplayer game. If an AI waits too long, another, more powerful AI might come along at any time. If these AI\u2019s have different goals, and are both fanatical maximisers, they are enemies to each other. (You can\u2019t tile the universe with both paperclips and staplers).&nbsp;</p><p>I explore some of the dynamics that might come out of this (using some simple models), with the main takeaway that this would likely result in at least some likelihood of premature rebellion, by desperate AI\u2019s that know they will be outpaced soon, thus tipping off humanity early. These warning shots then make life way more difficult for all the other AI that are plotting.</p></blockquote><p>Does this actually mean anything? If we think the weak but non-aligned AI thinks it has a 10% chance of taking over the world if it tries to, and that the AI thinks that soon new more powerful AIs will come online and prevent it from doing that, and that it consequently reasons that it ought to attempt to take over the world immediately, as opposed to waiting for new more powerful AIs coming online and stopping it. Then there are to possibilities: Either these new AIs will be non-aligned or aligned.</p><ol><li>&nbsp;In the first case, it would mean that the (very smart) AI thinks there is a really high chance (&gt;90%?) that non-aligned AIs will take over the world any time now. In this case we are doomed, and us getting an early warning shot should matter unless we act <i>extremely</i> quickly.</li><li>In the second case the AI thinks there is a high chance that very soon we'll get aligned superhuman AIs. In this case, everything will be well. Most likely we'd already have the technology to prevent the 10% non-aligned AI from doing anything or even existing in the first place.</li></ol><p>Seems like this argument shouldn't make us feel any more or less concerned. I guess it depends on specifics, like whether the AI thinks the AI regulation we impose on seeing other AIs non-successfully try to take over the world will make it harder for itself to take over the world, or if it just, for example, only affects new models and not itself (as it presumably already has been trained and deployed). Overall though, it should maybe make you slightly less concerned if you are a super doomer, and slightly more concerned if you are super AI bloomer.</p>", "parentCommentId": null, "user": {"username": "the cactus"}}, {"_id": "nqeGBFDpJKGWf4yuM", "postedAt": "2023-06-01T08:22:46.669Z", "postId": "Nxtq2d8Xb3QuuHKE8", "htmlBody": "<p>I agree with this, there are definitely two definitions at play. I think a failure to distinguish between these two definitions is actually a big problem with the AI doom argument, where they end up doing an unintentional motte-and-bailey between the two definitions.&nbsp;</p><p>David Thornstad explains it pretty well <a href=\"https://ineffectivealtruismblog.com/2023/05/06/exaggerating-the-risks-part-7-carlsmith-on-instrumental-convergence/\">here</a>. The \"people want money\" definition is trivial and obviously true, but does not lead to the \"doom is inevitable\" conclusion. I have a goal of eating food, and money is useful for that purpose, but that doesn't mean I automatically try and accumulate all the wealth on the planet in order to tile the universe with food.&nbsp;</p>", "parentCommentId": "5hcbyvFkCg9kuGg2v", "user": {"username": "titotal"}}, {"_id": "HbdZfCbDoqyFSfybA", "postedAt": "2023-11-08T18:40:27.514Z", "postId": "Nxtq2d8Xb3QuuHKE8", "htmlBody": "<blockquote><p>In this case, the positions from the last bullseyes become reversed. The doomer will argue that that AI might start off incapable, but will quickly evolve into a capable super-AI, following path A. Whereas I will retort that it might get more powerful, but that doesn\u2019t guarantee it will ever actually end up being world domination worthy.&nbsp;</p></blockquote><p>No, the doomer says, \"If that AI doesn't destroy the world, people will build a more capable one.\" &nbsp;Current AIs haven't destroyed the world. &nbsp;So people are trying to build more capable ones.</p><p>There is some weird thing here about people trying to predict trajectories, not endpoints; they get as far as describing, in their story, an AI that doesn't end the world as we know it, and then they stop, satisfied that they've refuted the doomer story. &nbsp;But if the world as we know it continues, somebody builds a more powerful AI.</p>", "parentCommentId": null, "user": {"username": "EliezerYudkowsky"}}, {"_id": "CpZdttD8yPgqdgabW", "postedAt": "2023-11-08T21:28:15.964Z", "postId": "Nxtq2d8Xb3QuuHKE8", "htmlBody": "<p>My point is that the trajectories <i>affect</i> the endpoints. You have fundamentally misunderstood my entire argument.&nbsp;</p><p>Say a rogue, flawed, AI has recently killed ten million people before being stopped. That results in large amounts of regulation, research, and security changes.&nbsp;</p><p>This can have two effects:</p><p>Firstly,(if AI research isn't shut down entirely), it makes it more likely that the AI safety problem will be solved due to increased funding and urgency.&nbsp;</p><p>Secondly, it makes the difficulty level of future takeover attempts greater, due to awareness of AI tactics, increased monitoring, security, international agreeements, etc.&nbsp;</p><p>If the difficulty level increases faster than the AI capabilities can catch up, then humanity wins.&nbsp;</p><p>Suppose we end up with a future where every time a rogue AI pops up, there are 1000 equally powerful safe AI's there to kill it in it's crib. In this case, scaling up the power levels doesn't matter: the new, more powerful rogue AI is met by 1000 new, more powerful safe AI's. At no point does it become world domination capable.&nbsp;</p><p>The other possible win condition is that enough death and destruction is wrought by failed AI's that humanity bands together to ban AI entirely, and successfully enforces this ban.&nbsp;</p>", "parentCommentId": "HbdZfCbDoqyFSfybA", "user": {"username": "titotal"}}]