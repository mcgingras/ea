[{"_id": "bkHAnhFFv6Qw5QyPs", "postedAt": "2022-12-30T15:46:48.104Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>Hello OP,</p><p>thanks for creating this write up. On the democratizing of grantmaking part of this post. One of the reasons that I gave my full internship pay of 14k as a rising senior to the EA animal welfare fund was that I didn't have to put in any effort into thinking about what organizations/opportunities were effective. I really valued that I could outsource this labor to lewis bollard and other grantmakers that have a good track record.&nbsp;<br>If I feel they are not doing a good job I will switch my donations to ACE though &gt;:3</p>", "parentCommentId": null, "user": {"username": "Daryl D'Souza"}}, {"_id": "DiHh8hnrpza3kbdoE", "postedAt": "2022-12-30T17:12:16.025Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>That makes sense. I think it would be best to retain these option alongside the ideas that I propose</p>\n", "parentCommentId": "bkHAnhFFv6Qw5QyPs", "user": {"username": "freedomandutility"}}, {"_id": "LGitmWH9v773LZbre", "postedAt": "2022-12-30T18:33:41.499Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<blockquote><p>If this seems like a bad idea, try the reversal test: do you think EA orgs should become more integrated?</p></blockquote><p>For what it's worth, this does seem good to me: even the largest EA organizations are tiny compared to for-profit companies, and we miss out on a bunch of economies of scale as a result. There are reasonable criticisms to be made of how EVF (my employer) has done fiscal sponsorship (e.g. perhaps more stuff should have been based in the US instead of the UK) but I would still encourage any new organization to get fiscal sponsorship (from someone besides EVF, if they want) instead of being independent.</p>", "parentCommentId": null, "user": {"username": "Ben_West"}}, {"_id": "ggJrpE5YiXTe4QkGW", "postedAt": "2022-12-30T19:01:31.339Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>Thanks for sharing these!</p><blockquote><p>Regardless of whether my proposed interventions work or fail, there would be no evidence for it.&nbsp;</p></blockquote><p>I guess this is maybe true for some strict definition of \"evidence\", but I would find these suggestions much more helpful if they came with:</p><ol><li>More concreteness. E.g. what things do you think organizations should be transparent about? Is it just that you think grantmakers should publish grant writeups more quickly?</li><li>Actual calculations of trade-offs. E.g. how many additional hours of labor would it take to be transparent in the way that you suggest? What are the actual odds that this transparency results in getting suggestions that improve the organization? Can you make a BOTEC which quantifies the benefits here?</li><li>Specific examples of how these suggestions would have been helpful in the past. E.g. are there historical instances of corruption that your transparency proposal would have caught? How valuable would this have been?</li></ol><p>Right now I can't even tell<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefphwz02ribas\"><sup><a href=\"#fnphwz02ribas\">[1]</a></sup></span>&nbsp;if I'm one of the people you're criticizing (maybe my work is as transparent as you want, I don't know) much less whether I agree with your suggestions.</p><p><i>(Note: it's obviously way more expensive to do what I suggest than to just briefly list your suggestions. But my guess is that it would be substantially more impactful to go into one of these in detail than to give this current high-level list.)</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnphwz02ribas\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefphwz02ribas\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See also <a href=\"https://forum.effectivealtruism.org/posts/Pz7RdMRouZ5N5w5eE/ea-should-taboo-ea-should\">EA should taboo \"EA should\"</a></p></div></li></ol>", "parentCommentId": null, "user": {"username": "Ben_West"}}, {"_id": "vur3vnL5nbbj8hhBk", "postedAt": "2022-12-30T19:49:47.859Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>When you say that \"EA philosophy . . . glosses over key factors other than distance,\" do you mean that EAs do not <em>believe</em> that \"nationalism, localism and racism\" are meaningful factors in explaining mainstream Western charitable priorities, or that EAs do not spend enough time <em>talking</em> about those factors? I would be surprised if you polled a number of EAs and any significant number disagreed with this belief.</p>\n<p>My take is that telling potential donors that they have been doing charity in a nationalist/racist manner is much more likely to get them to stop listening to you than it is to change their practices -- and much, if not most, of the critique of mainstream Western charitable priorities is geared toward outsiders. So it may be more instrumentally effective to lead with and focus on a rationale based on a universal cognitive bias that potential donors can accept without having to label their past charitable behavior as racist/nationalist.</p>\n<p>Do you think this may be an instance where the difference lies mainly in considerations of inherent vs. instrumental value? Or do you think EAs tend to get the tactical approach here wrong on instrumental grounds alone?</p>\n", "parentCommentId": null, "user": {"username": "Jason"}}, {"_id": "egyJp24he26HnFpFn", "postedAt": "2022-12-30T20:03:51.368Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>Thanks for your comment!</p>\n<p>In terms of things EAs actually believe, I think EAs overestimate the contribution of cognitive biases relating to distance, and underestimate the contributions of nationalism, localism and racism, to charity priorities in rich countries.</p>\n<p>Luckily, I don\u2019t think my disagreement with most EAs here is super action-relevant, other than that I think EAs who are interested in promoting broad social values should consider promoting internationalism.</p>\n<p>In terms of strategy, I agree that it mostly makes sense to emphasise factors that will offend potential donors less, such as cognitive biases relating to distance (and maybe it is this strategy that causes EAs to overestimate the importance of this factor compared to other factors).</p>\n<p>Although I think when pitching effective giving to people we know are left wing or progressives, it might be more effective to emphasise the nationalism and racism elements, since I expect left-wingers to be keen to position themselves against these ideologies.</p>\n", "parentCommentId": "vur3vnL5nbbj8hhBk", "user": {"username": "freedomandutility"}}, {"_id": "XDQNhaZAmKo7Q9E5F", "postedAt": "2022-12-30T20:25:53.121Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>Thanks for your comment!</p>\n<ol>\n<li></li>\n</ol>\n<p>By transparency, I mean publishing explanations behind important decisions much more regularly and quickly to the EA Forum. This is mostly relevant for grantmakers and grantmaking organisations and isn\u2019t super relevant for your role.</p>\n<p>But for example, if you made a decision behind a big change to the karma system on the EA Forum, I would like you to publish an explanation behind your decision for the sake of transparency.</p>\n<ol start=\"2\">\n<li></li>\n</ol>\n<p>Agree that this would be better but as you say it is obviously very time consuming. I (ironically) don\u2019t really have capacity soon to do this, but would encourage others to have a go at some BOTECs related to this post.</p>\n<ol start=\"3\">\n<li></li>\n</ol>\n<p>I\u2019m not aware of any examples of outright corruption in EA.</p>\n<p>I think an example of the kind of decision for which reasoning should be published on the EA Forum is when 80 000 hours starts listing multiple jobs in a new organisation on its job board. Doing this for OpenAI might have led to earlier scrutiny.</p>\n<p>Another example might be the Wytham Abbey purchase but I\u2019m not sure how much time had passed between the purchase and the discussion on this forum.</p>\n<p>I think a great example of transparency was this post (<a href=\"https://forum.effectivealtruism.org/posts/4JF39v548SETuMewp/?commentId=R2Axqfvbyq89fSRYQ\">https://forum.effectivealtruism.org/posts/4JF39v548SETuMewp/?commentId=R2Axqfvbyq89fSRYQ</a>) from the EAG organisers explaining why they\u2019re making a set of changes to EAG, allowing scrutiny from the EA community.</p>\n<p>(This meta-analysis (<a href=\"https://journals.sagepub.com/doi/full/10.1177/00208523211033236\">https://journals.sagepub.com/doi/full/10.1177/00208523211033236</a>) suggests that transparency has a small effect on government corruption, but I would not put too much weight on the results since effects seem to be context specific and I\u2019m not sure how much we can extrapolate from governments to a network of organisations. )</p>\n", "parentCommentId": "ggJrpE5YiXTe4QkGW", "user": {"username": "freedomandutility"}}, {"_id": "ao3p6jPtNFKrDJozf", "postedAt": "2022-12-31T01:33:12.329Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>I would love to see more EA's succeed as entrepreneurs so that we're less reliant on OpenPhilanthropy, not only so that we have more money, but also to balance out OpenPhilanthropy's influence.</p><blockquote><p>I would recommend that individuals are only allowed to hold leadership, board or governance positions in one EA organisation each.</p></blockquote><p>I would heavily bet on this leading to worse governance as it would mean you couldn't recruit someone who was demonstrating their competence by leading an EA organisation well to your board. And having such a person on your board could be of great assistance, especially for a new org.</p>", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "bKcptn9dJvheveKTv", "postedAt": "2022-12-31T02:02:35.856Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>While I don't think it would be that difficult to write up a BOTEC on the costs side (e.g., here are some ways EA Funds could be more transparent and I estimate the cost of the package as $50K over five years), quantifying benefits for this kind of thing seems awfully difficult. For instance, I could point to some posts on the forum as evidence that some people are bothered by what they perceive as inadequate transparency, and might be reasonably expected to donate less, not apply / get disillusioned, etc. My sense is that is true of quite a bit in the meta space, and am not sure it is reasonable to expect transparency spend to quantify in a clean manner if similar spends aren't held to the same standard.&nbsp;</p>", "parentCommentId": "ggJrpE5YiXTe4QkGW", "user": {"username": "Jason"}}, {"_id": "QwohystjEg8ySp83y", "postedAt": "2022-12-31T04:22:13.214Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>One specific thing I appreciate about GiveWell is the policy that no one donor can fund more than 20% of their operating expenses. I think there is a <i>particular </i>need for certain work to be broadly funded; generally, that work is at places like GiveWell, RP, ACE, etc. and will have an significant influence on what else gets funded / gets done.</p><p>There's probably a happy medium between a hard one-organization limit and &nbsp;having no rules on multiple board service / board overlap.</p>", "parentCommentId": "ao3p6jPtNFKrDJozf", "user": {"username": "Jason"}}, {"_id": "MqjErBCuQzBCcqT8H", "postedAt": "2022-12-31T04:29:27.240Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<blockquote><p>we have less money</p></blockquote><p>I think you mean \"more money\"</p>", "parentCommentId": "ao3p6jPtNFKrDJozf", "user": {"username": "evelynciara"}}, {"_id": "eo9aZbXbaTcbCknib", "postedAt": "2022-12-31T05:16:33.158Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>Thanks, corrected. Reasons why I shouldn't multitask.</p>", "parentCommentId": "MqjErBCuQzBCcqT8H", "user": {"username": "casebash"}}, {"_id": "Hnm2kTnG9JN2gtwk8", "postedAt": "2022-12-31T05:18:51.143Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>I'm actually not convinced that we need a policy here, especially since people have very limited time and I suspect if someone is on like 12 different boards then they will have very little influence at each org because they're spreading their time so thin. But I don\u2019t have board experience, so I could be wrong here.</p>", "parentCommentId": "QwohystjEg8ySp83y", "user": {"username": "casebash"}}, {"_id": "Gq7BDs58qiimgDFvC", "postedAt": "2022-12-31T10:17:27.337Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>Thanks for your comment!</p>\n<p>I would have thought that board experience at a non-EA org would be very similar to board experience at an EA org, but interested to hear why this may not be the case.</p>\n", "parentCommentId": "ao3p6jPtNFKrDJozf", "user": {"username": "freedomandutility"}}, {"_id": "GCqsbuFvbvGWyx7nx", "postedAt": "2022-12-31T10:22:40.447Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>Asking individuals to quantify such benefits seems like a de facto way of not actually considering them - individuals very rarely have time to do a thorough job, and any work they publish will be inevitably speculative, and easy enough to criticise on the margins that orgs that don't want to change their behaviour will be able to find a reason not to.</p><p>Since EA orgs' lack of transparency is a widespread concern among EAs, it seems a reasonable use of resources for EA orgs that don't think it's worth it to produce a one-off (or perhaps once-every-n-years) report giving their <i>own</i> reasons as to why it isn't. Then the community as a whole can discuss the report, and if the sentiment is broadly positive the org can confidently go on as they are, and if there's a lot of pushback on it, a) the org might choose to listen &nbsp;and change its practices and b) if they don't, it will at least be more evident that they've explicitly chosen not to heed the community's views, which I'd hope would guide them towards more caution, and gradually separate the visionaries from the motivated reasoners.</p>", "parentCommentId": "ggJrpE5YiXTe4QkGW", "user": {"username": "Arepo"}}, {"_id": "8dLqaHogXMECAMEfN", "postedAt": "2022-12-31T11:43:48.655Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<blockquote><p>Another potential intervention could be to split up existing organisations into more organisations. I can't think of an organisation where this would be obviously suitable so am not advocating for this happening right now, but I think it would make sense for organisations to split as they grow further in the future.</p></blockquote><p>I explicitly argued for this <a href=\"https://forum.effectivealtruism.org/posts/J3pZ7fY6yvypvJrJE/should-large-ea-nonprofits-consider-splitting\">here</a>, albeit mainly on the grounds that it gives better fidelity of feedback, thus allowing for better competition. My current views, tentatively:</p><ul><li>CEA has substantially too many (and too unrelated) responsibilities.&nbsp;</li><li>I'm less clear about 80k and Founders Pledge, who have the next widest remits, but who each have a clearer unifying theme running through their projects.&nbsp;</li><li>I'm also very worried about 5 trustees having <a href=\"https://forum.effectivealtruism.org/posts/xaMLvzbaBMFjX88Z2/cea-disambiguation?commentId=nkvTzhHmyR4c32Cat\">theoretical authority</a> over all the orgs within <a href=\"https://ev.org/\">effective ventures</a>, which collectively constitute basically the whole 'movement' part of 'the EA movement'. There's a case that the legal upside of this is so high that it outweighs the risks, which I discussed in a thread starting <a href=\"https://forum.effectivealtruism.org/posts/o6LNeNoHBA7Bv9kGE/bad-omens-in-current-ea-governance?commentId=qjKZgjJZ6d2mghgKe\">here</a> with Peter Wildeford, but after his most recent response as well as Brendan_Wong's comment, I still have the impression that this case is not very robust, and that healthier alternative paths exist.</li></ul><p>In each of the above discussions &nbsp;I've generally been quite downvoted, so perhaps I should take that as evidence that my views are wrong - but in the subsequent discussions I've never felt like the points I raised had been adequately resolved.</p>", "parentCommentId": null, "user": {"username": "Arepo"}}, {"_id": "FMKRkzjHqnSvRTzy6", "postedAt": "2022-12-31T11:51:15.502Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>I'd be interested to hear your response to my most recent questions about <a href=\"https://forum.effectivealtruism.org/posts/o6LNeNoHBA7Bv9kGE/bad-omens-in-current-ea-governance?commentId=tfgoviHqCudkGmSd7\">alternatives</a> here, as well as Brendon's response.</p><p>My current sense from that thread is still that having to set up an org without fiscal sponsorship is a cost that could be substantially reduced by being provided as a service, and that unifying the organisations has large and subtle costs that aren't being sufficiently acknowledged.</p>", "parentCommentId": "LGitmWH9v773LZbre", "user": {"username": "Arepo"}}, {"_id": "Q3ootSv9C8n8FyiqN", "postedAt": "2022-12-31T11:55:06.717Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>We consider CEOs of large companies as being capable of steering the whole company for better or worse, and they can &nbsp;have far more staff and decision-making requirements than the whole EA nonprofit world combined.</p>", "parentCommentId": "Hnm2kTnG9JN2gtwk8", "user": {"username": "Arepo"}}, {"_id": "44Bi6b6YrPSNGcr6g", "postedAt": "2022-12-31T12:29:51.181Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>Some model of <a href=\"https://en.m.wikipedia.org/wiki/Liquid_democracy\">liquid democracy</a> could help with that kind of thing. In short, it allows voters to either cast their vote themselves or delegate it to others (who can delegate their pool further, and so on).</p>\n", "parentCommentId": "bkHAnhFFv6Qw5QyPs", "user": {"username": "Guy Raveh"}}, {"_id": "fjz8PWHYRujAzRTD2", "postedAt": "2022-12-31T12:46:53.984Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>Strongly upvoted, but I specifically disagree with the suggestions on which group of people should be able to vote on things:</p>\n<ol>\n<li>\n<p>\"High karma users\" selects, at minimum, for people with lots of time to spend on the internet. This means it will be more accessible to e.g. rich people. It probably also selects for people who agree with most of the popular opinions in EA (although I might be a counterexample), which goes against diversity of thought.</p>\n</li>\n<li>\n<p>\"People admitted to EAG\" lets the funders choose the people who'll vote about their decisions.</p>\n</li>\n</ol>\n<p>In line with what (I think) happens in EA Germany and EA Czech Republic, I'd propose a much simpler criterion - membership in an organisation which requires a small yearly fee.</p>\n", "parentCommentId": null, "user": {"username": "Guy Raveh"}}, {"_id": "FDru2DHjRGqMPfhqL", "postedAt": "2022-12-31T13:40:21.781Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>Interesting post! Broadly I agree on most of the stuff in the meta-section, which I think has been under-researched and under-explained by the orgs in question, and disagree on the interventions, which I think have been extremely well researched and explained.&nbsp;</p><blockquote><p>But importantly, the function of transparency is primarily&nbsp;<strong>as a long-term&nbsp;</strong><i><strong>safeguard</strong></i><strong> and&nbsp;</strong><i><strong>disincentive&nbsp;</strong></i><strong>against these things</strong>. *Detecting* poor reasoning, bias and corruption is only a secondary function of transparency.&nbsp;</p></blockquote><p>I think this is a really important point, which seems to have been overlooked by many of the responses to recent lack-of-transparency criticisms (cf eg Owen's explanation of <a href=\"https://forum.effectivealtruism.org/posts/xof7iFB3uh8Kc53bG/why-did-cea-buy-wytham-abbey?commentId=u3yJfbm2pes8TFpYX\">Wytham Abbey</a>, which says that they didn't tell anyone because 'I'm not a fan of trying to create hype' and 'it felt a bit gauche', which sounds like they gave basically no thought to the precedents and incentives not announcing it was establishing).</p><blockquote><p>EA grant making decisions are too technocratic</p></blockquote><p>Inasmuch as this is a problem, ironically, it seems like FTX Foundation were the organisation doing the most to redress this via their <a href=\"https://forum.effectivealtruism.org/posts/CQQtKkMGeGLxbgLjP/the-future-fund-s-regranting-program\">regranting</a> program.</p><p>That said, I think a focus on technocracy per se is misguided. As I understand it, Tuna and Moskovitz have completely relinquished control <a href=\"https://www.facebook.com/groups/OMfCT/posts/3331770010471227/\">and sometimes knowledge</a> of the money they donated. If there is a concern to be had with OP and formerly FTX, it's that the people they relinquished control to are a small number of closely networked individuals who a) tend to be involved with multiple EA orgs and b) tend not to worry about conflict of interest (eg as I understand it Nick Beckstead has simultaneously been a trustee for Effective Ventures and a fund manager for OP directing large amounts of money to EV subsidiaries).</p><blockquote><p>restricted to users with a certain amount of karma</p></blockquote><p>I would like to see a more transparent alternative to EA funds, but karma is a <i>really</i> bad proxy for contribution value - the highest karma people are, almost inevitably, those with the biggest network of other high-karma users to strong-upvote them. Plus newer posts get <a href=\"https://forum.effectivealtruism.org/posts/JECLcLTCYF4JA7rxM/sort-forum-posts-by-occlumency-old-and-upvoted\">far more upvoted</a> than older posts, both because of more users and general karma inflation.&nbsp;</p><p>Perhaps a more substantial issue with the alternatives you propose is that, assuming the money would go to organisations, it would be very difficult for them to work with such uncertain income sources. Small orgs benefit greatly from a clear conversation about what their funders' expectations of them are, and what milestones would be necessary/sufficient to secure them funding. Without such predictability, it's very difficult for them to hire staff, which often (especially for meta-orgs) constitutes a majority of their expenses.&nbsp;</p><p>Worth mentioning also that EA funds are a tiny pool of money relative to OpenPhil and (at least on paper) the Founders Pledge commitments.&nbsp;</p><blockquote><p>EAs underestimate the tractability of party politics... EAs underestimate the expected value of advocacy, campaigning and protest</p></blockquote><p>I'm not sure either of these claims are true. 80k have a longstanding problem profile on the value of being a civil servant, which has been extremely influential in the UK at least (I'm not sure why there specifically), to the extent that it's been one of the most popular career paths for UK-based EAs. And many reports have strongly recommended giving to advocacy organisations (off the top of my head, ACE advocate giving to the Humane League campaigns, FP advocate Clean Air Task Force and formerly Coalition for Rainforest Nations).</p><p>That said, I still think <a href=\"https://slatestarcodex.com/2015/09/22/beware-systemic-change/\">Scott's warning</a> about systemic change is a strong argument for caution.</p>", "parentCommentId": null, "user": {"username": "Arepo"}}, {"_id": "mRKmANuFLqTkxFd9W", "postedAt": "2022-12-31T16:26:31.457Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>If someone is on so many boards that they have minimal influence at each, that is an independent reason to limit their service and ask someone else to serve. I'm really impressed, for instance, by RP's open call for board member applications.</p>\n<p>I'm more concerned about someone being on the board / in leadership of 3-4 particularly important organizations than in 12.</p>\n<p>To be fair to OpenPhil, it's common to have a major donor in a board seat as a means of providing transparency/accountability to that donor...</p>\n", "parentCommentId": "Hnm2kTnG9JN2gtwk8", "user": {"username": "Jason"}}, {"_id": "XuXiTJzu7c4Gne7q9", "postedAt": "2022-12-31T17:06:35.217Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>There are also potentially other options to manage the five-trustee problem.&nbsp;</p><p>If EVF is basically supposed to be a fiscal sponsor-like entity that makes the trains run on time and supports (rather than dictate to) its constituent organizations, it's not clear why people like Will MacAskill need to be on the board at all (or why it is the highest and best use of their time). The problem could be mitigated by expanding the board to seven, nine, or 11 members and by choosing people who do not have loads of \"soft power\" in the community for most of the seats.</p><p>I am not sure about UK law, but on the US end, you can have a nonprofit corporation whose board is elected by members (and you can define members however you want, it doesn't have to be open enrollment). Those members could even be other EA organizations if desired. &nbsp;So if the costs of splitting up EVF were thought too high, adding a layer of members (maybe 50-100?) whose sole purpose would basically be to re-elect (or remove) board members as necessary would at least provide some protection against the concentration of control.</p>", "parentCommentId": "8dLqaHogXMECAMEfN", "user": {"username": "Jason"}}, {"_id": "4SWSFLaA9MJohDQ3r", "postedAt": "2022-12-31T18:39:03.080Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>Another option would be a one-time or periodic \"EA Governance and Transparency Red Teaming Contest\" with volunteer judges who were not affiliated with the large meta organizations. I do <i>not</i> think a six-figure prize fund would be necessary; to be honest, a major purpose of there being a prize fund for this contest would be to credibly signal to would-be writers that the organizations are seriously interested in ideas about improving governance and transparency.&nbsp;</p><p>To build off of what you said, it's really hard for people to feel motivated to do even a moderately thorough job on a proposal or a cost-effectiveness analysis without a credible signal that there is a sufficient likelihood that the organization(s) in question will actually be responsive to a proposal/analysis. Right now, it would feel like sending an unsolicited grant proposal to an organization that doesn't list your cause area as one of its interests and has not historically funded in that area. At least in that example, the author potentially stands to gain from a grant acceptance, while the author of a governance/transparency proposal benefits no more than any other member of the community.</p>", "parentCommentId": "GCqsbuFvbvGWyx7nx", "user": {"username": "Jason"}}, {"_id": "wg4M8aPYPXNanjXWM", "postedAt": "2022-12-31T18:47:04.909Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>I mean, I don't even know what the claim is that I'm supposed to produce a report giving my own reasons for. I guess <a href=\"https://forum.effectivealtruism.org/posts/SBSC8ZiTNwTM8Azue/a-libertarian-socialist-s-view-on-how-ea-can-improve?commentId=XDQNhaZAmKo7Q9E5F#FMKRkzjHqnSvRTzy6\">the answer is</a> \"nothing.\"</p><p>(Which obviously is fine! Not all forum posts need to be targeted at getting me to change my behavior. In fact, almost none are. But I thought I might have been in the target audience, so hence I wrote the comment.)</p>", "parentCommentId": "GCqsbuFvbvGWyx7nx", "user": {"username": "Ben_West"}}, {"_id": "XyzMWfQEN3kz3gJoh", "postedAt": "2022-12-31T20:11:46.583Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>I think the suggestion is something like this (I am elaborating a bit)-- certain organizations should consider producing a report that explains:</p><p>(1) How their organization displays good governance, accountability, and transparency (\"GAT\");</p><p>(2) Why the organization believes its current level of GAT is sufficient under the circumstances; and possibly</p><p>(3) Why the organization believes that future improvements in GAT that might be considered would not be cost-effective / prudent / advisible.</p><p>Of course, if the organization thought it should improve its GAT, it could say that instead.</p><p>(3) would probably need a crowdsourced list of ideas and a poll on which ones the community was most interested in.</p>", "parentCommentId": "wg4M8aPYPXNanjXWM", "user": {"username": "Jason"}}, {"_id": "CLDDgQtQgrEx2ieW3", "postedAt": "2023-01-02T08:03:51.412Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>Your suggestion of having multiple individuals or groups independently calculate the expected value of an intervention is an interesting one. It could increase objectivity and reduce the influence of motivated reasoning or self-serving biases and help us end up with not only better judgments but several times more research &amp; considerations.&nbsp;</p><p>Do you know of any EA organizations that are considering it or any prior debate about this idea in the forum?</p><p>It would be interesting to see if this method would lead to more accurate expected value calculations in practice. Additionally, I am curious about how the process of comparing results and coming to a consensus would be handled in this approach.</p>", "parentCommentId": null, "user": {"username": "Michael Simm"}}, {"_id": "kqDEA86Qc2haqvfjX", "postedAt": "2023-01-02T23:36:24.227Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>You mention a few times that EV calculations are susceptible to motivated reasoning. But this conflicts with my understanding, which is that EV calculations are useful partly (largely) because they help to prevent motivated reasoning from guiding our decisions too heavily&nbsp;</p><p>(e.g. You can imagine a situation where charity Y performs an intervention that is more cost effective than charity X. By following on EV calculation, one might switch their donation from charity X to charity Y, despite that charity X sounds intuitively better.)</p><p>Maybe you could include some examples/citations of where you think this \"EV motivated reasoning\" has occurred. Otherwise I find it hard to believe that EV calculations are worse than the alternative, from a \"susceptible-to-motivated-reasoning\" perspective (here, the alternative is not using EV calculations).</p>", "parentCommentId": null, "user": {"username": "harrygietz@gmail.com"}}, {"_id": "5f733KAtZftAGy9ya", "postedAt": "2023-01-03T00:37:01.926Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>Here are some places where motivated reasoning can come in. It's past 2am here so I'll only give an example to some.</p>\n<ol>\n<li>\n<p>In which interventions you choose to compare or to ignore, or which aspects you choose to include in your assessment.</p>\n</li>\n<li>\n<p>In how you estimate the consequences of your choices. Often, EV calculations in EA rely on guesses or deference to prediction markets (or even play-money ones like Metaculus). These all have as strong biases as you'd find anywhere else. As an explicit example, some Longtermists like Bostrom rely on figures for how many people may live in the future (10^(a lot), allegedly) and these figures are almost purely fictional.</p>\n</li>\n<li>\n<p>In how you choose to apply EV-maximisation reasoning in situations where it's unclear if that's the right thing to do. For example, if you're not entirely risk-neutral, it only makes sense to try to maximise the expected value of decisions if you know there is a large number of independent ones. But this is not what we do:\na. We rank charities in ways that make donation decisions highly correlated with each other.\nb. We treat sequential decisions as if they were independent even when that's not true.\nc. We use EV reasoning on big one-off decisions (like double-or-nothing experiments).</p>\n</li>\n</ol>\n", "parentCommentId": "kqDEA86Qc2haqvfjX", "user": {"username": "Guy Raveh"}}, {"_id": "wmoZS7zjq2c6nFBBk", "postedAt": "2023-01-04T10:01:59.446Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>Thanks for your comment.</p><p>I don't think EV calculations directly guard against motivated reasoning.</p><p>I think the main benefit of EV calculations is that they allow more precise comparison between interventions (compared to say, just calling many interventions 'good').</p><p>However, many EV calculations involve probabilities and estimates derived from belief rather than from empirical evidence. These probabilities and estimates are highly prone to motivated reasoning and cognitive biases.&nbsp;</p><p>For example, if I was to calculate the EV of an EA funding org investing in more transparency, I might need to estimate a percentage of grants which were approved but ideally should not have been. As someone who has a strong prior in favour of transparency, I might estimate this to be much higher than someone who has a strong prior against transparency. This could have a large effect on my calculated EV.&nbsp;</p><p>That being said, there are certainly EV calculations where all the inputs can be pegged to empirical evidence, especially in the cause area of international development and global health. These EV calculations are <i>less</i> prone to motivated reasoning, but motivated reasoning remains nonetheless, because where there is empirical evidence available from different sources, motivated reasoning may affect the source used. (Guy Raveh points out some other ways that motivated reasoning can affect these calculations too)</p><p>With sufficient transparency, I think EV calculations <i>can</i> help reduce motivated reasoning since people can debate the inputs into the EV calculation, allowing the probabilities and estimates derived from belief to be refined, which may make them more accurate than before.</p><p>I agree that EV calculations are <i>less</i> susceptible to motivated reasoning than alternative approaches, but I think they are very susceptible nonetheless, which is why I think we should make certain changes to how they are used and implement stronger safeguards against motivated reasoning.</p>", "parentCommentId": "kqDEA86Qc2haqvfjX", "user": {"username": "freedomandutility"}}, {"_id": "9uF4xccuWx3trtdrK", "postedAt": "2023-01-05T15:54:08.632Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p><a href=\"https://decidim.org/\">https://decidim.org/</a> seems like a natural fit for what the OP and you are suggesting. The app and the org can help in more participation when it comes to projects, more transparency when it comes to implementation and funding and you can gate certain features trough a membership fee as well.</p>\n", "parentCommentId": "fjz8PWHYRujAzRTD2", "user": {"username": "isle9"}}, {"_id": "gF7NJ7AbbfxHWjqq9", "postedAt": "2023-01-11T09:16:54.142Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>Thanks for writing this!</p><blockquote><p>EA relies on highly-uncertain, imprecise, vulnerable-to-motivated-reasoning expected value (EV) calculations. These calculations often use probabilities derived from belief which aren\u2019t based on empirical evidence.</p></blockquote><p>Having now done a few explicit cost-effectiveness analyses myself, I can see how this point is quite important. It is easy to underestimate the uncertainty of inputs which are not much based on empirical evidence. However:</p><ul><li>I think it motivates (even) more efforts to assess the effect of interventions, not moving away from EV calculations.&nbsp;</li><li>I would also say that it applies not only to explicit EV calculations, but also (or even more) to other tools/mechanism used in decision-making.&nbsp;</li></ul>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "4YTCqANiDc86nZF7z", "postedAt": "2023-01-11T11:06:44.892Z", "postId": "SBSC8ZiTNwTM8Azue", "htmlBody": "<p>Agree, I don\u2019t advocate for moving away from EV calculations, just for improving the way we use them!</p>\n", "parentCommentId": "gF7NJ7AbbfxHWjqq9", "user": {"username": "freedomandutility"}}]