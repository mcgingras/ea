[{"_id": "LR9y5xc3HZWpsKGyr", "postedAt": "2023-03-27T17:53:31.721Z", "postId": "MP9qDZCXMaTJhiJ9u", "htmlBody": "<p>I strongly agree with this, but worry that protecting those three accidentally sneaks in a lot of baggage. As I wrote <a href=\"https://forum.effectivealtruism.org/s/Ytf8JFpkWGFkHHjaq\">at length</a>, I think there are a lot of different pieces that are easily conflated, and I'm concerned that saying yes <i>to the movement</i> without being clear on which parts you disagree with is going to lead to bad epistemic practice.&nbsp;<br><br>Given that, I think it's especially valuable to say what you disagree with EA consensus about, or which things you're not willing to keep as key values even if you think they are OK, as a way to keep a clearer scout mindset. (This is, of course, socially much harder, but it's also part of what keeps <a href=\"https://www.lesswrong.com/posts/yEjaj7PWacno5EvWa/every-cause-wants-to-be-a-cult\">a cause from becoming a cult</a>.)</p>", "parentCommentId": null, "user": {"username": "Davidmanheim"}}, {"_id": "u8wCzzPS6t2AA7Duc", "postedAt": "2023-03-27T19:26:01.076Z", "postId": "MP9qDZCXMaTJhiJ9u", "htmlBody": "<p>Let me make a case that we should call it Radical Compassion instead of Radical Empathy. This is a very minor point of course, but then again, people have endlessly debated whether Effective Altruism is a sub-optimal label and what a better label would be. People clearly care about what important things are called (and maybe rightly so from a linguistic precision and marketing perspective).</p><p>You probably know this literature, but there's a <a href=\"https://psycnet.apa.org/record/2009-02253-001\">lot of confusion</a> around what empathy should be defined as. Sometimes, empathy refers to various perspective-taking processes, like feeling what another feels (let's call it Empathy 1). I think this is the most common lay definition. Sometimes, it refers to valuing others' welfare, also referred to as empathic concern or compassion (let's call it Empathy 2). Sometimes, definitions reference both processes (let's call it Empathy 3), which doesn't seem like the most helpful strategy to me.</p><p>Holden briefly points to the debate in his <a href=\"https://forum.effectivealtruism.org/posts/ehZK259et52Xnvw5F/radical-empathy\">post</a> which you link to, but it's not clear to me why he chose the empathy term despite this confusion and disagreement. In one place, he seems to endorse Empathy 3, but in another, he separates empathy from moral concern, which is inconsistent with Empathy 3.</p><p>I think most EA's want people to care about the welfare of others. It doesn't matter if people imagine what it feels like being a chicken that is pecked to death in a factory farm (that's going to be near-impossible), or if they imagine how they would feel in factory farm conditions (again, very difficult to imagine). We just want them to care about the chicken's welfare. We therefore want to promote Empathy 2, not 1 or 3. Given the confusion around the empathy term, it seems better to stick with compassion. Lay definitions of compassion also align with the \"just care about their welfare\" view.</p>", "parentCommentId": null, "user": {"username": "bxjaeger"}}, {"_id": "xqoQ67Z98bDG4AC9Y", "postedAt": "2023-03-27T19:59:49.618Z", "postId": "MP9qDZCXMaTJhiJ9u", "htmlBody": "<p>Peter - excellent short piece; I agree with all of it.</p><p>The three themes you mentioned -- radical empathy, scope-sensitivity, scout mindset -- are really the three key takeaways that I try to get my students to learn about in my undergrad classes on EA. Even if they don't remember any about the details of global public health, AI X-risk, or factory farming, I hope they remember those principles.</p>", "parentCommentId": null, "user": {"username": "geoffreymiller"}}, {"_id": "vZwhAkJ4enaSk57Hy", "postedAt": "2023-03-27T20:02:20.507Z", "postId": "MP9qDZCXMaTJhiJ9u", "htmlBody": "<p>bxjaeger -- fair point. It's worth emphasizing Paul Bloom's <a href=\"https://www.vox.com/conversations/2017/1/19/14266230/empathy-morality-ethics-psychology-compassion-paul-bloom\">distinction</a> between rational compassion and emotional empathy, and the superiority of the former when thinking about evidence-based policies and interventions.&nbsp;</p>", "parentCommentId": "u8wCzzPS6t2AA7Duc", "user": {"username": "geoffreymiller"}}, {"_id": "zkcknPWfyouhvskSR", "postedAt": "2023-03-27T20:17:09.482Z", "postId": "MP9qDZCXMaTJhiJ9u", "htmlBody": "<p>Agreed - I think Paul Bloom's distinction makes a lot of sense. Many prominent empathy researchers have pushed back on this, mostly to argue for the Empathy 3 definition that I listed, but I don't see any benefit in conflating these very different processes under one umbrella term.</p>", "parentCommentId": "vZwhAkJ4enaSk57Hy", "user": {"username": "bxjaeger"}}, {"_id": "tqAhose5zPgEhzSdZ", "postedAt": "2023-03-27T21:20:04.390Z", "postId": "MP9qDZCXMaTJhiJ9u", "htmlBody": "<p>Yep -- I think Paul Bloom makes an important point in arguing that 'Empathy 2' (or 'rational compassion') is more consistent with EA-style scope-sensitivity, and less likely to lead to 'compassion fatigue', compared to 'Empathy 1' (feeling another's suffering as if it's one's own).</p>", "parentCommentId": "zkcknPWfyouhvskSR", "user": {"username": "geoffreymiller"}}, {"_id": "Ri6TxQctsfZwQMeYM", "postedAt": "2023-03-29T02:21:20.048Z", "postId": "MP9qDZCXMaTJhiJ9u", "htmlBody": "<p>Thank you, Peter. These are the things that initially attracted me to effective altruism and I appreciate you articulating them so effectively. I will also say that these are ideas I admire you for obviously fostering, both through rethink priorities and your forecasting work.</p>\n<p>Unfortunately it seems to me that the first and third ideas are far less prominent of a feature of EA than they used to be.</p>\n<p>The first idea seems to me to be less prominent as a result of so many people believing in extremely high short term catastrophic ai risk. It seems that this has encouraged an attitude of animal welfare being trivial by comparison and the welfare of humans in the far future being irrelevant (because if we don't solve it, humans will go extinct within decades). Attitudes about animal welfare seem in my opinion to be compounded by the increasing influence of Eliezer, who does not believe that non human animals (with the possible exception of chimps) are sentient.</p>\n<p>The third idea also seems to be declining as a result of hard feelings related to internal culture warring. In my view, bickering about the integrity of various prominent figures, about the appropriate reaction to sbf, about whose fault sbf was, about how prevalent sexual assault is in EA, about how to respond to sexual assault in EA, about whether those responses are cultish or at least bigoted, etc etc etc has just made the general epistemics a lot worse. I see these internal culture wars bleeding into cause areas and other ostensibly unrelated topics. People are frustrated with the community and regardless of whatever side of these culture wars they are on, they are annoyed about the existence of the other side and frustrated that these seemingly fundamental issues of common decency are even a discussion. It puts them in no mood to discuss malaria vaccines with curiosity.</p>\n<p>I personally deactivated my real-name forum account and stopped participating in the in person community and talking to people about ea. I still really really value these three ideas and see pockets of the community that still embody them. I really hope the community once again embodies them like I think they used to.</p>\n", "parentCommentId": null, "user": {"username": "justsaying"}}, {"_id": "GnRQcsrbyQ6aAKgq4", "postedAt": "2023-03-29T09:36:43.059Z", "postId": "MP9qDZCXMaTJhiJ9u", "htmlBody": "<p>I will probably use this framing to communicate EA from now on</p>", "parentCommentId": null, "user": {"username": "ElliotJDavies"}}, {"_id": "yQspAn3pxbLiHxTgX", "postedAt": "2023-03-29T21:24:46.454Z", "postId": "MP9qDZCXMaTJhiJ9u", "htmlBody": "<p>Great piece. Short and sweet.&nbsp;</p><p>Given the stratospheric karma this post has reached, and the ensuing likelihood it becomes a referenced classic, I thought it'd be a good time to descend to some pedantry.&nbsp;</p><p>\"<strong>Scope sensitivity\" </strong>as a phrase doesn't click with me. For some reason, it bounces off my brain. Please let me know if I seem alone in this regard. What scope are we sensitive to? The scope of impact? Also some of the related slogans \"shut up and multiply\" and \"cause neutral\" aren't much clearer. \"Shut up and multiply\" which seems slightly offputting / crass as a phrase stripped of context, gives no hint at what we're multiplying<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkubrw7iir8j\"><sup><a href=\"#fnkubrw7iir8j\">[1]</a></sup></span>. \"Cause neutral\" without elaboration, seems objectionable. We shouldn't be neutral about causes! We should prefer the ones that do the most good! They both require extra context and elaboration. If this is something that is used to introduce EA, which now seems likelier, I think this section confuses a bit. A good slogan should have a clear, and difficult to misinterpret meaning that requires little elaboration. \"Radical compassion / empathy\" does a good job of this. \"Scout mindset\" is slightly more in-groupy, but I don't think newbies would be surprised that thinking like a scout involves careful exploration of ideas and emphasizes the importance of reporting the truth of what you find.&nbsp;</p><p>Some alternatives to \"scope sensitivity\" are:&nbsp;</p><ul><li>\"Follow the numbers\" / \"crunch the numbers\": we don't quite primarily \"follow the data / evidence\" anymore, but we certainly try to follow the numbers.&nbsp;</li><li>\"More is better\" / \"More-imization\" okay, this is a bit silly, but I assume that Peter was intentionally avoiding saying something like \"Maximization mindset\" which is more intuitive than \"scope sensitivity\", but has probably fallen a bit out of vogue. We think that doing more good for the same cost is always better.</li><li>\"Cost-effectiveness guided\" while it sounds technocratic, that's kind of the point. Ultimately it all comes back to cost-effectiveness. Why not say so?&nbsp;</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkubrw7iir8j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkubrw7iir8j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>If I knew nothing else, I'd guess it's a suggestion of the profound implications of viewing probabilities as dependent (multiplicative) instead of dependent (addictive) and, consequently, support for complex systems approaches /GEM modelling instead of reductive OLSing with sparse interaction terms. /Joke</p></div></li></ol>", "parentCommentId": null, "user": {"username": "JoelMcGuire"}}, {"_id": "QoJiLGLEL5ra73h4t", "postedAt": "2023-03-30T15:51:12.892Z", "postId": "MP9qDZCXMaTJhiJ9u", "htmlBody": "<p>+1. Most people I speak to that 'only have heard of EA' explain EA as the idea that one should make more money to donate more (and more effectively). However, the principles described here are more encompassing.&nbsp;</p>", "parentCommentId": "GnRQcsrbyQ6aAKgq4", "user": {"username": "Robert_Praas"}}, {"_id": "McNhMPKEXG8Qx887Y", "postedAt": "2023-03-31T10:15:11.428Z", "postId": "MP9qDZCXMaTJhiJ9u", "htmlBody": "<p>Thanks for the piece. It brings me two contradicting emotions of warmth and unsettling sadness for \"excluding\" the rest of humanity. I don't think I need to explain the first one, but I want to explore the second.<br><br>I note two things which I can unite under the \"honest EA\" concept:<br>1) EA, as a concept, is a very human way of thinking. If you ask your average Joe if they want to do good, they'd likely say \"yes,\" and if you'd ask them if they'd like to do it effectively, they'd probably also say \"yes.\" &nbsp;So, I really believe that an honest version of EA is close to universal moral (*might be too universal, honestly, but that is a problem with the word \"good\")<br>2) All three features you point out, as well as point 1) above, are not binary. For each individual, there is a distribution of the range of empathy, &nbsp;there is scope sensitivity (it is just that it gets overridden by moral circle concerns), and surely there are environments and conditions in which almost any human can experience the scout mindset, just that few people bother to create those environments. Being effective in one's altruism is also the point on the distribution.&nbsp;<br><br>As you notice, we appreciate that it's ok to care more about our family and friends, and in those moments, we are not \"absolute EA,\" but we are very normal humans.&nbsp;<br><br>I believe that honestly appreciating the fact that \"we are just points on the distribution that, due to a privilege of economic, intellectual or emotional stability, are on the \"high\" end of the distribution\" can provide us with the humility to empathize with a fellow non-EA human being and recognize that the three features you've listed, are, in fact, everywhere. It is just that it takes much more than these three qualities to make a human.&nbsp;<br><br>I believe this recognition is essential for the future of the community and the psychological health of the citizens of this forum.&nbsp;<br>I want to talk to non-EA, not as to someone who doesn't share my values, but to someone who wasn't lucky enough to have a chance to make space for EA activities in their days, minds, and hearts but deep inside, we share the same <strong>honest EA idea </strong>\"we aim to do good effectively, while also doing those other mysterious things that make us into a wholesome human being.\"&nbsp;<br><br>*This might be the very same way you feel. But I still thought it was important to share.</p>", "parentCommentId": null, "user": {"username": "Ivan Madan"}}, {"_id": "vkazr7tbSp3auoYGK", "postedAt": "2023-04-02T03:30:56.984Z", "postId": "MP9qDZCXMaTJhiJ9u", "htmlBody": "<blockquote><p>one more subtle form of misaligned AI might be an AI that treats humans ok but adopts common human views on nonhuman animal welfare and perpetuates factory farming or abuse of a massive number of digital minds</p></blockquote><p>This is unrelated to the core messages of the post, but I think there's an important point to consider. A sufficiently intelligent system could improve cultured meat technology or invent other technological innovations for producing meat without factory farms.</p>", "parentCommentId": null, "user": {"username": "jakubkraus07@gmail.com"}}, {"_id": "Z2sZMKjJvgqkmCweR", "postedAt": "2023-04-05T20:35:25.783Z", "postId": "MP9qDZCXMaTJhiJ9u", "htmlBody": "<p>I don't think compassion is the right term descriptively for EA views, and it seems worse than empathy here. Compassion is (by the most common definitions, I think) a response to (ongoing) suffering (or misfortune).</p><p>Longtermism might not count as compassionate because it's more preventative than responsive, and the motivation to ensure future happy people come to exist probably isn't a matter of compassion, because it's not aimed at addressing suffering (or misfortune). But what Holden is referring to is meant to include those. I think what we're aiming for is counting all interests and anyone who has interests, as well as <a href=\"https://en.wikipedia.org/wiki/Equal_consideration_of_interests\">the equal consideration of interests</a>.</p><p>Of course, acts that are supported by longtermism or that ensure future happy people come to exist can be compassionate, but maybe not for longtermist reasons and probably not because they ensure future happy people exist, and instead because they <i>also</i> address suffering (or misfortune). And longtermists and those focused on ensuring future happy people come to exist can still be compassionate in general, but those motivations (or at least ensuring future happy people come to exist) don't seem to be compassionate, i.e. they're just not aimed at ongoing suffering in particular.</p>", "parentCommentId": "u8wCzzPS6t2AA7Duc", "user": {"username": "MichaelStJules"}}, {"_id": "CTqXDrq3kG4t8fzHT", "postedAt": "2023-06-06T11:19:17.992Z", "postId": "MP9qDZCXMaTJhiJ9u", "htmlBody": "<p>You're right that both empathy and compassion are typically used to describe what determines people's motivation to relieve someone's suffering. Neither perfectly captures preventive thinking or consideration of interests (beyond welfare and suffering) that characterize longtermist thinking. I think you are right that compassion doesn't lead you to want future people to exist. But I do think that it leads you to want future people to have positive lives. This point is harder to make for empathy. Compassion often means caring for others because we value their welfare, so it can be easily applied to animals or future people. Empathy means caring for others because we (in some way) feel what it's like to be them or in their position. It seems like this is more difficult when we talk about animals and future people.&nbsp;</p><p>I would argue that empathy, how it is typically described, is even more local and immediate, whereas compassion, again, how it is typically described, gets somewhat closer to the idea of putting weight on others' welfare (in a potentially fully calculated, unemotional way), which I think is closer to EA thinking. This is also in line with how Paul Bloom frames it: empathy is the more emotional route to caring about others, whereas compassion is the more reflective/rational route. So I agree that neither label captures the breadth of EA thinking and motivations, especially not when considering longtermism. I am not even arguing very strongly for compassion as the label we should go with. My argument more is that empathy seems to be a particualrly bad choice.</p>", "parentCommentId": "Z2sZMKjJvgqkmCweR", "user": {"username": "bxjaeger"}}, {"_id": "pKNZewcxZvexETk6g", "postedAt": "2023-06-08T10:48:42.248Z", "postId": "MP9qDZCXMaTJhiJ9u", "htmlBody": "<p>\"Scale Matters\" ?</p>\n", "parentCommentId": "yQspAn3pxbLiHxTgX", "user": {"username": "SiebeRozendal"}}, {"_id": "SfxacCZ7WvqtwcJcr", "postedAt": "2023-11-29T17:12:18.216Z", "postId": "MP9qDZCXMaTJhiJ9u", "htmlBody": "<blockquote><p>While these issues are serious, it\u2019s normal for social movements to go through crisis \u2013 what\u2019s more important is how we respond to that crisis.</p></blockquote><p>I like CEA's timely addition last summer of <a href=\"https://www.effectivealtruism.org/articles/introduction-to-effective-altruism#what-principles-unite-effective-altruism\">collaborative spirit</a> to the other three values you have here (which they called impartial altruism, prioritization, and open truthseeking).</p>", "parentCommentId": null, "user": {"username": "Holly"}}]