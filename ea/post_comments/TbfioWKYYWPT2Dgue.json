[{"_id": "dmAhiGgxMpXvARHTd", "postedAt": "2023-05-18T13:11:56.244Z", "postId": "TbfioWKYYWPT2Dgue", "htmlBody": "<p>I feel like I have a much better sense of what the current approaches to alignment are, what people are working on and how underdeveloped the field is. In general, it\u2019s been a while since I\u2019ve spent time studying anything so it felt fun just to dedicate time to learning. It also felt empowering to take a field that I\u2019ve heard a lot about at a high level and make it clearer in my mind.</p><p>I think doing the&nbsp;<a href=\"https://www.agisafetyfundamentals.com/ai-alignment-tabs/week-0\"><u>Week 0 readings</u></a> are an easy win for anyone who wants to demystify some of what is going on in ML systems, which I think should be interesting to anyone, even if you\u2019re not interested in alignment.</p><p>I became much more motivated to work on making AI go well over the period of the course, I think mainly because it made the problem more concrete but likely just spending more time thinking about it. That said, it\u2019s hard to disentangle this increased motivation from recent events and other factors.</p>", "parentCommentId": null, "user": {"username": "imben"}}, {"_id": "7QB5bdwph5Qr6DnDe", "postedAt": "2023-05-18T13:53:05.969Z", "postId": "TbfioWKYYWPT2Dgue", "htmlBody": "<ul><li>As a result of AGI SF readings and other sporadic AI safety readings\u2026&nbsp;<ul><li>\u2026 I feel more confident asking questions of people who know more than I do<ul><li>I feel like I know the vocabulary, main threat scenarios, and rough approaches to solving the problem, such that I can situate new facts into existing taxonomies&nbsp;</li></ul></li><li>\u2026 I\u2019m better able to tell when prominent people disagree / things have more texture</li></ul></li><li>Some (self-)critiques&nbsp;<ul><li>Honestly thought content for some of the weeks was a bit weak if you just wanted an overview of the alignment problem (e.g., adversarial techniques for scalable oversight probably isn\u2019t what you need to understand if you\u2019re trying to assess the risk)&nbsp;</li><li>I wish I\u2019d set up strong accountability</li><li>Wish it had more counter-arguments to classic AI risk.&nbsp;<ul><li>Apparently Stanford AI group modified curriculum to have more of these?&nbsp;</li></ul></li><li>Wish I\u2019d been more active in discussion or questions beforehand: give yourself a chance to be wrong!</li></ul></li><li>Tips I\u2019d recommend for learning more about AI risk<ul><li>Just start! Always feels daunting to dive into but just find a few explainer articles and dive in</li><li>Spaced repetition for learning really does go hard</li><li>Talk to knowledgeable people who give you space to be wrong</li><li>Write up&nbsp;your thoughts and have knowledgeable people poke holes/ show you where you\u2019re missing something&nbsp;</li></ul></li><li>A few resources that I keep coming back to (<i>not from AGI SF</i>):&nbsp;<ul><li>*&nbsp;<a href=\"https://www.safe.ai/ai-risk\"><u>Taxonomy of AI risks from CAIS</u></a></li><li>*&nbsp;<a href=\"https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to\"><u>Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover</u></a></li><li><a href=\"https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators\"><u>Simulators</u></a></li></ul></li></ul>", "parentCommentId": null, "user": {"username": "MJusten"}}, {"_id": "5pYnFY5b6pYkwbst6", "postedAt": "2023-05-19T06:33:32.468Z", "postId": "TbfioWKYYWPT2Dgue", "htmlBody": "<p>For anyone who is considering the course: TYPE III AUDIO is making audio narrations for the <a href=\"https://preview.type3.audio/playlists/agi-safety-fundamentals-alignment\">Alignment</a> and <a href=\"https://preview.type3.audio/playlists/agi-safety-fundamentals-governance\">Governance</a> courses. The series is due to launch later this month, but some 50+ episodes are already available.</p>\n", "parentCommentId": null, "user": {"username": "Peter_Hartree"}}, {"_id": "kqEbcMRD6xCR7sMZQ", "postedAt": "2023-05-19T14:06:27.861Z", "postId": "TbfioWKYYWPT2Dgue", "htmlBody": "<p>Thank you for making this thread Clifford, and we're really grateful for all feedback! We're working hard as a team to improve the course and the infrastructure we have for hosting other courses, and everyone's feedback has been incredibly valuable on our journey thus far :)&nbsp;</p>", "parentCommentId": null, "user": {"username": "dewierwan"}}, {"_id": "PrzaotfoyavgGjnow", "postedAt": "2023-05-19T17:40:28.444Z", "postId": "TbfioWKYYWPT2Dgue", "htmlBody": "<p>I think the value I got from the course is that my interactions with AI content are richer. It\u2019s like trying to understand an ecosystem after getting a better prescription for my glasses.</p>", "parentCommentId": null, "user": {"username": "jpaddison"}}, {"_id": "CFgSZezbQsSA6JkmF", "postedAt": "2023-05-20T23:59:22.296Z", "postId": "TbfioWKYYWPT2Dgue", "htmlBody": "<p>I definitely feel it was worth the time for me personally. It was great for learning about the field of AI alignment (problems and proposed solutions). I was hoping the course would spend more time on arguments for and against AI being an x-risk, but unfortunately there was little of that, so it didn't change my mind much.</p>", "parentCommentId": null, "user": {"username": "syc"}}, {"_id": "EjzNdpAXqeHn9qmpb", "postedAt": "2023-05-23T16:50:43.885Z", "postId": "TbfioWKYYWPT2Dgue", "htmlBody": "<p>I'm quite glad I took the course!&nbsp;</p><p>Quick takes:&nbsp;</p><ol><li>The main<strong> types of value</strong> I was getting from the course were:&nbsp;<ol><li>Accountability for doing the readings</li><li>The chance to use wrong terminology / say things that don't make sense (either when I'm trying to explain something, or when I'm asking a question), and then get corrected (This helped me to develop a more coherent model of what's going on and catch unknown unknowns (at least by transforming them into known unknowns).)</li><li>Other resources: links to other readings and explanations</li><li>Corrections and clarifications during the sessions</li></ol></li><li><strong>Personal lessons for next time:&nbsp;</strong><ol><li>Set aside time to do the readings &gt;24 hours in advance, and send in questions early<ol><li>(Agree with the facilitator about doing this!)</li></ol></li><li>Use Claude<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefufyr74rzec\"><sup><a href=\"#fnufyr74rzec\">[1]</a></sup></span>&nbsp;as a personal tutor from the start<ol><li>This was great. The thing I'd do was, if I was reading something that I was having a hard time following, I'd give Claude some context, then say something like, \"I'll now explain this in terms that I understand, or with an analogy or visualization that makes sense to me. Please correct me where I'm misusing terms or saying something wrong.\" Claude would generally be over-positive and would miss some things, but I'd often get a more technical restatement of what I was trying to say, and this helped me a lot. This was relatively introductory material that wasn't specific to AI <i>safety</i>, so I think Claude was actually performing pretty well.&nbsp;</li></ol></li><li>Set up a notes and questions doc for myself from the beginning<ol><li>I now have a messy doc with notes from the different weeks, further readings that we were recommended, and short summaries \u2014 I find it quite useful, but the first few weeks are missing because I just hadn't set it up, and was just scribbling down some questions as I read. Having a doc in advance would have been great and would have prompted me to use it.</li></ol></li><li>Actually read the little intro notes for each week on the AGISF website<ol><li>I think they would have been helpful context for the readings, but I was often skipping them (especially at the beginning), as I didn't really see them as part of the reading.&nbsp;</li></ol></li><li>I spent a fair amount of time on this and endorse doing that.<ol><li>&nbsp;I think the more work I was putting in, the more value I was getting out of the course (there weren't diminishing returns). I was spending probably around 2-5 hours of time before the session each week (there might have been a week when I spent less than 2 hours, and the median time was probably closer to something like 2-3) \u2014 in higher-effort weeks, I would explore related writing or referenced texts, and try to get to the point where I could notice the potential weaknesses and confusions I had about the assigned readings. I should flag that I wouldn't always do all the readings very carefully.&nbsp;</li></ol></li></ol></li><li><strong>Having some amount of context before starting the course could be useful. </strong>If you're not familiar with the overall shape of the argument for existential risk from AI, I imagine that the course structure might be a bit jarring; I moved cohorts a bit in the beginning, but I did feel a bit like it began somewhat abruptly.</li><li><strong>Minor points/asides:&nbsp;</strong><ol><li>It was just pretty fun.</li><li>I have notes on readings I found more and less helpful, and will try to pass those on at some point.&nbsp;</li><li>Facilitators probably matter a lot.&nbsp;</li><li>I enjoyed the readings for some weeks a lot more than I enjoyed them for other weeks.&nbsp;</li><li>I was worried that approaches seemed outdated, in some cases.</li><li>I still have pretty broad confusions that I hope to resolve.&nbsp;</li></ol></li></ol><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnufyr74rzec\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefufyr74rzec\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I was using Claude for dumb reasons \u2014 I don't have a strong sense for how it compares to GPT-4 on this.&nbsp;</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Lizka"}}]