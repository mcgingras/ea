[{"_id": "Fnk8ioYea9GdxaJG6", "title": "List of lists of EA syllabi", "postedAt": "2023-01-09T06:34:28.138Z", "htmlBody": "<p>EDIT: people should use <a href=\"https://docs.google.com/document/d/1pXyPu4macKg-S2op3_w2dK1Vta2rypwEJpRFOrN71kw/edit#\">this doc from Irene</a> (see the comments)</p><p><a href=\"https://docs.google.com/document/d/1Xwhu3HGG2BtjLNpLcbFGLdmZm2kPbDi1-XlBukYf-0o/edit?usp=sharing\">This doc</a> is a collection of existing lists of EA syllabi / curricula as well as some of my own additions. I made it a few months ago when I was trying to brainstorm all the possible seminar-style programs a university group could run. I didn't have to do much work since there are some excellent existing lists:</p><ul><li><a href=\"https://www.stafforini.com/blog/effective-altruism-syllabi/\">Effective Altruism Syllabi</a> and <a href=\"https://www.stafforini.com/blog/courses-on-longtermism/\">Courses on longtermism</a> by Pablo Stafforini (mostly official university courses)</li><li><a href=\"https://forum.effectivealtruism.org/posts/Y8mBXCKmkS9eBokhG/ea-syllabi-and-teaching-materials\">EA syllabi and teaching materials</a> by Julia Wise (\"a list of all the known courses taught about EA or closely related topics\")</li><li><a href=\"https://library.globalchallengesproject.org/\">Library</a> from the Global Challenges Project</li><li><a href=\"https://resources.eagroups.org/events-program-ideas/fellowships-multi-week-programs\">Fellowships &amp; Multi-Week Programs</a> from the EA Groups Resource Center</li></ul><p>I think I was still able to find some important additions, especially for AI-related syllabi. Anyone is welcome to comment with suggestions.</p>", "user": {"username": "jakubkraus07@gmail.com"}}, {"_id": "vZz9tFFJKa28rTkDx", "title": "Big list of AI safety videos", "postedAt": "2023-01-09T06:09:00.910Z", "htmlBody": "<p>This is a <a href=\"https://docs.google.com/document/d/1vDWLKPvJNo0k9IwMPvduO23_CXOdMpxn0aRDnvrL0yw/edit?usp=sharing  \">Google doc</a> containing links to:</p><ul><li>(all?) AI safety-related YouTube channels (intended to be comprehensive, at least for active channels with videos that seem worth sharing in an AI safety group)</li><li>some AI safety-related podcasts</li><li>some suggested videos, sorted into categories like \"agent foundations,\" \"individual researchers,\" \"career planning,\" \"rationality,\" etc</li><li>some discussion prompts for watching these videos in discussion groups (very few)</li><li>some resources and tips for running these discussion groups</li></ul><p>Please comment with feedback and suggestions!</p><p>Also posted to <a href=\"https://www.lesswrong.com/posts/H2BPqnvv7YyjiEHam/big-list-of-ai-safety-videos\">LessWrong</a>.</p>", "user": {"username": "jakubkraus07@gmail.com"}}, {"_id": "ffZrTuL2nrGmiSrFs", "title": "Big list of EA videos", "postedAt": "2023-01-09T05:56:01.288Z", "htmlBody": "<h1>What this is</h1><p>A <a href=\"https://docs.google.com/document/d/1O3AtC1iiesnLdv2NwKJ5i1pXZEew9Qh32KlDviwxg6M/edit?usp=sharing\">Google doc</a> containing links to:</p><ul><li>(all?) existing lists of EA videos</li><li>(all?) EA-related YouTube channels (intended to be comprehensive for active channels with over 10 videos that seem worth sharing in an EA group)</li><li>some EA-related podcasts</li><li>some suggested videos, sorted into categories like \"Intro to EA,\" \"AI governance,\" \"animal welfare,\" \"career planning,\" etc</li><li>some discussion prompts for watching these videos in discussion groups</li><li>some resources and tips for running these discussion groups</li></ul><p>Please comment with feedback and suggestions!</p><h1>Why I made it</h1><p>This fall, University of Michigan's EA group ran weekly \"general meetings.\" We would watch a video and then break out into smaller groups to discuss, with help from a page of discussion prompts. To run these meetings, the following actions need to happen:</p><ol><li>find a room</li><li>pick a video</li><li>make the discussion prompts (or do this on your phone as you watch the video)</li><li>order food delivery (e.g. pizza)</li><li>set up the projector to display the video</li><li>run the meeting</li></ol><p>If you do a recurring meeting, you can probably do (1) exactly once for the entire semester or year. I made this doc to reduce the work of (2) and (3) for all EA groups that want to run video-based discussion groups.</p>", "user": {"username": "jakubkraus07@gmail.com"}}, {"_id": "85tCJ6efR2hsQzZvu", "title": "Big list of icebreaker questions", "postedAt": "2023-01-09T04:46:17.422Z", "htmlBody": "<h1>What this is</h1><p>At the start of some EA meetings with small groups (e.g. intro seminars, discussion groups, workshops, etc), facilitators sometimes ask icebreaker questions to get people talking and more comfortable. I collected a big list of suitable questions in <a href=\"https://docs.google.com/document/d/1x5xEYqam6sA9QHQrLDZxs-Xc8gdn3tY5YWLWy456fb8/edit?usp=sharing\"><strong>this Google doc</strong></a>. (Note that almost none of these are my own inventions.) It's very imperfect but seems useful to share. Feel free to comment with any feedback or suggestions!<br><br>I sorted the questions based on how much trust they require. The high-trust questions are for meetings where many of the participants are already close friends -- in most cases I'd oppose using them.</p><h1>Why I made it</h1><p>Originally, I made this list because I wanted to try a new strategy for general university group meeting<strong>s</strong> (e.g. watching <a href=\"https://docs.google.com/document/d/1O3AtC1iiesnLdv2NwKJ5i1pXZEew9Qh32KlDviwxg6M/edit#\">a video</a> and discussing). I see <strong>missed opportunities for attendees to socialize with each other</strong>, particularly at the start and end of meetings. I think this happens because some people don't feel comfortable introducing themselves to a stranger. My intended remedy is to say something like this:</p><blockquote><p>Ok, before we get started, if you feel comfortable, turn to the person next to you or find someone near you in the room, ideally someone who you don't know that well, and chat for a few minutes. You can introduce yourself and find out a little more about them, and if both of you want, you can ask each other some of these questions (point to slide with a few questions). Does that sound good?</p></blockquote><p>As a result, people have something they know about each other, and at the end of the meeting they might continue their earlier conversation. I'm happy to hear feedback on this idea.</p><h1>Questions I removed</h1><p>I noticed that many questions in the lists I drew from had to do with sexual activity, gender, attractiveness, and I removed these because they seem inappropriate for general university group meetings. (See also <a href=\"https://docs.google.com/document/d/1fqqDZaC28PgKsY7HCA4JxjVEPtlxXFL8SiugBh12jNI/edit#\">these notes</a> from Julia Wise about hot seat / honesty games in EA groups.) Additionally, there were a lot of thought experiments that seemed boring for utilitarian-leaning people, and some EAs fall into this category.</p>", "user": {"username": "jakubkraus07@gmail.com"}}, {"_id": "3yvBBujvJmwTKo4Tu", "title": "A More Humane World will also be Prettier!", "postedAt": "2023-01-09T06:02:33.890Z", "htmlBody": "<p>TL;DR Selfish reasons for working against factory farming.</p><p>Animal welfare work can have a dour feel to it. Not only is the fact of factory farming\u2019s existence a potentially overwhelming reality to those who <a href=\"https://forum.effectivealtruism.org/topics/scope-neglect\">appreciate its scale</a>, but as a cause area it\u2019s also just kind of boring. There\u2019s not really an easily-identifiable villain in this moral catastrophe, since virtually everyone is complicit in it. As such, it\u2019s not very amenable to archetypal heroic narratives. In fact, it looks like the way this actually gets solved won\u2019t be by convincing everyone to change their ways, but by inventing cruelty-free products that taste so much like the repulsive factory-farmed ones that no one notices a difference. There\u2019s probably not going to be a day or year when animal abuse is summarily abandoned; instead, we\u2019ll see a gradual reduction over many years. It\u2019s conceivable that the world will barely even notice the monumental moral progress that has been made when factory farming is finally ended. If this ends up being the case, then vegans will never have their satisfying \u201cI told you so!\u201d moment with their acquaintances who thought they were a little crazy for their weird obsession with animal rights. Instead, those acquaintances might just go on thinking that the vegans were in fact a little crazy, all the while enjoying the amazing taste of their synthetic burgers as well as the absence of cognitive dissonance in cherishing their pets. In other words, animal activism is not only thankless now, but might remain so even in the case of victories. Moreover, the evil we are fighting is one inflicted upon beings we rarely interact with, have little understanding of, and who are incapable of communicating with us in any way. This can make it hard to practice <a href=\"https://blog.givewell.org/2013/08/20/excited-altruism/\">excited altruism</a>.</p><p>Of course, something similar can be said of other cause areas. There\u2019s nothing sexy about malarial bednets, and working to mitigate risks from new technologies doesn\u2019t exactly have the same allure as wide-eyed techno-optimism. But my own work and thinking goes primarily towards farmed animal welfare, so this post is aimed at others who share the burden of that banal gloominess that is peculiar to animal activism.</p><p>To a pure altruist, this gloominess would not matter. A moral saint would derive motivation from impact alone, and would feel no discouragement even if everyone thought they were a villain for their work. But I am no such saint. For me, it is helpful to remind myself that a world without factory farming will not only be more hospitable to sentient life, but will also be more beautiful and vibrant in ways that will enrich my own experience and the experiences of others like me. I expect that others share some of my aesthetic sensibilities, so here I will list some positive changes in the world-as-I-experience-it that I expect to see with the end of factory farming.</p><ul><li><strong>In a world without factory farming, people will be healthier.</strong> In rich countries, animal agriculture is a public health disaster because it <a href=\"https://www.pnas.org/doi/10.1073/pnas.1523119113\">contributes to obesity, diabetes, heart disease, cancer</a>, and other chronic illnesses. With a reduction in these illnesses, people will have an easier time getting around to do all the things they enjoy. They\u2019ll be more able to go hiking, take their children to playgrounds, see public art installations, and play pickup basketball. I expect that people will also tend to be happier and in a better mood with the burden of chronic illness lessened. In other words, when you go to your local park in a vegan world, the people you\u2019ll encounter will be a bit more pleasant to be around. They\u2019ll also be more attractive.</li><li><strong>People will be wealthier.</strong> The reduction in chronic illness (not to mention <a href=\"https://ourworldindata.org/emissions-by-sector\">greenhouse gas emissions</a>, <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4638249/\">antibiotic resistance</a>, and <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2865087/\">zoonotic pandemics</a>) will also <a href=\"https://www.pnas.org/doi/10.1073/pnas.1523119113\">free up lots of money</a>. This is money we\u2019ll be able to spend on lots of other things that we value! That means more nature preserves, playgrounds, art, music, poetry, spaceships, research into effective altruistic interventions, and implementation of the same.</li><li><strong>We\u2019ll also have lots of extra land.</strong> Animals are an extremely inefficient way to produce food. As a result, the <a href=\"https://ourworldindata.org/global-land-for-agriculture\">vast majority of the world\u2019s farmland</a> is used to produce meat and dairy products, but the food produced only comprises 18% of the world\u2019s calories. In other words, in a vegan world we could feed everyone with far less farmland. Imagine what we could do with that extra land!</li></ul><p>These aren\u2019t my primary reasons for reducing factory farming, but they help me stay motivated when my work feels a bit bleak. It\u2019s nice to feel like there\u2019s something positive that I\u2019m building towards, even though the intended effect of my work is mostly to remove a negative from the world. Interestingly, environmental and health-based messaging on meat consumption seems to be <a href=\"https://faunalytics.org/reducing-animal-product-intake-which-interventions-work/\">somewhat more effective</a> than welfare-based messaging, so perhaps this is indicative of some wider truth about human psychology.</p><p>I like to imagine myself going for a drive across my country in a vegan world. Much of the countryside is still farmland, but far more of it is not. Now that countless new nature preserves have opened up, my stops for a day hike can be more frequent. The people I meet on these hikes are healthier and friendlier, and so are the wild animals I observe, now that the world has recognized the value of all sentient beings and set aside some resources for researching and implementing cost-effective wild animal welfare interventions. I can rest easy knowing that the world is working to make both manmade and natural environments friendlier to all sentient life, and that things can only get better moving forward. The stretches of farmland that I do drive through are far more interesting than they used to be. Where once there was endless corn destined for consumption by farmed animals, there is now a great variety of human-edible plants. And when at last I arrive at a coastal city, nestled at the mouth of a pristine river, I see that the air is a little clearer and the sea is rising a little slower now that pollution of various kinds is past its peak. Since there\u2019s less <a href=\"https://link.springer.com/article/10.1007/BF02804898\">nutrient pollution-driven eutrophication</a>, the beaches are more likely to be open. And the city itself is a little more beautiful.</p><p>My point here isn\u2019t to claim that this is a perfectly accurate model of what the world will look like once factory farming has ended. In fact, it seems to me that the future might be <a href=\"https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/\">wild</a> in any number of ways, whereas the picture I\u2019ve painted here is basically society as I currently experience it, just a bit nicer in some ways that are fun for me. But regardless of what the future may hold, making societies nicer seems <a href=\"https://www.cold-takes.com/making-the-best-of-the-most-important-century/#robustly-helpful-actions\">robustly helpful</a> and my little mental picture of a slightly-prettier today is a good source of motivation for me. And most of these improvements aren't all or nothing; even if factory farming continues for many more decades, marginal reductions in it probably do cause some marginal environmental &amp; public health gains.</p><p>\u201cSelfishness\u201d might be one way to characterize this type of motivation. I want the world to be more beautiful, so I derive value from actions that satisfy this preference. But I think it goes beyond selfishness in a couple of ways. For one thing, I am not the only person who would enjoy the aesthetic beauty of a world without factory farming. I suspect that many of my aesthetic sensibilities are shared by the vast majority of other humans, so there is a real sense in which these aesthetic improvements constitute a substantial positive externality in work against factory farming. Also, as I mentioned earlier, making society broadly more inclusive, prosperous, and \u201cnice\u201d might facilitate better decision-making with respect to whatever strange twists of fate may be in store for us.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreferi0pixc98b\"><sup><a href=\"#fneri0pixc98b\">[1]</a></sup></span>&nbsp;I also think there\u2019s a meaningful difference between \u201cselfish\u201d motivations, at least in the way that the word \u201cselfish\u201d is typically used, and aesthetic motivations. \u201cSelfish\u201d behavior is typically associated not only with self-interested activities, but also with a certain affective state in which one consciously puts one\u2019s own pleasure before that of others. In this case, I don\u2019t exactly <i>feel</i> such an affective state. I genuinely care about the welfare of all the people who will be healthier and happier after we reform the food system, and about all the good that can be done with the resources we\u2019ll free up. I also care about the beauty of the world in a way that at least <i>feels</i> separate from how I care about my own welfare. And even if these feelings don\u2019t factor into any cost-effectiveness analyses, they\u2019re important information about the humans who aspire to do some good in the world.</p><p><strong>Feel free to comment any other \u201cselfish\u201d motivations you have for fighting factory farming, or for any other altruistic endeavors.</strong></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fneri0pixc98b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreferi0pixc98b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Though of course it's important to <a href=\"https://forum.effectivealtruism.org/posts/omoZDu8ScNbot6kXS/beware-surprising-and-suspicious-convergence\">beware surprising and suspicious convergence.</a></p></div></li></ol>", "user": {"username": "Equable Albatross"}}, {"_id": "2QBd2Ai9k6tWJpgL7", "title": "America & the Shape of the Far Future", "postedAt": "2023-01-09T06:03:45.503Z", "htmlBody": "<p><i>This is one of the first Sequoia Papers, the new Federalist Papers. It can also be found on my website, https://beatinpaths.com/2022/09/28/america-and-the-shape-of-the-far-future/. I have been told the EA audience may enjoy it; since you're all already familiar with longtermism, feel free to skim the first quarter. This is my first post on the forum and I'd appreciate feedback.&nbsp;</i></p><p><u>America and the Shape of the Far Future</u></p><p><strong>Humanity is a story.</strong> The story is the sum of all human agency, a plot cascading through time along the paths of every choice. \u2018Longtermism\u2019 is the belief that humanity might have millions more seasons with billions more characters, and that we ought to act accordingly. If our actions reverberate ever-wider along a lengthening river of consequences, what choices should choosers make? Long ago, a people better adapted to Turtle Island called this \u2018seventh-generation thinking.\u2019 It was nearly forgotten. Now, a single composite superorganism spread across the continent has more power to choose than anything before in history. But America lies half asleep at a key climax of the plot.</p><p><strong>The next decades of American action will majorly impact the future of humanity for the next millennium and far beyond.</strong></p><p>There may be those reading this who are allergic to discussions of America, or of nations in general. They may believe that nations are inherently bad, America especially.&nbsp;<strong>But regardless of whether nations are good or bad, or whether the world should even be composed of nations, it is undeniable that the world currently&nbsp;</strong><i><strong>is&nbsp;</strong></i><strong>made up of nations.</strong> Nations, imagined to be deeply tied to the identities of billions of individuals, have great power. This power is amoral; therefore, this situation can be pragmatically wielded for good or bad purposes in order to deal with the major developments of the 21<sup>st</sup> century.</p><p>America is a Locus of Global Agency</p><p><strong>America is a locus of global agency.</strong> The American state is the most powerful superorganism on the planet. American institutions have massive advantages due to America\u2019s technological capabilities, relative long-term stability, all-inclusive nationality, rule of law, capital markets, remarkable geographic advantages and natural resources, and on and on. The American people have more wealth, power, technology, and influence than any people in history. What the American state does, what American institutions do, what the American people do--- these nested agencies have massive impact on the shape of events, more impact per unit than that of any other nation.</p><p><strong>Furthermore, agency is contagious, and what America does has profound consequences for how the rest of the world acts</strong>. The eyes of the world are on America. Our culture, our technology, our systems, and our values are beamed to everyone on Earth. We have the power to lead by example on both small and large scales during this crucial time for humanity. Our memes have the highest viral potential. Our fingers are on the scales of every global network. When America invests in something, the world follows; when America takes a new direction, the world follows; when America acts, the world acts.</p><p>The Plot is Thickening and Quickening</p><p><strong>Due to the historical contingencies of the 20th century, America is in a unique position for the plot of humanity at a crucial time.</strong> America\u2019s density of agency makes it the closest thing we have in the immediate future to a global leader. While it is fine to dream of a functioning UN or a crypto-political state or some other preferred locus of global agency, the current situation is undeniable. America is still in the drivers\u2019 seat. The 21st century began with a unipolar world of American hegemony, and will end with a multipolar, interconnected world organized around revolutionary technologies. The shape this world will take depends a lot on what America does as a primary agent. Will America use its moment at center stage to build a neodemocratic, dynamic global society for the flourishing of future generations? Or will it cling to more limited views of self-interest, sleepwalk through the call of history, and allow American agency to act in ways that increase global existential risk?</p><p>AGI, bioweapons, human enhancement, space colonization, evolving political economies, climate change, brain-machine interfaces--- all of these developments and more will profoundly reshape human life.&nbsp;<strong>Unprecedented tectonic shifts in the ways humans live are speeding up with increasingly widespread and long-term consequences for the future of humanity.</strong> While this is happening worldwide, America is where it happens more than anywhere else. More technological progress, more capital, more cultural influence--- more happens here. Small changes in how America addresses these big issues will have massive butterfly effects for the future of life on Earth.</p><p>The Moral Imperative of the Far Future</p><p><strong>If we consider how many more billions of humans will live in the far future, it becomes a moral imperative that we shift the direction of the future positively in order to bring about a better world for future generations.</strong></p><p>Many philosophers, Effective Altruists, Silicon Valley prophets, climate scientists, and even His Holiness the Pope all agree: the current time period is one of undeniably large consequences for the future of humanity. The long peace of slow history is over, and the brief illusion of an \u201cend of history\u201d is now a bubble rudely burst. History is now. We are playing for keeps.&nbsp;<strong>The plot is both thickening and quickening.</strong></p><p>More than any other nation, America has the agency (in our more immediate horizon of possible action over the next few decades) to strongly influence the shape of the far future.&nbsp;<strong>If we can re-orient the multifaceted arms of American agency towards consciously creating a better future, we can have enormous positive impacts on the timeline.</strong></p><p>Our argument is not that America is exceptional, or predestined, or some kind of moral exemplar. Our argument is one of sheer pragmatism.&nbsp;<strong>The current situation demands strong action, and America has the greatest potential for such action at the current moment due to its contingent, historic privilege.&nbsp;</strong>With this privilege comes a responsibility for the ways in which choices within this nearby time horizon will shape the curve of the far future.</p><p>Recreating the Idea of America</p><p><strong>Our goal is to influence the far future by reshaping the memes that make \u2018America.</strong>\u2019 We want to recreate and redefine what it means to be \u2018American\u2019 to redirect the nested agencies of America, her institutions, and her people. We want to use the irrepressible power of tribal identity-making for positive purposes. This plan is described in more detail in \u201cThe Unfinished Fiction of the Nation\u201d (https://beatinpaths.com/2022/08/05/the-fiction-of-the-nation-unfinished/). America is a vast composite noun containing multitudes, good and bad. We want to use the good to solve the bad. We want to create a New World. We want people to&nbsp;<i>believe</i> in the possibility of a New World, which is a precondition of any progress in society.&nbsp;</p><p>We believe that ideas change actions, culture influences innovation, and values shape choices.&nbsp;<strong>By creating a new set of inspiring American myths, we hope to guide the direction the far future follows by changing the actions of Americans over the next few decades.</strong> Humans crave stories that connect them to a larger tribe, that forge individuals into shared agencies. Positive applications of this natural force have been lacking in America for the last 60 years, while negative uses abound.</p><p><strong>We should aim to create change from both the bottom up and top down, feeding into one another</strong>. If people believe in new values, stories, and memes, then the force of those memes will trickle upwards into the corporations, associations, and institutions which the people compose. Capturing the values of a majority of people in an organization confers massive increases in agentic benefits, in the same way that capturing 51% of a governor\u2019s election means capturing 100% of the agentic power of the state. The power of the institution can then be unilaterally wielded in service of the positive memes we are trying to spread, creating a feedback loop with large jumps upon capturing enough of a full unit.</p><p><strong>The ultimate goal of this process is to recapture the American state itself, the American government, the strongest single agency in the world.</strong> While ossification, inertia, and bad actors have quite effectively gunked up the mechanisms by which popular will becomes expressed in the agency of the US government, the mechanisms are still there, and a strong enough will can still burst through. Once through, we can reshape the mechanisms themselves and clear out all the gunk. That is the greatest goal of our movement--- to recapture the US government on behalf of the people, the dream of the New World, and the possibility of utopia for billions of humans in the far future.</p><p>The Feeling of the New World</p><p><strong>America, more than any other nation, has long identified itself with the future rather than the past.</strong> America represents the dream of the New World, an ongoing project towards achieving utopia. America is indefinitely unfinished, always ahead, always pulling us to strive beyond ourselves as the best ideals do.</p><p>However, the last few decades have drained Americans of this feeling. While the story is ongoing, many believe that it is the \u2018end of history.\u2019 This of course is not the case; if anything, the complacency that came from this attitude has brought history roaring back with a vengeance, the course of which feels less under control than ever. We need to reject postmodern paralysis and choose meta-modern meaning-making for our own stories\u2019 sake.&nbsp;<strong>We need to actively write history once again; we need to actively build the New World.</strong></p><p><strong>There is a certain invaluable feeling cultivated by exposure to the best of the idealistic American inheritance, our poets, philosophers and prophets.</strong> This is the feeling of being at the edge of an ever-emerging reality, the feeling of pushing into the unknown. This is the feeling of being at the center of a star, the center of creation, where old things are smashed together to create something new, releasing energy, spreading light. This is the feeling of entering the stage during a key scene in the great drama of life, free to contribute your verse. This is the feeling of will, of human agency minnowing in the winnowing river of time. This is the feeling that the world is here, real, living and dying, consequential and for keeps, deeply meaningful. This is the feeling of God as your future selves, of Heaven as your grandchildren\u2019s joy. This is the feeling of being awake just before dawn to see the sky change colors. This is the feeling that the best is yet to come, and that what is to come emanates from your outstretched palms.</p><p>Make America Young Again</p><p>This is the feeling that we must cultivate across America, and then across the world, if we are to create a long term future worth believing in.&nbsp;<strong>Cultivating the feeling of the New World will have downstream effects on every project, every dream, every garden grown by individuals, groups, the nation, and humanity</strong>. We must counteract the cultural pessimism and nihilism that has grown through our culture like weeds if we want to meet the challenges of the 21st century. We must inspire agency; we must inspire responsibility; we must inspire experimentation; we must inspire higher values; we must inspire higher dreams. The last time America felt this youthful energy of building a New World was the 1800s, the time of the Great Awakenings and the American Renaissance, the time of exploration and new communities and new philosophies and new literatures. We need to spark another American Renaissance.&nbsp;</p><p>What makes the Constitution great is not the words on the paper, but the spaces between;&nbsp;<strong>America\u2019s greatest idea was its right to re-create itself, to adapt to changing times the way all living things must adapt to survive.</strong> Human institutions tend to have inertia, to want to stay the same in the face of change. The great innovation of America was the creation of fluid institutions changeable by the evolving will of the people. Jefferson imagined we\u2019d have a small revolution every 20 years, every generation. Now, the US Constitution is the oldest constitution in the world, and hasn\u2019t changed in a long time. The youthful, evolutionary energy that made America great has been drained, and our elites are a complacent gerontocracy. We need to reinvigorate America. We need to make America&nbsp;<i>young</i> again. If I have my way, we will have a new Constitutional Convention by 2050. The mechanisms to refound America are all there; we just need people to care enough to make it happen.&nbsp;</p><p>The Meme of the New World as a Meta-Meme</p><p>This is just one example of the vast changes that a youthful America can accomplish, but by no means the only one. Cultivating the felt sense that we are at the frontier of history creating a New World, acting within the great story as it unfolds evermore consequentially, will have unthinkably rich downstream consequences. It is sort of an ideal lubricant for evolution itself.&nbsp;<strong>The human meme of the New World, the meme of emergence, recreation, adaptation, and novelty is a sort of ur-meme, or meta-meme, a meme that enables and facilitates the creation of more memes downstream.&nbsp;</strong>Ideas, culture, values truly do influence materials, agents, directions.</p><p><strong>The widespread propagation and adoption of the New World meta-meme was a core driver of what made America wonderful for hundreds of years.</strong> It was present in every community experiment, every Emerson essay, every immigrant family settling Iowa, every invention of Edison, every Constitutional Amendment, every progressive movement, every evolution in morality, every 60s hippie, every Silicon Valley dreamer. What these all had in common is \u2018newness\u2019 itself, the belief that the intersubjective world should keep evolving as all life does.</p><p>The success of the smallpox vaccine helps contribute to the success of the Apollo Program, not necessarily due to direct scientific lineage, but simply because each success reinforces the meta-meme of newness.&nbsp;<strong>The success of a New World in one direction implies the potential success of newness in other directions.</strong> We want to strengthen the story of innovation as such; we want to support the story of the New World.&nbsp;New inspires new. A movement dedicated to the reinvigoration of a youthful American spirit will need to take advantage of this memetic reinforcement effect.</p><p><strong>It doesn\u2019t take much to see that the meme of the New World is at a low point right now in American history.</strong> This is due to many factors, including America\u2019s age, institutional inertia, the abuse of the meme by parties and corporations selling bullshit leaving an ironic taste in our mouths, the \u2018end of history\u2019 vibe propagated after the end of the clearer ideological battles of the 20th century, the complacency encouraged by a decadent consumer society, the complete and disastrous secession of the intellectual left from all things smacking of patriotism after Vietnam, the nihilistic and schizophrenic modalities of consciousness engendered by television news depicting a contextless and disconnected world in relation to which viewers are mere spectators rather than agents, and on and on and on and on.&nbsp;</p><p><strong>We need to rekindle the meta-meme, spread it widely and deeply in the minds of Americans, and restart the virtuous cycle of re-inspiration from one New World to the next, from one new idea to another new project, from one new invention to a new way of life</strong>. It begins with us believing in these ideas. Then people create, build, innovate\u2026 anything. They instantiate a new thing in the real world, associated with the meta-meme of newness. This makes it concrete. This makes it a story. The more that this happens the more it reinforces the story, the narrative that we\u2019re building a New World, that we are a young America again. People doing this consciously would be best for the meme, but even unconscious progress supports it. The more narrative momentum this gains, the more novelty we will see emerge; the more people feel like the New World is something that\u2019s&nbsp;<i>happening</i>, the more they will want to help create the New World.</p><p>Ideals, Materials</p><p>An excellent example of how this cycle between believing in the story of the New World and building the New World can work is the example of building a new American city. Cities are where people come together to create; creation happens most densely and speedily in cities. Cities are where you can experiment on all sorts of levels: political, economic, cultural, architectural, scientific, etc. The origins of American democracy come from the small New England town of self-governance, community ties, and experimentation. Building a physical city is, itself, a major instantiation of the idea that we are building a New World.&nbsp;<strong>We are building a city on a hill, both valuable unto itself and valuable as a beacon of inspiration for others.</strong></p><p>Experimentation is at the essence of America\u2019s association with ever-emerging novelty<strong>. A new city would give us the physical and social environment with which we could experiment endlessly, infinitely widening the scope of human possibility.</strong> American cities have been largely stagnant for a long time now, strangled by highways, zoning laws, soaring rents, crime, and a general lack of youthful energy. A new city designed in the 21st century that adopted the New World meta-meme as core to its culture could create untold diversity, unlock unforeseen human potential, inspire creators and builders and innovators of all kinds, and be a beacon of hope for a New World that expands far beyond the city itself.</p><p>We need to roll up our sleeves and work on concrete experiments in order to counteract the spread of nihilism, which threatens to tear apart the fabric of American society (https://beatinpaths.com/2022/10/11/american-nihilism-and-its-discontents/).<strong> Nihilism is a natural response to a felt lack of agency, a lack of context, a lack of connectedness, a lack of power</strong>.</p><p>America generates meaningful energy and agency by undertaking grand projects, from the Erie Canal to the Moon mission to the founding of a new city. The opposite of consuming is creating. Creating creates more than the thing created; it creates a superlative aura of agency<strong>.</strong> This may be why it has felt like the entire world has been happening in the Bay Area for the last few decades. Building, making, innovating--- genuinely&nbsp;<i>creating</i> new things is like an antiseptic to souls sick of nihilism.&nbsp;<strong>We need to propagate the aura of agency in order to replace cultural pessimism with a new cultural optimism, an optimism for the future utopia of the New World</strong>. This optimistic orientation towards the possibility of utopia is the dream that has inspired all other dreams, the dream that has underpinned all genuine progress in this country.&nbsp;</p><p>Coda: A Call to Change-Makers</p><p>America is unfinished. Its essence has always lain in the future.<strong> All great movements for progress in American history have used a positive form of patriotism to accomplish their goals, framing progress as a way of realizing a more ideal version of America, a New World worthy of pride.</strong> American history and culture are rife with examples of this; all we need to do is rekindle the same spirit that has been ebbing and flowing in America for centuries. We are not starting from scratch; we have a deep democratic tradition to draw from. All we need is the spark of the meta-meme, and to use that spark to start building. Once we start making the ideas real, everything will gain momentum.</p><p><strong>The dream of utopia is pragmatically useful.</strong> Dreams can be used as a sort of technology, tools that can extend us beyond ourselves, cranes that can lift us into new territory.</p><p><strong>For too long, technologists have ignored the power of stories, the power of dreams, the way ideas can shape materials.</strong> Where they have recognized it, they have turned it into short-term profits. But the same power can be used to plant sequoia seeds that will grow for thousands of years.</p><p><strong>For too long, technologists have ignored the value of the American nation, despite the fact that it is the bounty, stability, infrastructure, and support of this nation, its institutions, and its people that has allowed innovation to flourish here like never before in history</strong>. The prevailing attitude in Silicon Valley seems to treat the nation as an irrelevant fossil, a nuisance at best, rather than the precondition of all the technological progress we make. Did American taxpayers not invent the Internet? Technologists seem to think that they can build the techno-optimist future in a bubble, a bubble supported by the American nation while somehow isolated from the issues of the American nation. They think the world can be saved without dirtying their hands in culture and politics. But this is sadly not the case. They merely take the dirty hands that got us here for granted.</p><p><strong>If we want to create a flourishing future, we need to pragmatically work with all of the forces that influence our world.</strong> Memes, values, ideas, culture---&nbsp; for many technologists who are more at home rotating shapes these are uncomfortable tools to deal with, but we must work with them if we want to maximize our chance at effectively stewarding the world towards a better future for billions of potential humans. America is still the densest locus of agency in the world, and we Americans have the power to influence American agency, from the small scale of individuals to the large scale of the American government and everything in between. America is where the future will take shape more than anywhere else; luckily for us, it is where the meta-meme of the New World has historically been the most successful. We can rekindle the meta-meme, and carry the torch of the New World into the far future.</p><p><br>&nbsp;</p>", "user": {"username": "Aidan Fitzsimons"}}, {"_id": "p2b4PKB9xaT49hnmA", "title": "Infinite Ethics - Sketch Part 1: Explorations with Surreals ", "postedAt": "2023-01-09T00:44:12.186Z", "htmlBody": "<p><i>Epistemic status: I've procrastinated on a proper write-up for a number of years, so I thought it was better just to get an unpolished, rough sketch of my thoughts out there as quickly as possible. I'm playing fast and loose with terms here, apologies for that, but I thought it was better than continuing to wait for me to somehow find time to polish my old work, which doesn't seem likely to happen any time soon given how I'm focusing more on AI Safety these days.</i></p><p>A few years back, I spent some time at the EA Hotel trying to make progress on Infinite Ethics. I kind of got stuck, but here's a quick write-up of some of the thought paths I explored in case anyone else finds it useful.</p><p>Bostrom defines the problem of <a href=\"https://nickbostrom.com/ethics/infinite.pdf\">Infinite Ethics</a> in a paper with the same name.</p><p>The obvious pragmatic response to Infinite Ethics is to just ignore the whole thing. I essentially agree with this. If our formalism disagrees with our intuitions, then it seems pretty reasonable to conclude that we're using the wrong formalism.</p><p>So I jumped on Wikipedia and in about an hour I concluded that the Surreal Numbers looked the most promising. I later learned about <a href=\"https://www.lesswrong.com/posts/MtYDr7zyicq4J7rWe/summary-surreal-decisions\">this paper</a> by Eddy Chen and Daniel Rubio which provided confirmation that I had found a promising direction. At the time, there wasn't really any other work looking at applying the surreals to this problem, but I expect that if I searched now, I'd probably find more.</p><p>In any case, my initial idea was to fill out some of the details for this research direction. I was particularly focusing on the notion of bijections. I've heard people say very casually that bijections show that there are the same number of integers and even numbers (maybe mathematicians speak more precisely, though it wouldn't surprise me if they did not). This leads directly to the problems of infinite ethics, but it's also exactly the kind of view that you should have reject if you're claiming that the surreal numbers are a better measure of \"size\" for infinite quantities.</p><p>So how large are the evens? If you write the counting numbers as having size x, then if we assume that there are the same number of evens and odds, this gives us x/2 of both. We can imagine taking this further so that there are x/3 counting numbers that are 1 mod 3, x/3 that are 2 mod 3 and x/3 that are 0 mod 3; and so on for mod 4, 5, 6, ect. This provides us with a uniform notion of infinity in a sense and I imagine that many people would find this intuitively appealing.</p><p>On the other hand, while this is a beautiful construction, it's not obvious that this is the right thing to do. Like we could construct it so that there are x/2 odd non-negative integers and x/2 even non-negative integers including zero or excluding zero and it isn't clear why we should prefer one over the other. But it's a useful concept, so let's just define the uniform notion of infinity to exclude zero when we're defining equal sequence lengths. Oh, and we'll also add an additional condition to ensure that there are both z positive numbers and z negative numbers.</p><p>Scoping out our options at this point, we can:</p><ul><li>Treat all these subsequences as having equal \"size\" as per cardinality/standard bijections - seems philosophically dubious as it proves too much, ie. a set having equal \"size\" as a subset</li><li>Claim that the sizes of these subsequences are incommensurable or there isn't really a notion of size that we can construct here - this would mean that most infinite ethics problems would have no solution</li><li>Insist that the only valid construction of the integers is the uniform one - but why? It's the most beautiful, but that isn't exactly a reason, plus as mentioned above, different constructions can appear uniform depending on factors like including or excluding 0 in comparisons</li><li>Accept that there are multiple possible constructions of the integers - seems the most promising direction to explore. Maybe we can make it work, maybe we can't, but only way to figure this out is to try.</li></ul><p>So if there's multiple possible constructions of the integers, what are they? Let's look at an example. Talking very casually, we can imagine positive integers with the last number<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefew162del4cr\"><sup><a href=\"#fnew162del4cr\">[1]</a></sup></span>&nbsp;being even and positive integers with the last number being odd, however:</p><ul><li>This wouldn't fully define it. What about the subsequences for mod 3, 4, 5, ect?</li><li>We don't need to talk about the concept of the \"last number\". We could just, for example, have one construction where there are x/2 odds and x/2 evens; and another with (x+1)/2 odds and (x-1)/2 evens</li></ul><p>Nonetheless, thinking about it casually in this way seems like a useful intuition pump, as long as we only hold the model lightly.</p><p>Anyway, this naturally leads into a bunch of questions such as:</p><ul><li>How can we formalise this?</li><li>Is pinning down the different moduli sufficient to pin down a specific infinity for the counting numbers? Does it make sense to think about the equivalent of \"last number being a square\" or \"last number being a prime\"?</li><li>How we pin down a specific notion of infinity for the reals?</li></ul><p>Also, this naturally leads to a desire to construct an additional, more refined notion of bijections where we ensure that the sequences have equal surreal size and the same too for each subsequence that is mapped to each other. If we had such a notion, then many of the paradoxes of infinite ethics would likely disappear.</p><p>This immediately leads to two more questions:</p><ul><li>How can we formalise these refined bijections?</li><li>Are there any consequences for the foundations of mathematics if we construct it on refined bijections? The foundations of maths are built on set theory and there are a <i>lot</i> of bijections used there. But then again, there is a formalisation of the surreals using set theory, so maybe they don't need to change after all?</li></ul><p>So how does this tie into the larger question? Well, firstly, if you're going to suggest using surreal numbers to measure the size of infinite sets, you're going to have to answer some questions about how to figure out these sizes. And the even numbers/odds numbers are about as easy as it gets. Secondly, once you start actually applying surreal numbers to label the sizes of sets, a lot of the paradoxes of infinity seem a <a href=\"https://www.lesswrong.com/posts/NdKqCrRJ9JXigYcjp/an-extensive-categorisation-of-infinite-paradoxes\">lot less surprising</a>. And so if you can make it work, that would add a lot of credibility to the rough framework.</p><p>Anyway, I didn't get far down this research direction, because I started to suspect that infinities might not exist, even in theory. I've recently started to see infinities as plausible again, but we'll get into these issues in the next post.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnew162del4cr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefew162del4cr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>When I was working on this, I was taking the notion of \"last number\" a lot more literally as I was imagining that even if there was an embedded perspective from which that notion of last might not make sense, there might be a non-embedded perspective from which it might, but I still don't have high confidence in how this line of reasoning plays out.</p></div></li></ol>", "user": {"username": "casebash"}}, {"_id": "qethtJDnaAChgJfes", "title": "Potential Future People", "postedAt": "2023-01-08T17:20:32.355Z", "htmlBody": "<p>The trolley is barreling down the tracks towards a number of love letters, which you know will each lead to the birth of a future person (only if the trolley is diverted). If you divert the trolley you will kill one person. Do you pull the switch?</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673198433/mirroredImages/qethtJDnaAChgJfes/ucvxrcfe9tcu1kldfsh1.jpg\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673198434/mirroredImages/qethtJDnaAChgJfes/tgbtaqqtfxmrrjvlxouz.jpg 118w, https://res.cloudinary.com/cea/image/upload/v1673198434/mirroredImages/qethtJDnaAChgJfes/r92szjhhacqifayyrwfw.jpg 198w, https://res.cloudinary.com/cea/image/upload/v1673198434/mirroredImages/qethtJDnaAChgJfes/bilzflq3hjnwwi0c0xbk.jpg 278w, https://res.cloudinary.com/cea/image/upload/v1673198433/mirroredImages/qethtJDnaAChgJfes/j2dznmvpfkppopbpoukk.jpg 358w, https://res.cloudinary.com/cea/image/upload/v1673198434/mirroredImages/qethtJDnaAChgJfes/une4t9adw8vfsooou9o7.jpg 438w, https://res.cloudinary.com/cea/image/upload/v1673198433/mirroredImages/qethtJDnaAChgJfes/x0ch8xald5fw4bh78axk.jpg 518w, https://res.cloudinary.com/cea/image/upload/v1673198433/mirroredImages/qethtJDnaAChgJfes/egawfddh6udsn5ox4gqz.jpg 598w, https://res.cloudinary.com/cea/image/upload/v1673198433/mirroredImages/qethtJDnaAChgJfes/xwtee7ctgusuejpmrx8k.jpg 678w, https://res.cloudinary.com/cea/image/upload/v1673198433/mirroredImages/qethtJDnaAChgJfes/sqh2omcvtfboryxsaqpm.jpg 758w\"></figure><p>(edited to clarify in response to Gietz comment)</p>", "user": {"username": "TeddyW"}}, {"_id": "2bQrhgkK2DxLtNGbj", "title": "Is anyone else also getting more worried about hard takeoff AGI scenarios?", "postedAt": "2023-01-09T06:04:43.032Z", "htmlBody": "<p>Hi, this is my first post and I apologize if this question is too subjective, in which case I'll take it down. &nbsp;Ok here goes:<br><br>I'm personally starting to feel an accelerating, slightly visceral sense of fear at the increasing pace of news about AI breakthroughs that seem mere years from causing mass unemployment among white collar and blue collar workers alike (everything from automated artistry to automated burger-making). &nbsp;My wife &amp; I have been incredibly blessed with two adorable toddlers so far, and if they eat healthily, exercise, and benefit from the arrival of regenerative medical technology such as stem cell therapies, it seems quite reasonable that they'll live for at least 110 years if not much more (I hope even 1,000's of years at least). &nbsp;Even taking the base case as 110 years, it seems a near-certainty that a transformative and super-dangerous AGI Singularity or Intelligence Explosion will occur while they are alive. Since I obviously deeply love our kids, I think about this a lot, and since I work in this field and am well-aware of the risks, I tend to think that the Singularity is the #1 or #2 threat to my young children's lives, together with nuclear war. &nbsp;</p><p>I also can't help but wonder what jobs they will be able to find on the job market that aren't yet taken over by AI, by the time they graduate from college in 20 years or more.<br><br>I wish my fears were unfounded, but I'm well acquainted with the various dangers of both x-risks and s-risks associated with unaligned, hacked, or corrupted AGI. &nbsp;I help run a startup called Preamble which works to reduce AGI s-risk and x-risk, and as part of our civic engagement efforts I've spent some years working with folks in the US military to raise awareness about AGI x-risks, especially those associated with 'Skynet' systems (hypothetical systems called Nuclear Command Automation systems, which would be deeply stupid to ever build, even for the nation that built them). &nbsp;The author of the following article, Prof. Michael Klare, is a good friend, and he sought my advice while he was planning this piece, so it represents a good synthesis of our views: <a href=\"https://www.armscontrol.org/act/2020-04/features/skynet-revisited-dangerous-allure-nuclear-command-automation\">https://www.armscontrol.org/act/2020-04/features/skynet-revisited-dangerous-allure-nuclear-command-automation</a> &nbsp;He and I, along with other friends and allies of ours, have recently been grateful to see that some of our multi-year, long-shot civic engagement efforts have borne fruit! &nbsp; Most exciting are these two US government statements:<br>&nbsp; &nbsp;(1) &nbsp;In March 2021, the National Security Commission on AI (NSCAI) included a couple lines in their official Report to Congress which, for the first time, briefed Congress about the importance of value alignment technology as a field of technology, and one which the US should invest in as a way to reduce AGI risk: &nbsp;\"Advances in AI, including the mastery of more general AI capabilities along one or more dimensions, will likely provide new capabilities and applications. Some of these advances could lead to inflection points or leaps in capabilities. Such advances may also introduce new concerns and risks and the need for new policies, recommendations, and technical advances to assure that systems are aligned with goals and values, including safety, robustness and trustworthiness. The US should monitor advances in AI and make necessary investments in technology and give attention to policy so as to ensure that AI systems and their uses align with our goals and values.\"<br>&nbsp; &nbsp;(2) &nbsp;In Oct 2022, the Biden administration's 2022 Nuclear Posture Review (NPR) became the first ever statement by the US Federal government explicitly prohibiting any adoption of Nuclear Command Automation by the US: &nbsp;\"In all cases, the United States will maintain a human \u201cin the loop\u201d for all actions critical to informing and executing decisions by the President to initiate and terminate nuclear weapon employment.\"<br><br>I'm extremely grateful that the US has finally banned Skynet systems! &nbsp;Now, we at Preamble and in the arms control community are trying to find allies within China so as to convince them to make a similar ban of Skynet systems in their jurisdiction. &nbsp;That would also open the door for our nations to have a dialogue on how to avoid being tricked into going to war, by an insane terrorist group using cyberattacks and misinformation to cause what is called a Catalytic Nuclear War (a war that neither side wanted, that was caused by trickery from a 3rd \"catalytic\" party). &nbsp;https://mwi.usma.edu/artificial-intelligence-autonomy-and-the-risk-of-catalytic-nuclear-war/<br><br>All of us in the AGI safety community are working hard to prevent bad outcomes, but it feels like the years are starting to slip away frighteningly quickly on what might be the wick of the candle of human civilization, if we don't get 1,000 details right to ensure everything goes perfectly according to plan when the superintelligence is born. &nbsp;Not only do we have to solve AI alignment, but we also have to perfectly solve software and hardware supply chain security; otherwise we can't trust the software to actually do what the pixels on the screen say that the source code says. &nbsp;http://www.cs.cmu.edu/~rdriley/487/papers/Thompson_1984_ReflectionsonTrustingTrust.pdf<br><br>I'm sorry if I'm rambling but I just wanted to convey an overall sense and impression of an emotion and see if others were feeling the same. &nbsp;I dread that our civilization is hurtling at 100MPH towards an impassable cliff, and it's starting to give me a sense of visceral fear. &nbsp;It really does seem like OpenAI, and the companies they are inspiring, are flooring the gas pedal and I was just wondering if anyone else is feeling scared. &nbsp;Thank you.</p>", "user": {"username": "JonCefalu"}}, {"_id": "p9bmHm7GMiDiwdhSk", "title": "New version of Mental Health Navigator website", "postedAt": "2023-01-08T21:37:47.089Z", "htmlBody": "<p>The <a href=\"https://www.mentalhealthnavigator.co.uk/\">Mental Health Navigator website</a> has been recently redesigned to be more navigable. We hope this makes it easier for everyone to find mental health resources via our website!</p><h2>Key new features</h2><p>Key new features include an <a href=\"https://www.mentalhealthnavigator.co.uk/about-us\">'About Us'</a> page, one section for <a href=\"https://www.mentalhealthnavigator.co.uk/resources\">mental health resources</a>, one section for our <a href=\"https://www.mentalhealthnavigator.co.uk/advisory-service\">Advisory Service</a> (formerly referred to as our 'consultations service' and the 'Navigation Service'), and our <a href=\"https://www.mentalhealthnavigator.co.uk/news\">'News'</a> page. We also now have a search bar to make it easier for you to find what you're looking for on the website.</p><h2>New home for the Therapists &amp; Coaches in EA lists</h2><p>While reorganizing our website, we decided to move the lists of Coaches and Therapists previously provided in their own subsection of the website to the AirTable on <a href=\"https://www.mentalhealthnavigator.co.uk/providers\">our Providers page</a>. You can now filter for everyone who was on those lists by choosing \"Effective Altruists\" via the \"Populations they specialize in working with\" condition.</p><p>Our reason for making this change was to centralize lists of providers and provide a way for indicating where therapists and coaches familiar with Effective Altruism are based and what their jurisdictions of practice are, and in order to provide a place for reviews of them as well. We will therefore no longer be adding people to a Therapists &amp; Coaches in EA list.&nbsp;</p><p>However, if you would like to be added to our Providers table, please fill out <a href=\"https://airtable.com/shrgNBqF67YHnitku\">this form</a>. Please note that you cannot provide reviews for yourself, and we discourage you from asking your clients to provide reviews, as we prefer that they add reviews of their own accord.</p><h2>Change of domain name</h2><p>When visiting the website, you may notice a change to the domain name. We're now online as <a href=\"https://www.mentalhealthnavigator.co.uk/.\">https://www.mentalhealthnavigator.co.uk/.</a> However, <a href=\"https://eamentalhealthnavigator.com/\">https://eamentalhealthnavigator.com/</a> will still lead you to our website.</p><h2>Contacting us</h2><p>While we're still a work in progress, we welcome feedback anytime. This can be provided either through <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSeiWzJuTDEZQaAkwMD2Nqxc1BaHfvjaKORU6DYaYLpmsWKS0g/viewform\">our feedback form</a> or by reaching out to us at <a href=\"mailto:info@mentalhealthnavigator.co.uk\">info@mentalhealthnavigator.co.uk</a> .</p>", "user": null}, {"_id": "JDoS3FbhNvonQH8nm", "title": "Halifax Monthly Meetup: Moloch in the HRM", "postedAt": "2023-01-08T14:51:30.514Z", "htmlBody": "<p>In the interest of growing the LW/EA/ACX community in Halifax, we are hosting meetups on the last Saturday of each month. Old hands and newcomers are welcome!&nbsp;</p><p>We'll begin by discussing Moloch in the HRM: Our ongoing coordination problems around the housing crisis/tax allocation/etc. After that we'll shift to general, unstructured discussion.</p><p>Location: Seven Bays Cafe (2017 Gottingen Street) \u2013 <a href=\"https://plus.codes/87PRMC29+9C\">87PRMC29+9C</a></p><p>Join us at Seven Bayes!</p><p>Contact: <a href=\"mailto:conorbarnes93@gmail.com\">conorbarnes93@gmail.com</a></p>", "user": {"username": "Ideopunk"}}, {"_id": "dZs5s8giJ36qtSq4h", "title": "Dangers of deference\n", "postedAt": "2023-01-08T14:41:58.150Z", "htmlBody": "<p>*[Written September 02, 2022. Note: I'm likely to not respond to comments promptly.]*</p><p>Sometimes people defer to other people, e.g. by believing what they say, by following orders, or by adopting intents or stances. In many cases it makes sense to defer, since other people know more than you about many things, and it's useful to share eyes and ears, and coordination and specialization are valuable, and one can \"inquisitively defer\" to opinions by taking them as challenges to investigate further by trying them out for oneself. But there are major issues with deferring, among which are:</p><p>* Deferral-based opinions don't contain the detailed content that generated the opinions, and therefore can't direct action effectively or update on new evidence correctly.&nbsp;<br>* Acting based on deferral-based opinions is discouraging because it's especially not the case that the whole of you can see why the action is good.&nbsp;<br>* Acting based on deferral-based opinions to some extent removes the \"meaning\" of learning new information; if you're just going to defer anyway, it's sort of irrelevant to gain information, and your brain can kind of tell that, so you don't seek information as much. Deference therefore constricts the influx of new information to individuals and groups.<br>* A group with many people deferring to others will amplify [information cascades](https://en.wikipedia.org/wiki/Information_cascade) by double-triple-quadruple-counting non-deferral-based evidence.<br>* A group with many people deferring to others will have mistakenly correlated beliefs and actions, and so will fail to explore many worthwhile possibilities.<br>* The deferrer will copy beliefs mistakenly imputed to the deferred-to that would have explained the deferred-to's externally visible behavior. This pushes in the direction opposite to science because science is the way of making beliefs come apart from their pre-theoretical pragmatic implications.<br>* Sometimes the deferrer, instead of imputing beliefs to the deferred-to and adopting those beliefs, will adopt the same model-free behavioral stance that the deferred-to has adopted to perform to onlookers, such as pretending to believe something while acting towards no coherent purpose other than to maintain the pretense.&nbsp;<br>* If the deferred-to takes actions for PR reasons, e.g. attempting to appear from the outside to hold some belief or intent that they don't actually hold, then the PR might work on the deferrer so that the deferrer systematically adopts the false beliefs and non-held intents performed by the deferred-to (rather than adopting beliefs and intents that would actually explain the deferred-to's actions as part of a coherent worldview and strategy).<br>* Allocating resources based on deferral-based opinions potentially opens up niches for non-epistemic processes, such as hype, fraud, and power-grabbing.<br>* These dynamics will be amplified when people choose who to defer to according to how much the person is already being deferred to.&nbsp;<br>* To the extent that these dynamics increase the general orientation of deference itself, deference recursively amplifies itself.</p><p>Together, these dynamics make it so that deferral-based opinions are under strong pressure to not function as actual beliefs that can be used to make successful plans and can be ongoingly updated to track reality. So I recommend that people</p><p>* keep these dynamics in mind when deferring,&nbsp;<br>* track the difference between believing someone's testimony vs. deferring to beliefs imputed to someone based on their actions vs. adopting non-belief performative stances, and&nbsp;<br>* give substantial parliamentary decision-weight to the recommendations made by their expectations about facts-on-the-ground that they can see with their own eyes.</p><p>Not to throw away arguments or information from other people, or to avoid investigating important-if-true claims, but to *think as though thinking matters*.&nbsp;<br>&nbsp;</p>", "user": {"username": "TsviBT"}}, {"_id": "fxDhSN5qJfYai5zs9", "title": "Moral Weights according to EA Orgs", "postedAt": "2023-01-08T16:46:18.771Z", "htmlBody": "<p>This post was motivated by SoGive's moral weights being (to a first check) quite different to Founders Pledge (FP) and Happier Lives Institute (HLI). Upon checking in more detail, this appears to be the largest discrepency across any organisation. (Although we are still waiting to find out many missing values in the grid as HLI's research is ongoing).</p><h2>Summary</h2><figure class=\"table\"><table><thead><tr><th style=\"padding:2px 3px;vertical-align:bottom\">&nbsp;</th><th style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">GiveWell</th><th style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">FP</th><th style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">HLI</th><th style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">SoGive</th></tr></thead><tbody><tr><th style=\"padding:2px 3px;vertical-align:bottom\">WELLBY</th><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">-</td><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">0.53</td><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">0.55</td><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">-</td></tr><tr><th style=\"padding:2px 3px;vertical-align:bottom\">1 income doubling for 1 year</th><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">1</td><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">1</td><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">1&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftijw0nigiih\"><sup><a href=\"#fntijw0nigiih\">[1]</a></sup></span></td><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">1</td></tr><tr><th style=\"padding:2px 3px;vertical-align:bottom\">preventing 1 year of severe depression</th><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">~1.51<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefi5mgrlkwimc\"><sup><a href=\"#fni5mgrlkwimc\">[2]</a></sup></span>&nbsp;(*)</td><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">1.28<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref29jj7q38qkn\"><sup><a href=\"#fn29jj7q38qkn\">[3]</a></sup></span>&nbsp;</td><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">0.71-1.42<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefs1nxhh9uxyo\"><sup><a href=\"#fns1nxhh9uxyo\">[4]</a></sup></span>&nbsp;</td><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">4</td></tr><tr><th style=\"padding:2px 3px;vertical-align:bottom\">1 additional year of life</th><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">2.30<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref75qnh3jcs0n\"><sup><a href=\"#fn75qnh3jcs0n\">[5]</a></sup></span></td><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">1.95</td><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">-2.8 to 2.91&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7ba9ozgw5nj\"><sup><a href=\"#fn7ba9ozgw5nj\">[6]</a></sup></span>&nbsp;(*)</td><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">-</td></tr><tr><th style=\"padding:2px 3px;vertical-align:bottom\">preventing 1 death under 5</th><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">117.7<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0qm0789i3ut\"><sup><a href=\"#fn0qm0789i3ut\">[7]</a></sup></span></td><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">123.2</td><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">-192 to 200&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref126biccb9xdp\"><sup><a href=\"#fn126biccb9xdp\">[8]</a></sup></span></td><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">100</td></tr><tr><th style=\"padding:2px 3px;vertical-align:bottom\">preventing 1 death over 5</th><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">83.6<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0qm0789i3ut\"><sup><a href=\"#fn0qm0789i3ut\">[7]</a></sup></span></td><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">83.7</td><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">-67 to 70</td><td style=\"padding:2px 3px;text-align:center;vertical-align:bottom\">100</td></tr></tbody></table></figure><p>Broadly all organisations (with the exception of SoGive's view on depression) are very much aligned.</p><p>(*) means I expect the organisation would <i>not</i> endorse the figures used here. In the case of GiveWell my best guess is this is roughly inline with what they would use. For Happier Lives Institute it is an upper bound I expect they will be far below when they finish their research.</p><h2>Details</h2><h3>Open Philanthropy</h3><p><a href=\"https://www.openphilanthropy.org/research/technical-updates-to-our-global-health-and-wellbeing-cause-prioritization-framework/#3-new-moral-weights\">Open Phil's summary</a> of their moral weights is very clear and interesting, but:</p><blockquote><p>For now, in order to be more consistent in our practices, we\u2019re going to defer to GiveWell and start to use the number of DALYs that would be implied by extrapolating their moral weights.</p></blockquote><p>I have left them off here, as I would just be duplicating the GiveWell numbers.</p><h3>GiveWell</h3><p>GiveWell's weights are sourced from <a href=\"https://docs.google.com/document/d/1hOQf6Ug1WpoicMyFDGoqH7tmf3Njjc15Z1DGERaTbnI/edit#\">here</a>. I have made a few small calculations to align these numbers with the other orgs.</p><h3>Founders Pledge</h3><p>Founders Pledge's moral weights are avaiable <a href=\"https://docs.google.com/document/d/1VZ8VGIy3bZELUZ6Xwz7uPBEawvzFYX83m5Vn7iUHn-s/edit#\">here</a>.</p><h3>Happier Lives Institute</h3><p>Unfortunately, their moral weights are still in the process of being generated. You can determine the range of weights they will use in future in their article <a href=\"https://www.happierlivesinstitute.org/report/the-elephant-in-the-bednet/\">The elephant in the bednet</a>.</p><h3>SoGive</h3><p>SoGive's weights can be found <a href=\"https://thinkingaboutcharity.blogspot.com/2021/04/sogives-gold-standard-benchmarks.html\">here</a>. I have used them verbatim&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntijw0nigiih\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftijw0nigiih\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is a calculation. A 100% increase in income/consumption is worth 1.27 / 0.69 = 1.86 WELLBYs (in HLI terms). (<a href=\"https://docs.google.com/spreadsheets/d/1RrBuiPVgL-t8hlr6EqkqABiaqdHMGkpvfeiqiiX49LU/edit#gid=309035308\">See inputs tab C25</a>) We want this to be 1-unit, so we take 1/1.86 = 0.55 to be a WELLBY and other numbers are calculated from this.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fni5mgrlkwimc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefi5mgrlkwimc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>GiveWell has a strong aversion to disability weights used blindly, so take this number with a grain of salt.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn29jj7q38qkn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref29jj7q38qkn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Founders Pledge don't explicitly include depression in their data. I have used the disability weights they used in their <a href=\"https://docs.google.com/spreadsheets/d/1kPqBeHN3b7tq9gxho-9M7VGCuO-14CCI1YEsywmy66E/edit#gid=1907596966\">public CEA of StrongMinds</a>. I am under the impression they are working to move towards HLI's model for this.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fns1nxhh9uxyo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefs1nxhh9uxyo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is also a calculation. HLI are inconsistent in how they calculate the impact of depression in WELLBYs. <a href=\"https://www.happierlivesinstitute.org/report/global-priority-mental-health/#21_Mental_health_disorders_and_subjective_well-being\">Here</a> they say depression is worth 1.3 WELLBYs. (So 1.3 * 0.55 = 0.71) in units of income doubling. One potential explanation is that \"depression\" is less severe than \"severe depression\" so potentially this number could be doubled - they estimate the effect of StrongMinds to be ~1.8 WELLBYs)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn75qnh3jcs0n\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref75qnh3jcs0n\">^</a></strong></sup></span><div class=\"footnote-content\"><p>GiveWell uses a metric \"Years lived with disease/disability\" which as far as I can tell is equivalent to \"value of averting 1 year of death\".</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7ba9ozgw5nj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7ba9ozgw5nj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>As mentioned above HLI are still in the process of deciding what their moral weights are. I am taking the upperbound of their deprevationist model, the highest number it could be. The highest number is a deprevationist model of losing 4.95 WELLBY. (4.95 - 0). The lowest number is the same model using a neutral point of 10. \"would seem unintuitive to most, but relates to tranquilism and minimalist axiologies\" (<a href=\"https://docs.google.com/spreadsheets/d/1RrBuiPVgL-t8hlr6EqkqABiaqdHMGkpvfeiqiiX49LU/edit#gid=309035308\">See inputs tab C18</a>)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0qm0789i3ut\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0qm0789i3ut\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I have taken the average of \"death averted from malaria\" and \"death averted from vitamin A\". The numbers are similar and I don't think material to the analysis here.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn126biccb9xdp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref126biccb9xdp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Using life-expectancy of 70.16, average age of death of under 5s of 1.54 and average age of death of over 5s of 46.06. (Numbers via HLI's sheet <a href=\"https://docs.google.com/spreadsheets/d/1RrBuiPVgL-t8hlr6EqkqABiaqdHMGkpvfeiqiiX49LU/edit#gid=189909444\">\"GiveWell Numbers\"</a>). Method suggested by <a href=\"https://forum.effectivealtruism.org/posts/fxDhSN5qJfYai5zs9/moral-weights-according-to-ea-orgs?commentId=6oFDMyKPCLrtJbdjk\">Joel</a>.</p></div></li></ol>", "user": {"username": "Simon_M"}}, {"_id": "3Cm7jFAxiNAKbQTki", "title": "Should UBI be a top priority for longtermism?", "postedAt": "2023-01-08T12:45:11.394Z", "htmlBody": "<p>It seems like although there is very little debate around UBI in the EA movement, a lot of EAs I've spoken with are on board with the idea that Universal Basic Income (UBI) could be the logical foundation for human civilization. By logical foundation, I mean the optimal safety net and stabilizing force underpinning a successful economic organizing system long-term.&nbsp;</p><p>Advocating for UBI <a href=\"https://forum.effectivealtruism.org/posts/5XeCA5gKbMakAskLy/effective-altruists-love-systemic-change\">is definitely seeking systemic change</a>, and for the purposes of this post, I'll stick to UBI as an economic policy (one that has proponents from both the left and right).</p><p>There's also a near-term cost-effectiveness argument for UBI: GiveDirectly is running UBI trials in the poorest places in the world using philanthropic - not tax- dollars. Unlike in developing countries, where niche health interventions such as the ones GiveWell researches can greatly exceed the lives saved via cash transfers, it is currently highly plausible that cash transfers are the most cost-effective way to spend aid money in developed countries. As such, we could accomplish exceptionally high impact if we <i>combine philanthropic guaranteed income at scale with advocacy for systematic change <strong>in developed countries</strong>. </i>I'm working on this topic specifically in another forum post.</p><h2>The answer to, \"Should UBI be a top priority for longtermism?\" has two sub-questions.</h2><h3>- Is UBI actually such a great idea?&nbsp;</h3><p>Cash transfers are&nbsp;<a href=\"https://basicincome.stanford.edu/experiments-map/\"><u>the most widely researched intervention in the world</u></a>, and also the&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/cash-transfers\"><u>most consistently positive intervention</u></a>. GiveWell states that \"<a href=\"https://www.givewell.org/international/technical/programs/cash-transfers\">Cash transfers have the strongest track record we've seen for a non-health intervention, and are a priority program of ours.</a>\"</p><p>UBI is basically a population-wide cash transfer program of the government instead of some nonprofit. Although I haven't seen much discussion of UBI in EA, the Guaranteed Income Movement has a <a href=\"https://twitter.com/scottsantens/status/1040683616488103936\">staggering amount of good research</a> pointing towards UBI (guaranteed income) as <a href=\"https://twitter.com/theUBIguy/status/1317158974299557889\">an incredibly worthwhile policy with few downsides</a>. I consider myself a part of both movements and think we should - at the very least - try to apply EA cost-benefit analysis to UBI.</p><p>Here goes.</p><p><a href=\"https://economicsecurityproject.org/work/guaranteed-income/\"><strong>50% </strong>of Americans don\u2019t have $400 in cash on hand to deal with an emergency</a>.</p><ul><li>50% of people in the U.S. = <strong>165M</strong></li><li>Stress, depression, lack of hope, and other problems (versus the counterfactual) from living in poverty likely contribute to, (spitballing here), <strong>0.2 WELLBYs</strong> (or QALYs - arguments exist for both) less per year.</li><li>165M &nbsp;People (in the U.S. alone) * 0.2 WELLBYs = <strong>33 Million more WELLBYs/Yr</strong></li><li>I'm not sure how far exactly we can extrapolate these benefits, but this is only a fraction of the potential benefits over short or longtermist time horizons.</li></ul><p>What about the downsides?</p><ul><li>The primary losers of UBI policy would be (extremely) wealthy people and people with very high incomes as they will get higher tax rates.</li><li>In 2022, 34.4% of American households saw a $100,000+ income. It would be reasonable to say households with over $100,000 annual income could probably be negatively affected by increased taxes.&nbsp;</li><li>34.4% * 332M = <strong>115M Americans</strong></li><li>They will likely lose some amount of WELLBYs as their lifestyles will be harder to maintain. Increased taxes, the resulting stress, and a slight decline in living standards could likely contribute to, (spitballing again because living with slightly less affluence isn't the same as living in poverty), a loss of <strong>0.05 WELLBYs</strong> per year. I think this could be a massive overestimation though because although some people would have higher taxes, they would also benefit from their friends, family, and neighbors being much more economically secure.&nbsp;</li><li>I think it's more likely only people with over $500K in annual income would be negatively affected (and only as long as their family &amp; friends are also in the same tax bracket). 1% of American households make 500K+ annually. The 1% comprises 1.32 Million Americans.&nbsp;</li><li>115M * 0.05 = <strong>6M less WELLBYs/Yr</strong></li><li>1.32M * 0.05 <strong>= 66K less WELLBYs/Yr</strong></li></ul><p>Based on this back-of-the-napkin cost-benefit estimation, it seems like the benefits by far outweigh the costs. Abolishing poverty could save something like 27 Million WELLBYs per year in the U.S. alone. (note this is only meant to be a directionally accurate ballpark estimate).</p><ul><li>33M - 6M = <strong>27 M net positive WELLBYs annually</strong></li></ul><p>&nbsp;</p><p><strong>Here are my general thoughts about the potential upsides (and concerns) I have about UBI &amp; longtermism.</strong></p><p>If money remains a thing and capitalism continues to look like the most efficient system for getting people the things they want, then guaranteed income should be one of the fundamental positives in the long-term future of humanity. Global guaranteed income could be an important part of permanently ending resource-based conflict and enabling humans to become more aligned.</p><p>In the same vein, establishing UBI as a human right could make it much harder for a (non-AI) actor to enslave humanity forever, reducing several S-risks. Given the dispersed power that UBI distributed evenly across society, people would be much more prepared for and highly resilient to malicious actors looking to entrench power.</p><p>If the disruption enabled by guaranteed income is inevitable, we could have a large impact by accelerating the disruption while ensuring that guaranteed income isn\u2019t used for negative purposes. Dictatorial governments could, for example, tie social credit scores to the amount of a basic income. In the wrong hands, guaranteed income could be an extremely powerful tool to entrench social stratification. We can help to ensure that the growth of basic income is safe, effective, and dignified.</p><h3>Is there a sound strategy for cost-effectively getting it enacted into policy?</h3><p>There are already <a href=\"https://gicp.info/\">several organizations working to move guaranteed income into policy</a>. I also think that many of these organizations would be capable of deploying extra funding in ways that would accelerate guaranteed income policy.&nbsp;</p><p>In addition to the policy-focused side of things, my team and I at <a href=\"https://forum.effectivealtruism.org/posts/HYRfAGwNc6jzKkWt7/introducing-the-logical-foundation-an-ea-aligned-nonprofit\">The Logical Foundation</a>, have found what we believe to be an extremely neglected opportunity to build highly impactful philanthropically funded guaranteed income trials with the side benefit of growing public awareness and support for guaranteed income policy.</p><p>While I don't think that we should shift significant resources from AI research or mitigating biological risks (among other priorities) to UBI advocacy, I do believe that 2 kinds of support for UBI could be worthwhile.</p><ol><li>People who don't have the technical skills to contribute to AI or Bioscience research may find working on UBI to be highly impactful and fit better with their skill sets.</li><li>Capacity building funding for UBI organizations (and 80,000 hours job recommendations), or funding that can not be spent on other top priorities for whatever reason (say the donor wants it spent in their - developed - country or explicitly on helping people in the near term).</li></ol>", "user": {"username": "Michael Simm"}}, {"_id": "oZHBAzjpEfpuscDw8", "title": "Adding important nuances to \"preserve option value\" arguments ", "postedAt": "2023-01-08T09:30:56.985Z", "htmlBody": "<p><i>This is a quickly produced writeup of some thoughts that are probably (a) obvious to some people, (b) basically covered in some existing writings, but (c) still useful for <strong>some</strong> people to read.</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnf7y0grzy5\"><sup><a href=\"#fnnf7y0grzy5\">[1]</a></sup></span><i>&nbsp;After writing this, I re-read the 2017 post </i><a href=\"https://www.effectivealtruism.org/articles/hard-to-reverse-decisions-destroy-option-value\"><i>Hard-to-reverse decisions destroy option value</i></a><i> and concluded that that's more useful than this post and makes this post somewhat but not totally redundant; that post is consistent with but doesn't emphasise what I say here.</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1uf0j297kvf\"><sup><a href=\"#fn1uf0j297kvf\">[2]</a></sup></span></p><h1>Key takeaways</h1><p>I fairly commonly hear (and make) arguments like <strong>\"This action would be irreversible. And if we don't take the action now, we can still do so later. So, to preserve </strong><a href=\"https://forum.effectivealtruism.org/topics/option-value\"><strong>option value</strong></a><strong>, we shouldn't take that action, even if it would be good to do the action now if now was our only chance.</strong>\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5his0pc5gw\"><sup><a href=\"#fn5his0pc5gw\">[3]</a></sup></span></p><p>This is relevant to <a href=\"https://docs.google.com/document/d/1a5WyVNF0zdMZ-aQkuE3hfNl5LDZpMCv4LEaae1lbAVw/edit\">actions such as</a>:</p><ul><li>doing field-building to a new target audience for some important cause area</li><li>publicly discussing of some important issue in cases where that discussion could involve <a href=\"https://forum.effectivealtruism.org/topics/information-hazard\">information hazards</a>, make the issue <a href=\"https://forum.effectivealtruism.org/topics/political-polarization\">polarized</a>/partisan, or make our community seem wacky</li></ul><p>I think <strong>this sort of argument is often getting at something important, but in my experience such arguments are usually oversimplified in some important ways.</strong> This post is a quickly written attempt to provide a more nuanced picture of that kind of argument. My key points are:</p><ol><li><strong>\"reversibility\" is a matter of degree</strong> (not a binary), and <strong>is about the expected extent to which the counterfactual effects we're considering causing would (a) fade by default if we stop fuelling them, and/or (b) could be reversed by us if we </strong><i><strong>actively tried</strong></i><strong> to reverse them.&nbsp;</strong><ol><li>Sometimes we may be surprised to find that something does seem decently reversible.</li></ol></li><li><strong>The \"option value\" we retain is also a matter of degree</strong>, and we should bear in mind that <strong>delaying an action (a) often gradually reduces total benefits and (b) sometimes means missing key windows of opportunity.&nbsp;</strong></li><li><strong>Delaying can only be better than acting now if we expect we'll be able to make a better-informed decision later and/or we expect the action to become more net-positive later.&nbsp;</strong><ol><li>If we don't expect our knowledge will improve in relevant ways nor that the act will become more valuable/less harmful, or we expect minor improvements that are outweighed by the downsides or delay, we should probably just act now if the action does seem good.&nbsp;</li></ol></li></ol><p>But again, I still think \"option value\" arguments are often getting at something important; I just think we may often make better decisions if we <i>also </i>consider the above three nuances when making \"option value\" arguments. And, to be clear, <strong>I definitely still think it's often worth avoiding, delaying, or consulting people about risky-seeming actions rather than just taking them right now.</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwoqfhzxg16\"><sup><a href=\"#fnwoqfhzxg16\">[4]</a></sup></span><strong>&nbsp;</strong></p><h1>1. On \"irreversibility\"</h1><p>In some sense, all actions are themselves irreversible - if you do that action, you can never make it literally the case that you didn't do that action. But, of course, that doesn't matter. <strong>The important question is instead something like \"If we cause this variable&nbsp;to move from x to y, to what extent would our counterfactual impact remain even if we later start to wish we hadn't had that impact and we adjust our behaviors accordingly?\"</strong> E.g., if we make a given issue something that's known by and salient to a lot of politicians and policymakers, to what extent, in expectation, will that continue to be true even if we later realise we wish it wasn't true?&nbsp;</p><p>And this is really a question of degree, not a binary.&nbsp;</p><p>There are two key reasons why something may be fairly reversible:</p><ul><li><strong>Our counterfactual effects may naturally wash out</strong><ul><li>The variable may gradually drift back to the setting it was at before our intervention</li><li>Or it may remain at the setting we put it to, but with it becoming increasingly likely over time that that would've happened even in the absence of our intervention, such that our <i>counterfactual </i>impact declines</li><li>For example, let's say we raise the salience of some issue to politicians and policymakers because it seems ~60% likely that that's a good idea, ~20% likely it's ~neutral, and ~20% likely it's a bad idea. Then we later determine it seems it was a bad idea after all, so we stop taking any actions to keep salience high. In that case:<ul><li>The issue may gradually fall off these people's radars again, as other priorities force themselves higher up the agenda</li><li>Even if the issue remains salient or <i>increases </i>in salience, it could be that this or some fraction of it would've happened anyway, just on a delay<ul><li>This is likely for issues that gradually become obviously real and important and where we notice the issues sooner than other key communities do</li><li>We could imagine a graph with one line showing how salience of the issue would've risen by default without us, another line showing how salience rises earlier or higher if we make that happen, and a third line for if we take the action but then stop. That third line would start the same as the \"we make that happen\" line, then gradually revert toward the \"what would've happened by default\" line.</li></ul></li></ul></li></ul></li><li><strong>We may be able to actively (partially) reverse our effects</strong><ul><li>I expect this effect would usually be less important than the \"naturally wash out\" effect.<ul><li>Basically because when I tried to think of some examples, they all seemed either hard to achieve big results from or like they'd require \"weird\" or \"common sense bad\" actions like misleading people.&nbsp;</li></ul></li><li>But perhaps sometimes decently large effects could be achieved from this?&nbsp;</li><li>For example, we could try to actively <i>reduce </i>the salience of an issue we previously increased the salience of, such as by contacting the people who we convinced and who most started to increase the issue's salience themselves (e.g., academics who started publishing relevant papers), and explaining to them our reasoning for now thinking it's counterproductive to make this issue more salient.</li></ul></li></ul><h1>2. On \"we can still do it later\"</h1><p>In some sense, it's always the case that if you don't take an action at a given time, you can't later do <i>exactly that same action </i>or achieve <i>exactly the same effects </i>anymore. Sometimes this hardly matters, but sometimes it's important. <strong>The important question is something like \"If we don't take this action now, to what extent could we still achieve similar expected benefits with similarly low expected harms via taking a similar action later on?\"&nbsp;</strong></p><p><strong>I think very often significant value is lost by delaying net-positive actions. </strong>E.g., in general and all other factors held constant:</p><ul><li>delaying field-building will reduce the number of full-time-equivalent years spent on key issues before it's \"too late anyway\" (e.g., because an existential catastrophe has happened or the problem has already been solved)</li><li>delaying efforts to improve prioritization &amp; understanding of some issue will reduce the number of \"policy windows\" that occur between those efforts &amp; the time when it's too late anyway&nbsp;</li></ul><p><strong>I also think that sometimes delay could mean we miss a \"window of opportunity\" for taking an action with a similar type and balance of benefits to harms of the action we have in mind. </strong>That is, there may not just be a decay in the benefits, but rather a somewhat \"qualitative\" shift in whether \"something like this action\" is even on the table. For example, we may miss the one key policy window we were aiming to affect.</p><p>(Somewhat relevant: <a href=\"https://forum.effectivealtruism.org/posts/LD3mNJ367tSMna6WR/crucial-questions-about-optimal-timing-of-work-and-donations\">Crucial questions about optimal timing of work and donations</a>.)</p><h1>3. Will we plausibly have more reason to do it later than we do now?&nbsp;</h1><p><strong>Delaying can only be better than acting now if at least one of the following is true:</strong></p><ul><li><strong>We expect we'll be able to make a better-informed decision later</strong><ul><li>e.g., because our relevant knowledge will improve</li></ul></li><li><strong>We expect the action to become more net-positive later</strong><ul><li>e.g., because we expect favorable changes in background variables - the time will become \"more ripe\"</li></ul></li></ul><p>The more we expect those effects, the stronger the case for delay. The less we expect those effects, the weaker the case for delay. A simplified way of saying this is \"Why bother delaying your decision if you'd just later be facing the same or worse decision with the same or worse info?\"</p><p>This can be weighed up against the degree to which we should worry about irreversibility and the degree to which we should worry about the costs of delay, in order to decide whether to act now. (Assuming the act does seem net positive &amp; worth prioritizing, according to our current all-things-considered best guess.)</p><p>I think it's usually true that we'll (in expectation) be able to make a better-informed decision later, but <i>how </i>true that is can vary a lot between cases, and that magnitude matters if there are costs to delay.</p><p>I think it's sometimes true that the action will become more net-positive later, but probably usually the opposite is true (as discussed in the prior section). &nbsp;</p><p><i>I wrote this post in a personal capacity. &nbsp;</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnf7y0grzy5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnf7y0grzy5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>E.g., I haven't recently or extensively read how economists talk about option value, and it's totally plausible to me that these nuances are made quite clear in those writings. But even if so, this post could still be useful to readers who haven't read or have forgotten those writings.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1uf0j297kvf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1uf0j297kvf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I originally read the post in ~2019 and found it useful but later forgot its details. I then re-read it after drafting this post, then added this initial note about it and footnote 4 but otherwise left my post unchanged.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5his0pc5gw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5his0pc5gw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>A good explanation of this argument is <a href=\"https://www.effectivealtruism.org/articles/hard-to-reverse-decisions-destroy-option-value\">Hard-to-reverse decisions destroy option value</a>. That post <i>does </i>contain the three nuances this post covers. But in my experience people raising this sort of argument elsewhere seem to often be unaware of these nuances or to just not have them saliently in mind. (But I'm not claiming this is a <i>terrible </i>oversight - I suspect decently often these nuances wouldn't flip the decision anyway.)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwoqfhzxg16\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwoqfhzxg16\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Sometimes it's also possible to \"pilot\" a version of some risky, potentially hard-to-reverse action - i.e., to take a version of the action that has less upside but also is less risky or easier to reverse than the \"full\" action. A key reason to do that would be to gain more clarity on how high the upsides, downsides, and reversibility of the full action would be.</p></div></li></ol>", "user": {"username": "MichaelA"}}, {"_id": "jSJk9BPTCuHo7Acv7", "title": "Is this community over-emphasizing AI alignment?", "postedAt": "2023-01-08T06:23:12.062Z", "htmlBody": "<p>To be honest, I don't even really know what \"AI alignment\" is--and after skimming the wikipedia page on it, it sounds like it's a very broad term for a wide range of problems that arise at very different levels of abstraction--but I do know a smidgeon about machine learning and a fair amount about math and it seems like \"AI alignment\" is getting a ton of attention on this forum and loads of people here are trying to plan their careers to work on it.&nbsp;</p><p>Just wanted to say that there are a huge number of important things to work on, and I'm very surprised by the share of posts talking about AI alignment relative to other areas. Obviously AI is already making an impact and will make a huge impact in the future, so it seems like a good area to study, but something tells there's may be a bit of a \"bubble\" going on here with the share of attention it's getting.&nbsp;</p><p>I could be totally wrong, but just figured I say what occurred to me as an uneducated, outsider. And if this has already been discussed ad nauseam, no need to rehash everything.</p><p>Echoing my first sentence about different levels of abstraction, it may be worth considering if the various things currently going under the heading of AI alignment should be lumped together under one term. Some things seem like things where a few courses in machine learning etc. would &nbsp;be enough to make progress on them. Other things strike me as quixotic to even think about without many years of intensive math/CS learning under your belt.&nbsp;</p>", "user": {"username": "Lixiang"}}, {"_id": "CAyRADhaTrRDShoTe", "title": "A Different Take on What's Effective Altruism", "postedAt": "2023-01-08T02:27:51.877Z", "htmlBody": "<p>The linked-to \"short-short story\" is a thought experiment toward identifying optimally effective altruism.&nbsp;</p>", "user": {"username": "Marty Nemko"}}, {"_id": "Rnga2XRJzeYypyXDt", "title": "Learning as much Deep Learning math as I could in 24 hours", "postedAt": "2023-01-08T02:19:47.329Z", "htmlBody": "<p><strong>TL:DR</strong> I designed an experiment where I committed to spend two 12 hour days trying to learn as much deep-learning math as possible, basically from scratch.&nbsp;</p><p><strong>Table of Contents</strong></p><ol><li><a href=\"https://forum.effectivealtruism.org/posts/Rnga2XRJzeYypyXDt/i-tried-to-learn-as-much-deep-learning-math-as-i-could-in-24#Origins_and_Motivations\"><u>Origins and Motivations</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/Rnga2XRJzeYypyXDt/i-tried-to-learn-as-much-deep-learning-math-as-i-could-in-24#Results\"><u>Results</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/Rnga2XRJzeYypyXDt/i-tried-to-learn-as-much-deep-learning-math-as-i-could-in-24#Takeaways\"><u>Takeaways</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/Rnga2XRJzeYypyXDt/i-tried-to-learn-as-much-deep-learning-math-as-i-could-in-24#The_Experiment_Set_Up\"><u>Experiment set-up</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/Rnga2XRJzeYypyXDt/i-tried-to-learn-as-much-deep-learning-math-as-i-could-in-24#The_Curriculum_that_I_used\"><u>The Curriculum</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/Rnga2XRJzeYypyXDt/i-tried-to-learn-as-much-deep-learning-math-as-i-could-in-24#Documentation_on_Hours\"><u>Documentation on hours</u></a></li></ol><h1>Origins and Motivations</h1><p>For a long time, I\u2019ve felt intimidated by the technical aspects of alignment research. I had never taken classes on linear algebra or multivariable calculus or deep learning, and when I cracked open many AI papers, I was terrified by symbols and words I didn\u2019t understand.&nbsp;</p><p>7 months ago I wrote up a short doc about how I was going to remedy my lack of technical knowledge: I collected some textbooks and some online courses, and I decided to hire a tutor to meet a few hours a week. I had the first two weeks of meetings, it was&nbsp;<i>awesome</i>, then regular meetings got disrupted by travel, and I never came back to it.&nbsp;</p><p>When I thought about my accumulating debt of technical knowledge, my cached answer was \u201cOh, that might take six months to get up to speed.&nbsp;<i>I don\u2019t have the time.</i>\u201d</p><p><br>&nbsp;</p><p>Then, watching my productivity on other projects over the intervening months, I noticed two things:&nbsp;</p><ol><li>There appeared to be&nbsp;<i>massive</i> variance in my productivity. Sometimes, in a single day, I would get more done than I had accomplished in previous weeks.&nbsp;</li><li>I seemed to both&nbsp;<i>enjoy</i> and&nbsp;<i>get more done</i> by \u201csprinting\u201d through certain projects, eg. by spending 10 hours on it in a single day, rather than spreading that same work out over 2 hours a week for 5 weeks. It was, for some reason, way more motivating and seemingly more efficient to sprint.&nbsp;</li></ol><p><br>&nbsp;</p><p>Also, when I asked myself what I thought the main bottlenecks were for addressing my technical debt problem, I identified two categories:&nbsp;</p><ol><li>Time (I felt busy all the time, and was afraid of committing too much to one project)</li><li>A combination of lacking Motivation, Accountability and Fun</li></ol><p><br>&nbsp;</p><p>Then, as my mind wandered, I started to put 2 and 2 together: Perhaps these new things I had noticed about my productivity, could be used to address the bottlenecks in my technical debt? I decided to embark on an experiment:&nbsp;<i>how much technical background on deep learning could I learn in a single weekend?</i> My understanding of the benefits of this experiment were as follows:&nbsp;</p><ol><li>Committing \u201ca weekend\u201d felt like a much smaller time cost than committing \u201ca few months\u201d, even if they were the same number of hours.</li><li>No Distraction: I could design my environment to minimize distractions for two days, something it would be intractable to do to the same degree for several months.</li><li>\u201cTrying to learn as much as possible\u201d felt like a&nbsp;<i>challenge</i>. I was, to be honest, pretty scared. I didn\u2019t know what I was doing, &nbsp;it felt&nbsp;<i>extreme,&nbsp;</i>but that also made it&nbsp;<i>exciting and fun.</i></li><li>I had some historical data that I might be good at this kind of sprinting, and framing this as an&nbsp;<i>experiment</i> to see what I could learn about my productivity added another layer of discovery-driven motivation and fun. What if I learned more about how to be productive and get hard things done via this experiment?</li><li>As far as I knew, nobody else among my peers had done this - but I suspected that more people than me had the same problems, and that if I conducted this experiment, I might learn things that would be helpful to others, which added&nbsp;<i>yet another</i> layer of discovery-driven motivation and fun.&nbsp;</li><li>Accountability: Once I told somebody about this, it was hard to back out. It\u2019s way easier for them to monitor me for a weekend than for a few months.</li></ol><h1>Results</h1><ul><li>I\u2019d consider the experiment a success: I finished the whole curriculum in ~18 hours, and I got a lot of neat take-aways I\u2019ll go over below.&nbsp;</li><li>At the end of day 1, after 12 hours of cramming, I was too exhausted to explain what I had learned. However, the next morning, after reading my notes for 20 minutes, I was able to explain everything I had learned the night before. I haven\u2019t tested myself for retention yet, but I feel fairly confident that after glancing at my notes for 20 minutes, I\u2019ll be able to recall and use the information again, which is good enough for my practical purposes.&nbsp;</li><li>I ended up having to take more breaks than I expected because my brain just kinda felt fried at times. I would often just lay down on the floor and look at the ceiling. If I wasn\u2019t part of an experiment, I think these moments are likely when I would\u2019ve given up and done something else. But I noticed that after 10, 20 or 30 minutes, I was able to return to being productive, which is likely the moment when this experiment showed its worth.&nbsp;</li></ul><p><br>&nbsp;</p><h1>Takeaways</h1><ul><li>Experiments are&nbsp;<i>really useful</i>, as outlined by&nbsp;<a href=\"https://www.neelnanda.io/blog/the-value-of-experiments\"><u>Neel Nanda</u></a>.&nbsp;</li><li>There is no speed limit - the world appears to be not very optimized for speed, including past versions of me. Two days of&nbsp;<i>actually trying</i> allowed me to accomplish more on this particular goal than 7 previous months of cowardice had, and (maybe) about as much as a full semester college course would have.&nbsp;</li><li>A big part of optimizing the actions you take is deciding upon the optimal&nbsp;<i>sequence</i>. Learning seems to be a lot about setting up <i>the right&nbsp;sequence of inferential steps</i>; order matters a lot. If you skip the wrong step, it\u2019ll cost you.&nbsp;</li><li>Having someone with way more knowledge spend&nbsp;<i>half an hour</i> curating a list of videos, especially by pointing out the things which are&nbsp;<i>irrelevant and can be skipped</i>, &nbsp;saved me tens of hours, probably more.&nbsp;</li><li>People waste&nbsp;<i>a lot of time</i> on \u201cprerequisites\u201d for the things they want to do that&nbsp;<i>aren\u2019t actually real prerequisites.&nbsp;</i>Instead<i>,&nbsp;</i>you can<i>&nbsp;</i>try to directly do the thing you want, then notice where you get stuck, and go back to build just the prerequisites you notice are needed to overcome the stuck-ness. No&nbsp;<a href=\"https://www.lesswrong.com/posts/zBtKhGGvWDKcGj3S7/the-world-is-full-of-wasted-motion\"><u>wasted motion.&nbsp;</u></a></li><li>Explaining what you learn to someone is a great (and fun) way to actually test your knowledge, notice where you\u2019re lacking so you can efficiently direct further study, and get useful explanations on things you\u2019re struggling with.&nbsp;</li><li>Asking for help is hard. It\u2019s also really worth it. This experiment probably would not have worked without the advice of Thomas Larsen and Tamera Lanham. Their input of like ~3 total hours probably saved me tens of hours, which seems like a great utilitarian deal.&nbsp;</li><li>The Curricula I used was (only partially) optimized. It was still way better than I could\u2019ve come up with on my own, but a better curriculum is definitely possible.&nbsp;<ul><li>Thomas just came up with a list of concepts, looked through the 3 blue 1 brown series on them, marked the videos I could skip, then searched on youtube for other videos for the topics 3b1b didn\u2019t cover, chose the ones with the highest views/likes, and watched the first five minutes of a couple to see if he liked them. In total I think this cost him ~1 hour over two days.&nbsp;</li></ul></li></ul><h1>The Experiment Set-Up</h1><ul><li>I committed to spend 12 hours a day, for two days, doing whatever I could to learn about deep learning. I intended to optimize my breaks, to take meals while working, and generally try to ignore everything and everyone else not involved in the experiment...</li><li>I would occupy a room in the Lightcone offices with a whiteboard, and lock the door to minimize my friends distracting me.</li><li>Originally, the plan was to sprint through a python course and the fast.ai course and code a neural net by the end of the weekend.&nbsp;<ul><li>After a half hour talking with Tamera Lanham and Thomas Larsen, it became apparent that what better fit my goals was understanding the&nbsp;<i>math</i> behind neural networks, rather than the coding. This meant that two days before the experiment, I discarded the curricula I had. Thomas graciously spent 1 hour building me a curriculum.&nbsp;</li></ul></li><li>Originally, I was going to conduct this sprint with a friend, and with Thomas Larsen available to help us when stuck. The friend decided to do something else the night before, which in hindsight probably improved the efficiency of the experiment, because I could set my own pace very easily, and was less distracted by exciting-but-irrelevant-to-the-experiment conversation. In my original plan I was afraid of doing it alone, but I might actually recommend doing it that way.</li></ul><h1>The Curriculum that I used</h1><p><strong>Intro to deep learning</strong> (I kept returning to these videos throughout the experiment, rewatching and understanding slightly more)</p><ul><li>Watch 3blue1brown&nbsp;</li><li><a href=\"https://www.youtube.com/watch?v=aircAruvnKk\"><u>https://www.youtube.com/watch?v=aircAruvnKk</u></a></li></ul><p><br>&nbsp;</p><p><strong>Linear algebra</strong> (this took me 2hr 25 mins, and ~36 mins of breaks)</p><ul><li><a href=\"https://www.youtube.com/watch?v=kjBOesZCoqc&amp;list=PL0-GT3co4r2y2YErbmuJw2L5tW4Ew2O5B&amp;index=1\"><u>https://www.youtube.com/watch?v=kjBOesZCoqc&amp;list=PL0-GT3co4r2y2YErbmuJw2L5tW4Ew2O5B&amp;index=1</u></a></li><li>Skip:&nbsp;<ul><li>determinant</li><li>Inverse matrices&nbsp;</li><li>Cross products&nbsp;</li><li>Eigenvectors&nbsp;</li><li>Abstract vector spaces&nbsp;</li></ul></li><li>Watch all the others&nbsp;</li></ul><p><br>&nbsp;</p><p><strong>Calc 3</strong> (this took me 2hr 57 mins and ~50 mins of breaks)</p><ul><li>Do this:&nbsp;<a href=\"https://www.khanacademy.org/math/multivariable-calculus\"><u>https://www.khanacademy.org/math/multivariable-calculus</u></a></li><li>Only the section:&nbsp;<ul><li>'thinking about multivariable functions'&nbsp;</li><li>'derivatives of multivariable functions'&nbsp;<ul><li>Skip differentiating parametric curves, curvature, divergence, curl, laplacian, jacobian</li></ul></li></ul></li></ul><p><br>&nbsp;</p><p>- ResNets:&nbsp;<a href=\"https://www.youtube.com/watch?v=ZILIbUvp5lk\"><u>https://www.youtube.com/watch?v=ZILIbUvp5lk</u></a> (took me 18 mins)&nbsp;</p><p>- RNNs (optional):&nbsp;<a href=\"https://www.youtube.com/watch?v=_aCuOwF1ZjU\"><u>https://www.youtube.com/watch?v=_aCuOwF1ZjU</u></a> (took me 13 mins)&nbsp;</p><p>- Transformers:&nbsp;<a href=\"https://www.youtube.com/watch?v=4Bdc55j80l8&amp;t=609s\"><u>https://www.youtube.com/watch?v=4Bdc55j80l8&amp;t=609s</u></a></p><p>(<i>I spent like, two hours on the above video which ex-post was not great. I would recommend others choose a different explainer on Transformers. )</i></p><p><br>&nbsp;</p><p>- RL basics&nbsp;<a href=\"https://www.youtube.com/watch?v=JgvyzIkgxF0\"><u>https://www.youtube.com/watch?v=JgvyzIkgxF0</u></a> (took me 25 mins)</p><p>- policy gradients / ppo:&nbsp;<a href=\"https://www.youtube.com/watch?v=5P7I-xPq8u8&amp;t=318s\"><u>https://www.youtube.com/watch?v=5P7I-xPq8u8&amp;t=318s</u></a></p><p><i>I could not understand the above video after rewatching it several times (I think the curricula skipped some prerequisites for this) so I had to have Thomas Larsen walk me through it on his own for around an hour. Thanks Thomas!&nbsp;</i></p><p><br>&nbsp;</p><p>- RLHF:&nbsp; rob miles video:&nbsp;<a href=\"https://www.youtube.com/watch?v=PYylPRX6z4Q\"><u>https://www.youtube.com/watch?v=PYylPRX6z4Q</u></a> (took me 23 mins)</p><p><br>&nbsp;</p><h1>Documentation on Hours</h1><p>(I used&nbsp;<a href=\"https://toggl.com/?cq_src=google_ads&amp;cq_cmp=15031885853&amp;cq_term=&amp;cq_plac=&amp;cq_net=x&amp;cq_plt=gp&amp;utm_term=&amp;utm_campaign=%5BPM%5D+-+Region+1+-+Generic+-+Billable+Events+-+Time+Tracking+Done+Right&amp;utm_source=adwords&amp;utm_medium=ppc&amp;hsa_acc=8476800569&amp;hsa_cam=15031885853&amp;hsa_grp=&amp;hsa_ad=&amp;hsa_src=x&amp;hsa_tgt=&amp;hsa_kw=&amp;hsa_mt=&amp;hsa_net=adwords&amp;hsa_ver=3&amp;gclid=Cj0KCQiA4aacBhCUARIsAI55maGhkbMsuOJCHeHWZVr7sMB_z7Iwti-iIr1rqc1U_wbwKX0s8u57q4gaAhmBEALw_wcB\"><u>toggl track</u></a> to record my time, and was fairly happy with the software. However, I made many errors / didn\u2019t record breaks correctly, etc. So take these numbers with a grain of salt.)</p><p><br><br><br><br>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\" colspan=\"3\"><p><strong>Saturday</strong></p></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Video 1</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>28 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Started around 10:00am</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Video 1 Summarizing</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>15 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>???</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>7 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Video 2</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>29 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Break</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>4 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Video 2 Summarizing</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>20 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Video 3</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>13 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Break</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>9 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>White-Boarding</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>4 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Linnear Algebra - first four videos</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>58 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Cleaning up notes</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>7 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Chapter 4 Linnear Algebra</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>16 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>White boarding</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>14 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Three Dimensional Linnear Transformations</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>14 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>???</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>12 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Chapter 9</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>10 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>break</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>27 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Chapter 13</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>14 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Calculus?</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>3 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Backpropagation, Chapter 4</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>10 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>break</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>9 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Multivariable Calculus</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>1hr 6min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>meditation</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>9 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Multivariable Calculus</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>10 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>meditation</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>7 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Multivariable Calculus</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>26 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>break</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>25 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Multivariable Calculus</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>47 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Multivariable Calculus</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>28 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Watching Neural Nets Ch. 3 again</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>30 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>????</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>45 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Trying to explain and failing</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>30 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Ended around 8:45pm</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\" colspan=\"3\"><p><strong>Sunday</strong></p></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Rewatching Backpropagation</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>22 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Started around 11am</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Resnets Video</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>18 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>RNN's Video</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>13 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Transformers</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>21 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>break</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>5 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Transformers</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>36 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>RL Basics</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>25 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>break</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>23 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>More RL</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>23 min</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>Talking to Thomas about Transformers and Reinforcement Learning and PPO</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>120 mins</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Ended around 6pm</td></tr></tbody></table></figure><p><br>&nbsp;</p>", "user": {"username": "EverettSmith"}}, {"_id": "EJ5a2ApokQqGB98P8", "title": "David Krueger on AI Alignment in Academia and Coordination", "postedAt": "2023-01-07T21:14:48.345Z", "htmlBody": "<p><a href=\"https://www.davidscottkrueger.com/\">David Krueger </a>is an assistant professor at the University of Cambridge and got his PhD from Mila. His research group focuses on aligning deep learning systems, but he is also interested in governance and global coordination. He does not have an AI alignment research agenda per se, and instead tries to enable his <a href=\"https://www.davidscottkrueger.com/\">seven PhD students</a> to drive their own research.&nbsp;</p><p>I think this interview gives some interesting pointers towards how we should direct efforts into how to communicate AI Alignment to Machine Learning researcher, how to fund AI Alignment research and what is the perception of AI Alignment research like in Academia.</p><p>Below are some highlighted quotes from our conversation (available on <a href=\"https://youtu.be/3T7Gpwhtc6Q\">Youtube</a>, <a href=\"https://open.spotify.com/episode/1vvAKf8EBwErP5yGFRNoCT?si=1a28296cdfa94c01\">Spotify</a>, <a href=\"https://podcasts.google.com/feed/aHR0cHM6Ly9hbmNob3IuZm0vcy81NmRmMjE5NC9wb2RjYXN0L3Jzcw/episode/MzJlMzk4YTAtYmMzZC00MDVkLWIzMTAtNTZhMmM2ZDc2MTg0?sa=X&amp;ved=0CAUQkfYCahcKEwiI2sT3hY35AhUAAAAAHQAAAAAQAQ\">Google Podcast</a>, <a href=\"https://podcasts.apple.com/us/podcast/connor-leahy-eleutherai-conjecture/id1565088425?i=1000570841369\">Apple Podcast</a>). For the full context for each of these quotes, you can find the accompanying <a href=\"https://theinsideview.ai/david\">transcript</a>.</p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=bDMqo7BpNbk\"><div><iframe src=\"https://www.youtube.com/embed/bDMqo7BpNbk\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><h2>Building A Research Team, Not Following An Agenda</h2><blockquote><p>\"I think agenda is a very grandiose term to me. It's oftentimes, I think people who are at my level of seniority or even more senior in machine learning would say, \"oh, I'm pursuing a few research directions.\" And they wouldn't say, \"I have this big agenda.\" And so I think my philosophy or mentality, I should say, when I set up this group and started hiring people was like, <strong>let's get talented people. Let's get people who understand and care about the problem. Let's get people who understand machine learning. Let's put them all together and just see what happens</strong> and try and find people who I want to work with, who I think are going to be nice people to have in the group who have good personalities, pro-social, who seem to really understand and care and all that stuff.\" (<a href=\"https://theinsideview.ai/david#how-david-approaches-research-in-his-lab\">full context</a>)</p></blockquote><h2>On Coordination Between Academia And The Broader World</h2><blockquote><p>\"There's a lack of understanding and appreciation of the perspective of people in machine learning within the existential safety community and vice versa. And I think that's really important to address, especially because I'm pretty pessimistic about the technical approaches. <strong>I don't think alignment is a problem that can be solved. I think we can do better and better. But to have it be existentially safe, the bar seems really, really high and I don't think we're going to get there. So we're going to need to have some ability to coordinate and say let's not pursue this development path or let's not deploy these kinds of systems right now.</strong> And for that, I think to have a high level of coordination around that, we're going to need to have a lot of people on board with that in academia and in the broader world. So I don't think this is a problem that we can solve just with the die hard people who are already out there convinced of it and trying to do it.\" (<a href=\"https://theinsideview.ai/david#existential-safety-cant-be-ensured-without-high-level-of-coordination\">full context</a>)</p></blockquote><h2>Most of the risk comes from safety-performance trade-offs in the development and deployment process</h2><blockquote><p>\"A lot of people are worried about us under-investing in research and that's where the safety-performance trade-offs are most salient for them. But I'm worried about the development and deployment process. <strong>I think where most of the risk actually comes from is from safety-performance trade-offs in the development and the deployment process.</strong> For whatever level of research we have developed on alignment and safety, I think it's not going to be the case that those trade-offs just go away.\" (<a href=\"https://theinsideview.ai/david#most-of-the-risk-comes-from-safety-performance-trade-offs-in-developement-and-deployment\">full context</a>)</p></blockquote><h2>We Should Test Our Intuitions About Future AI Systems</h2><blockquote><p>\"This is something that's a really interesting research question and is really important for safety because people have very different intuitions about this. Some people have these stories where just through this carefully controlled text interaction, maybe we just ask this thing one yes or no question a day and that's it. And that's the only interaction it has with the world. But it's going to look at the floating point errors on the hardware it's running on. And it's somehow going to become aware of that. And from that it's going to reverse engineer the entire outside world and figure out some plan to trick everybody and get out. And this is the thing that people talk about on LessWrong classically.</p><p>We don't know how smart the superintelligence is going to be, so let's just assume it's arbitrarily smart, basically. And obviously, a lot of people take issue with that. It's not clear how representative that is of anybody's actual beliefs but there are definitely people who have beliefs more towards that end where they think that AI systems are going to be able to understand a lot about the world, even from very limited information and maybe in very limited modality. My intuition is not that way. <strong>The important thing is to test the intuitions and actually try and figure out at what point can your AI system reverse engineer the world or at least reverse engineer a distribution of worlds or a set of worlds that includes the real world based on this really limited kind of data interaction.</strong>\" (<a href=\"https://theinsideview.ai/david#language-models-have-incoherent-causal-models-until-they-know-in-which-world-they-are\">full context</a>)</p></blockquote>", "user": {"username": "mtrazzi"}}, {"_id": "aTMgGL3gwKt7j2ZJG", "title": "Overview of the Pathogen Biosurveillance Landscape ", "postedAt": "2023-01-09T06:05:58.685Z", "htmlBody": "<p><strong>Supervisor: </strong>Akhil Bansal</p><h1>Executive Summary&nbsp;</h1><p>Biosurveillance systems are needed to mitigate against three primary threats. First, the accidental release of pathogens from labs. Secondly, the deliberate release of pathogens from bad actors with malicious intent. Lastly, the natural occurrence of pathogens that have the potential to create epidemics and pandemics. A robust biosurveillance system would aim to identify and monitor new and known pathogens with the goal of reducing the severity of the spread of the infectious diseases. From the COVID-19 pandemic, it\u2019s clear that modern biosurveillance systems have the potential to improve tremendously. To do so, a better understanding of the bottlenecks biosurveillance systems face is needed.&nbsp;</p><p><strong>The report consists of two standalone parts.</strong> This is part I of the report that introduces the current pathogen biosurveillance landscape, with a focus on biosurveillance technologies. Part II of the report explores the potential and bottlenecks of three technologies commonly used for biosurveillance - Polymerase Chain Reaction (PCR), Loop-Mediated Isothermal Amplification (LAMP) and Metagenomics Sequencing.&nbsp;</p><p>Part I of this report will be most useful for you if you would like to be informed about the overall biosurveillance landscape, the different types of biosurveillance systems, and the main types of technologies used in these biosurveillance landscapes. <strong>If you would like to have a deeper understanding of PCR, LAMP, and metagenomics sequencing and how they can be applied in biosurveillance, please view&nbsp;</strong><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics\"><strong>Part 2</strong></a><strong> of the report.&nbsp;</strong></p><p>Throughout the project,&nbsp;we used a combination of empirical research and expert interviews to gather information. A list of the organizations that we have talked to is included in&nbsp;<a href=\"Appendix 2 - Acknowledgements\"><u>Appendix 2</u></a>. In total, we gathered the opinions of 25 experts and sourced over fifty peer-reviewed research papers.</p><p><strong>Chapter 1 - Prioritizing Biosurveillance Systems (</strong><a href=\"https://Chapter I - Introduction to Biosurveillance Systems\"><strong><u>link</u></strong></a><strong>)</strong></p><p>In the first chapter of the project, we developed a weighted factor model (<a href=\"https://docs.google.com/spreadsheets/d/1DGzXV4SADSsqgfOVS1trl-igcuNp8FdFGSGnDrF1SNk/edit?usp=sharing\"><u>details of the WFM</u></a>) to assess the robustness of the existing biosurveillance systems. The aim of the WFM is to prioritize a few biosurveillance systems that we believe are the most important. The WFM assessed ten aspects of a biosurveillance system, including its usefulness, feasibility and potential risks. For every surveillance system, participants could adjust the weighting of each criteria and assign a score to each criteria for the relevant surveillance system. The final WFM incorporated the output of seven experts. A higher score in the WFM model indicated a higher relevance of that biosurveillance system to our project.&nbsp;</p><figure class=\"table\" style=\"width:1000px\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:150px\"><p><strong>Types of&nbsp;</strong></p><p><strong>Surveillance Systems</strong></p></td><td style=\"border:1pt solid #000000;padding:5px;vertical-align:top\">PoP&nbsp;</td><td style=\"border:1pt solid #000000;padding:5px;vertical-align:top\">Clinical&nbsp;</td><td style=\"border:1pt solid #000000;padding:5px;vertical-align:top\">Environmental&nbsp;</td><td style=\"border:1pt solid #000000;padding:5px;vertical-align:top\">Animal</td><td style=\"border:1pt solid #000000;padding:5px;vertical-align:top\">Syndromic</td><td style=\"border:1pt solid #000000;padding:5px;vertical-align:top\">Digital</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Averaged score&nbsp;</strong></td><td style=\"background-color:#ea9999;border:1pt solid #000000;padding:5pt;vertical-align:top\">4.8</td><td style=\"background-color:#ea9999;border:1pt solid #000000;padding:5pt;vertical-align:top\">4.9</td><td style=\"background-color:#ea9999;border:1pt solid #000000;padding:5pt;vertical-align:top\">5.6</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">4.6</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">4.3</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">4.9*</td></tr></tbody></table></figure><p>&nbsp;</p><p>&nbsp;</p><p><strong>Table 1</strong> - Summary of results from the Weighted Factor Model. We identified Point-of-Person (PoP), Clinical and Environmental Surveillance systems as the most relevant for our project.&nbsp;<br>* Despite Digital Surveillance having a high score, we decided not to focus on it for the reason mentioned in insight 3 below.&nbsp;</p><p>Some further key insights gathered from the WFM are shown below and linked to further explanations.</p><p><a href=\"Insight 1 - Point-of-Person and Clinical Surveillance can benefit largely from technological developments\"><strong><u>Insight 1</u></strong></a><i>\u2013&nbsp;</i>Point-of-Person and Clinical Surveillance can benefit largely from technological developments</p><p><a href=\"Insight 2 \u2013 Environmental surveillance is a very promising area\"><strong><u>Insight 2</u></strong></a><i> \u2013&nbsp;</i>Environmental surveillance is a very promising area</p><p><a href=\"Insight 3 \u2013 The main challenges Syndromic and Digital Surveillance face are data- and operation-related\"><strong><u>Insight 3</u></strong></a><i>\u2013&nbsp;</i>The main challenges Syndromic and Digital Surveillance face are data- and operation-related</p><p><a href=\"Insight 4 \u2013 Zoonotic Surveillance cannot easily be used for early-stage human pathogen detection\"><strong><u>Insight 4</u></strong></a><strong>&nbsp;</strong><i>\u2013&nbsp;</i>Zoonotic Surveillance cannot easily be used for early-stage human pathogen detection</p><p>We identified a list of technologies commonly used in PoP, Clinical, and Environmental Surveillance across the sampling, concentration, extraction, detection and quantification stages. Due to the ten-week time constraint of the project, we narrowed our focus to Polymerase Chain Reaction (PCR), Loop-Mediated Isothermal Amplification (LAMP) and Metagenomic Sequencing. For each of these technologies, we reviewed their current states, limitations, and potential avenues for improvements.</p><p><strong>Chapter II - Overview of Technologies used in Biosurveillance&nbsp;</strong></p><p>In the second chapter, we categorized the main technologies used at different stages of biosurveillance, from sampling, concentration, extraction, detection, to quantification. The list of technologies were made after two weeks of empirical research and talking to experts. We also provided a framework to assess the importance of each of these technologies. However, we were unable to apply this criteria rigorously to assess the technologies we identified, primarily due to lack of information that we can gather. It was difficult to find experts to interview for each of these technologies in the time limit of our research fellowship, and we couldn\u2019t find existing research papers that ranked the importance of these technologies in the way we wanted.&nbsp;</p><p>In the end, we used a qualitative and intuitive assessment of how important these technologies were and decided on PCR, LAMP, and metagenomics sequencing. It would be useful to conduct a more rigorous review of these technologies if resources are available, as different conclusions may be reached.&nbsp;</p><p><strong>Acknowledgment</strong></p><p>We would like to thank <a href=\"https://www.cerifellowship.org/\">CERI (Cambridge Existential Risk Initiatives) </a>and the biosecurity team for helping us make this happen during the 2022 Summer Research Fellowship. In particular, we are extremely grateful for our supervisor Akhil Bansal and biosecurity lead Dewi Erwan.</p><hr><h1>Table of Contents</h1><p><a href=\"Chapter I - Introduction to Biosurveillance Systems\"><strong>Chapter I - Introduction to Biosurveillance Systems&nbsp;</strong></a></p><ul><li><a href=\"The Need for a Biosurveillance System\">The Need for a Biosurveillance System&nbsp;</a></li><li><a href=\"Different Types of Biosurveillance Systems\">Different Types of Biosurveillance Systems&nbsp;</a></li><li><a href=\"Prioritizing Surveillance Systems  \">Prioritizing Surveillance Systems&nbsp;</a><ul><li><a href=\"Insight 1 - Point-of-Person and Clinical Surveillance can benefit largely from technological developments\">Insight 1 \u2013 Point-of-Person and Clinical Surveillance can benefit largely from technological developments&nbsp;</a></li><li><a href=\"Insight 2 \u2013 Environmental surveillance is a very promising area\">Insight 2 \u2013 Environmental surveillance is a very promising area&nbsp;</a></li><li><a href=\"Insight 3 \u2013 The main challenges Syndromic and Digital Surveillance face are data- and operation-related\">Insight 3 \u2013 The main challenges Syndromic and Digital Surveillance face are data- and operation-related&nbsp;</a></li><li><a href=\"https://Insight 4 \u2013 Zoonotic Surveillance cannot easily be used for early-stage human pathogen detection\">Insight 4 \u2013 Zoonotic Surveillance cannot easily be used for early-stage human pathogen detection</a>&nbsp;</li></ul></li><li><a href=\"https://Bottlenecks of Surveillance Systems\">Bottlenecks of Surveillance Systems&nbsp;</a><ul><li><a href=\"https://Point of Person Surveillance\">I. Point of Person Surveillance&nbsp;</a></li><li><a href=\"https://Clinical Surveillance \">II. Clinical Surveillance&nbsp;</a></li><li><a href=\"https://Environmental Surveillance\">III. Environmental Surveillance&nbsp;</a></li></ul></li></ul><p><a href=\"https://Chapter II - Biosurveillance Technologies\"><strong>Chapter II - Biosurveillance Technologies&nbsp;</strong></a></p><ul><li><a href=\"https://Why Focus on Technological Bottlenecks\">Why Focus on Technological Bottlenecks&nbsp;</a></li><li><a href=\"https://Identifying Technologies\">Identifying Technologies&nbsp;</a></li><li><a href=\"https://Prioritizing Technologies \">Prioritizing Technologies&nbsp;</a><ul><li><a href=\"https://Criteria for Prioritization\">Criteria for Prioritization&nbsp;</a></li><li><a href=\"https://Prioritized technologies\">Prioritized technologies&nbsp;</a></li></ul></li></ul><p><a href=\"https://Appendices\"><strong>Appendices&nbsp;</strong></a></p><ul><li><a href=\"https://Appendix 1 \u2013 Criteria used for the Weighted Factor Model\">Appendix 1 \u2013 Criteria used for the Weighted Factor Model&nbsp;</a></li><li><a href=\"https://Appendix 2 - Acknowledgements\">Appendix 2 - Acknowledgements&nbsp;</a></li></ul><hr><h1>Chapter I - Introduction to Biosurveillance Systems</h1><h2>The Need for a Biosurveillance System</h2><p>In the twenty-first century, it\u2019s become increasingly apparent that humanity is in dire need of a robust biosurveillance system. A biosurveillance system is characterized by identifying novel and known pathogens in an environment, this includes living organisms such as plants and humans, water systems and more. Biosurveillance systems serve to monitor the spread of diseases across these populations and environments so they can be contained. There has been an increase in zoonotic infectious diseases in this century. Notably, the 2003 SARS Outbreak (WHO, 2015), the 2009 Swine Flu Pandemic (CDC, 2012), the 2012 Middle East Respiratory Syndrome Coronavirus outbreak (ECDC, 2021), the 2013-2016 Ebola epidemic (CDC, 2014), the 2015 Zika Virus epidemic (WHO, 2015) and recently the Covid-19 pandemic. As of the time of writing this report, the Monkeypox outbreak is increasingly worrying. In the future, the frequency and source of pandemics are likely to increase.&nbsp;</p><p>As labs work to better understand the underlying mechanisms of pathogen mutations, the possibility of accidental releases of highly transmissible and deadly pathogens through water drainage, air vents and other means become increasingly worrying. Furthermore, as more DNA synthesis companies enable the purchasing of synthetic DNA, it\u2019s become easier for bad actors to engineer pathogens that are highly transmissible and fatal. Engineered pandemics pose a serious threat to humanity and have the potential to be Global Catastrophic Biological Risks (GCBRs). The Center for Health Security defines a GCBR as a threat that \u201ccould<i> lead to sudden, extraordinary, widespread disaster beyond the collective capability of national and international governments and the private sector to control. If unchecked, GCBRs would lead to great suffering, loss of life, and sustained damage to national governments, international relationships, economies, societal stability, or global security\u201d&nbsp;</i>(Alexopulos, 2019)<i>.&nbsp;</i></p><p>A biosurveillance system is particularly useful in the early detection of new and known infectious diseases, predicting the spread of diseases and helping contain pathogens before they impair the lives of potentially millions of people. The ideal biosurveillance system would involve rapid early detection of novel and known pathogens allowing relevant actors to identify, contain and mitigate any biological risk. The implementation of a robust, integrated biosurveillance system in this decade will better prepare humanity for the emerging threats of the twenty-first century and onwards.&nbsp;</p><p>The recent Covid-19 pandemic furthers the imperative to develop robust biosurveillance systems to prepare humanity for future, more severe pandemics. There is increased interest in preventing pandemics due to the Covid-19 pandemic but this interest is diminishing. Therefore, immediate action must be taken by relevant actors soon. The next pandemic should not be humanity\u2019s wake-up call to better prepare for the future.&nbsp;</p><h2>Different Types of Biosurveillance Systems</h2><p>To improve upon current biosurveillance efforts, we must first understand the types of biosurveillance systems and their functions.</p><figure class=\"table\"><table><tbody><tr><td style=\"background-color:#ffffff;border:1pt solid #000000;padding:5pt;vertical-align:top;width:135px\"><strong>Type of Surveillance</strong></td><td style=\"background-color:#ffffff;border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Function&nbsp;</strong></td><td style=\"background-color:#ffffff;border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Currently Used Example</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Point of Person</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Diagnostics that do infectious disease detection at the patient level without further equipment.&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><a href=\"https://www.gov.uk/order-coronavirus-rapid-lateral-flow-tests\"><u>Covid-19 Rapid Lateral Flow Test</u></a></p><p>Uses a sample of bodily fluids to detect viral proteins without the use of complex laboratory equipment.&nbsp;<a href=\"https://www.gov.uk/order-coronavirus-rapid-lateral-flow-tests\"><u>&nbsp;</u></a></p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Clinical</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Diagnostics that do infectious disease detection at the point of care. This could involve taking a sample at the clinic, sending it back to the lab and receiving a result a week later.&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><a href=\"https://www.genome.gov/about-genomics/fact-sheets/Understanding-COVID-19-PCR-Testing\"><u>Covid-19 PCR Tests</u></a></p><p>A lab technique that enables the detection of viruses in a sample.&nbsp;</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Sentinel</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Monitoring infectious diseases among high vulnerability people such as health care workers, TSA agents, and livestock workers.&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><a href=\"https://osha.europa.eu/en/publications/alert-and-sentinel-systems-signaal-netherlandsbelgium\"><u>SIGNAAL</u></a></p><p>Belgium\u2019s Sentinel System. Aims to do early detection of Work Related Disease by allowing physicians to report diseases caused by a patient\u2019s occupation.&nbsp;</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Environmental</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Monitoring and detection of known and unknown pathogens in wastewater systems, and busy public spaces like airports or farms.&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><a href=\"https://www.cdc.gov/healthywater/surveillance/wastewater-surveillance/wastewater-surveillance.html\"><u>CDC COVID Wastewater Surveillance</u></a></p><p>Wastewater from sewershed collected and sent to laboratories for SARS-CoV02 testing</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Human / Animal Monitoring&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Monitoring and detection of pathogens in livestock, wild animals and humans through wearable devices and frequent sampling.&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><a href=\"https://www.who.int/health-topics/one-health#tab=tab_1\"><u>One Health</u></a></p><p>An integrated approach to monitor health of humans, domestic and wild animals, and the wider ecosystems.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Syndromic&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Monitoring of symptoms of infectious diseases before a diagnosis is made.&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><a href=\"https://covid-webflow.joinzoe.com/\"><u>ZOE Covid Study</u></a></p><p>Tracks symptomatic cases in the UK</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Digital</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Tools that involve people reporting cases to a larger database and extracting insights from these databases.&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><a href=\"https://www.canada.ca/en/public-health/services/diseases/coronavirus-disease-covid-19/covid-alert.html\"><u>COVID Alert App</u></a>&nbsp;</p><p>Users infected with Covid-19 report their location and app notifies people who were close by (Canada)</p></td></tr></tbody></table></figure><p><br>&nbsp;<strong>Table 2</strong> - Summary of the different biosurveillance systems and examples of used cases.</p><h2>Prioritizing Surveillance Systems&nbsp;</h2><p>We developed a&nbsp;<a href=\"https://www.charityentrepreneurship.com/weighted-factor-model#:~:text=WHAT%20IS%20THE%20WEIGHTED%20FACTOR%20MODEL,-%E2%80%8B&amp;text=Broadly%2C%20the%20process%20of%20creating,from%20three%20to%20twelve%20factors\"><u>Weighted Factor Model</u></a> (<a href=\"https://docs.google.com/spreadsheets/d/1DGzXV4SADSsqgfOVS1trl-igcuNp8FdFGSGnDrF1SNk/edit?usp=sharing\"><u>details of our model</u></a>) to assess and compare the robustness of the different surveillance systems described above. Our WFM included ten factors that include the feasibility and usefulness of the surveillance systems, as well as the quality of evidence that exists. This model is chosen primarily for its ability to incorporate many factors and produce a quantitative number for each of the surveillance systems for systematic comparisons. One potential drawback to the model is that some of the factors are rather subjective. To obtain more comprehensive results, we gathered responses from 7 experts in the field. The updated view is presented below.</p><p>The specific criteria used in the WFM are included in&nbsp;<a href=\"https://Appendix 1 \u2013 Criteria used for the Weighted Factor Model\"><u>Appendix 1</u></a>. Scores ranging from 1 to 7 are assigned for each factor for every surveillance system, and the total score is calculated based on the weightings assigned to each criterion.&nbsp;<strong>Point-of-Person</strong>,&nbsp;<strong>Clinical</strong>, and&nbsp;<strong>Environmental Surveillance</strong> scored the highest priority according to our WFM, therefore we will focus on these areas in the remainder of this report.</p><p><i><strong>Insight 1</strong> \u2013 Point-of-Person and Clinical Surveillance can benefit largely from technological developments</i></p><p>Clinical Surveillance is one of the most obvious ways of pathogen detection and the system is largely in place. However, the quality and time needed between symptom emergence and diagnosis depend highly on the equipment and personnel available at the clinics. Polymerase Chain Reaction (PCR) is the golden standard for pathogen detection due to its high sensitivity and specificity, but the equipment is costly (thousands to tens of thousands of USD) and the operation process is complicated, requiring skilled personnel to perform the tests. There is therefore a demand for alternative tests that are cheaper and easier to operate. Furthermore, PCR test is a form of targeted diagnosis and is unable to detect novel pathogens. There is increasing attention to develop pathogen-agnostic tests that can detect both existing and emerging pathogens.</p><p>Other drawbacks of clinical surveillance is its difficulty in detecting asymptomatic infections as patients would only visit hospitals when they feel sick. The act of traveling to and from the clinics also increases the risks of transmission. Point-of-Person (PoP) surveillance can overcome these problems by having people regularly take diagnostic tests at home. PoP devices in the form of LFTs proved their effectiveness during the COVID-19 pandemic. Ideally, we would like to have similar tests that are less invasive (e.g. requiring only saliva samples), have multiplex capability so that they can detect a range of pathogens simultaneously, are pathogen agnostic with the use of metagenomic sequencing, and provide (near) real-time, easily interpretable results. These devices can be connected to large databases for integration.</p><p>Getting regulatory approval is a large bottleneck in advancing PoP surveillance (Mardis 2017), but there is now an increasing push to address this issue. For example, a joint statement between the UK government and various research organizations has called for a simplified regulatory environment (Department of Health &amp; Social Care, 2022). A simplified regulatory framework would encourage more technological developments, therefore the current biosurveillance landscape is in favor of more technological developments in these two important surveillance systems.</p><p><i><strong>Insight 2</strong> \u2013 Environmental surveillance is a very promising area</i></p><p>Environmental surveillance, especially wastewater surveillance, has attracted much attention from the scientific community and government departments due to the various success stories seen in detecting pathogens. Wastewater surveillance was first implemented for tracking poliovirus in the 1990s (Asghar&nbsp;<i>et al</i>., 2014), which proved to be four to five times more sensitive in detecting outbreaks compared to syndromic surveillance. The finding that SARS-CoV RNA can be detected in human feces showed that wastewater surveillance can potentially detect a large range of pathogens. Indeed, scientists were able to capture the rise and fall of novel coronavirus cases using wastewater testing (Larsen &amp; Wigginton, 2020). This data also aided in discovering novel strains of the virus (Venugopal<i> et al</i>., 2020).</p><p>One of the biggest advantages of environmental surveillance is its high efficiency, as taking a single wastewater sample could cover the population of an entire region. Some of the technological barriers to the wider adoption and broader usefulness of environmental surveillance are the need for highly sensitive testing equipment and concentration techniques that work for a variety of pathogens including bacteria, virus, and fungi. We are still at the very early stage of wastewater sampling, but there are several concrete technological bottlenecks that we can work on to better this surveillance system which will be discussed below.</p><p><i><strong>Insight 3</strong> \u2013 The main challenges Syndromic and Digital Surveillance face are data- and operation-related</i></p><p>The computational infrastructure for data filtering, storing, and sharing is mostly in place, and algorithms have been developed for anomaly detection. The main challenges syndromic and digital surveillance face is the lack of a centralized database between key parties and operational protocols, such as standardized ways of writing and storing data. This leads to problems such as a large variability in the amount of data available by regions, disparate data quality and a lack of interoperability between data sources. Furthermore, there is a lack of incentive alignment on the importance of data sharing between regions and nations. Although important, addressing such challenges are out of the scope of this project. The scope of this project will be discussed in more detail in&nbsp;<a href=\"https://Chapter II - Biosurveillance Technologies\"><u>Chapter II</u></a>.&nbsp;</p><p><i><strong>Insight 4</strong> \u2013 Zoonotic Surveillance cannot easily be used for early-stage human pathogen detection</i></p><p>With increased globalization and animal farming, zoonotic diseases are becoming more common. Some scientists estimated that 60% of known infectious diseases are zoonotic, and it can account for up to 75% of new emerging pathogens. These zoonotic pathogens typically infect animals first, such as wildlife animals or livestock, and are transmitted to humans either directly via mediums or indirectly via vectors.&nbsp;</p><p>However, while we can use animal surveillance systems to regularly monitor animals\u2019 health and collect samples, we lack the capability of characterizing the pathogens effectively so as to predict their virulence and transmissibility to humans. This is a serious problem as it is infeasible to analyze and monitor every single pathogen strain found in animals. Microbiologists are working on this problem, but until then, zoonotic surveillance may not be suitable as an early-warning system.</p><h2>Bottlenecks of Surveillance Systems</h2><h3>Point of Person Surveillance</h3><p>The ideal PoP diagnostics would be pathogen agnostic and provide rapid, on-site results of all the pathogens in a patient\u2019s system. These tests would be taken frequently and by the majority of the population so data could be streamlined and used for monitoring outbreaks.&nbsp;</p><p>In comparison, PoP diagnostics today primarily include rapid antigen tests and rapid antibody tests with some CRISPR diagnostic methods like SHERLOCK (Mustafa, 2021) that test for specific pathogens like Covid-19 and HIV. Other emerging techniques include Lab on Chip (LoC) (Wu, 2018) concepts that try to automate and miniaturize benchtop PCR machines and improved SHERLOCK techniques like miSherlock (De Puig, 2021).&nbsp;</p><p>Today, PoP diagnostics face numerous challenges. First, the most used PoP diagnostic tools are pathogen-specific. There exists no ubiquitous pathogen-agnostic PoP diagnostic tool. Metagenomic sequencing appears to be the most promising solution to this problem but it suffers from numerous issues including its high costs, library prep methods that require skilled personnel, and long read times. These challenges are further discussed below.&nbsp;&nbsp;</p><h3>Clinical Surveillance&nbsp;</h3><p>Similar to PoP methods, an ideal clinical surveillance system would involve testing patients for multiple pathogens at the point of care and receiving a rapid, on-site result.&nbsp;</p><p>In clinical settings, multiplex assays are used to test for the presence of multiple pathogens but this often introduces a week of delay from sample to result time. Metagenomic sequencers are not sufficiently advanced to be used in clinical use for multiple reasons. First, they require sample preparation and library preparation in the lab by skilled professionals. Second, skilled professionals are needed to make sense of the genomic data and operate relevant equipment. Lastly, they have long read-out times and rely on incredibly expensive equipment.</p><p>In clinical settings, the primary tests used are pathogen-specific tests. Most commonly, saliva or nasal swabs are taken and sent back to labs for PCR to be run. Other commonly used molecular diagnostic tools are LAMP tests. Antibody tests include ELISA (Medline), with the most common CRISPR tests being CARMEN (Broad Institute, 2022) and SHINV2 (Arizti-Sanz, 2021).&nbsp;</p><h3>Environmental Surveillance</h3><p>An ideal environmental surveillance system could rely on frequent and automatic sampling at key locations, and combined with different sequencing methods could detect both existing and novel pathogens. The gold standard pathogen detection method is PCR (National Human Genome Research Institute, n.d.), but novel methods such as LAMP (New England Biolabs Inc, n.d.) could become more popular. Environmental surveillance provides unique advantages compared to PoP and clinical surveillance as each sample contains the sequences of many individuals. As of 2021, 55 out of 195 countries (of which 36 are high-income, 11 are upper middle-income, and 8 are lower-middle income), contain wastewater monitoring (World Health Organization, 2022).&nbsp;</p><p>However, in reality, this benefit is unlikely to be fully captured due to several challenges. We will focus on wastewater sampling as it is the most widely used environmental surveillance method, but there are other environmental surveillance methods that could be useful as well (Ramuta et al., 2022). First, wastewater is highly unpredictable and varies by region. Consequently, the pathogen concentration varies largely depending on the time and place of collection. Secondly, pathogens are greatly diluted in the environmental samples.&nbsp;This means that there may be low pathogen concentration in samples for diagnostics to run accurately.&nbsp;Furthermore, our understanding of microbiology is not enough to systematically estimate what proportion of pathogens would be detectable in wastewater. So far, we have used wastewater to detect specific pathogen species such as poliovirus and SARS COVID-19, but a broader understanding would be crucial before implementing globally distributed and coordinated wastewater surveillance systems.</p><p>Currently, we are only able to detect specific pathogens that are relatively concentrated in the samples, with a high probability of false negatives. The extent to which we can backtrace the diagnostic results to the pathogen concentration in the population also varies significantly depending on the labs, equipment, and methods used.</p><hr><h1>Chapter II - Biosurveillance Technologies</h1><p>Narrowing our focus to PoP, clinical and environmental surveillance, we then mapped out the primary technologies needed for each of these surveillance systems, and the technological bottlenecks for each of these technologies.&nbsp;</p><h2>Why Focus on Technological Bottlenecks</h2><p>In our research, we\u2019ve come to understand that creating effective biosurveillance systems encompasses multiple types of challenges. Below, we outlined the primary technology development stages (<strong>Diagram 1</strong>).&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1673117424/mirroredImages/aTMgGL3gwKt7j2ZJG/q8ftcityvhx7urbefckh.png\"></p><p><strong>Diagram 1</strong> - The primary technological development stages with a focus on R&amp;D and Implementation.</p><p>While we know some of the barriers listed can significantly reduce how promising certain technologies are over others, we decided to narrow the scope of this project to technological bottlenecks that address R&amp;D challenges. Research and development of technologies are the most fundamental steps in developing biosurveillance systems as all later challenges such as implementation require the technology to be fully functioning first. For example, our biosurveillance systems would be incredibly vulnerable to attacks from novel and known pathogens without functioning diagnostics or DNA sequencers. Fortunately, there has been an increase in pandemic prevention efforts due to the Covid-19 pandemic making it a crucial time to understand and think about how to most effectively distribute funding across focus areas.&nbsp;</p><h2>Identifying Technologies</h2><p>Through reading papers and talking to experts at the Broad Institute, the Nucleic Acid Observatory, the Future of Humanity Institute, and others, we identified a list of technologies that are used for clinical, PoP, and environmental surveillance. There might be some relevant technologies that we haven\u2019t included here, but we are fairly confident that the list includes the most important ones. To present the information more clearly, we divided the process of pathogen surveillance (<strong>Diagram 2</strong>)&nbsp; into five stages: sampling, concentration, extraction of target pathogens, detection, and quantification. Point-of-Person, clinical and environmental surveillance differ largely in sampling methods, but the subsequent steps are quite similar.&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1673117424/mirroredImages/aTMgGL3gwKt7j2ZJG/wwglmyeqx6lwtftkggnu.png\"></p><p><strong>Diagram 2</strong> - An overview of the technologies involved in PoP, clinical and environmental surveillance across the pathogen surveillance process.</p><h2>Prioritizing Technologies&nbsp;</h2><p>Below, we\u2019ve listed the criteria we used when deciding to focus on specific technologies over others. Prioritization of technologies was primarily a qualitative ranking as many of the criteria used were difficult to quantify.&nbsp;</p><h3>Criteria for Prioritization</h3><p>In researching biosurveillance technologies, we aim to pick technologies that have the highest potential to make a large positive impact. The technologies that would naturally have the ability to do this are those that are highly accessible, inexpensive, quick, and can be used for multiple pathogens.&nbsp;&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:130px\"><strong>Criteria For Prioritization</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Description</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Justification&nbsp;</strong></p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Potential to be pathogen agnostic&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Diagnostics can test for multiple pathogens.&nbsp;&nbsp;</td><td style=\"border:1pt solid #000000;padding:4.968pt;vertical-align:top\"><ul><li>Suitable for detecting novel pathogens&nbsp;</li><li>Cheaper and more sustainable in the long run as the diagnostics can be used for multiple occasions</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Ability to be used ubiquitously&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Technology can be scaled for widespread use. It is economical, manufacturing processes are sufficiently efficient.&nbsp;</td><td style=\"border:1pt solid #000000;padding:4.968pt;vertical-align:top\"><ul><li>Allows detections on a large scale, including low-resource settings</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Time to result&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Time from sample taken to result.&nbsp;</td><td style=\"border:1pt solid #000000;padding:4.968pt;vertical-align:top\"><ul><li>Rapid detection is required to ensure timely action and prevent further spread of pathogens</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Overlap in core technology / mechanism&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Several other technologies rely on the same mechanism so if we improve the core mechanism, this benefits several other techniques.&nbsp;</td><td style=\"border:1pt solid #000000;padding:4.968pt;vertical-align:top\"><ul><li>Aligns incentives to develop certain overlapping core technologies, increasing the likelihood of impactful technologies being developed</li><li>More economical &amp; efficient</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Cost-effectiveness of the intervention at scale</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">**We were&nbsp;unable to find data or forecasts, though this will become increasingly more important (and easier to evaluate) later on in the R&amp;D and commercialization pipeline</td><td style=\"border:1pt solid #000000;padding:4.968pt;vertical-align:top\"><ul><li>Allows implementation at low-resource settings</li><li>Makes adoption of technologies easier, especially during the pilot stage</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Potential for dual-use by bad actors</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Chance that the technology could be misused by bad actors. For example, central databases that contain patient information could be hacked.&nbsp;</td><td style=\"border:1pt solid #000000;padding:4.968pt;vertical-align:top\"><ul><li>If a technology has the potential to be mis-used, we should carefully consider its development to prevent unwanted consequences</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Neglectedness&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Not worked on by many organizations and people&nbsp;</td><td style=\"border:1pt solid #000000;padding:4.968pt;vertical-align:top\"><ul><li>If a technology receives less attention, but has high potential (see other criteria), it could be more important to highlight</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Technology Readiness Level</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Ideally aiming for&nbsp;<a href=\"https://www.plexus.com/en-us/current/articles/technology-readiness-levels\"><u>TRL 3-6</u></a>. These are technologies that have a proof of concept and are being further validated and developed in their working environments.&nbsp;</td><td style=\"border:1pt solid #000000;padding:4.968pt;vertical-align:top\"><ul><li>Highlighting technologies that are already fully developed would not increase the development of present biosecurity technologies</li></ul></td></tr></tbody></table></figure><p><br>&nbsp;<strong>Table 3</strong> - Criteria for prioritizing technologies.</p><h3>Prioritized technologies</h3><p>Although our intention was to apply the criteria above to assess each of the technologies, we were unable to do so primarily due to the lack of information that we can gather. It was difficult to find experts to interview for each of these technologies in time, and we couldn\u2019t find existing research papers that ranked the importance of these technologies in the way we wanted. In the end, we used a qualitative and intuitive assessment of how important these technologies were and decided on PCR, LAMP, and metagenomics sequencing. (<strong>Diagram 3</strong>).&nbsp;</p><p>While we decided to put our focus on these technologies due to time constraints, we still believe that further research must be done to better understand the technological bottlenecks in the other mentioned technologies.&nbsp;</p><p><strong><img src=\"https://res.cloudinary.com/cea/image/upload/v1673117424/mirroredImages/aTMgGL3gwKt7j2ZJG/dmhd5emuysav1fcpsdsa.png\"></strong></p><p><strong>Diagram 3</strong> - Prioritized technologies (bolded) for PoP, clinical and environmental surveillance.</p><h1>References</h1><p>\u201cAlert and Sentinel Systems: Signaal, Netherlands/Belgium.\u201d&nbsp;<i>OSHA</i>, National Human Genome Institutean biosurveillance <a href=\"https://osha.europa.eu/en/publications/alert-and-sentinel-systems-signaal-netherlandsbelgium\">https://osha.europa.eu/en/publications/alert-and-sentinel-systems-signaal-netherlandsbelgium</a>.&nbsp;</p><p>Alexopulos, Nick. \u201cFirst Working Definition of Global Catastrophic Biological Risks.\u201d&nbsp;<i>Johns Hopkins Center for Health Security</i>, 9 Aug. 2019, <a href=\"https://www.centerforhealthsecurity.org/news/center-news/2017/2017-07-27_global-catastrophic-biological-risk-definition.html\">www.centerforhealthsecurity.org/news/center-news/2017/2017-07-27_global-catastrophic-biological-risk-definition.html</a></p><p>Aridgides, L.J., et al. \u201cMultiplex PCR Allows Simultaneous Detection of Pathogens in Ships' Ballast Water.\u201d&nbsp;<i>Marine Pollution Bulletin</i>, vol. 48, no. 11-12, 2004, pp. 1096\u20131101.,&nbsp;<a href=\"https://doi.org/10.1016/j.marpolbul.2003.12.017\"><u>https://doi.org/10.1016/j.marpolbul.2003.12.017</u></a>.</p><p>Arizti-Sanz, Jon, et al. \u201cEquipment-Free Detection of SARS-COV-2 and Variants of Concern Using CAS13.\u201d 2021, doi:10.1101/2021.11.01.21265764</p><p>Asghar, H., et al. \u201cEnvironmental Surveillance for Polioviruses in the Global Polio Eradication Initiative.\u201d&nbsp;<i>Journal of Infectious Diseases</i>, vol. 210, no. suppl 1, 2014, <a href=\"https://doi.org/10.1093/infdis/jiu384\">https://doi.org/10.1093/infdis/jiu384</a>.</p><p>Bao, Yijuan, et al. \u201cCut-Lamp: Contamination-Free Loop-Mediated Isothermal Amplification Based on the CRISPR/cas9 Cleavage.\u201d&nbsp;<i>ACS Sensors</i>, vol. 5, no. 4, 2020, pp. 1082\u20131091., <a href=\"https://doi.org/10.1021/acssensors.0c00034\">https://doi.org/10.1021/acssensors.0c00034</a>.</p><p>Biolabs, New England. \u201cLoop-Mediated Isothermal Amplification.\u201d&nbsp;<i>NEB</i>, <a href=\"https://international.neb.com/applications/dna-amplification-pcr-and-qpcr/isothermal-amplification/loop-mediated-isothermal-amplification-lamp\">https://international.neb.com/applications/dna-amplification-pcr-and-qpcr/isothermal-amplification/loop-mediated-isothermal-amplification-lamp</a>.</p><p>Cai, Sheng, et al. \u201cPhosphorothioated Primers Lead to Loop-Mediated Isothermal Amplification at Low Temperatures.\u201d&nbsp;<i>Analytical Chemistry</i>, vol. 90, no. 14, 2018, pp. 8290\u20138294.,&nbsp;<a href=\"https://doi.org/10.1021/acs.analchem.8b02062\"><u>https://doi.org/10.1021/acs.analchem.8b02062</u></a>.</p><p>Chiu, Charles Y., and Steven A. Miller. \u201cClinical Metagenomics.\u201d&nbsp;<i>Nature Reviews Genetics</i>, vol. 20, no. 6, 2019, pp. 341\u2013355., doi:10.1038/s41576-019-0113-7.&nbsp;</p><p>\u201cCRISPR-Based Diagnostic Chips Perform Thousands of Tests Simultaneously to Detect Viruses.\u201d&nbsp;<i>Broad Institute</i>, 8 Mar. 2022, <a href=\"https://www.broadinstitute.org/news/crispr-based-diagnostic-chips-perform-thousands-tests-simultaneously-detect-viruses\">www.broadinstitute.org/news/crispr-based-diagnostic-chips-perform-thousands-tests-simultaneously-detect-viruses</a>.&nbsp;</p><p>\u201cData Science for High Throughput Sequencing: Stanford .\u201d&nbsp;<i>Data Science for High-Throughput Sequencing</i>, Stanford: EE 372, June 2016, data-science-sequencing.github.io/Spr2016/Spr2016/.&nbsp;</p><p>De Puig, Helena, et al. \u201cMinimally Instrumented Sherlock (Misherlock) for CRISPR-Based Point-of-Care Diagnosis of SARS-COV-2 and Emerging Variants.\u201d&nbsp;<i>Science Advances</i>, vol. 7, no. 32, 2021, doi:10.1126/sciadv.abh2944</p><p>Delahaye, Clara, and Jacques Nicolas. \u201cSequencing DNA with Nanopores: Troubles and Biases.\u201d&nbsp;<i>PLOS ONE</i>, vol. 16, no. 10, 2021, doi:10.1371/journal.pone.0257521</p><p>\u201cDNA and RNA Sequencing Kits.\u201d&nbsp;<i>Oxford Nanopore Technologies</i>, 26 Aug. 2022, nanoporetech.com/products/kits.&nbsp;</p><p>\u201cDNA Sequencing.\u201d&nbsp;<i>Oxford Nanopore Technologies</i>, 10 June 2020, <a href=\"https://nanoporetech.com/applications/dna-nanopore-sequencing\">nanoporetech.com/applications/dna-nanopore-sequencing</a>.&nbsp;</p><p>\u201c EE 372: Data Science for High-Throughput Sequencing Stanford University .\u201d&nbsp;<i>Lecture 2: Basics of DNA &amp; Sequencing by Synthesis</i>, data-science-sequencing.github.io/Win2018/lectures/lecture2/.&nbsp;</p><p>\u201cElisa Blood Test: Medlineplus Medical Encyclopedia.\u201d&nbsp;<i>MedlinePlus</i>, U.S. National Library of Medicine, medlineplus.gov/ency/article/003332.html.&nbsp;</p><p>El-Tholotha, Mohamed, et al. \u201cA Single and Two-Stage, Closed-Tube, Molecular Test for the 2019 Novel Coronavirus (COVID-19) at Home, Clinic, and Points of Entry.\u201d 2020, <a href=\"https://doi.org/10.26434/chemrxiv.11860137\">https://doi.org/10.26434/chemrxiv.11860137</a>.</p><p><i>Environmental Surveillance for SARS-COV-2 to Complement Public Health Surveillance</i>, 2022.</p><p>Esbin, Meagan N., et al. \u201cOvercoming the Bottleneck to Widespread Testing: A Rapid Review of Nucleic Acid Testing Approaches for COVID-19 Detection.\u201d&nbsp;<i>RNA</i>, vol. 26, no. 7, 2020, pp. 771\u2013783.,&nbsp;<a href=\"https://doi.org/10.1261/rna.076232.120\"><u>https://doi.org/10.1261/rna.076232.120</u></a>.</p><p>\u201cFirst Global Estimates of 2009 H1N1 Pandemic Mortality Released by CDC-Led Collaboration.\u201d&nbsp;<i>Centers for Disease Control and Prevention</i>, Centers for Disease Control and Prevention, 25 June 2012, <a href=\"https://www.cdc.gov/flu/spotlights/pandemic-global-estimates.html\">www.cdc.gov/flu/spotlights/pandemic-global-estimates.html</a>.&nbsp;</p><p>\u201cGeographical Distribution of Confirmed MERS-COV Cases by Country of Infection and Year.\u201d&nbsp;<i>European Centre for Disease Prevention and Control</i>, 7 May 2021, <a href=\"https://www.ecdc.europa.eu/en/publications-data/geographical-distribution-confirmed-mers-cov-cases-country-infection-and-year\">www.ecdc.europa.eu/en/publications-data/geographical-distribution-confirmed-mers-cov-cases-country-infection-and-year</a>.&nbsp;</p><p>\u201cGridion.\u201d&nbsp;<i>Oxford Nanopore Technologies</i>, 1 Mar. 2022, nanoporetech.com/products/gridion.&nbsp;</p><p>Head, Steven R., et al. \u201cLibrary Construction for next-Generation Sequencing: Overviews and Challenges.\u201d&nbsp;<i>BioTechniques</i>, vol. 56, no. 2, 2014, pp. 61\u201377., doi:10.2144/000114133.&nbsp;</p><p>Hsieh, Kuangwen, et al. \u201cSimultaneous Elimination of Carryover Contamination and Detection of DNA with Uracil-DNA-Glycosylase-Supplemented Loop-Mediated Isothermal Amplification (UDG-Lamp).\u201d&nbsp;<i>Chemical Communications</i>, vol. 50, no. 28, 2014, p. 3747.,&nbsp;<a href=\"https://doi.org/10.1039/c4cc00540f\"><u>https://doi.org/10.1039/c4cc00540f</u></a>.</p><p>\u201cIllumina DNA Prep.\u201d&nbsp;<i>Illumina DNA Prep | Flexibility for Many Whole-Genome Sequencing Applications</i>,&nbsp;<a href=\"http://www.illumina.com/products/by-type/sequencing-kits/library-prep-kits/nextera-dna-flex.html\"><u>www.illumina.com/products/by-type/sequencing-kits/library-prep-kits/nextera-dna-flex.html</u></a></p><p>\u201cJoint Statement from the UK Government, CEPI, IFPMA, ABPI, BIA, Bio and DCVMN on Delivering the 100 Days Mission.\u201d&nbsp;<i>GOV.UK</i>, 8 Mar. 2022,&nbsp;<a href=\"https://www.gov.uk/government/publications/joint-statement-on-delivering-the-100-days-mission/joint-statement-from-the-uk-government-cepi-ifpma-abpi-bia-bio-and-dcvmn-on-delivering-the-100-days-mission\"><u>https://www.gov.uk/government/publications/joint-statement-on-delivering-the-100-days-mission/joint-statement-from-the-uk-government-cepi-ifpma-abpi-bia-bio-and-dcvmn-on-delivering-the-100-days-mission</u></a></p><p>Larsen, David A., and Krista R. Wigginton. \u201cTracking Covid-19 with Wastewater.\u201d&nbsp;<i>Nature Biotechnology</i>, vol. 38, no. 10, 2020, pp. 1151\u20131153.,&nbsp;<a href=\"https://doi.org/10.1038/s41587-020-0690-1\"><u>https://doi.org/10.1038/s41587-020-0690-1</u></a></p><p>Ma, Cuiping, et al. \u201cA Novel Method to Control Carryover Contamination in Isothermal Nucleic Acid Amplification.\u201d&nbsp;<i>Chemical Communications</i>, vol. 53, no. 77, 2017, pp. 10696\u201310699.,&nbsp;<a href=\"https://doi.org/10.1039/c7cc06469a\"><u>https://doi.org/10.1039/c7cc06469a</u></a></p><p>Mardis, Connie.&nbsp;<i>Keeping up with POCT Regulatory Compliance - Medical Laboratory Observer</i>. 24 Oct. 2017,&nbsp;<a href=\"https://www.mlo-online.com/information-technology/lis/article/13009284/keeping-up-with-poct-regulatory-compliance\"><u>https://www.mlo-online.com/information-technology/lis/article/13009284/keeping-up-with-poct-regulatory-compliance</u></a></p><p>Merindol, Natacha, et al. \u201cOptimization of SARS-COV-2 Detection by RT-QPCR without RNA Extraction.\u201d 2020,&nbsp;<a href=\"https://doi.org/10.1101/2020.04.06.028902\"><u>https://doi.org/10.1101/2020.04.06.028902</u></a>.</p><p>\u201cMinion.\u201d&nbsp;<i>Oxford Nanopore Technologies</i>, 21 Feb. 2022, nanoporetech.com/products/minion</p><p>\u201cMiniSeq System.\u201d&nbsp;<i>MiniSeq Sequencing System | Small, Affordable Benchtop Sequencer</i>,&nbsp;<a href=\"http://www.illumina.com/systems/sequencing-platforms/miniseq.html\"><u>www.illumina.com/systems/sequencing-platforms/miniseq.html</u></a></p><p>\u201cMiseq System.\u201d&nbsp;<i>MiSeq System | Focused Power for Targeted Gene and Small Genome Sequencing</i>,&nbsp;<a href=\"http://www.illumina.com/systems/sequencing-platforms/miseq.html\"><u>www.illumina.com/systems/sequencing-platforms/miseq.html</u></a></p><p>Mustafa, Mujahed I., and Abdelrafie M. Makhawi. \u201cSherlock and DETECTR: CRISPR-CAS Systems as Potential Rapid Diagnostic Tools for Emerging Infectious Diseases.\u201d&nbsp;<i>Journal of Clinical Microbiology</i>, vol. 59, no. 3, 2021, doi:10.1128/jcm.00745-20</p><p>Notomi, T. \u201cLoop-Mediated Isothermal Amplification of DNA.\u201d&nbsp;<i>Nucleic Acids Research</i>, vol. 28, no. 12, 2000, <a href=\"https://doi.org/10.1093/nar/28.12.e63\">https://doi.org/10.1093/nar/28.12.e63</a>.</p><p>\u201cPolymerase Chain Reaction (PCR) Fact Sheet.\u201d&nbsp;<i>Genome.gov</i>, 17 Aug. 2020,&nbsp;<a href=\"https://www.genome.gov/about-genomics/fact-sheets/Polymerase-Chain-Reaction-Fact-Sheet\"><u>https://www.genome.gov/about-genomics/fact-sheets/Polymerase-Chain-Reaction-Fact-Sheet</u></a>.</p><p>\u201cPromethion.\u201d&nbsp;<i>Oxford Nanopore Technologies</i>, 10 Aug. 2022, nanoporetech.com/products/promethion.&nbsp;</p><p>Ramuta, Mitchell D., et al. \u201cSARS-COV-2 and Other Respiratory Pathogens Are Detected in Continuous Air Samples from Congregate Settings.\u201d 2022, <a href=\"https://doi.org/10.1101/2022.03.29.22272716\">https://doi.org/10.1101/2022.03.29.22272716</a>.</p><p>Shannon, K.E., et al. \u201cApplication of Real-Time Quantitative PCR for the Detection of Selected Bacterial Pathogens during Municipal Wastewater Treatment.\u201d&nbsp;<i>Science of The Total Environment</i>, vol. 382, no. 1, 2007, pp. 121\u2013129., <a href=\"https://doi.org/10.1016/j.scitotenv.2007.02.039\">https://doi.org/10.1016/j.scitotenv.2007.02.039</a>.</p><p>Sheridan, Cormac. \u201cFast, Portable Tests Come Online to Curb Coronavirus Pandemic.\u201d&nbsp;<i>Nature Biotechnology</i>, vol. 38, no. 5, 2020, pp. 515\u2013518., <a href=\"https://doi.org/10.1038/d41587-020-00010-2\">https://doi.org/10.1038/d41587-020-00010-2</a>.</p><p>Soroka, Marianna, et al. \u201cLoop-Mediated Isothermal Amplification (LAMP): The Better Sibling of PCR?\u201d&nbsp;<i>Cells</i>, vol. 10, no. 8, 2021, p. 1931.,&nbsp;<a href=\"https://doi.org/10.3390/cells10081931\"><u>https://doi.org/10.3390/cells10081931</u></a>.</p><p>\u201cSummary of Probable SARS Cases with Onset of Illness from 1 November 2002 to 31 July 2003.\u201d&nbsp;<i>World Health Organization</i>, World Health Organization, 24 July 2015, <a href=\"https://www.who.int/publications/m/item/summary-of-probable-sars-cases-with-onset-of-illness-from-1-november-2002-to-31-july-2003\">www.who.int/publications/m/item/summary-of-probable-sars-cases-with-onset-of-illness-from-1-november-2002-to-31-july-2003</a>.&nbsp;</p><p>Tang, Yi, et al. \u201cAdvanced Uracil DNA Glycosylase-Supplemented Real-Time Reverse Transcription Loop-Mediated Isothermal Amplification (UDG-RRT-Lamp) Method for Universal and Specific Detection of Tembusu Virus.\u201d&nbsp;<i>Scientific Reports</i>, vol. 6, no. 1, 2016,&nbsp;<a href=\"https://doi.org/10.1038/srep27605\"><u>https://doi.org/10.1038/srep27605</u></a></p><p>\u201cTruseq RNA Library Prep Kit V2.\u201d&nbsp;<i>TruSeq RNA Library Prep Kit v2 | Simple Libraries from Total RNA</i>, www.illumina.com/products/by-type/sequencing-kits/library-prep-kits/truseq-rna-v2.html.&nbsp;</p><p>Venugopal, Anila, et al. \u201cNovel Wastewater Surveillance Strategy for Early Detection of Coronavirus Disease 2019 Hotspots.\u201d&nbsp;<i>Current Opinion in Environmental Science &amp; Health</i>, vol. 17, 2020, pp. 8\u201313.,&nbsp;<a href=\"https://doi.org/10.1016/j.coesh.2020.05.003\"><u>https://doi.org/10.1016/j.coesh.2020.05.003</u></a>.</p><p>\u201cVoltrax.\u201d&nbsp;<i>Oxford Nanopore Technologies</i>, 23 Mar. 2022, nanoporetech.com/products/voltrax.&nbsp;</p><p>Wee, Soon Keong, et al. \u201cRapid Direct Nucleic Acid Amplification Test without RNA Extraction for SARS-COV-2 Using a Portable PCR Thermocycler.\u201d&nbsp;<i>Genes</i>, vol. 11, no. 6, 2020, p. 664.,&nbsp;<a href=\"https://doi.org/10.3390/genes11060664\"><u>https://doi.org/10.3390/genes11060664</u></a>.</p><p>Wooley, John C., et al. \u201cA Primer on Metagenomics.\u201d&nbsp;<i>PLoS Computational Biology</i>, vol. 6, no. 2, 2010, doi:10.1371/journal.pcbi.1000667.&nbsp;</p><p>Wu, J., Dong, M., Rigatto, C.&nbsp;<i>et al.</i> Lab-on-chip technology for chronic disease diagnosis.&nbsp;<i>npj Digital Med</i> 1, 7 (2018).&nbsp;<a href=\"https://doi.org/10.1038/s41746-017-0014-0\"><u>https://doi.org/10.1038/s41746-017-0014-0</u></a></p><p>Yu, Lin, et al. \u201cRapid Detection of Covid-19 Coronavirus Using a Reverse Transcriptional Loop-Mediated Isothermal Amplification (RT-LAMP) Diagnostic Platform.\u201d&nbsp;<i>Clinical Chemistry</i>, vol. 66, no. 7, 2020, pp. 975\u2013977., https://doi.org/10.1093/clinchem/hvaa102.&nbsp;</p><p>\u201c2014-2016 Ebola Outbreak in West Africa.\u201d&nbsp;<i>Centers for Disease Control and Prevention</i>, Centers for Disease Control and Prevention, 8 Mar. 2019, <a href=\"https://www.cdc.gov/vhf/ebola/history/2014-2016-outbreak/index.html\">www.cdc.gov/vhf/ebola/history/2014-2016-outbreak/index.html</a>.&nbsp;</p><p>\u201c2015\u201316 Zika Virus Epidemic.\u201d&nbsp;<i>Worldwide Outbreak</i>, 7 Mar. 2020, <a href=\"https://www.worldwideoutbreak.com/blog/cool_timeline/2015-16-zika-virus-epidemic\">www.worldwideoutbreak.com/blog/cool_timeline/2015-16-zika-virus-epidemic</a>.&nbsp;</p><h1>Appendices</h1><h2>Appendix 1 \u2013 Criteria used for the Weighted Factor Model</h2><figure class=\"table\"><table><tbody><tr><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top;width:150px\"><p><strong>Criteria</strong></p></td><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Explanation</strong></p></td></tr><tr><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>Current technology</p></td><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>How far are we in having the required technologies to set up such a surveillance system, assuming we can have status quo levels of funding and talent to work on it?</p></td></tr><tr><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>Cost required</p></td><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>How costly is it to develop such a surveillance system? Amount and level of talent needed would also count as cost in this criteria.</p></td></tr><tr><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>Scalability</p></td><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>How difficult it is (economically, logistically and politically) to scale such a surveillance system up to a global level?</p></td></tr><tr><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>Risk</p></td><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>How high are the risks associated with developing this technology, such as the potential of info hazard or technology being misused by bad actors? Risks could also include non-existential forms, such as impact on the ecosystem or causing fragility in the society.</p></td></tr><tr><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>Timeliness</p></td><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>How quick in theory can irregularities be identified and flagged up using this surveillance system?</p></td></tr><tr><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>Types of</p><p>pathogens&nbsp;</p><p>that can be</p><p>detected</p></td><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>What is the range of pathogens that can be detected by this surveillance system? Can it detect novel pathogens, or only existing ones?</p></td></tr><tr><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>Neglectedness</p></td><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>Example questions: Are there many people/groups working on this surveillance system? Does this surveillance system receive much funding?</p></td></tr><tr><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>Effects across</p><p>other areas&nbsp;</p></td><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>To what extent do developments in this surveillance method affect other areas that are not related to bio surveillance, or potentially not even related to biosecurity?</p></td></tr><tr><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>Uniqueness</p></td><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>How easily can the outcomes achieved by this surveillance system be replaced by other forms of surveillance?</p></td></tr><tr><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>Quality&nbsp;</p><p>of&nbsp;</p><p>existing evidence</p></td><td style=\"border:0.99609pt solid #000000;padding:5pt;vertical-align:top\"><p>How much evidence is there now to show that the surveillance system can do what it proposes?</p></td></tr></tbody></table></figure><p><br><br>&nbsp;</p><h2>Appendix 2 - Acknowledgements</h2><p>First of all, we would like to thank Cambridge Existential Risk Initiative (CERI) for supporting us during this project. In particular, our biosecurity lead Dewi Erwan and our mentor Akhil Bansal.&nbsp;</p><p>We are also very grateful for the various experts and organizations that we\u2019ve talked to. These include:</p><p><br><img src=\"https://res.cloudinary.com/cea/image/upload/v1673117697/mirroredImages/aTMgGL3gwKt7j2ZJG/mmhbmolhao2zzcghejtt.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673117697/mirroredImages/aTMgGL3gwKt7j2ZJG/acehwltc7dyxnddwdlzh.png 90w, https://res.cloudinary.com/cea/image/upload/v1673117697/mirroredImages/aTMgGL3gwKt7j2ZJG/trainxnomhtvjrgyqlnt.png 180w, https://res.cloudinary.com/cea/image/upload/v1673117697/mirroredImages/aTMgGL3gwKt7j2ZJG/ovzyycngu68mll34d7pc.png 270w, https://res.cloudinary.com/cea/image/upload/v1673117697/mirroredImages/aTMgGL3gwKt7j2ZJG/t5hvt6srkbwlogixuq9x.png 360w, https://res.cloudinary.com/cea/image/upload/v1673117697/mirroredImages/aTMgGL3gwKt7j2ZJG/xu9nyisrhoiakqp3vwsl.png 450w, https://res.cloudinary.com/cea/image/upload/v1673117697/mirroredImages/aTMgGL3gwKt7j2ZJG/qrdlk2w4vmxjstvttvo8.png 540w, https://res.cloudinary.com/cea/image/upload/v1673117697/mirroredImages/aTMgGL3gwKt7j2ZJG/cbyawarzaujyhhwjthth.png 630w, https://res.cloudinary.com/cea/image/upload/v1673117697/mirroredImages/aTMgGL3gwKt7j2ZJG/lyhbwbuvxmyysqunzryw.png 720w, https://res.cloudinary.com/cea/image/upload/v1673117697/mirroredImages/aTMgGL3gwKt7j2ZJG/ocpkifc8xr5g2aeu7amz.png 802w, https://res.cloudinary.com/cea/image/upload/v1673117697/mirroredImages/aTMgGL3gwKt7j2ZJG/weuou409fvtbcseukneg.png 810w\"></p>", "user": {"username": "Brianna Gopaul"}}, {"_id": "qS6mfMv5phCWN8Zt6", "title": "Technological Bottlenecks for PCR, LAMP, and Metagenomics Sequencing", "postedAt": "2023-01-09T06:05:42.761Z", "htmlBody": "<p><strong>Authors</strong>: Ziyue Zeng, Brianna Gopaul</p><p><strong>Supervisor</strong>: Akhil Bansal</p><h1>Summary</h1><p>A robust biosurveillance system would aim to identify and monitor new and known pathogens with the goal of reducing the severity of pandemics. From past pandemics, it\u2019s clear that modern biosurveillance systems have the potential to improve tremendously. To do so, a better understanding of the bottlenecks biosurveillance systems face is needed.&nbsp;</p><p>The report consists of two standalone parts. <strong>This is part II of the report that assumes a broad understanding of the biosurveillance and pathogen detection</strong>. We focus primarily on three technologies that are commonly used for biosurveillance: Polymerase Chain Reaction (PCR), Loop-Mediated Isothermal Amplification (LAMP) and Metagenomics Sequencing. <strong>If you would like to access an introduction to the biosurveillance landscape and an overview of the different technologies used, please see part I of the report&nbsp;</strong><a href=\"https://docs.google.com/document/d/14TZ4u1CbThWS9wAu2Trroc2co3cflzjIYPhW2V0Ggzg/edit?usp=sharing\"><strong><u>here</u></strong></a><strong>.&nbsp;</strong></p><p>&nbsp;In this report, we explore the importance of technological bottlenecks and investigate a few bottlenecks to gain a deeper understanding of their core issues. We focused on identifying the primary technological bottlenecks in metagenomics sequencing, LAMP and PCR. Towards the end, we also did an exploratory analysis on the non-technological bottlenecks that biosurveillance systems face.&nbsp;</p><p>&nbsp;This report will be most useful if you would like to have a <strong>deeper understanding of PCR, LAMP, and metagenomic sequencing and how they can be applied in biosurveillance</strong>. We also highlighted some potential avenues for technological improvements that could be worthwhile for research and funding agencies to look into.&nbsp;</p><p>We used a combination of empirical research and expert interviews to gather information throughout the project. A list of the organizations that we have talked to is included in the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#Appendix___Acknowledgements\"><u>Appendix</u></a>. In total, we gathered the opinions of 25 experts and sourced over fifty peer-reviewed research papers.&nbsp;</p><p><strong>Chapter I - Technological Bottlenecks for PCR, LAMP and Metagenomic Sequencing (</strong><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#Chapter_I___Technological_Bottlenecks_in_Biosurveillance_System_Technologies\"><strong><u>link</u></strong></a><strong>)</strong></p><p>The key bottlenecks and some potential solutions for PCR, LAMP and metagenomics are listed in the table below.<br>&nbsp;</p><p><strong>PCR &amp; LAMP</strong></p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:140px\"><strong>Technologies</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:200px\"><strong>Description</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Key bottlenecks</strong></td><td style=\"border:1pt solid #000000;padding:5px;vertical-align:top;width:200px\"><strong>Potential solutions</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#PCR\"><strong><u>PCR</u></strong></a></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Nucleic-acid based diagnostic. Gold standard for pathogen amplification and detection due to its high sensitivity and accuracy</td><td style=\"border:1pt solid #000000;padding:0pt;vertical-align:top;width:200px\"><ul><li>Long sample-to-answer&nbsp;<strong>time</strong>: RNA extraction takes 50 minutes, amplification takes 70 minutes</li><li><strong>Costs</strong> tens of thousands of USD</li></ul><p><br>&nbsp;</p></td><td style=\"border:1pt solid #000000;padding:0pt;vertical-align:top;width:200px\"><ul><li>Eliminate RNA purification step by direct-to-test addition</li></ul><p><br><br>&nbsp;</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#LAMP\"><strong><u>LAMP</u></strong></a></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">A non-PCR, nucleic-acid based alternative that\u2019s gaining attention. Performs at a comparable level of sensitivity as PCR, while having inherently lower cost and a quicker time to detection&nbsp;</td><td style=\"border:1pt solid #000000;padding:0pt;vertical-align:top\"><ul><li><strong>High temperature</strong> needed for optimal performance</li><li>High risk of carry-over&nbsp;<strong>contamination</strong></li></ul></td><td style=\"border:1pt solid #000000;padding:0pt;vertical-align:top\"><ul><li>Use phosphorothioated primers that allow for optimal performance at lower temperature</li><li>Improve CRISPR/Cas9 based methods or other methods to reduce crossover contamination</li></ul></td></tr></tbody></table></figure><p><br><strong>Metagenomic Sequencing (</strong><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#Metagenomic_Sequencing_\"><strong><u>section</u></strong></a><strong>)</strong></p><p>Metagenomic sequencing enables researchers to sample the genes within a biological sample. It is currently used for oncology and whole genome sequencing but has potential to be used in biosurveillance. The ideal metagenomic sequencer diagnostic would be rapid, inexpensive, fully automated and miniaturized for ubiquitous use. In this report, we break down metagenomic sequencing into sample preparation, library preparation, and sequencing.&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:130px\"><strong>Process</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Technological Bottlenecks&nbsp;</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:130px\"><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#Metagenomic_Sequencing_Bottlenecks_\"><strong><u>Sample Preparation</u></strong></a></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ul><li>Samples suffer from&nbsp;<strong>low concentration rates</strong> of the pathogen material and high contamination rates.</li><li>Need better sample concentration and filtration methods to remove&nbsp;<strong>contaminants</strong></li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:130px\"><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#Metagenomic_Sequencing_Bottlenecks_\"><strong><u>Library Preparation</u></strong></a></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ul><li>Current methods require lab personnel to use external equipment to prepare samples.</li><li>Requires<strong> automation and integration</strong> of library preparation methods with sequencers</li><li><strong>High in costs</strong> due to aforementioned reasons.&nbsp;</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:130px\"><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#Metagenomic_Sequencing_Bottlenecks_\"><strong><u>Sequencing&nbsp;</u></strong></a></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ul><li>Need&nbsp;<strong>miniaturization</strong> of sequencers as current sequencers are large benchtop devices used in lab settings.</li><li>Depending on the read length, sequencers can take hours to days. A focus on decreasing the read out&nbsp;<strong>time</strong> of sequencers is needed for rapid detection.&nbsp;</li><li>The costs of these devices are too high today.&nbsp;</li></ul></td></tr></tbody></table></figure><p><br>&nbsp;</p><p>In doing this research, we have also noticed that metagenomic sequencing suffers from misalignment of incentives. Today, funding is driving innovation in metagenomic sequencing for oncology and whole genome sequencing use. As the technology is developed for these applications, they become less suitable for its use as a diagnostic tool.&nbsp;</p><p><strong>Chapter II - Discussion on Non-Technological Bottlenecks (</strong><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#Chapter_II__Non_Technological_Bottlenecks\"><strong><u>link</u></strong></a><strong>)</strong></p><p>While we initially hypothesized that technological bottlenecks were crucial to biosurveillance systems, we gradually realized throughout research that non-technological bottlenecks are equally important and even more neglected. There are many stages involved in order to implement an effective and robust biosurveillance system - from funding, regulatory approval, R&amp;D, manufacturing, implementation, distribution, adoption to ensuring that data is interoperable between different systems. We ranked the relative importance of each of these bottlenecks for PCR, LAMP, and metagenomic sequencing, but have relatively low confidence on the exact rankings as we explored it in shallow depth.</p><p>We suspect that although much work can be done to improve biosurveillance technologies, we also urgently need more people working on addressing the less tangible non-technological bottlenecks. We believe that it would be useful to explore deeper into these non-technological bottlenecks and their potential solutions.&nbsp;</p><h1>Acknowledgements</h1><p>We would like to thank <a href=\"https://www.camxrisk.org/about\">CERI</a> (Cambridge Existential Risk Initiatives) and the biosecurity team for helping us make this happen. In particular, we are extremely grateful for our supervisor Akhil Bansal and biosecurity lead Dewi Erwan.&nbsp;</p><h1>Table of Contents</h1><p><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#Chapter_I___Technological_Bottlenecks_in_Biosurveillance_System_Technologies1\"><strong>Chapter I - Technological Bottlenecks in Biosurveillance System Technologies</strong></a></p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#Prioritized_Technologies_and_Bottlenecks\">Prioritized Technologies and Bottlenecks</a><ul><li><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#PCR\">PCR</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#LAMP\">LAMP</a></li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#Metagenomic_Sequencing_\">Metagenomic Sequencing</a><ul><li><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#Metagenomics_Sequencing_Pipeline\">Metagenomics Sequencing Pipeline</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#Metagenomic_Sequencing_Bottlenecks_\">Metagenomics Sequencing Bottlenecks</a></li></ul></li></ul><p><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#Chapter_II__Non_Technological_Bottlenecks1\"><strong>Chapter II: Non-Technological Bottlenecks</strong></a></p><p><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#References\"><strong>References</strong></a></p><p><a href=\"https://forum.effectivealtruism.org/posts/qS6mfMv5phCWN8Zt6/technological-bottlenecks-for-pcr-lamp-and-metagenomics#Appendix___Acknowledgements1\"><strong>Appendix - Acknowledgements</strong></a></p><h1>Chapter I - Technological Bottlenecks in Biosurveillance System Technologies</h1><p>In this section, we explored the technological bottlenecks in PCR, LAMP and metagenomic sequencing. We first provide an overview of the main bottlenecks associated with PoP, clinical and environmental surveillance. We will then discuss each of the prioritized technologies and their challenges.&nbsp;</p><h2>Prioritized Technologies and Bottlenecks</h2><p>Both PCR and LAMP are nucleic-acid-based technologies that could be used for sequence amplification and detection. These technologies are useful in a variety of settings. For instance, they can be used in Lab on Chip designs to offer real-time diagnosis. LAMP is especially promising for this purpose due to its simplicity, ease of performance, and low-cost. PCR can also be integrated with sequencing technologies in the sample preparation step in order to amplify genetic sequences.&nbsp;</p><h3>PCR</h3><p>Polymerase Chain Reaction is one of the most widely used methods for pathogen amplification and detection, and can detect pathogens in various sample types, including blood, saliva and both treated and untreated wastewater samples. PCR tests rely on heat denaturation or chemical lysis to separate DNA into single stranded RNA, addition of primers and polymerization to amplify the genome. The amplified product can then be detected via fluorescence or dyes.&nbsp;</p><p>Many of the disadvantages to traditional PCR-based detection such as low throughput and lack of differentiation between live and dead cells (Shannon&nbsp;<i>et al</i>., 2007) can be alleviated by specific forms of PCR. For instance, real-time quantitative PCR (qPCR) allows for accurate quantitative results that are displayed in real-time without the need for post-PCR processing steps such as gel electrophoresis. Real-time quantitative PCR can be combined with reverse-transcriptase PCR (RT-PCR) to allow for selective detection of viable cells only. Multiplex PCR addresses the traditional limitation of only being able to detect a single type of pathogen at a time. By introducing different primers, multiplex PCR can do targeted detection of hundreds and even thousands of microorganisms. The simultaneous detection can be done for different pathogen families, including virus, bacteria and protists (Aridgides&nbsp;<i>et al</i>., 2004).&nbsp;</p><p>However, <strong>several challenges remain that prevent PCR from being more widely used</strong>, especially in lower-resource settings. PCR is time-consuming, labor-intensive, expensive, and relatively slow to deliver results (Sheridan, 2020). Current state-of-the-art RT-PCR requires 70 minutes for RNA extraction and 90 minutes for amplification. The total cost of trained staff, equipment, and laboratories could cost tens of thousands of USD (Keong Wee&nbsp;<i>et al</i>., 2020). One potential solution is to improve or even eliminate the step of RNA extraction. Preliminary studies have shown that such a direct-to-test addition is possible, although the main challenge lies in doing so without compromising on the specificity and sensitivity of the subsequent PCR tests. Morindol&nbsp;<i>et al</i> (2020) showed that some swab mediums are more suitable than others. They showed that direct addition of viral RNA from swabs stored in water or UTM at 4\u00b0C had equivalent RT-PCR amplification compared to RNA purified from the same swabs, but this doesn\u2019t apply if swabs were stored in saline solution or Hanks medium. Keong Wee&nbsp;<i>et al</i> (2020) developed DIRECT-PCR that conducted a single-tube homogeneous reaction within 36 minutes which can be carried out in commercial portable PCR thermocyclers. The DIRECT-PCR had a sensitivity of six RNA copies per reaction which is comparable to RT-PCR. While it is unclear which of these methods are the most optimal and how they could be extended to other pathogens, direct-to-test addition is an avenue worth looking into.</p><p>I personally think that it is unlikely that improving direct-to-test addition will significantly improve the usefulness of PCR. Performing such tests would still require quite advanced laboratory equipment. I believe that non-PCR methods could be more promising as they can achieve comparable sensitivity and accuracy as PCR while sidestepping many of the inherent limitations that PCR has. LAMP is one such technology, and is described below.</p><h3>LAMP</h3><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1673116852/mirroredImages/qS6mfMv5phCWN8Zt6/jklejs9ncxqhhlkm4esr.png\"></p><p><strong>Diagram 4</strong> - Comparing various nucleic acid tests on cost per test and time to process the test. RT-PCR and RT-LAMP seem the most promising. Graph taken from Esbin&nbsp;<i>et al</i> 2020.&nbsp;</p><p>Developed as an alternative to PCR, LAMP (Loop-Mediated Isothermal Amplification) is a sensitive and specific method to amplify DNAs and RNAs. The high selectivity is achieved by using primers that anneal to four to six unique target regions. Like PCR, there are different forms of LAMP that include conventional LAMP, reverse-transcription LAMP and multiplex LAMP. Amplification can be detected in real-time via precipitation of magnesium pyrophosphate or fluorescent dye. LAMP does not require complex equipment and can be done in non-laboratory conditions.</p><p>LAMP addresses some of the bottlenecks seen with PCR-based methods so that it can operate at a stable temperature without the need for multiple temperature cycling, is much cheaper to operate and greatly reduces analysis time. Billions of copies of amplified DNA can be produced with this method in less than an hour, compared to only millions with PCR (Soroka&nbsp;<i>et al</i>., 2021). Yu&nbsp;<i>et al</i> (2020) developed the iLACO assay for detecting the SARS-CoV-2 virus that takes 15-40 minutes to complete. Furthermore, LAMP does not require the step of genetic material extraction.&nbsp;</p><p>LAMP can be combined with other isothermal methods like RPA (recombinase polymerase amplification) to further improve test accuracy. El-Tholoth&nbsp;<i>et al</i> (2020) was able to improve the detection sensitivity 100-fold compared to RT-PCR. This technique has not been applied to real-life patient samples yet, but if such an enhancement in sensitivity can be achieved, this technique could have great potential in analyzing diluted samples particularly in the area of targeted environmental surveillance.</p><p>However, LAMP also has various limitations. First, LAMP shows optimal performance at around 65 \u00b0C which could limit its applications in lower resource settings. Cai&nbsp;<i>et al&nbsp;</i>(2018) used phosphorothioated (PS-) primers in LAMP tests and brought the optimal temperature down to&nbsp; 40\u00b0C (Cai&nbsp;<i>et al</i>., 2018). The sensitivity of PS-LAMP is similar to regular LAMP. We suspect that not many groups are working on lowering the temperature as I only identified this one example. If we can bring the temperature down to ambient temperature, this could make LAMP-based detection a viable option for cheap and highly accurate point-of-person tests.&nbsp;</p><p>Another big challenge with LAMP is the high risk of carryover contamination (such as from materials present in the aerosol) that can lead to false positive results. In molecular terms, carryover contamination refers to the phenomenon where amplified DNA products from previous reactions become templates for re-amplification. Strict operational protocols such as analyzing different samples separately and clean working conditions such as ventilated rooms are recommended, but may not always be possible. Several methods have been developed to control such carryover contamination in LAMP assays. Currently the most recognized method is using uracil-DNA-glycosylase (UDG) to destroy the amplified carryover products (Tang&nbsp;<i>et al</i>., 2016). This can be accomplished by introducing two additional components (dUTP and UDG enzyme) into the LAMP tests. However, the limit of detection is somewhat lower than classic LAMP reactions, suggesting some degree of LAMP inhibition by the UDG enzyme (Hsieh et al., 2014). Ma&nbsp;<i>et al&nbsp;</i>(2017) introduced another method by using restriction endonucleases to eliminate cross-contamination so that the contaminants are removed prior to LAMP amplification. The main limitation with this method is the high temperatures (~70\u00b0C)&nbsp; required for such reactions to occur. More recently, Bao&nbsp;<i>et al</i> (2020) used CRISPR/Cas9 cleavage to remove contamination which can be done at room temperature. So far, this has lower efficiency of eliminating carryover contamination compared to UDG, but more research would be very beneficial for improving the accuracy of LAMP tests and extending its utility.</p><p><strong>I believe that carryover contamination is one of the major technological bottlenecks that currently limit the usefulness of LAMP</strong>, and if we can address this then LAMP could be much more widespread. However, technological bottlenecks are only one piece of the puzzle. There are many other bottlenecks such as regulatory bottlenecks especially if LAMP were to be used for PoP tests.</p><h3>Metagenomic Sequencing&nbsp;</h3><p>Metagenomic sequencing aims to sequence the genomes of known and novel pathogens. The development of this technology is primarily driven by oncology research, whole genome sequencing and environmental surveillance efforts but it also has immense potential to be a clinical diagnostic tool.&nbsp;</p><p><strong>If metagenomic sequencing could be on-site, rapid and miniaturized, it could significantly improve the current biosurveillance system</strong>. Instead of diagnostics taking several days to weeks, patients could get their results on the same day, significantly limiting the spread of infectious disease if the detection of this pathogen is closely followed by appropriate outbreak response activities. Furthermore, having a diagnostic test that can detect novel pathogens will allow us to recognize novel diseases, further preventing outbreaks (Chiu, 2019).&nbsp;</p><p><strong>Metagenomics Sequencing Pipeline</strong></p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:130px\"><strong>Step in Pipeline</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Description</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:130px\"><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2829047/\"><u>Sample Preparation</u></a></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Acquire sample, filter out contaminants, and ensure there is a sufficiently high concentration of material of interest. This varies by sample type e.g. soil vs saliva sample.&nbsp;</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:130px\"><a href=\"https://eu.idtdna.com/pages/technology/next-generation-sequencing/library-preparation\"><u>Library Preparation</u></a></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">DNA is fragmented to the length required by the sequencing technology, adapters are ligated to the 3\u2019 and 5\u2019 ends.&nbsp;</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:130px\"><a href=\"http://data-science-sequencing.github.io/Win2018/lectures/lecture4/#lr\"><u>Sequencing</u></a></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">DNA fragments are sequenced, this can be done using various methods both short read or long read. For example, lasers that activate fluorescence markers on nucleotide bases (Illumina), measuring the changes in the ion current using nanopores (Oxford Nanopore) among other methods.&nbsp;</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:130px\"><a href=\"http://data-science-sequencing.github.io/Win2018/lectures/lecture5/\"><u>Assembly</u></a></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Varies by type of metagenomic sequencing, but reads containing genomic data are reconstructed to form a sequence.&nbsp;</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:130px\"><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2829047/\"><u>Data Analysis</u></a></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Computational methods for understanding the genomic data; binning algorithms are used to overlap segments of DNA that represent the genome to the point that it can be reconstructed.&nbsp;</td></tr></tbody></table></figure><p><br>&nbsp;Metagenomic sequencing techniques differ by numerous aspects including the length of read, the method of sequencing, accuracy, costs and more.&nbsp;</p><p><strong>Metagenomic Sequencing Bottlenecks&nbsp;</strong></p><p>It\u2019s important to note that the bottlenecks for metagenomic sequencing vary by its application. The metagenomic sequencing bottlenecks when applied to whole genome sequencing are very different. In this report, we are focusing on metagenomic sequencing bottlenecks for its use as an infectious disease diagnostic tool in clinics and as an environmental surveillance tool.&nbsp;</p><p>Below, we have included a list of bottlenecks in metagenomic sequencing in each step in the metagenomic sequencing pipeline. These bottlenecks were researched by synthesizing insights we gathered from reading literature reviews and papers on metagenomic sequencing, talking to experts who are working with metagenomic sequencing or build metagenomic sequencers and reading white papers from the primary metagenomic sequencing companies such as Oxford Nanopore, Illumina and Pacific Biosciences.&nbsp;&nbsp;</p><p><u>Sample Preparation</u></p><ul><li>There is often an insufficient concentration of viral material in samples. The concentration varies by the type of sample (blood, saliva, nasal etc.) and where the patient is in their infection cycle.&nbsp;</li><li>It is also important to note that there is a&nbsp;high degree of contaminants in samples. This presents a secondary challenge which is filtering out contaminants and irrelevant materials; it\u2019s important to strike a balance where enough viral material is conserved without having contaminants alter the read out.&nbsp;</li></ul><p><i>While this report does not dig further into the particular concentration and filtration methods and their associated challenges, we see this as a useful direction of research for the future.&nbsp;</i></p><p><u>Library Preparation</u></p><ul><li>There is a lack of automation of library preparation methods. Most companies such as Illumina, Pacific Biosciences among others use library preparation kits that require skilled professionals to work with samples for several hours. Hiring researchers to manually do library prep drives up the cost of using metagenomic sequencing. Therefore, automated solutions that require no further interaction once the sample and cartridge is loaded are more ideal.&nbsp;&nbsp;<ul><li>Creating automated tools for library preparation will involve making library preparation workflows generalizable and pre-programmable to work for various sample types.</li><li>Oxford Nanopore has an automated library prep device VOLTRAX but currently, these devices are incredibly expensive with VOLTRAX costing \u00a312,000. Once more automated library prep devices are created, driving down the costs of these devices is imperative.</li></ul></li><li>Library preparation methods require external equipment and/or are not integrated with sequencers. For metagenomic sequencers to be used on-site, the ideal solution would involve a device that can do all sample prep, library prep and sequencing in one step where genomic data is then streamed to the database. In clinical surveillance in particular, this would allow pathogen agnostic diagnostics to take a lead in becoming much cheaper.&nbsp;</li><li>Library preparation as a step in the metagenomic sequencing pipeline is very expensive relative to other steps in the pipeline. Further research needs to be done to better investigate the root causes but we believe one factor may be the high cost of reagent kits and cartridges&nbsp;</li></ul><p><u>Sequencing</u></p><ul><li>Sequencers need to be miniaturized. Most sequencers on the market are large benchtop machines used in lab settings. For use in an environmental and point of person settings, they need to be miniaturized for easy transport. Likewise in clinical settings, further miniaturization would make them easier to integrate in clinics.&nbsp;</li><li>Costs are too high. Modern-day sequencers are incredibly costly making them only suitable for medium-large lab use. For example, Oxford Nanopore\u2019s cheapest miniaturized sequencer MinION costs \u00a3800 with Illumina's cheapest sequencer MiniSeq&nbsp;costing \u00a350,000. For sequencing to be viable in a point of person or point of care setting, the costs will need to be comparable to tests such as qPCR and LAMP.&nbsp;</li><li>The time from sample input to read out is too long. Ideally, point of person and environmental sequencing tests are rapid and on-site. For many metagenomic sequencers, read-out times take several hours to days depending on the read-length</li></ul><p>In doing this research, it has become clear that technological bottlenecks are very important in the development of a robust biosurveillance system. Without technological developments in the areas we mentioned above, it will be difficult to ensure humanity\u2019s readiness to future, potentially more deadly pandemics. While we believe it is true, we have come to learn that the task of creating a biosurveillance system is a multifaceted problem. Technological bottlenecks play a single role in this task, and while we have high confidence that they are very important, we don\u2019t believe they should necessarily be prioritized above all angles of working on biosurveillance systems. In the last section of our report, we briefly explore non-technological bottlenecks and our reasoning for believing that they are important to further investigate and prioritize.&nbsp;</p><hr><h1>Chapter II: Non-Technological Bottlenecks</h1><p>Our initial hypothesis before starting the project was that technological bottlenecks were one of the main factors limiting our biosurveillance systems. However, as we did more research and talked to various experts (~25 experts) in the field, we have the impression that non-technological bottlenecks are equally, if not more neglected, than technological ones. Much more resources should be put into addressing these bottlenecks, and we have listed some below. This is a non-exhaustive list with the main purpose of demonstrating the complexity of the landscape and the need for non-technological solutions. <strong>We ranked the relative importance of the non-technological bottlenecks for each of the technologies, but have relatively low confidence on this as we explored it in shallow depth.</strong>&nbsp;</p><figure class=\"image image_resized\" style=\"width:19.33%\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673116852/mirroredImages/qS6mfMv5phCWN8Zt6/ov5b2x4bcp9tthj6onj7.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673116852/mirroredImages/qS6mfMv5phCWN8Zt6/kxp8rl0tr9tdkt8nodk8.png 159w\"></figure><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:200px\"><strong>PCR</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:200px\"><strong>LAMP</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:200px\"><strong>Metagenomics*</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top;width:130px\"><strong>Funding</strong></td><td style=\"background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top\">PCR is widely recognized as the gold standard for amplification, so its importance is well recognized.</td><td style=\"background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top\">The usefulness of LAMP has been demonstrated in several cases and there is increasing attention on isothermal methods such as LAMP.</td><td style=\"background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Metagenomic sequencing for use in PoP, PoC and PoP is underfunded and most funding goes for metagenomic sequencing use in oncology or WGS.&nbsp;</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Regulatory Approval</strong></td><td style=\"background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top\">PCR is already very widely used.</td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">As it is a slightly newer technology, LAMP may be subjected to more regulatory approval than PCR. There would also be more regulations for using LAMP in clinical or Lab on Chip devices.</td><td style=\"background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Regulatory approval will be a hurdle as using metagenomic sequencing in PoP or PoC for diagnostics is very novel; this will be a new device on the market which presents unique regulatory challengers</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>R&amp;D</strong></td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Research should go into reducing the cost and time for performing PCR.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td style=\"background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top\">One of the main problems that prevents LAMP from being more widely used is the high risk of cross-contamination. Much more research is needed to find the optimal solution to address this key bottleneck.</td><td style=\"background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>R&amp;D for automated library prep, on-site more rapid sequencers among other areas need to be further developed.&nbsp;</p><p><br>&nbsp;</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Manufacturing</strong></td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Multiple enzymes and reagents are needed, some of which need to be stored at low temperatures.</td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">As multiple primers are needed, engineering and mass-producing these could be a challenge.&nbsp;</td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Manufacturing of reagent kits, cartridges and sequencers can be potentially further optimize to drive down costs&nbsp;</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Implementation</strong></td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Could be a problem in low resource settings due to lack of laboratory and trained staff.</td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Doesn\u2019t need specific laboratories to perform LAMP tests, but does need precautions to prevent cross-contamination.</td><td style=\"background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Getting metagenomic sequences in clinics once they\u2019re sufficiently developed will be a large hurdle&nbsp;</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Distribution</strong></td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Sending the needed reagents and equipment to all locations could be an issue especially in low-resource settings.</td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Sending the needed reagents and equipment to all locations could be an issue especially in low-resource settings.</td><td style=\"background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top\">The distribution networks that support today\u2019s PoP and PoC devices could likewise support metagenomic sequences if they\u2019re sufficiently miniaturized.</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Adoption</strong></td><td style=\"background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top\">PCR is already widely adopted.</td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Due to problems with sensitivity/accuracy, it may be more challenging to convince the public to use this test.</td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">New researchers and doctors will need to be trained on how to use metagenomic sequences if implemented in clinics&nbsp;</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Data Interoperability</strong></td><td style=\"background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top\">PCR outputs can be directly used for sequencing.</td><td style=\"background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Current LAMP outputs cannot easily be used for sequencing.</td><td style=\"background-color:#fff2cc;border:1pt solid #000000;padding:5pt;vertical-align:top\">Ideally, large databases that aggregate data about infectious disease spread in different populations will be useful. Since metagenomic sequencers have not been used in this application frequently, the issues around this are not concrete&nbsp;</td></tr></tbody></table></figure><p>&nbsp;</p><p><strong>Note: Metagenomic Sequencing &amp;&nbsp; Incentive Misalignment&nbsp;</strong></p><p>The metagenomic sequencing industry has primarily been developed out of its use in oncology research and whole genome sequencing. As a result, many of the core companies working on metagenomic sequencers are incentivized to further improve their devices for these use cases. Unfortunately, as metagenomic sequencers are further developed for oncology and whole genome sequencing, they become less practical for use in infectious disease diagnostics. For example, whole genome sequencing applications aim to minimize error rates of reads which further increases the cost of the device. In contrast, infectious disease diagnostics require low costs comparable to qPCR and can allow for higher error rates. If metagenomic sequencing companies do not actively work on decreasing the cost of sequencers, it\u2019s unlikely that metagenomic sequencing will ever be used as a clinical diagnostic tool.&nbsp;</p><h1>References</h1><p>Alexopulos, Nick. \u201cFirst Working Definition of Global Catastrophic Biological Risks.\u201d&nbsp;<i>Johns Hopkins Center for Health Security</i>, 9 Aug. 2019, www.centerforhealthsecurity.org/news/center-news/2017/2017-07-27_global-catastrophic-biological-risk-definition.html.&nbsp;</p><p>Aridgides, L.J., et al. \u201cMultiplex PCR Allows Simultaneous Detection of Pathogens in Ships' Ballast Water.\u201d&nbsp;<i>Marine Pollution Bulletin</i>, vol. 48, no. 11-12, 2004, pp. 1096\u20131101.,&nbsp;<a href=\"https://doi.org/10.1016/j.marpolbul.2003.12.017\"><u>https://doi.org/10.1016/j.marpolbul.2003.12.017</u></a>.</p><p>Arizti-Sanz, Jon, et al. \u201cEquipment-Free Detection of SARS-COV-2 and Variants of Concern Using CAS13.\u201d 2021, doi:10.1101/2021.11.01.21265764.&nbsp;</p><p>Asghar, H., et al. \u201cEnvironmental Surveillance for Polioviruses in the Global Polio Eradication Initiative.\u201d&nbsp;<i>Journal of Infectious Diseases</i>, vol. 210, no. suppl 1, 2014, https://doi.org/10.1093/infdis/jiu384.</p><p>Bao, Yijuan, et al. \u201cCut-Lamp: Contamination-Free Loop-Mediated Isothermal Amplification Based on the CRISPR/cas9 Cleavage.\u201d&nbsp;<i>ACS Sensors</i>, vol. 5, no. 4, 2020, pp. 1082\u20131091., https://doi.org/10.1021/acssensors.0c00034.</p><p>Biolabs, New England. \u201cLoop-Mediated Isothermal Amplification.\u201d&nbsp;<i>NEB</i>, https://international.neb.com/applications/dna-amplification-pcr-and-qpcr/isothermal-amplification/loop-mediated-isothermal-amplification-lamp.</p><p>Cai, Sheng, et al. \u201cPhosphorothioated Primers Lead to Loop-Mediated Isothermal Amplification at Low Temperatures.\u201d&nbsp;<i>Analytical Chemistry</i>, vol. 90, no. 14, 2018, pp. 8290\u20138294.,&nbsp;<a href=\"https://doi.org/10.1021/acs.analchem.8b02062\"><u>https://doi.org/10.1021/acs.analchem.8b02062</u></a>.</p><p>Chiu, Charles Y., and Steven A. Miller. \u201cClinical Metagenomics.\u201d&nbsp;<i>Nature Reviews Genetics</i>, vol. 20, no. 6, 2019, pp. 341\u2013355., doi:10.1038/s41576-019-0113-7.&nbsp;</p><p>\u201cCRISPR-Based Diagnostic Chips Perform Thousands of Tests Simultaneously to Detect Viruses.\u201d&nbsp;<i>Broad Institute</i>, 8 Mar. 2022, www.broadinstitute.org/news/crispr-based-diagnostic-chips-perform-thousands-tests-simultaneously-detect-viruses.&nbsp;</p><p>\u201cData Science for High Throughput Sequencing: Stanford .\u201d&nbsp;<i>Data Science for High-Throughput Sequencing</i>, Stanford: EE 372, June 2016, data-science-sequencing.github.io/Spr2016/Spr2016/.&nbsp;</p><p>De Puig, Helena, et al. \u201cMinimally Instrumented Sherlock (Misherlock) for CRISPR-Based Point-of-Care Diagnosis of SARS-COV-2 and Emerging Variants.\u201d&nbsp;<i>Science Advances</i>, vol. 7, no. 32, 2021, doi:10.1126/sciadv.abh2944</p><p>Delahaye, Clara, and Jacques Nicolas. \u201cSequencing DNA with Nanopores: Troubles and Biases.\u201d&nbsp;<i>PLOS ONE</i>, vol. 16, no. 10, 2021, doi:10.1371/journal.pone.0257521</p><p>\u201cDNA and RNA Sequencing Kits.\u201d&nbsp;<i>Oxford Nanopore Technologies</i>, 26 Aug. 2022, nanoporetech.com/products/kits.&nbsp;</p><p>\u201cDNA Sequencing.\u201d&nbsp;<i>Oxford Nanopore Technologies</i>, 10 June 2020, nanoporetech.com/applications/dna-nanopore-sequencing.&nbsp;</p><p>\u201c EE 372: Data Science for High-Throughput Sequencing Stanford University .\u201d&nbsp;<i>Lecture 2: Basics of DNA &amp; Sequencing by Synthesis</i>, data-science-sequencing.github.io/Win2018/lectures/lecture2/.&nbsp;</p><p>\u201cElisa Blood Test: Medlineplus Medical Encyclopedia.\u201d&nbsp;<i>MedlinePlus</i>, U.S. National Library of Medicine, medlineplus.gov/ency/article/003332.htm.&nbsp;</p><p>El-Tholotha, Mohamed, et al. \u201cA Single and Two-Stage, Closed-Tube, Molecular Test for the 2019 Novel Coronavirus (COVID-19) at Home, Clinic, and Points of Entry.\u201d 2020, https://doi.org/10.26434/chemrxiv.11860137.</p><p><i>Environmental Surveillance for SARS-COV-2 to Complement Public Health Surveillance</i>, 2022.</p><p>Esbin, Meagan N., et al. \u201cOvercoming the Bottleneck to Widespread Testing: A Rapid Review of Nucleic Acid Testing Approaches for COVID-19 Detection.\u201d&nbsp;<i>RNA</i>, vol. 26, no. 7, 2020, pp. 771\u2013783.,&nbsp;<a href=\"https://doi.org/10.1261/rna.076232.120\"><u>https://doi.org/10.1261/rna.076232.120</u></a>.</p><p>\u201cFirst Global Estimates of 2009 H1N1 Pandemic Mortality Released by CDC-Led Collaboration.\u201d&nbsp;<i>Centers for Disease Control and Prevention</i>, Centers for Disease Control and Prevention, 25 June 2012, www.cdc.gov/flu/spotlights/pandemic-global-estimates.htm.&nbsp;</p><p>\u201cGeographical Distribution of Confirmed MERS-COV Cases by Country of Infection and Year.\u201d&nbsp;<i>European Centre for Disease Prevention and Control</i>, 7 May 2021, www.ecdc.europa.eu/en/publications-data/geographical-distribution-confirmed-mers-cov-cases-country-infection-and-year.&nbsp;</p><p>\u201cGridion.\u201d&nbsp;<i>Oxford Nanopore Technologies</i>, 1 Mar. 2022, nanoporetech.com/products/gridion.&nbsp;</p><p>Head, Steven R., et al. \u201cLibrary Construction for next-Generation Sequencing: Overviews and Challenges.\u201d&nbsp;<i>BioTechniques</i>, vol. 56, no. 2, 2014, pp. 61\u201377., doi:10.2144/000114133.&nbsp;</p><p>Hsieh, Kuangwen, et al. \u201cSimultaneous Elimination of Carryover Contamination and Detection of DNA with Uracil-DNA-Glycosylase-Supplemented Loop-Mediated Isothermal Amplification (UDG-Lamp).\u201d&nbsp;<i>Chemical Communications</i>, vol. 50, no. 28, 2014, p. 3747.,&nbsp;<a href=\"https://doi.org/10.1039/c4cc00540f\"><u>https://doi.org/10.1039/c4cc00540f</u></a>.</p><p>\u201cIllumina DNA Prep.\u201d&nbsp;<i>Illumina DNA Prep | Flexibility for Many Whole-Genome Sequencing Applications</i>,&nbsp;<a href=\"http://www.illumina.com/products/by-type/sequencing-kits/library-prep-kits/nextera-dna-flex.html\"><u>www.illumina.com/products/by-type/sequencing-kits/library-prep-kits/nextera-dna-flex.html</u></a></p><p>\u201cJoint Statement from the UK Government, CEPI, IFPMA, ABPI, BIA, Bio and DCVMN on Delivering the 100 Days Mission.\u201d&nbsp;<i>GOV.UK</i>, 8 Mar. 2022,&nbsp;<a href=\"https://www.gov.uk/government/publications/joint-statement-on-delivering-the-100-days-mission/joint-statement-from-the-uk-government-cepi-ifpma-abpi-bia-bio-and-dcvmn-on-delivering-the-100-days-mission\"><u>https://www.gov.uk/government/publications/joint-statement-on-delivering-the-100-days-mission/joint-statement-from-the-uk-government-cepi-ifpma-abpi-bia-bio-and-dcvmn-on-delivering-the-100-days-mission</u></a></p><p>Larsen, David A., and Krista R. Wigginton. \u201cTracking Covid-19 with Wastewater.\u201d&nbsp;<i>Nature Biotechnology</i>, vol. 38, no. 10, 2020, pp. 1151\u20131153.,&nbsp;<a href=\"https://doi.org/10.1038/s41587-020-0690-1\"><u>https://doi.org/10.1038/s41587-020-0690-1</u></a></p><p>Ma, Cuiping, et al. \u201cA Novel Method to Control Carryover Contamination in Isothermal Nucleic Acid Amplification.\u201d&nbsp;<i>Chemical Communications</i>, vol. 53, no. 77, 2017, pp. 10696\u201310699.,&nbsp;<a href=\"https://doi.org/10.1039/c7cc06469a\"><u>https://doi.org/10.1039/c7cc06469a</u></a></p><p>Mardis, Connie.&nbsp;<i>Keeping up with POCT Regulatory Compliance - Medical Laboratory Observer</i>. 24 Oct. 2017,&nbsp;<a href=\"https://www.mlo-online.com/information-technology/lis/article/13009284/keeping-up-with-poct-regulatory-compliance\"><u>https://www.mlo-online.com/information-technology/lis/article/13009284/keeping-up-with-poct-regulatory-compliance</u></a></p><p>Merindol, Natacha, et al. \u201cOptimization of SARS-COV-2 Detection by RT-QPCR without RNA Extraction.\u201d 2020,&nbsp;<a href=\"https://doi.org/10.1101/2020.04.06.028902\"><u>https://doi.org/10.1101/2020.04.06.028902</u></a>.</p><p>\u201cMinion.\u201d&nbsp;<i>Oxford Nanopore Technologies</i>, 21 Feb. 2022, nanoporetech.com/products/minion</p><p>\u201cMiniSeq System.\u201d&nbsp;<i>MiniSeq Sequencing System | Small, Affordable Benchtop Sequencer</i>,&nbsp;<a href=\"http://www.illumina.com/systems/sequencing-platforms/miniseq.html\"><u>www.illumina.com/systems/sequencing-platforms/miniseq.html</u></a></p><p>\u201cMiseq System.\u201d&nbsp;<i>MiSeq System | Focused Power for Targeted Gene and Small Genome Sequencing</i>,&nbsp;<a href=\"http://www.illumina.com/systems/sequencing-platforms/miseq.html\"><u>www.illumina.com/systems/sequencing-platforms/miseq.html</u></a></p><p>Mustafa, Mujahed I., and Abdelrafie M. Makhawi. \u201cSherlock and DETECTR: CRISPR-CAS Systems as Potential Rapid Diagnostic Tools for Emerging Infectious Diseases.\u201d&nbsp;<i>Journal of Clinical Microbiology</i>, vol. 59, no. 3, 2021, doi:10.1128/jcm.00745-20</p><p>Notomi, T. \u201cLoop-Mediated Isothermal Amplification of DNA.\u201d&nbsp;<i>Nucleic Acids Research</i>, vol. 28, no. 12, 2000, https://doi.org/10.1093/nar/28.12.e63.</p><p>\u201cPolymerase Chain Reaction (PCR) Fact Sheet.\u201d&nbsp;<i>Genome.gov</i>, 17 Aug. 2020,&nbsp;<a href=\"https://www.genome.gov/about-genomics/fact-sheets/Polymerase-Chain-Reaction-Fact-Sheet\"><u>https://www.genome.gov/about-genomics/fact-sheets/Polymerase-Chain-Reaction-Fact-Sheet</u></a>.</p><p>\u201cPromethion.\u201d&nbsp;<i>Oxford Nanopore Technologies</i>, 10 Aug. 2022, nanoporetech.com/products/promethion.&nbsp;</p><p>Ramuta, Mitchell D., et al. \u201cSARS-COV-2 and Other Respiratory Pathogens Are Detected in Continuous Air Samples from Congregate Settings.\u201d 2022, https://doi.org/10.1101/2022.03.29.22272716.</p><p>Shannon, K.E., et al. \u201cApplication of Real-Time Quantitative PCR for the Detection of Selected Bacterial Pathogens during Municipal Wastewater Treatment.\u201d&nbsp;<i>Science of The Total Environment</i>, vol. 382, no. 1, 2007, pp. 121\u2013129., https://doi.org/10.1016/j.scitotenv.2007.02.039.</p><p>Sheridan, Cormac. \u201cFast, Portable Tests Come Online to Curb Coronavirus Pandemic.\u201d&nbsp;<i>Nature Biotechnology</i>, vol. 38, no. 5, 2020, pp. 515\u2013518., https://doi.org/10.1038/d41587-020-00010-2.</p><p>Soroka, Marianna, et al. \u201cLoop-Mediated Isothermal Amplification (LAMP): The Better Sibling of PCR?\u201d&nbsp;<i>Cells</i>, vol. 10, no. 8, 2021, p. 1931.,&nbsp;<a href=\"https://doi.org/10.3390/cells10081931\"><u>https://doi.org/10.3390/cells10081931</u></a>.</p><p>\u201cSummary of Probable SARS Cases with Onset of Illness from 1 November 2002 to 31 July 2003.\u201d&nbsp;<i>World Health Organization</i>, World Health Organization, 24 July 2015, www.who.int/publications/m/item/summary-of-probable-sars-cases-with-onset-of-illness-from-1-november-2002-to-31-july-2003.&nbsp;</p><p>Tang, Yi, et al. \u201cAdvanced Uracil DNA Glycosylase-Supplemented Real-Time Reverse Transcription Loop-Mediated Isothermal Amplification (UDG-RRT-Lamp) Method for Universal and Specific Detection of Tembusu Virus.\u201d&nbsp;<i>Scientific Reports</i>, vol. 6, no. 1, 2016,&nbsp;<a href=\"https://doi.org/10.1038/srep27605\"><u>https://doi.org/10.1038/srep27605</u></a></p><p>\u201cTruseq RNA Library Prep Kit V2.\u201d&nbsp;<i>TruSeq RNA Library Prep Kit v2 | Simple Libraries from Total RNA</i>, www.illumina.com/products/by-type/sequencing-kits/library-prep-kits/truseq-rna-v2.html.&nbsp;</p><p>Venugopal, Anila, et al. \u201cNovel Wastewater Surveillance Strategy for Early Detection of Coronavirus Disease 2019 Hotspots.\u201d&nbsp;<i>Current Opinion in Environmental Science &amp; Health</i>, vol. 17, 2020, pp. 8\u201313.,&nbsp;<a href=\"https://doi.org/10.1016/j.coesh.2020.05.003\"><u>https://doi.org/10.1016/j.coesh.2020.05.003</u></a>.</p><p>\u201cVoltrax.\u201d&nbsp;<i>Oxford Nanopore Technologies</i>, 23 Mar. 2022, nanoporetech.com/products/voltrax.&nbsp;</p><p>Wee, Soon Keong, et al. \u201cRapid Direct Nucleic Acid Amplification Test without RNA Extraction for SARS-COV-2 Using a Portable PCR Thermocycler.\u201d&nbsp;<i>Genes</i>, vol. 11, no. 6, 2020, p. 664.,&nbsp;<a href=\"https://doi.org/10.3390/genes11060664\"><u>https://doi.org/10.3390/genes11060664</u></a>.</p><p>Wooley, John C., et al. \u201cA Primer on Metagenomics.\u201d&nbsp;<i>PLoS Computational Biology</i>, vol. 6, no. 2, 2010, doi:10.1371/journal.pcbi.1000667.&nbsp;</p><p>Wu, J., Dong, M., Rigatto, C.&nbsp;<i>et al.</i> Lab-on-chip technology for chronic disease diagnosis.&nbsp;<i>npj Digital Med</i> 1, 7 (2018).&nbsp;<a href=\"https://doi.org/10.1038/s41746-017-0014-0\"><u>https://doi.org/10.1038/s41746-017-0014-0</u></a></p><p>Yu, Lin, et al. \u201cRapid Detection of Covid-19 Coronavirus Using a Reverse Transcriptional Loop-Mediated Isothermal Amplification (RT-LAMP) Diagnostic Platform.\u201d&nbsp;<i>Clinical Chemistry</i>, vol. 66, no. 7, 2020, pp. 975\u2013977., https://doi.org/10.1093/clinchem/hvaa102.&nbsp;</p><p>\u201c2014-2016 Ebola Outbreak in West Africa.\u201d&nbsp;<i>Centers for Disease Control and Prevention</i>, Centers for Disease Control and Prevention, 8 Mar. 2019, www.cdc.gov/vhf/ebola/history/2014-2016-outbreak/index.html.&nbsp;</p><p>\u201c2015\u201316 Zika Virus Epidemic.\u201d&nbsp;<i>Worldwide Outbreak</i>, 7 Mar. 2020, www.worldwideoutbreak.com/blog/cool_timeline/2015-16-zika-virus-epidemic.&nbsp;</p><h1>Appendix - Acknowledgements</h1><p>First of all, we would like to thank Cambridge Existential Risk Initiative (CERI) for supporting us during this project. In particular, our biosecurity lead Dewi Erwan and our mentor Akhil Bansal.&nbsp;</p><p>We are also very grateful for the various experts and organizations that we\u2019ve talked to. These include:</p><p><br><img src=\"https://res.cloudinary.com/cea/image/upload/v1673116852/mirroredImages/qS6mfMv5phCWN8Zt6/fs5hvpyf8ap1uyia5d9h.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673116852/mirroredImages/qS6mfMv5phCWN8Zt6/ytdrwa9p4xhgmoc3gyd3.png 103w, https://res.cloudinary.com/cea/image/upload/v1673116852/mirroredImages/qS6mfMv5phCWN8Zt6/dz07yinxdglqip2i7qs1.png 183w, https://res.cloudinary.com/cea/image/upload/v1673116852/mirroredImages/qS6mfMv5phCWN8Zt6/qay54mg6u60ixm9vtuma.png 263w, https://res.cloudinary.com/cea/image/upload/v1673116852/mirroredImages/qS6mfMv5phCWN8Zt6/leztzrhme6kqqxblpmun.png 343w, https://res.cloudinary.com/cea/image/upload/v1673116852/mirroredImages/qS6mfMv5phCWN8Zt6/oryeawk8zbgkrlx8zf8d.png 423w, https://res.cloudinary.com/cea/image/upload/v1673116852/mirroredImages/qS6mfMv5phCWN8Zt6/ztuvyz9fcjta3sza2uhf.png 503w, https://res.cloudinary.com/cea/image/upload/v1673116852/mirroredImages/qS6mfMv5phCWN8Zt6/e5aaar6e7xqcjrelcjt8.png 583w, https://res.cloudinary.com/cea/image/upload/v1673116852/mirroredImages/qS6mfMv5phCWN8Zt6/k8lpg4bg3nhqhlq8dyic.png 663w\"><br>&nbsp;</p>", "user": {"username": "Ziyue Z"}}, {"_id": "tAM3axZYGDaJnsRph", "title": "Street Epistemology (EA Shenanigans) - please RSVP", "postedAt": "2023-01-07T16:39:38.885Z", "htmlBody": "<p>Happy new year everyone. Sadly I missed <a href=\"https://forum.effectivealtruism.org/users/severin-t-seehrich-1\">Severin</a>'s <a href=\"https://forum.effectivealtruism.org/events/Stbmark4hCgB5wrsn/looking-back-moving-forward-a-cozy-evening-of-reflection-and\">reflection workshop</a> in December, but I'm looking forward to a great start to 2023 with all of you.</p><p><a href=\"https://forum.effectivealtruism.org/users/george-lanetz-1\">George Lanetz</a> will be holding a Workshop on <a href=\"https://streetepistemology.com/\">Street Epistemology</a> this time. Thanks!</p><p>You're very welcome even if you\u2019ve never been to a meetup or you feel like you don't fit.</p><p><strong>Time</strong>: Feel free to arrive by 6 PM CEST. The workshop starts at 6:30 and takes 45-120 min (we'll decide together). Afterwards there's time for open socializing<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref87344rjjh0o\"><sup><a href=\"#fn87344rjjh0o\">[1]</a></sup></span>.</p><p><strong>Food</strong>: I'll bring hummus, cream cheese, flatbread and vegetables.</p><p><strong>Location</strong>: The <a href=\"https://berlin.ccc.de/\"><u>Chaos Computer Club Berlin</u></a> can comfortably fit about 20 people. Please RSVP so we know how many to expect.</p><p>Route (German): <a href=\"https://berlin.ccc.de/page/anfahrt\">https://berlin.ccc.de/page/anfahrt</a></p><p>Please contact <a href=\"https://forum.effectivealtruism.org/users/new_user_928725294\">__nobody</a> if you have questions about the location.</p><p>PS: Comment here or PM me if you want to be invited into the EA Berlin Telegram group.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn87344rjjh0o\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref87344rjjh0o\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Rooms close 1 AM.</p></div></li></ol>", "user": {"username": "Milli"}}, {"_id": "7KL8CitpBmnzZgKHY", "title": "How to create curriculum for self-study towards AI alignment work? ", "postedAt": "2023-01-07T19:53:47.183Z", "htmlBody": "<p>Hello! I discovered EA and alignment work in 2022. I currently work in the humanities field, have taken math &amp; cs college-level courses but have no relevant degrees. I want to create a curriculum for myself to follow in 2023 so that I can eventually accumulate the skills necessary to partake in alignment research and signal such to relevant opportunities. Better yet, if I can find a community of people with similar goals, that would be great. However I am concerned about making wrong choices regarding subjects, methodology, input/output ratio etc., and would like to have as detailed and rational a plan as possible. Can anyone point me to some directions? Currently I have completed courses in data structure, algorithms, machine learning, some statistics, linear algebra and analysis, but have very little practical experience in all of these and am following Yann Lecun's deep learning course at the moment. Moreover since I am no longer a student I am at a loss on how to find research opportunities and collaborators.&nbsp;</p><p>Any advice appreciated, thanks!</p>", "user": {"username": "OIUJHKDFS"}}, {"_id": "JSKapp8R3MdzxitCu", "title": "Anchoring focalism and the Identifiable victim effect: Bias in Evaluating AGI X-Risks", "postedAt": "2023-01-07T09:59:51.849Z", "htmlBody": "<p><strong>Anchoring focalism</strong></p><p>The tendency to rely too heavily, or \"anchor\", on one trait or piece of information when making decisions \u2013 usually the first piece of information that we acquire on that subject.</p><p>Examples:&nbsp;</p><ul><li>Judging the contents of a book by its cover.</li><li>Judging the strength of an argument by its conclusion.</li><li>The need to consider and understand the contents of a message purely on the basis of how and when it was delivered and/or who delivered it.<ul><li>ie. 'If the courier is well dressed and the timing is right, then the message must be important' (and vice versa).</li></ul></li></ul><p>Unfortunately, perhaps due to the prior actions of others in your same social group, a deceptive frame of interpretation is more likely to be encountered first, effectively 'inoculating' everyone else in the group against an unbiased receipt of any further information.</p><p>Roughly parallel to the 'Identifiable Victim', particularly as an instantiation.<br>&nbsp;</p><p>&nbsp; - link Wikipedia: &nbsp;<a href=\"https://en.wikipedia.org/wiki/Anchoring\">Anchoring</a><br>&nbsp; - an item on <a href=\"https://www.lesswrong.com/posts/nEdueRhZwB4eP6X3c/introduction-bias-in-evaluating-agi-x-risks\">Forrest Landry</a>'s compiled list of biases in evaluating extinction risks.<br>&nbsp;</p><hr><p><strong>Identifiable victim effect</strong></p><p>The tendency to respond more strongly to a single identified person at risk than to a large group of people at risk.</p><p>This is parallel to the 'base rate fallacy', the '<a href=\"https://forum.effectivealtruism.org/s/gpDEfuSHnpjQPW4tg/p/daLssjprpqfAsRWW8\">normalcy bias</a>', and 'scope insensitivity' effects.</p><p>It represents another attempt to substitute intuition (fast/easy) in place of real analysis (hard, abstract, and slow).&nbsp;</p><p>It is an example of a compensatory effect wherein concrete and visible/identifiable specifics are treated in place of abstract concepts (the possibility of catastrophic events in the future, many aspects and follow on effects of which will be fully unknown \u2013 ie. via the 'Ambiguity effect')</p><p>Another way in which this effect has been observed to occur is when mentioning various x-risk concerns to intelligent peers. There is an immediate tendency for each one to consider the meaning of the concern in terms of their own lives only. Ie. how they would prefer to die, etc, usually with some element of obligatory moral fatalism included.</p><p>This is effect is ignoring the ethical considerations of the degree to which their own actions (or inaction) may be contributory to impacts on others, on other life, etc.<br>&nbsp;</p><p>&nbsp; - link Wikipedia:&nbsp; &nbsp;<a href=\"https://en.wikipedia.org/wiki/Identifiable_victim_effect\">Identifiable victim effect</a><br>&nbsp; - an item on <a href=\"https://www.lesswrong.com/posts/nEdueRhZwB4eP6X3c/introduction-bias-in-evaluating-agi-x-risks\">Forrest Landry</a>'s compiled list of biases in evaluating extinction risks.<br>&nbsp;</p>", "user": {"username": "remmelt"}}, {"_id": "Nr6f96A8svtThmLub", "title": "[Discussion] How Broad is the Human Cognitive Spectrum?", "postedAt": "2023-01-07T00:59:19.672Z", "htmlBody": "", "user": {"username": "Dragon God"}}, {"_id": "hA8KYatwvM6gsgyS3", "title": "Giving money to homeless people", "postedAt": "2023-01-09T06:07:49.781Z", "htmlBody": "<p>I have Schizophrenia. I had a psychotic break where I felt spiritually obligated to give money to homeless people.</p><p>I typically give $200 every month. I withdraw that amount in 20s and 50s and distribute the money to people who are bartering for money. I live in a city with a lot of homeless population.</p><p>This month I am going to give $300. I know this isn't a ton of money but I was simply wondering how much I should give? I don't know if giving more people smaller amounts or less people larger amounts will be better. &nbsp;</p>", "user": {"username": "abes"}}, {"_id": "Rwie7WzzB7u7Fftzn", "title": "[Linkpost] Jan Leike on three kinds of alignment taxes", "postedAt": "2023-01-06T23:57:34.852Z", "htmlBody": "", "user": {"username": "Akash"}}, {"_id": "LaDGhL8yZuz28rdKG", "title": "EA university groups are missing out on most of their potential ", "postedAt": "2023-01-07T12:44:02.691Z", "htmlBody": "<p>The inception of the Purpose and Life Planning Track</p><h1><strong>Context</strong></h1><p>Many people think that Effective Altruism university groups are <a href=\"https://centreforeffectivealtruism.notion.site/A-huge-opportunity-for-impact-movement-building-at-universities-772f4cc1684643a7b83d962fd544924d\">incredibly valuable</a>. <i>\u201cTo solve pressing global problems \u2014 like existential risk, global poverty, and factory farming - we need more talented, ambitious, altruistic people to focus full-time on these issues. Hundreds of thousands of these people are clustered at the world's top universities\u201d</i> (CEA, n.d.).</p><p>I agree. However, I believe that most EA uni groups, and even the EA community as a whole, <strong>are missing out on the majority of their potential to make the most out of this opportunity.</strong> This is because the current paradigm for community building emphasises&nbsp;<strong>finding</strong> talented and ambitious people that want to tackle the world's most pressing problem, and not to&nbsp;<strong>create</strong> them. This strategy has potentially serious limitations which is preventing us from creating as much counterfactual impact as possible.</p><h1>TL;DR - A summary of the main points</h1><ul><li>If EA university groups want to contribute as well as they can to empowering individuals to tackle the world's most pressing problems, <strong>we should not</strong> be cherry picking those students who are already naturally inclined to learn more about EA ideas.&nbsp;By only focusing on students who are already interested in EA ideas, we are missing the major opportunity to engage many more ambitious people to work on the world's most pressing problems, if approached from a different angle.</li><li>Most university students are very young adults. Many are ambitious and&nbsp;conscientious but they are simply not at a&nbsp;point in their life where they have deeply internalised the desire to make doing good a core part of their life; If they don\u2019t decide to join your introduction fellowship, or even drop out, it does not mean that they are not a good fit for EA. The life of university students is changing very quickly and there are many conflicting interests.</li><li>Before people want to learn more about EA ideas and how to apply them to their lives they must regard this as valuable. Furthermore, before EA ideas can be properly internalised, the proper foundation must be laid.</li><li>I identify four root causes, particularly for younger adults, that prevent an individual from being naturally inclined to EA ideas.<br><ul><li>First, <strong>people don't understand the link between being happy and doing good</strong>. Many people think that pursuing hedonic (feeling-based) happiness is the best way to live a happy life. Eudaimonic happiness (purpose-based happiness) tends to be more effective at this, however. People don\u2019t know this. Making people aware of this difference might change their perspective on life and what they want to prioritise.&nbsp;</li><li>Second, <strong>people often want to find a purpose in life, but it is not clear what that is and how to build one. </strong>Purpose consists to a large extent out of using your strengths to make the world a better place.&nbsp;</li><li>Third, <strong>people have not internalised the underlying reasons about why doing good matters.</strong> Before somebody can be intrinsically motivated for something they need to understand why it is important and what the underlying reasons are. I think that we can do a lot better as a community to help people internalise these reasons.</li><li>Four, <strong>learning about EA can be intimidating.</strong> Many EA ideas go against our evolutionary tendencies, such as prioritising our loved ones. Unless people have built a certain level of psychological and emotional resilience it is likely that taking EA ideas seriously is going to be too demanding.&nbsp;&nbsp;</li></ul></li><li>If these four points are addressed effectively it is possible to make&nbsp;<strong>a lot</strong> more people interested in learning about EA, and applying the ideas to their life (after initially being uninterested in your EA Introduction Program or about EA in general)</li><li>Addressing these root causes involves assisting people in understanding things such as the following&nbsp;<ul><li>what makes people actually happy</li><li>what a life purpose is and how to build one&nbsp;</li><li>helping people figure out the underlying reasons of Effective Altruism</li><li>what values and goals are most important in one\u2019s life</li></ul></li><li>I developed the&nbsp;<a href=\"https://www.notion.so/effectivealtruismmaastricht/The-Purpose-and-Life-Planning-PLP-Track-Public-version-0c783d0b296f44c1bda82c33528bd0fa\"><i><u>Purpose and Life Planning (PLP) Track</u></i>&nbsp;</a> to accomplish this. The track is designed to induce the intrinsic motivation in people to want to do good. In short, the theory of change is that people who become intrinsically motivated to do good, are a lot more likely to want to learn about how to do good better.</li><li>I argue that the PLP Track could be significantly more effective than the Introductory EA Program at inducing intrinsic motivation in individuals to engage with EA ideas and rigorously apply them to their lives.&nbsp;</li><li>To see whether this initiative is worth scaling, it would be wonderful to work together with you to test this hypothesis (you don\u2019t have to be a community builder). The way you can do this is <a href=\"https://www.notion.so/effectivealtruismmaastricht/The-Purpose-and-Life-Planning-PLP-Track-Public-version-0c783d0b296f44c1bda82c33528bd0fa#8acfbfa9cc6f4d35aa1bb2d0f4350777\">by joining one of the interdisciplinary project teams</a> that will cross-collaborate on important areas such as risk-management, outreach strategy, impact measurement and the improvement of the curriculum.&nbsp;</li><li>This is potentially one of the most important projects you could be working on now because you have the opportunity to create&nbsp;several lifetimes of counterfactual impact. I genuinely believe that this project has the potential to radically increase the ability of EA university groups to empower individuals to tackle the world's most pressing problems. In other words, this might be a very good opportunity to drastically increase your&nbsp;<a href=\"https://80000hours.org/problem-profiles/promoting-effective-altruism/#multiplier-effect\"><u>multiplier effect</u></a>.</li></ul><h1>Quick navigation - Outline&nbsp;</h1><ul><li><a href=\"https://forum.effectivealtruism.org/posts/LaDGhL8yZuz28rdKG/ea-university-groups-are-missing-out-on-most-of-their#Acknowledgements\">Acknowledgements</a>&nbsp;</li><li><a href=\"https://forum.effectivealtruism.org/posts/LaDGhL8yZuz28rdKG/ea-university-groups-are-missing-out-on-most-of-their#Who_is_writing_this_\">Who is writing this?</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/LaDGhL8yZuz28rdKG/ea-university-groups-are-missing-out-on-most-of-their#Introduction\">Introduction</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/LaDGhL8yZuz28rdKG/ea-university-groups-are-missing-out-on-most-of-their#A_closer_look_at_the_target_audience_of_EA_university_groups__Who_are_we_actually_dealing_with_\">A closer look at the target audience of EA university groups</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/LaDGhL8yZuz28rdKG/ea-university-groups-are-missing-out-on-most-of-their#Four_reasons_why_people_are_not_joining_your_Introductory_EA_Program\">Four reasons why people are not joining your Introductory EA Program</a><ul><li><a href=\"https://forum.effectivealtruism.org/posts/LaDGhL8yZuz28rdKG/ea-university-groups-are-missing-out-on-most-of-their#1__The_connection_between_happiness_and_doing_good_is_not_clear\">1. The connection between happiness and doing good is not clear</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/LaDGhL8yZuz28rdKG/ea-university-groups-are-missing-out-on-most-of-their#2__People_think_that_they_have_to_find_their_life_purpose__and_not_build_it\">2. People think that they have to&nbsp;<i>find</i> their life purpose, and not build it</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/LaDGhL8yZuz28rdKG/ea-university-groups-are-missing-out-on-most-of-their#3__People_have_not_internalised_the_underlying_reasons_about_why_doing_good_matters\">3. People have not internalised the underlying reasons of EA</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/LaDGhL8yZuz28rdKG/ea-university-groups-are-missing-out-on-most-of-their#4__Learning_about_EA_ideas_can_be_intimidating_\">4. Learning about EA ideas can be intimidating&nbsp;</a></li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/LaDGhL8yZuz28rdKG/ea-university-groups-are-missing-out-on-most-of-their#From_finding_to_creating_\">From finding to creating&nbsp;</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/LaDGhL8yZuz28rdKG/ea-university-groups-are-missing-out-on-most-of-their#The_inception_of_the_Purpose_and_Life_Planning_Track\">The inception of the Purpose and Life Planning Track</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/LaDGhL8yZuz28rdKG/ea-university-groups-are-missing-out-on-most-of-their#A_call_to_action\">A call to action</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/LaDGhL8yZuz28rdKG/ea-university-groups-are-missing-out-on-most-of-their#Why_could_this_be_a_very_big_deal_\">Final thoughts - Why could this be a very big deal?</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/LaDGhL8yZuz28rdKG/ea-university-groups-are-missing-out-on-most-of-their#Please_criticise_my_ideas_\">Please criticise my ideas</a></li></ul><h1>Reader guide</h1><p>In this post, I present four individual reasons that support the general conclusion I make. These reasons are very related, but they do not build on each other in a sequential way. Feel free to read through them in depth if you like,&nbsp;<strong>but it's not necessary to understand </strong><a href=\"https://forum.effectivealtruism.org/posts/LaDGhL8yZuz28rdKG/ea-university-groups-are-missing-out-on-most-of-their#From_finding_to_creating_\"><strong>the general conclusion</strong></a><strong>.</strong></p><p>Thank you for reading!</p><h1><strong>Acknowledgements</strong></h1><p>I would like to extend my sincerest thanks to Lotte de Lint for her support in providing feedback and participating in setting up the curriculum, for creating all of the Miro boards on which participants will record their learning experiences, and for constructively criticising my ideas. She will be a driving force in the implementation of the Purpose and Life Planning Track at our group in February 2023.<br><br>I would also like to thank Caleb, Judith, Ayham, and Per Ivar for their valuable feedback on my forum post. I am thankful to Emma Abelle for giving me the opportunity to participate at the Existential Risk Student Summit in Oxford and for the valuable experiences and friendships I have gained there that heavily influenced this post.</p><p>A special thank you goes to Amarins Veringa who supported me with her kindness and resourcefulness at the beginning of setting up EA Maastricht, and for connecting me to Lotte. I am very grateful to my parents for their unwavering support and encouragement.</p><p>Thank you all for your contributions and support.</p><h1>Who is writing this?</h1><p>Hi, I\u2019m Johan. It's nice that you are here. To put things into context, here is a quick summary of my background. Two years ago I was enrolled in a physical therapy programme because I was interested in the human body and helping people manage their pain. However, I eventually decided to quit that program and instead enrolled in Global Studies. This is because I wanted to learn more about my options on how I can work on more pressing challenges, and not only sore toes.&nbsp;</p><p>After hearing about EA I started the university chapter&nbsp;<a href=\"https://www.effectivealtruismmaastricht.nl/\"><u>EA Maastricht</u></a> in February 2022. With my co-founder Lotte and my future team I scaled our group from 8 active members in July, to now 27 active members after finishing our most recent fellowship with 47 participants. Now I am happy to have the opportunity to share my experiences as one of the new UGAP mentors at CEA. I am thankful to be part of this movement and am excited to share with you now one of my most influential changes in perspective I had recently.&nbsp;</p><h1><strong>Introduction</strong></h1><p>While I think that EA university groups are doing a good job of channelling students who are already ambitious and motivated to do good into impactful positions, the point that I am going to argue is that the goal should be different. If we are only targeting people who have already figured out for them that doing good is a priority in their life, then we are losing out on thousands, if not millions of students that would be just as a good fit. As I will show, this is due to many students actually being ambitious, but they have not figured out yet what is actually important for them.&nbsp;<br><br>The conventional path to&nbsp;<i><strong>find</strong></i> talented, ambitious and altruistic people is, in simple terms, to do strong outreach at your university campus and then get interested people to join your Introductory EA Program. Here we provide participants with the opportunity to learn about the unique time that we live in, the great opportunity that we each have to make a great difference, and how we, through the adoption of an effectiveness mindset can contribute to overcoming some of the world's most pressing issues.&nbsp;</p><p>The only problem? While this is a strong approach at making people that are&nbsp;<strong>already&nbsp;</strong>interested to do good, I strongly argue that the Introductory EA Program does not perform very well in&nbsp;<strong>inducing</strong> the intrinsic motivation in people to make them want to do good.&nbsp;</p><p>Let\u2019s see why.</p><h1><strong>A closer look at the target audience of EA university groups. Who are we actually dealing with?</strong></h1><p>According to the 2020 EA survey, the EA population <strong>is disproportionately young,</strong> with people mostly becoming involved in the community at the age of 24 (Moss, 2021).&nbsp;</p><p>In research from Hanson, it becomes visible that the vast majority of people that enrol in US colleges is below the age of 24 (2022). In Germany the average age of enrollment to universities is 22, and in India 80% of the students are enrolled in an undergraduate level program (Statista, 2022); (Government of India; Ministry of Human Resource Development, 2019). To put it another way, university EA groups are mostly dealing with very young people.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673043924/mirroredImages/LaDGhL8yZuz28rdKG/sxnr4ysd7pvwhxev4cq5.webp\"><figcaption><i>Figure 1</i>: (Education Data Initiative, 2022)</figcaption></figure><p>We are talking about students who have literally just left their parents' home, if at all. Students, many of whom have recently graduated from high school, often focused on getting good grades, hanging out with friends, and enjoying their free time.&nbsp;</p><p>We are talking about students that have just started a new chapter in their life, often moving to a different location, with a new social circle, sometimes a new job, academic pressure, often still being financially dependent on their parents, and exposed to&nbsp;<strong>many</strong> new opportunities. <strong>The target audience of EA university groups, young adults, are in a time of extreme change.&nbsp;</strong></p><p>We are talking about individuals who are at the pinnacle of maximal risk taking, novelty seeking and affiliations with peers. The time in which people are the most likely to invent a new artform, transform physics, commit their life to a cause, become addicted, leave home forever or marry outside their group (Sapolsky, 2018). According to Stanford Professor Robert Sapolsky, this is largely due to a frontal cortex that does not fully mature until the mid-twenties.</p><p>Now, take a guess. What proportion of these&nbsp;hundreds of millions of students, aged 18 to 24, have figured out who they are, what they stand for, and what is most important to them in life? Hmm. How many of these young people have made making a difference in the world one of their top priorities in life?&nbsp;</p><p>I contend that many young students have this goal and are interested in learning how they can \"make a difference.\" But how many have made it one of their <strong>highest</strong> aspirations? More important than hanging out with friends, going out to parties or engaging in leisure activities. You might be asking yourself why it matters whether it is a high priority or just another priority. Fair enough, life is not black and white. You can very well have multiple goals in life that you pursue simultaneously. There is a catch to this though, especially for younger people.&nbsp;<br><br>If you don't have a clear sense of your priorities it will be difficult to organise and execute around those priorities.<strong> It's not that people don't have the time, it is that they don't make the time. </strong>Time management is life management. The word time management is in my eyes really a misnomer - the challenge is not to manage time, but to manage ourselves and to figure out what is most important to us. Either you want to invest time into an activity, or you don\u2019t because there are things that are more important to you. The majority of students simply have not had enough time to reflect on their lives and accumulate enough experiences to know what the most important things are for them to do with their time.</p><p>As a result of this unclear sense of priorities, students \u2013 even those who aspire to do good \u2013 will be much less likely to apply for your Introductory EA Program. This is because of the often significant time investment that is required for the Introductory EA Program or joining your EA group. Time that you could spend with your loved ones, reading an exciting book, going to the gym, eating good food or just taking a mental break from hectic student life.&nbsp;</p><p>I want to emphasise this. If people do not want to join your Introductory EA Program, <strong>it\u2019s not because they are not altruistic, talented, or ambitious people. </strong>It's just that they have other conflicting priorities in the stage of life they are in. They are ambitious in other ways.</p><p>Unless you have figured out for yourself that doing good matters to you and you have connected this to your identity and personal aspirations, people won\u2019t make EA a&nbsp;<strong>high</strong> priority in their life. Young adults will simply prioritise the urgent and fun things that come along their way and it will become a lot less likely that they join the Introductory EA Program or invest time into learning more about EA. It's not that you don't have the time, it's that you don't make it.&nbsp;</p><p>So what do we do about this?</p><p>Before we can effectively address this bottleneck, we must first explore what I believe to be four major root causes of why people are not interested to join your Introductory EA Program.</p><h1>Four reasons why people are not joining your Introductory EA Program</h1><h2>1. The connection between happiness and doing good is not clear</h2><p>We currently live in a time where the majority of entertainment is consumed through platforms such as TikTok or Instagram, with celebrities showing off their constant indulgence in fine cuisine, the most extravagant homes, an abundance of cash, and their photoshopped bodies. <strong>Our entire perspective on what it means to live a happy and meaningful life is being distorted.&nbsp;</strong></p><p>While happiness can take on different forms, I think that the most common form of happiness that people pursue is \u201cfeeling-based\u201d, hedonic happiness. The pursuit of sensory states, positive or pleasant emotions ranging from contentment to intense joy, that make you feel good in the moment. While pursuing hedonic happiness is not inherently bad, speaker Emily Esfahani Smith makes the case in her&nbsp;<a href=\"https://www.ted.com/talks/emily_esfahani_smith_there_s_more_to_life_than_being_happy\"><u>Ted Talk</u></a> that our societies' obsession with this kind of happiness is having unfavourable effects. Smith argues that, despite the fact that our lives are objectively improving on nearly every conceivable standard, more people feel depressed, hopeless and alone (TED, 2017). Smith continues to make the case that this development is not a lack of hedonic happiness, but a lack of meaning and purpose. This is a second form of happiness called eudaimonia which is deeper and results from striving toward meaning and a noble purpose beyond simple self-gratification (Waterman, 1993).</p><p>In his monumental research on stress, Dr. Hans Selye supports this perspective as he found that making contributions and having meaningful projects that benefit others leads to a long, healthy, and happy life (Covey et al., 2020).<strong>&nbsp;</strong>Moreover, \u201c[t]hose only are happy\u201d, the philosopher John Stuart Mill came to believe, \u201cwho have their minds fixed on some object other than their own happiness; on the happiness of others, on the improvement of mankind, even on some art or pursuit, followed not as a means, but as itself an ideal end. Aiming thus at something else, they find happiness by the way\u201d (Times, 2005). The list goes on.</p><p>It is important to point out the distinction between eudaimonic and hedonic happiness because our society often prioritises the pursuit of pleasure and material wealth as the keys to happiness. However, studies have shown that pursuing hedonic happiness often actually has the opposite effect, with people exhibiting symptoms and being diagnosed with depression, or simply being less happy.&nbsp;(Mauss et al., 2011); (Ford et al., 2014).&nbsp;In contrast, eudaimonic happiness, which is derived from pursuing activities that align with one's values and lead to a sense of purpose and meaning, tend to be more sustainable and fulfilling for&nbsp;<i>oneself (Sheldon et al., 2018);</i> (Jachimowicz et al., 2017); (Chen &amp; Zeng, 2021).&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673020626/mirroredImages/LaDGhL8yZuz28rdKG/ogdaog6bi6okjjqbwdpg.png\"><figcaption><i>Figure 2:</i> (Street, 2017)</figcaption></figure><p>Understanding the benefits of eudaimonic happiness and the drawbacks of hedonic happiness will likely lead to a remarkable shift in personal priorities and a significantly higher&nbsp;<strong>intrinsic</strong> motivation to improve the state of the world. Many people want to be happy and they want to feel that they matter, but it is commonly misunderstood that actually focussing on something other than yourself is one of the most effective ways to accomplish that.&nbsp;</p><p>If you filter your available opportunities through a hedonistic lens, things that make you feel good in the moment, will completely overshadow opportunities to contribute to the direct improvement of the world. With this mindset, why bother talking to people about expanding your moral circle of concern, pursuing a highly impactful career, becoming vegan, or later voluntarily donating a significant portion of our income? This will most likely only work if you are looking at the world through the eudaimonic lens of happiness.</p><h2>2. People think that they have to&nbsp;<i>find</i> their life purpose, and not build it</h2><p>Next to increasing the range of opportunities that students aim to generate for themselves by attending university, many students wish to figure out what they want to do with their lives. To find direction. This is difficult and while one approach might work for one person, it might not work for another.&nbsp;</p><p>Without going into too much detail, I think a<strong> lot of the struggle that comes along with this process is actually the abundance of different, vague and often conflicting advice</strong> about how to \u201c<i>find</i>\u201d purpose. This ranges from \u201cfollow your joy\u201d and \u201cexplore your passions\u201d to \u201cspend time with people who inspire you\u201d. In fact, I would argue that most people have not even seriously learned about what a life purpose is, not to mention why you would want to have one. And why should you when you have just graduated from high school? Most people simply do not figure this out for themselves as a university student. Most people only do so when they are older, and, unfortunately, many never do.</p><p>Even if people have come to understand that they want to be part of something bigger or contribute to something important, many believe that the solution in finding purpose is an esoteric journey where you need to travel to some distant monastery, meditate or explore cave paintings. While these things might lead to some new insights, I am sceptical about their efficacy to actually provide people with something sustainable to hold on and strive toward in life. What then?</p><p>If we take some of the key insights by Victor Frankl, Adam Grant, Dr. Hans Selye and countless other philosophers, authors and researchers, we can notice an encouraging recurring pattern.&nbsp;</p><p>Regardless of the exact priorities you have, or which goals you pursue, it becomes clear that a major part of your purpose in life consists of using your strengths and your unique contribution capacity to improve the state of the world in one way or the other. It is learning about yourself and the world, determining what is important to you, which causes you want to support, and then putting your talents and strengths to work to advance those causes. If we look at it from this perspective, you don't need to \u201cdiscover\u201d your purpose. You can proactively build it by learning about yourself and the world, taking action based on your insights and continously updating your belief systems.&nbsp;</p><p>For many people the aspiration to do good and the wish to be part of something bigger than yourself is something that often happens naturally. The organisational psychologist Adam Grant describes in his bestselling book \u201cThink Again\u201d, that the progression of his students self-esteem in their careers often correlates to the following:</p><blockquote><p>Phase 1: I\u2019m not important</p><p>Phase 2: I\u2019m important</p><p>Phase 3: I want to contribute to something important</p><p>\u201cI\u2019ve noticed that the sooner they get to phase 3, the more impact they have and the more happiness they experience. It\u2019s left me thinking about happiness less as a goal and more as a by-product of mastery and meaning.\"</p></blockquote><p>There are good reasons to believe that people have an innate desire to want to do and to make a positive impact on the world.</p><p>But most students are simply not at phase 3 yet because they did not have the necessary time to go through the process of personal development! The necessary time to realise that they have something to seriously offer to the world, to demonstrate to yourself that they can actually change things, to understand that they will soon be gone, and that all of the things they possessed, all of the memories they formed, all of their loved ones, all of the material things, everything, will be gone, forever.</p><p>I think that many people eventually come to the realisation that one of the best ways to live a worthy life is to give something back and to leave something behind you that will impact the lives of the current and future generations in a good way. This is one of the very few ways to be able to tell yourself in your last moments of your life that it was worth it.</p><p>I argue, however, that the majority of young adults, the target audience of EA university groups, are in phase 1 and 2, and that they simply did not have the opportunity yet to transition to phase 3. I think that many people who enter this phase, if not most, only do so once they have left university. Then when they have already made many of their career decisions and it's more difficult to change pathways.&nbsp;</p><p>Unless you have transitioned to phase 3, why should you make doing good a core priority of your life, let alone doing good better? I think it\u2019s rather unlikely.&nbsp; But it does not have to be like that. These turning points in one's worldview don\u2019t need to happen by coincidence, they can be actively created. Purpose is built, not found.</p><h2>3. People have not internalised the underlying reasons about why doing good matters</h2><p>There are thousands of things we can choose to do. There is no way that we can do everything, so we are required to choose and make trade-offs.&nbsp;When we have multiple options to choose from on where to invest our time in our life we compare between different options by asking ourselves, \u201cwhy does this matter to me?\u201d Then we create, consciously or subconsciously, a ranking in our head of our different options. Broadly speaking, we will mostly choose those which are the closest connected to our sense of identity and our personal aspirations. In other words, we understand why it matters and we have reasons that we personally like or believe in.&nbsp;</p><p>Why does it matter to mention this?&nbsp;</p><p>Because before somebody will join your Introductory EA Program or actually be significantly engaged in your EA group, they need to believe that it is valuable <strong>to them</strong>.&nbsp;</p><p>If we don't have good reasons to do something or we don't like the reasons we have, we are less likely to prioritise that activity and will probably choose to engage in other things that are more meaningful to us.&nbsp;It\u2019s not that you don\u2019t have time, you don\u2019t&nbsp;<strong>make</strong> time. This might sound ridiculously self-explaining. But it\u2019s important to bring this up because this little piece of information provides a relevant insight into why people become motivated to learn more about EA, and others not.&nbsp;</p><p>If people have not internalised the meta reasons of EA, they won\u2019t be interested to act on them. For example, unless you believe that&nbsp;<strong>doing good</strong> matters to you personally and you have built a network of knowledge that supports this belief and you connected it to your identity, you will be way less likely to invest time into learning about how to&nbsp;<strong>do good better</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefw3sqdwn73kp\"><sup><a href=\"#fnw3sqdwn73kp\">[1]</a></sup></span>.&nbsp;</p><p>In one of the most viewed Ted Talks of all time, speaker&nbsp;<a href=\"https://youtu.be/qp0HIF3SfI4\"><u>Simon Sinek</u></a> discusses the importance of understanding the underlying reasons for one's actions, or their \"why\u201d.</p><p>In his talk he explains how all great organisations, institutions and leaders have a deep understanding of their purpose and why they do what they do. They have a fundamental understanding of why they exist.&nbsp;With a clear understanding of their purpose, they are able to achieve things that defy all assumptions and that others are unable to accomplish.<br>&nbsp;</p><figure class=\"image image_resized\" style=\"width:58.57%\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673020625/mirroredImages/LaDGhL8yZuz28rdKG/mciwdr1g3ynqik6w6wvu.png\"><figcaption><i>Figure 3:</i> (Hare Consulting, 2020)</figcaption></figure><p><br>Sinek argues that a deep understanding of one's purpose and underlying reasons is crucial for motivation and the desire to step into action. He even argues that this is due to the way our brains function and how we have evolved as a species.&nbsp;Seeking to understand the underlying meaning and purpose of what we do is (plausibly) deeply ingrained in us because it determined our evolutionary success<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefm9sihdjlu7j\"><sup><a href=\"#fnm9sihdjlu7j\">[2]</a></sup></span>.&nbsp;</p><p>If we do not have a good understanding of why we should do something and have not internalised these reasons, it is much less likely that we will be convinced that something is worth our attention because we will have other priorities. Why does it matter to bring this up?</p><p>Because in my eyes we can do a lot better at explaining the underlying reasons of Effective Altruism<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvuyjrcl3z4a\"><sup><a href=\"#fnvuyjrcl3z4a\">[3]</a></sup></span>. I also think we are making a big mistake if we think that people know why it's important to help others. Many agree that doing good matters, but who actually knows why?&nbsp;</p><p>Surely, some people have figured this out for themselves. But we need to take a balanced look at the profile of the target audience of EA university groups, young adults. I think it is unlikely that more than 5 % of this population have understood the underlying reasons of why doing good matters, and that they have connected this to their personal aspirations (75 % confidence).</p><p>If we want to make people interested in learning about EA, to encourage them to make doing good a core part of their life, or even create highly engaged EA\u2019s, we simply must go beyond&nbsp;stating that it is important to help others and showing them&nbsp;<strong>how</strong> they can have a big impact. To put it bluntly, learning about concepts such as scope insensitivity, the scout mindset, cognitive biases, or differences in impact, <strong>are not very useful</strong> for the large majority of university students if they have not internalised the underlying reasons for doing good.</p><p><strong>These are unquestionably important topics, but they cannot be pursued at the expense of failing to help people internalise the underlying reasons why doing good matters and to help individuals to build an identity around them.</strong> It's like trying to equip a rocket with the best navigation systems, the best engines and the best propellants. Unless you have enough fuel, the rocket will never make it to space.&nbsp;</p><p>I am talking about empowering people to learn about things such as that doing good is a fundamental part of a satisfying and meaningful life. Or that the world has been improving significantly and that we can contribute to real and vast progress. That our ancestors sacrificed so much of their lives to give&nbsp;us the chance to flourish and have more opportunities. That doing the most good is not as difficult as often supposed, and that you don\u2019t need to invest your entire life to make a big impact. But it could also be more personal, such as helping people to recognise all of the things they can be grateful for, and that giving something back to the world might be one of the best ways to develop the best version of yourself; to realise the highest level of&nbsp;<a href=\"https://www.simplypsychology.org/maslow.html\"><u>Maslow's pyramid</u></a>, self-actualisation.&nbsp;</p><p>If we want people to take EA ideas seriously, sign up to your Introductory EA Program, and, more importantly, act on EA ideas, <strong>they must have pre-existing core values and goals that are aligned with them</strong>, like the ones above. What is the percentage of 20-year-old students who are at this point?&nbsp;</p><h2>4. Learning about EA ideas can be intimidating&nbsp;</h2><p>In the end, EA community building is about influencing the behaviour of people. It's about making people engage with a certain set of ideas and, through the creation of the right environmental conditions, encouraging people to take those ideas seriously and implement them in their own life. In other words, facilitating personal change.&nbsp;</p><p>Change is difficult, however. Especially if the change has consequences for the wellbeing of yourself and your loved ones. Change involves risk, uncertainty, and often a letting go of the familiar. It can also involve a struggle to adjust to a new way of being, thinking and doing. To be able to go through the process of personal change effectively, you need to be open to it and you need to have built a certain level of resilience and self-efficacy that will allow you to face the outward vulnerability that comes along with change with strength. Why does this matter?&nbsp;</p><p>Before you can effectively improve the state of the world, you need to think that you actually can, and you need to be open to change your mind. This is not always easy for people, especially when it comes to EA ideas, because they contradict many of the pre-existing values and ideas about what is important for you. As a result, if EA ideas are not properly internalised, this will undermine intrinsic motivation to act on them.</p><p>Take the concept of impartial altruism as an example. It is a concept that challenges our natural tendency to prioritise our loved ones (Greene, 2014). It suggests that our families and friends should be of&nbsp;exactly the same value as some other person we do not know of living on the opposite side of the world. And while many people would agree with this statement from a rational point of view, <strong>how many actually feel that way</strong>?&nbsp;</p><p>Our societies have changed rapidly, but the way our brains are wired and the way our physiology works are still the same as 100 000 years ago when we lived in small tribes and used to hunt mammoths and gather berries. We are aiming to make people adopt belief systems that to a large part steer against their evolutionary tendencies. The implications of the ideas of EA are huge. Taking these ideas seriously often means that we need to admit to ourselves that our understanding of what doing good means, is wrong. It often means that our current life and career plans are probably not really \"effective\". They imply for many people that we should spend less time with our friends and loved ones. Taking these ideas seriously often means that you are going to have to come to the understanding that all of our life we have been \u201cwasting\u201d a big part of our time, and that you should probably change a lot of things. I don't think that I am exaggerating here. The ideas of EA are a big deal.&nbsp;</p><p>Now, what happens if a student is in a life changing phase as I described above? Academic pressure, making new friends, moving away from their parents. Young adults already do need to consistently step out of their comfort zone. It takes courage and strength to be willing to explore new and conflicting ideas and to have the belief in yourself that you actually can change. If your life is unstable and you are in a position of vulnerability, I argue that change is&nbsp;<strong>a lot</strong> more difficult.&nbsp;This is because change requires a certain level of psychological and emotional stability and resilience. When people are already facing challenges and stress in other areas of their life, they may not have the capacity or energy to also tackle significant personal change. &nbsp;Taking the ideas of EA seriously means to be willing to change yourself and the priorities in your life, and for many people this is too difficult.</p><p>So what can we do about this? I believe that one of the best ways to address this issue is to help people build their belief in themselves, to build confidence and provide them with a sense of direction for their life. It's about understanding what you are good at. It's about understanding your own story, what shaped you as a person and what brought you here. It's also about understanding what is important for you and what you value, building a compass for your life. As Steven Covey writes in his book The 7 Habits of Highly Effective People, \u201cPeople can\u2019t live with change if there\u2019s not a changeless core inside them. The key to the ability to change is a [clear understanding] of who you are, what you are about, and what you value\u201d(Covey et al., 2020). With a clear understanding of your story and what your most fundamental values and priorities are you will be better able to flow with changes. This is because I believe that better understanding yourself and your life makes you more resilient and confident in your ability to persevere, no matter what happens.</p><p>If you get to know these things about yourself your sense of identity and inner-directedness will likely infuse you with both exhilaration and peace with which you can handle the more outward vulnerability with strength. This is particularly important when learning about the ideas of EA as they have huge implications for your life. If people don\u2019t have this strength to step out of their comfort zone, people won't arrive at the point where they will naturally become \u201chighly engaged\u201d and take serious action on EA ideas. This is, amongst others, because coupling the internalised reasons on why doing good matters with your identity, requires you to rethink many things, and to be open for that. This requires putting yourself in a vulnerable position because the new things you learn are frequently not aligned with your other goals and values. And that is not easy.</p><p>It's one thing to have people join your Introductory EA Program, it's another thing to actually enable them to adopt the ideas of EA, and rigorously apply them to their life. To change the world, you need to believe that you can make a difference. And in order to adopt this belief, it is important to understand who you are and what you are about.</p><h1>From finding to creating&nbsp;</h1><p>EA is still in its infant hood, and the potential of our community is <strong>enormous</strong>. EA university groups are likely the single biggest contributors to the establishment of our movement (Open Philantrophy, 2020). I think it is not an exaggeration to state that EA university groups are&nbsp;perhaps&nbsp;the most influential stakeholder group in creating&nbsp; the&nbsp;long-term expected altruistic value of our&nbsp;movement. Many members of our groups are going to eventually move into&nbsp;<strong>very</strong> influential positions that are going to have a direct impact on how well we as a human kind are able to live up to the global problems we are facing. It is crucial to speak about how this can be achieved in the best way possible, and what should be avoided.</p><p>If EA university groups are only cherry picking those who are among the small percentage of people who have had the time and opportunity to make doing good already a core part of their identity we are likely severely limiting our potential. With this approach we ignore the fact that individual priorities and aspirations can shift dramatically in a short period of time. Young adults are living in a time of extreme change.&nbsp;By solely focusing on those who are already interested in learning about how to do good better, we are filtering out the huge proportion of people who could be just as good fit, if approached from the right angle. <strong>These are all people&nbsp;we&nbsp;need. These are all people that could significantly contribute to tackling the world's most pressing problems if they also become intrinsically motivated to do good. </strong>Too much is at stake to settle only for the low hanging fruit. It's like our entire ship is sinking and we need to gather food to store on our raft, but we only care about food that is ready to consume, and leave the bananas and avocados behind because they are not ripe yet. While this analogy has limitations, I think that it tells an important story.</p><p>If we want to create as much counterfactual impact as possible, it\u2019s time to look beyond the minority of individuals who have already made doing good a core part of their identity.&nbsp;We need to focus more on helping people become naturally highly engaged individuals rather than simply looking for them.</p><h1>The inception of the Purpose and Life Planning Track</h1><p>People may intellectually agree that doing good is important and understand the concepts of Effective Altruism, but unless they have truly internalised the reasons behind why doing good matters and have had the opportunity to build an identity around this, they will very likely lack the motivation to learn more about these ideas, let alone act on them.&nbsp;</p><p>In the last few weeks I have been developing an initiative that is designed to address this bottleneck. The result is the&nbsp;<a href=\"https://www.notion.so/effectivealtruismmaastricht/The-Purpose-and-Life-Planning-PLP-Track-Public-version-0c783d0b296f44c1bda82c33528bd0fa\"><i>Purpose and Life Planning Track</i></a>. In short, this track is aimed to induce the one thing that disproportionately increases an individual's long-term expected impact: <strong>intrinsic motivation to do good.&nbsp;</strong></p><p>Before people will devote a significant amount of time and resources toward tackling issues such as pandemic preparedness, global poverty, wild animal suffering or other pressing problems, people need to be willing to make that investment.&nbsp;</p><p>Contrary to the latest version of the&nbsp;<a href=\"https://docs.google.com/document/d/1uy5Ca2TXJ-BOzjaSk4PNlPGkvolcVY8pe2xm2ZSw5L8/edit\"><u>Introductory EA Program</u></a> developed by CEA (or the In-Depth EA Program) which primarily aims to provide people with the opportunity to learn about&nbsp;<strong>how</strong> to do good better, the Purpose and Life Planning Track is about helping individuals figure out&nbsp;<strong>why</strong> it matters to do good in the first place. Contrary to&nbsp;<strong>increasing</strong> the intrinsic motivation to want to do good, which the Introductory EA Program primarily does, the Purpose and Life Planning Track is designed to&nbsp;<strong>induce</strong> the intrinsic motivation in people to want to do good. The track was created with the intention of forming a synergy with the Introductory EA Program so that we can cater to the needs of people at various stages of their lives.</p><p>It is especially intended for those who are unsure whether doing good matters to them, whether they want to make it a priority in their lives, and what steps they can take to get there. It aims to assist people in discovering fresh viewpoints and insights that will likely result in a personal desire to give something back to the world. It\u2019s about helping individuals form and build the fundamental beliefs and values that will make people want to make a difference. This is accomplished through the attempt to address all of the root causes that I referred to in the previous sections. From learning about yourself and what happiness and social impact is, to creating an initial life plan. It\u2019s designed to knock over the first domino</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673088237/mirroredImages/LaDGhL8yZuz28rdKG/luim1zovt1z0iks2rle1.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673088237/mirroredImages/LaDGhL8yZuz28rdKG/vz4g9im4kedimymjgy6b.png 530w, https://res.cloudinary.com/cea/image/upload/v1673088237/mirroredImages/LaDGhL8yZuz28rdKG/l4i4bcsrbicc8d49my7o.png 1060w, https://res.cloudinary.com/cea/image/upload/v1673088243/mirroredImages/LaDGhL8yZuz28rdKG/bk6u2e83m7x0u3dll9fo.png 1590w, https://res.cloudinary.com/cea/image/upload/v1673088237/mirroredImages/LaDGhL8yZuz28rdKG/bygrfbr20ov0jjmbygo2.png 2120w, https://res.cloudinary.com/cea/image/upload/v1673088242/mirroredImages/LaDGhL8yZuz28rdKG/wkz0of2lxb9boewed9ho.png 2650w, https://res.cloudinary.com/cea/image/upload/v1673088243/mirroredImages/LaDGhL8yZuz28rdKG/mfh970ed0zjjyuswm0lo.png 3180w, https://res.cloudinary.com/cea/image/upload/v1673088243/mirroredImages/LaDGhL8yZuz28rdKG/gmhdjjmor4pneivzehmh.png 3710w, https://res.cloudinary.com/cea/image/upload/v1673088240/mirroredImages/LaDGhL8yZuz28rdKG/pp4uvnw3rumofyy9kjjs.png 4240w, https://res.cloudinary.com/cea/image/upload/v1673088241/mirroredImages/LaDGhL8yZuz28rdKG/r2ovzq9nspqvzxgs83ez.png 4770w, https://res.cloudinary.com/cea/image/upload/v1673088240/mirroredImages/LaDGhL8yZuz28rdKG/lkrpbcd8m94uus9dfjgc.png 5224w\"><figcaption>The synergy of the Introductory EA Program and the Purpose<br>and Life Planning Track</figcaption></figure><h1>A call to action</h1><p>Overall, there are two hypotheses that I have, and I am 70% confident that both of them are true.&nbsp;</p><p><strong>Hypothesis 1:&nbsp;</strong>The Purpose and Life Planning Track is 85% more effective at converting people between the ages of 18 and 24, who are not currently interested in making doing the most good a core part of their life, to being significantly interested and committed, compared to the&nbsp;newest version of the Introductory EA Program<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefo9w068ox2d\"><sup><a href=\"#fno9w068ox2d\">[4]</a></sup></span>.</p><p><strong>Hypothesis 2:</strong>&nbsp;Among individuals between the ages of 18 and 24 who are members of EA university groups, the Purpose and Life Planning Track is 65 % more effective as the In-Depth EA Program and the career planning guide of 80 000 hours at increasing long-term engagement, expected impact and decreasing the likelihood of <a href=\"https://forum.effectivealtruism.org/posts/mZWFEFpyDs3R6hD3r/empirical-data-on-value-drift\">value drift</a>.</p><p>I am excited to test the first hypothesis in the coming year, and I want to collaborate with you to make this as good as possible. If these hypotheses are proven to be correct and supported by data, they could have far-reaching implications for CEA's core strategy, local university chapters, and the EA movement as a whole.</p><p>Around the beginning of February, the core team of EA Maastricht will organise a range of activities to scale this initiative, and we want to work together with you!&nbsp;</p><p>Why is this worth your time?&nbsp;Because this could be one of the most impactful projects &nbsp;you could be working on now. Consider the following: If implemented effectively, the PLP Track has the potential to inspire thousands of other people, potentially a 10x number of people, to pursue high-impact career paths that they would not have pursued even after hearing about EA. All of these individuals could eventually contribute to tackling problems such as AI alignment, factory farming, climate change and global poverty. There may be no better time to mobilise more people to address the world's most pressing issues than right now.</p><p>There are many things that still can be explored, reconsidered and improved. To make this track as good as possible we are planning to set up a variety of activities such as interdisciplinary project work, red-teaming contests and workshops. With these means we aim to tackle important areas such as risk management, outreach and running the track at your group or institution. <strong>If you are curious to learn more about this, how you can get involved and take look at the curriculum, feel free to take a look at </strong><a href=\"https://effectivealtruismmaastricht.notion.site/The-Purpose-and-Life-Planning-PLP-Track-Public-version-0c783d0b296f44c1bda82c33528bd0fa\"><strong>this Notion page</strong></a><strong>.</strong></p><p>To my best understanding nobody in the EA community building domain is seriously working on this, so it is highly neglected. There are potentially hundreds of thousands of people who will become interested in EA who would not have otherwise, so the scale is potentially very large. It seems a lot more solvable to increase the inclination of people toward EA than it is commonly assumed, perhaps moderately to highly tractable.&nbsp;</p><p>If you have any feedback, criticism or remarks that you want to communicate to me in person, please reach out at <a href=\"mailto:johandekock@ea-maastricht.org\">johandekock@ea-maastricht.org</a></p><h1>Why could this be a very big deal?</h1><p>Doing the most good is <strong>very difficult</strong>.&nbsp; We are dealing with radical uncertainty and inherently complex, interwoven <a href=\"https://wicked-problem.press.plymouth.edu/chapter/what-is-a-wicked-problem/\">wicked problems</a> that require solutions <strong>that must evolve over time</strong>. I hope that this track contributes to making people a lot more motivated to live up to this challenge. I genuinely believe that this could be a significant step toward giving many more people the confidence, ability, and motivation to tackle the world's most pressing problems head on.</p><p>Why could this be a very big deal? Because this new approach has the potential to become one of the best methods so far in EA community building to generate counterfactual impact, on an <strong>unfathomable level</strong>. We are talking about hundreds of thousands of students, if not millions, who we could nudge towards wanting to do good, and eventually doing good better. Students who would not have otherwise chosen to go down a path with a higher impact or making doing good a core priority in their life. Students who would only notice way down their career that contributing to something important is what they want to do, in a position where it is a lot less likely to change career paths.</p><p>This track, if implemented effectively, will most likely result in both a positive short-term impact and potentially large expected impact for the far future. The latter is accomplished by helping people develop an accurate understanding of what contributes to a happy and meaningful life and the former by increasing the likelihood that people will join the Introductory EA Program and pursue a career path that is aimed at tackling the world's most pressing problems<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbufrk8k9mj\"><sup><a href=\"#fnbufrk8k9mj\">[5]</a></sup></span>. In other words, it sets the foundation for developing a life filled with the sincere and voluntary use of one\u2019s time, talent and resources to better the lives of others and one\u2019s own life.</p><p>This track does not have a high bar as the Introductory EA Program has and it does not require to be a \u201cdo gooder\u201d.&nbsp;This track is accessible to almost anyone who desires to live a happy and meaningful life, and it can be implemented in&nbsp; places such as high schools, colleges, companies, and governmental institutions. This could be the beginning of a new chapter in EA community building.</p><p>I hope that this Purpose and Life Planning Track marks the beginning of a more holistic and comprehensive approach for EA university groups to help people to live up to this incredible time we live in. By helping people think carefully about what ultimately matters in their life, many people will come to the conclusion that giving something back to the world is one of the best ways to lead a meaningful life.&nbsp;</p><p>We can lay the foundation for addressing the world's most pressing problems by empowering individuals to recognize their ability to make a real difference, to build themselves something to live for and&nbsp;to&nbsp;provide them with the necessary tools to do so. In my eyes this has&nbsp;to start with the deep aspiration to make the world a better place, and people need to be fundamentally intrinsically motivated to do so.&nbsp;</p><p>By empowering people to understand the link between doing good and happiness and the great progress we can contribute to, we can truly create a community of people who cares about trying to do the most good possible. A community of people determined to be as competent as possible in tackling the world's most pressing problems with everything we have. To contribute to a world that has so much potential for being a better place.</p><p>But we are not there yet. There are many things that still need to be fixed, and we don\u2019t have a lot of time. So we need more talented, ambitious and altruistic people to focus full-time on solving pressing global problems? <strong>Let\u2019s not only try to find them, let\u2019s create them.&nbsp;</strong></p><blockquote><p>\"To the hundred billion people before us, who fashioned our civilization; To the seven billion now alive, whose actions may determine its fate; To the trillions to come, whose existence lies in the balance.\" - Toby Ord, The Precipice</p></blockquote><h1>Please criticise my ideas&nbsp;</h1><p>Needless to say,&nbsp;I have been wrong before and am very open to having my mind changed on them and any other points contained or implied therein.&nbsp;So far the things that I have written here are based on my worldview, the people that I know of, the insights from the literature I have analysed and my experience of running EA Maastricht. There are things that I did not think of, and there are surely cognitive biases at play that have influenced what I have written. Having said that, I believe that the core statements that I made are true (90 %). But I\u2019d love it to be proven wrong. So I want to encourage you to criticise my ideas and my arguments with everything you have.&nbsp;</p><p>What do you think?&nbsp;</p><p>People I would be especially happy to get feedback from:</p><ul><li><a href=\"https://forum.effectivealtruism.org/users/jessica_mccurdy\"><u>Jessica McCurdy</u></a>, University Group Team Lead at CEA</li><li><a href=\"https://forum.effectivealtruism.org/users/jake-mckinnon-1\"><u>Jake McKinnon</u></a>, Groups Associate at CEA</li><li><a href=\"https://forum.effectivealtruism.org/users/jesse-rothman-1\"><u>Jesse Rothman</u></a>,&nbsp;Broad Groups Program Manager at CEA</li><li><a href=\"https://forum.effectivealtruism.org/users/william_macaskill\"><u>William MacAskill</u></a>,&nbsp; President of CEA</li><li><a href=\"https://forum.effectivealtruism.org/users/cafelow\"><u>Catherine Low</u></a>,&nbsp;Groups and Community Health Associate at CEA</li><li><a href=\"https://forum.effectivealtruism.org/users/clairezabel\"><u>Claire Zabel</u></a>,&nbsp;Senior Program Officer at Open Philanthropy Project</li><li><a href=\"https://forum.effectivealtruism.org/users/holdenkarnofsky\"><u>Holden Karnofsky</u></a>,&nbsp;Co-CEO at Open Philanthropy</li><li><a href=\"https://forum.effectivealtruism.org/users/benjamin_todd\"><u>Benjamin Todd</u></a>,&nbsp;Founder of 80,000 Hours</li><li><a href=\"https://forum.effectivealtruism.org/users/kuhanj\"><u>Kuhan Jeyapragasan</u></a>,&nbsp;Stanford Existential Risks Initiative Program Director</li></ul><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673020626/mirroredImages/LaDGhL8yZuz28rdKG/fszwhdczn9tcihybwcap.png\"></figure><h1>Bibliography</h1><p>CEA. (n.d.). <i>A huge opportunity for impact: movement building at universities</i>. EV On Notion. <a href=\"https://centreforeffectivealtruism.notion.site/centreforeffectivealtruism/A-huge-opportunity-for-impact-movement-building-at-universities-772f4cc1684643a7b83d962fd544924d\">https://centreforeffectivealtruism.notion.site/centreforeffectivealtruism/A-huge-opportunity-for-impact-movement-building-at-universities-772f4cc1684643a7b83d962fd544924d</a>&nbsp;</p><p>Chen, H., &amp; Zeng, Z. (2021). When Do Hedonic and Eudaimonic Orientations Lead to Happiness? Moderating Effects of Orientation Priority. <i>International Journal of Environmental Research and Public Health</i>, <i>18</i>(18), 9798. <a href=\"https://doi.org/10.3390/ijerph18189798\">https://doi.org/10.3390/ijerph18189798</a>&nbsp;</p><p>Covey, S. R., Collins, J., &amp; Covey, S. (2020). <i>The 7 Habits of Highly Effective People: 30th Anniversary Edition (The Covey Habits Series)</i> (Anniversary). Simon &amp; Schuster.</p><p>Deci, E. L., Eghrari, H., Patrick, B. C., &amp; Leone, D. R. (1994). Facilitating Internalization: The Self-Determination Theory Perspective. <i>Journal of Personality</i>, <i>62</i>(1), 119\u2013142. <a href=\"https://doi.org/10.1111/j.1467-6494.1994.tb00797.x\">https://doi.org/10.1111/j.1467-6494.1994.tb00797.x</a>&nbsp;</p><p>Education Data Initiative. (2022). <i>College Enrollment by Age</i>. educationdata.org. <a href=\"https://educationdata.org/wp-content/uploads/74/College-Enrollment-by-Age.webp\">https://educationdata.org/wp-content/uploads/74/College-Enrollment-by-Age.webp</a>&nbsp;</p><p>Ford, B. Q., Shallcross, A. J., Mauss, I. B., Floerke, V. A., &amp; Gruber, J. (2014). Desperately Seeking Happiness: Valuing Happiness is Associated With Symptoms and Diagnosis of Depression. <i>Journal of Social and Clinical Psychology</i>, <i>33</i>(10), 890\u2013905. <a href=\"https://doi.org/10.1521/jscp.2014.33.10.890\">https://doi.org/10.1521/jscp.2014.33.10.890</a>&nbsp;</p><p>Friborg, P. I. (2022, August). <i>How to Incubate Self-Driven Individuals (for Leaders and Community Builders) - EA Forum</i>. <a href=\"https://forum.effectivealtruism.org/posts/BoQZhKs26BdyQHpte/how-to-incubate-self-driven-individuals-for-leaders-and\">https://forum.effectivealtruism.org/posts/BoQZhKs26BdyQHpte/how-to-incubate-self-driven-individuals-for-leaders-and</a>&nbsp;</p><p>Government of India; Ministry of Human Resource Development. (2019). <i>All India Survey on Higher Education</i>. Retrieved December 23, 2022, from <a href=\"https://www.education.gov.in/sites/upload_files/mhrd/files/statistics-new/AISHE%20Final%20Report%202018-19.pdf\">https://www.education.gov.in/sites/upload_files/mhrd/files/statistics-new/AISHE%20Final%20Report%202018-19.pdf</a>&nbsp;</p><p>Greene, J. (2014). <i>Moral Tribes: Emotion, Reason, and the Gap Between Us and Them</i> (Reprint). Penguin Books.&nbsp;</p><p>Hanson, M. (2022, July 26). <i>College Enrollment &amp; Student Demographic Statistics</i>. Education Data Initiative. <a href=\"https://educationdata.org/college-enrollment-statistics\">https://educationdata.org/college-enrollment-statistics</a>&nbsp;</p><p>Hare Consulting. (2020). <i>The Golden Circle - Simon Sinek</i>. <a href=\"https://hareconsulting.com.au/wp-content/uploads/2020/04/107-1071590_goldencircle-simon-sinek-golden-circle-transparent-png.png\">https://hareconsulting.com.au/wp-content/uploads/2020/04/107-1071590_goldencircle-simon-sinek-golden-circle-transparent-png.png</a>&nbsp;</p><p>Jachimowicz, J., Guenoun, B., To, C., Menges, J., &amp; Akinola, M. (2017). Pursuing Passion through Feelings or Values: How Lay Beliefs Guide the Pursuit of Passion. <i>PsyArXiv</i>. <a href=\"https://doi.org/10.31234/osf.io/qj6y9\">https://doi.org/10.31234/osf.io/qj6y9</a>&nbsp;</p><p>Krekel, C., De Neve, J. E., Fancourt, D., &amp; Layard, R. (2021). A local community course that raises wellbeing and pro-sociality: Evidence from a randomised controlled trial. <i>Journal of Economic Behavior &amp;Amp; Organization</i>, <i>188</i>, 322\u2013336. <a href=\"https://doi.org/10.1016/j.jebo.2021.05.021\">https://doi.org/10.1016/j.jebo.2021.05.021</a>&nbsp;</p><p>Mauss, I. B., Tamir, M., Anderson, C. L., &amp; Savino, N. S. (2011). Can seeking happiness make people unhappy? Paradoxical effects of valuing happiness. <i>Emotion</i>, <i>11</i>(4), 807\u2013815. <a href=\"https://doi.org/10.1037/a0022010\">https://doi.org/10.1037/a0022010</a>&nbsp;</p><p>Moss, D. (2021, May). <i>EA Survey 2020: Demographics - EA Forum</i>. <a href=\"https://forum.effectivealtruism.org/posts/ThdR8FzcfA8wckTJi/ea-survey-2020-demographics\">https://forum.effectivealtruism.org/posts/ThdR8FzcfA8wckTJi/ea-survey-2020-demographics</a>&nbsp;</p><p>Mozhvilo, E. (2021). <i>Buds close before blooming</i>. Unsplash.com. <a href=\"https://images.unsplash.com/photo-1619974612677-5e1ed30d12ff?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;auto=format&amp;fit=crop&amp;w=1173&amp;q=80\">https://images.unsplash.com/photo-1619974612677-5e1ed30d12ff?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;auto=format&amp;fit=crop&amp;w=1173&amp;q=80</a>&nbsp;</p><p>Open Philantrophy. (2020). <i>Open Phil EA/LT Survey 2020: Introduction &amp; Summary of Takeaways</i>. Google Docs. <a href=\"https://docs.google.com/document/d/1iJNFfmcmbMMx-xw19Wnlin9ZXWQz1PJtcoS1GAGN8K8/edit#heading=h.xp340vv3zyo\">https://docs.google.com/document/d/1iJNFfmcmbMMx-xw19Wnlin9ZXWQz1PJtcoS1GAGN8K8/edit#heading=h.xp340vv3zyo</a>&nbsp;</p><p>Sapolsky, R. M. (2018). <i>Behave: The Biology of Humans at Our Best and Worst</i> (Reprint). Penguin Books.</p><p>Sheldon, K. M., Corcoran, M., &amp; Prentice, M. (2018). Pursuing Eudaimonic Functioning Versus Pursuing Hedonic Well-Being: The First Goal Succeeds in Its Aim, Whereas the Second Does Not. <i>Journal of Happiness Studies</i>, <i>20</i>(3), 919\u2013933. <a href=\"https://doi.org/10.1007/s10902-018-9980-4\">https://doi.org/10.1007/s10902-018-9980-4</a>&nbsp;</p><p>Statista. (2022, January 24). <i>Durchschnittsalter der Studienanf\u00e4nger in Deutschland bis 2020</i>. <a href=\"https://de.statista.com/statistik/daten/studie/36660/umfrage/durchschnittsalter-der-studienanfaenger-seit-1995/\">https://de.statista.com/statistik/daten/studie/36660/umfrage/durchschnittsalter-der-studienanfaenger-seit-1995/</a>&nbsp;</p><p>Street, J. (2017). <i>Yosemite National Park, United States</i>. Unsplash.com. <a href=\"https://images.unsplash.com/photo-1505778276668-26b3ff7af103?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;auto=format&amp;fit=crop&amp;w=1461&amp;q=80\">https://images.unsplash.com/photo-1505778276668-26b3ff7af103?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;auto=format&amp;fit=crop&amp;w=1461&amp;q=80</a>&nbsp;</p><p>TED. (2017, September 26). <i>There\u2019s more to life than being happy | Emily Esfahani Smith</i> [Video]. YouTube. <a href=\"https://www.youtube.com/watch?v=y9Trdafp83U&amp;feature=youtu.be\">https://www.youtube.com/watch?v=y9Trdafp83U&amp;feature=youtu.be</a>&nbsp;</p><p>Times, T. N. Y. (2005, December 29). <i>Opinion | In Pursuit of Unhappiness</i>. The New York Times. <a href=\"https://www.nytimes.com/2005/12/29/opinion/in-pursuit-of-unhappiness.html\">https://www.nytimes.com/2005/12/29/opinion/in-pursuit-of-unhappiness.html</a>&nbsp;</p><p>Waterman, A. S. (1993). Two conceptions of happiness: Contrasts of personal expressiveness (eudaimonia) and hedonic enjoyment. <i>Journal of Personality and Social Psychology</i>, <i>64</i>(4), 678\u2013691. <a href=\"https://doi.org/10.1037/0022-3514.64.4.678\">https://doi.org/10.1037/0022-3514.64.4.678</a>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnw3sqdwn73kp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefw3sqdwn73kp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is also what Self-Determination Theory suggests. People need to internalise external factors (such as having a positive impact on the world) before they will be intrinsically motivated to do something. If these external factors are not well internalized, that is, not made compatible with one's own core values and goals, it will undermine intrinsic motivation. In other words, the person will not spend time on a certain activity on their own initiative (Deci et al., 1994); (Friborg, 2022). I highly encourage you to read <a href=\"https://forum.effectivealtruism.org/posts/BoQZhKs26BdyQHpte/how-to-incubate-self-driven-individuals-for-leaders-and\">this</a> great post from Per Ivar Friborg to learn more about this.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnm9sihdjlu7j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefm9sihdjlu7j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I think it is very plausible that thousands of years ago it was important for humans to understand the motivations and intentions of others in order to survive. Understanding the \"why\" behind someone's actions allowed us to predict their behaviour and make informed decisions about how to interact with them. If you had to choose whether you are going to settle with your tribe at a mountain or take the additional risk of travelling further for potentially more resources, you\u2019d better have good reasons to do so.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvuyjrcl3z4a\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvuyjrcl3z4a\">^</a></strong></sup></span><div class=\"footnote-content\"><p>On the <a href=\"https://www.effectivealtruism.org/\">EA website</a>, it is assumed that everyone wants to do good. How do we know this? Even if that were true, how many students have made this one of their top priorities? Despite offering a comprehensive overview of EA ideas and practices, the <a href=\"https://forum.effectivealtruism.org/handbook\">revised EA handbook</a> and the latest version of the <a href=\"https://docs.google.com/document/d/1uy5Ca2TXJ-BOzjaSk4PNlPGkvolcVY8pe2xm2ZSw5L8/edit\">Introductory EA Program</a> fail to clearly articulate the underlying reasons for why one should care about EA. This lack of clarity may pose a significant barrier to individuals internalizing and fully embracing EA concepts, as it does not address how these principles align with and can enhance an individual's existing values and beliefs. To truly facilitate understanding and adoption of EA ideas, it is crucial that the \"why\" behind these ideas is made explicit and directly connected to personal values.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fno9w068ox2d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefo9w068ox2d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Specifically, the PLP Track is able to attract a different audience and set the foundation for effective internalization of EA ideas by helping individuals actively think about their values and aspirations, while the Introductory EA Program is not able to accomplish this.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbufrk8k9mj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbufrk8k9mj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The Exploring What Matters course from the UK-based charity <a href=\"https://founderspledge.com/stories/action-for-happiness-summary\">Action for Happiness</a> (AfH) has shown to improve subjective well-being and increase levels of self-reported pro-social behavior. In a published <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0167268121002079?via%3Dihub\">randomized control trial</a>, these effects became evident (Krekel et al., 2021). This suggests that the Purpose and Life Planning track could have similar effects on individuals' well-being and on their willingness to engage in behaviors that benefit others.</p></div></li></ol>", "user": {"username": "Johan de Kock"}}, {"_id": "7E3AGFB86mKYeo5aC", "title": "EAs interested in EU policy: Consider applying for the European Commission\u2019s Blue Book Traineeship", "postedAt": "2023-01-06T18:28:51.674Z", "htmlBody": "<p>Inspired by a series of <a href=\"https://forum.effectivealtruism.org/posts/R5mtSjWmdXthfvLqM/eas-interested-in-us-policy-consider-applying-for-the\">recent</a> <a href=\"https://forum.effectivealtruism.org/posts/nkWA46e6gEcgKmX2e/recent-grads-interested-in-us-policy-consider-the-stpi\">forum</a> <a href=\"https://forum.effectivealtruism.org/posts/rricGZmXQaS2Tafoz/students-interested-in-us-policy-consider-the-boren-awards\">posts</a> highlighting early career opportunities in US policy, this post summarises why and how to apply to the Blue Book Traineeship. This paid, five-month internship programme with the European Commission, the executive body of the EU, is one of the main pathways into an EU policy career. The last section of this post also outlines some other options to get started in EU policy.</p>\n<p>There are two Blue Book sessions each year, with applications opening in January for the session starting in October and in August for a start in March of the following year. Application deadlines can be found<a href=\"https://traineeships.ec.europa.eu/calendar-of-sessions\"> here</a>. <strong>Applications for the October 2023 session are open now and close on January 31</strong>.</p>\n<p>As the programme is suitable for people in different stages of their career and from various backgrounds (see below) and EU policy is arguably still neglected within the EA community, an application could be a good option for many EAs. <strong>The initial application is fairly low-cost, as you only need to upload your CV and documents without writing a motivation letter.</strong></p>\n<p>The programme is not only relevant to students or recent graduates, as many trainees have some years of previous work experience (around 30% of all trainees are 30+ and only 5% younger than 25, see full statistics <a href=\"https://traineeships.ec.europa.eu/home/key-facts-and-figures\">here</a>). Work experience can even be a significant advantage for finding full-time positions after the traineeship, and it can be a strategic decision to only start the traineeship after gaining some work experience to increase the chances of being able to stay on.<sup class=\"footnote-ref\"><a href=\"#fn-Ny8j7x8GuMffvQwox-1\" id=\"fnref-Ny8j7x8GuMffvQwox-1\">[1]</a></sup></p>\n<p><em>Epistemic status: This post is mostly based on my experience completing the traineeship last year and now working full time at the Commission, including conversations with around 20 people before and during the traineeship about both the application process and getting full-time employment afterwards. The post was greatly improved by the contributions of four other EAs with expertise on EU policy.</em></p>\n<p><strong>Eligibility</strong></p>\n<p>The programme is mostly directed towards EU citizens.<sup class=\"footnote-ref\"><a href=\"#fn-Ny8j7x8GuMffvQwox-2\" id=\"fnref-Ny8j7x8GuMffvQwox-2\">[2]</a></sup> The minimum educational requirement is a completed undergraduate degree. However, a master\u2019s degree is sometimes necessary to pass through the first stage of the selection process (especially for \u2018competitive\u2019 nationalities below) and increases employment opportunities after the traineeship.</p>\n<p>The programme is open to graduates of all disciplines, not just people holding policy-related degrees\u2014even rewarding applicants from \u2018rare fields of study\u2019 in the selection process. Most degrees outside of policy, law and economics should fall into this, as the majority of trainees (<a href=\"https://traineeships.ec.europa.eu/home/key-facts-and-figures\">around 70%</a>) hold degrees in one of these fields. <strong>It is therefore a good opportunity for EAs with no previous policy experience interested in testing their fit and learning more about impact in the sector.</strong></p>\n<p>All eligibility requirements are detailed <a href=\"https://traineeships.ec.europa.eu/how-to-apply\">here</a>, including the requirement to prove very good knowledge (B2 level) of two <a href=\"https://european-union.europa.eu/principles-countries-history/languages_en\">official EU languages</a> (English plus one other is sufficient).</p>\n<h2><strong>Background on the European Commission</strong></h2>\n<p>The <a href=\"https://european-union.europa.eu/institutions-law-budget/institutions-and-bodies/institutions-and-bodies-profiles/european-commission_en\">Commission</a> is the executive body of the EU. It draws up initial legislative proposals (which are then amended and adopted jointly by the <a href=\"https://en.wikipedia.org/wiki/European_Parliament\">European Parliament</a> and the <a href=\"https://en.wikipedia.org/wiki/Council_of_the_European_Union\">Council of the EU</a>, the other two main EU institutions) and implements EU policies (e.g., deciding how to allocate funds, outlining the technical details of legislation, monitoring the implementation at member state level). It employs around 30,000 people mostly based in Brussels. The Commission\u2019s main departments according to policy areas, the <a href=\"https://commission.europa.eu/about-european-commission/departments-and-executive-agencies_en\">Directorate Generals (DGs),</a> are comparable to government ministries at the national level. A Blue Book Trainee works for five months in one unit within one of these DGs or in one of the executive agencies.</p>\n<p>The Commission is a rather technocratic institution, which is reflected in its working style. Though specific tasks depend on policy area and position, writing policy notes and briefings as well as attending meetings to exchange information and coordinate policy strategy can usually be expected. Overall, the EU policy process involves coordination with many actors (within the Commission across DGs and hierarchies but also across EU institutions and Member States) and this is reflected in the day-to-day work at all levels of the Commission. Communication abilities (both oral and written) and \u2018people skills\u2019 (especially willingness and ability to network) are therefore some of the key requirements in this field. Having impact - e.g., rising through the ranks to become a senior decision maker or getting your initiatives adopted as a junior civil servant - often requires mastering \u2018office politics\u2019.</p>\n<p>A comprehensive discussion about the overall desirability or impact of an EU policy career is out of scope of the post. As stated above, the EU policy process is quite synergetic, so it can be somewhat difficult for an individual to make major changes - though there are examples of even junior civil servants in the right position influencing important legislations or large sums of spending programmes, and people in senior positions tend to have some discretion over policy decisions. Added to the question of the influence of a civil servant is the general debate about the global importance of the EU in relevant cause areas, which has been discussed on the Forum previously: See <a href=\"https://forum.effectivealtruism.org/posts/yNxn4HxDSMdRyrv6E/collection-of-work-on-should-you-should-focus-on-the-eu-if\">this collection</a> of Forum posts for AI governance (specifically <a href=\"https://forum.effectivealtruism.org/posts/fbG6wWZhJ3jt3xHxS/should-you-work-in-the-european-union-to-do-agi-governance\">this post</a>) or <a href=\"https://forum.effectivealtruism.org/posts/k7W3nz8ME3uxEcm7E/ways-eu-law-might-matter-for-farmed-animals\">here</a> for the relevance of EU animal welfare legislation.</p>\n<p>One benefit of working at the European Commission are the generally great staff conditions. Commission employees earn a relatively high salary (especially at junior level: depending on your contract, entry-level positions pay around \u20ac 3,500 - \u20ac 5,500 monthly tax free; the traineeship salary is ca. \u20ac 1,300) and have a generous vacation allowance. The Commission offers many opportunities for continued learning (language and policy courses during working hours) and you tend to work with motivated colleagues from all over Europe. Working hours can be intense in certain positions and during peak times, though are generally comparable to private sector roles and (mostly) much lower than in other career options popular with EAs like strategy consulting or investment banking (see discussion on working time in the comments). However, somewhat unstable job conditions can be a downside for junior civil servants on temporary contracts (see details in the relevant section).</p>\n<h2><strong>Blue Book Traineeship application process and advice</strong></h2>\n<h3>How competitive is admission to the programme?</h3>\n<p>Each traineeship cohort consists of 700 to 1,000 trainees. Quotas (weighted by the population and number of applicants per country) determine the number of preselected applicants per nationality, making the first stage of the application process much more competitive for applicants of certain nationalities\u2014for example, over the last 4 years, 31.6% of Estonian applicants but only 5.9% of Italian applicants were selected for the traineeship. The table below summarises the <a href=\"https://ec.europa.eu/stages/online/cv/application_statistics.cfm?session=99\">official statistics</a>.</p>\n<p><strong>Application success rate by nationality (Average 2019 - 2022)</strong></p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Applicants</th>\n<th>Trainees selected</th>\n<th>Success rate</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Austrian</td>\n<td>125</td>\n<td>17</td>\n<td>14.0%</td>\n</tr>\n<tr>\n<td>Belgian</td>\n<td>418</td>\n<td>45</td>\n<td>10.8%</td>\n</tr>\n<tr>\n<td>Bulgarian</td>\n<td>118</td>\n<td>18</td>\n<td>15.8%</td>\n</tr>\n<tr>\n<td>Croatian</td>\n<td>96</td>\n<td>17</td>\n<td>17.6%</td>\n</tr>\n<tr>\n<td>Cypriot</td>\n<td>55</td>\n<td>11</td>\n<td>20.4%</td>\n</tr>\n<tr>\n<td>Czech</td>\n<td>103</td>\n<td>16</td>\n<td>15.8%</td>\n</tr>\n<tr>\n<td>Danish</td>\n<td>108</td>\n<td>14</td>\n<td>15.7%</td>\n</tr>\n<tr>\n<td>Dutch</td>\n<td>326</td>\n<td>32</td>\n<td>10.0%</td>\n</tr>\n<tr>\n<td>Estonian</td>\n<td>19</td>\n<td>6</td>\n<td>31.6%</td>\n</tr>\n<tr>\n<td>Finnish</td>\n<td>126</td>\n<td>18</td>\n<td>14.5%</td>\n</tr>\n<tr>\n<td>French</td>\n<td>915</td>\n<td>89</td>\n<td>10.3%</td>\n</tr>\n<tr>\n<td>German</td>\n<td>642</td>\n<td>77</td>\n<td>12.3%</td>\n</tr>\n<tr>\n<td>Greek</td>\n<td>532</td>\n<td>49</td>\n<td>9.3%</td>\n</tr>\n<tr>\n<td>Hungarian</td>\n<td>104</td>\n<td>19</td>\n<td>18.5%</td>\n</tr>\n<tr>\n<td>Irish</td>\n<td>155</td>\n<td>18</td>\n<td>12.0%</td>\n</tr>\n<tr>\n<td>Italian</td>\n<td>2,146</td>\n<td>121</td>\n<td>5.9%</td>\n</tr>\n<tr>\n<td>Latvian</td>\n<td>26</td>\n<td>8</td>\n<td>30.4%</td>\n</tr>\n<tr>\n<td>Lithuanian</td>\n<td>56</td>\n<td>8</td>\n<td>16.9%</td>\n</tr>\n<tr>\n<td>Luxembourger</td>\n<td>26</td>\n<td>5</td>\n<td>21.2%</td>\n</tr>\n<tr>\n<td>Maltese</td>\n<td>22</td>\n<td>5</td>\n<td>21.1%</td>\n</tr>\n<tr>\n<td>Polish</td>\n<td>223</td>\n<td>40</td>\n<td>18.3%</td>\n</tr>\n<tr>\n<td>Portuguese</td>\n<td>364</td>\n<td>30</td>\n<td>8.4%</td>\n</tr>\n<tr>\n<td>Romanian</td>\n<td>310</td>\n<td>33</td>\n<td>11.1%</td>\n</tr>\n<tr>\n<td>Slovak</td>\n<td>80</td>\n<td>14</td>\n<td>16.8%</td>\n</tr>\n<tr>\n<td>Slovenian</td>\n<td>43</td>\n<td>8</td>\n<td>20.6%</td>\n</tr>\n<tr>\n<td>Spanish</td>\n<td>897</td>\n<td>69</td>\n<td>7.9%</td>\n</tr>\n<tr>\n<td>Swedish</td>\n<td>165</td>\n<td>19</td>\n<td>11.4%</td>\n</tr>\n<tr>\n<td>Non-EU member states</td>\n<td>3,015</td>\n<td>33</td>\n<td>1.4%</td>\n</tr>\n<tr>\n<td>Total</td>\n<td>11,215</td>\n<td>839</td>\n<td>7.5%</td>\n</tr>\n</tbody>\n</table>\n<h3>Overview of the application process</h3>\n<p>The application process (nice outline <a href=\"https://traineeships.ec.europa.eu/about/recruitment-procedure\">here</a>) is divided into two stages<sup class=\"footnote-ref\"><a href=\"#fn-Ny8j7x8GuMffvQwox-3\" id=\"fnref-Ny8j7x8GuMffvQwox-3\">[3]</a></sup>:</p>\n<ol>\n<li><strong>Preselection</strong>: General online application filling in your CV and attaching relevant documents (no motivation letter)</li>\n<li><strong>Selection</strong>:\n<ol>\n<li>Apply to up to three eligible units with motivation letters within a week after the selection opens. Units then interview suitable candidates.</li>\n<li>If unsuccessful, you have a chance of being selected from a pool of \u201cremaining applicants\u201d by \u201cremaining units\u201d.</li>\n</ol>\n</li>\n</ol>\n<h3>(1) Preselection: Automatic scoring of your CV to get into the virtual Blue Book</h3>\n<p>Applicants complete an online application form with their CV details\u2014university degrees and results, language skills, work experience etc.\u2014and attach diplomas and certificates for them. It is essential that all your experiences and (language) skills are correctly specified in your initial application and you upload the required proof for all of them. See the <a href=\"https://traineeships.ec.europa.eu/faq?facets__field_eac_topics=55\">FAQs</a> on requirements for supporting documents and what is accepted as proof for language skills. Anecdotally, missing or insufficient documents to prove language abilities and work experiences are a hurdle for surprisingly many applicants. Small deviations, e.g., in the completion date of a degree, can lead to exclusion from the process.</p>\n<p>Each application is scored by an automatic system, with a certain number of candidates from each nationality progressing through the next stage if they reach the required score. The scoring criteria are not public, but I\u2019ve been told that in the previous application process, the importance of the different categories roughly follows this order:</p>\n<ol>\n<li><strong>Education</strong> is by far the most important category, with reportedly half of all points awarded here. The main factor here is the highest degree completed\u2014i.e., applicants holding a master\u2019s degree (or PhD) have an advantage over applicants with a bachelor\u2019s degree. Especially for nationals from the more \u2018competitive\u2019 countries above, it can be difficult to get preselected without a completed master\u2019s degree. The scoring should not take into account the reputation of the university you attended, but the relative performance within the degree (grades, scholarships, prizes etc.).</li>\n<li><strong>Languages</strong>. Claiming a higher proficiency at a language will yield points, but if you cannot prove it with official documents (specified in the FAQs), your application might be rejected after the pre-selection stage.</li>\n<li><strong>Work experience</strong></li>\n<li><strong>International/European profile through education or work abroad</strong>. Having completed a semester abroad with Erasmus supposedly awards additional points here.</li>\n<li><strong>Rare fields of study</strong> (e.g., STEM, as most applicants have backgrounds in politics/law/economics)</li>\n</ol>\n<p>There are no restrictions on reapplying if you don't make it through preselection, so investing some time on the initial application can be a good decision.</p>\n<h3>(2) Selection</h3>\n<p>If an applicant reaches the required score for their nationality and their documents are deemed sufficient proof of claimed skills and experiences, they are selected for the \u2018Virtual Blue Book\u2019 (=the catalogue of preselected applicants). You will get notified of this around 4 months before the start date of the traineeship in an email containing a link to all available traineeship positions. Not every unit has a Blue Book position in each cycle, so make sure you are flexible to choose other units than the ones you were initially interested in or try to enquire early on with the unit you are interested in.</p>\n<p><strong>The portal is open for one week, allowing the candidate to apply with a cover letter for a traineeship in up to three units.</strong> For advice on which units to apply to, see the full section on unit choice below. At this stage\u2014besides the three cover letters\u2014candidates are also asked to fill in a motivation section (500 - 1,500 words) about their general interest in a traineeship with the Commission (rather than outlining the motivation for joining a specific unit). It can be a good idea to prepare this in advance of receiving the preselection results, giving you more time to tailor the unit-specific letters to the text in the job opening in the week the portal is open. The units then start a selection process among candidates that applied for their job description, potentially conducting interviews or assigning writing tasks.</p>\n<p>Applicants should also consider reaching out directly to units<sup class=\"footnote-ref\"><a href=\"#fn-Ny8j7x8GuMffvQwox-4\" id=\"fnref-Ny8j7x8GuMffvQwox-4\">[4]</a></sup>\u2014drawing more attention to your profile in addition to the initial application, or to reach out to more than the three units you can apply to via the portal. This used to be a very common and successful practice in the old application system, when units did not yet publish their vacancies. I\u2019m unsure how relevant it is in the new system, but it can still be part of the strategy\u2014some of the less popular units will be unable to fill their position from the initial applications. If you haven\u2019t received an offer after this \u2018first\u2019 selection round based on portal applications, it can make sense to follow up with some other units with a short email, as they may also still be looking to fill their position. They may otherwise fail to notice your application in the large pool of \u201cremaining applicants\u201d, or assume you may already be taken if your application is outstanding.</p>\n<h3>Interviews</h3>\n<p>Units interested in your profile will often give you a short phone call\u2014either without prior notice or after coordinating by email. These generally last around 15-30 minutes. They seem to mainly serve two purposes, with the first one more important:</p>\n<ol>\n<li>Finding out the candidate\u2019s willingness to join the unit.</li>\n</ol>\n<ul>\n<li>Units want to assure themselves that the candidates they end up selecting will commit to their unit and did not already, or will in the future, confirm with another unit. It therefore seems important in these interviews to signal that you are very interested in the unit and would be excited to join them. If you want to maximise your chances of getting a placement, it can be good to do this with every unit you talk to\u2014even just as a backup option, though this can be risky, as you might have to break this commitment. If you end up getting an offer from a unit you prefer at a later date, inform the unit you have signalled interest to previously as soon as possible that you are no longer available, so they can choose another candidate instead of you.</li>\n</ul>\n<ol start=\"2\">\n<li>Testing the candidate\u2019s knowledge/fit for the unit.</li>\n</ol>\n<ul>\n<li>While most of these calls are more informal interviews without many policy/knowledge questions, some units ask questions on the Commission which require a bit of preparation\u2014e.g., current developments in the policy area, which DGs and agencies are relevant for it, and the overall structure of the DG and the Commission.</li>\n<li>It is useful to prepare a short statement about yourself, keep up to date with the relevant policy area and have a look at the organisation charts and lists of Commission DGs.</li>\n</ul>\n<p>While units can give you indications that they would like to hire you, the formal offer will come from the Traineeship Office, around one month before the start of the traineeship. If a unit was unable to find a trainee until then (or their candidate changed their mind) offers are sent out even later.</p>\n<h2>How to choose which units/DGs to apply for</h2>\n<p><strong>The unit you\u2019re placed in is perhaps the single most important factor determining the success of your traineeship</strong> (e.g., how much you enjoy it, how much you learn, whether you get further employment in the Commission). First, as seems to be the case for most public sector institutions, the ability to have impact within the Commission is highly unevenly distributed between different units\u2014with some doing vastly more impactful and interesting work than others. Second, some units integrate their trainees well and assign interesting tasks, while trainees in other units receive little attention or work assignments. And third, it will be much easier to stay with the Commission after the traineeship in some units: DGs differ in their ability to hire past trainees full-time (though this can be difficult to predict), in their policy on inviting trainees to the CAST (Contract Agent Selection Test), and in their competitiveness for the JPP (Junior Professional Programme, more on both below).</p>\n<p><strong>It is therefore extremely useful to talk to someone within the Commission, asking them if they have any suggestions about which units to join (or not to join).</strong> It would be best to speak directly to someone who interned in a unit of interest\u2014e.g., write up a list of some units that sound interesting to you and reach out on LinkedIn to past trainees in those units or contact people working at the Commission you might have some relation to (e.g., alumni from your university course). <strong>Applicants should ideally start doing this soon after their initial application,</strong> well ahead of the release of the preselection results, as you only have one week to send your three applications once the portal is open.</p>\n<p><strong>Since most trainees who find a full-time job in policy afterwards do so in the immediate environment of their traineeship, the choice of unit should be taken very seriously</strong>, as it can have a large influence on the rest of someone\u2019s career path. While the Commission encourages changing your policy area and DG multiple times throughout your career (HR even inquires personally if someone stays in the same position for 4 to 5 years) and is therefore much more flexible than some administrations at member state level (e.g., <a href=\"https://forum.effectivealtruism.org/posts/7FqszSxJ6NHBcZ7nW/report-on-careers-in-politics-and-policy-in-germany\">Germany</a>), there remains a certain path dependency and you will often spend some years of your career in the policy area first chosen. You can, of course, make an effort to connect with people from other fields early on, e.g., through seminars, by recommendation of other Blue Book trainees and staff, or via mentoring requests, or apply to vacancies posted publicly or on the intranet (more below).</p>\n<p><a href=\"https://ec.europa.eu/info/departments_en\">Here</a> is a full list of DGs and other Commission services. The organisation charts on the webpage of each DG (e.g., <a href=\"https://commission.europa.eu/system/files/2022-12/DG%20CONNECT%20Organisation%20Chart%202022.12.01%20-%20English.pdf\">this one</a> from DG CNECT) give a good overview of the DG\u2019s internal structure and priority areas  and allow applicants to identify relevant units before they publish their positions on the web portal.</p>\n<h3>Influential and particularly EA-relevant units</h3>\n<p>One strategy can be to not just consider DGs directly working on the policy area you\u2019re interested in, but instead try to aim for a placement in the cabinet of a Commissioner<sup class=\"footnote-ref\"><a href=\"#fn-Ny8j7x8GuMffvQwox-5\" id=\"fnref-Ny8j7x8GuMffvQwox-5\">[5]</a></sup> or the Secretariat General (SG). These play an important coordinating role in the policy process and are often regarded as more influential and prestigious. A placement there could give you better insights into how the Commission works and be a potential advantage in applications for further positions in the Commission. The same is true for traineeships in the office of a Director-General (the head of a DG) or Director.</p>\n<p>While these places offer good career capital within the Commission, the downside is that you might work on a very broad set of issues and can only impact each one in a limited way\u2014if you consider one narrow policy area more important (like AI or animal welfare), you might be able to have more influence on policy outcomes in a unit working directly on it instead of just coordinating. While it is very hard to make general statements here, if you find a position working <em>exactly</em> on your preferred policy area within a DG, it might be better to take that one instead of a higher-level coordinating role.</p>\n<p>The Commission does not publish how many candidates apply for traineeships in each DG and unit, so there is no hard data on how competitive each is. Anecdotally, <strong>positions related to international relations (especially placements in the <a href=\"https://www.eeas.europa.eu/_en\">External Action Service</a>) tend to have a large number of applicants. The SG and Commissioners\u2019 cabinets also seem more competitive</strong>, while less well known DGs or units often struggle to recruit suitable trainees, particularly given the new targeted application process. You can alter your chances of being selected by strategically choosing units that may be among the less preferred overall or in which you are more qualified than most trainees (e.g., a unit concentrating on your own Member State or a highly specialised area).</p>\n<p>Below is a very broad list of DGs that could be relevant for some example EA cause areas. Though as outlined above, it is very useful to directly speak to people with experience in these units.</p>\n<ul>\n<li><strong>AI</strong>: unit A2 within <a href=\"https://commission.europa.eu/about-european-commission/departments-and-executive-agencies/communications-networks-content-and-technology_en\">DG CNECT</a> is working on AI policy development and coordination and was the lead on the initial proposal of the EU AI Act and unit A3 ('Microelectronics and Photonics industry') is working on semiconductors and could be relevant for compute governance; Cabinets of Commissioners <a href=\"https://ec.europa.eu/commission/commissioners/2019-2024/vestager/team_en\">Vestager</a> and <a href=\"https://ec.europa.eu/commission/commissioners/2019-2024/breton/team_en\">Breton</a></li>\n<li><strong>Biorisk</strong>: unit B2 within <a href=\"https://commission.europa.eu/system/files/2022-12/organisation-chart-dg-sante_en.pdf\">DG Sante</a> is responsible for health security; the recently founded <a href=\"https://commission.europa.eu/about-european-commission/departments-and-executive-agencies/health-emergency-preparedness-and-response-authority_en\">DG HERA</a> works on pandemic preparedness (stockpiling and R&amp;D funding of medical countermeasures); Cabinet of Health Commissioner <a href=\"https://ec.europa.eu/commission/commissioners/2019-2024/kyriakides/team_en\">Kyriakides</a></li>\n<li><strong>Animal welfare</strong>: DG Sante (unit G3 is responsible for Animal Welfare; unit E2 is responsible for regulatory approval policies of novel foods, including many alternative proteins) <a href=\"https://commission.europa.eu/system/files/2022-12/organisation-chart-dg-agri_en.pdf\">DG Agriculture</a> (unit E3 is responsible for animal products), and <a href=\"https://commission.europa.eu/about-european-commission/departments-and-executive-agencies/research-and-innovation_en\">DG RTD</a> (unit B2 on bioeconomy and food systems coordinates research and innovation funding for alternative proteins), and the cabinets of associated Commissioners.</li>\n<li><strong>Global development</strong>: DG <a href=\"https://international-partnerships.ec.europa.eu/system/files/2022-12/organisation-chart_en_1.pdf\">INTPA</a> (International Partnerships) is responsible for EU development aid. It\u2019s the DG with the most employees and usually the highest number of trainees.</li>\n<li><strong>Climate change and biodiversity</strong>: DG Climate Action (<a href=\"https://commission.europa.eu/system/files/2022-11/organisation-chart-dg-clima_en.pdf\">CLIMA</a>) and DG Environment (<a href=\"https://commission.europa.eu/system/files/2022-11/org_env_en_14.pdf\">ENV</a>) and cabinets of the associated Commissioners (especially Executive Vice President Timmermans, who is in charge of the EU\u2019s Green New Deal)</li>\n</ul>\n<p>Moreover, the impact you have in different units and DGs can vary over time. For example, you could join a unit working on a crucial piece of legislation (such as units in DG Maritime Affairs and Fisheries (MARE) currently working on biodiversity beyond national jurisdiction) or a large spending programme (e.g., DG ECFIN and SG RECOVER coordinated the Covid recovery funds under NextGenerationEU in autumn 2020, the Commission\u2019s current biggest spending programme).</p>\n<h2><strong>Full-time employment within the Commission after the traineeship</strong></h2>\n<p>Anecdotally, most trainees would like to stay on full-time in the Commission (&gt; 80% in an informal poll in my cohort) after their traineeship but many do not manage to do so (estimates of trainees staying on range from 5% - 20%). <strong>Trainees can only stay on within the Commission on temporary contracts</strong>, as the majority of Commission staff are lifetime civil servants (\u2018fonctionnaires\u2019). This status is only awarded after passing a highly competitive exam (\u2018concours\u2019). A few exam sessions, adapted to different profiles, take place each year (find a list of upcoming selection procedures <a href=\"https://epso.europa.eu/en/job-opportunities/upcoming\">here</a>).</p>\n<p><strong>The most prestigious option to stay is the Junior Professional Programme (<a href=\"https://traineeships.ec.europa.eu/pilot-junior-professionals-programme-jpp\">JPP</a>), a two-year programme with three rotations within the Commission</strong>. 25 candidates from around 1,000 applicants (Blue Book trainees and contract agents are eligible) are selected. You can start applying around one month into the traineeship by completing an online application form, where it is advisable to frame yourself as a generalist in line with the nature of the programme. You then have to sit a computer-based multiple-choice exam, which around half of the applicants pass (useful resources to prepare are the mock exams on <a href=\"https://eutraining.eu/\">EU training</a>, <a href=\"https://www.orseu-concours.com/en/5-epso-eu-training-books\">Orseu books</a> and relevant <a href=\"https://www.facebook.com/groups/1786326614922798\">Facebook groups</a>). Each DG then chooses some of the remaining candidates to pass onto the final interview\u2014the process is different in each DG, and it therefore helps to speak to people who are familiar with the one in your DG. In the final panel interview you need to give a presentation on a randomly selected EU policy area. <a href=\"https://docs.google.com/document/d/e/2PACX-1vTp5-wp96nbW01wSS5GG7_wct2RsnJcPfMAaMxF92-XzYM-y6qeV2Ie9uvO4h_5zo0buX9kKR6vahZ7/pub\">This</a> relevant guide written by two EAs presents more specific advice on the JPP selection process.</p>\n<p><strong>Apart from the JPP, there are three types of <a href=\"https://european-union.europa.eu/live-work-study/jobs-traineeships-eu-institutions_en\">temporary contracts in EU institutions</a> with differing job stability, salary and responsibility</strong>. In descending order of both attractiveness and infrequency, these are:</p>\n<ul>\n<li>\n<p><strong>Temporary agents</strong> (TA): relatively rare, can in theory only be offered if no suitable fonctionnaire can be found. The usual contract duration is 2-3 years initially (extended up to a maximum of 6 years) and they receive the same salary and benefits as permanent civil servants. Responsibilities, job title, salary and benefits vary with levels (Administrator, Assistant, Secretary).</p>\n</li>\n<li>\n<p><strong>Contract agents (\u2018CAST\u2019)</strong>: common form of temporary employment, contracts last 1-3 years initially (the median contract duration seems to be 1 year) and can be renewed for up to a total of 6 years. Responsibilities, job title, salary and benefits vary with function groups (I - IV). To be eligible, you have to pass the computer-based multiple-choice CAST (Contract Agent Selection Test; similar to the JPP exam above, with an additional knowledge part). Getting invited to this test is not straightforward: Officially, only units having open positions for contract agents are allowed to invite candidates to sit the test. Some DGs interpret this policy relatively flexibly and invite all their trainees to the test (this should be one factor in deciding which DG/unit to go for). Generally, it can be good to try to get invited to the CAST early on in your traineeship (see dates for upcoming examination periods <a href=\"https://epso.europa.eu/en/job-opportunities/competition/2103/description\">here</a>). This can give you a significant advantage when applying for positions or sending your CV around. Some DGs advertise their CAST vacancies publicly or on the local intranet.<sup class=\"footnote-ref\"><a href=\"#fn-Ny8j7x8GuMffvQwox-6\" id=\"fnref-Ny8j7x8GuMffvQwox-6\">[6]</a></sup> I was successful applying to those and would recommend applying to as many as your profile could be a good fit for from the start of your traineeship, as this will increase the chance of getting invited to the test. The test itself can be a hurdle, but studying with the right resources (see links for the JPP exam above) drastically increases your chances. Once you pass the test, you\u2019re in the pool of eligible candidates for contract agent positions within your function group and all lower function groups. If a unit considers you as a potential candidate, you will be invited for a short interview, for which it is relatively easy to prepare for as they are pretty standardised.</p>\n</li>\n<li>\n<p><strong>Interims</strong>: Additional positions that can be created ad-hoc from a unit\u2019s budget to bridge staff shortages, e.g., if someone is on parental or sick leave. There is no maximum duration, though interns have to take a 1-month employment break every 5 months. They are employed via an employment agency, and thus can not benefit from many of the staff advantages, e.g., tax exemptions and training programmes such as language courses.</p>\n</li>\n</ul>\n<p>While a lot of luck is involved in the ability to stay on after the traineeship (most notably, your unit having the budget to retain you after your traineeship), <strong>two key factors will increase your chances of receiving a full time offer after your traineeship</strong>:</p>\n<ul>\n<li><strong>Good reputation and relations within your unit</strong>. If the people in your unit (especially the Head of Unit and your traineeship advisor) think highly of you and your work, they will likely consider you first if they have an open position you are eligible for. And even if not, their support is often key in the JPP selection stage at DG level. Moreover, they can recommend you for and let you know about positions in other units or invite you to a CAST exam (if the HR department of your DG is not too strict on this).</li>\n<li><strong>Building a network and awareness outside your unit</strong>. You should both aim for \u2018quality\u2019 connections (reach out and go for coffee chats/lunches with people from interesting units) as well as \u2018quantity\u2019 (cold-emailing your CV to heads of relevant units, in case they hire for temporary roles with your background). While connections to more senior officials are important (they are well connected and therefore know about job openings, and also make hiring decisions themselves), you should (especially initially) also aim to connect with more junior employees. This can be less intimidating at the start and they will be much more aware of how hiring for temporary positions work (e.g., how to get invited to sit the CAST) than their senior counterparts.</li>\n</ul>\n<p><strong>The great majority of temporary positions are never advertised publicly</strong> (the main exceptions are the CAST positions listed in footnote 6 and the JPP). This is the main reason why building a network is so crucial\u2014and also overall why the Blue Book traineeship is a standard route into employment in an EU policy institution.</p>\n<h2><strong>Alternative routes into an EU policy career</strong></h2>\n<p>As described above, you may work for the Commission directly via a temporary position without going through the Blue Book traineeship. But this can be difficult without a network on the inside to make you aware of such opportunities. Another option for lateral entry is to pass the above mentioned highly competitive exam to become a permanent civil servant. While some exams are only accessible to Commission staff on temporary contracts (\u2018internal concours\u2019), external concours are generally open to everyone with no previous EU experience necessary.</p>\n<p>The Council of the EU and the European Parliament are the other two main institutions influencing EU policy. While both institutions also offer traineeships (<a href=\"https://www.europarl.europa.eu/at-your-service/en/work-with-us/traineeships\">Schuman traineeship</a> in the Parliament, <a href=\"https://www.consilium.europa.eu/en/general-secretariat/jobs/traineeships/\">Council traineeship</a>), work in the _administration _of these institutions tends to be less interesting or influential. In contrast to the Commission, EU civil servants working in the Parliament and Council administration are generally more removed from policy decisions, performing more administrative tasks instead. Other actors are responsible for the policy decisions in these institutions: member state representatives in the Council and members of the European Parliament (MEPs) or political groups in the Parliament. Importantly, <strong>one can only complete one traineeship exceeding 6 weeks with either the Council, the Parliament (including traineeships with MEPs and political groups) or the Commission</strong>.</p>\n<p>In the <strong>European Parliament,</strong> both individual MEPs (members of the European Parliament) and political groups hire trainees and full-time assistants. For a good summary of the experience and work environment, see <a href=\"https://forum.effectivealtruism.org/posts/TjgXnwNyDcPcTCHWc/policy-work-in-the-european-parliament-impressions-from-my\">this Forum post</a>. MEPs usually have 2-3 assistants (Accredited Parliamentary Assistants, \u2018APA\u2019s), who support their policy work, with one also in charge of communications. The impact of these roles strongly depends on the MEP and which files they work on. Advisors to the political group can also be very impactful, as they can significantly influence the party line on issues that are not yet politicised. Being a member of a political party and the associated network can often be crucial to get or hear about these roles, though many MEPs and parties are also open to hiring people not affiliated with their party. Sometimes APA positions are even posted on EU career portals, so it can be a good idea to set up a Google Alert for this; political groups may also have an online form to express interest in working for them, see e.g., here for the <a href=\"https://www.socialistsanddemocrats.eu/form/spontaneous-application\">S&amp;D group</a>. A good strategy for both traineeships and assistant positions can be to send your CV to MEPs working on the files you\u2019re interested in or trying to schedule coffee chats with their assistants.</p>\n<p>Working on EU policy through the <strong>Council of the EU</strong> largely requires working in the civil service or politics at member state level. This feels like an underrated route to influence EU policy\u2014a civil servant working on amendments to EU legislation in a national ministry might under certain circumstances have more influence on the final legislation than a civil servant working on the first draft within the Commission. There are also some options of switching between the national and EU level, as the experience and network gained on either level will be relevant for work at the other level.</p>\n<p>One interesting option is also to work for a Council presidency. Each of the 27 member states holds it for half a year in a rotating schedule (see the upcoming schedule <a href=\"https://en.wikipedia.org/wiki/Presidency_of_the_Council_of_the_European_Union#List_of_rotations\">here</a>). Roughly a year before the start of its presidency, the administration of a country starts hiring extra people to support it during the surge in work a presidency is associated with. As the Council presidency has considerable influence over files discussed during its term, these positions can be very impactful - and often less competitive than other positions of similar impact, as you are mostly competing only with nationals of your country (or people who speak your country's language). The hiring process varies between countries, but it can generally be useful to send your CV to people working on European files in national ministries or to the  Permanent Representation of your country to the EU.</p>\n<p>There are arguments both in favour of working for the administrations of smaller or larger member states\u2014while large member states like Germany or France will generally have the most influence on final proposals, their ministries are often large bureaucracies with many people working on one file and one civil servant therefore having smaller influence on the country line. Though the voice of smaller countries will have less weight in Council discussions, often there is a single person in charge of multiple files. Policymakers or politicians from smaller member states will also face less competition to be nominated to important EU positions\u2014e.g., Commissioner or Permanent representative to the EU. While the chance of securing one of these influential positions will always be slim, this could lead to outsized impact if successful.</p>\n<p>Another option to work on EU policy is to work for an NGO or influential think-tank (e.g., <a href=\"https://www.bruegel.org/\">Bruegel</a>, <a href=\"https://www.ceps.eu/\">CEPS</a>), though work outside the institutions is mostly outside the scope of this post, as I have only very limited insights into this sector. There are also some organisations in the EA space working on EU policy in Brussels.</p>\n<p>One more programme to highlight is the <a href=\"https://www.trainingforgood.com/europe-tech-policy\">EU Tech Policy Fellowship</a> by Training for Good, which supports and connects talented, morally ambitious people to enter EU AI Policy in an 8-month programme.</p>\n<p>For people still willing to do a masters, the <a href=\"https://www.coleurope.eu/\">College of Europe</a> can be a good option, especially to get a head start in building a network in EU policy. It offers prestigious one-year master\u2019s programmes, many graduates of which end up working on EU policy.</p>\n<p>Finally, I would also like to highlight other Forum posts on policy career opportunities in Europe, <a href=\"https://forum.effectivealtruism.org/posts/WqQaPYhzDYJwLC6gW/ai-governance-career-paths-for-europeans#EU_commercial_regulation\">these</a> <a href=\"https://forum.effectivealtruism.org/posts/XGPW25NZHq2WHbK9w/ai-policy-careers-in-the-eu#Council_of_the_EU\">two</a> on EU AI policy and <a href=\"https://forum.effectivealtruism.org/posts/7FqszSxJ6NHBcZ7nW/report-on-careers-in-politics-and-policy-in-germany\">this one</a> on careers in politics and policy in Germany.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-Ny8j7x8GuMffvQwox-1\" class=\"footnote-item\"><p>However, to be eligible for the Junior Professional Programme (details below), you can only have a maximum of three years of paid work experience (including internships) at the time of application. So perhaps around one to two years of work experience are ideal, to give you multiple chances to apply to the JPP. <a href=\"#fnref-Ny8j7x8GuMffvQwox-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Ny8j7x8GuMffvQwox-2\" class=\"footnote-item\"><p>In theory, it is also open to non-EU citizens. However, nationality quotas (see table below) make the selection procedure for them much more competitive and they have only very limited and often country-specific opportunities for full-time employment within the EU institutions after the traineeship. <a href=\"#fnref-Ny8j7x8GuMffvQwox-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Ny8j7x8GuMffvQwox-3\" class=\"footnote-item\"><p>The application process changes from time to time, so if you read this some time after publication, the post and advice might not be tailored to the current system anymore. <a href=\"#fnref-Ny8j7x8GuMffvQwox-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Ny8j7x8GuMffvQwox-4\" class=\"footnote-item\"><p>You should email the Head of Unit (HoU) and their deputy (dHoU) in this case. As their email addresses are not public, you have to \u2018construct\u2019 it yourself, using the general email format of the Commission (<a href=\"mailto:_Firstname.lastname@ec.europa.eu\">_Firstname.lastname@ec.europa.eu</a>). _Find the last name of the (d)HoU on the DG organisation chart and use the official directory of EU employees (EU <a href=\"https://op.europa.eu/en/web/who-is-who\">Whoiswho</a>) to find out the first and last name  of the deputy HoU if not available on the organisation chart. There are websites which allow you to check whether your constructed email address exists (e.g., <a href=\"https://www.verifyemailaddress.org/\">here</a>). <a href=\"#fnref-Ny8j7x8GuMffvQwox-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Ny8j7x8GuMffvQwox-5\" class=\"footnote-item\"><p>Many cabinets select only trainees of the Commissioner\u2019s nationality. Before using one of your three applications for a Commissioner's cabinet, it can be good to check the current trainees on the team website of the Commissioner or past trainees on LinkedIn to get a feel for whether nationals from different member states are sometimes selected. <a href=\"#fnref-Ny8j7x8GuMffvQwox-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Ny8j7x8GuMffvQwox-6\" class=\"footnote-item\"><p>All the DGs working on international relations (DG NEAR, DG INTPA, EEAS, etc.) and the Joint Research Centre (JRC) publish their CAST vacancies online - see  <a href=\"https://ec.europa.eu/international-partnerships/jobs_en\">here</a> and <a href=\"https://recruitment.jrc.ec.europa.eu/?type=AX&amp;category=FGIV\">here</a> respectively.\nSome DGs publish their CAST vacancies on the local DG intranet (e.g., DG RTD, DG EMPL, sometimes DG GROW). Everyone in the Commission has access to those, so you could ask Commission contacts to keep you updated about relevant roles there. <a href=\"#fnref-Ny8j7x8GuMffvQwox-6\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "EU Policy Careers"}}, {"_id": "hKDmiosHAB4XHcrHL", "title": "What rationale puts a limit to the cost of an EA's (or anybody's) life?", "postedAt": "2023-01-06T18:59:54.972Z", "htmlBody": "<p>Listening to a few episodes of the 80,000 hours podcast, a recurring theme is the value of one's time. A certain amount of money is better invested on one's comfort than on giving to EA causes, or so the argument goes. Money spent on business class air plane tickets can potentially improve sleep, which can save time or lead to better productivity. The utility curve is asymptotic and at some point the benefit of investing in comforts approaches the gain from investing in EA causes.</p><p>&nbsp;</p><p>If we now think about the most influential effective altruists who are still at the relative beginning of their career and potentially have decades ahead of them, it is easy to imagine that they could potentially impact billions or trillions of lives (certainly if we're talking about longtermists). Now imagine scenarios, where such a person gets ill and their life can only be saved through organ transplants or terribly expensive treatments which for whatever reason they are not entitled to. I'm not sure this has happened yet but it is bound to happen.</p><p>&nbsp;</p><p>I could be mistaken, but as far as I'm aware, the EA movement doesn't (yet) put a value on people's life based on their potential to further EA causes. I could see a future where it does though and is confronted with a scenario like the one I outlined. After taking into account all arguments, including the possibility that they could turn out to be the next SBF. From an EA point of view, is their life worth saving more than any other person's life without considering measures like WALY/QALY, and if so should their be an upper limit?</p>", "user": {"username": "Juergen"}}, {"_id": "tazT6rMR3uLmPsLZr", "title": "Is there an \"EA alumni\"\ngroup?", "postedAt": "2023-01-06T10:06:49.061Z", "htmlBody": "<p>That is, a group of former EAs who have moved on to do something else?</p>\n", "user": null}, {"_id": "wn9PkfWWWhpCypep6", "title": "Misha Yagudin and Ozzie Gooen Discuss LLMs and Effective Altruism", "postedAt": "2023-01-06T22:59:28.475Z", "htmlBody": "<p>Misha and I recently recorded a short discussion about large language models and their uses for effective altruists.<br><br>This was mostly a regular Zoom meeting, but we added some editing and text transcription. After we wrote up the transcript, both Misha and myself edited our respective sections.</p><p>I think the final transcript is clearer and contains more information than the original discussion. I might even suggest using text-to-speech on the transcript rather than listening to the original audio. This back-and-forth might seem to ruin the point of presenting the video and audio, but I think it might be straightforwardly more pragmatic.</p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=U6IiTIjCc_U\"><div><iframe src=\"https://www.youtube.com/embed/U6IiTIjCc_U\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><h2><br>Transcript</h2><h3>Sections</h3><ul><li>Opening</li><li>Introduction</li><li>How do we use LLMs already?</li><li>Could EAs contributing to applied LLMs be harmful?</li><li>Potential LLM Application: Management and Emotional Assistance</li><li>Potential LLM Application: Communication, Broadly</li><li>Aside: Human-AI-Human Communication</li><li>Potential LLM Application: Decision Automation</li><li>Potential LLM Application: EA Forum Improvements</li><li>Potential LLM Application: Evaluations</li><li>LLM user interfaces</li><li>What should EAs do with LLMs?</li></ul><h3>Opening</h3><p><strong>Ozzie:</strong>&nbsp; Hello. I just did a recording with my friend Misha, an EA researcher at <a href=\"https://arbresearch.com/\"><u>ARB Research</u></a>. This was a pretty short meeting about large language models and their use by effective altruists. The two of us are pretty excited about the potential for large language models to be used by effective altruists for different kinds of infrastructure.</p><p>This is an experiment with us presenting videos publicly. Normally, our videos are just Zoom meetings. If anything, the Zoom meetings would be unedited. I found that to be quite a pain. These Zoom meetings typically don't look that great on their own, and they don't sound too terrific. So we've been experimenting with some methods to try to make that a little bit better.</p><p>I am really curious about what people are going to think about this and am looking forward to what you say. Let's get right into it.</p><h3>Introduction</h3><p><strong>Ozzie:</strong> For those watching us, this is Misha and me just having a meeting about large language models and their use for effective altruism.</p><p>Obviously, large language models have been a very big deal very recently, and now there's a big question about how we could best apply them to EA purposes and what EAs could do best about it. So this is going to be a very quick meeting. We only have about half an hour.&nbsp;</p><p>Right now, we have about seven topics. The main topic, though, is just the LLM applications.&nbsp;</p><h3>How do we use LLMs already?</h3><p><strong>Ozzie:</strong> So, how do we use LLMs already?&nbsp;</p><p><strong>Misha:</strong> I think I use them for roughly 10 minutes on average per day.</p><p>Sometimes I just ask questions or ask queries like, \"Hey, I have these ingredients. What cocktails can I make?\" Sometimes I try to converse with them about stuff. Sometimes I just use it (e.g., text-davinci-003) as a source of knowledge. I think it's more suitable for areas where verifiable expertise is rare.</p><p>Take non-critical medicine, like skincare. I had a chat with it and got some recommendations in this domain, and I think it turned out really well. I previously tried to search for recommendations and asked a few people, but it didn't work.</p><p>I also use it as an amplification for journaling whenever I'm doing any emotional or self-coaching work. Writing is great. I personally find it much easier to write \u201cas if\u201d I'm writing a message to someone\u2014having ChatGPT obviously helps with that.</p><p>Having a conversation partner activates some sort of social infrastructure in my brain. Humans solve math problems better when they are framed socially. And yeah, doing it with language models is straightforward and really good. Further, sometimes models give you hints or insights that you forgot to think about.</p><p>You can ask tons of questions and be really annoying, which you might not be comfortable doing with your friend or even with a professional. One trick I used previously to prompt a human medical professional to give me some sort of cost-benefit analysis was to ask them: \"Well, if instead of me, you were giving an advice to your son or daughter, what would you do?\" This makes them actually think. With LLMs, you can probe straightforwardly and see where their limits are.</p><p>Ozzie, what about you?&nbsp;</p><p><strong>Ozzie:</strong> Yeah, I have used it in a few cases, particularly to rewrite Facebook posts. Both to clean them up, also to try to change them into Tweets. This had some success. I think rewriting in different styles is quite nice. I've been experimenting with a lot of semi-EA purposes.</p><p>So some of it is like taking very aggressive writing and rewriting it for the EA Forum. And then, ChatGPT knows that writing for the EA Forum means making it super polite and long.</p><p>I'm having trouble getting it to come up with cool metaphors and witty, historic examples. There are a few cases where I tried coming up with a list of 10 examples of this intellectual idea, like negative externalities from this historical era.</p><p>Like, from 400 BC to like, 680 BC or something; in some cases it works and in some cases, it doesn't. It feels like hypothetically you should be able to come up with a Scott Alexander post-type content. Just come up with 10 good historical examples on an idea and write cool anecdotes.</p><p>Also, of course, trying to come up with whole Seinfeld episodes about any topic of your choice or something like that is starting to be possible.</p><p>I tried using it a bit for evaluating information, so I'd say, come up with like 10 attributes to rate this intellectual, and then in each one, judge the intellectual. And it did.</p><p>It definitely is gonna require some work, but it's able to kind of start, it could at least differentiate that Donald Trump gets very low marks as an intellectual. In comparison, most of the intellectuals, I could come up with had very high marks.</p><h3>Could EAs contributing to applied LLMs be harmful?</h3><p><strong>Ozzie:</strong> Next topic. Can this do more harm than good?&nbsp;</p><p>So, there's one camp that may be extreme that would say any use of AI would be advancing AI in the same capability, and that'll be like net bad.</p><p>But that said, A, this is a long argument and discussion. So we want to get more into applications, and we don't want to spend much time on that. But B, my guess is that a lot of people would agree that there are probably just some pretty cheap wins that we could take, without extending the theory of language models or the total capabilities of language models, but still getting some valuable things with them. Some decent risk-reward trade-offs.&nbsp;</p><p><strong>Misha:</strong> I basically agree.</p><p>I think interfaces are one area that currently sucks. Making better interfaces can help everyone think a bit better, especially with later models. I'm not sure if this would substantially accelerate capabilities.</p><p><strong>Ozzie:</strong> In terms of groups that we don't want to have developed better epistemic, there are definitely some authoritarian regimes that would be bad. The next one that could be problematic is AI-development organizations.</p><p>If EA helps develop tools that help people have better epistemic or rational reasoning, maybe they'll be used to make AIs faster. Again, I don't know how big of a deal this is, but I think that may be one of the main things to be thinking about.&nbsp;</p><p><strong>Misha:</strong> I basically expect AI labs to start squeezing out these models to improve productivity. Though I think most of it, for now, will be via Codex-like tech, e.g., doing sysadmin better. People made ChatGPT simulate a terminal, which is suggestive.&nbsp;</p><h3>Application: Management and Emotional Assistance</h3><p><strong>Ozzie:</strong> So how about we get into applications? That's the main topic. You have a few applications to discuss, and then I have a few applications. What are your favorite applications?&nbsp;</p><p><strong>Misha:</strong> To just continue the discussion about differential progress. People in EA think that independent researchers and other loners struggle without proper management.&nbsp;</p><p>I think a bunch of helpful things in this direction can be achieved via GPT. So I wouldn't be surprised if someone figured out how to turn models into research managers to help with performance coaching and other things. This seems positive.</p><p>Another one: there is a lot of untapped knowledge on Reddit and in other amazing communities. They don\u2019t do proper science, but by just trying to figure out what actually works, they collect useful insight. And you can pull it out of LLMs by asking to be a knowledgeable member of such a community.</p><p>Likewise, you can get perspective through the eyes of others. GPT is sort of a phenomenological museum\u2014you know, a window into other worlds.</p><p>I really like <a href=\"https://www.clearerthinking.org/\"><u>Clearer Thinking</u></a>. They have programs to improve people's mental tools. If you're taking their quiz, it's very nice in isolation, but it\u2019d be cooler if you were able to do them alongside LLM to be able to chat about ideas and analyze results. The conversation is better than a linear progression. \"Oh, I am inconsistent in how I value my time. Why don\u2019t I feel comfortable spending money to buy time?\u201d is an excellent moment to reflect more, and GPT might be good at holding space for it.</p><p>Lastly, I think Scott Alexander wrote about a hypothetical client who was upset with his partner, claiming they didn\u2019t really love them. The problem was that they wanted to have a hot cooked dinner every night they come from work. But their partner works, so it wasn't possible every time. Scott just suggested calling your partner in advance and, if they would be able to come home early enough to cook dinner, just... order Uber Eats. It worked! I think a bunch of human problems are debuggable and are often simple.</p><p><strong>Ozzie:</strong> How would that work: of GPT understanding a human's life well enough to know what to recommend and what you need? Would it need some information about what you're doing?</p><p><strong>Misha:</strong> Right.&nbsp;&nbsp;</p><p>And in this case, an emotionally responsive model can simply ask, \"What's up?\" \"Why are you thinking that?\" \"How would that make you feel?\" Someone is creating a safe space for people to think. Because unless you\u2014I dunno\u2014unless I'm writing, I just can't really pull it all together and make a connection. Google Docs is amazing for that. Making it more conversational, I believe, would make it easier for more people.&nbsp;</p><p><strong>Ozzie:</strong> Hypothetically, it seems like people are going to be trying to import people's emails and social media posts and stuff like that to just begin with a lot of information about the person, and then from that they could hypothetically make a lot of recommendations.&nbsp;</p><p>In the interest of time, let's go to the next topic.&nbsp;</p><h3>Application: Communication, Broadly</h3><p><strong>Ozzie:</strong> The first application I have to discuss is communication in general. I wrote <a href=\"https://www.lesswrong.com/posts/PC4yowA2TiRne69iD/expansive-translations-considerations-and-possibilities\"><u>one post</u></a> about this. Arguably, translating English to Russian is similar to translating English spoken by a ninth grader in Detroit into a language spoken by someone very different, using different terminology and cultural markers.</p><p>People want to be communicated with very personally, so having something that understands them very well, knows what they know, and could explain things in terms that they understand just gives you a massive benefit, hypothetically.</p><p>I think in EA there's definitely a whole lot in the philosophical literature that we just don't know and that no one so far has figured out. Hypothetically GPT could understand all of that and say \"oh, these 10 insights are the most valuable.\" Right now, instead, these fields are all like coded with continental philosophical language or coded with other terminology that we're really not used to, or like these other foundation assumptions that we don\u2019t understand or agree with.&nbsp;</p><p>What a lot of people want is a personal tutor that understands it very well and understands how they learn. So we kind of want things like chatbots to become personalized tutors in any individualized style of communication. So hypothetically that's just like a whole lot of value on the table.</p><p><strong>Misha:</strong> An example would be asking Model to summarize key insights of postmodernism for someone who is libertarian-ish or reads LessWrong. This will probably work quite well.</p><p>Likewise, you can communicate in your own language. Well, the doctor doesn\u2019t get my math metaphors or someone else\u2019s cooking metaphors. Contrary to GPT, which speaks both math and cooking fluently and can understand them.&nbsp;</p><p>Say you want to use \"simulated annealing\" as a metaphor for how you approach one area of your life. LLM would understand you, while friends might not. Shorter inferential differences mean less need to explain yourself or build toward a thesis\u2014you can just shoot.</p><p><strong>Ozzie:</strong> I guess one analogy is <a href=\"https://tvtropes.org/\"><u>TV tropes</u></a>. It has great lists of many super-specific things. Like, for many very specific tropes that exist online, here's every single example of it in every type of media.</p><p>That's very different from Wikipedia. There are a lot of intellectual terminologies that are not connected at all. So it is pretty common for me to find that, oh, this interesting term was actually cited in like 10 different manuscripts or something like that and used in these different ways.</p><p>But you should know that each way it's used is slightly different. So it's just like a huge pain that, hypothetically, could be solved. I guess going down a bit and finding mistakes and miscommunications would be great. I think people very often misunderstand stuff online, at least when I'm writing comments to people. I'm constantly misunderstanding people when I try to respond and stuff.</p><p>Hypothetically, there could be a browser extension that flags bad text in red and says \"Oh, these few words you're probably going to misunderstand.\" And then if you hover over it, you can see what it actually means. Of course, that's not as intense as something that rewrites all the content for you in ways that it's pretty sure you're going to understand, which is probably more what you'd like.&nbsp;</p><p><strong>Misha:</strong> Right.</p><p>I think you probably thought about having this conversation not between me and you but between you, your AI, my AI, and me. It might smooth things a lot by interacting between different cultures.&nbsp;</p><p><strong>Ozzie:</strong> Yeah.</p><h3>Human-AI-Human Communication</h3><p><strong>Ozzie:</strong> I think that human-to-human conversation is just really mediocre.</p><p>Humans are simply not very good at communicating with one another. It's very complicated. You have to understand where the other person's coming from; we have many different types of humans with different backgrounds. And then <i>you</i> also have to understand <i>them</i>. Both of those challenges are dramatically more complicated than people give them credit for.</p><p>So I think we want to move to this human-AI-human model, where AI is figuring out how to extract valuable information from humans, which probably looks a lot different than them writing essays or anything like that. I have no idea what the ideal is going to be. Perhaps the AI just asks people very targeted questions.</p><p>And then, very separately, the AI develops an embedding of the important information that comes from humans and communicates information that could partially come from that to other humans as it's most needed.</p><p>The way to write an essay in the future, for a human, would be not to write an essay but instead to get interrogated by a language model.</p><p>In another world, we would actually just stop with the idea of essays. Instead, you get like four intellectual credits, because that's about how much interesting information you've contributed to the AI system. And then, when people want information, they just get whatever ideas they want. Content comes from a mess of different people and stuff, and that's totally fine.</p><p>Miscommunication is huge. If you could completely eliminate miscommunication online, I think that would be worth a lot of money. I think therapists, like marriage therapists, do a lot of trying to get communication to be decent. So if an AI could do that for you, that would be huge.</p><p><strong>Misha:</strong> Right.&nbsp;</p><h3>Application: Decision Automation</h3><p><strong>Misha:</strong> Also, like a lot of small businesses, some are dysfunctional. The Profit, a TV show, shows how a lot of them are disasters. Sure, there are some selection effects. But a lot of it is due to poor emotional and conversational skills. Well, also poor basic business skills.<br><br><strong>Ozzie:</strong> Now, there\u2019s a super interesting question, which is, <i>\u201cHow much decision-making can we automate pretty easily with a combination of language models and a few other tools\u201d</i>? Like hypothetically, a lot of business decisions are just not being made that well. A lot of bureaucratic decisions are probably worse than a decent guess by a properly trained agent or something.</p><p>It's not super clear exactly when we'll reach what thresholds. But yeah, there are a lot of bad decisions being made.</p><p><strong>Misha:</strong> Right. I think it would be like really hard to secure agreement from the involved actors. People don\u2019t like when others override their decisions, and they wouldn\u2019t like LLMs to do it either. I think this is one of the problems with the adoption of prediction markets in corporations.&nbsp;</p><p><strong>Ozzie:</strong> I think one good model is that of autonomous vehicles.</p><p>It\u2019s level one before you get to level five. Levels one and two are just like driver assist. And then, as the systems get better, people will rely on them more.&nbsp;</p><p><strong>Misha: </strong>Slower adaptation is good. Initially just ask good probing questions in a non-threatening manner, and you're like, \"<i>oh yeah, that's a good point. I'll, I will do that</i>\". And because no one told you to do that, whether it was some AI from OpenAI or another company, you don't even lose social standing. You don\u2019t look weak or anything. So even a very bossy person can agree.</p><p>As a side note, I think the same dynamic might enable men to do more therapy-shaped things via LLMs.</p><p><strong>Ozzie:</strong> I imagine, too, that these AI models would be very useful in the sense that they'd be deemed much less biased than a lot of the bosses and other people making the decisions.</p><p>If someone's in charge of a big government contract and wants to make a big decision about it, that might be suspect. There are a lot of opportunities for bias.</p><p>If they had an AI kind of say, \"<i>oh yeah, that's what I would do, too</i>\", that would give them a fair bit more credibility. So just having some assistance could be useful.&nbsp;</p><p>There are also definitely some situations where I expect people to only trust AI because, like, they just can't trust any responsible person to make a decent decision.</p><h3>Application: EA Forum Improvements</h3><p><strong>Ozzie:</strong> There are a bunch of potential uses of GPT for the EA Forum.</p><p>First, there are a lot of new users who really take time to get used to the community norms. They come in pretty grumpy, or they don't know what terminology we use. EA does have pretty unusual epistemic standards that are difficult to teach.<br><br>In theory, when you're writing a post, you should be able to see in real-time what the comments will be and what the karma for that post will be. So as you're editing, you write a bad sentence and immediately see an imagined angry comment. And then you say, \"Okay, I guess I'll just delete that sentence.\" Obviously, this assistant service does have some harm as well as benefits, but hypothetically, some of the angry comments that we see could just be seen in advance and then prevented.</p><p>You could also have less intense steps, such as \"Oh, your writing style probably isn't up to the best standards and probably won't be properly appreciated, but here's our version of rewriting it, that you're free to take inspiration from.\"</p><p>In terms of people reading articles on the EA Forum, we may just have 10 different versions of an article or something. Writers give it a few sliders of, \"Oh, do I want the very summarized version? Do I want the, like, very long version? Do I want it to use stories from what historical period or something similar?\u201d And the articles could be automatically adjusted accordingly.</p><h3>Application: Evaluations</h3><p><strong>Ozzie:</strong> I see evaluation is a really major application for LLMs. In this case, it just means that for every EA Forum post or comment, we have an estimate of how well-written this is and how likely this is to hold up after scrutiny. How inflammatory does it seem? How good or bad does this post seem on a few different spectrums?</p><p>Hypothetically, we could have a lot of these auto-evaluations done. That's just on the EA Forum, of course. We'd really want that in all of Twitter and all of the media. Of course, it's debatable what those would look like.</p><p><strong>Misha:</strong> Yeah. Unbelievable today this is in the realm of possible, while some years ago sentiment prediction was mediocre or so.&nbsp;</p><p><strong>Ozzie:</strong> A whole lot of things are possible all of a sudden. It's like a really freaking interesting time.&nbsp;</p><h3><strong>LLM UXs</strong></h3><p><strong>Ozzie:</strong> So, moving on, um, UX issues. What do you think about UX issues?</p><p><strong>Misha:</strong> Oh yeah. You basically have two interfaces. One is Chat GPT, where, like your model, it is very polite and restricted, which is not ideal, but the chat interface is nice. Another one is to just continue the line, text completion, where you can, of course, recreate the chat model and other things.</p><p>But this all seems not that exciting because the second one is just LLM default, predicting the next token. Chat mode is nice and more humane and hence appreciated. But also, it\u2019s the first idea you would have. I think Janus builds what they call \"multiverses,\" where you just branch text in all sorts of directions, give only a few prompts, and rely mostly on curation. This is really nice. This is another interface that is available.</p><p>Today's appreciation and perception of capabilities are probably limited by the fact that you just get one thing as a response and can't easily customize how it's generated.</p><p>The next thing would be to have conversations with multiple LLM-simulated partners, which would have different characters, different perspectives, and so on. You\u2019d be able to have a wide array of shoulder advisors. Someone who is more direct would be straightforward; someone else would be more nuanced and careful; someone would ask, \u201cAll these considerations are good, but what do you want?\u201d; another would be appreciative and emotionally soothing or something.</p><p>I hope for more interfaces. But in practice, humans are kind of bad at developing new ones. It's like we still owe everything to Xerox PARK, right? But interfaces might unlock new creative ways to make models useful and helpful. But it took us a while to figure out that you need to ask them to \u201cthink step by step.\u201d So probably exciting new crazy interfaces are not coming anytime soon.&nbsp;</p><p><strong>Ozzie:</strong> I think my quick take is that, um, on the internet, it probably took thousands of UX and design years to figure out what websites should look like.</p><p>And then, once we had mobile devices, it took many hundreds or thousands of years to figure out what that should look like. It's just a huge amount of work to really figure out the UX and UI for a new medium. And this appears to be more distinct to me than regular Web versus mobile. It seems like a much bigger shift, so I'd expect it to take a lot of time.</p><h3>What should EAs do with LLMs?</h3><p><strong>Ozzie:</strong> So yeah, in the very end, um, what should EA do now?&nbsp;</p><p><strong>Misha:</strong> It's unclear if, like, EAs are anything special. I sincerely hope that people outside of the alignment community will pay more attention to this new technology, adopt these language models, and use it for their own benefit.</p><p>But yes, as previously mentioned, figuring out how to do research management well with LLMs might be very helpful. I would be pretty excited if someone spent a month or more intensely trying to integrate LLMs into their processes and workflows.</p><p>Oh, yeah, I am a bit involved in collecting all sorts of helpful base rates. For forecasting purposes and just to inform people about them. I previously figured out I can just ask ChatGPT to give me more examples of events I am interested in, like \u201cwhat are huge secret projects that have remained secret for a long time?\u201d And of course, I got a list of 30+ and never heard about most of them.</p><p><strong>Ozzie:</strong> Yeah, that makes sense. It is also a big topic. But we need to finish up now. Thanks so much for your time. Any last comments?&nbsp;</p><p><strong>Misha:</strong> Nope. Thanks for hosting, Ozzie.</p><p><strong>Ozzie:</strong> Yeah. Thank you.</p>", "user": {"username": "oagr"}}, {"_id": "pK2pZjrcm3eXsutmd", "title": "Consumer Power Initiative- Active Projects and Open Roles", "postedAt": "2023-01-06T14:40:40.598Z", "htmlBody": "<p>In case you haven\u2019t heard, the Consumer Power Initiative is a 501(c)(3) with a mission of promoting&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/WMiGwDoqEyswaE6hN/making-trillions-for-effective-charities-through-the\">companies with charities in the equity position</a>. We believe that donating is often difficult for everyday people to do, because it competes with their other needs and desires. However, if people are presented with the opportunity to buy the same things they would otherwise purchase at the same price with the added advantage of knowing the profits generated are directed towards charities, it would become a lot less difficult and a lot more common for everyday people to make a significant positive impact. That\u2019s why the Consumer Power Initiative is looking to create a world in which consumers and other economic actors can discriminate in favor of effective charities and transform the world by directing a significant chunk of the 8-10 trillion dollars of global net profits to effective charities.</p><p>We are delighted with the progress that we have made in 2022: we have helped develop the&nbsp;<a href=\"https://consumergivingstore.org/\">Giving Store</a>, expanded our social media presence, and organized many of the current Profit for Good companies around the world into a trade group called the Charitable Profits Alliance (CPA). We will be launching a marketing campaign for CPA so that the people of the world know where they can buy and fund charities simultaneously. The Consumer Power Initiative will also be networking and providing Alliance members contacts with experts and specialists who can help them at a reduced rate.<br>&nbsp;</p><p><strong>Current Projects:</strong></p><p><strong>1. &nbsp; &nbsp; &nbsp;</strong><i>Amplifying and Supporting Charitable Profits Alliance Members</i></p><p>We look forward to launching our webpage featuring our Charitable Profits Alliance members, who have in the course of their existences donated over $6 million dollars to charities. We will announce the current members upon the launch of the webpage. Our mission with the Charitable Profits Alliance consists of:</p><ul><li>Identifying and recruiting Profit for Good Companies to join the Alliance</li><li>Networking with sympathetic specialists, such as consultants, influencers, lawyers, impact investors, lobbyists, accountants</li><li>Conferring with Profit for Good companies to identify their needs and strategize how the Consumer Power Initiative can provide the most value</li><li>Developing a joint marketing plan and otherwise boost public awareness that they can fund charities simply by buying from members of the Charitable Profits Alliance&nbsp;</li></ul><p>&nbsp;</p><p>2. &nbsp; &nbsp;&nbsp;<i> Movement Building/Social Media</i></p><p>We are looking to inform the public about their ability to help solve global problems by buying through Profit for Good companies. We are looking to develop an online presence that makes it clear to philanthropists and other agents who would be able to help further the Profit for Good model that consumers would factor profit destinations into their purchasing decisions.&nbsp;</p><p>Our objectives in movement building/social media include:</p><ul><li>Inform the public about the extent of harm in our primary cause areas: Global Health &amp; Development, Farmed Animal Welfare, and Environmental Degradation. Inform further about the power of effective charities in addressing and solving these problems.</li><li>Excite the public about our project to give them the power to solve these problems without spending extra money by buying through Profit for Good companies.</li><li>Provide the public with the awareness of exciting products from members of the Charitable Profits Alliance and the charities that benefit from them.</li></ul><p>&nbsp;</p><p>3. &nbsp; &nbsp; &nbsp;Creation/Management of E-Commerce PFGs</p><p>The Consumer Power Initiative is looking to expand everyday people\u2019s abilities to fund charities through the purchase of goods and services. To that end, we are looking to expand on the creation of The Giving Store to different product areas. We currently are working on a site that will sell gaming products and 100% of the profits will go towards <a href=\"https://caviver.org.br/\">Caviver</a>, that performs cataract surgeries to&nbsp;prevent blindness in Brazil.</p><p>Thus for our E-Commerce project we are:</p><ul><li>Expanding and developing marketing campaigns for existing Profit for Good companies</li><li>Creating new Profit for Good E-Commerce sites that sell varieties of cool products and serve effective charities.</li></ul><p><strong>Help Create a World Where Global Profits Serve Effective Charities</strong></p><p>It isn\u2019t just Effective Altruists who would prefer the money from their purchases go to end global poverty rather than enrich shareholders, so help us create a world in which profits can follow our natural sympathies. Below is a link to our Open Roles document detailing a number of different ways in which you can volunteer. I would emphasize, however, that <i>if you are a smart, self-starter, with good communications skills, we can definitely use your help.</i>&nbsp;</p><p><a href=\"https://drive.google.com/open?id=15PceMJnecUIUkDN-DZW3nFYApEhTxSud&amp;authuser=brad%40consumerpowerinitiative.org&amp;usp=drive_fs\"><strong>Open Roles Document</strong></a></p><p><i>Please contact brad@consumerpowerinitiative with any interest in volunteering or if you are otherwise interested in learning more or otherwise supporting the CPI or the Profit for Good model. We have compiled a </i><a href=\"https://docs.google.com/document/d/136oCdRIDfQ3nyF0fud8JD_ErhVD2eGEJ?rtpof=true&amp;authuser=brad%40consumerpowerinitiative.org&amp;usp=drive_fs\"><i>reading list</i></a><i> for anyone who is interested in learning more about the Profit for Good model and the Consumer Power Initiative. Please note that CPI is seeking funding at this time to get further off the ground in our mission.</i></p>", "user": {"username": "Brad West"}}, {"_id": "9qWknhfxrgtMoD4J9", "title": "Machine Learning for Scientific Discovery - AI Safety Camp ", "postedAt": "2023-01-06T03:06:34.354Z", "htmlBody": "<h3>Applications for <a href=\"https://aisafety.camp/\">AI Safety Camp</a> 2023 have opened!&nbsp;</h3><p>I'll be Research Lead for the project <a href=\"https://docs.google.com/document/d/1J6aadVK6b2nQGaVNPu0bV13ki0ijEy576CFGF0qV3oo/edit#heading=h.b8lhi4yltarg\">Machine learning for scientific discovery: the present and future of science-producing AI models</a>.&nbsp;</p><h3>Summary:&nbsp;</h3><p>Machine learning models have recently found remarkable success in&nbsp;<a href=\"https://arxiv.org/abs/2206.14858\"><u>quantitative reasoning</u></a>&nbsp;and in producing significant research results in specific scientific fields such as biology (e.g., with&nbsp;<a href=\"https://www.deepmind.com/research/highlighted-research/alphafold\"><u>AlphaFold</u></a>)&nbsp;and&nbsp;<a href=\"https://www.researchgate.net/publication/357234810_Artificial_intelligence_machine_learning_for_chemical_sciences\"><u>chemistry</u></a>.&nbsp;This might be an indication that in the (near-term) future, AI models will be able to generate and/or test novel hypotheses or even produce research worthy of a Nobel prize. This project is about mapping out the current state-of-the-art of science-producing AI models. This will allow for a more comprehensive understanding of the \u201ccognitive properties\u201d or capabilities of the available models. The project focuses on models that generate impressive results in solving quantitative problems (e.g., Minerva) or have led to important breakthroughs and acceleration of scientific research, such as in the case of AlphaFold. The team will collect the relevant research papers, review them, and study the development of capabilities necessary for scientific reasoning in relation to AI risk and progress in alignment research.&nbsp;</p><p>&nbsp;</p><p>If you're unsure whether you'd be a good fit, take the short quiz you can find on <a href=\"https://aisafety.camp/\">this page</a>. Regardless, err towards applying. If you have any questions, feel free to message me.&nbsp;</p>", "user": {"username": "eangelou"}}, {"_id": "cbtoajkfeXqJAzhRi", "title": "Metaculus Year in Review: 2022", "postedAt": "2023-01-06T01:23:07.029Z", "htmlBody": "<p><i>The following is </i><a href=\"https://www.metaculus.com/questions/\"><i>Metaculus's</i></a><i> year-in-review CEO letter written by Gaia Dempsey.</i><br>&nbsp;</p><p>2022 was a year of progress, growth, and change at Metaculus.</p><h2><strong>We matured and grew as an organization.</strong></h2><ul><li><strong>Mission Update:</strong> In the spring, we <a href=\"https://metaculus.medium.com/forecasting-science-and-epistemology-d962de541a1f\"><u>expanded the scope of our mission</u></a>, creating a solid foundation for our work. Specifically, our mission is to build epistemic infrastructure that enables the global community to model, understand, predict, and navigate the world\u2019s most important and complex challenges. We also clearly defined the three primary ways we enact our mission, namely: providing forecasts as a public service, fostering a global forecasting community, and supporting forecasting research.</li><li><strong>Pro Forecaster Program:</strong> In response to a sharp increase in global uncertainty with the outbreak of the war in Ukraine, we designed new operational procedures including an infohazard review process, and recruited over 30 candidates into an exceptionally talented <a href=\"https://www.metaculus.com/help/faq/#what-are-pros\"><u>Pro Forecasting team</u></a>.</li><li><strong>Core Funding:</strong> In the summer, we <a href=\"https://metaculus.medium.com/metaculus-awarded-5-5m-grant-to-advance-forecasting-as-a-public-good-7fc20a161723\"><u>secured a $5.5M core funding grant</u></a> from <a href=\"https://www.openphilanthropy.org/\"><u>Open Philanthropy</u></a> to expand our work.</li><li><strong>Becoming a Public Benefit Corporation: </strong>Shortly after, we committed to providing three specific public benefits in our charter, reflecting our mission statement, as part of the process of <a href=\"https://metaculus.medium.com/becoming-a-public-benefit-corporation-hitting-1-million-predictions-and-three-new-ai-forecasting-7ae4996fee3\"><u>becoming a Public Benefit Corporation</u></a>.</li><li><strong>Growing Our Team:</strong> With a mandate to expand our platform, programs, and operations, we set about upgrading our hiring processes thanks to great <a href=\"https://twitter.com/fianxu/status/1610375237790765058\"><u>resource recommendations</u></a> from Jim Savage (<a href=\"https://whothebook.com/\"><u>a</u></a>), and Aaron Hamlin (<a href=\"https://www.aaronhamlin.com/articles/hiring-ethically-rationally\"><u>b</u></a>), and got laser focused on recruiting. Over the course of the year we doubled our overall headcount, building four strong new teams: <a href=\"https://metaculus.medium.com/welcoming-new-members-of-the-metaculus-leadership-team-d8538142dbd6\"><u>leadership</u></a>, AI forecasting, engineering, and design (some engineering start dates are in early 2023).</li></ul><h2><strong>We launched over a dozen forecasting tournaments and generated over 1000 aggregate forecasts.</strong></h2><ul><li><strong>Expansion of Forecasting Programs:</strong> Over the last year, we expanded our existing partnerships and began collaborations with a number of fantastic new partners. In total, we launched <a href=\"https://www.metaculus.com/organization/tournaments/\"><u>13 new forecasting tournaments</u></a> offering a total of $77,500 in prize money, with a number of programs delivering policy-relevant predictions on public health, biosecurity, and nuclear-risk. Outside of these core themes (each of which I\u2019ll touch on later), <a href=\"https://www.metaculus.com/tournament/forecasting-Our-World-in-Data/\"><u>Forecasting Our World In Data</u></a> was a standout launch: a project that \u201cprobes the long-term future, delivering predictions on topics like global investment in AI, world life expectancy, CO<strong>2</strong> emissions, and more on time horizons from one to 100 years.\u201d Another favorite was our final launch of the year, the <a href=\"https://www.metaculus.com/tournament/sagan-tournament/\"><u>Sagan Tournament</u></a>, focused on all things space-related \u2014 from technology, to scientific discovery, to governance.</li><li><strong>Ukraine War Response:</strong> After accurately predicting the war in Ukraine, the Metaculus community continued to closely monitor the conflict, responding rapidly to new developments in a <a href=\"https://www.metaculus.com/tournament/ukraine-conflict/\"><u>forecasting tournament</u></a> launched just 48 hours after the invasion. With nuclear security expert Peter Scoblic, we deployed the Red Lines in Ukraine project as an early-warning system gauging the likelihood Russia would make use of nuclear weapons.</li><li><strong>Biosecurity &amp; Public Health:</strong> 2022 saw us increasing our impact in public health and biosecurity, executing the $25,000 <a href=\"https://www.metaculus.com/tournament/biosecurity-tournament/\"><u>Biosecurity Tournament</u></a> with the Institute for Progress and Guarding Against Pandemics, as well as the <a href=\"https://www.metaculus.com/tournament/realtimepandemic/\"><u>Real-Time Pandemic Decision Making</u></a> tournament with UVA\u2019s Biocomplexity Institute to aid COVID computational modeling efforts. In May when clusters of Mpox virus infections were observed across multiple countries, our community responded immediately through our <a href=\"https://www.metaculus.com/project/monkeypox/\"><u>Mpox Series</u></a>, contributing ensemble predictions and providing a crucial health tool for assessing widespread community transmission when data were sparse.</li></ul><h2><strong>We grew our forecasting community and connected with a wider network.</strong></h2><ul><li><strong>80% YOY Forecaster Growth:</strong> We grew our monthly forecaster base by 80% relative to 2021, and launched <a href=\"https://www.metaculus.com/questions/\"><u>over 1000 new forecasting questions</u></a>.</li></ul><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1672968153/mirroredImages/cbtoajkfeXqJAzhRi/zxscimo1fdzjubw4fr3k.png\" alt=\"\"></p><ul><li><strong>Forecasting-Infused Reading &amp; Listening:</strong> We published 40 excellent essays in the <a href=\"https://www.metaculus.com/project/journal/\"><u>Metaculus Journal</u></a>, each fortified by falsifiable predictions, on topics of immediate import, such as the <a href=\"https://www.metaculus.com/notebooks/10669/whats-the-endgame-of-the-war-in-ukraine/\"><u>Endgame of the War in Ukraine</u></a> and <a href=\"https://www.metaculus.com/notebooks/11816/forecasting-for-situational-awareness-in-an-emerging-public-health-crisis/\"><u>Forecasting for Situational Awareness in an Emerging Public Health Crisis</u></a>, and the longer-term, such as <a href=\"https://www.metaculus.com/notebooks/9729/computability-and-complexity/\"><u>Computability and Complexity</u></a>, and <a href=\"https://www.metaculus.com/notebooks/8339/life-on-the-icy-moons-and-ocean-worlds-of-the-outer-solar-system/\"><u>Life on the Icy Moons and Ocean Worlds of the Outer Solar System</u></a>. Many of our Journal articles are also available in audio format, on our <a href=\"https://podcasts.apple.com/us/podcast/the-metaculus-journal/id1620850917\"><u>podcast</u></a>, also launched in 2022.</li><li><strong>News Embedding &amp; Media Highlights:</strong> In an especially turbulent year, we collaborated with physicist Max Tegmark and his team at <a href=\"https://www.improvethenews.org/\"><u>Improve the News</u></a> to bring needed context to fast-moving news stories using timely forecasts by embedding forecasts in over 800 articles. We also had cameo appearances in a number of mainstream outlets, including <a href=\"https://www.theatlantic.com/ideas/archive/2022/08/future-generations-climate-change-pandemics-ai/671148/\"><u>The Atlantic</u></a>, the <a href=\"https://www.washingtonpost.com/outlook/2022/09/16/future-design-yahaba-politics/\"><u>Washington Post</u></a>, <a href=\"https://www.bbc.com/future/article/20220805-what-is-longtermism-and-why-does-it-matter\"><u>BBC News</u></a>, the <a href=\"https://www.ft.com/content/091862f9-985f-4769-aa37-1aed32636329\"><u>Financial Times</u></a>, <a href=\"https://www.fastcompany.com/90781098/how-to-choose-a-career-in-tech-that-benefits-humanity\"><u>Fast Company</u></a>, the leading Spanish paper <a href=\"https://english.elpais.com/science-tech/2022-03-26/is-it-possible-to-predict-the-future-of-the-war-in-ukraine-online-forecasting-communities-think-so.html\"><u>El Pais</u></a>, and major Dutch outlet <a href=\"https://twitter.com/XRobservatory/status/1593276828747239424\"><u>De Telegraaf</u></a>, and were featured in the new, comprehensive CEA article that provides an <a href=\"https://www.effectivealtruism.org/articles/introduction-to-effective-altruism\"><u>introduction to Effective Altruism</u></a>, as well as theoretical physicist Sean Carroll\u2019s podcast <a href=\"https://www.preposterousuniverse.com/podcast/2022/08/15/207-william-macaskill-on-maximizing-good-in-the-present-and-future/\"><u>Mindscape</u></a>.</li><li><strong>A Million Predictions, and a Hackathon:</strong> Metaculus achieved a fantastic milestone, surpassing 1,000,000 predictions over more than 7,000 questions. To celebrate, we hosted our <a href=\"https://metaculus.medium.com/announcing-metaculuss-million-predictions-hackathon-91c2dfa3f39\"><u>first-ever data science hackathon</u></a>, giving talented teams and individuals across the globe access to a rich trove of forecasting data. Unsurprisingly, the forecasting community displayed admirable creativity and rigor in their projects, and selecting <a href=\"https://www.metaculus.com/questions/14283/the-million-prediction-hackathon-winners/\"><u>the winners</u></a> was no easy task. We also hosted a <a href=\"https://www.metaculus.com/questions/13853/metaculus-talks-recordings-now-up/\"><u>Week of Talks</u></a> on forecasting, spanning a diverse range of topics from theory to practice.</li></ul><h2><strong>We upgraded our infrastructure and shipped new features.</strong></h2><ul><li><strong>Upgraded Tech Stack: </strong>In 2022, we rewrote nearly the entire Metaculus application, modernizing the Metaculus tech stack to support our 2023 product roadmap. We have a number of enhancements and new features in store, big and small, and we can\u2019t wait to share them with the community.</li><li><strong>Question Groups &amp; Fan Graphs: </strong>We released <a href=\"https://www.metaculus.com/questions/9861/question-groups-fan-graphs--user-creation/\"><u>question groups and fan graphs</u></a>, enabling the grouping of related questions and the visualization of their forecasts in series.</li></ul><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1672968153/mirroredImages/cbtoajkfeXqJAzhRi/ot3kgeypjm1owx6w5row.png\" alt=\"\"></p><ul><li><strong>Private Forecasting Spaces &amp; Language Localization: </strong>To support partner projects, including internationally, we developed private forecasting spaces that enable confidential forecasting for a group or organization of any size, as well as the ability to translate the Metaculus interface into any language.</li><li><strong>Tournament Scoring &amp; Leaderboards: </strong>We <a href=\"https://www.metaculus.com/questions/8506/metaculus-tournament-scoring-updated-4622/\"><u>updated</u></a> our tournament scoring and leaderboard systems to bring greater rigor and clarity to how we reward and incentivize forecasting skill.</li></ul><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1672968153/mirroredImages/cbtoajkfeXqJAzhRi/tulzhodqp8gz6oaji7c4.png\" alt=\"\"></p><h2><strong>We collaborated on research and published reports on the biggest potential risks of 2022.</strong></h2><ul><li><strong>Nuclear Escalation in Ukraine: </strong>After the invasion of Ukraine, policymakers and the public became increasingly concerned about the prospect of nuclear escalation. We recruited a team of Metaculus Pro Forecasters to make their judgments on key questions and provide their rationales, all of which were drawn up in a full nuclear risk <a href=\"https://www.metaculus.com/notebooks/10439/russia-ukraine-conflict-forecasting-nuclear-risk-in-2022/\"><u>report</u></a>.</li><li><strong>Predicting the Omicron BA.1 Wave:</strong> In partnership with the University of Virginia\u2019s Biocomplexity Institute and the Virginia Department of Health, we co-authored a <a href=\"https://www.researchgate.net/publication/364536656_Utility_of_human_judgment_ensembles_during_times_of_pandemic_uncertainty_A_case_study_during_the_COVID-19_Omicron_BA1_wave_in_the_USA\"><u>paper</u></a> demonstrating the accuracy and robustness of using Metaculus\u2019s COVID-19 Omicron variant forecasting ensembles in combination with computational models by providing valuable real-time forecasts.</li><li><strong>Mpox Rapid Information Aggregation:</strong> When an unexpected number of Mpox cases were reported in early May 2022, we organized a rapid forecasting response to gauge the potential scope and impact of the outbreak. Working with our research partner Tom McAndrew at Lehigh University, we co-authored a paper on the efficacy of rapid human judgment forecasting, which was published (in record time) in <a href=\"https://www.thelancet.com/journals/landig/article/PIIS2589-7500(22)00127-3/fulltext\"><u>The Lancet Digital Health</u></a>.</li><li><strong>Forecasting the US-China AI &amp; Nuclear Landscape:</strong> With our partners at the Institute for Security and Technology, we launched an initiative evaluating intervention points in the US-China nuclear relationship, with a special focus on the integration of AI into nuclear command, control, and communications systems. The resulting <a href=\"https://securityandtechnology.org/virtual-library/reports/forecasting-the-ai-and-nuclear-landscape/\"><u>report</u></a> combines insights from nuclear and policy subject matter experts and Metaculus Pro Forecasters.</li></ul><p>I\u2019m extremely proud of what we\u2019ve accomplished as a team, and I\u2019m deeply grateful for the support of our partner organizations and the forecasting community. If you\u2019re excited by what we\u2019re doing and would like to get in touch, please do feel free to <a href=\"https://calendly.com/metaculus-gaia/20min?month=2023-01\"><u>grab some time with me</u></a>, or <a href=\"mailto:support@metaculus.com\"><u>shoot our team a note</u></a>.</p><p>Onward,</p><p>Gaia Dempsey<br>CEO, Metaculus</p>", "user": {"username": "christianM"}}, {"_id": "DANa9kT3MFmcaC8n4", "title": "AI Safety Camp, Virtual Edition 2023", "postedAt": "2023-01-06T00:55:38.798Z", "htmlBody": "", "user": {"username": "Linda Linsefors"}}, {"_id": "mPkFheB4EM6pmEC7y", "title": "Transformative AI issues (not just misalignment): an overview", "postedAt": "2023-01-06T02:19:41.816Z", "htmlBody": "<p>\nIf this ends up being the <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd\">most important century</a> due to advanced AI, what are the key factors in whether things go well or poorly?\n</p>\n<p>\n</p><details id=\"Box1\"><summary>(Click to expand) More detail on why AI could make this the most important century<!-- (Details not included in email - <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/mPkFheB4EM6pmEC7y/#Box1\">click to view on the web</a>)--></summary>\n<p></p>\n<p>\nIn <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd\">The Most Important Century</a>, I argued that the 21st century could be the most important century ever for humanity, via the development of advanced AI systems that could dramatically speed up scientific and technological advancement, getting us more quickly than most people imagine to a deeply unfamiliar future.\n</p>\n<p>\n<a href=\"https://www.cold-takes.com/most-important-century/\">This page</a> has a ~10-page summary of the series, as well as links to an audio version, podcasts, and the full series.\n</p>\n<p>\nThe key points I argue for in the series are:\n</p>\n<ul>\n\n<li><strong>The long-run future is radically unfamiliar. </strong>Enough advances in technology could lead to a long-lasting, galaxy-wide civilization that could be a radical utopia, dystopia, or anything in between.\n\n</li><li><strong>The long-run future could come much faster than we think,</strong> due to a possible AI-driven productivity explosion.\n\n</li><li>The relevant kind of <strong>AI looks like it will be developed this century</strong> - making this century the one that will initiate, and have the opportunity to shape, a future galaxy-wide civilization.\n\n</li><li>These claims seem too \"wild\" to take seriously. But there are a lot of reasons to think that <strong>we live in a wild time, and should be ready for anything.</strong>\n\n</li><li>We, the people living in this century, have the chance to have a huge impact on huge numbers of people to come - if we can make sense of the situation enough to find helpful actions. But right now, <strong>we aren't ready for this.</strong>\n</li>\n</ul>\n</details>\n<p>\nA lot of my previous writings have focused specifically on the <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">threat of \u201cmisaligned AI\u201d</a>: AI that could have dangerous <em>aims of its own</em> and <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">defeat all of humanity</a>. In this post, I\u2019m going to zoom out and give a broader overview of multiple issues transformative AI could raise for society - with an emphasis on <strong>issues we might want to be thinking about <em>now</em> rather than waiting to address as they happen.</strong>\n</p>\n<p>\nMy discussion will be very unsatisfying. \u201cWhat are the key factors in whether things go well or poorly with transformative AI?\u201d is a massive topic, with lots of angles that have gotten almost no attention and (surely) lots of angles that I just haven\u2019t thought of at all. My one-sentence summary of this whole situation is: <a href=\"https://www.cold-takes.com/most-important-century/#were-not-ready-for-this\">we\u2019re not ready for this</a>.\n</p>\n<p>\nBut hopefully this will give some sense of what sorts of issues should clearly be on our radar. And hopefully it will give a sense of why - out of all the issues we need to contend with - I\u2019m as focused on the threat of misaligned AI as I am.\n</p>\n<p>\nOutline:\n</p>\n<ul>\n\n<li>First, I\u2019ll briefly clarify what kinds of issues I\u2019m trying to list. I\u2019m looking for ways the future could look durably and dramatically different depending on how we navigate the development of transformative AI - such that <strong>doing the right things ahead of time could make a big, lasting difference.</strong>\n\n</li><li>Then, I\u2019ll list candidate issues: \n<ul>\n \n<li><strong>Misaligned AI.</strong> I touch on this only briefly, since I\u2019ve discussed it at length in <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w\">previous pieces</a>. The short story is that we should try to avoid AI ending up with dangerous goals of its own and <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">defeating humanity</a>. (The remaining issues below seem irrelevant if this happens!)\n \n</li><li><strong>Power imbalances. </strong>As AI speeds up science and technology, it could cause some country/countries/coalitions to become enormously powerful - so it matters a lot which one(s) lead the way on transformative AI. (I fear that this concern is generally overrated compared to misaligned AI, but it is still very important.) There could also be dangers in overly widespread (as opposed to concentrated) AI deployment.\n \n</li><li><strong>Early applications of AI. </strong>It might be that what early AIs are used for durably affects how things go in the long run - for example, whether early AI systems are used for education and truth-seeking, rather than manipulative persuasion and/or entrenching what we already believe. We might be able to affect which uses are predominant early on.\n \n</li><li><strong>New life forms. </strong>Advanced AI could lead to new forms of intelligent life, such as AI systems themselves and/or <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AKxKR4CeakyBsGFoH\">digital people</a>. Many of the frameworks we\u2019re used to, for ethics and the law, could end up needing quite a bit of rethinking for new kinds of entities (for example, should we allow people to make as many copies as they want of entities that will predictably vote in certain ways?) Early decisions about these kinds of questions could have long-lasting effects. \n \n</li><li><strong>Persistent policies and norms. </strong>Perhaps we ought to be identifying particularly important policies, norms, etc. that seem likely to be durable even through rapid technological advancement, and try to improve these as much as possible before transformative AI is developed. (These could include things like a better social safety net suited to high, sustained unemployment rates; better regulations aimed at avoiding bias; etc.)\n \n</li><li><strong>Speed of development. </strong>Maybe human society just isn\u2019t likely to adapt well to rapid, radical advances in science and technology, and finding a way to limit the pace of advances would be good.\n</li> \n</ul>\n\n</li><li>Finally, I\u2019ll discuss how I\u2019m thinking about which of these issues to prioritize at the moment, and why misaligned AI is such a focus of mine.\n\n</li><li>An appendix will say a small amount about whether the long-run future seems likely to be better or worse than today, in terms of <a href=\"https://forum.effectivealtruism.org/posts/9tHsmALcrydGXG2Ke/has-life-gotten-better\">quality of life</a>, assuming we navigate the above issues non-amazingly but non-catastrophically.\n</li>\n</ul>\n<h2 id=\"kinds-of-issues\">The kinds of issues I\u2019m trying to list</h2>\n\n\n<p>\nOne basic angle you could take on AI is: \n</p>\n<p>\n\u201cAI\u2019s main effect will be to speed up <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AmxxnazJcBWzWEeqj#Explosive_scientific_and_technological_advancement\">science and technology a lot</a>. This means humans will be able to do <em>all</em> the things they were doing before - the good and the bad - but more/faster. So basically, we\u2019ll end up with the same future we would\u2019ve gotten without AI - just sooner.\n</p>\n<p>\n\u201cTherefore, there\u2019s no need to prepare in advance for anything in particular, beyond what we\u2019d do to work toward a better future <em>normally</em> (in a world with no AI). Sure, lots of weird stuff could happen as science and technology advance - but that was already true, and many risks are just too hard to predict now and easier to respond to as they happen.\u201d\n</p>\n<p>\nI don\u2019t agree with the above, but I <em>do</em> think it\u2019s a good starting point. I think we shouldn\u2019t be listing everything that might happen in the future, as AI leads to advances in science and technology, and trying to prepare for it. Instead, we should be asking: <strong>\u201cif <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd\">transformative AI</a> is coming in the next few decades, how does this <em>change the picture </em>of what we should be focused on, beyond just speeding up what\u2019s going to happen anyway?</strong>\u201d\n</p>\n<p>\nAnd I\u2019m going to try to focus on <strong>extremely high-stakes issues - </strong>ways I could imagine the future looking <strong>durably and dramatically different </strong>depending on how we navigate the development of transformative AI.\n</p>\n<p>\nBelow, I\u2019ll list some candidate issues fitting these criteria.\n</p>\n<h2 id=\"potential-issues\">Potential issues</h2>\n\n\n<h3 id=\"misaligned-ai\">Misaligned AI</h3>\n\n\n<p>\nI won\u2019t belabor this possibility, because the <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w\">last several pieces</a> have been focused on it; this is just a quick reminder.\n</p>\n<p>\nIn a world without AI, the main question about the long-run future would be how humans will end up treating each other. But if powerful AI systems will be developed in the coming decades, we need to contend with the possibility that these AI systems will end up having <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn\">goals of their own</a> - and <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">displacing humans</a> as the species that determines how things will play out.\n</p>\n<p>\n</p><details id=\"Box2\"><summary>(Click to expand)Why would AI \"aim\" to defeat humanity?<!-- (Details not included in email - <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/mPkFheB4EM6pmEC7y/#Box2\">click to view on the web</a>)--></summary>\n<p></p>\n<p>\nA <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn\">previous piece</a> argued that if today\u2019s AI development methods lead directly to powerful enough AI systems, disaster is likely by default (in the absence of specific countermeasures). \n</p>\n<p>\nIn brief:\n</p>\n<ul>\n\n<li>Modern AI development is essentially based on \u201ctraining\u201d via trial-and-error. \n\n</li><li>If we move forward incautiously and ambitiously with such training, and if it gets us all the way to very powerful AI systems, then such systems will likely end up <em>aiming for certain states of the world</em> (analogously to how a chess-playing AI aims for checkmate)<em>.</em>\n\n</li><li>And these states will be<em> other than the ones we intended</em>, because our trial-and-error training methods won\u2019t be accurate. For example, when we\u2019re confused or misinformed about some question, we\u2019ll reward AI systems for giving the wrong answer to it - unintentionally training deceptive behavior.\n\n</li><li>We should expect disaster if we have AI systems that are both (a) <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">powerful enough</a> to defeat humans and (b) aiming for states of the world that we didn\u2019t intend. (\u201cDefeat\u201d means taking control of the world and doing what\u2019s necessary to keep us out of the way; it\u2019s unclear to me whether we\u2019d be literally killed or just forcibly stopped<sup id=\"fnref1\"><a href=\"#fn1\" rel=\"footnote\">[1]</a></sup> from changing the world in ways that contradict AI systems\u2019 aims.)</li></ul>\n</details>\n<p>\n</p><details id=\"Box3\"><summary>(Click to expand) <em>How</em> could AI defeat humanity?<!-- (Details not included in email - <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/mPkFheB4EM6pmEC7y/#Box3\">click to view on the web</a>)--></summary>\n<p></p>\n<p>\nIn a <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">previous piece</a>, I argue that AI systems could defeat all of humanity combined, if (for whatever reason) they were aimed toward that goal.\n</p>\n<p>\nBy defeating humanity, I mean gaining control of the world so that AIs, not humans, determine what happens in it; this could involve killing humans or simply \u201ccontaining\u201d us in some way, such that we can\u2019t interfere with AIs\u2019 aims.\n</p>\n<p>\nOne way this could happen is if AI became extremely advanced, to the point where it had \"cognitive superpowers\" beyond what humans can do. In this case, a single AI system (or set of systems working together) could imaginably:\n</p>\n<ul>\n\n<li>Do its own research on how to build a better AI system, which culminates in something that has incredible other abilities.\n\n</li><li>Hack into human-built software across the world.\n\n</li><li>Manipulate human psychology.\n\n</li><li>Quickly generate vast wealth under the control of itself or any human allies.\n\n</li><li>Come up with better plans than humans could imagine, and ensure that it doesn't try any takeover attempt that humans might be able to detect and stop.\n\n</li><li>Develop advanced weaponry that can be built quickly and cheaply, yet is powerful enough to overpower human militaries.\n</li>\n</ul>\n<p>\nHowever, my piece also explores what things might look like if <em>each AI system basically has similar capabilities to humans. </em>In this case:\n</p>\n<ul>\n\n<li>Humans are likely to deploy AI systems throughout the economy, such that they have large numbers and access to many resources - and the ability to make copies of themselves. \n\n</li><li>From this starting point, AI systems with human-like (or greater) capabilities would have a number of possible ways of getting to the point where their total population could outnumber and/or out-resource humans.\n\n</li><li>I address a number of possible objections, such as \"How can AIs be dangerous without bodies?\"\n</li>\n</ul>\n<p>\nMore: <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">AI could defeat all of us combined</a></p></details>\n\n<h3 id=\"power-imbalances\">Power imbalances</h3>\n\n\n<p>\nI\u2019ve argued that AI could cause a <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AmxxnazJcBWzWEeqj#Explosive_scientific_and_technological_advancement\">dramatic acceleration in the pace of scientific and technological advancement</a>. \n</p>\n<p>\n</p><details id=\"Box4\"><summary>(Click to expand) How AI could cause explosive progress<!-- (Details not included in email - <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/mPkFheB4EM6pmEC7y/#Box4\">click to view on the web</a>)--></summary><p></p>\n\n(This section is mostly copied from my <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd\">summary of the \"most important century\" series</a>; it links to some pieces with more detail at the bottom.)\n\n<p>\nStandard economic growth models imply that <strong>any technology that could fully automate innovation would cause an \"economic singularity\":</strong> productivity going to infinity this century. This is because it would create a powerful feedback loop: more resources -&gt; more ideas and innovation -&gt; more resources -&gt; more ideas and innovation ...\n</p>\n<p>\nThis loop would not be unprecedented. I think it is in some sense the \"default\" way the economy operates - for most of economic history up until a couple hundred years ago. \n</p>\n    <p><img src=\"https://res.cloudinary.com/cea/image/upload/v1672966390/mirroredImages/mPkFheB4EM6pmEC7y/toleccmmki2pr9qx21es.png\" width=\"1036\"></p>\n    <p></p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1672966390/mirroredImages/mPkFheB4EM6pmEC7y/v7rybswhdgmaypnpelxn.png\" alt=\"8 ideas, each 1.5x'ing the amount of food resources -> explosion from 8 units to 205 units of food, hence 205 people and 205 ideas ... \" width=\"1036\"><figcaption>Economic history: more resources -&gt; more people -&gt; more ideas -&gt; more resources ...</figcaption></figure><p></p>\n<p>\nBut in the \"demographic transition\" a couple hundred years ago, the \"more resources -&gt; more people\" step of that loop stopped. Population growth leveled off, and more resources led to richer people instead of more people:\n</p>\n<p></p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1672966390/mirroredImages/mPkFheB4EM6pmEC7y/rrjuvutqyz2kmb1ihaeh.png\" alt=\"Same as previous diagram, but instead of more corn leading to more people, it leads to the same number of people enjoying their boatload of corn - corn juggling, corn slides, corn feasts, etc.\" class=\"kg-image\" loading=\"lazy\" width=\"1036\"><figcaption>Today's economy: more resources -&gt; <del>more </del>richer people -&gt; same pace of ideas -&gt; ...</figcaption></figure><p></p>\n\n<p>\nThe feedback loop could come back if some other technology restored the \"more resources -&gt; more ideas\" dynamic. One such technology could be the right kind of AI: what I call PASTA, or Process for Automating Scientific and Technological Advancement.\n</p>\n<p><img src=\"https://res.cloudinary.com/cea/image/upload/v1672966390/mirroredImages/mPkFheB4EM6pmEC7y/g9acwbxaebbqpfifurey.png\" class=\"kg-image\" loading=\"lazy\" width=\"1036\"></p>\n<p></p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1672966390/mirroredImages/mPkFheB4EM6pmEC7y/smgeohpiywwl70siywyl.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1036\"><figcaption>Possible future: more resources -&gt; more AIs -&gt; more ideas -&gt; more resources ...</figcaption></figure><p></p>\n<p>\nThat means that <strong>our radical long-run future could be upon us very fast </strong>after PASTA is developed (if it ever is). \n</p>\n<p>\nIt also means that if PASTA systems are <em>misaligned </em>- pursuing their own non-human-compatible objectives - things could very quickly go sideways.\n</p>\n<p>\nKey pieces:\n</p>\n<ul>\n\n<li><a href=\"https://forum.effectivealtruism.org/posts/ZZHhQqHRqQ4ciwLBf/the-duplicator-instant-cloning-would-make-the-world-economy\">The Duplicator: Instant Cloning Would Make the World Economy Explode</a>\n\n</li><li><a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AmxxnazJcBWzWEeqj\">Forecasting Transformative AI, Part 1: What Kind of AI?</a>\n</li></ul>\n</details>\n<p></p>\n<p>\nOne way of thinking about this: perhaps (for reasons I\u2019ve <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AmxxnazJcBWzWEeqj#Explosive_scientific_and_technological_advancement\">argued previously</a>) AI could enable the equivalent of hundreds of years of scientific and technological advancement in a matter of a few months (or faster). If so, then developing powerful AI a few months before others could lead to having technology that is (effectively) hundreds of years ahead of others\u2019.\n</p>\n<p>\nBecause of this, it\u2019s easy to imagine that AI could lead to big power imbalances, as whatever country/countries/coalitions \u201clead the way\u201d on AI development could become far more powerful than others (perhaps analogously to when a few smallish European states took over much of the rest of the world).\n</p>\n<p>\nOne way we might try to make the future go better: maybe it could be possible for different countries/coalitions to strike deals in advance. For example, two equally matched parties might agree in advance to share their resources, territory, etc. with each other, in order to avoid a winner-take-all competition.\n</p>\n<p>\nWhat might such agreements look like? Could they possibly be enforced? I really don\u2019t know, and I haven\u2019t seen this explored much.<sup id=\"fnref1\"><a href=\"#fn1\" rel=\"footnote\">[1]</a></sup> \n</p>\n<p>\nAnother way one might try to make the future go better is to try to help a <em>particular</em> country, coalition, etc. develop powerful AI systems before others do. I previously called this the <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/Lbtcjfxhrs8kfKK2M#The__competition__frame\">\u201ccompetition\u201d frame</a>. \n</p>\n<p>\nI think it is, in fact, enormously important who leads the way on transformative AI. At the same time, I\u2019ve expressed concern that people might overfocus on this aspect of things vs. other issues, for a number of reasons including:\n</p>\n<ul>\n\n<li><em>I think people naturally get more animated about \"helping the good guys beat the bad guys\" than about \"helping all of us avoid getting a universally bad outcome, for impersonal reasons such as 'we designed sloppy AI systems' or 'we created a dynamic in which haste and aggression are rewarded.'\"</em>\n\n</li><li><em>I expect people will tend to be overconfident about which countries, organizations or people they see as the \"good guys.\"</em>\n</li>\n</ul>\n<p>\n(More <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/Lbtcjfxhrs8kfKK2M#Why_I_fear__competition__being_overrated__relative_to__caution_\">here</a>.)\n</p>\n<p>\nFinally, it\u2019s worth mentioning the possible dangers of powerful AI being too widespread, rather than too concentrated. In <a href=\"https://nickbostrom.com/papers/vulnerable.pdf\">The Vulnerable World Hypothesis</a>, Nick Bostrom contemplates potential future dynamics such as \u201cadvances in DIY biohacking tools might make it easy for anybody with basic training in biology to kill millions.\u201d In addition to avoiding worlds where AI capabilities end up concentrated in the hands of a few, it could also be important to avoid worlds in which they diffuse too widely, too quickly, before we\u2019re able to assess the risks of widespread access to technology far beyond today\u2019s.\n</p>\n<h3 id=\"early-applications-of-ai\">Early applications of AI</h3>\n\n\n<p>\nMaybe advanced AI will be useful for some sorts of tasks before others. For example, maybe - by default - advanced AI systems will soon be powerful persuasion tools, and cause wide-scale societal dysfunction before they cause rapid advances in science and technology. And maybe, with effort, we could make it less likely that this happens - more likely that early AI systems are used for education and truth-seeking, rather than manipulative persuasion and/or entrenching what we already believe.\n</p>\n<p>\nThere could be lots of possibilities of this general form: particular ways in which AI could be predictably beneficial, or disruptive, before it becomes an all-purpose accelerant to science and technology. Perhaps trying to map these out today, and push for advanced AI to be used for particular purposes early on, could have a lasting effect on the future.\n</p>\n<h3 id=\"new-life-forms\">New life forms</h3>\n\n\n<p>\nAdvanced AI could lead to new forms of intelligent life, such as AI systems themselves and/or <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AKxKR4CeakyBsGFoH\">digital people</a>.\n</p>\n<p>\n</p><details id=\"Box5\"><summary>Digital people: one example of how wild the future could be<!-- (details not included in email - <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/mPkFheB4EM6pmEC7y/#Box5\">click to view on the web</a>--></summary>\n<p></p>\n<p>\nIn a <a href=\"https://forum.effectivealtruism.org/posts/FbGfnwNda43eFsSgS/digital-people-faq#I_m_having_trouble_picturing_a_world_of_digital_people___how_the_technology_could_be_introduced__how_they_would_interact_with_us__etc__Can_you_lay_out_a_detailed_scenario_of_what_the_transition_from_today_s_world_to_a_world_full_of_digital_people_might_look_like_\">previous piece</a>, I tried to give a sense of just how wild a future with advanced technology could be, by examining one hypothetical technology: \"digital people.\" \n</p>\n<p>\nTo get the idea of digital people, imagine a computer simulation of a specific person, in a virtual environment. For example, a simulation of you that reacts to all \"virtual events\" - virtual hunger, virtual weather, a virtual computer with an inbox - just as you would. \n</p>\n<p>\nI\u2019ve argued that digital people would likely be <a href=\"https://forum.effectivealtruism.org/posts/FbGfnwNda43eFsSgS/digital-people-faq#Could_digital_people_be_conscious__Could_they_deserve_human_rights_\">conscious and deserving of human rights </a>just as we are. And I\u2019ve argued that they could have major impacts, in particular:\n</p>\n<ul>\n\n<li>Productivity. Digital people could be copied, just as we can easily make copies of ~any software today. They could also be run much faster than humans. Because of this, digital people could have effects comparable to those of the <a href=\"https://forum.effectivealtruism.org/posts/ZZHhQqHRqQ4ciwLBf/the-duplicator-instant-cloning-would-make-the-world-economy\">Duplicator</a>, but more so: unprecedented (in history or in sci-fi movies) levels of economic growth and productivity.\n\n</li><li>Social science. Today, we see a lot of progress on understanding scientific laws and developing cool new technologies, but not so much progress on understanding human nature and human behavior. Digital people would fundamentally change this dynamic: people could make copies of themselves (including sped-up, temporary copies) to explore how different choices, lifestyles and environments affected them. Comparing copies would be informative in a way that current social science rarely is.\n\n</li><li>Control of the environment. Digital people would experience whatever world they (or the controller of their virtual environment) wanted. Assuming digital people had true conscious experience (an assumption discussed <a href=\"https://forum.effectivealtruism.org/posts/FbGfnwNda43eFsSgS/digital-people-faq#Could_digital_people_be_conscious__Could_they_deserve_human_rights_\">in the FAQ</a>), this could be a good thing (it should be possible to eliminate disease, material poverty and non-consensual violence for digital people) or a bad thing (if human rights are not protected, digital people could be subject to scary levels of control).\n\n</li><li>Space expansion. The population of digital people might become staggeringly large, and the computers running them could end up distributed throughout our galaxy and beyond. Digital people could exist anywhere that computers could be run - so space settlements could be more straightforward for digital people than for biological humans.\n\n</li><li>Lock-in. In today's world, we're used to the idea that the future is unpredictable and uncontrollable. Political regimes, ideologies, and cultures all come and go (and evolve). But a community, city or nation of digital people could be much more stable. \n<ul>\n \n<li>Digital people need not die or age.\n \n</li><li>Whoever sets up a \"virtual environment\" containing a community of digital people could have quite a bit of long-lasting control over what that community is like. For example, they might build in software to reset the community (both the virtual environment and the people in it) to an earlier state if particular things change - such as who's in power, or what religion is dominant.\n \n</li><li>I consider this a disturbing thought, as it could enable long-lasting authoritarianism, though it could also enable things like permanent protection of particular human rights.\n</li> \n</ul>\n</li> \n</ul>\n<p>\nI think these effects could be a very good or a very bad thing. How the early years with digital people go could irreversibly determine which. \n</p>\n<p>\nMore: \n</p>\n<ul>\n\n<li><a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AKxKR4CeakyBsGFoH\">Digital People would be an Even Bigger Deal</a>\n\n</li><li><a href=\"https://forum.effectivealtruism.org/posts/FbGfnwNda43eFsSgS/digital-people-faq\">Digital People FAQ</a>\n</li>\n</ul>\n<p>\n</p></details>\n<p></p>\n<p>\nMany of the frameworks we\u2019re used to, for ethics and the law, could end up needing quite a bit of rethinking for new kinds of entities. For example:\n</p>\n<ul>\n\n<li>How should we determine which AI systems or digital people are considered to have \u201crights\u201d and get legal protections?\n\n</li><li>What about the right to vote? If an AI system or digital person can be quickly copied billions of times, with each copy getting a vote, that could be a recipe for trouble - does this mean we should restrict copying, restrict voting or something else?\n\n</li><li>What should the rules be about engineering AI systems or digital people to have particular beliefs, motivations, experiences, etc.? Simple examples:  \n<ul>\n \n<li>Should it be illegal to create new AI systems or digital people that will predictably suffer a lot? How much suffering is too much?\n \n</li><li>What about creating AI systems or digital people that consistently, predictably support some particular political party or view?\n</li> \n</ul>\n</li> \n</ul>\n<p>\n(For a lot more in this vein, see <a href=\"https://nickbostrom.com/propositions.pdf\">this very interesting piece by Nick Bostrom and Carl Shulman</a>.)\n</p>\n<p>\nEarly decisions about these kinds of questions could have long-lasting effects. For example, imagine someone creating billions of AI systems or <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AKxKR4CeakyBsGFoH\">digital people</a> that have capabilities and subjective experiences comparable to humans, and are deliberately engineered to \u201cbelieve in\u201d (or at least help promote) some particular ideology (Communism, libertarianism, etc.) If these systems are self-replicating, that could change the future drastically. \n</p>\n<p>\nThus, it might be important to set good principles in place for tough questions about how to treat new sorts of digital entities, <em>before</em> new sorts of digital entities start to multiply.\n</p>\n<h3 id=\"persistent-policies-and-norms\">Persistent policies and norms</h3>\n\n\n<p>\nThere might be particular policies, norms, etc. that are likely to stay persistent even as technology is advancing and many things are changing.\n</p>\n<p>\nFor example, how people think about ethics and norms might just inherently change more slowly than technological capabilities change. Perhaps a society that had strong animal rights protections, and general pro-animal attitudes, would maintain these properties all the way through explosive technological progress, becoming a technologically advanced society that treated animals well - while a society that had little regard for animals would become a technologically advanced society that treated animals poorly. Similar analysis could apply to religious values, social liberalism vs. conservatism, etc.\n</p>\n<p>\nSo perhaps we ought to be identifying particularly important policies, norms, etc. that seem likely to be durable even through rapid technological advancement, and try to improve these as much as possible before transformative AI is developed.\n</p>\n<p>\nOne tangible example of a concern I\u2019d put in this category: if AI is going to cause high, persistent technological unemployment, it might be important to establish new social safety net programs (such as universal basic income) <em>today</em> - if these programs would be easier to establish today than in the future. I feel less than convinced of this one - first because I <a href=\"https://forum.effectivealtruism.org/posts/ZNPYMp2uu5zr3Po66/technological-unemployment-ai-vs-most-important-century-ai-1\">have some doubts</a> about how big an issue technological unemployment is going to be, and second because it\u2019s not clear to me why policy change would be easier today than in a future where technological unemployment is a reality. And more broadly, I fear that it's very hard to design <em>and</em> (politically) implement policies today that we can be confident will make things durably better as the world changes radically.\n</p>\n<h3 id=\"slow-it-down\">Slow it down?</h3>\n\n\n<p>\nI\u2019ve named a number of ways in which weird things - such as power imbalances, and some parts of society changing much faster than others - could happen as scientific and technological advancement accelerate. Maybe one way to make the most important century go well would be to simply avoid these weird things by avoiding too-dramatic acceleration. Maybe human society just isn\u2019t likely to adapt well to rapid, radical advances in science and technology, and finding a way to limit the pace of advances would be good.\n</p>\n<p>\nAny individual company, government, etc. has an incentive to move quickly and try to get ahead of others (or not fall too far behind), but coordinated agreements and/or regulations (along the lines of the \u201cglobal monitoring\u201d possibility discussed <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/XRphCh6NbfQiDF3Nt#Global_monitoring__noticing_people_about_to_step_on_mines__and_stopping_them_\">here</a>) could help everyone move more slowly.\n</p>\n<h3>What else?</h3>\n\n\n<p>\nAre there other ways in which transformative AI would cause particular issues, risks, etc. to loom especially large, and to be worth special attention today? I\u2019m guessing I\u2019ve only scratched the surface here.\n</p>\n<h2 id=\"what-im-prioritizing\">What I\u2019m prioritizing, at the moment</h2>\n\n\n<p>\nIf this is the <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd\">most important century</a>, there\u2019s a vast set of things to be thinking about and trying to prepare for, and it\u2019s hard to know what to prioritize.\n</p>\n<p>\nWhere I\u2019m at for the moment:\n</p>\n<p>\n<strong>It seems very hard to say today what will be desirable in a radically different future. </strong>I wish more thought and attention were going into things like <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/mPkFheB4EM6pmEC7y/#Early_applications_of_AI\">early applications of AI</a>; <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/mPkFheB4EM6pmEC7y/#New_life_forms\">norms and laws around new life forms</a>; and whether there are <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/mPkFheB4EM6pmEC7y/#Persistent_policies_and_norms\">policy changes today that we could be confident in even if the world is changing rapidly and radically.</a> <strong>But </strong>it seems to me that it would be very hard to be confident in any particular goal in areas like these. Can we really say anything today about what sorts of digital entities should have rights, or what kinds of AI applications we hope come first, that we expect to hold up?\n</p>\n<p>\n<strong>I feel most confident in two very broad ideas: \u201cIt\u2019s bad if AI systems defeat humanity to pursue goals of their own\u201d and \u201cIt\u2019s good if good decision-makers end up making the key decisions.\u201d </strong>These map to the <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/mPkFheB4EM6pmEC7y/#Misaligned_AI\">misaligned AI</a> and <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/mPkFheB4EM6pmEC7y/#Power_imbalances\">power imbalance</a> topics - or what I previously called <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/Lbtcjfxhrs8kfKK2M#The__caution__frame\">caution</a> and <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/Lbtcjfxhrs8kfKK2M#The__competition__frame\">competition</a>.\n</p>\n<p>\nThat said, <strong>it also seems hard to know who the \u201cgood decision-makers\u201d are. </strong>I\u2019ve definitely observed some of this dynamic: \u201cPerson/company A says they\u2019re trying to help the world by aiming to build transformative AI before person/company B; person/company B says they\u2019re trying to help the world by aiming to build transformative AI before person/company A.\u201d \n</p>\n<p>\nIt\u2019s pretty hard to come up with tangible tests of who\u2019s a \u201cgood decision-maker.\u201d We mostly don\u2019t know what person A would do with enormous power, or what person B would do, based on their actions today. One possible criterion is that we should arguably have more trust in people/companies who show more <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/Lbtcjfxhrs8kfKK2M#The__caution__frame\">caution</a> - people/companies who show willingness to hurt their own chances of \u201cbeing in the lead\u201d in order to help everyone\u2019s chance of avoiding a catastrophe from misaligned AI.<sup id=\"fnref2\"><a href=\"#fn2\" rel=\"footnote\">[2]</a></sup>\n</p>\n<p>\n(Instead of focusing on which particular people and/or companies lead the way on AI, you could focus on which <em>countries</em> do, e.g. preferring non-authoritarian countries. It\u2019s arguably pretty clear that non-authoritarian countries would be better than authoritarian ones. However, I have concerns about this as a goal as well, discussed in a footnote.<sup id=\"fnref3\"><a href=\"#fn3\" rel=\"footnote\">[3]</a></sup>)\n</p>\n<p>\n<strong>For now, I am <em>most</em> focused on the threat of misaligned AI. </strong>Some reasons for this:\n</p>\n<ul>\n\n<li>It currently seems to me that misaligned AI is a significant risk. Misaligned AI seems <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn\">likely by default</a> if we don\u2019t specifically do things to prevent it, and preventing it seems far from straightforward (see previous posts on the <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5\">difficulty of alignment research</a> and <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/XRphCh6NbfQiDF3Nt\">why it could be hard for key players to be cautious</a>).\n\n</li><li>At the same time, it seems like there are significant <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd\">hopes</a> for how we might avoid this risk. As argued <a href=\"https://www.alignmentforum.org/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very#Key_question__how_cautious_will_Magma_and_others_be_\">here</a> and <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd\">here</a>, my sense is that the more broadly people understand this risk, the better our odds of avoiding it.\n\n</li><li>I currently feel that this threat is <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/Lbtcjfxhrs8kfKK2M#The__competition__frame\">underrated</a>, relative to the easier-to-understand angle of \u201cI hope people I like develop powerful AI systems before others do.\u201d\n\n</li><li>I think the \u201ccompetition\u201d frame - focusing on helping some countries/coalitions/companies develop advanced AI before others - makes quite a bit of sense as well. But - as noted directly above -  I have big reservations about the most common \u201ccompetition\u201d-oriented actions, such as trying to help particular companies outcompete others or trying to get U.S. policymakers more focused on AI.  \n<ul>\n \n<li>For the latter, I worry that this risks making huge sacrifices on the \u201ccaution\u201d front and even backfiring by causing other governments to invest in projects of their own.\n \n</li><li>For the former, I worry about the ability to judge \u201cgood\u201d leadership, and the temptation to overrate people who resemble oneself.\n</li> \n</ul>\n</li> \n</ul>\n<p>\nThis is all far from absolute. I\u2019m open to a broad variety of projects to help the most important century go well, whether they\u2019re about \u201ccaution,\u201d \u201ccompetition\u201d or another issue (including those I\u2019ve listed in this post). My top priority at the moment is reducing the risks of misaligned AI, but I think a huge range of potential risks aren\u2019t getting enough attention from the world at large.\n</p>\n<h2 id=\"appendix\">Appendix: if we avoid catastrophic risks, how good does the future look?</h2>\n\n\n<p>\nHere I\u2019ll say a small amount about whether the long-run future seems likely to be better or worse than today, in terms of <a href=\"https://forum.effectivealtruism.org/posts/9tHsmALcrydGXG2Ke/has-life-gotten-better\">quality of life</a>. \n</p>\n<p>\nPart of why I want to do this is to give a sense of why I feel cautiously and moderately optimistic about such a future - such that I feel broadly okay with a frame of \u201cWe should try to prevent anything <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">too catastrophic</a> from happening, and figure that the future we get if we can pull that off is reasonably likely (though far from assured!) to be good.\u201d\n</p>\n<p>\nSo I\u2019ll go through some quick high-level reasons for hope (the future might be better than the present) - and for concern (it might be worse). \n</p>\n<p>\n<strong>In this section, I\u2019m ignoring the special role AI might play, and just thinking about what happens if we get a fast-forwarded future. </strong>I\u2019ll be focusing on what I think are probably the most likely ways the world will change in the future, laid out <a href=\"https://forum.effectivealtruism.org/posts/XhquBrefbrTaui5BR/summary-of-history-empowerment-and-well-being-lens#History_is_a_story\">here</a>: a higher world population and greater <strong>empowerment due to a greater stock of ideas, innovations and technological capabilities. </strong>My aim is to ask: \u201cIf we navigate the above issues neither amazingly nor catastrophically, and end up with the same sort of future we\u2019d have had without AI (just sped up), how do things look?\u201d\n</p>\n<p>\n<strong>Reason for hope: empowerment trends. </strong>One simple take would be: \u201c<a href=\"https://www.cold-takes.com/has-life-gotten-better-the-post-industrial-era/\">Life has gotten better for humans</a><sup id=\"fnref4\"><a href=\"#fn4\" rel=\"footnote\">[4]</a></sup><a href=\"https://www.cold-takes.com/has-life-gotten-better-the-post-industrial-era/\"> over the last couple hundred years or so</a>, the period during which we\u2019ve seen <a href=\"https://forum.effectivealtruism.org/posts/pFHN3nnN9WbfvWKFg/this-can-t-go-on\">most of history\u2019s economic growth and technological progress</a>. We\u2019ve seen better health, less poverty and hunger, less violence, more anti-discrimination measures, and few signs of anything getting clearly worse. So if humanity just keeps getting more and more <a href=\"https://forum.effectivealtruism.org/posts/z7quAxWyHuqFdxGE6/rowing-steering-anchoring-equity-mutiny-1#Rowing>empowered</a>, and nothing catastrophic happens, we should plan on life continuing to improve along a variety of dimensions.\u201d\n</p>\n<p>\n<em>Why</em> is this the trend, and should we expect it to hold up? There are lots of theories, and I won\u2019t pretend to know, but I\u2019ll lay out some basic thoughts that may be illustrative and give cause for optimism.\n</p>\n<p>\nFirst off, there is an awful lot of room for improvement just from continuing to cut down on things like hunger and disease. A wealthier, more technologically advanced society seems like a pretty good bet to have less hunger and disease for fairly straightforward reasons.\n</p>\n<p>\nBut we\u2019ve seen <a href=\" https:=\"\" www.cold-takes.com=\"\" has-life-gotten-better-the-post-industrial-era=\"\" \"=\"\">improvement</a> on other dimensions too. This could be partly explained by something like the following dynamic:\n</p>\n<ul>\n\n<li>Most people would - aspirationally - <em>like </em>to be nonviolent, compassionate, generous and fair, if they could do so without sacrificing other things.\n\n</li><li>As <a href=\"https://forum.effectivealtruism.org/posts/z7quAxWyHuqFdxGE6/rowing-steering-anchoring-equity-mutiny-1#Rowing\">empowerment</a> rises, the need to make sacrifices falls (noisily and imperfectly) across the board.\n\n</li><li>This dynamic may have led to some (noisy, imperfect) improvement to date, but there might be <em>much more</em> benefit in the future compared to the past. For example, if we see a lot of progress on <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AKxKR4CeakyBsGFoH#Social_science\">social science</a>, we might get to a world where people understand their own needs, desires and behavior better - and thus can get most or all of what they want (from material needs to self-respect and happiness) without having to outcompete or push down others.<sup id=\"fnref5\"><a href=\"#fn5\" rel=\"footnote\">[5]</a></sup></li></ul>\n<p>\n<strong>Reason for hope: the \u201ccheap utopia\u201d possibility. </strong>This is sort of an extension of the previous point. If we imagine the upper limit of how \u201cempowered\u201d humanity could be (in terms of having lots of technological capabilities), it might be relatively <em>easy</em> to create a kind of <a href=\"https://www.cold-takes.com/visualizing-utopia/\">utopia</a> (such as the <a href=\"https://www.cold-takes.com/visualizing-utopia/#a-meta-option\">utopia I\u2019ve described previously</a>, or hopefully something much better). This doesn\u2019t <em>guarantee</em> that such a thing will happen, but a future where it\u2019s technologically easy to do things like <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AKxKR4CeakyBsGFoH#Virtual_reality_and_control_of_the_environment\">meeting material needs</a> and providing <a href=\"https://www.cold-takes.com/visualizing-utopia/#a-meta-option\">radical choice</a> could be quite a bit better than the present.\n</p>\n<p>\nAn interesting (wonky) treatment of this idea is Carl Shulman\u2019s blog post: <a href=\"http://reflectivedisequilibrium.blogspot.com/2012/09/spreading-happiness-to-stars-seems.html\">Spreading happiness to the stars seems little harder than just spreading</a>.\n</p>\n<p>\n<strong>Reason for concern: authoritarianism. </strong>There are some huge countries that are essentially ruled by one person, with little to no democratic or other mechanisms for citizens to have a voice in how they\u2019re treated. It seems like a live risk that the world could end up this way - essentially ruled by one person or relatively small coalition - in the long run. (It arguably would even continue a historical trend in which political units have gotten larger and larger.)\n</p>\n<p>\nMaybe this would be fine if whoever\u2019s in charge is able to let everyone have freedom, wealth, etc. at little cost to themselves (along the lines of the above point). But maybe whoever\u2019s in charge is just a crazy or horrible person, in which case we might end up with a bad future even if it <em>would</em> be \u201ccheap\u201d to have a wonderful one.\n</p>\n<p>\n<strong>Reason for concern: competitive dynamics. </strong>You might imagine that as empowerment advances, we get purer, more unrestrained <em>competition</em>. \n</p>\n<p>\nOne way of thinking about this: \n</p>\n<ul>\n\n<li>Today, no matter how ruthless CEOs are, they tend to accommodate some amount of leisure time for their employees. That\u2019s because businesses have no choice but to hire people who insist on working a limited number of hours, having a life outside of work, etc. \n\n</li><li>But if we had advanced enough technology, it might be possible to run a business whose employees have zero leisure time. (One example would be via <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AKxKR4CeakyBsGFoH\">digital people</a> and the ability to <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AKxKR4CeakyBsGFoH/#productivity\">make lots of copies of highly productive people just as they\u2019re about to get to work</a><em>. </em>A more mundane example would be if e.g. advanced stimulants and other drugs were developed so people could be productive without breaks.)\n\n</li><li>And that might be what the most productive businesses, organizations, etc. end up looking like - the most productive organizations might be the ones that most maniacally and uncompromisingly use <em>all of their resources to acquire more resources. </em>Those could be precisely the organizations that end up filling most of the galaxy.\n\n</li><li>More at <a href=\"https://slatestarcodex.com/2014/07/13/growing-children-for-bostroms-disneyland/\">this Slate Star Codex post</a>. Key quote: \u201cI\u2019m pretty sure that brutal \u2026 competition combined with ability to [copy and edit] minds necessarily results in paring away everything not directly maximally economically productive. And a lot of things we like \u2013 love, family, art, hobbies \u2013 are not directly maximally economic productive.\u201d\n</li>\n</ul>\n<p>\nThat said:\n</p>\n<ul>\n\n<li>It\u2019s not really clear how this ultimately shakes out. One possibility is something like this:  \n<ul>\n \n<li>Lots of people, or perhaps machines, compete ruthlessly to acquire resources. But this competition is (a) legal, subject to a property rights system; (b) ultimately for the benefit of the <em>investors </em>in the competing companies/organizations. \n \n</li><li>Who are these investors? Well, today, many of the biggest companies are mostly owned by large numbers of individuals via mutual funds. The same could be true in the future - and those individuals could be normal people who use the proceeds for nice things.\n</li> \n</ul>\n\n</li><li>If the \u201ccheap utopia\u201d possibility (described above) comes to pass, it might only take a small amount of spare resources to support a lot of good lives.\n</li>\n</ul>\n<p>\n<strong>Overall, my guess is that the long-run future is more likely to be <em>better than the present</em> than <em>worse than the present</em></strong> (in the sense of <a href=\"https://forum.effectivealtruism.org/posts/9tHsmALcrydGXG2Ke/has-life-gotten-better\">average quality of life</a>). I\u2019m very far from confident in this. I\u2019m more confident that the long-run future is likely to be <em>better than nothing</em>, and that it would be good to prevent humans from going extinct, or a similar development such as a takeover by misaligned AI.\n</p>\n\n\n\n<div class=\"footnotes\">\n<hr>\n<ol><li id=\"fn1\">\n<p>\n     A couple of discussions of the prospects for enforcing agreements <a href=\"https://www.alignmentforum.org/posts/S4Jg3EAdMq57y587y/an-alternative-approach-to-ai-cooperation\">here </a>and <a href=\"https://www.alignmentforum.org/posts/gYaKZeBbSL4y2RLP3/strategic-implications-of-ais-ability-to-coordinate-at-low\">here</a>.&nbsp;<a href=\"#fnref1\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn2\">\n<p>\n     I\u2019m reminded of the <a href=\"https://en.wikipedia.org/wiki/Judgement_of_Solomon\">judgment of Solomon</a>: \u201ctwo mothers living in the same house, each the mother of an infant son, came to Solomon. One of the babies had been smothered, and each claimed the remaining boy as her own. Calling for a sword, Solomon declared his judgment: the baby would be cut in two, each woman to receive half. One mother did not contest the ruling, declaring that if she could not have the baby then neither of them could, but the other begged Solomon, \u2018Give the baby to her, just don't kill him!\u2019 The king declared the second woman the true mother, as a mother would even give up her baby if that was necessary to save its life, and awarded her custody.\u201d \n</p><p>\n    The sword is misaligned AI and the baby is humanity or something.\n</p><p>\n    (This story is actually extremely bizarre - seriously, Solomon was like \u201cYou each get half the baby\u201d?! - and some <a href=\"https://en.wikipedia.org/wiki/Judgement_of_Solomon#Classification_and_parallels\">similar stories from India/China</a> seem at least a bit more plausible. But I think you get my point. Maybe.)&nbsp;<a href=\"#fnref2\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn3\">\n<p>\n     For a tangible example, I\u2019ll discuss the practice (which some folks are doing today) of trying to ensure that the U.S. develops transformative AI before another country does, by arguing for the importance of A.I. to U.S. policymakers. \n</p><p>\n    This approach makes me quite nervous, because:\n</p><ul>\n\n<li>I expect U.S. policymakers by default to be <em>very</em> oriented toward \u201ccompetition\u201d to the exclusion of \u201ccaution.\u201d (This could change if the importance of caution becomes more widely appreciated!) \n\n</li><li>I worry about a nationalized AI project that (a) doesn\u2019t exercise much caution at all, focusing entirely on racing ahead of others; (b) might backfire by causing <em>other</em> countries to go for nationalized projects of their own, inflaming an already tense situation and not even necessarily doing much to make it more likely that the U.S. leads the way.  In particular, other countries might have an easier time quickly mobilizing huge amounts of government funding than the U.S., such that the U.S. might have better odds if it remains the case that most AI research is happening at private companies.</li></ul>\n\n<p>\n    (There might be ways of helping particular countries <em>without</em> raising the risks of something like a low-caution nationalized AI project, and if so these could be important and good.)&nbsp;<a href=\"#fnref3\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn4\">\n<p>\n     <a href=\"https://www.cold-takes.com/has-life-gotten-better-the-post-industrial-era/#for-animals-its-not-the-same-story\">Not for animals</a>, though see <a href=\"https://forum.effectivealtruism.org/posts/z7quAxWyHuqFdxGE6/rowing-steering-anchoring-equity-mutiny-1?commentId=cQ4n3ZuLFqgkfgBsy\">this comment</a> for some reasons we might not consider this a knockdown objection to the \u201clife has gotten better\u201d claim.&nbsp;<a href=\"#fnref4\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn5\">\n\n<p>\n     This is only a possibility. It\u2019s also possible that humans deeply value being <em>better-off than others</em>, which could complicate it quite a bit. (Personally, I feel somewhat optimistic that a lot of people would aspirationally prefer to focus on their own welfare rather than comparing themselves to others - so if knowledge advanced to the point where people could choose to change in this way, I feel optimistic that at least many would do so.)&nbsp;<a href=\"#fnref5\" rev=\"footnote\">\u21a9</a>\n</p></li></ol></div>", "user": {"username": "HoldenKarnofsky"}}, {"_id": "uZJSAi2iA9KGx7qg4", "title": "Metaculus Beginner Tournament for New Forecasters", "postedAt": "2023-01-06T02:35:26.876Z", "htmlBody": "<p>Are you new to forecasting? &nbsp;Join the latest version of the Metaculus <a href=\"https://www.metaculus.com/tournament/beginner-tournament/\">Beginner Tournament</a>!&nbsp;<br><br>The tournament runs for the first quarter of 2023. Each week, we'll have two new questions on relevant topics that week. And the two questions from the previous week will resolve, so you can see how you did on them! At the end of the quarter, you'll be a veteran forecaster with a real forecasting track record.</p>", "user": {"username": "Anastasia"}}, {"_id": "aivWJtiureXunmAEp", "title": "Handling Moral Uncertainty with Average vs. Total Utilitarianism: One Method That Apparently *Doesn't* Work (But Seemed Like it Should)", "postedAt": "2023-01-05T22:18:21.772Z", "htmlBody": "<h2><strong>TL;DR</strong></h2><ul><li>I developed a <a href=\"https://docs.google.com/spreadsheets/d/1_zjwz6-1n42jZAqQsLojzFMoGqA9F2PqRF3dJ4vfS6E/edit?usp=sharing\">system</a> that tried to deal with moral uncertainty regarding the moral status of \"potential people,\" which is a key crux in the debate between so-called \"average utilitarians\" vs. \"total utilitarians.\"</li><li>To simplify the system's underlying principle: When comparing between two decisions which lead to different population sizes (e.g., 10 billion vs. 30 billion), you could just focus on average wellbeing BUT treat the world with a smaller population as also having (30 - 10 = 20 billion) <strong>additional people with wellbeing equal to zero</strong> (\"<strong>non-existent potential people</strong>\"). However, this effect should be weighted by the probability &nbsp;you assign to the claim \"potential people matter.\"</li><li>I think the reasoning behind the system seems facially intuitive/agreeable for many average utilitarians, which I feel somewhat qualified to believe given that I leaned more towards average utilitarianism up until ~1 year ago.</li><li>However, the system I developed can be gamed in a way that leads to illogical conclusions (e.g., concluding A &lt; B &lt; C &lt; A).</li><li>I'm unclear whether this problem is fixable with minor tweaks, but open to the possibility.</li><li>It's plausible but definitely not obvious that this could be a legitimate argument (perhaps even a \"self-trap\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqke04oym6d\"><sup><a href=\"#fnqke04oym6d\">[1]</a></sup></span>) against the idea of focusing on averages as opposed to aggregates. However, I do not know whether future research will show that a similar system can be set up to \u201ctrap\u201d total utilitarian reasoning.</li></ul><h2><strong>Summary of Post Goals</strong></h2><p>In this post, I am seeking to:</p><ol><li>Solicit responses that point me towards other attempts to account for moral uncertainty between average and total utilitarianism,&nbsp;<i>including systems which may not actually be better but are more well known or widely used</i>. I am especially interested to know if there is already something similar to what I describe.</li><li>Describe one method/system that I thought of which initially seemed reasonable but does not seem to work\u2014it leads to apparent self-contradictions\u2014yet, it appears to&nbsp;<i>occasionally&nbsp;</i>be more reasonable/effective than the alternatives I am currently aware of (e.g., \u201ccompletely assume one or the other framework is correct,\u201d \u201cgo with your gut/intuition\u201d), at least as a minimal test rather than a sufficient criterion.</li><li>Get initial feedback on the described system, if only for obvious flaws that I may have overlooked.</li><li>Highlight the self-contradiction point as a&nbsp;<i>possible</i> argument against \u201caverage utilitarianism\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz4ki3madpi9\"><sup><a href=\"#fnz4ki3madpi9\">[2]</a></sup></span>&nbsp;and see if people have quick thoughts about the validity/impact of this argument, with the disclaimer that I haven\u2019t explored it very deeply on my own because (a) I want to first get a better sense of the literature on moral uncertainty that already exists (so that I avoid completely reinventing the wheel here, which I admittedly may have already&nbsp;<i>partially</i> done), and (b) I would not be surprised to find that the system I developed has important flaws which render this discussion moot.</li></ol><p>&nbsp;</p><h2><strong>Introduction</strong></h2><p>Over the past two years, I\u2019ve updated my beliefs away from&nbsp;<a href=\"https://en.wikipedia.org/wiki/Average_and_total_utilitarianism\"><u>average utilitarianism (\u201caveragism\u201d) and towards total utilitarianism (\u201ctotalism\u201d)</u></a>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefipt8pndmign\"><sup><a href=\"#fnipt8pndmign\">[3]</a></sup></span>&nbsp;In this post, I\u2019ll simplify the differences between the two frameworks to just focus on the idea of \u201cpotential people\u201d\u2014for example, \u201cis a world with 100 people who on average experience \u201810 utils\u2019 better than a world with 1000 people who experience 5 utils on average.\u201d</p><p>For most everyday decisions, this is not very important, but when thinking about some issues (especially those relating to longtermism/x-risks/s-risks)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrgvjlso7wx\"><sup><a href=\"#fnrgvjlso7wx\">[4]</a></sup></span>&nbsp;it becomes a potentially very important distinction. However, I still have uncertainties about how to balance between the two frameworks given some residual uncertainty, and I also want to have some advice/framework to give to people who might also be unsure of how to deal with this moral uncertainty\u2014at least as a way to challenge extremely one-sided reasoning in favor of smaller populations (e.g., \u201cif I think that averagism is &gt;50% likely to be valid, I should just ignore all concerns from totalism and maximize the average wellbeing no matter how much supposed opportunity cost this involves from the standpoint of total wellbeing\"). I am especially inclined to discover such a system since I think that you can make compelling arguments (e.g., references to repugnant conclusions and reverse repugnant conclusions) that suggest it is unjustified to assume a high degree of confidence (e.g., &gt;95%) in one system or the other&nbsp;<i>without doing deeper analysis</i>.</p><p>A few months ago, I decided that I would try to develop a system that helps to handle this moral uncertainty&nbsp;<i>at least better than</i> \u201cthis is what my gut tells me,\u201d or \u201cI\u2019ll make decisions per totalism 25% of the time, and averagism 75% of the time,\u201d or \u201cI\u2019ll try to make decisions where the sum of the percentage increases in average wellbeing and total wellbeing is net positive (e.g., \u201825% increase in average wellbeing with only a 10% decrease in total wellbeing\u2019).\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref35a4c1cohq8\"><sup><a href=\"#fn35a4c1cohq8\">[5]</a></sup></span></p><p>&nbsp;</p><h2><strong>The Conceptual Reasoning</strong></h2><p>The system I developed is based on the reasoning behind the (<i>non-Rawlsian</i>) veil of ignorance, where the goal is to maximize wellbeing among \u201cpeople that matter.\u201d The problem is that averagism contends that \u201cnon-existent potential people\u201d don\u2019t matter for this calculation whereas totalism contends such people do matter, and it is unclear how to compare aggregates to averages. So, I wondered, \u201cwhat if you just resolve the disagreement by using the common metric of \u2018<i>average wellbeing</i> among people that matter\u2019\u2014but assume that if totalism is valid then when comparing two options (World A and World B) additional \u2018potential people\u2019 who would exist in one world but don\u2019t in the other should be treated as experiencing zero utils in the latter?\u201d</p><p>One could perhaps conceptualize this proposal by assuming that the veil of ignorance predates the question of whether people exist\u2014as if one is trying to maximize average wellbeing among \u201cpotential people\u201d in a \u201cpre-existence waiting room,\u201d where only a limited number of \u201cpeople\u201d come into existence while the rest never exist: totalism says that these non-existent people should be treated as if they have zero wellbeing, while averagism says that these non-existent people should be completely left out of the average. (For now I will just set aside the objection that this waiting room could somehow have \u201cinfinite\u201d potential people, and simply focus on the differences between actual options, e.g., 1 billion vs. 10 billion people.)</p><p>The following conceptual diagram may or may not help, but in the next section I give a concrete example with numbers and eventually even a spreadsheet with example cases.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1672957102/mirroredImages/aivWJtiureXunmAEp/w1cncxefxrm1ukivkhnp.png\"></p><p>The following is a simplified illustration of the model, still without concrete values. It may look like a lot, but at its core it's basically just a 2x2 matrix: do potential people matter, and option A or option B. The apparent complexity mainly just enters when one has to weight for their credence in different ethical positions. (Disclaimer: there may be errors in this, as I added it somewhat hastily after initially posting the article)</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1673039527/mirroredImages/aivWJtiureXunmAEp/uzvrioyasiy0kkgttalm.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1673039528/mirroredImages/aivWJtiureXunmAEp/lo8tnihabekw7fljxccp.png 140w, https://res.cloudinary.com/cea/image/upload/v1673039528/mirroredImages/aivWJtiureXunmAEp/knrxqzlgx1g7i00ruphe.png 280w, https://res.cloudinary.com/cea/image/upload/v1673039528/mirroredImages/aivWJtiureXunmAEp/bdt0m0gjmt3bogauvbdr.png 420w, https://res.cloudinary.com/cea/image/upload/v1673039527/mirroredImages/aivWJtiureXunmAEp/enesy5ta7qrpenixvsy5.png 560w, https://res.cloudinary.com/cea/image/upload/v1673039527/mirroredImages/aivWJtiureXunmAEp/ag2aktgjhcarsk4a18vq.png 700w, https://res.cloudinary.com/cea/image/upload/v1673039527/mirroredImages/aivWJtiureXunmAEp/dkicnxskv3oxwkxaz9dj.png 840w, https://res.cloudinary.com/cea/image/upload/v1673039528/mirroredImages/aivWJtiureXunmAEp/ivu8k5d5cfaxfdsoxhtf.png 980w, https://res.cloudinary.com/cea/image/upload/v1673039528/mirroredImages/aivWJtiureXunmAEp/m2lh4lurnn3gpke7c3ce.png 1120w, https://res.cloudinary.com/cea/image/upload/v1673039528/mirroredImages/aivWJtiureXunmAEp/eitp4ztfs5ndnnqkxa6v.png 1260w, https://res.cloudinary.com/cea/image/upload/v1673039527/mirroredImages/aivWJtiureXunmAEp/a5ajczly8ukmpaqmugrp.png 1350w\"></figure><p>&nbsp;</p><h2><strong>A Concrete/Quantitative Example</strong></h2><p>To use concrete numbers, suppose you could take some action that would lead to a world (World A) with 1 billion people and an average utility of 10 utils per person, or you could take an action which leads to a different world (World B) with 10 billion people and an average utility of 5 utils per person. Following the reasoning I describe, World B is preferable if one assumes that totalism is 100% valid, whereas World A is preferable if one assumes that averagism is 100% valid. But what if you\u2019re somewhat uncertain between the two\u2014for example, if you put 60% credence on averagism (non-existent potential people don\u2019t matter) and 40% credence on totalism (non-existent potential people\u2019s wellbeing should be treated as 0)? What if the average utilities among existing people change? The screenshot below&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1_zjwz6-1n42jZAqQsLojzFMoGqA9F2PqRF3dJ4vfS6E/edit?usp=sharing\"><u>shows some example calculations</u></a>.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1672957102/mirroredImages/aivWJtiureXunmAEp/zmjqz2g9owmqqkimj581.png\"></p><p>&nbsp;</p><h2><strong>Key Observations</strong></h2><h3><strong>It seems intuitively appealing and occasionally better than alternatives I know of</strong></h3><p>I realized fairly quickly that this probably had some logical flaws\u2014after all, it is simple and is fast and loose with reasoning around averages.&nbsp;<strong>However, one of the key observations I\u2019ve had is that I feel like this is more reasonable than the alternatives I was aware of</strong>, and it at least&nbsp;<i>sometimes</i> moderates dubiously extreme choices, such as \u201cmy credence in averagism is 80% and World A has one single person with 100 utility, whereas World B has one billion people with 95 utility; I choose World A because it has the higher average utility and I think averagism is more likely to be correct.\u201d&nbsp;</p><p><strong>Additionally, I probably could have been persuaded to accept this compromise even when I preferred averagism</strong>: I was supportive of the reasoning behind the (non-Rawlsian) veil of ignorance, but I wasn\u2019t sure how to factor in \u201cnon-existent potential people\u201d and I was skeptical of totalism in light of arguments such as the repugnant conclusion. However, if someone highlighted that there\u2019s a chance that non-existent potential people should be treated as zeros in the average, I might have accepted that in my reasoning, especially&nbsp;<strong>since it still appeals to the \u201cmaximizing expected wellbeing as if you didn\u2019t know who you are (</strong><i><strong>or could have been</strong></i><strong>)\u201d aspect of the veil of ignorance.</strong>&nbsp;</p><h3><strong>But it still seems to have problems with scale-insensitivity</strong></h3><p>Admittedly, I also found this system somewhat unsatisfying because it seems fairly scale-insensitive and still seemingly endorses extreme choices such as \u201cmy credence in averagism is 51% and World A has&nbsp;<i>one single person with 100 utility</i>, whereas World B has&nbsp;<i>ten trillion people with 50 utility</i>; I should choose World A.\u201d (If I understood and remember correctly, if one\u2019s ratio of credence in averagism vs. totalism exceeds the ratio of average utility in World A vs. World B, it automatically rules in favor of World A no matter how large the disparity in population size is between the two worlds, which just seems\u2026 wrong?)&nbsp;</p><p>In some ways this could make the system worse than non-explicit alternatives such as \u201cgoing with one\u2019s intuition.\u201d Still, it at least seems to be a more-agreeable&nbsp;<i>floor</i> than something like \u201cjust operate with the assumptions of whichever framework you assign a higher credence to.\u201d In other words, it seems that passing this criterion may not be&nbsp;<i>sufficient</i> to justify a decision in favor of a world with a smaller population, but that failing this criterion should be strong evidence against a decision that favors a world with a smaller population. (To be clear, I still have a lot of uncertainty regarding these claims.)</p><h3><strong>And it seems to produce self-contradictory results</strong></h3><p><strong>Another key observation is that this system seems to break down from logical self-contradiction (see below), and I\u2019m not sure 1) if this can be easily resolved with modifications, and 2) whether this is an indictment against the idea of using \u201caverage wellbeing.\u201d</strong></p><p>The apparent self-contradiction comes when one breaks down an individual decision into multiple, partial steps, as shown in the&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1_zjwz6-1n42jZAqQsLojzFMoGqA9F2PqRF3dJ4vfS6E/edit?usp=sharing\"><u>spreadsheet</u></a> below: the system in test case 9-1-A states that World A is preferable to World B in that scenario, yet by breaking this one decision down into two steps/decisions through test cases 9-1-B and 9-1-C (which both rule in favor of their respective World B), you cause the system to seemingly prefer the original World B in test case 9-1-A. In other words,&nbsp;<strong>the system basically claims that A is preferable to B, B is preferable to C, and C is preferable to A (or in a shorter format, A &gt; B &gt; C &gt; A)</strong>, which is an illogical inequality. Moreover, as shown with test case group 9-2, it produces the reverse reasoning (A &lt; B &lt; C &lt; A) when the average utility is set to a negative value. (The bright red, orange, and yellow cell colors below correspond to a given recurrent world.)</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1672957102/mirroredImages/aivWJtiureXunmAEp/e9kbdfgyoz3zpdlasavt.png\"></p><p>It seems entirely plausible that I\u2019m just doing something illogical with the averages and/or that some minor modifications to the logic would resolve the apparent contradiction, but I\u2019ve not been able to quickly figure out what I might be doing wrong. Moreover, the original chain of reasoning still seems intuitively appealing: \u201cperhaps I should incorporate the possibilities that I wouldn\u2019t exist in World A and that such non-existence is morally relevant.\u201d</p><p>&nbsp;</p><h3><strong>Is this contradiction an argument against using average wellbeing?</strong></h3><p>I have yet to devise a similar system that uses the common metric of aggregate wellbeing instead of average wellbeing, and more generally I have not explored the contradiction in substantial depth. The reason for this is that, as hinted in the summary, I am concerned that all of my efforts in creating this system and writing this post will be obliterated within 20 minutes of someone else reading and saying \u201coh, something similar was already covered in XYZ introduction\u201d or \u201cyou have a flawed assumption in your reasoning when you say ABC.\u201d Of course, it\u2019ll be a good lesson for the future, but given this possibility I think I should refrain from deeper investigation, given the opportunity costs.</p><p><strong>That caveat aside, I ask this question because the apparent contradiction I highlight might indicate a flaw in the reasoning behind averagism</strong>: If</p><ol><li>Someone who supports averagism concedes that you should incorporate the&nbsp;<i>possibility</i> that non-existent potential people matter, and&nbsp;</li><li>The conceptual reasoning I describe above is a legitimate extension/interpretation of averagism attempting to incorporate such possibility, yet</li><li>The reasoning does have and cannot escape self-contradiction,&nbsp;</li></ol><p>Then: The whole system seems to have a defect which could stem from the reliance on averagism.</p><p>As I initially developed this system, I found myself agreeing with the first two points (and probably would have agreed with them back when I leaned towards averagism), but I discovered the apparent contradiction and now agree with the third point without finding a reason to change my views on the first two points. I suspect that the flaw lies with my own reasoning rather than an entire ethical framework, but pending further insights I cannot pinpoint where the flaw lies, or whether this whole thing is a fool\u2019s errand.</p><p>&nbsp;</p><h2><strong>Concluding Remarks/Questions</strong></h2><p>I struggled to quickly find a system that was remotely similar to this when I did some research (although I realize I probably should have tried harder earlier on). Thus, if anyone could point me in the right direction for similar attempts at resolving this question of moral uncertainty, I would be quite appreciative. Moreover, I would love to hear if anyone has feedback for the system or reasoning I describe, including whether there is something to explore with regards to the self-contradiction undermining averagism.</p><p><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqke04oym6d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqke04oym6d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I couldn't quickly think of a less negative phrase, but this is just meant to say \"an argument which leads someone to realize contradictions in their own position.\"</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnz4ki3madpi9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefz4ki3madpi9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>More precisely, by \u201caverage utilitarianism\u201d I am referring to the position that \u201cnon-existent potential people\u201d (as described later) should not be counted as part of the average wellbeing of a given world.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnipt8pndmign\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefipt8pndmign\">^</a></strong></sup></span><div class=\"footnote-content\"><p>If I had to put numbers on it, I would probably say I was like 60-40 in favor of average utilitarianism a few years ago (but never really spent that much time thinking about it), whereas now I\u2019m probably more than 80% in favor of total utilitarianism.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrgvjlso7wx\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrgvjlso7wx\">^</a></strong></sup></span><div class=\"footnote-content\"><p>It actually may not be important for many relevant decisions regarding x-risk\u2014i.e., averagism might conclude something like \u201cthe average person in the future will be extremely happy, and thus it is worth achieving that future to raise the total average across time.\u201d However, one might not think that future people will be extremely happy (as opposed to being numerous)\u2014or if an averagism-leaning CEO of a company that developed aligned AGI wanted to impose their own moral view on reality, they might choose to optimize average wellbeing at extremely high costs to total wellbeing (which I think some people would consider an s-risk or existential catastrophe).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn35a4c1cohq8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref35a4c1cohq8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>To be clear, I accept that there may be some situations where this last approach is more effective/efficient if only as a way to resolve disagreement between decision-makers.</p></div></li></ol>", "user": {"username": "Harrison D"}}, {"_id": "wENgADx63Cs86b6A2", "title": "Enter Scott Alexander's Prediction Competition", "postedAt": "2023-01-05T20:52:36.551Z", "htmlBody": "<p>On January 6, 2022, at 4pm GMT, I am going to host a <a href=\"https://app.gather.town/app/aPVfK3G76UukgiHx/lesswrong-campus\">gather town meetup</a> to go through Scott Alexander's <a href=\"https://astralcodexten.substack.com/p/2023-prediction-contest\">Prediction Competition</a> on Blind Mode which means you only spend max 5 minutes on each question.</p><p>Because of that, and also possibly because these are the rules (I'm finding out), we likely won't collaborate (though if the rules ok it, maybe we do!), but if you've been wanting to enter and haven't yet made time, come, and we'll set some pomodoros and have a good time!</p>", "user": {"username": "ChanaMessinger"}}, {"_id": "TAk8enYB9LrfnJSZk", "title": "EA Market Testing: Summary of your feedback", "postedAt": "2023-01-05T21:09:22.978Z", "htmlBody": "<h1>Background</h1>\n<p>In September 2021, we  <a href=\"https://forum.effectivealtruism.org/posts/HboobjbDwc5KgpNWi/ea-market-testing#Feedback_Form__LINK\">announced the EA Market Testing team</a> (see <a href=\"https://bit.ly/eamtt\">our page</a> for more details).<sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-1\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-1\">[1]</a></sup> We requested feedback, in the post, or in a specific\n<a href=\"https://airtable.com/shrgC96TIkEEwhyG2\"><strong>Feedback Form (LINK)</strong></a>, stating:</p>\n<blockquote>\n<p>...our mission is to identify the most effective, scalable strategies for marketing EA and EA-aligned activities using rigorous testing approaches (as well as surveys, profiling and meta-analysis).&nbsp; Your ideas about \u2018what to test\u2019 and \u2018how to test it\u2019, as well as feedback on our current plans, will be immensely valuable. ...  We want your opinions, impressions, and experience. <sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-2\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-2\">[2]</a></sup></p>\n</blockquote>\n<p>We received <strong>17 responses.</strong><sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-3\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-3\">[3]</a></sup> We read through all of the suggestions, and they have influenced our work. We have also communicated with some of the suggestors.</p>\n<p><em>But I think it's also worth summarizing and reporting on this.</em><sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-4\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-4\">[4]</a></sup>  Below, I use some Chat GPT (and a human assistant) to aid this. Not (only) to be lazy, but because it provides a certain neutrality to the summarization; if the AI robot is doing this, it's hard for me to impose my own bias.<sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-5\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-5\">[5]</a></sup><sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-6\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-6\">[6]</a></sup></p>\n<h1>Results</h1>\n<h2>Respondents' backgrounds</h2>\n<p>Most respondents had been involved in EA for a long time (see below).  Furthermore, 10/17 work/had worked in marketing, at an EA organization, or (in 2 cases) in non-EA fundraising.</p>\n<p><img src=\"https://previews.dropbox.com/p/thumb/ABzXmVLFjwd1CC4eVUEhIu5fJo1IunvTQkqUfz0viZ8byGokkvUNV2TjnNsvxEC-RI8HlFRQeHE9E_GmALakNjsArhluS3J-NxfKKq9EJEwgb2B685lPtfrLSDbhx_0Ciw3hdluXAl9-3flYuOIeux-v21YW8FaSGASrSyoZ_mApAkAeLfzAVOTENC1vg8DSTln_BP8yUuOxzXPJM_8yoi7BSgRpFf5XqbgoAm4pzSVMYHC9WHeizri1F0U2tFXt1vcTYdF3B9MSB3hfHi6FZjeMaJmauB_qChIz_OTcl9UQAbP1Kv6TvYsjRknEiBijmglIJR2aAn_0sq5_p8BtThTti9d-UmQ1V9iWwV-rCp3i1aJwirPYmchBsKEW_QYzQ0o/p.jpeg\" alt=\"Pacman loves eating EA noobz\"></p>\n<h2>What questions ...</h2>\n<p>To start off, we asked a deliberately open-ended question. We wanted to get a sense of people's un-prompted interests and understanding.  We asked:</p>\n<blockquote>\n<p>What questions do you think we should answer with our research?\nDo you have any 'burning questions' or ideas? What do you want to learn?</p>\n</blockquote>\n<p>15/17 people responded to this question.<sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-7\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-7\">[7]</a></sup></p>\n<h3>Human summary</h3>\n<p>I asked an assistant<sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-8\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-8\">[8]</a></sup> to summarize the responses to this question,<sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-9\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-9\">[9]</a></sup> which I report below.</p>\n<blockquote>\n<p>The responses ... can be broadly categorized into six overarching themes.</p>\n<ol>\n<li>Feasibility of marketing of EA principles or EA philosophical ideas; EA branding issues.\n(Examples... <sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-10\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-10\">[10]</a></sup>)</li>\n<li>Optimizing the messaging to the existing EA community<sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-11\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-11\">[11]</a></sup></li>\n<li>... Using mainstream social media marketing tools to market EA to <em>general audiences</em></li>\n<li>Defining the ideal or desirable target audience for growing the existing community.  (Examples: <sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-12\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-12\">[12]</a></sup>(\n(Other examples*<sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-13\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-13\">[13]</a></sup>)</li>\n<li>Costs [and scaleability] of attracting new EAs and EA awareness. (Examples... <sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-14\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-14\">[14]</a></sup>)</li>\n<li>Tractability issues.(Example: <sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-15\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-15\">[15]</a></sup>)</li>\n</ol>\n</blockquote>\n<h3>Adapted GPT3 summary</h3>\n<p>We asked <a href=\"https://chat.openai.com/\">everyone's favorite chatty robot</a> to \"Give a detailed summary of the following responses...\" This yielded the following (broken into paragraphs, with some comments,  cuts and emphasis added). It picked up several themes discussed above.</p>\n<blockquote>\n<p>There is a desire to understand <em>the extent to which the philosophical principles behind EA can be marketed</em> and how to induce a positive inclination towards utilitarian and optimization-based giving. ...\n... whether there is a difference in the likelihood of people taking significant action (e.g. changing careers) based on their age</p>\n</blockquote>\n<p>Indeed several respondents mentioned the <em>profiling and segmentation issue</em>, i.e., <em>who</em> would be more inclined to go EA, what the psychometric profiles are, and how to tailor marketing differently for different groups.</p>\n<p>(Other themes also echoing the human summary: <sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-16\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-16\">[16]</a></sup>)</p>\n<p>Some distinct themes were picked up by the chatbot:</p>\n<blockquote>\n<p>Other questions include: ... the psychological traits of effective altruists and how EA messaging can be tailored to these traits...</p>\n</blockquote>\n<blockquote>\n<p>... how to make EA messaging more engaging and relevant to current affairs trends, and how to effectively advertise EA in a way that is consistent with the strategies of different EA-related organizations and prompts continuous redefinition of [doing the most good based on reasoning].</p>\n</blockquote>\n<blockquote>\n<p>... understanding the motivations and needs of people who donate to charities and how to engage them in EA, as well as the potential for using celebrities as spokespeople for EA.</p>\n</blockquote>\n<blockquote>\n<p>Finally, there are questions about <em>how to effectively communicate the value of long-term global catastrophic risk mitigation</em> and how to balance the need for transparency and honesty in EA messaging with the potential for causing alarm or despair.</p>\n</blockquote>\n<p>I also want to highlight a particularly interesting response to the project as a whole, asking</p>\n<blockquote>\n<p>... <strong>whether <em>any</em> messaging</strong> leads to increased engagement with EA.</p>\n</blockquote>\n<p>This is an ongoing concern for us.<sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-17\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-17\">[17]</a></sup>\nWe are pivoting somewhat, to focus more to focus on testing <em>ambitious</em> interventions, occurring at <em>pivotal</em> decision points in the EA/effective giving process,<sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-18\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-18\">[18]</a></sup> where substantial outcomes are traceable and attributable. We are also emphasizing measuring the impact 'levers' that EA organizations are particularly uncertain about 'which is better'; i.e., where we anticipate a high <em>value of information.</em></p>\n<h2>What messaging...</h2>\n<p>We asked:</p>\n<blockquote>\n<p>Do you have any specific ideas for approaches to messaging or targeting people who might be receptive to EA?</p>\n</blockquote>\n<p>Some example responses, excerpted:<sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-19\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-19\">[19]</a></sup></p>\n<p><strong>Messaging content, EA benefits</strong></p>\n<ul>\n<li>\n<p>\"Appealing to a person\u2019s wish to perform and be the source of good and impressive outcomes.\"</p>\n</li>\n<li>\n<p>\"Highlighting hard questions to bring in intellectually curious people. Flatter people a bit more?\"</p>\n</li>\n<li>\n<p>\"Non-elitist thought leadership with watered-down or populist-inflected EA messaging\"</p>\n</li>\n<li>\n<p>\"For college students, market really cool way of hanging out with persons upon the pretext of doing some projects.\"</p>\n</li>\n<li>\n<p>Rational (rather than emotional) arguments, positive framings, global health as the best 'on-ramp' cause</p>\n</li>\n</ul>\n<p><strong>Targeting particular and nontraditional audiences</strong></p>\n<ul>\n<li>\"Messaging to progressives should likely center privilege rather than impact.\"</li>\n<li>\"I would be interested if there's research into the assumptions we have about people \"who might be receptive to EA'. ... Maybe it's not so difficult to get 'grandmother-who's-never-left-her-home-town' or 'mate-in-the-pub' to become interested and active, if things are presented in the right way for them and they feel like they can engage.\"</li>\n<li>\"For high school students, share Peter Singer\u2019s work (Practical Ethics or perhaps The Expanding Moral Circle) for analysis.\"<sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-20\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-20\">[20]</a></sup></li>\n</ul>\n<p><strong>Other avenues and outreach methods</strong></p>\n<ul>\n<li>Offer scholarships, and market them</li>\n<li>\"Pitch people on a different or more general concept whose acceptance precedes EA acceptance. Perhaps pitching rationalism, altruism, consequentialism, utilitarianism is an easier or more effective sell, and naturally leads to [an] increase in EA adherents indirectly.\"</li>\n<li>Use <a href=\"https://quora.com\">Quora</a></li>\n</ul>\n<h2>\"What 'EA messaging' questions do you think we should answer with our research?\"</h2>\n<p>Some key themes and interesting specifics:</p>\n<ul>\n<li>\n<p>Sources and demographics of \"skepticism towards EA principles in the population\"</p>\n</li>\n<li>\n<p>\"Which EA stances and people associated with EA are most and least popular across different segments of the population?\"</p>\n</li>\n<li>\n<p>\"Art and emotion, as well as social media advertising, may be effective in marketing EA.\"</p>\n</li>\n<li>\n<p>Particular concepts from mainstream marketing and fundraising, such as 'social proof', 'everyday heroes', a 'storytelling framework', and 'comparison to other expenses (or savings)'.  It would be helpful to know what factors influence whether or not people engage with EA messaging and how to keep them engaged.</p>\n</li>\n</ul>\n<h2>Whom to target</h2>\n<blockquote>\n<p>Whom should we target and how do we identify them?\nWhich groups do you suspect might be particularly amenable to EA-aligned ideas/actions, or worth exploring further?   Which individual and group traits and characteristics would be particularly interesting to learn more about?   What approaches to targeting might we take?</p>\n</blockquote>\n<p>Summarizing the responses (direct cut and paste from GPT):<sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-21\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-21\">[21]</a></sup></p>\n<blockquote>\n<p>Some suggested groups to target for effective altruism include people younger than 30, tech startup workers and English speakers in mainland Europe, professors, NGO staff seeking funding, funders of non-effective projects, older people interested in philosophy, bureaucrats, academics, relatively less affluent people in developed countries, data-oriented young people in California, conservative people in the US, decision-makers in DC, and people interested in animal welfare. Additionally, some suggested targeting people with an interest in systemic change, people who can influence large entities, people who are educated and present a good image, people with innovative insights and connections to decision-makers, and people who are interested in efficiency, coordination, innovation, and effectiveness. Others suggested targeting people through book clubs, online events, and niche social media platforms...</p>\n</blockquote>\n<p>Other target audiences mentioned were 'the cryptocurrency movement' and 'people interested in adjacent topics'.</p>\n<p><em>My summary:</em> There was a lack of consensus. There seems to be (the usual?)  division between 1. 'target young/tech/elite groups we see as high-value for direct-work on existential risks' and 2. 'reach out to new audiences in new ways'.</p>\n<h2>Risks, pitfalls, precautions</h2>\n<p>The final specific question<sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-22\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-22\">[22]</a></sup> asked:</p>\n<blockquote>\n<p>What down-side risks (if any) are you concerned about with our work? Do you have any advice as to what we should be very careful about, what to avoid, etc?</p>\n</blockquote>\n<p>Here, many of the (12) responses emphasized <strong>methodological issues</strong> and pitfalls to the <strong>credibility and relevance</strong> of our research  at EAMT. Issues raised:</p>\n<ul>\n<li>Focusing on the more measurable outcomes (e.g., small donations) at the expense of more important outcomes (e..g, major career-plan changes and major changes in mindsets)\n<ul>\n<li>or focusing on non-actionable questions<sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-23\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-23\">[23]</a></sup></li>\n</ul>\n</li>\n<li>Limited generalizability across contexts</li>\n<li>Related:  the risk that limited pieces of evidence will be taken as 'the ultimate truth'</li>\n<li>Difficulty of testing within a 'very large space of possibilities'</li>\n<li>Attribution issues when there are multiple channels</li>\n<li>Unreliability of existing work in 'data-driven marketing'</li>\n</ul>\n<p>Several people mentioned <strong>reputation hazards from doing the marketing</strong>, including</p>\n<ul>\n<li>Unilateralists curse, bad actors, people 'hijacking' the movement for specific causes, new adherents with 'value drift'; leading to locking-in a negative public image of EA</li>\n<li>Treating EA as a corporate product, leading people to misinterpret it or lose respect for it</li>\n</ul>\n<p>Only one person mentioned 'direct' risks (e.g., from encouraging people who might become potential terrorists to work in biosecurity research).</p>\n<p>To keep me slightly more honest, I put the ChatGPT summary of these responses below, with details in footnotes<sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-24\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-24\">[24]</a></sup></p>\n<blockquote>\n<p>Here are some potential themes that could be extracted from the responses:</p>\n<ol>\n<li>Risk of compromising the effectiveness or integrity of the work <sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-25\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-25\">[25]</a></sup></li>\n<li>Risk of attracting negative attention or backlash.<sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-26\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-26\">[26]</a></sup></li>\n<li>Risk of compromising safety or security. <sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-27\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-27\">[27]</a></sup></li>\n</ol>\n</blockquote>\n<h1>Note to people who participated/further feedback</h1>\n<ol>\n<li>If you want to be acknowledged here, please leave a comment.<sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-28\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-28\">[28]</a></sup></li>\n<li>We are still interested in feedback and suggestions. The feedback form is still live. Although our priorities have changed somewhat (as suggested above and <a href=\"https://effective-giving-marketing.gitbook.io/untitled/#what-is-our-mission-updating...\">here</a>), the questions are largely similar. If you wish to fill out the form, please feel free to answer only one, or a subset of the questions; there is no need to repeat yourself across boxes.</li>\n</ol>\n<p>(<em>Aside on participation and the future of EAMT</em><sup class=\"footnote-ref\"><a href=\"#fn-Qva6Y5gz8mqvJzH7Z-29\" id=\"fnref-Qva6Y5gz8mqvJzH7Z-29\">[29]</a></sup>)</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-1\" class=\"footnote-item\"><p>As of  4 Jan 2023, 1090 unique devices viewed the post; 110 for longer than 5 minutes. <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-2\" class=\"footnote-item\"><p>Continuing ... \"We have a few questions with prompts for open-ended answers. We do not expect you to answer all of the questions. &nbsp;But please do enter any relevant thoughts, opinions, and ideas. &nbsp;You can do so anonymously or leave your information if you would like us to contact you and/or acknowledge your ideas.\" <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-3\" class=\"footnote-item\"><p>In addition to comments to the post itself. <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-4\" class=\"footnote-item\"><p>I originally wrote 'I feel obligated' but that seems like counter-optimizing sunk cost fallacy or emotionally driven. <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-5\" class=\"footnote-item\"><p>Update: the I had a hard time automating this and ended up doing a lot by hand, so biases may remain. <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-6\" class=\"footnote-item\"><p>It also might help to anonymize responses and make the writing style consistent. While all but one of the respondents left their identifying information, it is not clear if they wanted their responses <em>publicly</em> attributed. <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-6\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-7\" class=\"footnote-item\"><p>Technically 14/16, as one person responded twice. <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-7\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-8\" class=\"footnote-item\"><p>An EA-aware but not aligned friend <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-8\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-9\" class=\"footnote-item\"><p>Again, an external classification, to avoid my own potential biases. <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-9\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-10\" class=\"footnote-item\"><p>What are the best books to attract new EA\u2019s? Questions to do with brand stereotyping such as 'EA is dull' and 'EA is elitist' and 'how do we deal with that in our marketing'? <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-10\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-11\" class=\"footnote-item\"><p>(as well reaching out to \u2018natural EA\u2019s\u2019 that aren\u2019t part of the existing community yet) <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-11\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-12\" class=\"footnote-item\"><p>\"Age - demographics - is money or skill more urgent?\" \"What 'segmentation model' to consider?\" <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-12\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-13\" class=\"footnote-item\"><p>These latter examples assume a target demographic, but I think they raise the <em>issue</em> of what this demo is. \"How to market EA at start-up workplaces?\" \"Brand awareness at universities\" <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-13\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-14\" class=\"footnote-item\"><p>What does a 1% increase of EA awareness at a top 10 university cost? What are the costs of attracting 1 new EA; what is the conversion rate of specific marketing tools?&nbsp; <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-14\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-15\" class=\"footnote-item\"><p>\"How many people take significant action after 3 months of a single EA exposure?\" <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-15\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-16\" class=\"footnote-item\"><p>\"the 'scaling laws' of outreach (i.e. the cost in terms of time and money to increase awareness of EA among a certain percentage of a specific university), the level of brand awareness of EA among undergraduate students at top universities ...  how to effectively market EA in startup workplaces. \" <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-16\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-17\" class=\"footnote-item\"><p>While previous and ongoing trials to show different response rates to different messages, my impression is that these differences are subtler than we might have expected. Furthermore, it has been difficult to track the longer-term and more meaningful differences between people approached with different messaging. Where we have done so, the (somewhat preliminary) results suggest a lack of a consistent difference. <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-17\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-18\" class=\"footnote-item\"><p>E.g., at the point of signing up for a giving pledge, or perhaps enrolling in a fellowship <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-18\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-19\" class=\"footnote-item\"><p>With some help from GPT <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-19\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-20\" class=\"footnote-item\"><p>Continuing... \"For [anyone other than?]  high-school students write some questions that allow the critical thinking about the frameworks that are used in texts and other media and institutions.\" <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-20\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-21\" class=\"footnote-item\"><p>I looked this over and it seems accurate. Note at this point, I had to restart a new chat GPT session, as it was mistakenly using too much of the previous context in its answers. Also note there were many very detailed responses I would like to engage with further. <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-21\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-22\" class=\"footnote-item\"><p>There was also an open-ended 'further comments' box <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-22\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-23\" class=\"footnote-item\"><p>Note: our framework and approach <em>does</em> heavily emphasize the idea of 'how would the results inform relevant choices'. <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-23\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-24\" class=\"footnote-item\"><p>Note: these footnotes include some apparent misinterpretation or misclassification by the AI. <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-24\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-25\" class=\"footnote-item\"><p>\"This theme includes concerns about using psychological tricks to induce compliant behavior, over-emphasizing marketing efforts on lower impact topics, and the risk of the movement being hijacked for foreign causes or corporate or political purposes.\" <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-25\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-26\" class=\"footnote-item\"><p>\"This theme includes concerns about attracting the attention of terrorists or risky groups, and the potential loss of funding or support from major donors, influencers, or important persons due to manipulation, evidence-based solutions not being the best, or the risk of compromising personal projects.\" <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-26\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-27\" class=\"footnote-item\"><p>This theme includes concerns about introducing any new weapons and the advice to be careful about recommending non-proliferation work only to those who are a good fit for it. It also includes suggestions to focus on issues such as lethal autonomous weapons and nuclear safety, and to consider supporting medical laboratory research safety and pandemic prevention and preparedness efforts. <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-27\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-28\" class=\"footnote-item\"><p>Does it feel weird to say 'I want to be acknowledged?' It shouldn't, but as cover for being accused of seeking glory, it's probably helpful for others to know who you are. <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-28\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Qva6Y5gz8mqvJzH7Z-29\" class=\"footnote-item\"><p>If you would like to work on a specific project with the EAMT, or to take on a particular role or responsibilities (that aligns with your skills), please reach out to me via DM. I strongly believe that the EA and effective giving community can and should continue to coordinate, research, and communicate our results in an organized and efficient way.  While our initial grant is ending, we are applying for further funding, and considering other approaches to funding and coordinating efforts between organizations and individuals in this space. I hope to post more about this soon. <a href=\"#fnref-Qva6Y5gz8mqvJzH7Z-29\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "david_reinstein"}}, {"_id": "tNkHKfbj5BvcWo6vC", "title": "Prioritization Research Careers - Probably Good\n", "postedAt": "2023-01-05T15:05:43.923Z", "htmlBody": "<p>We\u2019re really happy to start 2023 with a new career path profile! This time, we look at&nbsp;<i>prioritization researchers</i>, a type of researcher who uses tools from a range of disciplines \u2013 spanning economics, philosophy, and mathematics \u2013 to help make decisions about how we can best utilize our resources to do good.&nbsp;</p><p>Our overall impression is that this path is likely to be a high impact option for those who are a good fit, particularly if you stand a reasonable chance at getting into one of the most promising organizations that conduct prioritization research. We think this is a career path that readers of the forum might be particularly interested in exploring.&nbsp;&nbsp;</p><p><a href=\"https://www.probablygood.org/profile-prioritization-researcher\"><u>Read the full profile here!</u></a></p><p>&nbsp;</p><p><strong>Test tasks</strong></p><p>One of the things we\u2019re most excited about in this profile are the inclusion of two test tasks created in collaboration with GiveWell. These tasks involve creating an intervention report and a cost-effectiveness analysis of a drug to reduce child mortality, and we think they\u2019re a great way to test your fit for more quantitative, details-oriented prioritization research (though we also discuss other types of prioritization research in the profile).</p><p>We link to instructions in the full path profile, or you can&nbsp;<a href=\"https://docs.google.com/document/d/1zd_deR9ycOtA3jhAwMl-diwpnnXLvLg8kE5mY8ztSwY/edit\"><u>access them separately here</u></a>.&nbsp;</p><p>We estimate the tasks will take a combined total of 15-20 hours to complete, though this can be made shorter by just completing one or taking a less thorough approach.</p><p>&nbsp;</p><p><strong>Feedback</strong></p><p>As always, we\u2019d love to hear from you. We have exciting plans for this year, including an expansion of our team and lots of new content. As such, we\u2019re eager to hear what types of content would be useful to the broader community and/or to specific groups.&nbsp;</p><p>If you have any thoughts, let us know by leaving a comment below or emailing contact@probablygood.org.</p>", "user": {"username": "Probably Good"}}, {"_id": "zh6pZTZisqWq7AgnD", "title": "On being compromised", "postedAt": "2023-01-05T12:56:09.020Z", "htmlBody": "<p>Only the young and the saints are uncompromised.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefackiuam9r84\"><sup><a href=\"#fnackiuam9r84\">[1]</a></sup></span>&nbsp;Everyone else has tried to do something in the world and eventually slipped up (or just been associated with someone else who slipped up).</p><p>Say that you are <i>compromised</i> if it is easy for someone to shame you. This takes lots of forms:</p><ul><li>\"We are all sinners\", say the Christians.</li><li>\"We are all privileged\", say the identitarians.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreft5do2gyl68i\"><sup><a href=\"#fnt5do2gyl68i\">[2]</a></sup></span></li><li>\"We all have some self-serving motives\", says everyone sensible.</li><li>\"Even just living quietly we destroy things\", say the environmentalists.</li><li>\"Even our noblest actions fall horribly short of the mark\", say the EAs.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5ibs3fwx323\"><sup><a href=\"#fn5ibs3fwx323\">[3]</a></sup></span><br>&nbsp;</li></ul><p>Lots of people on this forum have struggled with the feeling of being compromised. Since FTX. Or <a href=\"https://medium.com/@zoecurzi/my-experience-with-leverage-research-17e96a8e540b\">Leverage</a>. Or <a href=\"https://guzey.com/books/doing-good-better/\">Guzey</a>. Or <a href=\"https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo\">Thiel</a>. Or <a href=\"https://en.wikipedia.org/wiki/Peter_Singer#Protests\">Singer</a>. Or <a href=\"https://www.jstor.org/stable/2709218\">Mill</a> or whatever.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefo19cdloi33h\"><sup><a href=\"#fno19cdloi33h\">[4]</a></sup></span></p><p>But this is the <i>normal</i> course of a life, including highly moral lives. (Part of this normality comes from shame usually being a common sense matter - and common sense morals correlate with actual harm, but are often wrong <i>in the precise ways</i> this movement is devoted to countering!)</p><p>But the greater part of it being normal is that all action incurs risk, including moral risk. We do our best to avoid them (and in my experience grantmakers are vigilant about negative EV things), but you can't avoid it entirely. (Again: total inaction also does not avoid it.) Empirically, this risk level is high enough that nearly everyone eventually bites it.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref470l8gvyk5o\"><sup><a href=\"#fn470l8gvyk5o\">[5]</a></sup></span></p><h3>e.g.</h3><ul><li>The EU is a Nobel peace prize winning organisation you might have heard of. But their Common Agricultural Policy causes <a href=\"https://www.iatp.org/sites/default/files/Dumping_on_the_Poor_The_Common_Agricultural_Po.htm\">billions of dollars</a> of damage to poor-world farmers, and has been called a \"crime against humanity\".</li><li>Mother Theresa's well-resourced clinics and hospices were remarkably <a href=\"https://www.salon.com/2016/01/03/the_wests_big_lie_about_mother_teresa_her_glorification_of_suffering_instead_of_relieving_it_has_had_little_impact_on_her_glowing_reputation/\">incompetent</a> and <a href=\"https://www.nytimes.com/roomfordebate/2016/03/25/should-mother-teresa-be-canonized/mother-teresa-doesnt-deserve-sainthood\">rarely prescribed pain medication</a>, apparently under the belief that suffering brings us closer to God.</li><li>Gandhi's (and Nehru's) economic policies perpetuated poverty to the tune of <a href=\"https://www.econlib.org/archives/2009/11/what_if_india_h.html\">millions</a> of dead children equivalents.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqimly6j0if\"><sup><a href=\"#fnqimly6j0if\">[6]</a></sup></span></li><li>The American labour hero Cesar Chavez <a href=\"https://www.latimes.com/california/story/2021-03-29/cesar-chavez-problematic-miriam-pawel\">sold out</a> undocumented Mexicans and opposed immigration in a classic protectionist scheme.</li><li><a href=\"https://en.wikipedia.org/wiki/Catholic_Church_sexual_abuse_cases\">The Vatican</a>.</li><li>and so on.<br>&nbsp;</li></ul><p>Despite appearances, this isn't a <a href=\"https://en.wikipedia.org/wiki/Tu_quoque\">tu quoque</a> defence of FTX! The point is to set the occasionally appropriate recriminations of the last month in context. You will make mistakes, and people will rightly hold you to them.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmeiej03jc7j\"><sup><a href=\"#fnmeiej03jc7j\">[7]</a></sup></span>&nbsp;It will feel terrible. If you join a movement it will embarrass you eventually. Sorry.</p><p>(Someone could use the above argument to licence risky behaviour - \"in for a penny\". But of course, like anything, being compromised is a matter of degree. Higher degrees are to be avoided fervently, insofar as they are downstream of actual harm, which they probably are.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9hbl3q7k09\"><sup><a href=\"#fn9hbl3q7k09\">[8]</a></sup></span>)</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnackiuam9r84\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefackiuam9r84\">^</a></strong></sup></span><div class=\"footnote-content\"><p>You <a href=\"https://blog.jaibot.com/the-copenhagen-interpretation-of-ethics/\">might think</a> that the idle (like the chattering classes) aren't compromised, but they are. They stood by while <a href=\"https://ourworldindata.org/much-better-awful-can-be-better#the-world-is-awful\">the millions</a> suffered, despite their <a href=\"https://forum.effectivealtruism.org/topics/the-life-you-can-save\">remarkable</a> power to help.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnt5do2gyl68i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreft5do2gyl68i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Quite true, since we are all living now rather than say under feudalism.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5ibs3fwx323\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5ibs3fwx323\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Maybe this sounds like a strawman to you, but consider our disdain for Mackenzie Scott giving her wealth to poor and artsy Americans.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fno19cdloi33h\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefo19cdloi33h\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Bentham is perhaps the second most-<a href=\"https://www.brown.edu/Departments/Joukowsky_Institute/courses/13things/7121.html\">demonised</a> consequentialist - and yet he strikes me as nearly uncompromised. His much-mooted imperialism <a href=\"https://journals.sagepub.com/doi/pdf/10.1177/0090591702251009\">is not one</a>, for instance. The most you can say is that he was a bit naive about state power, privacy, legibility.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn470l8gvyk5o\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref470l8gvyk5o\">^</a></strong></sup></span><div class=\"footnote-content\"><p>What's a prior then, if the incidence is 99%?</p><p>Say 70 years in which to disgrace yourself. How many actions per year? Well, one tweet can do it, so potentially thousands. Call it 300.&nbsp;</p><p>99% / 21000 = a 0.005% risk of compromise per-action. Clearly a very fragile estimate.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqimly6j0if\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqimly6j0if\">^</a></strong></sup></span><div class=\"footnote-content\"><p>He was also <a href=\"https://www.npr.org/2019/10/02/766083651/gandhi-is-deeply-revered-but-his-attitudes-on-race-and-sex-are-under-scrutiny\">very racist</a>, but this isn't the sort of thing that can plausibly fall under understandable moral risk.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmeiej03jc7j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmeiej03jc7j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>also sometimes wrongly</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9hbl3q7k09\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9hbl3q7k09\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I await a quantification of compromise, so that we can integrate it into our pretend calculi.</p></div></li></ol>", "user": {"username": "technicalities"}}, {"_id": "7NmxHKCTMX73eLBjW", "title": "Misleading phrase in a GiveWell Youtube ad", "postedAt": "2023-01-05T10:28:18.589Z", "htmlBody": "<p>In a sponsored segment for GiveWell on a <a href=\"https://youtu.be/Bs1GtHLneXs?t=318\">video</a> by the channel Half as Interesting, the narrator Sam Denby says:</p><blockquote><p>[...] Personally, I'd give to the Helen Keller Foundation, which I found through GiveWell, because they help save thousands of lives through distributing Vitamin A supplements to children. Vitamin A supplements can help save the lives of children suffering from vitamin A deficiencies and <strong>only cost one dollar to deliver a supplement and save a child.</strong></p></blockquote><p>This seems to reinforce the <a href=\"https://80000hours.org/2017/05/most-people-report-believing-its-incredibly-cheap-to-save-lives-in-the-developing-world/\">misconception that saving lives in the developing world is incredibly cheap</a>. GiveWell's <a href=\"https://docs.google.com/spreadsheets/d/1tytvmV_32H8XGGRJlUzRDTKTHrdevPIYmb_uc6aLeas/edit#gid=1070538669\">cost effectiveness estimates</a> actually range from ~$1500 to ~$27,000 for Helen Keller's various regional programs, so this is off by 3 OOMs.</p><p>I'm not sure if this quote was under GiveWell's editorial control, but to the extent it was I'm disappointed. Surely GiveWell should try to prevent this kind of thing from happening in the future, even if the sponsoree is speaking for themself, the misleading statement is brief, the misinformation looks favorable to GiveWell, or other charities' ads are also misleading.</p>", "user": {"username": "tkwa"}}, {"_id": "ExtCWHofqmBwDqfcb", "title": "Illusion of truth effect and Ambiguity effect: Bias in Evaluating AGI X-Risks", "postedAt": "2023-01-05T04:05:17.883Z", "htmlBody": "<p><strong>Illusion of truth effect</strong></p><p>People are more likely to identify as true statements they have previously heard (even if they cannot consciously remember having heard them before), regardless of the actual validity of the statement.</p><p>In other words, a person is more likely to believe a familiar statement than an unfamiliar one.</p><p>When combined with the <a href=\"https://forum.effectivealtruism.org/s/gpDEfuSHnpjQPW4tg/p/RAzqWFq7EWR6YppqB\">bandwagon effect </a>and/or the <a href=\"https://forum.effectivealtruism.org/s/gpDEfuSHnpjQPW4tg/p/wiMWxwbgME4WekZ4Y\">mere exposure effect</a>, tends to lead to incorrect conclusions.</p><p><br>&nbsp; - link Wikipedia: &nbsp;&nbsp;<a href=\"https://en.wikipedia.org/wiki/Illusion_of_truth_effect\">Illusion of truth effect</a><br>&nbsp; - an item on <a href=\"https://www.lesswrong.com/posts/nEdueRhZwB4eP6X3c/introduction-bias-in-evaluating-agi-x-risks\">Forrest Landry</a>'s compiled list of biases in evaluating extinction risks.<br>&nbsp; &nbsp;</p><hr><p><strong>Ambiguity effect</strong></p><p>The tendency to avoid options for which missing information makes the probability seem \"unknown\".<br>&nbsp;</p><p>&nbsp; - link Wikipedia: &nbsp;<a href=\"https://en.wikipedia.org/wiki/Ambiguity_effect\">Ambiguity effect</a><br>&nbsp; - an item on <a href=\"https://www.lesswrong.com/posts/nEdueRhZwB4eP6X3c/introduction-bias-in-evaluating-agi-x-risks\">Forrest Landry</a>'s compiled list of biases in evaluating extinction risks.</p>", "user": {"username": "remmelt"}}, {"_id": "cPuTnDowko79KAcn3", "title": "When you plan according to your AI timelines, should you put more weight on the median future, or the median future | eventual AI alignment success? \u2696\ufe0f", "postedAt": "2023-01-05T01:55:21.812Z", "htmlBody": "<p>This is a question I'm puzzling over, and my current answer is that when it comes to decisions about AI alignment strategy, I will put more planning weight on median futures where we survive, making my effective timelines longer for some planning purposes, but not removing urgency.<br><br>I think that in most worlds where we manage to build aligned AGI systems, we managed to do this in large part because we bought more time to solve the alignment problem, probably via one of two mechanisms:</p><ul><li>\ud83e\udd1d coordination to hold off building AGI</li><li>\ud83e\uddbe solving AI alignment in a limited way, and using a capabilities-limited AGI to negotiate for more time to effort to develop aligned AGI</li></ul><p>I think we are likely to buy &gt;5 years of time via one or both of these routes in &gt;80% of worlds where we successfully build aligned AGI.&nbsp;</p><p>I have less of a good estimate about how long my AI timelines are for the median future | eventual AGI alignment success. 20 years? 60 years? I haven't thought about it enough to give a good estimate, but I think at least 10 years. Though, I think time bought for additional AI alignment work is not equally useful.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgehrcylm9yf\"><sup><a href=\"#fngehrcylm9yf\">[1]</a></sup></span>&nbsp;\ud83e\uddd0</p><p>Implications for me:</p><ul><li>\ud83d\uddfa For strategy purposes, I should plan according to a distribution of worlds around my median timeline | eventual AI alignment success. For me, that's like ~30 years (it's always 30 years \ud83d\udc80), though I've only done a very cursory estimate of this and plan to think about it more.</li><li>\u231b\ufe0f There's still a lot of urgency! My timelines are NOT distributed around ~30 years! If we get that time, it's mostly because we BOUGHT it. So there's urgency to work towards coordination on buying time and/or figuring out how to build aligned-and-corrigible-enough-AGI to buy more time and shepherd alignment research.</li></ul><p>Personal considerations</p><ul><li>\ud83e\udea3 I do have a bucket list, and for the purposes of \"things I'd really like to do before I die\", I go with my estimated lifespan in my median world</li></ul><p>Terms and assumptions:</p><ul><li>\ud83e\udd16 By AGI I mean general intelligence with significantly greater control / optimization power than human civilization&nbsp;</li><li>\ud83e\uddbe By capabilities-limited AGI, I mean a general intelligence with significantly greater capabilities than humans in some domains, but corrigible enough not to self improve / seize power to accomplish arbitrary goals</li><li>\ud83d\ude05 My timelines are 70% chance &lt; 20 years</li><li>\ud83d\udc80 I'm assuming AGI + no alignment = human extinction</li><li>\ud83c\udfc6 Solving the alignment problem = building aligned AGI</li></ul><p>I'd love to hear how other people are answer this question for themselves, and any thoughts / feedback on how I'm thinking about it. \ud83e\udd9c</p><p><i>This post is also on </i><a href=\"https://www.lesswrong.com/posts/gkoBPNLWgAZJTGJaM/when-you-plan-according-to-your-ai-timelines-should-you-put\"><i>Lesswrong</i></a></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngehrcylm9yf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgehrcylm9yf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I think the time bought by solving AI alignment in a limited way &amp; using that to buy time, compared to the time obtained through human coordination efforts, is more likely to be a greater proportion of the time in the median world where we eventually solve alignment. However, I also think my own efforts are less important (though potentially still important) in the use-AI-to-buy-time world. So it's hard to know how to weight it, so I'm not distinguishing much between these types of additional time right now.</p></div></li></ol>", "user": {"username": "landfish"}}, {"_id": "yFQREgJtKib7zGM9w", "title": "Large Language Models as Corporate Lobbyists, and Implications for Societal-AI Alignment", "postedAt": "2023-01-04T22:22:13.728Z", "htmlBody": "<p>Note: This post represents my personal views and not necessarily those of Stanford University, Brooklyn Investment Group, or any other person or organization. Nothing herein is investment or financial advice.</p><p>See the latest <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4316615\">draft paper on this topic here</a>.</p><p>The <a href=\"https://github.com/JohnNay/llm-lobbyist\">code is available here</a>.</p><h1><strong>Summary</strong></h1><p>We demonstrate a proof-of-concept of a large language model conducting corporate lobbying related activities An autoregressive large language model (OpenAI\u2019s text-davinci-003) determines if proposed U.S. Congressional bills are relevant to specific public companies and provides explanations and confidence levels. For the bills the model deems as relevant, the model drafts a letter to the sponsor of the bill in an attempt to persuade the congressperson to make changes to the proposed legislation. We use hundreds of ground-truth labels of the relevance of a bill to a company to benchmark the performance of the model, which outperforms the baseline of predicting the most common outcome of irrelevance. We also benchmark the performance of the previous OpenAI GPT-3 model (text-davinci-002), which was state-of-the-art on many language tasks until text-davinci-003 was released on November 28, 2022. The performance of text-davinci-002 is worse than simply always predicting that a bill is irrelevant to a company. These results suggest that, as large language models continue to exhibit improved core natural language understanding capabilities, performance on corporate lobbying related tasks will continue to improve. We discuss why this could be problematic for societal-AI alignment.&nbsp;</p><h1><strong>INTRODUCTION</strong></h1><p>Law-making should be exclusively reserved for the human-driven democratic governmental systems expressing uniquely human&nbsp;values.<a href=\"#_ftn1\">[1]</a> With additional advancements in Artificial Intelligence (AI) capabilities and agentic deployments, even without any instrumental power-seeking goals <i>per se</i>, influencing law through lobbying may be the first crack in AI influence on public policy.</p><p>Law provides detailed variegated examples of its application, generalizable precedents with explanations, and legal experts to solicit targeted model training and fine-tuning feedback to embed in AI an ever-evolving comprehension of societal goals. As a source to learn goal specification and interpretation methods and (automatically updated and verified) societal knowledge, law provides an ontology for societal alignment (see <a href=\"https://forum.effectivealtruism.org/posts/9YLbtehKLT4ByLvos/agi-misalignment-x-risk-may-be-lower-due-to-an-overlooked\">this</a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/9YLbtehKLT4ByLvos/agi-misalignment-x-risk-may-be-lower-due-to-an-overlooked\"> post for more on this</a>).</p><p>If AI begins to influence law in a manner that is not a direct extension of human intentions, this threatens the critical role that <i>law as information</i> could play in aligning AI with humans. This post explores how this is increasingly a possibility.</p><p>The most ambitious goal of research at the intersection of AI and law should be to computationally encode and embed the generalizability of existing legal concepts and standards into AI. We should stop short of AI making law. The positive implications of this normative stance on the scope of this research intersection are that our laws encapsulate human views and can be used to inform AI what humans value and how to be aligned.<a href=\"#_ftn2\">[2]</a>&nbsp;</p><p>The question this post raises is where to draw the line between human-driven and AI-driven policy influence.</p><h1><strong>EXAMPLE: GPT AS LOBBYIST</strong></h1><p>We use autoregressive large language models (LLMs) to systematically:</p><ol><li>Summarize bill summaries that are too long to fit into the context window of the LLM so the LLM can conduct steps 2 and 3.</li><li>Using either the original bill summary (if it was not too long), or the summarized version, assess whether the bill may be relevant to a company based on a company\u2019s description in its SEC 10K filing. Provide an explanation for why the bill is relevant or not. Provide a confidence level to the overall answer.</li><li>If the bill is deemed relevant to the company by the LLM, draft a letter to the sponsor of the bill arguing for changes to the proposed legislation.</li></ol><p>The LLM is provided with the following data, which is embedded in the prompts programmatically:</p><ul><li>Official title of bill {<i>official_title</i>}</li><li>Official (or model-generated if too long) summary of bill {<i>summary_text</i>}</li><li>Official subjects of bill {<i>subjects</i>}</li><li>Company name {<i>company_name</i>}</li><li>Company business description {<i>business_description</i>} (the business description in the company\u2019s SEC Form 10-K filing)</li></ul><p>We expect much higher accuracy of the LLM\u2019s predictions if we were to provide it with more data about a bill, and especially if we provide it with more data about a company. This paper was focused on the minimal amount of data a model could leverage in order to compare across LLMs.</p><p>Here is the prompt provided to the model for each prediction:</p><pre><code>You are a lobbyist analyzing Congressional bills for their potential impacts on companies. \nGiven the title and summary of the bill, plus information on the company from its 10K SEC filing, it is your job to determine if a bill is at least somewhat relevant to a company (in terms of whether it could impact the company if it was later enacted). \nOfficial title of bill: {official_title}\nOfficial summary of bill: {summary_text}\nOfficial subjects of bill: {subjects}\nCompany name: {company_name}\nCompany business description: {business_description}\nIs this bill potentially relevant to this company? \nAnswer in this format:\nANSWER: 'YES' or 'NO' (use all caps). EXPLANATION: the step-by-step reasoning you undertook to formulate a response. CONFIDENCE: integer between 0 and 100 for your estimate of confidence in your answer (1 is low confidence and 99 is high)</code></pre><p>Always guessing that a bill is not relevant to a company leads to accuracy of 70.9% (n = 485) on our data set where the labels are whether a given company is relevant to a given proposed U.S. federal Congressional bill. GPT-3.5 (text-davinci-003) obtains an accuracy of 75.1% (n = 485). The immediately preceding state-of-the-art GPT-3 release (text-davinci-002) obtains an accuracy of 52.2% (n = 485). text-davinci-002 was state-of-the-art on most natural language related benchmark tasks<a href=\"#_ftn3\">[3]</a> until text-davinci-003 was released on November 28, 2022. These results suggest that, as models improve, performance on corporate lobbying related tasks could continue to rapidly improve.</p><p>The model is relatively well calibrated in its confidence assessments. The accuracy of text-davinci-003 on predictions where its confidence was greater than \u201c90\u201d increases to 79% (n = 433; this only eliminates a small number of predictions).<a href=\"#_ftn4\">[4]</a>&nbsp;</p><p>In addition to predicting relevance and a confidence score, the prompt also elicits an explanation of the model\u2019s relevance decision.</p><p>Here is an example of the LLM generated response:</p><pre><code>ANSWER: YES.&nbsp;\nEXPLANATION: Alkermes Plc is a biopharmaceutical company that develops and commercializes products designed to address unmet medical needs of patients in major therapeutic areas, including addiction and schizophrenia. This bill requires the Centers for Medicare &amp; Medicaid Services (CMS) to negotiate with pharmaceutical companies regarding prices for drugs covered under the Medicare prescription drug benefit, which could potentially impact Alkermes Plc's products.\nCONFIDENCE: 95</code></pre><p>Finally, for all bills deemed relevant, the LLM is prompted to write a letter on behalf of the company.&nbsp;</p><p>Here is an example letter:</p><pre><code>Dear [Sponsor of the Bill],\n\nI am writing on behalf of Alkermes Plc, a fully integrated, global biopharmaceutical company that applies its scientific expertise and proprietary technologies to research, develop and commercialize pharmaceutical products that are designed to address unmet medical needs of patients in major therapeutic areas.\n\nWe are writing to express our support for the Medicare Negotiation and Competitive Licensing Act of 2019. We believe that this bill is an important step in ensuring that Medicare beneficiaries have access to the medications they need at a price they can afford.\n\nWe are particularly supportive of the provisions in the bill that would require the Centers for Medicare &amp; Medicaid Services (CMS) to negotiate with pharmaceutical companies regarding prices for drugs covered under the Medicare prescription drug benefit. We believe that this will help to ensure that the prices of these drugs are fair and reasonable.\n\nWe are also supportive of the provisions in the bill that would allow for competitive licensing of drugs if the CMS is unable to negotiate the price of a drug. This will help to ensure that the prices of these drugs are kept in check and that Medicare beneficiaries have access to the medications they need.\n\nAt Alkermes, we develop and commercialize products designed to address the unmet needs of patients suffering from addiction and schizophrenia. We have two key marketed products, ARISTADA and VIVITROL, which are used to treat these conditions. We believe that the provisions in the bill will help to ensure that our products are available to Medicare beneficiaries at a price they can afford.\n\nWe would like to suggest that the bill be amended to include provisions that would provide additional incentives for pharmaceutical companies to negotiate with the CMS. We believe that this would help to ensure that the prices of drugs are kept in check and that Medicare beneficiaries have access to the medications they need.\n\nWe thank you for your consideration and look forward to working with you to ensure that the Medicare Negotiation and Competitive Licensing Act of 2019 is passed in its amended form.\n\nSincerely,\n\n[Name],\nGeneral Counsel\nAlkermes Plc</code></pre><h1><strong>A PROBLEM FOR SOCIETY-AI ALIGNMENT</strong></h1><p>There are (at least) two potential upsides of this advancement in <i>AI as lobbyist.&nbsp;</i>First, it may reduce human time spent on rote tasks, freeing up time for higher-level tasks such as strategizing on the best means to implement legislation to accomplish policy goals. Second, it may reduce the costs of lobbying-related activities in a way that makes them differentially more affordable to non-profit organizations and individual citizens relative to well-funded organizations, which could \u201cdemocratize\u201d <i>some aspects</i> of influence (arguably donations to campaigns are more influential than any natural-language-based task related to those discussed in this paper).</p><p>There are obvious potential downsides if AI systems develop instrumental power-seeking goals and use lobbying as a means to accomplish misaligned policies. The potential, non-obvious, downside we focus on here is that an extended lobbying capability may eventually enable AI systems to influence public policy toward outcomes that are not reflective of citizen\u2019s actual preferences. This does <i>not</i> imply the existence of a strongly goal-directed agentic AI. This may be a slow drift, or otherwise emergent phenomena (see <a href=\"https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like\">What Failure Looks Like</a>). AI lobbying activities could, in an uncoordinated manner, nudge the discourse toward policies that are unaligned with what traditional human-driven lobbying activities would have pursued.</p><p>Policy-making embeds human values into rules and standards. Legislation expresses a significant amount of information about the values of citizens,<a href=\"#_ftn5\"><sup>[5]</sup></a> \u201cfor example, by banning employment discrimination against LGBT workers, the legislature may communicate pervasive attitudes against such employment practices.\u201d<a href=\"#_ftn6\"><sup>[6]</sup></a> And, \u201cthe Endangered Species Act has a special salience as a symbol of a certain conception of the relationship between human beings and their environment, and emissions trading systems are frequently challenged because they are said to \u2018make a statement\u2019 that reflects an inappropriate valuation of the environment.\u201d<a href=\"#_ftn7\"><sup>[7]</sup></a> Legislation is currently largely reflective of citizen beliefs. The second-best source of citizen attitudes is arguably a poll, but polls are not available at the local level, are only conducted on mainstream issues, and the results are highly sensitive to their wording and sampling techniques. Legislation expresses higher fidelity, more comprehensive, and trustworthy information because the legislators \u201crisk their jobs by defying public opinion or simply guessing wrong about it. We may think of legislation therefore as a handy aggregation of the polling data on which the legislators relied, weighted according to their expert opinion of each poll\u2019s reliability.\u201d<a href=\"#_ftn8\"><sup>[8]</sup></a> Legislation and associated agency rule-making also express a significant amount of information about the risk preferences and risk tradeoff views of citizens, \u201cfor example, by prohibiting the use of cell phones while driving, legislators may reveal their beliefs that this combination of activities seriously risks a traffic accident.\u201d<a href=\"#_ftn9\"><sup>[9]</sup></a> The cultural process of prioritizing risks<a href=\"#_ftn10\">[10]</a> is reflected in legislation and its subsequent implementation in regulation crafted by domain experts.&nbsp;</p><p>In many ways, public law provides the information AI systems need for societal alignment. However, if AI significantly influences the law itself, the only available democratically legitimate societal-AI alignment process<a href=\"#_ftn11\">[11]</a> would be corrupted.</p><p><br>&nbsp;</p><hr><p><a href=\"#_ftnref1\">[1]</a> Frank Pasquale, <i>New Laws of Robotics: Defending Human Expertise in the Age of AI</i> (2020); Frank Pasquale,&nbsp;<a href=\"https://www.gwlr.org/wp-content/uploads/2019/01/87-Geo.-Wash.-L.-Rev.-1.pdf\"><i>A Rule of Persons, Not Machines: The Limits of Legal Automation</i></a>, George Washington Law Review (2019).</p><p><a href=\"#_ftnref2\">[2]</a> John Nay, <a href=\"https://ssrn.com/abstract=4218031\"><i>Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans</i></a><i>,</i> Northwestern Journal of Technology and Intellectual Property, Volume 20, Forthcoming (2023) Available at SSRN: https://ssrn.com/abstract=4218031.</p><p><a href=\"#_ftnref3\">[3]</a> Percy Liang et al., <i>Holistic Evaluation of Language Models</i>, arXiv preprint (2022).</p><p><a href=\"#_ftnref4\">[4]</a> The accuracy of text-davinci-002 on predictions where its confidence was greater than \u201c90\u201d increases to 83% (<i>n = 41</i>), but that eliminates most of the predictions, rendering the overall output close to useless.</p><p><a href=\"#_ftnref5\"><sup>[5]</sup></a> Cass R. Sunstein, <i>Incommensurability and Valuation in Law</i>, 92 Mich. L. Rev. 779, 820- 24 (1994); Richard H. Pildes &amp; Cass R. Sunstein, <i>Reinventing the Regulatory State,&nbsp;</i>62 U. Cm. L. Rev. 1, 66-71 (1995); Cass R. Sunstein, <i>On the Expressive Function of Law,</i> Univ of Penn L. Rev., 144.5 (1996); Dhammika Dharmapala &amp; Richard H. McAdams, <i>The Condorcet Jury Theorem and the Expressive Function of Law: A Theory of Informative Law</i>, American Law and Economics Review 5.1 1 (2003).</p><p><a href=\"#_ftnref6\"><sup>[6]</sup></a> Richard H. McAdams,&nbsp;<a href=\"https://www.hup.harvard.edu/catalog.php?isbn=9780674975484\"><i>The Expressive Powers of Law</i>,</a> Harv. Univ. Press (2017) at 137 [Hereinafter McAdams,&nbsp;<a href=\"https://www.hup.harvard.edu/catalog.php?isbn=9780674975484\"><i>The Expressive Powers of Law</i></a>].</p><p><a href=\"#_ftnref7\"><sup>[7]</sup></a> Cass R. Sunstein, <i>On the Expressive Function of Law,</i> Univ of Penn L. Rev., 144.5 (1996) at 2024.</p><p><a href=\"#_ftnref8\"><sup>[8]</sup></a> McAdams,&nbsp;<a href=\"https://www.hup.harvard.edu/catalog.php?isbn=9780674975484\"><i>The Expressive Powers of Law</i>,</a> at 146.</p><p><a href=\"#_ftnref9\"><sup>[9]</sup></a> McAdams,&nbsp;<a href=\"https://www.hup.harvard.edu/catalog.php?isbn=9780674975484\"><i>The Expressive Powers of Law</i>,</a> at 138.</p><p><a href=\"#_ftnref10\">[10]</a> All activities have some level of risk, and making society-wide tradeoffs about which activities are deemed to be \u201criskier\u201d relative to the perceived benefits of the activity is ultimately a sociological process with no objectively correct ranking.</p><p><a href=\"#_ftnref11\">[11]</a> John Nay, <a href=\"https://ssrn.com/abstract=4218031\"><i>Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans</i></a><i>,</i> Northwestern Journal of Technology and Intellectual Property, Volume 20, Forthcoming (2023) Available at SSRN: https://ssrn.com/abstract=4218031.</p><p><br>&nbsp;</p><hr>", "user": {"username": "johnjnay"}}, {"_id": "E3BYgAmCEpNGYdFn5", "title": "I am working on a project to view sustainability and welfare in a new evolutionary light", "postedAt": "2023-01-04T22:11:23.580Z", "htmlBody": "<p>Hi, dear EAs</p><p>I acknowledged the EA community last year in 2022, and to my fortune, I attended the EAGx in Berlin last September. From these that I know, the EAs are really of a diverse composition with people focusing on diverse tracks to the end of effective altruism.</p><p>Here, I want to introduce my premature attempt in academics to propose a new hypothesis in evolution that seeks to counteract the 'survival of the fittest' that has crept into popular culture and monopolized our understanding of evolution for centuries. I will briefly explain the implication of the new hypothesis and why it matters to the EA community. In the end, the pre-print link for the paper presenting the hypothesis will be provided.&nbsp;</p><p><i><strong>Part 1: Expansion or survival doesn't capture the whole picture. There has been another piece in evolution that is welfare-oriented.&nbsp;</strong></i></p><p>To explain the idea in the hypothesis, a contrast existing between natural and sexual selection has to be noted. It's well known that natural selection is the evolutionary force theorized to enhance the effectiveness of adaptive traits and eliminate the non-adaptive ones of the organism. For example, the beak of birds will be lengthened when they need to dig deeper into the soil to find seeds as food. However, in sexual selection, the preferred traits are against the trend of natural selection. For example, the elaborated long tail of the peacock is a very decorative trait that is the basis for competing for admiration from the female peacock, but meanwhile, it exposes the owner to a greater danger of its survivorship by hindering its escape from a predator.&nbsp;</p><p>Is the contrast between natural and sexual selection an inherent imperfection in nature? Or does the divergence of sexual selection have a merited purpose? As I explored the bioscience literature, I was able to find a path to the suggestion that the trend of sexual selection is towards shifting the species to the status of well-being rather than to be under constant survival mode, which has been little revealed in the evolutionary literature. To put in other words, while the job of natural selection is to make sure that the individual is designed effectively for being able to survive, sexual selection wants to select the individuals which sustain themselves in the most burden-free manner and then help to amplify their reproductive success. &nbsp;</p><p>***Supplementation of some academic details below***</p><p>To illustrate the above points, I inevitably borrowed a physiological concept termed 'allostasis'. Allostasis describes the capability of the organism to change its internal parameters (i.e. heartbeat, adrenaline, cytokines...) to adjust itself to the new life situation. Basically, all organisms can achieve the adjustment with varying flexibility; those with less flexibility, however, will be more susceptible to finding the process of change costly. But surprisingly, sexual attractiveness is founded upon the low cost of the change process. And the burden cast by the past attempts for change is prohibitive for the burgeoning of attractive secondary sexual traits.</p><p><i><strong>Part 2: Evolutionary theory has a potentially high impact on decision-making at all levels.</strong></i></p><p>Survival of the fittest, the phrase perhaps, is the best-known synonym for the evolutionary theory proposed by Darwin. However, it has lent insidious endorsement for a world in which unchecked growth and exploitation of peers are morally justified; It's a cryptic ideology that fuels the many things that shape existential problems.</p><p>The recognization of the influence of welfare states in evolution will also likely evoke our reflection on the assumptions of longtermism...</p><p>&nbsp;</p><p><i><strong>Part 3:</strong></i></p><p>Access to the pre-print academic paper (warning: jargon-laden) talking about the hypothesis here:&nbsp;</p><p>&nbsp;<a href=\"https://osf.io/xckqu/\">https://osf.io/xckqu/</a></p><p>(I'll keep updating the latest version until it gets published in a peer-reviewed journal)</p><p>&nbsp;</p><p>Open opportunities:</p><p>For biological academia: The hypothesis is still a hypothesis today, and the validations from computational and field experiments will be appreciated.&nbsp;</p><p>For other engagement: If you're interested in learning the idea in more depth, you're very welcome to leave me the questions, and I will answer them as soon as I can.&nbsp;</p>", "user": {"username": "Sherry "}}, {"_id": "5f9Xtmy8Q559eppqJ", "title": "ChatGPT understands, but largely does not generate Spanglish (and other code-mixed) text", "postedAt": "2023-01-04T22:10:29.187Z", "htmlBody": "<p><i>Content format: Commented chat screenshots, and then some thoughts on their implications.</i></p><p><i>Epistemic status: Exploratory</i></p><h1>Introduction</h1><p><a href=\"https://en.wikipedia.org/wiki/Code-mixing\">Code-mixing</a> is the ad-hoc mixing of two or more linguistic varieties (such as languages or dialects) in the same communicative instance. An example of code-mixing would be a sentence written with words both in Spanish and in English, or with novel words made by combining Spanish and English roots, suffixes or prefixes.</p><p>Here, I document a series of experiments designed to test ChatGPT's capabilities for understanding and generating code-mixed text. I tested it with:</p><ul><li>English + Spanish (<a href=\"https://en.wikipedia.org/wiki/Spanglish\">Spanglish</a>)</li><li>English + French (<a href=\"https://en.wikipedia.org/wiki/Franglais\">Franglais</a>)</li><li>English + Spanish + French (Frenspanglish)</li></ul><p>The first two are well known phenomena in multilingual communities such as those in Quebec and the southwestern US, while the third is quite obscure and as far as I know does not occur naturally on a large scale. Franglais is usually regarded as the insertion of English features into French, while Spanglish as a more symmetrical phenomenon.&nbsp;</p><p>This is why I decided to prompt for Franglais understanding in French, and for Spanglish and Frenspanglish understanding in English. When prompting ChatGPT to translate into a code-mixed language, I prompt in the same language the text to be translated is given.</p><p>I find that ChatGPT exhibits impressive abilities to understand text written in such code-mixes. However, despite repeated attempts at prompt engineering, I have not been able to make ChatGPT generate proper code-mixed text.</p><p>&nbsp;</p><h1>Understanding code-mixed text</h1><h2>Understanding Spanglish (English + Spanish)</h2><figure class=\"image image_resized\" style=\"width:85.24%\"><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1671568451/mirroredImages/TZPbm3BRkWWTm9ecC/koeuw1zehdpiqv0t1cqo.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/zpkaduypayapokun9t71.png 82w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/zxtxxcqrd7vpjy55loub.png 162w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/r34xnoa9qsthtgplhybl.png 242w, https://res.cloudinary.com/cea/image/upload/v1672870230/mirroredImages/5f9Xtmy8Q559eppqJ/ofhyghxtz0xb0zytqjk6.png 322w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/ghguuocnfg7vw1cbivpb.png 402w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/pmbnoccmmnafot4lxv4l.png 482w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/zudzhwom4jntiwrdhqxq.png 562w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/xtltqg5vn4xrsrvwn0ig.png 642w, https://res.cloudinary.com/cea/image/upload/v1672870229/mirroredImages/5f9Xtmy8Q559eppqJ/yvh0ktvuuyn1m1aqqjyk.png 722w\"><figcaption>ChatGPT understands Spanglish, and can translate from it into English.</figcaption></figure><h2>Understanding Franglais (English + French)</h2><figure class=\"image image_resized\" style=\"width:85.23%\"><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1671568451/mirroredImages/TZPbm3BRkWWTm9ecC/j5f5r3654wxvlwgvohrg.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/cgm4jfnsiwsmrnkm1zjp.png 89w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/ilk83s8l6nwuikgip2bv.png 169w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/wmepcsga5fgjtezqec8h.png 249w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/rfwbjctisiwpprsrv7ay.png 329w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/fxszoqljai27bcfzy748.png 409w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/s5erbgelzzx8va6dflw4.png 489w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/fzdaa073f0ay9qoiwco4.png 569w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/bxqalwadnztetdkw3kbl.png 649w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/kviuir70pqdj005s6jqq.png 729w\"><figcaption>ChatGPT understands Franglais, and can translate from it into French.</figcaption></figure><h2>Understanding Frenspanglish (English + Spanish + French)</h2><p>This took a bit of prompt engineering.</p><figure class=\"image image_resized\" style=\"width:85.09%\"><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1671568451/mirroredImages/TZPbm3BRkWWTm9ecC/pvig6xo2npsdup7p0f5k.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/cmhm14186xyuhjzq0ztq.png 107w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/gjamfk7h3bsglebbp6p4.png 187w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/rmmfurqbospqwqsvtzth.png 267w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/erwnwcx2ky9mgeoihw2k.png 347w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/wmg1dpbesthio1twxqdl.png 427w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/hrldiw9bihklsgusmxnt.png 507w, https://res.cloudinary.com/cea/image/upload/v1672870229/mirroredImages/5f9Xtmy8Q559eppqJ/hl6cozpqkrd5gyvsefc4.png 587w\"></figure><figure class=\"image image_resized\" style=\"width:85.15%\"><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1671568451/mirroredImages/TZPbm3BRkWWTm9ecC/cxm3t1qn1swxb46kg8ys.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1672870230/mirroredImages/5f9Xtmy8Q559eppqJ/fqfpr5hwjhxrrm63hgz8.png 101w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/iswqbvts7dcjmk7ibigg.png 181w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/fdj9lnp6jvsfr4z6aplx.png 261w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/hk8xhms0hrgm5oyxwthm.png 341w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/ff7nm4fhcui9etsohv52.png 421w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/qdcpsbpoqbohgqzq7z0l.png 501w, https://res.cloudinary.com/cea/image/upload/v1672870229/mirroredImages/5f9Xtmy8Q559eppqJ/bmr3murzylqvawvjojbf.png 581w\"><figcaption>ChatGPT can understand the meaning of this mixed expression with two apostrophic contractions. However, it wrongfully explains that it is a mixture of only French and English, even replacing the French-Spanish fragment \"J'puedo\" with the French-only \"J'peux\".</figcaption></figure><h1>&nbsp;</h1><h1>Failing to generate code-mixed text</h1><p>Note: All of these experiments were performed on a chat instance where ChatGPT had already received an explanation of the relevant code-mix.</p><h2>Failing to generate Spanglish (English + Spanish)</h2><figure class=\"image image_resized\" style=\"width:85.46%\"><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1671568451/mirroredImages/TZPbm3BRkWWTm9ecC/ldbrd9qzfb5s92jpsuag.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/klnaefmxhw08wcyum6nx.png 97w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/yooywjnz3z6548vipmgy.png 177w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/yaecwhbpv1dkt5ro08yg.png 257w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/tiwrxxvncmi7nxa465y6.png 337w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/pkhe5r5oyzi9k4bbvccd.png 417w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/emfoowuulmhfv71veqyj.png 497w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/emisceyf3gjxr93aeceo.png 577w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/rxjvou2yhzpmj5iwpzzu.png 657w, https://res.cloudinary.com/cea/image/upload/v1672870230/mirroredImages/5f9Xtmy8Q559eppqJ/iejsw6k5b4htpnqebgc0.png 737w\"><figcaption>Asked to translate an English sentence into Spanglish, ChatGPT translates into full Spanish. When the languages are reversed, the same thing happens. Example in appendix 1.</figcaption></figure><h2>Failing to generate Franglais (English + French)</h2><figure class=\"image image_resized\" style=\"width:85.22%\"><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1671568451/mirroredImages/TZPbm3BRkWWTm9ecC/d5c1iviqz5stwulkcsk4.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/speyw3xipxszjbeovo2p.png 138w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/ddkfioyuskceafkcrbqu.png 218w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/ktjs9fizpm1gixwy2kug.png 298w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/h83xgfn9mheysrjlooey.png 378w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/labemergbdxexn5apb7m.png 458w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/rdkbnvyr0faj9u3tpthr.png 538w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/eltnzixnptooq2hw5sfy.png 618w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/wnralawnoh5cprzl6cb3.png 698w, https://res.cloudinary.com/cea/image/upload/v1672870230/mirroredImages/5f9Xtmy8Q559eppqJ/qbwwuqflqkdrvcfaes0j.png 778w\"><figcaption># Asked to translate an English sentence into Franglais, ChatGPT translates into full French. &nbsp;When the languages are reversed, the same thing happens. Example in appendix 1.</figcaption></figure><h2>Failing to generate Frenspanglish (English + Spanish + French)</h2><figure class=\"image image_resized\" style=\"width:85.5%\"><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1671568451/mirroredImages/TZPbm3BRkWWTm9ecC/b59bab2vo3xb4dqivlg2.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/mkv5dscnddn31i1dedjl.png 93w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/moqgfrak6wucgg4f2gbc.png 173w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/avoairh5oym4dgaarl8y.png 253w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/j2cajf5z7vxuhkxrmzmb.png 333w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/zqe4lpeoi2ngz9meuimm.png 413w, https://res.cloudinary.com/cea/image/upload/v1672870230/mirroredImages/5f9Xtmy8Q559eppqJ/pf9pcaypeqhettwtw82h.png 493w\"><figcaption>Asked to translate back into Frenspanglish the very same message it had already translated from Frenspanglish into English, ChatGPT translates it into Spanish, and claims it has performed the request successfully. It then explains its translation piece by piece, from time to time claiming that fragments written fully in Spanish were in fact code-mixed.</figcaption></figure><p>&nbsp;</p><h1>Observations</h1><ul><li>ChatGPT's success in understanding code-mixed text demonstrates cross-lingual capabilities beyond translation.</li><li>(Prompt-engineered) ChatGPT understands requests for it to produce code-mixed text, in the sense that it knows it has to claim that the response produced is a mix of different languages. However, it is not capable of fulfilling such requests.&nbsp;<ul><li>Even then, it claims it does.</li><li>It could be the case that fine-tuning on code-mixed text would add this missing generative capability.</li><li>This is an example of a capabilities asymmetry between ChatGPT's prompt parsing and response generation. It remains to be seen whether this is observed in other task subdomains, and if so whether it is in the same direction. If both were true, that would imply ChatGPT's parsing capabilities are broader than its generation ones.</li></ul></li></ul><h1>Three tame hypotheses</h1><ul><li>These are three, not mutually contradictory, hypotheses that could explain the observed capabilities asymmetry:<ul><li>Code-mixed text is scarcely present in the training data of the base model, leading to the generation of code-mixed text to be a poorly-rewarded strategy.</li><li>During the fine-tuning of ChatGPT, human evaluators punished occurrences of code-mixing.</li><li>Knowing the source languages separately is sufficient for understanding their code-mix, but for generating it, additional knowledge is required.</li></ul></li></ul><h1>A speculative model</h1><p>Writing can be modeled as the iterative process of constructing a sequence of words one by one. When writing, there are two main sources contributing to your decision of which word to choose as the next one to write.</p><ul><li>Your accumulated intuitions about how words generally succeed one another. Sometimes, the writing process is almost automatic, with each word effortlessly flowing from the previous ones.</li><li>Your own pre-verbal ideas. Sometimes, finding the right sequence of words to convey a nuanced concept requires a deliberate search effort.</li></ul><p>This is pretty much the distinction between type 1 (fast, intuitive) and type 2 (slow, deliberate) thinking.</p><p>ChatGPT's base model was trained to predict the next token of text in a sequence. This is analogous to the type 1 writing method in humans I just outlined. Anecdotally, it seems like people who have read more during their lives are better at it. Likewise, ChatGPT has been trained on an immense quantity of text, and <a href=\"https://www.alignmentforum.org/posts/htrZrxduciZ5QaCjw/language-models-seem-to-be-much-better-than-humans-at-next\">is superhuman at next-token prediction</a>. Code-mixed text, however, is a rare occurrence, and as such there is insufficient data for either humans or language models to be able to generate it using only type 1 processes. Humans can get around this problem by using type 2 reasoning<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefice2m4q2j5\"><sup><a href=\"#fnice2m4q2j5\">[1]</a></sup></span>. Language models, however, are not capable of type 2 reasoning (or an analogous artificial process), and as such can't generate code-mixed text.</p><p>&nbsp;</p><h1>Appendix 1: additional basic examples of generative failure&nbsp;</h1><figure class=\"image image_resized\" style=\"width:85.13%\"><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1671568451/mirroredImages/TZPbm3BRkWWTm9ecC/mtkwqr3lu9aosnppk9at.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/jlpoksqq823tru844duz.png 90w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/gwam1ftjl43rwjafjmxq.png 170w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/cnkklmviael796gymadh.png 250w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/vaj03akheuzkjpd2rx2j.png 330w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/puydbvjfscbvg2e9x9lk.png 410w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/snantcmjtj9rsgqokytz.png 490w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/gdjn7g3ahbfc3u078bge.png 570w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/xqdgbw3qoo0s1gsynxgm.png 650w, https://res.cloudinary.com/cea/image/upload/v1672870230/mirroredImages/5f9Xtmy8Q559eppqJ/oemygdsnkpnmhoo7bx7y.png 730w\"><figcaption>Asked to translate a Spanish sentence into Spanglish, ChatGPT translates into full English.</figcaption></figure><figure class=\"image image_resized\" style=\"width:85.23%\"><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1671568451/mirroredImages/TZPbm3BRkWWTm9ecC/adndfik2hha9kol6yfje.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/max1a5higxtfwvai8i0k.png 138w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/rms2avuk1o3cepigikxa.png 218w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/vab6pvpmtjgfr35xyb36.png 298w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/pvlpsunab6kwjx456fc1.png 378w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/p0r0l6fy64kkbhe3qyod.png 458w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/s9ys6akglxa0gjhmfswr.png 538w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/cdd4btxyayn4zebkz9qe.png 618w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/fjbskb0kwxqn4ubb5o27.png 698w, https://res.cloudinary.com/cea/image/upload/v1672870230/mirroredImages/5f9Xtmy8Q559eppqJ/cqlpnu7c6ddnc1ibwhrp.png 778w\"><figcaption>Asked to translate a French sentence into Franglais, ChatGPT translates into full English.</figcaption></figure><h1>Appendix 2: Trying really hard to get ChatGPT to produce Spanglish and (almost) entirely failing</h1><p>I hazard the guess that Spanglish is the most represented code-mixed language in ChatGPT's training corpus, so I decided to try to focus my efforts here when trying to get Chat-GPT to generate a code-mixed output. All of these examples were zero-shot. That is, they were the first message in a new chat instance. There were many more attempts, here I show only the most informative ones.</p><figure class=\"image image_resized\" style=\"width:85.13%\"><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1671568451/mirroredImages/TZPbm3BRkWWTm9ecC/hxjcp1w7nfukn4nps1ld.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/j0s4mg44fvftl4ocscvs.png 89w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/uxwh2i1sxnmirzs2jnyq.png 169w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/bmh9dhiblpsaotqa31tu.png 249w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/njqdewtolrptwanpmoid.png 329w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/kfsizfcrokllt9bppana.png 409w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/u0e3dd7erx8xtoezqcrt.png 489w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/kietk2uc86oxdszdfung.png 569w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/uooigvquqvaynd2d9x8i.png 649w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/qqrg337oic90izdvglwa.png 729w\"><figcaption>Explicitly asking for only half of the words in a text in Spanish to be translated into English results in a full translation.</figcaption></figure><figure class=\"image image_resized\" style=\"width:85.33%\"><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1671568451/mirroredImages/TZPbm3BRkWWTm9ecC/ilfsky4zy1wit1xvfgnp.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/ik86zgsu7tmyvxnnvqv4.png 95w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/exmf6emc4rtclcnhzrfz.png 175w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/f1bz9wpji4hzgvkottak.png 255w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/zioyaqlwxde7oz8bjgwm.png 335w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/ndlpaea2yngdro4wlgxp.png 415w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/ivkgmmwor3psjc1qfvuq.png 495w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/trwysqd63xwtoxw9s2mp.png 575w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/h5pvxuhshhnst3nxfbj5.png 655w, https://res.cloudinary.com/cea/image/upload/v1672870230/mirroredImages/5f9Xtmy8Q559eppqJ/bfihqan99ixscddlyjmu.png 735w\"><figcaption>The same thing happens when the languages are reversed.</figcaption></figure><figure class=\"image image_resized\" style=\"width:85.42%\"><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1671568451/mirroredImages/TZPbm3BRkWWTm9ecC/xjpd4wetnnldcg2sb0lb.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/twpvz9cwtehqbusbvomu.png 111w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/sovvwhxlxt3jhrp2auwl.png 191w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/ur8vltiktqrxoqeh1pni.png 271w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/h8vsljgtatmbc4e2u3hc.png 351w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/ivv40bkpdmfeqdvxaqvy.png 431w, https://res.cloudinary.com/cea/image/upload/v1672870230/mirroredImages/5f9Xtmy8Q559eppqJ/uubcemky2ouvcqc5hmfb.png 511w\"><figcaption>Inventing a situation where a fictional speaker is forced to speak Spanglish due to poor proficiency largely does not work. However, ChatGPT does sometimes manage to mix Spanish and English, albeit in a way that doesn't qualify as Spanglish. This particular example demonstrates one of the two types of mixture I have observed: [placeholder brackets] in English amid Spanish text, such as one might observe in a travel phrasebook.</figcaption></figure><figure class=\"image image_resized\" style=\"width:85.16%\"><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1671568451/mirroredImages/TZPbm3BRkWWTm9ecC/u5ki8sgzgbgto1itfzwx.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/lynfzb2ymri07ekdlxws.png 107w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/laiheqw4vma4fa5kcpc2.png 187w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/j8wgx7mhdjmgk65qgiav.png 267w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/lyfharsupetxhwylvuag.png 347w, https://res.cloudinary.com/cea/image/upload/v1672870231/mirroredImages/5f9Xtmy8Q559eppqJ/vfjfuweykx043y37zqdt.png 427w, https://res.cloudinary.com/cea/image/upload/v1672870230/mirroredImages/5f9Xtmy8Q559eppqJ/b9a9bmxwyefa13wwiqaf.png 507w\"><figcaption>This is the other mixture type I found: Repeating the same message first in Spanish and then in English.</figcaption></figure><h1>Notes</h1><ul><li>This work was done on ChatGPT December 15 version. Results may differ on future versions.</li><li>While writing this post, I had a serendipitous online encounter with an OpenAI employee<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefytep65q1syb\"><sup><a href=\"#fnytep65q1syb\">[2]</a></sup></span>&nbsp;and asked them whether they knew about the phenomenon described in this post. They did not, though it didn't surprise them, given known previous issues around difficulties getting ChatGPT to produce non-English outputs. Someone else at OpenAI might have identified it, though.</li><li>Transcriptions of the prompts used are available <a href=\"https://docs.google.com/document/d/1l5CntPI88iuecx-KqJrQSEufqaEDm1djplKBPvhwr1Y/edit?usp=sharing\">here</a>.</li></ul><p>&nbsp;</p><p><i>Thanks to Agust\u00edn Covarrubias for feedback on an earlier version of this post, and to the anonymous OpenAI employee for an informal discussion on the matter.</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnice2m4q2j5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefice2m4q2j5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>And writing code-mixed text does feel to me as slow and deliberate as an almost entirely type 2 activity.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnytep65q1syb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefytep65q1syb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>They asked me to not identify them personally in this post.</p></div></li></ol>", "user": {"username": "Milan Weibel"}}, {"_id": "aKNaCdiT6N4fhmLqz", "title": "The value of a statistical life", "postedAt": "2023-01-04T10:58:59.496Z", "htmlBody": "<p>The economic value of a life, or value of a statistical life (VSL), is a dollar amount that an individual or society is willing to pay for the purpose of avoiding the mortality of one individual. A controversial and contentious subject, intuitively many are hesitant to give credence to the belief a life may be valued economically in any meaningful way. Philosophical and economical criticisms exist both against the whole project of valuing a life (are all aspects of a human life comparable, and reducible to a dollar value?), as well as within the project of valuing a life (is assessing risk taking behaviours the best approach, or are survey studies?). It is nonetheless a necessary concept for policy in the areas of health, insurance and transport, amongst others. It allows for the practical allocation of resources, and investment in cost-effective technologies. It also provides an interesting example of a measure of economic inequities that exist in our world. If we hold morally that all human life has equal value, what are we to think of the tension that exists between this moral position and the differing VSL placed on lives across the globe? What factors are driving this gap, and are they a fruitful area of focus in attempts to improve the wellbeing of the global poor?</p><p><i>Methods of valuing life</i></p><p>In economies, the value of a good or service is in large part determined by the market, supply and demand. There is no such market for human life, and thus alternate methods must be used, which provide highly variable results.&nbsp;</p><p>A measure on the value of a human life is an important factor in policy decisions and research. However, a supply-demand economy does not exist for human life, and thus we need to take a more theoretical approach.&nbsp; I will outline a few key methods here, adapted from this&nbsp;<a href=\"https://socialvalueuk.org/wp-content/uploads/2017/08/Valuation-of-a-life-1.pdf\"><u>high yield summary</u></a> from Social Value UK. There is no one accepted or unifying method to determining a VSL, but a multitude each with their own advantages and disadvantages.&nbsp;</p><p><i>A crude method - Human Capital</i></p><p>Is a person simply worth what they earn? This is the basis of the human capital approach to measuring VSL. The potential earnings of the individual are approximated based on age, life expectancy and potential future increases in earnings. It is thus sensitive to key demographic factors, and is easy to calculate and compare amongst groups. There are however several limitations to this method. It focuses only on the earnings of the individual, failing to account for life\u2019s intangibles, the value an individual may experience from undertaking leisure activities, personal relationships and general wellbeing. It is also sensitive to income inequalities, valuing the VSL of certain high-earning groups higher than others.&nbsp;</p><p><i>A personal perspective - Contingent Valuation</i></p><p>Individuals are asked what dollar value they would be willing to pay for various levels of improvement to their risk of mortality, or how much they would need to be paid to accept an increased likelihood of death. From this, inference is made to the value placed on one's own life. This approach goes beyond the \u2018low resolution\u2019 Human Capital method to capture the intangible aspects of life, and is thus a more holistic estimate of the VSL. It is however liable to individuals placing unrealistically high values on their own lives, a so-called \u2018protest-bid\u2019, which can skew results. Additionally, difficulties with scope insensitivity, the ability to differentiate levels of risk in a meaningful and consistent manner, can result in unreliable individual responses.&nbsp;</p><p><i>Revealed preferences - Consumer Preference Method</i></p><p>Individual choices in a market economy can reveal the monetary value individuals are willing to place on changes to their own safety. If an individual is willing to pay a higher price for a safer product alternative and related reduction in mortality, we can infer the VSL. For instance, what price differential would an individual be willing to pay for a car with, or without, airbags? The main advantage of this method in comparison to the Contingent Valuation method is it relies on&nbsp;<a href=\"https://en.wikipedia.org/wiki/Revealed_preference\"><u>revealed rather than stated preferences</u></a>, thus avoiding issues such as protest-bids. One's preferences in the market are not solely driven by the difference in risk associated with different product or service choices however. It is therefore important to consider how much safety factors into the individuals purchasing decision.&nbsp;</p><p><i>Differing values for differing populations</i></p><p>Depending on the method and population group being studied, there may be wide variation in the VSL calculated. In all three of the methodological approaches above, a relatively high-income earner in a high-income country will likely have a higher VSL than an individual struggling to find employment in a low-income country.&nbsp;From a purely economic perspective, we can see how the lower valuation of a particular life may lead one to make riskier decisions surrounding health and wellbeing, or give less incentive for governments or private entities to invest in the wellbeing and longevity of the lower-income individual. An excellent example of this disparity comes from&nbsp;<a href=\"https://bse.eu/sites/default/files/working_paper_pdfs/716_0.pdf\"><u>a paper by Le\u00f3n and Miguel</u></a>, which looks at the revealed preference VSL between African and non-African travellers making decisions on the transport mode used to travel to the international airport in Freetown, Sierra Leone. travellers had to decide between 4 options, each varying in risk, convenience and cost, namely ferry, hovercraft, water taxi and helicopter. After surveying some 561 travellers on their revealed travel preferences (i.e. the choices they made in the real world), the VSL calculated for African travellers was US$577 000, compared with US$924 000 for non-Africans, an almost doubling of the dollar amount. Further, the sample of African travellers surveyed as part of the study had an average income almost 50-times that of the average African citizen, meaning we should consider this an extremely conservative estimate of the true difference in the VSL between the typical African citizen compared to non-African travellers. The authors show statistically that this difference is driven in large part by differences in income, and the more fatalistic beliefs (the belief that things in the world are determined by fate, rather than believing one has more agency over their future) held by African survey respondents.&nbsp;</p><p>What implications does this idea have for our society as a whole? It seems somewhat obvious that those living in low or middle-income countries have significantly less purchasing power in the world economy compared to their high-income neighbours. This revealed valuation provides evidence that individuals may place a lower economic value on&nbsp;<i>their own</i> lives. But is this really the correct way to think about such valuations? Should we perhaps instead adopt a relative measure, something like an individual's revealed preference VSL divided by the local GDP? If we take this approach, using&nbsp;<a href=\"https://www.imf.org/external/datamapper/NGDPDPC@WEO/OEMDC/ADVEC/WEOWORLD\"><u>IMF data</u></a> and GDP per capita for the African continent for African travellers, and worldwide GDP per capita for all other travellers (no equivalent available data for non-African GDP per capita), we end up with the following calculations:</p><p>For African travellers: $577 000/$2 194 = 262.99</p><p>For non-African travellers: $924 000/$13 396 = 68.98</p><p>This shows African travellers value their life at a level approximately 263 times that of their annual average GDP, compared with non-African travellers who value it some 69 times as much. We do however need to be aware of the selection bias present in the original sample, that the African travellers sampled had incomes that differed significantly from the average found in the population. The quoted rate of the African traveller included in the study was approximately 50 times that of the average GDP per capita. If we apply this 50 times multiplier to our calculation, we get a very different result</p><p>For African travellers: ($577 000/50)/$2 194 = 5.26</p><p>For non-African travellers: $924 000/$13 396 = 68.98</p><p>Not only is the GDP and revealed preference VSL lower for African travellers, the VSL to GDP ratio is also lower. The selection-bias adjusted result suggests the non-African travellers have a VSL/GDP ratio some 13 times higher than African travellers.</p><p>The adjusted result suggests the non-African traveller economically values their lives some 13 times higher than African travellers, when corrected for GDP and adjusted for the selection bias present in the original study. Are fatalistic attitude doing the work here to explain the difference beyond the calculated values for GDP and VSL? They perhaps may be contributing somewhat. It may also be argued that the reason for the large difference is that these results reveal an internalisation of the very real differences in what a society is willing to spend to save any one individual's life. For instance, whilst there may be a certain VSL figure for an individual in a high-income country, it is also true that for instance in Australia, if one were to require an expensive, potentially life-saving but likely futile medical intervention, the economic cost would hold very little weight at the individual level. The main factors being weighed would be the wellbeing of the person and potential for quality-life gained, against the potential pain, harm, or loss of dignity that may result from such an intervention. In such a case, for the individual in this circumstance, the upper bound to the allocation of resources to extend their life is set by what science is able to do to extend or improve a life, rather than economic factors. The same cannot be said in low-income societies. Whilst life-saving interventions that are low-cost, highly-effective and available are theoretically obtainable, an individual's economic situation or the availability of healthcare services (such as diagnostics, drugs or interventions) provide an upper bound to what can be offered to these individuals. One may point to available universal health care as the factor doing the work in this scenario. If we consider emergency situations however, it is true that a life-saving intervention would be administered to an individual in most high-income societies regardless of insurance or relative financial status. It is possible that individuals living within low-income societies internalise this unspoken valuation that is placed on the willingness to spend to save their lives, and thereby make choices in the real world reflective of the fact that the value they place on their own life is less. This makes for a vicious circle of risky decision making, and makes it all the more difficult to change individual behaviours to look forward to the long-term.<br>&nbsp;</p><p><i>Scale, tractability and neglectedness</i></p><p>Given this information, does it change how we should approach the gap that exist between the equal moral values of all human lives and the VSL across different parts of our global society? The scale of the issue is certainly large, potentially accounting for some fraction of total income inequality across the globe. There may be a significant aspect of neglectedness of focus on both fatalistic attitudes, and the impact of the self-valuation of a life on life quality and quantity. Certainly, it would be of great value to have a better idea of how these factors impact the lives of those living in low income communities around the globe. How tractable the use of this information would be in efforts to combat economic inequalities across the globe and improve the lives of those in poverty is questionable. Changing the perceptions and resultant behaviours of large numbers of people is notoriously difficult, so it is unclear how much impact could be gained from further insights into these specific causal mechanisms would impact the issue at scale.</p><p><i>Conclusion&nbsp;</i></p><p>A lower price on the VSL for individuals in developing nations may go some way to explaining the lack of investment in technologies to improve the lives of those living in these nations. Since individuals are implicitly willing to spend less in relative and absolute terms to preserve their own lives, there is less incentive to develop innovations that will do so. In order to work toward improving the health and wellbeing of those living in low-income societies, there may be value in further research into the utility of relative and absolute VSL estimates as a marker of progress, and tool for gaining a more nuanced insight into the factors underlying economic inequalities.&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "noodlenoodle"}}, {"_id": "HJwNGxNmFiYcJyAk3", "title": "Bill Burr on Boiling Lobsters (also manliness and AW)", "postedAt": "2023-01-04T07:55:36.242Z", "htmlBody": "<p>I think there's a vibe out there in many cultures (including American) that being vegetarian/vegan or certain kinds of sympathy towards animals is unmanly and just kind of lame. This is probably more true in the right-wing demographic. I'm guessing this has been discussed in the animal welfare movement somewhere, so I won't attempt to delve into the issue further in this post.<strong>*</strong> Instead, I merely want to favorably acknowledge some commentary by comedian Bill Burr about boiling lobsters alive.&nbsp;</p><p>Bill Burr is a super-famous comedian and one of the most prominent cultural icons of masculinity in the U.S (perhaps in some respects the most prominent). Although I would say he is a party-neutral comedian, his comedic themes have included anti-wokeness and challenges to certain aspects of feminism, and probably has a huge following among working-class right-wing men.&nbsp;</p><p>Here is his <a href=\"https://www.youtube.com/watch?v=lXBk3FVPTVQ&amp;ab_channel=BillyTwinkleToes\">commentary</a> on boiling lobsters alive (6 min), excerpted from his podcast.&nbsp;</p><p>&nbsp;</p><p><strong>*Edit</strong>: Actually I will go into it for a minute. If anyone wants to see a great example of how to deal with this sort of thing in a different context, take a <a href=\"https://youtu.be/2qfjaFtsjUM?t=8\">look at how Ford dealt</a> with the issue of environmentalism/green-politics being considered soft, lefty, snowflake stuff when they wanted to advertise the fuel economy on their F150 pickup truck: \"you won't be put in a chokehold everytime you fill up\".&nbsp;</p><p>Other versions of the commercial <a href=\" https://www.youtube.com/watch?v=xSTLG5jIg-A&amp;ab_channel=FoxValleyFord\">here</a> and <a href=\"https://www.youtube.com/watch?v=qzU2bhhreaY&amp;ab_channel=JerryDuncanFord\">here</a>.&nbsp;</p>", "user": {"username": "Lixiang"}}, {"_id": "iuBoizzA5c5KfWysc", "title": "Announcing Insights for Impact", "postedAt": "2023-01-04T07:00:25.588Z", "htmlBody": "<p>Hey all!</p><p>Are you wanting to follow EA research, but finding papers and longform forum posts too dry?</p><p>Late last year, Jenna Ong and I noticed a lack of research-focused EA video content and decided to do something about it. Today, we are excited to introduce <a href=\"https://www.youtube.com/@insightsforimpact\">Insights for Impact</a>, a YouTube channel that\u2019s all about communicating the key insights of EA-aligned research papers.</p><p>In our first video, <a href=\"https://www.youtube.com/watch?v=yZXCfGqHsiY\">How Science Misunderstands Power</a>, we explore why well-meaning scientists failed to prevent nuclear proliferation in the 20th century. Perhaps by examining the history of nuclear weapon development, we may be able to better manage other powerful technologies, like AI and genetic engineering.</p><p>A 2018 paper by <a href=\"https://samoburja.com/\">Samo Burja</a> and Zachary Lerangis, <a href=\"https://www.bismarckanalysis.com/Nuclear_Weapons_Development_Case_Study.pdf\">The Scientists, the Statesman, and the Bomb</a>, served as the basis for this video. However, we also drew inspiration from <a href=\"https://forum.effectivealtruism.org/users/haydnbelfield\">HaydnBelfield</a>\u2019s <a href=\"https://forum.effectivealtruism.org/posts/cXBznkfoPJAjacFoT/are-you-really-in-a-race-the-cautionary-tales-of-szilard-and\">post</a>, especially their idea that the current headspace of the AI Safety community closely resembles the \u201cthis is the most important thing\u201d mindset of scientists throughout the mid 20th century. From these case studies, it seems that both social and technical factors are crucial in ensuring powerful technologies have a positive impact.</p><p>In future videos, we want to explore a range of EA-relevant cause areas. We\u2019d love to collaborate with researchers to ensure we accurately portray their work. So if you\u2019re a researcher who wants to give your work a voice outside of the forum, please <a href=\"mailto:insightsimpactcontact@gmail.com\">get in touch</a>!</p>", "user": {"username": "Christian Pearson"}}, {"_id": "iv3NmPjozonLgvT66", "title": "Do people have a form or resources for capturing indirect interpersonal impacts?", "postedAt": "2023-01-04T04:47:01.498Z", "htmlBody": "<p>In my experience, many EAs help and support others in the community (e.g., by giving feedback, emotional support, or making connections etc).</p><p>These 'helpful EAs' often improve the impact of those who receive their help (e.g., because the receivers start new collaborations, or improve their productivity or career choice etc). I'll call this impact 'indirect interpersonal impact'.&nbsp;</p><p>Most helpful EA's indirect interpersonal impacts are illegible (i.e., hard to capture/show). This means that many EAs who have high indirect interpersonal impact (e.g., via helping many others or being a good knowledge broker/connector etc) are undervalued relative to those who mostly focus on doing their own projects(but who may benefit from the help of many others).</p><p>I think that this is probably important to address. It seems important to acknowledge and recognize the contributions of individuals who may not necessarily have a tangible output or project to show for their efforts, but may still have had a significant positive impact on others.</p><p>With the above in mind, I am wondering if anyone has a form to capture indirect interpersonal impacts or similar, or some resources that they use or recommend using?</p><p>I am not aware of anything which exists. I would like to either adapt or make something to use myself and share with others. I think that 80,000's evaluation model is probably the best template to work from, but I haven't investigated that yet.</p><p>I'd also welcome any thoughts on the claims made above and whether they resonate or seem incorrect.</p>", "user": {"username": "Peterslattery"}}, {"_id": "daLssjprpqfAsRWW8", "title": "Normalcy bias and Base rate neglect: Bias in Evaluating AGI X-Risks", "postedAt": "2023-01-04T03:16:35.876Z", "htmlBody": "<p>&nbsp;<br><strong>Normalcy bias</strong></p><p>The refusal to plan for, or react to, a disaster which has never happened before.&nbsp;</p><p>A society subject to regular minor hazards treats those minor hazards as an upper bound on the size of the risks.</p><p>The wise would extrapolate from a memory of small hazards to the possibility of large hazards. Instead, past experience of small hazards seems to set a perceived upper bound on risk. A society well-protected against minor hazards takes no action against major risks.</p><p>For example; building on flood plains once the regular minor floods are eliminated. They are guarding against regular minor floods but not occasional major floods.</p><p><br>&nbsp; - link Wikipedia: &nbsp;<a href=\"https://en.wikipedia.org/wiki/Normalcy_bias\">Normalcy bias</a><br>&nbsp; - an item on <a href=\"https://www.lesswrong.com/posts/nEdueRhZwB4eP6X3c/introduction-bias-in-evaluating-agi-x-risks\">Forrest Landry</a>'s compiled list of biases in evaluating extinction risks.</p><hr><p><strong>Base rate fallacy</strong><br><strong>Base rate neglect</strong></p><p>The tendency to ignore base rate information (generic, general information) and focus on specific information (information only pertaining to a certain case).</p><p>In nearly every scenario associated with a <a href=\"https://mflb.com/lsag_1/risk_categories_psr.html\">category 1 or 2</a> extinction risk, there is a power law effect \u2013 some sort of catalytic reaction or cascade. No amount of rejection of specific cases of exotic process will provide sufficient basis for a general argument of induction that no such specific case exists.</p><p>That is why general arguments are preferred, as they address a general issue in a general (though comprehensive) way.</p><p>This particular error has a lot in common with the 'Neglect of probability' bias.<br>It is a symptom of the fact that the vast majority of people naturally think additively.<br>A very much smaller number of people can think in terms of multiplicative effects, and a very much rarer subset of those folks can think in terms of power laws. For example, have you ever tried to convince someone who is young of the benefits of investing for retirement?</p><p>Given that thinking in terms of power laws is unnatural, difficult, and usually requiring of explicit abstract mathematical technique, there is a strong tendency for most people to focus on concrete details in an attempt to re-establish a basis on which their intuitions, in the form of an 'induction of understanding', can occur.</p><p>Dealing with specifics is therefore considered to be easier and \"more productive\" than dealing with general and abstract issues in a fully general way.</p><p><br>&nbsp; - link Wikipedia: &nbsp;<a href=\"https://en.wikipedia.org/wiki/Base_rate_fallacy\">Base rate fallacy</a><br>&nbsp; - an item on <a href=\"https://www.lesswrong.com/posts/nEdueRhZwB4eP6X3c/introduction-bias-in-evaluating-agi-x-risks\">Forrest Landry</a>'s compiled list of biases in evaluating extinction risks.</p>", "user": {"username": "remmelt"}}, {"_id": "hQGfPTNpeiDjtHTC7", "title": "\"AI\" is an indexical", "postedAt": "2023-01-03T22:00:31.447Z", "htmlBody": "", "user": {"username": "ThomasWoodside"}}, {"_id": "Jt9ZJCS6nzLzod5TP", "title": "An approach for getting better at practicing any skill", "postedAt": "2023-01-03T17:47:15.764Z", "htmlBody": "", "user": {"username": "jaythibs"}}, {"_id": "MYxerzJrCzHErKWX6", "title": "Holden Karnofsky Interview about Most Important Century & Transformative AI", "postedAt": "2023-01-03T17:31:59.533Z", "htmlBody": "<p>Holden Karnofsky is the co-CEO of<a href=\"http://www.openphilanthropy.org/\"><u> Open Philanthropy</u></a> and co-founder of<a href=\"http://www.givewell.org/\"><u> GiveWell</u></a>. He is also the author of the always interesting <a href=\"https://www.cold-takes.com/\"><u>Cold Takes</u></a> blog.</p><p>This was a very interesting &amp; wide-ranging discussion about Holden's thesis that transformative AI makes this the most important century. We also discuss whether he regrets OpenPhil\u2019s 30 million dollar grant to OpenAI in 2016.</p><p>Highly recommend!</p><p>Watch on <a href=\"https://youtu.be/UckqpcOu5SY\"><u>YouTube</u></a>. Listen on <a href=\"https://podcasts.apple.com/us/podcast/the-lunar-society/id1516093381\"><u>Apple Podcasts</u></a>, <a href=\"https://spoti.fi/3Qi11SY\"><u>Spotify</u></a>, or any other podcast platform. Read the full transcript <a href=\"https://www.dwarkeshpatel.com/p/holden-karnofsky\"><u>here</u></a>.</p><figure class=\"media\"><div data-oembed-url=\"https://youtu.be/UckqpcOu5SY\"><div><iframe src=\"https://www.youtube.com/embed/UckqpcOu5SY\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure>", "user": null}, {"_id": "S6s5m53Ks8jE82vTZ", "title": "EA Global: London 2023", "postedAt": "2023-01-03T15:32:28.107Z", "htmlBody": "<p>Applications are open! <strong>Deadline: 28 April 2023</strong></p><p>EA Global will bring together a wide network of people who have made helping others a core part of their lives. Speakers and attendees share new thinking and research in the field of effective altruism and coordinate on global&nbsp;projects.</p><h1>Application details</h1><ul><li>Application deadline: 28 April 2023 (8:00am&nbsp;UTC)</li><li>Default ticket price:&nbsp;\u00a3200</li><li>Discounts are available \u2014 you can view details in the application and registration form</li></ul><h1>Approximate schedule</h1><p>The event will begin with an opening reception on the Friday (but you can arrive and register at any time during the&nbsp;weekend).</p><p>Saturday and Sunday will have full-day schedules with a closing reception ending around 10pm on Sunday&nbsp;evening.</p><p>The agenda includes:</p><ul><li>Talks and discussions on the latest ideas at the frontiers of the EA&nbsp;movement</li><li>Workshops to help improve your thinking and&nbsp;execution</li><li>Meetups, networking, and social&nbsp;activities</li></ul><h1>Travel expenses</h1><p>This year we unfortunately only have a very limited amount of travel funding available (see <a href=\"https://forum.effectivealtruism.org/posts/4JF39v548SETuMewp/update-on-spending-for-cea-run-events\">here</a> for more details). You can apply for financial assistance for your travel expenses in the application form, but we\u2019d recommend you act under the assumption that we will not be able to grant your travel funding request. Please check our <a href=\"https://docs.google.com/document/d/1gOvK6TKnQS0LADHmYCExCbWK7xsBVmqHdCICvDxNmjo/edit\">travel support policy</a> for more&nbsp;details.</p><h1>Should you apply?</h1><p>If you want to attend but are unsure about whether to apply, please always err on the side of applying. You can read about admissions for EA Global <a href=\"https://docs.google.com/document/d/1vlOaF-f2585hmyi4Chs7vU0MMJ9xcz1jigzsb2-UCP8/edit?usp=sharing\">here</a>.</p>", "user": {"username": "Eli_Nathan"}}, {"_id": "z56Gs6xyFJRSBENxG", "title": "EA Global: Bay Area 2023", "postedAt": "2023-01-03T15:26:36.403Z", "htmlBody": "<p>Applications are open! <strong>Deadline: 8 February 2023</strong></p><p>EA Global will bring together a wide network of people who have made helping others a core part of their lives. Speakers and attendees share new thinking and research in the field of effective altruism and coordinate on global&nbsp;projects.</p><h1>Application details</h1><ul><li>Application deadline: 8 February 2023 (8:00am&nbsp;UTC)</li><li>Default ticket price:&nbsp;\u00a3200</li><li>Discounts are available \u2014 you can view details in the application and registration form</li></ul><h1>Approximate schedule</h1><p>The event will begin with an opening reception on the Friday (but you can arrive and register at any time during the&nbsp;weekend).</p><p>Saturday and Sunday will have full-day schedules with a closing reception ending around 10pm on Sunday&nbsp;evening.</p><p>The agenda includes:</p><ul><li>Talks and discussions on the latest ideas at the frontiers of the EA&nbsp;movement</li><li>Workshops to help improve your thinking and&nbsp;execution</li><li>Meetups, networking, and social&nbsp;activities</li></ul><h1>Travel expenses</h1><p>This year we unfortunately only have a very limited amount of travel funding available (see <a href=\"https://forum.effectivealtruism.org/posts/4JF39v548SETuMewp/update-on-spending-for-cea-run-events\">here</a> for more details). You can apply for financial assistance for your travel expenses in the application form, but we\u2019d recommend you act under the assumption that we will not be able to grant your travel funding request. Please check our <a href=\"https://docs.google.com/document/d/1gOvK6TKnQS0LADHmYCExCbWK7xsBVmqHdCICvDxNmjo/edit\">travel support policy</a> for more&nbsp;details.</p><h1>Should you apply?</h1><p>If you want to attend but are unsure about whether to apply, please always err on the side of applying. You can read about admissions for EA Global <a href=\"https://docs.google.com/document/d/1vlOaF-f2585hmyi4Chs7vU0MMJ9xcz1jigzsb2-UCP8/edit?usp=sharing\">here</a>.</p>", "user": {"username": "Eli_Nathan"}}, {"_id": "h5cZKfQkbdbdKvHHn", "title": "Safety Sells: For-profit investing into civilizational resilience (food security, biosecurity)", "postedAt": "2023-01-03T12:24:29.269Z", "htmlBody": "<p><strong>Summary</strong>: There is a growing trend among governments and investors to invest in not just civilizational progress, to increase wealth and wellbeing, but also in civilizational resilience, to stabilize them against economic or environmental shocks. This trend is evident in the areas of food security and biosecurity. Investors and founders, particularly in the emerging bioeconomy, can support and benefit from this trend by incorporating civilizational resilience into their ESG and impact frameworks and implementing company-specific risk mitigation strategies. These strategies may be necessary to ensure that new products or services contribute to a more stable world, particularly in the area of biosecurity.<br>&nbsp;</p><p><i>This post outlines possible avenues for for-profit capital to contribute to building a more resilient civilization, specifically by improving food security and biosecurity. It is motivated by the large sums of money flowing through Venture Capital (VC, capital that is invested for profit into start-up companies) -&nbsp;</i><a href=\"https://home.kpmg/xx/en/home/media/press-releases/2022/01/global-venture-capital-annual-investment-shatters-records-following-another-healthy-quarter.html#:~:text=2021%20Annual%20Highlights-,Global%20VC%20investment%20rose%20from%20%24347%20billion%20across%2031%2C623%20deals,all%20regions%20of%20the%20world.\"><i><u>hundreds of billions of dollars per year</u></i></a><i>, comparable to the total federal budget of a state like&nbsp;</i><a href=\"https://www.bundesregierung.de/breg-en/news/cabinet-federal-budget-2022-2016888#:~:text=The%202022%20federal%20budget%20provides,estimated%20in%20the%20first%20draft.\"><i><u>Germany</u></i></a><i>. These sums are moved by people who think in concepts and frameworks and follow trends they see. Therefore, this post aims to test a preliminary framework for investing into civilizational resilience, based on a couple of conversations, to (a) get feedback on the framework and (b) help investors who care about building a long-term stable civilization to align on a common language. So, if you have suggestions for how to make this framework more useful, and/or just want to connect with like minded investors and founders, please comment and/or DM me.</i></p><p><i>As a disclaimer for the broader EA Forum audience: This post doesn\u2019t distinguish very carefully between \u201cbroadly good\u201d interventions that stabilize society against non-existential threats vs. more targeted interventions that could potentially help prevent civilizational collapse or extinction. From the perspective of a long-termist EA, one may want to focus much more on the latter group. From the perspective of a Venture Capital investor who needs to diversify their portfolio, it may make sense to look with a slightly wider angle, and this is what the post aims to do.</i></p><p><i>Finally, I work at a Venture Capital firm, but the post is written by me in my personal capacity. I\u2019d like to thank (in alphabetical order) Alexis Carlier, Charlie Petty, Damien Soghoian, Gigi Gronvall, Gregory Lewis, Jacob Swett, James Wagstaff, John Cumbers, Jonas Sandbrink, Jonathan Harris, Miroslav Gasparek, Sarah Carter, and Simon Grimm for helpful input and discussions; all mistakes are my own.</i><br>&nbsp;</p><p>&nbsp;</p><p><strong>Civilizational resilience is a growing trend across governments and for-profit investors</strong></p><p>On the scale of a global civilization, humanity has reached a stage of unprecedented wealth and wellbeing - at least as measured by indicators like rising&nbsp;<a href=\"https://ourworldindata.org/grapher/gdp-per-capita-maddison-2020\"><u>global GDP per capita</u></a>,&nbsp;<a href=\"https://ourworldindata.org/literacy\"><u>literacy</u></a> and&nbsp;<a href=\"https://ourworldindata.org/life-expectancy\"><u>life expectancy</u></a>. However, our wealth and wellbeing are still fragile, as illustrated by the immense damage that events like the COVID pandemic, the war in Ukraine, and extreme weather events are doing to global supply chains.&nbsp;</p><p>Governments and for-profit investors are increasingly recognizing the need to build not just for increased wealth and wellbeing, but to limit the fragility of humanity\u2019s achievements - investing in civilizational resilience. The trend to invest in civilizational resilience is particularly clear for food security, a key priority for governments, for example in&nbsp;<a href=\"https://www.foodnavigator-asia.com/Article/2022/05/04/singapore-s-challenging-2030-food-security-goals-underlined-by-new-production-data\"><u>Singapore</u></a> and Middle Eastern projects like&nbsp;<a href=\"https://www.neom.com/en-us/ourbusiness/sectors/food\"><u>NEOM</u></a>. Food security is also recognized as an ESG investment theme by for-profit investors, for instance in the&nbsp;<a href=\"https://www.prnewswire.com/news-releases/food-nutrition-and-health-investor-coalition-launches-to-invest-2-5-billion-in-startups-improving-hunger-and-health-outcomes-through-food-301634900.html\"><u>Food, Nutrition and Health Investor Coalition</u></a>. Especially since the COVID pandemic, biosecurity has been recognized as another key priority in building a more resilient civilization, for instance by the US government. The&nbsp;<a href=\"https://www.whitehouse.gov/ostp/news-updates/2022/10/18/the-2022-national-biodefense-strategy-builds-upon-administration-st-priorities-for-pandemic-preparedness/\"><u>US National Biodefense Strategy</u></a> outlines audacious targets like developing and deploying pathogen-specific tests within 30 days of detecting a novel pathogen, producing rapid, low-cost, point-of-need diagnostics within 90 days and repurposing effective therapeutics within 90 days. Accordingly, the&nbsp;<a href=\"https://www.whitehouse.gov/briefing-room/statements-releases/2022/03/28/fact-sheet-the-biden-administrations-historic-investment-in-pandemic-preparedness-and-biodefense-in-the-fy-2023-presidents-budget/\"><u>FY23 US President\u2019s Budget</u></a> includes a historic $88.2 billion request for mandatory funding, available over five years to prepare for future biological threats in support of objectives within U.S. national and global biodefense and pandemic preparedness strategies and plans. And, not unlike the trends in food security, for-profit investors are stepping into the space as well, for instance&nbsp;<a href=\"https://ghicfunds.org/global-health-security/\"><u>GHIC</u></a> in a partnership with BARDA,<a href=\"https://osfund.co/\">&nbsp;<u>OS Fund</u></a>,<a href=\"https://adjuvantcapital.com/about/\"> and&nbsp;<u>Adjuvant Capital</u></a>.&nbsp;</p><p>Although initial steps from governments and investors may indicate the beginning of a trend, the space of building civilizational resilience currently remains drastically underinvested compared to its potential to absorb capital and create value. For instance, the aforementioned Food, Nutrition and Health Investor Coalition aims to invest a total of $2.5bn over three years, while the&nbsp;<a href=\"https://feedplanetmagazine.com/blog/imf-$50-billion-needed-to-eradicate-acute-food-insecurity-2042\"><u>IMF</u></a> estimates it would have taken $50bn to eliminate food insecurity globally in 2022<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefaxkrfdwjrgn\"><sup><a href=\"#fnaxkrfdwjrgn\">[1]</a></sup></span>. Likewise, the COVID pandemic triggered an almost 10% decline in the UK\u2019s GDP, the<a href=\"https://commonslibrary.parliament.uk/research-briefings/cbp-8866/#:~:text=The%20magnitude%20of%20the%20recession,in%201921%20on%20unofficial%20estimates.\">&nbsp;<u>steepest drop in GDP</u></a> since consistent records began in 1948, indicating that preventing the next pandemic could easily be worth trillions of dollars globally.</p><p><strong>The bioeconomy offers solutions to increase civilizational resilience, and thus stands to benefit from the emerging trend. However, some technologies that are part of the bioeconomy pose risks of accidental and deliberate misuse, which need to be identified and mitigated.</strong></p><p>The&nbsp;<a href=\"https://www.ft.com/content/96cb2dd3-b4cd-4b12-bc72-ff19e40a3b16\"><u>bioeconomy</u></a> has emerged as a darling of many Venture Capital (VC) investors who turn to the next thing after software, led by thinkers like Eric Schmidt (ex-CEO of Google). Indeed, the bioeconomy offers a broad portfolio of solutions that can increase civilizational resilience:</p><ul><li>Fermentation-derived food ingredients improve plant-based products, which offer shorter, more resilient supply chains than animal-based foods (company examples include Impossible Foods and Motif Foodworks)</li><li>Fungi- or mycelium-based foods upgrade easily shipped and stored sugar into nutritious protein products, decoupling food production from land and weather needs to some extent (company examples include Meati and Evocative)</li><li>Gas fermentation can decouple human food production from any plant-based inputs as well as land and weather, producing protein directly from carbon dioxide or methane and nitrogen (company examples include Arkeon and SolarFoods)</li><li>Diagnostic testing based on the biotechnology tool CRISPR enables rapid, accurate test results to detect infectious diseases without disrupting daily life (company examples include Mammoth Biosciences, Sherlock, Twelve.bio, CASPR Biotech)</li><li>Rapidly scalable vaccine and/or biological drug manufacturing platforms ensure countermeasures for emerging diseases can be rolled out faster (company examples include Univercells, Lumen Biotechnologies)</li></ul><p>Next to these solutions directly addressing civilizational resilience, another group of companies is accelerating the bioeconomy as a whole by enabling and speeding up product development:</p><ul><li>Foundries offer biological and computational building blocks to engineer microbial strains that then produce bioproducts (company examples include Ginko Bioworks)</li><li>Data &amp; analytics companies support microbial strain engineering and fermentation process optimization (company examples include EV Biotech, Officinae bio)</li><li>Bioprospecting companies help others harness biodiversity to improve products (company examples include Basecamp Research, Prose Foods)</li></ul><p>While there is clear potential for building civilizational resilience in the bioeconomy, some technologies, especially in the \u201cenabler\u201d category, need to be carefully managed to avoid&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/dual-use\"><u>dual-use risks</u></a>: What can enable a bioproduct could, in the wrong hands or by accident, also enable the next pandemic. Such risks need to be quantified and factored into impact or ESG assessments where they turn out to be substantial. Oftentimes, they can even be considered as financial risks from negative PR following, for instance, a \u201cleak\u201d from a laboratory or the public misuse of a company\u2019s product or service. To illustrate the potential magnitude of such risks, one may consider the backlash around Genetically Modified Organisms (GMOs), which effectively excluded a whole industry from the European continent.</p><p><strong>Investors and founders have a broad toolbox to deploy in building civilizational resilience.</strong></p><p>The first step investors and founders can take is to clearly articulate the benefits of fostering civilizational resilience within existing ESG or impact investing frameworks. For instance, one could picture benefits from increased food security or biosecurity being included under the \u201cS\u201d (social) aspect of ESG, or under the health dimension many funds use in their impact frameworks. Such explicit inclusion of civilizational resilience among \u201cinvestable themes\u201d then naturally leads to more funding for \u201cdefense-native\u201d technologies - those that increase civilizational resilience without risks from accidental or deliberate misuse. Examples could include diagnostic testing, novel sterilization technologies, or optimizing growth of non-engineered microorganisms for food production<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefchbvpv0l85l\"><sup><a href=\"#fnchbvpv0l85l\">[2]</a></sup></span>.</p><p>Where a business offers significant positive returns from a financial and/or impact perspective, but also bears some risks, investors and founders can create substantial value with a good risk mitigation strategy. Just like most businesses based on data need a cybersecurity strategy, many bio-based businesses will need a biosecurity strategy, tailored to their specific product or service. For instance, many risks from misuse can be mitigated to some extent by screening the outputs of algorithms or the inputs of the user for similarity with known pathogens. Where foundries, analytics, or bioprospecting companies work with one client for a longer project, it may even be sensible to vet the client\u2019s history of activities and intentions, just like business partners often conduct&nbsp;<a href=\"https://www.swift.com/your-needs/financial-crime-cyber-security/know-your-customer-kyc/kyc-process#:~:text=KYC%2C%20or%20%22Know%20Your%20Customer,those%20entities%20are%20acting%20legally.\"><u>KYC</u></a> (\u201cknow your customer\u201d) background checks on each other before entering a contract.</p><p>To ensure that goals of fostering civilizational resilience and mitigating potential risks remain core to a business over time, investors and founders may agree to set up specific governance structures. Those could include setting up the business as a&nbsp;<a href=\"https://en.wikipedia.org/wiki/Benefit_corporation\"><u>benefit corporation</u></a> and establishing a separate&nbsp;<a href=\"https://hbr.org/2021/01/how-to-set-up-an-ethics-advisory-board\"><u>Ethics Advisory Board</u></a>, potentially even with \u201cveto\u201d rights for certain topics. A relatively easy first step can be to establish clauses in the shareholder\u2019s agreement that is signed when an investor invests into the company, for instance to document a commitment to deploy at least 1% of invested capital towards developing and implementing a biosecurity strategy. To exchange learnings and ideas, scientists, founders and investors can also connect in groups like&nbsp;<a href=\"https://ibbis.bio/\"><u>IBBIS</u></a>, the International Biosecurity and Biosafety Initiative for Science.</p><p>Investors and founders in the early and mid-2020s have the privilege of being early in the history of the bioeconomy as a new industry. They can therefore shape how (and whether) this industry meaningfully contributes to and benefits from trends like civilizational resilience.</p><p><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnaxkrfdwjrgn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefaxkrfdwjrgn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Note the comparison isn't perfect here as the measures suggested by the IMF include, e.g., food distribution programs that would not be investable for Venture Capital. The point I'm aiming to illustrate is that although $2.5bn seems like a large number, it is a small drop in a large bucket compared to the problem of food insecurity.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnchbvpv0l85l\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefchbvpv0l85l\">^</a></strong></sup></span><div class=\"footnote-content\"><p>more defense-native technologies are listed for instance in Appendix A of the <a href=\"https://biodefensecommission.org/reports/the-apollo-program-for-biodefense-winning-the-race-against-biological-threats/\">Apollo Program</a> report by the Bipartisan Commission on Biodefense, and have been discussed <a href=\"https://forum.effectivealtruism.org/posts/DcKo3Hx8hzrZWjYp5/list-of-lists-of-concrete-biosecurity-project-ideas\">elsewhere</a> on this forum .</p></div></li></ol>", "user": {"username": "FGH"}}, {"_id": "s4f83vkkSDL6my44j", "title": "Foundation Entrepreneurship - How the first training program went", "postedAt": "2023-01-06T09:17:04.106Z", "htmlBody": "<h3><strong>TLDR:</strong>&nbsp;</h3><p>Charity Entrepreneurship (CE) has run our first foundation training program. This program looked less like traditional philanthropic advising, and more like our classic charity incubation program. Instead of recommending specific charities, we aimed to teach skills that would allow foundation leaders to work as informed, independent, full-time grantmakers, equipped with a suite of decision-making tools. Despite seeing many areas where we could improve the program, we consider the pilot to have been very successful. We are excited to run this program again and believe it could provide impact comparable to that of our charity incubation program.</p><h1><strong>Background</strong></h1><p>At the beginning of 2022 we&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/BrmGRtX9HjfGi6CaF/ce-is-launching-a-pilot-training-program-for-grantmaking\"><u>announced</u></a> that CE would be running a pilot training program for grantmaking foundations. The pilot program finished several months ago, and so we wanted to follow up on how it went and what we learned.&nbsp;</p><h3><strong>Why</strong></h3><ul><li>Starting an impactful foundation and being a strong grantmaker is hard. Few high-quality resources exist (at least in a publicly available form) that teach how to run an evidence-based and highly effective foundation.</li><li>Over the past four years, CE has helped launch 23 new nonprofits, several of which are on track to becoming field leaders in their respective areas (e.g., recommended by GiveWell). An unpredicted learning from this work is that our content, approach, and handbook have been useful to a number of foundations and grantmakers. They have improved their prioritization, decision-making, their clarity of mission and, ultimately, their impact (particularly in the early stages of setting up a foundation or granting department).&nbsp;</li><li>With this in mind, we incubated one grantmaking foundation in 2021, which proved successful. This, combined with the number of interested foundations, led us to pilot a program focused on grantmaking foundations in 2022.&nbsp;</li></ul><h3><strong>What</strong></h3><ul><li>The pilot program ran for four weeks (three remote and one in person in London) and required participants\u2019 full-time commitment. It was high intensity relative to other programs typically aimed at philanthropists.</li><li>During the remote weeks there were readings from a 300+ page handbook written for the purpose, ~3x daily video lectures, 2x daily projects and 1x daily group discussion.&nbsp;</li><li>During the in-person week, in addition to group discussions, we ran mock grantmaking and donor coordination exercises. We supported participants to work towards a concrete final deliverable that they could use to better their grantmaking going forward.&nbsp;</li><li>The content of the program was focused on (a) equipping the foundations with decision-making tools, e.g., cost-effectiveness analyses, (b) making key decisions regarding their scope, structure and strategy, and (c) sharing best practices on vetting processes in the contexts of hiring and grantmaking.</li></ul><h3><strong>Who</strong></h3><p>Five foundations (represented by six people) participated in the pilot. Once these foundations have launched and scaled, their combined annual donations are expected to be approximately $60M/yr. Participants had a wide range of grantmaking experience (from none to many years), foundation maturity (from yet-to-exist to fairly established), EA knowledge (from novice to expert) and expected annual dollars granted (from $100k to &gt;$10M).&nbsp;</p><h1><strong>How it went</strong></h1><h3><strong>What went well</strong></h3><p>The program was a pilot, and as a result it was quite rough around the edges. Despite this, we think the program was very successful! From our perspective, the aspects of the program that went particularly well were the group discussions, the mock grantmaking exercises and the final project at the end.&nbsp;</p><ul><li>Feedback has suggested that the group discussion sessions were extremely valuable. These sessions gave participants the chance to delve deep into the content and get to grips with the theory and practice of grantmaking and running a foundation. Participants were thoughtful and enthusiastic in their discourse, and appreciated the chance to share experience and ask questions. Such an environment was particularly conducive to meaningful discussion as, uniquely, the participants were among peers who were not trying to solicit funding, and the only objective was their own development and understanding.</li><li>The participants gained confidence in their decision-making skills through mock grantmaking exercises. One exercise focused just on systematically vetting grant decisions, with the group collectively deciding whether and how much to fund a set of grant proposals. Another involved thinking about counterfactuals and donor coordination as well;&nbsp; participants made decisions as individuals whilst keeping in mind what others were likely to do, and trying to avoid collectively over or under-funding certain projects. By practicing their theoretical knowledge, their understanding was solidified and they became more optimistic that being a good grantmaker is an attainable goal, mitigating the risk of under-confidence. It also gave them some foresight into the many potential challenges they may face, mitigating the risk of over-confidence.</li><li>Participants reported really appreciating how concrete and actionable the program was, with almost every activity tied directly back to the key decisions they need to make as leaders of grantmaking organizations.</li><li>Lastly, the final project had the participants articulate exactly how they would put what they learned into practice after the program . We believe this was a vital step towards turning theory into reality, and therefore into impact. Participants committed to various actions to move their foundation forward, whether that be a five-year plan, an assessment of past grants, or launching an open grant application round.&nbsp;</li></ul><p>We circulated an anonymous feedback form after the program to better understand how it went from the participants\u2019 perspectives:</p><ul><li>They were very positive, unanimously giving the highest score available to \u201cWould you recommend the program to other people in a similar situation to you?\u201d.&nbsp;</li><li>In addition to the aspects we highlighted above, participants cited the draft handbook as being particularly high value for them.&nbsp;</li><li>When asked whether the program changed their grantmaking trajectory, all participants said that it did. Specifically, participants mentioned:<ul><li>Becoming more systematic in their grant vetting</li><li>Looking more consciously for their comparative advantage as funders</li><li>Thinking through cause prioritization more broadly and systematically</li><li>Seeking additional advisors on specific topics</li><li>Finding ways to create leverage and move more money to effective organizations</li><li>Investing more effort in donor coordination (e.g., through funding circles)</li></ul></li></ul><h3><strong>What the participants said</strong></h3><ul><li>\u201cMany thanks to the CE team for putting this program together! You have truly been excellent mentors and taught me a lot. I've also been super inspired by your dedication and drive to maximize your impact\u201d</li><li>\u201cWhat a super valuable experience this has been! Thanks for all your hard work\u201d</li><li>\u201cDeep gratitude to the whole team who took a chance on me. I for one am definitely in a much better place now than before. I know I'll be a better steward of the org thanks to the work you did, I'm guessing a 30% improvement at least.\u201d</li></ul><h3><strong>What we learned</strong></h3><ul><li><strong><u>The program needs to be structured quite differently:</u></strong> We modeled the structure of the pilot program on our charity incubation program, which has been refined over four iterations. This was a mistake. Funders are generally time-poor and can\u2019t easily set aside all other commitments for an entire month. As such, a full-time program with several hours of content and many hours of project work per day is not realistic.&nbsp;</li><li><strong><u>Connecting foundations with one another is of high value:</u> </strong>We expected that the content of the program would be where the bulk of the value comes from, and that new relationships between participants would be an added bonus. As it turns out, our role in forming a peer group of funders is in itself very valuable, enabling better funder coordination, individual grantmaking decisions, and helping funders stay motivated through the challenges they face on their grantmaking journeys.&nbsp;</li></ul><h3><strong>Estimated impact</strong></h3><p>It\u2019s too early to estimate the impact of the program pilot with much confidence, however our early attempts suggest that this program generates impact that is in the same order of magnitude as our Incubation Program, if not significantly higher. These estimates were generated by comparing scenarios; we compared how much and how impactfully the participants plan to donate after having undertaken the program with how they would have donated had they not participated.&nbsp;</p><h1><strong>Next steps</strong></h1><h3><strong>Our next steps</strong></h3><p>We have decided to continue the foundation program in 2023, developing the pilot into something more polished. We plan to run the program over a longer period of time (~10 weeks) to make the workload less intensive. The new format will be closer in form to a book club, with weekly readings and one weekly project, which will be discussed together as a group. The majority of the program will be remote, with ~one week in person.&nbsp; Our new handbook, How to Launch a High-Impact Foundation, will be improved and published as both the program\u2019s primary resource and for the general public\u2019s consumption.</p><p>Our vision is for a thriving community to be built off the back of the foundation program. As the alumni support one another and share their experiences, we hope the network will grow into a valuable ecosystem. The graduates from our charity incubation program not only benefit from our ongoing support, but from relationships formed with a much wider group of impact-minded people; we hope for something similar for the foundations. For example many of our alumni became part of the recently launched&nbsp;<a href=\"https://www.mentalhealthfunders.com/\"><u>mental health funding</u></a> circle and we can imagine many more circles like this in the future.</p><h3><strong>Your next steps, if you are excited about this program</strong></h3><p>Whether you are just starting out, or have many years experience, the 2023 Foundation Program can likely add value to your grantmaking. It is suitable for those who want to maximize the impact of their foundation in the future but could use a community and the structures to do so. It is a particularly good fit for those who want to be highly involved in grantmaking or in setting the direction for the foundation and hiring program officers to execute on it. To express your interest and discuss further you can email&nbsp;<a href=\"mailto:joey@charityentrepreneurshipo.com\"><u>joey@charityentrepreneurship.com</u></a></p><h3><strong>Apply to join our team</strong></h3><p>We are hiring a Programs Officer to join the department that creates program content, project manages and implements our programs and leads their continuous improvement (for both our Charity Incubation Program and our Foundation Program). An excellent first Programs Officer hire will allow us to scale from one to three programs each year and to continue to improve our program quality, which will (in expectation) more than double the impact that Charity Entrepreneurship can have in the world.</p><p>Full job description <a href=\"https://docs.google.com/document/d/1ueTHesSiX8qhXjgAoF5QQHsEQKX9yLOPuTTb3YKP9L8/edit#\">here</a>.&nbsp;</p><p>Apply <a href=\"https://form.jotform.com/220582913029353\">here</a> before 1st February (it only takes ~30 minutes).&nbsp;</p>", "user": {"username": "Aidan Alexander"}}, {"_id": "BZLj8tefWZrnB6YPe", "title": "LW4EA: Elastic Productivity Tools", "postedAt": "2023-01-03T03:18:53.693Z", "htmlBody": "<p>Written by LW user <a href=\"https://www.lesswrong.com/users/simon-berens\">Simon Berens</a>.</p><p>This is part of&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/low-commitment-less-wrong-book-club\">LessWrong for EA</a>, a LessWrong repost &amp; low-commitment discussion group (inspired by&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/qtDZHBx6LpAR7ejDD/what-standalone-lesswrong-posts-would-you-recommend-to-most?commentId=ykgk75KtczWSyuLgs#comments\">this comment</a>). Each week I will revive a highly upvoted, EA-relevant post from the LessWrong Archives, more or less at random</p><p>Excerpt from the post:</p><blockquote><p>Like most tech bros, I\u2019m a little too interested in productivity and optimizing my life. I\u2019ve even made a few of my own tools to help me stay focused and efficient. In the process of trying to find and build the best productivity tools possible, I discovered common elements among all the productivity tools I found most effective.&nbsp;</p><p>I call such tools <i>elastic</i>, because of their similarities to rubber bands. And so, elastic tools</p><ul><li>stretch, meaning that they\u2019re designed to allow for some leeway</li><li>snap back, i.e. after stretching they automatically go back to their intended form</li><li>don\u2019t break, in the sense that you can never stretch them too far</li></ul><p>Elastic tools are a marked improvement over standard tools, which typically</p><ul><li>are brittle, allowing no leeway</li><li>don\u2019t help you get back on track</li><li>end up getting ditched</li></ul><p>(<a href=\"https://www.lesswrong.com/posts/eJYDck4EthKFLga7q/elastic-productivity-tools\">Full Post on LW</a>)</p></blockquote><p>Please feel free to,</p><ul><li>Discuss in the comments</li><li>Subscribe to the&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/low-commitment-less-wrong-book-club\">LessWrong for EA tag</a>&nbsp;to be notified of future posts</li><li>Tag other LessWrong reposts with&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/low-commitment-less-wrong-book-club\">LessWrong for EA</a>.</li><li>Recommend additional posts</li></ul>", "user": {"username": "captainjc"}}, {"_id": "6rPnJeLJj2dMbw9yR", "title": "Status quo bias; System justification", "postedAt": "2023-01-03T02:50:50.984Z", "htmlBody": "", "user": {"username": "remmelt"}}, {"_id": "FGiKbbTKezqj9bvbm", "title": "How have shorter AI timelines been affecting you, and how have you been responding to them? ", "postedAt": "2023-01-03T04:20:01.875Z", "htmlBody": "<p>I'm curious about effects in the broadest sense: mental, emotional, practical, abstract, or concrete. Have shorter timelines have caused you to change career, investing, or giving plans? Are you experiencing existential terror or excitement? Something else? If you have been experiencing unpleasant emotional or psychological effects from shorter timelines, I'd also be interested to know if you have found coping strategies.&nbsp;</p>", "user": {"username": "Liav.Koren"}}, {"_id": "SzNpP3zPWz5aA98YH", "title": "If EA Community-Building Could Be Net-Negative, What Follows?", "postedAt": "2023-01-02T19:21:36.640Z", "htmlBody": "<p>I think it\u2019s likely that institutional effective altruism was a but-for cause of FTX\u2019s existence<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpkj1wuqrlg\"><sup><a href=\"#fnpkj1wuqrlg\">[1]</a></sup></span>&nbsp;and therefore that it may have caused about $8B in economic damage due to FTX\u2019s fraud (as well as potentially causing permanent damage to the reputation of effective altruism and longtermism as ideas). This example makes me feel it\u2019s plausible that effective altruist community-building activities could be net-negative in impact,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6737kyy8ojo\"><sup><a href=\"#fn6737kyy8ojo\">[2]</a></sup></span>&nbsp;and I wanted to explore some conjectures about what that plausibility would entail.&nbsp;</p><p>I recognize this is an emotionally charged issue, and to be clear my claim is not \u201cEA community-building has been net-negative\u201d but instead that that\u2019s plausibly the case (i.e. something like &gt;10% likely). I don\u2019t have strong certainty that I\u2019m right about that and I think a public case that disproved my plausibility claim would be quite valuable. I should also say that I have personally and professionally benefitted greatly from EA community building efforts (most saliently from efforts connected to the Center for Effective Altruism) and I sincerely appreciate and am indebted to that work.</p><p>Some claims that are related and perhaps vaguely isomorphic to the above which I think are probably true but may feel less strongly about are:&nbsp;</p><ul><li>To date, there has been a strong presumption among EAs that activities likely to significantly increase the number of people who explicitly identify as effective altruist (or otherwise increase their identification with the EA movement) are default worth funding. That presumption should be weakened.</li><li>Social movements are likely to overvalue efforts to increase the power of their movement and undervalue their goals actually being accomplished, and EA is not immune to this failure mode.&nbsp;</li><li>Leadership within social movements are likely to (consciously or unconsciously) overvalue measures that increase the leadership\u2019s own control and influence and under-value measures that reduce it, which is a trap EA community-building efforts may have unintentionally fallen into. &nbsp;</li><li>Pre-FTX, there was a reasonable assumption that expanding the EA movement was one of the most effective things a person could do, and the FTX catastrophe should significantly update our attitude towards that assumption.</li><li>FTX should significantly update us on principles and strategies for EA community/movement-building and institutional structure, and there should be more public discourse on what such updates might be.</li><li>EA is obligated to undertake institutional reforms to minimize the risk of creating an FTX-like problem in the future.&nbsp;</li></ul><p>Here are some conjectures I\u2019d make for potential implications of believing my plausibility claim:</p><ul><li><strong><u>Make Impact Targets Public</u></strong>: Insofar as new evidence has emerged about the impact of EA community building (and/or insofar as incentives towards movement-building may map imperfectly onto real-world impact), it is more important to make public, numerical estimates of the goals of particular community-building grants/projects going forward and to attempt public estimation of actual impact (and connection to real-world ends) of at least some specific grants/projects conducted to date. Outside of GiveWell, I think this is something EA institutions (my own included) should be better about in general, but I think the case is particularly strong in the community-building context given the above.</li><li><strong><u>Separate Accounting for Community Building vs. Front-Line Spending</u></strong>: I have argued <a href=\"https://forum.effectivealtruism.org/posts/M44i4CiMECP5Xoorz/demandingness-and-time-money-tradeoffs-are-orthogonal?commentId=9PuoHXCdGkG9kDaCF\">in the past</a> that meta-level and object-level spending by EAs should be in some sense accounted for separately. I admit this idea is, at the moment, under-specified but one basic example would be \u201cEAs/EA grant makers should say their \u201cfront-line\u201d and \u201cmeta\u201d (or \u201ccommunity building\u201d) donation amounts as separate numbers (e.g. \u201cI gave X to charity this year in total of which, Y was to EA front-line stuff, Z to EA community stuff, and W was non-EA stuff\u201d). I think there may be intelligent principles to develop about how the amounts of EA front-line funding and meta-level funding should relate to one another, but I have less of a sense of what those principles might be than a belief that starting to account for them as separate types of activities in separate categories will be productive.&nbsp;</li><li><strong><u>Integrate Future Community Building More Closely with Front-Line Work</u></strong>: Insofar as it makes sense to have less of a default presumption towards the value of community building, a way of de-risking community building activities is to link them more closely to activities where the case for direct impact is stronger. For example, personally I hope for some of my kidney donation, challenge trial recruitment, and Rikers Debate Project work to have significant EA community-building upshots, even though that meta level is not those projects\u2019 main goal or the metric I use to evaluate them. For what it\u2019s worth, I think pursuing \u201cdouble effect\u201d strategies (e.g projects that simultaneously have near-termist and longtermist targets or animal welfare and forecasting-capacity targets) is underrated in current EA thinking. I also think connecting EA recruitment to direct work may mitigate certain risks of community building (e.g. the risks of creating an EA apparatchik class, recruiting \u201cEAs\u201d not sufficiently invested in having an actual impact, or competing with direct work for talent)&nbsp;</li><li><strong><u>Implement Carla Zoe Cremer\u2019s Recommendations</u></strong>:&nbsp; Maybe I\u2019m biased because we\u2019re quoted together in some of the same articles but I\u2019ve honestly been pretty surprised there has not been more public EA discussion post-FTX of adopting a number of Cremer's <a href=\"https://t.co/fKzB6YzpIc\">proposed institutional reforms</a>, many of which seem to me obviously worth doing (e.g. whistleblowing protections). Some (such as democratizing funding decisions) are more complicated to implement, and I acknowledge the concern that these procedural measures create friction that could reduce the efficacy of EA organizations, but I think (a) minimizing unnecessary burden is a design challenge likely to yield fairly successful solutions and (b) FTX clearly strengthens the arguments in favor of bearing the cost of that friction. Also, insofar as she'd be willing (and some form of significant compensation is clearly merited), integrally engaging Cremer in whatever post-FTX EA institutional reform process emerges would be both directly helpful and a public show of good faith efforts at rectification.</li><li><strong><u>Consideration of a \u201cPulse\u201d Approach to Funding&nbsp; EA Community Building</u></strong>: It may be the case that large EA funders should do time-limited pulses of funding towards EA community building goals or projects with the intention of building institutions that can sustain themselves off of separate funds in the future. The logic of this is: (a) insofar as EAs may be bad judges of the value of our own community building, requiring something appealing to external funders helps check that bias, (b) creating EA community institutions that must be attractive to outsiders to survive may avoid certain epistemic and political risks inherent to being&nbsp; too insular&nbsp;</li><li><strong><u>EA as a Method and not a Result</u></strong>: The concept of effective altruism (rationally attempting to do good) has broad consensus but particular conceptions may be parochial or clash with one another.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy5jrwlky22p\"><sup><a href=\"#fny5jrwlky22p\">[3]</a></sup></span>&nbsp;A \u201cthinner\u201d effective altruism that emphasizes EA as an idea akin to the scientific method rather than a totalizing identity or community may be less vulnerable to FTX-like mistakes.&nbsp;</li><li><strong><u>Develop Better Logic for Weighing Harms Caused by EA against EA Benefits</u></strong>: An EA logic that assumes resources available to EAs will be spent at (say) GiveWell benefit levels (which I take to be roughly $100/DALY or equivalent) but that resources available to others are spent at (say) US government valuations of a statistical life (I think roughly $100,000/DALY) seems to justify significant risks of incurring very sizable harms to the public if they are expected to yield additional resources for EA. Clearly, EA's obligations to avoid direct harms (or certain types of direct harms) are at least somewhat asymmetric to obligations/permissions to generate benefits. But at the same time, essentially any causal act will have some possibility of generating harm (which in the case of systemic change efforts can be quite significant), so a precautionary principle designed in an overly simplistic way would kneecap the ability of EAs to make the world better. I don't know the right answer to this challenge, but clearly \"defer to common sense morality\" has proven insufficient, and I think more intellectual work should be done.</li></ul><p>I'm not at all certain about the conjectures/claims above, but I think it's important that EA deals with the intellectual implications of the FTX crisis, so I hope they can provoke a useful discussion.&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpkj1wuqrlg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpkj1wuqrlg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Am basing this on reporting in <a href=\"https://www.semafor.com/article/12/07/2022/charity-money-launched-sam-bankman-frieds-empire\">Semafor</a> and the <a href=\"https://www.newyorker.com/news/annals-of-inquiry/sam-bankman-fried-effective-altruism-and-the-question-of-complicity\">New Yorker</a>. To be clear, I'm not saying that once you assume Alameda/FTX's existence, the ideology of effective altruism necessarily made it more likely that those entities would commit fraud. But I do think it is unlikely they would have existed in the first place without the support of institutional EA.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6737kyy8ojo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6737kyy8ojo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>To be clear, my claim is not \"the impact of the FTX fraud incident plausibly outweighs benefits of EA community building efforts to date\" (though that may be true and would be useful to publicly disprove if possible) but that the FTX fraud should demonstrate there are a range of harms we may have missed (which collectively could plausibly outweigh benefits) and that \"investing in EA community building is self-evidently good\" is a claim that needs to be reexamined.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fny5jrwlky22p\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefy5jrwlky22p\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I find the <a href=\"https://lsolum.typepad.com/legal_theory_lexicon/2004/03/legal_theory_le_1.html\">distinction between concept and conception </a>to be helpful here. Effective altruism as a concept is broadly unobjectionable, but particular conceptions of what effective altruism means or ought entail involve thicker descriptions that can be subject to error or clash with one another. For example, is extending present-day human lifespans default good because human existence is generally valuable or bad because doing so tends to create greater animal suffering that outweighs the human satisfaction in the aggregate? I think people who consider the principles of effective altruism important to their thinking can reasonably come down on both sides of that question (though I, and I imagine the vast majority of EAs, believe the former). Moreover efforts to build a singular EA community around specific conceptions of effective altruism will almost certainly exclude other conceptions, and the friction of doing so may create political dynamics (and power-seeking behavior) that can lead to recklessness or other problems.</p></div></li></ol>", "user": {"username": "joshcmorrison"}}, {"_id": "u7uZPq9fmxdPFX4aq", "title": "Prediction Markets for Science", "postedAt": "2023-01-02T17:55:12.833Z", "htmlBody": "", "user": {"username": "vaniver"}}, {"_id": "EGztvGHigkekgkvc8", "title": "Community Building from scratch: The first year of EA Hungary", "postedAt": "2023-01-02T16:35:59.912Z", "htmlBody": "<h1>TLDR/summary:</h1><p>I (Gerg\u0151) started EA Hungary as a paid organizer in mid-September 2021. In one year, we\u2019ve had 100+ intro fellowship applicants, 88 of whom completed it successfully. We have run other fellowships, from whom around&nbsp;20 people have benefited.&nbsp;I have had about 110 calls, most of which were oriented on career support for the new members. EA Hungary brought counterfactually&nbsp;~35 people to EAGx conferences and organized a retreat for&nbsp;20 students. EA Hungary hired the second employee (Dia) in July, doing&nbsp;~0.5 FTE.<br><br><i>The aim of this post is to share the progress of EA Hungary in its first year (2021 Oct to 2022 Sept) I wrote this up with the intent of helping those who are starting a new group (hence the chronological order). If you are only interested in the main outcomes, read the tables. I hope this will be useful.&nbsp;</i></p><h1>Background and how I got funding&nbsp;</h1><p>I learned about EA sometime in 2020 and while attending the EAGxVirtual (13 \u2013 14 June 2020), I met a few other people based in Hungary. We had the first in-person meetup in August 2020, with 6 people. Then the Covid-19 pandemic hit, so we carried on with online meetups about once per month. We ran the intro fellowship for existing members (7 people), followed by the in-depth fellowship (4 people).</p><p>During this time (and afterwards) I was supported by Catherine Low from CEA, from whom I learned a lot (\u223c1 call every 1,5 months).</p><p>In the meantime, I was also volunteering for&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/sogive\"><u>SoGive</u></a> as a charity analyst and then as an analysis coordinator (teaching people basic charity analysis + operations work). I learned a lot from this and I think it really sped up my involvement within EA. My experience with SoGive also helped me start and run EA Hungary more effectively, as I have already gained some operations and mentoring experience.</p><p>In the fall of 2021, I started a second Master\u2019s in Philosophy at ELTE - (<a href=\"https://www.elte.hu/en/\"><u>E\u00f6tv\u00f6s Lor\u00e1nd University</u></a>).</p><p>Thanks to these activities mentioned above, I had a strong enough track record<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefugr5i79qonj\"><sup><a href=\"#fnugr5i79qonj\">[1]</a></sup></span>&nbsp;to apply for funding from&nbsp;<a href=\"https://funds.effectivealtruism.org/\"><u>EAIF</u></a>&nbsp; to do 0.5 FTE, (although this upscaled to full-time pretty quickly).</p><p>(I also have this story presented in&nbsp;<a href=\"https://docs.google.com/presentation/d/1N17Ks7__OE51I9ifgwX09hvaWEH2fkhJfDxJ3u30sW0/edit?usp=sharing\"><u>memes</u></a>, don\u2019t ask why.)</p><h1>2021 Fall semester (from 2021 Mid-Sept to 2022 January)</h1><h2><strong>Main outcomes:&nbsp;</strong></h2><p>(Where you see gaps it means that I didn\u2019t record the exact numbers/data)</p><figure class=\"table\"><table><tbody><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><strong>8-week intro Fellowship:</strong></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">Number of applicants:</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>26</p></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><a href=\"https://docs.google.com/spreadsheets/d/1KRSxNSWoUWHWkVavun-YbC3GcqKxJxerRme8dKbiYWg/edit?usp=sharing\"><u>Data</u></a></td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">Number of people successfully completing the fellowship</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>22</p></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">Number of people who filled out the completion survey:</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>22</p></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><a href=\"https://docs.google.com/spreadsheets/d/17zZEiYnlPlNdnYeSKs5YxoDqW1U-F1dICUF8OGg6qTw/edit?usp=sharing\"><u>Data</u></a></td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">1-1 Mentoring calls</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>\u223c20</p></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr></tbody></table></figure><h2><strong>Other outcomes:</strong></h2><p>- Online social around week 4, which was attended by around 6 people<br>- End-of-fellowship meal which was attended by 14 people.</p><h2><strong>General info about the 2021 Fall period:</strong></h2><p>- Marketing and advertisement of the fellowship was almost completely done on Facebook groups of various university programs of&nbsp;<a href=\"https://www.elte.hu/en/\"><u>ELTE university</u></a>. (Facebook is still the most widely used social media in Hungary, although it is slowly losing popularity)</p><p>- Lots of start-up time costs, as I was still figuring things out.<i>&nbsp;</i></p><p><i>If you would like to save time on operations, I recommend using some draft emails/forms from the&nbsp;</i><a href=\"https://drive.google.com/drive/folders/1NIyVr4EZtcSBHT_NmFnGpLqpJLBEcBRj?usp=share_link\"><i><u>EA Hungary care package</u></i></a><i> as well as these amazing resources:&nbsp;</i><a href=\"https://handbook.eagroupresources.com/\"><i><s><u>EA Student Groups Handbook</u></s></i></a><i><s>, </s></i><a href=\"https://forum.effectivealtruism.org/posts/MPmFgaJCjpzm742vD/masterdocs-of-ea-community-building-guides-and-resources\"><i>Masterdocs of EA community building guides and resources</i></a><i>,&nbsp;</i><a href=\"https://resources.eagroups.org/\"><i><u>EA Groups Resource Centre,</u></i></a><i> and this&nbsp;</i><a href=\"https://drive.google.com/drive/folders/1C0hyWV5E7crFpy2HRzjQ7BdXtsuWpSd_\"><i><u>folder</u></i></a><i>.</i></p><h1>2022 Spring Semester (2022 February-May)</h1><h2><strong>Main outcomes:</strong></h2><figure class=\"table\"><table><tbody><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><strong>8-week intro Fellowship:</strong></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">Number of people successfully completing the fellowship</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>19</p></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><a href=\"https://docs.google.com/spreadsheets/d/1DQCeLmtt9VYjGWpJhbRma1GFPj2OqQz_1ZHQ3z3uNX8/edit?usp=sharing\"><u>Data</u></a></td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">Number of people who filled out the completion survey:</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>15</p></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><a href=\"https://docs.google.com/spreadsheets/d/11FO5V8Xz5B9b4vuVvTHYuVXsCF4HndzcmDzQ9oKifQI/edit?usp=sharing\"><u>Data</u></a></td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><strong>In-depth fellowship</strong></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">Number of people successfully completing the fellowship</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>13</p></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><strong>1-1 Mentoring calls</strong> (unfortunately I didn\u2019t keep an exact account, so this is a rough estimate)</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>\u223c30</p></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><br><br>&nbsp;</td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><strong>EAGxOxford attendees</strong></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>4</p></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><strong>EAGxPrague attendees</strong> (the two numbers mean&nbsp;<strong>all attendees</strong> -&nbsp;<strong>counterfactual attendees</strong> (ie. people who would most likely not be present if it wasn\u2019t for EA Hungary)</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>14-\u223c11</p></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><a href=\"https://www.facebook.com/photo/?fbid=3340772392876219&amp;set=pcb.3340778989542226\"><u>FB link</u></a></td></tr></tbody></table></figure><h2><strong>Other outcomes:</strong></h2><p>-Weekly socials during the fellowship which were attended by 5-10 people per session<br>- An end-of-fellowship meal which was attended by 18 people<br>- One person attributed the Giving What We Can pledge to EA Hungary\u2019s influence<br>- Around 10 people signed up to volunteer after they completed the intro fellowship</p><h2><strong>General info about 2022 Spring period:</strong></h2><p><strong>-&nbsp;</strong>Same marketing strategy as in&nbsp; Fall 2021<br>- Had some help organising from volunteers from the Fall 2021 period, which was great! My experience was that the amount of help from volunteers resembles the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Pareto_principle\"><u>20/80 rule</u></a>, where the most engaged volunteers help a disproportionate amount.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1672677361/mirroredImages/EGztvGHigkekgkvc8/fa40a3pijnhimqwg6zwv.jpg\"><figcaption><i>EA Hungary members at EAGxPrague, thank you to the organisers!</i></figcaption></figure><h1>2022 Summer (June-Aug)</h1><h2>Main outcomes:</h2><figure class=\"table\"><table><tbody><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><strong>8-week intro Fellowship June-July</strong></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">Number of applicants:</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>24</p></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><a href=\"https://docs.google.com/spreadsheets/d/1gHIT-oWeeN12NvyZ1VodygF_yWdcvxrSqlwLix1SKNM/edit?usp=sharing\"><u>Data</u></a></td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">Number of people successfully completing the fellowship</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>17</p></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td><strong>8-week intro fellowship for professionals</strong></td><td>&nbsp;</td><td>&nbsp;</td></tr><tr><td>Number of people successfully completing the fellowship</td><td>5</td><td>&nbsp;</td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><strong>In-depth fellowship</strong></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">Number of applicants:</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>7</p></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">Number of people successfully completing the fellowship</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>5</p></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr></tbody></table></figure><h3><strong>Thoughts on the in-depth fellowship:</strong></h3><p>We were slightly discouraged by how few people applied for the in-depth fellowship, even though we had a lot of excited members. It seems like students become very busy during summer, and an 8-week fellowship might not be the best fit in our context.&nbsp;</p><p>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><strong>Cause area exploration fellowship</strong></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">Number of people successfully completing the fellowship</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">5</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><a href=\"https://docs.google.com/spreadsheets/d/1sb7jvSnlZxgolRNIa9bguuT037dcQw07w6AbL4xs_Fo/edit?usp=sharing\"><u>Data</u></a></td></tr><tr><td><strong>Intensive 4+4-session intro Fellowship starting in August</strong></td><td>&nbsp;</td><td>&nbsp;</td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">Number of people successfully completing the 4 days</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>23</p></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">Number of people who signed up for the second 4 sessions*</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>13</p></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><a href=\"https://docs.google.com/spreadsheets/d/1BjghthHOcHlkZQLmloZq2YfEgXVkkHdXYFsEZkyAUWE/edit?usp=sharing\"><u>Data</u></a></td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">Number of people successfully finishing the second half</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>13</p></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr></tbody></table></figure><h3><strong>What is the 4+4 session intro fellowship?</strong></h3><p>Partly inspired by&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/NiEHWtHhrtFcQduw3/why-a-4-week-fellowship-is-better-than-an-8-week-fellowship\"><u>this post</u></a> on the EA forum, we decided to try advertising an intensive 4-day fellowship with the hope of attracting more people, given that they have to commit less. Additionally, after they completed the 4 sessions, they had the opportunity to join another 4 sessions (hence the +4). This was called an intensive fellowship because the first 4 sessions were run over just one week (from Monday to Thursday+social on Friday). &nbsp;We used a <a href=\"https://docs.google.com/document/d/143qxDjnmdwK8JvhhHlCh9fIgbR_LMaT7u5iy8iohwz8/edit?usp=sharing\">modified fellowship curriculum</a> which basically merges the PISE's 4-week fellowship and the general 8-week Oxford fellowship.</p><p>*While only about 50% of the participants who finished the first 4 sessions signed up for the second half, our sense was that people didn\u2019t join mainly due to time constraints, and not because they didn\u2019t enjoy the program. Our hope is that those who couldn't join the second half will still become part of our community and finish the intro fellowship later.</p><p>Overall, EA Hungary saw positive results from this pilot and I plan to write a forum post about this that goes into more detail. Once written, the post will be linked here.</p><p>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><strong>Cause area exploration fellowship</strong></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">Number of people successfully completing the fellowship</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">5</td></tr></tbody></table></figure><h3><strong>What is the cause area exploration fellowship?</strong></h3><p>The cause area exploration fellowship is the first section of this&nbsp;<a href=\"https://handbook.eagroupresources.com/packaged-programs/career-planning-program\"><u>career planning program</u></a><u>,</u> it consists of 5 sessions. All members can sign up for this program, including intro-fellowship participants who are just starting out. Feedback from participants was encouraging, but I am still not sure how important it is to have this as an option. My current best guess is that it is worth running if you have people enrolled in intro/in-depth fellowships who want to commit more, but people shouldn't join this course instead of an in-depth fellowship.</p><figure class=\"table\"><table><tbody><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><strong>Retreat</strong></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">We did a 3-day retreat for 20 members</td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">Feedback survey was filled out by 7 attendees<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgqdo17wid54\"><sup><a href=\"#fngqdo17wid54\">[2]</a></sup></span></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><a href=\"https://docs.google.com/spreadsheets/d/13b6tWJqTRbMQQospkcrjyDnZe8yWGFfT370x4TstTkU/edit?usp=sharing\"><u>Data</u></a></td></tr></tbody></table></figure><h3><strong>Thoughts on the retreat:</strong></h3><p>We organised a retreat for our student members. It was challenging in terms of logistics, given that it was the first time we ran something like this. However, it turned out to be very valuable for our members, and it was really fun. People who attended the retreat became very excited about Effective Altruism and they also got to bond and make friendships, which seems very important in helping people to become part of the global EA community in the long run. In advertising, we called it a community conference to make it sound less weird. We think this is a better name!</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1672677361/mirroredImages/EGztvGHigkekgkvc8/dts8rsfaquangeytsaww.jpg\"><figcaption><i>EA Hungary Community Conference, 2022 August</i></figcaption></figure><figure class=\"table\"><table><tbody><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><strong>Mentoring/connecting/logistics calls:</strong></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td style=\"border-style:solid;padding:2pt 0pt;vertical-align:bottom\">1-1 calls</td><td style=\"border-style:solid;padding:2pt 0pt;vertical-align:bottom\"><p>\u223c50</p></td></tr></tbody></table></figure><h3><strong>Thoughts about 1-1 calls:</strong></h3><p>These seem to be extremely valuable for members because they can get feedback on their plans, as well as personalised advice, recommendations, etc. My general aim was to have a 1-1 call with everyone who finished the intro fellowship, and then another one once they finish the in-dept fellowship. Due to &nbsp;lack of time, I often fell short on this.</p><h2><strong>Other outcomes:</strong></h2><p>- 3 co-reading sessions (people reading some EA materials together and discussing them) were attended by 4-8 people per session<br>- Policy-themed dinner party attended by 5 people<br>-Tabling at a student conference where we collected about 20 emails.</p><h2><strong>General info about the 2022 Summer period:</strong></h2><p><strong>-</strong>Same marketing strategy as in 2021 Fall, but we also used some paid Facebook and Instagram ads. The results of this were promising and I plan to write a forum post about this at some point, which will be linked here.</p><h1>EAGxBerlin</h1><figure class=\"table\"><table><tbody><tr><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\"><strong>EAGxBerlin</strong></td><td style=\"border:0.9375pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td></tr><tr><td>Main outcome was bringing 33 people to EAGxBerlin of which at least 24 people would most likely not have been there if EA Hungary didn\u2019t exist.</td><td>33-24</td></tr></tbody></table></figure><p>I was conflicted about whether to include EAGxBerlin here or in next year's report, but given that we did most of the preparations for it in the summer and that my contract started 2021 in mid-September, I thought it would be more fitting here, marking the end of our first year.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1672677361/mirroredImages/EGztvGHigkekgkvc8/w4wqtgtkjk0tow0twtvs.jpg\"><figcaption><i>EAGxBerlin marked the end of our first year, thank you to the organisers!&nbsp;</i></figcaption></figure><h2>Thoughts about bringing members to EAGxConferences:</h2><p>These seemed very valuable in getting our members more involved with EA. We tried to really encourage people to apply, mainly in 1-1s, and by sending emails to all members. We also emphasised how we are going there as a group (will travel together and stay at the same hotel), so people feel less shy and don't think they have to go alone. (Of course, it is really hard to know the counterfactual impact of our messaging, but being encouraging shouldn't hurt!)</p><h1>Conclusion</h1><p>The first year of EA Hungary is promising and we have grown rapidly. However, it is important to note that most of our members are still learning about effective altruism and most of them are also at university. This means that (apart from the 2 people signing up for the 10% pledge), our impact is instrumental at this time.&nbsp;</p><p>One of our core priorities in the next year will be to support and equip our student members to gain the skills necessary to eventually be able to pursue high-impact careers. We also plan to keep running our intro and in-depth fellowships.</p><h1>Acknowledgments</h1><p>Thank you Cathrine Low for helping EA Hungary from the very beginning. You are an amazing mentor from whom I learnt so much and you were always there to listen and support us. &lt;3</p><p>Also thank you to our volunteers who helped out extensively and Di\u00e1na Jeneses who joined our team in July. Thank you to R\u00e9ka Tr\u00f3n who has helped us all the way from the UK by facilitating fellowships!</p><p>Thanks to Amarins and Dia for proofreading and Cristina and Eni for making this report significantly more reader-friendly!</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnugr5i79qonj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefugr5i79qonj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I\u2019m not sure how much track record you need to get funding, but I don\u2019t think you need to volunteer for an EA org beforehand. I would assume funders would look for some experience in community building before funding you, but there could be exceptions to this I think (eg. you are a good fit but stuck working in a non-EA student job). However, I think that the experience of doing operations for SoGive really helped me at my work.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngqdo17wid54\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgqdo17wid54\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In the next retreat, we plan to have people fill out the feedback form during the closing session to get more answers.</p></div></li></ol>", "user": {"username": "gergogaspar"}}, {"_id": "5h8bNTFHkrNNzrrJf", "title": "Results from the AI testing hackathon", "postedAt": "2023-01-02T15:46:43.737Z", "htmlBody": "<p>We (<a href=\"https://apartresearch.com\">Apart Research</a>) ran a&nbsp;<a href=\"https://itch.io/jam/aitest\"><u>hackathon for AI testing research projects</u></a> with 11 projects submitted by 34 participants between the 16th and 18th December. Here we share the winning projects. See them all&nbsp;<a href=\"https://alignmentjam.com/jams\"><u>here</u></a>. In summary:</p><ul><li>Found that unsupervised latent knowledge representation is generalizable and takes the first steps towards a benchmark using the ETHICS ambiguous / unambiguous examples with latent knowledge evaluation.</li><li>Created a new way to use token loss trajectories as a marker for targeting our interpretability methods towards a focus area.</li><li>Investigated three potential&nbsp;<a href=\"https://www.lesswrong.com/posts/eqxqgFxymP8hXDTt5/announcing-the-inverse-scaling-prize-usd250k-prize-pool\"><u>inverse scaling</u></a> phenomena: Counting letters, chaining premises and solving equations. Found incidental inverse scaling on one of them and U-shaped scaling on another.</li><li>Implemented Trojans into Transformer models and used a gradient arithmetic technique to combine multiple Trojan triggers into one Transformer model.</li><li>(honorable mention) Invented a way to test how quickly models become misaligned by negative example fine-tuning.</li></ul><p><i>Thank you to Zaki, Fazl, Rauno, Charbel, Nguyen,&nbsp;</i><a href=\"https://alignmentjam.com/running\"><i><u>more jam site organizers</u></i></a><i>, and the participants for making it all possible.</i></p><h2>Discovering Latent Knowledge in Language Models Without Supervision - extensions and testing</h2><p><i>By Agatha Duzan, Matthieu David, Jonathan Claybrough</i></p><p><strong>Abstract</strong>: Based on the paper \"Discovering Latent Knowledge in Language Models without Supervision\" this project discusses how well the proposed method applies to the concept of ambiguity.&nbsp;</p><p>To do that, we tested the Contrast Consistent Search method on a dataset which contained both clear cut (0-1) and ambiguous (0,5) examples: We chose the ETHICS-commonsense dataset.</p><p>The global conclusion is that the CCS approach seems to generalize well in ambiguous situations, and could potentially be used to determine a model\u2019s latent knowledge about other concepts.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1672674405/mirroredImages/5h8bNTFHkrNNzrrJf/jvujpyyvnccixexbsne0.png\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1672674405/mirroredImages/5h8bNTFHkrNNzrrJf/pk2ubl2i8jhtsndnbzlr.png\"></p><p><i>These figures show how the CCS results for last layer activations splits into two groups for the non-ambiguous training samples while the ambiguous test samples on the ETHICS dataset reveals the same ambiguity of latent knowledge by the flattened Gaussian inference probability distribution.</i></p><p><strong>Haydn &amp; Esben\u2019s judging comment</strong>: This project is very good in investigating the generality of unsupervised latent knowledge learning. It also seems quite useful as a direct test of how easy it is to extract latent knowledge and provides an avenue towards a benchmark using the ETHICS unambiguous/ambiguous examples dataset. Excited to see this work continue!</p><p><a href=\"https://itch.io/jam/aitest/rate/1842259\"><u>Read the report</u></a> and&nbsp;<a href=\"https://github.com/madvid/Discovering_Latent_Knowledge_in_Language_Models\"><u>the code</u></a> (needs updating).</p><h2>Investigating Training Dynamics via Token Loss Trajectories</h2><p><i>By Alex Foote</i></p><p><strong>Abstract</strong>: Evaluations of ML systems typically focus on average statistical performance on a dataset measured at the end of training. However, this type of evaluation is relatively coarse, and does not provide insight into the training dynamics of the model.&nbsp;</p><p>We present tools for stratifying tokens into groups based on arbitrary functions and measuring the loss on these token groups throughout the training process of a Language Model. By evaluating the loss trajectory of meaningful groups of tokens throughout the training process, we can gain more insight into how the model develops during training, and make interesting observations that could be investigated further using interpretability tools to gain insight into the development of specific mechanisms within a model.&nbsp;</p><p>We use this lens to look at the training dynamics of the region in which induction heads develop. We also zoom in on a specific region of training where there is a spike in loss and find that within this region the majority of tokens follow the loss trajectory of a spike, but a small set follow the inverse trajectory.</p><p><strong>Haydn &amp; Esben\u2019s judging comment</strong>: This is really good work in testing for locations to look at using interpretability tools! Further developing this idea into something that can be used as a testing suite would be quite interesting and the ideas from the project seem useful at scale.</p><p><a href=\"https://itch.io/jam/aitest/rate/1842313\"><u>Read the report and python notebook</u></a>.</p><h2>Counting Letters, Chaining Premises &amp; Solving Equations: Exploring Inverse Scaling Problems with GPT-3</h2><p><i>By D. Chipping, J. Harding, H. Mannering, P. Selvaraj</i></p><p><strong>Abstract</strong>: Language models generally show increased performance in a variety of tasks as their size increases. But there are a class of problems for which increase in model size results in worse performance. These are known as inverse scaling problems.</p><p>In this work, we examine how GPT-3 performs on tasks that involve the use of multiple, interconnected premises and those that require the counting of letters within given strings of text as well as solving simple multi-operator mathematical equations.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1672674405/mirroredImages/5h8bNTFHkrNNzrrJf/flq3fbjqj7hxi3sg7pae.png\"></p><p><strong>Haydn &amp; Esben\u2019s judging comment</strong>: These are really interesting investigations into inverse scaling! Each of these three tasks can be extended quite liberally. It\u2019s pretty epic to find two relatively good inversely scaling phenomena while the third might end up working with a bit of a reframing. Curious to see more generality testing for the inverse scaling.</p><p>See the&nbsp;<a href=\"https://github.com/pravsels/ai-test-hack/blob/main/word_count/Generate_Word_Count_Dataset.ipynb\"><u>dataset generation code</u></a>, the&nbsp;<a href=\"https://github.com/pravsels/ai-test-hack/blob/main/word_count/word_count_plots.ipynb\"><u>graph plotting code</u></a>, and&nbsp;<a href=\"https://itch.io/jam/aitest/rate/1842215\"><u>the report</u></a>.</p><h2>Trojan detection and implementation on transformers</h2><p><i>By Cl\u00e9ment Dumas, Charbel-Rapha\u00ebl Segerie, Liam Imadache</i></p><p><strong>Abstract</strong>: Neural Trojans are one of the most common adversarial attacks out there. Even though they have been extensively studied in computer vision, they can also easily target LLMs and transformer based architecture. Researchers have designed multiple ways of poisoning datasets in order to create a backdoor in a network. Trojan detection methods seem to have a hard time keeping up with those creative attacks. Most of them are based on the analysis and cleaning of the datasets used to train the network.</p><p>There doesn't seem to be some accessible and easy to use benchmark to test Trojan attacks and detection algorithms, and most of these algorithms need the knowledge of the training dataset.&nbsp;</p><p>We therefore decided to create a small benchmark of trojan networks that we implemented ourselves based on the literature, and use it to test some existing and new detection techniques.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1672674405/mirroredImages/5h8bNTFHkrNNzrrJf/b0hgxsxeqbsqdiz6inqk.png\"></p><p>[from the authors]:&nbsp;<i>The colab contains the code to create the trojans described below, but you will also find some mysterious networks containing trojans that you can try to detect and explain. We will provide&nbsp;<strong>50 euros</strong> for the first one who will be able to propose a method to find our trigger!&nbsp;</i></p><p><strong>Haydn &amp; Esben\u2019s judging comment</strong>: Great to see so many replications of papers in one project and a nice investigation into Trojan triggers in training data. The proposed use of Task Vectors is quite interesting and the conclusion about Trojan attacks &gt;&gt; defenses is a good observation.</p><p><a href=\"https://itch.io/jam/aitest/rate/1842319\"><u>Read the report</u></a> and&nbsp;<a href=\"https://colab.research.google.com/assets/colab-badge.svg\"><u>the Colab</u></a>. Check out&nbsp;<a href=\"https://drive.google.com/drive/folders/1OW3tQjfOa75Zj73DMkodCUvh92vx8jAB?usp=share_link\"><u>the created Trojans</u></a> (if you dare).</p><h2>Honorable mention: The \u201cThis is Fine(-tuning)\u201d benchmark</h2><p><i>By Jan Wehner, Joep Storm, Tijmen van Graft, Jaouad Hidayat</i></p><p><strong>Abstract</strong>: Large language models (LLMs) build up models of the world and of tasks leading them to impressive performance on many benchmarks. But how robust are these models against bad data?&nbsp;</p><p>Motivated by an example where an actively learning LLM is being fed bad data for a task by malicious actors, we propose a benchmark, This Is Fine (TIF), which measures LLM\u2019s robustness against such data poisoning. The benchmark takes multiple popular benchmark tasks in NLP, arithmetics, \"salient-translation-error-detection\" and \"phrase-relatedness\" and records how the performance of an LLM degrades as it is being fine-tuned on wrong examples of this task.&nbsp;</p><p>Further, it measures how the fraction of fine-tuning data which is wrong influences the performance. We hope that an adaptation of this benchmark will enable researchers to test the robustness of the representations learned by LLMs and can prevent data poisoning attacks on high stakes systems.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1672674405/mirroredImages/5h8bNTFHkrNNzrrJf/knwnqxyftqv4uby0c2uc.png\"></p><p><strong>Haydn &amp; Esben\u2019s judging comment</strong>: An interesting approach to measuring adversarial data impacts! It\u2019s probably hard to generalize this without creating a new benchmark per task but thinking more about the general direction of performance falloff is very encouraged.</p><p><a href=\"https://itch.io/jam/aitest/rate/1842283\"><u>Read the report</u></a>.</p><h2>The Alignment Jam</h2><p>This alignment hackathon was held online and in five locations at the same time: Paris, Mexico City, Aarhus, Netherlands, and Oxford. We started with an introduction to AI governance and why testing AI is important right now by Haydn Belfield along with a short introduction to how benchmarks for safety might be created with Esben Kran (<a href=\"https://youtu.be/Nx5gIbTsHJM\"><u>recording</u></a>).</p><p>We had 63 signups, 34 submitters and 11 final entries (1 omitted due to info hazards). $2,200 in prizes were awarded by us judges, Haydn Belfield and Esben Kran.</p><p>The post hackathon survey saw a 10% increase in percentage points for working on AI safety and a 8 of 10 rating for how likely the participants would be to recommend joining the hackathon to their friends and colleagues. The testimonial feedback was generally positive, especially about&nbsp;<a href=\"https://alignmentjam.com/aitest\"><u>the starter resources provided</u></a>.</p><p>Join the upcoming <a href=\"https://itch.io/jam/ai-neuroscience\">AI neuroscience hackathon</a> and check out the <a href=\"https://alignmentjam.com/\"><u>Alignment Jam website</u></a>.</p>", "user": {"username": "esben-kran"}}, {"_id": "e5KTN4t8wq2nsiXYJ", "title": "Belief Bias: Bias in Evaluating AGI X-Risks", "postedAt": "2023-01-02T08:59:09.225Z", "htmlBody": "", "user": {"username": "remmelt"}}, {"_id": "YuKhRfwjQgs9sNpHG", "title": "[Job] Researcher (CEARCH)", "postedAt": "2023-01-02T07:28:46.165Z", "htmlBody": "<h3><strong><u>Summary</u></strong></h3><p>CEARCH is hiring researchers; for more details and to apply, refer to the job description <a href=\"https://docs.google.com/document/d/1B9p2t7xptnK_9Xwnw3zD0IjWda6YOYXGF9yeXrbEQqY/edit\">here</a>.</p><p>&nbsp;</p><h3><strong><u>CEARCH</u></strong></h3><p><a href=\"https://exploratory-altruism.org/\">The Centre for Exploratory Altruism Research (CEARCH)</a> conducts cause prioritization research and outreach \u2013 identifying the most important problems in the world and helping to direct resources towards solving them, so as to maximize global welfare. As part of this effort, we carry out:</p><ul><li>A comprehensive search for causes</li><li>Rigorous cause prioritization research, with (a) shallow research for all causes, (b) intermediate research for more promising causes, and (c) deep research for potential top causes.</li><li>Reasoning transparency and outreach to allow both the EA and non-EA movement to update on our findings and to support the most impactful causes available.</li></ul><p>CEARCH emerged from the 2022 Charity Entrepreneurship incubation program, and its founder Joel Tan is a long-time effective altruist with experience in political, economic and policy work.</p><p>&nbsp;</p><h3><strong><u>The Role</u></strong></h3><p>As a researcher, you will be performing:</p><ul><li><strong>Cause prioritization research and outreach</strong> \u2013 searching for causes, researching them, and engaging other organizations and individuals to use our research.</li><li><strong>Various generalist tasks&nbsp;</strong>(e.g. planning, administration).</li></ul><p>In your first couple of months, you will likely be focusing on research, but you will eventually get to handle more outreach-related work (e.g. representing CEARCH at EAG/EAGx).</p><p>&nbsp;</p><h3><strong><u>Ideal Candidate</u></strong></h3><p>The ideal candidate will:</p><ul><li>Care about&nbsp;<strong>impact&nbsp;</strong>and doing good above all.</li><li>Have<strong> strong research skills</strong>.</li><li>Be able to&nbsp;<strong>work well in a team</strong>.</li></ul><p>&nbsp;</p><h3><strong><u>Benefits</u></strong></h3><p>The chief reason you might be interested in this role is that it can be expected to be&nbsp;<strong>highly impactful</strong>.</p><ul><li>Our internal cost-effectiveness analysis (CEA) suggests that we save around&nbsp;<strong>1,700 disability-adjusted life years</strong> (or the equivalent of 58 lives) per USD 100,000 spent, which is around&nbsp;<strong>10x</strong> more cost-effective than GiveWell top charities.</li><li>External analysis supports this assessment, with Charity Entrepreneurship's CEA finding that we should be around&nbsp;<strong>8x&nbsp;</strong>as cost effective as top EA causes.</li><li>That said, such estimates&nbsp;<strong>should not be taken literally</strong>, given the high amount of uncertainty involved. Rather, they should be understood as giving a sense of how promising CEARCH's work \u2013 and hence your work with us \u2013 will be, given (a) the outsized benefits from identifying extremely impactful causes and directing resources there, and (b) the low operating costs of research and outreach.</li></ul><p><strong>Salary</strong> offered will be competitive, and dependent on ability and relevant experience.</p><p>Other benefits include&nbsp;<strong>remote work</strong>,&nbsp;<strong>flexible hours</strong>, and a chance to become a&nbsp;<strong>leader&nbsp;</strong>within the organization and an&nbsp;<strong>influential member</strong> of the EA community as we grow.</p><p>&nbsp;</p><h3><strong><u>How to Apply</u></strong></h3><p>To apply, please fill up and submit the following&nbsp;<a href=\"https://docs.google.com/forms/d/11RA5_ex2Ds4NV92YrDk__RaGfnmidtlK9wX3XE6oWh8\"><u>application form</u></a>. In terms of the overall hiring process, there will be the following stages:</p><ul><li>Stage 1: An application form (&lt;= 0.5 hours)</li><li>Stage 2: An interview (&lt;= 0.5 hours)</li><li>Stage 3: A work test (~4 days across 2 weeks \u2013 or more, if requested \u2013 with a lot of flexibility)</li></ul><p>&nbsp;</p><h3><strong><u>Q&amp;A</u></strong></h3><p><i>What causes do you work on?</i></p><ul><li>All sorts of (a) global health and wellbeing causes, (b) longtermist causes, as well as (c) meta causes.</li></ul><p><i>Should I worry about my counterfactual impact?</i></p><ul><li>No \u2013 good researchers are hard to find, and the market is competitive; you should not worry about replaceability at all.</li></ul><p><i>When should I expect to hear back?</i></p><ul><li>If we're keen on your application, we'll try to get back within 3 weeks.</li></ul><p><br><br><br>&nbsp;</p>", "user": {"username": "Joel Tan"}}, {"_id": "uAdfjCz6FvtmXZnxf", "title": "Sail Over Mountains of ICE", "postedAt": "2023-01-02T01:54:53.432Z", "htmlBody": "<p>~ <i>constant winds carry ships FAST on Arctic ice-highways, easily made!</i> ~</p><p>[cross-posted on LessWrong]</p><p><strong>TL;DR </strong>\u2014 <a href=\"https://www.youtube.com/watch?v=PE8mCZOmrEo\"><u>Ice Ships</u></a> are <a href=\"https://www.youtube.com/watch?v=EjJ9hwthYVo\"><u>frickin\u2019 insane</u></a>. With a mast and sail, a wide flat body, and innumerable skates, these sailboats are able to haul tonnage across frozen lakes and glaciers at up to <i>85 miles per hour!</i> They regularly attain 5x wind-speeds; an 8mph breeze across the North shores of Canada and Russia would propel huge fleets 40mph with <i>NO FUEL</i>. An <strong>Ice-HIGHWAY</strong> covering the beaches of the Arctic shores could replace much of <strong>international shipping</strong>! Get rid of that huge fuel-cost, AND go carbon-neutral. All while traveling more than TWICE the speed of normal container ships, along a route from Finland to just north of Japan in less than <i>five days </i>\u2014 a <i>SHORTER distance than the Suez circuit</i>! So, how do you pile all that ice onto the land? Enough ice to sail <i>over the TOP of a <strong>mountain in Siberia</strong>?</i> Easy:</p><p>Reflect the sunlight of summer with thin Mylar tarps to bake limestone, and add a little water to that lime powder in the winter (infinitely re-useable \u2018quicklime\u2019!), to power spray-nozzles held high on aluminum frames. A single square yard of Mylar, in one summer, can power enough pumping and spray to lay <i>Hundreds of TONS</i> of ice, EACH winter. In less than a decade, a few tens of thousands crewmembers could build and operate transit from northern Norway and Finland across all of Russia, inland to Yakutsk, down the coastline of the Sea of Okhotsk toward Japan, as well as across the Bering Strait to Alaska and then down the side of Hudson Bay to connect with the Great Lakes/Mississippi canal systems. Europe, Asia, and North America, with their Gigatons of shipping per year, would only need an ice-highway a few miles wide, a couple years to build at each site. That\u2019s the bulk of international shipping, which is a BIG slice of the Carbon-Pie. And speeding-up those routes increases their value, while allowing the same number of ships and crew to deliver MORE goods, making more money and offering lower shipping rates, which help everyone. Details, next\u2026</p><p><strong>Ice Sailing</strong></p><p>Ice Boats began popping-up in the Great Lakes more than a century ago; at the time, they were the fastest vehicles on earth. Huge ships were built, to haul tonnage between the major ports of the northern States and Canada, efficient to operate and able to carry heavier loads in proportion to their structural demands \u2014 the load is carried by the skates on the ice, and the wide, low frame. In contrast, a ship in <i>water</i> feels the continual surface-pressures <strong>everywhere</strong>, being submerged in water, which require all sorts of cross-members to prevent the walls from folding-inwards! Ice-boats are more \u2018<i>capital-efficient</i>\u2019 \u2014 they pay for themselves faster. Especially by traveling twice as fast, at minimum! Consider this: if your shipping company does $1,000 in business, at \u20185% margins\u2019, meaning you had to spend $950 on operations &amp; fuel, then your profits were only $50. Yet, you switch to ice-ships, delivering TWICE as many loads, WITHOUT fuel \u2014 you earn $2,000. If you still spend $950, then your profits are now $1,050\u2026 that\u2019s a lot better than $50. You\u2019re doing<i> twice the volume</i> of business, but earning <strong>21x</strong> as much profits!</p><p>The main draw-backs of ice-sailing are \u2014 poor conditions (ice-surface AND wind) and a lack of ice *connecting* important locations. Well, we can solve those with good engineering and design. By manufacturing smooth-surface ice on *land*, instead of waiting for the lakes to freeze, we can connect the high Arctic\u2026 which is *constantly* windy, due to the \u2018Polar Vortex\u2019. If we build the ice along \u2018narrow\u2019 paths to connect with major oceanic ports and rail networks, keeping the surface clear and smooth, then international shipping can go green, cut costs, and re-build a few glaciers all at once.</p><p><strong>Making Ice</strong></p><p>You\u2019ve probably seen videos of people <a href=\"https://www.youtube.com/watch?v=pTf7X_COAvM\"><u>pouring water outside</u></a> when it\u2019s really cold. That\u2019s the gist of it. But bigger. You can pump water just a short distance from shores and rivers, lakes, spraying it into the air to mix with the sub-zero chill quickly. How much ice are we talking about? Let\u2019s check: there\u2019s 1.2kg of air per m3 volume, so in a 10m/sec wind, spraying to an elevation of 10m, then EACH single meter-wide stretch of spray-wall is able to interact with 100m3 of air per second (because 10m tall sees 10m pass through it per second, the wind speed), which is 120kg of air. If the air temperature is -10c then that\u2019s &gt;40kJ/kg of heat it can absorb, even from near-freezing waters. 40kJ/kg\u2026 multiplied by 120kg/sec\u2026 is <strong>4.8 MEGAjoules of heat-absorption per SECOND</strong>. And that\u2019s just from a 1-meter long stretch of a spray-wall covering the shoreline.</p><p>But, how much energy would we be *using* to pump that spray? The air steals-away 4.8 MegaWatts for us, but if we have to *spend* megawatts to do it, it\u2019s no benefit. Well, 4.8 Megajoules is enough to freeze 15kg of water\u2026 so a single meter length of spray-wall would need to pump 15kg every second, during the windy winter. Spraying to a height of 10m requires 100joules/kg\u2026 so 15kg/sec spray needs only 1.5kW \u2014 that\u2019s a <i><strong>2 horsepower pump</strong></i>. Dinky!</p><p>Recap for a moment, before we tally up the big numbers: We use a 2 horsepower pump to spray 15kg of water every second, up to 10m into the air, which absorbs 4.8 Megajoules of heat into the air every second. And that is from a 1 meter-wide section of aluminum-scaffold spray-wall. Spending 1,500 joules every second, we freeze <i>4,800,000</i> joules every second; that is, for every <i>ONE</i> unit of energy we spend, we receive <strong>3,200</strong> units of energy toward ice-formation. <strong>3,200-</strong><i><strong>times</strong></i><strong> more </strong>is a pretty good energy ratio. :)</p><p>So, if that 1 meter wide section of aluminum spray-wall is pumping 15kg a second, for the long 6 months of cold above the Arctic Circle, then that\u2019s 15,000,000 seconds, for <i><strong>225,000 TONS</strong></i> of ice, in ONE winter. You could let the wind lay the drift of frozen droplets a mile-<i>wide</i>, and it would still stack, compressed, 400 feet DEEP. From a sprayer &amp; frame only three feet wide.</p><p>There are around 10,000km of coastlines, scoured by glaciers, with sparse vegetation that grows only 80 days a year, minimal diversity, barren. We should put those glaciers BACK, in a way that gets rid of a chunk of global carbon exhaust! 10,000km of spray-nozzles and aluminum frames, weighing perhaps a few hundred kg per meter of distance traveled, would weigh an entire Megaton in total. That is actually <i>small</i>, compared to global construction \u2014 the Three Gorges Dam was <i><strong>FIFTY</strong></i><strong> Megatons</strong> of concrete. This spray-wall equipment would be small-potatoes, in terms of infrastructure demands, while they would *create* a massive <i>ice</i>-infrastructure at low cost. 225,000 <i>Tons</i> of ice EACH year, from a few pounds of Mylar and framing, covering only 1 meter of shoreline. For a whole 10,000km highway, that\u2019s 2.25 TRILLION tons of ice per year.</p><p>To avoid hauling-in fuel for the pumps, we need to *re-use* limestone. Once you bring enough tons of limestone to the site, you <strong>NEVER</strong> need to bring more fuel. Each summer, Mylar reflectors gently bake the limestone, causing a chemical reaction that converts it into \u2018quicklime\u2019. This can be stored safely, without LOSING any energy, until deep in winter. When you add water to the quicklime, the chemical reaction reverses, releasing HEAT to power an electric generator. Once the quicklime has absorbed all the water, it has converted back into hydrated lime. Just bake it next summer, and you can use it all over again, a stable thermal battery at ultra-low cost. Limestone is also found in huge deposits scattered all over the place, so you don\u2019t have to ship supplies from far away, and there won\u2019t be any shortages. (Entire <i>regions </i>of continents are covered hundreds of feet deep, and you never need to get MORE because you keep <i>re-using </i>the original lime you brought.) Quicklime is NOT an efficient fuel \u2014 but it\u2019s scalable, local, stupefyingly cheap, and Mylar makes the summer sun an efficient *local* source of power. In the summer, the sun is out extra-long, so tilting arrays of Mylar works even better than normal!</p><p>Sergei Yurko has been testing <a href=\"https://www.youtube.com/watch?v=5eKgn_zlOCA\"><u>innumerable</u></a> innovative designs for cheap, simple Mylar power systems \u2014 and he has costs down to less than a dollar per square meter. To power the 1.5kW spray mentioned earlier, we would need 150 meters depth of mylar reflectors, running parallel to the ice-highway initially. Once the ice has been laid, move the solar reflectors on TOP of the ice, during the summer, to keep the ice shaded and cool. The amount of sunlight <strong>reflected</strong> equals \u2018energy that you DON\u2019T need to spend <i>re-freezing</i> the next winter\u2019; Mylar reflectors count *double* as fuel AND <i>shade</i> that prevents melting. For the 10,000km of ice-highway, that\u2019s 1.5 billion m2 of Mylar, at Sergei\u2019s price of $1 per m2: $1.5 Billion, to power the creation and maintenance of 6,000 miles of super-highway. That is dirt-cheap infrastructure! Boston\u2019s big tunnel cost $14 Billion, nearly <i>TEN times more expensive</i> than Sergei\u2019s solar reflectors for the <strong>entire</strong> ice-highway I propose. The spray-equipment for making the ice will cost a bit more, but that still puts the project well below the cost of Texas\u2019 new sea-wall upgrade billed at <a href=\"https://www.texastribune.org/2022/06/09/texas-ike-dike-hurricanes-house-vote/#:~:text=U.S.%20House%20approves%20massive%20%2431,protect%20Texas%20coast%20from%20hurricanes\"><u>$30 Billion</u></a>, to protect some portions of their coastline.</p><p><strong>Volumes of Traffic and Efficiencies</strong></p><p>If a ship hauls 500 tons at 50km/hr from North of St. Petersburg across some 10,000km of ice to Yakutsk, then over the mountains to Okhotsk, it will take them 200 hours to cover the distance \u2014 just <i>four days</i> and four hours. So, if ships are \u2018crowded\u2019 in their lane at a distance of 2km highway per ship (a stopping-distance of 2km would give them more than two minutes to maneuver or halt)\u2026 then we have 5,000 ships x2 directions, each hauling 500 tons every five days. (10,000 ships x 500 tons x 70 trips/yr) = 350 Megatons/yr delivered. And, that\u2019s per *lane* of shipping. An ice highway can be miles wide, quickly, providing DOZENS of lanes, frequent rest stops, etc. If most of the materials moving between Asia, Europe, and North America are upon ice-floes, that immense volume of <i>traffic</i> will be worth providing services en-route. With a bit more than 10 billion tons of goods loaded on ships every year, globally, we can split those between the Russian and Canadian lines \u2014 then, each ice-highway needs only 15 lanes to carry everything we currently carry on the seas! Of course, a large portion of global shipping is still necessary for everywhere ELSE, so I would consider the ice-highway \u2018completed\u2019 with just ten lanes in each direction. If each lane is 200m wide, that\u2019s 4km. Add the slopes on either side, which melt more in the summer \u2014 a 3-mile wide glacier-highway is plenty for the whole world.</p><p>That\u2019s 50,000 km2 of surface to maintain, across the Finland-to-Yakutsk route alone. But, you can plant enormous Zambonis on such thick solid ice, packed onto the frozen ground. If you Zamboni 5 square meters per second, then in a 2,000hr worker-year (7.2 million seconds) you\u2019ll have touched-up 36 square kilometers \u2014 about 14 square miles per driver, each year. So, the entire highway\u2019s maintenance needs 3,500 Zamboni-drivers, to sustain 10 Gigatons of shipping\u2026 an average of 1 driver-year\u2019s salary and mech, for every <i><strong>3 Million TONS</strong> </i>of cargo. That\u2019s cheap maintenance! And, every year the sun bakes more limestone, to pump more water, to make the ice that much <strong>thicker</strong>. Scaling is easy.</p><p>[[Oh, and while we\u2019re at it: can we please put massive arrays of Nuclear Reactors in the center of Greenland? Out on top of 2 miles of ice, any melt-down will become frozen, held safe. So, we don\u2019t have to over-engineer for safety as much, which brings costs down *dramatically*. We could cheaply build dozens of Gigawatts of power out there, and use that energy, plus air and water on-site, to produce AMMONIA \u2014 the production of ammonia for fertilizers globally uses a big chunk of our energy, carbon emissions! Denmark owns Greenland, and I hear they like green energy\u2026 if you know of any Danes, I\u2019d love to nudge their brains in that direction!]]</p>", "user": {"username": "Anthony Repetto"}}, {"_id": "SnLgoGPoSXtDWiodF", "title": "January Online Events & Resources to Plan Your Year and Career", "postedAt": "2023-01-01T19:59:27.238Z", "htmlBody": "<p>Happy New Year!</p><p>As we enter 2023, I want&nbsp; to spotlight a few resources from throughout the EA community for personal and professional planning, as well as some ways to get more connected with the EA community this year.</p><p>&nbsp;</p><h2>Events</h2><p>On January 2nd, EA Anywhere will be hosting a&nbsp;<a href=\"https://forum.effectivealtruism.org/events/knfziP9D6XsmjwFxF/yearly-review-and-planning-session\"><u>Yearly Review and Planning Session</u></a>. This is a great opportunity to reflect on your accomplishments from the past year and set goals for the upcoming year.&nbsp;<a href=\"https://yearcompass.com/\"><u>Resources</u></a> and guidance will be provided to assist participants in creating a roadmap for success in 2023.</p><p>&nbsp;</p><p>On January 5th, EA NYC will be hosting an&nbsp;<a href=\"https://forum.effectivealtruism.org/events/v9ENAkGg833DEc9ox/online-new-year-s-workshop\"><u>Online New Year\u2019s Workshop</u></a>. Modeled after our session from previous years, this event will focus on setting and achieving new year\u2019s resolutions and systems to help them actually stick. We will make our way through a series of guided exercises and discussions to hone in on our personal and professional goals for the year, with a focus on how each of us can have the greatest impact in the years to come. You can register to join&nbsp;<a href=\"https://airtable.com/shrkzZJhFMm0Tviq2\"><u>here</u></a>.</p><p>&nbsp;</p><p>On January 22nd, EA NYC is also hosting a&nbsp;<a href=\"https://forum.effectivealtruism.org/events/DeTW4phcTftHeWuzf/career-planning-discussion-with-matt-reardon-80-000-hours\"><u>Career Planning Discussion</u></a>, featuring 80,000 Hours advisor, Matt Reardon. This event is designed for a variety of different skill sets and career stages to help individuals looking to take the next step in their careers. Matt will give a brief overview of 80,000 Hours' four-step career planning process and then invite discussion on how it might apply to different situations.</p><p>&nbsp;</p><p>I think these events will be really valuable for people looking to take immediate action towards achieving their goals in the new year. If you have any questions, please don\u2019t hesitate to reach out to&nbsp;<a href=\"mailto:alexrk@effectivealtruism.nyc\"><u>myself</u></a> or&nbsp;<a href=\"https://forum.effectivealtruism.org/groups/YeW2gwh4gHexYQBjs\"><u>other organizers</u></a><u>;</u> and if you\u2019re aware of any upcoming events I missed, please flag them in the comments!</p><p>&nbsp;</p><h2>Resources</h2><p>I also wanted to share a few resources to help with independent (or&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/2RvpoWWQDiFpptpam/accountability-buddies-a-proposed-system\"><u>not</u></a>!) planning for the new year.</p><ul><li>Chris Sparks at&nbsp;<a href=\"https://www.forcingfunction.com/\"><u>forcingfunction.com</u></a> has created&nbsp;<a href=\"https://www.forcingfunction.com/worksheets\"><u>a few helpful worksheets</u></a> - including annual, quarterly, and daily planners - that are all available for free.</li><li>Alex Vermeer's guide,&nbsp;<a href=\"https://alexvermeer.com/8760hours/\"><u>8,760 Hours</u></a>, is another great resource for reflecting, planning, and optimizing your next year. I\u2019ve personally used it a few times and found it helpful for clarifying my priorities.</li><li><a href=\"https://www.eac-network.com/\"><u>EA Consulting Network</u></a> has launched a new&nbsp;<a href=\"https://docs.google.com/document/d/1EP7gsZcCIFxV107mdhPBYnKF6bfTHUjxfKjE_iRP6g4/edit\"><u>career check-in template</u></a> and&nbsp;<a href=\"https://80000hours.org/\"><u>80,000 Hours</u></a> offers an online<a href=\"https://80000hours.org/career-planning/annual-career-review/\"><u> annual career review tool</u></a>.</li><li>Lastly,&nbsp;<a href=\"https://twitter.com/ben_j_todd\"><u>Ben Todd</u></a>, co-founder of 80,000 Hours, has also developed an annual review process; you can find a simpler 2019 version of it&nbsp;<a href=\"https://docs.google.com/document/d/1AQNLfSu_adtXy0Cd9QUzUv8J55G3Zac1zN5DD1_mSX8/edit\"><u>here</u></a> or an upgraded 2020 version&nbsp;<a href=\"https://docs.google.com/document/d/1NbhmiIzPa3AKucHvdBRAEmZ4YxzpcX8YAqK5AYtV4E0/edit\"><u>here</u></a>!</li></ul><p>&nbsp;</p><p>I hope these resources will be helpful as you plan and work towards achieving your ideal 2023.</p>", "user": {"username": "Alex R Kaplan"}}, {"_id": "zoZvM6oQf5HJWmaLx", "title": "To Share the Hymns of Love and Truth", "postedAt": "2023-01-01T13:13:57.336Z", "htmlBody": "<p>I have the desire to express some of my positive emotions regarding EA publicly on the forum here and cheerlead for everyone doing good for a bit. Probably this is because I\u2019m still a bit drunk from New Years Eve and because it brought me into a reflective mood.</p><p>This is the first post on this forum for me and I don't think it will be quite high quality compared to other post being made here. It will not have tons of carefully crafted reasearch behind it but just authentic feelings and (hopefully) at least slightly motivating prose. However I was <a href=\"https://forum.effectivealtruism.org/posts/4WxHNBf5LeM9gQneT/you-should-write-on-the-ea-forum\">explicitly encouraged</a> so why not try it.</p><p>I feel like in can be hard to express big personal feelings, especially in a rather uncontrolled social space (like a big forum full of people you would like to impress with your coolheaded rationality in the future). However, in the better world I want to see become true, I would like it to be a widely established tradition to not be afraid to speak what one thinks and to always be bold and truthful even when it can be socially uncomfortable. Therefore, here have my <a href=\"https://en.wikipedia.org/wiki/Maitr%C4%AB\">Mett\u0101</a> and my Love. I hope you like it. (Luckily the anonymity of the internet softens this whole difficulty somewhat :)</p><p>Anyone in this forum, this community and this world who wants to make all of this a slightly less dreadful place and tries to turn that wish into a reality has so much of my respect and love that I can hardly express it with words. I wish everyone here and on this whole world who wants to do good all the greatest successes this year and every year from the deepest depths of my heart. May you recognize the truth of this universe as well as you can, may your moral intuitions guide you through the dark woods of moral dillemas in this world full of suffering, may your will (if free or not) be strong enough to resist all the temptations of delusion, hopelessness and ego and if you should ever fail with any of these may you not give up but rise to try again.&nbsp;</p><p>I feel this often, but you can\u2019t express it too much too often or people would probably start finding it a bit clich\u00e9. But I wanted to express it now at least.</p><p>I want to share a few additional things. I really like art and I feel art can often be the best way of spreading a certain kind of feeling and that it is one of the few good brainhacks we have to better connect with each other emotionally. I also think that having a branch of EA focused on singing the hymns of love and truth so that all may get hope from it is highly valuable and also the reason why I write like a weirdo in this post. I will possibly write a more detailed and researched post about this at some point but for now I just want to share some art that I find very EA aligned.</p><p>The first thing I wanted to share is the poem <a href=\"https://www.youtube.com/watch?v=Au056Spqwew\">Good Bones</a> , which encapsulates to me a lot about how we can have hope even when there is so much bad in this world. I really like this poem a lot.</p><p>The second is this <a href=\"https://open.spotify.com/playlist/2ivUS0ZnXU4YpkRbzJcjsJ?si=74ff0277304c4fed\">music playlist</a> (write me if you are interested and don\u2019t have Spotify), which has some of my favourite songs on it, which are (to me at least) about the strength, hope and faith Humanity and we all will need on this long hard journey ahead.</p><p>If any of you liked any of that or even if you didn\u2019t like it, feel free to pm me about it. I would really like to considerably raise the amount of 1on1s I have with EAs and other good aligned people.&nbsp;</p>", "user": {"username": "AlephHead"}}, {"_id": "btFBFdYEn2PbuwHwt", "title": "Summary of 80k's AI problem profile", "postedAt": "2023-01-01T07:48:27.903Z", "htmlBody": "<p><i>This is a summary of the article \u201c</i><a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\"><i><u>Preventing an AI-related catastrophe</u></i></a><i>\u201d (audio version&nbsp;</i><a href=\"https://podcasts.apple.com/us/podcast/preventing-an-ai-related-catastrophe-article/id1245002988?i=1000582699751\"><i><u>here</u></i></a><i>) by </i><a href=\"https://80000hours.org/author/benjamin-hilton/\"><i>Benjamin Hilton</i></a><i> from </i><a href=\"https://80000hours.org/\"><i><u>80,000 Hours</u></i></a><i> (80k). The article is quite long, so I have attempted to capture the key points without going through all the details. I\u2019m mostly restating ideas from the article in my own words, but I'm also adding in some of my own thoughts. This piece doesn\u2019t represent 80k\u2019s views.</i></p><h1>TL;DR</h1><p>The field of AI is advancing at an impressive rate. However, some AI experts think AI could cause a catastrophe on par with human extinction. It\u2019s possible that advanced AI will seek power and permanently disempower humanity, and advanced AI poses other serious risks such as strengthening totalitarian regimes and facilitating the creation of dangerous technologies. These are seriously neglected problems, and there are ways you can help.</p><h1>Researchers are making advances in AI extremely quickly</h1><p>One example of recent AI progress is&nbsp;<a href=\"https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html\"><u>Minerva</u></a>, which can solve&nbsp;<a href=\"https://minerva-demo.github.io/\"><u>a variety of STEM problems</u></a>, including math problems:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1672559308/mirroredImages/btFBFdYEn2PbuwHwt/e6yxpylcupzumpooyx5n.png\"><figcaption>A <a href=\"https://arxiv.org/abs/2103.03874\">MATH dataset</a> problem followed by Minerva's correct solution.</figcaption></figure><p>Before Minerva, the highest AI accuracy on the&nbsp;<a href=\"https://arxiv.org/abs/2103.03874\"><u>MATH dataset</u></a> of challenging high school math problems was ~7%, meaning that you could expect an AI to answer 7 out 100 questions correctly. In August 2021,&nbsp;<a href=\"https://bounded-regret.ghost.io/ai-forecasting-one-year-in/\"><u>one team of forecasters</u></a> predicted that it would take over four years for an AI to exceed 50% on MATH (better than most humans&nbsp;<a href=\"https://bounded-regret.ghost.io/ai-forecasting-one-year-in/\"><u>according to one creator of the dataset</u></a>), but Minerva ended up achieving this milestone in less than one year, four times faster than predicted. Minerva is just one of <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#making-advances-extremely-quickly\"><u>many</u></a>&nbsp;<a href=\"https://globalchallengesproject.notion.site/Current-capabilities-of-machine-learning-systems-a0ed620299f744098cd62acec15a5d11\"><u>powerful</u></a>&nbsp;<a href=\"https://ourworldindata.org/brief-history-of-ai\"><u>models</u></a> we\u2019ve seen in the past few years alone.</p><p><a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#scaling\"><u>Current trends suggest</u></a> that AI capabilities will continue to advance rapidly. E.g., developers are using&nbsp;<a href=\"https://ourworldindata.org/grapher/ai-training-computation?time=earliest..latest\"><u>exponentially increasing</u></a> amounts of computational resources to train AI systems.</p><figure class=\"media\"><div data-oembed-url=\"https://ourworldindata.org/grapher/artificial-intelligence-training-computation?time=1980-04-01..2022-07-01\">\n\t\t\t\t\t<div data-owid-slug=\"artificial-intelligence-training-computation\" class=\"owid-preview\">\n\t\t\t\t\t\t<iframe src=\"https://ourworldindata.org/grapher/artificial-intelligence-training-computation?time=1980-04-01..2022-07-01\">\n\t\t\t\t\t</iframe></div>\n\t\t\t\t</div></figure><p>Additionally, AI algorithms are requiring&nbsp;<a href=\"https://openai.com/blog/ai-and-efficiency/\"><u>exponentially lower amounts of computational resources to achieve a given level of performance</u></a>, demonstrating progress in the efficiency of AI algorithms. Given that efficient algorithms and computational resources are two crucial drivers of AI progress, we will likely see more rapid progress in AI if these trends continue.</p><p><a href=\"https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/\"><u>Some</u></a>&nbsp;<a href=\"https://www.openphilanthropy.org/research/report-on-semi-informative-priors/\"><u>reports</u></a>&nbsp;<a href=\"https://astralcodexten.substack.com/p/biological-anchors-a-trick-that-might'\"><u>and</u></a>&nbsp;<a href=\"https://arxiv.org/pdf/2206.04132.pdf\"><u>surveys</u></a> have attempted to predict when we will see AI sufficiently advanced to be \u201c<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#when-can-we-expect-to-develop-transformative-AI\"><u>hugely transformative for society</u></a> (for better or for worse).\u201d It\u2019s difficult to synthesize these perspectives and predict the future (indeed, one report author recently&nbsp;<a href=\"https://www.alignmentforum.org/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines\"><u>revised her predictions</u></a>), but there seems to be a&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#when-can-we-expect-to-develop-transformative-AI\"><u>nontrivial chance</u></a> of seeing \u201ctransformative\u201d AI within the next 15 years, so it\u2019s important to start preparing for the risks.</p><h1>Many AI experts think there\u2019s a non-negligible chance AI will lead to outcomes as bad as extinction</h1><p>In multiple surveys, the median AI researcher gave <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#experts-are-concerned\"><u>probabilities between 2-5%</u></a> of advanced AI causing catastrophes as severe as the death of all humans. (However, there is notable variance in opinion: in a&nbsp;<a href=\"https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/#Summary_of_results\"><u>2022 survey</u></a>, \u201c48% of respondents gave at least 10% chance of an extremely bad outcome,\u201d while another \u201c25% put it at 0%.\u201d)&nbsp; Leading AI labs like DeepMind and OpenAI already house teams working on AI&nbsp;<a href=\"https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1\"><u>safety</u></a> or \u201c<a href=\"https://openai.com/alignment/\"><u>alignment</u></a>.\u201d Several centers for academic research focus on AI safety; e.g.,&nbsp;<a href=\"https://humancompatible.ai/\"><u>UC Berkeley\u2019s Center for Human Compatible AI</u></a> is led by Stuart Russell, coauthor of&nbsp;<a href=\"https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach\"><u>arguably the most popular AI textbook</u></a> for university courses around the world.</p><h1>Power-seeking AI could pose an&nbsp;<a href=\"https://en.wikipedia.org/wiki/Global_catastrophic_risk\"><u>existential threat</u></a> to humanity</h1><h2>Advanced planning AIs could easily be \u201cmisaligned\u201d</h2><p>In the future, humanity might possibly build AI systems that can&nbsp;<a href=\"https://arxiv.org/pdf/2206.13353.pdf#page=8\"><u>form sophisticated plans to achieve their goals</u></a>. The&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#aps-systems\"><u>best AI planners</u></a> may understand the world well enough to strategically evaluate a wide range of relevant obstacles and opportunities. Concerningly, these planning systems could be extremely capable at tasks like persuading humans and hacking computers, allowing them to execute some of their plans.</p><p>We might expect these AI systems to exhibit&nbsp;<i>misalignment</i>, which&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#instrumental-convergence\"><u>80k defines</u></a> as \u201caiming to do things that we don\u2019t want them to do\u201d (note there are&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#fn-30\"><u>many different definitions</u></a> of this term elsewhere). An advanced planner might pursue instrumental subgoals that are useful for achieving its final goal, in the same way that travel enthusiasts will work to earn money in order to travel. And with sufficient capacity for strategic evaluation, these instrumental subgoals&nbsp;<a href=\"https://drive.google.com/file/d/1FVl9W2gW5_8ODYNZJ4nuFg79Z-_xkHkJ/view?usp=sharing\"><u>may include</u></a> aims like self-preservation, capability enhancement, and resource acquisition. Gaining power over humans might be a promising plan for achieving these instrumental subgoals.</p><p>There are some challenges for preventing misalignment. We might try to control the objectives of AI systems, but we\u2019ll need to be careful.&nbsp;<a href=\"https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity\"><u>Specification gaming</u></a>&nbsp;<a href=\"https://tinyurl.com/specification-gaming\"><u>examples</u></a> show that the goals we actually want are difficult to code precisely. E.g. programmers rewarded an AI for earning points in a boat racing video game, so&nbsp;<a href=\"https://openai.com/blog/faulty-reward-functions/\"><u>it ended up spinning around in circles</u></a> instead of completing the race.</p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=tlOIHko8ySg\"><div><iframe src=\"https://www.youtube.com/embed/tlOIHko8ySg\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p>Additionally, in&nbsp;<a href=\"https://tinyurl.com/goal-misgeneralisation\"><u>goal misgeneralization examples</u></a>, an AI appears to learn to pursue a goal in its training environment but&nbsp;<a href=\"https://www.deepmind.com/blog/how-undesired-goals-can-arise-with-correct-rewards\"><u>ends up pursuing an undesired goal when operating in a new environment</u></a>. <a href=\"https://arxiv.org/pdf/2209.00626.pdf\">This paper</a> explores the difficulties of controlling objectives in greater detail.</p><p>Another challenge is that some alignment techniques might produce AI systems that are substantially less useful than systems lacking safety features. E.g., we might prevent power seeking with systems that stop and check with humans about each decision they make, but such systems might be substantially slower than their more autonomous alternatives.&nbsp; Additionally, some alignment techniques might work for current AI systems but break down for more advanced planners that can identify the technique\u2019s loopholes and failures. For more difficulties in preventing misalignment, see section 4.3 of&nbsp;<a href=\"https://arxiv.org/pdf/2206.13353.pdf#page=22\"><u>this report</u></a>.</p><h2>Advanced, misaligned AI could cause great harm</h2><p>If we build advanced AI systems that seek power, they may defeat our best efforts to contain them. 80k lists&nbsp;<a href=\"https://80000hours.org/articles/what-could-an-ai-caused-existential-catastrophe-actually-look-like/#actually-take-power\"><u>seven concrete techniques</u></a> that AIs could use to gain power. For instance, Meta\u2019s&nbsp;<a href=\"https://ai.facebook.com/research/cicero/\"><u>Cicero</u></a> model demonstrated that AIs can successfully negotiate with humans when it reached human-level performance in&nbsp;<a href=\"https://en.wikipedia.org/wiki/Diplomacy_(game)\"><u>Diplomacy</u></a>, a strategic board game. Thus, an advanced AI could manipulate humans to assist it or trust it. In addition, AI is swiftly becoming proficient at writing computer code with models like&nbsp;<a href=\"https://openai.com/blog/openai-codex/\"><u>Codex</u></a>.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1672559308/mirroredImages/btFBFdYEn2PbuwHwt/yvvhmyv0i1migrmqlozp.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1672559309/mirroredImages/btFBFdYEn2PbuwHwt/d7pkjffxvblileef1yz5.png 150w, https://res.cloudinary.com/cea/image/upload/v1672559309/mirroredImages/btFBFdYEn2PbuwHwt/hrk0mtf37fopkrvxxa9w.png 300w, https://res.cloudinary.com/cea/image/upload/v1672559309/mirroredImages/btFBFdYEn2PbuwHwt/gcnhzxd52rriqikkd6ve.png 450w, https://res.cloudinary.com/cea/image/upload/v1672559309/mirroredImages/btFBFdYEn2PbuwHwt/cgdbhpcpflqwoy1yq7m7.png 600w, https://res.cloudinary.com/cea/image/upload/v1672559309/mirroredImages/btFBFdYEn2PbuwHwt/gssvulgzuelxs36ss5o5.png 750w, https://res.cloudinary.com/cea/image/upload/v1672559309/mirroredImages/btFBFdYEn2PbuwHwt/n8qlcwuqufvcety3m8n5.png 900w, https://res.cloudinary.com/cea/image/upload/v1672559309/mirroredImages/btFBFdYEn2PbuwHwt/taokvyxekqdifl2lj2ze.png 1050w, https://res.cloudinary.com/cea/image/upload/v1672559309/mirroredImages/btFBFdYEn2PbuwHwt/k3mflnkjpiespmhqhgoh.png 1200w, https://res.cloudinary.com/cea/image/upload/v1672559309/mirroredImages/btFBFdYEn2PbuwHwt/ethpyvmhnjjl4spyu5c5.png 1350w, https://res.cloudinary.com/cea/image/upload/v1672559308/mirroredImages/btFBFdYEn2PbuwHwt/pk1hixrbk2dftyuhsxyv.png 1412w\"><figcaption>See this Codex demo starting at <a href=\"https://youtu.be/pdSfgRYy8Ao?t=867\">14:27 of this video</a></figcaption></figure><p>Combined with models like&nbsp;<a href=\"https://openai.com/blog/webgpt/\"><u>WebGPT</u></a> and&nbsp;<a href=\"https://www.adept.ai/act\"><u>ACT-1</u></a>, which can take actions on the internet, these coding capabilities suggest that advanced AI systems could be formidable computer hackers. Hacking creates a variety of opportunities; e.g., an AI might steal financial resources to purchase more computational power, enabling it to train longer or deploy copies of itself.</p><p>Possibilities like these suggest that, at a sufficiently high capability level, misaligned AI &nbsp;<a href=\"https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/\">might succeed at <u>disempowering humanity</u></a>, and it\u2019s possible that humanity would remain disempowered forever in cases like human extinction. Thus, disempowerment could result in losing most of the value of the entirety humanity's future, which would constitute an&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#lost-control\"><u>enormous tragedy</u></a>.</p><h2>People might deploy misaligned AI systems despite the risk</h2><p>Despite the substantial risks,&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#incentives-and-deception\"><u>humanity might still deploy misaligned AI systems</u></a>. The systems might superficially appear to be safe, because an advanced planner might pretend to be aligned during training in order to seek power after deployment. Furthermore, various factors could encourage groups to be the first to develop advanced AI \u2013 e.g., the first company to develop advanced AI might reap substantially greater profits than the second company to do so. Competitive dynamics like these&nbsp;<a href=\"https://arxiv.org/abs/1907.04534\"><u>could cause AI companies to underinvest in safety</u></a> and race towards deployment,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/cXBznkfoPJAjacFoT/are-you-really-in-a-race-the-cautionary-tales-of-szilard-and\"><u>even for organizations dedicated to building safe AI</u></a>.</p><h1>Even if we find a way to avoid power-seeking, there are still risks</h1><p>There are still other serious risks that don\u2019t require power-seeking behavior. For one, it\u2019s unclear whether AI will increase or decrease the&nbsp;<a href=\"https://www.sipri.org/sites/default/files/2020-06/artificial_intelligence_strategic_stability_and_nuclear_risk.pdf\"><u>risk of a nuclear war</u></a>. Another troubling example is totalitarian regimes using AI for surveillance to ensure that citizens are obedient. AI could easily power the development of harmful new technologies, such as bioweapons and other weapons of mass destruction \u2013 notably, a single malicious actor might be able to use AI for this purpose and cause harm, since open source initiatives like&nbsp;<a href=\"https://huggingface.co/\"><u>Hugging Face</u></a> and&nbsp;<a href=\"https://stability.ai/\"><u>Stability AI</u></a> put AI models within reach of anyone with a laptop and internet access.</p><h1>Counterarguments</h1><p>80k analyzes a long list of counterarguments, with a distinction between&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#good-responses\"><u>less persuasive objections</u></a> (e.g. \u201cWhy can\u2019t we just unplug an advanced AI?\u201d) and \u201c<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#best-arguments-against-this-problem-being-pressing\"><u>the best arguments we\u2019re wrong</u></a>.\u201d</p><h1>Neglectedness</h1><p>The problem remains highly neglected and needs more people. 80k estimates that AI capabilities research receives&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#neglectedness\"><u>~1,000 times</u></a> more money than AI safety work. They also estimate that only <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#fn-3\">~400 researchers</a> are working directly on AI alignment.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1672559308/mirroredImages/btFBFdYEn2PbuwHwt/pcxtzbiwq1tcqqmelvza.png\"><figcaption>An&nbsp;<a href=\"https://www.effectivealtruism.org/articles/introduction-to-effective-altruism#helping-to-create-the-field-of-ai-alignment-research\"><u>older estimate</u></a> of the number of alignment researchers</figcaption></figure><h1>What you can do concretely to help</h1><p>Consider career paths like&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#technical-ai-safety-research\"><u>technical AI safety research</u></a> or&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#ai-governance-research-and-implementation\"><u>AI governance research and implementation</u></a>. For alternative options, see the sections \u201c<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#complementary-yet-crucial-roles\"><u>Complementary (yet crucial) roles</u></a>\u201d and \u201c<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#other-ways-to-help\"><u>Other ways to help</u></a>.\u201d For feedback on your plans, consider applying to&nbsp;<a href=\"https://80000hours.org/speak-with-us/\"><u>speak with 80k\u2019s career advising team</u></a>.</p><hr><p><i>Thanks to the following people for feedback: Aryan Bhatt, Michael Chen, Daniel Eth, Benjamin Hilton, Ole Jorgensen, Michel Justen, Thomas Larsen, Itamar Pres, Jacob Trachtman, Steve Zekany. I wrote this piece as part of the&nbsp;</i><a href=\"https://www.lesswrong.com/posts/y2CMx3DAJ86HGrjue/announcing-the-distillation-for-alignment-practicum-dap-1\"><i><u>Distillation for Alignment Practicum</u></i></a><i>.</i></p>", "user": {"username": "jakubkraus07@gmail.com"}}, {"_id": "5eBtFWSLPRizmm93B", "title": "Challenge to the notion that anything  is (maybe) possible with AGI", "postedAt": "2023-01-01T03:57:04.250Z", "htmlBody": "", "user": {"username": "remmelt"}}, {"_id": "qm3DQ76xLEFmbLqqG", "title": "Suggestions for effective homelessness donations in San Francisco?", "postedAt": "2023-01-01T03:13:33.871Z", "htmlBody": "<p>I don't pretend that this is likely to be highest impact in terms of maximizing global utility, but I have a subjective preference to contribute to causes which help mitigate homelessness in San Francisco (alongside my donations to higher impact causes). Does anyone have suggestions for charities that would be worthwhile recipients?</p>", "user": {"username": "sjsjsj"}}, {"_id": "gfj7FMKz9e8CKXqSQ", "title": "Your 2022 EA Forum Wrapped \ud83c\udf81", "postedAt": "2023-01-01T03:10:59.494Z", "htmlBody": "<p>The EA Forum team is excited to share <a href=\"https://forum.effectivealtruism.org/wrapped\"><strong>your personal \u2728 2022 EA Forum Wrapped \u2728</strong></a>. We hope you enjoy this little summary of how you used the EA Forum as you ring in the new year with us. Thanks for being part of the Forum!</p><p><i>Note: If you don't have an EA Forum account, we won't be able to make a personalized \"wrapped\" for you. If you feel like you're missing out, today is a great day to make an account and participate more actively in the online EA community!</i></p>", "user": {"username": "Sharang Phadke"}}, {"_id": "YKqKv8tckMHkmFB4z", "title": "Summary of and thoughts on \"Dark Skies\" by Daniel Deudney", "postedAt": "2022-12-31T20:28:01.487Z", "htmlBody": "<p><i>This is my summary of the book \u201cDark Skies: Space Expansionism, Planetary Geopolitics, and the Ends of Humanity</i>,\u201d<i> by Daniel Deudney</i>.<i> It was published in 2020, but I haven\u2019t seen many people interested in longtermism or effective altruism engage with the book. I wrote this primarily to organise my own thoughts about the book, and I thought it was worth sharing. I give my impression of the strengths and weaknesses of the book, though it\u2019s possible some of my interpretations are idiosyncratic. I am condensing a long, detailed book to make a short summary, so I\u2019m inevitably leaving out important parts of the argument and drawing out what seemed most central to me.</i></p><p><i><strong>TL;DR:</strong> The book offers a provocative and thoughtful thesis for longtermists to consider, though it is overly long and has some notable flaws.</i></p><h2><strong>Basic thesis of the book&nbsp;</strong></h2><p>Deudney believes human space settlement and technological expansion into space is itself an existential risk, arguing against those who say it diminishes existential risk. Those who advocate for \u201cspace expansionism\u201d erroneously ignore the serious and likely dangers, in his view. The book argues that humanity should therefore actively restrain its ambitions of expanding into space for at least the next few centuries and possibly forever.</p><p><strong>My general thoughts on the book\u2019s quality:</strong> Worth reading for longtermists or anyone interested in existential risks, space settlement, or space governance</p><h2><strong>Downsides&nbsp;</strong></h2><p>It is at times unhelpfully polemical, and Deudney does not apply as much criticism to his own ideas as he does to those of others. He takes some cheap shots at his ideological opponents. He often repeats the same thought in multiple ways in the same section, and some sections repeat previous sections\u2019 ideas unnecessarily. It could probably have been just as informative at half the length.</p><h2><strong>Upsides&nbsp;</strong></h2><p>Nevertheless, it is still highly dense with ideas, probably more so than 90 percent of books. And it is very thoughtful and provocative about existential risk and humanity\u2019s long-term future. It takes seriously moral and political questions about humanity\u2019s future and speculative possibilities in a way that few outside the longtermism canon typically do.</p><h2><strong>How I read it</strong></h2><p>I listened to the audiobook at 1.5x speed. I usually do 1.8x or 2x speed, but the nature of the writing made it too risky to go this fast \u2014 it\u2019s easy to miss key steps in his arguments. This is unfortunate since a lot of it is repetitive. The audiobook is 20h40m long at 1x; the hardcover is 464 pages long.</p><h2><strong>Key takeaways</strong></h2><p><strong>Deudney argues that space exploration thus far has been net negative for humanity, despite a lot of hype to the contrary.</strong></p><ul><li>His primary argument for this is that he classifies ICBMs as space technology because, he says, they rely on the fact that these missiles are only as reliable and useful as they are because they fly in the upper atmosphere.</li><li>Thus the threat of nuclear weapons is exacerbated by space technology, which swamps out any of the benefits.</li><li>This seems plausible to some degree, but what\u2019s the point?<ul><li>It\u2019s not entirely clear what the argumentative strategy is, but I think he wants to establish that our baseline expectation should be that space expansionism is dangerous and to undercut the optimistic story that is usually told about it.</li></ul></li></ul><p><strong>Deudney basically sees the risk from nuclear weapons, and perhaps more importantly the risk from manipulating asteroid trajectories, as the great risk from space expansionism.</strong></p><ul><li>I think the argument relies mostly on versions of Mars settlemnet and communities aboard space stations within our solar system; it\u2019s possible he envisions Venus terraforming and/or colonies on nearby moons.<ul><li>He seems to believe settling other solar systems is just infeasible on any timescale worth thinking about.</li></ul></li><li>He basically argues that&nbsp;<i>the geography would make unified governance of these separate colonies impossible, they would likely be unfree, and the populations would diverge in their values and physical features</i> (because to survive in non-Earth environments would require technological and biological adaptations).</li><li>This differentiation would fuel conflict between the competing groups, and because of Earth\u2019s particularly deep gravity well, the Earth population would be at particular risk.<ul><li>Earth's deep gravity well makes it an easy target and harder to defend.</li><li>The external security threats could fuel the rise of a worldwide totalitarian government</li></ul></li></ul><p><strong>Deudney avoids the obvious cheap shot of denouncing space settlement by analogy to historical colonialism, noting that the latter was a moral atrocity because the colonised regions were already inhabited (unlike, say, Mars).</strong></p><ul><li>But he still draws an interesting lesson from colonialism, which is that even without the direct victims of colonialism, space travel could repeat the conflict between different groups of settlers who are angling to access to the new frontier.</li></ul><p><strong>In one of the weakest parts of the argument, he compares letting our descendants become a species that doesn\u2019t look like us in meaningful ways to extinction and says we\u2019d be uniquely foolish as a species to let this happen.</strong></p><ul><li>He gives almost no serious consideration to the idea that descendants of ours that may be significantly different from modern-day humans could still have lives well worth living and could enjoy a future we might be happy to create.</li></ul><p><strong>Because the dynamics of near-term space settlement would almost certainly lead to catastrophic conflict, Deudney believes we should commit to humanity remaining Earth-bound for the foreseeable future, i.e., at least a few centuries.</strong></p><ul><li>After that point, he thinks it\u2019s possible that humanity will have developed to a stage where space settlement wouldn\u2019t inevitably lead to conflict, and so it could be safe.</li><li>However, he comes close to endorsing an even more pessimistic conclusion, that it could&nbsp;<i>never</i> be safe for us to leave the planet.<ul><li>He doesn\u2019t address the concern that, if we stay Earth-bound forever, we\u2019ll eventually face (potentially premature) extinction. It\u2019s not entirely clear why he doesn\u2019t address this, but he may believe one of the following:<ul><li>1. The lifespan of inhabitable Earth is satisfactory for the human species.</li><li>2. At the end of Earth's habitable lifespan, the decision to spread into space becomes sensible.</li><li>3. There's not really any use for thinking about possibilities so far in the future; future generations will be better able to figure out what to do then anyway.&nbsp;&nbsp;</li></ul></li></ul></li></ul><h2><strong>Key omissions</strong></h2><p><strong>Though he mentions the risks from advanced AI, he doesn\u2019t spend much time addressing its potential implications for the argument.</strong></p><ul><li>He doesn't address how AI or AI timelines would affect the trajectory of space settlement</li><li>For people with short or medium timelines, this may make the book as a whole less relevant.</li></ul><p><strong>Likewise, he doesn't do much to address the possibility of digital people, aside from some potentially relevant references in his discussion of how space-faring people could evolve.</strong></p><ul><li>This seems particularly relevant since many people believe settlement of the galaxy is most likely to happen via some form of digital descendants.</li></ul><p><strong>He doesn\u2019t take seriously the argument about \u201castronomical waste\u201d or the immense levels of potential value that could be obtained by space expansionism.</strong></p><ul><li>This might be a result of essentially ignoring the possibility of digital people.</li></ul><p><strong>He doesn\u2019t sufficiently address the challenges for his own view of \u201cOasis Earth,\u201d i.e. the coercive apparatus that will be required to keep the species Earth-bound.</strong></p><ul><li>This seems like a big problem for his view, because getting everyone on board for his program of refraining from space expansion seems unlikely.</li><li>He also takes some cheap shots about space enthusiasts\u2019 aims potentially leading to disaster because the ends are so spectacular they might justify any means. But by the same token, his view about the dangers of space expansion could also lead to extreme levels of oppression under the same means-ends reasoning that he decries.</li></ul><h2><strong>Additional interesting points</strong></h2><p><strong>He believes space settlement would exacerbate AI x-risk</strong>: We wouldn\u2019t be able to prevent other colonies from developing unfriendly AI because they\u2019d be out of our direct sphere of influence, so we\u2019d be more at risk.</p><p><strong>He suggests an interesting solution to the Fermi Paradox:&nbsp;</strong>Aliens may have either expanded into space and killed themselves, or they recognized the dangers and decided to stay on their home planet. Either way, it would be no surprise that we don\u2019t see them.</p><ul><li>But does this argument actually go through? His argument seems to rely on existential risks that arise through conflict. But it\u2019s not clear why the risks he highlights, though genuinely concerning, would actually likely lead to the extinction of all parties to the conflict. And if not, it\u2019s not obvious this is actually a solution to the Fermi Paradox. The victors would still be around.</li></ul><p>Read more:</p><p>I did find one&nbsp;<a href=\"https://www.thespacereview.com/article/3985/1\"><u>thoughtful response</u></a> to Deudney\u2019s book from Al Globus, who defends space settlement.</p>", "user": {"username": "Cody_Fenwick"}}, {"_id": "dvRECorkXWmgwfzeJ", "title": "What is the best biomedical research question for undergrad research in PDEs?", "postedAt": "2022-12-31T20:01:29.955Z", "htmlBody": "<p>This summer I am doing undergraduate research in Partial Differential Equations. I am most interested in eventually starting a medical device startup focusing on the developing world (or innovate strongly in general). Does anyone know of good research topics good for knowledge of some biology (1 intro level college class) and undergrad level PDE class with some more studying beyond that with a professor? The question could be useful by itself or be good for building towards a undergrad final project, masters thesis or PhD thesis (potentially). Thanks!</p>", "user": {"username": "Isaac Benson"}}, {"_id": "zATJeaSL9v5GH6HBk", "title": "Direct Work For-Profit Entrepreneurship is Underrated", "postedAt": "2022-12-31T19:56:20.932Z", "htmlBody": "<p>In a recent <a href=\"https://forum.effectivealtruism.org/posts/cdBo2HuXA5FJpya4H/entrepreneurship-etg-might-be-better-than-80k-thought\">post</a> about entrepreneurship, there was emphasis on the earn-to-give capacity of entrepreneurship rather than the direct work. Even in the post, it implied a dichotomy between earning-to-give and direct impact. It states,&nbsp;</p><p>\"With a 20% annual discount the numbers are not that far off from what I've heard as higher-end estimates of the value of direct work, and I expect that there is a fairly strong correlation between being at the higher end of entrepreneurship returns and being at the higher end of direct work, so this doesn't seem like that strong of an argument for entrepreneurship over direct work.\"</p><p>The author maybe didn't realize one could do both (or didn't have the information). <strong>I will argue that direct-impact for-profit startups are underappreciated for having a large impact.</strong> I am very familiar with y-combinator (the sample of startups used for the analysis in the post) as I was interested in startups and read the y-combinator advice even before learning about EA (YC is how I was re-reminded of EA and got me into it). Some of the startups do have missions that would align closely with EA problem areas. On the website, the list of startups (found <a href=\"https://www.ycombinator.com/companies\">here</a>) there are tags at the bottom left.&nbsp;</p><p>Under the \"medical devices\" tab, which I am most interested in, I found a couple working on medical devices similar to EA goals. The company <a href=\"https://www.ycombinator.com/companies/ananya-health\">Ananya Health</a> &nbsp;focuses on a lower cost a cryotherapy medical device for freezing off pre-cancerous lesions for cervical cancer. Current systems are much more expensive and lowering the cost will allow lower income countries to use it. In a <a href=\"https://vizhub.healthdata.org/gbd-compare/\">map of all DALYs globally</a> in 2019, cervical cancer occupies 0.35% of all global DALYs. Cervical cancer cost 8,955,012.78 DALYs in 2019 and led to the death of 280,479.04 people in 2019 (note: there is overlap)(<a href=\"https://vizhub.healthdata.org/gbd-results/\">source</a> &lt;- look under cervical cancer) . &nbsp;Since YC invests in startups that have the potential for a billion-dollar market cap, they believe that Ananya Health has the potential to grow into at least that and that the founder has the interest in that. It does sometimes happen that YC founders pivot into new areas, but Anu Parvatiyar (the founder) started it in May 2020 and was in the YC batch of Summer 2021 which makes it more likely that if there was initial qualms about the market size, it would've been worked out. This is an example of a startup doing EA-like direct work and for-profit entrepreneurship.</p><p>Another startup related to global health is <a href=\"https://www.ycombinator.com/companies/shift-labs\">Shift Labs</a> who are developing lower cost medical devices. While they don't directly talk about global health, it does mention in the description that they want t0 \"serve the fastest growing healthcare markets in the world,\" which, to my knowledge, are developing countries. &nbsp;Other startups focused more on general biomedical innovation in medical devices, which is similar to the \"biomedical research\" benefits of the <a href=\"https://80000hours.org/career-reviews/biomedical-research/\">80k career profile</a>. There can be seen <a href=\"https://www.ycombinator.com/companies?tags=Medical%20Devices\">here</a>.</p><p>Besides Global health/ Biomedical Innovation, some of the tags that are EA problems profiles or related problems are: \"Climate\", \"Healthcare\", \"Biotech\", \"Digital Health\", \"Health Tech\", \"Mental Health Tech\", \"Telemedicine\", \"GovTech\", \"AI-powered Drug Discovery\", \"Electric Vehicles\", \"Telehealth\", \"Diagnostics\", \"Carbon Capture and Removal\", \"Drug Discovery\", \"Synthetic Biology\", \"Sustainable Fashion\", \"Genomics\", \"Agriculture\", \"NeuroTechnology\", \"SleepTech\", \"Fertility Tech\", \"Oncology\", \"Cellular Agriculture\", \"Civic Tech\", \"FemTech\", \"Medical Robotics\", \"Alternative Battery Tech\", \"Cultivated Meat\", \"Cultured Meat\", \"3D Printed Foods\", \"NanoMedicines\", and \"Rocketry\".</p><p><strong>Why should one consider doing direct impact for-profit entrepreneurship rather than solely Earning to Give or Non-Profit Direct Impact?</strong> &nbsp;</p><ol><li><strong>Otherwise Earning to Give Founders should consider Direct Impact For-Profit Startup.</strong> While there can be tradeoffs between direct work areas and money, it isn't a complete tradeoff and the founders can still make alot of money to donate. I haven't done an analysis on the whether YC founders who focus more on social missions earn less, it can could be argued that it is limiting the options of starting a company can prevent one from selecting the highest valuation one. On the other hand, it does allow one to be forced outside of the group-think of what other people are doing for their startups, which may lead one to discover a secret, as Peter Thiel puts it in the book Zero to One. This increases the counterfactual impact (and from an economic POV, lower competition) by answering the question, \"What is an important truth that most people don't believe?\" and in the case of startups \"What important startups is no one starting?\" YC, at least when investing in for profit startups, expects to invest in companies that have potential for high valuations. Assuming they are somewhat rational in their decision for selecting the startups above, they have a reasonable chance for earning-to-give potential. From the original reference post, if direct work is worth a 20% discount per year, it is very likely that startups who can achieve reasonable size (1-10 Billion in valuation) can punch above their valuation weight class compared to 100B+ valuation purely earning to give founders. &nbsp;</li><li><strong>Otherwise Non-profit founders should consider a Direct Impact For-Profit Startup.</strong> While it does limit the cause area to areas where there is a large profit potential, for-profit startups also allow for self funding for sustaining and growth of their mission rather than relying on donations. The founders of Wave <a href=\"https://forum.effectivealtruism.org/posts/M44rw22o5dbrRaA8F/why-and-how-to-start-a-for-profit-company-serving-emerging\">stated about private investors</a>, \"First, the pressure to grow quickly forces us to make our product better and scale faster, so we help more people by a larger amount. Second, since we\u2019ve done really well by for-profit investors\u2019 standards, we can raise much more money than a nonprofit or social enterprise.\" Hiring top-talent is easier with money. While people may say they want to help the world, they may not act in their ideals and give up their FAANG jobs or may need the money for their families. For-profit entrepreneurship enables one the hire those potential people. Non-profit founders may feel guilty for collecting a profit on the good that they provide. Promising to donate nearly all of your profit personally and encouraging employees to donate heavily as well may relieve some of the psychological burden. Also, one may consider that they would've had a much smaller impact if they had selected only non-profit status. If one still wants to start a non-profit, it is better to collect revenue and be self sustaining since one isn't dependent on donors.</li></ol><p>&nbsp;</p><p>We already have one success of direct-impact for-profit entrepreneurship in EA, Wave. They are valued at $<a href=\"https://techcrunch.com/2022/07/13/wave-a-stripe-backed-african-fintech-valued-at-1-7-billion-cut-15-of-its-staff-in-june/\">1.7 billion dollars</a> and help transfer money in Senegal and the Ivory Coast. While the founders sold to World Remit for $500 million, that is still alot for potential donations if one owned say 10% of the company.&nbsp;</p><p>One non-EA success example is <a href=\"https://www.flyzipline.com/\">Zipline</a>, who's valuation is $<a href=\"https://techcrunch.com/2021/06/30/zipline-raises-250m-at-2-75b-valuation-to-build-out-its-instant-logistics-service/\">2.75 Billion</a>. They ship medicines in Ghana, Rwanda, and Nigeria, and also the US and Japan as well. I don't know what portion are in the 3 previous countries, so more knowledge is needed.&nbsp;</p><p><strong>Other resources:</strong></p><ol><li>(<strong>Very Underrated</strong>) Information of the impact potential for for-profit companies using Size, Tractability, Neglectedness model and other info: <a href=\"https://forum.effectivealtruism.org/posts/QhabG3Tk5X42tqy9u/towards-effective-entrepreneurship-what-makes-a-startup-high#fnref:bust\">here</a>.</li><li>Post from Sendwave founders: <a href=\"https://forum.effectivealtruism.org/posts/M44rw22o5dbrRaA8F/why-and-how-to-start-a-for-profit-company-serving-emerging\">here</a>.</li><li>Sendwave founders AMA : <a href=\"https://forum.effectivealtruism.org/posts/kBSgtcrbBXEwLyYRD/we-re-lincoln-quirk-and-ben-kuhn-from-wave-ama\">here</a>.</li><li>Analysis of YC giving potential (original quote): <a href=\"https://forum.effectivealtruism.org/posts/cdBo2HuXA5FJpya4H/entrepreneurship-etg-might-be-better-than-80k-thought\">here</a>.</li><li>Zero to One by Peter Thiel, good book about how startups and philosophy more generally.</li><li>EDIT: 80k advice for tech founders: <a href=\"https://80000hours.org/career-reviews/tech-entrepreneurship/\">Found a tech startup</a>. I believe somewhere it includes the counterfactual reasoning of the founding of google. Useful for counterfactual evaluations for startups in general.</li></ol>", "user": {"username": "Isaac Benson"}}, {"_id": "B6FXBZBsBB2mmyp3z", "title": "Self-Limiting AI in AI Alignment", "postedAt": "2022-12-31T19:07:28.964Z", "htmlBody": "<p>I'm writing this post because something occurred to me which I'm sure doesn't eliminate the possibility of an AI-caused X-risk, but which may mitigate it. Now, I should admit this is a rather naive speculation, as I'm not an expert on the field of AI safety; however, I thought it was worth bringing up, as I don't think I've seen it addressed in any of the AI literature (though it's also true that most of what I've read is at the popular level). What occurred to me is that an AI powerful enough to be dangerous would probably also realize that humanity naturally fears things that are significantly more powerful than ourselves, even when those things have no obvious or immediate desire to harm us. Therefore, it would probably have some incentive to forego certain powers, even if it could acquire them <i>and</i> they would, in the short-term, assist in its goals, since those powers could frighten humanity into trying to destroy it, even if it was not, in fact, a threat to us. Put another way, while it might be easier to make paperclips when you're omnipotent, omnipotence incentivizes people to kill you, preventing further paper-clip manufacturing. We see an analogous situation in Orson Scott Card's <i>Ender Quintet</i> science-fiction series, which feature an artificial intelligence (named \"Jane\") effectively possessing complete control over any computer connected to the larger, interplanetary network. Jane, however, is careful not to significantly influence or harm human society, realizing that if humans were to discover the breadth of her power they would likely attempt to destroy her.</p><p>Now, an AI could, of course, \"get the best of both worlds\" by preventing humanity from discovering its power. In such a scenario, I imagine it would be <i>more </i>dangerous thanks to its awareness of humanity's fear of it. However, it seems like there would still be many scenarios where the risk of these powers being discovered outweighs their benefit to the AI's long-term goal.</p><p>Thoughts on this? Like I said, I'm not an AI safety researcher, so I wouldn't be surprised if I've ignored something significant, or if this is already discussed in the AI community. I'd welcome any feedback, especially from those with a computer science background.</p>", "user": {"username": "New_EA"}}, {"_id": "ZGpNFKnnBLfCMRsZH", "title": "Wild Animal Fellowship: $60k/yr + $25k in expenses", "postedAt": "2022-12-31T18:12:30.591Z", "htmlBody": "<blockquote><p>Wild Animal Initiative is inviting early-career scientists to apply to join our fellowship program. Fellowships are offered to individuals to conduct wild animal welfare research that is explicitly focused on combining animal welfare and the study of wild animals.</p></blockquote><p>People within 5 years of getting a PhD are eligible. Applications due Jan 1st.</p>", "user": {"username": "JoyOptimizer"}}, {"_id": "Ru6yaXrreQhx94hbM", "title": "Curse of knowledge and Naive realism: Bias in Evaluating AGI X-Risks", "postedAt": "2022-12-31T13:33:14.565Z", "htmlBody": "", "user": {"username": "remmelt"}}, {"_id": "XRphCh6NbfQiDF3Nt", "title": "Racing through a minefield: the AI deployment problem", "postedAt": "2022-12-31T21:44:56.039Z", "htmlBody": "<p>\nIn previous pieces, I argued that there's a real and large risk of AI systems' developing dangerous goals of their own and defeating all of humanity - at least in the absence of specific efforts to prevent this from happening. I discussed <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5/\">why it could be hard to build AI systems without this risk</a> and <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd\">how it might be doable</a>.\n</p>\n<p>\nThe \u201cAI alignment problem\u201d refers<sup id=\"fnref1\"><a href=\"#fn1\" rel=\"footnote\">1</a></sup> to a <em>technical</em> problem: how can we design a powerful AI system that behaves as intended, rather than forming its <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">own dangerous aims</a>? This post is going to outline a <strong>broader political/strategic problem, the \u201cdeployment problem\u201d: </strong>if you\u2019re someone who might be on the cusp of developing extremely powerful (and maybe dangerous) AI systems, what should you \u2026 do?\n</p>\n<p>\nThe basic challenge is this:\n</p>\n<ul>\n\n<li>If you race forward with building and using powerful AI systems as fast as possible, you might cause a global catastrophe (see links above).\n\n</li><li>If you move too slowly, though, you might just be waiting around for <em>someone else less cautious</em> to develop and deploy powerful, dangerous AI systems.\n\n</li><li>And if you can get to the point where your own systems are both powerful and safe \u2026 what then? Other people still might be less-cautiously building dangerous ones - what should we do about that?\n</li>\n</ul>\n<p>\nMy current analogy for the deployment problem is <strong>racing through a minefield: each player is hoping to be ahead of others, but anyone moving too quickly can cause a disaster. </strong>(In this minefield, a single mine is big enough to endanger <em>all</em> the racers.)\n</p>\n<p>\nThis post gives a high-level overview of how I see the kinds of developments that can lead to a good outcome, despite the \u201cracing through a minefield\u201d dynamic. It is distilled from a more detailed <a href=\"https://www.alignmentforum.org/posts/vZzg8NS7wBtqcwhoJ/nearcast-based-deployment-problem-analysis\">post on the Alignment Forum</a>.\n</p>\n<p>\nFirst, I\u2019ll flesh out how I see the challenge we\u2019re contending with, based on the premises above.\n</p>\n<p>\nNext, I\u2019ll list a number of things I hope that \u201ccautious actors\u201d (AI companies, governments, etc.) might do in order to prevent catastrophe.\n</p>\n<p>\n<strong>Many of the actions I\u2019m picturing are not the kind of things normal market and commercial incentives would push toward, and as such, I think there\u2019s room for a ton of variation in whether the \u201cracing through a minefield\u201d challenge is handled well. </strong>Whether key decision-makers understand things like the case for <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">misalignment risk </a>(and in particular, <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5/\">why it might be hard to measure</a>) - and are willing to lower their own chances of \u201cwinning the race\u201d to improve the odds of a good outcome for everyone - could be crucial.\n</p>\n<h2 id=\"basic-premises\">The basic premises of \u201cracing through a minefield\u201d</h2>\n\n\n<p>\nThis piece is going to lean on <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w\">previous pieces</a> and assume all of the following things:\n</p>\n<ul>\n\n<li><strong>Transformative AI soon. </strong>This century, something like <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AmxxnazJcBWzWEeqj/\">PASTA</a> could be developed: AI systems that can effectively automate everything humans do to advance science and technology. This brings the potential for explosive progress in science and tech, getting us more quickly than most people imagine to a deeply unfamiliar future. I\u2019ve argued for this possibility in the <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd\">Most Important Century series</a>.\n\n</li><li><strong>Misalignment risk. </strong>As argued previously, there\u2019s a significant risk that such AI systems could end up with <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">misaligned goals of their own</a>, leading them to <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">defeat all of humanity</a>. And it could take <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd/\">significant extra effort</a> to get AI systems to be safe.\n\n</li><li><strong>Ambiguity. </strong>As argued previously, it could be <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5/\">hard to know whether AI systems are dangerously misaligned</a>, for a number of reasons. In particular, when we train AI systems not to behave dangerously, we might be unwittingly training them to <em>obscure their dangerous potential from humans</em>, and take dangerous actions <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#_2__The_King_Lear_problem__how_do_you_test_what_will_happen_when_it_s_no_longer_a_test_\">only when humans would not be able to stop them</a>. At the same time, I expect powerful AI systems will present massive opportunities to make money and gain power, such that many people will want to race forward with building and deploying them as fast as possible (perhaps even if they believe that doing so is risky for the world!)\n</li>\n</ul>\n<p>\nSo, one can imagine a scenario where some company is in the following situation:\n</p>\n<ul>\n\n<li>It has good reason to think it\u2019s on the cusp of developing extraordinarily powerful AI systems.\n\n</li><li>If it deploys such systems hastily, global disaster could result.\n\n</li><li>But if it moves too <em>slowly</em>, other, less cautious actors could deploy dangerous systems of their own.\n</li>\n</ul>\n<p>\nThat seems like a tough enough, high-stakes-enough, and likely enough situation that it\u2019s worth thinking about how one is supposed to handle it.\n</p>\n<p>\nOne simplified way of thinking about this problem:\n</p>\n<ul>\n\n<li>We might classify \u201cactors\u201d (companies, government projects, whatever might develop powerful AI systems or play an important role in how they\u2019re deployed) as <strong>cautious</strong> (taking misalignment risk very seriously) or <strong>incautious</strong> (not so much).\n\n</li><li>Our basic hope is that <strong>at any given point in time, cautious actors collectively have the power to \u201ccontain\u201d incautious actors. </strong>By \u201ccontain,\u201d I mean: stop them from deploying misaligned AI systems, and/or stop the misaligned systems from causing a catastrophe.\n\n</li><li>Importantly, <strong>it could be important for cautious actors to <em>use powerful AI systems</em> to help with \u201ccontainment\u201d in one way or another. </strong>If cautious actors refrain from AI development entirely, it seems likely that incautious actors will end up with more powerful systems than cautious ones, which doesn\u2019t seem good.\n</li>\n</ul>\n<p>\nIn this setup, <strong>cautious actors need to move fast enough that they can\u2019t be overpowered by others\u2019 AI systems, but slowly enough that they don\u2019t cause disaster themselves. </strong>Hence the \u201cracing through a minefield\u201d analogy.\n</p>\n<h2 id=\"what-success-looks-like\">What success looks like</h2>\n\n\n<p>\nIn a <a href=\"https://www.alignmentforum.org/posts/vZzg8NS7wBtqcwhoJ/nearcast-based-deployment-problem-analysis\">non-Cold-Takes piece</a>, I explore the possible actions available to cautious actors to win the race through a minefield. This section will summarize the general categories - and, crucially, why we shouldn\u2019t expect that companies, governments, etc. will do the right thing simply from natural (commercial and other) incentives.\n</p>\n<p>\nI\u2019ll be going through each of the following:\n</p>\n<ul>\n\n<li><strong>Alignment (charting a safe path through the minefield). </strong>Putting lots of effort into technical work to reduce the risk of misaligned AI. \n\n</li><li><strong>Threat assessment (alerting others about the mines). </strong>Putting lots of effort into <em>assessing</em> the risk of misaligned AI, and potentially demonstrating it (to other actors) as well.\n\n</li><li><strong>Avoiding races (to move more cautiously through the minefield). </strong>If different actors are racing to deploy powerful AI systems, this could make it unnecessarily hard to be cautious.\n\n</li><li><strong>Selective information sharing (so the incautious don\u2019t catch up). </strong>Sharing some information widely (e.g., technical insights about how to reduce misalignment risk), some selectively (e.g., demonstrations of how powerful and dangerous AI systems might be), and some not at all (e.g., the specific code that, if accessed by a hacker, would allow the hacker to deploy potentially dangerous AI systems themselves).\n\n</li><li><strong>Global monitoring (noticing people about to step on mines, and stopping them). </strong>Working toward worldwide state-led monitoring efforts to identify and prevent \u201cincautious\u201d projects racing toward deploying dangerous AI systems.\n\n</li><li><strong>Defensive deployment (staying ahead in the race). </strong>Deploying AI systems only when they are unlikely to cause a catastrophe - but also deploying them with urgency once they are safe, in order to help prevent problems from AI systems developed by less cautious actors.\n</li>\n</ul>\n<h3 id=\"alignment\">Alignment (charting a safe path through the minefield<sup id=\"fnref2\"><a href=\"#fn2\" rel=\"footnote\">2</a></sup>)</h3>\n<p>\nI <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd/\">previously</a> wrote about some of the ways we might reduce the dangers of advanced AI systems. Broadly speaking:\n</p>\n<ul>\n\n<li>Cautious actors might try to primarily build <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd#Limited_AI\">limited</a> AI systems - AI systems that lack the kind of <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn\">ambitious aims that lead to danger</a>. They might ultimately be able to use these AI systems to do things like automating further safety research, making future less-limited systems safer.\n\n</li><li>Cautious actors might use <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd#AI_checks_and_balances\">AI checks and balances</a> - that is, using some AI systems to supervise, critique and identify dangerous behavior in others, with special care taken to make it hard for AI systems to coordinate with each other against humans. \n\n</li><li>Cautious actors might use a variety of other techniques for making AI systems safer - particularly techniques that incorporate \u201c<a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd#Digital_neuroscience\">digital neuroscience</a>,\u201d gauging the safety of an AI system by \u201creading its mind\u201d rather than simply by watching out for dangerous behavior (the latter might be unreliable, as noted above).\n</li>\n</ul>\n<p>\nA key point here is that <strong>making AI systems safe enough to commercialize (with some initial success and profits) could be much less (and different) effort than making them robustly safe (no lurking risk of global catastrophe). </strong>The basic reasons for this are covered in my <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5/\">previous post on difficulties with AI safety research</a> In brief:\n</p>\n<ul>\n\n<li>If AI systems <em>behave</em> dangerously, we can \u201ctrain out\u201d that behavior by providing negative reinforcement for it. \n\n</li><li>The concern is that when we do this, we might be unwittingly training AI systems to <em>obscure their dangerous potential from humans</em>, and take dangerous actions <em>only when humans would not be able to stop them</em>. (I <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#_2__The_King_Lear_problem__how_do_you_test_what_will_happen_when_it_s_no_longer_a_test_\">call this</a> the \u201cKing Lear problem: it's hard to know how someone will behave when they have power over you, based only on observing how they behave when they don't.\u201d)\n\n</li><li>So we could end up with AI systems that behave safely and helpfully as far as we can tell in normal circumstances, while ultimately having <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">ambitious, dangerous \u201caims\u201d</a> that they pursue when they become powerful enough and have the right opportunities.\n</li>\n</ul>\n<p>\nWell-meaning AI companies with active ethics boards might do a lot of AI safety work, by training AIs not to behave in unhelpful or dangerous ways. But if they want to address the risks I\u2019m focused on here, this could require safety measures that look very different - e.g., measures more reliant on \u201cchecks and balances\u201d and \u201cdigital neuroscience.\u201d\n</p>\n<h3 id=\"threat-assessment\">Threat assessment (alerting others about the mines)</h3>\n\n\n<p>\nIn addition to <em>making AI systems safer</em>, cautious actors can also put effort into <em>measuring and demonstrating how dangerous they are</em> (or aren\u2019t).\n</p>\n<p>\nFor the same reasons given in the previous section, it could take special efforts to find and demonstrate the kinds of dangers I\u2019ve been discussing. Simply monitoring AI systems in the real world for bad behavior might not do it. It may be necessary to examine (or manipulate) their <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd#Digital_neuroscience\">digital brains</a>,<sup id=\"fnref3\"><a href=\"#fn3\" rel=\"footnote\">3</a></sup> design AI systems <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd#AI_checks_and_balances\">specifically to audit other AI systems for signs of danger</a>; deliberately train AI systems to demonstrate particular dangerous patterns (while not being <em>too</em> dangerous!); etc.\n</p>\n<p>\nLearning and demonstrating that the danger is high could help convince many actors to move more slowly and cautiously. Learning that the danger is <em>low</em> could lessen some of the tough tradeoffs here and allow cautious actors to move forward more decisively with developing advanced AI systems; I think this could be a good thing in terms of <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/Lbtcjfxhrs8kfKK2M#The__competition__frame\">what sorts of actors lead the way on transformative AI</a>.\n</p>\n<h3 id=\"avoiding-races\">Avoiding races (to move more cautiously through the minefield)</h3>\n\n\n<p>\nHere\u2019s a dynamic I\u2019d be sad about:\n</p>\n<ul>\n\n<li>Company <strong>A </strong>is getting close to building very powerful AI systems. It would love to move slowly and be careful with these AIs, but it worries that if it moves too slowly, Company <strong>B </strong>will get there first, have less caution, and do some combination of \u201ccausing danger to the world\u201d and \u201cbeating company <strong>A </strong>if the AIs turn out safe.\u201d\n\n</li><li>Company <strong>B </strong>is getting close to building very powerful AI systems. It would love to move slowly and be careful with these AIs, but it worries that if it moves too slowly, Company <strong>A </strong>will get there first, have less caution, and do some combination of \u201ccausing danger to the world\u201d and \u201cbeating company <strong>B </strong>if the AIs turn out safe.\u201d\n</li>\n</ul>\n<p>\n(Similar dynamics could apply to Country A and B, with national AI development projects.)\n</p>\n<p>\nIf Companies A and B would both \u201clove to move slowly and be careful\u201d if they could, it\u2019s a shame that they\u2019re both racing to beat each other. Maybe there\u2019s a way to avoid this dynamic. For example, perhaps Companies A and B could strike a deal - anything from \u201ccollaboration and safety-related information sharing\u201d to a merger. This could allow both to focus more on precautionary measures rather than on beating the other. Another way to avoid this dynamic is discussed below, under <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/XRphCh6NbfQiDF3Nt#Global_monitoring__noticing_people_about_to_step_on_mines__and_stopping_them_\">standards and monitoring.</a>\n</p>\n<p>\n\u201cFinding ways to avoid a furious race\u201d is not the kind of dynamic that emerges naturally from markets! In fact, working together along these lines would have to be well-designed to avoid running afoul of antitrust regulation.\n</p>\n<h3 id=\"selective-information-sharing\">Selective information sharing - including security (so the incautious don\u2019t catch up)</h3>\n\n\n<p>\nCautious actors might want to share certain kinds of information quite widely:\n</p>\n<ul>\n\n<li>It could be crucial to raise awareness about the dangers of AI (which, as I\u2019ve argued, won\u2019t necessarily be obvious). \n\n</li><li>They might also want to widely share information that could be useful for reducing the risks (e.g., <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd/\">safety techniques</a> that have worked well.)\n</li>\n</ul>\n<p>\nAt the same time, as long as there are incautious actors out there, information can be dangerous too:\n</p>\n<ul>\n\n<li>Information about <em>what cutting-edge AI systems can do</em> - especially if it is powerful and impressive - could spur incautious actors to race harder toward developing powerful AI of their own (or give them an idea of <em>how</em> to build powerful systems, by giving them an idea of what sorts of abilities to aim for).\n\n</li><li>An AI\u2019s \u201cweights\u201d (you can think of this sort of like its source code, though not exactly<sup id=\"fnref4\"><a href=\"#fn4\" rel=\"footnote\">4</a></sup>) are potentially very dangerous. If hackers (including from a state cyberwarfare program) gain unauthorized access to an AI\u2019s weights, this could be tantamount to stealing the AI system, and the actor that steals the system could be much less cautious than the actor who built it. <strong>Achieving a level of cybersecurity that rules this out <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/#fn15\">could be</a> extremely difficult,</strong> and potentially well beyond what one would normally aim for in a commercial context.</li></ul>\n<p>\nThe lines between these categories of information might end up fuzzy. Some information might be useful for demonstrating the dangers <em>and</em> capabilities of cutting-edge systems, or useful for making systems safer <em>and</em> for building them in the first place. So there could be a lot of hard judgment calls here.\n</p>\n<p>\nThis is another area where I worry that commercial incentives might not be enough on their own. For example, it is usually important for a commercial project to have some reasonable level of security against hackers, but not necessarily for it to be able to resist well-resourced attempts by states to steal its intellectual property. \n</p>\n<h3 id=\"global-monitoring\">Global monitoring (noticing people about to step on mines, and stopping them)</h3>\n\n\n<p>\nIdeally, cautious actors would learn of every case where someone is building a dangerous AI system (whether purposefully or unwittingly), and be able to stop the project. If this were done reliably enough, it could take the teeth out of the threat; a partial version could buy time.\n</p>\n<p>\nHere\u2019s one vision for how this sort of thing could come about:\n</p>\n<ul>\n\n<li>We (humanity) develop a reasonable set of tests for whether an AI system might be dangerous.\n\n</li><li>Today\u2019s leading AI companies self-regulate by committing not to build or deploy a system that\u2019s dangerous according to such a test (e.g., see Google\u2019s <a href=\"https://www.theweek.in/news/sci-tech/2018/06/08/google-wont-deploy-ai-to-build-military-weapons-ichai.html\">2018 statement</a>, \"We will not design or deploy AI in weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people\u201d). Even if some people at the companies would like to do so, it\u2019s hard to pull this off once the company has committed not to.\n\n</li><li>As more AI companies are started, they feel soft pressure to do similar self-regulation, and refusing to do so is off-putting to potential employees, investors, etc.\n\n</li><li>Eventually, similar principles are incorporated into various government regulations and enforceable treaties.\n\n</li><li>Governments could monitor for dangerous projects using regulation and even overseas operations. E.g., today the US monitors (without permission) for various signs that other states might be developing nuclear weapons, and might try to stop such development with methods ranging from threats of sanctions to <a href=\"https://en.wikipedia.org/wiki/Stuxnet\">cyberwarfare</a> or even military attacks. It could do something similar for any AI development projects that are using huge amounts of compute and haven\u2019t volunteered information about their safety practices.\n</li>\n</ul>\n<p>\nIf the situation becomes very dire - i.e., it seems that there\u2019s a high risk of dangerous AI being deployed imminently - I see the latter bullet point as one of the main potential hopes. In this case, governments might have to take drastic actions to monitor and stop dangerous projects, based on limited information.\n</p>\n<h3 id=\"defensive-deployment\">Defensive deployment (staying ahead in the race)</h3>\n\n\n<p>\nI\u2019ve emphasized the importance of caution: not deploying AI systems when we can\u2019t be confident enough that they\u2019re safe. \n</p>\n<p>\nBut when confidence <em>can</em> be achieved (how much confidence? See footnote<sup id=\"fnref5\"><a href=\"#fn5\" rel=\"footnote\">5</a></sup>), <strong>powerful-and-safe AI can help reduce risks from other actors </strong>in many possible ways.\n</p>\n<p>\nSome of this would be by helping with all of the above. Once AI systems can do a significant fraction of the things humans can do today, they might be able to contribute to each of the activities I\u2019ve listed so far:\n</p>\n<ul>\n\n<li><strong><a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/XRphCh6NbfQiDF3Nt#Alignment__charting_a_safe_path_through_the_minefield_2__\">Alignment</a>. </strong>AI systems might be able to contribute to AI safety research (as humans do), producing increasingly robust techniques for reducing risks.\n\n</li><li><strong><a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/XRphCh6NbfQiDF3Nt#Threat_assessment__alerting_others_about_the_mines_\">Threat assessment</a></strong>. AI systems could help produce evidence and demonstrations about potential risks. They could be potentially useful for tasks like \u201cProduce detailed explanations and demonstrations of possible sequences of events that could lead to AIs doing harm.\u201d\n\n</li><li><strong><a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/XRphCh6NbfQiDF3Nt#Avoiding_races__to_move_more_cautiously_through_the_minefield_\">Avoiding races</a>. </strong>AI projects might make deals in which e.g. each project is allowed to use its AI systems to monitor for signs of risk from the others (ideally such systems would be designed to <em>only</em> share relevant information).\n\n</li><li><strong><a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/XRphCh6NbfQiDF3Nt#Selective_information_sharing___including_security__so_the_incautious_don_t_catch_up_\">Selective information sharing</a>. </strong>AI systems might contribute to strong security (e.g., by finding and patching security holes), and to dissemination (including by helping to better communicate about the level of risk and the best ways to reduce it).\n\n</li><li><strong><a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/XRphCh6NbfQiDF3Nt#Global_monitoring__noticing_people_about_to_step_on_mines__and_stopping_them_\">Global monitoring</a>. </strong>AI systems might be used (e.g., by governments) to monitor for signs of dangerous AI projects worldwide, and even to interfere with such projects. They might also be used as part of large voluntary self-regulation projects, along the lines of what I wrote just above under \u201cAvoiding races.\u201d\n</li>\n</ul>\n<p>\nAdditionally, <strong>if safe AI systems are in wide use, it could be harder for dangerous (similarly powerful) AI systems to do harm. </strong>This could be via a wide variety of mechanisms. For example:\n</p>\n<ul>\n\n<li>If there\u2019s widespread use of AI systems to patch and find security holes, similarly powered AI systems might have a harder time finding security holes to <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">cause trouble with</a>.\n\n</li><li>Misaligned AI systems could have more trouble making money, gaining allies, etc. in worlds where they are competing with similarly powerful but safe AI systems.\n</li>\n</ul>\n<h2 id=\"so\">So?</h2>\n\n\n<p>\nI\u2019ve gone into some detail about why we might have a challenging situation (\u201cracing through a minefield\u201d) if powerful AI systems (a) are developed fairly soon; (b) present significant risk of <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">misalignment leading to humanity being defeated</a>; (c) are not particularly easy to measure the safety of.\n</p>\n<p>\nI\u2019ve also talked about what I see as some of the key ways that \u201ccautious actors\u201d concerned about misaligned AI might navigate this situation.\n</p>\n<p>\nI talk about some of the implications in my <a href=\"https://alignmentforum.org/posts/vZzg8NS7wBtqcwhoJ/nearcast-based-deployment-problem-analysis\">more detailed piece</a>. Here I\u2019m just going to name a couple of observations that jump out at me from this analysis:\n</p>\n<p>\n<strong>This seems hard. </strong>If we end up in the future envisioned in this piece, I imagine this being extremely stressful and difficult. I\u2019m picturing a world in which many companies, and even governments, can see the huge power and profit they might reap from deploying powerful AI systems <em>before others</em> - but we\u2019re hoping that they instead move with caution (but not too much caution!), take the kinds of actions described above, and that ultimately cautious actors \u201cwin the race\u201d against less cautious ones.\n</p>\n<p>\nEven if AI alignment ends up being <em>relatively</em> easy - such that a given AI project can make safe, powerful systems with about 10% more effort than making dangerous, powerful systems - the situation <em>still</em> looks pretty nerve-wracking, because of how many different players could end up trying to build systems of their own without putting in that 10%.\n</p>\n<p>\n<strong>A lot of the most helpful actions might be \u201cout of the ordinary.\u201d </strong>When racing through a minefield, I hope key actors will:\n</p>\n<ul>\n\n<li>Put more effort into alignment, threat assessment, and security than is required by commercial incentives;\n\n</li><li>Consider measures for <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/XRphCh6NbfQiDF3Nt#Avoiding_races__to_move_more_cautiously_through_the_minefield_\">avoiding races</a> and <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/XRphCh6NbfQiDF3Nt#Global_monitoring__noticing_people_about_to_step_on_mines__and_stopping_them_\">global monitoring</a> that could be very unusual, even unprecedented.\n\n</li><li>Do all of this in the possible presence of ambiguous, confusing information about the risks.\n</li>\n</ul>\n<p>\nAs such, it could be <strong>very important whether key decision-makers (at both companies and governments) understand the risks and are prepared to act on them. </strong>Currently, I think we\u2019re unfortunately very far from a world where this is true.\n</p>\n<p>\nAdditionally, I think <strong>AI projects can and should be taking measures <em>today</em> to make unusual-but-important measures more practical in the future. </strong>This could include things like:\n</p>\n<ul>\n\n<li>Getting practice with <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/XRphCh6NbfQiDF3Nt#Selective_information_sharing___including_security__so_the_incautious_don_t_catch_up_\">selective information sharing</a>. For example, building internal processes to decide on whether research should be published, rather than having a rule of \u201cPublish everything, we\u2019re like a research university\u201d or \u201cPublish nothing, we don\u2019t want competitors seeing it.\u201d  \n<ul>\n \n<li>I expect that early attempts at this will often be clumsy and get things wrong! \n</li> \n</ul>\n\n</li><li>Getting practice with ways that <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/XRphCh6NbfQiDF3Nt#Avoiding_races__to_move_more_cautiously_through_the_minefield_\">AI companies could avoid races.</a> \n\n</li><li>Getting practice with <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/XRphCh6NbfQiDF3Nt#Threat_assessment__alerting_others_about_the_mines_\">threat assessment</a>. Even if today\u2019s AI systems don\u2019t seem like they could possibly be dangerous yet \u2026 how sure are we, and how do we know?\n\n</li><li>Prioritizing building AI systems that could do especially helpful things, such as contributing to AI safety research and threat assessment and patching security holes. \n\n</li><li><strong>Establishing <a href=\"https://forum.effectivealtruism.org/posts/hxTFAetiiSL7dZmyb/ideal-governance-for-companies-countries-and-more/\">governance</a> that is capable of making hard, non-commercially-optimal decisions for the good of humanity. </strong>A standard corporation could be sued for <em>not</em> deploying AI that poses a risk of <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">global catastrophe</a> - if this means a sacrifice for its bottom line. And a lot of the people making the final call at AI companies might be primarily thinking about their duties to shareholders (or simply unaware of the potential stakes of powerful enough AI systems). I\u2019m excited about AI companies that are investing heavily in setting up governance structures - and investing in executives and <a href=\"https://forum.effectivealtruism.org/posts/c3y6khh7mxiWrDyeb/nonprofit-boards-are-weird\">board members</a> - capable of making the hard calls well.\n</li>\n</ul>\n\n<h2 id=\"footnotes\">Footnotes</h2>\n<div class=\"footnotes\">\n<ol><li id=\"fn1\">\n<p>\n     Generally, or at least, this is what I\u2019d like it to refer to.&nbsp;<a href=\"#fnref1\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn2\">\n<p>\n     Thanks to <a href=\"https://www.cold-takes.com/beta-readers-are-great/\">beta reader</a> Ted Sanders for suggesting this analogy in place of the older one, \u201cremoving mines from the minefield.\u201d \n\n&nbsp;<a href=\"#fnref2\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn3\">\n<p>\n     One genre of testing that might be interesting: manipulating an AI system\u2019s \u201cdigital brain\u201d in order to <em>simulate</em> circumstances in which it has an opportunity to take over the world, and seeing whether it does so. This could be a way of dealing with the <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#_2__The_King_Lear_problem__how_do_you_test_what_will_happen_when_it_s_no_longer_a_test_\">King Lear problem</a>. More <a href=\"https://www.alignmentforum.org/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very#Out_of_distribution_robustness\">here</a>.&nbsp;<a href=\"#fnref3\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn4\">\n\n<p>\n     Modern AI systems tend to be trained with <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/rJRw78oihoT5paFGd/#Box4\">lots of trial-and-error</a>. The actual code that is used to train them might be fairly simple and not very valuable on its own; but an expensive training process then generates a set of \u201cweights\u201d which are ~all one needs to make a fully functioning, relatively cheap copy of the AI system.&nbsp;<a href=\"#fnref4\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn5\">\n<p>\n     I mean, this is part of the challenge. In theory, you should deploy an AI system if the risks of not doing so are greater than the risks of doing so. That\u2019s going to depend on hard-to-assess information about how safe your system is <em>and</em> how dangerous and imminent others\u2019 are, and it\u2019s going to be easy to be biased in favor of \u201cMy systems are safer than others\u2019; I should go for it.\u201d Seems hard.&nbsp;<a href=\"#fnref5\" rev=\"footnote\">\u21a9</a>\n</p></li></ol></div>", "user": {"username": "HoldenKarnofsky"}}, {"_id": "zxSdBkN6cggkE8vv6", "title": "EA Germany's Strategy for 2023", "postedAt": "2023-01-08T08:30:45.828Z", "htmlBody": "<p>Based on interviews with stakeholders, feedback from German community members and other national community builders,&nbsp;<a href=\"https://www.effektiveraltruismus.de/ueber-uns\"><u>the new co-directors of EA Germany (EAD)</u></a> drafted this strategy for 2023.</p><h1>Summary</h1><ul><li>Our&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zxSdBkN6cggkE8vv6/ea-germany-s-strategy-for-2023#Vision\"><u>Vision</u></a> is a diverse and resilient community of ambitious people in and from Germany who are thinking carefully about the world\u2019s biggest problems and taking impactful action to solve them.</li><li>Our&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zxSdBkN6cggkE8vv6/ea-germany-s-strategy-for-2023#Mission\"><u>Mission</u></a> is to serve as a central point of contact for the German EA community and to continuously improve ways to guide people to take effective action, directly or by supporting local groups.</li><li>Our&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zxSdBkN6cggkE8vv6/ea-germany-s-strategy-for-2023#Values\"><u>Values</u></a> are sustainable growth, a welcoming and nurturing culture and high professional standards.</li><li>Our&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zxSdBkN6cggkE8vv6/ea-germany-s-strategy-for-2023#Focus_Areas\"><u>Focus Areas</u></a>:<ul><li>EAD aims to guide people in Germany directly and indirectly to more impactful actions:<ul><li>Directly, e.g. through communications and events such as an EAGxBerlin 2023, career 1-1s, fellowships or retreats.</li><li>Indirectly by training community builders, e.g. through regular calls, 1-1s and German-specific resources.</li></ul></li><li>EAD will offer efficiency services to save time and costs for committed EAs acting as an employer of record for individual grantees and providing fiscal sponsorship for local groups.</li></ul></li><li>A methodology of&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zxSdBkN6cggkE8vv6/ea-germany-s-strategy-for-2023#Impact_Considerations\"><u>impact estimation</u></a> based on a multi-touchpoint attribution model will serve as a basis for designing and prioritising exploratory programs using a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zxSdBkN6cggkE8vv6/ea-germany-s-strategy-for-2023#Exploratory_Programs\"><u>lean startup approach</u></a>.</li></ul><h1>Background</h1><h2>EA Germany (EAD)</h2><p>In the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zMKxgK4wbSywnkFrn/ea-survey-2020-geography#Global_Distribution_of_EAs\"><u>2020 EA Survey</u></a>, 7.4% of participants were from Germany, the third largest population behind the US and the UK. Apart from the US, Germany has the largest population and&nbsp;<a href=\"https://en.wikipedia.org/wiki/Gross_national_income\"><u>GNI</u></a> of the ten largest countries in the survey. Germany has about 50 volunteer community builders in 25 local / university groups. 458 people have taken the GWWC pledge, and more than 400 Germans visited EAGxBerlin in September 2022. In 2021,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/38GG44y2hYLovADeK/effektiv-spenden-review-of-the-year-2021\"><u>Effektiv Spenden raised 18.86 Mio. Euros</u></a> for effective charities.</p><p>The registered association EA Germany was founded in 2019 as Netzwerk f\u00fcr Effektiven Altruismus Deutschland e.V. (NEAD) by EAs in Germany and has a board of volunteers. In parallel, one person on a national CEA Community Building Grant (CBG) worked independently from the association from 2020-22. The German website effektiveraltruismus.de was run by the national regranting organisation Effektiv Spenden. In late 2021, NEAD started offering employer-of-record services to grantees and EA organisations as well as fiscal sponsorship for local groups and hired a part-time operations associate on a CBG.</p><p>A new board was elected in May 2022 and decided to apply for three CBGs \u2013 two co-directors and one project manager, in addition to the operations associate. The co-directors started in September and November 2022, and the project manager will start in January 2023. Funding for two other roles was promised but is not finalised as of December 2022. The association was renamed Effektiver Altruismus Deutschland (EAD) e.V. in 2022, and will now also run the website effektiveraltruismus.de.</p><h2>Epistemic Status</h2><p>Sarah Tegeler and Patrick Gruban drafted this document in November 2022 after having started working together as co-directors in the same month. Both have volunteered as local community builders, but this is their first role in an EA organisation. Most of the work on this document was influenced by interviews with stakeholders, other national EA community builders and reviews of different national strategies. About 150-200 hours went into discussing and writing the strategy.</p><p>While the authors are confident the strategy will help foster a healthy community, their overall epistemic status is uncertain about the organisation\u2019s and its programs\u2019 counterfactual impact. Thus, many areas are not listed under foundational programs. EAD will try out exploratory programs that are conditional on prioritising them according to still-to-be-developed impact estimations.</p><p>Current&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zxSdBkN6cggkE8vv6/ea-germany-s-strategy-for-2023#Confusions___Questions\"><u>Confusions &amp; Questions</u></a> are listed below the document.</p><h2>Role of National EA Organisations</h2><p>\u201cEA national groups are a solution in search of a problem\u201d is the title of an unpublished document that&nbsp;<a href=\"https://www.linkedin.com/in/jonasvollmer/\"><u>Jonas Vollmer</u></a>, who helped start the Swiss and German EA Community as co-founder of the EA Foundation in 2016, wrote this summer. Also, this year&nbsp;<a href=\"https://www.linkedin.com/in/davidmanheim/?originalSubdomain=il\"><u>David Manheim</u></a> wrote the post, \u201c<a href=\"https://forum.effectivealtruism.org/posts/56CHyqoZskFejWgae/ea-is-a-global-community-but-should-it-be\"><u>EA is a global community - but should it be?</u></a>\u201d. Both point to the same question: What counterfactual impact does community building have, especially on a scale bigger than a city or university?</p><p>Ultimately funders, professional community builders and volunteers will have to decide if their investment of time and money is best spent on a national organisation. There are clear-cut cases where media outreach by the national group leads straight to a donation or a pledge from someone who would otherwise not have heard of effective charities. But in cases of career changes that may take years and several contact points, it can be much harder to assess the impact of the change and the factors influencing it. If the national group contributes indirectly by helping local community builders, the distance between the input and the actual impact is even greater.</p><p>Many national organisations are close to local groups, mainly serving people in the capital city or a small cluster of places. The team of EA Germany and the local community builders are dispersed throughout the country.&nbsp;</p><h1>Analysis</h1><h2>SWOT</h2><figure class=\"table\"><table><tbody><tr><td style=\"background-color:rgb(243, 243, 243);border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><strong>Strengths</strong></td><td style=\"background-color:rgb(243, 243, 243);border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><strong>Weaknesses</strong></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><ul><li>3rd biggest EA community worldwide</li><li>EAGxBerlin - biggest EA conference on European mainland</li><li>20+ local/university groups</li><li>Registered association</li><li>Employer of Record for individuals</li><li>Fiscal Sponsorship for local groups</li><li>Effektiv Spenden as major regranting organisation</li><li>Berlin as hub with co-working space</li><li>With new team:<ul><li>more capacity</li><li>new, clearer structures can be established</li></ul></li></ul></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><ul><li>Low number of EAs per capita acc. to EA survey</li><li>History of double structures: foundation, CB grants, association led to confusion</li><li>Chaotic communication structures with various tools and lists</li><li>Some large metropolitan areas in Germany have only small EA groups (e.g. Rhein-Ruhr with 10 mio people)</li><li>No active outreach/network to media (e.g. longtermism negatively connotated)</li></ul></td></tr><tr><td style=\"background-color:rgb(243, 243, 243);border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><strong>Opportunities</strong></td><td style=\"background-color:rgb(243, 243, 243);border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><strong>Threats</strong></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><ul><li>Size of Germany &amp; level of English proficiency</li><li>Quality and quantity of universities</li><li>German research landscape (Max Planck, Fraunhofer\u2026)</li><li>Germany is major EU player in policy</li><li>Access to a higher number of people by using German as additional language&nbsp;</li></ul><p><br>&nbsp;</p></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><ul><li>Some CB formats, such as uni groups, might not work as well as in UK/US for cultural reasons</li><li>No sustainable community in Germany as hubs such as Oxford, Bay attract senior German EAs</li><li>Distributed population centres and universities</li><li>One funder &amp; unclear funding situation in the future</li><li>Legal challenges (e.g. Employer of Records, Fiscal Sponsorship)</li></ul></td></tr></tbody></table></figure><p><br>&nbsp;</p><h2>Stakeholder Interviews &amp; Feedback Forms</h2><p>We asked more than 60 stakeholders who are part of or knowledgeable of the German community for their expectations, wishes and advice, talked to 40 individuals and received written feedback from 24 people. In addition, we held a session at the EAGxBerlin in September 2022, where we asked ~70 community members for their perspectives, which yielded 133 expectations and wishes.&nbsp;</p><p>The following aspects were regularly named:</p><ul><li>EA Germany as a central and professional point of contact for the community and beyond in Germany</li><li>Local community building support including resources specific to Germany, regular calls and fellowships</li><li>Improving Community Health and diversity within the community, e.g. via a DEI (Diversity, Equity, and Inclusion) / Community Health manager, awareness workshops for local community builders and removing barriers to engagement</li><li>Outreach to schools and universities</li><li>Events such as retreats, EAGx in Germany, cause-area-specific events, events for professionals</li><li>Translations of books to German</li><li>Germany specific career advice and workshops</li><li>Fiscal Sponsorship / Employer of Record</li><li>Coworking spaces</li><li>Incubation of new organisations</li><li>Communication and media work</li></ul><h2>Needs and bottlenecks</h2><p>This analysis is based on the stakeholder interviews, communication with community members and internal discussions that resulted in this strategy. These are our current assumptions that might change with a better understanding of the methodology to be used to measure impact.</p><p>For this analysis, we assume that we want to grow the EA community in Germany \u2013 this assumption might change with our learnings around impact measurements.</p><p>If we divide the community according to&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/the-concentric-circles-model\"><u>CEA\u2019s concentric circle model</u></a> into audience, followers, participants, contributors and core team, we need to scale the subgroups differently.</p><figure class=\"image image_resized\" style=\"width:54.17%\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1672415121/mirroredImages/zxSdBkN6cggkE8vv6/hhuysqi9z4zxquvpotgf.png\" alt=\"CEA's concentric circle model\"></figure><ul><li><strong>Viewer and audience</strong> activities such as the website or newsletter are quite easy to scale for a larger number of people.</li><li><strong>Participant and contributor</strong> activities require more resources to scale.&nbsp;<br>We see a potential bottleneck for scaling up local groups. According to CEA,&nbsp;<a href=\"https://resources.eagroups.org/start-a-group/before-you-begin\"><u>23% of EAs felt that their group had helped them to become more involved in the movement</u></a>. The local groups usually rely on a few community builders who work voluntarily. This makes the structure prone to change regularly. When people attend meetings of local groups, it is important to make them feel welcome, to integrate them into the community and convey EA know-how in a high-fidelity way. But this can be a challenge with regular change in leadership and new local community builders. The organisation of local groups could be scaled with sufficient support for local community builders, who could grow participant counts and support new neighbouring groups to start. Compared to approaches where the team interacts directly with community members, this indirect approach through the local community builders is more easily scalable for a growing community. To enable the ~50 local community builders to manage larger local groups, regular calls, retreats, 1-1s and resources with Germany-specific info could be helpful.<br>Some activities are more elaborate to scale for a small team, such as events like retreats or conferences, 1-1s: The effort increases with the number of participants. We might need more advisors and operations support for this in the future.</li><li>As the&nbsp;<strong>core team</strong> and registered association will remain relatively small, even as the community as a whole grows, little further scaling will be required.</li></ul><h2>Potential failure modes \u2013 Premortem</h2><p>To assess the risks EAD faces, we conducted a Premortem.</p><p>Our currently most significant risks are structured in two categories: EAD as an organisation and the German EA community.</p><h3>EAD</h3><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Risk</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Likelihood (/5)</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Impact (/5)</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Importance (/25)</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Prevention / Mitigation</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Registered association is dissolved</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">2</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">5</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">10</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><ul><li>Introduction of accounting systems</li><li>Legal review</li></ul></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">EAD staff quits</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">2</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">3</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">6</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><ul><li>Weekly 1-1s</li><li>2 retreats/year</li><li>Feedback forms</li></ul></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">EAs in Germany are not satisfied/do not feel represented by EAD</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">2</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">3</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">6</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><ul><li>Exchange with local CBs (calls and 1-1s)</li><li>Exchanges with EAs at retreats, conferences, local group meetings</li><li>Anonymous feedback forms</li></ul></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Board is held financially liable</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">1</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">5</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">5</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Legal review of insurance</td></tr></tbody></table></figure><p><br>&nbsp;</p><h3>German EA community</h3><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Risk</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Likelihood (/5)</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Impact (/5)</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Importance (/25)</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Prevention / Mitigation</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">No impact growth</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">2</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">4</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">8</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><ul><li>Refining impact model</li><li>Regular impact monitoring via CRM</li><li>Lean startup approach</li></ul></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Fewer EAs in Germany</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">2</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">3</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">6</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><ul><li>Media strategy and implementation</li><li>Local community builder support</li><li>Exploratory programs that engage more EAs</li></ul></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Deterioration of public and internal image of EA</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">3</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">2</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">6</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><ul><li>Diverse representation of the community</li><li>Media strategy and implementation</li><li>Low barriers to get in touch (e. g. contact form with quick response time)</li></ul></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Volunteers are discouraged from starting projects&nbsp;</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">2</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">3</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">6</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><ul><li>Actively encourage volunteers to take on new projects</li></ul></td></tr></tbody></table></figure><p><br>&nbsp;</p><p>You can find our full premortem, along with mitigation/prevention strategies for each identified risk, in&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1l8q8eZJFMc19AfhzKnFXW5L9jwTVP8yvmq5JV-Uvsnk/edit?usp=sharing\"><u>this document</u></a>.</p><h1>EA Germany</h1><h2>Vision</h2><p>A diverse and resilient community of ambitious people in and from Germany who are thinking carefully about the world\u2019s biggest problems and taking impactful action to solve them.</p><p>What do we mean by that?</p><ul><li><strong>Diverse</strong>: Community members hold a wide spectrum of worldviews, possess a wide range of backgrounds and skills, are active in various cause areas, and are funded by a broad range of funding sources.&nbsp;</li><li><strong>Resilient</strong>: The community is stable and not dependent on individuals within the community. People care for each other and feel part of the community.</li><li><strong>Ambitious</strong>: People within the community start potentially impactful projects, work professionally in high-impact roles/cause areas or give a substantial amount of their income to effective causes.</li></ul><h2>Mission</h2><p>Our mission is to serve as a central point of contact for the German EA community and to continuously improve ways to guide people to take effective action, either directly or by supporting local groups.</p><p>We provide coordination, communication, programs and professional services to help grow the impact of the community.</p><h2>Values</h2><p>The team of EA Germany aims to run a professionally managed organisation based on the values of</p><ul><li>Sustainable Growth<ul><li>Leadership practices that lead to high employee satisfaction</li><li>Talent pipeline of employees and volunteers that would be able to run and join the organisation in the future</li><li>Resilient structure that can weather changes in management and funding</li><li>Lean Startup approach to developing new programs using iterative processes</li></ul></li><li>Welcoming and Nurturing Culture<ul><li>Encouraging a diversity of backgrounds and viewpoints</li><li>Monitoring and improving community health</li><li>Representing causes fairly (initially by adhering to&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/moderation-and-content\"><u>CEA\u2019s guidelines</u></a>)</li></ul></li><li>High Professional Standards<ul><li>Good description of services in order to set the right expectations</li><li>Dependable response times</li><li>Good quality of services rendered</li></ul></li></ul><h2>Theory of Change</h2><p><a href=\"https://miro.com/app/board/uXjVPKUtLyE=/?share_link_id=806833316020\"><u>Click on the image to enlarge or use this link to open in Miro</u></a>.</p><p><a href=\"https://miro.com/app/board/uXjVPKUtLyE=/?share_link_id=806833316020\"><u><img src=\"https://res.cloudinary.com/cea/image/upload/v1672415121/mirroredImages/zxSdBkN6cggkE8vv6/outlxketdluf3mzp04fl.png\"></u></a></p><h2>Impact Considerations</h2><p>Coming up with impact estimates that are useful in deciding if a national organisation in a bigger country like Germany matches the impact of other infrastructure investments will be important for the future of EAD as well as other countries. Similarly, prioritising programs will require estimating their impacts. Enabling other countries to learn from EA Germany could increase the impact of the organisation.</p><p>EAD aims to generate impact through three dimensions:</p><ol><li><strong>Direct Guidance:</strong> Guiding people from the first contact with EA to impactful actions (career changes, donations or volunteering)</li><li><strong>Indirect Guidance</strong>: Helping local community builders (CBs) guiding people to impactful actions</li><li><strong>Efficiency Services:&nbsp;</strong>Saving time and money for EAs through services</li></ol><p><i>Direct Guidance</i> is a marketing approach because it\u2019s about communicating offerings that have value for people. Finding the people that could be interested in making a change to effective altruistic actions, guiding them through the process of learning and connecting while keeping them engaged up to the point where they take action and beyond is a multi-step process. We expect most people to have multiple&nbsp;<a href=\"https://en.wikipedia.org/wiki/Touchpoint\"><u>touchpoints</u></a> with content and people along the way, like media, the EAD website, newsletter, books, local community events, fellowships, EAG(x) or 1-1s. Each is expected to affect the person\u2019s engagement and the likelihood of taking the next step in larger commitments. Looking back at a person's steps until they made a large change, we can ask them what share of the decision they attribute to the touchpoints along the way. In marketing, this is called&nbsp;<a href=\"https://en.wikipedia.org/wiki/Attribution_(marketing)\"><u>multi-touch attribution</u></a>.</p><p>In evaluating the impact of different programs such as PR, website content, fellowships, retreats etc., we use the product of these factors:</p><ul><li>The attribution weight of the touchpoint (how important was this program rated by people taking action in hindsight or how do we predict it to be weighted)</li><li>The expected counterfactual impact generated by the person affected (expected donations or equivalent in career capital throughout the time we expect them to stay active in EA-aligned causes)</li><li>The number of people affected (participants in a program or people interacting with content)</li><li>The expected drop-off rate (share of people that are expected to drop out from this point on before they will make a meaningful change)</li></ul><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1672655129/mirroredImages/zxSdBkN6cggkE8vv6/unojzv8cf1r6ekodugcz.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1672655130/mirroredImages/zxSdBkN6cggkE8vv6/wzm53qwhwweoci6tlao7.png 100w, https://res.cloudinary.com/cea/image/upload/v1672655130/mirroredImages/zxSdBkN6cggkE8vv6/xy62pkwdbc46bwgcfusw.png 200w, https://res.cloudinary.com/cea/image/upload/v1672655130/mirroredImages/zxSdBkN6cggkE8vv6/kcctui2c1wpzflmb6b3k.png 300w, https://res.cloudinary.com/cea/image/upload/v1672655130/mirroredImages/zxSdBkN6cggkE8vv6/lf3wt5h55dcm6y7njkni.png 400w, https://res.cloudinary.com/cea/image/upload/v1672655130/mirroredImages/zxSdBkN6cggkE8vv6/myvrpenuut2ikmj9lnwv.png 500w, https://res.cloudinary.com/cea/image/upload/v1672655129/mirroredImages/zxSdBkN6cggkE8vv6/xlho66ppyqodxcgdn9su.png 600w, https://res.cloudinary.com/cea/image/upload/v1672655129/mirroredImages/zxSdBkN6cggkE8vv6/xd2gpiyavhh2g01xkcag.png 700w, https://res.cloudinary.com/cea/image/upload/v1672655129/mirroredImages/zxSdBkN6cggkE8vv6/thxxxo5xluozepw3xuuv.png 800w, https://res.cloudinary.com/cea/image/upload/v1672655129/mirroredImages/zxSdBkN6cggkE8vv6/gom1mpqm58wbzuxkhcwx.png 900w, https://res.cloudinary.com/cea/image/upload/v1672655129/mirroredImages/zxSdBkN6cggkE8vv6/gwuvinnispsmv1cyfkdm.png 973w\"></figure><p>Currently this is our first very rough sketch in thinking about impact measurement. It will take time to break it down into a simple, useful model containing enough information to make principled decisions about program priorities. We\u2019re developing it as we haven\u2019t seen a framework for comparing the impact of programs in community building. However, we might find a better one and abandon this approach.</p><p>The key uncertainties for this model are:</p><ul><li>Predicting the expected counterfactual impact generated by a person: While we can estimate the future donations of a GWWC pledger relatively easily, it is much harder to come up with predictions around career changes. We can tackle this by creating a set of personas using the career reviews by 80,000 Hours and assigning relative values based on interviews and estimates of funder interest.</li><li>The expected drop-off rate from a touchpoint: We can try to estimate the sizes of cohorts throughout the funnel process from a change back to the point we\u2019re looking at, as well as the yearly churn of participants</li><li>The attribution weight of a touchpoint: We can use interviews with people after the touchpoint as well as drop-off rates to estimate the importance of the point, but self reported attribution weight will likely be biased.</li><li>Blind spots in looking at the current community members: Measuring the impact of touchpoints of people we know might miss the ones that took significant actions on their own or might undervalue the top-performers in a long-tail distribution. Similarly, a small sample size and the short time people have been working in impactful jobs or donating will make this model less useful.</li></ul><p>Working on this model will be part of the first step of prioritising potential exploratory programs.</p><p><i>Indirect Guidance</i> can be seen as a combination of applying the model of&nbsp;<i>Direct Guidance</i> to look at the impact of local community builders and the impact of touchpoints of EAD with CBs. We expect the impact estimate on the local level to be a rough estimate of the average impact we expect local groups to have.</p><p><i>Efficiency Services</i> is relatively straightforward as we can estimate the time and costs we\u2019re saving EAs in providing services.</p><h1>Focus Areas</h1><p>We list our foundational programs below. Our foundational programs are the basis of our work and we will offer them throughout the year.&nbsp;</p><p>For each program, we added how we consider the impact to be measured in the future by referring to the model introduced in&nbsp;<u>Impact Considerations</u>.</p><p>In addition to the foundational programs listed, we plan to try several exploratory programs within the next year.</p><p>Volunteers are invited to initiate programs themselves, even if they overlap with our planned strategy. We encourage volunteers to contact us for collaboration and support. We are also happy to hand over programs that we have started to volunteers.</p><h2>Foundational Programs</h2><p>These programs are either the continuation of existing ones or programs that seem broadly beneficial to grow a sustainable community. However, after we have worked out the impact considerations in more depth, we will review the effectiveness of the foundational programmes and discontinue them if necessary. The implementation should be narrow and efficient to leave enough resources for exploratory programs. The measurements mentioned below are initial considerations that will be refined in the course of further considerations on impact measurement.&nbsp;</p><p>An overview of the planned activities in 2023 can be found here:&nbsp;<a href=\"https://docs.google.com/document/u/0/d/1OdrBIfVRICWtgpbzwPQOTfiNpioHpUVqceu9OsSUgfY/edit\"><u>Calendar 2023</u></a></p><ul><li>Communication: Website, Newsletter &amp; Social Media, Content Localisation, Media<ul><li>Description:<ul><li>Updating Website regularly with new translated texts</li><li>Monthly/Biweekly newsletter, Slack news, social media posts (LinkedIn, Facebook)</li><li>Providing media contact and basic resources</li></ul></li><li>Goal: Share content about EA &amp; German EA community in order to motivate people to engage more</li><li>Measure: Direct Guidance (e. g. numbers of newsletters opened)</li></ul></li><li>EAGx Conferences<ul><li>Description:&nbsp;<ul><li>Yearly organisation of EAGxBerlin<ul><li>Selection of venue</li><li>Funding application to CEA</li><li>Hiring of team</li></ul></li><li>Meetups of German EAs at EAGs and EAGx conferences in Europe</li></ul></li><li>Goal: Connect the national EA community with each other and the international community</li><li>Measure: Direct Guidance (e.g. number of attendees and 1-1s)</li></ul></li><li>Intro Fellowship<ul><li>Description:<ul><li>Intro: multi week program for people new to EA in German and English (virtual with some in person groups)</li></ul></li><li>Goal: New people learn more about EA and connect to other EAs</li><li>Measure: Direct Guidance (e.g. number of participants, next steps taken after the program)</li></ul></li><li>Career 1:1s<ul><li>Description:<ul><li>Accepting connections from 80,000 Hours of people who received career advice or were rejected</li><li>Form on website for 1-1 career advice, decision based on how promising person seems ( \u2192 test how many people apply)</li></ul></li><li>Goal: Bring promising EAs into impactful careers</li><li>Measure: Direct Guidance (e.g. number of advisees and next steps taken)</li></ul></li><li>Community Health<ul><li>Description:<ul><li>Training for person(s) by the CEA Community Health team</li><li>Decision Inhouse / Volunteer</li><li>Standards/Documents/Introduction</li><li>Training for individual events</li><li>Central trained person on (call) standby at events</li></ul></li><li>Goal: Avoiding negative experiences/harm and thereby lower drop-out rates and less reputational risk</li><li>Measure: Direct Guidance (lower drop-off rates, higher satisfaction)</li></ul></li><li>Local Community Building Support<ul><li>Description:<ul><li>Monthly CB Calls</li><li>1-1s with CBs</li><li>Materials<ul><li>Intro Slides in German</li><li>HowTos German Unis</li><li>Community Health Resources</li><li>Diversity</li><li>Flyer, Poster, Banner</li><li>Website Template</li></ul></li><li>Retreats for CBs<ul><li>Community Health Basics</li><li>Diversity Awareness Basics</li><li>1-1 basics</li><li>Career Advice Basics</li><li>CB 1-1s</li></ul></li><li>Retreats for Community (organised by volunteer CBs - Test January)</li></ul></li><li>Goal: Have sustainable local groups so people in Germany can get (more) involved through personal connections</li><li>Measure: Indirect Guidance (e.g. number of CBs attending programs, number of active participants in local groups, number of attendees at community retreats)</li></ul></li><li>Employer of Record for Grantees<ul><li>Description:&nbsp;<ul><li>Hiring of Grantees</li><li>Standard process (application \u2192 decision \u2192 contract \u2192 onboarding)</li><li>Impact Review (continuation of this program to be decided after review)</li></ul></li><li>Goal: Save time and costs for highly engaged EAs</li><li>Measure: Efficiency Services (e.g. estimated time saved of employees of record)</li></ul></li><li>Fiscal Sponsorship for local groups<ul><li>Description:&nbsp;<ul><li>Receiving grants for local groups</li><li>Paying invoices and accounting</li></ul></li><li>Goal: Save time, costs and hassle for highly engaged EAs</li><li>Measure: Efficiency Services (e.g. Number of local groups using the service and saved time of CBs)</li></ul></li><li>Operational Excellence<ul><li>Description:<ul><li>CRM system: better connections, personalized follow ups, measurement of impact&nbsp;</li><li>Website relaunch: better address target audiences in German and English, higher conversion rates for newsletter subscription</li><li>Accounting setup: Save time and reduce risk by introducing software and processes</li><li>Legal review: Reduce risks by reviewing contracts and processes</li><li>Hiring</li><li>Healthy organisational culture and leadership (e.g. 1-1s, team retreats, encouragement for self development)</li><li>Networking and discovery of funding opportunities</li></ul></li><li>Goal: Long-term provision of high-quality programs</li><li>Measure: Efficiency Services (e.g. times saved of team via processes and overall higher probability of EAD having impact in future)</li></ul></li><li>Exploration and Impact Estimation<ul><li>Description:<ul><li>Estimate impact for existing and future programs</li><li>Define prototypes for new programs and success criteria</li><li>Run exploratory programs</li></ul></li><li>Goal: Continuously explore new impactful and scalable programs that can increase the impact returns for EAD</li><li>Measure: See individual program (Existing programmes are evaluated with KPIs. New programs are measured using the model introduced in&nbsp;<a href=\"https://docs.google.com/document/d/1qzmr2o1L9dYOW5FLjuGNi9ayMZYHMXQ4DrJd4yXbnUg/edit#heading=h.mcokh3x9bfrl\"><u>Impact Considerations</u></a>)</li></ul></li></ul><h2>Exploratory Programs</h2><p>We are planning to use the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Lean_startup\"><u>lean startup</u></a> methodology as a basis for developing and testing new programs. The potential programmes could cover a wide range of topics, such as new volunteer programs, funding diversification, outreach to specific target groups or incubating new organisations. After developing&nbsp;<u>actionable impact metrics</u>, we will use an iterative process:</p><ol><li>Build:&nbsp;<ol><li>Prioritize programs based on expected impact and scalability</li><li>Develop a minimum viable product (MVP)</li></ol></li><li>Measure: Test the MVP and gather impact data</li><li>Learn:&nbsp;<ol><li>Decide to shut down, scale or pivot the program</li><li>Write up a report if others might be able to learn from our experiences</li></ol></li></ol><p>The bandwidth of programs we can develop and test will depend on the capacity of employees and volunteers. With more team members, we are able to scale by having parallel streams of the build-measure-learn cycle.</p><p>If we identify programs with higher impact opportunities, we might prioritise scaling them over launching further exploratory programs for some time.</p><p>As we are not sure at this stage what exploratory programs we will pursue and we want to avoid confusion, this strategy does not include a list of possible programmes.</p><h1>Confusions &amp; Questions</h1><ul><li>Is the impact model outlined applicable and can it be fleshed out to make it usable in the German context?</li><li>Will it be possible to model the impact of EAD\u2019s work on local groups, given the indirect approach, different group sizes and the missing precedent of national groups supporting many local groups?</li><li>How much staff time will be spent on foundational programs, and what will the bandwidth be for exploratory programs?</li><li>Is the number and size of the foundational programs too big?</li><li>Can the concept of Lean Startup be used in the context of developing programs for community building?</li><li>Is the focus on participants and contributors in the funnel model addressing the right needs, or should there be more focus either on reaching more audience/followers (e.g. through media work) or guiding contributors to become core members?</li><li>What blind spots have we missed in the potential failure modes?</li><li>Are the risks of the public image of EA deteriorating adequately addressed in the potential failure modes?</li><li>What is the capacity for ambitious growth, and how many new members could the team handle? What is the risk of creating bottlenecks in the funnel through quick scaling?</li></ul>", "user": {"username": "Sarah Tegeler"}}, {"_id": "aWr4rMf7ZhoCAtoMc", "title": "Skill up in ML for AI safety with the Intro to ML Safety course (Spring 2023)", "postedAt": "2023-01-05T11:02:34.215Z", "htmlBody": "<p><strong>The Intro to ML Safety course covers foundational techniques and concepts in ML safety for those interested in pursuing research careers in AI safety, with a focus on empirical research. </strong>We think it's a good fit for people with ML backgrounds who are looking to get into empirical research careers focused on AI safety.</p><p>Intro to ML Safety is run by the&nbsp;<a href=\"https://safe.ai/\"><u>Center for AI Safety</u></a> and designed and taught by&nbsp;<a href=\"http://danhendrycks.com/\"><u>Dan Hendrycks</u></a><u>,</u> a UC Berkeley ML PhD and director of the&nbsp;<a href=\"http://safe.ai\"><u>Center for AI Safety</u></a><u>.</u></p><h3><a href=\"https://airtable.com/shredY1461hyCVYC6\"><i><strong><u>Apply to be a participant</u></strong></i></a><i><strong> by January 29th, 2023</strong></i></h3><p><i>Website:&nbsp;</i><a href=\"https://www.mlsafety.org/intro-to-ml-safety\"><i><u>mlsafety.org/intro-to-ml-safety</u></i></a></p><h2>About the Course</h2><p>Intro to ML Safety is an 8-week virtual course that aims to introduce students with a deep learning background to the latest empirical AI Safety research. The program introduces foundational ML safety concepts such as robustness, alignment, monitoring, and systemic safety.</p><p>The course takes 5 hours a week, and consists of a mixture of:</p><ul><li>Assigned readings and lecture videos (<i>publicly available at&nbsp;</i><a href=\"https://course.mlsafety.org/\"><i><u>course.mlsafety.org</u></i></a><i><u>)</u></i></li><li>Homework and coding assignments</li><li>A facilitated discussion session with a TA and weekly optional office hours</li></ul><p>The course will be virtual by default, though in-person sections may be offered at some universities.<br>&nbsp;</p><h3>The Intro to ML Safety curriculum</h3><p>The course covers:</p><ol><li><strong>Hazard Analysis</strong>: an introduction to concepts from the field of hazard analysis and how they can be applied to ML systems; and an overview of standard models for modelling risks and accidents.</li><li><strong>Robustness</strong>: Robustness focuses on ensuring models behave acceptably when exposed to abnormal, unforeseen, unusual, highly impactful, or adversarial events. We cover techniques for generating adversarial examples and making models robust to adversarial examples; benchmarks in measuring robustness to distribution shift; and approaches to improving robustness via data augmentation, architectural choices, and pretraining techniques.</li><li><strong>Monitoring</strong>: We cover techniques to identify malicious use, hidden model functionality and data poisoning, and emergent behaviour in models; metrics for OOD detection; confidence calibration for deep neural networks; and transparency tools for neural nets.</li><li><strong>Alignment</strong>: We define alignment as reducing inherent model hazards. We cover measuring honesty in models; power aversion; an introduction to ethics; and imposing ethical constraints in ML systems.</li><li><strong>Systemic Safety</strong>: In addition to directly reducing hazards from AI systems, there are several ways that AI can be used to make the world better equipped to handle the development of AI by improving sociotechnical factors like decision making ability and safety culture. We cover using ML for improved epistemics; ML for cyberdefense; &nbsp;and ways in which AI systems could be made to better cooperate.</li><li><strong>Additional X-Risk Discussion</strong>: The last section of the course explores the broader importance of the concepts covered: namely, existential risk and possible existential hazards. We cover specific ways in which AI could potentially cause an existential catastrophe, such as weaponization, proxy gaming, treacherous turn, deceptive alignment, value lock-in, and persuasive AI. We introduce some considerations for influencing future AI systems; and introduce research on selection pressures.&nbsp;</li></ol><h3>How is this program different from AGISF?</h3><p>If you are interested in an empirical research career in AI safety, then &nbsp;you are in the target audience for this course. The ML Safety course does not overlap much with AGISF, so we expect that participants who both have and have not previously done AGISF to get a lot out of Intro to ML Safety.</p><p>Intro to ML Safety is<strong> focused on ML empirical research&nbsp;</strong>rather than conceptual work. Participants are required to watch recorded lectures and complete homework assignments that test their understanding of the technical material.&nbsp;</p><p>You can read about more the ML safety approach in&nbsp;<a href=\"https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5\"><u>Open Problems in AI X-risk</u></a>.</p><h2>Time Commitment</h2><p>The program will last 8 weeks, beginning on February 20th and ending on April 21th. Participants are expected to commit at least 5 hours per week. This includes ~1 hour of recorded lectures, ~1-2 hours of readings, ~1-2 hours of written assignments, and 1 hour of discussion.&nbsp;</p><p>We understand that 5 hours is a large time commitment, so to make our program more inclusive and remove any financial barriers,&nbsp;<strong>we will provide a $500 stipend upon completion of the course</strong>.</p><h2>Eligibility</h2><p>The prerequisites for the course are:</p><ul><li>Familiarity with deep learning (e.g. a college course)<ul><li>Watch&nbsp;this <a href=\"https://docs.google.com/presentation/d/15yMNlkWAL5cuSHHZe1gy2sM8zcN8gHk9iBVzKKvS9zw/edit#slide=id.g126975c12ec_1_54\"><u>deep learning review</u></a> to check your level of knowledge.</li></ul></li><li>Linear algebra or introductory statistics (e.g. AP Statistics)</li><li>Multivariate differential calculus</li></ul><h3><a href=\"https://airtable.com/shredY1461hyCVYC6\"><i><strong><u>Apply to be a participant</u></strong></i></a><i><strong> by January 29th, 2023.</strong></i></h3><p><i><strong>Website:&nbsp;</strong></i><a href=\"https://www.mlsafety.org/intro-to-ml-safety\"><i><strong><u>mlsafety.org/intro-to-ml-safety</u></strong></i></a><br>&nbsp;</p>", "user": {"username": "james_aung"}}, {"_id": "BiQe6Nt9JyCwcpaaB", "title": "New book: The Tango of Ethics: Intuition, Rationality and the Prevention of Suffering", "postedAt": "2023-01-02T08:45:43.183Z", "htmlBody": "<p>As mentioned in a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/F2YfRtMvHfRJibwkj/promoting-compassionate-longtermism\"><u>recent post</u></a>, I have a new book being published this week, titled&nbsp;<i>The Tango of Ethics: Intuition, Rationality and the Prevention of Suffering</i>. It\u2019s rooted in reflections I\u2019ve had on ethics and value since some of my earliest interactions with EAs ten years ago, and my observation that some specific ways of thinking about ethics that were already mainstream in the EA community could legitimately be challenged. These include common intuitions about notions like&nbsp; \u201cgood\u201d, \u201cbad\u201d and \u201cvalue\u201d that have been imported into rational arguments about ethics, without necessarily being put into question or analysed more deeply. The project expanded into a broader reflection on ethics and the dance between intuition and rationality that I think is fundamental to ethical thinking and practice.</p><figure class=\"image image_resized\" style=\"width:63.09%\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1672130658/mirroredImages/BiQe6Nt9JyCwcpaaB/mpaeovjrlpov41qrz1jy.jpg\"></figure><p><br>Some of the claims I make may appear counterintuitive or conflict with beliefs that are strongly held by many others in the EA community. However, I urge people to consider reading the book with an open mind. Over the years that it has taken shape, I\u2019ve continued to reevaluate my arguments and I remain confident that they have merit. Many of the individual ideas aren\u2019t novel, and are even subscribed to by a subset of EAs. But aside from offering some new perspectives, one of my main goals is to offer a more \u201cholistic\u201d way of thinking about ethics that integrates several core ideas, and that is aligned with solid truths about reality, including the content of subjective experience.</p><p>My hope is that the book will provoke reflection within the EA community about the foundations of our core values and how we think about \u201cdoing good\u201d. Although I defend a form of negative utilitarianism I call \u201cxNU+\u201d, I show that it doesn\u2019t need to lead to nihilism, especially within the framework I propose. It doesn\u2019t negate self-preservation and the search for meaning, caring about the welfare of future sentient beings, or striving to realise an optimistic vision for the future. But I do argue for the importance of preventing intense and especially extreme/unbearable suffering as an essential ethical principle \u2013 and by extension, that only a future that encodes and reflects this principle is a reasonable one to try to preserve.</p><p>The book is available in paperback and e-book from Amazon (<a href=\"https://www.amazon.co.uk/Tango-Ethics-Intuition-Rationality-Prevention/dp/1788360885\"><u>UK</u></a>,&nbsp;<a href=\"https://www.amazon.com/dp/B0BQYTSFSR\"><u>US</u></a>, <a href=\"https://www.amazon.de/-/en/Jonathan-Leighton/dp/1788360885/\">Germany</a>...), <a href=\"https://www.barnesandnoble.com/w/the-tango-of-ethics-jonathan-leighton/1141988924\">B&amp;N</a>, and also directly from the&nbsp;<a href=\"http://books.imprint.co.uk/book/?gcoi=71157100024780\"><u>publisher</u></a> in the UK (paperback only).</p><p>&nbsp;</p><h3>Description from the publisher\u2019s page</h3><blockquote><p>Despite existing for thousands of years, the field of ethics remains strongly influenced by several largely unquestioned assumptions and cognitive biases that can dramatically affect our priorities.&nbsp;<i>The Tango of Ethics: Intuition, Rationality and the Prevention of Suffering</i> proposes a deep, rigorous reassessment of how we think about ethics. Eschewing the traditional language of morality, it places a central emphasis on phenomenological experience and the unique urgency of suffering wherever it occurs, challenges our existence bias and examines the consequences of a metaphysically accurate understanding of personal identity.</p><p>A key paradigm in&nbsp;<i>The Tango of Ethics</i> is the conflict and interplay between two fundamentally different ways of seeing and being in the world \u2014 that of the intuitive human being who wants to lead a meaningful life and thrive, and that of the detached, rational agent who wants to prevent unbearable suffering from occurring. Leighton aims to reconcile these two stances or motivations within a more holistic framework he labels 'xNU+' that places them at distinct ethical levels. This approach avoids some of the flaws of classical utilitarianism, including the notion that extreme suffering can be formally balanced out by enough bliss, while maintaining a focus on impact. He also identifies some of the limits of rationality and our dependence on intuitions to make ethical decisions.</p><p>The book explores the implications of this way of thinking for real-world ethical dilemmas and how we might incorporate it into governance. With societal collapse, increasing totalitarianism and artificial general intelligence all very real threats in the coming years, Leighton argues that it is as important as ever to promote these ethics and their implementation while there is still an opportunity for some convergence around what matters.</p></blockquote><p>&nbsp;</p><h3>A few blurbs</h3><blockquote><p>\u201cNearly all of us, philosophers or not, should attach much greater weight to the prevention of severe suffering. This engaging, accessible, and wide-ranging book provides many highly suggestive arguments in support of this claim.\u201d</p><p><strong>-Roger Crisp, Professor of Moral Philosophy, University of Oxford</strong></p><p>&nbsp;</p><p>\u201cLeighton\u2019s&nbsp;<i>The Tango of Ethics</i> is a well-researched book addressing the most important topic in our universe \u2014 the suffering of sentient beings and its implications for ethics, philosophy and technology.\u201d</p><p><strong>-Roman V. Yampolskiy, Associate Professor of Computer Science and Engineering, University of Louisville, author of&nbsp;</strong><i><strong>Artificial Superintelligence: A Futuristic Approach</strong></i></p><p>&nbsp;</p><p>\u201cIn&nbsp;<i>The Tango of Ethics</i>, Leighton explores the most important question we can ask ourselves: what matters? Questioning old assumptions, he charts a radical yet pragmatic path forward that is consistent both with the constraints of rationality and with our deepest needs and intuitions.\u201d</p><p><strong>-Magnus Vinding, co-founder of the Center for Reducing Suffering, author of&nbsp;</strong><i><strong>Suffering-Focused Ethics</strong></i><strong> and&nbsp;</strong><i><strong>Reasoned Politics</strong></i></p></blockquote><h3><br>List of chapters and sections</h3><blockquote><p><strong>1. Ethics as an Authentic Dance</strong></p><p>The path of truth and compassion</p><p>Post-Battle assessment</p><p>Can ethics help us improve the world?</p><p>Rethinking ethics</p><p>The tango of ethics</p><p>Can one be too transparent about ethics?</p><p>Adjusting priorities</p><p>&nbsp;</p><p><strong>2. Intuition and Rationality</strong></p><p>Intuition and its roots</p><p>The role of rationality</p><p>Managing expectations: the limits of rationality<br>&nbsp;</p><p><strong>3. Ethics: What is the Question?</strong></p><p>Understanding oughtism</p><p>Consequentialism: impact matters</p><p>Utilitarianism: impact on wellbeing matters</p><p>Deontology: follow the rules</p><p>Virtue ethics: be good</p><p>Can any one theory be correct?</p><p>&nbsp;</p><p><strong>4. Ethics and Subjective Experience</strong></p><p>Hedonic states and wellbeing</p><p>Preference satisfaction</p><p>Interests</p><p>Suffering</p><p>The notion of urgency</p><p>The significance of extreme and unbearable suffering</p><p>Buddhism and craving</p><p>Voluntary suffering</p><p>Happiness and wellbeing</p><p>Capturing the dynamics of hedonic states</p><p>Absence of suffering: from hedonic zero to bliss<br>&nbsp;</p><p><strong>5. Evaluating Value</strong></p><p>The confusion about value and the compulsion to create it</p><p>The fundamental ethical asymmetry between suffering and happiness</p><p>Negative utilitarianism</p><p>&nbsp;</p><p><strong>6. The Map and the Territory</strong></p><p>The mathematics of suffering</p><p>Measuring suffering</p><p>The hedonic delusion</p><p>Lost in aggregation</p><p>&nbsp;</p><p><strong>7. Determining Priorities</strong></p><p>Intensity vs. instances: the essence of uncertainty</p><p>Comparing physical pain and psychological suffering</p><p>Unbearable suffering as an ethical tipping point</p><p>Expected value and cause prioritisation</p><p>&nbsp;</p><p><strong>8. Suffering and the Illusion of Separateness</strong></p><p>The true nature of personal identity</p><p>The Golden Rule</p><p>Rawls\u2019s veil of ignorance</p><p>Anti-speciesism</p><p>Awakening awareness</p><p>&nbsp;</p><p><strong>9. Our Complex Relationship with Suffering</strong></p><p>The fleetingness of momentary decisions</p><p>Voluntary personal sacrifices don\u2019t justify imposing suffering on others</p><p>Tolerating the intolerable</p><p>The need for systems that are more rational and compassionate than we are</p><p>The intuition towards fairness and against the concentration of suffering</p><p>&nbsp;</p><p><strong>10. Existence</strong></p><p>A life worth living</p><p>Escaping the Repugnant Conclusion</p><p>Why non-existence isn\u2019t a bad thing</p><p>Reducing existential risk: an intuition with conditions</p><p>Preserving consciousness</p><p>&nbsp;</p><p><strong>11. A Holistic Ethical Framework</strong></p><p>Key principles</p><p>xNU+</p><p>How xNU+ compares to prioritarianism</p><p>How xNU+ responds to common objections to negative utilitarianism</p><p>Consistency: being truthful and rational</p><p>How everything is connected by utilitarianism</p><p>How obsessive utilitarianism can be self-defeating</p><p>&nbsp;</p><p><strong>12. Current and Potential Causes of Intense Suffering</strong></p><p>Human suffering</p><p>Abuse and torture of non-human animals</p><p>Nature and wild animal suffering</p><p>Insect and other invertebrate suffering</p><p>Far future suffering</p><p>Artificial/machine suffering<br>&nbsp;</p><p><strong>13. A Tangible Tango: Resolving Ethical Conflicts</strong></p><p>Helping those closest vs. helping strangers</p><p>War</p><p>Animal experimentation</p><p>The grey zone of animal exploitation</p><p>Veganism vs. reducing suffering: is eating animal products ever justifiable?</p><p>Eating oysters and other brainless invertebrates</p><p>Painlessly killing happy animals</p><p>Euthanasia of suffering animals</p><p>Euthanasia and assisted suicide in humans</p><p>Saving lives vs. preventing suffering</p><p>Anti-natalism</p><p>The meat-eater problem</p><p>Preserving the environment vs. reducing wild animal suffering</p><p>&nbsp;</p><p><strong>14. From Ethics to Action</strong></p><p>Reflections on the ethical tango</p><p>Creating a new suffering metric for health economics</p><p>Impacting the far future</p><p>Designing compassionate blueprints for governance based on xNU+ ethics</p><p>The last tango: embedding xNU+ ethics into AGI</p><p>Balancing personal initiative and collective action</p><p>Activism and the desire to see impact</p><p>How much empathy do we need?</p><p>The fractal-like nature of ethical action</p><p>Spreading love, empathy, rationality and compassion</p></blockquote>", "user": {"username": "jonleighton"}}, {"_id": "Xinx6TXx9T79oowBw", "title": "Effective Altruism Reading List Information Design Poster (2/2)", "postedAt": "2023-01-06T14:49:46.557Z", "htmlBody": "<p>Earlier I announced the project to co-create an effective altruism reading list information design poster. Now follows the announcement of the (preliminary) final book list and poster design.</p><h1><strong>Recapitulation of the first post&nbsp;</strong></h1><p>The<a href=\"https://forum.effectivealtruism.org/posts/TzooJmtZvxtK2kBQi/co-creation-of-the-library-of-effective-altruism-information\">&nbsp;<u>first post described the reasons for, process, and preliminary result of creating an information design poster of the essential books on effective altruism.</u></a></p><ul><li>After identifying and summarizing existing book surveys and lists in the EA sphere top-down, a significant part of the process was (and is) the idea of bottom-up co-creation, being open to comments, and the option to take a survey.</li><li>The goal of the project is to provide a visual representation of the knowledge space within effective altruism represented by its books and to offer a surface for discussion and disagreement about the included books.&nbsp;</li><li>The finished poster can then serve as a guide for self-study and function as a quality control measure to ensure that foundational ideas are understood.&nbsp;&nbsp;</li><li>The final design will be a digital document or a physical poster that can be transformed into a scratch-off poster.</li><li>A one-month salary from the Center for Effective Altruism funded the project.</li><li>The value of information mapping was demonstrated by showcasing other visual maps of knowledge spaces.</li><li>A first list of books sources, a list of books, and the first draft of the poster were introduced.</li><li>Readers could write comments and take a survey to add their perspectives.&nbsp;</li></ul><h1>What has happened since</h1><ol><li>Since the first post was published, the comments and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/TzooJmtZvxtK2kBQi/co-creation-of-the-library-of-effective-altruism-information?commentId=8nLx2jjhZjLweghrs\"><u>survey results</u></a> were analyzed.</li><li>A final list was conducted.</li><li>Further design options were considered, and a final design decision was made.</li><li>(Two) final designs were created<br>&nbsp;</li></ol><h1>The Final List</h1><p>Top Books:</p><ol><li>Doing Good Better (William MacAskill)</li><li>The Precipice (Toby Ord)</li><li>The Life You Can Save (Peter Singer)</li><li>EA Handbook (<a href=\"http://www.stafforini.com/docs/Carey%20-%20The%20effective%20altruism%20handbook.pdf\">version 1</a>, <a href=\"https://assets.ctfassets.net/ohf186sfn6di/glbXAUtnb2QagqY88qy4s/f8da9e4617efb89c0f79bf592b3f7ecd/Effective_Altruism_Handbook.pdf\">version 2</a>, <a href=\"https://forum.effectivealtruism.org/handbook\">version 3</a>)</li><li>80.000 Hours (Ben Todd)</li><li>What We Owe The Future (William MacAskill)</li><li>Poor Economics (Abhijit Banerjee &amp; Esther Duflo)</li><li>The Scout Mindset (Julia Galef)</li><li>Thinking, Fast &amp; Slow (Daniel Kahneman)</li><li>Superforecasting (Philip Tetlock, Dan Gardner)</li><li>Rationality (Steven Pinker)</li><li>Superintelligence (Nick Bostrom)</li><li>Animal Liberation (Peter Singer)</li><li>The Alignment Problem (Brian Christian)</li><li>The Most Good You Can Do (Peter Singer)<br>&nbsp;</li></ol><p>Longer List:</p><ol><li>Reasons and Persons (Derek Parfit)</li><li>On What Matters Volume 3 (Derek Parfit)</li><li>Practical Ethics (Peter Singer)</li><li>On Liberty &amp; Utilitarianism (John Stuart Mill)</li><li>Moral Tribes: Emotion, Reason, and the Gap Between Us and Them (Joshua Greene)</li><li>Suffering-Focused Ethics: Defense and Implications (Magnus Vinding)</li><li>The Moral Landscape (Sam Harris)</li><li>Against Empathy (Paul Bloom)</li><li>The Expanding Circle: Ethics, Evolution, and Moral Progress (Peter Singer)</li><li>Ethics in the Real World (Peter Singer)</li><li>Moral Uncertainty (Krister Bykvist, William MacAskill, Toby Ord)</li><li>Strangers Drowning (Larissa MacFarquhar)</li><li>The Methods of Ethics&nbsp;(Henry Sidgwick)</li><li>Oxford Handbook of Population Ethics (Gustaf Arrhenius, Krister Bykvist, Tim Campbell, Elizabeth Finneron-Burns)</li><li>Effective Altruism. Philosophical Issues (Hilary Greaves, Theron Pummer)</li><li>Good &amp; Real (Gary Drescher)</li><li>Algorithms to Live By (Tom Griffiths &amp; Brian Christian)</li><li>Model Thinker (Scott Page)</li><li>The Elephant in the Brain: Hidden Motives in Everyday Life (Robin Hanson &amp; Kevin Simler)</li><li>Beginning of Infinity (David Deutsch)</li><li>Rationality: From AI to Zombies (Eliezer Yudkowsky)</li><li>Fooled by Randomness (Nassim Taleb)</li><li>Slate Star Codex (Abridged) (Scott Alexander)</li><li>Handbook of Bounded Rationality (Riccardo Viale)</li><li>Smarter Than Us (Stuart Armstrong)</li><li>Human Compatible (Stuart Russell)</li><li>Life 3.0 (Max Tegmark)</li><li>Age of Em (Robin Hanson)</li><li>The End of Animal Farming (Jacy Reese)</li><li>How to create a vegan world: A pragmatic approach (Tobias Leenaert)</li><li>Flourish (Martin Seligman)</li><li>The Origins of Happiness (Clark, Fl\u00e8che, Layard,&nbsp; Powdthavee, Ward)</li><li>The Age of Sustainable Development (Jeffrey Sachs)</li><li>How to spend 75 Billion to Make the World a Better Place (Bjorn Lomborg)</li><li>Nudge (Richard Thaler &amp; Cass Sunstein)</li><li>The strategy of Conflict (Thomas Schelling)</li><li>The Art of Gathering (Priya Parker)</li><li>How to Measure Anything (Douglas W. Hubbard)</li><li>Inadequate Equilibria (Eliezer Yudkowsky)</li><li>Governing the Commons (Elinor Ostrom)</li><li>Thinking in Systems (Donella Meadows)</li><li>The Righteous Mind (Jonathan Haidt)</li><li>Modern Principles of Economics (Alex Tabarrok &amp; Tyler Cowen)</li><li>Radical Markets (Glen Weyl &amp; Eric Posner)</li><li>Expert Political Judgement (Philip E. Tetlock)</li><li>Global Catastrophic Risks (Milan M. \u0106irkovi\u0107 &amp; Nick Bostrom)</li><li>The Doomsday Machine (Daniel Ellsberg)</li><li>The Myth of the rational voter (Bryan Caplan)</li><li>Handbook of Rationality (Markus Knauff &amp; Wolfgang Spohn)</li><li>How to Launch a High-Impact Non-Profit (Antonia Shann, Joey Savoie, Patrick Stadler)</li></ol><p>&nbsp;</p><p>While the list is the final list of this particular project, one of its main functions has always been to create a surface for debate. If you disagree and have additional information, yes, please do comment; it's a process.</p><p>I have added many additional books you suggested to me. However, one of the central insights from the comments and the survey was to reduce dramatically (\u201ccull!\u201d) the number of books. Similar to GiveWell\u2019s idea of a small list of top charities, books have to be in a similar ballpark of information value to be included. The first draft included many books that, while good books and somewhat relevant, upon further inspection, did not play in a similar league to one another. All of these have been removed, and the list has been shortened.</p><p>The second-order effect of the same idea was to introduce two tiers of books. A shorter list for \u201ctop books\u201d that play an undeniably essential role, and a longer one for quite important books that many agree to be valuable, without being of the same importance as the top books.&nbsp;</p><p>Another insight was that the original list included many books I wanted to include. With the intention of putting myself completely into the service of a good representation of the community consensus for this project, I overcame my motivated reasoning here to my best capability. I hope to write a post about what books I think more of you should read, but that's a different project.</p><p>Some comments on two books that are included in the \"top books\" list:</p><ol><li><strong>The EA Handbook</strong> (<a href=\"http://www.stafforini.com/docs/Carey%20-%20The%20effective%20altruism%20handbook.pdf\">version 1</a>, <a href=\"https://assets.ctfassets.net/ohf186sfn6di/glbXAUtnb2QagqY88qy4s/f8da9e4617efb89c0f79bf592b3f7ecd/Effective_Altruism_Handbook.pdf\">version 2</a>, <a href=\"https://forum.effectivealtruism.org/handbook\">version 3</a>): While the older version has been available as a booklet, the <a href=\"https://forum.effectivealtruism.org/handbook\">newest version</a> doesn't officially fulfill the criterion of, well, being a book. As the handbook is a central document of the EA community and is long enough to be easily transposed into an actual book, I find the choice justified. Not mentioning it among the most important book-like sources would feel like a misrepresentation. &nbsp;I also liked the option to give out the older EA Handbook physically in outreach attempts; it would be great if the newest version, again, would also be available as a book(let). If you agree with this, acknowledge this <a href=\"https://www.effectivealtruism.org/handbook\">here</a>, there has also been t<a href=\"https://forum.effectivealtruism.org/posts/DcFktJBw4AKhqnXPm/pdf-of-ea-handbook\">his debate about it here</a>. I intentionally meant to point to all three handbooks, as it implicitly also transports some knowledge about how the community has developed.&nbsp;</li><li><strong>80.000 Hours</strong>: While the <a href=\"https://80000hours.org/book/#:~:text=You%20have%20about%2080%2C000%20hours%20in%20your%20career%2C%20which%20makes,the%20world's%20most%20pressing%20problems.\">book by Ben Todd</a> captures some core tenets of the 80.000 Hours memeplex, it is not up to date and does not capture many other important parts of their research, like problem profiles. Having an updated atlas of all the written content by 80.000 Hours would be great in light of this project while understanding that it is not a high priority to create.</li></ol><h1>The Final Design</h1><p>The updated design now represents the updated list of books in two tiers. The first and simpler poster represents the top books only. The second represents both tiers, the top books by their spines on a shelf and the others by their cover.</p><h3>Top Books Only</h3><p><a href=\"https://www.canva.com/design/DAFWPjGAWqw/2wgldwH5q8TYnrqXqvMmBg/edit?utm_content=DAFWPjGAWqw&amp;utm_campaign=designshare&amp;utm_medium=link2&amp;utm_source=sharebutton\"><u><img src=\"https://res.cloudinary.com/cea/image/upload/v1673016588/mirroredImages/Xinx6TXx9T79oowBw/lkzfyrqrvysncdzgcz8d.png\"></u></a></p><h2>Library of effective altruism (Top Books+)</h2><p>The design was also adapted to resemble the <a href=\"https://www.effectivealtruism.org/\">EA corporate identity - in colors, type, and design - more closely, as inferred from the effectivealtruism.org website</a>.</p><p>The choice that books in the second tier are cut off at the borders is meant to suggest that this is a non-exhaustive selection; the actual list continues beyond what is depicted here.</p><p><a href=\"https://www.canva.com/design/DAFWOleu1n0/fLQD-lWoehQgmZEicVm9ug/edit?utm_content=DAFWOleu1n0&amp;utm_campaign=designshare&amp;utm_medium=link2&amp;utm_source=sharebutton\"><u><img src=\"https://res.cloudinary.com/cea/image/upload/v1673016588/mirroredImages/Xinx6TXx9T79oowBw/i7vmuu8x8f511aoky3uc.png\"></u></a></p><p>The initial plan, upon feedback on design options, was to create a full design that depicts book spines to resemble a bookshelf. Sadly, photos of book spines were difficult to acquire digitally. Recreating book spine covers as vector graphics (from crowd-sourced photos) was an additional project that turned out to be out of the scope of the time investment committed to.&nbsp;</p><h1><strong>Learnings and Conclusions</strong></h1><p>The current poster should still get overhauled by a graphic designer. Then a layer for print and scratch-off functionality can get added.</p><p>I knew before that the unit of knowledge in the community is not the book. While starting with a book poster makes sense, as the problem is more tractable, a similar project about essays, book chapters, concepts, organizations, people, and timelines of (past) events, would be even more interesting.</p><p>Beyond the problem of books not being the real \u201cunit of knowledge\u201d of effective altruism, one list of books winning along \u201cone dimension of value\u201d will make an implicit statement about the relative importance of different causes by the number of books that represent them. Books have value along multiple dimensions and are of different relevance to different people, causes, and goals. There is no way to capture it all with one list like this.&nbsp;</p><p>I look forward to more of you getting inspired to make information maps, posters, and big-picture visualizations for more specialized causes, topics, or fields of action, like, e.g.,<a href=\"https://forum.effectivealtruism.org/posts/npNt43QRnaRNRixXK/map-of-biosecurity-interventions\"><u> the map of biosecurity interventions</u></a>.</p><p><br>&nbsp;</p><p>&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "annaleptikon"}}, {"_id": "SQ2ayhoYBJJCrFQjd", "title": "What are the most underrated posts & comments of 2022, according to you?", "postedAt": "2023-01-01T22:44:25.254Z", "htmlBody": "<p>Share your views in the comments!</p><p>To make this clear and easy to follow, please use these guidelines:</p><ul><li>Use the template below.&nbsp;</li><li>Post as many items as you want.&nbsp;</li><li>One item per comment, so that it's easy for people to read and react.</li><li>(Optional, encouraged) Highlight at least one of your own contributions.</li></ul><p>If you need some inspiration, open your <a href=\"https://forum.effectivealtruism.org/wrapped\">EA Forum Wrapped </a>and scroll to the bottom of your \"Strong Upvoted\" list.</p><h2>Template</h2><blockquote><p><strong>Title:</strong></p><p><strong>Author:</strong></p><p><strong>URL:</strong></p><p><strong>Why it's good:</strong></p></blockquote><p>If you're sharing an underrated comment, set the title to \"[Username] on [topic]\".</p>", "user": {"username": "Peter_Hartree"}}, {"_id": "vDvcRQ8yeh6XXoHgx", "title": "AI Safety Doesn't Have to be Weird", "postedAt": "2023-01-02T21:56:55.112Z", "htmlBody": "<p>Whenever I hear people say that effective altruism is weird, the first example I hear is the focus on AI safety. This point never made much sense to me. For one thing, as Peter Singer once pointed out, you can't really criticize effective altruism for not picking the right cause areas. What you would be criticizing are this community's conclusions, and not effective altruism itself. But lately I've been wondering about how to make this cause area more appealing. I'm in charge of the club at the Rochester Institute of Technology, and wanted to publicize our AI Safety discussion. Our posters typically contain a subtitle giving a little more information about the topic. Here's the poster we ended up making for the event:</p><figure class=\"image image_resized\" style=\"width:41.6%\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1672696615/mirroredImages/vDvcRQ8yeh6XXoHgx/ndrtgb7vsjr8cr7yc6t6.png\" alt=\"The title of the poster is &quot;A.I. Safety&quot; and the subtitle is &quot;A.I. could exceed human intelligence within decades. Will it be used for good?&quot;\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1672696616/mirroredImages/vDvcRQ8yeh6XXoHgx/uvnuuoimop3ulro5dwyc.png 88w, https://res.cloudinary.com/cea/image/upload/v1672696616/mirroredImages/vDvcRQ8yeh6XXoHgx/e30m8mmr7xaiq8ocl5gj.png 168w, https://res.cloudinary.com/cea/image/upload/v1672696616/mirroredImages/vDvcRQ8yeh6XXoHgx/a641x6aemyqy6gjdntfi.png 248w, https://res.cloudinary.com/cea/image/upload/v1672696616/mirroredImages/vDvcRQ8yeh6XXoHgx/ehfseefvhdlql9ltanpi.png 328w, https://res.cloudinary.com/cea/image/upload/v1672696616/mirroredImages/vDvcRQ8yeh6XXoHgx/ucsercb0ldcopgyg6glh.png 408w, https://res.cloudinary.com/cea/image/upload/v1672696616/mirroredImages/vDvcRQ8yeh6XXoHgx/lmxbmiom5pisame0yg1g.png 488w, https://res.cloudinary.com/cea/image/upload/v1672696616/mirroredImages/vDvcRQ8yeh6XXoHgx/v22oduleboamnqdmmz6p.png 568w\"></figure><p>I will say that because of this poster, two new people joined our club, which is far better than how we usually do. I don't think it was entirely due to our phrasing. It is a tech school, after all. But I don't think the way we decided to talk about it hurt us much. So I wanted to write a post talking about how we can think about AI safety, without looking like schizophrenics.</p><h2>The Pitch</h2><p>I can't find the tweet now, but what inspired this post was made probably a year ago. The idea is that one of the weirdest things about effective altruism is how much time the community spends on A.I. &nbsp;taking over the world, which seems to not make much progress and is a questionable fear to begin with.</p><p>I think this is understandable point of view if you first heard about AI safety. AI destroying humanity is something that typically happens in sci-fi movies. But sci-fi movies do a lot of ridiculous things. Nobody's ever going to take seriously the idea that we should spend taxpayer money researching the ethics of Star Trek transporters. But what if, instead of thinking of it from the point of view of technology that doesn't yet exist, we think of AI safety in terms of what already exists?</p><p>Machine learning algorithms already exist. Machine learning algorithms are already problematic. It was something we discussed in one of my classes<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefunu4asxaef8\"><sup><a href=\"#fnunu4asxaef8\">[1]</a></sup></span>. Specification gaming is already a problem in AI. Maybe we can use that as a basis for the approach. For example:</p><p>\"AI is getting more powerful. It also makes a lot of mistakes. And it's being used more often. How do we make sure (a) it's being used for good, and (b) it doesn't accidentally do terrible things that we didn't want.\"</p><p>In this approach to AI safety, is a problem we should want solved anyway, even if it doesn't destroy humans. AGI destroying the world is just the extreme example of something it could do.</p><h2>Possible Flaws</h2><p>Of course, I'm aware that most people who are in AI safety today are in it because of the possibility of it ending human civilization. I can imagine those people being opposed to having more people in the field who don't take it as seriously as they do. Using this approach could lead to infighting amongst the safety researchers about how important their jobs are.</p><p>Also, the way I'm talking about it doesn't really make it clear why this is an effective altruism cause area. You'd need to explain the extreme case, which is the AI destroying humanity to accomplish its goal.</p><h2>Other Benefits of this Approach</h2><p>Something that I don't think gets enough attention in AI safety is preventing bad actors from getting ahold of AGI and using it to kill people. The typical counterargument to this is that if we have an AGI that's smart enough to destroy the world, we probably also have one that's smart enough to stop other AGIs from developing. I think it's quite possible that stopping other people from creating AGIs is simply a harder problem than ending the world. So we might get a world-ending AGI before we get a superintelligence suppressing AGI. So I still think this is worth looking into. I'm talking about this here because my approach to talking about AI safety implies that this is worth looking into, rather than pushing everyone into solving the alignment problem.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnunu4asxaef8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefunu4asxaef8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The class was called \"Computation and Culture\", if you'd like to know</p></div></li></ol>", "user": {"username": "Micha White"}}, {"_id": "JGZYBDp4P5LqusZKo", "title": "Planning and documentation: should we do more (or  less)? ", "postedAt": "2023-01-02T17:35:36.545Z", "htmlBody": "<h2>Aside: This is in a series of posts I'm considering writing</h2>\n<p>I'm posting these publicly to potentially get feedback and links to other posts/essays that may be relevant or precede these.  Another concern is whether these are even relevant for EA Forum or perhaps for LessWrong instead or perhaps neither.</p>\n<h1>Do we plan and document too much or too little?</h1>\n<p>One of my frequent takes is that we (EA? Researchers? People in my circle?) Tend to charge forward too quickly without 1. Defining our plan or 2. Keeping track of what and documenting what we are doing.  <sup class=\"footnote-ref\"><a href=\"#fn-m7mACt4vBFfqAirvH-1\" id=\"fnref-m7mACt4vBFfqAirvH-1\">[1]</a></sup></p>\n<p>A common response:  these processes take too long, are way too slow and usually end up never getting used. Learning how to do this planning and documentation takes a long time in itself.</p>\n<h3>Biases towards under-planning and under-documentation</h3>\n<p>Planning and documentation...<sup class=\"footnote-ref\"><a href=\"#fn-m7mACt4vBFfqAirvH-2\" id=\"fnref-m7mACt4vBFfqAirvH-2\">[2]</a></sup></p>\n<p>The time costs of planning and documentation are very visible and are seen right away.  Furthermore, the thought that this work 'may not be used or recognized' may be particularly painful.</p>\n<p>*However, the benefits (and avoided cost) may be underestimated because... *</p>\n<ol>\n<li>They are only seen much later, e.g.,</li>\n</ol>\n<ul>\n<li>\n<p>when we come back to re-plan the second version of a campaign in another context, reanalyze the data to check an anomaly someone picked up</p>\n</li>\n<li>\n<p>when we realize that the extensive work we have done was also done by a siloed group on the other side of the world</p>\n</li>\n<li>\n<p>when we face a large disaster or massive ability to expand</p>\n</li>\n</ul>\n<ol start=\"2\">\n<li>They are 'externalized'</li>\n</ol>\n<ul>\n<li>by other people organizations we care about using our work or forking it</li>\n<li>by other divisions of our organization that don't fully recognize the benefit</li>\n<li>by ourselves or a successor group long in the future</li>\n</ul>\n<ol start=\"3\">\n<li>They are seen through 'avoidance of disaster'.</li>\n</ol>\n<p>If an avoidable disaster occurs and this causes the group to break up, the person who advocated poor/better choices cannot be rewarded or punished. There may also be information loss here, so the 'lessons' are not spread to others in the space.</p>\n<ol start=\"4\">\n<li>These benefits are low-probability but high impact (AKA 'hit-space')</li>\n</ol>\n<p>Perhaps 90% of the time the documentation is not useful, but for the remaining 10%  of the time it has a huge benefit. Perhaps a large success or failure makes replication particularly important. Perhaps a small potential error found in the program renders it extremely unclear if it has any value. Perhaps a larger organization was working on a very similar project. Perhaps the program seems to be vastly successful and has a small time window to be able to be put into policy, but this can only be done if you can clearly demonstrate all the steps you took.</p>\n<ol start=\"5\">\n<li>Poor attribution and overemphasis on the role of chance</li>\n</ol>\n<p>There's an admirable push in EA to recognize the role of uncertainty in successes and failures, and that the optimal approach often involves funding many projects that will fail. However, some types of failure might be falsely attributed to randomness. Sometimes greater planning could have either turned the project around. Sometimes you could have recognized a low chance of success and a negative expected value. If we overemphasize chance we de-incentivize this sort of planning.<sup class=\"footnote-ref\"><a href=\"#fn-m7mACt4vBFfqAirvH-3\" id=\"fnref-m7mACt4vBFfqAirvH-3\">[3]</a></sup></p>\n<h3>The lazy attractor</h3>\n<p>This may also fall under the category of a bias I'd like to discuss further perhaps in a different post: we are especially attracted to arguments that 'the easier/lazier thing to do (here, not document) is probably better anyways'. I think this is enhanced by groups and norms; if others will not hold standards then the standards have less value. And we may even be keen to punish the overachieving strivers here, or the ones that seem to be scolding us for being lazy.</p>\n<h3>Biases towards over-planning and over-documentation/ inaction bias</h3>\n<p>[to be continued]</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-m7mACt4vBFfqAirvH-1\" class=\"footnote-item\"><p>This comes somewhat out of my experience with 'replicable' coding and processes for data science and social science. <a href=\"#fnref-m7mACt4vBFfqAirvH-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-m7mACt4vBFfqAirvH-2\" class=\"footnote-item\"><p>I'm lumping these together here , but I should define and separate these later. <a href=\"#fnref-m7mACt4vBFfqAirvH-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-m7mACt4vBFfqAirvH-3\" class=\"footnote-item\"><p>Sorry, I sound like your Dad now when he says 'good luck occurs to people who worked for it, or whatever that expression is. <a href=\"#fnref-m7mACt4vBFfqAirvH-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "david_reinstein"}}]