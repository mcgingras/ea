[{"_id": "KkvXsoWZXfEcdbceo", "postedAt": "2023-12-02T12:28:17.296Z", "postId": "M6zAzCBAsBxem7Lu4", "htmlBody": "<blockquote>\n<p>extraordinary evidence would be required to move up sufficiently many orders of magnitude for an AI or bio terrorist attack to have a decent chance of causing human extinction.</p>\n</blockquote>\n<p>How extraordinary does the evidence need to be?  You can easily get many orders of magnitude changes in probabilities given some evidence.  For example, as of 1900 on priors the  probability that &gt;1B people would experience powered flight in the year 2000 would have been extremely low, but someone paying attention to technological developments would have been right to give it a higher probability.</p>\n<p>I've written something up on why I think this is likely: <a href=\"https://forum.effectivealtruism.org/posts/zPHBuMJGeJqnAec3E/out-of-distribution-bioattacks\">Out-of-distribution Bioattacks</a>.  Short version: I expect a technological change which expands which actors would try to cause harm.</p>\n<p>(Thanks for sharing a draft with me in advance so I could post a full response at the same time instead of leaving \"I disagree, and will say why soon!\" comments while I waited for information hazard review!)</p>\n", "parentCommentId": null, "user": {"username": "Jeff_Kaufman"}}, {"_id": "Rng5AXECBfBEpjaL4", "postedAt": "2023-12-02T13:11:08.942Z", "postId": "M6zAzCBAsBxem7Lu4", "htmlBody": "<p>Thanks for the comment, Jeff!</p><blockquote><p>How extraordinary does the evidence need to be?</p></blockquote><p>I have not thought about this in any significant detail, but it is a good question! I think David Thorstad's series <a href=\"https://ineffectivealtruismblog.com/category/exaggerating-the-risks/\">exagerating the risks</a> has some relevant context.</p><blockquote><p>You can easily get many orders of magnitude changes in probabilities given some evidence.&nbsp; For example, as of 1900 on priors the&nbsp; probability that &gt;1B people would experience powered flight in the year 2000 would have been extremely low, but someone paying attention to technological developments would have been right to give it a higher probability.</p></blockquote><p>Is that a fair comparison? I think the analogous comparison would involve replacing terrorist attack deaths per year by the number of different people travelling by plane per year. So we would have to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/M6zAzCBAsBxem7Lu4/can-a-terrorist-attack-cause-human-extinction-not-on-priors#Terrorist_attacks_basic_stats\"><u>assume</u></a>, in the last 51.5 years, only:</p><ul><li>9.63 k different people travelled by plane in a random single calendar year.</li><li>At most 44.6 k different people travelled by plane in a single calendar year.</li></ul><p>Then, analogously to asking about a terrorist attack causig human extinction next year, we would ask about the probability that every single human (or close) would travel by plane next year, which would a priori be astronomically unlikely given the above.</p><blockquote><p>I've written something up on why I think this is likely: <a href=\"https://forum.effectivealtruism.org/posts/zPHBuMJGeJqnAec3E/out-of-distribution-bioattacks\">Out-of-distribution Bioattacks</a>. Short version: I expect a technological change which expands which actors would try to cause harm.</p></blockquote><p>I am glad you did. It is a useful complement/follow-up to my post. I qualitatevely agree with the points you make, although it is still unclear to me how much higher the risk will become.</p><blockquote><p>(Thanks for sharing a draft with me in advance so I could post a full response at the same time instead of leaving \"I disagree, and will say why soon!\" comments while I waited for information hazard review!)</p></blockquote><p>You are welcome, and thanks for letting me know about that too!</p>", "parentCommentId": "KkvXsoWZXfEcdbceo", "user": {"username": "vascoamaralgrilo"}}, {"_id": "4AETr9RDEndzr95HC", "postedAt": "2023-12-02T21:30:56.289Z", "postId": "M6zAzCBAsBxem7Lu4", "htmlBody": "<blockquote>\n<p>I think the analogous comparison would involve replacing terrorist attack deaths per year by the number of different people travelling by plane per year. So we would have to assume, in the last 51.5 years, only:</p>\n<ul>\n<li>9.63 k different people travelled by plane in a random single calendar year.</li>\n<li>At most 44.6 k different people travelled by plane in a single calendar year.</li>\n</ul>\n</blockquote>\n<p>I don't really understand what you're getting at here? Would you be able to spell it out more clearly?</p>\n<p>(Or if someone else understands and I'm just missing it, feel free to jump in!)</p>\n", "parentCommentId": "Rng5AXECBfBEpjaL4", "user": {"username": "Jeff_Kaufman"}}, {"_id": "S9pWghFMcgdkf2vYr", "postedAt": "2023-12-02T22:39:09.993Z", "postId": "M6zAzCBAsBxem7Lu4", "htmlBody": "<p>Hi Vasco, nice post thanks for writing it! I haven't had the time to look into all your details so these are some thoughts written quickly.</p><p>I worked on a project for Open Phil quantifying the likely number of terrorist groups pursuing bioweapons over the next 30 years, but didn't look specifically at attack magnitudes (I appreciate the push to get a public-facing version of the report published - I'm on it!). That work was as an independent contractor for OP, but I now work for them on the GCR Cause Prio team. All that to say these are my own views, not OP's.</p><p>I think this is a great post grappling with the empirics of terrorism. And I agree with the claim that the history of terrorism implies an extinction-level terrorist attack is unlikely. However, for similar reasons to <a href=\"https://forum.effectivealtruism.org/posts/zPHBuMJGeJqnAec3E/out-of-distribution-bioattacks\">Jeff Kaufman</a>, I don't think this strongly undermines the existential threat from non-state actors. This is for three reasons, one methodological and two qualitative:</p><ol><li>The track record of bioterrorism in particular is too sparse to make empirical projections with much confidence. I think the rarity of bioterror and general small magnitudes of attacks justifies a prior against bioterror as a significant threat, but only a weak one. It also justifies a prior that we should expect at least a handful of groups to attempt bioterror over the next 10-30 years. To take the broader set of terror attacks as having strong implications for future bioterror, one would need to think that 'terrorism' is a compelling reference class for bio x-risk - which my next two points dispute.</li><li>The vast majority of terror groups ('violent non-state actors' is a more generally applicable handle) would not want to cause extinction. Omnicidality is a fairly rare motivation - most groups have specific political aims, or ideological motivations that are predicated on a particular people/country/sect/whatever thriving and overcoming its enemies. Aiming for civilisational collapse is slightly more prevalent, though still uncommon. And for all of history, there hasn't been a viable path to omnicide or x-risk anyway. So the kind of actor that presents a bio x-risk is probably going to be very different to the kinds of actor that make up the track record of terrorism.</li><li>The vast majority of terror attacks are kinetic - involving explosives, firearms, vehicles, melee weapons. The exceptions are chemical and biological weapons. The biological weapons chosen are generally non-transmissible - anthrax, botulinum toxin, ricin, etc. This means that chem and bio attacks also rely on delivery mechanisms that have to get each individual victim to come into contact with the agent. An attack with a pandemic-class agent would not rely on such delivery. It would be strikingly different in complexity of development, attack modality, targeting specificity, and many other dimensions. I.e. it would be very unlike almost all previous terrorist attacks. The ability to carry out such an attack is also fairly unprecedented - it may only emerge with subsequent developments in biotechnology, especially from the convergence of AI and biotechnology.</li></ol><p>So overall, compared to the threat model of future bio x-risk, I think the empirical track record of bioterrorism is too weak (point 1), and the broader terrorism track record is based on actors with very different motivations (point 2) using very different attack modalities (point 3). The latter two points are grounded in a particular worldview - that within coming years/decades biotechnology will enable biological weapons with catastrophic potential. I think that worldview is certainly contestable, but I think the track record of terrorism is not the most fruitful line of attack against it.</p><p>On a meta-level, the fact that XPT superforecasters are so much higher than what your model outputs suggests that they also think the right reference class approach is OOMs higher. And this is despite my suspicion that the XPT supers are too low and too indexed on past base-rates.</p><p>You emailed asking for reading recommendations - in lieu of my actual report (which will take some time to get to a publishable state), here's my structured <a href=\"https://docs.google.com/spreadsheets/d/1ROhsGWl5r3X183sk_4OqxY9RwHh1jq47QYwUKTssItM/edit#gid=0\">bibliography</a>! In particular I'd recommend <a href=\"https://oxfordre.com/internationalstudies/display/10.1093/acrefore/9780190846626.001.0001/acrefore-9780190846626-e-706\">Binder &amp; Ackermann 2023</a> (CBRN Terrorism) and <a href=\"https://www.tandfonline.com/doi/full/10.1080/1057610X.2022.2034852\">McCann 2021</a> (Outbreak: A Comprehensive Analysis of Biological Terrorism).</p>", "parentCommentId": null, "user": {"username": "BenStewart"}}, {"_id": "bZRKgy3JqxWJGM9E5", "postedAt": "2023-12-03T08:47:53.510Z", "postId": "M6zAzCBAsBxem7Lu4", "htmlBody": "<p>Sorry for the lack of clarity. Basically, I was trying to point out that the structure of the data we had on people travelling by plane in 1900 (<a href=\"https://en.wikipedia.org/wiki/Wright_Flyer\">only</a> in 1903) is different from that we have on terrorist attack deaths now. Then I described hypothetical data on travelling by plane with a similar structure to that we have on terrorist attack deaths now.</p><p>One could argue no people having travelled by plane until 1900 is analogous to no people having been killed in terrorist attacks, which would set an even lower prior probability of human extinction due to a terrorist attack (I would just be extrapolating based on e.g. 50 or so 0s, respecting 50 or so years of no terrorist attack deaths), whereas apparently <a href=\"https://news.gallup.com/poll/1579/Airlines.aspx?utm_source=genericbutton&amp;utm_medium=organic&amp;utm_campaign=sharing#:~:text=Dec%202%2D6-,55,-25\">45 %</a> of people in the US travelled by plane in 2015.</p><p>However, in the absence of data on people travelling by plane, it would make sense to use other reference class instead of extrapolating based on a bunch of 0s. Once one used an appropriate reference class, it is possible lots of people travelling now by plane does not seem so surprising. In addition, one may be falling prey to <a href=\"https://en.wikipedia.org/wiki/Hindsight_bias\">hindsight bias</a> to some extent. Maybe so many people travelling by plane (e.g. instead of having more remote work) was not that likely ex ante.</p>", "parentCommentId": "4AETr9RDEndzr95HC", "user": {"username": "vascoamaralgrilo"}}, {"_id": "DYTh3YuD3XCcusbbe", "postedAt": "2023-12-03T10:04:37.694Z", "postId": "M6zAzCBAsBxem7Lu4", "htmlBody": "<p>Great points, and thanks for the reading suggestions, Ben! I am also happy to know you plan to publish a report describing your findings.</p><p>I qualitatively agree with everything you have said. However, I would like to see a detailed quantitative model estimating AI or bio extinction risk (which handled well <a href=\"https://forum.effectivealtruism.org/topics/information-hazard\">infohazards</a>). Otherwise, I am left wondering about how much higher extinction risk will become accounting not only for increased capabilities, but also increased safety.</p><blockquote><p>On a meta-level, the fact that XPT superforecasters are so much higher than what your model outputs suggests that they also think the right reference class approach is OOMs higher. And this is despite my suspicion that the XPT supers are too low and too indexed on past base-rates.</p></blockquote><p>To clarify, my best guess is also many OOMs higher than the headline number of my post. I think XPT's superforecaster <a href=\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64f0a7838ccbf43b6b5ee40c/1693493128111/XPT.pdf\">prediction</a> of 0.01 % human extinction risk due an engineered pathogen by 2100 (Table 3) is reasonable.</p><p>However, I wonder whether superforecasters are overestimating the risk because their nuclear extinction risk by 2100 of 0.074 % seems way too high. I <a href=\"https://forum.effectivealtruism.org/posts/gktZ8zuzyh7HEgjfc/famine-deaths-due-to-the-climatic-effects-of-nuclear-war?commentId=HDWwNtF2STuf7a5gd\">estimated</a> a 0.130 % chance of a nuclear war before 2050 leading to an injection of soot into the stratosphere of at least 47 Tg, so around 0.39 % (= 0.00130*75/25) before 2100. So, for the superforecasters to be right, extinction conditional on at least 47 Tg would have to be around 20 % (= 0.074/0.39) likely. This appears extremely pessimistic. From <a href=\"https://www.nature.com/articles/s43016-022-00573-0\">Xia 2022</a> (see top tick in the 3rd bar from the right in Fig. 5a):</p><blockquote><p>With the most optimistic case\u2014100% livestock crop feed to humans, no household waste and equitable global food distribution\ufeff\u2014there would be enough food production for everyone under the 47\u2009Tg case.</p></blockquote><p>This scenario is the most optimistic in Xia 2022, but it is pessimist in a number of ways (search for \"High:\" <a href=\"https://forum.effectivealtruism.org/posts/gktZ8zuzyh7HEgjfc/famine-deaths-due-to-the-climatic-effects-of-nuclear-war#Famine_death_rate_due_to_the_climatic_effects_of_large_nuclear_war\">here</a>):</p><ul><li>\u201cScenarios assume that all stored food is consumed in Year 1\u201d, i.e. no <a href=\"https://en.wikipedia.org/wiki/Rationing\"><u>rationing</u></a>.</li><li>\u201cWe do not consider farm-management adaptations such as changes in cultivar selection, switching to more cold-tolerating crops or greenhouses31 and alternative food sources such as mushrooms, seaweed, methane single cell protein, insects32, hydrogen single cell protein33 and cellulosic sugar34\u201d.</li><li>\u201cLarge-scale use of alternative foods, requiring little-to-no light to grow in a cold environment38, has not been considered but could be a lifesaving source of emergency food if such production systems were operational\u201d.</li><li>\u201cByproducts of biofuel have been added to livestock feed and waste27. Therefore, we add only the calories from the final product of biofuel in our calculations\u201d. However, it would have been better to redirect to humans the crops used to produce biofuels.</li></ul><p>So 20 % chance of extinction conditional on at least 47 Tg does sound very high to me, which makes me think superforecasters are overestimating nuclear extinction risk quite a lot. This in turn makes me wonder whether they are also overestimating other risks which I have investigated less.</p><blockquote><p>So overall, compared to the threat model of future bio x-risk, I think the empirical track record of terrorism is too weak (point 1)</p></blockquote><p>Nitpick. I think you meant bioterrorism, not terrorism which includes more data.</p>", "parentCommentId": "S9pWghFMcgdkf2vYr", "user": {"username": "vascoamaralgrilo"}}, {"_id": "BPmgYkrKTn4W4FjZy", "postedAt": "2023-12-04T00:44:09.710Z", "postId": "M6zAzCBAsBxem7Lu4", "htmlBody": "<blockquote><p>Nitpick. I think you meant bioterrorism, not terrorism which includes more data.</p></blockquote><p>Thanks! Fixed.<br><br>I don't know the nuclear field well, so don't have much to add. If I'm following your comment though, it seems like you have your own estimate of the chance of nuclear war raising 47+ Tg &nbsp;of soot, and on the basis of that infer the implied probability supers give to extinction conditional on such a war. Why not instead infer that supers have a higher forecast of nuclear war than your 0.39% by 2100? E.g. a ~1.6% chance of nuclear war with 47+ Tg and a 5% chance of extinction conditional on it.&nbsp;I may be misunderstanding your comment. Though to be clear, I think it's very possible the supers were not thinking things through in similar detail to you - there were a fair number of questions in the XPT.<br>&nbsp;</p><blockquote><p>I am left wondering about how much higher extinction risk will become accounting not only for increased capabilities, but also increased safety</p></blockquote><p>I don't think I follow this sentence? Is it that one might expect future advances in defensive biotech/other tech to counterbalance offensive tech development, and that without a detailed quant model you expect the defensive side to be under-counted?</p>", "parentCommentId": "DYTh3YuD3XCcusbbe", "user": {"username": "BenStewart"}}, {"_id": "RL8b6hdWAJDvfkD7r", "postedAt": "2023-12-04T10:42:34.167Z", "postId": "M6zAzCBAsBxem7Lu4", "htmlBody": "<blockquote><p>Why not instead infer that supers have a higher forecast of nuclear war than your 0.39% by 2100? E.g. a ~1.6% chance of nuclear war with 47+ Tg and a 5% chance of extinction conditional on it.</p></blockquote><p>Fair point! Here is another way of putting my point. I <a href=\"https://forum.effectivealtruism.org/posts/gktZ8zuzyh7HEgjfc/famine-deaths-due-to-the-climatic-effects-of-nuclear-war#Longterm_perspective\">estimated</a> a probability of 3.29*10^-6 for a 50 % population loss due to the climatic effects of nuclear war before 2050, so around 0.001 % (= 3.29*10^-6*75/25) before 2100. Superforecasters' 0.074 % nuclear extinction risk before 2100 is 74 times my risk for a 50 % population loss due to climatic effects. My estimate may be off to some extent, and I only focussed on the climatic effects, not the indirect deaths caused by infrastructure destruction, but my best guess has to be many OOMs off for superforecasters prediction to be in the right OOM. This makes me believe superforecasters' are overestimating nuclear extinction risk.</p><blockquote><p>Is it that one might expect future advances in defensive biotech/other tech to counterbalance offensive tech development [?]</p></blockquote><p>Yes, in the same way that the risk of global warming is often overestimated due to neglecting adaptation.</p><blockquote><p>without a detailed quant model you expect the defensive side to be under-counted?</p></blockquote><p>I expect the defensive side to be under-counted, but not necessarily due to lack of quantitative models. However, I think using quantitative models makes it less likely that the defensive side is under-counted. I have not thought much about this; I am just expressing my intuitions.</p>", "parentCommentId": "BPmgYkrKTn4W4FjZy", "user": {"username": "vascoamaralgrilo"}}]