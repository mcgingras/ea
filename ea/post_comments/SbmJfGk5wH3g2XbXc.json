[{"_id": "5dALq58bparJdCPR3", "postedAt": "2014-08-06T21:21:00.000Z", "postId": "SbmJfGk5wH3g2XbXc", "htmlBody": "<p>&quot;Reducing astronomical waste need not involve preventing human extinction\u2014it can involve other changes in humanity\u2019s long-term trajectory.&quot;</p><p>Glad to see this gaining more traction in the x-risk community!</p>", "parentCommentId": null, "user": {"username": "Jacy_Anthis2"}}, {"_id": "TvENzgYcv7w22rfa7", "postedAt": "2014-08-07T07:54:00.000Z", "postId": "SbmJfGk5wH3g2XbXc", "htmlBody": "<p>Nice post. It's also worth noting that this version of the far-future argument appeals even to negative utilitarians, strongly anti-suffering prioritarians, Buddhists, antinatalists, and others who don't think it's important to create new lives for reasons other than holding a person-affecting view.</p><p>\nI also think even if you want to create lots of happy lives, most of the relevant ways to tackle that problem involve changing the direction in which the future goes rather than whether there is a future. The most likely so-called &quot;extinction&quot; event in my mind is human replacement by AIs, but AIs would be their own life forms with their own complex galaxy-colonization efforts, so I think work on AI issues should be considered part of &quot;changing the direction of the future&quot; rather than &quot;making sure there is a future&quot;.</p>", "parentCommentId": null, "user": {"username": "Brian_Tomasik"}}, {"_id": "tgmN8a4AQ49vtEDFQ", "postedAt": "2014-08-08T02:27:00.000Z", "postId": "SbmJfGk5wH3g2XbXc", "htmlBody": "<p>Thanks Nick. I like the abstraction to see precisely which features allow you to run these arguments.</p><p>\nAlthough my best guess agrees with it, I am a little more hesitant about the principle of scale than you are. There are some reasons for scepticism:</p><p>\n1) Very many population axiologies reject it. Indeed it looks as though it will cut somewhere close to where a suitable separability axiom would -- which already gets you to summing utility functions (not necessarily preference-based ones). But perhaps I'm wrong about quite where it cuts; it could be interesting to explore this.</p><p>\n2) As well as doing the work in this argument, the principle of scale is a key part of what can make you vulnerable to Pascal's Mugging. I'd hope we can resolve that without giving up this principle, but I don't think it's entirely settled.</p><p>\n3) You say you see no great justification for the principle to break down when large numbers are at stake. But when not-so-large numbers are at stake, there are very compelling justifications to endorse the principle (and not just for improving quality of life). And these reasons do apply for a larger range of ethical views than would agree with it at large scale. So you might think that you only believed it for these reasons, and have no reason to support it in their absence.</p>", "parentCommentId": null, "user": {"username": "Owen_Cotton-Barratt"}}, {"_id": "uAonT36ExTfE4x3WQ", "postedAt": "2014-08-08T12:35:00.000Z", "postId": "SbmJfGk5wH3g2XbXc", "htmlBody": "<p>Re 1, yes it is philosophically controversial, but it also does speak to people with a number of different axiologies, as Brian Tomasik points out in another comment. One way to frame it is that it's doing what separability does in my dissertation, but noticing that astronomical waste can run without making assumptions about the value of creating extra people. So you could think of it as running that argument with one less premise.</p><p>Re 2, yes it pushes in an unbounded utility function direction, and that's relevant if your preferred resolution of Pascal's Mugging is to have a bounded utility function. But this is also a problem for standard presentations of the astronomical waste argument. As it happens, I think you can run stuff like astronomical waste with bounded utility functions. Matt Wage has some nice stuff about this in his senior thesis, and I think Carl Shulman has a forthcoming post which makes some similar points. I think astronomical waste can be defended from more perspectives than it has been in the past, and it's good to show that. This post is part of that project.</p><p>Re 3, I'd frame this way, &quot;We use this all the time and it's great in ordinary situations. I'm doing the natural extrapolation to strange situations.&quot; Yes, it might break down in weird situations, but it's the extrapolation I'd put most weight on.</p>", "parentCommentId": "tgmN8a4AQ49vtEDFQ", "user": {"username": "Nick_Beckstead"}}, {"_id": "FFL2da4YFef6dEsDd", "postedAt": "2014-08-08T14:01:00.000Z", "postId": "SbmJfGk5wH3g2XbXc", "htmlBody": "<p>I think it's an open question whether &quot;even if you want to create lots of happy lives, most of the relevant ways to tackle that problem involve changing the direction in which the future goes rather than whether there is a future.&quot; But I broadly agree with the other points. In a recent talk on astronomical waste stuff, I recommended thinking about AI in the category of &quot;long-term technological/cultural path dependence/lock in,&quot; rather than the GCR category (though that wasn't the main point of the talk). Link here: <a href=\"http://www.gooddoneright.com/#!nick-beckstead/cxpp\">http://www.gooddoneright.com/#!nick-beckstead/cxpp</a>, see slide 13.</p>\n", "parentCommentId": "TvENzgYcv7w22rfa7", "user": {"username": "Nick_Beckstead"}}, {"_id": "F92FSHnQLNoZZ3XFd", "postedAt": "2014-08-08T15:56:00.000Z", "postId": "SbmJfGk5wH3g2XbXc", "htmlBody": "<p>Yes, I really like this work in terms of pruning the premises. Which is why I'm digging into how firm those premises really are (even if I personally tend to believe them).</p><p>\nIt seems like the principle of scale is in fact implied by separability. I'd guess it's rather weaker, but I don't know of any well-defined examples which accept scale but not separability.</p><p>\nI do find your framing of 3 a little suspect. When we have a solid explanation for just why it's great in ordinary situations, and we can see that this explanation doesn't apply in strange situations, it seems like the extrapolation shouldn't get too much weight. Actually most of my weight for believing the principle of scale comes the fact that it's a consequence of separability.</p><p>\nOne more way the principle might break down:</p><p>\n4) You might accept the principle for helping people at a given time, but not as a way of comparing between helping people at different times. </p><p>\nIndeed in this case it's not so clear most people would accept the small-scale version (probably because intuitions are driven by factors such as improving lives earlier gets more time to have indirect effects acting to improve lives later).</p>", "parentCommentId": "uAonT36ExTfE4x3WQ", "user": {"username": "Owen_Cotton-Barratt"}}, {"_id": "v7ogACXTfqSQ22AS2", "postedAt": "2014-08-14T15:59:00.000Z", "postId": "SbmJfGk5wH3g2XbXc", "htmlBody": "<p>Assuming I'm understanding the principle of scale correctly, I would have thought that the Average View is an example of something where Scale holds, but Separability fails. As it seems that whenever Scale is applied, the population is the same size in both cases (via a suppressed other-things-equal clause).</p>\n", "parentCommentId": "F92FSHnQLNoZZ3XFd", "user": {"username": "Toby_Ord"}}, {"_id": "EGc75nCdAkprjiJeT", "postedAt": "2014-08-14T16:17:00.000Z", "postId": "SbmJfGk5wH3g2XbXc", "htmlBody": "<p>Yes, good example.</p>\n", "parentCommentId": "v7ogACXTfqSQ22AS2", "user": {"username": "Owen_Cotton-Barratt"}}]