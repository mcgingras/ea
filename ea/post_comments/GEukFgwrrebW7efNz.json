[{"_id": "HYcZJqTz3LNXdR6CS", "postedAt": "2022-10-07T05:27:37.267Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "", "parentCommentId": null, "user": {"username": "trevorw96"}}, {"_id": "GSgARa9PQQmpHxvC9", "postedAt": "2022-10-07T06:09:56.250Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<p>If an alignment-minded person is currently doing capabilities work under the assumption that they'd be replaced by an equally (or more) capable researcher less concerned about alignment, I think that's badly mistaken. &nbsp;The number of people actually pushing the frontier forward is not all that large. &nbsp;Researchers at that level are not fungible; the differences between the first-best and second-best available candidates for roles like that are often quite large. &nbsp;The framing of an arms race is mistaken; the prize for \"winning\" is that you die sooner. &nbsp;Dying later is better. &nbsp;If you're in a position like that I'd be happy to talk to you, or arrange for you to talk to another member of the Lightcone team.</p><p>I do not significantly credit the possibility that Google (or equivalent) will try to make life difficult for people who manage to successfully convince the marginal capabilities researcher to switch tracks, absent evidence. &nbsp;I agree that historical examples of vaguely similar things exist, but the ones I'm familiar with don't seem analogous, and we do in fact have fairly strong evidence about the kinds of antics that various megacorps get up to, which seem to be strongly predicted by their internal culture.</p>", "parentCommentId": "HYcZJqTz3LNXdR6CS", "user": {"username": "T3t"}}, {"_id": "aCBo6wiHT2myNsqxy", "postedAt": "2022-10-07T08:49:58.176Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<blockquote>\n<p>Following Eliezer, I think of an AGI as \"safe\" if deploying it carries no more than a 50% chance of killing more than a billion people</p>\n</blockquote>\n<p>Is this 50% from a the point of view of some hypothetical person who knows as much as is practical about this AGI\u2019s consequences, or from your point of view or something else?</p>\n<p>Do you imagine that deploying two such AGIs in parallel universes with some minor random differences has only a 25% chance of them both killing more than a billion people?</p>\n", "parentCommentId": null, "user": {"username": "David Johnston"}}, {"_id": "FfAQHxCwzRLZvdo8p", "postedAt": "2022-10-07T08:51:21.390Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<p>I like the term <strong>AGI x-safety</strong>, to get across the fact that you are talking about safety from <i>existential</i> (x) catastrophe, and sophisticated AI. \"AI Safety\" can be conflated with more mundane risks from AI (e.g. isolated incidents with robots, self-driving car crashes etc). &nbsp;And \"AI Alignment\" is only part of the problem. Governance is also required to implement aligned AI and prevent unaligned AI.</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "TZfeqzzeQdqR3ocAi", "postedAt": "2022-10-07T08:53:47.762Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<p>Perhaps ASI x-safety would be even better though (the SI being SuperIntelligent), if people are thinking \u201cwe win if we can build a useless-but-safe AGI\u201d.&nbsp;</p>", "parentCommentId": "FfAQHxCwzRLZvdo8p", "user": {"username": "Greg_Colbourn"}}, {"_id": "tA2uAKMi8QBXjmm32", "postedAt": "2022-10-07T14:59:37.918Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<p>Good post, thank you.<br><br>\"or other such nonsense that advocates never taking on risks even when the benefits clearly dominate\"<br><br>An important point to note here - the people who suffer the risks and the people who reap the benefits are very rarely the same group. Deciding to use an unsafe AI system (whether presently or in the far future) using a risks/benefits analysis goes wrong so often because one man's risk is another's benefit.<br><br>Example: The risk of lung damage from traditional coal mining compared to the industrial value of the coal is a very different risk/reward analysis for the miner and the mine owner. Same with AI.</p>", "parentCommentId": null, "user": {"username": "Luke Chambers"}}, {"_id": "uHGFxpKgjKyo5L4Gu", "postedAt": "2022-10-07T17:31:37.330Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<p>I'd guess not. From my perspective, humanity's bottleneck is almost entirely that we're clueless about alignment. If a meme adds muddle and misunderstanding, then it will be harder to get a critical mass of researchers who are <i>extremely</i> reasonable about alignment, and therefore harder to solve the problem.</p><p>It's hard for muddle and misinformation to spread in exactly the right way to offset those costs; and attempting to strategically sow misinformation so will tend to erode our ability to think well and to trust each other.</p>", "parentCommentId": "TZfeqzzeQdqR3ocAi", "user": {"username": "RobBensinger"}}, {"_id": "vhm5KKPgfNqKgTRSz", "postedAt": "2022-10-07T19:24:33.796Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<p>I don't think it's a good plan to build an AI that enacts some pivotal act ensuring that nobody ever builds a misaligned AGI. See Critch <a href=\"https://forum.effectivealtruism.org/posts/q6t5zKCg5peZA92Zu/pivotal-act-intentions-negative-consequences-and-fallacious\">here</a> and <a href=\"https://forum.effectivealtruism.org/posts/vqX25ML2vBN6cvmkx/pivotal-outcomes-and-pivotal-processes\">here</a>. When I think about building AI that is safe, I think about multiple layers of safety including monitoring, robustness, alignment, and deployment. Safety is not a single system that doesn't destroy the world; it's an ongoing process that prevents bad outcomes. &nbsp;See Hendrycks <a href=\"https://arxiv.org/pdf/2109.13916.pdf\">here</a> and <a href=\"https://arxiv.org/pdf/2109.13916\">here</a>.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Aidan O'Gara"}}, {"_id": "MWhziDDkpPbgYMra7", "postedAt": "2022-10-08T04:53:34.490Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<p>Unfortunately, people (and this includes AI researchers) tend to hear what they want to hear, and not what they don't want to hear. What to call this field is extremely dependent on the nature of those misinterpretations. And the biggest misinterpretation right now does not appear to be \"oh so I guess we need to build impotent systems because they'll be safe\".</p>\n<p>\"Alignment\" is already broken, in my view. You allude to this, but I want to underscore it. Instruct GPT was billed as \"alignment\". Maybe it is, but it doesn't seem to do any good for reducing x risk.</p>\n<p>\"Safety\", too, lends itself to misinterpretation. Sometimes of the form \"ok, so let's make the self-driving cars not crash\". So you're not starting from an ideal place. But at least you're starting from a place of AI systems behaving badly in ways you didn't intend and causing harm. From there, it's easier to explain existential safety as simply an extreme safety hazard, and one that's not even unlikely.</p>\n<p>If you tell people \"produce long term near optimal outcomes\" and they are EAs or rationalists, they probably understand what you mean. If they are random AI researchers, this is so vague as to be completely meaningless. They will fill it in with whatever they want. The ones who think this means full steam ahead toward techno utopia will think that. The ones who think this means making AI systems not misclassify images in racist ways will think that. The ones who think it means making AI systems output fake explanations for their reasoning will think that.</p>\n<p>Everyone wants to make AI produce good outcomes. And you do not need to convince the vast majority of researchers to work on AI capabilities. They just do it anyway. Many of them don't even do it for ideological reasons, they do it because it's cool!</p>\n<p>The differential thing we need to be pushing on is AI not creating an existential catastrophe. In public messaging (and what is a name except public messaging?) we do not need to distract with other considerations at this present moment. And right now, I don't think we have a better term than safety that points in that direction.</p>\n", "parentCommentId": null, "user": {"username": "ThomasWoodside"}}, {"_id": "fCaFZrAN8E3poAggx", "postedAt": "2022-10-08T10:28:57.232Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<p>I'm not sure I get your point here. Surely the terms \"AI Safety\" and \"AI Alignment\" are already causing muddle and misunderstanding? I'm saying we should be more specific in our naming of the problem.</p>\n", "parentCommentId": "uHGFxpKgjKyo5L4Gu", "user": {"username": "Greg_Colbourn"}}, {"_id": "nevNJJ3wXnvF9hseS", "postedAt": "2022-10-08T12:36:18.513Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<p>I'd be interested in the historical record for similar industries, could you quickly list some examples that come to your mind? No need to elaborate much.</p>", "parentCommentId": "HYcZJqTz3LNXdR6CS", "user": {"username": "Leksu"}}, {"_id": "zKhHhDtwPKB8XPM8E", "postedAt": "2022-10-08T13:23:00.849Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<p>(After writing this I thought of one example where the goals are in conflict: permanent surveillance that stops the development of advanced AI systems. Thought I'd still post this in case others have similar thoughts. Would also be interested in hearing other examples.)</p><p>&nbsp;</p><p>I'm assuming a reasonable interpretation of the proxy goal of safety means roughly this: \"be reasonably sure that we can prevent AI systems we expect to be built from causing harm\". Is this a good interpretation? If so, when is this proxy goal in conflict with the goal of having \"things go great in the long run\"?</p><p>I agree that it's epistemically good for people to not confuse proxy goals with goals, but in practice I have trouble thinking of situations where these two are in conflict. If we've ever succeeded in the first goal, it seems like making progress in the second goal should be much easier, and at that point it would make more sense to advocate using-AI-to-bring-a-good-future-ism.</p><p>&nbsp;</p><p>Focusing on the proxy goal of AI safety seems also good for the reason that it makes sense across many moral views, while people are going to have different thoughts on what it means for things to \"go great in the long run\". Fleshing out those disagreements is important, but I would think there's time to do that when we're in a period of lower existential risk.</p>", "parentCommentId": null, "user": {"username": "Leksu"}}, {"_id": "HaFrGDqEpB4yoxX6n", "postedAt": "2022-10-08T17:14:10.403Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<p>Economic degrowth/stagnation is another example of something that prevents AI doom but will be very bad to have in the long run.</p>", "parentCommentId": "zKhHhDtwPKB8XPM8E", "user": {"username": "Linch"}}, {"_id": "Cnt6tMructhggxX3P", "postedAt": "2022-10-09T00:36:31.057Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<p>\"ASI x-safety\" might be a better term for other reasons (though Nate <a href=\"https://twitter.com/So8res/status/1578475159719407616\">objects to it here</a>), but by default, I don't think we should be influenced in our terminology decisions by 'term T will cause some alignment researchers to have falser beliefs and pursue dumb-but-harmless strategies, and maybe this will be good'. (Or, by default this should be a reason <i>not </i>to adopt terminology.)</p><p>Whether current terms cause muddle and misunderstanding doesn't change my view on this. In that case, IMO we should consider changing to a new term in order to reduce muddle and misunderstanding. We shouldn't strategically confuse and mislead people in a new direction, just because we accidentally confused or misled people in the past.</p>", "parentCommentId": "fCaFZrAN8E3poAggx", "user": {"username": "RobBensinger"}}, {"_id": "gP9xCkRs8ZtesLngp", "postedAt": "2022-10-09T01:06:57.958Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<blockquote><p>I like the term <strong>AGI x-safety</strong></p></blockquote><p>A lot of people misunderstand \"existential risk\" as meaning something like \"extinction risk\", rather than as meaning 'anything that would make the future go way worse than it optimally could have'. Tacking on \"safety\" might contribute to that impression; we're still making it <i>sound</i> like the goal is just to prevent bad things (be it at the level of an individual AGI project, or at the level of the world), leaving out the \"cause good things\" part. That said, \"existential safety\" seems better than \"safety\" to me.</p><p>(<a href=\"https://twitter.com/So8res/status/1578475159719407616\">Nate's thoughts here.</a>)</p><blockquote><p>And \"AI Alignment\" is only part of the problem. Governance is also required to implement aligned AI and prevent unaligned AI.</p></blockquote><p>I don't know what you mean by \"governance\". The <a href=\"https://forum.effectivealtruism.org/topics/ai-governance\">EA Forum wiki</a> currently defines it as:</p><blockquote><p><strong>AI governance</strong> (or the <strong>governance of artificial intelligence</strong>)<strong> </strong>is the study of norms, policies, and institutions that can help humanity navigate the transition to a world with advanced <a href=\"https://forum.effectivealtruism.org/topics/artificial-intelligence\">artificial intelligence</a>. This includes a broad range of subjects, from global coordination around regulating AI development to providing incentives for corporations to be more cautious in their AI research.</p></blockquote><p>... which makes it sound like governance ignores plans like \"just build a really good company and save the world\". If I had to guess, I'd guess that the world is likeliest to be saved because an <a href=\"https://www.lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects\">adequate organization</a> existed, with excellent <i>internal</i> norms, policies, talent, and insight. Shaping external incentives, regulations, etc. can help on the margin, but it's a sufficiently blunt instrument that it can't carve orgs into the exact right shape required for the problem structure.</p><p>It's possible that the adequate organization is a government, but this seems less likely to me given the absolute number of exceptionally competent governments in history, and given that govs seem to play little role in ML progress today.</p><p>Open Phil's <a href=\"https://www.openphilanthropy.org/research/our-ai-governance-grantmaking-so-far/#Our_priorities_within_AI_governance\">definition</a> is a bit different:</p><blockquote><p>By AI governance we mean local and global norms, policies, laws, processes, politics, and institutions (not just governments) that will affect social outcomes from the development and deployment of AI systems.</p></blockquote><p>Open Phil goes out of its way to say \"not just governments\", but its list (\"norms, policies, laws, processes, politics, and institutions\") still makes it sound like the problem is shaped more like 'design a nuclear nonproliferation treaty' and less like 'figure out how to build an adequate organization', 'cause there to exist such an organization', or the various activities involved in actually running such an organization and steering it to an existential success.</p><p>Both sorts of activities seem useful to me, but dividing the problem into \"alignment\" and \"governance\" seems weird to me on the above framings\u2014like we're going out of our way to cut skew to reality.</p><p>On my model, the proliferation of AGI tech destroys the world, as a very strong default. We need some way to prevent this proliferation, even though AGI is easily-copied software. The strategies seem to be:</p><ol><li>Using current tech, limit the proliferation of AGI indefinitely. (E.g., by creating a stable, long-term global ban on GPUs outside of some centralized AGI collaboration, paired with pervasive global monitoring and enforcement.)</li><li>Use early AGI tech to limit AGI proliferation.</li><li>Develop other, non-AGI tech (e.g., whole-brain emulation and/or nanotech) and use it to limit AGI proliferation.</li></ol><p>1 sounds the most like \"AGI governance\" to my ear, and seems impossible to me, though there might be more modest ways to improve coordination and slow progress (thus, e.g., buying a little more time for researchers to figure out how to do 2 or 3). 2 and 3 both seem promising to me, and seem more like tech that could enable a long (or short) reflection, since e.g. they could also help ensure that humanity never blows itself up with other technologies, such as bio-weapons.</p><p>Within 2, it seems to me that there are three direct inputs to things going well:</p><ul><li><strong>Target selection: </strong>You've chosen a specific set of tasks for the AGI that will somehow (paired with a specific set of human actions) result in AGI nonproliferation.</li><li><strong>Capabilities</strong>: The AGI is powerful enough to succeed in the target task. (E.g., if the best way to save the world is by building fast-running WBE, you have AGI capable enough to do that.)</li><li><strong>Alignment</strong>: You are able to reliably direct the AGI's cognition at that specific target, without any catastrophic side-effects.</li></ul><p>There's then a larger pool of enabling work that helps with one or more of those inputs: figuring out what sorts of organizations to build; building and running those organizations; recruiting, networking, propagating information; prioritizing and allocating resources; understanding key features of the world at large, like tech forecasting, social dynamics, and the current makeup of the field; etc.</p><p>\"In addition to alignment, you also need to figure out target selection, capabilities, and (list of enabling activities)\" seems clear to me. And also, you might be able to side-step alignment if 3 (or 1) is viable. \"Moreover, <a href=\"https://www.lesswrong.com/posts/DJRe5obJd7kqCkvRr/don-t-leave-your-fingerprints-on-the-future\">you need a way to hand back the steering wheel and hand things off to a reasonable decision-making process</a>\" seems clear to me as well. \"In addition to alignment, you also need governance\" is a more opaque-to-me statement, so I'd want to hear more concrete details about what that means before saying \"yeah, of course you need governance too\".</p>", "parentCommentId": "FfAQHxCwzRLZvdo8p", "user": {"username": "RobBensinger"}}, {"_id": "BCYHq2nX2awdvM7Rt", "postedAt": "2022-10-10T10:00:45.057Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<blockquote><p>'design a nuclear nonproliferation treaty'</p></blockquote><p>This is along the lines of what I'm thinking when I say AGI Governance. The <a href=\"https://worldbuild.ai/W-0000000281/\">scenario</a> outlined by the winner of FLI's World Building Contest is an optimistic vision of this.</p><blockquote><p>2. Use early AGI tech to limit AGI proliferation.</p></blockquote><p>This sounds like something to be done unilaterally, as per the 'pivotal act' that MIRI folk talk about. To me it seems like such a thing is pretty much as impossible as safely fully aligning an AGI, so working towards doing it unilaterally seems pretty dangerous. Not least for its potential role in exacerbating race dynamics. Maybe the world will be ended by a hubristic team who are convinced that their AGI is safe enough to perform such a pivotal act, and that they need to run it because another team is very close to unleashing their potentially world ending un-aligned AGI.&nbsp;Or by another team seeing all the GPUs starting to melt and pressing go on their (still-not-fully-aligned) AGI... It's like <a href=\"https://en.wikipedia.org/wiki/Mutual_assured_destruction\">MAD</a>, but for well intentioned would-be world-savers.<br><br>I think your view of AGI governance is idiosyncratic because of thinking in such unilateralist terms. Maybe it could be a move that leads to (the world) winning, but I think that even though effective broad global-scale governance of AGI might seem insurmountable, it's a better shot. See also aogara's <a href=\"https://forum.effectivealtruism.org/posts/GEukFgwrrebW7efNz/what-does-it-mean-for-an-agi-to-be-safe-1?commentId=vhm5KKPgfNqKgTRSz\">comment</a> and its links.</p>", "parentCommentId": "gP9xCkRs8ZtesLngp", "user": {"username": "Greg_Colbourn"}}, {"_id": "ktjENXLvimniwtNKm", "postedAt": "2022-10-10T10:03:35.814Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<p>What are some better options? Or, what are your current favourites?</p>", "parentCommentId": "Cnt6tMructhggxX3P", "user": {"username": "Greg_Colbourn"}}, {"_id": "2SYzLwMTJqhP688Gy", "postedAt": "2022-10-15T16:12:48.753Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<p>\"AGI existential safety\" seems like the most popular relatively-unambiguous term for \"making the AGI transition go well\", so I'm fine with using it until we find a better term.</p><p>I think \"AI alignment\" is a good term for the technical side of differentially producing good outcomes from AI, though it's an imperfect term insofar as it collides with Stuart Russell's \"value alignment\" and Paul Christiano's \"intent alignment\". (The latter, at least, better subsumes a lot of the core challenges in making AI go well.)</p>", "parentCommentId": "ktjENXLvimniwtNKm", "user": {"username": "RobBensinger"}}, {"_id": "KoTBjkRfWk6oYHNva", "postedAt": "2022-10-15T17:20:55.261Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<blockquote><p>I don't think it's a good plan to build an AI that enacts some pivotal act ensuring that nobody ever builds a misaligned AGI. See Critch <a href=\"https://forum.effectivealtruism.org/posts/q6t5zKCg5peZA92Zu/pivotal-act-intentions-negative-consequences-and-fallacious\">here</a> and <a href=\"https://forum.effectivealtruism.org/posts/vqX25ML2vBN6cvmkx/pivotal-outcomes-and-pivotal-processes\">here</a>.</p></blockquote><p>My reply to Critch is <a href=\"https://forum.effectivealtruism.org/posts/vqX25ML2vBN6cvmkx/pivotal-outcomes-and-pivotal-processes?commentId=M7eMi4tsXtv5NDqWP\">here</a>, and Eliezer's is <a href=\"https://www.lesswrong.com/posts/etNJcXCsKC6izQQZj/pivotal-outcomes-and-pivotal-processes?commentId=denytBnTnKadLGjwt\">here</a> and <a href=\"https://www.lesswrong.com/posts/Jo89KvfAs9z7owoZp/pivotal-act-intentions-negative-consequences-and-fallacious?commentId=7wovrz8eWMBbiRkFR\">here</a>.</p><p>I'd also point to Scott Alexander's <a href=\"https://www.lesswrong.com/posts/Jo89KvfAs9z7owoZp/pivotal-act-intentions-negative-consequences-and-fallacious?commentId=sYyrcRgcFLYdgPRLd\">comment</a>, Nate's \"<a href=\"https://www.lesswrong.com/posts/DJRe5obJd7kqCkvRr/don-t-leave-your-fingerprints-on-the-future\">Don't leave your fingerprints on the future</a>\", and <a href=\"https://www.lesswrong.com/posts/Jo89KvfAs9z7owoZp/pivotal-act-intentions-negative-consequences-and-fallacious?commentId=MJugAcvCB8P4Y88Zi\">my</a>:</p><blockquote><p>\"<i>Also, it's a little odd if you end up making decisions that have huge impacts on the rest of humanity that they had no say in; from a certain perspective that is inappropriate for you to do.</i>\"</p><p>Keep in mind that 'this policy seems a little odd' is a very small cost to pay relative to 'every human being dies and all of the potential value of the future is lost'. A fire department isn't a government, and there are cases where you should put out an immediate fire and <i>then</i> get everyone's input, rather than putting the fire-extinguishing protocol to a vote while the building continues to burn down in front of you. (This seems entirely compatible with the OP to me; 'governments should be involved' doesn't entail 'government responses should be put to direct population-wide vote by non-experts'.)</p><p>Specifically, when I say 'put out the fire' I'm talking about 'prevent something from killing all humans in the near future'; I'm not saying 'solve all of humanity's urgent problems, e.g., end cancer and hunger'. That's urgent, but it's a qualitatively different sort of urgency. (Delaying a cancer cure by two years would be an incredible tragedy on a human scale, but it's a rounding error in a discussion of astronomical scales of impact.)</p></blockquote><p>What, concretely, do you think humanity should do as an alternative to \"build an AI that enacts some pivotal act ensuring that nobody ever builds a misaligned AGI\"? If you aren't sure, then what's an example of an approach that seems relatively promising to you? What's a concrete scenario where you imagine things going well in the long run?</p><p>To sharpen the question: Eventually, as compute becomes more available and AGI techniques become more efficient, we should expect that individual consumers will be able to train an AGI that destroys the world using the amount of compute on a mass-marketed personal computer. (If the world wasn't already destroyed before that.) What's the likeliest way you expect this outcome to be prevented, or (if you don't think it ought to be prevented, or don't think it's preventable) the likeliest way you expect things to go well if this outcome isn't prevented?</p><p>(If your answer is \"I think this will never happen no matter how far human technology advances\" and \"in particular, the probability seems low enough to me that we should just write off those worlds and be willing to die in them, in exchange for better focusing on the more-likely world where [scenario] is true instead\", then I'd encourage saying that explicitly.)</p><blockquote><p>When I think about building AI that is safe, I think about multiple layers of safety including monitoring, robustness, alignment, and deployment.</p></blockquote><p>At that level of abstraction, I'd agree! Dan defines robustness as \"create models that are resilient to adversaries, unusual situations, and Black Swan events\", monitoring as \"detect malicious use, monitor predictions, and discover unexpected model functionality\", alignment as \"build models that represent and safely optimize hard-to-specify human values\", and systemic safety as \"use ML to address broader risks to how ML systems are handled, such as cyberattacks\". All of those seem required for a successful AGI-mediated pivotal act.</p><p>If this description is meant to point at a specific alternative approach, or meant to exclude pivotal acts in some way, then I'm not sure what you have in mind.</p><blockquote><p>Safety is not a single system that doesn't destroy the world; it's an ongoing process that prevents bad outcomes.</p></blockquote><p>I agree on both fronts. Destroying the world is insufficient (you need to <i>save</i> the world; we already know how to build AI systems that don't destroy the world), and a pivotal act fails if it merely delays doom, rather than indefinitely putting a pause on AGI proliferation (an \"ongoing process\", albeit one initiated by a fast discrete action to ensure no one destroys the world <i>tomorrow</i>).</p><p>But I think you mean to gesture at some class of scenarios where the \"ongoing process\" doesn't begin with a sudden discrete phase shift, and more broadly where no single actor ever uses AI to do anything sudden and important in the future. What's a high-level description of how this might realistically play out?</p><blockquote><p>See Hendrycks <a href=\"https://arxiv.org/pdf/2109.13916.pdf\">here</a> and <a href=\"https://arxiv.org/pdf/2109.13916\">here</a>.&nbsp;</p></blockquote><p>You linked to the same Hendrycks paper twice; is there another one you wanted to point at? And, is there a particular part of the paper(s) you especially wanted to highlight?</p>", "parentCommentId": "vhm5KKPgfNqKgTRSz", "user": {"username": "RobBensinger"}}, {"_id": "s3s5auf6KEYWswPL3", "postedAt": "2022-10-17T01:50:11.732Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<p>Thanks for the thoughtful response. My original comment was simply to note that some people disagree with the pivotal act framing, but it didn\u2019t really offer an alternative and I\u2019d like to engage with the problem more.</p>\n<p>I think we have a few worldview differences that drive disagreement on how to limit AI risk given solutions to technical alignment challenges. Maybe you\u2019d agree with me in some of these places, but a few candidates:</p>\n<ol>\n<li>\n<p>Stronger AI can protect us against weaker AI. When you imagine a world where anybody can train an AGI at home, you conclude that anybody will be able to destroy the world from home. I would expect that governments and corporations will maintain a sizable lead over individuals, meaning that individuals cannot take over the world. They wouldn\u2019t necessarily need to preempt the creation of an AGI; they could simply contain it afterwards, by denying it access to resources and exposing its plans for world destruction. This is especially true in worlds where intelligence alone cannot take over the world, and instead requires resources or cooperation between entities, as argued in Section C of Katja Grace\u2019s recent post. I could see somw of these proposals overlapping with your definition of a pivotal act, though I have more of a preference for multilateral and government action.</p>\n</li>\n<li>\n<p>Government AI policy can be competent. Our nuclear non-proliferation regime is strong, only 8 countries have nuclear capabilities. Gain-of-function research is a strong counter example, but the Biden administration\u2019s export controls on selling advanced semiconductors to China for national security purpose again support the idea of government competence. Strong government action seems possible with either (a) significant AI warning shots or (b) convincing mainstream ML and policy leaders of the danger of AI risk. When Critch suggested that governments build weapons to monitor and disable rogue AGI projects, Eliezer said it\u2019s not realistic but would be incredible if accomplished. Those are the kinds of proposals I\u2019d want to popularize early.</p>\n</li>\n<li>\n<p>I have longer timelines, expect a more distributed takeoff, and have a more optimistic view of the chances of human survival than I\u2019d expect you do. My plan for preventing AI x-risk is to solve the technical problems, and to convince influential people in ML and policy that the solutions must be implemented. They can then build aligned AI, and employ measures like compute controls and monitoring of large projects to ensure widespread implementation. If it turns out that my worldview is wrong and an AI lab invents a single AGI that could destroy the world relatively soon, I\u2019d be much more open to dramatic pivotal acts that I\u2019m not excited about in my mainline scenario.</p>\n</li>\n</ol>\n<p>Three more targeted replies to your comments:</p>\n<p>Your proposed pivotal act in your reply to Critch seems much more reasonable to me than \u201cburn all GPUs\u201d. I\u2019m still fuzzy on the details of how you would uncover all potential AGI projects before they get dangerous, and what you would do to stop them. Perhaps more crucially, I wouldn\u2019t be confident that we\u2019ll have AI that can run whole brain emulation of humans before we have AI that brings x-risk, because WBE would likely require experimental evidence from human brains that early advanced AI will not have.</p>\n<p>I strongly agree with the need for more honest discussions about pivotal acts / how to make AI safe. I\u2019m very concerned by the fact that people have opinions they wouldn\u2019t share, even within the AI safety community. One benefit of more open discussion could be reduced stigma around the term \u2014 my negative association comes from the framing of a single dramatic action that forever ensures our safety, perhaps via coercion. \u201cBurn all GPUs\u201d exemplifies these failure modes, but I might be more open to alternatives.</p>\n<p>I really like \u201cdon\u2019t leave you fingerprints on the future.\u201d If more dramatic pivotal acts are necessary, I\u2019d endorse that mindset.</p>\n<p>This was interesting to think about and I\u2019d be curious to answer any other questions. In particular, I\u2019m trying to think how to ensure ongoing safety in Ajeya\u2019s HFDT world. The challenge is implementation, assuming somebody has solved deceptive alignment using e.g. interpretability, adversarial training, or training strategies that exploit inductive biases. Generally speaking, I think you\u2019d have to convince the heads of Google, Facebook, and other organizations that can build AGI that these safety procedures are technically necessary. This is a tall order but not impossible. Once the leading groups are all building aligned AGIs, maybe you can promote ongoing safety either with normal policy (e.g. compute controls) or AI-assisted monitoring (your proposal or Critch\u2019s EMPs). I\u2019d like to think about this more but have to run.</p>\n", "parentCommentId": "KoTBjkRfWk6oYHNva", "user": {"username": "Aidan O'Gara"}}, {"_id": "zaqHKBHnCsr8jgRw7", "postedAt": "2022-10-19T17:24:39.132Z", "postId": "GEukFgwrrebW7efNz", "htmlBody": "<p>Perhaps using \"<a href=\"https://forum.effectivealtruism.org/posts/8mLm9bnRnEdrSE7ZE/we-should-give-extinction-risk-an-acronym?commentId=WPmWEbfpSrX2qbnbr\">doom</a>\" more could work (doom encompasses extinction, permanent curtailment of future potential, and fates worse than extinction).</p>", "parentCommentId": "2SYzLwMTJqhP688Gy", "user": {"username": "Greg_Colbourn"}}]