[{"_id": "ig3sQPqskxcZtAi4t", "postedAt": "2023-11-27T13:15:24.576Z", "postId": "Xw2JRx48oYiR7jtfs", "htmlBody": "<p><strong>Executive summary</strong>: The degree of \"slack\" in AI training refers to how intensely the process pressures models to maximize rewards versus allowing suboptimal behavior. More slack means more uncertainty about the resulting goal, while less slack increases confidence. The author leans towards preferring less slack.</p><p><strong>Key points</strong>:</p><ol><li>Slack refers to how much training ruthlessly optimizes for maximum rewards versus allowing flexibility and uncertainty. Low slack regimes pressure intense reward maximization.</li><li>Slack affects the likelihood training produces models that imperfectly pursue proxy goals not perfectly aligned with training rewards.</li><li>Slack is conceptually distinct from efforts in training to root out goal misgeneralization through adversarial techniques.</li><li>Biological reward processes like human dopamine likely involve more slack than ML training. Evolutionary selection also leaves more slack.</li><li>We can likely control the slack during training. Less slack provides more confidence about the resulting goal, while more slack risks wishful thinking.</li><li>The author leans towards preferring less slack to increase certainty about goals, but slack may vary across training stages.</li></ol><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}]